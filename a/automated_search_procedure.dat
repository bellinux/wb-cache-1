8|4240|Public
40|$|Molecular {{replacement}} (MR) generally {{becomes more}} difficult {{as the number}} of components in the asymmetric unit requiring separate MR models (i. e. the dimensionality of the search) increases. When the proportion of the total scattering contributed by each search component is small, the signal in the search for each component in isolation is weak or nonexistent. Maximum-likelihood MR functions enable complex asymmetric units to be built up from individual components with a ‘tree search with pruning ’ approach. This method, as implemented in the <b>automated</b> <b>search</b> <b>procedure</b> of the program Phaser, has been very successful in solving many previously intractable MR problems. However, {{there are a number of}} cases in which the <b>automated</b> <b>search</b> <b>procedure</b> of Phaser is suboptimal or encounters difficulties. These include cases where there are a large number of copies of the same component in the asymmetric unit or where the components of the asymmetric unit have greatly varying B factors. Two case studies are presented to illustrate how Phaser can be used to best advantage in the standard ‘automated MR ’ mode and two case studies are used to show how to modify the automated search strategy for problematic cases...|$|E
40|$|ABRIDGED] The new {{generation}} of wide field optical imaging like the Canada France Hawaii Telescope Legacy Survey (CFHTLS) enables discoveries {{of all types of}} gravitational lenses present in the sky. The Strong Lensing Legacy Survey (SL 2 S) project has started an inventory, respectively for clusters or groups of galaxies lenses, and for Einstein rings around distant massive ellipticals. Here we attempt to extend this inventory by finding lensing events produced by massive edge-on disk galaxies which remains a poorly documented class of lenses. We implement and test an <b>automated</b> <b>search</b> <b>procedure</b> of edge-on galaxy lenses in the CFHTLS Wide fields with magnitude 18 Comment: several major edits, 8 pages, A&A accepte...|$|E
40|$|Methods for fitting {{models to}} mark-recapture-recovery studies are now well {{established}} in the literature. Classical model selection methods for identifying those models which best represent the population under investigation are perhaps less satisfactory. One class of methods implements manual model searches on a model space that is restricted by strong physical understandings of the biological plausibility of each model. This can lead to highly subjective analyses requiring "a priori" expert knowledge, which are slow to implement and can be error prone. More automated search algorithms are now available and can be implemented with ease to consider larger classes of models. We investigate the utility of such automated algorithms and consider in particular the situation {{where there is a}} large set of near optimal models according to the model ranking function. We present a modification of an <b>automated</b> <b>search</b> <b>procedure</b> on an unrestricted model space and propose a procedure for model selection {{in the absence of a}} single clear optimal model. We investigate this approach through a classical mark-recapture-recovery analysis of a red deer population from the island of R�m and conduct an investigation into senesence, which is theorized to occur in wild animal populations. Copyright (c) 2009 Royal Statistical Society. ...|$|E
40|$|In [1] Andrews studies {{elementary}} type theory, {{a form of}} Church’s type theory [12] without extensionality, descriptions, choice, and infinity. Since most of the <b>automated</b> <b>search</b> <b>procedures</b> {{implemented in}} Tps [4] do not build in principles of extensionality, descriptions, choice or infinity, they are essentiall...|$|R
40|$|The current {{activity}} in NASA's SETI Program {{is a research}} and development program jointly {{carried out by the}} NASA Ames Research Center and the Jet Propulsion Laboratory. The purpose of this R&D phase is to develop all prerequisites for a well defined microwave observing program. The specific objectives include the development of a prototype search system which will ultimately be capable of processing data rates up to 10 gigabytes/second. Specialized signal detection algorithms and <b>automated</b> <b>search</b> <b>procedures</b> will be developed and tested with the prototype hardware. A series of field tests will be carried out with the NASA Deep Space Network facilities at Goldstone, CA and with the 305 -m radio telescope at the Arecibo Ionospheric Observatory in Puerto Rico...|$|R
40|$|Comprehensive {{two-dimensional}} {{gas chromatography}} coupled to time-of-flight mass spectrometry (GC × GC-TOF-MS) was {{applied in the}} identification of organic compounds in atmospheric aerosols from coniferous forest. The samples were collected at Hyytiälä, Finland, as part of the QUEST campaign, in Spring 2003. Manual and <b>automated</b> <b>search</b> <b>procedures</b> were compared in the identification. An automated procedure is preferable when a large number of data files need to be processed; but manual search was more accurate with the present samples, where the number of compounds was large and most of the compounds of interest were present at trace level. Altogether, about 50 compounds were identified on the basis of mass spectra and linear retention indices. The identified compounds included oxidised monoterpenes, acyclic alkanes, alkenes, ketones and aldehydes, as well as a few alcohols, acids, and aromatic compounds. © 2006 Elsevier B. V. All rights reserved...|$|R
40|$|This paper {{provides}} a direct {{comparison between the}} Linear Matching Method (LMM) and the numerical procedures currently being employed within the Rolls-Royce Power Engineering (plc) Hierarchical Finite Element Framework (HFEF) {{for the assessment of}} shakedown and ratcheting behaviour. These numerical methods include the application of Direct Cyclic Analysis (DCA), utilised in an <b>automated</b> <b>search</b> <b>procedure</b> for load-interaction plot generation and the recently developed Hybrid procedure. The Hybrid procedure is based on a similar premise to the LMM in that the load history is decomposed into cyclic and constant components. The LMM allows for the direct evaluation of shakedown and ratchet limits to be obtained in a traditional Bree load interaction format, along with the subsequent maximum plastic strain range for low-cycle fatigue considerations. Three problems have been used for comparison in this paper; the classic Bree cylinder, a nozzle-in-sphere with a cold media injection transient typical of nuclear power plant loading and a pressurised two-bar structure for multi-axial failure analysis. The accuracy of each method has been verified using ABAQUS step-by-step inelastic analysis. The variations in the implementation strategies associated with each method have also been discussed along with computational efficiency and effectiveness, which show that the LMM has the significant potential to improve analysis speeds via obtaining the ratchet limit boundary directly for a specified level of cyclic loading, instead of conducting an iterative search procedure...|$|E
40|$|This thesis {{concerns}} the computer-assisted modelling of additions to ketones and aromatic substitution, {{and how this}} modelling can be used within an existing program for synthesis planning. I include {{a review of existing}} computer-assisted organic synthesis (CAOS) programs, and discuss how some of these programs deal with knowledge representation. The diastereoselectivities have been modelled for the addition to ketones. The steric requirements for two hypothetical transition states are calculated and the diastereoselectivity of the reaction is calculated in a very fast way (less than one second). For 399 reactions involving 223 substrates and eight reagents, diastereoselectivity data are taken from the literature and fitted to the model. The quotient between the calculated and the reported selectivity is taken as the minimization criterion. I found that steric interactions cannot alone explain the selectivity; it is necessary to include a correction term that includes the effects of torsional strain between the incoming reagent and the b-substituents of the substrate. A mean deviation of 2. 17 between the literature and calculated selectivities is found, a fully satisfactory value for the purpose. For the aromatic substitution reactions, a fast calculational scheme is derived for calculating the regioselectivity. From the literature, 976 data points from 176 substrates with 22 reagents, representing approximately 400 different positions, were correlated within the model with a correlation factor, r = 0. 905. Similar models have previously been correlated with much fewer data points, and have thus given somewhat higher correlation factors. The diverse types of substrates that can be treated in this method makes it well suited for the inclusion into an <b>automated</b> <b>search</b> <b>procedure...</b>|$|E
40|$|Context. The new {{generation}} of deep wide field optical imaging like the Canada France Hawaii Telescope Legacy Survey (CFHTLS) enables discoveries {{of all types of}} gravitational lenses present in the sky. The Strong Lensing Legacy Survey (SL 2 S) project has started an inventory, respectively for clusters or groups of galaxies lenses, and for Einstein rings around distant massive ellipticals. Aims. Here we attempt to extend this inventory by finding lensing events produced by massive edge-on spiral galaxies which remains a poorly documented class of lenses. Methods. We implement and test an <b>automated</b> <b>search</b> <b>procedure</b> of edge-on galaxy lenses in the CFHTLS Wide field with magnitude 18 < i < 21, inclination angle lower than 25 ◦ and having a photometric redshift determination. The procedure estimates the lensing convergence of each galaxy from the R band Tully-Fisher law and selects only the few candidates which display a possible nearby arc configuration at a radius compatible with this convergence (rarc < 2 rE). The efficiency of the procedure is tested after a visual examination of the whole initial sample of 30 444 individual edge-on spirals. Results. We calculate the surface density of edge-on lenses possibly detected in a survey for a given seeing. We deduce that this theoretical number is about 10 for the CFHTLS Wide, a number compatible with the 3 good candidates detected during this work. We show that the efficiency of the Tully-Fisher selection criterium in selecting lens candidates depends crucially on the accuracy of lens photometric redshift. Eventually, we argue that the LSST will detect at least a hundred of such lens candidates. Key words. Gravitational lensing: strong – Surveys – dark matter – Galaxies: spiral – Galaxies: halos 1...|$|E
40|$|Leo-II, a {{resolution}} based theorem prover for classical higherorder logic, {{is currently being}} developed in a one year research project at the University of Cambridge, UK, with support from Saarland University, Germany. We report on the current stage of development of Leo-II. In particular, we sketch some main aspects of Leo-II’s <b>automated</b> proof <b>search</b> <b>procedure,</b> discuss its cooperation with first-order specialist provers, show that Leo-II is also an interactive proof assistant, and explain its shared term data structure and its term indexing mechanism...|$|R
40|$|AbstractFor {{nearly a}} century, {{investigators}} {{in the social}} sciences have used regression models to deduce cause-and-effect relationships from patterns of association. Path models and <b>automated</b> <b>search</b> <b>procedures</b> are more recent developments. In my view, this enterprise has not been successful. The models tend to neglect the difficulties in establishing causal relations, and the mathematical complexities tend to obscure rather than clarify the assumptions on which the analysis is based. Formal statistical inference is, by its nature, conditional. If maintained hypotheses A,B,C,… hold, then H can be tested against the data. However, if A,B,C,… remain in doubt, so must inferences about H. Careful scrutiny of maintained hypotheses should therefore be a critical part of empirical work—a principle honored more often in the breach than the observance. This paper focuses on modeling techniques that seem to convert association into causation. The object is to clarify the differences among the various uses of regression, as well as the source of the difficulty in making causal inferences by modeling. The discussion will proceed mainly by examples, ranging from 71 to 63...|$|R
40|$|For {{nearly a}} century, {{investigators}} {{in the social}} sciences have used regression models to deduce cause-and-effect relationships from patterns of association. Path models and <b>automated</b> <b>search</b> <b>procedures</b> are more recent developments. In my view, this enterprise has not been successful. The models tend to neglect the difficulties in establishing causal relations, and the mathematical complexities tend to obscure rather than clarify the assumptions on which the analysis is based. Formal statistical inference is, by its nature, conditional. If maintained hypotheses A, B, C, [...] . hold, then H can be tested against the data. However, if A, B, C, [...] . remain in doubt, so must inferences about H. Careful scrutiny of maintained hypotheses should therefore be a critical part of empirical work [...] a principle honored more often in the breach than the observance. This paper focuses on modeling techniques that seem to convert association into causation. The object is to clarify the differences among the various uses of regression, as well as the source of the difficulty in making causal inferences by modeling. The discussion will proceed mainly by examples, ranging fro...|$|R
40|$|With the {{widespread}} availability of digital elevation models (DEM) and regional surveys of soils, topographic and physiographic features of landscapes {{are now more}} easily characterized. Within southwestern Oregon 391 field plots were registered within a geographic information system (GIS) to digitized topographic and soils coverages and properties extracted from the digital coverages compared with those estimated in the field. The initial comparison showed major differences in estimates of aspect, slope, and maximum available soil water content (theta), although the location of plots showed general agreement with elevations recorded on the maps. To extrapolate climatic data and interpret hydrologic responses accurately, an <b>automated</b> <b>search</b> <b>procedure</b> was developed whereby the initial location of each plot was, if necessary, shifted within specified bounds to give closer agreement with field estimates of aspect, slope, and theta. Specifically, the search routine sequentially identifies the nearest 100 m-resolution cell within a search radius of 3 or 5 cells in which differences are within +- 22. 5 degree of aspect, +- 20 % of slope, and in closest agreement with field estimates of theta. The search procedure resulted in improved agreement with field estimates: r 2 s = 0. 82 for aspect, 0. 56 for slope, 0. 54 for theta. To obtain these improvements required that the initial plot locations be shifted, on the average, 289 m within the 3 -pixel search radius, and 435 m within the 5 pixel radius. With the terrain analysis procedures developed in this paper, {{it is possible to}} overcome many problems associated with registering the precise location of field plots upon digitized topographic and soil maps. The procedure is particularly appropriate in situations where the environmental regimes associated with a specified field location are to be extrapolated across landscapes. The approach also permits a wealth of historical survey plot data to be incorporated into a GIS format and to be spatially extendedCoops "Comparison of topographic and physiographic properties measured on the ground with those derived from digital elevation models. " Northwest Science. 2000; 74 (2) : 116 - 13...|$|E
40|$|In {{this work}} a {{numerical}} experiment is conducted {{to study the}} effect of the combination of complex nozzle sweep and lean on the performance of the steam turbine LPC last stage. To perform the numerical experiment, an <b>automated</b> <b>search</b> <b>procedure</b> has been developed using the CFD package NUMECA and the program IOSO. This procedure is designed to search for a combination of the nozzle tilt angles, which are key ones to determine the laws of the nozzle sweep and lean. The target function of the optimization process is the maximum efficiency level at constant mass flow rate of steam. The sweep and lean angles and the stagger of the nozzle were varying values during the search. To calculate the span-wise distribution of kinetic energy losses in last stage nozle on the basis of CFD calculation of vapor flow, using the Numeca CFView software, the article offers a method based on the calculation of steam parameters along the individual conditional streamlines. In the CFView program this method is implemented using the integrated programming language Python. As a result of the numerical experiment, the combination of angles has been found to improve the efficiency level by 1. 8 %, and reduce the total kinetic energy losses in the nozzle by 1. 6 %. Application the combined sweep and lean resulted in a decrease in the mass flow rate of steam in the shroud area, and due to this, increase in the hub area. The redistribution of the mass flow rate of steam and preload of the vapor flow to the hub led to decreasing the static pressure gradient and the reactivity degree in the nozzle height. Reduction of the pressure gradient and the preload of the vapor stream to the hub led to a decrease in the intensity of secondary flows at the hub area. The local preload of the steam flow to the shroud together with decreasing pressure gradient resulted in reduced intensity of secondary flows and reduced losses of kinetic energy in the shroud area. Increase in the static pressure in the lower third of the nozzle in the section between the nozzle and the bucket under constant static pressure before the nozzle led to a decrease of the Mach number and the decrease in the intensity of shock waves. An increasing gap between the nozzle and bucket in the shroud area because of the nozzle sweep should reduce the erosive wear of {{the leading edge of the}} last stage bucket. </p...|$|E
40|$|Abstract. Leo-II, a {{resolution}} based theorem prover for classical higherorder logic, {{is currently being}} developed in a one year research project at the University of Cambridge, UK, with support from Saarland University, Germany. We report on the current stage of development of Leo-II. In particular, we sketch some main aspects of Leo-II’s <b>automated</b> proof <b>search</b> <b>procedure,</b> discuss its cooperation with first-order specialist provers, show that Leo-II is also an interactive proof assistant, and explain its shared term data structure and its term indexing mechanism. ...|$|R
40|$|For {{nearly a}} century, {{investigators}} {{in the social}} and life sciences have used regression models to deduce cause-and-effect relationships from patterns of association. Path models and <b>automated</b> <b>search</b> <b>procedures</b> are more recent developments. However, these formal procedures tend to neglect the difficulties in establishing causal relations, and the mathematical complexities tend to obscure rather than clarify the assumptions on which the analysis is based. This paper focuses on statistical procedures that seem to convert association into causation. Formal statistical inference is, by its nature, conditional. If maintained hypotheses A, B, C, [...] . hold, then H can be tested against the data. However, if A, B, C, [...] . remain in doubt, so must inferences about H. Careful scrutiny of maintained hypotheses should therefore be a critical part of empirical work—a principle honored more often in the breach than the observance. Spirtes, Glymour, and Scheines have developed algorithms for causal discovery. We have been quite critical of their work. Korb and Wallace, as well as SGS, have tried to answer the criticisms. This paper will continue the discussion. Their responses may lead to progress in clarifying assumptions behind the methods, but there is little progress in demonstrating that the assumptions hold true for any real applications. The mathematical theory may be of some interest, but claims to have developed a rigorous engine for inferring causation from association are premature at best. The theorems have no implications for samples of any realistic size. Furthermore, examples used to illustrate the algorithms are diagnostic of failure rather than success. There remains a wide gap between association and causation. 1...|$|R
40|$|Cost {{optimization}} {{is one of}} the {{key elements}} of the EU regulatory framework concerning the energy performance of buildings. From this economic point of view, the optimum occurs when the global cost over the lifecycle of a building is minimized, and the cost-optimal energy performance level is that related to the minimum global cost. To determine this cost-optimal level by evaluating a great number of design alternatives, it is necessary to exploit <b>automated</b> optimization <b>search</b> <b>procedures.</b> The work presented here concerns the application of cost-optimal methodology, as defined by European regulation, to a low-consumption single-family house in France. The calculation is performed through an iterative input-output process in a computing environment that combines TRNSYS®, transient system simulation tool, with GenOpt®, generic optimization program. The methodology that was adopted allowed around ten thousand building configurations to be simulated in a reasonable computational time. The paper focuses on how the energy system affects the technical and economic optimal design solutions of the building in two different French climate condition...|$|R
40|$|This project aims {{to explore}} which {{combinations}} of meteorological conditions {{are associated with}} extreme ground level ozone conditions. Our approach focuses only on the tail by optimizing the tail dependence between the ozone response and functions of meteorological covariates. Since {{there is a long}} list of possible meteorological covariates, the space of possible models cannot be explored completely. Consequently, we perform data mining within the model selection context, employing an <b>automated</b> model <b>search</b> <b>procedure.</b> Our study is unique among extremes applications as optimizing tail dependence has not previously been attempted, and it presents new challenges, such as requiring a smooth threshold. We present a simulation study which shows that the method can detect complicated conditions leading to extreme responses and resists overfitting. We apply the method to ozone data for Atlanta and Charlotte and find similar meteorological drivers for these two Southeastern US cities. We identify several covariates which help to differentiate the meteorological conditions which lead to extreme ozone levels from those which lead to merely high levels...|$|R
40|$|Subject of Research. Representation {{features}} of education results for competence-based educational programs are analyzed. Solution importance of decoding and proficiency estimation for elements and components of discipline parts of competences is shown. The {{purpose and objectives}} of research are formulated. Methods. The paper deals with methods of mathematical logic, Boolean algebra, and parametrical analysis of complex diagnostic test results, that controls proficiency of some discipline competence elements. Results. The method of logical conditions analysis is created. It will give the possibility to formulate logical conditions for proficiency determination of each discipline competence element, controlled by complex diagnostic test. Normalized test result is divided into noncrossing zones; a logical condition about controlled elements proficiency is formulated for each of them. Summarized characteristics for test result zones are imposed. An example of logical conditions forming for diagnostic test with preset features is provided. Practical Relevance. The proposed method of logical conditions analysis is applied in the decoding algorithm of proficiency test diagnosis for discipline competence elements. It will give the possibility to <b>automate</b> the <b>search</b> <b>procedure</b> for elements with insufficient proficiency, and is also usable for estimation of education results of a discipline or a component of competence-based educational program...|$|R
40|$|Abstract. In this paper, {{we present}} a major {{improvement}} in the <b>search</b> <b>procedures</b> in constraint programming. First, we integrate various <b>search</b> <b>procedures</b> from AI and OR. Second, we parallelize the search on shared-memory computers. Third, we add an object-oriented extensible control language to implement complex complete and incomplete <b>search</b> <b>procedures.</b> The result is a powerful set of tools which offers both brute force search using simple <b>search</b> <b>procedures</b> and parallelism, and finely tuned <b>search</b> <b>procedures</b> using that expressive control language. With this, we were able both to solve difficult and open problems using complete <b>search</b> <b>procedures,</b> and to quickly produce good results using incomplete <b>search</b> <b>procedures.</b> ...|$|R
40|$|We {{describe}} a unification {{of old and}} recent ideas for formulating graphical models to explain time series data, including Granger causality, semi-automated <b>search</b> <b>procedures</b> for graphical causal models, modeling of contemporaneous influences in times series, and heuristic generalized additive model corrections to linear models. We illustrate the procedures by finding a structure of exogenous variables and mediating variables among time series of remote geospatial indices of ocean surface temperatures and pressures. The analysis agrees with known exogenous drivers of the indices, not assumed in the analysis. <b>Automated</b> <b>search</b> applied to the residuals after regressing each series on its lags and the lags of its Granger causes yields a graphical model of “comtemporaneous ” causal relations identical with the qualitative graphical relations from the time series. A similar analysis produces reasonable results when applied to candidate climate indices obtained by a clustering method from sea surface temperature and sea level pressure data...|$|R
40|$|* PhyloWS for <b>automated</b> <b>searching</b> using a contextual query {{language}} and retrieval using a clearly defined URL API. |$|R
5000|$|Observe that [...] and [...] unify {{syntactically}} {{in their}} predicate arguments. An (<b>automated)</b> <b>search</b> thus finishes in two steps: ...|$|R
5000|$|... {{extremely}} short identifiers (such as 'i' or 'j') {{are very}} difficult to uniquely distinguish using <b>automated</b> <b>search</b> and replace tools ...|$|R
40|$|More {{stringent}} legislative {{guidelines on}} emissions and fuel economy {{have led to}} an increased number of engine parameters to be optimised. Steady state engine mapping to produce an empirical engine model remains a fundamental step in this optimisation process. Recently statistical techniques such as design of experiments have been introduced to improve the efficiency of this modeling phase. Before undertaking an experimental design {{it is first necessary}} to determine the permissible envelope of the various design parameters. The importance of this limit space is two fold, firstly to ensure that the engine is not operated in regions, which may cause damage to it, and secondly so that subsequent experimentation yields test data wholly valid for the subsequent engine model. Currently this limit space is defined in a largely manual process, requiring expert input many times in the test and characterization process. This is a constraint on the activity since progress is inhibited by the availability of the engineer. This paper introduces a fully <b>automated</b> parameter limit <b>search</b> <b>procedure</b> prototyped in MATLAB and run on a turbocharged gasoline engine test-bed. The aim of the software is to automatically move through multiple engine operating points and to carry out a limit search at each. The software supervises the test-bed and engine, dealing intelligently with limit violations to ensure the search and robust. Two different search methods are accessed as to their respective performances. In addition a prior knowledge mechanism has been developed to feed-forward limit space information from one operating point to the next, in order to enhance search efficiency...|$|R
50|$|In {{addition}} to the <b>automated</b> <b>search,</b> Text Engine also provides a way for users to place orders and answer questions through the help of human operators.|$|R
40|$|The U. S. Patent and Trademark Office has {{completed}} the initial installation of an <b>automated</b> <b>search</b> system to enable patent examiners {{to search the}} full text of U. S. patents issued since 1975. This represents a first step in its automation program. The development effort began in 1982 and will continue into the 1990 s. The full text database structure, content and creation are described. Further enhancements of the <b>automated</b> <b>search</b> system, including additional text files and digital image fascimiles are planned by the end of 1986 and will continue in 1987. ...|$|R
40|$|A common {{practice}} is {{to design a}} controller by plant observations (i. e. experiments) and to optimize some of its parameters by trial-and-error. This paper proposes a genetic algorithm for the automation of the <b>search</b> <b>procedure</b> and its implementation on a programmable logic controller. The details of this implementation will be discussed along with an example one carried out for the control of a plant simulation problem introduced by Eastman Chemical Co. The advantage of such an approach consists of <b>automating</b> the <b>search</b> for a good solution. Moreover the genetic algorithm can be easily programmed in the PLC and reused for different plants with the only need of string encoding and fitness evaluation reprogramming...|$|R
40|$|COSAL employs finite-difference {{method to}} solve {{compressible}} stability equations in original form. Code includes two eigenvalue <b>search</b> <b>procedures.</b> Global procedure provided for use when no initial guess available. Fast local eigenvalue <b>search</b> <b>procedure</b> provided for use when good initial guess available. COSAL written in FORTRAN IV...|$|R
40|$|Current search tools on the Web, such as {{general-purpose}} {{search engines}} (e. g. Google) and domain-specific portals (e. g. MEDLINEplus), {{do not provide}} <b>search</b> <b>procedures</b> that guide users to form appropriately ordered sub-goals. The lack of such procedural knowledge often leads users searching in unfamiliar domains to retrieve incomplete information. In critical domains such as in healthcare, such ineffective searches can have dangerous consequences. To address this situation, we developed {{a new type of}} domain portal called a Strategy Hub. Strategy Hubs provide the critical <b>search</b> <b>procedures</b> and associated high-quality links that enable users to find comprehensive and accurate information. This paper describes how we collaborated with skin cancer physicians to systematically identify generalizeable <b>search</b> <b>procedures</b> to find comprehensive information about melanoma, and how these <b>search</b> <b>procedures</b> were made available through the Strategy Hub for healthcare. A pilot study suggests that this approach can improve the efficacy, efficiency, and satisfaction of even expert searchers. We conclude with insights on how to refine the design of the Strategy Hub, and how {{it can be used to}} provide <b>search</b> <b>procedures</b> across domains...|$|R
30|$|In the DTS method, the exploration, diversification, and {{intensification}} <b>search</b> <b>procedures</b> {{are applied}} to develop the search process on a global strategy. Indeed, these <b>search</b> <b>procedures</b> are used to get a vast exploration and an extensive diversification. The DTS {{has been used in}} this work is stated in Algorithm 1.|$|R
5000|$|Questions of {{completeness}} of {{a hypothesis}} <b>search</b> <b>procedure</b> of specific ILP system arise. For example, Progol's hypothesis <b>search</b> <b>procedure</b> {{based on the}} inverse entailment inference rule is not complete by Yamamoto's example. On the other hand, Imparo is complete by both anti-entailment procedure [...] and its extended inverse subsumption [...] procedure.|$|R
30|$|The bisection <b>search</b> <b>procedure</b> can {{be applied}} to solve (40).|$|R
5000|$|<b>Search</b> <b>Procedures</b> - 1996 (nominated for a Governor General's Award) ...|$|R
5000|$|Repeat the <b>search</b> <b>procedure</b> to find {{location}} with least weight ...|$|R
40|$|This paper {{introduces}} {{an alternative}} variable selection method {{for use in}} regression analysis {{that is based on}} the Tabu <b>search</b> <b>procedure.</b> The Tabu <b>search</b> was compared to traditional regression analysis procedures using various size data sets. The results indicate the superiority of the Tabu <b>search</b> <b>procedure</b> for model selection in multiple regression analysis...|$|R
