32|162|Public
5000|$|The <b>abstract</b> <b>file</b> {{system model}} of GIO {{consists}} {{of a number of}} interfaces and base classes for I/O and files.|$|E
5000|$|Virtual file system: <b>abstract</b> <b>file</b> {{system that}} transparently allows access to disk files, memory files, [...]zip files and [...]gz files.|$|E
50|$|A handle is an {{abstract}} reference, {{and may be}} represented in various ways. A common example are file handles (the FILE data structure in the C standard I/O library), used to <b>abstract</b> <b>file</b> content. It usually represents both the file itself, as when requesting a lock on the file, and a specific position within the file's content, as when reading a file.|$|E
50|$|Higher-level {{operating}} system and programming facilities employ separate, more abstract I/O concepts and primitives. For example, most {{operating system}}s provide application programs {{with the concept}} of files. The C and C++ programming languages, and operating systems in the Unix family, traditionally <b>abstract</b> <b>files</b> and devices as streams, which can be read or written, or sometimes both. The C standard library provides functions for manipulating streams for input and output.|$|R
40|$|<b>Abstract.</b> XML <b>file</b> {{comparison}} and clustering are two challenging tasks still accomplished predominantly manually. XML schema contains {{information about}} data structure, types, and labels {{found in an}} XML file. By reducing the XML schema tree to its significant nodes the task of finding structural equivalent schemas, and implicit XML files that refer to the same entities, is simplified. ...|$|R
40|$|<b>Abstract.</b> This <b>file</b> {{contains}} {{some of the}} straightforward but technical calculations from the paper “The triangle-free process and R(3, k) ” which were omitted from the proof {{in order not to}} distract from the main argument. We have gathered these calculations here in order to save the interested reader the trouble of reproving them herself. 1...|$|R
40|$|This is a {{template}} in LATEX format for your full-text paper. The title, au-thor list, author affiliation, abstract text, and list of keywords must be identical with those {{appearing on the}} accepted abstract. For your convenience, it is recom-mended that you copy the text enclosed by the commands and from your accepted <b>abstract</b> <b>file</b> and paste it right beneath the com-mand (before your first) in the main file of your paper. If your <b>abstract</b> <b>file</b> name is wong-athit!rk. tex, for example, the main file of your paper must be named (i. e., this template file must be renamed) wong-athitrk. tex. (Notice the mandatory “ ” mark in place of “!”. ...|$|E
40|$|Abstract. Tool {{interoperability}} as a mean {{to achieve}} integration {{is among the}} main goals of the international Grand Challenge initiative. In the context of the Verifiable file system mini-challenge put forward by Rajeev Joshi and Gerard Holzmann, this paper focuses on the integra-tion of different formal methods and tools in modelling and verifying an <b>abstract</b> <b>file</b> system inspired by the IntelR © Flash File System Core. We combine high-level manual specification and proofs with current state of the art mechanical verification tools into a tool-chain which involves Alloy, VDM++ and HOL. The use of (pointfree) relation modelling pro-vides the glue which binds these tools together. ...|$|E
40|$|Tool {{interoperability}} as a mean {{to achieve}} integration {{is among the}} main goals of the international Grand Challenge initiative. In the context of the Verifiable file system mini-challenge put forward by Rajeev Joshi and Gerard Holzmann, this paper focuses on the integration of different formal methods and tools in modelling and verifying an <b>abstract</b> <b>file</b> system inspired by the Intel (R) Flash File System Core. We combine high-level manual specification and proofs with current state of the art mechanical verification tools into a tool-chain which involves Alloy, VDM++ and HOL. The use of (pointfree) relation modelling provides the glue which binds these tools together. Mondrian Project funded by the Portuguese NSF under contract PTDC/EIA-CCO/ 108302 / 200...|$|E
40|$|Information {{about the}} UNL Digital Commons for {{potential}} depositors and series editors. Topics addressed include reasons for depositing, copyright, {{how to set}} up a series, file types, PDF issues, who can use the archive, how to deposit non-PDF files, how to format <b>abstracts,</b> best <b>file</b> types for art, where to find more information, etc. Contains links. Length = about 2, 000 word...|$|R
40|$|Presented at the American Medical Informatics Association Spring Congress in Orlando, Florida, May 23, 2007, 8 : 30 - 10 am. This panel {{discussion}} and workshop will help AMIA members learn {{the skills necessary}} to participate in open access publishing, which include understanding the NIH Policy, choosing an optimal publication venue, and managing copyright. See full <b>abstract</b> in <b>file</b> attached to this record...|$|R
50|$|The ATK <b>abstract</b> headers <b>files</b> are freely {{available}} to help developers {{who want to make}} their GUI toolkit accessible. Developers who use stock widgets of GUI toolkits that implements the ATK headers don't have to worry too much for making their applications accessible. However, if they develop their own widgets, they will have to ensure that they are exposing all the accessible information.|$|R
40|$|<b>Abstract.</b> <b>File</b> carvers are {{forensic}} {{software tools}} used to recover data from storage devices {{in order to}} find evidence. Every legal case requires different trade-offs between precision and runtime performance. The resulting required changes to the software tools are performed manually and under the strictest deadlines. In this paper we present a model-driven approach to file carver development that enables these trade-offs to be automated. By transforming high-level file format specifications into approximations that are more permissive, forensic investigators can trade precision for performance, without having to change source. Our study shows that performance gains up to a factor of three can be achieved, at the expense of up to 8 % in precision and 5 % in recall. ...|$|E
40|$|<b>Abstract.</b> <b>File</b> inclusion, {{conditional}} compilation and macro processing {{has made}} the preprocessor {{a powerful tool for}} programmers. Preprocessor directives are extensively used in C/C++ programs and have various purposes. However, program code with lots of directives often causes problems in program understanding and maintenance. The main source of the problem is the difference between the code that the programmer sees and the preprocessed code that the compiler is given. We designed a Preprocessor Schema and implemented a preprocessor which produces both preprocessed (. i) files and schema instances. The Schema is general purpose and its instances model both the whole preprocessed compilation unit and the transformations made by the preprocessor. Therefore it facilitates program comprehension and tool interoperability...|$|E
40|$|<b>Abstract</b> <b>File</b> {{swarming}} is {{a popular}} method of coordi-nated download by which peers obtain a le from an under-provisioned server. Critical problems arise within this domain when users act selshly, yet most systems are built with altru-ism assumed. Working {{under the assumption that}} all peers are greedy, we introduce the Fair, Optimal eXchange (FOX) protocol. FOX, in addition to effective and robust application of the tit-for-tat incentive mechanism, provides theoretically optimal download times when everyone cooperates. Under our assumption of server and peer capabilities, we develop a strong threat model that provides peers with the incentives to not deviate from the protocol. From a theoretical perspective, we prove FOX’s optimality and incentive properties, even when the network consists only of purely self-interested peers. We also discuss issues in implementing and deploying such a system, and address the cost of ensuring fairness in a domain where efciency is so important. I...|$|E
40|$|A {{major concern}} of {{researchers}} using state data sets for population-based analyses and market share {{studies in the}} health care sector is the potential bias caused by 'border crossing' [...] patients receiving care out of state. By using the Health Care Financing Administration (HCFA) discharge <b>abstract</b> <b>files</b> for 1987 and 1988, we found that 'border crossing' is not a serious problem for the two large states we examined. Only 4. 4 % of New York patients and 2. 15 % of California patients received care out of state. At the county and zip code level, 'border crossing' is more frequent but tends to be concentrated in areas adjacent to other states. Even excluding all zips with more than 10 % of patients crossing the 'border' results in a small loss of patients (2. 2 % for New York and 1. 0 % for California). hospital use geographic patterns border crossing discharge data...|$|R
40|$|Current {{models of}} the low {{latitude}} electric field are largely {{based on data from}} incoherent scatter radars. These observations are extended through the addition of the rather extensive high quality electric field measurements from the Ion Drift Meter (IDM) aboard the Atmosphere Explorer (AE) spacecraft. Some preliminary results obtained from the Unified <b>Abstract</b> <b>files</b> of satellite AE-E are presented. This satellite was active from the end of 1975 through June 1981 in various elliptical and circular orbits having an inclination near 20 deg. The resulting data can be examined for the variation of ion drift with latitude, longitude, season, solar cycle, altitude, and magnetic activity. The results presented deal primarily with latitudinal variations of the drift features. Diagrams of data are given and briefly interpreted. The preliminary results presented here indicate that IDM data from the AE and the more recent Dynamics Explorer B spacecraft should continue to disclose some interesting and previously unobserved dynamical features of the low latitude F region...|$|R
40|$|<b>Abstract.</b> HFS+ <b>file</b> {{system is}} a file system of the Mac OS. In order to achieve data {{manipulation}} of the file system based on the Windows OS for further computer forensics, {{not only do we}} introduce the principle and structure of HFS+ file system, but also propose a efficient method to analyze the file system. Research contains the exploration of the file system and program implementation to analyze the file system...|$|R
40|$|Tool {{interoperability}} {{is among}} the main goals of the international Grand Challenge initiative. In {{the context of the}} Verifiable File System mini-challenge put forward by Joshi and Holzmann, our work has been focused on the integration of different formal methods and tools in a tool-chain for modelling and verification. The current paper shows how to adapt such a tool-chain to the task in hands, aiming at reducing tool integration costs. The refinement of an <b>abstract</b> <b>file</b> store model into a journaled (flash) data model catering for wear leveling and recovery from power loss is taken as case study. This shows that refinement steps can be carried out within a shorter, reduced life-cycle where model checking in Alloy {{goes hand in hand with}} manual proofs carried out in the (pointfree) algebra of binary relations. This provides ample evidence of the positive impact of Alloy’s lemma ’everything is a relation’ on software verification, in particular in carrying out induction-free proofs about data structures such as finite maps and lists...|$|E
40|$|<b>Abstract.</b> <b>File</b> format {{obsolescence}} is a {{major risk}} factor threatening the sustainability of and access to digital information. While the preservation community has become increasingly interested in tools for migration and transformation of file formats, the National Library of Australia is developing mechanisms specifically focused on monitoring and assessing the risks of file format obsolescence. This paper reports on the AONS II project, undertaken by the National Library of Australia (NLA) {{in conjunction with the}} Australian Partnership for Sustainable Repositories (APSR). The project aimed to develop a software tool which allows users to automatically monitor the status of file formats in their repositories, make risk assessments based on a core set of obsolescence risk questions, and receive notifications when file format risks change or other related events occur. This paper calls for the preservation community to develop a co-operating file format obsolescence community which includes registries, software tool creators and end users to effectively curate digital content in order to maintain long-term access. 1...|$|E
40|$|Data {{from the}} low energy {{electron}} (LEE) experiments on the Atmosphere Explorer C and D satellites {{have been used to}} determine the average global distribution of the energy flux of precipitating auroral electrons and their average energy for different levels of geomagnetic activity. Measurements from the Atmosphere Explorer unified <b>abstract</b> <b>file</b> (15 -s resolution) have been binned according to invariant latitude (in the range 50 - 90 deg), magnetic local time, and geomagnetic activity as measured by the Kp and auroral electrojet (AE) indices, separately. Bin-averaged values of precipitating energy flux and average energy have been calculated, and a smoothing algorithm used to reduce stochastic variations in the raw data. The results indicate that, for the parameters studied, the AE inces does a superior job of ordering the data with regard to geomagnetic activity. The global distribution of the auroral enhancement porition of the Pedersen and Hall conductances were inferred from the data by means of an empirical fit to detailed energy deposition calculations...|$|E
40|$|<b>Abstract.</b> Large <b>file</b> {{uploading}} is {{a problem}} often occurs in Web applications. Existing uploading components does not support Resume-broken-transfer while most of them cannot display uploading progress, also Cross-platform compatibility is not very well. This paper adopts Fragments-access, Asynchronous-uploading strategy, implements large file Resume-broken-transfer using ADODB. Stream and XMLHttp, obtains higher rates of transmission and real-time display of uploading progress and good Cross-platform compatibility, meets the large file uploading needs in web applications...|$|R
50|$|Since {{the data}} {{represented}} by property lists is somewhat <b>abstract,</b> the underlying <b>file</b> format {{can be implemented}} many ways. Namely, NeXTSTEP used one format to represent a property list, and the subsequent GNUstep and macOS frameworks introduced differing formats.|$|R
40|$|This CD-ROM {{contains}} the <b>abstracts</b> and associated <b>files</b> for the Eleventh Annual V. M Goldschmidt Conference. Topics include: Organic Geochemistry; Metamorphic Processes; Igneous Processes; Stable and Radiogenic Isotopes; Planetary Geochemistry and Mineralogy; Mineralogy and Crystallography; Ore Deposits; and Aqueous Geochemistry...|$|R
40|$|The article {{describes}} how the Australian Patent Office constructed search files to fulfi; its responsibilities as a PCT International Searching Authority. Search files are established on an ad hoc basis as the need arises and comprise classified sets of abstracts of patent documents included in the PCT minimum documentation. When an international search is received a search strategy is decided upon by an examiner {{in conjunction with the}} Search Quality Assurance Unit (SQAU) and search lists are established setting out the numbers of the documents to be considered by the examiner. Abstracts of these selected documents are prepared by the Documentation Centre and forwarded to the examiner to conduct the search, on completion of which the <b>abstract</b> <b>file</b> is returned to the Documentation Centre for storage and re-use. It is claimed that the system provides a relatively quick method of establishing comprehensive classified search files and obviates the establishment of classified search files in areas of subject matter that may never be used. ...|$|E
40|$|SCRIPT to your A-disk. You will {{be using}} Xedit to change this example file to make it produce your own thesis. 2. Each chapter of your thesis {{should be in a}} {{separate}} file. Name the files CHAP 19 SCRIPT, CHAP 29 SCRIPT, etc. This way, the commands ". im chap 19 ", ". im chap 29 ", etc. in the DRIVER 9 file will use your separate chapters to produce a single thesis. By convention, all files having to do with Thesis 9 end in 9 (except the <b>ABSTRACT</b> <b>file).</b> Of course, you don't have to hold to this convention. 3. Xedit the DRIVER 9 file. Change the example title to the title of your thesis; change the example author shown to your name, and so on. As you read the THESIS 9 documentation, you'll see how to use the other commands in DRIVER 9. 4. When you want to see what your thesis will look like, typ...|$|E
40|$|AbstractIn this note, {{we define}} an <b>abstract</b> <b>file</b> {{system as a}} partial {{function}} from (absolute) paths to data. Such a file system determines the set of valid paths. It allows the file system to be read and written at a valid path, and it allows the system to be modified by the Unix operations for removal (rm), making of directories (mkdir), and moving (mv). We present abstract definitions (axioms) for these operations. This specification is refined towards a pointer implementation. To mitigate the problems attached to partial functions, we do this in two steps. First a refinement towards a pointer implementation with total functions, followed by one that allows partial functions. These two refinements are proved correct {{by means of a}} number of invariants. Indeed, the insight gained mainly consists of the invariants of the pointer implementation that are needed for the refinement functions. Finally, each of the three specification levels is enriched with a permission system for reading, writing, or executing, and the refinement relations between these permission systems are explored...|$|E
40|$|The modern {{operating}} systems must integrate various Internet services, especially World-Wide Web facilities to access Web resources using file systems mechanisms. In this paper {{we present a}} high-level model describing an <b>abstract</b> distributed <b>file</b> system. The proposed description is based on Resource Description Framework (RDF) recommendation of the World-Wide Web Consortium, a standardized foundation for processing metadata. To represent the RDF statements about various file characteristics, we propose an XML-based language called Extensible File Properties Markup Language...|$|R
40|$|The Pacific Northwest Laboratory (PNL) (1) {{effort on}} the overall Baseline Environmental Management Report (BEMR) project {{consists}} of four installation-specific work components performed in succession. These components include (1) development of source terms, 92) collection of data and preparation of environmental settings reports, (3) calculation of unit risk factors, and (4) utilization of the unit risk factors in Automated Remedial Action Methodology (ARAM) for computation of target concentrations and cost estimates. This report documents work completed for the Nevada Test Site, Nevada, for components 2 and 3. The product of {{this phase of the}} BEMR project is the development of unit factors (i. e., unit transport factors, unit exposure factors, and unit risk factors). Thousands of these unit factors are gene rated and fill approximately one megabyte of computer information per installation. The final unit risk factors (URF) are transmitted electronically to BEMR-Cost task personnel as input to a computer program (ARAM). <b>Abstracted</b> <b>files</b> and exhibits of the URF information are included in this report. These visual formats are intended to provide a sample of the final task deliverable (the URF files) which can be easily read without a computer...|$|R
40|$|This thesis, largely {{describing}} diverse {{studies in}} organic synthesis, {{is divided into}} three parts. Part I, titled ‘Heterocycles’, describes in two chapters studies directed towards elaborating certain thiazole and oxazole derivatives as useful synthons. Part II, titled ‘Hydride transfers’, describes in two chapters synthetic and some mechanistic studies involving the Cannizzaro and Tishchenko reactions, apart from work with chirally-modified alumino and borohydride reagents. Finally, Part III, titled ‘Miscellaneous studies’, describes structural studies on cyclic carbonates. (For structural formula see the <b>abstract.</b> pdf <b>file.</b> ...|$|R
40|$|Abstract Alloy is a {{lightweight}} modeling language based on first-order relational logic. The language is expressive enough to describe structurally complex systems, but simple {{enough to be}} amenable to fully automated analysis. The Alloy Analyzer, with its SATbased analysis engine, allows one to simulate traces of a system, visualize them, or search for counterexamples to a property. This article illustrates key concepts of Alloy using, as an example, the construction and analysis of a design for a flash file system. In addition to basic file operations, the design includes features that are crucial to NAND flash memory but contribute to increased complexity of the file system, such as wear leveling and erase-unit reclamation. The design also addresses the issues of fault-tolerance by providing a mechanism for recovering from unexpected hardware failures. The article describes the modeling process and discusses {{the results of the}} design analysis, which has been carried out by checking trace inclusion of the flash file system against a POSIX-compliant <b>abstract</b> <b>file</b> system. Key words: software design; formal specification; modeling; analysis; Allo...|$|E
40|$|In this note, {{we define}} an <b>abstract</b> <b>file</b> {{system as a}} partial {{function}} from (absolute) paths to data. Such a file system determines the set of valid paths. It allows the file system to be read and written at a valid path, and it allows the system to be modified by the Unix operations for removal (rm), making of directories (mkdir), and moving (mv). We present abstract definitions (axioms) for these operations. This specification is refined towards a pointer implementation. To mitigate the problems attached to partial functions, we do this in two steps. First a refinement towards a pointer implementation with total functions, followed by one that allows partial functions. These two refinements are proved correct {{by means of a}} number of invariants. Indeed, the insight gained mainly consists of the invariants of the pointer implementation that are needed for the refinement functions. Finally, each of the three specification levels is enriched with a permission system for reading, writing, or executing, and the refinement relations between these permission systems are explored. ...|$|E
40|$|Abstract—Relational algebra {{offers to}} {{software}} engineering {{the same degree}} of conciseness and calculational power as linear algebra in other engineering disciplines. Binary relations play the role of matrices with similar emphasis on multiplication and transposition. This matches with Alloy’s lemma “everything is a relation ” and with the relational basis of the Algebra of Programming (AoP). Altogether, it provides a simple and coherent approach to checking and calculating programs from abstract models. In this paper, we put Alloy and the Algebra of Programming together in a case study originating from the Verifiable File System mini-challenge put forward by Joshi and Holzmann: verifying the refinement of an <b>abstract</b> <b>file</b> store model into a journaled (FLASH) data model catering to wear leveling and recovery from power loss. Our approach relies on diagrams to graphically express typed assertions. It interweaves model checking (in Alloy) with calculational proofs in a way which offers the best of both worlds. This provides ample evidence of the positive impact in software verification of Alloy’s focus on relations, complemented by induction-free proofs about data structures such as stores and lists. Index Terms—Model checking, algebra of programming, software verification, grand challenges in computing Ç...|$|E
5000|$|The I/O {{functionality}} of C {{is fairly}} low-level by modern standards; C <b>abstracts</b> all <b>file</b> operations into operations on streams of bytes, {{which may be}} [...] "input streams" [...] or [...] "output streams". Unlike some earlier programming languages, C has no direct support for random-access data files; to read from a record {{in the middle of}} a file, the programmer must create a stream, seek to the middle of the file, and then read bytes in sequence from the stream.|$|R
50|$|Filesystem {{access is}} <b>abstracted</b> {{allowing}} platform-independent <b>file</b> and folder access, and transparent access to files within ZIP archives. Other I/O features include an XML reader and writer, {{the ability to}} take screenshots, manipulate images and meshes and then save them in several different file formats.|$|R
40|$|<b>Abstract</b> TEX <b>files</b> are {{text files}} which are {{readable}} by other programs. Mathemat-ical proofs written using TEX can be checked by a Python program pro-vided they {{are expressed in}} a sufficiently strict proof language. Such a language can be constructed using only a few extensions beyond the syntax of Morse’s book [5], one being the incorporation of explicit theorem number references into the syntax. Such a program {{has been applied to}} and success-fully checked the theorems in a significant initial segment of a book length mathematical manuscript. ...|$|R
