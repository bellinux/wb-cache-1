1|26|Public
40|$|Network Coding (NC) {{has emerged}} as a {{ubiquitous}} technique of communication networks and has extensive applications in both practical implementations and theoretical developments. While the Avalanche P 2 P file system from Microsoft, the MORE routing protocol, and the COPE coding architecture from MIT have implemented the idea of NC and exhibited promising performance improvements, {{a significant part of the}} success of NC stems from the continuing theoretic development of NC capacity, e. g., the Shannon capacity results for the single-flow multi-cast network and the packet erasure broadcast channel with feedback. However, characterizing the capacity for the practical wireless multi-flow network setting remains a challenging topic in NC. For example, the difficulties of finding the optimal NC strategy over multiple flows under varying-channel qualities and the rate adaption scenarios hinder any further advancement in this area. Despite the difficulty of characterizing the full capacity for large networks, there are evidences showing that even when using only local operations, NC can still recover substantial NC gain. We believe that a deeper understanding of multi-flow local network coding will play a key role in designing the next-generation high-throughput coding-based wireless network architecture. This thesis consists of three parts. In the first part, we characterize the full Shannon capacity region of the 2 ̆ 2 COPE 2 ̆ 2 principle when applied to a 2 -flow wireless butterfly network with broadcast packet erasure channels. The capacity results allow for random overhearing probabilities, arbitrary scheduling policies, network-wide channel state information (CSI) feedback after each transmission, and potential use of non-linear network codes. We propose a theoretical outer bound and a new class of linear network codes, named the Space-Based Linear Network Coding (SBLNC), that achieves the capacity outer bound. Numerical experiments show that SBLNC provides close-to-optimal throughput even in the scenario with opportunistic routing. In the second part, we further consider the complete network dynamics of stochastic arrivals and queueing and study the corresponding stability region. Based on dynamic packet arrivals, the resulting solution would be one step closer to practical implementation, when compared to the previous block-code-based capacity study. For the 2 -flow downlink scenario, we propose the first opportunistic INC + scheduling solution that is provably optimal for time-varying channels, i. e., the corresponding stability region matches the optimal Shannon capacity. Specifically, we first introduce a new binary INC operation, which is distinctly different from the traditional wisdom of XORing two overheard packets. We then develop a queue-length-based scheduling scheme, which, with the help of the new INC operation, can robustly and optimally adapt to time-varying channel quality. We then show that the proposed algorithm can be easily extended for rate adaptation and it again robustly achieves the optimal throughput. In the third part, we propose an 802. 11 -based MAC layer protocol which incorporates the rate <b>adaption</b> <b>solution</b> developed in the second part. The new MAC protocol realizes the promised intersession network coding gain for two-flow downlink traffic with short decoding delay. Furthermore, we delicately retain the CSMA-CA distributed contention mechanism with only 17 bits new header field changes, and carefully ensure the backward compatibility. In summary, the new solution demonstrates concrete throughput improvement without alternating the too much packet-by-packet traffic behavior. Such a feature is critical in practical implementation since it allows the network coding solution to be transparent to any arbitrary upper layer applications...|$|E
30|$|This paper {{does not}} cover the {{fairness}} issues of the codec <b>adaption</b> <b>solutions,</b> meaning the case when calls which are not affected directly by the rate change have to lower their VoIP codec and hence, observe a somewhat worse quality in terms of MOS. The fundamental assumption, that this change would allow to maintain the system stability, is not completely valid as {{there is always the}} alternative to simply drop or block the call which is causing the instability. Thus, some incentives to the user which accepts to participate on the codec adaptation should be introduced.|$|R
40|$|International audienceResults {{from the}} Sixth AIAA CFD Drag Prediction Workshop Cases 2 to 5 are presented. These cases focused on force/moment and {{pressure}} predictions for the NASA Common Research Model wing–body and wing–body–nacelle–pylon configurations. The Common Research Model geometry differed from previous workshops {{in that it}} was deformed to the appropriate static aeroelastic twist and deflection at each specified angle of attack. The grid refinement study and nacelle–pylon drag increment prediction (Case 2) used a common set of overset and unstructured grids, as well as user-created multiblock structured, unstructured, and Cartesian-based grids. Solutions were requested for both the wing–body and wing–body–nacelle–pylon at a fixed Mach number and lift coefficient. The wing–body static aeroelastic/buffet study (Case 3) specified an angle-of-attack sweep at finely spaced intervals through the zone where wing separation was expected to begin. The optional Case 4 requested grid <b>adaption</b> <b>solutions</b> of the wing–body at a specified flight condition. Optional Case 5 requested coupled aerostructural wing–body solutions. Results from this workshop highlight the progress made since the last workshop, and the continuing need for computational fluid dynamics (CFD) improvement, particularly for conditions with significant flow separation. These comparisons also suggest the need for improved experimental diagnostics to guide future CFD development...|$|R
40|$|Physical {{development}} of Ambient Assisted Living environments is expensive and time consuming. In order {{to realize that}} a certain environment design matches user requirements, {{it needs to be}} built and tested in a life-size build-up. The VAALID project creates a development and simulation system that allows creating simulations of their systems and testing them in a Mixed Reality environment. This allows faster and easier <b>adaption</b> of AAL <b>solutions</b> to user requirements...|$|R
40|$|AbstractVariational grid {{generation}} {{techniques are}} now {{used to produce}} grids suitable for solving numerical partial differential equations in irregular geometries. Variational grid generation methods are very robust but slow. The method considered here is a discrete variational method. This method preserves the robustness of the variational method; also, it is very fast {{so it can be}} used in time-dependent problems. Here we discuss geometry and <b>solution</b> <b>adaption</b> for the direct discrete grid generation method, and some examples are presented to demonstrate its capabilities...|$|R
40|$|Magister Artium (Development Studies) - MA(DVS) Climate Change (CC) {{is arguably}} the most {{pressing}} topic of our modern society. The acceleration in magnitude and frequency of climate variability associated with it, along with the overall change of climate patterns threatens to push their adaptive capacity to breaking point, hinting at the significant impact that CC will have on the livelihoods of small-scale farmers of the developing world, and on South Africa in particular. This research project aims to investigate how local knowledge and agriculture-based coping practices of small-scale farmers of the Ebenhaeser community are adapted to deal with and attempt to reduce the vulnerability of their livelihood strategies to CC. This illustrative study followed a qualitative methodology, using qualitative data collection (in-depth and semistructured interviews, as well as special focus group discussions) and analysis (thematic ordering) methods to fulfil its aim. This study revealed that local farmers were able to identify changes in climate which were hazardous to their livelihoods and that they have been developing coping practices in response the CC. Furthermore, this analysis showed that local small-scale farmers used their local body of knowledge as a basis for the development of these coping practices, and that this local knowledge base itself has been affected by CC. An important finding of this study was the extent to which local social, historic, economic, political and physical conditions influence the sensitivity and adaptive capacity of the smallscale farmers of the Ebenhaeser community. The findings of this study opened our eyes to the realities of CC and its impacts on and adaptation efforts of small-scale farmers of the Ebenhaeser community. The study show ed that unless these issues are addressed in a comprehensive and holistic manner, there is no real prospect of sustainable, long-term CC <b>adaption</b> <b>solutions</b> for the small-scale farmers of this area, and conceivably none for many more rural communities in South Africa...|$|R
40|$|This {{review paper}} {{examines}} {{a synthesis of}} adaptive mesh methods {{with the use of}} symmetry to solve ordinary and partial dierential equations. It looks at the eectiveness of numerical methods in preserving geometric structures of the underlying equations such as scaling invariance, conservation laws and solution orderings. Studies are made of a series of examples including the porous medium equation and the nonlinear Schr 7 ̆fodinger equation. key words: Mesh <b>adaption,</b> self-similar <b>solution,</b> scaling invariance, conservation laws, maximum principles, equidistribution. March 31, 2000 1 Dept. of Mathematical Sciences, University of Bath, Claverton Down, Bath, BA 2 7 AY, UK. cjb@maths. bath. ac. uk 2 Dept. of Mathematical Sciences, University of Bath, Claverton Down, Bath, BA 2 7 AY, UK. mapmdp@maths. bath. ac. uk 1 1 Introduction When we wish to nd a numerical approximation to the solution of a partial dierential equation, a natural technique is to discretise the PDE so that local trunc [...] ...|$|R
40|$|Results {{from the}} Sixth AIAA CFD Drag Prediction Workshop Common Research Model Cases 2 to 5 are presented. As with past workshops, {{numerical}} calculations are performed using industry-relevant geometry, methodology, and test cases. Cases 2 to 5 focused on force/moment and pressure predictions for the NASA Common Research Model wing-body and wing-body-nacelle-pylon configurations, including Case 2 - a grid refinement study and nacelle-pylon drag increment prediction study; Case 3 - an angle-of-attack buffet study; Case 4 - an optional wing-body grid adaption study; and Case 5 - an optional wing-body coupled aero-structural simulation. The Common Research Model geometry differed from previous workshops {{in that it}} was deformed to the appropriate static aeroelastic twist and deflection at each specified angle-of-attack. The grid refinement study used a common set of overset and unstructured grids, as well as user created Multiblock structured, unstructured, and Cartesian based grids. For the supplied common grids, six levels of refinement were created resulting in grids ranging from 7 x 10 (exp 6) to 208 x 10 (exp 6) cells. This study (Case 2) showed further reduced scatter from previous workshops, and very good prediction of the nacelle-pylon drag increment. Case 3 studied buffet onset at M= 0. 85 using the Medium grid (20 to 40 x 10 (exp 6) nodes) from the above described sequence. The prescribed alpha sweep used finely spaced intervals through the zone where wing separation was expected to begin. Although the use of the prescribed aeroelastic twist and deflection at each angle-of-attack greatly improved the wing pressure distribution agreement with test data, many solutions still exhibited premature flow separation. The remaining solutions exhibited a significant spread of lift and pitching moment at each angle-of-attack, much of which can be attributed to excessive aft pressure loading and shock location variation. Four Case 4 grid <b>adaption</b> <b>solutions</b> were submitted. Starting with grids less than 2 x 10 (exp 6) grid points, two solutions showed a rapid convergence to an acceptable solution. Four Case 5 coupled aerostructural solutions were submitted. Both showed good agreement with experimental data. Results from this workshop highlight the continuing need for CFD improvement, particularly for conditions with significant flow separation. These comparisons also suggest the need for improved experimental diagnostics to guide future CFD development...|$|R
40|$|Ant colony {{optimisation}} {{has proved}} suitable to solve static optimisation problems, that is problems {{that do not}} change with time. However {{in the real world}} changing circumstances may mean that a previously optimum solution becomes suboptimal. This paper explores the ability of the ant colony optimisation algorithm to adapt from the optimum solution for one set of circumstances to the optimal solution for another set of circumstances. Results are given for a preliminary investigation based on the classical travelling salesman problem. It is concluded that, for this problem at least, the time taken for the <b>solution</b> <b>adaption</b> process is far shorter than the time taken to find the second optimum solution if the whole process is started over from scratch...|$|R
40|$|Abstract. Ant Colony {{optimisation}} {{has proved}} suitable to solve static optimisation problems, that is problems {{that do not}} change with time. However {{in the real world}} changing circumstances may mean that a previously optimum solution becomes suboptimal. This paper explores the ability of the ant colony optimisation algorithm to adapt from the optimum solution to one set of circumstances to the optimal solution to another set of circumstances. Results are given for a preliminary investigation based on the classical travelling salesperson problem. It is concluded that, for this problem at least, the time taken for the <b>solution</b> <b>adaption</b> process is far shorter than the time taken to find the second optimum solution if the whole process is started over from scratch. Keywords: Meta-heuristics, Optimisation, Ant Colony Optimisation. ...|$|R
40|$|Tiling is a loop {{transformation}} that decomposes computations into {{a set of}} smaller computation blocks. The transformation {{has proved to be}} useful for many high-level program optimizations, such as data locality optimization and exploiting coarse-grained parallelism, and crucial for architecture with limited resources, such as embedded systems, GPUs, and the Cell. Data locality and parallelism will continue to serve as major vehicles for achieving high performance on modern architectures. Parameterized tiling is tiling where the size of blocks is not fixed at compile time but remains a symbolic constant that can be selected/changed even at runtime. Parameterized tiled loops facilitate iterative and runtime optimizations, such as iterative compilation, auto-tuning and dynamic program <b>adaption.</b> Existing <b>solutions</b> to parameterized tiled loop generation are either restricted to perfectly nested loops or difficult to parallelize on distributed memory systems and even on shared memory systems when a program does not have synchronization free parallelism. We present an approach for parameterized tiled loop generation for imperfectly nested loops. We empoly a direct extension of the tiled code generation technique for perfectly nested loops and three simple optimizations on the resulting parameterized tiled loops. The generation as well as the optimizations are achieved purely syntactic processing, hence loop generation time remains negligible. Our code generation technique provides comparabl...|$|R
40|$|A <b>solution</b> <b>adaption</b> {{capability}} for curvilinear near-body grids has been {{implemented in the}} OVERFLOW overset grid computational fluid dynamics code. The approach follows closely that used for the Cartesian off-body grids, but inserts refined grids in the computational space of original near-body grids. Refined curvilinear grids are generated using parametric cubic interpolation, with one-sided biasing based on curvature and stretching ratio of the original grid. Sensor functions, grid marking, and solution interpolation tasks are implemented {{in the same fashion}} as for off-body grids. A goal-oriented procedure, based on largest error first, is included for controlling growth rate and maximum size of the adapted grid system. The adaption process is almost entirely parallelized using MPI, resulting in a capability suitable for viscous, moving body simulations. Two- and three-dimensional examples are presented...|$|R
40|$|An {{improved}} <b>solution</b> <b>adaption</b> capability {{has been}} {{implemented in the}} OVERFLOW overset grid CFD code. Building on the Cartesian off-body approach inherent in OVERFLOW and the original adaptive refinement method developed by Meakin, the new scheme provides for automated creation of multiple levels of finer Cartesian grids. Refinement can {{be based on the}} undivided second-difference of the flow solution variables, or on a specific flow quantity such as vorticity. Coupled with load-balancing and an inmemory solution interpolation procedure, the adaption process provides very good performance for time-accurate simulations on parallel compute platforms. A method of using refined, thin body-fitted grids combined with adaption in the off-body grids is presented, which maximizes the part of the domain subject to adaption. Two- and three-dimensional examples are used to illustrate the effectiveness and performance of the adaption scheme...|$|R
40|$|IT {{solutions}} are integrated bundles of hardware, software {{and services that}} create value for the customer by meeting their individual business needs. We found, that handling IT solutions is complex which results in several challenges that confront decision makers by developing IT solutions, such as the <b>adaption</b> of IT <b>solution</b> components to each other, or the integration to the customer 2 ̆ 7 s environment. To overcome these challenges, we propose a simulation model for prioritizing IT solution developments. The proposed model combines system dynamics and fuzzy logic {{and is based on}} a decision framework, which we derive from a broad literature review. To show the model applicability, we apply it by a mid-sized German company. The simulation results show the priority ranking of IT solution developments. Based on these results decision makers are able to determine the developing and integrating sequence of IT solutions...|$|R
40|$|In {{the public}} {{promotion}} of advanced manufacturing in Germany the several manufacturing technology programmes of the Federal Ministry of Research and Technology (BMFT) and particulary the indirect-specific CIM programme {{play an important}} role. Primarily aimed at the acceleration {{of the use of}} computer integrated manufacturing in German manufacturing technologies industries, the promotion concept also stressed the relevance of planning, organizational considerations and training. Nevertheless, results form the evaluation based on a survey with 850 firms show that the implementation of CIM in industry mostly follows technically-oriented concepts. Even though the programme focussed on the application of CIM techniques it had an important impact on technological development via the amplification of the demand for CIM technologies and support of RTD aimed at the <b>adaption</b> of standard <b>solutions</b> to individual needs. Supplier-user-relationships observed underline the need for a more detailed dis cussion of this factor in social shaping of work and technology...|$|R
40|$|AbstractStandard {{strategies}} for grid <b>adaption</b> in numerical <b>solutions</b> of two-point boundary value problems for systems of ODEs rely on equidistribution of some local {{measure of the}} error. A different approach to grid adaption is to construct grids on which a given discretization method yields higher order approximations. Such a strategy was shown to be successful for some finite difference schemes {{and the results were}} superior to those obtained with other grid adaptions. In this paper we discuss order increasing grid adaption for Runge-Kutta methods. We show that for linear problems simple modifications of basic methods allow the construction of grids with an order-increasing property. These techniques extend to fully nonlinear problems, but the algorithms become costly for most Runge-Kutta schemes. It is shown that a class of saturated Runge-Kutta methods retains the simplicity of the linear case. We give a characterization of this class of methods, propose an order-increasing grid adaption algorithm and present numerical results of some problems with boundary layers to support our findings...|$|R
40|$|As cloud {{computing}} grows in popularity, {{a number of}} techniques enable cloud. Large scale distributed resource management and computation paradigms such as MapReduce, Pregel, enable developers to easily write fault tolerant and scalable applications. Virtualization pro-vides elasticity and lowers down cost and feature rich enterprise storage enables fast <b>adaption</b> of virtualized <b>solutions.</b> We face a number of challenges when designing resource management techniques for both native and virtualized data centers. Firstly, MapReduce job parameter tuning for efficient resource utilization is a daunting and time consuming task. Secondly, while MapReduce model is designed for and leverages information from the native clusters to operate efficiently, cloud can present a virtual cluster topology overlying or hiding actual network information. This results in two placement anomalies: loss of data locality and loss of job locality, where jobs are placed physically away from their data or other associated jobs, adversely affect-ing their performance. Finally, enterprise suffers from expensive centralized storage system provisioning for peak loads...|$|R
40|$|Shock-wave {{flows of}} non-viscous, non-conducting gas are {{considered}} in the paper {{with the aim of}} the development of the flexible discrete model and calculation reliable method for perfect gas non-stationary and steady-state discontinuous flows, uniting advantages of modern quasimonotonic schemes of the approximation heightened order with the use of adaptive non-structurized nets. As a result the investigation new numerical method of gas discontinuous flows, uniting three directions-non-structurized nets, quasimonotonic schemes of the approximation heightened order with the bounded complete variation, the net <b>adaption</b> to the <b>solution</b> by means of local breaking-mergence of meshes has been suggested. Some shock-wave problems have been solved. The G 811 /UG application rackage, developed on the base of the suggested method, makes it possible to calculate two-dimensional and axially symmetric non-stationary and stationary discontinuous flows of non-viscous gas and makes it possible to reduce the calculation time and required memory in comparison with traditional approaches, based on structurized nets for the orderAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Viscous {{calculations}} about geometrically complex {{bodies in}} which there is relative motion between component parts {{is one of the most}} computationally demanding problems facing CFD researchers today. This presentation documents results from the first two years of a CHSSI-funded effort within the U. S. Army AFDD to develop scalable dynamic overset grid methods for unsteady viscous calculations with moving-body problems. The first pan of the presentation will focus on results from OVERFLOW-D 1, a parallelized moving-body overset grid scheme that employs traditional Chimera methodology. The two processes that dominate the cost of such problems are the flow solution on each component and the intergrid connectivity solution. Parallel implementations of the OVERFLOW flow solver and DCF 3 D connectivity software are coupled with a proposed two-part static-dynamic load balancing scheme and tested on the IBM SP and Cray T 3 E multi-processors. The second part of the presentation will cover some recent results from OVERFLOW-D 2, a new flow solver that employs Cartesian grids with various levels of refinement, facilitating <b>solution</b> <b>adaption.</b> A study of the parallel performance of the scheme on large distributed- memory multiprocessor computer architectures will be reported...|$|R
40|$|There are {{relatively}} few studies {{based on an}} individual adult perspective on ADHD which takes into account how various factors interact and affect everyday life. Compensatory strategies can facilitate life for people with ADHD and those strategies might {{include the use of}} assistive technology or cognitive support. The {{purpose of this paper is}} to present and discuss how various factors can facilitate or challenge a person's tendency to adopt technology to better meet the demands and expectations of adult life. Participant observations and narrative interviews were used while co-exploring the life of three adults diagnosed with ADHD. By using this method the participants' own <b>solutions,</b> <b>adaptions</b> and preferences regarding cognitive support and assistive technology become visible. Results showed that factors like negative symptoms, stress, sleep deprivation, financial- or social problems effected the participant's motivation, feelings of competence, ability to identify prioritized activities and to maintain supportive routines – things that have been proven to be important for assistive technology use. Developing useworthy support and technology that meets the needs of people with ADHD is important in order to enable autonomy and compensate for the impairment...|$|R
40|$|The multidimensional self-adaptive grid code, SAGE, {{has proven}} to be a {{flexible}} and useful tool in the solution of complex flow problems. Both 2 - and 3 -D examples given in this report show the code to be reliable and to substantially improve flowfield solutions. Since the adaptive procedure is a marching scheme the code is extremely fast and uses insignificant CPU time compared to the corresponding flow solver. The SAGE program is also machine and flow solver independent. Significant effort was made to simplify user interaction, though some parameters still need to be chosen with care. It is also difficult to tell when the adaption process has provided its best possible solution. This is particularly true if no experimental data are available or if there is a lack of theoretical understanding of the flow. Another difficulty occurs if local features are important but missing in the original grid; the <b>adaption</b> to this <b>solution</b> will not result in any improvement, and only grid refinement can result in an improved solution. These are complex issues that need to be explored within the context of each specific problem...|$|R
40|$|The port {{systems have}} been {{identified}} as major energy consumers. They represent the systems that have difficulty in the <b>adaption</b> of innovative <b>solutions</b> with regard to energy savings and energy efficiency. The most of port systems are using the outdated technology for the measurement of energy consumption and because of the mentioned facts they do not contribute to energy efficiency, environmental protection and sustainable development. On the contrary, seaports are one of the main drivers of the pressure on the environment, especially {{because of the fact that}} most of the seaports and terminals are located close to the urban areas (city areas). This paper presents the proposals relevant for the transformation of seaports into environmental friendly ports, based on the “GREEN PORT” DEVELOPMENT project proposal, which is submitted by the Intermodal Transport Cluster (Croatia), on Adriatic-Ionian Programme INTERREG V-B Transnational 2014 - 2020. Some proposals require advanced technology and resources, while others, such as, thimplementation of the so called model of “Green Port” development do not require any special skills. These proposals represent a small contribution to the great effort to energy efficiency, environment protection and sustainable development...|$|R
40|$|Because of the {{increasingly}} complex geometries involved in flow problems of industrial relevance, numerical methods based on unstructured meshes have become popular in CFD. However, the corresponding meshing methods require a high-quality CAD description of the geometry, which {{is not part of}} the traditional workflow in fields like architecture or medicine. Many professionals also lack the expertise required to build appropriate meshes for flow problems. Nevertheless, recent progresses in meshing technology could overcome these barriers. In this talk, we propose to use anisotropic adaption to generate a nearly body-fitted mesh. The mesh is locally refined depending on a level-set function that describes the geometry without resorting to a CAD model. Dirichlet boundary conditions can then be imposed in a strong manner by node collocation, just as with classical body-fitted meshes. Unlike other treatments of embedded geometries, this technique only requires a standard finite element formulation, without basis enrichement or Lagrange multipliers that alter its numerical properties. In a first step, we apply this method to academic Poisson problems. We show that an appropriate level of local refinement around the geometry recovers the optimal grid convergence rate for the solution, whereas uniform refinement yields first-order convergence. Controlling the anisotropic character of the adaption further enables the error of the geometrical discretization to decrease at optimal rate, while there is no geometrical convergence with isotropic refinement. This affects particularly the computation of integral quantities, such as lift and drag, in practical simulations. Anisotropic adaptive refinement also slows down the growth of the number of unknowns, which limits the computational overhead. Then, we combine the embedded geometry treatment with iterative anisotropic <b>adaption</b> to the <b>solution,</b> for two incompressible flow problems involving respectively a cylinder and a NACA 0012 airfoil. The methodology yields accurate flow solutions, despite very limited user interaction...|$|R
40|$|Hypersonic flow {{analysis}} is performed on an inflatable aerocapture device called a "Ballute" for Titan's Mission. An existing unstructured Cartesian grid methodology {{is used as}} a starting point by taking advantage of its ability to automatically generate grids over any deformed shape of the flexible ballute. The major effort for this thesis work is focused on advancing the existing unstructured Cartesian grid methodology. This includes implementing thermochemical nonequilibrium capability and porting it to a parallel computing environment using a Space-Filling-Curve (SFC) based domain decomposition technique. The implemented two temperature thermochemical nonequilibrium solver governs the finite rate chemical reactions and vibrational relaxation in the high temperature regimes of hypersonic flow. In order to avoid the stiffness problem in the explicit chemical solver, a point implicit method is adopted to calculate the chemical reaction source term. The AUSMPW+ scheme with MUSCL data reconstruction is adopted as the numerical scheme to avoid non-physical oscillations and the carbuncle phenomenon. The results for five species air model and for thirteen species N 2 -CH 4 -Ar model to simulate Titan entry are included for verification against DPLR (NASA Ames' structured grid hypersonic flow solver). The efficient parallel computation of any unstructured grid flow solver requires an adequate grid decomposition strategy because of its complex spatial data structure. The difficulties of even and block-contiguous partitioning in frequently adapting unstructured Cartesian grids are overcome by implementing the 3 D Hilbert SFC. Grids constructed by the SFC for parallel environment promise short inter-CPU communication time while maintaining perfect load balancing between CPUs. The load imbalance due to the local <b>solution</b> <b>adaption</b> is simply apportioned by re-segmenting the curve into even pieces. The detailed structure of the 3 D Hilbert SFC and parallel computing efficiency results based on this grid partition method are also presented. Finally, a structural dynamics tool (LS-DYNA) is loosely coupled with the present parallel thermochemical nonequilibrium flow solver to obtain the deformed surface definition of the ballute. Ph. D. Committee Chair: Stephen M. Ruffin; Committee Member: Marilyn J. Smith; Committee Member: Mitchell Walker; Committee Member: Robert D. Braun; Committee Member: Shuangzhang T...|$|R
40|$|Despite {{the high}} cost of memory and CPU time {{required}} to resolve the boundary layer, a viscous unstructured grid solver has many advantages over a structured grid solver such as the convenience in automated grid generation and shock or vortex capturing by <b>solution</b> <b>adaption.</b> Since the geometry and flow phenomenon of a helicopter are very complex, unstructured grid-based methods are well-suited to model properly the rotor-fuselage interaction than the structured grid solver. In present study, an unstructured Cartesian grid solver is developed {{on the basis of the}} existing solver, NASCART-GT. Instead of cut-cell approach, immersed boundary approach is applied with ghost cell boundary condition, which increases the accuracy and minimizes unphysical fluctuations of the flow properties. The standard k-epsilon model by Launder and Spalding is employed for the turbulence modeling, and a new wall function approach is devised for the unstructured Cartesian grid solver. It is quite challenging and has never done before to apply wall function approach to immersed Cartesian grid. The difficulty lies in the inability to acquire smooth variation of y+ in the desired range due to the non-body-fitted cells near the solid wall. The wall function boundary condition developed in this work yields stable and reasonable solution within the accuracy of the turbulence model. The grid efficiency is also improved with respect to the conventional method. The turbulence modeling is validated and the efficiency of the developed boundary condition is tested in 2 -D flow field around a flat plate, NACA 0012 airfoil, axisymmetric hemispheroid, and rotorcraft applications. For rotor modeling, an actuator disk model is chosen, since it is efficient and is widely verified in the study of the rotor-fuselage interaction. This model considers the rotor as an infinitely thin disk, which carries pressure jump across the disk and allows flow to pass through it. The full three dimensional calculations of Euler and RANS equations are performed for the GT rotor model and ROBIN configuration to test implemented actuator disk model along with the developed turbulence modeling. Finally, the characteristics of the rotor-fuselage interaction are investigated by comparing the numerical solutions with the experiments. Ph. D. Committee Chair: Ruffin, Stephen; Committee Member: Menon, Suresh; Committee Member: Sankar, Lakshmi; Committee Member: Smith, Marc; Committee Member: Smith, Marily...|$|R
40|$|This thesis {{deals with}} the direct {{simulation}} Monte Carlo (DSMC) method of analysing gas flows. The DSMC method was initially proposed as a method for predicting rarefied flows where the Navier-Stokes equations are inaccurate. It has now been extended to near continuum flows. The method models gas flows using simulation molecules which represent {{a large number of}} real molecules in a probabilistic simulation to solve the Boltzmann equation. Molecules are moved through a simulation of physical space in a realistic manner that is directly coupled to physical time such that unsteady flow characteristics are modelled. Intermolecular collisions and moleculesurface collisions are calculated using probabilistic, phenomenological models. The fundamental assumption of the DSMC method is that the molecular movement and collision phases can be decoupled over time periods that are smaller than the mean collision time. Two obstacles to the wide spread use of the DSMC method as an engineering tool are in the areas of simulation configuration, which is the configuration of the simulation parameters to provide a valid solution, and the time required to obtain a solution. For complex problems, the simulation will need to be run multiple times, with the simulation configuration being modified between runs to provide an accurate solution for the previous run's results, until the solution converges. This task is time consuming and requires the user to have a good understanding of the DSMC method. Furthermore, the computational resources required by a DSMC simulation increase rapidly as the simulation approaches the continuum regime. Similarly, the computational requirements of three-dimensional problems are generally two orders of magnitude more than two-dimensional problems. These large computational requirements significantly limit the range of problems that can be practically solved on an engineering workstation or desktop computer. The first major contribution of this thesis is {{in the development of a}} DSMC implementation that automatically adapts the simulation. Rather than modifying the simulation configuration between solution runs, this thesis presents the formulation of algorithms that allow the simulation configuration to be automatically adapted during a single run. These adaption algorithms adjust the three main parameters that effect the accuracy of a DSMC simulation, namely the solution grid, the time step and the simulation molecule number density. The second major contribution extends the parallelisation of the DSMC method. The implementation developed in this thesis combines the capability to use a cluster of computers to increase the maximum size of problem that can be solved while simultaneously allowing excess computational resources to decrease the total solution time. Results are presented to verify the accuracy of the underlying DSMC implementation, the utility of the <b>solution</b> <b>adaption</b> algorithms and the efficiency of the parallelisation implementation...|$|R
40|$|Mobile {{commerce}} through smart phones as {{an extension}} of the traditional electronic commerce is becoming widely implemented together with the IT development and increased usage of smart phones. The thesis is limited to the design factors to consider when adapting to m-commerce for the apparel industry, since this industry has a long experience in online and mail order business. Sales of clothing and footwear have its roots in the mail order business, which means that many users are already accustomed to purchase at a distance, which had facilitated the development. Adapting to the m-commerce trend has resulted in both profitable and user related benefits, but there are also some difficulties during the adaption to the several m-commerce solutions as well as to offer the user an accessible design and user experience. It is common that the smartphone is the only mobile device one has at the moment when a need occurs for product search or media consumption. Even though the screen size and the connection speed of a smart phone, people might at least make an attempt to use their smartphones for these purposes. For example, users keep having experiences of struggling with an unwieldy website on the smartphone to get something done. In conjunction with faster mobile networks and more powerful mobile devices, this behavior will become only more prevalent. The purpose is therefore to investigate the important design factors that exist when adapting to m-commerce and to find out what of these factors are important for the apparel industry to consider when adapting to m-commerce in order to develop an m-commerce design most suitable for the user experience. To obtain the purpose the main research question ”What are the design factors to consider for the apparel industry today when adapting to m-commerce?” is being answered in this thesis. To achieve the purpose of the study, qualitative process and data as well as quantitative process and data were used. From scientific articles, related work and independent surveys a theoretical review was carried out. The literature review has been conducted through search engines such as Summon from Högskolan i Borås and Google Scholar. The empirical study consist of an online self-completion questionnaire done by 40 respondents that has been distributed through social media, and two person-to-person interviews with the two Swedish companies in the apparel industry, Eton AB and a Swedish web shop. In the information collection we have always been concerned to get relevant, unbiased and reliable information. We have evaluated all the materials by the alternative criteria, trustworthiness as mentioned in Bryman & Bell (2011). This is done in order to uphold credibility, transferability, dependability and conformability in the material and conclusions founded in the study. The results from this study involve several factors focusing on the design that may be important when adapting to an m-commerce solution in the apparel industry. These factors are drawn from analysis and discussion of the collected theory and the empirical data, from different perspectives including; user habits, user expectations, perceived security contra actual security, the transaction chain, and payment methods. Factors that are found are: The importance to adapt to m-commerce, even if the smart phone traffic and orders are low at the moment; To achieve a good and trustworthy user interface; To have different options for the user, including payment and access options; The ease of use and navigation on the web shop; The visibility of products and to have clear information on the web shop; Learnability: to make it simple for the users to use the web shop; Efficiency: to secure so that the users don’t has to spend too much time and energy to complete their task; Effectiveness: concerning the speed and to which degree the user successfully complete their task; Satisfaction is critical since if the m-commerce system does not appeals to the user they may feel less inclined to use it; Payment systems concerning the <b>adaption</b> to payment <b>solutions</b> for m-commerce web shops; Liability consider who is responsible of what parts as well as making it easy for the users to understand who to turn to if they have questions or complaints; Privacy and security, considering how to defend the m-commerce solution towards increased security threats users may face as well as making the users trusting the web shop enough to use the solution...|$|R

