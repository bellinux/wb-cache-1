19|10000|Public
30|$|The {{analysis}} of dyes in cultural heritage samples is <b>a</b> <b>well-known</b> <b>challenging</b> task, {{due to their}} inherent high tinting strength and consequent low concentration in the carrying matrix a fact that severely limits the number of analytical techniques that can be efficiently and micro-destructively employed for their detection and unambiguous identification.|$|E
40|$|Burst {{contention}} is <b>a</b> <b>well-known</b> <b>challenging</b> problem in Optical Burst Switching (OBS) networks. Deflection routing {{is used to}} resolve contention. Burst retransmission is used to reduce the Burst Loss Ratio (BLR) by retransmitting dropped bursts. Previous works show that combining deflection and retransmission outperforms both pure deflection and pure retransmission approaches. This paper proposes a new Adaptive Hybrid Deflection and Retransmission (AHDR) approach that dynamically combines deflection and retransmission approaches based on network conditions such as BLR and link utilization. Network Simulator 2 (ns- 2) is used to simulate the proposed approach on different network topologies. Simulation {{results show that the}} proposed approach outperforms static approaches in terms of BLR by using an adaptive decision threshold. Comment: 6 pages, conferenc...|$|E
40|$|Sunspot {{series is}} {{a record of the}} {{activities}} of the surface of the sun. It is chaotic and is <b>a</b> <b>well-known</b> <b>challenging</b> task for time series analysis. In this paper, we show that we can approximate the transformed sequence with a discrete-time recurrent neural networks. We apply a new smoothing technique by integrating the original sequence twice with mean correction and also normalize the smoothened sequence to [- 1, 1]. The smoothened sequence is divided into a few segments and each segment is approximated by a neuron of a discrete-time fully connected neural network. Our approach is based on the universal approximation property of discrete-time recurrent neural network. The relation between the least square error and the network size are discussed. The results are compared with the linear time series models. Department of Applied Mathematic...|$|E
40|$|Encouraging {{contribution}} to online communities is <b>a</b> <b>well-known</b> <b>challenge.</b> CompEdNet, <b>an</b> online professional networking community for computer science teachers, {{have been working}} towards a solution for motivating contribution and engagement. They have developed a framework for rewarding positive behaviours by their members, through issuing digital badges using Mozilla’s Open Badge Infrastructure (OBI) ...|$|R
40|$|Placement of {{constricting}} devices {{around the}} penis and scrotum for autoerotic purposes or increasing sexual performance represents <b>a</b> <b>well-known</b> <b>challenge</b> for urologists and {{can result in}} serious complications. The removal of the constricting devices can be challenging and often requires resourcefulness and multidisciplinary approach. We report one case of successful removal of a penoscrotal constricting metal ring in a 49 -year-old male using a hand-held orthopaedic saw under ketamine and midazolam sedation in the emergency department...|$|R
40|$|Abstract. Representing part-whole and mereotopological {{relations}} in an ontology is <b>a</b> <b>well-known</b> <b>challenge.</b> We have structured 23 types of part-whole relations and hidden {{the complexities of}} the underlying mereotopological theory behind a user-friendly tool: OntoPartS. It automates modelling guidelines using, mainly, the categories from DOLCE so as to take shortcuts in the selection process, and it includes examples and verbalizations to increase understandability. The modeller’s domain ontology, represented in any of the OWL species, can be updated automatically with the selected relation with a simple one-click button. ...|$|R
40|$|The minimum-time control problem {{consists}} {{in finding a}} control policy that will drive a given dynamic system from a given initial state to a given target state (or a set of states) as quickly as possible. This is <b>a</b> <b>well-known</b> <b>challenging</b> problem in optimal control theory for which closed-form solutions exist {{only for a few}} systems of small dimensions. This paper presents a very generic solution to the minimum-time problem for arbitrary discrete-time linear systems. It is a numerical solution based on sparse optimization, that is the minimization of the number of nonzero elements in the state sequence over a fixed control horizon. We consider both single input and multiple inputs systems. An important observation is that, contrary to the continuous-time case, the minimum-time control for discrete-time systems is not necessarily entirely bang-bang...|$|E
40|$|Abstract—Quantifying the {{end-to-end}} delay {{performance in}} multihop wireless networks is <b>a</b> <b>well-known</b> <b>challenging</b> problem. In this paper, we propose a new joint congestion control and scheduling algorithm for multihop wireless networks with fixed-route flows operated under a general interference model with interference degree K. Our proposed algorithm not only achieves a provable throughput guarantee (which {{is close to}} at least 1 =K of the system capacity region), but also leads to explicit upper bounds on the end-to-end delay of every flow. Our end-to-end delay- and throughput-bounds are in simple and closed forms, and they explicitly quantify the tradeoff between throughput and delay of every flow. Further, the per-flow end-to-end delay bound increases linearly {{with the number of}} hops that the flow passes through, which is order-optimal with respect to the number of hops. Unlike traditional solutions based on the back...|$|E
40|$|Multi-mode {{resource}} and precedence-constrained project scheduling is <b>a</b> <b>well-known</b> <b>challenging</b> real-world optimisation problem. An important {{variant of the}} problem requires scheduling of activities for multiple projects considering availability of local and global resources while respecting a range of constraints. A critical aspect of the benchmarks addressed in this paper is that the primary objective is to minimise {{the sum of the}} project completion times, with the usual makespan minimisation as a secondary objective. We observe that this leads to an expected different overall structure of good solutions and discuss the effects this has on the algorithm design. This paper presents a carefully-designed hybrid of Monte-Carlo tree search, novel neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The implementation is also engineered to increase the speed with which iterations are performed, and to exploit the computing power of multicore machines. Empirical evaluation shows that the resulting information-sharing multi-component algorithm significantly outperforms other solvers on a set of “hidden” instances, i. e. instances not available at the algorithm design phase...|$|E
30|$|From a {{historical}} radiological point of view, air and bony structures {{have been considered}} enemies of ultrasonography. Air from bowel gas is <b>a</b> <b>well-known</b> <b>challenge</b> in ultrasonography, e.g. the air reduces the diagnostic view to underlying abdominal parenchyma. Several studies have, however, established that the air artefacts can often be used in clinical practice, rather than being an annoyance to the physician performing sonography [2]. By understanding the generated air artefacts seen with ultrasonography, the information can serve as an important diagnostic tool. The technical explanation of air artefacts and how to understand the information gleaned from them are explained below.|$|R
40|$|Placement of {{constricting}} devices {{around the}} penis for autoerotic purposes or increasing of sexual performance represents <b>a</b> <b>well-known</b> <b>challenge</b> for urologists. Penile incarceration is a urologic emergency with potentially severe clinical consequences. In many cases a rapid intervention and a sudden {{removal of the}} foreign body it is enough so that patients need no further intervention. We report three different cases of strangulating objects (metallic ring, metal bearing and plumbing pipe) presented at our emergency department and three different methods of devices extraction practiced. Remove these devices can be challenging and often requires resourcefulness and multidisciplinary approach...|$|R
40|$|Low {{organizational}} commitment of employees is <b>a</b> <b>well-known</b> <b>challenge</b> for organizations nowadays. In order to actively manage and promote {{organizational commitment}}, organizations {{need to be}} aware of its most important influencing factors. This thesis focuses on the exploration of the potential of human resource development to improve employee commitment. An empirical study has been conducted at the shop floor area of the project partner thyssenkrupp Aufzugsweke GmbH in order to investigate the negative influences on organizational commitment, identify the potential positive impacts of human resources development practices and provide the organization with a recommendation about suitable measures to retain and develop organizational commitment...|$|R
40|$|Burst {{contention}} is <b>a</b> <b>well-known</b> <b>challenging</b> problem in Optical Burst Switching (OBS) networks. Contention resolution approaches are always reactive {{and attempt to}} minimize the BLR based on local information available at the core node. On the other hand, a proactive approach that avoids burst losses before they occur is desirable. To reduce the probability of burst contention, a more robust routing algorithm than the shortest path is needed. This paper proposes a new routing mechanism for JET-based OBS networks, called Graphical Probabilistic Routing Model (GPRM) that selects less utilized links, on a hop-by-hop basis by using a bayesian network. We assume no wavelength conversion and no buffering to be available at the core nodes of the OBS network. We simulate the proposed approach under dynamic load to demonstrate that it reduces the Burst Loss Ratio (BLR) compared to static approaches by using Network Simulator 2 (ns- 2) on NSFnet network topology and with realistic traffic matrix. Simulation results clearly show that the proposed approach outperforms static approaches in terms of BLR. Comment: 6 page...|$|E
40|$|The {{probability}} density {{function of the}} sum of Log-normally distributed random variables (RVs) is <b>a</b> <b>well-known</b> <b>challenging</b> problem. For instance, an analytical closed-form expression of the Log-normal sum distribution does not exist and is still an open problem. A crude Monte Carlo (MC) simulation is of course an alternative approach. However, this technique is computationally expensive especially when dealing with rare events (i. e. events with very small probabilities). Importance Sampling (IS) is a method that improves the computational efficiency of MC simulations. In this paper, we develop an efficient IS method for the estimation of the Complementary Cumulative Distribution Function (CCDF) of the sum of independent and not identically distributed Log-normal RVs. This technique is based on constructing a sampling distribution via twisting the hazard rate of the original probability measure. Our main {{result is that the}} estimation of the CCDF is asymptotically optimal using the proposed IS hazard rate twisting technique. We also offer some selected simulation results illustrating the considerable computational gain of the IS method compared to the naive MC simulation approach...|$|E
40|$|Estimating the {{probability}} that a sum of random variables (RVs) exceeds a given threshold is <b>a</b> <b>well-known</b> <b>challenging</b> problem. A naive Monte Carlo simulation is the standard technique for the estimation of this type of probability. However, this approach is computationally expensive, especially when dealing with rare events. An alternative approach is represented by the use of variance reduction techniques, known for their efficiency in requiring less computations for achieving the same accuracy requirement. Most of these methods have thus far been proposed to deal with specific settings under which the RVs belong to particular classes of distributions. In this paper, we propose a generalization of the well-known hazard rate twisting Importance Sampling-based approach that presents the advantage of being logarithmic efficient for arbitrary sums of RVs. The wide scope of applicability of the proposed method is mainly due to our particular way of selecting the twisting parameter. It is worth observing that this interesting feature is rarely satisfied by variance reduction algorithms whose performances were only proven under some restrictive assumptions. It comes along with a good efficiency, illustrated by some selected simulation results comparing the performance of the proposed method with some existing techniques...|$|E
40|$|AbstractThe prompt {{diagnosis}} and treatment of massive pulmonary embolism is <b>a</b> <b>well-known</b> <b>challenge</b> for physicians. We report a case of a 61 -year-old hemodynamically unstable man who presented to the emergency department with complaints of acute dyspnea. After performing a focused history and physical, we used bedside ultrasound to diagnose significant right heart strain, which suggested massive bilateral pulmonary embolisms. This diagnosis was further supported by the visualization of deep venous thrombosis in the left lower extremity. The patient was treated with IV tissue plasminogen activator in the emergency department and survived to discharge in his usual state of health...|$|R
40|$|Lack of robustness, manifesting as an {{inability}} to suppress the growth of numerical round-off errors, is <b>a</b> <b>well-known</b> <b>challenge</b> with many conventional two-fluid models. In the current work, a hyperbolic two-phase mixture model in conservative form is analysed and assessed {{in relation to a}} single-pressure two-fluid model. The robustness of the simulation results indicates that the mixture model and the associated numerical methods may be well suited to enable high-resolution simulations of more complex two-phase phenomena. In particular, significant advantages in terms of stability and robustness of the mixture model formulation are found in comparison to the non-hyperbolic and non-conservative single-pressure two-fluid model...|$|R
40|$|<b>A</b> <b>well-known</b> <b>challenge</b> in data {{warehousing}} is the efficient incremental maintenance of warehouse {{data in the}} presence of source data updates. In this paper, we identify several critical data representation and algorithmic choices that must be made when developing the machinery of an incrementally maintained data warehouse. For each decision area, we identify various alternatives and evaluate them through extensive experiments. We show that picking the right alternative can lead to dramatic performance gains, and we propose guidelines for making the right decisions under different scenarios. All of the issues addressed in this paper arose in our development of WHIPS, a prototype {{data warehousing}} system supporting incremental maintenance. ...|$|R
40|$|Estimating the {{probability}} that a sum of random variables (RVs) exceeds a given threshold is <b>a</b> <b>well-known</b> <b>challenging</b> problem. Closed-form expressions for the sum distribution do not generally exist, which has led to an increasing interest in simulation approaches. A crude Monte Carlo (MC) simulation is the standard technique for the estimation of this type of probability. However, this approach is computationally expensive, especially when dealing with rare events. Variance reduction techniques are alternative approaches that can improve the computational efficiency of naive MC simulations. We propose an Importance Sampling (IS) simulation technique based on the well-known hazard rate twisting approach, that presents the advantage of being asymptotically optimal for any arbitrary RVs. The wide scope of applicability of the proposed method is mainly due to our particular way of selecting the twisting parameter. It is worth observing that this interesting feature is rarely satisfied by variance reduction algorithms whose performances were only proven under some restrictive assumptions. It comes along with a good efficiency, illustrated by some selected simulation results comparing the performance of our method with that of an algorithm based on a conditional MC technique...|$|E
40|$|Abstract — Optical burst {{switching}} (OBS) is a switching {{technique that}} was proposed as a hybrid switching technology {{to support the}} next generation Internet. In OBS, incoming IP packets are assembled into super-sized packets called data bursts. Burst contention is <b>a</b> <b>well-known</b> <b>challenging</b> problem in Optical Burst Switching (OBS) networks. Burst contention can be resolved using several approaches, such as wavelength conversion, buffering based on fiber delay line (FDL) or deflection routing Retransmission technique is used to reduce the Burst Loss Ratio (BLR) by deflecting dropped bursts. Segmentation also resolves contention by dividing the contended burst into smaller parts called segments. Combining deflection routing technique and retransmission technique outperforms both pure deflection and pure retransmission techniques to improve the performance. Previous work uses only static combination of retransmission and deflection of bursts to reduce contention. This paper proposes a dynamic protocol to resolve contention based on combining deflection, retransmission and delaying bursts to improve the OBS performance. Experiments were conducted to test the proposed protocol. The proposed technique was tested on complex models such as NSFNET and COST 238 topologies. Results show that the proposed protocol outperforms existing techniques in terms of burst lost ratio. Index Terms — OBS networks, contention resolution techniques, retransmission techniques...|$|E
40|$|Quantifying the {{end-to-end}} delay {{performance in}} multihop wireless networks is <b>a</b> <b>well-known</b> <b>challenging</b> problem. In this paper, we propose a new joint congestion control and scheduling algorithm for multihop wireless networks with fixedroute flows operated under a general interference model with interference degree K. Our proposed algorithm not only achieves a provable throughput guarantee (which {{is close to}} at least 1 =K of the system capacity region), but also leads to explicit upper bounds on the end-to-end delay of every flow. Our end-to-end delay- and throughput-bounds are in simple and closed forms, and they explicitly quantify the tradeoff between throughput and delay of every flow. Further, the per-flow end-to-end delay bound increases linearly {{with the number of}} hops that the flow passes through, which is order-optimal with respect to the number of hops. Unlike traditional solutions based on the backpressure algorithm, our proposed algorithm combines windowbased flow control with a new rate-based distributed scheduling algorithm. A key contribution of our work is to use a novel stochastic dominance approach to bound the corresponding perflow throughput and delay, which otherwise are often intractable in these types of systems. Our proposed algorithm is fully distributed and requires a low per-node complexity that does not increase with the network size. Hence, it can be easily implemented in practice...|$|E
40|$|ETL {{jobs are}} used to {{integrate}} data from distributed and heterogeneous sources into a data warehouse. <b>A</b> <b>well-known</b> <b>challenge</b> {{in this context is}} the development of incremental ETL jobs for efficiently maintaining warehouse data in the presence of source data updates. In this paper, we present a new transformation-based approach to automatically derive incremental ETL jobs. To this end, we consider a simplification of the underlying update propagation process based on the computation of so-called safe updates instead of true ones. Additionally, we identify the limitations of already proposed incremental solutions, which are cured by employing Magic Sets leading to dramatic performance gains. 1...|$|R
40|$|Up to now, all {{existing}} completeness {{results for}} ordered paramodulation and Knuth-Bendix completion require the term ordering to be well-founded, monotonic and total(izable) on ground terms. For several applications, these requirements are too strong, and hence weakening {{them has been}} <b>a</b> <b>well-known</b> research <b>challenge.</b> Here w...|$|R
40|$|Natural {{language}} ambiguity is <b>a</b> <b>well-known</b> <b>challenge</b> for the NLP {{community in}} deepening {{the performances of}} the computer programs when dealing with human languages. Language ambiguity as produced by humans, is often unnoticed and as such, is {{most of the times}} involuntary. However, in many cases ambiguity is purposely used for various reasons. In an original context, a sentence might be very clear with respect to the producer's intentions, but if it contains some unnoticed ambiguities (obliterated by the context), when put in another context, might convey a very different meaning, sometimes funny, sometimes embarrassing. We describe a system which is able to pinpoint such potential misinterpretations and advise the writer to make other lexical choices for his/her wording. ...|$|R
40|$|Filling the {{gap between}} {{molecular}} structure and reactivity is <b>a</b> <b>well-known</b> <b>challenging</b> task in chemistry. The rational design of catalysts may greatly benefit of computational aid, provided state-of-the-art methodologies are employed. The case of metal catalyzed [2 + 2 + 2] cycloaddition of alkynes/alkynes-nitriles to benzene/pyridine is investigated in detail, due to the paramount importance of these reactions for the synthesis of cyclic and polycyclic organic compounds. Catalysts of general formula Cp’M are considered, where Cp’ is the cyclopentadienyl anion or the cyclopentadienyl moiety of larger polycyclic aromatic/heteroaromatic ligands, and M=Co, Rh, Ir. Energy profiles of the whole cycles {{with a number of}} intermediates ranging from 5 to 9 connected by the corresponding transition states are computed and the catalyst performance is evaluated based on its turnover frequency (TOF), by implementing the equations of the energy span model. TOF values are related to peculiar structural features of the Cp’M fragment, i. e. to the M-Cp’ bonding mode which results in slippage phenomena during the catalytic cycle. In fact, the metal is never coordinated to the five carbon atoms ring in highly symmetric fashion (eta 5), but is slipped and the amount of this distortion changes during the various steps of the catalysis. This fluxionality is found to affect importantly the efficiency of the catalyst...|$|E
40|$|Estimating the {{probability}} that a sum of random variables (RVs) exceeds a given threshold is <b>a</b> <b>well-known</b> <b>challenging</b> problem. Closed-form expression of the sum distribution is usually intractable and presents an open problem. A crude Monte Carlo (MC) simulation is the standard technique for the estimation of this type of probability. However, this approach is computationally expensive especially when dealing with rare events (i. e events with very small probabilities). Importance Sampling (IS) is an alternative approach which effectively improves the computational efficiency of the MC simulation. In this paper, we develop a general framework based on IS approach for the efficient estimation of {{the probability}} that the sum of independent and not necessarily identically distributed heavy-tailed RVs exceeds a given threshold. The proposed IS approach is based on constructing a new sampling distribution by twisting the hazard rate of the original underlying distribution of each component in the summation. A minmax approach is carried out for the determination of the twisting parameter, for any given threshold. Moreover, using this minmax optimal choice, the estimation of the probability of interest is shown to be asymptotically optimal as the threshold goes to infinity. We also offer some selected simulation results illustrating first the efficiency of the proposed IS approach compared to the naive MC simulation. The near-optimality of the minmax approach is then numerically analyzed...|$|E
40|$|The Iterated Prisoner’s Dilemma (IPD) is <b>a</b> <b>well-known</b> <b>challenging</b> {{problem for}} researching multi-agent {{interactions}} in competitive and cooperative situations. In this paper, we present the Ask-First (AF) strategy for playing multi-agent non-Iterated PD (nIPD) {{that is based}} on evolving trust chains between agents. Each agent maintains a (relatively small) table containing trust values of other agents. When agents are to play each other, they ask their neighbours what trust they put in the opponent. Chains are then followed until an agent is found that knows the opponent and the trust value is propagated back through the chain. The played move is then decided based upon this trust value. When two agents have played each other, they update their trust tables {{on the basis of the}} outcome of the game. The strategy is first evaluated in a benchmark scenario where it is shown that it outperforms a number of benchmark strategies. Secondly, we evaluate the strategy in a scenario with a group of colluding agents. The experiments show that the AF strategy is successful here as well. We conclude that the AF strategy is a highly flexible, scalable and distributed way (the chain topology adapts to the way that agents are picked to play each other) to deal with a difficult multi-agent nIPD problem (i. e., robust against collusions). ...|$|E
40|$|AbstractFinding {{the plane}} {{shape of the}} double curved {{surfaces}} is <b>a</b> <b>well-known</b> <b>challenge</b> for every design engineer dealing with either fiber reinforced plastics lightweight designs or textile architectural membranes. A novel approach for generating optimized cutting patterns including nonlinear isotropic and anisotropic material behavior is presented. The so-called Variation of Reference Strategy {{can be seen as}} an inverse approach, defining the nodal positions in the material configuration as design variables holding the spatial configuration fixed. Thereby, the stress-free state of the cutting pattern which is an important characteristic of the manufacturing process is preserved. In order to demonstrate the abilities and robustness of the Variation of Reference Strategy several numerical examples considering different kind of materials are presented...|$|R
40|$|Abstract: Synthesis of nanocrystals with exposed high-energy facets is <b>a</b> <b>well-known</b> <b>challenge</b> in many {{fields of}} science and technology. The higher {{reactivity}} of these facets simultaneously makes them desirable catalysts for sluggish chemical reactions and leads to their small populations in an equilibrated crystal. Using anatase TiO 2 as an example, we demonstrate a facile approach for creating high surface area, stable nanosheets comprised of nearly 100 % exposed (001) facets. Our approach relies on spontaneous assembly of the nanosheets into three-dimensional, hierarchical spheres that stabilizes them from collapse. We show that the high surface density of exposed TiO 2 (001) facets leads to fast lithium insertion/deinsertion processes in batteries that mimic features seen in high power electrochemical capacitors...|$|R
40|$|Debugging {{distributed}} applications is <b>a</b> <b>well-known</b> <b>challenge</b> {{within the realm}} of Computer Science. Common problems faced by developers include: lack of an observable global state, lack of a central location from where to monitor possible states, non-deterministic execution, heisenbugs, and many others. There are currently many good techniques available which could be employed in building a tool for circumventing some of those issues, especially when considering widespread middleware-induced models such as Java RMI, CORBA or Microsoft. NET based applications. In this paper, we introduce an extended symbolic debugger for Eclipse which besides usual source-level debugging capabilities, adds to the abstraction pool a distributed thread concept, central to causality in any synchronous-call distributed object application. ...|$|R
40|$|AbstractRecently {{data stream}} has been {{extensively}} explored due to its emergence in {{a great deal of}} applications such as sensor networks, web click streams and network flows. One of the most important challenges in data streams is concept change where data underlying distributions change from time to time. A vast majority of researches in the context of data stream mining are devoted to labeled data, whereas, in real word human practice label of data are rarely available to the learning algorithms. Moreover, most of the methods that detect changes in unlabeled data stream merely deal with numerical data sets, and also, they are facing considerable difficulty when dimension of data tends to increase. In this paper, we present a Precise Statistical approach for Concept Change Detection in unlabeled data streams, which, abbreviated as PSCCD, detects changes using an exchangeable test. This hypothesis test is driven from a martingale which is based on Doob’s Maximal Inequality. The advantages of our approach are three fold. First, it does not require a sliding window on the data stream whose size is <b>a</b> <b>well-known</b> <b>challenging</b> issue; second, it works well in multi-dimensional data stream, and last but not the least, it is applicable to different types of data including categorical, numerical and mixed-attribute data streams. To explore the advantages of our approach, quite a lot of experiments with different settings and specifications are conducted. The obtained results are very promising...|$|E
40|$|Abstract. Most {{computational}} {{problems for}} matrix semigroups and groups are inherently difficult to solve and even undecidable starting from dimension three. The {{questions about the}} decidability and complexity of problems for two-dimensional matrix semigroups remain open and are directly linked with other challenging problems in the field. In this paper we study the computational complexity {{of the problem of}} determining whether the identity matrix belongs to a matrix semigroup (the Identity Problem) generated by a finite set of 2 × 2 integral unimodular matrices. The Identity Problem for matrix semigroups is <b>a</b> <b>well-known</b> <b>challenging</b> problem, which has remained open in any dimension until recently. It is currently known that the problem is decidable in dimension two and undecidable starting from dimension four. In particular, we show that the Identity Problem for 2 × 2 integral unimodular matrices is NP-hard by a reduction of the Subset Sum Problem and several new encoding techniques. An upper bound for the nontrivial decidability result by C. Choffrut and J. Karhumäki is unknown. However, we derive a lower bound on the minimum length solution to the Identity Problem for a constructible set of instances, which is exponential in the number of matrices of the generator set and the maximal element of the matrices. This shows that the most obvious candidate for an NP algorithm, which is to guess the shortest sequence of matrices which multiply to give the identity matrix, does not work correctly since the certificate would have a length which is exponential {{in the size of the}} instance. Both results lead to a number of corollaries confirming the same bounds for vector reachability, scalar reachability and zero in the right upper corner problems. ...|$|E
40|$|<b>A</b> <b>well-known</b> <b>challenge</b> during {{processor}} {{design is}} to obtain best possible results {{for a typical}} target applica-tion domain by combining flexibility and computational performance. ASIPs (Application Specific Instruction Set Processors) provide a tradeoff between generality of processor (flexibility) and its physical characteristics (computational performance and silicon area). This paper evaluates an ASIP design methodology based on the extension of an existing instruction set and architecture described with LISA 2. 0 language. The objective is to accelerate the ASIPs design process by using partially predefined, configurable RISC-like em-bedded processor cores that can be quickly tuned to given applications by means of ISE (Instruction Set Extension) techniques. A case study demonstrates the methodological approach for the JPEG algorithm and motion estimation encoding algorithm of H. 264 encod-ing standard. 1...|$|R
40|$|Abstract. Representing and {{reasoning}} over mereotopological relations (parthood and location) in an ontology is <b>a</b> <b>well-known</b> <b>challenge,</b> {{because there are}} many relations to choose from and OWL has limited expressiveness in this regard. To address these issues, we structure mereotopological relations based on the KGEMT mereotopological theory. A correctly chosen relation counterbalances some weaknesses in OWL’s representation {{and reasoning}} services. To achieve effortless selection of the appropriate relation, we hide the complexities of the underlying theory through automation of modelling guidelines in the new tool OntoPartS. It uses, mainly, the categories from DOLCE [17], which avoids lengthy question sessions, and it includes examples and verbalizations. OntoPartS was experimentally evaluated, which demonstrated that selecting and representing the desired relation was done efficiently and more accurately with OntoPartS. ...|$|R
40|$|The {{problem of}} speech {{segmentation}} is <b>a</b> <b>well-known</b> <b>challenge</b> for various studies, such as language acquisition: how do children correctly infer {{the position of}} word boundaries in the continuous stream of speech? One solution to this problem, {{referred to as the}} utterance-boundary strategy, is to reuse the information provided by the occurrence of specific phonemes sequences at utterance edges in order to hypothesize boundaries inside utterances. In this paper, we describe a probabilistic and incremental implementation of this approach and discuss the results observed for a word segmentation task on a phonemically transcribed and child-oriented French corpus. We show in particular that the first boundaries inferred by this algorithm seem to be reliable enough to make useful generalizations for later decisions...|$|R
40|$|Resistance to broad-spectrum {{antimicrobials}} is <b>a</b> <b>well-known</b> <b>challenge</b> {{in treating}} Enterobacteriaceae infection worldwide. Carbapenems {{are an important}} class of antimicrobials used in treatment against these organisms, although increasing resis-tance to carbapenems has been reported among Enterobacteria-ceae [1]. The production of OXA- 48 was first described in Klebsiella pneumoniae isolates from Istanbul [2]. Although most reports describe single cases [3], important outbreaks have also been reported. K. pneumoniae and other Enterobacteriaceae strains with OXA- 48 carbapenemase are now spreading from the Mid-dle East to Europe, Asia, and North America. In addition, NDM- 1 (another carbapenemase) was first identified in a K. pneumoniae isolate from a Swedish patient who had been treated in India in 2009 [4]. NDM- 1 has since been reported in numerous isolates, predominantly in Escherichia coli and K. pneumonia, in man...|$|R
