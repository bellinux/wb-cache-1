17|55|Public
40|$|Abstract: The project {{includes}} {{implementation of}} gateway antivirus with open source methodologies along with benchmark testing and performance improvement. The gateway antivirus solution implementation would include antivirus protection <b>at</b> <b>HTTP,</b> FTP, SMTP & POP 3 protocols at the gateway level. Benchmark testing with various attack scenarios {{would be included}} along with performance analysis {{for the improvement of}} gateway level antivirus solutions in various deployment scenarios...|$|E
40|$|User-perceived latency is {{recognized}} as the central performance problem in the Web. We systematically measure factors contributing to this latency, across several locations. Our study reveals that DNS query times, TCP connection establishment, and start-of-session delays <b>at</b> <b>HTTP</b> servers, more so than transmission time, are major causes of long waits. Wait due to these factors also afflicts high-bandwidth users and has detrimental effect on perceived performance...|$|E
30|$|Scenario B {{is based}} on the system {{architecture}} B shown in Fig. 2 and described above. Here, we use a RESTful web service of commercial software for collecting data from energy analyzers. Due to the limits of web services by requesting a large amount of data [24], among other things, we test how many data are possible to request over the web service before the request run into a FULL HEAD error <b>at</b> <b>HTTP</b> header.|$|E
40|$|There {{has been}} {{tremendous}} progress in understanding how bandwidth {{is shared by}} TCP-like connections. By associating each TCPlike connection with a utility function, the bandwidth sharing problem of TCP-like connections can be modelled as a distributed optimization problem for utility functions. However, little is known on how bandwidth is shared by HTTP-like connections through their utility functions at the TCP level. One of the main objectives {{of this paper is}} to provide a theory for bandwidth sharing of a large number of HTTP-like connections. Based on certain technical assumptions, we show that there is a utility function <b>at</b> the <b>HTTP</b> level for an HTTP-like connection and such a utility function can be derived from the utility function at the TCP level. The bandwidth is then shared by HTTP-like connections through utility functions <b>at</b> the <b>HTTP</b> level. Moreover, there is a probabilistic interpretation for how the utility function <b>at</b> the <b>HTTP</b> level is related to the utility function at the TCP level. This is done by relating utility functions to large deviation rate functions...|$|R
50|$|Web2py {{is unique}} {{in the world of}} Python web {{frameworks}} because models and controllers are executed, not imported. They are not modules. They are executed in a single global environment which is initialized <b>at</b> each <b>http</b> request. This design decision has pros and cons.|$|R
5000|$|<b>At</b> the <b>HTTP</b> level, a 404 {{response}} code {{is followed by}} a human-readable [...] "reason phrase". The HTTP specification suggests the phrase [...] "Not Found" [...] and many web servers by default issue an HTML page that includes both the 404 code and the [...] "Not Found" [...] phrase.|$|R
40|$|This {{document}} {{defines the}} HTTP Cookie and Set-Cookie header fields. These header fields {{can be used}} by HTTP servers to store state (called cookies) <b>at</b> <b>HTTP</b> user agents, letting the servers maintain a stateful session over the mostly stateless HTTP protocol. Although cookies have many historical infelicities that degrade their security and privacy, the Cookie and Set-Cookie header fields are widely used on the Internet. This document obsoletes RFC 2965. Status of This Mem...|$|E
40|$|Network data {{restoration}} technology includes five basic modules: network packet capture, packet unpack, IP fragment reassembly, TCP stream aggregation, and WEB business extraction. In a network {{data restoration}} process, it should first parse the key field information of packets according to TCP/IP standard protocol. Then effectively restructure the IP fragment {{in line with}} the fragmentation identification and fragmentation offset in the Flags field in IP protocol. And also complete the efficient aggregation of TCP flows by using SEQ and ACK fields in TCP protocol at the same time. Finally, it can extract WEB business from TCP stream aimed <b>at</b> <b>HTTP</b> protocol...|$|E
40|$|HTTP State Management Mechanism This {{document}} {{defines the}} HTTP Cookie and Set-Cookie header fields. These header fields {{can be used}} by HTTP servers to store state (called cookies) <b>at</b> <b>HTTP</b> user agents, letting the servers maintain a stateful session over the mostly stateless HTTP protocol. Although cookies have many historical infelicities that degrade their security and privacy, the Cookie and Set-Cookie header fields are widely used on the Internet. This document obsoletes RFC 2965. Status of This Memo This is an Internet Standards Track document. This document {{is a product of the}} Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by th...|$|E
40|$|We {{extend the}} {{classification}} of complete polynomial vector fields on C^ 2 given by Marco Brunella (Topology 43 (2) : 433 - 445, 2004) to cover the case of holomorphic (non-polynomial) vector fields whose underlying foliation is however still polynomial. Comment: The original publication is available <b>at</b> this <b>http</b> URL: [URL]...|$|R
40|$|The Antwerp {{database}} on targe subunit ribosomal RNA now contains 607 complete {{or nearly}} complete aligned sequences. The alignment incorporates secondary structure information for each sequence. Other {{information about the}} sequences, such as literature references, accession numbers and taxonomic information is also available. Information from the database can be downloaded or searched on the rRNA WWW Server <b>at</b> URL <b>http</b> ://rrna. uia. ac. be/...|$|R
30|$|In {{addition}} to forms, in RIAs any input element {{can be used}} to gather and submit the data to the server. Furthermore, input data are usually submitted in JSON format, and there is no information about the input elements inside the submitted request. Therefore, by simply looking <b>at</b> an <b>HTTP</b> request, it is no longer possible to detect whether the request belongs to a user-input action or not.|$|R
40|$|User-perceived latency is {{recognized}} as the central performance problem in the Web. We systematically measure factors contributing to this latency, across several locations. Our study reveals that DNS query times, TCP connection establishment, and start-of-session delays <b>at</b> <b>HTTP</b> servers, more so than transmission time, are major causes of long waits. Wait due to these factors also afflicts high-bandwidth users and has detrimental effect on perceived performance. We propose simple techniques that address these factors: (i) pre-resolving host-names (pre-performing DNS lookup), (ii) preconnecting (prefetching TCP connections prior to issuance of HTTP request), and (iii) pre-warming (sending a "dummy" HTTP HEAD request to Web servers). Trace-based simulations demonstrate a potential to reduce perceived latency dramatically. Our techniques surpass document prefetching in performance improvement per bandwidth used {{and can be used}} with non-prefetchable URLs. Deployment of these techniques [...] ...|$|E
40|$|We discuss from a {{practical}} point of view {{a number of issues}} involved in writing Internet and WWW applications using LP/CLP systems. We describe Pd_l_oW, a public-domain Internet and WWW programming library for LP/CLP systems which we argüe significantly simplifies the process of writing such applications. Pd_l_oW provides facilities for generating HTML structured documents, producing HTML forms, writing form handlers, accessing and parsing WWW documents, and accessing code posted <b>at</b> <b>HTTP</b> addresses. We also describe the architecture of some application classes, using a high-level model of client-server interaction, active modules. We then propose an architecture for automatic LP/CLP code downloading for local execution, using generic browsers. Finally, we also provide an overview of related work on the topic. The PiLLoW library has been developed {{in the context of the}} &- Prolog and CIAO systems, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality...|$|E
40|$|Abstract—The traffic {{generated}} by the Hypertext Transfer Protocol has a dominating position in current Internet traffic. The analyzing and characterizing of HTTP traffic is significant for understanding the nature of Internet traffic. Many studies on HTTP traffic were carried out when HTTP 1. 1 appeared. Along with the prevailing of Web 2. 0 the research on new features of HTTP traffic becomes a new issue in network measuring. In this paper comprehensive characterization of both the user behavior and the transportation feature on HTTP traffic are discussed. The characterization of HTTP behavior <b>at</b> <b>HTTP</b> conversation, TCP connection and Web flow level is parsed based on data collected on-line from access link of an institute’s LAN with more than 1500 users. The HTTP request and response length, temporal characteristics of HTTP message, and the interval of TCP connections as well as Web flows are discussed in detail. Some transformations of the investigated characterization caused by the influence of Web 2. 0 are discovered in the experiment contrasting with previous result...|$|E
40|$|Many {{uncertainty}} propagation software exist, written in different programming languages, {{but not all}} of them are able to handle functional correlation between quantities. In this paper we review one strategy to deal with {{uncertainty propagation}} of quantities that are functionally correlated, and introduce a new software offering this feature: the Julia package Measurements. jl. It supports real and complex numbers with uncertainty, arbitrary-precision calculations, mathematical and linear algebra operations with matrices and arrays. Comment: 3 pages. The Julia package Measurements. jl can be found <b>at</b> this <b>http</b> URL [URL]...|$|R
40|$|Abstract. As SPARQL {{endpoints}} {{are increasingly}} used to serve linked data, {{their ability to}} scale becomes crucial. Although much {{work has been done}} to improve query evaluation, little has been done to take advantage of caching. Effective solutions for caching query results can improve scala-bility by reducing latency, network IO, and CPU overhead. We show that simple augmentation of the database indexes found in common SPARQL implementations can directly lead to effective caching <b>at</b> the <b>HTTP</b> pro-tocol level. Using tests from the Berlin SPARQL benchmark, we evaluate the potential of such caching to improve overall efficiency of SPARQL query evaluation. ...|$|R
50|$|The major pros is {{the ease}} of development, {{specifically}} for rapid prototyping. Another pro {{is that all the}} objects defined within this environment are cleanly reset <b>at</b> each <b>http</b> request and never shares across requests. This means the developer does not need to worry about changing the state of an object (for example the readable attribute of a database field) or worry about a change leaking to other concurrent requests or other applications. A third advantage is that web2py allows the coexistence of multiple applications under the same instance without conflicts even if they use different versions of the same modules or different modules with the same name.|$|R
40|$|Author {{enquiries}}. For enquiries {{relating to}} the submission of articles (including electronic submission where available) please visit Elsevier's Author Gateway <b>at</b> <b>http</b> authors elsevier. com. The Author Gateway also provides the facility to track accepted articles and set up e-mail alerts to inform you of when an article's status has changed, as well as detailed artwork guidelines, copyright information, frequently asked questions and more. Contact details for questions arising after acceptance of an article, especially those relating to proofs, are provided after registration of an article for publication. C 2005 Elsevier Ltd. All rights resetted. This journal and the individual contributions contained in it are protected under copyright by Elsevier Ltd, and the following terms and conditions apply to their use: Photocopying Single photocopies of single articles may be made for personal use as allowed by national copyright laws. Permission of the Publisher and payment of a fee is required for all other photocopying, including multiple or systematic copying, copying for advertising or promotional purposes, resale, and all forms of document delivery Special rales are available for educational institutions that wish to make photocopies for non-profit educational classroom use...|$|E
40|$|Traditionally, {{cellular}} {{wide area}} networks like UMTS {{are used as}} Internet access networks for particular users but, in some cases, they can be employed to provide Internet access to other smaller networks as well. The main inconvenient is that cellular networks have not the same bandwidth than wired networks and therefore, the cellular channel becomes a network bottle-neck. To help to mitigate this situation {{and in order to}} improve the user’s experience different optimization techniques exist, especially in web traffic. This paper studies first the existing synergies <b>at</b> <b>HTTP</b> layer between device capabilities expression, content negotiation, channel optimization and content adaptation. And secondly, it presents a system where HTTP requests transmission is optimized, showing a significant improvement in response time by means of HTTP header reduction over the cellular channel. In order to obtain a successful browsing experience, headers should be restored when reaching the Internet. This dynamic header reconstruction allows giving enriched and more expressive information about user’s device and browser capabilities. Thus navigation speed and user’s QoE can be enhanced by means of dynamic content negotiation in order to obtain adapted (and lighter) content and responses from web servers and adaptation proxies alike. 1...|$|E
40|$|The aim of {{this paper}} is to {{describe}} the Web site of the Italian Project on CT Colonography (Research Project of High National Interest, PRIN No. 2005062137) and present the prototype of the online database. The Web site was created with Microsoft Office Publisher 2003 software, which allows the realisation of multiple Web pages linked through a main menu located on the home page. The Web site contains a database of computed tomography (CT) colonography studies in the Digital Imaging and Communications in Medicine (DICOM) standard, all acquired with multidetector-row CT according to the parameters defined by the European Society of Abdominal and Gastrointestinal Radiology (ESGAR). The cases present different bowel-cleansing and tagging methods, and each case has been anonymised and classified according to the Colonography Reporting and Data System (C-RADS). The Web site is available <b>at</b> <b>http</b> address www. ctcolonography. org and is composed of eight pages. Download times for a 294 -Mbyte file were 33 min from a residential ADSL (6 Mbit/s) network, 200 s from a local university network (100 Mbit/s) and 2 h and 50 min from a remote academic site in the USA. The Web site received 256 accesses in the 22 days since it went online. The Web site is an immediate and up-to-date tool for publicising the activity of the research project and a valuable learning resource for CT colonograph...|$|E
50|$|CWMP is a text based protocol. Orders sent {{between the}} device (CPE) and auto {{configuration}} server (ACS) are transported over HTTP (or more frequently HTTPS). <b>At</b> this level (<b>HTTP)</b> CPE is behaving {{in the role}} of client and ACS {{in the role of}} HTTP server. This essentially means that control over the flow of the provisioning session is the sole responsibility of the device.|$|R
50|$|Abrahams first {{networked}} {{performance from}} the UK, If Not You Not Me, took place <b>at</b> the <b>HTTP</b> gallery in London in 2010. It incorporated previous {{projects such as}} the One the Puppet of Other (2007) and The Big Kiss (2008) with its central communicative medium entitled Shared Still Life/Nature Morte Partagée. The exhibition held the event entitled Shared Still Life/Nature Morte Partagée which was centralized with a telematic still life for mixed media and LED message board. Participants were {{given the opportunity to}} communicate with those at Kawenga - territoires numériques, situated in the media arts centre in Montpellier, France where they played and rearranged objects in still life whilst sending messages between each other.|$|R
40|$|In this paper, {{we report}} on our {{experience}} in building ecoBrowse, an extensible Web co-navigation framework. Extensibility is realized through dynamic content transformation <b>at</b> the <b>HTTP</b> response/request stream level. The framework also provides an API (Application Programming Interface) that gives developers high-level support for creating groupware features. To illustrate the potential of e-coBrowse we developed a number of multiuser browser extensions. The most interesting one is chatpointer, which allows conferees to simultaneously annotate at mouse-clicked positions inside any Web document. Conferees can comment on shared annotations similar to chat, but with direct visual reference to relevant parts of the document. A characterization of existing approaches for building Web co-navigation systems is also described...|$|R
40|$|Purpose: The aim of {{this paper}} is to {{describe}} the Web site of the Italian Project on CT Colonography (Research Project of High National Interest, PRIN No. 2005062137) and present the prototype of the online database. Materials and methods: The Web site was created with Microsoft Office Publisher 2003 software, which allows the realisation of multiple Web pages linked through a main menu located on the home page. The Web site contains a database of computed tomography (CT) colonography studies in the Digital Imaging and Communications in Medicine (DICOM) standard, all acquired with multidetector-row CT according to the parameters defined by the European Society of Abdominal and Gastrointestinal Radiology (ESGAR). The cases present different bowel-cleansing and tagging methods, and each case has been anonymised and classified according to the Colonography Reporting and Data System (C-RADS). Results: The Web site is available <b>at</b> <b>http</b> address www. ctcolonography. org and is composed of eight pages. Download times for a 294 -Mbyte file were 33 min from a residential ADSL (6 Mbit/s) network, 200 s from a local university network (100 Mbit/s) and 2 h and 50 min from a remote academic site in the USA. The Web site received 256 accesses in the 22 days since it went online. Conclusions: The Web site is an immediate and up-to-date tool for publicising the activity of the research project and a valuable learning resource for CT colonography...|$|E
40|$|International audienceAlthough {{there have}} been great {{advances}} in understanding bacterial pathogenesis, {{there is still a}} lack of integrative information about what makes a bacterium a human pathogen. The advent of high-throughput sequencing technologies has dramatically increased the amount of completed bacterial genomes, for both known human pathogenic and non-pathogenic strains; this information is now available to investigate genetic features that determine pathogenic phenotypes in bacteria. In this work we determined presence/absence patterns of 814 different virulence-related genes among more than 600 finished bacterial genomes from both human pathogenic and non-pathogenic strains, belonging to different taxonomic groups (i. e: Actinobacteria, Gammaproteobacteria, Firmicutes, etc.). An accuracy of 95 % using a cross-fold validation scheme with in-fold feature selection is obtained when classifying human pathogens and non-pathogens. A reduced subset of highly informative genes (120) is presented and applied to an external validation set. The statistical model was implemented in the BacFier v 1. 0 software (freely available <b>at</b> <b>http</b> : ==bacfier:googlecode:com=files=Bacfier v 1 0 :zip), that displays not only the prediction (pathogen/non-pathogen) and an associated probability for pathogenicity, but also the presence/absence vector for the analyzed genes, so it is possible to decipher the subset of virulence genes responsible for the classification on the analyzed genome. Furthermore, we discuss the biological relevance for bacterial pathogenesis of the core set of genes, corresponding to eight functional categories, all with evident and documented association with the phenotypes of interest. Also, we analyze which functional categories of virulence genes were more distinctive for pathogenicity in each taxonomic group, which seems to be a completely new kind of information and could lead to important evolutionary conclusions...|$|E
40|$|Unconventional roof {{technologies}} such as cool roofs and green roofs {{have been shown to}} reduce building heating and cooling load. Although previous studies suggest potential for energy savings through such technologies, many factors affect potential savings. To further investigate these factors, a tool has been developed to allow architects and designers the ability to quickly assess the energy saving potential of different roof technologies and roof constructions for various sites around the world. A first principles heat transfer model is developed for each of the roof technologies, with particular care for green roof heat and mass transfer. Two sets of experimental data from Japan and Florida validate the models by predicting roof surface temperature. The predicted roof surface temperatures in Japan agree with measured values within 10 and 26 % of peak roof temperature fluctuations for the cool and green roof respectively, while the same models in Florida agree with measured values there within 7. 2 and 14 % for the cool and green roof respectively. The models have been integrated into a free online building simulation tool, MIT's Design Advisor, available <b>at</b> <b>http</b> : //designadvisor. mit. edu. Numerous simulations are run, showing that potential energy savings are found to strongly vary with many parameters, particularly roof type, climate, and amount of insulation. For example, a one-story building in Boston with an uninsulated modified-bitumen roof can save 82 % in cooling and heating energy by adding 3 m 2 K/W of roof insulation, whereas only 34 % if an uninsulated green roof is installed instead. However, in Lisbon, the same addition of roof insulation to the same building results in 54 % savings, while the installation of an uninsulated green roof results in a 67 % reduction. Such findings and their implications are discussed for other locations and design parameters. by Stephen Douglas Ray. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 159 - 168) ...|$|E
40|$|See {{paper for}} full list of authors - 8 pages plus author list + cover pages (25 pages total), 2 figures, 1 tables, {{submitted}} to Physical Review Letters, All figures including auxiliary {{figures are available}} <b>at</b> this <b>http</b> URLInternational audienceThis Letter presents {{a search for a}} heavy neutral particle decaying into an opposite-sign different-flavor dilepton pair, e±μ∓, e±τ∓, or μ±τ∓ using 20. 3 fb− 1 of pp collision data at s√= 8 TeV collected by the ATLAS detector at the LHC. The numbers of observed candidate events are compatible with the Standard Model expectations. Limits are set on the cross section of new phenomena in two scenarios: the production of ν~τ in R-parity-violating supersymmetric models and the production of a lepton-flavor-violating Z′ vector boson...|$|R
50|$|There are two {{different}} ways compression {{can be done in}} <b>HTTP.</b> <b>At</b> a lower level, a Transfer-Encoding header field may indicate the payload of a HTTP message is compressed. At a higher level, a Content-Encoding header field may indicate that a resource being transferred, cached, or otherwise referenced is compressed. Compression using Content-Encoding is more widely supported than Transfer-Encoding, and some browsers do not advertise support for Transfer-Encoding compression to avoid triggering bugs in servers.|$|R
25|$|Initially, The Pirate Bay's four Linux servers ran {{a custom}} web server called Hypercube. An old version is open source. On 1 June 2005, The Pirate Bay updated its website {{in an effort}} to reduce {{bandwidth}} usage, which was reported to be <b>at</b> 2 <b>HTTP</b> requests per millisecond on each of the four web servers, as well as to create a more user friendly interface for the front-end of the website. The website now runs Lighttpd and PHP on its dynamic front ends, MySQL at the database back end, Sphinx on the two search systems, memcached for caching SQL queries and PHP-sessions and Varnish in front of Lighttpd for caching static content. , The Pirate Bay consisted of 31 dedicated servers including nine dynamic web fronts, a database, two search engines, and eight BitTorrent trackers.|$|R
40|$|ARPENTEUR is a web {{application}} for digital photogrammetry mainly dedicated to architecture (Architectural PhotogrammEtry Network Tool for EdUcation and Research) available <b>at</b> <b>http</b> ://www. arpenteur. net. ARPENTEUR {{has been developed}} by two complementary research teams: the “Photogrammetry and Geomatics” group of ENSAIS- LERGEC's laboratory (Strasbourg, France) and the Gamsau-MAP CNRS laboratory located {{in the school of}} Architecture of Marseilles (France). This paper focuses on a new approach of stone-by-stone surveying in which formalised architectural knowledge is used as a prerequisite to the photogrammetric measurement process. In addition to this morphological definition, the structure point of view is implemented in the model in order to consider some architectural elements as containing a set of ashlar blocks. In our approach of stone-by-stone surveying, which has been conceived for the study of historical ashlar masonry but can be applied to other types of investigation, the measurement is performed directly on each individual stone in its built context. A previous edifice analysis, conducted by an archaeologist, is necessary to define the construction characteristics and chronology, and the properties of all the measured architectural entities. This results in the definition of an approximate depth for each type of stone, allowing a limited survey to the blocks visible part. An extrusion vector is computed in order to inform lacking geometrical description of the block. Once the instanced block is measured a polyhedron representation of its morphology is generated. The instance is also added to a data structure in which it is positioned according to topological or geometrical order. The result is therefore a collection of ordered blocks that includes, for instance, for each block data on its neighbours (adjacent blocks). When completed, the tool will create a direct link between the architectural object and the database, enabling to locate and thus identify the properties of each block registered in the database. The possibilities of this new type of approach to architecture extend from archaeology to restoration and maintenance to any type of structure treated by photogrammetrical survey. Retrieving architectural information (for example the intrados radius of an arch) is the approach we have developed in architectural surveying. We are currently working on giving this possibility in the stone-by-stone surveying process on which this paper focuses...|$|E
40|$|In this thesis, {{new models}} and methodologies are {{introduced}} {{for the analysis}} of dynamic processes characterized by image sequences with spatial temporal overlapping. The spatial temporal overlapping exists in many natural phenomena and should be addressed properly in several Science disciplines such as Microscopy, Material Sciences, Biology, Geostatistics or Communication Networks. This work is related to the Point Process and Random Closed Set theories, within Stochastic Geometry. The proposed models are an extension of Boolean Models in R 2 by adding a temporal dimension. The study has been motivated for its application in a multidisciplinary project that combined Statistics, Computer Sciences, Biology and Microscopy, with the aim of analysing the cell exocytosis and endocytosis. Exocytosis is the process by which cells secrete vesicles outside the plasma membrane and endocytosis is the opposite mechanism. Our data were image sequences obtained by Electron Microscopy and Total Internal Reflection Fluorescence Microscopy. Fluorescent tagged-proteins are observed as overlapped clusters with random shape, area and duration. They can be modelled as realizations of a stationary and isotropic stochastic process. The methodology herein proposed could be used to analyze similar phenomena in other Fields of Science. First, the temporal Boolean model is introduced and some estimation methods for the parameters of the model are presented. Second, we proposed a method for the estimation of the event duration distribution function of a univariate temporal Boolean model based on spatial temporal covariance. A simulation study is performed with several duration probability density functions, and an application to the cell endocytosis is realized. Third, we introduce the bivariate temporal Boolean model to study interactions between two overlapped spatial temporal processes and to quantify their overlapping and dependencies. We propose a non-parametric approach based on a generalization of the Ripley K-function, the spatial-temporal covariance and the pair correlation functions for a bivariate temporal random closed set. A Monte Carlo test was performed to test the independence hypothesis. This methodology is not only a test procedure but also allows us to quantify the degree and spatial temporal interval of the interaction. No parametric assumption is needed. A simulation study has been conducted and an application to the study of proteins that mediate in cell endocytosis has been performed. Fourth, from high spatial resolution EM images, we model the distribution of exocytic vesicles (granules) within the cell cytoplasm as a realization of a finite point process (a point pattern), and the point patterns of several cell groups are considered replicates of different point processes. Our aim was to study differences between two treatment groups in terms of granule location. We characterize the spatial distribution of granules with respect to the plasma membrane by means of several functional descriptors, that allowed us to detect {{significant differences between the two}} cell groups that would not be observed by a classical approach. To perform image segmentation, we developed an automatic granule detection tool with similar performance to that of the manual one-by-one marking. Finally, we have implemented a software toolbox for the simulation and analysis of temporal Boolean models (available <b>at</b> <b>http</b> : ==www:uv:es=tracs=), so scientists and technicians of any discipline can apply the proposed methods. In summary, the spatial temporal stochastic models proposed allow modelling of dynamic processes from image sequences where several forms of random shape, size and duration overlap. It is the first time these tools are applied to the study of cell exo and endocytosis, and they would contribute to improve their understanding. Our methodologies will help future research in Cell Biology, e. g. in the study of diseases related to secretion dysfunctions, such as diabetes. En esta tesis presentamos nuevos modelos y metodolog as para el an alisis de pro- cesos din amicos a partir de secuencias de im agenes, con solapamiento espacial y tem- poral de los objetos de an alisis, un fen omeno habitual en la naturaleza. El trabajo realizado se enmarca en la teor a de Procesos Puntuales y Conjuntos Aleatorios Ce- rrados (RACS), dentro de la Geometr a Estoc astica. Los modelos propuestos son una extensi on de la teor a de modelos booleanos en R 2 incorporando una componente temporal. La motivaci on del trabajo fue su aplicaci on a un proyecto multidisciplinar donde analizamos la exocitosis y la endocitosis celular, procesos en que la c elula segrega o absorbe sustancias a trav es de la membrana citoplasm atica, respectivamente. El es- tudio se realiz o utilizando secuencias de im agenes obtenidas con microscop a TIRFM, donde se observan las prote nas como agrupaciones uorescentes superpuestas. Mo- delizamos las im agenes como realizaciones de un proceso estoc astico estacionario e isotr opico. Esta metodolog a permite analizar fen omenos reales en otros campos de la Ciencia con superposici on espacio-temporal de objetos con formas y duraciones aleatorias, como Geolog a, Qu mica, Comunicaciones, etc. Primero, introducimos el modelo booleano temporal. Presentamos un m etodo de estimaci on de la funci on de distribuci on de la duraci on basado en la covarianza espacio-temporal, y el estudio de simulaci on realizado. Segundo, estudiamos la in- terrelaci on entre dos procesos espacio-temporales mediante la K-funci on de Ripley, la covarianza espacio-temporal y la funci on de correlaci on para conjuntos aleatorios bivariados. Realizamos un estudio de simulaci on y una aplicaci on a la endocitosis celular. Tercero, modelizamos la distribuci on de ves culas exoc ticas (gr anulos) en el cito- plasma celular como un proceso puntual nito. Caracterizamos su distribuci on espa- cial respecto a la membrana mediante varios descriptores funcionales. Para segmentar las im agenes, desarrollamos una herramienta autom atica de detecci on de gr anulos. Hemos desarrollado una herramienta de software completa para la simulaci on y es- timaci on de modelos booleanos temporales (disponible en http : ==www:uv:es=tracs=) ...|$|E
50|$|As vital data (like user {{names and}} passwords) may be {{transmitted}} to CPE via CWMP, {{it is essential}} to provide a secure transport channel and always authenticate the CPE against the ACS.Secure transport and authentication of the ACS identity can easily be provided by usage of HTTPS and verification of ACS certificate. Authentication of the CPE is more problematic. The identity of the device is verified based on a shared secret (password) <b>at</b> the <b>HTTP</b> level. Passwords may be negotiated between the parties (CPE-ACS) at every provisioning session. When the device contacts the ACS for the first time (or after a factory-reset) default passwords are used. In large networks {{it is the responsibility of}} the procurement to ensure each device is using unique credentials, their list is delivered with the devices themselves and secured.|$|R
50|$|Initially, The Pirate Bay's four Linux servers ran {{a custom}} web server called Hypercube. An old version is open source. On 1 June 2005, The Pirate Bay updated its website {{in an effort}} to reduce {{bandwidth}} usage, which was reported to be <b>at</b> 2 <b>HTTP</b> requests per millisecond on each of the four web servers, as well as to create a more user friendly interface for the front-end of the website. The website now runs Lighttpd and PHP on its dynamic front ends, MySQL at the database back end, Sphinx on the two search systems, memcached for caching SQL queries and PHP-sessions and Varnish in front of Lighttpd for caching static content. , The Pirate Bay consisted of 31 dedicated servers including nine dynamic web fronts, a database, two search engines, and eight BitTorrent trackers.|$|R
40|$|Annual Report 2008, Laboratori Nazionali di Frascati (LNF) The {{research}} topics {{investigated by}} this project {{can be divided}} into the following areas: • Flavour physics, precision tests and physics beyond the Standard Model • Light flavour spectroscopy and tau-physics • Hadronic Form Factors and meson spectroscopy • Hadronic cross-sections • QCD and the Higgs boson at the LHC The activity of the phenomenology group at Frascati can be seen in detail <b>at</b> the site <b>http</b> : //www. lnf. infn. it/theory/pheno 2. htm...|$|R
40|$|We present Spitzer IRAC 3. 6 - 8 μm and MIPS 24 μm point-source catalogs for M 31 and 15 other mostly large, star forming galaxies at {{distances}} ∼ 3. 5 - 14 Mpc including M 51, M 83, M 101 and NGC 6946. These catalogs contain ∼ {{1 million}} sources including ∼ 859, 000 in M 31 and ∼ 116, 000 {{in the other}} galaxies. They were created following the procedures described in Khan et al. (2015) {{through a combination of}} point spread function (PSF) fitting and aperture photometry. These data products constitute a resource to improve our understanding of the IR-bright (3. 6 - 24 μm) point-source populations in crowded extragalactic stellar fields and to plan observations with the James Webb Space Telescope. Comment: Accepted for publication in ApJ. 4 figures, 17 tables. Catalog files available <b>at</b> this <b>http</b> url [URL]...|$|R
