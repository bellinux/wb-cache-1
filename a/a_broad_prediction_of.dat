2|10000|Public
40|$|AbstractHeadwater {{streams in}} the Ovens catchment, {{southeast}} Australia, have 3 H activities between 1. 63 and 2. 45 TU that {{are lower than}} those of local rainfall (∼ 3 TU). The variation of 3 H activities and major ion concentrations with streamflow implies that simple dilution of older groundwater with rainfall during high flow does not occur. Rather different stores of water from within the catchment (e. g. from the soil or regolith) are mobilized during rainfall events. Mean residence times estimated using an exponential-piston flow model 12 to 36 yearsduringsummer baseflow and 6 to 14 yearsduring recession from higher flows. A broad correlation between 3 H activities and the percentage of rainfall exported from each catchment may reflect local differences in recharge and evapotranspiration; this correlation allows <b>a</b> <b>broad</b> <b>prediction</b> <b>of</b> likely residence times. These streams are buffered against rainfall variations on timescales of a few years;however, the impacts of changes to landuse or longer timescale changes to rainfallmay take years to decades to manifest itself in changes to streamflow or water quality...|$|E
40|$|Increased {{utilization}} of terrestrial vehicles requires devotion of huge {{efforts to provide}} road infrastructure for increased accessibility. Meanwhile, {{there is no doubt}} that transportation infrastructure has major impacts on ecosystems, ecological processes and biodiversity at a landscape scale. These impacts result from habitat loss, fragmentation and degradation, spreading of invasive species, noise pollution, road mortality but also more unknown impacts such as altered isolation patterns. This study aims to picture probable impacts of noise in the current transportation infrastructure at a national scale in Sweden. The study involves mapping of potential conflict zones between roads and ecological networks by defining road-effect zones around roads based on traffic volume, surrounding vegetation type, land cover type and consequently noise level for each segment of the roads. Focal to this study are impacts of noise on areas of importance for nature conservation like key habitat areas, meadows, pastures and wetlands. A method called "Calculation of Road Traffic Noise" (CRTN) was selected for calculation of noise generated by traffic flows in Sweden. The analysis was performed by using data on roads from the National Road Database and SAMPERS, land cover data, data on biodiversity conservation, and national databases concerning nature values. The analysis was done with a spatial prediction model based on a Geographic Information System (GIS), using Sweden as a case study. According to the results, 813 ha of nature values are exposed to noise levels higher than 55 dB; same wise, 2190 ha of outlined areas with nature values are exposed to noise higher than 45 dB. The study provided baseline information and <b>a</b> <b>broad</b> <b>prediction</b> <b>of</b> main impacts on biodiversity at landscape and regional scales. Impact prediction of alternative planning scenarios will be facilitated using the GIS-based prediction models and assist decision making on allocation of future roads. Using such methods when discussing future localization of road infrastructure will be useful for integrated sustainability assessment and strategic environmental assessment. Thus, it provides possibilities for minimizing adverse impacts of infrastructure on biodiversity at an early planning stage...|$|E
40|$|I s the {{education}} system income-equalizing? The evidence from developing countr ies shows that whi le education has expanded tremendously in recent decades, income distribution has not become more equal. This article seeks to resolve this seeming paradox. A theoretical model is constructed {{for the relationship between}} education and income in which ethnicity {{plays a key role in}} the distribution process. The model predicts that {{the education}} system is not income-equalizing. <b>A</b> <b>broader</b> set <b>of</b> <b>predictions</b> are derived from the theoretical model and then compared to Peruvian data. Statistical tests produce results that are consistent with the model’s predictions...|$|R
40|$|The {{problem of}} self-consistent {{estimates}} of the deconfinement temperature Tc {{in the framework of}} the bottom-up holographic approach to QCD is scrutinized. It is shown that the standard soft wall model gives Tc for planar gluodynamics around 260 MeV in a good agreement with the lattice data. The extensions of soft wall model adjusted for descriptions of realistic meson spectra result in <b>a</b> <b>broad</b> range <b>of</b> <b>predictions.</b> This uncertainty is related with a poor experimental information on the radially excited mesons. Comment: 19 pages, 4 figures, typos are corrected, matches the version published in Eur. Phys. J. ...|$|R
40|$|We {{consider}} large margin estimation in <b>a</b> <b>broad</b> range <b>of</b> <b>prediction</b> models where inference involves solving {{combinatorial optimization}} problems, for example, weighted graphcuts or matchings. Our {{goal is to}} learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for e#cient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods...|$|R
40|$|This paper {{addresses}} {{the use of}} smart-home sensor streams for continuous <b>prediction</b> <b>of</b> energy loads of individual households which participate as an agent in local markets. We introduces a new device level energy consumption dataset recorded over three years wich includes high resolution energy measurements from electrical devices collected within a pilot program. Using data from that pilot, we analyze the applicability of various machine learning mechanisms for continuous load prediction. Specifically, we address short-term load prediction that is required for load balancing in electrical micro-grids. We report on the prediction performance and the computational requirements <b>of</b> <b>a</b> <b>broad</b> range <b>of</b> <b>prediction</b> mechanisms. Furthermore we present an architecture and experimental evaluation when this prediction is applied in the stream. Comment: 11 pages, 12 figure...|$|R
40|$|The {{spontaneous}} loss of {{angular momentum}} of a spinning cosmic string due to particle emission is discussed. The rate of particle production between two assymptotic spacetimes: the spinning cosmic string spacetime in the infinite past and a non-spinning cosmic string spacetime in the infinite future is calculated. Pacs numbers: 03. 60. Bz, 04. 20. Cv, 11. 10. Qr 1 There is <b>a</b> <b>broad</b> class <b>of</b> <b>predictions</b> <b>of</b> various models in grand unified theories (GUT’s). Between them, phase transitions {{may result in}} the formation of cosmic strings [1]. They are extended one dimensional infinite topological defects. The cosmic strings affect the spacetime mainly topologically, given a conical structure to the space region around the cone of the string. The conical topology may be responsible for several gravitational effects, e. g., gravitational lense [2] and particle production due to the changing gravitational field during the formation of such object [3]. There are a lot of papers studying quantum processes in a cosmic string spacetime. Of special interest for us are the following: Ref. [4] where pair production in a straight cosmic string spacetime i...|$|R
40|$|We {{develop a}} general theory for three states of {{equilibrium}} of amyloid peptides: the monomer, oligomer, and fibril. We {{assume that the}} oligomeric state is a disordered micelle-like collection of a few peptide chains held together loosely by hydrophobic interactions into a spherical hydrophobic core. We assume that fibrillar amyloid chains are aligned and further stabilized by `steric zipper' interactions [...] hydrogen bonding and steric packing, in addition to specific hydrophobic sidechain contacts. The model makes <b>a</b> <b>broad</b> set <b>of</b> <b>predictions,</b> consistent with experiments: (i) Similar to surfactant micellization, amyloid oligomerization should increase with bulk peptide concentration. (ii) The onset of fibrillization limits the concentration of oligomers in the solution. (iii) The average fibril length vs. monomer concentration agrees with data on α-synuclein, (iv) Full fibril length distributions follow those of α-synuclein, (v) Denaturants should `melt out' fibrils, and (vi) Added salt should stabilize fibrils by reducing repulsions between amyloid peptide chains. Interestingly, small changes in solvent conditions can: (a) tip the equilibrium balance between oligomer and fibril, and (b) cause large changes in rates, through effects on the transition-state barrier. This model may provide useful insights into the physical processes underlying amyloid diseases...|$|R
40|$|AbstractWe {{develop a}} theory for three states of {{equilibrium}} of amyloid peptides: the monomer, oligomer, and fibril. We {{assume that the}} oligomeric state is a disordered micellelike collection of a few peptide chains held together loosely by hydrophobic interactions into a spherical hydrophobic core. We assume that fibrillar amyloid chains are aligned and further stabilized by steric zipper interactions—hydrogen bonding, steric packing, and specific hydrophobic side-chain contacts. The model makes <b>a</b> <b>broad</b> set <b>of</b> <b>predictions</b> {{that are consistent with}} experimental results: 1), Similar to surfactant micellization, amyloid oligomerization should increase with peptide concentration in solution. 2), The onset of fibrillization limits the concentration of oligomers in the solution. 3), The extent of Aβ fibrillization increases with peptide concentration. 4), The predicted average fibril length versus monomer concentration agrees with data on α-synuclein. 5), Full fibril length distributions agree with data on α-synuclein. 6), Denaturants should melt out fibrils. And finally, 7), added salt should stabilize fibrils by reducing repulsions between amyloid peptide chains. It is of interest that small changes in solvent conditions can tip the equilibrium balance between oligomer and fibril and cause large changes in rates through effects on the transition-state barrier. This model may provide useful insights into the physical processes underlying amyloid diseases...|$|R
40|$|Empirical Bayes is a {{versatile}} approach to `learn from a lot' in two ways: first, {{from a large}} number of variables and second, from a potentially large amount of prior information, e. g. stored in public repositories. We review applications of a variety of empirical Bayes methods to <b>a</b> <b>broad</b> spectrum <b>of</b> <b>prediction</b> methods including penalized regression, random forest, linear discriminant analysis, and Bayesian models with sparse or dense priors. We discuss `formal' empirical Bayes methods which maximize the marginal likelihood, but also more informal approaches based on other data summaries. We contrast empirical Bayes to cross-validation and full Bayes, and discuss hybrid approaches. To study the relation between the quality of an empirical Bayes estimator and $p$, the number of variables, we derive the expected mean squared error of a simple empirical Bayes estimator in a linear model setting. We argue that empirical Bayes is particularly useful when the prior contains multiple parameters, modeling a priori information on variables, termed `co-data'. In particular, we present two novel examples that allow for co-data. First, a Bayesian spike-and-slab setting that facilitates inclusion of multiple co-data sources and types; second, a hybrid empirical Bayes-full Bayes ridge regression approach for estimation of the posterior predictive interval...|$|R
40|$|We {{develop a}} theory for three states of {{equilibrium}} of amyloid peptides: the monomer, oligomer, and fibril. We {{assume that the}} oligomeric state is a disordered micellelike collection of a few peptide chains held together loosely by hydrophobic interactions into a spherical hydrophobic core. We assume that fibrillar amyloid chains are aligned and further stabilized by steric zipper interactions—hydrogen bonding, steric packing, and specific hydrophobic side-chain contacts. The model makes <b>a</b> <b>broad</b> set <b>of</b> <b>predictions</b> {{that are consistent with}} experimental results: 1), Similar to surfactant micellization, amyloid oligomerization should increase with peptide concentration in solution. 2), The onset of fibrillization limits the concentration of oligomers in the solution. 3), The extent of Aβ fibrillization increases with peptide concentration. 4), The predicted average fibril length versus monomer concentration agrees with data on α-synuclein. 5), Full fibril length distributions agree with data on α-synuclein. 6), Denaturants should melt out fibrils. And finally, 7), added salt should stabilize fibrils by reducing repulsions between amyloid peptide chains. It is of interest that small changes in solvent conditions can tip the equilibrium balance between oligomer and fibril and cause large changes in rates through effects on the transition-state barrier. This model may provide useful insights into the physical processes underlying amyloid diseases...|$|R
40|$|We present stellar {{kinematics}} and orbit superposition {{models for}} the central regions of four Brightest Cluster Galaxies (BCGs), based upon integral-field spectroscopy at Gemini, Keck, and McDonald Observatories. Our integral-field data span radii from < 100 pc to tens of kpc. We report black hole masses, M_BH, of 2. 1 +/- 1. 6 x 10 ^ 10 M_Sun for NGC 4889, 9. 7 + 3. 0 - 2. 6 x 10 ^ 9 M_Sun for NGC 3842, and 1. 3 + 0. 5 - 0. 4 x 10 ^ 9 M_Sun for NGC 7768. For NGC 2832 we report an upper limit of M_BH < 9 x 10 ^ 9 M_Sun. Stellar orbits {{near the center of}} each galaxy are tangentially biased, on comparable spatial scales to the galaxies' photometric cores. We find possible photometric and kinematic evidence for an eccentric torus of stars in NGC 4889, with a radius of nearly 1 kpc. We compare our measurements of M_BH to the predicted black hole masses from various fits to the relations between M_BH and stellar velocity dispersion, luminosity, or stellar mass. The black holes in NGC 4889 and NGC 3842 are significantly more massive than all dispersion-based predictions and most luminosity-based predictions. The black hole in NGC 7768 is consistent with <b>a</b> <b>broader</b> range <b>of</b> <b>predictions.</b> Comment: 24 pages, 18 figures. Accepted for publication in Ap...|$|R
40|$|Abstract—Predictability of home energy usage {{forms the}} basis of many home energy {{management}} and demand-response systems. While existing studies focus on designing more accu-rate prediction algorithms, a comprehensive energy management solution requires <b>a</b> <b>broad</b> understanding <b>of</b> <b>prediction</b> accuracy at different granularities, for example appliance and home, as well as different time horizons, for example an hour, day, or week into the future. In this paper, we undertake an analysis of predictability of power draw of appliances and whole-home energy consumption at four different time horizons: an hour, a quarter-day, a day, and {{a week in the}} future. Our analysis presents two research contributions. Our first contribution is a diverse dataset, GreenHomes, that includes appliance power draw and whole-home energy consumption data from seven homes across three states in the United States over a two-year period. Our second and primary contribution is a set of insights into the predictability of home energy usage. We show that simple statistic-based algorithms perform as well as sophisticated machine learning algorithms and time-series based predictors. These simple algorithms can considerably reduce the computational need for large-scale predictive analysis of home energy data. We also show that appliance-level power draw is more predictable than whole-home energy consumption at shorter time horizons while home-level energy consumption is more predictable at longer time horizons. Finally, we show that there is large variation in predictability across homes. This variation may be attributed to home type and points to the need for personalized energy management systems. I...|$|R
40|$|Using a {{simplified}} model for in-medium dipole evolution accounting for color filtering effects we study production of hadrons at large transverse momenta p_T in heavy ion collisions. In {{the framework of}} this model, several important sources of the nuclear suppression observed recently at RHIC and LHC have been analysed. A short production length of the leading hadron l_p causes a strong onset of color transparency effects manifested themselves as a steep rise of the nuclear modification factor R_AA(p_T) at large hadron p_T's. A dominance of quarks with higher l_p leads to a weaker suppression at RHIC than the one observed at LHC. In the RHIC kinematic region we include an additional suppression factor steeply falling with p_T, which is tightly related to the energy conservation constraints. The latter is irrelevant at LHC up to p_T≲ 70 GeV while it causes a rather flat p_T dependence of the R_AA(p_T) factor at RHIC c. m. energy √(s) = 200 GeV and even an increasing suppression with p_T at √(s) = 62 GeV. The calculations contain only a medium density adjustment, and for an initial time scale t_ 0 = 1 fm we found the energy-dependent maximal values of the transport coefficient, q̂_ 0 = 0. 7, 1. 0 and 1. 3 GeV^ 2 /fm corresponding to √(s) = 62, 200 GeV and 2. 76 TeV, respectively. We present <b>a</b> <b>broad</b> variety <b>of</b> <b>predictions</b> for the nuclear modification factor and the azimuthal asymmetry which are in a good agreement with available data from experiments at RHIC and LHC. Comment: 14 pages, 17 figures; extra clarifications added in Sects. II and III (with additional Figs. 1 - 6) and in the extended Sect. V B (with additional Fig. 11), references added, conclusions unchange...|$|R
40|$|The {{results of}} each of the three parts stand on their own {{independently}} of each other. Together this theory-in-fieri makes several <b>broad</b> <b>predictions,</b> some <b>of</b> which are:|$|R
40|$|This thesis aims to {{investigate}} pension funds'investment behavior {{in times of}} a capital markets crisis; to discuss actions taken and changes of an investment policy; and to point out legislative issues. Another topic is a discussion over the contribution of each investment tool to the portfolio performance, and a <b>prediction</b> <b>of</b> the investment policy of pension funds. An {{important part of this}} thesis is the analysis of the detailed portfolio of PF Stabilita. The outcome of this paper is not to predict a future developement of capital markets, although <b>a</b> <b>broad</b> <b>prediction</b> for each investment tool is made, but {{to investigate}} a reaction of pension funds to the 2008 capital markets turmoils...|$|R
40|$|Here, I review current state-of-the-arts in {{many areas}} of AI to {{estimate}} when it's reasonable to expect human level AI development. <b>Predictions</b> <b>of</b> prominent AI researchers vary broadly from very pessimistic <b>predictions</b> <b>of</b> Andrew Ng to much more moderate <b>predictions</b> <b>of</b> Geoffrey Hinton and optimistic <b>predictions</b> <b>of</b> Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and this <b>broad</b> range <b>of</b> <b>predictions</b> <b>of</b> AI experts, AI safety questions are also discussed. Comment: version 2 includes grant informatio...|$|R
40|$|In {{this paper}} we propose a {{modelling}} approach for labor supply and consumption decisions that is firmly grounded within a utility maximizing framework and allows for a role of such institutional constraints as limited access to borrowing and involuntary unemployment. We report estimations for a system of dynamic probit models with data from the Panel Study of Income Dynamics. These estimations test <b>broad</b> <b>predictions</b> <b>of</b> the theoretical model. One of our models describes a household's propensity to be liquidity constrained in a given period. The second is a dynamic ordered probit model for a labor constraint indicator describing qualitative aspects of the conditions of employment, that is whether the household head is involuntarily overemployed, voluntarily employed, or involuntarily underemployed or unemployed. These models are estimated separately as well as jointly. Our results provide strong support for the basic theory of constrained behavior and the interaction between liquidity constraints and exogenous constraints on labor supply. Intertemporal optimization, quantity constraints, liquidity constraints, unemployment, dynamic probit models, simulation estimation...|$|R
40|$|Because organisms {{have limited}} {{resources}} to allocate to multiple life history traits, the Optimal Defense Theory (ODT) and the Growth-Differentiation Balance Hypothesis (GDBH) {{were developed by}} terrestrial plant ecologists to predict intraindividual defense allocation based {{on the cost of}} defense and these life history trade-offs. However, these theories have garnered equivocal experimental support over the years and are rarely experimentally extended from <b>predictions</b> <b>of</b> plant physiology to the palatability of the tissues an herbivore experiences. We therefore examined tissue palatability, nutritional value, and defense mechanisms in multiple Dictyotalean seaweeds in two Caribbean locations, using two herbivores. Relative palatability of tissues varied greatly with algal species, grazer species, and location. Because older bases were not consistently defended, GDBH did not predict relative palatability. We could not reject ODT without intensive measures of tissue fitness value and herbivore risk, and this theory was therefore not useful in making <b>broad</b> <b>predictions</b> <b>of</b> tissue palatability. In testing the physiological <b>predictions</b> <b>of</b> these theories, we found the young, growing apices of these seaweeds to be generally more nutritionally valuable than the old, anchoring bases and found organic-rich apices to be more chemically deterrent, thus supporting ODT. However, the combined chemical, nutritional, and structural traits of these algae all influenced herbivore choice. As a result, these patterns of apical value and chemical defense reflected palatability of live tissues for only one of five algal species, which rendered ODT and GDBH poor predictors of relative palatability for most algae. MSCommittee Chair: Hay, Mark E.; Committee Co-Chair: Kubanek, Julia; Committee Member: Duffy, Meghan A.; Committee Member: Jiang, Li...|$|R
40|$|Homeland Security {{efforts have}} focused on {{procurement}} of emergency equipment continuing a reactive, response oriented tradition historically proven ineffective in large-scale disasters. To date, infrastructure analysis and predictive modeling for disaster impacts are largely academic studies with little operational relevancy. Hindsight has shown that <b>broad</b> <b>predictions</b> and warnings <b>of</b> damage have not improved local planners and emergency managers capability. An operational process is needed whereby local decision makers can understand and predict the time-phased, dependency-driven impact of regional infrastructure failures...|$|R
40|$|<b>Predictions</b> <b>of</b> the {{distribution}} of fungal crop diseases have previously been made solely from climatic data. To our knowledge {{there has been no}} study that has used biotic variables, either alone or in combination with climate factors, to make <b>broad</b> scale <b>predictions</b> <b>of</b> {{the presence or absence of}} fungal species in particular regions. The work presented in this paper used multi-layer perceptrons (MLP) to predict the presence and absence of several species of fungal crop diseases across world-wide geographical regions. These predictions were made using three sets of variables: abiotic climate variables; biotic variables, represented by host plant assemblages; And finally the combination <b>of</b> <b>predictions</b> <b>of</b> the climate and host assemblage MLP using a cascaded MLP architecture, such that final predictions were made from both abiotic and biotic factors. Michael J. Watts and Sue P. Worner[URL]...|$|R
40|$|In 2006, India began {{formally}} reconstructing {{its national}} food safety policy, subsuming over seven laws and agencies {{into a single}} streamlined regulatory authority. This moment of reform offers a "most likely" test case for theories of global policy convergence. Scholars across multiple fields predict that national politics are becoming more similar over time. Those predictions are especially strong {{in the field of}} food safety policy, as the WTO now mandates that member states align with an encyclopedic policy resource called the Codex Alimentarius. The dissertation asks whether, how, and why we see both global pressures for and actual evidence of convergence in the Indian case. I ask if the details of the case map onto the prevailing account in sociology, which predicts convergence as a result of spreading political culture; the sociology <b>of</b> food's <b>broad</b> <b>predictions</b> <b>of</b> both convergence and low political autonomy vis a vis global trade mandates; or the prevailing account in political science, which sees domestic regulatory change as a result of global competitions for consumer markets. I find very limited convergence in the Indian case, mostly limited to a nascent movement toward norms of "science-based" regulation. I also find that theories of regulatory competition best explain why India has converged to the extent it has, though the case suggests new causal mechanisms whereby trade agreements and economic competition generate regulatory change...|$|R
40|$|Each year worldwide, {{millions}} of {{babies are born}} very preterm (before 32 weeks postmenstral age). Very preterm birth puts infants at higher risk for delayed or altered neurodevelopment. While the mechanisms causing these alterations are not fully understood, {{it has been shown}} that image-based biomarkers of the fragile connective white matter brain tissue are correlated with neurodevelopmental outcomes. Diffusion MRI (dMRI) is a non-invasive imaging modality that allows in-vivo analysis of an infant 2 ̆ 7 s white matter brain network (known as a structural connectome) and can be used to better understand neurodevelopment. The purpose of this thesis is to study how the structural connectome can be used for analysis of development and early <b>prediction</b> <b>of</b> outcomes for better informed care. The thesis begins with a thorough examination of the literature on studies that have applied machine learning to brain network data from MRI. It proceeds with a connectome based analysis of the early neurodevelopment of normative preterm infants. Finally, this thesis tackles the problem <b>of</b> early <b>prediction</b> <b>of</b> cognitive and motor neurodevelopmental outcomes using machine learning on connectome data. Three novel prediction methods are proposed for this task, which are found to be able to accurately predict the 18 -month neurodevelopmental outcomes of a cohort of preterm infants from the BC Childrens 2 ̆ 7 Hospital. The thesis concludes with a discussion of how the proposed models may be applicable to <b>a</b> <b>broader</b> set <b>prediction</b> problems and <b>of</b> important future directions for research...|$|R
40|$|<b>A</b> <b>broad</b> {{reliability}} <b>prediction</b> {{method that}} {{can deal with}} complex thermo-fluidic systems is introduced. The procedure provides an engineering tool by integrating multiple computational packages that enable the simulation of {{a wide array of}} systems, especially those involving physics interactions such as fluid flow and solid medium. Computational Fluid Dynamics, Finite Element Method, Monte Carlo Simulation and fatigue analysis tools are integrated within this physics-based reliability prediction approach. The complete procedure is demonstrated using a simple example, and then validated using boiler pipes experimental data. CFD simulations are used to determine the convective terms necessary for the transient FEM thermal analysis. The thermal analysis provides maximum thermal stress whereby the fatigue life of the component is evaluated. As a result of input parameters uncertainty, the expected life will be {{in the form of a}} Probability Density Function, which enables the calculation of the reliability of the component...|$|R
40|$|The {{two areas}} of theory upon which this {{research}} was based were „strategy development process?(SDP) and „complex adaptive systems? (CAS), as part of complexity theory, focused on human social organisations. The literature reviewed showed {{that there is a}} paucity of empirical work and theory in the overlap of the two areas, providing an opportunity for contributions to knowledge in each area of theory, and for practitioners. An inductive approach was adopted for this research, in an effort to discover new insights to the focus area of study. It was undertaken from within an interpretivist paradigm, and based on a novel conceptual framework. The organisationally intimate nature of the research topic, and the researcher?s circumstances required a research design that was both in-depth and long term. The result was a single, exploratory, case study, which included use of data from 44 in-depth, semi-structured interviews, from 36 people, involving all the top management team members and significant other staff members; observations, rumour and grapevine (ORG) data; and archive data, over a 5 ½ year period (2005 – 2010). Findings confirm the validity of the conceptual framework, and that complex adaptive systems theory has potential to extend strategy development process theory. It has shown how and why the strategy process developed in the case study organisation by providing deeper insights to the behaviour of the people, their backgrounds, and interactions. <b>Broad</b> <b>predictions</b> <b>of</b> the „latent strategy development? process and some elements of the strategy content are also possible. Based on this research, it is possible to extend the utility of the SDP model by including peoples? behavioural characteristics within the organisation, via complex adaptive systems theory. Further research is recommended to test limits of the application of the conceptual framework and improve its efficacy with more organisations across a variety of sectors...|$|R
40|$|Cold regions, {{which are}} {{expected}} to suffer particularly severe future climate effects, will pose very challenging geotechnical conditions in the 21 st century involving ground freezing and thawing. Given the uncertainty of future environmental changes and the vast expanses of the cold regions, {{it is appropriate to}} address problems such as pipeline or road construction with analytical methods that have multiple scales and layers. High- and middle-level predictive tools are described that integrate climatic predictions from AOGCMs and their down-scaling schemes, geological and topographical (DEM) information, remotely-sensed vegetation data and non-linear finite element analysis for soil freezing and thawing. These tools output <b>broad</b> scale <b>predictions</b> <b>of</b> geothermal responses, at a regional scale, that offer hazard zoning schemes related to permafrost thawing. A more intensive localscale predictive tool is then outlined that considers fully-coupled thermo-hydro-mechanical processes occurring at the soil-element level and outputs detailed predictions for temperature changes, pore water behaviour, ground stresses and deformation in and around geotechnical structures. Applications of these tools to specific problems set in Eastern Siberia and pipeline heave tests are illustrated. Postprint (published version...|$|R
40|$|This {{article has}} been {{published}} in the journal, Journal of Physics: Condensed Matter [© Institute of Physics]. It is available at: [URL] magnetization distributions in a series of ternary intermetallic compounds based on the composition Co 2 YZ where Y is Ti, Mn or Fe and Z a subgroup-B element have been determined from polarized neutron diffraction measurements. Comparison of the magnetic structure factors with model calculations shows that the magnetization is associated principally with those atoms which in their elemental state are themselves magnetic. The observed deviations of the magnetic moment distributions from spherical symmetry have been used to deduce which of the 3 d sub-bands are active at the Fermi energy. A small moment close to the limits of resolution is observed at some of the Z sites, together with a small delocalized moment which in most cases is negative. The results have been compared with the <b>predictions</b> <b>of</b> band models, which indicate that the Fermi level falls in <b>a</b> <b>broad</b> minimum in the minority-spin density of d states. Although the identity of the bands active at the Fermi surface is in <b>broad</b> agreement with <b>predictions</b> <b>of</b> band-structure calculations (Ishida S, Akazawa S, Kubo Y and Ishida J 1982 J. Phys. F: Met. Phys. 12 1111), the results suggest that there is a finite density of states in the minority-spin d band of manganese. Hence the compounds cannot be classified as half-metallic ferromagnets...|$|R
40|$|Nilanjana Dasgupta’s (this issue) {{stereotype}} inoculation model (SIM) {{helps explain}} why what feels like a free choice to pursue one life path over another “is often constrained by subtle cues in achievement environments that signal who naturally belongs there and {{is most likely to}} succeed and who else is a dubious fit ” (p. 231). She posits that seeing others like themselves in successful roles inoculates women against negative stereotypes that impede their success and persistence in specific achievement contexts. As is true of classic theoretical positions (see Nagel, 1961), Dasgupta presents postulates from which she deduces a specific set of hypotheses, and she reviews the relevant empirical/ observational data in support of them. It is precisely what this area of research has long needed— moving beyond demonstrations of identity threats to a theory about their underlying causes, conditions, and interventions. This proposal leads her to four <b>broad</b> <b>predictions,</b> the first <b>of</b> which is the primary focus of our comment. Exposure to Successful Ingroup Peers/Experts A central tenet of Dasgupta’s theory is that ingroup members who are peers or experts in...|$|R
40|$|Present day chiral nucleon-nucleon {{potentials}} up to N 3 LO {{and three}} nucleon forces at N 2 LO {{are used to}} analyze nucleon-deuteron radiative capture at deuteron lab energies below E_d= 100 MeV. The differential cross section and the deuteron analyzing powers A_y(d) and A_{yy} are presented and compared to data. The theoretical predictions are obtained in the momentum-space Faddeev approach using the nuclear electromagnetic current operator with exchange currents introduced via the Siegert theorem. The chiral forces provide the same quality of data description as {{a combination of the}} two-nucleon AV 18 and the three-nucleon Urbana IX interactions. However, the different parametrizations of the chiral potentials lead to <b>broad</b> bands <b>of</b> <b>predictions.</b> Comment: 20 pages, 12 ps figure...|$|R
40|$|We {{begin an}} {{exploration}} of the physics associated with the general CP-conserving MSSM with Minimal Flavor Violation, the pMSSM. The 19 soft SUSY breaking parameters in this scenario are chosen so as to satisfy all existing experimental and theoretical constraints assuming that the WIMP is a conventional thermal relic, i. e., the lightest neutralino. We scan this parameter space twice using both flat and log priors for the soft SUSY breaking mass parameters and compare the results which yield similar conclusions. Detailed constraints from both LEP and the Tevatron searches play a particularly important role in obtaining our final model samples. We find that the pMSSM leads to <b>a</b> much <b>broader</b> set <b>of</b> <b>predictions</b> for the properties of the SUSY partners as well as for a number of experimental observables than those found in any of the conventional SUSY breaking scenarios such as mSUGRA. This set of models can easily lead to atypical expectations for SUSY signals at the LHC...|$|R
40|$|Based on the {{experience}} of the Monitoring Agriculture with Remote Sensing (MARS) project in Europe, in 2001 the European Commission decided to enlarge the monitoring activities to other regions of the world. The MARS-FOOD group was established to support the Food Aid and Food Security policies of the European Commission. The methodologies developed for crop monitoring combine remote sensing and meteorological data, which can be used directly in a GIS environment or as input for crop growth simulation models (CGMS and FAO-CSWB). Several techniques are used for extracting crop specific temporal profiles of the Normalized Difference Vegetation Index (NDVI) and additionally dry matter production maps are calculated according to the Monteith approach. Both remote sensing and agrometeorological indicators are then used together with agricultural statistics for crop yield forecasting. A new method has been recently developed by MARS-FOOD to produce quantitative yield forecasts for regions with generally low data availability by using a multistep procedure. The method starts with a time series analysis of the historical yield to have <b>a</b> first <b>broad</b> <b>prediction</b> range. Similarity analysis is then used to identify the year with the most similar crop growing conditions over a time series. The next step is a regression analysis involving the different meteorological and remote sensing indicators available. Finally, {{based on the results of}} the previous 3 steps, the most likely yield value is estimated. The method is still in a test phase, but is already used in a preoperational way in the MARS-FOOD crop monitoring bulletins for the Mediterranean Basin, Russia and Central Asia and Somalia. JRC. G. 3 -Agricultur...|$|R
40|$|We {{present a}} parsing system {{designed}} to parse sentences containing unknown words as accurately as possible. Our post-mortem parsing algorithm combines syntactic parsing rules, morphological recognition, and closed-class lexicon with a method that attempts to parse a sentence first with a limited prediction for unknown words, and later reparse the sentence with <b>a</b> more <b>broad</b> <b>prediction</b> if first attempts fail. This allows great flexibility while parsing, and can offer improved accuracy and efficiency for parsing sentences that contain unknown words. Experiments involving hand-created and computer-generated morphological recognizers are performed. We also develop a part-of-speech tagging {{system designed to}} accurately tag sentences, including sentences containing unknown words. The system {{is based on a}} basic hidden Markov model, but uses second-order approximations for the probability distributions (instead of first-order). The second order approximations give increased tagging accuracy, without increasing asymptotic running time over traditional trigram taggers. A dynamic smoothing technique is used to address sparse data by attaching more weight to events that occur more frequently. Unknown words are predicted using statistical estimation from the training corpus based on word endings only. Information from different length suffixes is included in a weighted voting scheme, smoothed in a fashion similar to that used for the second-order HMM. This tagging model achieves state-of-the-art accuracies. Finally, the use of syntactic parsing rules to increase tagging accuracy is considered. By allowing a parser to veto possible tag sequences due to violation of syntactic rules, it is shown that tagging errors were reduced by 28 % on the Timit corpus. This enhancement is useful for corpora that have rules sets defined. ...|$|R
3000|$|... /d, and {{concrete}} strength, [...] f_c^' on the load-carrying capacity <b>predictions</b> <b>of</b> the strut-and-tie based method {{and the five}} codes of practice examined for the double-sided corbel model only, respectively. On the whole, the <b>predictions</b> <b>of</b> the proposed method are very consistent for <b>a</b> <b>broad</b> range of concrete strengths and shear span-to-depth ratio, indicating that Avg, SD and COV are 1.39, 0.29, and 21 [...]...|$|R
40|$|Traditional {{studies of}} {{agricultural}} technology adoption {{have long been}} constrained by a limited ability to include spatially-differentiated data. Typically, crude proxies or location dummy variables are used to approximate spatial effects. GIS tools, however, now allow spatially explicit data {{to be included in}} household econometric models of technology adoption. This paper describes a study that combined GIS and survey variables to examine the cattle feeding strategies on farms in highland Kenya. Data from a large geo-referenced household survey were combined with GIS-derived variables to comprehensively evaluate the spatial, agro-ecological, market and farm resource factors that determine variability of feeding strategies on smallholder dairy farms. Roads, urban populations, milk collection and processing facilities were digitised, and integrated with spatial coverages of agro-ecology. These were then combined, using econometric methods, to quantify the main spatial and local determinants of the probability of adoption of: a) stall feeding or zero-grazing, and b) planted fodder in the form of Napier grass. The results show the influence not only of agro-ecology, but also of market infrastructure and support services on the adoption of improved feeding strategies. A comparison of predicted uptake using GIS and household variables shows that after first calibrating GIS-derived variables through <b>a</b> household survey, <b>broad</b> but reliable <b>predictions</b> <b>of</b> technology uptake in other areas may be possible. Department for International Development, United Kingdo...|$|R
30|$|The {{institutional}} environment comprised formal institutions, such as legal, {{financial and}} political systems {{as well as}} informal institutions, such as cultural, values, norms and beliefs (Lubatkin et al. 2005). The nature {{of the political and}} legal system at the country level (Matten and Moon 2008) enables the <b>prediction</b> <b>of</b> <b>a</b> <b>broader</b> stakeholder orientation versus a more discrete shareholders’ perspective.|$|R
40|$|A {{simple and}} yet quite {{accurate}} <b>prediction</b> <b>of</b> volume {{as a function}} of pressure for metals and alloys is presented. Thermal expansion coefficients and melting temperatures are predicted by simple, analytic expressions and results compare favorably with experiment for <b>a</b> <b>broad</b> range of metals. All <b>of</b> these <b>predictions</b> are made possible by the discovery of universality in binding energy relations for metals...|$|R
