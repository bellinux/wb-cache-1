4|0|Public
40|$|We {{describe}} {{a strategy for}} code modernisation of Gadget, a widely used community code for computational astrophysics. The focus of this work is on node-level performance optimisation, targeting current multi/many-core IntelR architectures. We identify and isolate a sample code kernel, which is representative of a typical Smoothed Particle Hydrodynamics (SPH) algorithm. The code modifications include threading parallelism optimisation, change of the data layout into Structure of Arrays (SoA), <b>auto-vectorisation</b> and algorithmic improvements in the particle sorting. We obtain shorter execution time and improved threading scalability both on Intel XeonR (2. 6 × on Ivy Bridge) and Xeon PhiTM (13. 7 × on Knights Corner) systems. First few tests of the optimised code result in 19. 1 × faster execution on second generation Xeon Phi (Knights Landing), thus demonstrating the portability of the devised optimisation solutions to upcoming architectures. Comment: 8 pages, 2 columns, 4 figures, accepted as paper at HPCS Proceedings 2017, IEEE XPLOR...|$|E
40|$|This white-paper {{reports on}} {{our efforts to}} enable an SPH-based Fortran code on the Intel Xeon Phi. As {{a result of the}} work {{described}} here, the two most computationally intensive subroutines (rates and shepard_beta) of the UCD-SPH code were refactored and parallelised with OpenMP for the first time, enabling the code to be executed on multi-core and many-core shared memory systems. This parallelisation achieved speedups of up to 4. 3 x for the rates subroutine and 6. 0 x for the shepard_beta subroutine resulting in overall speedups of up to 4. 2 x on a 2 processor Sandy Bridge Xeon E 5 machine. The code was subsequently enabled and refactored to execute in different modes on the Intel Xeon Phi co-processor achieving speedups of up to 2. 8 x for the rates subroutine and up to 3. 8 x for the shepard_beta subroutine producing overall speedups of up to 2. 7 x compared to the original unoptimised code. To explore the capabilities of <b>auto-vectorisation</b> the shepard_beta subroutine was refactored which results in speedups of up to 6. 4 x for the shepard_beta subroutine relative to the original unoptimised version of the shepard_beta subroutine. The development and testing phases of the project were carried out on the PRACE EURORA machine. 1...|$|E
40|$|For modern x 86 based CPUs with {{increasingly}} longer vector lengths, achieving good vectorization {{has become}} very important for gaining higher performance. Using very explicit SIMD vector programming techniques {{has been shown to}} give near optimal performance, however they are difficult to implement for all classes of applications particularly ones with very irregular memory accesses and usually require considerable re-factorisation of the code. Vector intrinsics are also not available for languages such as Fortran which is still heavily used in large production applications. The alternative is to depend on compiler auto-vectorization which usually have been less effective in vectorizing codes with irregular memory access patterns. In this paper we present recent research exploring techniques to gain compiler auto-vectorization for unstructured mesh applications. A key contribution is details on software techniques that achieve <b>auto-vectorisation</b> for a large production grade unstructured mesh application from the CFD domain so as to benefit from the vector units on the latest Intel processors without a significant code re-write. We use code generation tools in the OP 2 domain specific library to apply the auto-vectorising optimisations automatically to the production code base and further explore the performance of the application compared to the performance with other parallelisations such as on the latest NVIDIA GPUs. We see that there is considerable performance improvements with autovectorization. The most compute intensive parallel loops in the large CFD application shows speedups of nearly 40 % on a 20 core Intel Haswell system compared to their nonvectorized versions. However not all loops gain due to vectorization where loops with less computational intensity lose performance due to the associated overheads...|$|E
40|$|The Finite-Difference Time-Domain (FDTD) {{method is}} applied to the {{analysis}} of vibroacoustic problems and to study the propagation of longitudinal and transversal waves in a stratified media. The potential of the scheme and the relevance of each acceleration strategy for massively computations in FDTD are demonstrated in this work. In this paper, we propose two new specific implementations of the bidimensional scheme of the FDTD method using multi-CPU and multi-GPU, respectively. In the first implementation, an open source message passing interface (OMPI) has been included in order to massively exploit the resources of a biprocessor station with two Intel Xeon processors. Moreover, regarding CPU code version, the streaming SIMD extensions (SSE) and also the advanced vectorial extensions (AVX) have been included with shared memory approaches that take advantage of the multi-core platforms. On the other hand, the second implementation called the multi-GPU code version is based on Peer-to-Peer communications available in CUDA on two GPUs (NVIDIA GTX 670). Subsequently, this paper presents an accurate analysis of the influence of the different code versions including shared memory approaches, vector instructions and multi-processors (both CPU and GPU) and compares them in order to delimit the degree of improvement of using distributed solutions based on multi-CPU and multi-GPU. The performance of both approaches was analysed and it has been demonstrated that the addition of shared memory schemes to CPU computing improves substantially the performance of vector instructions enlarging the simulation sizes that use efficiently the cache memory of CPUs. In this case GPU computing is slightly twice times faster than the fine tuned CPU version in both cases one and two nodes. However, for massively computations explicit vector instructions do not worth it since the memory bandwidth is the limiting factor and the performance tends to be the same than the sequential version with <b>auto-vectorisation</b> and also shared memory approach. In this scenario GPU computing is the best option since it provides a homogeneous behaviour. More specifically, the speedup of GPU computing achieves an upper limit of 12 for both one and two GPUs, whereas the performance reaches peak values of 80 GFlops and 146 GFlops for the performance for one GPU and two GPUs respectively. Finally, the method {{is applied to}} an earth crust profile in order to demonstrate the potential of our approach and the necessity of applying acceleration strategies in these type of applications. Peer ReviewedPostprint (author's final draft...|$|E

