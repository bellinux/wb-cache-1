3|10000|Public
5000|$|The factor theorem is {{also used}} to remove known zeros from a {{polynomial}} while leaving all unknown zeros intact, thus producing <b>a</b> <b>lower</b> <b>degree</b> <b>polynomial</b> whose zeros may be easier to find. Abstractly, the method is as follows: ...|$|E
40|$|In this report, {{we present}} two {{mathematical}} results {{which can be}} useful in a variety of settings. First, we present an analysis of Legendre polynomials triple product integral. Such integrals arise whenever two functions are multiplied, with both the operands and the result represented in the Legendre polynomial basis. We derive a recurrence relation to calculate these integrals analytically. We also establish the sparsity of the triple product integral tensor, and derive the Legendre polynomial triple product integral theorem, giving the exact closed form expression for the sparsity structure. Secondly, we derive a truncation scheme to approximate a polynomial with <b>a</b> <b>lower</b> <b>degree</b> <b>polynomial,</b> while keeping the approximation error low under the L ∞ norm. We use the Chebyshev polynomials to derive our truncation scheme. We present empirical results which suggest that the approximation error is quite low, even for fairly lo...|$|E
40|$|We {{present a}} novel way of {{deciding}} {{when and where}} to refine a mesh in probability space {{in order to facilitate}} the uncertainty quantification in the presence of discontinuities in random space. A discontinuity in random space makes the application of generalized polynomial chaos expansion techniques prohibitively expensive. The reason is that for discontinuous problems, the expansion converges very slowly. An alternative to using higher terms in the expansion is to divide the random space in smaller elements where <b>a</b> <b>lower</b> <b>degree</b> <b>polynomial</b> is adequate to describe the randomness. In general, the partition of the random space is a dynamic process since some areas of the random space, particularly around the discontinuity, need more refinement than others as time evolves. In the current work we propose a way to decide when and where to refine the random space mesh based on the use of a reduced model. The idea is that a good reduced model can monitor accurately, within a random space element, the cascade of activity to higher degree terms in the chaos expansion. In terms, this facilitates the efficient allocation of computational sources to the areas of random space where they are more needed. For the Kraichnan-Orszag system, the prototypical system to study discontinuities in random space, we present theoretical results which show why the proposed method is sound and numerical results which corroborate the theory. Comment: 26 page...|$|E
40|$|The {{polarization}} {{process of}} polar codes over a ternary alphabet is studied. Recently {{it has been}} shown that the scaling of the blocklength of polar codes with prime alphabet size scales polynomially with respect to the inverse of the gap between code rate and channel capacity. However, except for the binary case, the <b>degree</b> of the <b>polynomial</b> in the bound is extremely large. In this work, it is shown that <b>a</b> much <b>lower</b> <b>degree</b> <b>polynomial</b> can be computed numerically for the ternary case. Similar results are conjectured for the general case of prime alphabet size. Comment: Submitted to ISIT 201...|$|R
40|$|Given an {{algebraic}} geometry code CL(D, αP), the Guruswami-Sudan algorithm produces {{a list of}} all codewords in CL(D, αP) within a specified distance of a received word. The initialization step in the algorithm involves parameter choices that bound the degree of the interpolating polynomial and hence the length of the list of codewords generated. In this paper, we use simple properties of discriminants of polynomials over finite fields to provide improved parameter choices for the Guruswami-Sudan list decoding algorithm for {{algebraic geometry}} codes. As a consequence, we obtain a better bound on the list size as well as <b>a</b> <b>lower</b> <b>degree</b> interpolating <b>polynomial...</b>|$|R
40|$|Abstract—This paper {{presents}} an alternative box-spline filter for the body-centered cubic (BCC) lattice, the 7 -direction quartic boxspline M 7 {{that has the}} same approximation order as the 8 -direction quintic box-spline M 8 but <b>a</b> <b>lower</b> <b>polynomial</b> <b>degree,</b> smaller support, and is computationally more efficient. When applied to reconstruction with quasi-interpolation prefilters, M 7 shows less aliasing, which is verified quantitatively by integral filter metrics and frequency error kernels. To visualize and analyze distributional aliasing characteristics, each spectrum is evaluated on the planes and lines with various orientations. Index Terms—Volume reconstruction, BCC lattice, box-spline, quasi-interpolation...|$|R
40|$|Traveling wave {{solutions}} of degenerate coupled ℓ-KdV equations are studied. Due to symmetry reduction these equations {{reduce to}} one ODE, (f') ^ 2 =P_n(f) where P_n(f) is a polynomial function of f of degree n=ℓ+ 2, where ℓ≥ 3 in this work. Here ℓ {{is the number}} of coupled fields. There is no known method to solve such ordinary differential equations when ℓ≥ 3. For this purpose, we introduce two different type of methods to solve the reduced equation and apply these methods to degenerate three-coupled KdV equation. One of the methods uses the Chebyshev's Theorem. In this case we find several solutions some of which may correspond to solitary waves. The second method is a kind of factorizing the polynomial P_n(f) as <b>a</b> product of <b>lower</b> <b>degree</b> <b>polynomials.</b> Each part of this product is assumed to satisfy different ODEs. Comment: 25 pages, 14 figures. arXiv admin note: text overlap with arXiv: 1308. 564...|$|R
40|$|Let f be a <b>polynomial</b> of <b>degree</b> d in n {{variables}} over {{a finite}} field F. The polynomial {{is said to}} be unbiased if the distribution of f(x) for a uniform input x ∈F^n is close to the uniform distribution over F, and is called biased otherwise. The polynomial is said to have low rank if it can be expressed as a composition of <b>a</b> few <b>lower</b> <b>degree</b> <b>polynomials.</b> Green and Tao [Contrib. Discrete Math 2009] and Kaufman and Lovett [FOCS 2008] showed that bias implies low rank for fixed <b>degree</b> <b>polynomials</b> over fixed prime fields. This {{lies at the heart of}} many tools in higher order Fourier analysis. In this work, we extend this result to all prime fields (of size possibly growing with n). We also provide a generalization to nonprime fields in the large characteristic case. However, we state all our applications in the prime field setting for the sake of simplicity of presentation. As an immediate application, we obtain improved bounds for a suite of problems in effective algebraic geometry, including Hilbert nullstellensatz, radical membership and counting rational points in low degree varieties. Using the above generalization to large fields as a starting point, we are also able to settle the list decoding radius of fixed degree Reed-Muller codes over growing fields. The case of fixed size fields was solved by Bhowmick and Lovett [STOC 2015], which resolved a conjecture of Gopalan-Klivans-Zuckerman [STOC 2008]. Here, we show that the list decoding radius is equal the minimum distance of the code for all fixed degrees, even when the field size is possibly growing with n...|$|R
40|$|This paper {{compares the}} XL {{algorithm}} with Gröbner basis algorithm. We explain {{the link between}} XL computation result and Gröbner basis with the well-known notion of D-Gröbner basis. Then we compare these algorithms in two cases: in the fields q≫ n. For the field we have proved that if XL needs to compute <b>polynomial</b> with <b>degree</b> D to terminate, the whole Gröbner basis is computated without exceeding the degree D. We have studied the XL algorithm and F_ 5 algorithm on semi-regular sequences introduced in report F 5 _complexite. We show {{that the size of}} matrix constructed by XL is huge compared to the ones of F_ 5. So the complexity of XL is worth than F_ 5 algorithm on these systems. For the field we introduce an emulated algorithm using Gröbner basis computation to have a comparison between XL and Gröbner basis. We have proved that this algorithm will always reach <b>a</b> <b>lower</b> <b>degree</b> for intermediate <b>polynomials</b> than XL algorithm. A study on semi-regular sequences shows that F_ 5 always has a better behavior than XL algorithm especially when m is near from n...|$|R
40|$|We study Diophantine {{equations}} of type f(x) =g(y), {{where both}} f and g {{have at least}} two distinct critical points and equal critical values at at most two distinct critical points. Some classical families of polynomials (f_n) _n are such that f_n satisfies these assumptions for all n. Our results cover and generalize several results in the literature on the finiteness of integral solutions to such equations. In doing so, we analyse the properties of the monodromy groups of such polynomials. We show that if f has coefficients in a field K, at least two distinct critical points and all distinct critical values, and char(K) is not a divisor of the degree of f, then the monodromy group of f is a doubly transitive permutation group. This is the same as saying that (f(x) -f(y)) /(x-y) is irreducible over K. In particular, f cannot be represented as <b>a</b> composition of <b>lower</b> <b>degree</b> <b>polynomials.</b> We further show that if f has at least two distinct critical points and equal critical values at at most two of them, and if f(x) =g(h(x)), where g and h have coefficients in K and g is of degree at least 2, then either the degree of h is less or equal than 2, or f is of special type. In the latter case, in particular, f has no three simple critical points, nor five distinct critical points. Comment: 2...|$|R
40|$|PROBLEM SETTING: Support vector {{machines}} (SVMs) {{are very}} popular tools for classification, regression and other problems. Due {{to the large}} choice of kernels they can be applied with, a large variety of data can be analysed using these tools. Machine learning thanks its popularity to the good performance of the resulting models. However, interpreting the models is far from obvious, especially when non-linear kernels are used. Hence, the methods are used as black boxes. As a consequence, the use of SVMs is less supported in areas where interpretability is important and where people are {{held responsible for the}} decisions made by models. OBJECTIVE: In this work, we investigate whether SVMs using linear, polynomial and RBF kernels can be explained such that interpretations for model-based decisions can be provided. We further indicate when SVMs can be explained and in which situations interpretation of SVMs is (hitherto) not possible. Here, explainability is defined as the ability to produce the final decision based on a sum of contributions which depend on one single or at most two input variables. RESULTS: Our experiments on simulated and real-life data show that explainability of an SVM depends on the chosen parameter values (<b>degree</b> of <b>polynomial</b> kernel, width of RBF kernel and regularization constant). When several combinations of parameter values yield the same cross-validation performance, combinations with <b>a</b> <b>lower</b> <b>polynomial</b> <b>degree</b> or <b>a</b> larger kernel width have a higher chance of being explainable. CONCLUSIONS: This work summarizes SVM classifiers obtained with linear, polynomial and RBF kernels in a single plot. Linear and polynomial kernels up to the second degree are represented exactly. For other kernels an indication of the reliability of the approximation is presented. The complete methodology is available as an R package and two apps and a movie are provided to illustrate the possibilities offered by the method...|$|R
40|$|Support vector {{machines}} (SVMs) {{are very}} popular tools for classification, regression and other problems. Due {{to the large}} choice of kernels they can be applied with, a large variety of data can be analysed using these tools. Machine learning thanks its popularity to the good performance of the resulting models. However, interpreting the models is far from obvious, especially when non-linear kernels are used. Hence, the methods are used as black boxes. As a consequence, the use of SVMs is less supported in areas where interpretability is important and where people are {{held responsible for the}} decisions made by models. In this work, we investigate whether SVMs using linear, polynomial and RBF kernels can be explained such that interpretations for model-based decisions can be provided. We further indicate when SVMs can be explained and in which situations interpretation of SVMs is (hitherto) not possible. Here, explainability is defined as the ability to produce the final decision based on a sum of contributions which depend on one single or at most two input variables. Our experiments on simulated and real-life data show that explainability of an SVM depends on the chosen parameter values (<b>degree</b> of <b>polynomial</b> kernel, width of RBF kernel and regularization constant). When several combinations of parameter values yield the same cross-validation performance, combinations with <b>a</b> <b>lower</b> <b>polynomial</b> <b>degree</b> or <b>a</b> larger kernel width have a higher chance of being explainable. This work summarizes SVM classifiers obtained with linear, polynomial and RBF kernels in a single plot. Linear and polynomial kernels up to the second degree are represented exactly. For other kernels an indication of the reliability of the approximation is presented. The complete methodology is available as an R package and two apps and a movie are provided to illustrate the possibilities offered by the method...|$|R
40|$|Treatments of Galois {{groups of}} cubic and quartic polynomials usually avoid fields of {{characteristic}} 2. Here {{we will discuss}} these Galois groups and allow all characteristics. Of course, to have a Galois group of a polynomial we will assume our cubic and quartic polynomials are separable, and to avoid reductions to <b>lower</b> <b>degree</b> <b>polynomials</b> we wil...|$|R
40|$|The most {{frequently}} asked {{question in the}} p-adic lattice models of statistical mechanics is that whether a root of a polynomial equation belongs to domains Z_p^*, Z_p∖Z_p^*, Z_p, Q_p∖Z_p^*, Q_p∖(Z_p∖Z_p^*), Q_p∖Z_p, Q_p or not. However, this question was open even for <b>lower</b> <b>degree</b> <b>polynomial</b> equations. In this paper, we give local descriptions of roots of cubic equations over the p-adic fields for p> 3. Comment: 19 page...|$|R
40|$|AbstractWe study a {{discrete}} optimization problem introduced by Babai, Frankl, Kutin, and Štefankovič (2001), which provides bounds on <b>degrees</b> of <b>polynomials</b> with p-adically controlled behavior. Such polynomials {{are of particular}} interest because they furnish bounds {{on the size of}} set systems satisfying Frankl–Wilson-type conditions modulo prime powers, with <b>lower</b> <b>degree</b> <b>polynomials</b> providing better bounds. We elucidate the asymptotic structure of solutions to the optimization problem, and we also provide an improved method for finding solutions in certain circumstances...|$|R
40|$|We {{study the}} problem of how well a typical multivariate {{polynomial}} can be approximated by <b>lower</b> <b>degree</b> <b>polynomials</b> over F 2. We prove that, with very high probability, a random <b>degree</b> d <b>polynomial</b> has only an exponentially small correlation with all <b>polynomials</b> of <b>degree</b> d − 1, for all degrees d up to Θ(n). That is, a random <b>degree</b> d <b>polynomial</b> does not admit good approximations of lesser degree. In order to prove this, we prove far tail estimates on the distribution of the bias of a random low <b>degree</b> <b>polynomial.</b> As part of the proof, we also prove tight lower bounds on the dimension of truncated Reed–Muller codes. ...|$|R
3000|$|..., has <b>a</b> <b>lower</b> <b>degree.</b> <b>A</b> <b>lower</b> <b>degree</b> {{determinant}} implies less zero padding for the packets {{and hence}} a reduced overall overhead. The new parity matrix [...]...|$|R
40|$|Galois {{theory is}} {{developed}} using elementary polynomial and group algebra. The method follows closely the original prescription of Galois, {{and has the}} benefit of making the theory accessible to a wide audience. The theory is illustrated by a solution in radicals of <b>lower</b> <b>degree</b> <b>polynomials,</b> and the standard result of the insolubility in radicals of the general quintic and above. This is augmented by the presentation of a general solution in radicals for all polynomials when such exist, and illustrated with specific cases. A method for computing the Galois group and establishing whether a radical solution exists is also presented...|$|R
40|$|Abstract—In this paper, we {{introduce}} a generalized Chebyshev collocation method (GCCM) {{based on the}} generalized Chebyshev polynomials for solving stiff systems. For employing a technique of the embedded Runge-Kutta method used in explicit schemes, {{the property of the}} generalized Chebyshev polynomials is used, in which the nodes for the higher <b>degree</b> <b>polynomial</b> are overlapped with those for the <b>lower</b> <b>degree</b> <b>polynomial.</b> The constructed algorithm controls both the error and the time step size simultaneously and further the errors at each integration step are embedded in the algorithm itself, which provides the efficiency of the computational cost. For the assessment of the effectiveness, numerical results obtained by the proposed method and the Radau IIA are presented and compared. Keywords—Generalized Chebyshev Collocation method, I...|$|R
40|$|We study Diophantine {{equations}} of type f(x) =g(y), where f and g are lacunary polynomials. According to a {{well known}} finiteness criterion, {{for a number}} field K and nonconstant f, g∈ K[x], the equation f(x) =g(y) has infinitely many solutions in S-integers x, y only if f and g are representable as a functional composition of <b>lower</b> <b>degree</b> <b>polynomials</b> in <b>a</b> certain prescribed way. The behaviour of lacunary polynomials with respect to functional composition is a topic of independent interest, and has been studied by several authors. In this paper we utilize known results and develop some new results on the latter topic. Comment: older paper (from 2015 / 2016...|$|R
40|$|In this paper, Euler {{gives the}} general trionomial {{coefficient}} as a {{sum of the}} binomial coefficients, the general quadrinomial coefficient as a sum of the binomial and trinomial coefficients, the general quintonomial coefficient as a sum of the binomial and quadrinomial coefficients, and gives a general determination of the coefficients of the expansion of any polynomial (1 +x+x^ 2 + [...] . +x^m) ^n as a sum of the coefficients of <b>lower</b> <b>degree</b> <b>polynomial</b> coefficients. Comment: 8 pages. Seems to be first English translation of Euler's Latin original ``De evolutione potestatis polynomialis cuiuscunque (1 +x+x^ 2 +x^ 3 +x^ 4 +etc.) ^n, Nova Acta Academiae Scientarum Imperialis Petropolitinae 12 (1801), 47 - 5...|$|R
5000|$|Efficiency: {{parametric}} estimating requires {{less time}} and <b>a</b> <b>lower</b> <b>degree</b> of project definition {{for supporting the}} estimates.|$|R
5|$|Differences in {{pronunciation}} between Ottawa {{and other}} dialects of Ojibwe, resulting in <b>a</b> <b>lower</b> <b>degree</b> of mutual intelligibility.|$|R
50|$|<b>A</b> <b>lower</b> <b>degree</b> of {{testability}} {{results in}} increased test effort. In extreme cases {{a lack of}} testability may hinder testing parts of the software or software requirements at all.|$|R
50|$|In Italy, {{gold-plating}} {{has often}} {{been used as a}} device to pass through controversial measures and to ensure <b>a</b> <b>lower</b> <b>degree</b> of parliamentary scrutiny, particularly in periods of weak government.|$|R
40|$|Efficient and {{unconditionally stable}} high order time marching schemes are very {{important}} but not easy to construct for nonlinear phase dynamics. In this paper, we propose and analysis an efficient stabilized linear Crank-Nicolson scheme for the Cahn-Hilliard equation with provable unconditional stability. In this scheme the nonlinear bulk force are treated explicitly with two second-order linear stabilization terms. The semi-discretized equation is a linear elliptic system with constant coefficients, thus robust and efficient solution procedures are guaranteed. Rigorous error analysis show that, when the time step-size is small enough, the scheme is second order accurate in time with aprefactor controlled by some <b>lower</b> <b>degree</b> <b>polynomial</b> of 1 /ε. Here ε is the interface thickness parameter. Numerical results are presented to verify the accuracy and efficiency of the scheme. Comment: 26 pages, 2 figures. arXiv admin note: substantial text overlap with arXiv: 1708. 0976...|$|R
5000|$|The term piquancy [...] is {{sometimes}} applied to foods with <b>a</b> <b>lower</b> <b>degree</b> of pungency that are [...] "agreeably stimulating to the palate." [...] Examples of piquant food include mustard and curry.|$|R
5000|$|As the American term PhD (i.e. {{doctor of}} philosophy) {{is much more}} widely {{understood}} internationally, the degree is frequently translated as [...] "MD, PhD". This, however, is somewhat misleading, as the PhD is officially considered <b>a</b> <b>lower</b> <b>degree</b> than the Dr.Med. in Denmark, where both degrees exist, and also because Dr.Med. is not a doctorate of philosophy, but a doctorate of medicine (as opposed to a doctorate of philosophy in medicine). In Denmark, Dr.Med. is frequently {{referred to as a}} higher doctorate along with other traditional doctorates (in Denmark, the PhD is not considered a doctorate, strictly speaking, but <b>a</b> <b>lower</b> <b>degree).</b>|$|R
50|$|If C < 1 then a {{monopolist}} {{country has}} <b>a</b> <b>lower</b> <b>degree</b> of monopoly power in segment Mp than segment Ms and therefore {{this country has}} a greater incentive to specialize in Ms.|$|R
40|$|This work {{proposes a}} method for solving linear {{stochastic}} optimal control (SOC) problems using sum of squares and semidefinite programming. Previous work had used polynomial optimization to approximate the value function, requiring a high <b>polynomial</b> <b>degree</b> to capture local phenomena. To improve the scalability of the method to problems of interest, a domain decomposition scheme is presented. By using local approximations, <b>lower</b> <b>degree</b> <b>polynomials</b> become sufficient, and both local and global properties of the value function are captured. The domain {{of the problem is}} split into a non-overlapping partition, with added constraints ensuring $C^ 1 $ continuity. The Alternating Direction Method of Multipliers (ADMM) is used to optimize over each domain in parallel and ensure convergence on the boundaries of the partitions. This results in improved conditioning of the problem and allows for much larger and more complex problems to be addressed with improved performance. Comment: 8 pages. Accepted to CDC 201...|$|R
5000|$|The term [...] "{{telephone}} apprehension" [...] {{refers to}} <b>a</b> <b>lower</b> <b>degree</b> of telephone phobia, in which sufferers experience {{anxiety about the}} use of telephones, but to a less severe degree than that of an actual phobia.|$|R
6000|$|O had she but been of <b>a</b> <b>lower</b> <b>degree,</b> [...] I then might hae hop'd she wad smil'd upon me! [...] O how past descriving {{had then}} been my bliss, [...] As now my {{distraction}} nae words can express.|$|R
40|$|One aspect in {{the theory}} of {{orthogonal}} polynomials is their study as special functions. Most important orthogonal polynomials can be written as terminating hypergeometric series and during the twentieth century people have been working on a classification of all such hypergeometric orthogonal polynomial and their characterizations. The very classical orthogonal polynomials are those named after Jacobi, Laguerre, and Hermite. In this paper we will always be considering monic polynomials, but in the literature one often uses a different normalization. Jacobi polynomials are (monic) <b>polynomials</b> of <b>degree</b> n which are orthogonal to all <b>lower</b> <b>degree</b> <b>polynomials</b> with respect to the weight function (1 −x) α (1 +x) β on [− 1, 1], where α, β> − 1. The change of variables x ↦ → 2 x− 1 gives Jacobi polynomials on [0, 1] for the weight function w(x) = xβ (1 − x) α, and we will denote these (monic) polynomials by P (α,β) n (x). They are defined by the orthogonality condition...|$|R
40|$|A-splines are {{implicit}} real algebraic curves in Bernstein-B'ezier (BB) {{form that}} are smooth. We use these in an algorithm for active contouring of images. One advantage of A-splines {{is that any}} change to the controlling weights only affects the curve locally, which results in fast convergence. Our active A-splines are also level sets of a time-dependent function with the added flexibility of a dynamic unstructured mesh. Other advantages include {{the ability to use}} <b>lower</b> <b>degree</b> <b>polynomials</b> than traditional polynomial parametric B-splines. A-splines also avoid the necessity of dealing with poles that can arise from rational parametric B-splines (NURBS), and they also allow an easy specification of orientation, so that they may be driven to converge to an interior or exterior contour. Our algorithm finds image contours by using a level-set method to obtain an initial close fitting polygon, constructing a physical A-spline contour by minimizing the image energy, and then minimizing the tota [...] ...|$|R
40|$|A-splines are {{implicit}} real algebraic curves in Bernstein-Bezier (BB) {{form that}} are smooth. We use these in an algorithm for active contouring of images. One advantage of A-splines {{is that any}} change to the controlling weights only a ects the curve locally, which results in fast convergence. Our active A-splines are also level sets of a time-dependent function with the added exibility of a dynamic unstructured mesh. Other advantages include {{the ability to use}} <b>lower</b> <b>degree</b> <b>polynomials</b> than traditional polynomial parametric B-splines. A-splines also avoid the necessity of dealing with poles that can arise from rational parametric B-splines (NURBS), and they also allow an easy speci cation of orientation, so that they may be driven to converge to an interior or exterior contour. Our algorithm nds image contours by using a level-set method to obtain an initial close tting polygon, constructing a physical A-spline contour by minimizing the image energy, and then minimizing the total energy by considering the energy in each spline segment individually. ...|$|R
3000|$|... 5 Using {{the mean}} method when {{defining}} the educational norm leads to <b>a</b> <b>lower</b> <b>degree</b> of persistence for all groups. The mean method {{implies that the}} norm is an interval for years of schooling and not an absolute {{number of years of}} schooling.|$|R
