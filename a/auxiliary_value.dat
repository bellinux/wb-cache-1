13|78|Public
50|$|A user of RSA creates {{and then}} publishes a public key {{based on two}} large prime numbers, along with an <b>auxiliary</b> <b>value.</b> The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with {{currently}} published methods, if the public key is large enough, only someone {{with knowledge of the}} prime numbers can feasibly decode the message.Breaking RSA encryption is known as the RSA problem; whether it is as hard as the factoring problem remains an open question.|$|E
5000|$|The {{benchmarking}} procedure {{begins by}} first breaking the population into benchmarking cells. Cells are formed by grouping units together that share common characteristics, for example, similar , yet {{anything can be}} used that enhances {{the accuracy of the}} final estimates. For each cell , we let [...] be the sum of all , where the sum is taken over all sampled [...] in the cell [...] For each cell , we let [...] be the <b>auxiliary</b> <b>value</b> for cell , which is commonly called the [...] "benchmark target" [...] for cell [...] Next, we compute a benchmark factor [...] Then, we adjust all weights [...] by multiplying it by its benchmark factor , for its cell [...] The net result is that the estimated [...] by summing [...] will now equal the benchmark target total [...] But the more important benefit is that the estimate of the total of [...] by summing [...] will tend to be more accurate.|$|E
40|$|We {{present a}} novel method for solving {{a class of}} time-inconsistent optimal {{stopping}} problems by reducing them to a family of standard stochastic optimal control problems. In particular, we convert an optimal stopping problem with a non-linear function of the expected stopping time in the objective into optimization over an <b>auxiliary</b> <b>value</b> function for a standard stochastic control problem with an additional state variable. This approach differs from the previous literature which primarily employs Lagrange multiplier methods or relies on exact solutions. In contrast, we characterize the <b>auxiliary</b> <b>value</b> function as the unique viscosity solution of a non-linear elliptic PDE which satisfies certain growth constraints and investigate basic regularity properties. We demonstrate {{the construction of an}} optimal stopping time under additional regularity assumptions on the <b>auxiliary</b> <b>value</b> function. Finally, we discuss extensions to more general dynamics and time-inconsistencies, as well as potential connections to degenerate Monge-Ampere equations. Comment: Final version. To appear in SIAM Journal on Control and Optimization. Keywords: Non-linear optimal stopping, time-inconsistency, Hamilton-Jacobi-Bellman equation, sequential optimization, stochastic optimal contro...|$|E
40|$|Chambers and Dorfman (2002) {{constructed}} bootstrap {{confidence intervals}} in model based estimation for finite population totals assuming that <b>auxiliary</b> <b>values</b> are available throughout a target population {{and that the}} <b>auxiliary</b> <b>values</b> are independent. They also assumed that the cluster sizes are known throughout the target population. We now extend to two stage sampling in which the cluster sizes are known only for the sampled clusters, and we therefore predict the unobserved {{part of the population}} total. Jan and Elinor (2008) have done similar work, but unlike them, we use a general model, in which the <b>auxiliary</b> <b>values</b> are not necessarily independent. We demonstrate that the asymptotic properties of our proposed estimator and its coverage rates are better than those constructed under the model assisted local polynomial regression model. </p...|$|R
5000|$|Only rows k and ℓ {{and columns}} k and ℓ of A will be affected, and that A&prime; will remain symmetric. Also, an {{explicit}} matrix for Qkℓ is rarely computed; instead, <b>auxiliary</b> <b>values</b> are computed and A is updated in an efficient and numerically stable way. However, for reference, we may write the matrix as ...|$|R
5000|$|This {{algorithm}} typically requires computing [...] <b>auxiliary</b> <b>values</b> (where [...] is {{the number}} of state variables [...] ), and should only require reusing previously calculated values [...] An important factor in this since [...] is an integer value, then there is a minimum value by which it can change, preventing the relative change in [...] being bounded by 0, which would result in [...] also tending to 0.|$|R
40|$|International audienceWe study a {{class of}} Markovian optimal {{stochastic}} control problems in which the controlled process Z^ν is constrained to satisfy an a. s. constraint Z^ν(T) ∈ G⊂^d+ 1 at some final time T> 0. When the set is of the form G:={(x,y) ∈^d : g(x,y) > 0 }, with g non-decreasing in y, we provide a Hamilton-Jacobi-Bellman characterization of the associated value function. It {{gives rise to a}} state constraint problem where the constraint can be expressed in terms of an <b>auxiliary</b> <b>value</b> function w which characterizes the set D:={(t,Z^ν(t)) ∈ [0,T]^d+ 1 : Z^ν(T) ∈ G a. s. for some ν}. Contrary to standard state constraint problems, the domain D is not given a-priori and we do not need to impose conditions on its boundary. It is naturally incorporated in the <b>auxiliary</b> <b>value</b> function w which is itself a viscosity solution of a non-linear parabolic PDE. Applying ideas recently developed in Bouchard, Elie and Touzi (2008), our general result also allows to consider optimal control problems with moment constraints of the form g(Z^ν(T)) > 0 or g(Z^ν(T)) > 0 > p...|$|E
40|$|We {{consider}} the classical optimal dividends problem under the Cramér-Lundberg model with exponential claim sizes {{subject to a}} constraint {{on the time of}} ruin. We introduce the dual problem and show that the complementary slackness conditions are satisfied, thus there is no duality gap. Therefore the optimal value function can be obtained as the point-wise infimum of <b>auxiliary</b> <b>value</b> functions indexed by Lagrange multipliers. We also present a series of numerical examples...|$|E
40|$|We study a {{class of}} Markovian optimal {{stochastic}} control problems in which the controlled process Zν is constrained to satisfy an a. s. constraint Zν (T) ∈ G ⊂ Rd+ 1 P − a. s. at some final time T> 0. When the set is of the form G: = {(x, y) ∈ Rd × R: g(x, y) ≥ 0 }, with g non-decreasing in y, we provide a Hamilton-Jacobi-Bellman characterization of the associated value function. It {{gives rise to a}} state constraint problem where the constraint can be expressed in terms of an <b>auxiliary</b> <b>value</b> function w which characterizes the set D: = {(t, Zν (t)) ∈ [0, T] × Rd+ 1 : Zν (T) ∈ G a. s. for some ν}. Contrary to standard state constraint problems, the domain D is not given a-priori and we do not need to impose conditions on its boundary. It is naturally incorporated in the <b>auxiliary</b> <b>value</b> function w which is itself a viscosity solution of a non-linear parabolic PDE. Applying ideas recently developed in Bouchard, Elie and Touzi (2008), our general result also allows to consider optimal control problems with moment constraints of the form E [g(Z ν (T)) ] ≥ 0 or P [g(Z ν (T)) ≥ 0] ≥ p...|$|E
40|$|The {{well known}} Godambe-Joshi lower {{bound for the}} {{anticipated}} variance of design unbiased estimators of population totals treats the auxiliary variables as constants. We extend the result to models where these variables are random and show that the generalized difference estimator using the expected values conditional on all <b>auxiliary</b> <b>values</b> is optimal. This has several implications {{including the fact that}} collecting multiple survey variables does not reduce the lower bound...|$|R
3000|$|... (i[*]=[*] 1, 2, …, n), {{with the}} {{corresponding}} (measured in the first-phase) <b>auxiliary</b> variable <b>values</b> denoted as x [...]...|$|R
3000|$|... is unbounded. By Lemma 4.1, the <b>auxiliary</b> initial <b>value</b> problem (4.24), (1.6) has {{an escape}} {{solution}} for some [...]...|$|R
40|$|We {{introduce}} a longevity feature {{to the classical}} optimal dividend problem by adding a constraint {{on the time of}} ruin of the firm. We extend the results in HJ 15, now in context of one-sided Lévy risk models. We consider de Finetti's problem in both scenarios with and without fix transaction costs, e. g. taxes. We also study the constrained analog to the so called Dual model. To characterize the solution to the aforementioned models we introduce the dual problem and show that the complementary slackness conditions are satisfied and therefore there is no duality gap. As a consequence the optimal value function can be obtained as the pointwise infimum of <b>auxiliary</b> <b>value</b> functions indexed by Lagrange multipliers. Finally, we illustrate our findings with a series of numerical examples...|$|E
40|$|This work {{deals with}} the effect of {{nucleotide}} density vector's reduction on molecular identification of organisms. At {{the beginning of the}} theoretical part, the work explains the concept of species, its development and its mutations. The next section provides basics of taxonomy, particularly the molecular taxonomy and DNA barcoding, with a description of mitochondrial DNA and its usability in the identification of species. The end of the theoretical part provides information about the nucleotide density vectors which the practical part is focused on. Analysis of auxiliary values on nucleotide density vectors was accomplished in Matlab by evaluating 188 real DNA barcode sequences. The identification analysis of organism was performed with the best <b>auxiliary</b> <b>value</b> for 4 datasets of the reference barcode sequences and 5 datasets of analyzed sequences of same organisms. Afterwards, the evaluation of the analysis was done by using separate nucleotide density vectors of individual nucleotides and their amounts...|$|E
40|$|WO 2009053110 A 2 UPAB: 20090521 NOVELTY - The method {{involves}} associating {{the measuring}} {{values of the}} two sets to a class of {{a finite number of}} classes defined by indices such that a frequency distribution is defined for each of the two sets. The frequency distribution indicates for each class a frequency of the measurement values falling within the class. A distance measurement is calculated between the frequency distributions {{as a function of the}} final value of an <b>auxiliary</b> <b>value.</b> DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for the automated comparison of two sets of measuring values. USE - Method for the automated comparison of two sets of measuring values. ADVANTAGE - The method involves associating the measuring values of the two sets to a class of a finite number of classes defined by indices such that a frequency distribution is defined for each of the two sets, where the frequency distribution indicates for each class a frequency of the measurement values falling within the class, and thus a reliable and cost-effective measurement of the sets of the measuring values is ensured...|$|E
3000|$|... for {{the sample}} unit i by {{the sum of the}} <b>auxiliary</b> data <b>values</b> over the whole area of the {{probability}} layer (p [...]...|$|R
3000|$|To {{produce a}} {{simulated}} population data set {{for any one}} of the five examples, a set of 10, 000 <b>auxiliary</b> variable <b>values</b> were generated, based on the frequency distribution of that example. A set of 10, 000 corresponding target variable values were then generated by adding random normal deviates to these <b>auxiliary</b> <b>values</b> using a standard deviation chosen by trial and error to give a particular level of correlation between the target and auxiliary variables. For each of the five examples, nine such target variable data sets were generated with correlation levels chosen to be close to values of 0.1, 0.2, [...]..., 0.9. Scatter plots of target against auxiliary variables for the first 500 of the 10, 000 values generated for one of the examples for each of two correlation levels are shown in Fig.  1. As is evident there, the data were generated so that the target and auxiliary variables bore a straight-line relationship to each other that, over the whole population, passed through the origin. It was assumed also that the target-auxiliary variable relationship was homoscedastic as mentioned in the discussion following Eqs. (1) and (2).|$|R
40|$|Abstract – In this paper, {{a coordinated}} state-of-charge (SOC) control {{strategy}} for the energy storage system (ESS) operating under microgrid islanded mode to stabilize the frequency and voltage was proposed. The proposed SOC control loop {{is made up of}} PI controller, which uses a SOC state of the energy storage system as an input and an <b>auxiliary</b> reference <b>value</b> of secondary control as an output. The SOC controller changes the <b>auxiliary</b> reference <b>value</b> of secondary control to charge or discharge the ESS. To verify the proposed control strategy, PSCAD/EMTDC simulation study was performed. The simulation results show that the SOC of the ESS can be regulated at the desired operating range without degrading the stabilizing control performance by proposed coordinated SOC control method...|$|R
40|$|Abstract. We study {{stochastic}} motion planning {{problems which}} involve a controlled pro-cess, with possibly discontinuous sample paths, visiting certain subsets of the state-space while avoiding {{others in a}} sequential fashion. For this purpose, we first introduce two basic notions of motion planning, and then establish a connection to a class of stochastic optimal control problems concerned with sequential stopping times. A weak dynamic programming principle (DPP) is then proposed, which characterizes the set of initial states that admit a policy enabling the process to execute the desired maneuver with probability no less than some pre-specified value. The proposed DPP comprises <b>auxiliary</b> <b>value</b> functions {{defined in terms of}} discontinuous payoff functions. A concrete instance of the use of this novel DPP in the case of diffusion processes is also presented. In this case, we establish that the aforemen-tioned set of initial states can be characterized as the level set of a discontinuous viscosity solution to a sequence of partial differential equations, for which the first one has a known boundary condition, while the boundary conditions of the subsequent ones are determined by the solutions to the preceding steps. Finally, the generality and flexibility of the theoretical results are illustrated on an example involving biological switches. 1...|$|E
40|$|RSA is an {{algorithm}} for {{public-key cryptography}} {{that is based}} on the presumed difficulty of factoring large integers, the factoring problem. RSA stands for Ron Rivest, Adi Shamir and Leonard, who first publicly described it in 1978. A user of RSA creates and then publishes the product of two large prime numbers, along with an <b>auxiliary</b> <b>value,</b> as their public key. The prime factors must be kept secret. In RSA if one can factor modulus into its prime numbers then the private key is also detected and hence the security of the cryptosystem is broken. The Subset-Sum cryptosystem (Knapsack Cryptosystem) is also an asymmetric cryptographic technique. The Merkle-Hellman system is based on the subset sum problem (a special case of the knapsack problem) : An instance of the Subset Sum problem is a pair (S, t), where S = {x 1, x 2, [...] ., xn} is a set of positive integers and t (the target) is a positive integer. The decision problem asks for a subset of S whose sum is as large as possible, but not larger than t. This problem is NP-complete. However, if the set of numbers (called the knapsack) is super increasing, that is, each element of the set is greater than the sum of all the numbers before it; the problem is easy and solvable in polynomial time with a simple greedy algorithm. So in this paper we present a new algorithm (Modified Subset-Sum cryptosystem over RSA) which is secure against Mathematical attack, Brute-force attack, Factorization attack and Chosen-cipher-text attack on RSA as well as Shamir attacks. This paper also presents comparison between Modified Subset - Sum Cryptosystem and RSA cryptosystems in respect of security and performance...|$|E
3000|$|... [...]. The {{construction}} {{is done by}} using solutions of simple <b>auxiliary</b> elliptic boundary <b>value</b> problems and the eigenfunction of the [...]...|$|R
40|$|This article {{describes}} a very high-level language for clear description of distributed algorithms and optimizations necessary for generating efficient implementations. The language supports high-level control flows where complex synchronization conditions {{can be expressed}} using high-level queries, especially logic quantifications, over message history sequences. Unfortunately, the programs would be extremely inefficient, including consuming unbounded memory, if executed straightforwardly. We present new optimizations that automatically transform complex synchronization conditions into incremental updates of necessary <b>auxiliary</b> <b>values</b> as messages are sent and received. The core of the optimizations is the first general method for efficient implementation of logic quantifications. We have developed an operational semantics of the language, implemented a prototype of the compiler and the optimizations, and successfully used the language and implementation {{on a variety of}} important distributed algorithms...|$|R
3000|$|Furthermore, we {{also tested}} if {{the use of}} ALS data as {{auxiliary}} information would improve {{the accuracy of the}} stand level inventory by applying PPS sampling. The basic idea of this approach is to use the ALS metric to guide the selection of field plot locations (Pesonen et al. 2010 a, b). We applied the same number of sample plots {{as in the case of}} systematic and random sampling, but sampling probabilities varied according to ALS information. This was done to choose the most promising plot locations for aspen plots. First, probability layers were produced, i.e., the <b>auxiliary</b> data <b>values</b> were directly calculated for the whole stand that was divided into a grid of 20 m[*]×[*] 20 m sample units. When applying PPS sampling, the sample units were square and are referred to as grid cells. ALS based <b>auxiliary</b> data <b>values</b> were calculated for each sample unit (i[*]=[*] 1,…, N [...]...|$|R
40|$|We {{consider}} a probability proportional to size sampling scheme by using harmonic mean of an auxiliary variable, when {{the correlation between}} study variable and auxiliary variable is highly negative. This is achieved by considering inverse transformation of the auxiliary variable and then utilizing the transformed <b>auxiliary</b> variable <b>values</b> {{at the design stage}} of the survey operation in selecting a sample...|$|R
40|$|The {{standard}} enthalpies of {{solution of}} La(OH),NO, H,O and La(OH),NO, in HNO,(aq) have been measured calorimetrically. These results, {{when combined with}} <b>auxiliary</b> thermochemical <b>values,</b> yielded the standard enthalpies of formation of these lanthanum hydroxynitrates. The values obtained for A,Hm(cr, 298. 15 K) /(kJ’mol-‘) are La(OH),NO, H,O. -(1676. 6 * 1. 4); and La(OH),NO,. -(1390. 9 * 1. 5). Peer reviewe...|$|R
40|$|Object {{queries are}} {{essential}} in information seeking {{and decision making}} in vast areas of applications. However, a query may involve complex conditions on objects and sets, which can be arbitrarily nested and aliased. The objects and sets involved {{as well as the}} demand [...] -the given parameter values of interest [...] -can change arbitrarily. How to implement object queries efficiently under all possible updates, and furthermore to provide complexity guarantees? This paper describes an automatic method. The method allows powerful queries to be written completely declaratively. It transforms demand as well as all objects and sets into relations. Most importantly, it defines invariants for not only the query results, but also all <b>auxiliary</b> <b>values</b> about the objects and sets involved, including those for propagating demand, and incrementally maintains all of them. Implementation and experiments with problems from a variety of application areas, including distributed algorithms and probabilistic queries, confirm the analyzed complexities, trade-offs, and significant improvements over prior work...|$|R
40|$|Master of ScienceDepartment of MathematicsNathan AlbinA new {{methodology}} {{is introduced}} {{for use in}} discrete periodic extension of non-periodic functions. The methodology {{is based on a}} band-limited step function, and utilizes the computational efficiency of FC-Gram (Fourier Continuation based on orthonormal Gram polynomial basis on the extension stage) extension database. The discrete periodic extension is a technique for augmenting a set of uniformly-spaced samples of a smooth function with <b>auxiliary</b> <b>values</b> in an extension region. If a suitable extension is constructed, the interpolating trigonometric polynomial found via an FFT(Fast Fourier Transform) will accurately approximate the original function in its original interval. The discrete periodic extension is a key construction in the FC-Gram algorithm which is successfully implemented in several recent efficient and high-order PDEs solvers. This thesis focuses on a new flexible discrete periodic extension procedure that performs at least as well as the FC-Gram method, but with somewhat simpler implementation and significantly decreased setup time...|$|R
40|$|Abstract. We {{investigate}} the exponential turnpike property for finite horizon undiscounted discrete time optimal control problems without any terminal constraints. Considering {{a class of}} strictly dissipative systems we derive a boundedness condition for an <b>auxiliary</b> optimal <b>value</b> function which implies the exponential turnpike property. Two theorems illustrate how this boundedness condition can be concluded from structural properties like controllability and stabilizability of the control system under consideration...|$|R
40|$|Abstract. Classical {{boundary}} integral equations of {{the harmonic}} potential theory on Lip-schitz surfaces are studied. We obtain higher fractional Sobolev regularity results for their solutions under weak {{conditions on the}} surface. These results are derived from a theorem on the solvability of <b>auxiliary</b> boundary <b>value</b> problems for the Laplace equation in weighted Sobolev spaces. We show that classes of domains under consideration are optimal...|$|R
40|$|An {{approach}} to proportion estimation {{based on the}} notion of a mixture model, appropriate parametric forms for a mixture model that appears to fit observed remotely sensed data, methods for estimating the parameters in these models, methods for labelling proportion determination from the mixture model, and methods which use the mixture model estimates as <b>auxiliary</b> variable <b>values</b> in some proportion estimation schemes are addressed...|$|R
50|$|Linguistic {{properties}} {{can include}} such variables {{as the number}} of words in the problem or the mean sentence length. The logico-mathematical properties can be classified in numerous ways, but one such scheme is to classify the quantities in the problem (assuming the word problem is primarily numerical) into known quantities (the values given in the text of the problem), wanted quantities (the values that need to be found) and <b>auxiliary</b> quantities (<b>values</b> that may need to be found as intermediate stages of the problem).|$|R
50|$|The English officer William Miller, {{who served}} in Wellington's army during his {{campaign}} in Spain and then in South America, said that the montoneras in Peru served an invaluable function as an <b>auxiliary</b> force. Their <b>value</b> {{was similar to that}} of guerillas in the Peninsular War.|$|R
3000|$|..., respectively) in {{circumstances}} that assumed the <b>auxiliary</b> variable <b>value</b> had been measured on all sampling {{units in the}} population. As in the present work, they found that bias in estimates of the population mean was small (less than[*]±[*] 0.5 %) {{with any of the}} estimators. However, they found that estimates of the standard error of estimates of the population mean were often appreciably smaller (as much as 85 %) with model-assisted estimation. They did not state specifically if their target-auxiliary variable relationships passed through the origin and the advantages they found with model-assisted estimation may reflect that issue.|$|R
30|$|Three {{different}} second-phase {{sample sizes}} were considered {{because it can}} be expected that the precision of estimate of the population mean will increase as the sample size increases, that is, its standard error will decrease. Three arbitrarily chosen second-phase sample sizes (n) were tested, 10, 40 and 100. For estimators (4.1 – 8.1), the first-phase ‘sample’ included the <b>auxiliary</b> variable <b>values</b> of the entire 10, 000 sampling units of a population. For estimators (4.2 – 8.2), the corresponding first-phase sample sizes (f) were 25, 95 and 240, respectively, for examples 1 and 3 – 5 and 30, 290, 120 respectively for example 2; experience by the author with these examples had suggested that these might be appropriate first-phase sample sizes. In normal use of QPPS (Eqs. 8.1 and 8.2), the second-phase sampling process employed makes it impossible to ensure a particular second-phase sample size is achieved. However, in these simulations the <b>auxiliary</b> variable <b>values</b> were in fact known in all of the sampling units in the population. This meant it was possible to select a given number of sampling units that conformed to the selection criteria for a second-phase sample for QPPS sampling and a given number that did not conform to those criteria to complete both first- and second-phase samples of the required size; without this constraint, it would have been impossible to compare the QPPS estimators properly with the others because their first-phase sample sizes would have differed from that of the others.|$|R
40|$|We study {{a second}} order {{nonlinear}} system {{driven by the}} vector p-Laplacian, with a multivalued nonlinearity and defined on the positive time semi-axis ℝ+. Using degree theoretic techniques we solve an <b>auxiliary</b> mixed boundary <b>value</b> problem defined on the finite interval [0, n] and then via a diagonalization method we produce a solution for the original infinite time horizon system. ©Canadian Mathematical Society 2008...|$|R
40|$|Sampling {{functions}} in Gaussian process (GP) models is challenging {{because of the}} highly correlated posterior distribution. We describe an efficient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are <b>auxiliary</b> function <b>values</b> that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by continuously minimizing an objective function. We demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation. ...|$|R
