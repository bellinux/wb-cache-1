1031|142|Public
5|$|Ulam's work on non-Euclidean {{distance}} metrics in {{the context}} of molecular biology made a significant contribution to sequence analysis and his contributions in theoretical biology are considered watersheds in the development of cellular <b>automata</b> <b>theory,</b> population biology, pattern recognition, and biometrics generally. Colleagues noted that some of his greatest contributions were in clearly identifying problems to be solved and general techniques for solving them.|$|E
25|$|Kohavi, Z., Switching and Finite <b>Automata</b> <b>Theory.</b> McGraw-Hill, 1978.|$|E
25|$|For a more {{elementary}} {{introduction of}} the formal definition see <b>automata</b> <b>theory.</b>|$|E
25|$|Freeman Dyson {{expanded}} upon Neumann's <b>automata</b> <b>theories,</b> {{and advanced}} a biotechnology-inspired theory. See Astrochicken.|$|R
40|$|Abstract. The paper {{presents}} a new teaching conception for distance-learning based on using of so called living pictures. The field considered covers computer engineering, switching and <b>automata</b> <b>theories,</b> and more specifically, design and test of digital circuits and systems. A {{set of tools}} (interactive modules) is offered which support different stages of the learning process: class teaching, individual home training, self-testing, and examination as well. Key words. Distance-learning, Applets, Automata decomposition, Register-transfer level, test 1...|$|R
50|$|Astrochicken is {{the name}} given to a thought {{experiment}} expounded by theoretical physicist Freeman Dyson. In his book Disturbing the Universe (1979), Dyson contemplated how humanity could build a small, self-replicating automaton that could explore space more efficiently than a manned craft could. He attributed the general idea to John von Neumann, based on a lecture von Neumann gave in 1948 titled The General and Logical <b>Theory</b> of <b>Automata.</b> Dyson expanded on von Neumann's <b>automata</b> <b>theories</b> and added a biological component to them.|$|R
25|$|Kohavi, Zvi (1978), Switching and Finite <b>Automata</b> <b>Theory,</b> 1st edition, McGraw–Hill, 1970. 2nd edition, McGraw–Hill, 1978.|$|E
25|$|Methods {{based on}} Courcelle's theorem {{have also been}} applied to {{database}} theory, knowledge representation and reasoning, <b>automata</b> <b>theory,</b> and model checking.|$|E
25|$|<b>Automata</b> <b>Theory</b> is {{the study}} of self-operating virtual {{machines}} to help in logical understanding of input and output process, without or with intermediate stage(s) of computation (or any function / process).|$|E
40|$|Trust without {{control is}} a {{precarious}} solution to human nature. This belief has lead to many ways for guaranteeing secure software such as statically analyzing programs to check that they comply to the intended specifications which results in software certification. One problem with {{this approach is that}} the current systems can only accept all or nothing without knowing what the software is doing. Another way to complement is by run-time monitoring such that programs are checked during execution that they comply to security policy defined by the systems. The problem with this approach is the significant overhead which may not be desirable for some applications. This thesis describes a formalism, called <b>Automata</b> Modulo <b>Theory,</b> that allows us to have model of what programs do in more precise details thus giving semantics to certification. <b>Automata</b> Modulo <b>Theory</b> allows us to define very expressive policies with infinite cases while keeping the task of matching computationally tractable. This representation is suitable for formalizing systems with finitely many states but infinitely many transitions. <b>Automata</b> Modulo <b>Theory</b> consists of a formal model, two algorithms for matching the claims on the security behavior of a midlet (for short contract) with the desired security behavior of a platform (for short policy), and an algorithm for optimizing policy. The prototype implementations of <b>Automata</b> Modulo <b>Theory</b> matching using language inclusion and simulation have been built, and the results from our experience with the prototype implementations are also evaluated in this thesis...|$|R
40|$|Abstract. A set {{of tools}} (“interactive modules”) {{targeted}} to e-learning is presented for teaching logic level test generation and fault diagnosis in digital circuits. The tools support different university courses on computer engineering, switching and <b>automata</b> <b>theories,</b> digital electronics and design for testability to learn by hands-on excercises test and fault diagnosis related topics. A big reservoir of examples and the possibility to set up interesting engineering problems like how to generate test patterns for a digital circuit, or how to locate a faulty gate makes the learning process more interesting and allows learning at an individual depth and duration. The interactive modules are focused on easy action and reaction, multilingual descriptions, learning by doing...|$|R
40|$|The paper {{presents}} a new teaching concept based on using “living pictures ” supporting {{the learning process}} {{by the possibility of}} distance learning as well as a web-based computer-aided teaching. A set of tools (“interactive modules”) is offered to support this concept in teaching digital test and diagnosis related topics in the university courses of computer engineering, switching and <b>automata</b> <b>theories.</b> A big reservoir of examples and the possibility to generate own ones makes the learning process more interesting and allows learning at an individual depth and duration. The interactive modules are focused on correct solutions, easy action and reaction, multilingual descriptions, learning by doing, a game-like use, and fostering students in critical thinking, problem solving skills and creativity...|$|R
25|$|In <b>automata</b> <b>theory,</b> a {{deterministic}} pushdown automaton (DPDA or DPA) is {{a variation}} of the pushdown automaton. The class of deterministic pushdown automata accepts the deterministic context-free languages, a proper subset of context-free languages.|$|E
25|$|Formal {{methods are}} best {{described}} as {{the application of a}} fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, <b>automata</b> <b>theory,</b> and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.|$|E
25|$|In {{addition}} to their use in modeling reactive systems presented here, finite state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, and logic. Finite state machines are a class of automata studied in <b>automata</b> <b>theory</b> and the theory of computation.|$|E
40|$|The {{traditional}} {{realm of}} formal methods is off-line verification of formal properties of hardware and software. We report {{a different approach}} that uses formal methods (namely the integration of <b>automata</b> modulo <b>theory</b> with decision procedures) on-the-fly, at the time an application is downloaded on a mobile application such as PDA or a smart phone. The idea behind security-by-contract is that a mobile applications comes equipped with a signed contract describing the security relevant behavior of the application and such contract should be matched against the mobile platform policy. Both specified as <b>automata</b> modulo <b>theories</b> and the operation is an on-the-fly emptiness test where edges are not only finite states of labels, but rather expressions which capture infinite transitions such as “connect only to urls starting wit...|$|R
40|$|One way to {{understand}} an interactive system is firmly rooted in language theory, that a system is its set of runs (or words). Properties of systems are described in a linear time temporal logic. Relationships between <b>automata,</b> language <b>theory</b> and logic are then utilised, such as the theory of ω-regular languages and Büch...|$|R
40|$|AbstractA {{cellular}} automaton is a continuous function F defined on a full-shift AZ which commutes with the shift σ. Often, {{to study the}} dynamics of F one only considers implicitly σ. However, {{it is possible to}} emphasize the spatio-temporal structure produced by considering the dynamics of the Z×N-action induced by (σ,F). In this purpose we study the notion of directional dynamics. In particular, we are interested in directions of equicontinuity and expansivity, which generalize the concepts introduced by Gilman [Robert H. Gilman, Classes of linear <b>automata,</b> Ergodic <b>Theory</b> Dynam. Systems 7 (1) (1987) 105 – 118] and P. Kůrka [Petr Kůrka, Languages, equicontinuity and attractors in cellular <b>automata,</b> Ergodic <b>Theory</b> Dynam. Systems 17 (2) (1997) 417 – 433]. We study the sets of directions which exhibit this special kind of dynamics showing that they induce a discrete geometry in space-time diagrams...|$|R
25|$|Taylor Booth, Sequential Machines and <b>Automata</b> <b>Theory,</b> Wiley, New York, 1967. Cf. Chapter 9, Turing Machines. Difficult book, {{meant for}} {{electrical}} engineers and technical specialists. Discusses recursion, partial-recursion {{with reference to}} Turing Machines, halting problem. Has a Turing Machine model in it. References at end of Chapter 9 catch most of the older books (i.e. 1952 until 1967 including authors Martin Davis, F. C. Hennie, H. Hermes, S. C. Kleene, M. Minsky, T. Rado) and various technical papers. See note under Busy-Beaver Programs.|$|E
25|$|Theoretical {{computer}} science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical {{computer science}} {{is the study}} of algorithms for computing mathematical results. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time taken by computations. <b>Automata</b> <b>theory</b> and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.|$|E
25|$|The Chomsky hierarchy, {{sometimes}} referred to as the Chomsky-Schützenberger hierarchy, is a containment hierarchy of classes of formal grammars. The hierarchy imposes a logical structure across different language classes and provides a basis for understanding the relationship between grammars (devices that enumerate the valid sentences within languages). In order of increasing expressive power it includes regular (or Type-3) grammars, context-free (or Type-2) grammars, context-sensitive (or Type-1) grammars, and recursively enumerable (or Type-0) grammars. Each class is a strict subset of the class above it, i.e., each successive class can generate a broader set of formal languages (infinite sets of strings composed from finite sets of symbols, or alphabets) than the one below. In addition to being important in linguistics, the Chomsky hierarchy is also relevant in theoretical computer science, especially in programming language theory, compiler construction, and <b>automata</b> <b>theory.</b>|$|E
40|$|Several {{attempts}} have been made to find a suitable model for parallel processing. Cellular automata [9], Lindenmayer systems [18], systolic trellis automata [3], Russian parallel [4] and Indian parallel [4] grammars are some examples of such models based on formal language and <b>automata</b> <b>theories.</b> In these devices the parallelism is local. Symbols are rewritten independently of each other. No major cooperation between the parallel processes occurs, although, for instance, in L-systems with interactions some minor cooperation appears. However, the development of massively parallel processing systems increased the importance of interprocessor communication in the new generation computer design. Communication plays a major role in parallel processing architectures, where inappropriate interconnection topologies could lengthen the paths of messages, reduce the system reliability and introduce most embarassing performance limitations. Cooperating /distributed grammar systems are an attempt of modelling th...|$|R
40|$|Concepts of {{reduction}} and minimization are formulated {{in a general}} setting inorder to unify and classify the different constructions in <b>automata</b> and system <b>theory.</b> In addition to a unified theory for deterministic, partial, linear and topological automata, which is partly known, a common theory for nondeterministic, relational, stochastic and relation topological automata is developed using the notion of automata in pseudoclosed categories. Moreover, an approach to a general theory of {{reduction and}} minimization is given and applied to {{a great number of}} examples in <b>automata</b> and system <b>theory...</b>|$|R
40|$|This paper {{introduces}} {{the theory of}} twodimensional languages automata in comparision to the classic concept of languages and <b>automata.</b> This <b>theory</b> is used for developing special automata, {{that is capable of}} analyzing tables containing rules. These tables are conected with rules for selected european car license plates. Automata and tables are used by developed application, which can be used for car nationality identification...|$|R
25|$|Also in 1969 {{computer}} scientist Alvy Ray Smith completed a Stanford PhD dissertation on Cellular <b>Automata</b> <b>Theory,</b> the first mathematical treatment of CA {{as a general}} class of computers. Many papers came from this dissertation: He showed the equivalence of neighborhoods of various shapes, how to reduce a Moore to a von Neumann neighborhood or how to reduce any neighborhood to a von Neumann neighborhood. He proved that two-dimensional CA are computation universal, introduced 1-dimensional CA, and showed that they too are computation universal, even with simple neighborhoods. He showed how to subsume the complex von Neumann proof of construction universality (and hence self-reproducing machines) into a consequence of computation universality in a 1-dimensional CA. Intended as {{the introduction to the}} German edition of von Neumann's book on CA, he wrote a survey of the field with dozens of references to papers, by many authors in many countries over a decade or so of work, often overlooked by modern CA researchers.|$|E
500|$|It {{was first}} shown by [...] {{that the problem}} of testing {{reversibility}} of a given one-dimensional cellular automaton has an algorithmic solution. Alternative algorithms based on <b>automata</b> <b>theory</b> and de Bruijn graphs were given by [...] and , respectively.|$|E
500|$|Elements of the Theory of Computation (1981, with Christos H. Papadimitriou) covers <b>automata</b> <b>theory,</b> {{computational}} complexity theory, and {{the theory}} of formal languages; its inclusion of complexity theory and mathematical logic was innovative for its time. It has been called an [...] "excellent traditional text" [...] but one whose terse and heavily mathematical style can be intimidating. Although intended for undergraduates, {{it has also been}} used for introductory graduate courses.|$|E
50|$|Sheila Adele Greibach (born 6 October 1939 in New York City) is a {{researcher}} in formal languages in computing, <b>automata,</b> compiler <b>theory</b> (in particular), and computer science. She is an Emeritus Professor of Computer Science at the University of California, Los Angeles, {{and has worked}} with Seymour Ginsburg and Michael A. Harrison in context-sensitive parsing using the stack automaton model.|$|R
40|$|We {{present some}} results toward {{advanced}} algorithms which {{were carried out}} in our laboratoryand in the Advanced Algorithms Research Laboratory at the University of Electro-Communications. This paper consists of two parts. Part I deals with our historical and recent results on <b>automata</b> andlanguage <b>theory</b> together with algorithmic learning theory. Part II deals with mainly our recentresults on the maximum clique problem and its applications...|$|R
40|$|Abstract. In {{this paper}} {{a way to}} have {{structures}} with partiality in its internal structure in a categorical approach is presented and, with this, a category of partial graphs Grp is given and partial automata are constructed from Grp. With a simple categorical operation, computations of partial automata are given and {{can be seen as}} a part of the structure of partial automata. Keyworks: computation, partial <b>automata,</b> Category <b>Theory...</b>|$|R
2500|$|In <b>automata</b> <b>theory,</b> {{a finite}} state machine is called a {{deterministic}} finite automaton (DFA), if ...|$|E
2500|$|John E. Hopcroft and Jeffrey D. Ullman, Introduction to <b>Automata</b> <b>Theory,</b> Languages, and Computation, Addison-Wesley Publishing, Reading Massachusetts, 1979[...] (See chapter 2.) ...|$|E
2500|$|John Hopcroft, Jeffrey Ullman (1979). Introduction to <b>Automata</b> <b>Theory,</b> Languages and Computation, 1st ed., Reading Mass: Addison-Wesley[...] [...] A {{difficult}} book {{centered around}} the issues of machine-interpretation of [...] "languages", NP-Completeness, etc.|$|E
40|$|Abstract: Source code {{plagiarism}} is {{very common}} among undergraduate computer science students {{and a lot of}} research has been carried out on how it can be detected, penalised, controlled or even stopped. In this paper, we propose a new approach to the detection of possibly plagiarised programs written in C++ using deterministic finite automaton (DFA) abstractions. The two programs to be checked for similarity are first normalised, granulated and abstracted to a DFA structure referred to as Single Program Deterministic Finite Automata or SPDFA. Then a newly proposed algorithm is used to map the alphabets of the two SPDFAs. If there is a one-to-one mapping of the symbols in both alphabets, we conclude that the programs are totally similar. We have also presented a prototype software application called the Exact Code Matcher that implements this technique as a proof of concept. This detection technique is a new application of finite <b>automata</b> <b>theories,</b> it is efficient for cloned or lexically altered programs, precise with no false positives and portable across different platforms...|$|R
5000|$|Hopcroft is {{also the}} co-recipient (with Jeffrey Ullman) of the 2010 IEEE John von Neumann Medal“for laying the {{foundations}} for the fields of <b>automata</b> and language <b>theory</b> and many seminal contributions to theoretical computer science.” ...|$|R
30|$|Adaptive optimization. CWNs {{offer the}} ability to {{adaptive}} optimize against the varying wireless environmental conditions using AI-based techniques. CWN typically utilizes adaptive frameworks such as reinforcement learning, learning <b>automata,</b> game <b>theory,</b> etc., for adaptively optimizing {{the parameters of the}} network. There are numerous optimization-based applications of CWNs including dynamic spectrum access[144], parameter optimization[145, 188], optimized MAC[146] and routing[61, 189], enhanced reliability[148] and security[149, 189], QoS assurance and management[190, 191], channel assignment[192], etc.|$|R
