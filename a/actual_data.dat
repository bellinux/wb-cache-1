4591|5012|Public
25|$|The CDC 6600 {{series of}} {{computers}} were very early attempts at supercomputing and gained their {{advantage over the}} existing systems by relegating work to peripheral devices, freeing the CPU (Central Processing Unit) to process <b>actual</b> <b>data.</b> With the Minnesota FORTRAN compiler the 6600 could sustain 500kiloflops on standard mathematical operations.|$|E
25|$|Current {{versions}} of Btrieve support system transactions and user transactions, where system transactions are {{a bundle of}} non-transactional operations and/or user transactions, whereas user transactions are transactions that work on <b>actual</b> <b>data</b> in the database. System transactions were developed to allow multiple transactions {{to be done in}} a batch and to make data recovery easier.|$|E
25|$|Estimates {{of total}} costs range from 7 (for a 200 MW plant) and 21 (for a 5 MW plant) euro cents per kWh to 25–35 cents per kWh. Levelized cost are {{approximately}} 3 Euro cents per KWh for a 100 MW wind or natural gas plant. No <b>actual</b> <b>data</b> are available for a utility scale power plant.|$|E
5000|$|IT Resource: stores <b>actual</b> {{connection}} <b>data.</b> (Password {{is always}} encrypted.) ...|$|R
5000|$|<b>Actual</b> astronomical <b>data</b> is used {{rather than}} only {{theoretical}} mathematical calculations.|$|R
5000|$|Autocomplete (instant search as you type) {{using the}} <b>actual</b> index <b>data</b> ...|$|R
25|$|In physics or {{engineering}} education, a Fermi problem, Fermi quiz, Fermi question, Fermi estimate, {{or order}} estimation is an estimation problem {{designed to teach}} dimensional analysis, approximation, and such a problem is usually a back-of-the-envelope calculation. The estimation technique is named after physicist Enrico Fermi as he {{was known for his}} ability to make good approximate calculations with little or no <b>actual</b> <b>data.</b> Fermi problems typically involve making justified guesses about quantities and their variance or lower and upper bounds.|$|E
25|$|Digital Guardian allows {{businesses}} to host and manage on premise or choose managed security programs. When deployed as a managed service, Digital Guardian does not collect the <b>actual</b> <b>data</b> itself. Rather, it aggregates the metadata about the files and documents and watches for patterns of activity. Metadata is encrypted, hashed and digitally signed before being transferred to Digital Guardian's hosting facilities via FIPS 140-2 certified messaging protocol. Digital Guardian then provides updated analytics, alerts and reports. Administrators can continuously monitor data, application and system access and usage, whether end users are online, offline or in virtual environments. Organizations can apply specific risk-based policy controls {{to adhere to}} data governance and compliance rules.|$|E
25|$|The <b>actual</b> <b>data</b> mining task is the {{semi-automatic}} or automatic {{analysis of}} {{large quantities of}} data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then {{be seen as a}} kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.|$|E
5000|$|A back testing {{function}} allowing indicator {{performance tests}} using <b>actual</b> trading <b>data</b> ...|$|R
5000|$|... #Caption: Graph 1. <b>Actual</b> BSA <b>data</b> {{attained}} from a {{micro scale}} UV-Vis Spectrophotometer ...|$|R
5000|$|Observibility: the {{concepts}} occur with relatively high frequency in <b>actual</b> video <b>data</b> sets ...|$|R
25|$|Before the {{publication}} of Dogs That Know When Their Owners Are Coming Home, Sheldrake invited Richard Wiseman, Matthew Smith, and Julie Milton to conduct an independent experimental study with the dog Jaytee. They concluded that their evidence did not support telepathy as {{an explanation for the}} dog's behaviour, and proposed possible alternative explanations for Sheldrake's conclusions, involving artefacts, bias resulting from experimental design, and post hoc analysis of unpublished data. The group observed that Sheldrake's observed patterns could easily arise if a dog were simply to do very little for a while, before visiting a window with increasing frequency the longer that its owner was absent, and that such behaviour would make sense for a dog awaiting its owner's return. Under this behaviour, the final measurement period, ending with the owner's return, would always contain the most time spent at the window. Sheldrake argued that the <b>actual</b> <b>data</b> in his own and in Wiseman's tests did not bear this out, and that the dog went to wait at the window sooner when his owner was returning from a short absence, and later after a long absence, with no tendency for Jaytee to go to the window early in the way that he did for shorter absences.|$|E
25|$|ELINT {{operations}} {{aimed at}} China: After China tested its first nuclear weapons on 16 October 1964, at Lop Nur, Xinjiang, India and the USA {{shared a common}} fear about the nuclear capabilities of China. Owing to the extreme remoteness of Chinese testing grounds, strict secrecy surrounding the Chinese nuclear programme, and the extreme difficulty that an Indian or American would have passing themselves off as Chinese, {{it was almost impossible}} to carry out any HUMINT operation. So, the CIA in the late 1960s decided to launch an ELINT operation along with RAW and ARC to track China's nuclear tests and monitor its missile launches. The operation, in the garb of a mountaineering expedition to Nanda Devi involved celebrated Indian climber M S Kohli who along with operatives of Special Frontier Force and the CIA – most notably Jim Rhyne, a veteran STOL pilot – was to place a permanent ELINT device, a transceiver powered by a plutonium battery, that could detect and report data on future nuclear tests carried out by China. The monitoring device was near successfully implanted on Nanda Devi, when an avalanche forced a hasty withdrawal. Later, a subsequent mountain operation to retrieve or replant the device was aborted when {{it was found that the}} device was lost. Recent reports indicate that radiation traces from this device have been discovered in sediment below the mountain. However, the <b>actual</b> <b>data</b> is not conclusive.|$|E
500|$|Instances {{of value}} types {{do not have}} {{referential}} identity nor referential comparison semantics - equality and inequality comparisons for value types compare the <b>actual</b> <b>data</b> values within the instances, unless the corresponding operators are overloaded. Value types are derived from , always have a default value, and can always be created and copied. Some other limitations on value types are that they cannot derive from each other (but can implement interfaces) and cannot have an explicit default (parameterless) constructor. Examples of value types are all primitive types, such as [...] (a signed 32-bit integer), [...] (a 32-bit IEEE floating-point number), [...] (a 16-bit Unicode code unit), and [...] (identifies a specific point in time with nanosecond precision). Other examples are [...] (enumerations) and [...] (user defined structures).|$|E
50|$|Fault recorders, which record <b>actual</b> {{waveform}} <b>data</b> of {{the system}} primary voltages and currents.|$|R
30|$|SMART-PET {{produces}} images {{comparable to}} <b>actual</b> phantom <b>data.</b> The image characteristics of simulated and real PET images varied in similar ways as function of reconstruction protocols and noise levels. The change in image noise with variation of simulated TOF settings followed the theoretically expected behaviour. RC as function of sphere size agreed within 0.3 – 11 % between simulated and <b>actual</b> phantom <b>data.</b>|$|R
50|$|In 2011, InContext Solutions {{introduced}} Visualize InContext, {{a software}} tool that allows companies to view <b>actual</b> point-of-sales <b>data</b> within the virtual store environment. Through Visualize InContext, marketers and retailers {{can see in}} real time what is happening on the shelf, enabling them to tune their shelf restocking schedules and better optimize planograms for the most efficient product sell-through using <b>actual</b> point-of-sale <b>data.</b>|$|R
500|$|The SSEM's 32-bit {{word length}} was {{increased}} to 40 bits. Each word could hold either one 40-bit number or two 20-bit program instructions. The main store initially {{consisted of two}} double-density Williams tubes, each holding two arrays of 32 x 40-bit words– known as pages– backed up by a magnetic drum capable of storing an additional 32 pages. [...] The capacity was increased in the Final Specification version to eight pages of main store on four Williams tubes and 128 magnetic drum pages of backing store. The [...] diameter drum, initially known as a magnetic wheel, contained a series of parallel magnetic tracks around its surface, {{each with its own}} read/write head. Each track held 2,560bits, corresponding to twopages (2×32×40bits). One revolution of the drum took 30milliseconds, during which time both pages could be transferred to the CRT main memory, although the <b>actual</b> <b>data</b> transfer time depended on the latency, the time it took for a page to arrive under the read/write head. Writing pages to the drum took about twice as long as reading. The drum's rotational speed was synchronised to the main central processor clock, which allowed for additional drums to be added. Data was recorded onto the drum using a phase modulation technique still known today as Manchester coding.|$|E
2500|$|However, {{this issue}} is only {{relevant}} for model selection when the dimension of the data has been reduced. ABC-based inference, in which the <b>actual</b> <b>data</b> sets are directly compared—as {{is the case for}} some systems biology applications (e.g., see [...] )—circumvents this problem.|$|E
2500|$|Some hybrid {{solutions}} try {{to combine}} {{the advantages of the}} two representations. [...] Unrolled linked lists store several elements in each list node, increasing cache performance while decreasing memory overhead for references. CDR coding does both these as well, by replacing references with the <b>actual</b> <b>data</b> referenced, which extends off the end of the referencing record.|$|E
50|$|This is {{followed}} by 256 bytes of decompression table and finally the <b>actual</b> compressed <b>data.</b>|$|R
5000|$|BioAssayRetriever can {{automatically}} retrieve <b>actual</b> bioassay <b>data</b> and be {{merged into}} bulk download data files.|$|R
5000|$|You {{can see the}} {{visualisation}} (based on <b>actual</b> TLE <b>data)</b> of the Iridium Constellation at https://www.iridium.online ...|$|R
2500|$|... of m linear {{equations}} in n unknown coefficients, β1,β2,…,β'n, with m > n. (Note: for {{a linear}} model as above, {{not all of}} [...] contains information on the data points. The first column is populated with ones, , only the other columns contain <b>actual</b> <b>data,</b> and n = number of regressors + 1.) This can be written in matrix form as ...|$|E
2500|$|Some theoreticians have {{attempted}} to determine an optimal number of bins, but these methods generally make strong assumptions about {{the shape of the}} distribution. [...] Depending on the <b>actual</b> <b>data</b> distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.|$|E
2500|$|The [...] "connection speed" [...] of a V.92 {{voiceband}} modem typically {{refers to}} the gross bit rate, {{since there is no}} additional error-correction code. It can be up to 56,000bit/s downstreams and 48,000bit/s upstreams. A lower bit rate may be chosen during the connection establishment phase due to adaptive modulationslower but more robust modulation schemes are chosen in case of poor signal-to-noise ratio. Due to data compression, the <b>actual</b> <b>data</b> transmission rate or throughput (see below) may be higher.|$|E
50|$|The {{data are}} not {{subjected}} to peer review and the <b>actual</b> experimental <b>data</b> (raw or processed) should be provided.|$|R
30|$|And we use <b>actual</b> sales <b>data</b> of {{wood chip}} in 2011 to {{calculate}} increased demand by sales of wood chips.|$|R
50|$|A {{sequence}} of bytes representing the <b>actual</b> binary <b>data</b> {{for this type}} of value. All numbers are in big-endian order.|$|R
2500|$|Suppose b is a [...] "candidate" [...] {{value for}} the {{parameter}} vector β. The quantity , called the residual for the i-th observation, measures the vertical distance between the data point [...] and the hyperplane , and thus assesses the degree of fit between the <b>actual</b> <b>data</b> and the model. The sum of squared residuals (SSR) (also called the error sum of squares (ESS) or residual sum of squares (RSS)) {{is a measure of}} the overall model fit: ...|$|E
2500|$|In {{a review}} of Meyer's article The origin of {{biological}} information and the higher taxonomic categories, Alan Gishlick, Nick Matzke, and Wesley R. Elsberry claimed it contained poor scholarship, that it failed to cite and specifically rebut the <b>actual</b> <b>data</b> supporting evolution, and [...] "constructed a rhetorical edifice out of omission of relevant facts, selective quoting, bad analogies, knocking down straw men, and tendentious interpretations." [...] Further examination of the article revealed that it was substantially similar to previously published articles co-authored by Meyer.|$|E
2500|$|CLARREO {{could make}} highly {{accurate}} decadal change observations that are traceable to International Systems of Units (SI) standards. For example, at solar wavelengths this {{is intended to}} be confirmed after launch using comparison of <b>actual</b> <b>data</b> to theoretical simulations of lunar/solar radiance generated within a high-fidelity sensor model, although it is unclear how such a non-experimental approuch will ensure SI traceability. [...] The Earth observations then made by CLARREO have sensitivity to the most critical but least understood climate radiative forcings, responses, and feedbacks, such as: ...|$|E
5000|$|Optimization of the product's overall {{operations}} {{based on}} <b>actual</b> performance <b>data,</b> and reduction of downtimes through predictive maintenance and remote service.|$|R
40|$|In {{this paper}} we develop an econometric model of {{willingness}} to pay that integrates data on respondent uncertainty regarding their own {{willingness to pay}}. The integration is utility consistent and does not involve calibrating the contingent responses to <b>actual</b> payment <b>data,</b> and so the approach can "stand alone". In an application to a valuation study related to whooping crane restoration, we find that this model generates a statistically lower expected WTP than the standard CV model. Moreover, the WTP function estimated with this model is not statistically different from that estimated using <b>actual</b> payment <b>data,</b> suggesting that when properly analyzed using data on respondent uncertainty, contingent valuation decisions can simulate actual payment decisions. This method allows for more reliable estimates of WTP that incorporates respondent uncertainty {{without the need for}} collecting comparable <b>actual</b> payment <b>data.</b> ...|$|R
30|$|In future work, we plan {{to apply}} this method to <b>actual</b> sports <b>data.</b> An {{analysis}} of table tennis rallies is in progress.|$|R
