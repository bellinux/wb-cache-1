127|1|Public
40|$|Logic {{programs}} provide {{many opportunities}} for parallel execution. Among different forms of parallelism found in logic programs, <b>AND-parallelism</b> and OR-parallelism have shown {{to be most effective}} in speeding up the execution of logic programs. Research in the exploitation of <b>AND-parallelism,</b> OR-parallelism alone and combined AND/OR-parallelism has led to the proposals and implementations of various execution models and working systems. This paper offers a review of major activities in exploiting <b>AND-parallelism</b> and combined AND/ORparallelism. Keywords: Logic programming, Prolog, <b>AND-parallelism,</b> Combined AND/OR-parallelism 1. INTRODUCTION There has been a flurry of research activities in parallel processing of logic programs in the last decade due to the growing demand for fast reasoning capabilities on the current parallel computers. Among different forms of parallelism inherent in logic programs, <b>AND-parallelism</b> and ORparallelism have shown to be most effective in speeding up the [...] ...|$|E
40|$|One of the {{advantages}} of logic programming is the fact that one can exploit implicit parallelism in logic programs, such as <b>and-parallelism</b> and or-parallelism. Recently, research has been concentrated on integrating the different forms of parallelism into a single combined system. In this work we concentrate on the problem of integrating or-parallelism and independent <b>and-parallelism</b> for parallel Prolog systems. We contend that previous data structures require pure recomputation and therefore do not allow for orthogonality between <b>and-parallelism</b> and or-parallelism. In contrast, we submit that a simpler solution, the sparse binding array, does guarantee this goal, and explain in detail how independent <b>and-parallelism</b> and or-parallelism can thus be efficiently combined...|$|E
40|$|We {{consider}} {{the problem of}} exploiting non-deterministic dependent <b>and-parallelism</b> (DAP) from Prolog programs. The main issues that arise in designing a parallel Prolog system for exploiting DAP are discussed. Three criteria for efficient exploitation of DAP are also developed. These criteria are then used to evaluate and classify existing execution models proposed for exploiting DAP. These criteria also inspire new schemes for exploiting DAP that are arguably better than existing ones. Two such schemes for dependent <b>and-parallelism,</b> termed the Linked Model and the Filter Model are proposed. The Filter model has been incorporated in the ACE and-or parallel Prolog system running on a Sequent Symmetry and Sun Sparc Multiprocessors. Performance results (on the Symmetry) from this implementation are also presented. Keywords: Prolog, <b>And-parallelism,</b> Dependent <b>And-Parallelism.</b> 1 Introduction Logic programming is a popular computer programming paradigm {{that has been used}} in a wide variety o [...] ...|$|E
40|$|Parallelizing logic {{programming}} has attracted {{much interest in}} the research community, because of the intrinsic OR- and <b>AND-parallelisms</b> of logic programs. One research stream aims at transparent exploitation of parallelism in existing {{logic programming}} languages such as Prolog, whale the family of concurrent logic languages develops language constructs allowing programmers to express the concurrency—that is, the communication and synchronization between parallel processes—within their algorithms. This article concentrates mainly on transparent exploitation of parallelism and surveys the most mature {{solutions to the problems}} to be solved in order to obtain efficient implementations. These solutions have been implemented, and the most efficient parallel logic programming systems reach effective speedups over state-of-the-art sequential Prolog implementations. The article also addresses current and prospective research issues in extending the applicability and the efficiency of existing systems, such as models merging the transparent parallehsm and the concurrent logic languages approaches, combination of constraint logic programming with parallelism, and use of highly parallel architectures...|$|R
40|$|A {{backtracking}} algorithm for <b>AND-Parallelism</b> and {{its implementation}} at the Abstract Machine level are presented: first, {{a class of}} <b>AND-Parallelism</b> models based on goal independence is defined, and a generalized version of Restricted <b>AND-Parallelism</b> (RAP) introduced as characteristic of this class. A simple and efficient backtracking algorithm for R A P is then discussed. An implementation scheme is presented for this algorithm which offers minimum overhead, while retaining the performance and storage economy of sequent ial implementations and taking advantage of goal independence to avoid unnecessary backtracking ("restricted intelligent backtracking"). Finally, the implementation of backtracking in sequential and AND-Parallcl systems is explained {{through a number of}} examples...|$|E
40|$|Model [War 87 a] x x 16 Delphi [CA 88] x x 17 Randomize Method [JAM 88] x x 18 ORBIT [YN 84] ? ? ? 19 VMHW model [VXDRS 91] ? ? ? 20 VMBA model [VXDRS 91] ? ? ? Table 2. 1 : Time {{complexity}} of operations in OR-parallel methods [Gup 94]. A cross indicates constant time operation, a blank usually means linear time operation. IMPACT-NLI- 1998 - 3 12 2 Parallel Logic Programming IMPACT by proper selection of algorithms. For example, instead of computing the factorial by multiplying consecutive increasing numbers, {{it is also}} possible to use a divide and conquer technique. The latter will allow independent <b>AND-parallelism,</b> whereas the former will not allow any form of <b>AND-parallelism.</b> 2. 3. 1 Implementing Independent <b>AND-Parallelism</b> In order to exploit independent <b>AND-parallelism,</b> the system needs to implement at least three phases: the ordering phase, which deals with the detection of dependencies amongst subgoals, the forward execution phase, which deals with the selection and initiating execution of [...] ...|$|E
40|$|AbstractThis paper {{presents}} an extended and—or tree and an extended WAM (Warren Abstract Machine) for efficiently supporting both and-parallel and or-parallel execution of logic programs on shared-memory multiprocessors. Our approach for exploiting both and- and or-parallelism {{is based on}} the binding-arrays method for or-parallelism and the RAP (Restricted <b>And-Parallelism)</b> method for <b>and-parallelism,</b> two succesful methods for implementing or-parallelism and <b>and-parallelism,</b> respectively. Our combined and—or model avoids redundant computations when goals exhibit both and- and or-parallelism, by representing the cross product of the solutions from the and—or parallel goals rather than recomputing them. We extend the classical and—or tree with two new nodes: a “sequential” node (for RAPs sequencial goals), and a “cross-product” node (for the cross product of solutions from and—or parallel goals). The paper also {{presents an}} extension of the WAM, called AO—WAM, which is used to compile logic programs for and—or parallel execution based on the extended and—or tree. The AO—WAM incorporates a number of novel features: (i) inclusion of a base array with each processor's binding array for constant-time access to variables in the presence of <b>and-parallelism,</b> (ii) inclusion of new stack frames and instructions to express solution sharing, and (iii) novel optimizations which minimize the cost of binding-array updates in the presence of <b>and-parallelism...</b>|$|E
40|$|Parallelism {{and logic}} {{programming}} are two fields {{that have been}} successfully combined, as is shown by recent implementations of parallel logic programming systems. There are two main sources of parallelism in logic programming, namely or-parallelism and <b>and-parallelism.</b> Or-parallelism is exploited when several alternative clauses to a goal are executed in parallel. <b>And-parallelism</b> is exploited when we execute two or more goals of the same clause simultaneously. Exploitation of full or-parallelism and <b>and-parallelism</b> {{is limited by the}} number of physical processors available in a system. <b>And-parallelism</b> is also limited by the interdependence among goals in a clause. In parallel logic programming systems which exploit both <b>and-parallelism</b> and or-parallelism, a problem that arises is how to distribute processors between the dynamically varying amounts of and-work and or-work that are available. Solutions have been reported for distributing only or-work, or distributing only and-work, but the issue of distributing processors between both kinds of work has not yet been addressed. In this thesis we discuss the problem of distributing and-work and or-work in the context of Andorra-I, a parallel logic programming system that exploits determinate <b>and-parallelism</b> and or-parallelism, and propose scheduling strategies that aim at effi ciently distributing processors between and-work and or-work. We study general criteria that every scheduling strategy should meet to reconfi gure processors without incurring too high overheads in the Andorra-I system. We propose two different strategies to reconfi gure processors between and-work and or-work based on these criteria. One strategy, work-guided, guides its decisions by looking at the amount of current and-work and or-work available in [...] ...|$|E
40|$|This paper {{presents}} a new multiprocessor architecture for the parallel execution of logic programs, developed {{as part of}} the Aquarius Project. This architecture is designed to support <b>AND-parallelism,</b> OR-parallelism, and intelligent backtracking. We present the most comprehensive experi-mental results available to date on combined <b>AND-parallelism,</b> OR-parallelism, and intelligent backtracking in Prolog programs. Simulation results indicate that most Prolog programs in use today cannot effectively make use of multiprocessing. 1...|$|E
40|$|We {{present an}} {{overview}} of the ACE system, a sound and complete parallel implementation of Prolog that exploits parallelism transparently (i. e., without any user intervention) from AI programs and symbolic applications coded in Prolog. ACE simultaneously exploits all the major forms of parallelism [...] -Or-parallelism, Independent <b>And-parallelism,</b> and Dependent <b>And-parallelism</b> [...] -found in Prolog programs. These three varieties of parallelism are discussed in detail, along with the problems encountered in their practical exploitation. Our solutions to these problems, incorporated in the ACE system, are presented. The ACE system has been implemented on Sequent Symmetry and Sun Sparc Multiprocessors; performance results from this implementation for several AI programs are presented, that confirm the effectiveness of the choices made. Keywords: Prolog, Parallel AI, Or-Parallelism, <b>And-Parallelism.</b> 1 Introduction An important property of logic programming languages is that they are sin [...] ...|$|E
40|$|Most {{implementations}} of <b>AND-parallelism</b> {{tackle the}} shared variable problem by running literals in parallel {{only if they}} have no variables in common and thus are independent from each other. But even in its independent form, AND-parallel clause execution remains an intricate matter with numerous problems to be solved. First to mention, a carefully worked out synchronization mechanism is required to control process activities like dependency checking, literal ordering, message passing and intelligent back tracking. Due to the complex nature of these problems, implementation of <b>AND-parallelism</b> often results in a rather opaque interplay of processes. Therefore, based on {{a new kind of}} process, this paper presents a sequential- Prolog like- view of <b>AND-parallelism</b> for making it more understandable on one hand and easier to implement on the other. Processes of the new type will be called partial AND-processes. ...|$|E
40|$|ACE is a {{computational}} model for full Prolog capable of concurrently exploiting both Or and Independent <b>And-parallelism.</b> In this paper {{we focus on}} the specific implementation of the And-parallel component of the system, describing its internal organization, some optimizations to the basic model, and finally presenting some performance figures. Keywords: Independent <b>And-parallelism,</b> Orparallelism, implementation issues. 1 Introduction The ACE (And-Or/Parallel Copying-based Execution) model [6] uses stack-copying [1] and recomputation [5] to efficiently support combined Or- and Independent And-parallel execution of logic programs. ACE represents an efficient combination of Or- and independent <b>And-parallelism</b> in the sense that penalties for supporting either form of parallelism are paid only when that form of parallelism is actually exploited. Thus, in the presence of only Or-parallelism, execution in ACE is exactly as in the MUSE [2] system [...] -a stack-copying based purely Or-parallel s [...] ...|$|E
40|$|This paper {{presents}} an extended and-or tree and an extended WAM (Warren Abstract Machine) for efficiently supporting both and-parallel and or-parallel execution of logic programs on shared-memory multiprocessors. Our approach for exploiting both and- and or-parallelism {{is based on}} the binding-arrays method for or-parallelism and the RAP (Restricted <b>And-Parallelism)</b> method for <b>and-parallelism,</b> two successful methods for implementing or-parallelism and <b>and-parallelism</b> respectively. Our combined and-or model avoids redundant computations when goals exhibit both and- and or-parallelism, by representing the cross-product of the solutions from the and-or parallel goals rather than re-computing them. We extend the classical and-or tree with two new nodes: a `sequential' node (for RAP's sequential goals), and a `crossproduct' node (for the cross-product of solutions from and-or-parallel goals). The paper also {{presents an}} extension of the WAM, called AO-WAM, which is used to compile logic programs [...] ...|$|E
40|$|The {{problem of}} {{automatically}} detecting dependent <b>and-parallelism</b> in Prolog programs {{has not been}} studied so far. In this paper we discuss major issues involved in using static analysis to detect dependent <b>and-parallelism</b> in Prolog programs. We present a static analysis technique based on abstract interpretation that detects (fruitful) dependent <b>and-parallelism.</b> Our method makes use of several types {{of information about the}} program [...] -sharing and freeness, granularity of subgoals, and binding-times of program variables [...] -computed using abstract interpretation. The information collected from different sources is used to produce a suitable annotation of the original program. This annotation identifies: (i) the most fruitful sources of parallelism; and, (ii) the dependencies that must be respected during execution. A prototype compiler that incorporates these ideas has been implemented and tested on the ACE and-or parallel Prolog system. The (very encouraging) results obtained are reported h [...] ...|$|E
40|$|Occam and the {{transputer}} {{were chosen}} {{for the implementation of}} a parallel Prolog interpreter. The execution model exploits the full OR-parallelism paired with a restricted form of <b>AND-parallelism</b> (pipeline <b>AND-parallelism).</b> The depth-first, left-to-right search strategy of a sequential Prolog interpreter is preserved under the model. The interpreter is made up of a set of cooperating processes which explore different branches of the search space simultaneously. In the paper, an Occam description of processes is given along with the performance results obtained with a version of the interpreter running on a network of four transputers...|$|E
40|$|We discuss {{several issues}} {{involved}} {{in the implementation of}} ACE, a model capable of exploiting both <b>And-parallelism</b> and Or-parallelism in Prolog in a unified framework. The Orparallel model that ACE employs is based on the idea of stack-copying developed for Muse, while the model of independent <b>And-parallelism</b> is based on the distributed stack approach of &-Prolog. We discuss the organization of the workers, a number of sharing assumtions, techniques for work load detection, and issues relaed to which parts need to be copied when a flexible and-scheduling strategy is used...|$|E
40|$|In{{dependent}} <b>and-parallelism,</b> dependent <b>and-parallelism</b> and or-parallelism are {{the three}} main forms of implicit parallelism present in logic programs. In this paper we present a model, IDIOM, which exploits all three forms of parallelism in a single framework. IDIOM {{is based on a}} combination of the Basic Andorra Model and the Extended And-Or Tree Model. Our model supports both Prolog as well as the fíat concurrent logic languages. We discuss the issues that arise in combining the three forms of parallelism, and our solutions to them. We also present an implementation scheme, based on binding arrays, for implementing IDIOM...|$|E
40|$|Abstract. The growing {{popularity}} of multicore architectures has renewed interest in language-based approaches to the exploitation of parallelism. Logic programming has proved an interesting framework to this end, and there are parallel implementations which have achieved significant speedups, but {{at the cost of}} a quite sophisticated low-level machinery. This machinery has been found challenging to code and, specially, to maintain and expand. In this paper, we follow a different approach which adopts a higher level view by raising some of the core components of the implementation {{to the level of the}} source language. We briefly present an implementation model for independent <b>and-parallelism</b> which fully supports non-determinism through backtracking and provides flexible solutions for some of the main problems found in previous andparallel implementations. Our proposal is able to optimize the execution for the case of deterministic programs and to exploit unrestricted andparallelism, which allows exposing more parallelism among clause literals than fork-join-based proposals. We present performance results for an implementation, including data for benchmarks where <b>and-parallelism</b> is exploited in non-deterministic programs. Keywords: <b>And-Parallelism,</b> High-level Implementation, Prolog. ...|$|E
40|$|The growing {{popularity}} of multicore architectures has renewed interest in language-based approaches to the exploitation of parallelism. Logic programming has proved an interesting framework to this end, and there are parallel implementations which have achieved significant speedups, but {{at the cost of}} a quite sophisticated low-level machinery. This machinery has been found challenging to code and, specially, to maintain and expand. In this paper, we follow a different approach which adopts a higher level view by raising some of the core components of the implementation {{to the level of the}} source language. We briefly present an implementation model for independent <b>and-parallelism</b> which fully supports non-determinism through backtracking and provides flexible solutions for some of the main problems found in previous and-parallel implementations. Our proposal is able to optimize the execution for the case of deterministic programs and to exploit unrestricted <b>and-parallelism,</b> which allows exposing more parallelism among clause literals than fork-join-based proposals. We present performance results for an implementation, including data for benchmarks where <b>and-parallelism</b> is exploited in non-deterministic programs...|$|E
40|$|A new {{high-level}} interface to multi-threading in Prolog, {{implemented in}} hProlog, is described. Modern CPUs often contain multiple cores and through high-level multi-threading a programmer can leverage this power {{without having to}} worry about low-level details. Two common types of high-level explicit parallelism are discussed: independent <b>and-parallelism</b> and competitive or-parallelism. A new type of explicit parallelism, pipeline parallelism, is proposed. This new type can be used in certain cases where independent <b>and-parallelism</b> and competitive or-parallelism cannot be used. Comment: Online Proceedings of the 11 th International Colloquium on Implementation of Constraint LOgic Programming Systems (CICLOPS 2011), Lexington, KY, U. S. A., July 10, 201...|$|E
40|$|This paper {{concerns}} {{the exploitation of}} user transparent inherent parallelism of pure Prolog programs using program transformation. We describe a novel paradigm enumerate-and-filter for transforming generate-and-test programs for execution under the committed-choice model extended to incorporate multiple solutions based on set enumeration. The paradigm simulates OR-parallelism by stream <b>AND-parallelism</b> integrating OR-parallelism, <b>AND-parallelism,</b> and stream parallelism. Generate-and-test programs are classified into three categories:simple generate-and-test, recursively embedded generate-and-test, and deeply intertwined generate-and-test. The intermediate programs are further transformed to reduce structure copying and metacalls. Algorithms are presented and demonstrated by transforming the representative examples of different classes of generate-and-test programs to Flat Concurrent Prolog equivalents. Statistics show that the techniques are efficient...|$|E
40|$|Abstract or-work. This {{is a new}} {{and hard}} problem to be solved In {{parallel}} logic programming systems that exploit both <b>and-parallelism</b> and or-parallelism, a problem arises that is how to distribute processors between the dynamically varying amounts of and-work and or-work that are available. Solutions have been reported for dis-tributing only or-work, or distributing only and-work, but the issue of distributing processors between both kinds of work {{has not yet been}} addressed. In this work we discuss the problem of distributing and-work and or-work in the context of Andorra-I, a parallel logic programming system that exploits determinate <b>and-parallelism</b> and or-parallelism. We describe dynamic scheduling strategies that aim at efficiently distributing processors between and-work and or-work, and compare their performance with the performance produced by a static scheduling strategy, {{for a wide range of}} bench-marks...|$|E
40|$|We {{present the}} design and {{implementation}} of the and-parallel component of ACE. ACE is a computational model for the full Prolog language that simultaneously exploits both or-parallelism and independent <b>and-parallelism.</b> A high performance implementation of the ACE model has been realized and its performance reported in this paper. We discuss how some of the standard problems which appear when implementing and-parallel systems are solved in ACE. We then propose a number of optimizations aimed at reducing the overheads and the increased memory consumption which occur in such systems when using previously proposed solutions. Finally, we present results from an implementation of ACE which includes the optimizations proposed. The results show that ACE exploits <b>and-parallelism</b> with high efficiency and high speedups. Furthermore, they also show that the proposed optimizations, which are applicable to many other and-parallel systems, significantly decrease memory consumption and incr [...] ...|$|E
40|$|Andorra-I is an {{experimental}} parallel Prolog system which transparently exploits both dependent <b>and-parallelism</b> and or-parallelism. One {{of the main}} components of Andorra-I is its preprocessor. In order to obtain efficient execution of programs in Andorra-I, the preprocessor includes a compiler for Andorra-I. The compiler includes a determinacy analyser and a clause compiler, and generates code for a specialised abstract machine. In this paper we discuss the main issues in the Andorra-I compiler, presenting its abstract instruction set and describing the algorithms used in its implementation. 1 Introduction Andorra-I [13, 21] is a parallel logic programming system based on the Basic Andorra Model. It supports both dependent <b>and-parallelism,</b> by running determinate goals in parallel, and or-parallelism, by trying alternatives from nondeterminate goals in parallel. Experience in using Andorra-I has led to the following main conclusions: ffl Andorra-I performs well and exploits parallelis [...] ...|$|E
40|$|Andorra-I is {{the first}} {{implementation}} of a language based on the Andorra Principie, which states that determinate goals can (and shonld) be run before other goals, and even in a parallel fashion. This principie has materialized in a framework called the Basic Andorra model, which allows or-parallelism as well as (dependent) <b>and-parallelism</b> for determinate goals. In this report we show {{that it is possible}} to further extend this model in order to allow general independent <b>and-parallelism</b> for nondeterminate goals, withont greatly modifying the underlying implementation machinery. A simple an easy way to realize such an extensión is to make each (nondeterminate) independent goal determinate, by using a special "bagof" constract. We also show that this can be achieved antomatically by compile-time translation from original Prolog programs. A transformation that fulfüls this objective and which can be easily antomated is presented in this report...|$|E
40|$|We {{present the}} design and {{implementation}} of a recursion parallel Prolog engine based on the Reform execution model. This is a data-parallel approach to parallelizing Prolog on MIMD machines. The implementation {{is based on a}} conventional Prolog engine, WAM. The engine has been extended to handle recursion parallel execution on a shared memory architecture. List and integer recursive predicates that are binding deterministic with respect to variables shared between recursion levels are parallelized. These restrictions make it possible to extend a WAM obtaining a low parallelization overhead, between 2 % and 12 % on the measured benchmarks. We compare approaches to implementation decisions and discuss potential problems in our implementation. Our parallel implementation of Prolog has been measured to execute independent <b>AND-parallelism</b> up to 23. 15 times faster, and dependent <b>AND-parallelism</b> up to 22. 52 times faster, on 24 processors than on one processor...|$|E
40|$|Or-parallelism and <b>And-parallelism</b> {{have often}} been {{considered}} as two distinct forms of parallelism with not much in common. The {{purpose of this paper}} is to highlight the inherently dual nature of the two forms of parallelism and the similarities that exist between them. The dualities and similarities observed are then exploited for gaining new insights into the design, implementation, and optimization of and- and or-parallel systems. The ideas developed in this paper are illustrated with the help of ACE system [...] -a parallel Prolog system incorporating both and- and or-parallelism. Keywords: Prolog, <b>And-parallelism,</b> Or-parallelism, Optimizations. 1 Introduction Logic Programming is a declarative programming paradigm that is becoming increasingly popular. One of the distinguishing features of logic programming languages is that they allow considerable freedom in the way programs are executed. This latitude permits one to exploit parallelism implicitly (without the need for programmer [...] ...|$|E
40|$|One of the {{advantages}} of logic programming {{is the fact that it}} offers many sources of implicit parallelism, such as <b>and-parallelism</b> and or-parallelism. A major problem that a parallel model must address consists in represent the multiple values that shared variables can be binded to when exploited in parallel. Binding Arrays and Environment Copying are two or-parallel models that efficiently solve that problem. Recently, research in combining independent <b>and-parallelism</b> and or-parallelism within the same system has led to two new binding representation approaches: the Sparse Binding Array (an evolution of binding arrays) and the Copy-On-Write binding models. In this paper, we investigate whether for or-parallelism the newer models are practical alternatives to copying. To address this question, we experimented with YapOr, an or-parallel copying system using the YAP Prolog engine, and we implemented the Sparse Binding Array (SBA) and the Copy-On-Write (COWL) over the original sys [...] ...|$|E
40|$|<b>And-parallelism</b> {{arises in}} Prolog {{programs}} when conjunctive subgoals in a query or {{the body of}} a clause are executed in parallel. In this paper we present three optimizations, namely, the last parallel call optimization, the shallow parallelism optimization, and the processor determinacy optimization that take advantage of determinacy to improve efficiency of and-parallel execution of Prolog programs. All three optimizations depend on a posteriori knowledge of determinacy rather than on a priori knowledge detected at compile-time. With the help of these optimizations, data-and parallel Prolog programs can be efficiently executed on very general andparallel system (such as &ACE and &-Prolog) with the efficiency of dedicated dataand parallel systems (such as Reform Prolog system). These optimizations have been implemented in the &ACE system, and the results are also presented. 1 Introduction Two main types of (implicit) <b>and-parallelism</b> have been identified and successfully exploited in [...] ...|$|E
40|$|This article {{presents}} {{in an informal}} way some early results {{on the design of}} a series of paradigms for visualization of the parallel execution of logic programs. The results presented here refer to the visualization of or-parallelism, as in MUSE and Aurora, deterministic dependent <b>and-parallelism,</b> as in Andorra-I, and independent <b>and-parallelism</b> as in &-Prolog. A tool has been implemented for this purpose and has been interfaced with these systems. Results are presented showing the visualization of executions from these systems and the usefulness of the resulting tool is briefly discussed. 1 Introduction The difficulty of parallel programming and the lack of cost-effective parallel hardware have been traditionally considered the main hindrances to the wide spread use of parallelism. While cost-effective parallel computers are now appearing in the marketplace, the lack of software able to exploit parallelism in an efficient way is still an important bottleneck. In general, making a goo [...] ...|$|E
40|$|One of the {{advantages}} of logic programming {{is the fact that it}} offers several sources of implicit parallelism. One particularly interesting form of <b>And-Parallelism</b> is Independent <b>And-Parallelism</b> (IAP). Most work on the implementation of IAP is based on Hermenegildo's RAP-WAM. Unfortunately there are some drawbacks associated with the classical approaches based on the use of parcalls and markers. One first observation is that the introduction of parcall frames significantly slows down sequential execution. Moreover, it may result in fine-grained parallel work. We found these problems to be particularly significant in the context of the implementation of combined AND/OR systems. In this paper we take a fresh look at this issue. Our goal is to start from a standard sequential Prolog implementation and try to discover the minimal number of changes that would be required for an efficient implementation of IAP. The key ideas in our design are to (i) to always take advantage of analogy be [...] ...|$|E
40|$|In {{this paper}} {{we present a}} novel {{execution}} model for parallel implementation of logic programs which is capable of exploiting both independent <b>and-parallelism</b> and or-parallelism in an efficient way. This model extends the stack copying approach, which has been successfully applied in the Muse system to implement or-parallelism, by integrating it with proven techniques used to support independent <b>and-parallelism.</b> We show how all solutions to non-deterministic andparallel goals are found without repetitions. This is done through recomputation as in Prolog (and in various and-parallel systems, like &-Prolog and DDAS), i. e., solutions of and-parallel goals are not shared. We propose a scheme for the efficient management of the address space {{in a way that}} is compatible with the apparently incompatible requirements of both and- and or-parallelism. We also show how the full Prolog language, with all its extra-logical features, can be supported in our and-or parallel system so that its sequent [...] ...|$|E
40|$|Avery {{important}} component of a parallel system that generates irregular computational patterns is its work distribution strategy. Scheduling strategies {{for these kinds of}} systems must be smart enough in order to dynamically balance workload while not incurring a very high overhead. Logic programs running on parallel logic programming systems are examples of irregular parallel computations. The two main forms of parallelism exploited by parallel logic programming systems are: <b>and-parallelism,</b> that arises when several literals in the body of a clause can execute in parallel, and or-parallelism, that arises when several alternative clauses in the database program can be selected in parallel. In this work we show that scheduling strategies for distributing and-work and or-work in parallel logic programming systems must combine information obtained at compile-time with runtime information whenever possible, in order to obtain better performance. The information obtained at compile-time has two advantages over current implemented systems that use only runtime information: (1) the user does not need to adjust parameters in order to estimate sizes of and-work and orwork for the programs; (2) the schedulers can use more accurate estimates of sizes of and-work and or-work to make better decisions at runtime. We did our experiments with Andorra-I, a parallel logic programming system that exploits both determinate <b>and-parallelism</b> and or-parallelism. In order to obtain compile-time granularity information we used the ORCA tool. Our benchmark set ranges from programs containing <b>and-parallelism</b> only, or-parallelism only and a combination of both and-, and or-parallelism. Our results show that, when well designed, scheduling strategies can actually bene t from compile-time granularity information...|$|E
40|$|AbstractOne of the {{advantages}} of logic programming {{is the fact that it}} offers several sources of implicit parallelism. One particularly interesting form of <b>And-Parallelism</b> is Independent <b>And-Parallelism</b> (IAP). Most work on the implementation of IAP is based on Hermenegildo's RAP-WAM. Unfortunately there are some drawbacks associated with the classical approaches based on the use of parcalls and markers. One first observation is that the introduction of parcall frames significantly slows down sequential execution. Moreover, it may result in fine-grained parallel work. We found these problems to be particularly significant in the context of the implementation of combined AND/OR systems. In this paper we take a fresh look at this issue. Our goal is to start from a standard sequential Prolog implementation and try to discover the minimal number of changes that would be required for an efficient implementation of IAP. The key ideas in our design are to (i) to always take advantage of analogy between or-parallelism and IAP; (ii) to avoid creating new structures by adapting preexistingx WAM data-structures wherever possible; and (iii) to avoid major changes to the compiler...|$|E
40|$|In Horn-clause logic programming, it is {{necessary}} for all subgoals in a clause to succeed for the clause itself to succeed. As a result, many models of and-parallel execution of logic clauses have generally considered it desirable to execute as many subgoals as possible in parallel. Contributing to this idea has been the belief that <b>and-parallelism</b> is not speculative, as is or-parallelism. This is, unfortunately, not true. This paper shows that &quot;don't know &quot; <b>and-parallelism</b> does exhibit speculative parallelism; it is in fact inherently speculative. Due to its speculative nature, executing subgoals in parallel can lead to superlinear speedups in a regular manner. Unfortunately, its speculative nature can also lead to serious slowdown rather than to speedup. Examples of scheduling activities which would be considered quite normal (even desirable) in non-speculative parallel systems are described and shown to be able to lead to slowdown. A &quot;Slowdown Prevention Rule &quot; is then described which, if adhered to, can ensure the absence of slowdown, or to at least bound its damage. Several additional aspects of speculative parallelism are also discussed. 1...|$|E
40|$|Three general {{techniques}} are discussed for the source-to-source translation of sequential logic programs to AND-parallel logic programs. Producer-consumer information {{is used to}} translate deterministic programs with a single solution, utilizing stream-parallelism. Operationally nondeterministic programs with a single solution are translated by simulated OR-parallelism. Enumeration is used for generate-and-test programs. Enumeration needs a programming idiom for AND-parallel programs, called enumerate-and-filter. The enumerate-and-filter paradigm utilizes <b>AND-parallelism</b> along with stream-parallelism and pipelining. The {{techniques are}} illustrated with examples of Prolog programs translated to Flat Concurrent Prolog...|$|E
