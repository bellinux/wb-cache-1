21|0|Public
50|$|Anglia <b>Autoflow</b> has {{developed}} a chicken harvester named the Easyload Harvester.|$|E
5000|$|The RCA {{version of}} <b>Autoflow</b> sold only two licenses, {{but it became}} a {{commercial}} success in subsequent years as it was advertised, improved, and ported to other mainframes. [...] The rise of <b>Autoflow</b> and other software products like Informatics' MARK IV (software), coupled with IBM's decision to unbundle software from its mainframes, helped facilitate {{the growth of the}} commercial software industry in the 1970s and beyond.|$|E
50|$|Mark IV and Applied Data Research's <b>Autoflow</b> are {{generally}} considered to be the two most influential early software products.|$|E
50|$|In 1965, Applied Data Research was one {{of those}} custom {{software}} development firms. It wrote a software program for RCA mainframes called <b>Autoflow,</b> designed to create flowcharts documenting the structure of other computer programs (such flowcharts were an important tool for documenting and maintaining software). RCA decided not to license the product. Other computer manufacturers also refused to license <b>Autoflow,</b> so in 1965 Goetz decided to market it directly to RCA mainframe users. This is generally cited as {{the first time that a}} software program was marketed and sold as a standalone product.|$|E
5000|$|In Australia and New Zealand, {{there are}} {{numerous}} suppliers offering vermifilter systems for domestic greywater and/or blackwater treatment, with primary treated effluent disposal to subsurface leach fields. Examples include Wormfarm, Zenplumb, Naturalflow, SWWSNZ and <b>Autoflow.</b>|$|E
50|$|Martin A. Goetz (born April 22, 1930) was {{a pioneer}} in the {{development}} of the commercial software industry. He holds the first software patent, and was product manager of <b>Autoflow</b> from Applied Data Research (ADR), which is generally cited as the first commercial software application.|$|E
50|$|Dual-control modes are {{pressure}} controlled modes with an exhaled {{tidal volume}} target. They {{work on a}} breath-by-breath basis and provide pressure-limited time-cycled breaths, increasing or decreasing {{the pressure of the}} next breath as necessary to achieve a user-selected desired tidal volume. They are known by various vendor-specific terms such as pressure-regulated volume control (Siemens), <b>autoflow</b> (Dräger), adaptive-pressure ventilation (Hamilton Medical), volume-control plus (Covidien), among others.|$|E
50|$|Founded in 1959, ADR was {{originally}} a contract development company. ADR eventually built {{a series of}} its own products. ADR's widely used major packages included: <b>Autoflow</b> for automatic flowcharting, ROSCOE (Remote OS Conversational Operating Environment), and Librarian for source-code management. ADR later purchased the Datacom/DB database management system from Insyte Datacom and developed the companion product, IDEAL (Interactive Development Environment for an Application’s Life), a fourth-generation programming language.|$|E
5000|$|ADR instigated {{litigation}} in Federal Court against IBM [...] with accusations that IBM was [...] "retarding {{the growth of}} the independent software industry" [...] and [...] "monopolizing the software industry", leading to IBM's famous unbundling of software and services in 1969. In 1970, ADR and Programmatics, a wholly owned subsidiary of ADR, received an out-of-court settlement of $1.4 million from IBM. IBM also agreed to serve as a supplier of <b>Autoflow,</b> which meant another potential $600,000 in revenues for ADR.|$|E
40|$|The {{use of the}} <b>AUTOFLOW</b> {{system is}} {{discussed}} in terms of improving automated documentation. Flowcharts produced by <b>AUTOFLOW</b> are considered to be much more meaningful than those produced manually in that they are accurate, present complete references between all transfer points, and graphically portray the logical flow by automatic rearrangement of those segments of the program that interact...|$|E
40|$|Aspect-oriented {{programming}} (AOP) {{is gaining}} popu-larity with the wider adoption of languages such as As-pectJ. During AspectJ software evolution, when regression tests fail, {{it may be}} tedious for programmers {{to find out the}} failure-inducing changes by manually inspecting all code editing. To eliminate the expensive effort spent on debug-ging, we developed <b>AutoFlow,</b> an automatic debugging tool for AspectJ software. <b>AutoFlow</b> integrates the potential of delta debugging algorithm with the benefit of change im-pact analysis to narrow down the search for faulty changes. It first uses change impact analysis to identify a subset of re-sponsible changes for a failed test, then ranks these changes according to our proposed heuristic (indicating the likeli-hood that they {{may have contributed to the}} failure), and finally employs an improved delta debugging algorithm to determine a minimal set of faulty changes. The main fea-ture of <b>AutoFlow</b> is that it can automatically reduce a large portion of irrelevant changes in an early phase, and then locate faulty changes effectively. ...|$|E
40|$|The use of {{workflows}} {{to automate}} routine tasks {{is an absolute}} requirement in many bioinformatics fields. Current workflow manager systems usually compromise between providing a user-friendly interface and constructing complex, scalable pipelines. We present <b>AutoFlow,</b> a Ruby-based workflow engine devoid of graphic interface and tool repository, that is useful in most computer systems and most workflow requirements in any scientific field. It accepts any local or remote command-line software and converts one workflow {{into a series of}} independent tasks. It has been supplied with control patterns that allow for iterative task capability, supporting static and dynamic variables for decision-making or chaining workflows, as well as debugging utilities that include graphs, file searching, functional consistency and timing. Two proof-of-concept cases are presented to illustrate <b>AutoFlow</b> capabilities, and a case-of-use illustrates the automated construction of the best transcriptome for a non-model species (Vicia faba) after analysis of several combinations of Illumina reads and Sanger sequences with different assemblers and different parameters in a complex and repetitive workflow where branching and convergent tasks were used and internal, automated decisions were taken. The workflow finally produced an optimal transcriptome of 118, 188 transcripts, of which 38, 004 were annotated, 10, 516 coded for a complete protein, 3, 314 were putatively new faba-specific transcripts, and 23, 727 were considered the representative transcriptome of V. faba. Peer reviewe...|$|E
40|$|Today's {{enterprise}} {{systems and}} applications implement functionality that {{is critical to}} the ability of society to function. These complex distributed applications, therefore, must meet dynamic criticality objectives even when running on shared heterogeneous and dynamic computational and communication infrastructures. Focusing on the broad class of applications structured as distributed information flows, the premise of our research is that it is difficult, if not impossible, to meet their dynamic service requirements unless these applications exhibit autonomic or self-adjusting behaviors that are `vertically' integrated with underlying distributed systems and hardware. Namely, their autonomic functionality should extend beyond the dynamic load balancing or request routing explored in current web-based software infrastructures to (1) exploit the ability of middleware or systems to be aware of underlying resource availabilities, (2) dynamically and jointly adjust the behaviors of interacting elements of the software stack being used, and even (3) dynamically extend distributed platforms with enterprise functionality (e. g., network-level business rules for data routing and distribution). The resulting vertically integrated systems can meet stringent criticality or performance requirements, reduce potentially conflicting behaviors across applications, middleware, systems, and resources, and prevent breaches of the `performance firewalls' that isolate critical from non-critical applications. This paper uses representative information flow applications to argue the importance of vertical integration for meeting criticality requirements. This is followed by a description of the <b>AutoFlow</b> middleware, which offers methods that drive the control of application services with runtime knowledge of current resource behavior. Finally, we demonstrate the opportunities derived from the additional ability of <b>AutoFlow</b> to enhance such methods by also dynamically extending and controlling the underlying software stack, first to better understand its behavior and second, to dynamically customize it to better meet current criticality requirements...|$|E
40|$|Over {{half of the}} world's {{population}} will live in urban areas in the next decade, which will impose significant pressure on water security. The advanced {{management of water resources}} and their consumption is pivotal to maintaining a sustainable water future. To contribute to this goal, the aim {{of this study was to}} develop an autonomous and intelligent system for residential water end-use classification that could interface with customers and water business managers via a user-friendly web-based application. Water flow data collected directly from smart water meters connected to dwellings includes both single (e. g., a shower event occurring alone) and combined (i. e., an event that comprises several overlapping single events) water end use events. The authors recently developed an intelligent application called <b>Autoflow</b> which served as a prototype tool to solve the complex problem of autonomously categorising residential water consumption data into a registry of single and combined events. However, this first prototype application achieved overall recognition accuracy of 85 %, which is not sufficient for a commercial application. To improve this accuracy level, a larger dataset consisting of over 82 thousand events from over 500 homes in Melbourne and South-east Queensland, Australia, were employed to derive a new single event recognition method employing a hybrid combination of Hidden Markov Model (HMM), Artificial Neural Networks (ANN) and the Dynamic Time Warping (DTW) algorithm. The classified single event registry was then used as the foundations of a sophisticated hybrid ANN-HMM combined event disaggregation module, which was able to strip apart concurrently occurring end use events. The new hybrid model's recognition accuracy ranged from 85. 9 to 96. 1 % for single events and 81. 8 to 91. 5 % for combined event disaggregation, which was a 4. 9 % and 8. 0 % improvement, respectively, when compared to the first prototype model. The developed <b>Autoflow</b> tool has far-reaching implications for enhanced urban water demand planning and management, sustained customer behaviour change through more granular water conservation awareness, and better customer satisfaction with water utility providers. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|E
40|$|In this project, a {{predictive}} time {{model was}} developed for an Anglia <b>Autoflow</b> mechanical chicken catching system. At the completion of poultry growout, hand labor is currently used to collect the birds from the house, although some integrators are beginning to incorporate mechanical catching equipment. Several regression models were investigated {{with the objective of}} predicting the time taken to catch the chicken. A regression model relating distance to total time (sum of packing time, catching time, movement to catching and movement to packing) provided the best performance. The model was based on data collected from poultry farms on the Delmarva Peninsula during a six-month period. Statistical Analysis System (SAS) and NeuroShell Easy Predictor were used to build the regression and neural network models respectively. Model adequacy was established by both visual inspection and statistical techniques. The models were validated with experimental results not incorporated into the initial model. Livestock Production/Industries,...|$|E
40|$|We have {{developed}} QuasiFlow, a workflow designed in <b>AutoFlow</b> that {{takes advantage of}} NGS technologies to reconstruct quasispecies based in Illumina reads. QuasiFlow characterises and computes several key parameters, such as recombination events, SNPs, transitions, transversions, indels, quasispecies reconstruction, normalized Shannon index, nucleotide diversity and mutation networks. Moreover, it performs a comparative study of the samples comprising correlation, ANOVA and PCA analyses of the previously obtained virus population parameters. Using QuasiFlow we have analysed Illumina MiSeq reads from DNA samples obtained in mixed infections of ssDNA begomovirus in tomato plants amplified by rolling circle amplification. Further, we have extended the use of QuasiFlow {{to the analysis of}} the highly variable mitochondrial DNA. For that, we have used DNA Illumina MiSeq reads from 47 human mitochondrial samples from different cell lines obtained from the NCBI SRA databaseUniversidad de Málaga. Campus de Excelencia Internacional Andalucía Tech...|$|E
40|$|When {{regression}} tests fail unexpectedly {{after a long}} {{session of}} editing, it may be tedious for programmers {{to find out the}} failureinducing changes by manually inspecting all code edits. To eliminate the expensive effort spent on debugging, we present a hybrid approach, which combines both static and dynamic analysis techniques, to automatically identify the faulty changes. Our approach first uses static change impact analysis to isolate a subset of responsible changes for a failed test, then utilizes the dynamic test execution information to rank these changes according to our proposed heuristic (indicating the likelihood that they {{may have contributed to the}} failure), and finally employs an improved Three-Phase delta debugging algorithm, working from the coarse method level to the fine statement level, to find a minimal set of faulty statements. We implemented the proposed approach for both Java and AspectJ programs in our <b>AutoFlow</b> prototype. In our evaluation with two third-party applications, we demonstrate that this hybrid approach can be very effective: at least for the subjective programs we investigated, it takes significantly (almost 4 X) fewer tests than the original delta debugging algorithm to locate the faulty code. 1...|$|E
40|$|An {{important}} step in solving a problem is to choose the nota-tion. It should be done carefully. The time we spend now on choosing the notation may be well repaid {{by the time we}} save later avoiding hesitation and confusion. Moreover, choosing the notation carefully, we have to think sharply of the ele-ments of the problem which must be denoted. Thus, choosing a suitable notation may contribute essentially to understand-ing the problem. (Polya, 1957) Since the inception of the software industry, models have been a bene-ficial tool for managing complexity. In fact, the first commercial software package that was sold independent of a hardware manufacturer was an appli-cation for constructing flow chart models, i. e., ADR’s <b>AUTOFLOW</b> (ADR, 2002). In numerous disciplines, models are constructed to assist in the un-derstanding of the essential characteristics of some instance from a particular domain. Mechanical engineers, architects, computer scientists, and many other professionals create models to provide projected views over an entity that has been abstracted from the real-world. As tools for creative explora-tion, even children erect models of real-world structures using Legos, Tinker Toys, and other similar materials...|$|E
40|$|Background: Efficiency {{of a water}} {{management}} and irrigation control system is important for improved production and quality of crops. Electronic valve {{is one of the}} most important components for the irrigation management system, and the performance is characterized by the flow-pressure relationships. In the study, a test bench was developed to test electronic valves for irrigation management systems. Methods: A multistage pump (Model: DRL 10 - 8; DOOCH, Korea), flow rate sensors (Model: E-MAG-I, <b>AUTOFLOW,</b> Korea), and pressure sensors (Model: A- 10; WIKA, Germany) were used to construct the test bench. A control program was coded to control the pump operation. The flow rate was set at four different levels (20, 40, 60, 80 m 3 /h) using the multistage pump. Flow rate and pressure sensors were installed before and after the test electric valve. Data were collected through a wireless sensor network. The electric valve was controlled remotely by the software developed using C # (visual studio 2010, USA). Results& discussion: The experimental results showed that the error ranges were 2 % at 20 m 3 /h and 7 % at 80 m 3 /h. We found that the flow rate within 20 ~ 80 m 3 /h and the pressure loss within 2 ~ 10 bar measured before and after the electric valve were linearly plotted. The pressure decreased 10 % before and after the solenoid valve. The delay time need to be considered for stable flow control. To solve this problem, relief valve was installed to remove the flow rate exceeding the specified value. Conclusion: The test bench should a favourable performance, and will be used to test and compare electronic valves...|$|E
40|$|Most {{workflow}} research {{focuses on}} the modeling aspects of workows, i. e., the specification of how the execution of thoses tasks in a workflow is to be ordered. Correctness of a workflow is not de ned {{in terms of the}} outcome of the workflow, {{but in terms of the}} enforcement of the data and control dependency that are specified at design time. The semantics of each task is not modeled, thus the specification of these dependencies is based on the user's informal intuition and understanding of a particular workflow application. In this paper, we have developed a formal model that allows us to describe the desired outcome of a workflow and the behavior of the tasks in the library. Based on this model, we have developed an algorithm that, given a workow specification and a task library, will produce a workflow using those tasks if such a workflow exists. Thus data and control dependency are derived automatically rather than being specified manually. We also allow the workflow designer to impose additional restrictions on how the workflow is executed to achieve the outcome. We refer to such restrictions as business rules to differentiate them from the correctness aspect of a workflow. We have programmed this algorithm and tested it on some simple cases. We propose to extend this work in a number of directions: [...] The algorithm produces sequential workflows. We intend to extend it to produce concurrent workflows. [...] We intend to refine the algorithm to produce workflows with known optimality properties. We intend to incorporate more complex business rules into the model. [...] We intend to build a workflow control system, <b>AutoFlow,</b> based on the results of the research...|$|E
40|$|The {{development}} of Next Generation Sequencing (NGS) technologies has allowed deep characterization of highly variable sequences such as viral or mitochondrial genomes. With respect to RNA and ssDNA viruses, their low replication fidelity generates viral populations consisting of complex mutant spectra termed viral quasispecies. Their study is {{of special interest}} {{as they can be}} considered a phenotypic reservoir 1. Similarly, heteroplasmy of human mitochondrial genomes, in which different sequences are found within a single individual, might have important clinical consequences. For the analysis of the mutant spectrum of such hypervariable sequences from NGS data, we have developed QuasiFlow, a workflow designed in <b>AutoFlow</b> 2 that uses Illumina reads. QuasiFlow provides information about DNA variability, such as SNPs, indels and recombination events (Figure 1). Furthermore, it allows haplotype reconstruction of viral quasispecies and characterization of its diversity through normalized Shannon index, nucleotide diversity and mutation networks. Quasiflow performs also a comparative study among samples, based on correlation, ANOVA and PCA analysis, in order to determine which parameters are affected by the experiment and how the samples behave according to their biological origin. In this work, we have applied QuasiFlow to analyze the population structure of the begomovirus Tomato yellow leaf curl virus (TYLCV) infectious clone inoculated in Arabidopsis thaliana plants, using HiSeq or MiSeq reads. Their analysis allowed detection of minor quasispecies variants with a frequency of 10 - 4 to 10 - 5 and reconstructed the haplotypes present in the sample. In addition, QuasiFlow was used to discover variants and recombinants in mixed infections of tomato plants. These results show the fast generation of recombinant genomes in geminivirus mixed infections and demonstrate the potential of QuasiFlow for the analysis of mutant spectra using Illumina MiSeq sequencing data. We have extended the use of QuasiFlow to the analysis of highly variable sequences such as the mitochondrial DNA. For that, we have analyzed DNA Illumina Miseq reads from 47 human mitochondrial samples from different cell lines obtained from the NCBI SRA database. Quasiflow generated automatically SNPs, SNP frequencies, indels and analyzed up to 23 variables using PCA analysis and performed an hierarchical clustering of the samples. Our analysis was able to detect pathological variants presented in a frequency lower than 1 %. Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech. This research was funded by Junta de Andalucía and EU through the ERDF 2014 - 2020, Projects P 10 -CVI- 6075 to M. G. C. and P 10 -CVI- 6561 to A. G-P...|$|E

