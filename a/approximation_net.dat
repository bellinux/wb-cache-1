0|35|Public
40|$|We {{consider}} perturbative one loop QCD corrections to {{the light}} neutrino contribution to neutrinoless double beta decay and find large enhancement to the rate. QCD corrections also generate structures which mimic new physics contributions usually considered. Within some <b>approximations,</b> the <b>net</b> effect seem to almost saturate the experimental limits, and therefore seems to implt {{that this is all}} there is to neutrinoless double beta decay. Comment: 5 pages, 1 figure. arXiv admin note: text overlap with arXiv: 1406. 260...|$|R
40|$|AbstractIn {{this paper}} we develop tools for the {{analysis}} of net subdivision schemes, schemes which recursively refine nets of bivariate continuous functions defined on grids of lines, and generate denser and denser nets. Sufficient conditions for the convergence of such a sequence of refined nets, and for the smoothness of the limit function, are derived in terms of proximity to a bivariate linear subdivision scheme refining points, under conditions controlling some aspects of the univariate functions of the generated <b>nets.</b> <b>Approximation</b> orders of <b>net</b> subdivision schemes, which are in proximity with positive schemes refining points are also derived. The paper concludes with the construction of a family of blending spline-type net subdivision schemes, and with their analysis by the tools presented in the paper. This family is a new example of net subdivision schemes generating C 1 limits with approximation order 2...|$|R
40|$|The {{mathematical}} {{formulation of}} the engineering optimization problem is given. Evaluation of the objective function and constraint equations can be very expensive in a computational sense. Thus, it is desirable to use as few evaluations as possible in obtaining its solution. In solving the equation, one approach is to develop approximations to the objective function and/or restraint equations and then to solve the equation using the approximations {{in place of the}} original functions. These approximations are referred to as response surfaces. The desirability of using response surfaces depends upon the number of functional evaluations required to build the response surfaces compared to the number required in the direct solution of the equation without approximations. The present study is concerned with evaluating the performance of response surfaces so that a decision can be made as to their effectiveness in optimization applications. In particular, this study focuses on how the quality of approximations is effected by design selection. Polynomial <b>approximations</b> and neural <b>net</b> <b>approximations</b> are considered...|$|R
40|$|We study {{local and}} global <b>approximations</b> of smooth <b>nets</b> of {{curvature}} lines and smooth conjugate nets by respective discrete nets (circular nets and planar quadrilateral nets) with edges of order ǫ. It is shown that choosing the points of discrete nets on the smooth surface one can obtain the order ǫ 2 approximation globally. Also a simple geometric construction for approximate determination of principal directions of smooth surfaces is given. ...|$|R
40|$|AbstractLetDbe a {{set with}} a {{probability}} measureμ,μ(D) = 1, and letKbe a compact subset ofLq(D, μ), 1 ⩽q<∞. Forf∈Lq,n= 1,  2, …, letρn(f, K) =inf‖f−gn‖q, where the infimum {{is taken over}} allgnof the formgn=∑ni= 1 aiφi, with arbitraryφi∈Kandai∈R. It is shown that for[formula], under some mild restrictions,ρn(f, K) ⩽Cqεn(K) n− 1 / 2, whereεn(K) → 0 asn→∞. This fact is used to estimate the errors of certain neural <b>net</b> <b>approximations.</b> For the latter, also the lower estimates of errors are given...|$|R
40|$|This paper {{concerns}} computational {{models in}} environmental economics and policy, particularly so-called integrated assessment models. For the most part, such models are simply extensions of standard neoclassical growth models, extended by including {{the environment and}} pollution generation. We review the structure of integrated assessment models, distinguishing between finite horizon and infinite horizon models, both deterministic and stochastic. We present a new solution algorithm for infinite horizon integrated assessment models, relying on a neural <b>net</b> <b>approximation</b> of the value function within an iterative version of the Bellman equation...|$|R
40|$|In {{this paper}} a {{quantitative}} approximation result is obtained {{for a general}} class of function nets which is of interest in functional estimation. Specific applications are given to <b>approximation</b> by neural <b>nets,</b> radial basis function nets, and wavelet nets. For the proof we combine the empirical process based results of a paper of Yukich et al. (IEEE Trans. Inform. Theory 41 (4) (1995) 1021) with probabilistic based approximation results of Makovoz (J. Approx. Theory 85 (1996) 98) for the optimal approximation of functions by convex combination of n basis elements. Neural nets Radial basis functions Wavelet nets Functional estimation...|$|R
40|$|There is {{currently}} {{a great deal of}} interest in using response surfaces in the optimization of aircraft performance. The objective function and/or constraint equations involved in these optimization problems may come from numerous disciplines such as structures, aerodynamics, environmental engineering, etc. In each of these disciplines, the mathematical complexity of the governing equations usually dictates that numerical results be obtained from large computer programs such as a finite element method program. Thus, when performing optimization studies, response surfaces are a convenient way of transferring information from the various disciplines to the optimization algorithm as opposed to bringing all the sundry computer programs together in a massive computer code. Response surfaces offer another advantage in the optimization of aircraft structures. A characteristic of these types of optimization problems is that evaluation of the objective function and response equations (referred to as a functional evaluation) can be very expensive in a computational sense. Because of the computational expense in obtaining functional evaluations, the present study was undertaken to investigate under-determinined approximations. An under-determined approximation is one in which there are fewer training pairs (pieces of information about a function) than there are undetermined parameters (coefficients or weights) associated with the approximation. Both polynomial <b>approximations</b> and neural <b>net</b> <b>approximations</b> were examined. Three main example problems were investigated: (1) a function of one design variable was considered; (2) a function of two design variables was considered; and (3) a 35 bar truss with 4 design variables was considered...|$|R
40|$|Abstract: The {{present study}} {{classified}} global Ecosystem Functional Types (EFTs) derived from seasonal vegetation {{dynamics of the}} GIMMS 3 g NDVI time-series. Rotated Principal Component Analysis (PCA) was run on the derived phenological and productivity variables, which selected the Standing Biomass (<b>approximation</b> of <b>Net</b> Primary Productivity), the Cyclic Fraction (seasonal vegetation productivity), the Permanent Fraction (permanent surface vegetation), the Maximum Day (day of maximum vegetation development) and the Season Length (length of vegetation growing season) variables, describing 98 % {{of the variation in}} global ecosystems. EFTs were created based on Isodata classification of the spatial patterns of the Principal Components and were interpreted via gradient analysis using the selected remote sensing variables and climatic constraints (radiation, temperature, and water) of vegetation growth. The association of the EFTs with existing climate and land cover classifications was demonstrated via Detrended Correspondence Analysis (DCA). The ordination indicated good description of the global environmental gradient by the EFTs, supporting the understanding of phenological and productivity dynamics of global ecosystems. Climatic constraints of vegetation growt...|$|R
40|$|We {{discuss the}} <b>approximation</b> {{properties}} of <b>nets</b> of positive linear operators acting on function spaces defined on Hausdorff completely regular spaces. A particular attention {{is devoted to}} positive operators which are {{defined in terms of}} integrals with respect to a given family of Borel measures. We present several applications which, in particular, show the advantages of such a general approach. Among other things, some new Korovkin-type theorems on function spaces on arbitrary topological spaces are obtained. Finally, a natural extension of the so-called Bernstein-Schnabl operators for convex (not necessarily compact) subsets of a locally convex space is presented as well. 2000 Mathematics Subject Classification: 41 A 36, 47 A 58, 47 B 65. 1. Introduction. Korovkin’...|$|R
40|$|Owing to {{the analogy}} with the {{ordinary}} photons in the visible {{range of the}} electromagnetic spectrum, the Glauber theory is generalized to address the quantum coherence of the gauge field fluctuations parametrically amplified during an inflationary stage of expansion. The first and second degrees of quantum coherence of relic photons are then computed beyond the effective horizon defined by {{the evolution of the}} susceptibility. In the zero-delay limit the Hanbury Brown-Twiss correlations exhibit a super-Poissonian statistics which is however different from the conventional results of the single-mode approximation customarily employed, in quantum optics, to classify the coherence properties of visible light. While in the case of large-scale curvature perturbations the degrees of quantum coherence coincide with the naive expectation of the single-mode <b>approximation,</b> the <b>net</b> degree of second-order coherence computed for the relic photons diminishes thanks to the effect of the polarizations. We suggest that the Hanbury Brown-twiss correlations are probably the only tool to assess the quantum or classical origin of the large-scale magnetic fluctuations and of the corresponding curvature perturbations. Comment: 33 pages. arXiv admin note: text overlap with arXiv: 1608. 0584...|$|R
40|$|This paper {{shows how}} to define Petri nets through {{recursive}} equations. It specifically addresses this problem {{within the context}} of the box algebra, a model of concurrent computation which combines Petri nets and standard process algebras. The paper presents a detailed investigation of the solvability of recursive equations on nets in what is arguably the most general setting. For it allows an infinite number of possibly unguarded equations, each equation possibly involving infinitely many recursion variables. The main result is that by using a suitable partially ordered domain of nets, it is always possible to solve a system of equations by constructing the limit of a chain of successive <b>approximations.</b> Keywords: Petri <b>nets,</b> box algebra, refinement, recursion, approximation, limit construction, guardedness...|$|R
40|$|A {{technique}} for approximating the behaviour of graph transformation systems (GTSs) {{by means of}} Petri net-like structures has been recently defined in the literature. In this paper we introduce a monadic second-order logic over graphs expressive enough to characterise typical graph properties, and we show how its formulae can be effectively verified. More specifically, we provide an encoding of such graph formulae into quantifier-free formulae over Petri net markings and we characterise, via a type assignment system, a subclass of formulae F such that the validity of F over a GTS G is implied by {{the validity of the}} encoding of F over the Petri <b>net</b> <b>approximation</b> of G. This allows us to reuse existing verification techniques, originally developed for Petri nets, to model-check the logic, suitably enriched with temporal operators...|$|R
40|$|This paper {{presents}} a methodology for optimisation of vehicle drivetrain configuration and their design {{with respect to}} multiple parameters. Some preliminary results from such optimisations are presented. Multi-objective optimisation algorithms {{have been developed to}} improve on earlier work on "environomic" optimisation, in which costs and environmental parameters are optimised using an agglomerated objective function. The vehicle component models are based on a combination of experimental data and theory, and in some cases neural <b>net</b> <b>approximations</b> of complicated subsystems have been used to replace slow theoretical models. The components developed at LENI have been incorporated into the vehicle simulation system ADVISOR and the system has been optimised with respect to component sizes, as well as three configurations (one conventional and two hybrid drivetrains) for such diverse objectives as emissions, costs and performance...|$|R
30|$|Artificial neural networks, or neural nets, are {{computational}} structures {{inspired by}} biological networks of densely connected neurons, {{each of which}} is capable only of simple computations. Just as biological neural networks are capable of learning from their environment, neural nets are able to learn from the presentation of training data, as the free parameters (weights and biases) are adaptively tuned to fit the training data. Neural nets can be used to learn and compute functions for which the analytical relationships between inputs and outputs are unknown and/or computationally complex and are therefore useful for pattern recognition, classification, and function <b>approximation.</b> Neural <b>nets</b> are particularly appealing for the inversion of atmospheric remote sensing data, where relationships are commonly nonlinear and non-Gaussian, and the physical processes may not be well understood. Neural networks were perhaps first applied in the atmospheric remote sensing context by Escobar-Munoz et al. [9], and many other investigators have recently reported on the use of neural networks for inversion of microwave sounding observations for the retrieval of temperature and water vapor [8, 7][10 – 13] and hydrologic parameters [14 – 22], as well as inversion of infrared sounding observations for retrieval of temperature and water vapor [23 – 27] and trace gases [28]. Neural networks have also been used in the geophysical context for nonlinear data representation [29].|$|R
40|$|Abstract — Neural {{networks}} {{have been used}} for modeling the nonlinear characteristics of memoryless nonlinear channels using backpropagation (BP) learning with experimental training data. In order to better understand this neural network application, this paper studies the transient and convergence properties of a simplified two-layer neural network that uses the BP algorithm and is trained with zero mean Gaussian data. The paper studies the effects of the neural net structure, weights, initial conditions, and algorithm step size on the mean square error (MSE) of the neural <b>net</b> <b>approximation.</b> The performance analysis is based on the derivation of recursions for the mean weight update {{that can be used to}} predict the weights and the MSE over time. Monte Carlo simulations display good to excellent agreement between the actual behavior and the predictions of the theoretical model. I...|$|R
40|$|Dynamic Programming, Q-learning {{and other}} {{discrete}} Markov Decision Process solvers {{can be applied}} to continuous d-dimensional state-spaces by quantizing the state space into an array of boxes. This is often problematic above two dimensions: a coarse quantization can lead to poor policies, and fine quantization is too expensive. Possible solutions are variable-resolution discretization, or function <b>approximation</b> by neural <b>nets.</b> A third option, which has been little studied in the reinforcement learning literature, is interpolation on a coarse grid. In this paper we study interpolation techniques that can result in vast improvements in the online behavior of the resulting control systems: multilinear interpolation, and an interpolation algorithm based on an interesting regular triangulation of d-dimensional space. We adapt these interpolators under three reinforcement learning paradigms: (i) offline value iteration with a known model, (ii) Q-learning, and (iii) online value iteration wi [...] ...|$|R
40|$|This thesis {{extends the}} model of Klein and Inglis (2001) by {{taking into account the}} effect of a netting agreement, e. g. an ISDA Master Agreement, and the effect of {{portfolio}} diversification on the price of vulnerable European options. The model considers options traded mutually between two option writers one of which may default. Based on this model the credit-risk adjusted price of an option is a conditional price with respect to the portfolio of options to which it is added. Using a numerical <b>approximation</b> (Monte-Carlo simulation), <b>netting</b> and portfolio effects are shown to increase the credit-risk adjusted value of a trading position. The paper shows that the price which a counterparty is willing to receive (pay) for selling (buying) an option, is less (more) than the usual price if the option has a credit risk mitigation effect on the existent portfolio...|$|R
40|$|The Adaptive Base Station Positioning Algorithm #ABPA# is presented, {{which is}} based on a neural <b>net</b> <b>approximation</b> of the traffic density in the {{coverage}} area of a cellular mobile communication system. ABPA employs simulated annealing, therebyachieving quasi-optimal base station locations depending on the topography of the investigated area. Furthermore, ABPA considers the radio wave propagation within this area for the base station positions. Therefore, a three dimensional digital surface model is used to approximate the topography and two field strength prediction methods, a line-of-sight #LOS# approach and a ray-tracing technique, are investigated within the context of adaptive positioning. In particular, the results obtained by the ray-tracing technique are encouraging, showing supplying areas, which seem to be similar to those, stemming from real measurements. However, as simulations show, the more realistic field strength prediction by ray-tracing has a strong influence on the perfor [...] ...|$|R
40|$|The paper {{discusses}} {{the role of}} intangibles in banks' value creation. It was assumed that since the banks book value is a close <b>approximation</b> of their <b>net</b> assets value, the observed difference between market value and book value of banks {{is a result of}} existence of resources not included in the balance sheet which increase the income generation potential of those entities. The paper presents the main types of intangibles used by banks (including employees, clients, brands, quality). The results of the research concerning the information about intangibles disclosed by banks are also presented. Generally Polish public banks (having the biggest requirements concerning information disclosure and corporate governance!) disclose very little information about their intangibles. The potential users of the reports would experience difficulties with using this information when assessing the ability of value creation in the future...|$|R
40|$|Aims: We {{study the}} changes in the {{properties}} of turbulence driven by the magnetorotational instability in a shearing box, as the computational domain size in the radial direction is varied relative to the height Methods: We perform 3 D simulations in the shearing box <b>approximation,</b> with a <b>net</b> magnetic flux, and we consider computational domains with different aspect ratios Results: We find that in boxes of aspect ratio unity the transport of angular momentum is strongly intermittent and dominated by channel solutions in agreement with previous work. In contrast, in boxes with larger aspect ratio, the channel solutions and the associated intermittent behavior disappear. Conclusions: There is strong evidence that, as the aspect ratio becomes larger, the characteristics of the solution become aspect ratio independent. We conclude that shearing box calculations with aspect ratio unity or near unity may introduce spurious effects. Comment: 5 pages, 6 figures, Astronomy and Astrophysics accepte...|$|R
40|$|This {{thesis is}} {{concerned}} with verification and analysis techniques for software systems characterized by dynamically evolving structure, such as dynamic creation and deletion of objects, mobility and variable topology. Examples for such systems are pointer structures, object-based systems and communication protocols in which {{the number of participants}} is not constant. The approach taken here is based on graph transformation systems, an intuitive and—at the same time—powerful formalism for the modelling of distributed and mobile systems. So far there exists comparatively little research concerning the verification of graph rewriting. We will—in {{the first part of this}} thesis—introduce graph transformations and give an overview of existing analysis and verification methods, with a focus on the verification of systems with dynamically evolving structure. Then we will describe three original lines of research: behavioural equivalences, type systems and <b>approximation</b> by Petri <b>nets,</b> all of them concerned with the analysis o...|$|R
40|$|The {{influence}} of ultrasonic radiation on {{the flow of}} a liquid through a porous medium is analyzed. The analysis {{is based on a}} mechanism proposed by Ganiev et al. according to which ultrasonic radiation deforms the walls of the pores in the shape of travelling transversal waves. Like in peristaltic pumping, the travelling transversal wave induces a net flow of the liquid inside the pore. In this article, the wave amplitude is related to the power output of an acoustic source, while the wave speed is expressed in terms of the shear modulus of the porous medium. The viscosity as well as the compressibility of the liquid are taken into account. The Navier–Stokes equations for an axisymmetric cylindrical pore are solved by means of a perturbation analysis, in which the ratio of the wave amplitude to the radius of the pore is the small parameter. In the second-order <b>approximation</b> a <b>net</b> flow induced by the travelling wave is found. For various values of the compressibility of the liquid, the Reynolds number and the frequency of the wave, the net flow rate is calculated. The calculations disclose that the compressibility of the liquid has a strong influence on the net flow induced. Furthermore, by a comparison with the flow induced by the pressure gradient in an oil reservoir, the net flow induced by a travelling wave can not be neglected, although it is a second-order effect...|$|R
40|$|The {{present study}} {{classified}} global Ecosystem Functional Types (EFTs) derived from seasonal vegetation {{dynamics of the}} GIMMS 3 g NDVI time-series. Rotated Principal Component Analysis (PCA) was run on the derived phenological and productivity variables, which selected the Standing Biomass (<b>approximation</b> of <b>Net</b> Primary Productivity), the Cyclic Fraction (seasonal vegetation productivity), the Permanent Fraction (permanent surface vegetation), the Maximum Day (day of maximum vegetation development) and the Season Length (length of vegetation growing season) variables, describing 98 % {{of the variation in}} global ecosystems. EFTs were created based on Isodata classification of the spatial patterns of the Principal Components and were interpreted via gradient analysis using the selected remote sensing variables and climatic constraints (radiation, temperature, and water) of vegetation growth. The association of the EFTs with existing climate and land cover classifications was demonstrated via Detrended Correspondence Analysis (DCA). The ordination indicated good description of the global environmental gradient by the EFTs, supporting the understanding of phenological and productivity dynamics of global ecosystems. Climatic constraints of vegetation growth explained 50 % of variation in the phenological data along the EFTs showing that part of the variation in the global phenological gradient is not climate related but is unique to the Earth Observation derived variables. DCA demonstrated good correspondence of the EFTs to global climate and also to land use classification. The results show the great potential of Earth Observation derived parameters for the quantification of ecosystem functional dynamics and for providing reference status information for future assessments of ecosystem changes...|$|R
40|$|Analysis of {{subdivision}} {{schemes for}} nets of functions by proximity and controllability. (English summary) J. Comput. Appl. Math. 236 (2011), no. 4, 461 – 475. In this dense paper the authors develop tools {{for the analysis}} of net subdivision schemes. These schemes recursively refine nets of bivariate continuous functions defined on grids of lines, and generate denser and denser nets which (in the convergent case) converge uniformly to continuous bivariate functions. In particular, the convergence of such a sequence of refined nets, the smoothness of the limit function, and the <b>approximation</b> orders of <b>net</b> subdivision schemes are analyzed. Sufficient conditions for the convergence (in Section 7) and for the smoothness (in Section 8) are derived. The two main ingredients of this analysis (which explains the title of the paper) are: (i) control {{of the size of the}} Lipschitz constants in grid intervals of the univariate net function and/or their derivatives; (ii) proximity to a point subdivision scheme. Finally, the closing section (Section 10) introduces the new family of blending spline-type net subdivision schemes as an example which is analyzed, with great detail, by using the tool...|$|R
40|$|We {{consider}} a multi-cell, frequency-selective fading, uplink channel (network MIMO) where K user terminals (UTs) communicate simultaneously with B cooperative base stations (BSs). Although the potential benefit of multi-cell cooperation grows with B, the overhead {{related to the}} acquisition of channel state information (CSI) will rapidly dominate the uplink resource. Thus, there exists a non-trivial tradeoff between the performance gains of network MIMO and the related overhead in channel estimation for a finite coherence time. Using a close <b>approximation</b> of the <b>net</b> ergodic achievable rate based on recent results from random matrix theory, we study this tradeoff by taking some realistic aspects into account such as unreliable backhaul links and different path losses between the UTs and BSs. We determine the optimal training length, the optimal number of cooperative BSs and the optimal number of sub-carriers {{to be used for}} an extended version of the circular Wyner model where each UT can communicate with B BSs. Our results provide some insight into practical limitations as well as realistic dimensions of network MIMO systems. Comment: The paper has undergone a major revision during which the title was changed to: "Optimal Channel Training in Uplink Network MIMO Systems...|$|R
40|$|We {{consider}} collective oscillations of neutrinos, {{which are}} emergent nonlinear flavor evolution phenomena instigated by neutrino-neutrino interactions in astrophysical environments with sufficiently high neutrino densities. We investigate the symmetries {{of the problem}} in the full three flavor mixing scheme and in the exact many-body formulation by including the effects of CP violation and neutrino magnetic moment. We show that, similar to the two flavor scheme, several dynamical symmetries exist for three flavors in the single-angle <b>approximation</b> if the <b>net</b> electron background in the environment and the effects of the neutrino magnetic moment are negligible. Moreover, we show that these dynamical symmetries are present even when the CP symmetry is violated in neutrino oscillations. We explicitly write down the constants of motion through which these dynamical symmetries manifest themselves in terms of the generators of the SU(3) flavor transformations. We also show that the effects due to the CP-violating Dirac phase factor out of the many-body evolution operator and evolve independently of nonlinear flavor transformations if neutrino electromagnetic interactions are ignored. In the presence of a strong magnetic field, CP-violating effects can still be considered independently provided that an effective definition for neutrino magnetic moment is used...|$|R
40|$|The basic {{problems}} {{of the theory of}} homogeneous difference schemes for linear, quasi-linear aud non-linear equations of parabolic type have been studied in a number of works ([ll- 151). The stability, convergence, and also estimates of the rate of convergence (order of accuracy) of several families of homogeneous difference schemes in the classes of continuous aud ~s~tinu ~ coefficients of the differential equation have been established. In [sl attention was paid to the fact that on an arbitrary sequence of non-uniform nets difference schemes which have second order <b>approximation</b> on uniform <b>nets</b> have only first order approxi-mation. For this reason the problem of the order of accuracy on non-uui-form nets requires special study. A family of homogeneous difference schemes on non-uniform nets for the equation L’k. 90 0,- iz (w$) -!?(5) U+m =o (14 was given in Es]. The definition of this family was the sBme {{as in the case of}} uniform nets with h~ogeneous difference schemes, studied in [?I. It was shown that the order of accuracy of these schemes on non-uniform nets is equal to the order of their accuracy on uniform nets both in the case of continuous coefficients in the differential equation and in the case of discontinuous coefficients. It is interesting to note that this result is obtained if we use a priori estimates of the ssme type as in the case of discontinuous coefficients [71 end a uniform net. l%e effective characteristic of the net is the mean square ste...|$|R
40|$|Technology scaling has {{increased}} the transistor 2 ̆ 7 s susceptibility to process variations in nanometer very large scale integrated (VLSI) circuits. The effects of such variations are having {{a huge impact on}} performance and hence the timing yield of the integrated circuits. The circuit optimization objectives namely power, area, and delay are highly correlated and conflicting in nature. The inception of variations in process parameters have made their relationship intricate and more difficult to optimize. Traditional deterministic methods ignoring variation effects negatively impacts timing yield. A pessimistic worst case consideration of variations, on the other hand, can lead to severe over design. In this context, there is a strong need for re-invention of circuit optimization methods with a statistical perspective. In this dissertation, we model and develop novel variation aware solutions for circuit optimization methods such as gate sizing, timing based placement and buffer insertion. The uncertainty due to process variations is modeled using interval valued fuzzy numbers and a fuzzy programming based optimization is proposed to improve circuit yield without significant over design. In addition to the statistical optimization methods, we have proposed a novel technique that dynamically detects and creates the slack needed to accommodate the delay due to variations. The variation aware gate sizing technique is formulated as a fuzzy linear program and the uncertainty in delay due to process variations is modeled using fuzzy membership functions. The timing based placement technique, on the other hand, due to its quadratic dependence on wire length is modeled as nonlinear programming problem. The variations in timing based placement are modeled as fuzzy numbers in the fuzzy formulation and as chance constraints in the stochastic formulation. Further, we have proposed a piece-wise linear formulation for the variation aware buffer insertion and driver sizing (BIDS) problem. The BIDS problem is solved at the logic level, with look-up table based <b>approximation</b> of <b>net</b> lengths for early variation awareness. In the context of dynamic variation compensation, a delay detection circuit is used to identify the uncertainty in critical path delay. The delay detection circuit controls the instance of data capture in critical path memory flops to avoid a timing failure in the presence of variations. In summary, the various formulation and solution techniques developed in this dissertation achieve significantly better optimization compared to related works in the literature. The proposed methods have been rigorously tested on medium and large sized benchmarks to establish the validity and efficacy of the solution techniques...|$|R
40|$|The {{two-dimensional}} {{thrust nozzle}} presents a challenging problem. The loading is not axisymmetric {{as in the}} case of a cone and the internal flow presents some design difficulties. A two-sting system has been chosen to accomodate the internal flow and achieve some symmetry. The situation is complicated by the fact that with the small ramp angle and the internal pressure on the nozzle walls, loading is predominantly transverse. Yet it is the axial thrust which is to be measured (i. e., the tensile waves propagating in the stings). Although bending stress waves travel at most at only 60 % of the speed of the axial stress waves, the system needs to be stiffened against bending. The second sting was originally only used to preserve symmetry. However, the pressures on each thrust surface may be quite different at some conditions, so at this stage the signals from both stings are being averaged as a first order <b>approximation</b> of the <b>net</b> thrust. The expected axial thrust from this nozzle is not large so thin stings are required. In addition, the contact area between nozzle and sting needs to be maximized. The result was that it was decided to twist the stings through 90 deg, without distorting their cross-sectional shape, just aft of the nozzle. Finite element analysis showed that this would not significantly alter the propagation of the axial stress wave in the sting, while the rigidity of the system is greatly increased. A Mach 4 contoured nozzle is used in the experiments. The thrust calculated by integrating the static pressure measurements on the thrust surfaces is compared with the deconvolved strain measurement of the net thrust for the cases of air only and hydrogen fuel injected into air at approximately 9 MJ/kg nozzle supply enthalpy. The gain in thrust due to combustion is visible in this result...|$|R
40|$|Potential {{evaporation}} (ETP) is a {{basic input}} for many hydrological and agronomic models, {{as well as a}} key variable in most actual evaporation estimations. It has been approached through several diffusive and energy balance methods, out of which the Penman-Monteith equation is recommended as the standard one. In order to deal with the diffusive approach, ETP must be estimated at a sub-diurnal frequency, as currently done in land surface models (LSMs). This study presents an improved method, developed in the ORCHIDEE LSM, which consists of estimating ETP through an unstressed surface-energy balance (USEB method). The results confirm the quality of the estimation which is currently implemented in the model (Milly, 1992). The ETP underlying the reference evaporation proposed by the Food and Agriculture Organization, FAO, (computed at a daily time step) has also been analysed and compared. First, a comparison for a reference period under current climate conditions shows that USEB and FAO's ETP estimations differ, especially in arid areas. However, they produce similar values when the FAO's assumption of neutral stability conditions is relaxed, by replacing FAO's aerodynamic resistance by that of the model's. Furthermore, if the vapour pressure deficit (VPD) estimated for the FAO's equation, is substituted by ORCHIDEE's VPD or its humidity gradient, the agreement between the daily mean estimates of ETP is further improved. In a second step, ETP's sensitivity to climate change is assessed by comparing trends in these formulations for the 21 st century. It is found that the USEB method shows a higher sensitivity than the FAO's. Both VPD and the model's humidity gradient, as well as the aerodynamic resistance have been identified as key parameters in governing ETP trends. Finally, the sensitivity study is extended to two empirical <b>approximations</b> based on <b>net</b> radiation and mass transfer (Priestley-Taylor and Rohwer, respectively). The sensitivity of these ETP estimates is compared to the one provided by USEB to test if simplified equations are able to reproduce the impact of climate change on ETP...|$|R
40|$|Dynamic Programming, Q-Iearning {{and other}} {{discrete}} Markov Decision Process solvers can be-applied to continuous d-dimensional state-spaces by quantizing the state space into {{an array of}} boxes. This is often problematic above two dimensions: a coarse quantization can lead to poor policies, and fine quantization is too expensive. Possible solutions are variable-resolution discretization, or function <b>approximation</b> by neural <b>nets.</b> A third option, which has been little studied in the reinforcement learning literature, is interpolation on a coarse grid. In this paper we study interpolation tech-niques that can result in vast improvements in the online behavior of the resulting control systems: multilinear interpolation, and an interpolation algorithm based on an interesting regular triangulation of d-dimensional space. We adapt these interpolators under three reinforcement learning paradigms: (i) offline value iteration with a known model, (ii) Q-Iearning, and (iii) online value iteration with a previously unknown model learned from data. We describe empirical results, and the resulting implications for practical learning of continuous non-linear dynamic control. 1 GRID-BASED INTERPOLATION TECHNIQUES Reinforcement learning algorithms generate functions that map states to "cost-t<r go " values. When dealing with continuous state spaces these functions must be approximated. The following approximators are frequently used: • Fine grids {{may be used in}} one or two dimensions. Above two dimensions, fine grids are too expensive. Value functions can be discontinuous, which (as we will see) can lead to su boptimalities even with very fine discretization in two dimensions. • Neural nets have been used in conjunction with TD [Sutton, 1988] and Q-Iearning [Watkins, 1989] in very high dimensional spaces [Tesauro, 1991, Crites and Barto, 1996]. While promising, it is not always clear that they produce the accurate value functions that might be needed for fine near-optimal control of dynamic systems, and the most commonly used methods of applying value iteration or policy iteration with a neural-net value func-tion are often unstable. [Boyan and Moore, 1995]...|$|R
40|$|In human running, the ankle, knee, and hip {{moments are}} known to play {{different}} roles to influence the dynamics of locomotion. A recent study of hip moments and several hip-based legged robots have revealed that hip actuation can significantly improve the stability of locomotion, whether controlled or uncontrolled. Ankle moments are expected to also significantly affect running stability, but {{in a different way}} than hip moments. Here we seek to advance the current theory of dynamic running and associated legged robots by determining how simple open-loop ankle moments could affect running stability. We simulate a dynamical model, and compare it with a previous study on the role of hip moments. The model is relatively simple with a rigid trunk and a springy leg to represent the effective stiffness of the knee. At the hip we use a previously established proportional and derivative controlled moment with pitching angle as feedback. At the ankle we use the simplest ankle actuation, a constant ankle torque as a rough <b>approximation</b> of the <b>net</b> positive work done by the ankle moment during human locomotion. Even in this simplified model, we find that ankle and hip moments can affect the center of mass (COM) and pitching dynamics in distinct ways. Analysis of the governing equations shows that hip moments can directly influence the upper body balance, as well as indirectly influence the center of mass translation dynamics. However, ankle moments can only indirectly influence both. Simulation of the governing equations shows that the addition of ankle moment has significant benefits to the quality of locomotion stability, such as a larger basin of attraction. We also find that adding the ankle moments generally expands the range of parameters and velocities for which the model displays stable solutions. Overall, these findings suggest that ankle moments would {{play a significant role in}} improving the quality and range of running stability in a system with a rigid trunk and a telescoping leg, which would be a natural extension of current springy leg robots. Further, these results provide insights into the role that ankle moments might play in human locomotion...|$|R
40|$|Monte Carlo {{radiative}} transfer methods are employed here {{to estimate the}} plane-parallel albedo bias for marine stratocumulus clouds. This is the bias in estimates of the mesoscale-average albedo, which arises from the assumption that cloud liquid water is uniformly distributed. The authors compare such estimates with those based on a more realistic distribution generated from a fractal model of marine stratocumulus clouds belonging to the class of 'bounded cascade' models. In this model the cloud top and base are fixed, so that all variations in cloud shape are ignored. The model generates random variations in liquid water along a single horizontal direction, forming fractal cloud streets while conserving the total liquid water in the cloud field. The model reproduces the mean, variance, and skewness of the vertically integrated cloud liquid water, {{as well as its}} observed wavenumber spectrum, which is approximately a power law. The Monte Carlo method keeps track of the three-dimensional paths solar photons take through the cloud field, using a vectorized implementation of a direct technique. The simplifications in the cloud field studied here allow the computations to be accelerated. The Monte Carlo results are compared to those of the independent pixel <b>approximation,</b> which neglects <b>net</b> horizontal photon transport. Differences between the Monte Carlo and independent pixel estimates of the mesoscale-average albedo are on the order of 1 % for conservative scattering, while the plane-parallel bias itself is an order of magnitude larger. As cloud absorption increases, the independent pixel approximation agrees even more closely with the Monte Carlo estimates. This result holds {{for a wide range of}} sun angles and aspect ratios. Thus, horizontal photon transport can be safely neglected in estimates of the area-average flux for such cloud models. This result relies on the rapid falloff of the wavenumber spectrum of stratocumulus, which ensures that the smaller-scale variability, where the {{radiative transfer}} is more three-dimensional, contributes less to the plane-parallel albedo bias than the larger scales, which are more variable. The lack of significant three-dimensional effects also relies on the assumption of a relatively simple geometry. Even with these assumptions, the independent pixel approximation is accurate only for fluxes averaged over large horizontal areas, many photon mean free paths in diameter, and not for local radiance values, which depend strongly on the interaction between neighboring cloud elements...|$|R

