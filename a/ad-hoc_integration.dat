19|17|Public
40|$|Abstract — Service-oriented {{architectures}} {{are among}} the premier middleware approaches to coping with the dynamicity of ubiquitous computing environments. In this article, we propose {{a new way of}} integrating services, namely ad-hoc service integration. For a certain class of services, <b>ad-hoc</b> <b>integration</b> is capable of automatically combining services at run time. This allows generating new functionalities from services newly appearing or already being available in the Ubicomp environment, but leaving the services ’ interfaces unchanged. This way, the extension in service functionality can be kept transparent to applications or users of a service. Nevertheless, our service-integration framework can more precisely distinguish among services. To show the feasibility of <b>ad-hoc</b> <b>integration,</b> we have implemented our service-integration framework based on OSGi/Felix along with a toolkit providing two different techniques to realize the ad-hoc integration: Redirection, i. e. calling interfaces and replication, i. e. copying implementations of services. A first evaluation verifies the viability of our work 1. I...|$|E
40|$|International audienceReal {{problems}} in software evolution render impossible a fixed, one-size-fits-all approach, and {{these problems are}} usually solved by gluing together various tools and languages. Such <b>ad-hoc</b> <b>integration</b> is cumbersome and costly. With the Rascal meta-programming language the Software Analysis and Transformation research group at CWI explores whether it is feasible to develop an approach that offers all necessary meta-programming and visualization techniques in a completely integrated language environment. We have applied Rascal with success in constructing domain specific languages and experimental refactoring and visualization tools...|$|E
40|$|Seamless {{interoperability}} across {{system boundaries}} and optimally tailored solutions for customers are dominating {{concerns in the}} vision of future office infrastructures. This means {{to change the way}} how office systems are developed from today's <b>ad-hoc</b> <b>integration</b> of independent systems towards a collaborative approach for adequate systems of systems. In this paper, we propose a development method based on product line engineering to support the efficient production of customized solutions. The interoperability issues are tackled by a collaboration model separating solution providers and suppliers and by a reference architecture defining clear specifications of the components to be integrated. Together with our production method, interoperability in complex systems of systems is supported by construction...|$|E
40|$|Abstract. Heterogeneous {{specification}} {{becomes more}} and more important because complex systems are often specified using multiple viewpoints, involving multiple formalisms. Moreover, a formal software development process may lead to a change of formalism during the development. However, current research in integrated formal methods only deals with <b>ad-hoc</b> <b>integrations</b> of different formalisms. The heterogeneous tool set (Hets) is a parsing, static analysis and proof management tool combining various such tools for individual specification languages, thus providing a tool for heterogeneous multi-logic specification. Hets is based on a graph of logics and languages (formalized as so-called institutions), their tools, and their translations. This provides a clean semantics of heterogeneous specification, as well as a corresponding proof calculus. For proof management, the calculus of development graphs (known from other large-scale proof management systems) has been adapted to heterogeneous specification. Development graphs provide an overview of the (heterogeneous) specification module hierarchy and the current proof state, and thus may be used for monitoring the overall correctness of a heterogeneous development. ...|$|R
40|$|The {{purpose of}} this paper is twofold: First, a setting for the formal {{verification}} of transformations is presented. Second, an example verification of the classical "Linear Recursive Function to Tail Recursive Function" [BW 84] is actually performed with the generic theorem prover Isabelle [Paul 89]. The first part is oriented towards people who look for a practical, safe method to develop and verify transformations for program developments within their preferred framework, the second part addresses to experienced beginners of Isabelle (such as me) or both. I. Introduction The need for proving the correctness of transformations is at the core of formal transformational development such as CIP-L and PROSPECTRA (see [CIP 85], [HK 93]). This need always arises if one starts to implement such "transformations well known from the literature" into a new deductional framework, a specification language or logic. <b>Ad-hoc</b> <b>integrations</b> turned out to be extremely dangerous not only in theory, but [...] ...|$|R
40|$|Sharing data among {{disparate}} databases {{has so far}} mostly {{been achieved}} through some form of <b>ad-hoc</b> schema <b>integration.</b> This approach becomes less tractable {{as the number of}} participating database increases. In this paper,we first present the primary problems to be resolved in large multidatabase systems and discuss the autonomy and heteroheneity of large multidatabase systems and then describe a coalition agent mechanism to implement large multidatabase systems interoperation...|$|R
40|$|In {{this paper}} we sketch a {{framework}} supporting contract enactment {{within the context}} of virtual organisation units that are dynamically created in order to achieve a common objective by securely sharing resources, services and information. The framework is built on top of a joint extension of the policy deployment architecture for peer-to- peer communities that we proposed in [1] and the contract enactment capability described in [6] that enables monitoring, mediation, arbitration and enforcement of electronic contracts in multiple, simultaneous closed collaborations. A longer-term goal is to deliver a scalable method of setting up contract enforcement and contract performance management infrastructures for inter-organisational information systems that allow the on-demand creation and dynamic evolution of secure Virtual Organisations based on the <b>ad-hoc</b> <b>integration</b> of systems across Enterprise boundaries...|$|E
40|$|The {{popularity}} of Resource Oriented and RESTful Web Services is increasing rapidly. In these, resources are key {{actors in the}} interfaces, in contrast to other approaches where services, messages or objects are. This distinctive feature necessitates a new approach for modelling RESTful interfaces providing a more intuitive mapping from model to implementation than could be achieved with non-resource methods. With this objective we propose an approach to describe Resource Oriented and RESTful Web Services based on UML collaboration diagrams. Then use it to model scenarios from several problem domains, arguing that Resource Oriented and RESTful Web Services {{can be used in}} systems which go beyond <b>ad-hoc</b> <b>integration.</b> Using the scenarios we demonstrate how the approach is useful for: eliciting domain ontologies; identifying recurring patterns; and capturing static and dynamic aspects of the interface...|$|E
40|$|XQuery is {{increasingly}} {{being used for}} <b>ad-hoc</b> <b>integration</b> of heterogeneous data sources that are logically mapped to XML. For example, scientists need to query multiple scientific databases, which are distributed over a large geographic area, {{and it is possible}} to use XQuery for that. However, the language currently supports only the data shipping query evaluation model (through the document() function) : it fetches all data sources to a single server, then runs the query there. This is a major limitation for many applications, especially when some data sources are very large, or when a data source is only a virtual XML view over some other logical data model. We propose here a simple extension to XQuery that allows query shipping to be expressed in the language, in addition to data shipping. Example 1. 1 For a simple illustration, consider the following example...|$|E
5000|$|Planning is of {{poor quality}} and is {{generally}} a mere collection of schemes and works, {{many of the}} works suggested by elected panchayat members themselves is an <b>ad-hoc</b> manner. <b>Integration</b> of Gram and Taluk Panchayat plans into the District plan, even when done, also tends to be mere summation and not a synergistic integration. This is further distorted by placing funds with MPs and MLAs, whose utilization falls outside the pale of any planning.|$|R
40|$|Abstract — This paper {{describes}} the <b>Ad-hoc</b> network <b>integration</b> architecture being developed inside the IST project Daidalos. This architecture supports the efficient delivery of services, unicast and multicast, legacy and multimedia, to users {{connected to the}} ad-hoc network. For this purpose, several functionalities {{need to be in}} place. First, efficient routing and mobility mechanisms are proposed to enable mobility of users inside and between ad-hoc networks with decreased overhead. Second, distributed QoS mechanisms need to be developed to support service differentiation and resources control responsive to nodes mobility. Finally, security, charging and rewarding mechanisms are proposed to guarantee that only authorized users access the requested services, to increase the operators interest, and to ensure the correct behaviour of the users in the ad-hoc network...|$|R
40|$|Abstract. A Service Oriented Architecture {{promises}} a high potential for service integration. Business applications built with the SOA paradigm {{should be able}} to change and integrate additional business logic easily. However, replacing a service and integrating another service often needs to cope with interface mismatches. This requires transforming exchanged messages, and in general an efficient support for integration management. In this paper we introduce an automated approach for the management of service integration. It is based on a new Mediation Contract Extension (MECE) that allows semantic annotation for XSD type definition inside WSDL documents. Our discovery algorithm deduces automatically XSLT stylesheets in order to allow mediation between heterogeneous Web Ser-vice interfaces. This used in a distributed management framework to manage dynamic <b>ad-hoc</b> service <b>integration</b> with semantically related but syntactically different messages. The discovery engine along with XSLT stylesheet creation enables autonomous dynamic self-reconfiguration of the whole system. ...|$|R
40|$|Abstract. One of {{the salient}} {{features}} of Service-Oriented Architectures is that services can be deployed and removed at runtime. But service replacement and management for service processes is a demanding task in complex IT-Systems, especially under additional constraints like optimizing the Quality of Service of a service process. A self-managing system is desired but missing. We have already achieved self-healing and self-optimization with our service brokering system. We apply ontologies to discover service alternatives and their QoS. In this paper {{we not only}} present our work on quality-aware service discovery but also propose a self-manageable infrastructure for service processes. The infrastructure can be dynamically instantiated, configured and bound to management endpoints with semantic service discovery. The approach not only automates the binding of service management systems of multiple vendors but also the SLA monitoring and the <b>ad-hoc</b> <b>integration</b> of services in service processes. ...|$|E
40|$|The {{presented}} thesis {{provides a}} new approach for the integration of heterogeneous data in contexts, which (1) require data to remain in their original structural and semantic form {{in order to allow}} a dynamic <b>ad-hoc</b> <b>integration</b> based on individual use-cases and (2) need a particular focus on lexical and syntactical patterns, which are only bound to the instance-level of data and thus not covered by schema level integration. Based on preliminary work, the concept of a data transformation framework is developed on the contextual base of the formal interpretation of semi-structured schemata as regular tree grammars. As the content of the terminal nodes has been identified to be often not in an atomic form, the rule framework is applied on contained data in order to describe implicitly existing patterns and translate the data into semantically enriched forms...|$|E
40|$|One {{of the key}} {{technologies}} in future air traffic management is air-to-air communication. Especially data links, that is, direct wireless transmission of digital data will {{play a major role}} for air traffic applications like <b>ad-hoc</b> <b>integration</b> in air spaces, separation, collision avoidance and landing. Recently, the avionics engineering center of the Ohio University has developed a high-precision navigation solution for use in these areas, especially collision avoidance. Prerequisite for this is the availability of a reliable data link. DLR deals with the development and simulation of future data links within the project Air 2 Air. Working together, Ohio University and DLR have carried out a series of flight experiments in late 2012 and start of 2013, during which data link performance was recorded and evaluated with a special emphasis on transmission of high-framerate, high-precision navigational information. In this paper we will present some results from these recordings together with considerations concerning future applications of data links for collision avoidance methods...|$|E
40|$|Abstract—This paper proposes an Ad-hoc QoS architecture, {{using as}} basis some {{concepts}} from SWAN, extending it to fulfill our requirements. It also proposes the modules {{required in the}} network elements and its interaction to provide optimized QoS in the <b>ad-hoc</b> network and <b>integration</b> between both networks. We also present the overall integration QoS signaling protocol and the differentiation mechanisms to address end-to-end QoS for real-time multimedia applications. Furthermore, the proposed solution addresses the use of multipath routing {{in order to provide}} load balancing and increased network reliability. Index Terms—Ad-hoc networks, infrastructure networks, integration, quality of service. ...|$|R
40|$|The {{objective}} of this work is to place the present anthropology reasoning on the deliberation of penal dogmatics about crime, mostly because of its practical consequences which involve members of ethnic minorities. Kalinsky maintains, on one hand, that a democratic State does not imply an homogeneous way of setting forth and resolving life matters {{and on the other}} hand, that specific factors of a group of people must not give cause for going deeper into discrimination and exclusion of the general regulatory structures. There must be neither juridical isolation (penal rights <b>ad-hoc),</b> nor homogenous <b>integration</b> (ignorance of cultural difference), because none of these two attitudes respect cultural plurality...|$|R
40|$|Wireless {{networks}} {{have been widely}} deployed {{in recent years to}} provide high-speed Internet access to mobile users. In traditional IEEE 802. 11 wireless LANs, all users directly connect to an access point (AP) and all packets are forwarded by the AP. As a result, the coverage and capacity of the network is limited. If ad hoc mode is adopted in both the AP and mobile nodes, the one hop connections from AP can be extended to multiple hops. Such arhitecture, termed WIANI (Wireless Infrastructure and <b>Ad-Hoc</b> Network <b>Integration)</b> in this paper, is able to extend the network coverage beyond the coverage of APs. Furthermore, the users may take advantage of the ad hoc connections to forward local data and hence alleviating the traffic load through the AP and increasing the network capacity. In this paper, we propose a dynamic load-balancing protocol for WIANI in which all APs and nodes operate in ad hoc mode. Our protocol consists of two parts: the load-balancing zone forming and weighted x - hop routing algorithms. Using simulation, we show that our protocol improves system throughput and reduce packet delivery delay. © 2005 IEEE...|$|R
40|$|Facing {{with growing}} data volumes and deeper {{analysis}} requirements, current development of Business Intelligence (BI) and Data warehousing systems (DWHs) is a challenging and complicated task, which largely involves in <b>ad-hoc</b> <b>integration</b> and data re-engineerng. This arises an increasing requirement for a scalable application framework {{which can be}} used for the implementation and administration of diverse BI appliations in a straight forward and cost-efficient way. In this context, this paper presents a large-scale application framework for standardized BI applications, suporting the ability to define and construct data warehouse processes, new data analytics capabilities as well as to support the deployment requirements of multiscalable front-end applications. The core of the framework consists of defined metadata repositories with pre-built and function specific information templates as well as application definition. Moreover, the application framework is also based on workflow mechanisms for developing and running automatic data processing tasks. Hence, the framework is capable of offering an unified reference architecture to end users, which spans various aspects of development lifecycle and can be adapted or extended to better meet application- specific BI engineering process...|$|E
40|$|Spreadsheets {{contain a}} huge amount of high-value data but do not observe a {{standard}} data model and thus are difficult to integrate. A large number of data integration tools exist, but they generally can only work on relational data. Existing systems for extracting relational data from spreadsheets are too labor intensive to support <b>ad-hoc</b> <b>integration</b> tasks, in which the correct extraction target is only learned during the course of user interaction. This paper introduces a system that automatically extracts relational data from spreadsheets, thereby enabling relational spreadsheet integration. The resulting integrated relational data can be queried directly or can be translated into RDF triples. When compared to standard techniques for spreadsheet data extraction on a set of 100 random Web spreadsheets, the system reduces the amount of human labor by 72 % to 92 %. In addition to the system design, we present the results of a general survey of more than 400, 000 spreadsheets we downloaded from the Web, giving a novel view of how users organize their data in spreadsheets. 1...|$|E
40|$|Part 3 : Emerging Topics in EISInternational audienceFacing {{with growing}} data volumes and deeper {{analysis}} requirements, current development of Business Intelligence (BI) and Data warehousing systems (DWHs) is a challenging and complicated task, which largely involves in <b>ad-hoc</b> <b>integration</b> and data re-engineering. This arises an increasing requirement for a scalable application framework {{which can be}} used for the implementation and administration of diverse BI applications in a straight forward and cost-efficient way. In this context, this paper presents a large-scale application framework for standardized BI applications, supporting the ability to define and construct data warehouse processes, new data analytics capabilities as well as to support the deployment requirements of multi scalable front-end applications. The core of the framework consists of defined metadata repositories with pre-built and function specific information templates as well as application definition. Moreover, the application framework is also based on workflow mechanisms for developing and running automatic data processing tasks. Hence, the framework is capable of offering an unified reference architecture to end users, which spans various aspects of development lifecycle and can be adapted or extended to better meet application-specific BI engineering process...|$|E
40|$|The nascent data grid {{technology}} aims to make access to globally distributed structured and semi-structured databases possible. However, only integrated, transparent {{access to such}} data sources, abstracting not only from distribution and implementation details, {{but also from the}} diverse forms of heterogeneity, allows maximum leverage for the envisioned dynamic forms of inter-organisational cooperation. Traditional integration technologies rely on a human-driven development process and are therefore not applicable to the <b>ad-hoc</b> style of <b>integration</b> required in these scenarios. PALADIN aims to automate integration planning by using patterns that capture expert knowledge in a machine-processable way. A pattern describes a generic problem constellation encountered in information integration and provides a guideline to its solution, described as transformations on a graph-oriented representation of the data source schemas and their data...|$|R
40|$|Abstract — Wireless {{networks}} {{have been widely}} deployed {{in recent years to}} provide high-speed Internet access to mobile users. In traditional IEEE 802. 11 wireless LANs, all users directly connect to an access point (AP) and all packets are forwarded by the AP. As a result, the coverage and capacity of the network is limited. If ad hoc mode is adopted in both the AP and mobile nodes, the one hop connections from AP can be extended to multiple hops. Such arhitecture, termed WIANI (Wireless Infrastructure and <b>Ad-Hoc</b> Network <b>Integration)</b> in this paper, is able to extend the network coverage beyond the coverage of APs. Furthermore, the users may take advantage of the ad hoc connections to forward local data and hence alleviating the traffic load through the AP and increasing the network capacity. In this paper, we propose a dynamic load-balancing protocol for WIANI in which all APs and nodes operate in ad hoc mode. Our protocol consists of two parts: the load-balancing zone forming and weighted x−hop routing algorithms. Using simulation, we show that our protocol improves system throughput and reduce packet delivery delay. Index Terms — Multi-hop wireless LAN, load-balancing routing, infrastructure network, ad hoc network, IEEE 802. 1...|$|R
40|$|Following theOpen Data trend, {{governments and}} public {{agencies}} have started making their data {{available on the}} Web and established platforms such as data. gov or data. un. org. These Open Data platforms provide {{a huge amount of}} data for various topics such as demographics, transport, finance or health in various data formats. One typical usage scenario for this kind of data is their integration into a database or data warehouse in order to apply data analytics. However, in today’s business intelligence tools there is an evident lack of support for so-called situational or <b>ad-hoc</b> data <b>integration.</b> In this demonstration we will therefore present DrillBeyond, a novel database and information retrieval engine which allows users to query a local database as well as the Web of Open Data in a seamless and integrated way with standard SQL. The audience will be able to pose queries to our Drill-Beyond system which will be answered partly from local data in the database and partly from datasets that originate from the Web of Data. We will show how such queries are divided into known and unknown parts and how missing attributes are mapped to open datasets. We will demonstrate the integration of the open datasets back into the DBMS in order to apply its analytical features. 1...|$|R
40|$|The {{revolution}} of network centric operations will be achieved incrementally. Information Management is centric to this seamless transition of warfighter capabilities across enclaves, services, and platforms. Air Force Research Lab has teamed with industry, defense, and academic leaders {{to research and}} develop technologies to solve the information management needs of the DoD. Joint Battlespace Infosphere Mercury is an AFRL initiative to develop information management software using the best of breed commercial technologies. This session describes (1) the capabilities and goals of JBI Mercury, (2) design highlights and lessons learned, and (3) how to obtain a license-cost free copy of JBI Mercury and collaborate with the JBI community. JBI Mercury provides information management services for publish, subscribe, and query for XML/Binary information across a local or wide area network using Java, C#, and web services bindings. Multiple, dynamic messaging connectors and discovery technologies are mediated by a www. openwings. org compliant framework. The JBI Mercury services are dynamically connectable (dynamic plug-and-play connectors), dynamically configurable to meet mission requirements (context policies), and dynamically replaceable (technology change). Flexibility and reliability are provided by the ability to achieve <b>ad-hoc</b> <b>integration</b> of new users and new service components as they enter or leave the network...|$|E
40|$|Abstract. Increasing {{availability}} of RDF data covering different domains is en-abling <b>ad-hoc</b> <b>integration</b> {{of different kinds}} of data to suit varying needs. This usually results in large collections of data such as the Billion Triple Challenge datasets or SNOMED CT, that are not just “big ” in the sense of volume but also “big ” in variety of property and class types. However, techniques used by most RDF data processing systems fail to scale adequately in these scenarios. One major reason is that the storage models adopted by most of these systems, e. g., vertical partitioning, do not align well with the semantic units in the data and queries. While Big Data distributed processing platforms such as the Hadoop-based platforms offer the promise of “unlimited scale-out processing”, there are still open questions as to how best to physically partition and distribute RDF data for optimized distributed processing. In this poster, we present the idea of a semantics-oriented RDF storage model that partitions data into logical units that map to subqueries in graph patterns. These logical units can be seen as equiva-lence classes of star subgraphs in an RDF graph. This logical partitioning strategy enables more aggressive pruning of irrelevant query results by pruning irrelevant partitions. It also enables the possibility of semantic-query optimization for some queries such as eliminating joins under appropriate conditions. These benefits in addition to appropriate techniques for physically partitioning the logical parti-tions, translate to improved performance as shown by some preliminary results...|$|E
40|$|Fusionplex is {{a system}} for {{integrating}} multiple heterogeneous and autonomous in-formation sources, that uses data fusion to resolve factual inconsistencies among the individual sources. To accomplish this, the system relies on source features, which are meta-data {{on the quality of}} each information source; for example, the recentness of the data, its accuracy, its availability, or its cost. The fusion process is controlled with several parameters: (1) With a vector of feature weights, each user defines an individual notion of data utility; (2) with thresholds of acceptance, users ensure min-imal performance of their data, excluding from the fusion process data that are too old, too costly, or lacking in authority, or data that are too high, too low, or obvious outliers; and, ultimately, (3) in naming a particular fusion function to be used for each attribute (for example, average, maximum, or simply any) users implement their own interpretation of fusion. Several simple extensions to SQL are all that is needed to allow users to state these resolution parameters, thus ensuring that the system is easy to use. Altogether, Fusionplex provides its users with powerful and flexible, yet simple, control over the fusion process. In addition, Fusionplex supports other critical inte-gration requirements, such as information source heterogeneity, dynamic evolution of the information environment, quick <b>ad-hoc</b> <b>integration,</b> and intermittent source avail-ability. The methods described in this paper were implemented in a prototype system that provides complete Web-based integration services for remote clients. ...|$|E
40|$|As mobile devices {{become more}} {{powerful}} and common there will a greater capacity and need for more complex interactions between mobile device-resident software and other server-based software. This can be termed mobile application integration. Mobile computing introduces new opportunities and challenges for application integration. Some of the challenges include small device size, limited bandwidth, limited processing power and lack of <b>integration</b> protocol support. <b>Ad-hoc</b> networks are those that can be formed temporarily while particular devices are {{in the vicinity of}} each other and <b>ad-hoc</b> mobile application <b>integration</b> refers to the temporary integration of software modules across devices while such a network exists. This case poses even greater challenges for integration such as dynamically changing combinations of platforms to be integrated across and a requirement for integration to occur dynamically. This paper proposes a novel Web Servicesbased system that allows such dynamic and temporary integration to occur in ad-hoc mobile networks...|$|R
40|$|Abstract: The Web {{consists}} of {{a huge number of}} documents, but also large amounts structured information, for example in the form of HTML tables containing relational-style data. One typical usage scenario for this kind of data is their integration into a database or data warehouse in order to apply data analytics. However, in today’s business intelligence tools there is an evident lack of support for so-called situational or <b>ad-hoc</b> data <b>integration.</b> In this demonstration we will therefore present DrillBeyond, a novel database and information retrieval engine which allows users to query a local database as well as the web datasets in a seamless and integrated way with standard SQL. The audience will be able to pose queries to our DrillBeyond system which will be answered partly from local data in the database and partly from datasets that originate from the Web of Data. We will demonstrate the integration of the web tables back into the DBMS in order to apply its analytical features. 1 Open-World SQL Queries The system we want to demonstrate offers a novel way of integrating web tables into reg-ular query processing in a relational database. We present a modified RDBMS that is able to answer so-called open-world queries which are not restricted to the schema of the local database. Instead the user is allowed to use arbitrary attribute names that do not appear in the original schema. Consider the following running example query...|$|R
40|$|Abstract: The Webconsists of ahuge {{number of}} documents, but also large amounts {{structured}} information, {{for example in}} the form of HTML tables containing relationalstyle data. One typical usage scenario for this kind of data is their integration into adatabase or data warehouse in order to apply data analytics. However, in today’s business intelligence tools there is an evident lack of support for so-called situational or <b>ad-hoc</b> data <b>integration.</b> In this demonstration we will therefore present DrillBeyond, anovel database and information retrieval engine which allows users to query alocal database as well as the web datasets in aseamless and integrated way with standard SQL. The audience will be able to pose queries to our DrillBeyond system which will be answered partly from local data in the database and partly from datasets that originate from the WebofData. We will demonstrate the integration of the web tables back into the DBMS in order to apply its analytical features. 1 Open-World SQL Queries The system we want to demonstrate offers anovel way of integrating web tables into regular query processing in arelational database. We present amodified RDBMS that is able to answer so-called open-world queries which are not restricted to the schema of the local database. Instead the user is allowed to use arbitrary attribute names that do not appear in the original schema. Consider the following running example query: SELECT population, n_name, AVG(o_totalprice) FROM nation JOIN region ON n_regionkey=r_regionkey JOIN customer ON n_nationkey=c_nationkey JOIN orders ON c_custkey=o_custke...|$|R
40|$|Buildings consume 40 % of Ireland's {{total annual}} energy {{translating}} to 3. 5 billion (2004). The EPBD directive (effective January 2003) places an onus on {{all member states}} to rate the energy performance of all buildings in excess of 50 m 2. Energy and environmental performance management systems for residential buildings do not exist and consist of an <b>ad-hoc</b> <b>integration</b> of wired building management systems and Monitoring & Targeting systems for non-residential buildings. These systems are unsophisticated and do not easily lend themselves to cost effective retrofit or integration with other enterprise management systems. It is commonly agreed that a 15 - 40 % reduction of building energy consumption is achievable by efficiently operating buildings when compared with typical practice. Existing research has identified {{that the level of}} information available to Building Managers with existing Building Management Systems and Environmental Monitoring Systems (BMS/EMS) is insufficient to perform the required performance based building assessment. The cost of installing additional sensors and meters is extremely high, primarily due to the estimated cost of wiring and the needed labour. From this perspective wireless sensor technology provides the capability to provide reliable sensor data at the required temporal and spatial granularity associated with building energy management. In this paper, a wireless sensor network mote hardware design and implementation is presented for a building energy management application. Appropriate sensors were selected and interfaced with the developed system based on user requirements to meet both the building monitoring and metering requirements. Beside the sensing capability, actuation and interfacing to external meters/sensors are provided to perform different management control and data recording tasks associated with minimisation of energy consumption in the built environment and the development of appropriate Building information models(BIM) to enable the design and development of energy efficient spaces...|$|E
40|$|Virtual 3 D city {{models are}} {{integrated}} complex compositions of spatial data of different themes, origin, quality, scale, and dimensions. Within this paper, we {{address the problem}} of spatial compatibility of geodata aiming to provide support for <b>ad-hoc</b> <b>integration</b> of virtual 3 D city models including geodata of different sources and themes like buildings, terrain, and city furniture. In contrast to related work which is dealing with the integration of redundant geodata structured according to different data models and ontologies, we focus on the integration of complex 3 D models of the same representation (here: CityGML) but regarding to the geometric-topological consistent matching of non-homologous objects, e. g. a building is connected to a road, and their geometric homogenisation. Therefore, we present an approach including a data model for a Geodata Join and the general concept of an integration procedure using the join information. The Geodata Join aims to bridge the lack of information between fragmented geodata by describing the relationship between adjacent objects from different datasets. The join information includes the geometrical representation of those parts of an object, which have a specific/known topological or geometrical relationship to another object. This part is referred to as a Connector and is either described by points, lines, or surfaces of the existing object geometry or by additional join geometry. In addition, the join information includes the specification of the connected object in the other dataset and the description of the topological and geometrical relationship between both objects, which is used to aid the matching process. Furthermore, the Geodata Join contains object-related information like accuracy values and restrictions of movement and deformation which are used to optimize the integration process. Based on these parameters, a functional model including a matching algorithm, transformation methods, and conditioned adjustment methods can be established in order to facilitate ad-hoc 3 D homogenisation for consistent 3 D city models...|$|E
40|$|Nowadays we need {{to access}} {{information}} anytime from anywhere. This greatly increases our productivity and our quality of life. Previously web services were physically tied up to servers but wireless web services {{have proved to be}} very convenient and this convenience has been the most important factor behind the adoption of wireless technology. An example to demonstrate this might be the use of a wireless web service in a supermarket to get useful information and this is achieved by using an easy, client-side wireless web service to get the core web services required. This paper contains a description of an ad-hoc wireless web service that has been developed for a supermarket and will investigate upon the suitability of web services for implementing <b>ad-hoc</b> mobile application <b>integration.</b> Basically when a customer equipped with a Wi-Fi enabled device enters a region of the supermarket within range of wireless access points, he/she will be offered a list of services that are offered by the supermarket. Some of these services might include information about products at a discounted price, queries about particular products, marketing of new products being offered, location of particular items on shelves, identification of new point of sales just opened, just to name a few. This can be potentially beneficial especially considering the number of persons now having smart phones and mobile devices...|$|R
40|$|The {{paper is}} focused on {{necessary}} metadata definitions, playing the {{key role in the}} Grid construction. We briefly describe a realistic Grid development scenario and necessary programming and metamodel abstractions. 1. Data Grid development: assumed scenario In our research, we follow some patterns of federated database construction and extend them towards active objects and flexible (possibly – multi level) data source composition. From a practical point of view, we are skeptical about the feasibility of <b>ad-hoc</b> (or dynamic) <b>integration</b> solutions (except for perhaps very simple data structures). Thus, for any serious Grid project, a full development cycle and precise rules obliging every participant are required. We assume the following Grid construction scenario: 1. Strategic phase – a decision on creating a Grid is made, creates an initial integration analysis 2. Analysis phase – existing resources are elaborated and confronted with business requirements 3. Design phase – a precise definition of all schemas involved in Grid, defines roles of participants. 4. Finalization phase – participants sign the final agreement; all accept their roles and requirements. 5. Implementation phase – creation of all the necessary data adaptations described by all schemas. The resulting specifications determine the required form of data provided both by the global service (the Grid itself), as well as each of the participants. The task of adjusting local data into the form required by the Grid is distributed between participants and the integrator. It is performed by virtual (that is, not materialized) object views [3]. The views are necessary both at the Grid level (we call them global views) for mapping multiple contrib...|$|R

