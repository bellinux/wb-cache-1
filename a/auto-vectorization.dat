35|0|Public
25|$|Use <b>auto-vectorization</b> {{supported}} by GCC for loops via the optimization option -ftree-vectorize. The advantage of <b>auto-vectorization</b> {{is that the}} compiler can recognize scalar variables (which can be integer, fixed-point, or floating-point types) in order to utilize SIMD instructions automatically. In the ideal case, when <b>auto-vectorization</b> is used, {{there is no need}} to use SIMD variables explicitly.|$|E
50|$|This pragma {{gives the}} {{compiler}} permission to vectorize a loop {{even in cases}} where <b>auto-vectorization</b> might fail. It is {{the simplest way to}} manually apply vectorization.|$|E
5000|$|Vectorization Advisor {{supports}} {{analysis of}} scalar, SSE, AVX, AVX2 and AVX-512-enabled codes generated by Intel, GNU and Microsoft compilers <b>auto-vectorization.</b> It also supports analysis of [...] "explicitly" [...] vectorized codes which use OpenMP 4.x or Intel Cilk Plus {{as well as}} codes or written using C vector intrinsics or assembly language. Intel Advisor includes following main features: ...|$|E
50|$|Early {{computers}} generally had one {{logic unit}} that sequentially executed one instruction on one operand pair at a time. Computer programs and programming languages were accordingly designed to execute sequentially. Modern computers can do many things at once. Many optimizing compilers feature <b>auto-vectorization,</b> a compiler feature where particular parts of sequential programs are transformed into equivalent parallel ones, to produce code which will well utilize a vector processor. For a compiler to produce such efficient code for {{a programming language}} intended for use on a vector-processor would be much simpler, but, as much real-world code is sequential, the optimization is of great utility.|$|E
50|$|Theoretically Larrabee's x86 {{processor}} cores {{were able to}} run existing PC software, or even operating systems. A different version of Larrabee might sit in motherboard CPU sockets using QuickPath, but Intel never announced any plans for this. Though Larrabee Native's C/C++ compiler included <b>auto-vectorization</b> and many applications were able to execute correctly after having been recompiled, maximum efficiency was expected to have required code optimization using C++ vector intrinsics or inline Larrabee assembly code. However, as in all GPGPU, not all software would have benefited from utilization of a vector processing unit. One tech journalism site claims that Larrabee graphics capabilities were planned to be integrated in CPUs based on the Haswell microarchitecture.|$|E
5000|$|Recent {{versions}} of the GNU Compiler Collection (GCC), IBM VisualAge compiler and other compilers provide intrinsics to access VMX/AltiVec instructions directly from C and C++ programs. As of version 4, the GCC also includes <b>auto-vectorization</b> capabilities that attempt to intelligently create VMX/Altivec accelerated binaries {{without the need for}} the programmer to use intrinsics directly. The [...] "vector" [...] type keyword is introduced to permit the declaration of native vector types, e.g., [...] "" [...] declares a 128-bit vector variable named [...] "foo" [...] containing sixteen 8-bit unsigned chars. The full complement of arithmetic and binary operators is defined on vector types so that the normal C expression language can be used to manipulate vector variables. There are also overloaded intrinsic functions such as [...] "" [...] that emit the appropriate op code based on the type of the elements within the vector, and very strong type checking is enforced. In contrast, the Intel-defined data types for IA-32 SIMD registers declare only the size of the vector register (128 or 64 bits) {{and in the case of}} a 128-bit register, whether it contains integers or floating point values. The programmer must select the appropriate intrinsic for the data types in use, e.g., [...] "" [...] for adding two vectors containing eight 16-bit integers.|$|E
40|$|Abstract. We {{describe}} an <b>auto-vectorization</b> approach for the SPADE stream processing programming language, comprising two ideas. First, we {{provide support for}} vectors as a primitive data type. Second, we provide a C++ library with architecture-specific implementations {{of a large number}} of pre-vectorized operations as the means to support language extensions. We evaluate our approach with several stream processing operators, contrasting SPADE’s <b>auto-vectorization</b> with the native <b>auto-vectorization</b> provided by the GNU gcc and Intel icc compilers. ...|$|E
40|$|We {{describe}} language- and code generation-based {{approaches to}} {{providing access to}} architecture-specific vectorization support for high-performance data stream processing applications. We provide an experimental performance evaluation of several stream operators, contrasting our code generation approach with the native <b>auto-vectorization</b> support available in the GNU gcc and Intel icc compilers...|$|E
40|$|Although Single Instruction Multiple Data (SIMD) {{units are}} {{available}} in general purpose processors already since the 1990 s, state-of-the-art compilers are often still not capable to fully exploit them, i. e., they may miss to achieve the best possible performance. We present a new hardware-aware and adaptive loop tiling approach {{that is based on}} polyhedral transformations and explicitly dedicated to improve on <b>auto-vectorization...</b>|$|E
30|$|This section {{describes}} the optimizations proposed {{in this work}} that are implemented {{on top of the}} baseline BFs IP lookup algorithm as well as its parallelization targeting the Intel Phi. The parallel CPU version employs similar parallelization strategies, but differs with respect to the instruction-level parallelism that used <b>auto-vectorization.</b> The baseline implementation on which our work is built incorporates the following optimizations: the use of CBF to allow FIB updates, asymmetric memory allocation proposed in [17], and CPE {{to reduce the number of}} required data structures.|$|E
40|$|Parallelism dominates modern {{hardware}} design, from multi-core CPUs to SIMD and GPGPU. This {{bring with}} it, however, {{a need to}} program this hardware in a programmer-friendly manner. Tra-ditionally, managed languages like Java have struggled to take ad-vantage of data-parallel hardware, but projects like Aparapi provide a programming model that lets the programmer easily express the parallelism within their code, while still programming in a high-level language. This work takes advantage of this programmer-specified paral-lelism to perform source-level <b>auto-vectorization,</b> an optimization that is rarely considered in Java compilation. This is done using a source-to-source <b>auto-vectorization</b> transformation on the Aparapi Java program and a JNI vector library that is pre-compiled {{to take advantage of}} available SIMD instructions. This replaces the exist-ing Aparapi fallback path, for when no OpenCL device exists or if that device has insufficient memory for the program. We show that for all ten benchmarks tested the auto-vectori-zation tool produced an implementation that was able to beat the default Aparapi fallback path by a factor of 4. 56 x or 3. 24 x on average for a desktop and a server system respectively. In addition it was found that this improved fallback path even outperformed the GPU implementation for six of the ten benchmarks...|$|E
40|$|This {{bachelor}} thesis {{deals with}} support of automatic vectorization of code in the LLVM compilation framework and with extension of Codix processor model of SIMD instructions. As a result, LLVM {{is able to}} create reports {{about the process of}} <b>auto-vectorization</b> and it is possible to use special pragma directives to provide the compiler with additional information for optimizations of programs. Also a way of providing information about architectures of processors created using development environment Codasip Framework, needed for more effective vectorization, is introduced and implemented. Finally a set of integer vector instructions and related new registers for Codix is chosen and added to the model...|$|E
40|$|AbstractNowadays {{more and}} more {{processors}} are integrated with SIMD extensions, and many compilers have applied <b>auto-vectorization.</b> SLP is an vectorization algorithm that could vectorize scientific applications more effectively than traditional algorithm. However, if basic blocks have not vectorized efficiently by SLP then the vectorization performance will degrade. To solve that problem this paper brings SLP that applied recovery methodology. The algorithm adopts SLP algorithm to vectorize program and then esitimate the vectorization benifit based cost model, at last recover the basic blocks that haven’t vectorized efficiently to their original states. Experiment results indicate that with {{the adoption of the}} new policy, the speedup gain for some applications can reach 29. 4 %...|$|E
40|$|In {{order to}} obtain maximum performance, many {{applications}} require to extend parallelism from multi-threading to instruction-level (SIMD) parallelism that exists in many current (and future) multi-core architectures. While <b>auto-vectorization</b> technology {{has been used to}} exploit this SIMD level, it is not always enough due to OpenMP semantics and compiler technology limitations. In those cases, programmers need to resort to low-level intrinsics or vendor specific directives. We propose a new OpenMP directive: the simd directive. This directive will allow programmers to guide the vectorization process enabling a more productive and portable exploitation of the SIMD level. Our performance results show significant improvements over current auto-vectorizing technology of the Intel® Composer XE 2011. Peer ReviewedPostprint (published version...|$|E
40|$|For modern x 86 based CPUs with {{increasingly}} longer vector lengths, achieving good vectorization {{has become}} very important for gaining higher performance. Using very explicit SIMD vector programming techniques {{has been shown to}} give near optimal performance, however they are difficult to implement for all classes of applications particularly ones with very irregular memory accesses and usually require considerable re-factorisation of the code. Vector intrinsics are also not available for languages such as Fortran which is still heavily used in large production applications. The alternative is to depend on compiler <b>auto-vectorization</b> which usually have been less effective in vectorizing codes with irregular memory access patterns. In this paper we present recent research exploring techniques to gain compiler <b>auto-vectorization</b> for unstructured mesh applications. A key contribution is details on software techniques that achieve auto-vectorisation for a large production grade unstructured mesh application from the CFD domain so as to benefit from the vector units on the latest Intel processors without a significant code re-write. We use code generation tools in the OP 2 domain specific library to apply the auto-vectorising optimisations automatically to the production code base and further explore the performance of the application compared to the performance with other parallelisations such as on the latest NVIDIA GPUs. We see that there is considerable performance improvements with autovectorization. The most compute intensive parallel loops in the large CFD application shows speedups of nearly 40 % on a 20 core Intel Haswell system compared to their nonvectorized versions. However not all loops gain due to vectorization where loops with less computational intensity lose performance due to the associated overheads...|$|E
40|$|Accelerating program {{performance}} via SIMD vector units {{is very common}} in modern processors, {{as evidenced by the}} use of SSE, MMX, VSE, and VSX SIMD instructions in multimedia, scientific, and embedded applications. To take full advantage of the vector capabilities, a compiler needs to generate efficient vector code automatically. However, most commercial and open-source compilers fall short of using the full potential of vector units, and only generate vector code for simple innermost loops. In this paper, we present the design and implementation of an <b>auto-vectorization</b> framework in the backend of a dynamic compiler that not only generates optimized vector code but is also well integrated with the instruction scheduler and register allocator. The framework includes a novel compile-time efficient dynamic programming-based vector instruction selection algorithm for straight-line code that expands opportunities for vectorization in the following ways: (1) scalar packing explores opportunities of packing multiple scalar variables into short vectors; (2) judicious use of shuffle and horizontal vector operations, when possible; and (3) algebraic reassociation expands opportunities for vectorization by algebraic simplification. We report performance results on the impact of <b>auto-vectorization</b> on a set of standard numerical benchmarks using the Jikes RVM dynamic compilation environment. Our results show performance improvement of up to 57. 71 % on an Intel Xeon processor, compared to non-vectorized execution, with a modest increase in compile time in the range from 0. 87 % to 9. 992 %. An investigation of the SIMD parallelization performed by v 11. 1 of the Intel Fortran Compiler (IFC) on three benchmarks shows that our system achieves speedup with vectorization in all three cases and IFC does not. Finally, a comparison of our approach with an implementation of the Superword Level Parallelization (SLP) algorithm from [21], shows that our approach yields a performance improvement of up to 13. 78 % relative to SLP...|$|E
40|$|In {{the recent}} {{shift to the}} {{multi-core}} and many-core era, where systems tend to be heterogeneous even at chip level, SIMD instruction sets and accelerators that exploit parallelism {{in a similar way}} are coming into prominence in new multiprocessors and systems. This heterogeneity, even at chip level, is causing a lot of trouble to compilers and parallel programming models in terms of being able to maximize the profitability of the computational resources in an easy, generic, efficient and portable fashion. Although a lot of work on automatic vectorization/simdization techniques has been done over the years, compilers show important limitations when vectorizing code with pointers and function calls because of the traditional compiler analysis limitations, such as those in pointers aliasing analysis. Concerning parallel programming models, some of them are restricted to specific architectures while other portable ones, such as OpenCL, require programmers to face low-level architecture details and hard source code transformations, presenting important performance problems among different architectures, which requires new tuning efforts. In an attempt to offer a unified and generic solution to the auto-vectorization/simdization and portability problems, we propose User-directed Vectorization in OmpSs, a high-level programming model extension that offers developers the possibility to easily guide the compiler in the vectorization process just introducing some simple notations on the vectorizable areas of the code, such loops and functions. We focused our particular design, implementation and evaluation on the Intel SSE instruction set for CPUs, getting the same or higher speed-ups than using the GCC compiler <b>auto-vectorization</b> in easily-vectorizable codes, and a performance improvement of up to 2. 30 in more complex codes where GCC is not able to apply <b>auto-vectorization</b> and the hand-coded OpenCL version reaches a speed-up of 2. 23...|$|E
40|$|The {{advection}} of integral {{lines is}} an important computational kernel in vector field visualization. We investigate how this kernel can profit from vector (SIMD) extensions in modern CPUs. As a baseline, we formulate a streamline tracing algorithm that facilitates <b>auto-vectorization</b> by an optimizing compiler. We analyze this algorithm and propose two different optimizations. Our results show that particle tracing does not per se benefit from SIMD computation. Based on a careful analysis of the auto-vectorized code, we propose an optimized data access routine and a re-packing scheme which increases average SIMD efficiency. We evaluate our approach on three different, turbulent flow fields. Our optimized approaches increase integration performance up to 5 : 6 over our baseline measurement. We conclude {{with a discussion of}} current limitations and aspects for future work...|$|E
40|$|Abstract—We {{examine the}} Xeon Phi, {{which is based}} on Intel’s Many Integrated Cores architecture, for its {{suitability}} to run the FDK algorithm—the most commonly used algorithm to perform the 3 D image reconstruction in cone-beam computed tomography. We study the challenges of efficiently parallelizing the application and means to enable sensible data sharing between threads despite the lack of a shared last level cache. Apart from paral-lelization, SIMD vectorization is critical for good performance on the Xeon Phi; we perform various micro-benchmarks to investigate the platform’s new set of vector instructions and put a special emphasis on the newly introduced vector gather capability. We refine a previous performance model for the application and adapt it for the Xeon Phi to validate the performance of our optimized hand-written assembly implementation, as well as the performance of several different <b>auto-vectorization</b> approaches. I...|$|E
40|$|Augmenting a {{processor}} with special hardware that {{is able to}} apply a Single Instruction to Multiple Data(SIMD) {{at the same time}} is a cost effective way of improving processor performance. It also offers a means of improving the ratio of processor performance to power usage due to reduced and more effective data movement and intrinsically lower instruction counts. This paper considers and compares the NEON SIMD instruction set used on the ARM Cortex-A series of RISC processors with the SSE 2 SIMD instruction set found on Intel platforms {{within the context of the}} Open Computer Vision (OpenCV) library. The performance obtained using compiler <b>auto-vectorization</b> is compared with that achieved using hand-tuning across a range of five different benchmarks and ten different hardware platforms. On the ARM platforms the hand-tuned NEON benchmarks were between 1. 05 נand 13. 88 נfaster than the auto-vectorized code, while for the Intel platforms the hand-tuned SSE benchmarks were between 1. 34 נand 5. 54 נfaster. Full Tex...|$|E
40|$|Project Specification This project {{concerns}} {{the field of}} vectorization for Computing in High Energy Physics at CERN,Geneva. This paper summarises the results and progress of vectorizing two newly proposed counter based random number generators on Intel’s Haswell Architecture. Abstract This project studies SIMD optimizing two different newly proposed random number generators on Intel’s Haswell architecture with AVX 2 instruction sets. AVX 2 instruction set is necessary since many random number generators rely on 64 -bit integer multiplication. In first phase, mathematical algorithms behind the random number generators are studied and the places {{where they can be}} vectorized are identified. Then all internal data structures of random number generators are transformed from Array of Struct to Struct of Array for better <b>auto-vectorization.</b> To achieve better results intrinsics are used via a high-level C++ wrapping library. In second phase we performed benchmarks and studied the speed up obtained up to 1. 57 times for Threefry CBRNG due to vectorization on Haswell...|$|E
40|$|This paper {{demonstrates}} how modern software development methodologies {{can be used}} to give an existing sequential application a considerable performance speed-up on modern x 86 server systems. Whereas, in the past, speed-up was directly linked to the increase in clock frequency when moving to a more modern system, current x 86 servers present a plethora of “performance dimensions” that need to be harnessed with great care. The application we used is a real-life data analysis example in C++ analyzing High Energy Physics data. The key software methods used are OpenMP, Intel Threading Building Blocks (TBB), Intel Cilk Plus, and the <b>auto-vectorization</b> capability of the Intel compiler (Composer XE). Somewhat surprisingly, the Message Passing Interface (MPI) is successfully added, although our focus is on single-node rather than multi-node performance optimization. The paper underlines the importance of algorithmic redesign in order to optimize each performance dimension and links this to close control of the memory layout in a thread-safe environment. The data fitting algorithm {{at the heart of the}} application is very floating-point intensive so the paper also discusses how to ensure optimal performance of mathematical functions (in our case, the exponential function) as well as numerical correctness and reproducibility. The test runs on single-, dual-, and quad-socket servers show first of all that vectorization of the algorithm (with either <b>auto-vectorization</b> by the compiler or the use of Intel Cilk Plus Array Notation) gives more than a factor 2 in speed-up when the data layout in memory is properly optimized. Using coarse-grained parallelism all three approaches (OpenMP, Cilk Plus, and TBB) showed good parallel speed-up on the available CPU cores. The best one was obtained with OpenMP, but by combining Cilk Plus and TBB with MPI in order to tie processes to sockets, these two software methods nicely closed the gap and TBB came out with a slight advantage in the end. Overall, we conclude that the best implementation in terms of both ease of implementation and the resulting performance is a combination of the Intel Cilk Plus Array Notation for vectorization and a hybrid TBB and MPI approach for parallelization...|$|E
40|$|Although Single Instruction Multiple Data (SIMD) {{units are}} {{available}} in general purpose processors already since the 1990 s, state-of-the-art compilers are often still not capable to fully exploit them, i. e., they may miss to achieve the best possible performance. We present a new hardware-aware and adaptive loop tiling approach {{that is based on}} polyhedral transformations and explicitly dedicated to improve on <b>auto-vectorization.</b> It is an extension to the tiling algorithm implemented within the PluTo framework [4, 5]. In its default setting, PluTo uses static tile sizes and is already capable to enable the use of SIMD units but not primarily targeted to optimize it. We experimented with different tile sizes and found a strong re-lationship between their choice, cache size parameters and performance. Based on this, we designed an adaptive pro-cedure that specifically tiles vectorizable loops with dynam-ically calculated sizes. The blocking is automatically fitted to the amount of data read in loop iterations, the available SIMD units and the cache sizes. The adaptive parts are built upon straightforward calculations that are experimen-tally verified and evaluated. Our results show significant im-provements in the number of instructions vectorized, cache miss rates and, finally, running times...|$|E
40|$|We present initial {{comparison}} performance {{results for}} Intel many integrated core (MIC), Sandy Bridge (SB), and graphical processing unit (GPU). A 1 D explicit electrostatic particle-in-cell code {{is used to}} simulate a two-stream instability in plasma. We compare the computation times for various number of cores/threads and compiler options. The parallelization is implemented via OpenMP with a maximum thread number of 128. Parallelization and vectorization on the GPU is achieved with modifying the code syntax for compatibility with CUDA. We assess the speedup due to various <b>auto-vectorization</b> and optimization level compiler options. Our {{results show that the}} MIC is several times slower than SB for a single thread, and it becomes faster than SB when the number of cores increases with vectorization switched on. The compute times for the GPU are consistently about six to seven times faster than the ones for MIC. Compared with SB, the GPU is about two times faster for a single thread and about an order of magnitude faster for 128 threads. The net speedup, however, for MIC and GPU are almost the same. An initial attempt to offload parts of the code to the MIC coprocessor shows that there is an optimal number of threads where the speedup reaches a maximum. status: publishe...|$|E
40|$|Automatic {{vectorization}} {{is critical}} to enhancing performance of compute-intensive programs on modern processors. How-ever, {{there is much room}} for improvement over the <b>auto-vectorization</b> capabilities of current production compilers, through careful vector-code synthesis that utilizes a variety of loop transformations (e. g. unroll-and-jam, interchange, etc.). As the set of transformations considered is increased, the selection of the most effective combination of transformations becomes a significant challenge: currently used cost-models in vectorizing compilers are often unable to identify the best choices. In this paper, we address this problem using machine learning models to predict the performance of SIMD codes. In contrast to existing approaches that have used high-level features of the program, we develop machine learning models based on features extracted from the generated assembly code, The models are trained off-line on a number of benchmarks, and used at compile-time to discriminate between numerous possible vectorized variants generated from the input code. We demonstrate the effectiveness of the machine learning model by using it to guide automatic vectorization on a variety of tensor contraction kernels, with improvements ranging from 2 × to 8 × over Intel ICC’s auto-vectorized code. We also evaluate the effectiveness of the model on a number of stencil computations and show good improvement over auto-vectorized code. 1...|$|E
40|$|Achieving optimal {{performance}} on the latest multi-core and many-core architectures increasingly depends on making efficient use of the hardware's vector units. This paper presents results on achieving high performance through vectorization on CPUs and the Xeon-Phi on a key class of irregular applications: unstructured mesh computations. Using single instruction multiple thread (SIMT) and single instruction multiple data (SIMD) programming models, we show how unstructured mesh computations map to OpenCL or vector intrinsics {{through the use of}} code generation techniques in the OP 2 Domain Specific Library and explore how irregular memory accesses and race conditions can be organized on different hardware. We benchmark Intel Xeon CPUs and the Xeon-Phi, using a tsunami simulation and a representative CFD benchmark. Results are compared with previous work on CPUs and NVIDIA GPUs to provide a comparison of achievable {{performance on}} current many-core systems. We show that <b>auto-vectorization</b> and the OpenCL SIMT model do not map efficiently to CPU vector units because of vectorization issues and threading overheads. In contrast, using SIMD vector intrinsics imposes some restrictions and requires more involved programming techniques but results in efficient code and near-optimal performance, two times faster than non-vectorized code. We observe that the Xeon-Phi does not provide good performance for these applications but is still comparable with a pair of mid-range Xeon chips. ...|$|E
40|$|In {{this report}} {{we present a}} novel {{approach}} to model coupling for shared-memory multicore systems hosting OpenCL-compliant accelerators, which we call The Glasgow Model Coupling Framework (GMCF). We discuss {{the implementation of a}} prototype of GMCF and its application to coupling the Weather Research and Forecasting Model and an OpenCL-accelerated version of the Large Eddy Simulator for Urban Flows (LES) developed at DPRI. The first stage of this work concerned the OpenCL port of the LES. The methodology used for the OpenCL port is a combination of automated analysis and code generation and rule-based manual parallelization. For the evaluation, the non-OpenCL LES code was compiled using gfortran, fort and pgfortran, in each case with auto-parallelization and <b>auto-vectorization.</b> The OpenCL-accelerated version of the LES achieves a 7 times speed-up on a NVIDIA GeForce GTX 480 GPGPU, compared to the fastest possible compilation of the original code running on a 12 -core Intel Xeon E 5 - 2640. In the second stage of this work, we built the Glasgow Model Coupling Framework and successfully used it to couple an OpenMP-parallelized WRF instance with an OpenCL LES instance which runs the LES code on the GPGPI. The system requires only very minimal changes to the original code. The report discusses the rationale, aims, approach and implementation details of this work...|$|E
40|$|The {{introduction}} of the PowerPC 970 JS 20 blade server opens opportunities for vectorizing commercial applications using the integrated AltiVec unit. We examined the vectorization of applications from diverse fields such as XML parsing, UTF- 8 encoding, life sciences, string manipulations, and sorting. We obtained performance speedups (over optimized scalar code) for string comparisons (2 - 3), XML delimiter lookup (1. 5 - 5), and UTF- 8 conversion (2 - 4). The focus {{of this paper is}} on the process rather than on the results. Vectorizing commercial applications vastly differs from vectorizing graphic and image processing applications. In addition to the results achieved, we describe the pitfalls encountered, {{the advantages and disadvantages of}} the AltiVec unit, and what is missing in its current implementation. Sorting presents an interesting example. Vectorizing the quicksort algorithm was not successful due to low parallelism and misaligned data accesses. Vectorization of the combsort algorithm was very successful, with speedups of 5. 0, until the data spilled from the L 2 cache. Combining both approaches, by first partitioning the input using quicksort and then continuing with combsort, yielded speedups of over 2. 0. This research led to several patent disclosures, many algorithmic enhancements, and an insight into the correct integration of software with the AltiVec unit. The wealth of information collected during this study is being conveyed to the <b>auto-vectorization</b> teams of the relevant compilers. ...|$|E
40|$|With the ease-of-programming, {{flexibility}} and yet efficiency, MapReduce {{has become one}} of the most popular frameworks for building big-data applications. MapReduce was originally designed for distributed-computing, and has been extended to various architectures, e,g, multi-core CPUs, GPUs and FPGAs. In this work, we focus on optimizing the MapReduce framework on Xeon Phi, which is the latest product released by Intel based on the Many Integrated Core Architecture. To the best of our knowledge, this is the first work to optimize the MapReduce framework on the Xeon Phi. In our work, we utilize advanced features of the Xeon Phi to achieve high performance. In order to take advantage of the SIMD vector processing units, we propose a vectorization friendly technique for the map phase to assist the <b>auto-vectorization</b> as well as develop SIMD hash computation algorithms. Furthermore, we utilize MIMD hyper-threading to pipeline the map and reduce to improve the resource utilization. We also eliminate multiple local arrays but use low cost atomic operations on the global array for some applications, which can improve the thread scalability and data locality due to the coherent L 2 caches. Finally, for a given application, our framework can either automatically detect suitable techniques to apply or provide guideline for users at compilation time. We conduct comprehensive experiments to benchmark the Xeon Phi and compare our optimized MapReduce framework with a state-of-the-art multi-core based MapReduce framework (Phoenix++). By evaluating six real-world applications, the experimental results show that our optimized framework is 1. 2 X to 38 X faster than Phoenix++ for various applications on the Xeon Phi...|$|E
40|$|International audienceError-tolerating {{applications}} are increasingly {{common in the}} emerging field of real-time HPC. Proposals have been made at the hardware level {{to take advantage of}} inherent perceptual limitations, redundant data, or reduced precision input [20], as well as to reduce system costs or improve power efficiency [19]. At the same time, works on floating-point to fixed-point conversion tools [9] allow us to trade-off the algorithm exactness for a more efficient implementation. In this work, we aim at leveraging existing, HPC-oriented hardware architectures, while including in the precision tuning an adaptive selection of floating-and fixed-point arithmetic. Our proposed solution takes advantage of the application domain knowledge of the programmers by involving them in the first step of the interaction chain. We rely on annotations written by the programmer on the input file to know which variables of a computational kernel should be converted to fixed-point. The second stage replaces the floating-point variables in the kernel with fixed-point equivalents. It also adds to the original source code the utility functions to perform data type conversions from floating-point to fixed-point, and vice versa. The output of the second stage is {{a new version of the}} kernel source code which exploits fixed-point computation instead of floating-point computation. As opposed to typical custom-width hardware designs, we only rely on the standard 16 -bit, 32 -bit and 64 -bit types. We also explore the impact of the fixed-point representation on <b>auto-vectorization.</b> We discuss the effect of our solution in terms of time-to-solutions, error and energy-to-solution...|$|E
40|$|Abstract—With the ease-of-programming, {{flexibility}} and yet effi-ciency, MapReduce {{has become one}} of the most popular frameworks for building big-data applications. MapReduce was originally de-signed for distributed-computing, and has been extended to various architectures, e,g, multi-core CPUs, GPUs and FPGAs. In this work, we focus on optimizing the MapReduce framework on Xeon Phi, which is the latest product released by Intel based on the Many Integrated Core Architecture. To the best of our knowledge, this is the first work to optimize the MapReduce framework on the Xeon Phi. In our work, we utilize advanced features of the Xeon Phi to achieve high performance. In order to take advantage of the SIMD vector processing units, we propose a vectorization friendly technique for the map phase to assist the <b>auto-vectorization</b> as well as develop SIMD hash computation algorithms. Furthermore, we utilize MIMD hyper-threading to pipeline the map and reduce to improve the resource utilization. We also eliminate multiple local arrays but use low cost atomic operations on the global array for some applications, which can improve the thread scalability and data locality due to the coherent L 2 caches. Finally, for a given application, our framework can either automatically detect suitable techniques to apply or provide guideline for users at compilation time. We conduct comprehensive experiments to benchmark the Xeon Phi and compare our optimized MapReduce framework with a state-of-the-art multi-core based MapReduce framework (Phoenix++). By evaluating six real-world applications, the experi-mental results show that our optimized framework is 1. 2 X to 38 X faster than Phoenix++ for various applications on the Xeon Phi. I...|$|E
40|$|Achieving optimal {{performance}} on the latest multi-core and many-core architectures depends {{more and more on}} making efficient use of the hardware’s vector processing capabilities. While auto-vectorizing compilers do not require the use of vector processing constructs, they are only effective on a few classes of applications with regular memory access and com-putational patterns. Irregular application classes require the explicit use of parallel programming models; CUDA and OpenCL are well established for programming GPUs, but it is not obvious what model to use to exploit vector units on architectures such as CPUs or the Xeon Phi. Therefore it is of growing interest what programming models are avail-able, such as Single Instruction Multiple Threads (SIMT) or Single Instruction Multiple Data (SIMD), and how they map to vector units. This paper presents results on achieving high performance through vectorization on CPUs and the Xeon Phi on a key class of applications: unstructured mesh computations. By exploring the SIMT and SIMD execution and parallel pro-gramming models, we show how abstract unstructured grid computations map to OpenCL or vector intrinsics through the use of code generation techniques, and how these in turn utilize the hardware. We benchmark a number of systems, including Intel Xeon CPUs and the Intel Xeon Phi, using an industrially repre-sentative CFD application and compare the results against previous work on CPUs and NVIDIA GPUs to provide a contrasting comparison of what could be achieved on cur-rent many-core systems. By carrying out a performance analysis study, we identify key performance bottlenecks due to computational, control and bandwidth limitations. We show that the OpenCL SIMT model does not map efficiently to CPU vector units due to <b>auto-vectorization</b> is-sues and threading overheads. We demonstrate that while the use of SIMD vector intrinsics imposes some restrictions, and requires more involved programming techniques, it does result in efficient code and near-optimal performance, tha...|$|E

