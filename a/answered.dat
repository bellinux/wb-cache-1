10000|10000|Public
5|$|Each {{contestant}} had {{the same}} decision facing them as before, which was whether to attempt {{to answer the question}} or walk away with their pre-tournament total intact. Attempting the question and answering incorrectly incurred the same penalty as in regular play, with a reduction of their pre-tournament winnings to $25,000. If the question was <b>answered</b> correctly, the player that did so became the tournament leader. If another player after him/her <b>answered</b> correctly, that player assumed the lead and the previous leader kept their pre-tournament winnings. The highest remaining seed to have attempted and correctly <b>answered</b> their question {{at the end of the}} tournament on November 20, 2009 would be declared the winner and become the syndicated series' third millionaire.|$|E
5|$|His {{question}} was <b>answered</b> by GÃ¶del's incompleteness proof, Turing's machine and Church's Lambda calculus.|$|E
5|$|In the 2011 census, 72.8% of {{residents}} in the Uxbridge North ward <b>answered</b> {{that they had a}} religion, compared with 19.3% who did not and 7.9% who did not answer. Of those who <b>answered,</b> 53% identified as Christian, followed by 6.7% who identified as Muslim and 6.2% as Sikh. The percentage identifying as Hindu was 5.4%. Figures for residents identifying as either Jewish, Buddhist or other unspecified religions were each below 1%.|$|E
40|$|The authors {{investigate}} {{the interplay between}} <b>answer</b> quality and <b>answer</b> speed across question types in community question-answering sites (CQAs). The research questions addressed are the following: (a) How do <b>answer</b> quality and <b>answer</b> speed vary across question types? (b) How do the relationships between <b>answer</b> quality and <b>answer</b> speed vary across question types? (c) How do the best quality <b>answers</b> and the fastest <b>answers</b> differ in terms of <b>answer</b> quality and <b>answer</b> speed across question types? (d) How do trends in <b>answer</b> quality vary over time across question types? From the posting of 3, 000 questions in six CQAs, 5, 356 <b>answers</b> were harvested and analyzed. There {{was a significant difference}} in <b>answer</b> quality and <b>answer</b> speed across question types, and there were generally no significant relationships between <b>answer</b> quality and <b>answer</b> speed. The best quality <b>answers</b> had better overall <b>answer</b> quality than the fastest <b>answers</b> but generally took longer to arrive. In addition, although the trend in <b>answer</b> quality had been mostly random across all question types, the quality of <b>answers</b> appeared to improve gradually when given time. By highlighting the subtle nuances in <b>answer</b> quality and <b>answer</b> speed across question types, this study is an attempt to explore a territory of CQA research that has hitherto been relatively uncharted...|$|R
40|$|This paper {{presents}} a language-independent probabilistic <b>answer</b> ranking framework for question <b>answering.</b> The framework estimates {{the probability of}} an individual <b>answer</b> candidate given the degree of <b>answer</b> relevance {{and the amount of}} supporting evidence provided in the set of <b>answer</b> candidates for the question. Our approach was evaluated by comparing the candidate <b>answer</b> sets generated by Chinese and Japanese <b>answer</b> extractors with the re-ranked <b>answer</b> sets produced by the <b>answer</b> ranking framework. Empirical results from testing on NT-CIR factoid questions show a 40 % performance improvement in Chinese <b>answer</b> selection and a 45 % improvement in Japanese <b>answer</b> selection. ...|$|R
5000|$|... (defun {{factorial}} (n) (let ((<b>answer</b> 1)) (for i 2 n <b>answer</b> = <b>answer</b> * i) <b>answer)))</b> ...|$|R
5|$|Seaborg {{also became}} {{an expert in}} dealing with noted Berkeley {{physicist}} Robert Oppenheimer. Oppenheimer had a daunting reputation, and often <b>answered</b> a junior man's question before it had even been stated. Often the question <b>answered</b> was more profound than the one asked, but of little practical help. Seaborg learned to state his questions to Oppenheimer quickly and succinctly.|$|E
5|$|On April 8, 2017, Kenny and Mikey <b>answered</b> The Young Bucks' open {{challenge}} for the Ring of Honor (ROH) World Tag Team Championship, losing the subsequent title match.|$|E
25|$|If all {{applicable}} special {{issues were}} <b>answered</b> in the affirmative, then {{the result would}} be an automatic death sentence; if any special issue was not <b>answered</b> in the affirmative, the sentence would be life imprisonment.|$|E
40|$|We {{investigate}} {{the problem of}} complex <b>answers</b> in question <b>answering.</b> Complex <b>answers</b> consist of several simple <b>answers.</b> We describe the online question <b>answering</b> system shapaqa, and using data from this system we show {{that the problem of}} complex <b>answers</b> is quite common. We de ne nine types of complex questions, and suggest two approaches, based on <b>answer</b> frequencies, that allow question <b>answering</b> systems to tackle the problem. ...|$|R
40|$|Community Question <b>Answering</b> (QA) portals contain {{questions}} and <b>answers</b> contributed {{by hundreds of}} millions of users. These databases of {{questions and}} <b>answers</b> are of great value if they can be used directly to <b>answer</b> questions from any user. In this research, we address this collaborative QA task by drawing knowledge from the crowds in community QA portals such as Yahoo! <b>Answers.</b> Despite their popularity, {{it is well known that}} <b>answers</b> in community QA portals have unequal quality. We therefore propose a quality-aware framework to design methods that select <b>answers</b> from a community QA portal considering <b>answer</b> quality in addition to <b>answer</b> relevance. Besides using <b>answer</b> features for determining <b>answer</b> quality, we introduce several other quality-aware QA methods using <b>answer</b> quality derived from the expertise of answerers. Such expertise can be question independent or question dependent. We evaluate our proposed methods using a database of 95 K questions and 537 K <b>answers</b> obtained from Yahoo! <b>Answers.</b> Our experiments have shown that <b>answer</b> quality can improve QA performance significantly. Furthermore, question dependent expertise based methods are shown to outperform methods using <b>answer</b> features only. It is also found that there are also good <b>answers</b> not among the best <b>answers</b> identified by Yahoo! <b>Answers</b> users...|$|R
50|$|Knock-Off: A {{category}} was announced and 12 possible <b>answers</b> were shown; nine <b>answers</b> were correct while three were wrong. Each contestant, in turn, selected {{one of the}} <b>answers.</b> A correct <b>answer</b> turned gold and was worth cash (four $2 <b>answers,</b> three $5 <b>answers,</b> a $10 <b>answer,</b> and a $15 answer; Some boards had two $3 <b>answers</b> replacing two worth $2). An incorrect <b>answer</b> turned red and eliminated that contestant {{for the remainder of}} that round. Play continued until the last correct <b>answer</b> was found or all three players had been eliminated. In general, the less obvious an <b>answer</b> was, the more it was worth.|$|R
25|$|Dawkins rounds {{off this}} episode with a {{presentation}} of Bertrand Russell's celestial teapot analogy. He argues that just because science has not yet <b>answered</b> every conceivable question about the universe, {{there is no need}} to turn to faith, which has never <b>answered</b> anything of significance.|$|E
25|$|According to the Eurobarometer 2010, 37% of Belgian {{citizens}} {{responded that}} they {{believe there is a}} God. 31% <b>answered</b> that they believe there is some sort of spirit or life-force. 27% <b>answered</b> that they do not believe there is any sort of spirit, God, or life-force. 5% did not respond.|$|E
25|$|The obvious next {{question}} is <b>answered</b> positively as follows.|$|E
50|$|When a green buzzer is found, {{the host}} asks {{what are the}} 2 <b>answers</b> between which they are hesitating. Contestants will select, of course, the 2 <b>answers</b> that they {{consider}} the most plausible. One of those <b>answers</b> is wrong. Therefore, an incorrect, but plausible, <b>answer</b> is withdrawn (however, nothing guarantees that the other un-eliminated <b>answer</b> is the correct <b>answer,</b> because contestants may not have indicated the correct <b>answer</b> among the 2 <b>answers</b> that they consider most plausible). Therefore, there are still 3 <b>answers</b> to choose from and contestants have as many seconds to <b>answer</b> as there remain unused buzzers.|$|R
50|$|Each {{question}} {{gives the}} contestant three opportunities to answer: <b>answering</b> the question alone, <b>answering</b> the question {{with an additional}} clue to the <b>answer,</b> and <b>answering</b> the question {{with a choice of}} two <b>answers.</b>|$|R
5000|$|SeesawA {{question}} with multiple <b>answers</b> was read to both contestants. Contestants alternated giving correct <b>answers</b> until one contestant gave a wrong <b>answer,</b> repeated an <b>answer,</b> {{or could not}} think of an <b>answer</b> and the opponent won the box, unless the opponent could not <b>answer</b> either, which left the box unclaimed. The box could also be won by giving the last correct <b>answer.</b>|$|R
25|$|The 9/11 Family Steering Committee {{produced}} a website summarizing {{the questions they}} had raised to the Commission, indicating which they believe had been <b>answered</b> satisfactorily, which they believe had been addressed but not <b>answered</b> satisfactorily, and which they believe had been generally ignored in or omitted from the Report.|$|E
25|$|Found my {{need for}} {{knowledge}} <b>answered</b> gratefully by thee.|$|E
25|$|These {{ideas have}} been <b>answered</b> and {{rejected}} in mainstream scholarship.|$|E
2500|$|<b>A.N.S.W.E.R.</b> (also {{known as}} International <b>ANSWER</b> and <b>ANSWER</b> Coalition) ...|$|R
40|$|Along {{with the}} {{proliferation}} of the social web, question and <b>answer</b> (QA) sites attract millions of users around the globe. On these sites, users ask questions while others provide <b>answers.</b> These QA sites vary by their scope, size, and quality of answers; the most popular QA site is Yahoo! <b>Answers.</b> This chapter aims to examine the quality of information produced by the crowd on Yahoo! <b>Answers,</b> assuming that given enough eyeballs all questions can get good <b>answers.</b> Findings illustrate a process of <b>answer</b> quality improvement through crowd-sourcing questions. Improvement is achieved by having multiple <b>answers</b> to any given question instead of a singe <b>answer,</b> and through a mechanism of <b>answer</b> evaluation, by which users rank the best <b>answer</b> to any given question. Both processes contribute significantly to the quality of <b>answers</b> one can expect to find on Yahoo! <b>Answers...</b>|$|R
50|$|An <b>answer</b> {{given by}} a student is ruled correct or {{incorrect}} by the moderator. On short <b>answer</b> questions, if the <b>answer</b> given differs from the official one, the moderator uses his or her judgment to make a ruling (which is subject to a challenge by the competitors). On multiple choice questions, the <b>answer</b> given by the student is only correct if it matches the official <b>answer</b> exactly. Alternatively, the student may give the letter choice that corresponds to the correct <b>answer.</b> Although A, B, C, and D were once used as <b>answer</b> choice letters, W, X, Y, and Z are now favored due to a lower chance of confusion. On short <b>answers,</b> the <b>answer</b> {{does not have to}} exactly match the official <b>answer,</b> so any given <b>answer</b> that roughly means the same as the official <b>answer</b> is accepted.|$|R
25|$|This {{question}} can't be <b>answered</b> {{since the}} Holy Spirit exceeds human understanding.|$|E
25|$|An {{investigation}} in the early 1980s of 610 undergraduate women {{asked if they had}} ever said no to sex, even though they fully intended to have sexual intercourse. The majority, 68.5% of these women <b>answered</b> 'no' when their intention was 'maybe'. The other 39.3% <b>answered</b> that when they have said 'no' really meant 'yes'. The explanations in support of their answers were that they were fearful of being considered promiscuous. Some claimed they were inhibited about sex. Others <b>answered</b> that they intended to manipulate the male because they were angry, wanted him to become more aroused or more aggressive.|$|E
25|$|Two Border Patrol agents {{demanded to}} know where he was from. He <b>answered</b> their {{question}} accurately and produced a copy of a form showing his admission into the U.S. as a refugee. The agents asked whether he had been fingerprinted and photographed through NSEERS ("Special Registration"). He <b>answered,</b> accurately, that he had not: refugees are exempt from that program. They arrested him.|$|E
50|$|Each {{question}} has several <b>answers</b> and each <b>answer</b> carries different points, when asking the questioner to ask, each team leader {{who knows the}} <b>answer</b> must press the button to calculate the <b>answer</b> as correct, if the <b>answer</b> has {{the largest number of}} points, the <b>answer</b> is counted and the remaining family members are taken to identify the remaining <b>answers,</b> and if the <b>answer</b> of the leader of the team who pressed the button carries the largest number of points, it is given another chance for the leader of the competing family to give the <b>answer,</b> if the latter knows the <b>answer</b> with the highest number of points, it will be easy to pass on to the rest of his family in order to identify the rest of the <b>answers.</b> If the <b>answer</b> is less than the <b>answer</b> of the leader who pressed the button for the first time, in order to complete all the solutions to the question..|$|R
40|$|Question <b>Answering</b> (QA) is {{a focused}} way of {{information}} retrieval. Question <b>Answering</b> system {{tries to get}} back the accurate <b>answers</b> to questions posed in natural language provided a set of documents. Basically question <b>answering</b> system (QA) has three elements i. e. question classification, information retrieval (IR), and <b>answer</b> extraction. These elements {{play a major role}} in Question <b>Answering.</b> In Question classification, the questions are classified depending upon the type of its entity. Information retrieval component is used to determine success by retrieving relevant <b>answer</b> for different questions posted by the intelligent question <b>answering</b> system. <b>Answer</b> extraction module is growing topics in the QA in which ranking and validating a candidateâs <b>answer</b> is the major job. This paper offers a concise discussion regarding different Question <b>Answering</b> types. In addition we describe different evaluation metrics used to evaluate the performance of different question <b>answering</b> systems. We also discuss the recent question <b>answering</b> systems developed and their corresponding techniques...|$|R
50|$|An <b>answer</b> {{to either}} set of {{questions}} {{will allow us to}} devise a means of <b>answering</b> the other. <b>Answering</b> the former question set first is called particularism, whereas <b>answering</b> the latter set first is called methodism. A third solution is skepticism, which proclaims that since one cannot have an <b>answer</b> to the first {{set of questions}} without first <b>answering</b> the second set, and one cannot hope to <b>answer</b> the second set of questions without first knowing the <b>answers</b> to the first set, we are, therefore, unable to <b>answer</b> either. This has the result of our being unable to justify any of our beliefs.|$|R
25|$|The {{original}} points scheme, {{which was}} in operation from inception until 2001, consisted of three groups of ten questions. The first ten questions were worth three marks each, the next ten four marks each, and the last ten five marks each. Students were deducted {{a quarter of the}} marks for a given question if they <b>answered</b> incorrectly, so that a student randomly guessing the answers would gain no numerical benefit (on statistical average). Students started with 30 marks, so that a student who <b>answered</b> all questions incorrectly would record a total score of zero, while one who <b>answered</b> all questions correctly would record a score of 150.|$|E
25|$|The philandering {{character}} Sidney Dillon in Truman Capote's unfinished novel <b>Answered</b> Prayers {{is based}} on Paley.|$|E
25|$|<b>Answered</b> by Fire, a 2006 {{television}} drama {{concerning the}} crisis {{which led to}} the INTERFET mission.|$|E
50|$|Users {{are given}} {{four days to}} <b>answer</b> a question. Questions can be rated for their {{usefulness}} once a best <b>answer</b> has been selected. The scoring system is weighted to encourage users to participate and <b>answer</b> questions. Scores are awarded by <b>answering</b> questions and voting for <b>answers.</b> Bonus scores are awarded for creating the best <b>answer</b> or voting for the best <b>answer.</b> Generally, the user's QnA score will rise more quickly if their votes are for quality <b>answers.</b> A user is limited to 100 votes per day.|$|R
30|$|Mean {{utility of}} <b>answers</b> (MUAnswers): {{measures}} {{the ability of}} the user in <b>answering</b> a question compared to that of competing answerers. For each <b>answer</b> that a user provided to a question and for which there are competitive <b>answers,</b> the utility of this userâs <b>answer</b> is calculated by standardizing its voting balance compared to all other competing <b>answers</b> in that question (i.e., we calculate its z-score), or is set to zero if none of the <b>answers</b> have votes. A userâs MUAnswers is the average utility of the <b>answers</b> posted by this user that had competing alternatives, or zero if none of the userâs <b>answers</b> had competition. To better assess the quality of an <b>answer,</b> we add one positive vote in the calculation of voting balance if the <b>answer</b> was selected as the best.|$|R
40|$|We {{describe}} {{the architecture of}} a question <b>answering</b> system and systematically evaluate contributions of different system components to accuracy. The system differs from most question <b>answering</b> systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate <b>answers.</b> Because a wrong <b>answer</b> is often worse than no <b>answer,</b> we also explore strategies for predicting when the question <b>answering</b> system is likely to give an incorrect <b>answer...</b>|$|R
