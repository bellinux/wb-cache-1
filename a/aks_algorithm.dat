22|6|Public
25|$|In August 2002, Spencer {{provided}} commentary in an ABC online {{article about}} {{the interpretation of the}} <b>AKS</b> <b>algorithm.</b>|$|E
5000|$|Note {{that all}} primes satisfy this {{relation}} (choosing [...] in (...) gives (...) , which holds for n prime). This congruence can be checked in polynomial time when r is polynomial to the digits of n. The <b>AKS</b> <b>algorithm</b> evaluates this congruence {{for a large}} set of a values, whose size is polynomial to the digits of n. The proof of validity of the <b>AKS</b> <b>algorithm</b> shows that one can find an r {{and a set of}} a values with the above properties such that if the congruences hold then n is a power of a prime.|$|E
5000|$|The <b>AKS</b> <b>algorithm</b> {{can be used}} {{to verify}} the primality of any general number given. Many fast primality tests are known that work only for numbers with certain properties. For example, the Lucas-Lehmer test works only for Mersenne numbers, while Pépin's test can be applied to Fermat numbers only.|$|E
40|$|AbstractThis paper {{intends to}} propose a novel {{clustering}} method, ant K-means (<b>AK)</b> <b>algorithm.</b> <b>AK</b> <b>algorithm</b> modifies the K-means as locating the objects in a cluster with the probability, which is updated by the pheromone, while the rule of updating pheromone is according to total within cluster variance (TWCV). The computational results showed {{that it is better}} than the other two methods, self-organizing feature map (SOM) followed by K-means method and SOM followed by genetic K-means algorithm via 243 data sets generated by Monte Carlo simulation. To further testify this novel method, the questionnaire survey data for the plasma television market segmentation is employed. The results also indicated that the proposed method is the best among these three methods based on TWCV...|$|R
40|$|Jet {{production}} cross-section {{measurements are}} presented. The measurements are {{done with the}} data from Large Hadron Collider (LHC) proton-proton collisions, collected with the Compact Muon Solenoid (CMS) detector. The inclusive jet production measurements are carried out with data collected √(s) = 7 TeV and 8 TeV with total integrated luminosity (L_int) 5. 0 fb^- 1 and 10. 71 fb^- 1 respectively. The dijet production measurements are carried out with the √(s) = 7 TeV dataset. Jets are reconstructed with the anti-k_T clustering algorithm with size parameter R= 0. 7. The measured cross sections are corrected for detector effects and compared to perturbative QCD predictions at NLO, corrected for NP factors, using various sets of PDF. The inclusive jet cross-section ratio of the jets reconstructed with the anti-k_T (<b>AK)</b> <b>algorithm</b> and two radius parameter R = 0. 5 and R = 0. 7 are also presented. The data used is √(s) = 7 TeV CMS data corresponding to L_int = 5. 0 fb^- 1. Significant discrepancies are found comparing the data to leading order calculations and to fixed order calculations at NLO, corrected for NP effects, whereas simulations with NLO matrix elements matched to the parton showers describe the data quite well. A study of color coherence effects in pp collisions has been performed with the data collected at √(s) = 7 TeV and L_int = 36 pb^- 1. The measurement of the azimuthal angular correlation between {{the second and third}} jets is compared to the predictions of Monte Carlo models with different implementations of color coherence effects. Comment: 8 pages, 6 figures. Proceedings for EPS-HEP 201...|$|R
40|$|Actinic keratosis (AK) is {{a chronic}} skin disease in which {{multiple}} clinical and subclinical lesions co-exist across {{large areas of}} sun-exposed skin, resulting in field cancerisation. Lesions require treatment because of their potential to transform into invasive squamous cell carcinoma. This article aims to provide office-based dermatologists and general practitioners with simple guidance on AK treatment in daily clinical practice to supplement existing evidence-based guidelines. Novel aspects of the proposed treatment algorithm include differentiating patients according to whether they have isolated scattered lesions, lesions clustered in small areas or large affected fields without reference to specific absolute numbers of lesions. Recognising that complete lesion clearance is rarely achieved in real-life practice and that AK is a chronic disease, the suggested treatment goals are {{to reduce the number}} of lesions, to achieve long-term disease control and to prevent disease progression to invasive squamous cell carcinoma. In the clinical setting, physicians should select AK treatments based on local availability, and the presentation and needs of their patients. The proposed <b>AK</b> treatment <b>algorithm</b> is easy-to-use and has high practical relevance for real-life, office-based dermatology. SCOPUS: re. jinfo:eu-repo/semantics/publishe...|$|R
50|$|While the {{algorithm}} is of immense theoretical importance, {{it is not}} used in practice. For 64-bit inputs, the Baillie-PSW primality test is deterministic and runs many orders of magnitude faster. For larger inputs, {{the performance of the}} (also unconditionally correct) ECPP and APR tests is far superior to AKS. Additionally, ECPP can output a primality certificate that allows independent and rapid verification of the results, which is not possible with the <b>AKS</b> <b>algorithm.</b>|$|E
5000|$|In {{response}} {{to some of}} these variants, and to other feedback, the paper [...] "PRIMES is in P" [...] was updated with a new formulation of the <b>AKS</b> <b>algorithm</b> and of its proof of correctness. (This version was eventually published in Annals of Mathematics.) While the basic idea remained the same, r was chosen in a new manner, and the proof of correctness was more coherently organized. While the previous proof had relied on many different methods, the new version relied almost exclusively on the behavior of cyclotomic polynomials over finite fields. The new version also allowed for an improved bound on the time complexity, which can now be shown by simple methods to be [...] Using additional results from sieve theory, this can be further reduced to [...]|$|E
40|$|In August 2002, three Indian researchers, Manindra Agrawal and his {{students}} Neeraj Kayal and Nitin Saxena at the Indian Institute of Technology in Kanpur, presented a remarkable algorithm (the <b>AKS</b> <b>algorithm)</b> in their paper 2 ̆ 2 PRIMES is in P. 2 ̆ 2 It is a deterministic polynomial-time algorithm that determines whether an input number is prime or composite. No such algorithm was known so far and it has fundamental meaning for complexity theory. This project is centered around the <b>AKS</b> <b>algorithm</b> from the 2 ̆ 2 PRIMES is in P 2 ̆ 2 paper. The objectives are both experiments with the <b>AKS</b> <b>algorithm</b> and theorems and lemmas showing the correctness of the algorithm. One of the tasks of the project is to provide easy-to-follow explanations of the original paper for average mathematically mature readers. We also analyze the <b>AKS</b> <b>algorithm</b> in detail. Ideas and concepts in the algorithm are studied and possibilities of improvements of the algorithm are explored...|$|E
40|$|We {{investigate}} {{the problem of}} permuting n data items, each covering D global memory cells, on an EREW PRAM with n= log n processors and less than Dn additional storage. We present a family of <b>algorithms</b> (<b>Ak)</b> k, where k = 1; 2; : : :; log n, such that Ak needs time D log n log (k) n. Here log (k) n denotes k times application of log to n, and log n = minfkj log (k) n 1 g. All algorithms need Θ(n) operations which is optimal. The storage requirements for Ak are n global bits, n= log (kΓ 1) n global memory cells if k 2, and D log n= log (kΓ 1) n (resp. D) local memory cells per processor if k 2 (resp. k = 1). Hence, the family (Ak) k reveals a time-space tradeoff. The result can be generalized to machines with p ! n= log n processors such that the run time is (Dn=p) log (k) n, (Dn=p) = log (kΓ 1) n local cells per processor are needed, and the global storage requirements remain as given above...|$|R
40|$|AbstractThe {{expressions}} ϕ(n) +σ(n) − 3 n and ϕ(n) +σ(n) − 4 n are unusual among linear {{combinations of}} arithmetic functions {{in that they}} each vanish on a nonempty set of composite numbers. In 1966, Nicol proved that the set A:={n|(ϕ(n) +σ(n)) /n∈N⩾ 3 } contains 2 a⋅ 3 ⋅(2 a− 2 ⋅ 7 − 1) {{if and only if}} 2 a− 2 ⋅ 7 − 1 is prime and conjectured that A contains no odd integers. A 2008 paper by Luca and Sandor completely classifies the elements of A that have three distinct prime factors and observes that Nicol's conjecture holds for numbers with fewer than six distinct prime factors. In this paper we let AK denote the set of n∈A with exactly K distinct prime factors and present a computer-implementable algorithm that decides whether Nicol's conjecture holds for a given <b>AK.</b> Using this <b>algorithm,</b> we verify Nicol's conjecture for A 6 and completely classify the elements of A 4. We prove that all but finitely many n∈A 4 have the form 2 a⋅ 3 ⋅p 3 ⋅p 4, and that all but finitely many n∈A 5 are divisible by 6 and not 9. In addition, we prove that every AK is contained in a finite union of sequences that each have the form {p 1 a 1 i⋯pkaki⋅u⋅wi}i= 1 ∞, where k⩾ 1, p 1,…,pk are distinct primes, and each aji as well as the least prime factor of wi go to infinity as i does...|$|R
40|$|Strategic group {{research}} {{originated in}} the 1970 s {{and a number of}} notable studies centered on the US pharmaceutical industry. Results were however, conflicting. This thesis explores the nature of strategic groups and the related concept of competitive groups in the UK pharmaceutical industry during the period 1993 to 2002. The research follows three related themes. The first research theme identifies two stable strategic time periods each of five years duration across the period studied. Within each of these time periods strategic groups were identified using a combination of Ward's method and <b>aK</b> means clustering <b>algorithm</b> and the presence of a relatively stable strategic group structure was confirmed. A statistically significant relationship between these strategic groups and performance is demonstrated using three performance measures. The second research theme then explores the movement of firms between strategic groups and finds some support for the proposition that firms moving between strategic groups move to more advantageousp ositions. The relationship between strategicg roups and mergers is also investigated and this research finds that mergers between firms occur preferentially across strategic groups rather than within strategic groups. This relationship is confirmed as highly statistically significant. Finally in the third research theme the relationship between strategic groups, how firms compete and competitive groups, where firms compete, are investigated. Six different competitive groups are identified, all but one of which is concentrated around a dominant therapeutic area. This finding suggests that direct competition between firms is reduced by market segmentation. A weak relationship was found between competitive groups and performance but when competitive groups (where firms compete) and strategic groups how firms compete) are examined in combination a strong statistically significant relationship with performance was found. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The new Agrawal-Kayal-Saxena (<b>AKS)</b> <b>algorithm</b> {{determines whether}} a given number is prime or {{composite}} in polynomial time, but, unlike the previous algorithms developed by Fermat, Miller, and Rabin, the AKS test is deterministic. The new test {{is not without}} its disadvantages, however; the older, error-prone Miller-Rabin test is accurate enough for nearly all numbers, and both experiment and analysis show that the <b>AKS</b> <b>algorithm</b> is, at present, still too slow to replace its predecessor. ...|$|E
40|$|Agrawal, Kayal, and Saxena gave in [1] a {{deterministic}} algorithm {{to decide}} whether or not a given integer n is prime. We gave an exposition of this algorithm in the lecture notes [2]. We proved there that the <b>AKS</b> <b>algorithm</b> is correct. It is not obvious, however, that the <b>AKS</b> <b>algorithm</b> has a runtime that is polynomial in the number of digits of n, because the second step of the algorithm contains a while loop, which might have an exponential number of iterations, unless it terminates early. This second step is shown below...|$|E
40|$|Abstract. We combine {{ideas from}} the seminal paper of Agrawal, Kayal and Saxena [AKS] as {{improved}} by Lenstra [Le 3] with the particular case sharpening of Berrizbeitia and introduce the cyclotomy of rings setting [Le 2, BvdH, Mi 3] for the latter. Thus we deduce a new variant of the <b>AKS</b> <b>algorithm</b> which: (i) has running time O log 4 +o(1) (n) (ii) works on all prime candidates n> e...|$|E
40|$|This project {{presents}} some randomized and deterministic algorithms {{of number}} primality testing, and, {{for some of}} them, their implementation in C++. The algorithms studied here are the naive algorithm (deterministic), the Miller-Rabin algorithm (randomized), the Fermat algorithm (randomized), the Solovay-Strassen algorithm (randomized) and the <b>AKS</b> <b>algorithm</b> (deterministic). These algorithms are presented with the number theory {{they need to be}} understood and with some proofs of the theorems they use...|$|E
40|$|The present {{work has}} been carried out at CSIR Centre for Mathematical Modeling and Computer Simulation (C-MMACS). The paper deals with the {{implementation}} of AKS class primality tests and various issues relating to it. <b>AKS</b> <b>algorithm</b> is the first deterministic polynomial time primality test named after its authors Manindra Agarwal, Neeraj Kayal and Ntin Saxena. A primality-testing algorithm is one that checks whether a given number is prime or not. “Deterministic ” means that operations carried out by the algorithm are determined entirely by (1) the algorithm (2) the input. In particular the algorithm does not make any 1 random choices. “Polynomial time ” means that there is some polynomial p such that, for every input n, the algorithm takes at most p (length of n) steps. This algorithm is presented in the paper “ Primes in P ” [4] and since then many efforts are being made to implement the same. In this paper we have implemented the <b>AKS</b> <b>algorithm</b> and have provided the complexity at each step of the algorithm. We hav...|$|E
40|$|We {{remark that}} AKS primality testing {{algorithm}} needs about 1, 000, 000, 000 G (gigabyte) storage {{space for a}} number of 1024 bits. Such storage requirement is hard to meet in practice. To the best of our knowledge, it is impossible for current operating systems to write and read data in so huge storage space. Thus, the running time for <b>AKS</b> <b>algorithm</b> should not be simply estimated as usual in terms of the amount of arithmetic operations. ...|$|E
40|$|On August 2002, Agrawal, Kayal and Saxena {{announced}} the first deterministic and polynomial time primality testing algorithm. For an input n, the <b>AKS</b> <b>algorithm</b> runs in heuristic n). Verification takes {{roughly the same}} amount of time. On the other hand, the Elliptic Curve Primality Proving algorithm (ECPP) runs in random heuristic time O(log n) (O(log n) if the fast multiplication is used), and generates certificates which can be easily verified. More recently, Berrizbeitia gave a variant of the <b>AKS</b> <b>algorithm,</b> in which some primes cost much less time to prove than a general prime does. Building on these celebrated results, this paper explores the possibility of designing a more e#cient algorithm. A random primality proving algorithm with heuristic time complexity O(log n) is presented. It generates a certificate of primality which is O(log n) bits long and can be verified in deterministic time O(log n). The reduction in time complexity is achieved by first generalizing Berrizbeitia's algorithm to one which has higher density of easily-proved primes. For a general prime, one round of ECPP is deployed to reduce its primality proof to the proof of a random easily-proved prime...|$|E
40|$|Abstract. On August 2002, Agrawal, Kayal and Saxena {{announced}} the first deterministic and polynomial time primality testing algorithm. For an input n, the <b>AKS</b> <b>algorithm</b> runs in heuristic time Õ(log 6 n). Verification takes {{roughly the same}} amount of time. On the other hand, the Elliptic Curve Primality Proving algorithm (ECPP), runs in random heuristic time Õ(log 6 n) (Õ(log 5 n) if the fast multiplication is used), and generates certificates which can be easily verified. More recently, Berrizbeitia gave a variant of the <b>AKS</b> <b>algorithm,</b> in which some primes cost much less time to prove than a general prime does. Building on these celebrated results, this paper explores the possibility of designing a more efficient algorithm. A random primality proving algorithm with heuristic time complexity Õ(log 4 n) is presented. It generates a certificate of primality which is O(log n) bits long and can be verified in deterministic time Õ(log 4 n). The reduction in time complexity is achieved by first generalizing Berrizbeitia’s algorithm to one which has higher density of easily-proved primes. For a general prime, one round of ECPP is deployed to reduce its primality proof to the proof of a random easilyproved prime. ...|$|E
40|$|Abstract—AKS {{algorithm}} {{is the first}} deterministic polynomial time algorithm for primality proving. This project uses MPI (Message Passing Interface) along with GNU MP (Multi Pre-cision) library to implement a variant of <b>AKS</b> <b>algorithm.</b> Two different parallelization strategies have been implemented and we compare these strategies making relevant observations and pro-viding speedups. The method of fast polynomial exponentiation with precomputation {{has been used to}} achieve parallelization and improve run time. Further we show experimentally that master slave model of parallelization gives very good results. I...|$|E
40|$|In applied cryptography, RSA is {{a typical}} {{asymmetric}} algorithm, which is used in electronic transaction and many other security scenarios. RSA needs to generate large random primes. Currently, primality test mostly depends on probabilistic algorithms, such as the Miller-Rabin primality testing algorithm. In 2002, Agrawal et al. published the Agrawal–Kayal–Saxena (AKS) primality testing algorithm, {{which is the first}} generic, polynomial, deterministic and non-hypothetical algorithm for primality test. This paper proves the necessary and sufficient condition for AKS primality test. An improved <b>AKS</b> <b>algorithm</b> is proposed using Fermat’s Little Theorem. The improved algorithm becomes an enhanced Miller-Rabin probabilistic algorithm, which can generate primes as fast as the Miller-Rabin algorithm does...|$|E
40|$|For {{many years}} mathematicians and {{computer}} scientists have {{searched for a}} fast and reliable primality test. This is especially relevant nowadays, because the popular RSA public-key cryptosystem requires very large primes in order to generate secure keys. I will describe some efficient randomised algorithms that are useful, but have the defect of occasionally giving the wrong answer, or taking {{a very long time}} to give an answer. In 2002, Agrawal, Kayal and Saxena (AKS) found a deterministic polynomial-time algorithm for primality testing. I will describe the original <b>AKS</b> <b>algorithm</b> and some improvements by Bernstein and Lenstra. As far as theory is concerned, we now know that “PRIMES is in P”, and this appears {{to be the end of}} the story. However, I will explain why it is preferable to use randomised algorithms in practice...|$|E
40|$|Objective: Implementation of the Agrawal-Kayal-Saxena (AKS) {{deterministic}} polynomial-time test, the Random Quadratic Frobenius Test (RQFT) for probable primes, and the Miller-Rabin strong pseudoprimality test. Abstract: Current public-key cryptosystems rely on {{the difficulty}} of factoring large integers {{on the order of}} 231 - 340 decimal digits. Producing primes of this size has not been fast or easy. In 2002, Agrawal, Kayal and Saxena (AKS) developed a relatively simple deterministic algorithm that does not rely on unproven assumptions. The <b>AKS</b> <b>algorithm,</b> which runs in polynomial time, tests numbers for primality. We will implement this method for its simplicity and reliability. We will also implement the Random Quadratic Frobenius Test (RQFT). Our motivation for choosing RQFT is the fact that composites pass an iteration with probability 1 / 7710, in contrast to the 1 / 4 per-iteration rate of the Rabin-Miller algorithm...|$|E
40|$|Prime numbers play a {{very vital}} role in modern {{cryptography}} and especially the difficulties involved in factoring numbers composed of product of two large prime numbers have been put to use in many modern cryptographic designs. Thus, the problem of distinguishing prime numbers from the rest is vital and therefore {{there is a need}} to have efficient primality testing algorithms. Although there had been many probabilistic algorithms for primality testing, there wasn't a deterministic polynomial time algorithm until 2002 when Agrawal, Kayal and Saxena came with an algorithm, popularly known as the <b>AKS</b> <b>algorithm,</b> which could test whether a given number is prime or composite in polynomial time. This project is an attempt at understanding the ingenious idea behind this algorithm and the underlying principles of mathematics that is required to study it. In fact, through out this project, one of the major objectives has been to make it as much self contained as possible. Finally, the project provides an implementation of the algorithm using Software for Algebra and Geometry Experimentation (SAGE) and arrives at conclusions on how practical or otherwise it is...|$|E
40|$|In 2002 Manindra Agrawal, Neeraj Kayal, and Nitin Saxena {{discovered}} an algorithm {{to test a}} number for primality that is both deterministic and runs in polynomial time. The <b>AKS</b> <b>algorithm</b> hinges on a calculated value they call r, which is defined for a given integer n > 1 as the least value for which the order of n modulo r is greater than log 2 over 2 n. This r has a proven upper bound of log 5 over 2 n. In this paper, we prove that 2 + log 2 over 2 n is a lower bound of the value r, and if n is a square, there is a lower bound of 1 + 2 log 2 over 2 n. We also present data suggesting that 3 log 2 over 2 n is a smaller upper bound of r. If this is indeed an upper bound, the AKS Primality Test is shown to have a time complexity of O(log 6 + ?? over 2 n) in bit operations for any small ?? greater than 0. Data also suggests a number n is a square {{if and only if}} its corresponding r is greater than 3 log 2 over 2 n...|$|E
40|$|With {{the boom}} in {{information}} technology and the penetration of these technologies in {{an increasing number of}} areas, such as electronic business, many questions about security arise. Data in electronic form bring many benefits, but they are vulnerable to various abuses. Therefore, data need to be adequately protected. For instance, when we perform a transaction over the Internet, our browser encrypts the credit card number. To do this, the browser usually uses encryption algorithms that are based on the theory of primes. For example: RSA asymmetric algorithm needs two large prime numbers p and q to calculate the module n. In the past, the primes were interesting because of their uniqueness. Euclid wrote the first definition of primes around 300 BC. The Sieve of Eratosthenes is the first known algorithm for deciding whether or not a number is a prime. In 1640, Fermat wrote his famous theorem to prove the primality of a number. The theorem is the basis of many subsequent algorithms. However, Fermat failed to prove the theorem, so that it was only later proved by Leibniz and Euler. My bachelor thesis gives an overview of algorithms for primality testing from antique times to the present. First of all, we give an overview of the naive algorithms that are simple but not suitable for large numbers. Then we describe probabilistic algorithms, i. e., algorithms that use random data. Next we describe the Solovay-Strassen and Miller-Rabin's algorithm and analyze their correctness and time complexity. The two algorithms are the most commonly used in practice. Even though these algorithms do not always return a correct answer, they are often used due to their speed. Next, we describe the deterministic algorithm AKS, discovered in 2002 by Agrawal, Kayal and Saxena. Although there have been several attempts, they are the first who succeeded to prove that the primality testing problem is in the class P of problems solvable in polynomial time. Their algorithm is a major achievement for theoretical computer science. Next, we describe the basic idea of the first version of the <b>AKS</b> <b>algorithm</b> and two improvements due to Lenstra-Pomerance and Bernstein. Finally, we describe in detail the sixth version of this algorithm from 2004. ...|$|E

