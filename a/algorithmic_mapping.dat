9|33|Public
40|$|Handover {{performance}} {{is very important}} when evaluating IP mobility protocols. If not performed efficiently, handover delays, jitters, and packet loss directly impact application performance. We propose a new architecture for providing efficient handover, while being able to coexist with other protocols. We propose a paradigm for multicast-based micromobility (M&M), where a visiting mobile is assigned a multicast address to use while moving within a domain. The multicast address is obtained using <b>algorithmic</b> <b>mapping,</b> and handover is achieved using multicast join/prune mechanisms...|$|E
40|$|This paper {{describes}} an FPGA {{implementation of a}} solution-counting solver for the N-Queens Puzzle. The proposed <b>algorithmic</b> <b>mapping</b> utilizes the fast carrychain logic found on modern FPGA architectures {{in order to achieve}} a regular and efficient design. From an initial full chessboard mapping, several optimization strategies are explored. Also, the infrastructure is described, which we have constructed for the computation of the currently unknown solution count of the 26 - Queens Puzzle. Finally, we compare the performance of our used concrete FPGA device mappings also in contrast to general-purpose CPUs...|$|E
40|$|One of {{the most}} {{important}} metrics in the design of IP mobility protocols is the handover performance. Handover occurs when a mobile node changes its network point-ofattachment from one access router to another. If not performed efficiently, handover delays, jitters and packet loss directly impact and disrupt applications and services. With the Internet growth and heterogeneity, it becomes crucial to design efficient handover protocols that are scalable, robust and incrementally deployable. The current Mobile IP (MIP) standard has been shown to exhibit poor handover performance. Most other work attempts to modify MIP to slightly improve its efficiency, while others propose complex techniques to replace MIP. Rather than taking these approaches, we instead propose a new architecture for providing efficient and smooth handover, while being able to co-exist and inter-operate with other technologies. Specifically, we propose an intra-domain multicast-based mobility architecture, where a visiting mobile is assigned a multicast address to use while moving within a domain. Efficient handover is achieved using standard multicast join/prune mechanisms. Two approaches are proposed and contrasted. The first introduces the concept proxy-based mobility, while the other uses <b>algorithmic</b> <b>mapping</b> to obtain the multicast address of visiting mobiles. We show that the <b>algorithmic</b> <b>mapping</b> approach has several advantages over the proxy approach, and provide mechanisms to support it. Network simulation (using NS- 2) is used to evaluate our scheme and compare it to other major approaches to micromobility; hierarchical mobile IP and seamless handover. Results show that our scheme, although simpler, outperforms the other schemes in over 94 % of the scenarios, and provides, on average, handover p [...] ...|$|E
40|$|Ecological Niche Modeling (ENM) is {{a branch}} of biology that uses {{algorithms}} to predict the distribution of species in a geographic area {{on the basis of}} a numerical representation of their preferred habitat and environment. <b>Algorithmic</b> <b>maps</b> can be produced for suitable or native habitats and require a review by human experts. During the review operation biologists use their knowledge about a species to modify the maps. They usually take <b>algorithmic</b> <b>maps</b> as starting point in the review. In this paper we provide a methodology for biologists to use the automatic maps as references also during and after the review process. Our approach is based on a comparison between the reviewed map and two systems: an expert system and a Feed Forward Neural Network. Furthermore we suggest an evaluation procedure of the quality of the environmental features used as training set, for assessing the models reliability...|$|R
40|$|We {{develop and}} study {{averaging}} schemes for solving fixed point and variational inequality problems. Typically, researchers have established convergence results for solution methods for these problems by establishing contractive estimates for their <b>algorithmic</b> <b>maps.</b> In this paper, we establish global convergence results using nonexpansive estimates. After first establishing convergence {{for a general}} iterative scheme for computing fixed points, we consider applications to projection and relaxation algorithms for solving variational inequality problems and to a generalized steepest descent method for solving systems of equations. As part of our development, we also establish a new interpretation of a norm condition typically used for establishing convergence of linearization schemes, by associating it with a strong-f-monotonicity condition. We conclude by applying our results to transportation networks...|$|R
40|$|Abstract – Game {{theory is}} a {{promising}} approach for the system-level analysis of power control in wireless networks. This paper extends game-theoretic analysis {{to the study}} of link adaptation, which involves the variation of modulation parameters in addition to power control. We use the link adaptation scheme in General Packet Radio Service (GPRS) as an example, although the basic approach is applicable to any centralized wireless system with power and rate control. The action space of a player in our proposed game, called the Link Adaptation Game (LAG), consists of power as usual, and a discrete-valued Adaptable Link Parameter (ALP), e. g. code rate. The utility function is a sigmoid, fitted to the throughput characteristics of a link adaptation scheme, priced by the square of power. We first show the existence of a Nash Equilibrium (NE) in the game. Next, we propose a distributed algorithm to discover the NE. The algorithm is analytically shown to converge to a NE by treating it as a point-to-set map. Simulation results using the GPRS system demonstrate superior throughput and fair system-wide allocation of resources in comparison with other non-game-theoretic methods. Index Terms – Adaptive modulation, <b>algorithmic</b> <b>map,</b> cellula...|$|R
40|$|One of {{the most}} {{important}} metrics in the design of IP mobility protocols is the handover performance. The current Mobile IP (MIP) standard has been shown to exhibit poor handover performance. Most other work attempts to modify MIP to slightly improve its efficiency, while others propose complex techniques to replace MIP. Rather than taking these approaches, we instead propose a new architecture for providing efficient and smooth handover, while being able to co-exist and inter-operate with other technologies. Specifically, we propose an intra-domain multicast-based mobility architecture, where a visiting mobile is assigned a multicast address to use while moving within a domain. Efficient handover is achieved using standard multicast join/prune mechanisms. Two approaches are proposed and contrasted. The first introduces the concept proxy-based mobility, while the other uses <b>algorithmic</b> <b>mapping</b> to obtain the multicast address of visiting mobiles. We show that the <b>algorithmic</b> <b>mapping</b> approach has several advantages over the proxy approach, and provide mechanisms to support it. Network simulation (using NS- 2) is used to evaluate our scheme and compare it to other routing-based micro-mobility schemes - CIP and HAWAII. The proactive handover results show that both M&M and CIP shows low handoff delay and packet reordering depth as compared to HAWAII. The reason for M&M's comparable performance with CIP is that both use bi-cast in proactive handover. The M&M, however, handles multiple border routers in a domain, where CIP fails. We also provide a handover algorithm leveraging the proactive path setup capability of M&M, which is expected to outperform CIP in case of reactive handover. Comment: 12 pages, 11 figure...|$|E
40|$|One very {{important}} metric in evaluation of IP mobility protocols is handover performance. Handover {{occurs when a}} mobile node changes its network point-of-attachment. If not performed efficiently, handover delays, jitters and packet loss directly impact applications and services. With the Internet growth and heterogeneity, it becomes crucial to design efficient handover protocols that are scalable, robust and incrementally deployable. Mobile IP (MIP) {{has been shown to}} exhibit poor handover performance during micro-mobility. We propose a new architecture for providing efficient and smooth handover, while being able to co-exist and inter-operate with other technologies. Specifically, we propose an intra-domain multicast-based mobility architecture, where a visiting mobile is assigned a multicast address to use while moving within a domain. Efficient ha dover is achieved using standard multicast join/prune mechanisms. Two approaches are proposed and contrasted. The first introduces the concept of proxy-based mobility, while the other uses <b>algorithmic</b> <b>mapping</b> to obtain the multicast address of visiting mobiles. We show that the <b>algorithmic</b> <b>mapping</b> approach has several advantages over the proxy approach, and provide mechanisms to support it. Simulations used to evaluate our scheme and compare it to other micro-mobility schemes- CIP and HAWAII. The proactive handover results show that both M&M and CIP show low handoff delay and packet reordering depth as compared to HAWAII. The reason for M&M’s comparable performance with CIP is that both use bi-cast in proactive handover. M&M, however, handles multiple border outers in a domain, where CIP fails. Also using a proactive path setup mechanism, we show that M&M clearly outperforms CIP in case of reactive handover. I...|$|E
40|$|Handover {{performance}} {{is very important}} when evaluating IP mobility protocols. If not performed efficiently, handover delays, jitters and packet loss directly impact application performance. We propose a new architecture for providing efficient handover, while being able to co-exist with other protocols. We propose a paradigm for multicast-based micro mobility (M&M), where a visiting mobile is assigned a multicast address to use while moving within a domain. The multicast address is obtained using <b>algorithmic</b> <b>mapping,</b> and handover is achieved using multicast join/prune mechanisms. This study outlines a framework for the design and evaluation of micro-mobility protocols. We define a suite of protocols (called CAR-set) to enable multiple access routers to receive traffic for the mobile node. By changing {{the number of such}} routers, timing and buffering parameters, the protocol may be fine-tuned for specific technologies (e. g., 802. 11) and handover scenarios. Extensive NS- 2 simulations are used to compare M&M to other micro-mobility schemes- CIP and HAWAII. For proactive handover scenarios, our results show that M&M and CIP show lower handover delay and packet reordering than HAWAII. M&M, however, handles multiple border routers in a domain, where CIP fails. Also, for scenarios of reactive handover and coverage gaps M&M clearly outperforms CIP and HAWAII...|$|E
40|$|Introduction. Modern {{engineers}} must possess {{high potential}} of cognitive abilities, in particular, the algorithmic thinking (AT). In this regard, {{the training of}} future experts (university graduates) of technical specialities has to provide the knowledge of principles and ways of designing of various algorithms, abilities to analyze them, and to choose the most optimal variants for engineering activity implementation. For full formation of AT skills {{it is necessary to}} consider all channels of psychological perception and cogitative processing of educational information: visual, auditory, and kinesthetic. The aim of the present research is theoretical basis of design, development and use of resources for successful development of AT during the educational process of training in programming. Methodology and research methods. Methodology of the research involves the basic thesis of cognitive psychology and information approach while organizing the educational process. The research used methods: analysis; modeling of cognitive processes; designing training tools that take into account the mentality and peculiarities of information perception; diagnostic efficiency of the didactic tools. Results. The three-level model for future engineers training in programming aimed at development of AT skills was developed. The model includes three components: aesthetic, simulative, and conceptual. Stages to mastering a new discipline are allocated. It is proved that for development of AT skills when training in programming it is necessary to use kinesthetic tools at the stage of mental <b>algorithmic</b> <b>maps</b> formation; <b>algorithmic</b> animation and <b>algorithmic</b> mental <b>maps</b> at the stage of algorithmic model and conceptual images formation. Kinesthetic tools for development of students’ AT skills when training in algorithmization and programming are designed. Using of kinesthetic training simulators in educational process provide the effective development of algorithmic style of thinking and increase the level of understanding and learning of educational material on algorithms and programming. Scientific novelty. The developed tools and methods for developing algorithmic style of thinking during the educational process of training in programming is fundamentally different from existing ones that are aimed at kinesthetic channels of perception and activation of motor-memory area. According to the latest statistics, over 40 % of people have kinesthetic sensing of the world; however, researchers have not treated this phenomenon in much detail. On the whole, the use efficiency of the didactic means when training graduates of engineering specialties has been proved {{in the course of the}} carried out experiment on kinesthetic tools introduction into educational process with the subsequent diagnostics of the levels of AT skills development, and the quality of training in programming among the students of theSiberianFederalUniversity. Practical significance. The proposed tools and methods for developing algorithmic thinking can be used in the training process in the school course of computer science, as well as university courses of programming of various kinds. The presented kinesthetic tools can be used for other technical and natural-science specialities (e. g. Mathematics) after applying specific content adaptation. </p...|$|R
40|$|In this paper, {{we present}} a new {{methodology}} towards performing high-level synthesis. During high-level synthesis an <b>algorithmic</b> description is <b>mapped</b> to a structure of hardware components. In our approach, high-level synthesis is performed via program transformations. All transformations are performed within a higher order logic theorem prover thus guaranteeing correctness. Our approach is not restricted to data flow graphs but supports arbitrary computable functions, i. e. mixed control/data flow graphs. Furthermore, the treatment of algorithmic and interface descriptions is orthogonalised, allowing systematic reuse of designs...|$|R
40|$|International audienceOccupancy Grids (OGs) are {{a popular}} {{framework}} for robotic perception. They were recently adopted for performing multisensor fusion and environment mapping for autonomous vehicles. However, high computational requirements strongly hinder their integration into less powerful automotive ECUs. To overcome this problem, we propose an <b>algorithmic</b> improvement for <b>mapping</b> range measurements into OGs. Experiments {{were conducted on}} a vehicle equipped with 16 LIDAR scans. Results demonstrate that a single-core ARM cortex A 9 can build now in real-time OGs that map urban traffic scenarios of 100 m-by- 100 m...|$|R
40|$|The key to {{introducing}} CORBA-based object-oriented {{technologies in}} the real-time telecommunications signaling environment {{is to ensure}} interoperability by maintaining the existing, standard protocols using Signaling System No. 7 (SS 7) for the external communication interface between telecommunication equipment. It is anticipated that there will initially be islands of CORBAbased implementations which will have to interwork with the more widely deployed “legacy ” systems. As signaling applications, such as Intelligent Networks (IN), are defined using the concepts and constructs of the Remote Operations Service (ROS), interworking requires that a gateway serve as a “translator ” of operation invocations and their returns between the ROS and CORBA domains. Previous work by the authors had defined the “specifications translations ” for such a gateway, i. e., the <b>algorithmic</b> <b>mapping</b> of various ROS concepts and constructs defined in ASN. 1 to those of CORBA using IDL. In this paper we provide the “interaction specifications ” for such a gateway, namely the dynamic aspects of ROS-to-CORBA mappings. These include extending the CORBA object services to support the ROS concepts of “association contract” and “connection package ” {{as well as the}} mapping for the respective naming schema as well as the message formats in the two domains...|$|E
40|$|Reconfigurable {{computing}} is a {{new paradigm}} based on dynamically adapting the hardware to reconfigure the computation and communication structures on the chip. Re-configurable circuits and systems have evolved fromapplication specific accelerators to a general purpose computing paradigm. Various reconfigurable devices have been de-veloped by researchers and the industry. These devices promise {{a high degree of}} flexi-bility and superior performance. But, the algorithmic techniques and software tools are also heavily based on the hardware paradigm from which they have evolved. This thesis addresses the fundamental challenges in achieving high performance us-ing reconfigurable architectures. The diverse range of issues in mapping applications onto reconfigurable architectures are identified. A formal framework for mapping ap-plication tasks onto reconfigurable architectures is proposed in this thesis. The pro-posed framework includes a parameterized system level model, <b>algorithmic</b> <b>mapping</b> techniques and system level interpretive simulation environment. A parameterized model of hybrid reconfigurable architectures, Hybrid System Ar-chitecture Model (HySAM), is developed to facilitate application mapping. Hybrid re-configurable architectures include traditional processing units and memory on the same die as reconfigurable logic. The parameterized abstract model, HySAM, is general enough to capture a wide range of configurable systems. Loop statements in traditional pro-grams consist of regular, repetitive computations which are the most likely candidates for performance enhancement using configurable hardware. This thesis develops a for-mal methodology for mapping loops onto reconfigurable architectures. The abstract model is used to define and solve the problem of mapping loop statements onto reconfig-urable architectures. The complexity of the problems and our proposed solutions is also addressed. Performance improvements are achieved on various architectures using our algorithmic techniques for mapping. In addition, existing design and simulation tools do not include the reconfiguration aspect in their methodology. A simulation methodology for reconfigurable architectures is proposed and validated by implementing a proof of concept tool. The Dynamically Reconfigurable systems Interpretive simulation and Vi-sualization Environment (DRIVE) facilitates high level performance evaluation frame-work for design space exploration...|$|E
40|$|Abstract. In {{the course}} of {{developing}} an ontology-based data integration system (OBDI) that includes automatic integration of data sources, and thus, includes <b>algorithmic</b> ontology <b>mapping,</b> we have made the following observations. A mapping method may determine that an entity in one ontology maps with equal likelihood to two or more entities in the other ontology. The mapping and reformulation of certain queries is correct only if one pairing is chosen. The correct choice may be different for different queries. Finally, the query itself may lend additional semantics that correctly resolve the ambiguity. These observations suggest a targeted ontology mapping problem, query-specific ontology mapping. In addition to the two ontologies, a query serves as a third argument to the mapping algorithm. Further, the mapping algorithm need not produce a complete mapping, but only a partial mapping sufficient to correctly reformulate the query. We detail a number of open issues on how this problem statement might be refined, and consider features of its evaluation. Ambiguity in Ontology Mapping: Consider the idealized representation (Fig. 1...|$|R
40|$|Abstract. The {{advent of}} {{high-throughput}} sequencing technologies con-stituted a major advance in genomic studies, offering new prospects {{in a wide}} range of applications. We propose a rigorous and flexible <b>algorithmic</b> solution to <b>mapping</b> SOLiD color-space reads to a reference genome. The solution relies on an advanced method of seed design that uses a faith-ful probabilistic model of read matches and, on the other hand, a novel seeding principle especially adapted to read mapping. Our method can handle both lossy and lossless frameworks and is able to distinguish, at the level of seed design, between SNPs and reading errors. We illustrate our approach by several seed designs and demonstrate their efficiency. ...|$|R
40|$|Abstruct-We {{describe}} a new <b>algorithmic</b> framework for <b>mapping</b> CMOS circuit diagrams into area-efficient, high-performance layouts {{in the style}} of one-dimensional transistor arrays. Using efficient search techniques and accurate evaluation methods, the huge solution space that is typical to such problems is traversed extremely fast, yielding designs of hand-layout quality. In addition to generating circuits. that meet prespecified layout constraints {{in the context of a}} fixed target im-age, on-the-fly optimizations are performed to meet secondary opti-mization criteria. A practical dynamic programming routing algo-rithm is employed to accommodate the special conditions that arise in this context. This algorithm has been implemented and is currently used at IBM for cell library generation. I...|$|R
40|$|This paper {{presents}} a software package EDGE, an Earthquake Damage Generator/Estimator for Toscana, Italy. EDGE creates samples of multidimentional distributions of damage using models of geophysical processes, seismic-geophysical data and {{a catalog of}} vulnerability of buildings in the region. The main <b>algorithmic</b> elements: seismic <b>maps,</b> geophysical formulas, and stochastic modeling, are described in detail. The work contributes to a joint research program of Dynamic Systems, and Risk, Modeling and Society projects on data-based methodological support for decision making in the insurance industry against risks of natural catastrophes. The designed catalogs of expected damages {{can be used for}} actuarial calculations and optimization of the regional insurance portfolio...|$|R
40|$|The {{advent of}} {{high-throughput}} sequencing technologies constituted a major advance in genomic studies, offering new prospects {{in a wide}} range of applications. We propose a rigorous and flexible <b>algorithmic</b> solution to <b>mapping</b> SOLiD color-space reads to a reference genome. The solution relies on an advanced method of seed design that uses a faithful probabilistic model of read matches and, on the other hand, a novel seeding principle especially adapted to read mapping. Our method can handle both lossy and lossless frameworks and is able to distinguish, at the level of seed design, between SNPs and reading errors. We illustrate our approach by several seed designs and demonstrate their efficiency...|$|R
40|$|AbstractComposition of <b>Map</b> and Reduce <b>algorithmic</b> skeletons {{have been}} widely studied {{at the end of}} the last century and it has {{demonstrated}} effective on a wide class of problems. We recall the theoretical results motivating the introduction of these skeletons, then we discuss an experiment implementing three <b>algorithmic</b> skeletons, a <b>map,</b> a reduce and an optimized composition of a map followed by a reduce skeleton (map+reduce). The map+reduce skeleton implemented computes the same kind of problems computed by Google MapReduce, but the data flow through the skeleton is streamed rather than relying on already distributed (and possibly quite large) data items. We discuss the implementation of the three skeletons on top of ProActive/GCM in the MareMare prototype and we present some experimental obtained on a COTS cluster...|$|R
40|$|In {{the life}} sciences, genomic {{databases}} for sequence search {{have been growing}} exponentially in size. As a result, faster sequencesearch algorithms to search these databases continue to evolve to cope with algorithmic time complexity. The ubiquitous tool for such search is the Basic Local Alignment Search Tool (BLAST) [1] from the National Center for Biotechnology Information (NCBI). Despite continued algorithmic improvements in BLAST, it cannot {{keep up with the}} rate at which the database is exponentially increasing in size. Therefore, parallel implementations such as mpiBLAST have emerged to address this problem. The performance of such implementations depends on a myriad of factors including <b>algorithmic,</b> architectural, and <b>mapping</b> of the algorithm to the architecture. This paper describes modifications and extensions to a parallel and distributed-memory version o...|$|R
40|$|This paper {{presents}} {{the design and}} implementation of several fundamental dense linear algebra (DLA) algorithms in OpenCL. In particular, these are linear system solvers and eigenvalue problem solvers. Further, we give {{an overview of the}} clMAGMA library, an open source, high performance OpenCL library that incorporates the developments presented, and in general provides to heterogeneous architectures the DLA functionality of the popular LAPACK library. The LAPACK-compliance and use of OpenCL simplify the use of clMAGMA in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance OpenCL BLAS, hardware and OpenCL-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and <b>mapping</b> <b>algorithmic</b> requirements to the architectural strengths of the various heterogeneous hardware components...|$|R
40|$|OBJECTIVE: Identify the lexical {{content of}} a large corpus of {{ordinary}} medical records to assess the feasibility of large-scale natural language processing. METHODS: A corpus of 560 megabytes of medical record text from an academic medical center was broken into individual words and compared with the words in six medical vocabularies, a common word list, and a database of patient names. Unrecognized words were assessed for algorithmic and contextual approaches to identifying more words, while the remainder were analyzed for spelling correctness. RESULTS: About 60 % of the words occurred in the medical vocabularies, common word list, or names database. Of the remainder, one-third were recognizable by other means. Of the remaining unrecognizable words, over three-fourths represented correctly spelled real words and the rest were misspellings. CONCLUSIONS: Large-scale generalized natural language processing methods for the medical record will require expansion of existing vocabularies, spelling error correction, and other <b>algorithmic</b> approaches to <b>map</b> words into those from clinical vocabularies...|$|R
40|$|Abstract—To achieve high {{performance}} on modern computers, {{it is vital}} to <b>map</b> <b>algorithmic</b> parallelism to that inherent in the hardware. From an application developer’s perspective, it is also important that code can be maintained in a portable manner across a range of hardware. Here we present targetDP (target Data Parallel), a lightweight programming layer that allows the abstraction of data parallelism for applications that employ structured grids. A single source code may be used to target both thread level parallelism (TLP) and instruction level parallelism (ILP) on either SIMD multi-core CPUs or GPU-accelerated platforms. targetDP is implemented via standard C preprocessor macros and library functions, can be added to existing applications incrementally, and can be combined with higher-level paradigms such as MPI. We present CPU and GPU performance results for a benchmark taken from the lattice Boltzmann application that motivated this work. These demon-strate not only performance portability, but also the optimisation resulting from the intelligent exposure of ILP. I...|$|R
40|$|An {{important}} {{tool in the}} analysis of genomic sequences is the physical map. In this paper we examine the construction of physical maps from hybridization data between STS (sequence tag sites) probes and clones of genomic fragments. An algorithmic theory of the mapping process, a proposed performance evaluation procedure, and several new <b>algorithmic</b> strategies for <b>mapping</b> are given. A unifying theme for these developments is the idea of a "conservative extension. " An algorithm, measure of algorithm quality, or description of physical map is a conservative extension if it is a generalization for data with errors of a corresponding concept in the error-free case. In our algorithmic theory we show that the nature of hybridization experiments imposes inherent limitations on the mapping information recorded in the experimental data. We prove that only certain types of mapping information can be reliably calculated by any algorithm. A test generator is then presented along with quantitative m [...] ...|$|R
40|$|To achieve high {{performance}} on modern computers, {{it is vital}} to <b>map</b> <b>algorithmic</b> parallelism to that inherent in the hardware. From an application developer's perspective, it is also important that code can be maintained in a portable manner across a range of hardware. Here we present targetDP (target Data Parallel), a lightweight programming layer that allows the abstraction of data parallelism for applications that employ structured grids. A single source code may be used to target both thread level parallelism (TLP) and instruction level parallelism (ILP) on either SIMD multi-core CPUs or GPU-accelerated platforms. targetDP is implemented via standard C preprocessor macros and library functions, can be added to existing applications incrementally, and can be combined with higher-level paradigms such as MPI. We present CPU and GPU performance results for a benchmark taken from the lattice Boltzmann application that motivated this work. These demonstrate not only performance portability, but also the optimisation resulting from the intelligent exposure of ILP. Comment: 4 pages, 1 figure, to appear in proceedings of HPCC 2014 conferenc...|$|R
40|$|GPUs {{have been}} gaining {{popularity}} as general purpose parallel processors that deliver a performance to cost ratio superior {{to that of}} CPUs. However, programming on GPUs has remained a specialised area, as it often requires significant knowledge about the GPU architecture and platform-specific parallelisation of the algorithms that are implemented. Furthermore, the dominant programming models on GPUs limit functional decomposition of programs, as they require programmers to write separate functions to run on GPUs. I present and quantitatively evaluate a GPU programming system that provides a high-level abstraction to facilitate the use of GPUs for general purpose array processing. The presented programming system liberates programmers of low-level details and enables functional decomposition of programs, whilst providing the facilities to sufficiently exploit the parallel processing capability of GPUs. Fundamentally, the presented programming system allows programmers {{to focus on what}} to program on GPUs instead of how to program GPUs. The approach is based on <b>algorithmic</b> skeletons <b>mapped</b> to higher-order functions, which are commonly available in functional programming languages. The presented programming system (1) encapsulates the low-level control of GPUs and the GPU-specific parallelisation of the higher-order functions in algorithmic skeletons, (2) employs an embedded domain specific language as the interface that treats the parallel operations as expressions with parallel semantics, allowing functional decomposition of programs, and (3) manages the compilation of the embedded domain specific language into algorithmic skeleton instances and the execution of the algorithmic skeleton instances online. Programmers only need to write the operations to be executed on GPUs as expressions in the embedded domain specific language, and the presented programming system takes care of the rest. Although the concrete implementation presented in the thesis involves an embedded domain specific language in Haskell, Accelerate, and a specific GPU platform, CUDA, the approach in this thesis can be applied to other similar configurations with appropriate adjustments...|$|R
40|$|This paper {{presents}} {{the design and}} implementation of sev-eral fundamental dense linear algebra (DLA) algorithms for multicore with Intel Xeon Phi Coprocessors. In particular, we consider algorithms for solving linear systems. Further, we give {{an overview of the}} MAGMA MIC library, an open source, high performance library that incorporates the de-velopments presented here, and, more broadly, provides the DLA functionality equivalent to that of the popular LA-PACK library while targeting heterogeneous architectures that feature a mix of multicore CPUs and coprocessors. The LAPACK-compliance simplifies the use of the MAGMA MIC library in applications, while providing them with portably performant DLA. High performance is obtained through the use of the high-performance BLAS, hardware-specific tuning, and a hybridization methodology whereby we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly sched-uled over the heterogeneous hardware by minimizing data movements and <b>mapping</b> <b>algorithmic</b> requirements to the architectural strengths of the various heterogeneous hard-ware components. Our methodology and programming techniques are incorporated into the MAGMA MIC API, which abstracts the application developer from the specifics of the Xeon Phi architecture and is therefore applicable to algorithms beyond the scope of DLA...|$|R
40|$|Abstract. This paper {{presents}} {{the design and}} implementation of several fundamental dense linear algebra (DLA) algorithms for multicore with Intel Xeon Phi Coprocessors. In particular, we consider algorithms for solving linear systems. Further, we give {{an overview of the}} MAGMA MIC library, an open source, high performance library that incorporates the developments presented, and in general provides to heterogeneous architectures of multicore with coprocessors the DLA functionality of the popular LAPACK library. The LAPACK-compliance simplifies the use of the MAGMA MIC library in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance BLAS, hardware-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and <b>mapping</b> <b>algorithmic</b> requirements to the architectural strengths of the various heterogeneous hardware components. Our methodology and programming techniques are incorporated into the MAGMA MIC API, which abstracts the application developer from the specifics of the Xeon Phi architecture and is therefore applicable to algorithms beyond the scope of DLA. ...|$|R
40|$|Embedded {{software}} designers {{often use}} libraries {{that have been}} pre-optimized for a given processor to achieve higher code quality. However, using such libraries in legacy code optimization is nontrivial and typically requires manual intervention. This paper presents a methodology that <b>maps</b> <b>algorithmic</b> constructs of the software specification to a library of complex software elements. This library-mapping step is automated by using symbolic algebra techniques. We illustrate the advantages of our methodology by optimizing an algorithmic level description of MPEG Layer III (MP 3) audio decoder for the Badge 4 [2] portable embedded system. During the optimization process we use commercially available libraries with complex elements ranging from simple mathematical functions such as exp to the IDCT routine. We implemented and measured the performance and energy consumption of the MP 3 decoder software on Badge 4 running embedded Linux operating system. The optimized MP 3 audio decoder runs 300 {{times faster than the}} original code obtained from the standards body while consuming 400 times less energy. Since our optimized MP 3 decoder runs 3. 5 times faster than real-time, additional energy can be saved by using processor frequency and voltage scaling...|$|R
40|$|The {{issue of}} {{structural}} and semantic interoperability among learning objects {{and other resources}} on the Internet is increasingly pointing towards Semantic Web technologies in general and ontology in particular as a solution provider. Ontology defines an explicit formal specification of domains to learning objects. However, the effectiveness to interoperate learning objects among various learning object repositories are often reduced due {{to the use of}} different ontological schemes to annotate learning objects in each learning object repository. Hence, structural differences and semantic heterogeneity between ontologies need to be resolved in order to generate shared ontology to facilitate learning object reusability. This paper presents OntoDNA, an automated ontology mapping and merging tool. Significance of the study lies in an <b>algorithmic</b> framework for <b>mapping</b> the attributes of concepts/ learning objects and merging these concepts/ learning objects from different ontologies based on the mapped attributes; identification of a suitable threshold value for mapping and merging; an easily scalable unsupervised data mining algorithm for modeling existing concepts and predicting the cluster to which a new concept/ learning object should belong, easy indexing, retrieval and visualization of concepts and learning objects based on the merged ontology...|$|R
40|$|Abstract Numerical {{methods for}} {{elliptic}} partial differential equations (PDEs) within both continuous (CG) and hybridized discontinuous Galerkin (HDG) frameworks {{share the same}} general structure: local (elemental) matrix generation followed by a global linear system assembly and solve. The lack of inter-element commu-nication and easily parallelizable nature of the local matrix generation stage coupled with the parallelization techniques developed for the linear system solvers make a numerical scheme for elliptic PDEs a good candi-date for implementation on streaming architectures such as modern graphical processing units (GPUs). We propose an <b>algorithmic</b> pipeline for <b>mapping</b> an elliptic finite element method to the GPU and perform a case study for a particular method within the HDG framework. This study provides comparison between CPU and GPU implementations of the method as well as highlights certain performance-crucial implementation details. The choice of the HDG method for the case study was dictated by the computationally-heavy local matrix generation stage {{as well as the}} reduced trace-based communication pattern, which together make the method amenable to the fine-grained parallelism of GPUs. We demonstrate that the HDG method is well-suited for GPU implementation, obtaining total speedups on the order of 30 - 35 times over a serial CPU implementation for moderately sized problems...|$|R
40|$|Both authors contributed {{equally to}} this work. Abstract: The {{increased}} number of genomes being sequenced offers new opportunities for the mapping of closely related organisms. We propose an algorithmic formalization of a genome comparison approach to marker ordering. In order to integrate a comparative mapping approach in the <b>algorithmic</b> process of <b>map</b> construction and selection, we extend the usual statistical model describing the experimental data, here radiation hybrids (RH) data, in a statistical framework that models additionally the evolutionary relationships between a proposed map and a reference map: an existing map of the corresponding orthologous genes or markers in a closely related organism. This has concretely the effect of exploiting, {{in the process of}} map selection, the information of marker adjacencies in the related genome when the information provided by the experimental data is not conclusive for the purpose of ordering. In order to compute efficiently the map, we proceed to a reduction of the maximum likelihood estimation to the Traveling Salesman Problem. Experiments on simulated RH data sets as well as on a real RH data set from the canine RH project show that maps produced using the likelihood defined by the new model are significantly better than maps built using the traditional RH model...|$|R
40|$|This thesis {{presents}} an abstract {{model of the}} mammalian neocortex. The model was constructed by taking a top-down view on the cortex, where {{it is assumed that}} cortex to a first approximation works as a system with attractor dynamics. The model deals with the processing of static inputs from the perspectives of biological <b>mapping,</b> <b>algorithmic,</b> and physical implementation, but it does not consider the temporal aspects of these inputs. The purpose of the model is twofold: Firstly, it is an abstract model of the cortex and as such {{it can be used to}} evaluate hypotheses about cortical function and structure. Secondly, it forms the basis of a general information processing system that may be implemented in computers. The characteristics of this model are studied both analytically and by simulation experiments, and we also discuss its parallel implementation on cluster computers as well as in digital hardware. The basic design of the model is based on a thorough literature study of the mammalian cortex’s anatomy and physiology. We review both the layered and columnar structure of cortex and also the long- and short-range connectivity between neurons. Characteristics of cortex that defines its computational complexity such as the time-scales of cellular processe...|$|R
40|$|Motivation: Genome {{maps are}} {{fundamental}} {{to the study of}} an organism and essential in the process of genome sequencing which in turn provides the ultimate map of the genome. The increased number of genomes being sequenced offers new opportunities for the mapping of closely related organisms. We propose here an algorithmic formalization of a genome comparison approach to marker ordering. Results: In order to integrate a comparative mapping approach in the <b>algorithmic</b> process of <b>map</b> construction and selection, we propose to extend the usual statistical model describing the experimental data, here radiation hybrids (RH) data, in a statistical framework that models additionally the evolutionary relationships between a proposed map and a reference map: an existing map of the corresponding orthologous genes or markers in a closely related organism. This has concretely the effect of exploiting, in the process of map selection, the information of marker adjacencies in the related genome when the information provided by the experimental data is not conclusive for the purpose of ordering. In order to compute efficiently the map, we proceed to a reduction of the maximum likelihood estimation to the Traveling Salesman Problem. Experiments on simulated RH data sets as well as on a real RH data set from the canine RH project show that maps produced using the likelihood defined by the new model are significantly better than maps built using the traditional RH model. Availability: The comparative mapping approach is available in the last version of (de Givry et al., 2004), a free 1 mapping software in C++, including LKH (Helsgaun, 2000) for maximum likelihood computation...|$|R
40|$|This thesis {{investigates the}} role of cross-modal {{correspondence}} within audiovisual composition, presenting both a conceptual model and a methodological framework {{for the creation of}} abstract audiovisual art. While this research is specifically aimed at the field of abstract digital animation it is also intended to act as a platform for the future development of concurrent audiovisual synthesis techniques within the general field of audiovisual art. Referencing literature regarding the psychophysiological bases for audiovisual integration, it is argued that temporal congruence offers a mechanism for the manipulation of cross-modal correspondence within audiovisual media. Further to this, electroacoustic and formalist theory is discussed with specific reference to the interrelationship of medium structures to enable the identification of a conceptual model for audiovisual composition. Referencing theory from the fields of musical instrument design and <b>algorithmic</b> composition, parameter <b>mapping</b> is identified as a mechanism for the modulation of temporal congruence. Its implementation within audiovisual composition is then discussed. Derived from both this and a conceptual parallel between the organisational structures of audio grains and visual particles, the audiovisual particles framework is presented as a methodological basis for the creation of abstract audiovisual art. The presented theory is supported by a series of demonstrative studies exploring both the practical application of the audiovisual particles framework and {{the role of}} parameter mapping within the process of audiovisual media generation. Experiential observations are discussed for each to inform future praxis. In addition, two audiovisual compositions are presented as both implementations of developed theory and as artworks in their own right...|$|R
