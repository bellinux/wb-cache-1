3510|738|Public
5|$|Subsequently, Denis Sargan and Alok Bhargava {{extended}} {{the results for}} testing if the errors on a regression model follow a Gaussian random walk (i.e., possess a unit root) against the alternative {{that they are a}} stationary first order <b>autoregression.</b>|$|E
5|$|Von Neumann made {{fundamental}} {{contributions to}} mathematical statistics. In 1941, he derived the exact {{distribution of the}} ratio of the mean square of successive differences to the sample variance for independent and identically normally distributed variables. This ratio was applied to the residuals from regression models and is commonly known as the Durbin–Watson statistic for testing the null hypothesis that the errors are serially independent against the alternative that they follow a stationary first order <b>autoregression.</b>|$|E
25|$|Note {{that the}} ARMA {{model is a}} {{univariate}} model. Extensions for the multivariate case are the vector <b>autoregression</b> (VAR) and Vector <b>Autoregression</b> Moving-Average (VARMA).|$|E
40|$|We {{perform an}} out-of-sample real time {{forecasting}} exercixe using a model with the unemployment rate, inflation {{and the short}} term interest rate. We compare the time-varying <b>autoregressions</b> forecast with those produced by alternative fixed coefficient models. Our findings show that TV-VAR are very powerful in forecasting. In particular the time-varying <b>autoregressions</b> model is the only model producing forecasts that are accurate for all the three variables. Moreover the time-varying <b>autoregressions</b> model produces {{by far the most}} accurate forecast for inflation...|$|R
40|$|We {{construct}} bootstrap prediction intervals for linear <b>autoregressions,</b> nonlinear <b>autoregressions,</b> nonparametric <b>autoregressions</b> and Markov processes. Several {{forward and}} backward bootstrap methods using predictive residuals and fitted residuals are introduced and applied to those time series. We describe exact algorithms for these different models and show that the bootstrap intervals properly estimate {{the distribution of the}} future values. In simulations using standard time series models, we compare the prediction intervals of different methods with regards to coverage level and length of interva...|$|R
40|$|This paper {{proposes a}} new method for {{approximating}} vector <b>autoregressions</b> by a finite-state Markov chain. The method is more robust {{to the number}} of discrete values and tends to outperform the existing methods over a wide range of the parameter space, especially for highly persistent vector <b>autoregressions</b> with roots near the unit circle. ...|$|R
25|$|In the {{statistical}} analysis of time series, autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the <b>autoregression</b> and the second for the moving average. The general ARMA model was described in the 1951 thesis of Peter Whittle, Hypothesis testing in time series analysis, and it was popularized in the 1970 book by George E. P. Box and Gwilym Jenkins.|$|E
25|$|However, Stock and Watson used a four {{variable}} vector <b>autoregression</b> {{model to}} analyze output volatility {{and concluded that}} stability increased due to economic good luck. Stock and Watson {{believed that it was}} pure luck that the economy didn’t react violently to the economic shocks during the Great Moderation. While there were numerous economic shocks, there is very little evidence that these shocks are as large as prior economic shocks.|$|E
25|$|In {{statistic}}s, the Durbin–Watson statistic is a {{test statistic}} used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Later, John Denis Sargan and Alok Bhargava developed several von Neumann–Durbin–Watson type test statistics for the null hypothesis that the errors on a regression model follow a process with a unit root against the alternative hypothesis that the errors follow a stationary first order <b>autoregression</b> (Sargan and Bhargava, 1983). Note that the distribution of this test statistic {{does not depend on}} the estimated regression coefficients and the variance of the errors.|$|E
40|$|Vector <b>autoregressions</b> {{are used}} to {{evaluate}} dynamic relationships among prices in six important international wheat markets. The effects of freight rates and exchange rates are also considered. The results indicate that Canada is the dominant market, supporting views that Canada is an oligopolistic price leader. international trade, vector <b>autoregressions,</b> wheat markets, International Relations/Trade,...|$|R
40|$|A {{forecasting}} {{comparison is}} undertaken in which 49 univariate forecasting methods, plus various forecast pooling procedures, {{are used to}} forecast 215 U. S. monthly macroeconomic time series at three forecasting horizons over the period 1959 - 1996. All forecasts simulate real time implementation, that is, they are fully recursive. The forecasting methods are based on four classes of models: <b>autoregressions</b> (with and without unit root pretests), exponential smoothing, artificial neural networks, and smooth transition <b>autoregressions.</b> The best overall performance of a single method is achieved by <b>autoregressions</b> with unit root pretests, but this performance can be improved when it is combined with the forecasts from other methods. ...|$|R
40|$|A {{class of}} {{processes}} {{with a time}} varying spectral representation is established. As an example we study time varying <b>autoregressions.</b> Several results on the asymptotic norm behaviour and trace behaviour of covariance matrices of such processes are derived. As a consequence we prove a Kolmogorov formula for the local prediction error and calculate the asymptotic Kullback-Leibler information divergence. Locally stationary processes Evolutionary spectra Kullback-Leibler divergence time varying <b>autoregressions...</b>|$|R
2500|$|Some authors, {{including}} Box, Jenkins & Reinsel use {{a different}} convention for the <b>autoregression</b> coefficients. This allows all the polynomials involving the lag operator to appear in a similar form throughout. Thus the ARMA model would be written as ...|$|E
2500|$|Lu's {{interests}} in geology-related phenemena also include paleontology, {{which led to}} collaboration with his college and grad-school roommate Motohiro Yogo and Prof. Charles Marshall. Leveraging vector <b>autoregression</b> analysis upon an established marine fossil record, Lu, Yogo and Marshall found that a [...] "speed limit," [...] which was previously thought to restrict the reemergence of biodiversity following a mass extinction, may be an artifact of the incompleteness of the fossil record.|$|E
5000|$|... where B is the {{backshift}} operator, where [...] is {{the function}} defining the <b>autoregression,</b> and where [...] are the coefficients in the <b>autoregression.</b>|$|E
40|$|In {{this paper}} {{we use a}} battery of various mixed-frequency data models to {{forecast}} Czech GDP. The models employed are mixed-frequency vector <b>autoregressions,</b> mixed-data sampling models, and the dynamic factor model. Using a dataset of historical vintages of unrevised macroeconomic and financial data, we evaluate the performance of these models over the 2005 – 2012 period and compare them with the Czech National Bank’s macroeconomic forecasts. The results suggest that for shorter forecasting horizons {{the accuracy of the}} dynamic factor model is comparable to the CNB forecasts. At longer horizons, mixed-frequency vector <b>autoregressions</b> are able to perform similarly or slightly better than the CNB forecasts. Furthermore, moving away from point forecasts, we also explore the potential of density forecasts from Bayesian mixed-frequency vector <b>autoregressions...</b>|$|R
40|$|Abstract: This paper {{proposes a}} moment-matching method for {{approximating}} vector <b>autoregressions</b> by finite-state Markov chains. The Markov chain is constructed by targeting the conditional {{moments of the}} underlying continuous process. The proposed method is more robust {{to the number of}} discrete values and tends to outperform the existing methods for approximating multivariate processes over a wide range of the parameter space, especially for highly persistent vector <b>autoregressions</b> with roots near the unit circle...|$|R
40|$|The real GDP {{series of}} sixteen European {{countries}} along with Japan, Canada and the US are {{examined in this}} paper by means of fractional integration techniques. The results crucially depend on how we specify the I(0) disturbances, as white noise or <b>autoregressions.</b> Thus, in the former case the orders of integration are higher than 1 in all cases, while using <b>autoregressions</b> the values are all strictly smaller than 1 implying mean reverting behaviour. ...|$|R
50|$|Note {{that the}} ARMA {{model is a}} {{univariate}} model. Extensions for the multivariate case are the Vector <b>Autoregression</b> (VAR) and Vector <b>Autoregression</b> Moving-Average (VARMA).|$|E
50|$|In statistics, Bayesian vector <b>autoregression</b> (BVAR) uses Bayesian {{methods to}} {{estimate}} a vector <b>autoregression</b> (VAR). In that respect, the difference with standard VAR models {{lies in the}} fact that the model parameters are treated as random variables, and prior probabilities are assigned to them.|$|E
5000|$|Next, the <b>autoregression</b> is {{augmented}} by including lagged values of x: ...|$|E
40|$|This paper {{proposes a}} moment-matching method for {{approximating}} vector <b>autoregressions</b> by finite-state Markov chains. The Markov chain is constructed by targeting the conditional {{moments of the}} underlying continuous process. The proposed method is more robust {{to the number of}} discrete values and tends to outperform the existing methods for approximating multivariate processes over a wide range of the parameter space, especially for highly persistent vector <b>autoregressions</b> with roots near the unit circle. Markov Chain, Vector Autoregressive Processes, Functional Equation, Numerical Methods, Moment Matching...|$|R
40|$|A {{family of}} cointegrated vector autoregressive models with {{adjusted}} short-run dynamics is introduced. These models can describe evolving short-run dynamics {{in a more}} flexible way than standard vector <b>autoregressions,</b> and yet likelihood analysis is based on reduced rank regression using conventional asymptotic tables. The family of dynamics-adjusted vector <b>autoregressions</b> consists of three models: a model subject to short-run parameter changes, a model with partial short-run dynamics and a model with short-run explanatory variables. An empirical illustration using US gasoline prices is presented, together with some simulation experiments...|$|R
40|$|We {{propose the}} unified {{approach}} {{to construct the}} non–informative prior for time–series econometric models that are invariant under some group of transformations. We show that this invariance property characterizes {{some of the most}} popular models hence the applicability of the proposed framework is quite general. The suggested prior enjoys many desirable properties both from the Bayesian and non–Bayesian perspective. We provide detailed derivations of our prior in many standard time–series models including, <b>AutoRegressions</b> (AR), Vector <b>AutoRegressions</b> (VAR), Structural VAR and Error Correction Models (ECM). ...|$|R
50|$|This {{function}} {{is used to}} determine the appropriate lag length for an <b>autoregression.</b>|$|E
5000|$|As an example, {{suppose a}} discrete-time signal [...] is modeled as an autoregressive process,where [...] is {{additive}} white Gaussian noise and [...] are unknown deterministic parameters (the <b>autoregression</b> parameters). Then, from observations of , {{it is of}} interest to estimate the <b>autoregression</b> parameters. This may be done using the method of least squares, or using the more computationally efficient Yule-Walker equations.|$|E
5000|$|Veksler, A. (1997). Risk-effective {{estimation}} of an <b>autoregression</b> parameter. Problems of Information Transmission V.33, N2,124-138 ...|$|E
40|$|This paper {{develops}} {{methods for}} automatic selection of variables in forecasting Bayesian vector <b>autoregressions</b> (VARs) using the Gibbs sampler. In particular, I provide computationally efficient algorithms for stochastic variable selection in generic (linear and nonlinear) VARs. The {{performance of the}} proposed variable selection method is assessed in a small Monte Carlo experiment, and in forecasting 4 macroeconomic series of the UK using time-varying parameters vector <b>autoregressions</b> (TVP-VARs). Restricted models consistently improve upon their unrestricted counterparts in forecasting, showing the merits of variable selection in selecting parsimonious models. ...|$|R
40|$|We discuss {{intrinsic}} <b>autoregressions</b> for a first-order neighbourhood on {{a two-dimensional}} rectangular lattice and give an exact formula for the variogram that extends known {{results to the}} asymmetric case. We obtain a corresponding asymptotic expansion that is more accurate and more general than previous ones and use this to derive the de Wijs variogram under appropriate averaging, a result that {{can be interpreted as}} a two-dimensional spatial analogue of Brownian motion obtained as the limit of a random walk in one dimension. This provides a bridge between geostatistics, where the de Wijs process was once the most popular formulation, and Markov random fields, and also explains why statistical analysis using intrinsic <b>autoregressions</b> is usually robust to changes of scale. We briefly describe corresponding calculations in the frequency domain, including limiting results for higher-order <b>autoregressions.</b> The paper closes with some practical considerations, including applications to irregularly-spaced data. Copyright 2005, Oxford University Press. ...|$|R
40|$|This paper first {{characterizes the}} {{predictable}} components in excess rates of returns on major equity and foreign-exchange markets using lagged excess returns, dividend yields, and forward premiums as instruments. Vector <b>autoregressions</b> demonstrate one-step-ahead predictability and facilitate calculations of implied long-horizon statistics, such as variance ratios. Estimation of latent variable models then subjects the vector <b>autoregressions</b> to constraints derived from dynamic asset pricing theories. Examination of volatility bounds on intertemporal marginal rates of substitution provides summary statistics that quantify the challenge facing dynamic asset pricing models. Copyright 1992 by American Finance Association. ...|$|R
50|$|Recent {{research}} has shown that Bayesian vector <b>autoregression</b> is an appropriate tool for modelling large data sets.|$|E
50|$|This page {{shows the}} details for {{different}} matrix notations of a vector <b>autoregression</b> process with k variables.|$|E
5000|$|Vector <b>autoregression</b> {{involves}} simultaneous regressions {{of various}} time series variables {{on their own}} and each other's lagged values.|$|E
40|$|This paper compares several {{time series}} methods for short-run {{forecasting}} of Euro-wide inflation and real activity {{using data from}} 1982 - 1997. Forecasts are constructed from univariate <b>autoregressions,</b> vector <b>autoregressions,</b> single equation models that include Euro-wide and U. S aggregates, and large-model methods in which forecasts are based on estimates of common dynamic factors. Aggregate Euro-wide forecasts are constructed from models that utilize only aggregate Euro-wide variables and by aggregating country-specific models. The results suggest that forecasts constructed by aggregating the country-specific models are more accurate than forecasts constructed using the aggregate data...|$|R
40|$|We {{develop a}} small model for {{forecasting}} inflation for the euro area using quarterly data {{over the period}} June 1973 to March 1999. The model is used to provide inflation forecasts from June 1999 to March 2002. We compare the forecasts from our model with those derived from six competing forecasting models, including <b>autoregressions,</b> vector <b>autoregressions</b> and Phillips-curve based models. A considerable gain in forecasting performance is demonstrated using a relative root mean squared error criterion and the Diebold-Mariano test to make forecast comparisons. [*][*]Copyright Â© 2006 John Wiley & Sons, Ltd. ...|$|R
40|$|AbstractThe {{asymptotic}} {{theory of}} regression with integrated {{processes of the}} ARIMA type frequently involves weak convergence to stochastic integrals of the form ∫ 01 W dW, where W(r) is standard Brownian motion. In multiple regressions and vector <b>autoregressions</b> with vector ARIMA processes, the theory involves weak convergence to matrix stochastic integrals of the form ∫ 01 B dB′, where B(r) is vector Brownian motion with a non-scalar covariance matrix. This paper studies the weak convergence of sample covariance matrices to ∫ 01 B dB′ under quite general conditions. The theory is applied to vector <b>autoregressions</b> with integrated processes...|$|R
