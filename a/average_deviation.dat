742|2505|Public
25|$|ETFs are {{dependent}} on {{the efficacy of the}} arbitrage mechanism in order for their share price to track net asset value. While the <b>average</b> <b>deviation</b> between the daily closing price and the daily NAV of ETFs that track domestic indices is generally less than 2%, the deviations may be more significant for ETFs that track certain foreign indices. The Wall Street Journal reported in November 2008, during a period of market turbulence, that some lightly traded ETFs frequently had deviations of 5% or more, exceeding 10% in a handful of cases, although even for these niche ETFs, the <b>average</b> <b>deviation</b> was only a little more than 1%. The trades with the greatest deviations tended to be made immediately after the market opened.|$|E
25|$|The device he designed, later {{known as}} a Michelson interferometer, sent yellow light from a sodium flame (for alignment), or white light (for the actual observations), through a half-silvered mirror {{that was used to}} split it into two beams {{traveling}} at right angles to one another. After leaving the splitter, the beams traveled out to the ends of long arms where they were reflected back into the middle by small mirrors. They then recombined {{on the far side of}} the splitter in an eyepiece, producing a pattern of constructive and destructive interference whose transverse displacement would depend on the relative time it takes light to transit the longitudinal vs. the transverse arms. If the Earth is traveling through an aether medium, a beam reflecting back and forth parallel to the flow of aether would take longer than a beam reflecting perpendicular to the aether because the time gained from traveling downwind is less than that lost traveling upwind. Michelson expected that the Earth's motion would produce a fringe shift equal to 0.04 fringes—that is, of the separation between areas of the same intensity. He did not observe the expected shift; the greatest <b>average</b> <b>deviation</b> that he measured (in the northwest direction) was only 0.018 fringes; most of his measurements were much less. His conclusion was that Fresnel's hypothesis of a stationary aether with partial aether dragging would have to be rejected, and thus he confirmed Stokes' hypothesis of complete aether dragging.|$|E
5000|$|Average {{absolute}} deviation (or simply called <b>average</b> <b>deviation)</b> ...|$|E
30|$|The <b>average</b> {{standard}} <b>deviation</b> of spectral magnitude (in dB) {{was used}} to objectively measure the smoothness of synthesized speech. A lower <b>average</b> standard <b>deviation</b> indicates more over-smoothness.|$|R
30|$|The <b>average</b> <b>deviations</b> of {{residuals}} in the L 1 norm during iterative inversions {{are presented}} in Table  2. It {{can be seen that}} the inversion provides the reduction in residual deviations by approximately 30  % for P- and 47.5  % for S-wave data. The final <b>average</b> <b>deviations</b> are consistent with estimates of the picking accuracy (0.15 and 0.20  s for P and S phases, respectively). Larger reduction of residuals for the S data might be related to higher sensitivity of the S data to variations in physical parameters inside the Earth.|$|R
5000|$|The <b>average</b> {{absolute}} <b>deviation</b> (or mean absolute deviation) of a {{data set}} {{is the average}} of the absolute deviations from a central point. It is a summary statistic of statistical dispersion or variability. In this general form, the central point can be the mean, median, mode, or the result of another measure of central tendency. Furthermore, as described in the article about <b>averages,</b> the <b>deviation</b> <b>averaging</b> operation may refer to the mean or the median. Thus the total number of combinations amounts to at least four types of <b>average</b> absolute <b>deviation.</b>|$|R
5000|$|This {{polarization}} {{produces a}} deviation from [...] If the <b>average</b> <b>deviation</b> is to vanish, the total polarization summed {{over the two}} types of inclusion must vanish. Thus ...|$|E
5000|$|Climate (measured by two variables: the <b>average</b> <b>deviation</b> of {{minimum and}} maximum monthly {{temperatures}} from 14 degrees Celsius; {{and the number}} of months in the year with less than 30mm rainfall) ...|$|E
5000|$|... #Caption: <b>Average</b> <b>deviation</b> of five factor {{personality}} profile of heroin users from the population mean. N stands for Neuroticism, E for Extraversion, O for Openness to experience, A for Agreeableness and C for Conscientiousness.|$|E
40|$|AbstractIn this work, pseudo-ternary liquid–liquid {{equilibrium}} {{data were}} obtained for three systems composed for water+ethanol+ethyl biodiesel from crambe, fodder radish and macauba pulp oils, at T/K= 298. 2. Ethanol, which distributes in both phases, had greater affinity for the water-rich phase. Biodiesels and water showed almost complete immiscibility. Modeling with the NRTL and UNIQUAC thermodynamic models was performed, resulting in <b>average</b> <b>deviations</b> ranging from 0. 49 % to 1. 29 %. UNIFAC-LLE and UNIFAC-Dortmund were used in prediction of the liquid–liquid equilibrium of these systems, resulting in <b>average</b> <b>deviations</b> ranging from 1. 91 % to 2. 27 % for the systems containing biodiesel from crambe and fodder radish oils, and ranging from 3. 17 % to 3. 28 % for biodiesel from macauba oil...|$|R
40|$|CC 2 and CCSD coupled-cluster {{calculations}} of the sodium D line specific rotations of 13 chiral organic molecules are compared to HF and DFT/B 3 LYP calculations and to experiment. For 12 of the molecules, whose [alpha](D) values {{are in the}} range 0 - 200, CCSD and B 3 LYP [alpha](D) values are in very similar agreement with experiment: <b>average</b> <b>deviations</b> are 19. 8 and 19. 4, respectively. CC 2 and HF values are less accurate: <b>average</b> <b>deviations</b> are 24. 7 and 32. 2, respectively. For one molecule, norbornenone, the CCSD [alpha](D) value (741) {{is very different from the}} B 3 LYP value (1216) and in much worse agreement with experiment (1146). (C) 2003 Elsevier Science B. V. All rights reserved...|$|R
30|$|Thus the OCV-PD makes a {{significant}} improvement and outperforms the SDBR by reducing the <b>average</b> normalized <b>deviation</b> from 3.89 % to 0.6 %; moreover, the OCV-PD can protect system from overvoltage fault with an <b>average</b> normalized <b>deviation</b> of 1.02 %, while the SDBR cannot provide this protection.|$|R
50|$|Addenda: Leaving out {{the highest}} outlier each, the <b>average</b> <b>deviation</b> {{of the rest}} of the bematistss {{measurements}} would be 1.9% with Pliny and 1.5% with Strabo at a measured distance of 1,958 respectively 1,605 miles.|$|E
50|$|ETFs are {{dependent}} on {{the efficacy of the}} arbitrage mechanism in order for their share price to track net asset value. While the <b>average</b> <b>deviation</b> between the daily closing price and the daily NAV of ETFs that track domestic indices is generally less than 2%, the deviations may be more significant for ETFs that track certain foreign indices. The Wall Street Journal reported in November 2008, during a period of market turbulence, that some lightly traded ETFs frequently had deviations of 5% or more, exceeding 10% in a handful of cases, although even for these niche ETFs, the <b>average</b> <b>deviation</b> was only a little more than 1%. The trades with the greatest deviations tended to be made immediately after the market opened.|$|E
50|$|In other words, for {{a normal}} distribution, mean {{absolute}} deviation is about 0.8 times the standard deviation.However, in-sample measurements deliver {{values of the}} ratio of mean <b>average</b> <b>deviation</b> / standard deviation for a given Gaussian sample n with the following bounds: , with a bias for small n.|$|E
30|$|In the pre-computation stage, the <b>average</b> <b>deviations</b> of {{ratings from}} item a to item b {{is given in}} {{equation}} 1. The cloud application only maintains a list of items; their pairwise deviations and cardinalities but no other user data. The process of rating addition is described in algorithm 4.|$|R
5000|$|L1 norm statistics: {{the median}} {{minimizes}} <b>average</b> absolute <b>deviation,</b> ...|$|R
40|$|In {{this work}} {{solubility}} measurements of CO 2 in three vegetable oils, a high oleic sunflower oil (HOSO), a castor oil and a rapeseed oil, for mole fractions ranging from 0. 32 to 0. 93 in the temperature range 298 - 363 K {{and up to}} 75 MPa were performed. Moreover, the densities and viscosities of these oils are reported from 278. 15 to 373. 15 K at atmospheric pressure. These data were {{used to evaluate the}} predictive ability of the fragment based approach. Solubility data were modeled by means of the SRK EoS and predicted employing the Carvalho and Coutinho correlation. Global <b>average</b> <b>deviations</b> inferior to 6 % in CO 2 mole fraction composition were achieved with the SRK EoS and maximum percentage absolute <b>average</b> <b>deviations</b> of 13 % in pressure were obtained using the Carvalho and Coutinho correlation. (C) 2013 Elsevier B. V. All rights reserved...|$|R
50|$|The Jenks {{optimization}} method, {{also called}} the Jenks natural breaks classification method, is a data clustering method designed {{to determine the best}} arrangement of values into different classes. This is done by seeking to minimize each class’s <b>average</b> <b>deviation</b> from the class mean, while maximizing each class’s deviation from the means of the other groups. In other words, the method seeks to reduce the variance within classes and maximize the variance between classes.|$|E
5000|$|The {{index is}} named after John Loosemore and Victor J. Hanby, who first {{published}} the formula in 1971 in a paper entitled [...] "The Theoretical Limits of Maximum Distortion: Some Analytic Expressions for Electoral Systems". Along with Douglas W. Rae's, the formula {{is one of the}} two most cited disproportionality indices. Whereas the Rae index measures the <b>average</b> <b>deviation,</b> the Loosemore-Hanby index measures the total deviation. Michael Gallagher used least squares to develop the Gallagher index, which takes a middle ground between the Rae and Loosemore-Hanby indices.|$|E
5000|$|Firebaugh {{has shown}} that {{standard}} inequality indices reduce to a convenient common form. [...] He begins by noting that perfect equality exists when the inequality ratio, rj = Xj / [...] equals 1.0 for all j units in some population (for example, there is perfect income inequality when everyone’s income Xj equals the mean income , so that rj = 1.0 for everyone). Inequality, then, refers to deviations of the rj from 1.0; the greater the <b>average</b> <b>deviation,</b> the greater the inequality. Inequality indices reflect that fact because they have this common form: ...|$|E
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. These data are a time series of forty-six cases, 1869 - 1914, consisting of one hundred and thirty-four variables recording information on {{various aspects of the}} British economy. Main Topics : Variables include raw values, nine-year moving <b>averages,</b> <b>deviations</b> from the <b>average,</b> and <b>deviations</b> from the linear trend for such quantities as British investment abroad, British gross domestic fixed capital, British exports and imports to the British Empire and {{to other parts of the}} world, and the number of British alliances. Data were collected from the most recent available studies in economic history, econometrics, and political science...|$|R
40|$|A careful {{topographical}} characterization {{is important}} for reliable interpretation {{of the role of}} implant surface roughness in bone incorporation. In this paper, the currently available measuring instruments and evaluation techniques are described and discussed first, than literature on the role of surface roughness for cell and bone tissue reactions in vitro and, with special emphasis, the in vivo studies are reviewed. Finally, the results from a series of the authors own animal studies evaluating screw-shaped implants with different surface roughnesses are summarized. The results demonstrated firmer bone fixation for blasted implants than for turned ones. A blasted surface with an <b>average</b> height <b>deviation</b> (Sa) of 1. 5 µm had a better bone fixation than a blasted surface with an <b>average</b> height <b>deviation</b> (Sa) of 1. 2 µm. A tendency towards more bone in contact and higher removal torques was found for blasted implant surfaces with an <b>average</b> height <b>deviation</b> (Sa) of 1. 2 µm than with blasted surfaces with 2. 2 µm <b>average</b> height <b>deviation</b> (Sa) ...|$|R
3000|$|... 25 values {{determined}} by the LNISO methods (six data points vs. 11 data points) demonstrated similar deviations from the results by the conventional isothermal method. The <b>average</b> <b>deviations</b> were[*]±[*] 44 % by the LNISO method (6 data points) versus ± 43 % by the LNISO method (11 data points). For the above analysis, the two outlier compounds (A 11 and A 16) were excluded.|$|R
5000|$|The Gini {{coefficient}} {{and other}} standard inequality indices reduce {{to a common}} form. Perfect equality—the absence of inequality—exists when and only when the inequality ratio, , equals 1 for all j units in some population (for example, there is perfect income equality when everyones income [...] equals the mean income , so that [...] for everyone). Measures of inequality, then, are measures of the average deviations of the [...] from 1; the greater the <b>average</b> <b>deviation,</b> the greater the inequality. Based on these observations the inequality indices have this common form: ...|$|E
5000|$|The term [...] "fixed pattern noise" [...] usually {{refers to}} two parameters. One is the DSNU (dark signal non-uniformity), {{which is the}} offset from the average across the imaging array at a {{particular}} setting (temperature, integration time) but no external illumination and the PRNU (photo response non-uniformity), which describes the gain or ratio between optical power on a pixel versus the electrical signal output. The latter is often simplified as a single value measured at e.g. 50% saturation level, implying a linear approximation of the not perfectly linear photo response photo response non-linearity (PRNL). [...] Often PRNU as defined above is subdivided in pure [...] "(offset) FPN" [...] which is the part not dependent on temperature and integration time, and the integration time and temperature dependent [...] "DSNU".Sometimes pixel noise as the <b>average</b> <b>deviation</b> from the array average under different illumination and temperature conditions is specified. Pixel noise therefore gives a number (commonly expressed in rms) that identifies FPN in all permitted imaging conditions, which might strongly deteriorate if additional electrical gain (and noise) is included.|$|E
5000|$|As {{mentioned}} earlier, {{the weak}} law applies {{in the case}} of independent identically distributed random variables having an expected value. But it also applies in some other cases. For example, the variance may be different for each random variable in the series, keeping the expected value constant. If the variances are bounded, then the law applies, as shown by Chebyshev as early as 1867. (If the expected values change during the series, then we can simply apply the law to the <b>average</b> <b>deviation</b> from the respective expected values. The law then states that this converges in probability to zero.) In fact, Chebyshev's proof works so long as the variance of the average of the first n values goes to zero as n goes to infinity. As an example, assume that each random variable in the series follows a Gaussian distribution with mean zero, but with variance equal to [...] At each stage, the average will be normally distributed (since it is the average of a set of normally distributed variables). The variance of the sum is equal to the sum of the variances, which is asymptotic to [...] The variance of the average is therefore asymptotic to [...] and goes to zero.|$|E
40|$|A {{quantitative}} structure-property {{modeling of}} the log P (octanol/water partition coefficient) of 76 industrial chemicals is presented. Estimations are performed by means of correlation weighting of local invariants of labeled hydrogen-filled graphs. Results are quite satisfactory, with lower <b>average</b> <b>deviations</b> than other calculations performed with similar theoretical methods. Some possible applications and further extensions of the computation procedure to estimate other physico-chemical or biological properties are mentioned...|$|R
3000|$|The {{computed}} <b>average</b> percentage <b>deviations</b> of {{the lowest}} insolation from the highest insolation for 0 ° ≤ [...]...|$|R
30|$|Two {{objective}} {{measures were}} used. Those are the <b>average</b> standard <b>deviation</b> of spectral magnitude and the Mel-cepstral distortion.|$|R
50|$|The device he designed, later {{known as}} a Michelson interferometer, sent yellow light from a sodium flame (for alignment), or white light (for the actual observations), through a half-silvered mirror {{that was used to}} split it into two beams {{traveling}} at right angles to one another. After leaving the splitter, the beams traveled out to the ends of long arms where they were reflected back into the middle by small mirrors. They then recombined {{on the far side of}} the splitter in an eyepiece, producing a pattern of constructive and destructive interference whose transverse displacement would depend on the relative time it takes light to transit the longitudinal vs. the transverse arms. If the Earth is traveling through an aether medium, a beam reflecting back and forth parallel to the flow of aether would take longer than a beam reflecting perpendicular to the aether because the time gained from traveling downwind is less than that lost traveling upwind. Michelson expected that the Earth's motion would produce a fringe shift equal to 0.04 fringes—that is, of the separation between areas of the same intensity. He did not observe the expected shift; the greatest <b>average</b> <b>deviation</b> that he measured (in the northwest direction) was only 0.018 fringes; most of his measurements were much less. His conclusion was that Fresnel's hypothesis of a stationary aether with partial aether dragging would have to be rejected, and thus he confirmed Stokes' hypothesis of complete aether dragging.|$|E
5000|$|The first {{published}} work on creating speed-figures was E.W. Donaldson's Consistent Handicapping Profits (1936), which was cited by Jerry Brown as the method {{on which the}} Ragozin and Brown [...] "sheet" [...] figures are based. The Beyer numbers trace their roots back {{to the work of}} Ray Taulbot's parallel-time chart (1959), with Beyer pointing out the flaw of adding a fixed amount of time to slow or fast times at other distances, driving the numbers out of proportion. In 1963, Taulbot sent his parallel-time chart to Beyer's Harvard classmate, Sheldon Kovitz, who adjusted it to account for velocity (e.g., a horse who runs six furlongs in 1:09 will run its seventh furlong faster than one who runs 1:13, and so forth). From this work, using the same principle, Kovitz derived the beaten-lengths chart which Beyer published in Picking Winners. [...] Beyer's subsequent research added the last piece of the puzzle. In Picking Winners, Beyer claimed a breakthrough when a study of claiming races at Calder Race Course showed Beyer that 1:13 for six furlongs was equally fast to 1:26.1 at seven; from there, Kovitz's math was used to generate perfectly accurate parallel-time and beaten-lengths charts, which Beyer then used to make par times for classes, against which each race is measured to determine if the track is faster or slower than normal. Each day's races are compared to their pars, with the variant representing the <b>average</b> <b>deviation,</b> and then added to the raw speed rating to yield the par-time based figure. Once horses have built a figure history, Beyer projects a figure based on the figures earned by the horses in the race, in place of the par, making the numbers much more accurate. For example, a horse who earns three consecutive figures of 102, and defeats a horse with three consecutive figures of 92, would indicate a projected figure of 102 for that race is accurate. Sometimes, variants are split during the day if the surface changes drastically enough. In 1992, Beyer began making turf figures, which were made more accurate by his adjustment of the beaten-lengths chart, in which he uses the six-and-a-half furlong beaten-lengths chart for all races at that distance or longer, to reflect the nature of turf racing, where horses jockey for position most of the way, and then sprint home with almost all of their energy in reserve, making the competitive part of the race more akin to a sprint than to the race's actual distance.|$|E
3000|$|... 25 values {{obtained}} by the 8  weeks conventional isothermal stability with an <b>average</b> <b>deviation</b> of[*]±[*] 24 %. However, the k [...]...|$|E
50|$|<b>Average</b> {{absolute}} <b>deviation,</b> {{is the sum}} {{of absolute}} values of the deviations divided by the number of observations.|$|R
40|$|AbstractAccurate double {{ionization}} energies (DIEs) for low-lying doubly {{charged states}} of HCl, HBr, Cl 2 and Br 2 were calculated using non-relativistic CASSCF/MRCI level of theory with the aug-cc-pVQZ basis set. The {{results are in}} excellent agreement with experimental data, presenting absolute <b>average</b> <b>deviations</b> lower than 0. 3 eV. Some DIEs not previously assigned or well characterized are discussed. Further improvements {{can be reached by}} including relativistic effects...|$|R
30|$|The OCV-PD {{ensures that}} the DFIG can operate well during an {{overvoltage}} fault with an <b>average</b> normalized <b>deviation</b> of 1.02 %.|$|R
