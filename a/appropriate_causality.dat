5|13|Public
40|$|We {{study the}} alpha vacua of de Sitter space by {{considering}} the decay rate of the inflaton field coupled to a scalar field placed in an alpha vacuum. We find an alpha dependent Bose enhancement relative to the Bunch-Davies vacuum and, surprisingly, no non-renormalizable divergences. We also consider a modified alpha dependent time ordering prescription for the Feynman propagator and show that it leads to an alpha independent result. This result suggests {{that it may be}} possible to calculate in any alpha vacuum if we employ the <b>appropriate</b> <b>causality</b> preserving prescription. Comment: 16 pages, 1 figure, Revtex 4 preprin...|$|E
40|$|We present Pika, an {{implemented}} self-explanatory simulator that is {{more than}} 5000 times faster than SimGen Mk 2 [Forbus and Falkenhainer, 1992], the previous state of the art. Like SimGen, Pika automatically prepares and runs a numeric simulation of a physical device specified as a particular instantiation of a general domain theory, and it is capable of explaining its reasoning and the simulated behavior. Unlike SimGen, Pika's modeling language allows arbitrary algebraic and differential equations with no prespecified causal direction; Pika infers the <b>appropriate</b> <b>causality</b> and solves the equations as necessary to prepare for numeric integration. Introduction Science and engineering have used numeric simulation productively for years. Simulation programs, however, have been laboriously hand-crafted, intricate, and difficult to understand and change. There has been much recent work on automating their construction (e. g. [Yang, 1992, Rosenberg and Karnopp, 1983, Abelson [...] ...|$|E
40|$|Microarray time-course {{data can}} be used to explore {{interactions}} among genes and infer gene network. The crucial step in constructing gene network is to develop an <b>appropriate</b> <b>causality</b> test. In this regard, the expression profile of each gene can be treated as a time series. A typical existing method establishes the Granger causality based on Wald type of test, which relies on the homoscedastic normality assumption of the data distribution. However, this assumption can be seriously violated in real microarray experiments and thus may lead to inconsistent test results and false scientific conclusions. To overcome the drawback, we propose an estimating equation–based method which is robust to both heteroscedasticity and nonnormality of the gene expression data. In fact, it only requires the residuals to be uncorrelated. We will use simulation studies and a real-data example to demonstrate the applicability of the proposed method...|$|E
40|$|In this study, we {{investigate}} empirically the causal relationship between nominal exchange rates and inflation by using high-frequency data of nominal exchange rates and inflation of Turkey. To determine the <b>appropriate</b> Granger <b>causality</b> relations, unit root and cointegtration models are used. With time-series techniques, {{this study provides}} evidence that a long-run relationship between nominal exchange rates and inflation exist. However, our results indicate that a causal relationship occurs only one direction from nominal exchange rates to inflation. Nominal exchange rates, inflation, causality. ...|$|R
40|$|This paper applies {{recently}} developed unit root and cointegration models {{to determine the}} <b>appropriate</b> Granger <b>causality</b> relations between stock prices and exchange rates using recent Asian flu data. Coupled with impulse response functions, {{it is found that}} data from Japan and Thailand are in agreement with this approach, so that exchange rates leads stock prices with positive correlation. On the other hand, data of Taiwan suggests the result predicted by the portfolio approach: stock prices lead exchange rates with negative correlation. Data from Indonesia, Korea, Malaysia, and the Philippines indicate strong feedback relations while that of Singapore fails to reveal any recognizable patter...|$|R
40|$|Program are gratefully acknowledge. This {{paper is}} written during my tenure as a visiting scholar at UCSD. a Corresponding author. A Bivariate Causality Between Stock Prices and Exchange Rates: Evidence from the Recent Asian Flu This paper applies {{recently}} developed unit root and cointegration models {{to determine the}} <b>appropriate</b> Granger <b>causality</b> relations between stock prices and exchange rates using recent Asian flu data. Coupled with impulse response functions, {{it is found that}} data from Japan and Thailand are in agreement with this approach, so that exchange rates leads stock prices with positive correlation. On the other hand, data of Taiwan suggests the result predicted by the portfolio approach: stock prices lead exchange rates with negative correlation. Data from Indonesia, Korea, Malaysia, and the Philippines indicate strong feedback relations while that of Singapore fails to reveal any recognizable pattern. JEL Classification: G 15, C 3...|$|R
40|$|Airframe {{noise is}} a {{significant}} component of the aircraft signature. In this paper the impact of coating airfoils with an acoustic absorbing material is considered. The finite element method (FEM) is implemented to model the homogenous uniform air, the non-homogenous flow and the isotropic thin absorbing layer. Each domain has different governing equation which {{requires the use of}} a domain decomposition (DD) approach using appropriate coupling conditions. <b>Appropriate</b> <b>causality</b> conditions are implemented at the far field to ensure no reflections of waves in the near field. The numerical technique was verified using the method of manufactured solution (MMS) and validated with some of the results published in literature. Results show that the unsteady lift depends on the airfoil camber/chord ratio, reduced frequency of incident acoustic wave and direction, and absorbent impedance. The results show that a thin isotropic absorber coating an airfoil modifies both the acoustic pressure lobes in the far field, the unsteady lift and attenuates the propagating sound power. The sound power attenuated for the NACA 2412 coated with a thin absorber with acoustic permittivity = 5 - 0. 5 i, subject to an incident plane wave with reduced frequency k= 5, is 7. 3 dB...|$|E
40|$|Herbal {{hepatotoxicity}} is a {{rare and}} poorly described disease because reported cases are mostly scattered and lack an <b>appropriate</b> <b>causality</b> assessment. We now describe in detail the clinical picture of herbal he-patotoxicity by extracts of Greater Celandine (GC), syn. Chelidonium majus L. from the Papaveraceae fa-mily, which contain more than 20 ingredients including various biologically active isoquinoline alkaloids. For this purpose, we analyzed and reviewed published cases of 16 patients from various European countries. In all patients, herbal hepatotoxicity was of probable and highly probable causality for GC, using the original and updated scale of CIOMS (Council for International Organizations of Medical Sciences). GC associated hepatotoxicity usually has an acute clinical course exhibiting a hepatocellular pattern of injury and is co-rrelated to an idiosyncratic reaction with its metabolic subtype. Jaundice combined with high values of serum aminotransferases was present in virtually all cases with favourable outcome despite severe clinical course. In conclusion, GC hepatotoxicity is a typical herbal hepatotoxicity with a sound causality track for GC, but there is uncertainty regarding the respective causative compound(s). The present detailed review of GC hepatotoxicity may serve as an example for clinical causality assessments of future cases of liver in...|$|E
40|$|Due to {{significant}} contributions of textile and apparel industry {{in terms of}} employment, export opportunities and industrial value added, it has {{a crucial role in}} the Turkish economy. Even there are many factors influencing the export performance of this sector, real exchange rate is one of the significant factors. In this study, we investigated the causal relationship between the real exchange rate and export of textile and apparel industry by using monthly data, covering from 1998 - 2008. Exchange rate volatility was also considered in the model using GARCH approach. In order to determine <b>appropriate</b> Granger <b>causality</b> relations, unit root, cointegtration and error correction models were used. With time-series techniques, this study provided evidence that there is a long-run relationship exist between the real exchange rate and export of textile and apparel industry. Based on the results of the model it was concluded that increasing exchange rate volatility negatively impacts on textile and apparel export. Moreover, results indicated that there is a one way casual relationship from real exchange rate to export of textile and apparel industry, and bidirectional Granger causality from exchange rate volatility to textile and apparel industry. Textile and apparel industry, real exchange rate, GARCH, causality, cointegration, export. ...|$|R
40|$|In Nowak-Lehmann et al. (2012), we used time-series {{methods to}} {{investigate}} the impact of aid on per capita GDP. Lof, Mekasha, and Tarp (LMT, 2014) criticize our econometric approach, our interpretation, and our data-handling procedure which lead to {{a large share of}} missing observations in some specifications. Using a different time-series approach, a different aid variable, and a different sample, they claim to find a positive effect of aid on income, which contrasts with our own results. In this comment, we first explain why we disagree with LMT’s critique of our econometric method and show that our results do not depend on our way of dealing with missing data. Second, we show that the methods used by LMT are unsuitable and rely on similarly problematic data-handling procedures. Supplementing their approach with <b>appropriate</b> cointegration and <b>causality</b> tests shows that there is no robust effect of aid on income...|$|R
40|$|In formal {{theories}} for reasoning about actions, the qualification problem {{denotes the}} problem {{to account for the}} many conditions which, albeit being unlikely to occur, may prevent the successful execution of an action. By a simple counter-example {{in the spirit of the}} well-known Yale Shooting scenario, we show that the common straightforward approach of globally minimizing such abnormal disqualifications is inadequate as it lacks an <b>appropriate</b> notion of <b>causality.</b> To overcome this difficulty, we propose to incorporate causality by treating the proposition that an action is qualified as a fluent which is initially assumed away by default but otherwise potentially indirectly affected by the execution of actions. Our formal account of the qualification problem includes the proliferation of explanations for surprising disqualifications and also accommodates so-called miraculous disqualifications. We moreover sketch a version of the fluent calculus which involves default rules to address [...] ...|$|R
40|$|A {{well known}} {{argument}} in cosmology gives {{that the power}} spectrum of mass fluctuations produced from a uniform initial state by physics which is causally connected up to a finite scale has the behaviour P(k) ∝ k^ 4 at small k. We point out the implicit, and a priori unjustified, assumptions made in the standard demonstration of this result. Introducing a class of one dimensional models describing the generation of fluctuations by a mass and momentum conserving process, we show how the analyticity properties of P(k) at small k {{are determined by the}} convergence properties of the probability distribution f(l) for the spatial extent l of the fluctuations. Taking f(l) to have finite mean and variance, which we argue to be an <b>appropriate</b> definition of <b>causality</b> {{in the context of the}} early universe, we find that the small k behaviour P(k) ∝ k^n with n anywhere in the range 0 < n ≤ 4 can be obtained...|$|R
40|$|In formal {{theories}} for reasoning about actions, the qualification problem {{denotes the}} problem {{to account for the}} many conditions which, albeit being unlikely to occur, may prevent the successful execution of an action. While a solution to this problem must involve the ability to assume away by default these abnormal disqualifications of actions, the common straightforward approach of globally minimizing them is inadequate as it lacks an <b>appropriate</b> notion of <b>causality.</b> This is shown by a simple counter-example closely related to the well-known Yale Shooting scenario. To overcome this difficulty, we propose to incorporate causality by treating the fact that an action is qualified as ordinary fluent, i. e., a proposition which may change its truth value in the course of time by potentially being (indirectly) affected by the execution of actions. Abnormal disqualifications then are initially assumed away, unless there is evidence to the contrary. Our formal account of the qualification pro [...] ...|$|R
40|$|The Language E is {{a simple}} {{declarative}} language for describing the effects of action occurrences within a given narrative, using an ontology of actions, time points and fluents (i. e. properties which can change their truth values over time). This paper shows how E may be extended to deal with ramifications. More precisely, we show how Language E domain descriptions can include statements describing permanent relationships or constraints between fluents, and how the model theoretic semantics of E can be extended in an intuitive {{way to ensure that}} the effects of actions are appropriately propagated via such statements, whilst retaining E's simple approach to the frame problem. This results in a simple model of <b>causality</b> <b>appropriate</b> for many domains involving actions and change. Introduction The idea of action description languages was first introduced by Gelfond and Lifschitz [6, 7], with their proposal for the Language A. The intention was that such languages would have [...] ...|$|R
40|$|Our aim in {{this paper}} is threefold. First, to test the {{robustness}} {{of the relation between}} total factor productivity growth and inflation to the specification of the estimating model; second, to test the stability of their relationship in the short run and in the long run, and third, to investigate the direction of causality between these two variables. To accomplish the first objective, we esti-mate a generalized Box-Box cost function using data from the two-digit Standard Industrial Clas- sification of manufacturing industries in Greece during the period 1964 - 1980. The results show that: a) the acceleration of inflation from 1964 - 1972 to 1973 - 1980 reduced total factor productiv-ity growth {{in a way that was}} both statistically significant and sizeable, and b) even when the ef-fect of inflation is separated from the effects of technical change and economies of scale, the choice of functional form is most crucial. With respect to the second objective, somewhat to our surprise, we find that the inflation-productivity trade-off prevails even in the long run. And, fi-nally, regarding the third objective, it emerges that in the great majority of two-digit manufactur-ing industries the causality runs from inflation to productivity. On these grounds we conclude that for a precise estimation of the relationship under consideration it is imperative to sort out the three effects involved, do so by adopting the most general flexible functional form for the cost function, and run the <b>appropriate</b> stability and <b>causality</b> tests. inflation; productivity; scale economies; technical change; generalized Box-Cox cost function; stability; causality...|$|R
40|$|Conceptually, {{modeling}} of flexible, multi-body systems involves a formulation {{as a set}} of time-dependent partial differential equations. However, for practical, engineering purposes, this modeling is usually done using the method of Finite Elements, which approximates the set of partial differential equations, thus generalizing the approach to all continuous media. This research investigates the links between the Bond Graph method and the classical methods used to develop system models and advocates the Bond Graph Methodology and current bond graph tools as alternate approaches that will lead to a quick and precise understanding of a flexible multi-body system under automatic control. For long endurance, complex spacecraft, because of articulation and mission evolution the model of the physical system may change frequently. So a method of automatic generation and regeneration of system models that does not lead to implicit equations, as does the Lagrange equation approach, is desirable. The bond graph method {{has been shown to be}} amenable to automatic generation of equations with <b>appropriate</b> consideration of <b>causality.</b> Indeed human-interactive software now exists that automatically generates both symbolic and numeric system models and evaluates causality as the user develops the model, e. g. the CAMP-G software package. In this paper the CAMP-G package is used to generate a bond graph model of the International Space Station (ISS) at an early stage in its assembly, Zvezda. The ISS is an ideal example because it is a collection of bodies that are articulated, many of which are highly flexible. Also many reaction jets are used to control translation and attitude, and many electric motors are used to articulate appendages, which consist of photovoltaic arrays and composite a [...] ...|$|R
40|$|Hunting Causes and Using Them: Approaches in Philosophy and Economics (HC&UT) {{is about}} notions of <b>causality</b> <b>appropriate</b> to the sciences, mostly generic causal claims (causal laws) and {{especially}} notions that connect causality with probability. 1 Most {{of the work}} for the book {{is associated with the}} project ‘Causality: Metaphysics and Methods’. This project argued that metaphysics – our account of what causal laws are or what general causal claims say – should march hand-in-hand with our ways of establishing them. It should be apparent, given the kind of thing we think causality is, why our methods are good for finding it. If our metaphysics does not mesh with and underwrite the methods, we are willing to trust, we should be wary of both. Many philosophers nowadays look for a single informative feature that characterizes causal laws. HC&UT argues instead for causal pluralism, for a large variety of kinds of causal laws as well as purposes for which we call scientific claims causal. Correlatively different methods for testing causal claims are suited to different kinds of causal laws. No one analysis is privileged and no methods are universally applicable. Much of the argument for pluralism is provided by authors of different accounts of causality, who provide intuitively plausible counter-examples to each other. Still, most of these accounts seem adequate for the kinds of examples the authors focus on. From {{the point of view of}} HC&UT, these examples involve different kinds of causal laws or set causality to different jobs, and the concomitant characterizing feature marks out this one kind of causal law. Importantly for the argument, often we can specify what characteristics a system of laws should have in order for an account/method pair to be applicable. An example is James Woodward’s level invariance, which I see as a...|$|R
40|$|Introduction: Built {{environment}} interventions {{designed to}} reduce non-communicable diseases and health inequity, complement urban planning agendas focused on creating more ‘liveable’, compact, pedestrian-friendly, less automobile dependent and more socially inclusive cities. However, what constitutes a ‘liveable’ community is not well defined. Moreover, {{there appears to be}} a gap between the concept and delivery of ‘liveable’ communities. The recently funded NHMRC Centre of Research Excellence (CRE) in Healthy Liveable Communities established in early 2014, has defined ‘liveability’ from a social determinants of health perspective. Using purpose-designed multilevel longitudinal data sets, it addresses five themes that address key evidence-base gaps for building healthy and liveable communities. The CRE in Healthy Liveable Communities seeks to generate and exchange new knowledge about: 1) measurement of policy-relevant built environment features associated with leading non-communicable disease risk factors (physical activity, obesity) and health outcomes (cardiovascular disease, diabetes) and mental health; 2) causal relationships and thresholds for built environment interventions using data from longitudinal studies and natural experiments; 3) thresholds for built environment interventions; 4) economic benefits of built environment interventions designed to influence health and wellbeing outcomes; and 5) factors, tools, and interventions that facilitate the translation of research into policy and practice. This evidence is critical to inform future policy and practice in health, land use, and transport planning. Moreover, to ensure policy-relevance and facilitate research translation, the CRE in Healthy Liveable Communities builds upon ongoing, and has established new, multi-sector collaborations with national and state policy-makers and practitioners. The symposium will commence with a brief introduction to embed the research within an Australian health and urban planning context, as well as providing an overall outline of the CRE in Healthy Liveable Communities, its structure and team. Next, an overview of the five research themes will be presented. Following these presentations, the Discussant will consider the implications of the research and opportunities for translation and knowledge exchange. Theme 2 will establish whether and to what extent the neighbourhood environment (built and social) is causally related to physical and mental health and associated behaviours and risk factors. In particular, research conducted as part of this theme will use data from large-scale, longitudinal-multilevel studies (HABITAT, RESIDE, AusDiab) to examine relationships that meet causality criteria via statistical methods such as longitudinal mixed-effect and fixed-effect models, multilevel and structural equation models; analyse data on residential preferences to investigate confounding due to neighbourhood self-selection and to use measurement and analysis tools such as propensity score matching and ‘within-person’ change modelling to address confounding; analyse data about individual-level factors that might confound, mediate or modify relationships between the neighbourhood environment and health and well-being (e. g., psychosocial factors, knowledge, perceptions, attitudes, functional status), and; analyse data on both objective neighbourhood characteristics and residents’ perceptions of these objective features to more accurately assess the relative contribution of objective and perceptual factors to outcomes such as health and well-being, physical activity, active transport, obesity, and sedentary behaviour. At the completion of the Theme 2, we will have demonstrated and applied statistical methods <b>appropriate</b> for determining <b>causality</b> and generated evidence about causal relationships between the neighbourhood environment, health, and related outcomes. This will provide planners and policy makers with a more robust (valid and reliable) basis on which to design healthy communities...|$|R

