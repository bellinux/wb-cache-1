8|8|Public
40|$|We {{extend the}} vector {{autoregression}} (VAR) based expectations hypothesis test of term structure, considered in Bekaert & Hodrick (2001) using {{recent developments in}} bootstrap literature. Modifications {{include the use of}} wild bootstrap to allow for conditional heteroskedasticity in the VAR residuals without imposing strict parameterization, endogeneous model selection procedure in the bootstrap replications to reflect true uncertainty and the stationarity correction designed to prevent finitesample bias <b>adjusted</b> <b>VAR</b> parameters from becoming explosive. JEL classification: G 10; E 43...|$|E
40|$|International {{capital market}} {{integration}} together with increasing international volatility, requires an accurate {{evaluation of the}} potential losses that portfolio managers may face {{as a result of}} international turbulence. Assets with high liquidity standards can be evaluated by the traditional Value at Risk approach (VaR), however, this statistic underestimates the true value of the potential losses when the instrument is not liquid. This paper applies the methodology of liquidity <b>adjusted</b> <b>VaR</b> to the Chilean sovereign bond by incorporating bid-ask spread fluctuations when evaluating a portfolio risk. ...|$|E
40|$|We {{extend the}} vector {{autoregression}} (VAR) based expectations hypothesis tests of term structure using {{recent developments in}} bootstrap literature. Firstly, we use wild bootstrap to allow for conditional heteroskedasticity in the VAR residuals without imposing any parameterization on this heteroskedasticity. Secondly, we endogenize the model selection procedure in the bootstrap replications to reflect true uncertainty. Finally, a stationarity correction is introduced {{which is designed to}} prevent finitesample bias <b>adjusted</b> <b>VAR</b> parameters from becoming explosive. When the new methodology is applied to extensive US zero coupon term structure data ranging from 1 month to 10 years, we find less rejections for the theory in a subsample of Jan 1982 -Dec 2003 than in Jan 1952 -Dec 1978, and when it is rejected it occurs at only the very short and long ends of the maturity spectrum, in contrast to the U shape pattern observed in some of the previous literature. ...|$|E
40|$|This paper proposes {{the use of}} Bayesian {{approach}} to implement Value at Risk (VaR) model for both linear and non-linear portfolios. The Bayesian approach provides risk traders with the flexibility of <b>adjusting</b> their <b>VaR</b> models according to their subjective views. First, {{we deal with the}} case of linear portfolios. By imposing the conjugate-prior assumptions, a closed-form expression for the Bayesian VaR is obtained. The Bayesian VaR model can also be adjusted {{in order to deal with}} the ageing effect of the past data. By adopting Gerber-Shiu's option-pricing model, our Bayesian VaR model can also be applied to deal with non-linear portfolios of derivatives. We obtain an exact formula for the Bayesian VaR in the case of a single European call option. We adopt the method of back-testing to compare the non-adjusted and <b>adjusted</b> Bayesian <b>VaR</b> models with their corresponding classica' counterparts in both linear and non-linear cases. © Springer 2006. link_to_subscribed_fulltex...|$|R
40|$|The article {{presents}} a {{comparative study of}} parametric linear value-at-risk (VaR) models used for estimating the risk of financial portfolios. We illustrate how to <b>adjust</b> <b>VaR</b> for auto-correlation in portfolio returns. The {{article presents}} static and dynamic methodology to compute VaR, {{based on the assumption}} that daily changes are independent and identically distributed (normal or non-normal) or auto-correlated in terms of the risk factor dynamics. We estimate the parametric linear VaR over a risk horizon of 1  day and 10  days at 99 % and 95 % confidence levels for the same data. We compare the parametric VaR and a VaR obtained using Monte Carlo simulations with historical simulations and use the maximum likelihood method to calibrate the distribution parameters of our risk factors. The study investigated whether the parametric linear VaR applies to contemporary risk factor analysis and pertained to selected foreign rates...|$|R
40|$|Abstract—The {{traditional}} Value at Risk (VaR) is a {{very popular}} tool measuring market risk, {{but it does not}} incorporate liquidity risk. This paper proposes an extended VaR model to integrate liquidity risk for intraday trading strategies using high frequency order book data. We estimate the one step ahead liquidity <b>adjusted</b> intraday <b>VaR</b> called(LAIVaR) for both bid and ask positions, considering several threshold trading sizes. We also quantify the liquidity risk premium by comparing our result with the standard VaR approach...|$|R
40|$|Purpose – The aim is {{to study}} jump {{liquidity}} risk {{and its impact on}} risk measures: value at risk (VaR) and conditional VaR (CVaR). Design/methodology/approach – The liquidity discount factor is modelled with mean revision jump diffusion processes and the liquidity risk is integrated in the framework of VaR and CVaR. Findings – The standard VaR, CVaR, and the liquidity <b>adjusted</b> <b>VaR</b> can seriously underestimate the potential loss over a short holding period for rare jump liquidity events. A better risk measure is the liquidity adjusted CVaR which gives a more realistic loss estimation {{in the presence of the}} liquidity risk. An efficient Monte Carlo method is also suggested to find approximate VaR and CVaR of all percentiles with one set of samples from the loss distribution, which applies to portfolios of securities as well as single securities. Originality/value – The paper offers plausible stochastic processes to model liquidity risk. Liquidity, Monte Carlo methods, Risk analysis...|$|E
40|$|This {{research}} {{evaluate the}} main currency {{exchange rate risk}} of Taiwan using exchange rates between Taiwan and main trading counterparties. The base currency of exchange rates were transformed from USD into NTD and ICVI, then using ARCH model, full valuation methods and backtesting to evaluate VaR and ETL. The value of VaR were adjusted according to the capital multipliers announced by 1996 Basel Accord. Our research is concluded as follows: 1. According to the bilateral trading amount, the impotent currencies are CNY and YEN. According to the cumulative dynamic multipliers, the important currencies are USD and HKD. 2. When using NTD as base currency, through backtesting procedures the optimal full valuation VaR method of CNY and HKD is Bootstrap VaR approach, the optimal full valuation VaR method of JPY is Historical simulation VaR approach, the optimal full valuation VaR method of USD is Monte Carlo simulation VaR approach. When using ICVI as base currency, through backtesting procedure the optimal full valuation VaR method of CNY, HKD and USD is Bootstrap VaR approach, the optimal full valuation VaR method of JPY is Historical simulation VaR approach. 3. The full valuation VaR methods of two DSFM indices generate from exchange rates using NTD and ICVI as base currency, are both Bootstrap VaR approach, and their VaR and ETL are significant low than other four main currencies. 4. Using NTD as base currency, according to the <b>adjusted</b> <b>VaR,</b> the order from low to high of four main currencies is HKD, USD, CNY and JPY; according to the ETL, the order from low to high of four main currencies is CNY, HKD, USD and JPY. 5. Using ICVI as base currency, according to the <b>adjusted</b> <b>VaR,</b> the order from low to high of four main currencies is CNY, JPY, HKD and USD; according to the ETL, the order from low to high of four main currencies is CNY, JPY, HKD and USD. 本研究以台灣主要貿易對手國之匯率資料，評估台灣主要外幣匯率之風險。分別採用以新台幣為計價基礎以及ICVI為計價基礎之外幣匯率報酬率資料，經由ARCH模型及風險的全面評價法，再以回溯測試選出最適的風險全面評價法，來推估VaR以及ETL。其中VaR以巴賽爾協定於 1996 年公佈燈區尺規與資本調整乘數得出調整後VaR。本研究之重要結論如下： 一、以雙邊貿易量大小為標準，選出兩種主要外幣為人民幣及日圓；根據動態單一因素模型之累積動態乘數找出兩種主要外幣為美金及港幣。 二、以新台幣計價的外幣匯率，經回溯測試後，人民幣及港幣的最適風險全面評價法均為拔靴法，日圓的最適風險全面評價法為歷史模擬法，美金的最適風險全面評價法則為蒙地卡羅法；以ICVI 計價的外幣匯率中，經回溯測試後，人民幣、港幣及美金的最適風險全面評價法均為拔靴法，日圓的最適風險全面評價法則為歷史模擬法。 三、無論就新台幣計價及以ICVI計價之外幣匯率，所建構的動態單一因素指數之最適風險全面評價模型為拔靴法，且其VaR與ETL明顯低於台灣四種主要外幣之VaR及ETL。 四、以新台幣計價的台灣四種主要外幣匯率，根據調整後之VaR，由小到大排序為港幣、美金、人民幣、日圓；就台灣四種主要外幣的ETL而言，由小到大之排序為人民幣、港幣、美金、日圓。 五、以ICVI 計價的台灣四種主要外幣匯率，根據調整後之VaR，由小到大排序為人民幣、日圓、港幣、美金；就台灣四種主要外幣的ETL而言，由小到大排序為人民幣、日圓、港幣以及美金。第一章 緒論	 1 第一節 研究背景與動機	 1 第二節 研究目的	 1 第三節 研究架構	 2 第二章 文獻探討	 3 第一節 風險相關之理論	 3 第二節 風險值及期望尾端損失值之衡量	 7 第三節 匯率風險之相關文獻	 11 第四節 匯率資料的時間幅度	 16 第五節 不變貨幣價值指數	 16 第六節 動態單一因素模型	 17 第七節 恆定性檢定	 18 第八節 自我迴歸條件變異模型	 18 第九節 回溯測試	 19 第三章 研究方法	 21 第一節 研究架構	 21 第二節 不變貨幣價值指數	 23 第三節 動態單一因素模型	 24 第四節 恆定數列檢定	 26 第五節 ARCH模型	 28 第六節 VaR與ETL之估計-全面評價法	 32 第七節 回溯測試	 34 第四章 實證分析	 37 第一節 樣本選取與期間設定	 37 第二節 不變貨幣價值指數	 38 第三節 樣本敘述統計之分析	 40 第四節 單根檢定	 41 第五節 台灣外幣匯率的DSFM指數	 44 第六節 台灣主要外幣匯率的ARCH模型	 47 第七節 台灣主要外幣的匯率風險之VaR與ETL	 55 第八節 回溯測試	 56 第五章 結論與建議	 62 第一節 結論	 62 第二節 研究建議	 63 參考文獻	 65 附錄一	 73 附錄二	 77 附錄三	 78 附錄四	 79 附錄五	 81 附錄六	 84 附錄七	 87 附錄八	 9...|$|E
40|$|Conventional Value at Risk {{models are}} {{severely}} constrained while dealing with liquidity risk. This inevitably {{leads to an}} underestimation of overall risk and consequently misapplication of capital {{for the safety of}} financial institutions. Standard Value at Risk (VaR) model assumes that any quantity of securities can be traded without influencing market prices. In reality, most markets are less than perfectly liquid and many securities cannot be traded with ease in markets. This is especially true for emerging market economies where the process of financial sector reform and deepening is currently taking place. Despite episodic evidences of liquidity crisis in the Indian financial markets, risks associated with market illiquidity have not been effectively incorporated into the VaR models. In the face of sudden and persisting off-market prices of some of the securities in their portfolio, the Indian financial organizations often find it difficult to offload these securities without booking significant trading losses. As a consequence, several securities exhibit very low levels of turnover in the secondary segment of the debt market. Also, in most cases, measures of market risk fail to capture the costs of carrying illiquid assets in their portfolio. This becomes a constraining factor for market growth. In this context, the paper attempts to construct a Liquidity <b>adjusted</b> <b>VaR</b> model (L-VaR model) that incorporates liquidity risk in Value at Risk models. The paper tests the performance of L-VaR model vis-a-vis existing VaR models and finds that in the Indian context, the liquidity risk is {{an important component of the}} aggregate risks absorbed by the financial institutions...|$|E
40|$|The seminal work by Markowitz in 1959 {{introduced}} portfolio {{theory to}} the world. The prevailing notion {{since then has}} been that portfolio risk is non linear i. e. you cannot use Linear Programming (LP) to optimize your portfolio. We will in this paper show that simple portfolio drawdown constraints are indeed linear {{and can be used}} to find for example maximum risk <b>adjusted</b> return portfolios. <b>VaR</b> for these portfolios can then be estimated directly instead of using computer intensive Monte Carlo methods...|$|R
40|$|The {{recursive}} prediction and filtering formulas of the Kalman filter {{are difficult}} to implement in nonlinear state space models. For Gaussian linear state space models, or for models with qualitative state variables, the recursive formulas of the filter require the updating of {{a finite number of}} summary statistics. However, in the general framework a function has to be updated, which makes the approach computationally cumbersome. The aim {{of this paper is to}} consider the situation of a large number n of individual measurements, the so-called microinformation, and to take advantage of the large cross-sectional size to get closed-form prediction and filtering formulas at order 1 /n. The state variables have a macro-factor interpretation. The results are applied to the maximum likelihood estimation of a macro-parameter, and to the computation of a granularity <b>adjusted</b> Value-at-Risk (<b>VaR)</b> for large portfolios. The methodology of granularity adjustment for VaR is illustrated by an application of the Value of the Firm model [Merton (1974) ] to both default and loss given default...|$|R
40|$|We {{propose a}} robust {{portfolio}} optimization approach based on Value-at-Risk (<b>VaR)</b> <b>adjusted</b> Sharpe ratios. Traditional Sharpe ratio estimates using a limited series of historical returns {{are subject to}} estimation errors. Portfolio optimization based on traditional Sharpe ratios ignores this uncertainty and, as a result, is not robust. In this paper, we propose a robust portfolio optimization model that selects the portfolio with the largest worse-case-scenario Sharpe ratio within a given confi-dence interval. We show that this framework is equivalent to maximizing the Sharpe ratio reduced by a quantity proportional to the standard deviation in the Sharpe ratio estimator. We highlight {{the relationship between the}} VaR-adjusted Sharpe ratios and other modified Sharpe ratios proposed in the literature. In addition, we present both numerical and empirical results comparing optimal portfolios gener-ated by the approach advocated here with those generated by both the traditional and the alternative optimization approaches...|$|R
40|$|This thesis {{focuses on}} {{liquidity}} risk in capital markets. The main {{aim is to}} help practitioners to better understand and manage liquidity risk by analyzing the following three topics: modeling correlations in a Liquidity <b>Adjusted</b> <b>VaR</b> (L − VaR) (Chapter Three), impact of regulatory interventions on stock liquidity (Chapter Four) and liquidity commonality and option prices (Chapter Five). The first topic focuses on an appropriate way to measure expected stock losses by considering liquidity risk (see Chapter Three). The {{need for a new}} measure, which also includes stock liquidity, is based on the concern of investors only being able to sell stock at a huge discount or not at all. In reaction, various papers with new methodologies have been published, including the liquidity adjusted Value at Risk (L − VaR) models proposed by Bangia et al. (1998) and Ernst et al. (2012). Based on their approach, we analyze different ways to extend these models and to optimize performance. This is done using advanced conditional volatility models like AR − GARCH and AR − GJR models and by considering correlations between spread and return data. The new model is called correlation and liquidity <b>adjusted</b> <b>VaR</b> (CL – VaR) and shows (based on a five-year observation period) better performance compared to the models by Bangia et al. (1998) and Ernst et al. (2012). The models are calculated and back-tested using unique data called Xetra Liquidity Measure (XLM) provided by Deutsche Börse. The collapse of Lehman Brothers in 2008 marked the beginning of a financial crisis affecting the entire world of finance. This period is characterized by increasing fear of further defaults by corporations (including banks) or even by countries. In reaction, investors began shifting their assets to more stable and secure investments and this resulted in stock market crashes. Various interventions were made by government institutions to restore stability. The target of the second topic is to analyze the impact of these interventions on liquidity (measured by volume-weighted bid-ask spreads) and market reaction (measured by returns) at the announcement date (see Chapter Four). In the event, we study abnormal changes of stocks listed on the Dax. The interventions which we consider are published by the Federal Reserve Bank (FED) {{in the form of a}} crisis time-line. Here they are further combined to the following categories: bank liability guarantees, liquidity and rescue interventions, unconventional monetary policy and other market intervention. The results show that, for example, the market reacts positively to liquidity and rescue interventions, whereas bank liability guarantees reduce liquidity. In addition, we show that international events have a significant impact on the domestic market in a "spillover effect". By analyzing the spreads of different traded volumes, an asymmetric increase can be detected at the announcement date. The last topic focuses on the link between equity and option markets (see Chapter Five). There we analyze, on one hand, the link between stock market liquidity and option prices and, on the other hand, the impact of liquidity commonality in equity and option markets. We can show that systematic liquidity (rather than idiosyncratic liquidity) gives a better explanation of changes in “at-the-money” implied volatility. This effect was especially strong during the financial crisis in 2008. Another result is that liquidity risk of higher traded stock volumes is not properly reflected in the option price. This can result in higher hedging costs, as mentioned by Certin et al. (2006). To shed more light into liquidity commonality within the stock market we calculate the LiqCom measure as mentioned by Chordia et al. (2000). The results show a continuously changing liquidity commonality which decreases with increasing traded volume. This is because the market maker focuses for bigger stock positions more on the idiosyncratic liquidity risks while for smaller stock positions the systematic liquidity risk is more important. We confirmed our findings with a robustness check...|$|E
40|$|We {{extend the}} vector {{autoregression}} (VAR) based expectations hypothesis (EH) test of term structure, considered in Bekaert & Hodrick (2001), B&H thereafter, using {{recent developments in}} bootstrap literature. Modifications {{include the use of}} wild bootstrap to allow for conditional heteroskedasticity in the VAR residuals without imposing strict parameterization, while keeping their contemporaneous correlation, endogeneous model selection procedure in the bootstrap replications to reflect true uncertainty and the stationarity correction designed to prevent finite- sample bias <b>adjusted</b> <b>VAR</b> parameters from becoming explosive. Since Lagrange Miltiplier, Wald and Distance Metric test statistics used in this study are all asymptotically pivotal we estimate their finite sample distributions using a computer simulation, rather than relying on the approximation provided by the first order asymptotic theory. When the modified B&H methodology is applied to extensive US zero coupon term structure data ranging from 1 month to 10 years we find less rejections for the theory in a sub-sample of Jan 1982 - Dec 2003 than in Jan 1952 - Dec 1978, and when it is rejected it occurs at the very short and long ends of the maturity spectrum. It is also relieving to note that this inference seems to be robust to both AIC and SIC model selection methods. In terms of the conclusions made about the validity of the EH of term structure, the main difference between this study and its counterpart of Sarno, Thornton & Valente (2006), which uses the original B&H methodology, is that we reject the theory less often than they do. This is probably as one would expect, since we test the EH theory of term structure only as opposed to Sarno et al (2006) who, in effect, are testing a joint null hypothesis of the conditional homoskedasticity in the residuals and exogenous lag length of the VAR along with the EH. expectations hypothesis, term structure of interest rates, vector autoregression...|$|E
40|$|Forecasting {{the levels}} of vector autoregressive (VAR) log-transformed time series has shown to be awkward by Ari~no and Franses (1996) who {{realised}} that just exponentiating the forecasts was a naive procedure due to the ocurrence of bias. They pr oposed a new manner to forecast untransformed VAR through correcting the log-transformed forecasts, and they also showed that their procedure outperforms the naive scheme. Our objective {{is to show that}} from a Bayesian viewpoint does not exist any theoretical problem and that it is feasible to obtain forecasts, and interval forecasts, of the untransformed time series. To do this, we use Monte Carlo simulation of the posteri or distribution of the parameters of the <b>VAR</b> <b>adjusted</b> to the log-transformed data. Key Words: Bayesian VAR, forecasting, Monte Carlo Integration. 1 Introduction As pointed out by Ari~no and Franses (1996) it is quite common in economic applications to transform the data, mainly using logarithms. However, it is [...] ...|$|R
40|$|This thesis {{investigated}} reliable {{measures of}} market risk using high frequency data. The {{first part of}} the study, comprising of chapters 2,and 3, investigated the issue of risk measurement on a single time scale method basis. In Chapter 2, we explored the market risk measurement based on high-frequency measures of volatility with selected stocks in three different sectors. Parametric as well as non-parametric procedures are discussed. Furthermore, the backtesting results for comparing the risk forecasting performance of different risk measurements are also provided. Chapter 3 proceeds into the analysis of liquidity risk in high-frequency trading. We proposed an extended VaR measurement by incorporating the liquidity risk in intraday trading strategies when analysing limit order book data. The focus is on the integration of asymmetric information, upward or downward risks, into the input factors of forecast variables. We further find that there exists an asymmetric behavior of bid spread and ask spread between different trading volume. By taking account of the actual liquidity risk faced by investors with different trading size and positions, we proposed a liquidity <b>adjusted</b> intraday <b>VaR</b> (LAIVaR). We apply the bivariate analysis to investigate the asymmetric effect of the bid and ask side. The empirical results of liquidity adjustment show that the liquidity risk is a crucial factor in estimating VaR. The second part of the study, the analysis in Chapter 4, is extended to a multiple time scale framework by using an empirical scaling law method. This chapter proposed three new empirical scaling laws based on the original maximal price change (MPC) scaling law by Glattfelder, Dupuisy, and Olsen (2011). The most valuable property is the scale invariance which allows financial analysts to assess the maximum loss for any time interval. The strong evidence of the effectiveness of the new scaling law methods are provided in the model performance section. We find that the scaling law method with only one month in-sample data can already provide a good prediction of risk, whereas for the conventional VaR at least ten years length of data is needed to obtain a reasonable result. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

