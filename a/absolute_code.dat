7|52|Public
40|$|This diploma project’s {{goal was}} to develop a {{low-cost}} 3 DOF position sensor of small dimension that permits to find the position and orientation of the actuated object and thus, enable a closed loop control in 3 DOF. The range in X and Y should be at least 30 x 30 mm 2 and in Theta-z 360 °. An <b>absolute</b> <b>code</b> was implemented in the target in order to always track the absolute position...|$|E
40|$|We have designed, implemented, {{and proved}} the {{correctness}} of a compiler generator that accepts action semantic descriptions of imperative programming languages. The generated compilers emit <b>absolute</b> <b>code</b> for an abstract RISC machine language that currently is assembled into code for the SPARC and the HP Precision Architecture. Our machine language needs no run-time type-checking {{and is thus}} more realistic than those considered in previous compiler proofs. We use solely algebraic specifications; proofs are given in the initial model...|$|E
40|$|A sizable body of {{evidence}} {{has shown that the}} brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an <b>absolute</b> <b>code,</b> the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence...|$|E
40|$|AbstractFaces {{all have}} the same basic {{elements}} in the same overall arrangement, and must be discriminated using variations in this shared configuration. An efficient way to represent these variations would be to code how each configuration differs from an average face (norm-based coding model). Alternatively, configurations could be represented simply by <b>coding</b> their <b>absolute</b> values in some coordinate system (<b>absolute</b> <b>coding</b> model). The two models differ in the variables they predict will influence an image’s recognizability. <b>Absolute</b> <b>coding</b> predicts that recognizability will depend on an image’s distinctiveness and degree of distortion from its veridical target. Norm-based coding predicts that recognizability will also depend on the way the image differs from a norm or average face, namely its distance from the norm and/or its degree of displacement from the norm-deviation vector for the target. We determined the effects of these four critical variables on recognition of undistorted (veridical) images, caricatures, anticaricatures and ‘lateral’ distortions of famous faces (Experiment 1), newly learned faces (Experiment 2), and simple shapes that also share a configuration (Experiment 2). The results favored <b>absolute</b> <b>coding</b> of both faces and shapes, and indicate that caricatures derive their power from their distinctiveness...|$|R
40|$|AbstractA phase {{subdivision}} of <b>absolute</b> <b>coding</b> grating {{is proposed}} to improve the resolution and precision of displacement measurement. The used grating consists of multiple code channel grating of Gray code, and the basic code channel is high frequency Rochi grating. Fourier analysis method is adopted to extract the phase value of grating in decoding, and the wrapped phase distribution of basic code channel is obtained by Fourier transform, filtering, and inverse Fourier transform, then the unwrapped phase distribution could be recovered using the phase order information of Gray code. High resolution of phase subdivision can be achieved via linear fitting of phase distribution. Analysis of the basic principle and the subdivision of the phase to improve the resolution of displacement measurement have also been described in this paper. Experimental {{results show that the}} resolution and precision of <b>absolute</b> <b>coding</b> grating is enhanced greatly after phase subdivision and calibration...|$|R
40|$|Abstract. In {{the present}} study, {{we report on}} a series of coding schemes to {{classify}} errors in open-set recognition of speech and environmental sounds from previous perceptual learning experiments with cochlear implant simulations (Loebach and Pisoni, 2007; in press). Open-set responses to MRT and PB words were coded for place of articulation, manner of articulation and voicing errors in the identification of word initial and word final consonants. Open-set responses to meaningful and semantically anomalous Harvard sentences were coded for phonemic, lexical, and thematic errors in keywords. Open-set responses to environmental sounds were coded for errors in identifying the agent, action, and rhythm of the sounds. Overall, our coding scheme provided a more accurate assessment of performance producing higher percent correct recognition scores for the isolated words and environmental sounds than <b>absolute</b> <b>coding</b> schemes that simply identified entire words as correct or incorrect. Although time intensive, these coding schemes revealed perceptual elements with which subjects were having difficulty that were not apparent from the <b>absolute</b> <b>coding</b> scheme. The utility of open-set coding schemes is discussed for perceptual experiments with cochlear implant users, who often must make verbal responses to stimuli...|$|R
40|$|We have designed, implemented, {{and proved}} the {{correctness}} of a compiler generator that accepts action semantic descriptions of imperative programming languages. The generated compilers emit <b>absolute</b> <b>code</b> for an abstract RISC machine language that currently is assembled into code for the SPARC and the HP Precision Architecture. Our machine language needs no run-time type-checking {{and is thus}} more realistic than those considered in previous compiler proofs. We use solely algebraic specifications; proofs are given in the initial model. 1 Introduction The previous approaches to proving correctness of compilers for non-trivial languages all use target code with run-time type-checking. The following semantic rule is typical for these target languages: (FIRST : C; hv 1; v 2 i : S) ! (C; v 1 : S) The rule describes the semantics of an instruction that extracts the first component of the top-element of the stack, provided that the top-element is a pair. If not, then it is implicit that the [...] ...|$|E
40|$|We have designed, implemented, {{and proved}} the {{correctness}} of a compiler generator that accepts action semantic descriptions of imperative program-ming languages. We {{have used it}} to generate compilers for both a toy lan-guage and a non-trivial subset of Ada. The generated compilers emit <b>absolute</b> <b>code</b> for an abstract RISC machine language that is assembled into code for the SPARC and the HP Precision Architecture. The generated code is {{an order of magnitude}} better than that produced by compilers generated by the classical systems of Mosses, Paulson, and Wand. Our machine language needs no runtime type-checking and is thus more realistic than those con-sidered in previous compiler proofs. We use solely algebraic specifications; proofs are given in the initial model. The use of action semantics makes the processable language specifications easy to read and pleasant to work with. We view our compiler generator as a promising first step towards user-friendly and automatic generation of realistic and provably correct compilers...|$|E
40|$|We {{describe}} the automatic generation of a provably correct compiler for a non-trivial subset of Ada. The compiler is generated from an action semantic description; it emits <b>absolute</b> <b>code</b> for an abstract RISC machine language that currently is assembled into code for the SPARC and the HP Precision Architecture. The generated code is {{an order of}} magnitude better than what is produced by compilers generated by the classical systems of Mosses, Paulson, and Wand. The use of action semantics makes the processable language specification easy to read and pleasant to work with. In Proc. ICCL' 92, Fourth IEEE International Conference on Computer Languages, pages 117 [...] 126. 1 Introduction The purpose of a language designer's workbench, envisioned by Pleban, is to drastically improve the language design process. The major components in such a workbench are: ffl A specification language whose specifications are easily maintainable, and accessible without knowledge of the underlying theory; and f [...] ...|$|E
40|$|To answer Alsmith’s {{questions}} {{about the existence of}} a vestibular sense, we out-line {{in the first part of}} our reply why we believe the vestibular sense is a true “sixth sense”. We argue that vestibular information constitutes distinct sensory events and that <b>absolute</b> <b>coding</b> of body orientation and motion in the gravity-centered space is the important unique feature of the vestibular system. In the last part of our reply, we extend Alsmith’s experimental suggestions to investigate the vestibular contribution to various perspectival experiences...|$|R
50|$|Moresca, {{performed}} by Geraldine O'Doherty (hp), David O'Doherty (vn), Moya O'Grady (vc), on: <b>Absolute</b> Music label <b>code</b> (CD, 2009).|$|R
40|$|The Flosolver {{parallel}} computer {{designed and built}} at NAL for fluid dynamics problem solving is described. The computer has two nodes each having four processors based on Intel 8086 - 8087 chips. In each node one of the processors acts as host and has access to {{a section of the}} 13; private memory of the remaining three processors through the Multibus. Inter-node communication is done using parallel ports. Synchronization and inter-processor communication is done by passing the message through a global memory. Prior to execution the host processor loads 13; the <b>absolute</b> <b>codes</b> from a disk to the respective processors. Several fluid dynamical problems of practical interest have been programmed on Flosolver using concurrent algorithms, and show that it is comparable 13; in speed with mainframes available in the country...|$|R
40|$|In this paper, the {{set-theoretic}} {{approach in}} the logical theory of normative systems is extended using Broome’s definition of the normative code function. The syntax and semantics for first order metanormative language is defined, and metanormative language is applied in the formalization of the basic principles in Broome’s approach and {{in the construction of}} a logical typology of normative systems. Special attention is given to the types of normative systems which are not definable in terms of the properties of singular sets of requirements (e. g. the realization equivalence of codes, the social compatibility of codes, and the compatibility of codes issued by different normative sources). Examples are given of the application of the typology in the interpretation of philosophical texts. Von Wright’s hypothesis on the connection of logical properties of normative systems, conceived set-theoretically, with standard deontic logic is proved by introducing the translation function between the metanormative language and the restricted language of standard deontic logic. The translation reveals that von Wright’s hypothesis must be appended. The problems of narrow and wide scope readings of the deontic conditionals and of the meaning of iterated deontic operators are addressed using the distinction between relative and absolute normative codes. The theorem on the existence of a realization equivalent <b>absolute</b> <b>code</b> for any relative code is proved...|$|E
40|$|Multi-frequency {{modulation}} is {{a highly}} bandwidth efficient signalling technique for digital communications. In {{order to make the}} technique as insensitive as possible to unknown or fluctuating phase and amplitude changes in the channel transfer function between the transmitter and receiver, frequency domain differential encoding techniques have been developed and their error performance calculated. It is shown that in the range of 2 to 8 bits/Hz of channel bandwidth efficiencies, differential encoding results in a penalty of 3 to 5 db in required Eb /N when compared to fully coherent multi-frequency modulation. Design procedures are presented that provide near optimum QAM constellations for fully differential coding and for the hybrid scheme of differential phase <b>coding</b> and <b>absolute</b> <b>coding</b> of amplitudes. This report was prepared in conjunction with research conducted for the Naval Oceans System Command and funded by the Naval Postgraduate School. [URL] Direct Fundin...|$|R
5000|$|... #Caption: A Gray <b>code</b> <b>absolute</b> {{rotary encoder}} with 13 tracks. At the top, the housing, {{interrupter}} disk, and light source can be seen; {{at the bottom}} the sensing element and support components.|$|R
40|$|In this paper, a novel {{approach}} of an optical type <b>absolute</b> rotary encoder <b>coding</b> pattern is presented. The concept {{is based on the}} principle of the absolute encoder to find out a unique sequence that ensures an unambiguous shaft position of any angular. We design a single-ring and a n-by- 2 matrix <b>absolute</b> encoder <b>coding</b> pattern by using the variations of Hamiltonian graph principle. 12 encoding bits is used in the single-ring by a linear array CCD to achieve an 1080 -position cycle encoding. Besides, a 2 -by- 2 matrix is used as an unit in the 2 -track disk to achieve a 16 -bits encoding pattern by using an area array CCD sensor (as a sample). Finally, a higher resolution can be gained by an electronic subdivision of the signals. Compared with the conventional gray or binary code pattern (for a 2 nresolution), this new pattern has a higher resolution (2 n∗n) with less coding tracks, which means the new pattern can lead to a smaller encoder, which is essential in the industrial production. © 2017 SPIE. </p...|$|R
40|$|AbstractValentine’s (Valentine T. Q J Exp Psychol 1991; 43 A: 161 – 204) face {{recognition}} framework supports both a norm-based coding (NBC) and an exemplar-only, <b>absolute</b> <b>coding,</b> model (ABC). According to NBC; (1) faces {{are represented in}} terms of deviations from a prototype or norm; (2) caricatures are effective because they exaggerate this norm deviation information; and (3) other-race faces are coded relative to the (only available) own-race norm. Therefore NBC predicts that, for European subjects, caricatures of Chinese faces made by distorting differences from the European norm {{would be more effective}} than caricatures made relative to the Chinese norm. According to ABC; (1) faces are encoded as absolute values on a set of shared dimensions with the norm playing no role in recognition; (2) caricatures are effective because they minimise exemplar density and (3) the dimensions of face-space are inappropriate for other-race faces leaving them relatively densely clustered. ABC predicts that all faces would be recognised more accurately when caricatured against their own-race norm. We tested European subjects’ identification of European and Chinese faces, caricatured against both race norms. The ABC model’s prediction was supported. European faces were also rated as more distinctive and recognised more easily than Chinese faces. However, the own-race recognition bias held even when the races were equated for distinctiveness which suggests that the ABC model may not provide a complete account of race effects in recognition...|$|R
50|$|While SLOC is an {{accepted}} way of measuring the <b>absolute</b> size of <b>code</b> from the developer's perspective, metrics such as function points capture software size functionally from the user's perspective. The function-based sizing (FBS) metric extends function points so that hidden parts of software such as complex algorithms can be sized more readily. FBS is translated directly into unadjusted function points (UFP).|$|R
40|$|The viscous, Navier-Stokes solver for {{turbomachinery}} applications, MSUTC {{has been}} modified {{to include the}} rotating frame formulation. The three-dimensional thin-layer Navier-Stokes equations have been cast in a rotating Cartesian frame enabling the freezing of grid motion. This also allows the flow-field associated with an isolated rotor {{to be viewed as}} a steady-state problem. Consequently, local time stepping can be used to accelerate convergence. The formulation is validated by running NASA's Rotor 67 as the test case. results are compared between the rotating frame <b>code</b> and the <b>absolute</b> frame <b>code.</b> The use of the rotating frame approach greatly enhances the performance of the code with respect to savings in computing time, without degradation of the solution...|$|R
40|$|This article {{presents}} {{a brief summary}} of analysis of reliability of the current file production program in process at the Human Relations Area Files. The average level of <b>absolute</b> <b>coding</b> agreement between the original analyst (marker) and a secondary analyst or replicator for the seven sources comprising the replication unit {{was found to be}} 48 %. [Accepted for publication: September 1974. ] The replication unit that is discussed in this report consists of a total oaf seven sources, three on Korea and four on the Blackfoot. Each source was marked by an analyst in the usual manner, making full use of any previous marking decisions or other background information pertaining to the respective files. A 10 ~ sample of unmarked pages from each source, chosen {{on the basis of a}} table of random numbers, was then given to a second analyst, who, without any consultation with the original marker, reanalyzed the pages. Each replicated page was com-pared to the marker’s original page, and the level of exact cate-gory agreement between analysts was computed. The range in levels of absolute analyst agreement varied from a low of 34 % to a high of 58 %, or an average of 48 %. This is near the average of 47 % obtained using the same method of data analysis for the three sources reported by Beierle and Witkowski in the article &dquo;HRAF Coding Reliability&dquo; (1974 : 57 - 65). While an agreement proportion of 48 % does not seem impres-° John M. Beierle is Senior Analyst on the HRAF staff with more than fourteen years of experience in analysis procedure, and is a member o...|$|R
40|$|We define special Hamiltonian-paths {{and special}} {{permutations}} of the up-facing dark tiles on a checked triangular grid {{related to the}} generalized Sierpiński Gasket. Our definitions and observations make possible the generalization of the Sierpiński Arrowhead Curve for all orders. We produce these symmetric recursive curves {{in many ways by}} two kinds of asymmetric paths which are in a bijective relation and unambiguously transformable into each other in any order. These node-rewriting and edge-rewriting recursive curves keep their self-avoiding and simple properties after the transformation and their cardinality specifies a new integer sequence. We show a transformation table to change the curves into each other and we give another table to change them into Lindenmayer-system strings both by the <b>absolute</b> direction <b>codes</b> of their edges. Comment: 16 pages, 10 figures, 3 table...|$|R
5000|$|At the Tuskegee University's 79th Annual Scholarship Convocation/Parents' Recognition Program he {{made the}} {{following}} statement regarding the subject of evolution while urging his audience to take seriously their role as the higher species on this planet. Genesis' was right, {{and there was a}} creation, and that Creator is still involved... We are the only species that can destroy the Earth or take care of it and nurture all that live on this very special planet. I'm urging you to look on these things. For whatever reason, this planet was built specifically for us. Working on this planet is an <b>absolute</b> moral <b>code.</b> ... Let's go out and do what we were put on Earth to do." [...] Old Earth creationist and astronomer Hugh Ross spoke at Smalley's funeral, November 2, 2005.|$|R
40|$|Fifteen strong-motion accelerographs, {{each with}} the {{capability}} of writing the WWVB <b>absolute</b> time <b>code</b> on the recorded accelerogram, have been deployed in an elliptical array, at a station spacing of several kilometers, along the San Andreas Fault in the Bear Valley region of central California. Ten accelerograms were obtained for the June 22, 1973, earthquake (M = 3. 9), located {{near the center of}} the array. Preliminary analyses of these accelerograms support previous suggestions that the crystalline rocks of the Gabilan Range possess higher material velocities and lower intrinsic absorption than do the Cretaceous and Cenozoic sedimentary rocks northeast of the fault zone. These accelerograms clearly indicate that a strong-motion accelerograph array of this sort can provide the basic data for source mechanism, wave propagation, and local ground-motion studies for earthquakes with magnitudes as small as 3. 5 - 4. 0...|$|R
40|$|This paper {{advances}} {{an integrated}} learning and evolutionary computation methodology for approaching {{the task of}} learning the face space. The methodology is geared to provide a framework whereby enhanced and robust face coding and classification schemes can be derived and evaluated using both machine and human benchmark studies. In particular we take an interdisciplinary approach, drawing from the accumulated and vast knowledge of both the computer vision and psychology communities, and describe how evolutionary computation and statistical learning can engage in mutually beneficial relationships in order to define an exemplar (<b>Absolute)</b> -Based <b>Coding</b> (ABC) multidimensional face space representation for successfully coping with changing population (face) types, and to leverage past experience for incremental face space definition. 1. Introduction Among the most challenging tasks for visual form (`shape') analysis and object recognition are understanding how people process and recognize [...] ...|$|R
40|$|Alignment of {{parallel}} corpora {{is a crucial}} step prior to training statistical language models for machine translation. This paper investigates compression-based methods for aligning sentences in an English-Chinese parallel corpus. Four metrics for matching sentences required for measuring the alignment at the sentence level are compared: the standard sentence length ratio (SLR), and three new metrics, absolute sentence length difference (SLD), compression code length ratio (CR), and <b>absolute</b> compression <b>code</b> length difference (CD). Initial experiments with CR show that using the Prediction by Partial Matching (PPM) compression scheme, a method that also performs well at many language modeling tasks, significantly outperforms the other standard compression algorithms Gzip and Bzip 2. The paper then shows that for sentence alignment of a parallel corpus with ground truth judgments, the compression code length ratio using PPM always performs better than sentence length ratio and the difference measurements also work better than the ratio measurements...|$|R
40|$|The {{essential}} {{link between}} Volksmarchen and Kunstmarchen is the consistent {{theme in the}} Volksmarchen of restoration of a distorted world harmony. The impulse for the Romantic Kunstmarchen {{was an attempt to}} effect a reestablishment of order, both in the world and for the authors, by means of the Marchen. Connotations of Kunst in Kunstmarchen as either art or imitation correspond to the author's attitude towards the magical force: awe inspires artistry, but presumption produces imitations. Hebbel's abandonment of an <b>absolute</b> moral <b>code</b> results in a fundamental contradiction with the premises of the Marchen. Consequently, Der Rubin is an unconvincing attempt to reconstruct an all-encompassing harmony. Tieck, in Der blonde Eckbert, upholds the necessity of absolutes in the moral code as a foundation for the Marchen. Likewise, Huch shows in Lugenmarchen the unsatisfactory result of attempting to approach the magical realm through presumption and calculation...|$|R
50|$|The game simulates Newton's {{first law}} in that a ship will {{maintain}} its current speed and heading unless it expends {{power to change}} them. The game simulates Newtons second law in that the more massive the ship the more power required to accelerate, decelerate, or maneuver it. The game does not, however, simulate Newton's third law. First, every ship has its own unique maximum speed {{even though they have}} no limit in the amount of fuel they carry throughout the course of the game. Second, after spending several turns reaching maximum speed, in the course of one turn a ship can be brought about and end the turn traveling in the exact opposite direction at maximum speed. The game does have an <b>absolute</b> hard <b>coded</b> speed limit, but this is based on a limitation to the user interface and is in no way related to a simulation of Special Relativity.|$|R
40|$|This paper {{comparatively}} {{investigates the}} transmission performance of <b>absolute</b> added correlative <b>coding</b> (AACC) using non-return-to-zero (NRZ) and return-to-zero (RZ) pulse shapes with a binary intensity modulation direct detection receiver in 40 Gb/s optical metro-access networks operating at 1550 nm. It is shown that, for AACC transmission, the NRZ impulse shaping is superior {{in comparison to}} RZ in spectral efficiency, dispersion tolerance, residual dispersion and self-phase modulation (SPM) tolerance. However, RZ-AACC experiences a ~ 1 – 2 dB advantage in receiver sensitivity over NRZ-AACC for back-to-back configuration as well as after 300 -km single-mode fiber delivery...|$|R
40|$|Looking {{back a few}} years, we may {{notice that}} we finally {{abandoned}} assembly language programming in almost every domain. How did that happen? In part, improvements in compiler technology and hardware speed made high-level languages competitive. But the main reason is that assembly code is inherently not portable: one cannot recompile it for a new architecture. Since recompilation is an off-line process, let us say that assembly code is not off-line portable. The main problems this causes are: ¥ It is difficult to automatically translate assembly code to new architectures (with reasonable performance). ¥ New architectures have been emerging faster than any feasible rate of manual recoding for legacy software. New techniques can handle legacy assembly code, such as emulation and emulationbacked translation. But {{the combination of the}} two problems above has overwhelmed any consideration based on <b>absolute</b> <b>coding</b> efficiency. As a result, new programs are now written in off-line portable languages: they are routinely recompiled for different architectures. We can now draw an interesting analogy. Until very recently no major language was on-line portable. That is, one could not take a running program and port it to a different architecture while the program was running. This, however, is precisely what must happen with network computations, because: ¥ It is difficult to recompile source code on the fly for a new architecture (with reasonable performance). ¥ Connections to computers based on unknown architectures are established faster than {{the time it takes to}} recompile source code. Techniques have emerged to get some of the advantages of both off-line and on-line portability, such as just-in-time compilation and run-time linking. But the emphasis is now on mobility and quick compilation, not on optimized code generation. Mobility poses a new basic question: what is the effect of taking a running computation and moving it to another network site? In most current languages, this makes little sense; the mechanisms for doing so are usually unavailable, and the effect would likely be unpredictable. In order to move computations we need languages and models where mobility makes sense; that is, where its effects are well defined...|$|R
40|$|In this paper, {{a review}} is given on the {{principles}} of optical fiber Bragg grating (FBG) sensors and their applications in textile structural composites (TSCs). As a class of novel all-fiber based components, FBG consists of a spatially periodic modulation in the refractive index along a short length of an optical fiber. FBG sensors are immune to electromagnetic interference, small in size, and easily embedded in a variety of composite materials without compromising the host structures. They can be mass-produced in low cost and are unique in their sensing strategies for <b>absolute</b> wavelength <b>coding</b> and multiplexing. The intrinsic wavelength division multiplexing and localized sensing abilities of FBGs are their most important advantages as they provide an effective means for monitoring physical parameters along a single fiber path. This ability to determine quantitatively internal distributions of physical parameters within a TSC integrated with such sensors has been demonstrated by a number of workers and will lead {{to a better understanding of}} the relationship between the structure and physical properties. In this paper, a review is given {{on the principles}} of optical fiber Bragg grating (FBG) sensors and their applications in textile structural composites (TSCs). As a class of novel all-fiber based components, FBG consists of a spatially periodic modulation in the refractive index along a short length of an optical fiber. FBG sensors are immune to electromagnetic interference, small in size, and easily embedded in a variety of composite materials without compromising the host structures. They can be mass-produced in low cost and are unique in their sensing strategies for <b>absolute</b> wavelength <b>coding</b> and multiplexing. The intrinsic wavelength division multiplexing and localized sensing abilities of FBGs are their most important advantages as they provide an effective means for monitoring physical parameters along a single fiber path. This ability to determine quantitatively internal distributions of physical parameters within a TSC integrated with such sensors has been demonstrated by a number of workers and will lead to a better understanding of the relationship between the structure and physical properties. Institute of Textiles and ClothingDepartment of Electrical Engineerin...|$|R
25|$|In this era, {{the growing}} {{power of the}} state {{strengthened}} the monarchy, allowing it to undertake reforms to strengthen the monarch's authority. The most extensive of these reforms was carried out in Qin by Shang Yang, including the abolition of the feudal nobility, redistribution of nobles' land based on military merit, and allowing private ownership of land. He encouraged the cultivation of unsettled lands, gave noble ranks to soldiers who performed well in battle, and established an efficient and strict legal <b>code.</b> <b>Absolute</b> monarchy persisted in China until its gradual weakening under the Song and Ming dynasties.|$|R
40|$|We {{effortlessly}} perform reach {{movements to}} objects {{in different directions}} and depths. However, how networks of cortical neurons compute reach depth from binocular visual inputs remains largely unknown. To {{bridge the gap between}} behavior and neurophysiology, we trained a feed-forward artificial neural network to uncover potential mechanisms that might underlie the 3 D transformation of reach depth. Our physiologically-inspired 4 -layer network receives distributed 3 D visual inputs (1 (st) layer) along with eye, head and vergence signals. The desired motor plan was coded in a population (3 (rd) layer) that we read out (4 (th) layer) using an optimal linear estimator. After training, our network was able to reproduce all known single-unit recording evidence on depth coding in the parietal cortex. Network analyses predict the presence of eye/head and vergence changes of depth tuning, pointing towards a gain-modulation mechanism of depth transformation. In addition, reach depth was computed directly from eye-centered (relative) visual distances, without explicit <b>absolute</b> depth <b>coding.</b> We suggest that these effects should be observable in parietal and pre-motor areas...|$|R
40|$|This paper {{provides}} an {{ex ante analysis}} {{of the effect of}} financial insolvency codes on investment by examining the main characteristics embodied in several codes that may cause investment distortions. The results from the estimation of an extended version of the q model of investment show a negative relationship between ex ante insolvency costs and investment. Furthermore, most of the analysed characteristics of insolvency codes negatively impact on investment; however, the magnitude of this effect is greater concerning those of reorganization without creditors' consent and creditors' lack of control, as compared to those of automatic stay and the violation of <b>absolute</b> priority. Insolvency <b>codes</b> Investment Insolvency costs...|$|R
50|$|Position-independent code can be {{executed}} at any memory address without modification. This differs from relocatable code, {{in which a}} linker or program loader modifies a program before execution {{so it can be}} run only from a particular memory location. Generating position-independent code is often the default behavior for compilers, but they may place restrictions on the use of some language features, such as disallowing use of <b>absolute</b> addresses (position-independent <b>code</b> has to use relative addressing). Instructions that refer directly to specific memory addresses sometimes execute faster, and replacing them with equivalent relative-addressing instructions may result in slightly slower execution, although modern processors make the difference practically negligible.|$|R
40|$|The {{institutionalization of}} Utopia Studies {{in the last}} decade is premised upon a {{specifically}} aesthetic reception of Ernst Bloch’s theory of the “utopian impulse” during the 1980 s and 1990 s. A postmodern uneasiness to both left and right formulations of the "End of History" during this period imposes a resistance to concepts of historical and political closure or totality, resulting in a "Utopianism without Utopia". For all the attractiveness of this pan-utopianism, its failure to consider the relation between historical representation and fulfillment renders it consummate with liberalism as a merely inverted conservatism. In contrast to this specific recuperation of a Bloch, the continuing importance of Walter Benjamin’s theory of the dialectical image and the speculative concept of historical experience which underlies it becomes apparent. The intrusion of the historical <b>Absolute</b> is <b>coded</b> throughout Benjamin’s thought as the eruptive and mortuary figure of catastrophe, which stands as the dialectical counterpart to the utopian wish images of the collective dream. Indeed, the motto under which the Arcades Project was to be constructed derives from Adorno: “Each epoch dreams of itself as annihilated by catastrophe”...|$|R
40|$|ISSN: 0278 - 0070 The {{design of}} {{checkers}} {{aimed at the}} concurrent test of analog and mixed-signal circuits is considered in this paper. These checkers can on-line test duplicated and fully differential analog circuits. The test approach is based on exploiting the inherent redundancy of these circuits which results {{in the use of}} a code for the analog signals. The analog code is monitored by the checkers. An error signal which complies with existing digital self-checking parts is generated in the case that a code fails out of the valid code space. For the verification of the analog <b>codes,</b> <b>absolute</b> tolerance margins and tolerance margins which are made relative to signal amplitude are considered. A test pattern generator for off-line testing of the checkers is proposed...|$|R
