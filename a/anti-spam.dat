640|16|Public
5|$|In October 2009, AV-TEST {{conducted}} {{a series of}} trials on the final build of the product in which it detected and caught 98.44 percent of 545,034 computer viruses, computer worms and software Trojan horses as well as 90.95 percent of 14,222 spyware and adware samples. It also detected and eliminated all 25 tested rootkits, generating no false-positives. Between June 2010 to January 2013, AV-TEST tested Microsoft Security Essentials 14 times; in 11 out of 14 cases, MSE secured AV-TEST certification of outperforming AV industry average ratings. Microsoft Security Essentials 2.0 was tested and certified on March 2011. The product achieved a protection score of 2.5 out of 6, a repair score of 3.5 out of 6 and a usability score of 5.5 out of 6. Report details show that although version 2.0 was able to find all malware samples of the WildList (widespread malware), it was not able to stop all Internet-based attacks because it lacks personal firewall and <b>anti-spam</b> capabilities. In an April 2012 test, version 2.1 achieved scores of 3.0, 5.5 and 5.0 for protection, repair and usability. Version 4.0 for Windows 7 SP1 (x64) was tested in June 2012 and achieved scores of 2.5, 5.5 and 5.5 for protection, repair and usability. In October 2012, the product lost its AV-TEST certification when Microsoft Security Essentials 4.1 achieved scores of 1.5, 3.5 and 5.5 for its protection, repair and usability.|$|E
25|$|He was {{the only}} member of the House of Representatives to vote against an <b>anti-spam</b> email bill in 2000, and one of only 5 members of the entire Congress to vote against a {{subsequent}} <b>anti-spam</b> email bill in 2003.|$|E
25|$|Facebook {{implements}} its <b>anti-spam</b> {{programs in}} Haskell, as open-source software.|$|E
50|$|Spammers use Base64 {{to evade}} basic <b>anti-spamming</b> tools, which {{often do not}} decode Base64 and {{therefore}} cannot detect keywords in encoded messages.|$|R
50|$|An active UDP was {{implemented}} against CompuServe on 18 November 1997, which was lifted {{the following day}} after the company implemented <b>anti-spamming</b> measures and instituted a new acceptable use policy addressing spamming.|$|R
2500|$|In January 2011 {{the airline}} was fined $110,000 after {{breaking}} <b>anti-spamming</b> regulations. Consumers complained {{they were unable}} to unsubscribe from the airline's mailing list. The Australian Communications and Media Authority said the airline would [...] "Engage an independent third party to thoroughly assess its email marketing processes and to implement any recommended changes." ...|$|R
25|$|SlopsBox, a {{disposable}} e-mail address <b>anti-spam</b> service, {{also appeared in}} December, and was reviewed in 2009.|$|E
25|$|Greatest Contribution to <b>anti-spam</b> in {{the last}} 10 years {{presented}} to Spamhaus by Virus Bulletin Magazine.|$|E
25|$|E-mail servers can query blacklist.example to {{find out}} if a {{specific}} host connecting to them is in the blacklist. Many of such blacklists, either subscription-based or free of cost, are available for use by email administrators and <b>anti-spam</b> software.|$|E
40|$|Abstract. The {{rapidly growing}} {{commercial}} interest in Linked Data raises {{the prospect of}} “Linked Data spam”, which we define as “deliberately misleading information (data and links) published as Linked Data, {{with the goal of}} creating financial gain for the publisher”. Compared to conventional technologies affected by spamming, e. g. email and blogs, spammers targeting Linked Data {{may not be able to}} push information directly towards consumers, but rather may seek to exploit a lack of human involvement in automated data integration processes performed by applications consuming Linked Data. This paper aims to lay a foundation for future work addressing the issue of Linked Data spam, by providing the following contributions: i) a formal definition of spamming in Linked Data; ii) a classification of potential spamming techniques; iii) a sample dataset demonstrating these techniques, for use in evaluating <b>anti-spamming</b> mechanisms; iv) preliminary recommendations for <b>anti-spamming</b> strategies...|$|R
50|$|For example, in March 2002, Haselton won a $1000 {{award at}} King County District Court in Bellevue, Washington {{in each of}} three cases against Red Moss Media, Paulann Allison, and Richard Schueler (for sending misleading, unsolicited, {{commercial}} emails to its webmaster bearing deceptive information such as a forged return e-mail address or a misleading subject line), in a test of Washington's tough <b>anti-spamming</b> laws.|$|R
40|$|Search {{engines are}} playing a more and more {{important}} role in discovering information nowadays. Due to limi-tations of time-consuming, network bandwidth and hard-wares, we cannot obtain the whole information on the web and have to download important information first. In this {{paper we propose a}} novel crawling ordering strategy which is based on SiteRank. Experimental results running on over 15 million pages indicate that it can work efficiently in dis-covering important pages under the PageRank evaluation of page quality. Furthermore, it exhibits the ability of <b>anti-spamming.</b> ...|$|R
25|$|A false {{positive}} occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most <b>anti-spam</b> tactics can block or filter {{a high percentage}} of unwanted emails, doing so without creating significant false-positive results is a much more demanding task.|$|E
25|$|The Spamhaus Project is {{responsible}} for compiling several widely used <b>anti-spam</b> lists. Many internet service providers and email servers use the lists {{to reduce the amount}} of spam that reaches their users. In 2006, the Spamhaus services protected 650 million email users, including the European Parliament, US Army, the White House and Microsoft, from billions of spam emails a day.|$|E
25|$|In January 2007, a Sheriff Court in Scotland awarded Mr. Gordon Dick £750 (the then maximum sum {{that could}} be awarded in a Small Claim action) plus {{expenses}} of £618.66, a total of £1368.66 against Transcom Internet Services Ltd. for breaching <b>anti-spam</b> laws. Transcom had been legally represented at earlier hearings, but were not represented at the proof, so Gordon Dick got his decree by default. It is the largest amount awarded in compensation in the United Kingdom since Roberts v Media Logistics case in 2005.|$|E
30|$|Storm, a {{state-of-the-art}} botnet of its time, {{was known to}} comprise {{of at least a}} few million ‘bots’ when at its peak. It was involved in massive spamming activities in early 2007. Even the <b>anti-spamming</b> websites which targeted Storm came under a DDoS attack by the botnet [14]. Researchers have confirmed that the Waledac botnet is an improved version of the Storm botnet [15]. Waledac was capable of sending about 1.5 billion spam messages a day. It also had the capabilities to download and execute binaries and mine the infected systems for sensitive data. It was taken down in the year 2010.|$|R
40|$|Last week, we woke to {{news that}} the largest cyber attack ever was {{underway}} in Europe, with reports of global internet speeds falling {{as a result of}} an assault on the <b>anti-spamming</b> company Spamhaus. In recent weeks, the Reserve Bank of Australia has been the target of a cyber attack, as have South Korean banks and broadcasters and BBC Twitter accounts. The above stories were all reported as “hacking” – a blanket term readily used to encompass a whole range of attacks, from crashing a server to more sophisticated infiltration, such as stealing passwords. But, generally, news stories don’t discriminate. So what are hackers and their methods really like? What follows is something of a glossary, to cut out (or at least bookmark) and keep...|$|R
5000|$|Micropoetry is a {{genre of}} poetic verse {{including}} tweetku (also known as twihaiku, twaiku, or twitter poetry) and captcha poetry, which {{is characterized by}} text generated through CAPTCHA <b>anti-spamming</b> software. The novelist W. G. Sebald {{may have been the}} first to use the term [...] "micropoem," [...] in reference to the poems of about 20 words in length that made up his 2004 work, Unrecounted. The more recent popularity of [...] "micropoetry" [...] to describe poems of 140 characters in length or shorter appears to stem from a separate coinage, as a portmanteau of [...] "microblogging" [...] and [...] "poetry" [...] in a notice on Identica on January 23, 2009, announcing the formation of a group for fans of poetry on that microblogging service. A subsequent notice linked to an example of micropoetry by another user, which was clearly lyrical but didn't appear to fit any preexistent form such as haiku or tanka.|$|R
25|$|Within a few years, {{the focus}} of {{spamming}} (and <b>anti-spam</b> efforts) moved chiefly to email, where it remains today. Arguably, the aggressive email spamming {{by a number of}} high-profile spammers such as Sanford Wallace of Cyber Promotions in the mid-to-late 1990s contributed to making spam predominantly an email phenomenon in the public mind. By 1999, Khan C. Smith, a well known hacker at the time, had begun to commercialize the bulk email industry and rallied thousands into the business by building more friendly bulk email software and providing internet access illegally hacked from major ISPs such as Earthlink and Botnets.|$|E
25|$|During 2001 Xtra and Actrix (another New Zealand Internet service provider) won a High Court {{injunction}} to force Alan Brown, the maintainer of the Open Relay Behavior-modification System (ORBS) <b>anti-spam</b> blacklist, {{to remove them}} from the list. ORBS was a blacklist of IP addresses relating to open mail relays like those run by Xtra, which enable spammers to send unsolicited bulk e-mail. Hundreds of organisations subscribed to the list, including Bigfoot.com {{and at least one}} other large free mail provider. They rejected e-mail from any IP address listed in ORBS. The court action led (indirectly) to the end of one of the oldest DNSBL services.|$|E
25|$|Some see spam-blocking tools as {{a threat}} to free expression—and laws against {{spamming}} as an untoward precedent for regulation or taxation of e-mail and the Internet at large. Even though it is possible in some jurisdictions to treat some spam as unlawful merely by applying existing laws against trespass and conversion, some laws specifically targeting spam have been proposed. In 2004, United States passed the CAN-SPAM Act of 2003 that provided ISPs with tools to combat spam. This act allowed Yahoo! to successfully sue Eric Head, reportedly one of the biggest spammers in the World, who settled the lawsuit for several thousand U.S. dollars in June 2004. But the law is criticized by many for not being effective enough. Indeed, the law was supported by some spammers and organizations that support spamming, and opposed by many in the <b>anti-spam</b> community. Examples of effective anti-abuse laws that respect free speech rights include those in the U.S. against unsolicited faxes and phone calls, and those in Australia and a few U.S. states against spam.|$|E
40|$|The {{explosive}} use {{of social}} media {{also makes it}} a popular platform for malicious users, known as social spammers, to overwhelm normal users with unwanted content. One effective way for social spammer detection {{is to build a}} classifier based on content and social network information. However, social spammers are sophisticated and adaptable to game the system with fast evolving content and network patterns. First, social spammers continually change their spamming content patterns to avoid being detected. Second, reflexive reciprocity makes it easier for social spammers to establish social influence and pretend to be normal users by quickly accumulating a large number of “human ” friends. It is challenging for existing <b>anti-spamming</b> systems based on batch-mode learning to quickly respond to newly emerging patterns for effective social spammer detection. In this paper, we present a general optimization framework to collectively use content and network information for social spammer detection, and provide the solution for efficient online processing. Experimental results on Twitter datasets confirm the effectiveness and efficiency of the proposed framework...|$|R
40|$|Spamming {{has been}} a {{widespread}} problem for social networks. In recent years there is an increasing interest {{in the analysis of}} <b>anti-spamming</b> for microblogs, such as Twitter. In this paper we present a systematic research on the analysis of spamming in Sina Weibo platform, which is currently a dominant microblogging service provider in China. Our research objectives are to understand the specific spamming behaviors in Sina Weibo and find approaches to identify and block spammers in Sina Weibo based on spamming behavior classifiers. To start with the analysis of spamming behaviors we devise several effective methods to collect a large set of spammer samples, including uses of proactive honeypots and crawlers, keywords based searching and buying spammer samples directly from online merchants. We processed the database associated with these spammer samples and interestingly we found three representative spamming behaviors: Aggressive advertising, repeated duplicate reposting and aggressive following. We extract various features and compare the behaviors of spammers and legitimate users with regard to these features. It is found that spamming behaviors and normal behaviors have distinct characteristics. Based on these findings we design an automatic online spammer identification system. Through tests with real data it is demonstrated that the system can effectively detect the spamming behaviors and identify spammers in Sina Weibo...|$|R
40|$|<b>Anti-spamming</b> {{has become}} one of the most {{important}} challenges to web search engines and attracted increasing attention in both industry and academia recently. Since most search engines now use link-based ranking algorithms, link-based spamming has become a major threaten. In this paper, we show that the popular link-based ranking algorithm PageRank, while being successfully used in the Google search engine, has a "zero-one gap" flaw, which can be potentially exploited to spam PageRank results easily. The "zero-one gap" problem arises from the current ad hoc way of computing the transition probabilities in the random surfing model. We propose a novel DirichletRank algorithm in a more principled way of computing these probabilities based on Bayesian estimation with a Dirichlet prior. DirichletRank is a variant of PageRank, but it does not have the problem of "zero-one gap" and is analytically shown to be substantially more resistant to link farm spams than PageRank. Simulation experiments using real web data show that, compared with the original PageRank, DirichletRank is significantly more robust against several typical link spasm and is more stable under link perturbations, in general. Moreover, experiment results also show that DirichletRank is more effective than PageRank due to its more reasonable allocation of transition probabilities. Since DirichletRank can be computed as efficiently as PageRank, it is scalable to large-scale web applications...|$|R
500|$|In 2000, Marvin Johnson, a {{legislative}} {{counsel for the}} ACLU, stated that proposed <b>anti-spam</b> legislation infringed on free speech by denying anonymity and by forcing spam to be labeled as such, [...] "Standardized labeling is compelled speech." [...] He also stated, [...] "It's relatively simple to click and delete." [...] The debate found the ACLU joining with the Direct Marketing Association and the Center for Democracy and Technology in 2000 in criticizing a bipartisan bill in the House of Representatives. As early as 1997, the ACLU had taken a strong position that nearly all spam legislation was improper, although it has supported [...] "opt-out" [...] requirements in some cases. The ACLU opposed the 2003 CAN-SPAM act suggesting {{that it could have}} a chilling effect on speech in cyberspace. It has been criticized for this position.|$|E
2500|$|Gary Robinson, {{software}} engineer noted for <b>anti-spam</b> algorithms ...|$|E
2500|$|The Spamhaus Project {{consists}} {{of a number of}} independent companies which focus on different aspects of Spamhaus <b>anti-spam</b> technology or provide services based around it. At the core is The Spamhaus Project Ltd., which tracks spam sources and publishes free DNSBLs. Further companies include Spamhaus Logistics Corp., which owns the large server infrastructure used by Spamhaus and employs engineering staff to maintain it. Spamhaus Technology Ltd., a data delivery company which [...] "manages data distribution and synchronization services". Spamhaus Research Corp., a company which [...] "develops <b>anti-spam</b> technologies". The Spamhaus Whitelist Co. Ltd., which manages the Spamhaus Whitelist. Also there are several references on the Spamhaus website to The Spamhaus Foundation, whose charter is [...] "to assure the long-term security of The Spamhaus Project and its work".|$|E
40|$|Today from {{security}} {{point of}} view, {{the increasing demand}} of network connectivity makes the system insecure. So we need a complement that can cope / prevent the security breaches in system. Unfortunately, in many environments, {{it may not be}} feasible to render the computer system immune to all type of intrusions. The motivation behind this project is to develop a complement system i. e. Sybil detection that can prevent all possible breaks-ins. In Sybil attack each node can be preferred to as a separate webpage that the spammer creates, and each of these webpages interlink to each other thus forming a network similar to link farms. The spamming webpages link to other nodes and thus create a huge linked for improving popularity. In this paper we are introduce anti-image spamming technique for preventing to Sybil attack to attackers. Our objective is to work on to the real time research desired scenario where the work is needed to be done. Sybil attack is one of the scenario where a work has been done to deal with Sybil attack efficiently in some of the field but still there are more has to be done which we want to carry forward with the help of spamming image detection and prevention technique with the help of image <b>anti-spamming</b> technique. In this paper we present result analysis based on the Sybil detected by different technique performed by us...|$|R
40|$|Web spammers aim {{to obtain}} higher ranks for their web pages by {{including}} spam contents that deceive search engines {{in order to}} include their pages in search results {{even when they are}} not related to the search terms. Search engines continue to develop new web spam detection mechanisms, but spammers also aim to improve their tools to evade detection. In this study, we first explore the effect of the page language on spam detection features and we demonstrate how the best set of detection features varies according to the page language. We also study the performance of Google Penguin, a newly developed anti-web spamming technique for their search engine. Using spam pages in Arabic as a case study, we show that unlike similar English pages, Google <b>anti-spamming</b> techniques are ineffective against a high proportion of Arabic spam pages. We then explore multiple detection features for spam pages to identify an appropriate set of features that yields a high detection accuracy compared with the integrated Google Penguin technique. In order to build and evaluate our classifier, as well as to help researchers to conduct consistent measurement studies, we collected and manually labeled a corpus of Arabic web pages, including both benign and spam pages. Furthermore, we developed a browser plug-in that utilizes our classifier to warn users about spam pages after clicking on a URL and by filtering out search engine results. Using Google Penguin as a benchmark, we provide an illustrative example to show that language-based web spam classifiers are more effective for capturing spam contents...|$|R
2500|$|The {{founder of}} Automattic, Matt Mullenweg {{decided to create}} Akismet so that his mother could blog in safety. In 2005, there were ongoing discussions {{about how to deal}} with comment spam and a few {{plug-ins}} were available. Mullenweg's first attempt was a JavaScript plug-in which modified the comment form and hid fields, but within hours of its launch, spammers downloaded it, figured out how it worked, and bypassed it. This is a common pitfall for <b>anti-spam</b> plug-ins; once they get traction, spammers pay attention and quickly figure out how to bypass them.|$|E
2500|$|In December 2003, Burns and Senator Ron Wyden, an Oregon Democrat, {{were pleased}} that their {{legislation}} to combat spam, the CAN-SPAM Act, had been signed into law. Burns said, [...] "Senator Wyden and I have worked {{during this time}} {{to come up with}} common-sense legislation to deal with spam and I think we've been successful." [...] But in April 2005 <b>anti-spam</b> organization Spamhaus indicated that they were far from satisfied that the problem of spam coming from the U.S. had been addressed. Spamhaus head Steve Linford said, [...] "Until America makes changes, everyone will still be plastered with spam." ...|$|E
2500|$|Some {{companies}} and groups [...] "rank" [...] spammers; spammers who make the news are sometimes referred to by these rankings. The secretive nature of spamming operations {{makes it difficult to}} determine how prolific an individual spammer is, thus making the spammer hard to track, block or avoid. Also, spammers may target different networks to different extents, depending on how successful they are at attacking the target. Thus considerable resources are employed to actually measure the amount of spam generated by a single person or group. [...] For example, victims that use common <b>anti-spam</b> hardware, software or services provide opportunities for such tracking. Nevertheless, such rankings should be taken with a grain of salt.|$|E
2500|$|Bill {{entered the}} field of Internet routing {{research}} in 1989, while serving as the network architect and operations director for an international multiprotocol service-provision backbone network. In 1993 and 1994, Woodcock {{was one of the}} founders of Packet Clearing House, and has served in his current post as Executive Director since 1997. In that time, Woodcock has directly participated in the establishment of more than three hundred public Internet exchange points in Europe, Africa, Asia, and the Americas. He continues to serve on the boards of, and provide ongoing technical and policy advice to many of these institutions. In 1998, Woodcock and J.D. Falk's model spam regulation became the first <b>anti-spam</b> legislation in the world, California law 17538.4, and paved the way for other jurisdictions. [...] Woodcock has successfully concluded telecommunications regulatory reform efforts in several African countries.|$|E
2500|$|In {{an attempt}} to assess {{potential}} legal and technical strategies for stopping illegal spam, a study from the University of California, San Diego, and the University of California, Berkeley, [...] "Click Trajectories: End-to-End Analysis of the Spam Value Chain", cataloged three months of online spam data and researched website naming and hosting infrastructures. The study concluded that: 1) half of all spam programs have their domains and servers distributed over just eight percent or fewer of the total available hosting registrars and autonomous systems, with 80 percent of spam programs overall being distributed over just 20 percent of all registrars and autonomous systems; 2) of the 76 purchases for which the researchers received transaction information, there were only 13 distinct banks acting as credit card acquirers and only three banks provided the payment servicing for 95 percent of the spam-advertised goods in the study; and, 3) a [...] "financial blacklist" [...] of banking entities that do business with spammers would dramatically reduce monetization of unwanted e-mails. Moreover, this blacklist could be updated far more rapidly than spammers could acquire new banking resources, an asymmetry favoring <b>anti-spam</b> efforts.|$|E
2500|$|In {{the late}} 1990s, when the World Wide Web {{was in its}} infancy, courts were more {{receptive}} to extending the trespass to chattels tort to the electronic context. In CompuServe Inc. v. Cyber Promotions, Inc., a 1997 case {{that was the first}} to extend the trespass theory to computer networks, a federal district court held that a marketing company's mass mailing of a high volume of unsolicited advertisement emails to CompuServe subscribers constituted an actionable trespass to chattels. [...] CompuServe customers repeatedly received unwanted advertisements from Cyber Promotions, a company that specialized in sending marketing email in bulk. Cyber Promotions also modified its equipment and falsified other information to circumvent CompuServe's <b>anti-spam</b> measures. Due to the high volume of email, CompuServe claimed damage to its servers as well as money lost dealing with customer complaints and dissatisfaction. CompuServe also extended its damages claim to its subscribers who spent time deleting unwanted email. The court held that Cyber Promotions's intentional use of CompuServe's proprietary server was an actionable trespass to chattels and granted a preliminary injunction enjoining the spammer from sending unsolicited advertisements to any email address maintained by CompuServe. Cyber Promotions' persistence in sending email to CompuServe's servers after receiving notification that CompuServe no longer consented to the use weighed heavily in favor of a finding of trespass.|$|E
50|$|<b>Anti-Spam</b> “Beijing Declaration”2006 International <b>Anti-Spam</b> Summit was held.|$|E
