10|20|Public
2500|$|An <b>address</b> <b>locator</b> is a dataset in ArcGIS that stores {{the address}} attributes, {{associated}} indexes, and rules {{that define the}} process for translating nonspatial descriptions of places, such as street addresses, into spatial data that can be displayed as features on a map. An <b>address</b> <b>locator</b> contains {{a snapshot of the}} reference data used for geocoding, and parameters for standardizing addresses, searching for match locations, and creating output. <b>Address</b> <b>locator</b> files have a [...]loc file extension. In ArcGIS 8.3 and previous versions, an <b>address</b> <b>locator</b> was called a geocoding service.|$|E
40|$|Abstract. Geocoding is {{increasingly}} {{being used for}} public health surveillance and spatial epidemiology studies. Public health departments in the United States of America (USA) often use this approach to investigate disease outbreaks and clusters or assign health records to appropriate geographic units. We evaluated two commonly used geocoding software packages, ArcGIS and MapMarker, for automated geocoding {{of a large number}} of residential addresses from health administrative data in New York State, USA to better understand their features, performance and limitations. The comparison was based on three metrics of evaluation: completeness (or match rate), geocode similarity and positional accuracy. Of the 551, 798 input addresses, 318, 302 (57. 7 %) were geocoded by MapMarker and 420, 813 (76. 3 %) by the ArcGIS composite <b>address</b> <b>locator.</b> High similarity between the geocodes assigned by the two methods was found, especially in suburban and urban areas. Among addresses with a distance of greater than 100 m between the geocodes assigned by the two packages, the point assigned by ArcGIS was closer to the associated parcel centroid (“true ” location) compared with that assigned by MapMarker. In addition, the composite <b>address</b> <b>locator</b> in ArcGIS allows users to fully utilise available reference data, which consequently results in better geocoding results. However, the positional differences found were minimal, and a large majority of addresses were placed on the same locations by both geocoding packages. Using both methods and combining the results can maximise match rates and save the time needed for manual geocoding...|$|E
30|$|Several {{studies have}} offered a {{comparative}} analysis of various free or subscription-based geocoding services. For example, Karimi et al. [13] have evaluated the matching rate of geocoded addresses using web-based geocoding services, including Virtual Earth, Google Maps, Geocoder.us, MapQuest, and Yahoo Maps. In contrast, Swift {{and his team}} members [18] assessed seven commercial geocoding services and one open-source geocoding service—Centrus, Geolytics, ERSI <b>Address</b> <b>Locator,</b> Geocoder.us, Google Earth, Google Maps API, and the Yahoo API and USC Geocoding Platforms, respectively—to match accurately geocoded addresses. The authors selected 50 addresses for this purpose and found that only 42 % of samples matched their reference data, 54 % of addresses matched parcel centroids, and only 4 % addresses matched USPS ZIP code centroids [18]. All the geocoding tools tested produced varying results, indicating that analysts should indeed take care when geocoding physical locations, especially when doing so for purposes of location-based analysis, and should take that same care when selecting geocoding tools in the first place.|$|E
40|$|Addresses are {{important}} data for urban applications. About 80 % {{of the information}} local authorities use have a geographic component that is generally related to addresses. Addressing systems efficiency depend {{on the quality of}} <b>addresses</b> <b>locators.</b> There are several methods to collect data. Surveys from the field are essential: GPS and pre-printed maps can be used to achieve this goal. GPS surveys from the field may be a solution, but it remains practical only for limited areas. To insure an accepted accuracy, GPS methods need special considerations that are time and money consuming. For Casablanca’s <b>addressing</b> <b>locators,</b> an alternative approach was adopted to collect 400 000 points. It took two months, 200 operators and 3500 printed maps to cover a study area of 1, 226 km 2. This paper is to develop an optimized approach based on automated procedure for reintegrating printed maps in a geographic information system (GIS). It saves georeferencing time from 5 min to just seconds per document. It insures, more importantly, an accuracy that is between 20 cmto 1 mfor scales that are between 1 / 500 and 1 / 2500. It ensures maps’ integration, independently of base map and coordinates system by introducing the notion of Georeferencing Code (GC). </p...|$|R
5000|$|Four {{character}} sets {{are used in}} PostBar codes, known as [...] "A", [...] "N", [...] "Z" [...] and [...] "B" [...] characters. Three-bar A characters are used exclusively to encode letters, and two-bar N characters encode only digits. Three-bar Z characters can encode either letters or digits. A and N characters are typically used to encode postal codes and country codes. Z characters are used for <b>address</b> <b>locators,</b> product types, and customer and service information. B characters are one bar each, and are used to encode base-4 machine IDs for Canada Post's internal uses.|$|R
40|$|This paper {{describes}} {{and briefly}} evaluates a distributed and secure mechanism that allows IP <b>addresses</b> used as <b>locators</b> to be automatically distributed and assigned to routers inside an IP network. The routers then {{are responsible for}} the suballocation of these locators to their locally connected endsystems and customers...|$|R
40|$|The {{purpose of}} this study was to {{systematically}} examine the geographic disparity of geocoding error among different geocoding solutions. The research questions include: (1) What are the positional accuracies and matching rates of various geocoding techniques? (2) Are there any significant differences of geocoding quality in rural and urban areas? In this study, 1100 residential addresses scattered across Texas, USA, were address-matched using eight different geocoding platforms, including the ESRI ArcGIS <b>Address</b> <b>Locator,</b> CoreLogic PxPoint, Google Maps API, Yahoo! PlaceFinder, Microsoft Bing, Geocoder. us, Texas A 2 ̆ 6 M University Geocoder, and OpenStreetMap (OSM). The geocoded locations for each method were validated against the GPS data and manual digitization. Using GPS data as reference data, the desktop geocoding using parcel data achieved the highest positional accuracy with a mean error of 24. 8 m, whereas the Google Maps API was the best among the six Internet solutions with a mean error of 31. 7 m. All geocoding solutions, except Geocoder. us and OSM, achieved a matching rate 3 ̆e 95...|$|E
40|$|American cities {{historically}} have been places of significant residential segregation and socioeconomic inequality among neighborhoods. However, scholars rarely {{have attempted to}} map and spatially analyze household-level disparities within cities as they existed before 1880, as census takers did not record address information until that year. Meanwhile, the 1860 and 1870 censuses were unique in that they collected details about personal estate and real estate wealth of individuals, variables that were omitted from subsequent censuses. Historical urban geographers studying socioeconomic differences quantitatively in earlier years often rely instead on data aggregated at larger unit areas, often city wards, in order to draw conclusions about past cultural landscapes. ^ This study develops an historical <b>address</b> <b>locator</b> system for three geographically disparate large cities (Washington, D. C., Nashville, TN in 1860, and Omaha, NE in 1870), and pairs descriptive location information about residents from city directories with rich datasets about those individuals in the census in order to geolocate residents (and their attributes) and investigate historical residential segregation patterns in a more granular way. The research uses the index of dissimilarity at the city block level to evaluate the relative evenness among social and racial groups in the study area. Results indicate that residential segregation along lines of race was higher than previous studies had calculated, and reveals that specific ethnic groups were also moderately separated from the native-born population, albeit at lower rates than those experienced by free people of color. Moreover, residents with higher estate values or “higher status” occupations tended to cluster into specific areas, often along important thoroughfares or near civic buildings, creating distinct patterns across and within the arbitrary political boundaries (e. g. city wards) at which historical geodemographic data are often collected and analyzed. ...|$|E
40|$|OMSIUA GIS APPLICATION Th e OMSIUA GIS {{application}} {{consists of}} a toolbar featuring a mixture of native ArcGIS tools and several custom-designed tools using VBA for ArcObjects (fi g. 5). To locate mine subsidence claims, two tools are used to zoom into the claim location and load all geologic maps and documents. Th e toolbar contains the native ArcGIS Find Tool (fi g. 6), {{which is used to}} locate insurance claims using the <b>Address</b> <b>Locator</b> function. Th e second tool, the Selection Location Tool (fi g. 7), will load all known digital geologic maps and GIS data for a mine subsidence location into the ArcMap document for that location. Some of the GIS datasets include abandoned underground mines; permitted surface mines; 1 : 24, 000 -scale bedrock geology; and the one-foot resolution, digital orthophotography. One of the most important sets of historical records is the collection of 15 -minute thematic geologic maps. Th ese maps have been scanned and indexed. Th e Select Location Tool will identify all scanned maps within a one-mile radius and load them into ArcMap (fi g. 8). Once the information is loaded, the geologist can conduct a preliminary mine subsidence analysis. Th e Underground Mine Information Form (fi g. 9 A) will present the attribute information on abandoned underground mines. Using the form, georeferenced abandoned mine maps can be loaded into ArcMap (fi g. 9 B). Documents can be accessed using the native ArcGIS Hyperlink Tool (fi g. 10). Some of the documents that can be accessed are measured stratigraphic sections, core descriptions, and oil-and-gas well completion cards—all of which can have a description of a coal bed within them and possibly the notation that an underground mine is nearby. After the analysis is completed, portions of the preliminary mine subsidence report can be automated. A tool will export thematic, page-sized, PDF maps (fi g. 11 A). Th e page-sized maps are generated with the correct titles (fi g. 11 B). Th e PDF maps, along with all the geologic documents within one mile of the site will be exported to...|$|E
40|$|This {{document}} {{describes a}} framework for how the current IPv 4 address space {{can be divided into}} two new address categories: a core <b>address</b> space (Area <b>Locators,</b> ALOCs) that is globally unique, and an edge <b>address</b> space (Endpoint <b>Locators,</b> ELOCs) that is regionally unique. In the future, the ELOC space will only be significant in a private network or in a service provider domain. Therefore, a 32 x 32 bit addressing scheme and a hierarchical routing architecture are achieved. The hierarchical IPv 4 framework is backwards compatible with the current IPv 4 Internet. This document also discusses a method for decoupling the location and identifier functions [...] future applications can make use of the separation. The framework requires extensions to the existing Domain Name System (DNS), the existing IPv 4 stack of the endpoints, middleboxes, and routers in the Internet. The framework can b...|$|R
5000|$|In {{the context}} of Internet or Web <b>addresses</b> (Uniform Resource <b>Locators</b> or [...] "URLs"), cruft refers to the {{characters}} which are relevant or meaningful only {{to the people who}} created the site, such as implementation details of the computer system which serves the page. Examples of URL cruft include filename extensions such as [...]php or [...]html, and internal organizational details such as /public/ or /Users/john/work/drafts/.|$|R
40|$|One of {{the great}} {{advantages}} of the World Wide Web (WWW) is the enormous amount of information it makes available. Nevertheless, URL (Universal Resource <b>Locator)</b> <b>addresses</b> obtained from search engines often {{have little to do}} with what users are actually interested in finding. This proves especially problematic when one is searching for educational material. In order to improve on this situation we have designed a search engine that specialises in managing content previously selected from the Internet. The search engine is compatible wit...|$|R
40|$|Objectives: The {{association}} between geographic factors including transport distance and pediatric EMS run clustering on out-of-hospital pediatric endotracheal intubation are unclear. The {{objective of this}} study was to determine if endotracheal intubation procedures are more likely to occur at greater distances from the hospital and near clusters of pediatric calls. Methods: This was a retrospective observational study including all Emergency Medical Services (EMS) runs for patients less than 18 years of age from 2008 to 2014 in a geographically large and diverse Oregon county that includes densely populated urban areas near Portland and remote rural areas. We geocoded scene addresses using the automated <b>address</b> <b>locator</b> created in ArcGIS supplemented with manual address geocoding for remaining cases. We then use the Getis-Ord Gi spatial statistic feature in ArcGIS to map statistically significant spatial clusters (hot spots) of pediatric EMS runs throughout the county. We then superimposed all intubation procedures performed during the study period on maps of pediatric EMS run hot spots, pediatric population density, fire stations, and hospitals. We also performed multivariable logistic regression to determine if distance traveled to the hospital was associated with intubation after controlling for several confounding variables. Results: We identified a total of 7, 797 pediatric EMS runs during the study period and 38 endotracheal intubations. In univariate analysis we found that patients who were intubated were similar to those who were not in gender and whether or not they were transported to a children’s hospital. Intubated patients tended to be transported shorter distances and were older than non-intubated patients. Increased distance from the hospital was associated with reduced odds of intubation after controlling for age, sex, scene location, and trauma system entry status in a multivariate logistic regression. The locations of intubations were superimposed on hot spots of all pediatric EMS runs.   This map demonstrates that most of the intubations occurred within areas where pediatric EMS calls were highly clustered. A map with intubation procedures and pediatric population density we found that intubation procedures were not clustered in a similar distribution to the pediatric population in the county. Conclusions: In this geographically diverse county, the location of intubation procedures was similar to the clustering of pediatric EMS calls and increased distance from the hospital was associated with reduced odds of intubation after controlling for several potential confounding variables. ...|$|E
40|$|Research Task Force (IRTF) is {{currently}} discussing several architectural solutions {{to build an}} interdomain routing architecture that scales better than the existing one. The solutions family currently being discussed concerns the <b>addresses</b> separation into <b>locators</b> and identifiers, LISP being one of them. Such a separation provides opportunities in terms of traffic engineering. In this paper, we propose an open and flexible solution that allows an ISP using identifier/locator separation to engineer its interdomain traffic. Our solution relies on the utilization of a service that transparently ranks paths using cost functions. We implement a prototype server and demonstrate its benefits in a LISP testbed. I...|$|R
40|$|An Identifier / <b>Locator</b> <b>addressing</b> scheme can {{enable a}} new {{approach}} to mobile hosts and mobile networks. Identifier and Locator information is stored in Domain Name System (DNS) Resource Records (RRs). In our on-going work using the Identifier – Locator Network Protocol (ILNP), the DNS would be updated with new Locator values as hosts and/or networks move: new sessions would obtain the correct Locator(s) for a mobile host and/or network from the DNS, in much the same was as currently happens for IP address resolution. However, this use of the DNS is not currently required for mobility using IP. We examine the potential impact on DNS from using a naming approach to mobility...|$|R
40|$|The Routing Research Group (RRG) of the Internet Research Task Force (IRTF) is {{currently}} discussing several architectural solutions {{to build an}} interdomain routing architecture that scales better than the existing one. The solutions family currently being discussed concerns the <b>addresses</b> separation into <b>locators</b> and identifiers, LISP being one of them. Such a separation provides opportunities in terms of traffic engineering. In this paper, we propose an open and flexible solution that allows an ISP using identifier/locator separation to engineer its interdomain traffic. Our solution relies on the utilization of a service that transparently ranks paths using cost functions. We implement a prototype server and demonstrate its benefits in a LISP testbed. Anglai...|$|R
40|$|This paper {{proposes a}} new {{definition}} on names, <b>addresses,</b> identifiers, and <b>locators</b> {{based on a}} different framework. Starting with observation of the patterns how those terms are used, a new definition for the terms and their relations are presented. First, name and address are defined based on assignment, where a name denotes an entity itself and an address denotes a point to which the entity is attached. On the other hand identifier and locator are defined based on their use, where identifier is used for identifying an entity from others without ambiguity and locator is used for locating an entity within a given space. Next, the relationship among those four terms is presented. Finally, we show how communication is performed {{with respect to the}} new definition...|$|R
40|$|The Shim 6 {{architecture}} enables IPv 6 multihoming {{without compromising}} the scalability {{of the global}} routing system by using provider aggregatable addresses. To do so, hosts use different <b>addresses</b> as <b>locators</b> for data packet transmission, but present the same source and destination identifier pair to transport and upper layers. The components of this architecture are the Shim 6 entity, which maps and translates upper-layer identifiers and locators for remote hosts; the Shim 6 protocol, which exchanges mapping information between two hosts that communicate; and the REAP protocol, which monitors the existing unidirectional paths and finds new valid locator combinations in case of failure. To protect against new vulnerabilities this architecture may introduce compared to IPv 6, Shim 6 hosts use either cryptographically generated addresses or hash-based addresses. European Community's Seventh Framework ProgramThe work of Alberto García-Martínez {{is supported by the}} T 2 C 2 project (TIN 2008 - 06739 - C 04 - 01), funded by the Spanish Ministerio de Ciencia e Innovación. This research was supported by Trilogy ([URL] a research project (ICT- 216372) partially funded by the European Community under its Seventh Framework Programme. European Community's Seventh Framework Program This work was partly funded by POSDRU/ 89 / 1. 5 /S/ 62557 Publicad...|$|R
2500|$|In September 2007, AT {{changed its}} legal policy {{to state that}} [...] "AT may {{immediately}} terminate or suspend all or a portion of your Service, any Member ID, electronic mail address, IP <b>address,</b> Universal Resource <b>Locator</b> or domain name used by you, without notice for conduct that AT believes ... (c) tends to damage the name or reputation of AT, or its parents, affiliates and subsidiaries." [...] By October 10, 2007, AT had altered the terms and conditions for its Internet service to explicitly support freedom of expression by its subscribers, after an outcry claiming the company had given itself the right to censor its subscribers' transmissions. Section 5.1 of AT's new terms of service now reads [...] "AT respects freedom of expression and believes it is a foundation of our free society to express differing points of view. AT will not terminate, disconnect or suspend service because of the views you or we express on public policy matters, political issues or political campaigns." ...|$|R
40|$|A {{prototype}} {{course on}} biocomputing was delivered via international computer networks {{in early summer}} 1995. The course lasted 11 weeks, and was o ered free of charge. It was organized by the BioComputing Division of the Virtual School of Natural Sciences, which isamember school of the Globewide Network Academy. It brought together 34 students and 7 instructors {{from all over the}} world, and covered the basics of sequence analysis. Five authors from Germany and USA prepared ahypertext book which was discussed in weekly study sessions that took place in a virtual classroom at the BioMOO electronic conferencing system. The course aimed at students with backgrounds in molecular biology, biomedicine or computer science, complementing and extending their skills with an interdisciplinary curriculum. Special emphasis was placed on the use of Internet resources, and the development of new teaching tools. The hypertext book includes direct links to sequence analysis and databank search services on the Internet. A tool for the interactivevisualization of unit-cost pairwise sequence alignmentwas developed for the course. All course material will stay accessible at the World Wide Web <b>address</b> (Uniform Resource <b>Locator...</b>|$|R
5000|$|The Locator/Identifier Separation Protocol (LISP or Loc/ID split) [...] {{has been}} {{proposed}} by IETF {{as a solution to}} issues as the scalability of the routing system. LISP main argument is that the semantics of the IP address are overloaded being to be both locator and identiﬁer of an endpoint. LISP proposes to address this issue by separating the IP <b>address</b> into a <b>locator</b> part, which is hierarchical, and an identifier, which is flat. However this is a false distinction: in Computer Science it is impossible to locate something without identifying it and to identify something without locating it, since all the names are using for locating an object within a context. Moreover, LISP continues to use the locator for routing, therefore routes are computed between locators (inter- faces). However, a path does not end on a locator but on an identifier, in other words, the locator is not the ultimate destination of the packet but a point on the path to the ultimate destination. This issue leads to path-discovery problems, as documented by [...] whose solution is known not to scale.|$|R
40|$|The {{characteristics}} of Latin American network infrastructures have global consequences, {{particularly in the}} area of interdomain traffic engineering. As an example, Latin America shows the largest de-aggregation factor of IP prefixes among all regional Internet registries, being proportionally the largest contributor to the growth and dynamics of the global BGP routing table. In this article we analyze the peculiarities of LA interdomain routing architecture, and provide up-to-date data about the combined effects of the multihoming and TE practices in the region. We observe that the Internet Research Task Force initiative on the separation of the <b>address</b> space into <b>locators</b> and identifiers can not only alleviate the growth and dynamics of the global routing table, but can also offer appealing TE opportunities for LA. We outline one of the solutions under discussion at the IRTF, the Locator/Identifier Separation Protocol, and examine its potential in terms of interdomain traffic management in the context of LA. The key advantage of LISP is its nondisruptive nature, but the existing proposals for its control plane have some problems that may hinder its possible deployment. In light of this, we introduce a promising control plane for LISP that can solve these issues, {{and at the same time}} has the potential to bridge the gap between intradomain and interdomain traffic management. Peer ReviewedPostprint (published version...|$|R
40|$|Scholars are {{increasingly}} citing electronic ?web references? {{which are not}} preserved in libraries or full text archives. WebCite is a new standard for citing web references. To ?webcite? a document involves archiving the cited Web page through www. webcitation. org and citing the WebCite permalink instead of (or in addition to) the unstable live Web page. Almost 200 journals are already using the system. We discuss the rationale for WebCite, its technology, and how scholars, editors, and publishers can benefit from the service. Citing scholars initiate an archiving process of all cited Web references, ideally before they submit a manuscript. Authors of online documents and websites which {{are expected to be}} cited by others can ensure that their work is permanently available by creating an archived copy using WebCite and providing the citation information including the WebCite link on their Web document(s). Editors should ask their authors to cache all cited Web <b>addresses</b> (Uniform Resource <b>Locators,</b> or URLs) ?prospectively? before submitting their manuscripts to their journal. Editors and publishers should also instruct their copyeditors to cache cited Web material if the author has not done so already. Finally, WebCite can process publisher submitted ?citing articles? (submitted for example as eXtensible Markup Language [XML] documents) to automatically archive all cited Web pages shortly before or on publication. Finally, WebCite can act as a focussed crawler, caching retrospectively references of already published articles. Copyright issues are addressed by honouring respective Internet standards (robot exclusion files, no-cache and no-archive tags). Long-term preservation is ensured by agreements with libraries and digital preservation organizations. The resulting WebCite Index may also have applications for research assessment exercises, being able to measure the impact of Web services and published Web documents through access and Web citation metrics...|$|R
40|$|Ophthalmology and its adnexal {{disciplines}} are in {{the grip}} of an explosion in information technology (IT), as are most branches of medicine. Not only do we have a bewil-dering number of published (on paper that is) periodicals, textbooks, and society (or faculty or college) newsletters to sift, select, and assimilate information from, but we now also have an increasing array of electronic sources of information. Many peer reviewed journals (the BJO included) are now represented on the internet in world wide web (“web”) sites. 1 Some of these contain full text and graphics articles whereas others oVer samples of their content, or tables of contents. Inevitably, they almost always provide subscription information. Most usefully, the web sites usu-ally provide the current instructions for authors. The inter-net <b>addresses</b> (uniform resource <b>locators,</b> URLs) of some ophthalmology journals are provided in Table 1. In addition to formal “publications”, there are many web sites which contain information that has not undergone peer review. Some of these web sites oVer infor-mation about well established societies, faculties, or colleges. Examples of these are provided in Table 2. Others are sites of specific hospitals, laboratories, and even individual practitioners. These latter web sites are often advertisements for the services of the organisation or indi-vidual who runs the site and, as such, are publicity ventures and not necessarily rich or reliable sources of information. Web sites which are advertisements and publicity ventures for biomedical publishers, manufacturers, and traders also exist in abundance. The true nature of a web site is often not apparent until time has been expended, or wasted, in finding it and reading it. A further type of site that can be very useful also exists. This type of site has collected information from many dis-parate sources, presents them in a digestible format, and provides links to more information {{as a starting point for}} the inquisitive. Web sites of this nature are found within the internet presence of some of the organisations listed in Table 2. Of these, the web site of the American Association of Ophthalmology is particularly useful in its “eye care links ” section (URL...|$|R
2500|$|However, in 2007 the U.S. District Court {{struck down}} even the reauthorized NSLs because the gag power was unconstitutional as courts could still {{not engage in}} {{meaningful}} judicial review of these gags. On August 28, 2015, Judge Victor Marrero of the federal district court in Manhattan ruled the gag order of Nicholas Merrill was unjustified. In his decision, Judge Marrero described the FBI's position as, [...] "extreme and overly broad," [...] affirming that [...] "courts consistent with the First Amendment, simple accept the Government's assertions that disclosure would implicate and create a risk." [...] He {{also found that the}} FBI's gag order on Mr. Merrill [...] "implicates serious issues, both with respect to the First Amendment and accountability of the government to the people." [...] Initially, the ruling was released in redaction by Judge Marrero. The FBI was given 90 days to pursue any other alternative course of action, but elected not to do so. Upon release of the unredacted ruling on November 30, 2015, it was revealed {{for the first time the}} extent to which the FBI's NSL accompanied by a gag order sought to collect information. Through the court documents, it was revealed for the first time that through an NSL, the FBI believes it can legally obtain information including an individual's complete web browsing history, the IP addresses of everyone a person has corresponded with, and all the records of all online purchases within the last 180 days. The FBI also claims via the extension of a NSL, it can obtain cell site location information. In the landmark case of Nicholas Merrill the FBI in specific sought to seek the following information on an account: DSL account information, radius log, subscriber name and related subscriber information, account number, date the account opened or closed, addresses associated with the account, subscriber day/evening telephone numbers, screen names or other on-line names associated with the account, order forms, records relating to merchandise orders/shipping information for the last 180 days, all billing related to the account, internet service provider (ISP), all email addresses associated with the account, internet protocol address assigned to the account, all website information registered to the account, uniform resource <b>locator</b> <b>address</b> assigned to the account, any other information which you consider to be an electronic communication transactional record. This was the first time it was revealed the extent to which an NSL under the Patriot Act could request communication information.|$|R
5000|$|One of {{the most}} {{controversial}} aspects of the USA PATRIOT Act is in title V, and relates to National Security Letters (NSLs). An NSL is a form of administrative subpoena used by the FBI, and reportedly by other U.S. government agencies including the CIA and the Department of Defense (DoD). It is a demand letter issued to a particular entity or organization to turn over various records and data pertaining to individuals. They require no probable cause or judicial oversight and also contain a gag order, preventing the recipient of the letter from disclosing that the letter was ever issued. Title V allowed the use of NSLs to be made by a Special Agent in charge of a Bureau field office, where previously only the Director or the Deputy Assistant Director of the FBI were able to certify such requests.This provision of the Act was challenged by the ACLU on behalf of an unknown party against the U.S. government on the grounds that NSLs violate the First and Fourth Amendments of the U.S. Constitution because {{there is no way to}} legally oppose an NSL subpoena in court, and that it was unconstitutional not to allow a client to inform their Attorney as to the order because of the gag provision of the letters. The court's judgement found in favour of the ACLU's case, and they declared the law unconstitutional.Later, the USA PATRIOT Act was reauthorized and amendments were made to specify a process of judicial review of NSLs and to allow the recipient of an NSL to disclose receipt of the letter to an attorney or others necessary to comply with or challenge the order.However, in 2007 the U.S. District Court struck down even the reauthorized NSLs because the gag power was unconstitutional as courts could still not engage in meaningful judicial review of these gags. On August 28, 2015, Judge Victor Marrero of the federal district court in Manhattan ruled the gag order of Nicholas Merrill was unjustified. In his decision, Judge Marrero described the FBI's position as, [...] "extreme and overly broad," [...] affirming that [...] "courts consistent with the First Amendment, simple accept the Government's assertions that disclosure would implicate and create a risk." [...] He also found that the FBI's gag order on Mr. Merrill [...] "implicates serious issues, both with respect to the First Amendment and accountability of the government to the people." [...] Initially, the ruling was released in redaction by Judge Marrero. The FBI was given 90 days to pursue any other alternative course of action, but elected not to do so. Upon release of the unredacted ruling on November 30, 2015, it was revealed for the first time the extent to which the FBI's NSL accompanied by a gag order sought to collect information. Through the court documents, it was revealed for the first time that through an NSL, the FBI believes it can legally obtain information including an individual's complete web browsing history, the IP addresses of everyone a person has corresponded with, and all the records of all online purchases within the last 180 days. The FBI also claims via the extension of a NSL, it can obtain cell site location information. In the landmark case of Nicholas Merrill the FBI in specific sought to seek the following information on an account: DSL account information, radius log, subscriber name and related subscriber information, account number, date the account opened or closed, addresses associated with the account, subscriber day/evening telephone numbers, screen names or other on-line names associated with the account, order forms, records relating to merchandise orders/shipping information for the last 180 days, all billing related to the account, internet service provider (ISP), all email addresses associated with the account, internet protocol address assigned to the account, all website information registered to the account, uniform resource <b>locator</b> <b>address</b> assigned to the account, any other information which you consider to be an electronic communication transactional record. This was the first time it was revealed the extent to which an NSL under the Patriot Act could request communication information.|$|R

