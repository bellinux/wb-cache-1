6|3014|Public
40|$|Universities are {{increasingly}} offering courses online. Feedback, assessment, and guidance are important {{features of this}} online courseware. Together, {{in the absence of}} a human tutor, they aid the student in the learning process. We present a programming training environment for a database course. It aims to offer a substitute for classroom based learning by providing synchronous automated feedback to the student, along with guidance based on a personalized assessment. The <b>automated</b> <b>tutoring</b> <b>system</b> should promote procedural knowledge acquisition and skills training. An automated tutoring feature is an integral part of this tutoring system...|$|E
40|$|This study {{investigated}} the transitions between affective states (i. e., boredom, flow, confusion, frustration, delight, and surprise) during learning while college students were tutored in computer literacy by AutoTutor, an <b>automated</b> <b>tutoring</b> <b>system</b> with natural language dialogue. Videos of participants ’ faces and the interaction histories were recorded and then played back for the participants to judge their own affective states. We developed a metric to measure the relative likelihood of transitioning from an affective state at time ti to a subsequent affective state at time ti+ 1. Several significant trajectories between affective states were identified. Instructional implications {{are discussed in the}} context of an expanded version of a cognitive disequilibrium model...|$|E
40|$|The need {{to enhance}} {{information}} literacy education increases demand for effective Web-based learning environments for information retrieval instruction. The paper introduces the Query Performance Analyser, a unique instructional tool for information retrieval learning environments. On {{top of an}} information retrieval system and within a given search assignment, the Query Performance Analyser supports learning by instantly visualizing achieved query performance. Although the Query Performance Analyser is a useful tool in training searching skills, performance feedback {{is not enough for}} learners practicing alone in Web-based learning environments. The paper reports the findings of a log analysis on user problems in exercising Boolean and best-match queries. A blueprint of an <b>automated</b> <b>tutoring</b> <b>system</b> for IR instruction is presented...|$|E
40|$|Student {{modeling}} {{is one of}} the {{key factors}} that affects <b>automated</b> <b>tutoring</b> <b>systems</b> in making instructional decisions. A student model is a model to predict the probability of a student making errors on given problems. A good student model that matches with student behavior patterns often provides useful information on learning task difficulty and transfer of learning between related problems, and thus often yields better instruction. Manual construction of such models usually requires substantial human effort, and may still miss distinctions in content and learning that have important instructional implications. In this paper, we propose an approach that automatically discovers student models using a state-of-art machine learning agent, SimStudent. We show that the discovered model is of higher quality than human-generated models, and demonstrate how the discovered model can be used to improve a <b>tutoring</b> <b>system’s</b> instruction strategy. 1...|$|R
25|$|Also in 2014, LSI {{developed}} an <b>automated</b> <b>tutoring</b> referral <b>system</b> called Refer-Tutor-Report™ (RTR™). RTR accommodates four activities: teachers refer chosen students to tutoring for specified learning objectives, students receive notification of referrals {{and learn how}} to reach tutors, tutors find out what learning objectives students need to cover, and administrators garner reports about the tutoring referral process.|$|R
40|$|One-on-one {{tutoring}} {{is significantly}} {{more effective than}} traditional classroom instruction. In recent years, <b>automated</b> <b>tutoring</b> <b>systems</b> are approaching that level of effectiveness by engaging students in rich natural language dialogue that contributes to learning. A promising approach for further improving the effectiveness of tutorial dialogue systems is to model the differential effectiveness of tutorial strategies, identifying which dialogue moves or combinations of dialogue moves are associated with learning. It {{is also important to}} model the ways in which experienced tutors adapt to learner characteristics. This paper takes a corpus-based approach to these modeling tasks, presenting the results of a study in which task-oriented, textual tutorial dialogue was collected from remote one-on-one human tutoring sessions. The data reveal patterns of dialogue moves that are correlated with learning, and can directly inform the design of student-adaptive tutorial dialogue management systems. 1...|$|R
40|$|International audienceThe {{objective}} of any tutoring {{system is to}} provide a meaningful learning to the learner. Therefore an <b>automated</b> <b>tutoring</b> <b>system</b> should be able to know whether a concept mentioned in a document is a prerequisite for studying that document, or it can be learned from it. This paper addresses the problem of identifying defined concepts and prerequisite concepts from learning resources in html format. In this paper a supervised machine learning approach was taken to address the problem, based on linguistic features which enclose contextual information and stylistic features such as font size and font weight. This paper shows that contextual information in addition to format information can give better results when used with the SVM classifier than with the (LP) 2 algorithm...|$|E
40|$|AutoTutor is a fully <b>automated</b> <b>tutoring</b> <b>system</b> that {{attempts}} to comprehend learner contributions and formulate appropriate dialogue moves. This paper reports the mechanisms and performance of one of AutoTutor’s language modules, the word tagging module. AutoTutor’s word tagging module determines the part of speech tag for every word in the learner’s contributions. It uses a two part procedure: it first consults a lexicon to identify the set of possible tags for each word, then it uses a neural network to select a single tag for each word. Performance assessments were made on a corpus of oral tutorial dialogue, as opposed to well-formed printed text. The lexicon provided the correct tag, as one member of a set, for 97 % of the words and 91. 6 % of the neural network’s first-choice tags matched assignments by humans...|$|E
40|$|Programmed instruction, learn units, rule-governed performance, {{technology}} education. At {{the beginning}} of a Java computer programming course, nine students in an undergraduate class and nine students in a graduate class completed a web-based programmed instruction tutoring system that taught a simple computer program. All students exited the tutor with an identical level of skill, at least as determined by the tutor’s required terminal performance, which involved writing the program and passing multiple-choice tests on the program’s elements. Before entering and after exiting the tutor, students completed a test of rule-based performance that required applications of general programming principles to solve novel problems. In both classes, the number of correct rule answers observed before entering the tutor did not predict the number of learn units that students subsequently used to complete the tutor. However, the frequency of learn units was inversely related to post-tutor rule-test performance, i. e., as the number of learn units used in the tutor increased over students, the number of correct answers on the post-tutor rule test decreased. Since time to complete the tutor was unrelated to learn unit frequency, these data suggest that high achieving students may have generated autoclitic learn units while using the tutor. Interteaching, as an occasion for generating and sharing interlocking learn units, may be an effective complement to programmed instruction in promoting optimal learning in all students. The Behavior Analyst Today Volume 8, Issue 1, 2007 Citation: Emurian, H. H. (2007). Programmed instruction for teaching Java: Consideration of learn unit frequency and rule-test Performance. The Behavior Analyst Today, 8 (1), 70 - 88. The Behavior Analyst Today, © (2007) Joseph D. Cautilli, Publisher, The Behavior Analyst Today. URL: [URL] Programmed Instruction for Teaching Java: Consideration of Learn Unit Frequency and Rule-Test Performance 1 Henry H. Emurian Abstract At {{the beginning of}} a Java computer programming course, nine students in an undergraduate class and nine students in a graduate class completed a web-based programmed instruction tutoring system that taught a simple computer program. All students exited the tutor with an identical level of skill, at least as determined by the tutor’s required terminal performance, which involved writing the program and passing multiple-choice tests on the program’s elements. Before entering and after exiting the tutor, students completed a test of rule-based performance that required applications of general programming principles to solve novel problems. In both classes, the number of correct rule answers observed before entering the tutor did not predict the number of learn units that students subsequently used to complete the tutor. However, the frequency of learn units was inversely related to post-tutor rule-test performance, i. e., as the number of learn units used in the tutor increased over students, the number of correct answers on the post-tutor rule test decreased. Since time to complete the tutor was unrelated to learn unit frequency, these data suggest that high achieving students may have generated autoclitic learn units while using the tutor. Interteaching, as an occasion for generating and sharing interlocking learn units, may be an effective complement to programmed instruction in promoting optimal learning in all students. Keywords: Programmed instruction, learn units, rule-governed performance, technology education. 1 The Behavior Analyst Today Volume 8, Issue 1, 2007 Programmed Instruction for Teaching Java: Consideration of Learn Unit Frequency and Rule-Test Performance Programmed instruction (PI) is an effective tool to teach students in information systems to write and to understand a Java computer program as a first technical exercise in a computer programming course 2. A web-based PI tutoring system to accomplish that objective was presented in Emurian, Hu, Wang, and Durham (2000), and behavior principles supporting the design and implementation of the system were described by Emurian, Wang, and Durham (2003) and Emurian and Durham (2003). Since the initial prototype tutoring system was developed, the implementation and assessments have evolved over several updates, ranging from analyses of learning performance (Emurian, 2004) to considerations {{of the extent to which}} skill gained by completing the tutor fostered generalizable or rule-governed performances (Emurian, 2005, 2006 a). More recently, interteaching (Boyce & Hineline, 2002; Saville, Zinn, Neef, Norman, & Ferreri, 2006) was shown to support and confirm the students’ knowledge acquired initially from using the programmed instruction tutor (Emurian, 2006 b, in press). An obviously important consideration in the design of a programmed instruction tutoring system is the extent to which the knowledge or skill gained is generalizable to solve problems not explicitly taught or encountered with the tutor itself. Behavior analysts working in the areas of programmed instruction (e. g., Tudor & Bostow, 1991) and instructional design (e. g., Greer, 2002) recognize the importance of this issue. In a traditional formulation of learning, this is a transfer of training problem (Barnett & Ceci, 2002), and in behavior analysis, this is a rule-governed problem (Hayes, 1989). When learners apply knowledge successfully to novel situations, they are said to be demonstrating meaningful learning (Mayer, 2002). Generalizable rules, which may be the essence of meaningful learning, can be acquired by direct instruction and rehearsal or by induction, when many different situations are encountered that occasion the general rule (Kudadjie-Gyamfi & Rachlin, 2002). That recognition implies an instructional design to overcome the empirically verified shortcomings of teaching tactics that provide minimal guidance during a student’s learning experiences (Kirschner, Sweller, & Clark, 2006). The former tactic is consistent with our instructional system design. A tacit assumption in the design of programmed instruction is that all learners who exit the system do so having achieved equivalent skill. A framework for interpreting individual differences in the learning process is given by the learn unit formulation of Greer and McDonough (1999). A learn unit is defined as “a countable unit of teacher and student interaction that leads to important changes in student behavior” (Greer & McDonough, 1999, p. 6). In the <b>automated</b> <b>tutoring</b> <b>system</b> under consideration, each successive component, or learn unit, within eight tutor stages, to be described below, required accurate responding for the learner to transition from one component to the next. The occasion and events supporting such a transition constitute a natural fracture of instruction (Greer, 2002, p. 18). The importance of this framework is to be understood in terms of its assumed predictability of long-term outcomes. Although our previous work showed increases in the number of correct rule-test answers following completion of a PI tutoring system (Emurian, 2005; 2006 a, b), in comparison to pre-tutor answers, no attempt was made to relate such tests of rule-governed performance to the frequency of learn units that students encountered until successfully completing the tutor. If generalizable knowledge is equivalent and if individual differences have been overcome by repetition of learn units until mastery has occurred, there should be no differences in rule-governed performances among students upon completion of the tutoring The Behavior Analyst Today Volume 8, Issue 1, 2007 system, and the frequency of learn units should be unrelated to subsequent performance. This paper, then, presents an evaluation of learn unit frequency in relationship to students’ rule-test performance observed (1) prior to using the programmed instruction tutoring system (baseline) and (2) after successfully completing the tutoring system. METHOD Subjects Subjects were students in two classes of a course entitled Graphical User Interface Systems Using Java. The first class was for undergraduate students, and it met in the Fall semester of 2005. The class met twice each week for 75 min over a 14 -week semester. The second class was for graduate students, and it met in the Summer of 2006. This class met three times each week for three hrs over a five-week semester. The technical course content was equivalent between the classes, but the graduate students were required to write more journal article reviews than the undergraduate students. The prerequisite for both classes was at least one prior programming course, and the course design was intended to provide instruction to information systems majors. The primary professional interests of those students are in the areas of systems analysis and technical management, in contrast to computer programming and algorithms. For the Fall 2005 undergraduate class, nine of the 14 students produced usable records of tutor performance. There were six male and three female students (median age = 23 yrs, range = 19 to 28 yrs; median number of programming courses taken = 3, range = 2 to 5 courses). For the Summer 2006 graduate class, nine of the 13 students produced usable records of tutor performance. There were six male and three female students (median age = 26 yrs, range = 23 to 33 yrs; median number of programming courses taken = 3, range = 1 to 15 courses). Data were automatically recorded on a server at the transition points in the tutor, and occasional technical problems with the server would prevent all records from being recorded for a particular student. That accounts for the number of data records being fewer than the number of students in a class. Material At the first meeting for both classes, students completed a pre-tutor questionnaire (baseline) that included rule-based questions. Appendix A presents the 14 rule questions administered to the Fall 2005 students, and Appendix B presents the 12 rule questions administered to the Spring 2006 students. The questionnaire changed over classes in relationship to updates to the Java tutor. Other questionnaires were also administered, to include demographic items and software self-efficacy (see Emurian, 2005). Following completion of the Java tutor, students in both classes repeated the rule-based questions in a post-tutor assessment. The programmed instruction tutoring system taught a simple Java applet that would display a text string, as a JLabel object, within a browser on the World Wide Web 3. Table 1 presents the Java code for the tutor version presented to the Fall 2005 students. The program was arbitrarily organized into ten lines of code and 34 items of code. Each item occupies a cell in the table, and PI learn units were based upon cells and lines. Table 2 presents the Java code for the tutor version presented to the Summer 2006 students. The program was arbitrarily organized into 11 lines of code and 37 items of code. Each item occupies a cell in the table, and PI learn units were based upon cells and lines. The symbols in the tables may be cryptic on initial 3 The Behavior Analyst Today Volume 8, Issue 1, 2007 encounter, but the objective of the tutor was to teach students to understand and use the symbols in the tables, even if the student had never before written a Java computer program. The tutor and the open source code are freely available on the web, as referenced above. Running the tutor is the best way to understand its components and operation. In brief, the web-based Java tutor consists of the following eight stages: (1) introduction and example of the program running in a browser (learn units = 1), (2) learning to copy an item of code (learn units = 34 or 37), (3) learning to discriminate an item of code in a list (learn units = 34 or 37), (4) learning the semantics of an item of code (learn units = 34 or 37) and learning the syntax by typing the item by recall (learn units = 34 or 37), (5) learning to copy a line of code (learn units = 10 or 11), (6) learning to discriminate a line of code in a list (learn units = 10 or 11), (7) learning the semantics of a line of code (learn units = 10 or 11) and learning the syntax by typing the line by recall (learn units = 10 or 11), and (8) writing the entire program by recall (learn units = 1). Thus, for the Fall 2005 class, the minimum number of learn units to complete the tutor was 178; for the Summer 2006 class, the minimum number of learn units was 194. The multiple-choice 4 The Behavior Analyst Today Volume 8, Issue 1, 2007 tests for items and lines of code, which are embedded in the tutor, had five answer choices. For an incorrect items answer, there was a 5 -sec delay or “time-out” in the tutor’s interaction with the learner. For a correct items answer, a confirmation window appeared stating a general rule associated with the correct answer or an elaboration of the explanation of the meaning of the item. For both tutor versions, the lines stage had no delay interval or confirmation window. Experience suggested that most students in our courses could complete the tutor within two to three hrs. The tutor transitioned automatically between stages, and students were able to take breaks between and within stages. The instructions, however, encouraged students to complete each stage before taking a break. At the completion of each tutor stage, a record of performance, which included stage duration and performance errors, was transmitted to a database on the server. The only identifier was the student’s email username, similar to what appears in any email transmission on the Internet. Students were informed of this record’s transmission, and the instructional protocol was exempt from informed consent. Network interruptions sometimes prevented a record from being written, and this paper includes only those students who had records for all eight stages in the tutor. Procedure For the Fall 2005 class, the first meeting provided orientation to the course. The pre-tutor questionnaire was administered and collected. As homework, students were instructed to complete the Java tutor before the next class meeting. At the completion of the tutor, students downloaded the post-tutor questionnaire from the course Blackboard site, completed it electronically, and returned it to the instructor by email attachment. For the Summer 2006 class, the procedure was similar, but it was the expectation that all students could complete the tutor within the three-hr class period. Thus, the post-tutor questionnaire was administered and collected as students completed the tutor in class. Only one student was not able to complete the tutor during class, and that student was not included in the data to be presented here because the records were not obtained. RESULTS Figure 1 presents scatterplots of total learn units and total correct rule-test answers for the pre-tutor baseline and post-tutor assessments for each of the students in the Fall 2005 class. Regression lines and Pearson correlation parameters are presented on the figure. The figure shows graphically that correct rule answers increased from pre-tutor baseline to post-tutor assessment for all nine students. The figure also shows that pre-tutor correct rule answers were not demonstrably related to subsequent learn unit frequency, and the test of the relationship failed to provide evidence of an orderly relationship. In contrast, the figure shows graphically that total learn units encountered during the tutor were inversely related to subsequent post-tutor correct rule answers, and the test of the relationship was significant. Students who required comparatively fewer learn units during the tutor achieved a higher number of post-tutor correct rule answers in comparison to students who required comparatively more learn units to complete the tutor. Figure 2 presents scatterplots of total learn units and total correct rule-test answers for the pre-tutor baseline and post-tutor assessments for each of the students in the Summer 2006 5 The Behavior Analyst Today Volume 8, Issue 1, 2007 class. Regression lines and Pearson correlation parameters are presented on the figure. The figure shows graphically that correct rule answers increased from pre-tutor baseline to post-tutor assessment for eight of the nine students. The exception was a student who answered 11 correct rule questions on both occasions. The figure also shows that pre-tutor correct rule answers were not demonstrably related to subsequent learn unit frequency, and the test of the relationship failed to provide evidence of an orderly relationship. In contrast, the figure shows graphically that total learn units encountered during the tutor were inversely related to subsequent post-tutor correct Figure 1. Scatterplots of learn units and correct rule answers for pre-tutor and post-tutor assessments for nine students in the Fall 2005 class. Also presented are regression lines and R 2 values for each line. 6 The Behavior Analyst Today Volume 8, Issue 1, 2007 Figure 2. Scatterplots of learn units and correct rule answers for pre-tutor and post-tutor assessments for nine students in the Summer 2006 class. Also presented are regression lines and R 2 values for each line. rule answers, and the test of the relationship was significant. Students who required comparatively fewer learn units during the tutor achieved a higher number of post-tutor correct rule answers in comparison to students who required comparatively more learn units to complete the tutor. Figure 3 presents scatterplots of total learn units to complete the tutor and total time to complete the tutor for each of the students in the Fall 2005 class. The shortest time to complete the tutor was 60. 5 min, and the longest time was 543. 2 min. A Pearson correlation did not support a relationship between time and total learn units. 7 The Behavior Analyst Today Volume 8, Issue 1, 2007 8 Figure 3. Scatterplots of total time required to complete the tutor and total learn units required to complete the tutor for the nine students in the Fall 2005 class. Figure 4 presents scatterplots of total learn units to complete the tutor and total time to complete the tutor for each of the students in the Summer 2006 class. The shortest time to complete the tutor was 60. 0 min, and the longest time was 143. 1 min. A Pearson correlation did not support a relationship between time and total learn units. DISCUSSION The relationship between learn unit frequency and rule-test performance was orderly for students over two different classes. As the number of learn units to complete the tutor increased over students, the number of correct answers produced on the rule test decreased. However, the data did not support a similar relationship between baseline rule-test performance and subsequent learn unit frequency or between time in the tutor and learn unit frequency to complete the tutor. These effects were observed for undergraduate and graduate students, for two versions of the Java program, for two versions of the rule test, and for two ways to complete the tutor: (1) homework, which favored distributed learning, and (2) class work, which supported massed learning. The similarity in outcomes shows the reliability of the behavioral processes under conditions of systematic replication (Sidman, 1960), thereby demonstrating the generality of the findings. The Behavior Analyst Today Volume 8, Issue 1, 2007 Figure 4. Scatterplots of total time required to complete the tutor and total learn units required to complete the tutor for the nine students in the Summer 2006 class. All students in the Fall 2005 class and eight of the nine students in the Summer 2006 class showed improvement in rule-test performance between the pre-tutor baseline and the post-tutor assessment. However, the knowledge transfer from the tutor to the rule test (i. e., “far transfer”) differed across students, despite the fact that all students exited the tutor with equivalent competency, at least as operationalized by tutor performance. In fact, only one student (Summer 2006) showed perfect performance on the post-tutor assessment. The orderliness observed, then, was to predict post-tutor rule-test performance by the number of learn units required to complete the tutor. Surprisingly, however, the evidence was insufficient to show a similar orderly relationship between pre-tutor baseline rule-test performance and the number of learn units later required to complete the tutor. This outcome is consistent with the observation that frequent testing and test feedback that is immediate and corrective, such as occurred within the Java tutor, may sometimes hinder, rather than help, knowledge acquisition by giving learners an overly optimistic opinion of their competency (Mathan & Koedinger, 2005). Although students may sometimes prefer item-by-item feedback (e. g., Buzhardt & Semb, 2002), it is also acknowledged that multiple-choice recognition tests may assess only low-level learning processes and may lead to overconfidence in long-term retention (Halpern & Hakel, 2003). The observation that total correct rule answers on the post-tutor assessment were generally lower for students who used a relatively greater number of learn units suggests that equivalent “knowledge” was not achieved by all students at the completion of the tutor. When multiple-choice tests are used to assess learning, as was the case in the present tutor, the opportunity for repetition may not always occasion the “studying behavior” essential to apply a principle to the selection of the correct alternative on a test. Although the term “learn unit” was applied to the tutor components, the failure of transfer...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. Over {{the past twenty}} years, <b>automated</b> <b>tutoring</b> <b>systems</b> have gained an increasing recognition as a prominent area of Artificial Intelligence (AI). During this period, Intelligent Computer Aided Instruction (ICAI) systems have been developed {{using a variety of}} AI techniques to enhance the learning process. The core AI issue in designing these systems concerns knowledge representation. A review of current AI literature shows that there are numerous, distinctly different knowledge representation schemes, and most conventional programming environments do not readily support all of these representation schemes. This thesis proposes that <b>tutoring</b> <b>systems</b> are best designed in a programming environment that supports multiple, integrated knowledge representation schemes. Such an environment allows the designer to select and employ, with ease, the most natural knowledge representation scheme for each type of knowledge in the <b>tutoring</b> <b>system.</b> In this thesis we describe the components of a generalized ICAI system; discuss the various types of knowledge and knowledge representation schemes; and review the knowledge representation schemes used in several noted ICAI systems. In addition, we describe two prototype ICAI <b>systems</b> (Map Reading <b>Tutor</b> and Pilot Emergency Procedure Tutor) which we designed and developed in a specific programming environment that supports multiple, integrated knowledge representation schemes. Captain, United States Arm...|$|R
40|$|One of the {{key factors}} that affects <b>automated</b> <b>tutoring</b> <b>systems</b> in making {{instructional}} decisions {{is the quality of}} the student model built in the system. A student model is a model that can solve problems in various ways as human students. A good student model that matches with student behavior patterns often provides useful information on learning task difficulty and transfer of learning between related problems, and thus often yields better instruction on intelligent <b>tutoring</b> <b>systems.</b> However, traditional ways of constructing such models are often time consuming, and may still miss distinctions in content and learning that have important instructional implications. Automated methods can be used to find better student models, but usually require some engineering effort, and can be hard to interpret. In this paper, we propose an automated approach that finds student models using a clustering algorithm based on automaticallygenerated problem content features. We demonstrate the proposed approach using an algebra dataset. Experimental results show that the discovered model is as good {{as one of the best}} existing models, which is a model found by a previous automated approach, but without the knowledge engineering effort...|$|R
40|$|Although {{it shows}} {{enormous}} {{potential as a}} feature extractor, 2 D principal component analysis (2 DPCA) produces numerous coefficients. Using a feature-selection algorithm based on a multiobjective genetic algorithm to analyze and discard irrelevant coefficients offers a solution that considerably reduces the number of coefficients, while also improving recognition rates. 1521 - 9615 / 11 /$ 26. 00 © 2011 IEEE Copublished by the IEEE CS and the AIP Face recognition and facial-expression recognition have been active research fields for several years. Potential application areas include access control, searching mug shots, screening, security monitoring and surveillance systems, human-computer interaction, emotion analysis, and <b>automated</b> <b>tutoring</b> <b>systems.</b> Both face and facial-expression recognition continue to attract researchers from image processing, pattern recognition, machine learning, and computer vision. 1 Several {{attempts have been made}} to improve the reliability of these recognition systems. One highly successful approach is eigenfaces, which Matthew Turk and Alex Pentland proposed in 19912 based on principal component analysis (PCA). Since then, researchers have been investigating PCA and using it as a basis for developing successful techniques for face and facial-expression recognition. ...|$|R
40|$|This thesis {{considers}} {{the design of}} <b>automated</b> <b>tutoring</b> <b>systems</b> that customize teaching material to accommodate individual student learning styles. In particular, we consider the following problem: Begin {{with one or more}} presentations of a subject, and break them into fragments ("atoms") each expressing a single idea. Given information about an individual student's learning style, how can one select the optimal choice and sequence of atoms ("path of atoms") to create the most effective presentation for that student? We have implemented several algorithms that automatically create such paths, and we investigate the tradeoff between number of constraints imposed by the algorithms and the number of paths they can find. We have tested one of these algorithms ("partition search") in an experiment where student volunteers in computer science studied material about planning and artificial intelligence. The results of the experiment indicate that the algorithms can produce presentations that are effectively tailored to the different learning styles. by Thomas Lin. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2004. Includes bibliographical references (p. 77 - 78) ...|$|R
40|$|This article {{summarizes}} major {{steps in}} the evolution of Structural Learning Theory (SLT), a comprehensive, parsimonious, precise and operationally defined theory of complex human behavior. SLT covers knowledge represen-tation, methods for constructing same, cognitive processes, knowledge assessment and interactions with external agents (e. g., teachers). The article emphasizes major advances in recent years that make full automation possi-ble. It details and illustrates: a) ill-defined as well as well-defined knowledge, both represented in terms of SLT rules consisting of structural and procedur-al Abstract Syntax Trees (ASTs), b) how SLT rules can be represented at arbi-trary levels of detail and how the higher as well as lower order SLT rules needed to master any given problem domain can be constructed systematical-ly, c) cognitive mechanisms, including empirical data associated with a Universal Control Mechanism (UCM), which controls the use (and acquisi-tion) of all SLT rules, subject only to a fixed processing capacity and speed constraints characteristic of individuals, d) how the knowledge available to any given individual (behavior potential) is operationally defined relative to SLT rules and e) theoretical, empirical and practical implications for building <b>automated</b> <b>tutoring</b> <b>systems.</b> The concluding section shows why the theory makes a difference, how it can be tested and what this implies for building e-learning, intelligent and other advanced <b>tutoring</b> <b>systems...</b>|$|R
40|$|<b>Automated</b> <b>tutoring</b> <b>systems</b> {{that use}} machine {{learning}} algorithms are a relatively new development which promises to revolutionize education by providing students {{on a large scale}} with an experience that closely resembles one-on-one tutoring. Machine learning algorithms are essential for these systems, as they are able to perform, with fairly good results, certain data processing tasks that have usually been considered difficult for artificial intelligence. However, the high performance of several {{machine learning algorithms}} relies on the existence of information about what is being processed in the form of tags, which have to be manually added to the content. Therefore, there is a strong need today for tagged educational resources. Unfortunately, tagging can be a very time-consuming task. Proven strategies for the mass tagging of content already exist: collaborative tagging systems, such as Delicious, StumbleUpon and CiteULike, have been growing in popularity in recent years. These websites allow users to tag content and browse previously tagged content that is relevant to the user’s interests. However, attempting to apply this particular strategy towards educational resource tagging presents several problems. Tags for educational resources to be used in <b>tutoring</b> <b>systems</b> need to be highly accurate, as mistakes in recommending or assigning material to students can be very detrimental to their learning, so ideally subject-matter experts would perform the resource tagging. The issue with hiring experts is that they can sometimes be not only scarce but also expensive, therefore limiting the number of resources that could potentially be tagged. Even if non-experts are used, another issue arises from the fact that a large user base would be required to tag large amounts of resources, and acquiring large numbers of users can be a challenge in itself. To solve these problems, we present Linkify, a system that allows the more accurate tagging of large amounts of educational resources by combining the efforts of users with certain existing machine learning algorithms that are also capable of tagging resources. This thesis will discuss Linkify in detail, presenting its database structure and components, and discussing the design choices made during its development. We will also discuss a novel model for tagging errors based on a binary asymmetric channel. From this model, we derive an EM algorithm which can be used to combine tags entered into the Linkify system by multiple users and machine learning algorithms, producing the most likely set of relevant tags for each given educational resource. Our goal is to enable <b>automated</b> <b>tutoring</b> <b>systems</b> to use this tagging information in the future in order to improve their capability of assessing student knowledge and predicting student performance. At the same time, Linkify’s standardized structure for data input and output will facilitate the development and testing of new machine learning algorithms...|$|R
40|$|This {{demonstration}} shows {{a flexible}} <b>tutoring</b> <b>system</b> {{for studying the}} effects of different tutoring strategies enhanced by a spoken language interface. The hypothesis is that spoken language increases the effectiveness of <b>automated</b> <b>tutoring.</b> The domain is Navy damage control. 1 Technical Content This demonstration shows a flexible <b>tutoring</b> <b>system</b> for studying the effects of different tutoring strategies enhanced by a spoken language interface. The hypothesis is that spoken language increases the effectiveness of <b>automated</b> <b>tutoring.</b> Our {{focus is on the}} SCoT-DC spoken language tutor for Nav...|$|R
40|$|Providing domain {{knowledge}} {{needed by}} intelligent <b>tutoring</b> <b>systems</b> {{to teach a}} procedure to students is traditionally a difficult and time consuming task. This paper presents a system for making this process easier by allowing the <b>automated</b> <b>tutor</b> to acquire the knowledge it needs {{through a combination of}} programming by demonstration, autonomous experimentation, and direct instruction...|$|R
40|$|Abstract — The {{reported}} work advances the state-of-the-art in {{assistive technology}} for the blind by enhancing a low-cost <b>automated</b> <b>tutor</b> designed to teach braille writing skills to visually impaired children using voice feedback. We first provide some background on how the methodology of Intelligent <b>Tutoring</b> <b>Systems</b> correlates to an <b>automated</b> <b>tutor</b> for teaching braille writing skills. We then build on prior work to enhance our <b>automated</b> Braille Writing <b>Tutor</b> in three dimensions: (1) Initial field testing in three different countries; (2) Exploring customization needs for improving relevance in different cultures; and (3) Adding relevant games for increasing motivation. The outcome of this work is an enhanced low-cost tool that can help to increase braille literacy in blind communities around the world. I...|$|R
40|$|After their {{beginnings}} in computer-aided instruction, <b>automated</b> <b>tutors</b> have re-emerged as intelligent <b>tutoring</b> <b>systems.</b> These intelligent <b>tutors</b> {{have obtained}} considerable success by using results from cognitive psychology and artificial intelligence to permit non-traditional instruction which is tailored to their individual students. The {{success of these}} <b>automated</b> <b>tutors</b> is due to their precise understanding and modeling of both the student and the domain being taught. A common measure of the robustness of an <b>automated</b> <b>tutor</b> {{is the size of}} the domain that it can understand. The schema-based Prolog tutor described in this dissertation is capable of recognizing a larger class of programs than existing Prolog tutors. By using powerful generalized transformations, our Prolog tutor can generate this class of programs from a very small set of normal form programs. Thus, our Prolog tutor recognizes a larger class of programs using fewer normal form programs than existing Prolog tutors. One o [...] ...|$|R
40|$|We {{present a}} model for {{assessing}} the evolution of grades in higher education students. Sequential assessment encourages pupils to improve their marks in a general framework of feedback process. We have implemented the system in a Psychology degree annual subject two consecutive years. The contents of the subject were divided up according to the official program into theoretical and applied parts. First, theoretical part was assessed with four mid-course examinations which mean the 66. 67 % of the overall grade. On the other hand, the practical aspects (33. 33 % of the overall grade) were assessed by asking students to write an assay on two short-term research projects and by marking reading discussion in several sitting class. We have found a rise on the average marks as the program moves on and {{the process has been}} modelled and validated with a naïve Bayesian classifier. The Bayesian net is able to predict student grades and it improves the predictions {{as a function of the}} number of assessments. Our results are interesting in order to develop more efficient assessments methods and it could be used for designing <b>automated</b> intelligent <b>tutoring</b> <b>systems...</b>|$|R
40|$|An {{effective}} {{student performance}} review {{strategy is to}} provide positive feedback before providing critical guidance, then to intersperse positive feedback throughout the review. The amount of positive feedback must be balanced against the necessity to continuously impart current and relevant information. An early emphasis of positive feedback helps to engage the student, and variably reinforced positive feedback maintains that engagement, resulting in the student remaining open to critical learning content. This demands {{a high degree of}} interactivity throughout the review process, a strategy applicable to human instructors and <b>automated</b> intelligent <b>tutoring</b> <b>systems.</b> This paper describes a strategy for integrating automated, interactive After Action Reviews (AARs) with simulations to provide student-tailored feedback based on positive, session-specific information. The underlying methods rely on the meta-relations among hierarchies, including learning objectives, demonstrated student achievements and weaknesses, simulation events, and scenario-to-learning objective mapping. The generated AAR output allows the student to drill down to specific details of the AAR, explore how student decisions impact results, and obtain recommendations for learning objective–specific remediation. The approach presumes both that the simulation is assessing multiple learning objectives from a single scenario and that a cross-linkage of learnin...|$|R
40|$|Comparative {{analysis}} {{is the problem}} of predicting how a system will react to perturbations in its parameters, and why. For example, comparative analysis could be asked to explain why the period of an oscillating spring/block system would increase if the mass of the block were larger. This paper formalizes the problem of comparative analysis and presents a technique, differential qualitative (DQ) analysis, which solves the task, providing explanations suitable for use by design <b>systems,</b> <b>automated</b> diagnosis, intelligent <b>tutoring</b> <b>systems,</b> and explanation-based generalization. DQ analysis uses inference rules to deduce qualitative information about the relative change of system parameters. Multiple perspectives are used to represent relative change values over intervals of time. Differential analysis has been implemented, tested on a dozen examples, and proven sound. Unfortunately, the technique is incomplete; it always terminates, but does not always return an answer...|$|R
40|$|Providing domain {{knowledge}} {{needed by}} intelligent <b>tutoring</b> <b>systems</b> {{to teach a}} procedure to students is traditionally a difficult and time consuming task. This paper presents a system for making this process easier by allowing the <b>automated</b> <b>tutor</b> to acquire the knowledge it needs {{through a combination of}} programming by demonstration, autonomous experimentation, and direct instruction. Introduction Education researchers have long recognized that one-onone tutoring is a particularly powerful method of instruction (Bloom 1984). Unfortunately, it is a highly expensive one as well. Providing every student with a teacher to watch over him or her and provide situated, interactive instruction is often impractical. Intelligent <b>tutoring</b> <b>systems</b> (Wenger 1987, Sleeman 1982) attempt to make widespread one-on-one instruction possible by filling in for human instructors and providing some of the same types of interaction. Students can watch a <b>tutoring</b> <b>system</b> demonstrate how to perform a task o [...] ...|$|R
40|$|Comparative {{analysis}} {{is the problem}} of predicting how a system will react to perturbations in its parameters, and why. For example, comparative analysis could be asked to explain why the period of an oscillating spring/block system would increase if the mass of the block were larger. This thesis formalizes the task of comparative analysis and presents two solution techniques: differential qualitative (DQ) analysis and exaggeration. Both techniques solve many comparative analysis problems, providing explanations suitable for use by design <b>systems,</b> <b>automated</b> diagnosis, intelligent <b>tutoring</b> <b>systems,</b> and explanation based generalization. This thesis explains the theoretical basis for each technique, describes how they are implemented, and discusses the difference between the two. DQ {{analysis is}} sound; it never generates an incorrect answer to a comparative analysis question. Although exaggeration does occasionally produce misleading answers, it solves a larger class of problems than DQ analysis and frequently results in simpler explanations...|$|R
40|$|Abstract. This paper {{presents}} the first statistically reliable empirical evidence from a controlled {{study for the}} effect of human-provided emotional scaffolding on student persistence in an intelligent <b>tutoring</b> <b>system.</b> We describe an experiment that added human-provided emotional scaffolding to an <b>automated</b> Reading <b>Tutor</b> that listens, and discuss the methodology we developed to conduct this experiment. Each student participated in one (experimental) session with emotional scaffolding, and in one (control) session without emotional scaffolding, counterbalanced by order of session. Each session was divided into several portions. After each portion of the session was completed, the Reading Tutor gave the student a choice: continue, or quit. We measured persistence {{as the number of}} portions the student completed. Human-provided emotional scaffolding added to the <b>automated</b> Reading <b>Tutor</b> resulted in increased student persistence, compared to the Reading Tutor alone. Increased persistence means increased time on task, which ought lead to improved learning. If these results for reading turn out to hold for other domains too, the implication for intelligent <b>tutoring</b> <b>systems</b> is that they should respond with not just cognitive support – but emotional scaffolding as well. Furthermore, the general technique of adding human-supplied capabilities to an existing intelligent <b>tutoring</b> <b>system</b> should prove useful for studying other ITSs too...|$|R
40|$|An {{exploration}} {{environment and}} tutoring strategies were {{developed for the}} first few hours of learning the programming language LISP. In this environment, the amount of exploratory and receptive learning can be systematically manipulated. In an experimental study with three different learning conditions, learning in a basic exploration environment (without an <b>automated</b> <b>tutor)</b> was compared to learning with an <b>automated</b> <b>tutor</b> that provided help rather selectively, and with an <b>automated</b> <b>tutor</b> that provided help whenever possible. The results showed that the selective tutor condition was most effective: The students in this condition took the least time in acquiring knowledge and solving the criterion test tasks, while solving equal numbers ofthe tasks correctly...|$|R
40|$|A {{promising}} application {{area for}} proactive assistant agents is <b>automated</b> <b>tutoring</b> and training. Intelligent <b>tutoring</b> <b>systems</b> (ITSs) assist <b>tutors</b> and tutees by automating diagnosis and adaptive tutoring. These tasks are well modeled by a partially observable Markov decision process (POMDP) since {{it accounts for}} the uncertainty inherent in diagnosis. However, {{an important aspect of}} making POMDP solvers feasible for real-world problems is selecting appropriate representations for states, actions, and observations. This paper studies two scalable POMDP state and observation representations. State queues allow POMDPs to temporarily ignore less-relevant states. Observation chains represent information in independent dimensions using sequences of observations {{to reduce the size of}} the observation set. Preliminary experiments with simulated tutees suggest the experimental representations perform as well as lossless POMDPs, and can model much larger problems...|$|R
40|$|A Learning Companion System (LCS) is a {{variation}} of an Intelligent <b>Tutoring</b> <b>System</b> (ITS) where besides the <b>automated</b> <b>tutor</b> and the student a third agent is added: an automated Learning Companion (LC). The role of the LC {{is to be a}} peer for the human student and help her as another student would do. For example, the companion could be a role model, both students could collaborate and compete as equals, the companion could be a student of the human student, the companion could be a source of advice, etc. LCSs are relatively new systems so there are many questions to be answered. In particular, the expertise and behaviour of the companion must be carefully chosen so it can help a human student in her learning activities. My research in this area focuses on the exploration {{of the role of the}} companion as a student of the human student. ...|$|R
40|$|How to {{construct}} spoken dialogue interactions with {{children that are}} educationally effective and technically feasible? To address this challenge, we propose a design principle that constructs short dialogues in which (a) the user’s utterance are the external evidence of task performance or learning in the domain, and (b) the target utterances can be expressed as a well-defined set, in some cases even as a finite language (up to a small set of variables which may change from exercise to exercise.) The key approach is to teach the human learner a parameterized process that maps input to response. We describe how the discovery of this design principle came out of analyzing the processes of <b>automated</b> <b>tutoring</b> for reading and pronunciation and designing dialogues to address vocabulary and comprehension, show how it also accurately describes the design of several other language tutoring interactions, and discuss how it could extend to non-language tutoring tasks. Index Terms: spoken dialogue, intelligent <b>tutoring</b> <b>systems.</b> 1...|$|R
40|$|This paper {{presents}} {{a survey of}} Expert <b>Tutoring</b> <b>System</b> (ETS), designed {{for the improvement of}} teaching pedagogy. It talks about the flaws and designing issues that may occur in de-signing the expert <b>tutoring</b> <b>system,</b> and also suggests the cog-nitive approach for building a robust <b>tutoring</b> <b>system.</b> It de-scribes the ill defined domains, case based reasoning, and the sys-tem approach for the designing of a cognitive <b>tutoring</b> <b>system...</b>|$|R
40|$|Two {{intelligent}} <b>tutoring</b> <b>systems</b> were developed. These <b>tutoring</b> <b>systems</b> {{are being}} used to study the effectiveness of intelligent <b>tutoring</b> <b>systems</b> in training high performance tasks and the interrelationship of high performance and cognitive tasks. The two <b>tutoring</b> <b>systems,</b> referred to as the Console Operations Tutors, were built using the same basic approach to the design of an intelligent <b>tutoring</b> <b>system.</b> This design approach allowed researchers to more rapidly implement the cognitively based tutor, the OMS Leak Detect Tutor, by using the foundation of code generated {{in the development of the}} high performance based tutor, the Manual Select Keyboard (MSK). It is believed that the approach can be further generalized to develop a generic intelligent <b>tutoring</b> <b>system</b> implementation tool...|$|R
50|$|Online {{tutoring}} is {{one area}} {{for the application of}} various theories and implementations of tutoring provided to students by a computer. Companies involved in <b>automated</b> online <b>tutoring</b> include Wolfram Alpha, with its module called The Problem Generator (PG) Cognitive Tutor, and others. All <b>automated</b> <b>tutoring</b> involves an application of some form of artificial intelligence to emulate human tutoring, generate appropriate responses, and guide students interaction from one level of learning to the next.|$|R
40|$|Abstract: Tutored {{problem solving}} with <b>automated</b> <b>tutors</b> {{has proven to}} be an {{effective}} instructional method. Worked-out examples {{have been shown to be}} an effective complement to untutored problem solving, but it is largely unknown whether they are an effective complement to tutored problem solving. Further, while computer-based learning environments offer the possibility of adaptively transitioning from examples to problems while tailoring to an individual learner, the effectiveness of such machine-adapted example fading is largely unstudied. To address these issues, two studies were conducted which compared a standard Cognitive Tutor with two example-enhanced Cognitive Tutors. The results indicate that adaptively fading worked-out examples leads to the highest transfer performance on delayed post-tests compared to the other two methods. Background One very successful approach in learning and cognitive skill acquisition is the use of “tutored problem solving ” by intelligent <b>tutoring</b> <b>systems</b> (e. g., Koedinger & Aleven, 2007; VanLehn et al., 2005). Since Cognitive Tutors focus mainly on problem solving performance they can be further improved by providing worked-out solutions when the primary instructional goal is to gain understanding (e. g., Renkl & Atkinson...|$|R
40|$|The {{views and}} {{conclusions}} contained {{in this document}} {{are those of the}} authors and should not be interpreted as representing the official policies or The reported work advances the state-of-the-art in assistive technology for the blind by enhancing a low-cost <b>automated</b> <b>tutor</b> designed to teach braille writing skills to visually impaired children. The relatively low cost of this tutor makes it relevant and accessible to blind communities in both the developed and the developing world. We build on our prior work to extend this tutor along three dimensions. First, we provide a roadmap to formalize the tutor design according to the Intelligent <b>Tutoring</b> <b>System</b> methodology. Second, we conduct preliminary field tests of different aspects of the tutor in three countries and report on our findings. Third, and finally, we improve the tutor’s motivational factor by creating educational games that add to the fun of using the tutor. The outcome of this work is an enhanced low-cost tool that can help to promote braille RAILLE is a widely-used language that is the only means of literacy for blind people. Each braill...|$|R
40|$|Tutored {{problem solving}} with <b>automated</b> <b>tutors</b> {{has proven to}} be an {{effective}} instructional method. Worked-out examples {{have been shown to be}} an effective complement to untutored problem solving, but it is largely unknown whether they are an effective complement to tutored problem solving. Further, while computer-based learning environments offer the possibility of adaptively transitioning from examples to problems while tailoring to an individual learner, the effectiveness of such machine-adapted example fading is largely unstudied. To address these issues, two studies were conducted which compared a standard Cognitive Tutor with two example-enhanced Cognitive Tutors. The results indicate that adaptively fading worked-out examples leads to the highest transfer performance on delayed post-tests compared to the other two methods. Background One very successful approach in learning and cognitive skill acquisition is the use of “tutored problem solving ” by intelligent <b>tutoring</b> <b>systems</b> (e. g., Koedinger & Aleven, 2007; VanLehn et al., 2005). Since Cognitive Tutors focus mainly on problem solving performance they can be further improved by providing worked-out solutions when the primary instructional goal is to gain understanding (e. g., Renkl &...|$|R
40|$|Intelligent <b>tutoring</b> <b>systems</b> are {{specialized}} {{computer programs}} utilizing artificial intelligence {{in order to}} provide students with interactive <b>tutoring.</b> Many <b>tutoring</b> <b>systems</b> construct a model of the student s current knowledge and understanding of the subject matter in order to personalize the tutoring for a student. Plan recognition is one of the diagnostic methods available for student modeling. This thesis explores plan recognition and intelligent <b>tutoring</b> <b>systems,</b> and how plan recognition can be incorporated by intelligent <b>tutoring</b> <b>systems.</b> We examine the strength and weaknesses of plan recognition, and how intelligent <b>tutoring</b> <b>systems</b> can benefit from having knowledge of a student's plan...|$|R
