62|106|Public
2500|$|The new DAB+ {{standard}} has incorporated Reed-Solomon ECC as an [...] "inner layer" [...] of coding that {{is placed}} around the byte interleaved <b>audio</b> <b>frame</b> {{but inside the}} [...] "outer layer" [...] of convolutional coding used by the older DAB system, although on DAB+ the convolutional coding uses equal error protection (EEP) rather than UEP since each bit is equally important in DAB+. This combination of Reed-Solomon coding as the inner layer of coding, followed by an outer layer of convolutional coding – so-called [...] "concatenated coding" [...] – became a popular ECC scheme in the 1990s, and NASA adopted it for its deep-space missions. One slight difference between the concatenated coding used by the DAB+ system and that used on most other systems is that it uses a rectangular byte interleaver rather than Forney interleaving {{in order to provide}} a greater interleaver depth, which increases the distance over which error bursts will be spread out in the bit-stream, which in turn will allow the Reed-Solomon error decoder to correct a higher proportion of errors.|$|E
5000|$|Michael Wehr - arranger, keyboards, {{background}} vocals, producer, engineer, mixing, digital engineer, <b>audio</b> <b>frame</b> systems operator, {{digital system}} operator ...|$|E
5000|$|An <b>Audio</b> <b>Frame</b> section, which {{contains}} decoding information {{common to all}} audio blocks within the syncframe, including the necessary information to determine how exponents and mantissas are packed.|$|E
50|$|This {{stream of}} <b>audio</b> <b>frames,</b> as a whole, is then {{subjected}} to CIRC encoding, which segments and rearranges {{the data and}} expands it with parity bits {{in a way that}} allows occasional read errors to be detected and corrected. CIRC encoding also interleaves the <b>audio</b> <b>frames</b> throughout the disc over several consecutive frames so that the information will be more resistant to burst errors. Therefore, a physical frame on the disc will actually contain information from multiple logical <b>audio</b> <b>frames.</b> This process adds 64 bits of error correction data to each frame. After this, 8 bits of subcode or subchannel data are added to each of these encoded frames, which is used for control and addressing when playing the CD.|$|R
50|$|The {{comparison}} with hidden stations shows that RTS/CTS packages in each traffic class are profitable (even with short <b>audio</b> <b>frames,</b> which cause a high overhead on RTS/CTS frames).|$|R
50|$|MPEG-4 SL, the MPEG-4 {{synchronization}} layer {{manages the}} identification of access units like video or <b>audio</b> <b>frames,</b> and scene description commands and the time stamping of them independent of the media type within elementary streams to enable synchronization among them.|$|R
50|$|Smacker audio {{is one of}} {{the audio}} formats {{that can be used in}} the Smacker container. For compression, Differential pulse code {{modulation}} (DPCM) is used. The difference between two successive samples is compressed using Huffman coding. The Huffman tables are adapted once per <b>audio</b> <b>frame.</b>|$|E
50|$|An old-fashioned CD player reading subcode {{correctly}} sees {{a missing}} <b>audio</b> <b>frame</b> and interpolates any missing information that it cannot correct using information from neighbouring frames. Because these missing frames occur at points where the waveform was nearly {{a straight line}} anyway, this interpolation is very accurate and generally transparent to the user.|$|E
50|$|The header of a frame {{contains}} {{general information}} such as the MPEG Layer, the sampling frequency, the number of channels, whether the frame is CRC protected, whether the sound is the original:Although most of this information may {{be the same for}} all frames, MPEG decided to give each <b>audio</b> <b>frame</b> such a header in order to simplify synchronization and bitstream editing.|$|E
40|$|In this paper, {{we propose}} a {{statistical}} optimization framework for transmitting audio sequences over wireless links. Our proposed framework protects <b>audio</b> <b>frames</b> against both temporally cor-related random bit errors introduced by a fading channel and packet erasures caused by network buffering. Forming a two-dimensional grid of symbols, our framework forms horizontal packets that are compensated only vertically against {{both types of}} errors. The utilized one-dimensional error correction coding scheme of our framework assigns parity bits according to the perceptual importance of frames such that the Segmented SNR of a received audio sequence is maximized. In addition, the proposed framework suggests an effective way of reducing the packetization over-head of small <b>audio</b> <b>frames.</b> I...|$|R
40|$|We {{present an}} {{integrated}} approach of full-band audio time scale modification for Voice over IP communication. The concept {{is based on}} a low complexity adaptive playout method that uses <b>frame</b> dropping and <b>audio</b> concealment for time shrinking and stretching, respectively. The existing version of this method is improved using a classifier that assists in choosing which <b>audio</b> <b>frames</b> can be dropped with the least subjective impact on audio quality. To maintain low complexity, we exclusively use audio signal features that are available in the audio codec. The classification of <b>audio</b> <b>frames</b> improves <b>audio</b> quality of the existing method without classification by 0 : 5 Mean Opinion Score points while requiring significantly less computational complexity by a factor of ca 104...|$|R
40|$|Abstract—In this paper, {{we present}} an {{optimization}} framework for transmitting high quality audio sequences over error-prone wireless links. Our framework introduces apparatus and technique to optimally protect a stored audio sequence transmitted over a wireless link while considering the packetization overhead of <b>audio</b> <b>frames.</b> Utilizing rate compatible punctured RS codes and dynamic program-ming, it identifies the optimal assignment of parity to <b>audio</b> <b>frames</b> {{according to their}} perceptual importance such that the Segmented SNR of the received audio sequence is maximized. Our framework covers two cases. In the first case, a frame grouping technique is proposed to packetize <b>audio</b> <b>frames</b> and protect them against temporarily corre-lated bit errors introduced by a fading wireless channel. In this case, each packet is treated as a channel coding codeword. In the second case, a one-dimensional RS coder is applied vertically to a sequence of horizontally formed packets associated with an audio sequence {{in order to protect}} the sequence against both bit errors introduced by fading wireless channels and packet erasures introduced by network buffering. Our numerical results capture the performance advantage of our framework compared to existing techniques proposed in the literature of audio transmission. We also note that our framework can be generically applied to a variety of audio coders making it attractive in terms of implementation...|$|R
50|$|Most key {{features}} of MPEG-1 Audio were directly inherited from MUSICAM, including the filter bank, time-domain processing, <b>audio</b> <b>frame</b> sizes, etc. However, improvements were made, {{and the actual}} MUSICAM algorithm was not used in the final MPEG-1 Layer II audio standard. The widespread usage of the term MUSICAM to refer to Layer II is entirely incorrect and discouraged for both technical and legal reasons.|$|E
50|$|The audio coding {{algorithm}} {{used by the}} Eureka 147 Digital Audio Broadcasting (DAB) system has been subject to the standardization process within the ISO/Moving Pictures Expert Group (MPEG) in 1989-94. MUSICAM audio coding {{was used as a}} basis for some coding schemes of MPEG-1 and MPEG-2 Audio. Most key features of MPEG-1 Audio were directly inherited from MUSICAM, including the filter bank, time-domain processing, <b>audio</b> <b>frame</b> sizes, etc. However, improvements were made, and the actual MUSICAM algorithm was not used in the final MPEG-1 Layer II audio standard.|$|E
50|$|The signal survives {{temporal}} masking and {{sub-band coding}} by {{operating on the}} fundamental frequency and its subharmonic overtones, and by dealigning the phase relationship between the strongest signal and its subharmonics. Each phase discontinuity introduced by the encoder {{will result in a}} corresponding pulse of wideband white noise, so a further range of additional distortions are introduced as a noise mitigation strategy to compensate. The desired hidden digital data signal is combined in the distortion step using a pre-determined pseudorandom binary sequence for <b>audio</b> <b>frame</b> synchronization and large amounts of forward error correction for the hidden data to be embedded. The watermark is only embedded when certain signal-to-noise ratio thresholds are met and is not available as a continuous signal—the signal must be monitored {{for a period of time}} before the embedded data can be detected and recovered. Extraction of the hidden signal is not exact but is based on recovering the convolutional codes through statistical cross-correlation.|$|E
40|$|This paper {{describes}} a novel high capacity steganography algorithm for embedding {{data in the}} inactive frames of low bit rate audio streams encoded by G. 723. 1 source codec, which is used extensively in Voice over Internet Protocol (VoIP). This study reveals that, contrary to existing thoughts, the inactive frames of VoIP streams {{are more suitable for}} data embedding than the active frames of the streams, that is, steganography in the inactive <b>audio</b> <b>frames</b> attains a larger data embedding capacity than that in the active <b>audio</b> <b>frames</b> under the same imperceptibility. By analysing the concealment of steganography in the inactive frames of low bit rate audio streams encoded by G. 723. 1 codec with 6. 3 kbps, the authors propose a new algorithm for steganography in different speech parameters of the inactive frames. Performance evaluation shows embedding data in various speech parameters led to different levels of concealment. An improved voice activity detection algorithm is suggested for detecting inactive <b>audio</b> <b>frames</b> taking into packet loss account. Experimental results show our proposed steganography algorithm not only achieved perfect imperceptibility but also gained a high data embedding rate up to 101 bits/frame, indicating that the data embedding capacity of the proposed algorithm is very much larger than those of previously suggested algorithms...|$|R
40|$|International audienceEndpoints or conference servers {{of current}} audio-conferencing {{solutions}} {{use all the}} <b>audio</b> <b>frames</b> they receive in order to mix them into one final aggregate stream. However, at each time-instant, some of this content may not be audible due to auditory masking. Hence, sending corresponding frames through the network leads {{to a loss of}} bandwidth, while decoding them for mixing or spatial audio processing leads to increased processor load. In this paper, we propose a solution based on an efficient on-the-fly auditory masking evaluation. Our technique allows prioritizing <b>audio</b> <b>frames</b> in order to select only those audible for each connected client. We present results of quality tests showing the transparency of the algorithm. We describe its integration in a France Telecom audio conference server. Tests in a 3 D game environment with spatialized chat capabilities show a 70 % average reduction in required bandwidth, demonstrating the efficiency of our method...|$|R
40|$|In {{this paper}} {{we present a}} new method to compute <b>frame</b> based <b>audio</b> similarities, based on nearest {{neighbour}} density estimation. We do not recommend it is as a practical method for large collections {{because of the high}} runtime. Rather, we use this new method for a detailed analysis to get a deeper insight on how a bag of frames approach (BOF) determines similarities among songs, and in particular, to identify those <b>audio</b> <b>frames</b> that make two songs similar from a machine’s point of view. Our analysis reveals that <b>audio</b> <b>frames</b> of very low energy, which are of course not the most salient with respect to human perception, have a surprisingly big influence on current similarity measures. Based on this observation we propose to remove these low-energy frames before computing song models and show, via classification experiments, that the proposed frame selection strategy improves the audio similarity measure. 1...|$|R
5000|$|The new DAB+ {{standard}} has incorporated Reed-Solomon ECC as an [...] "inner layer" [...] of coding that {{is placed}} around the byte interleaved <b>audio</b> <b>frame</b> {{but inside the}} [...] "outer layer" [...] of convolutional coding used by the older DAB system, although on DAB+ the convolutional coding uses equal error protection (EEP) rather than UEP since each bit is equally important in DAB+. This combination of Reed-Solomon coding as the inner layer of coding, followed by an outer layer of convolutional coding - so-called [...] "concatenated coding" [...] - became a popular ECC scheme in the 1990s, and NASA adopted it for its deep-space missions. One slight difference between the concatenated coding used by the DAB+ system and that used on most other systems is that it uses a rectangular byte interleaver rather than Forney interleaving {{in order to provide}} a greater interleaver depth, which increases the distance over which error bursts will be spread out in the bit-stream, which in turn will allow the Reed-Solomon error decoder to correct a higher proportion of errors.|$|E
40|$|Audio decoder {{device for}} {{decoding}} a bitstream, the audio decoder device comprising: a predictive decoder for producing a decoded <b>audio</b> <b>frame</b> from the bitstream, wherein the predictive decoder comprises a parameter decoder for producing {{one or more}} audio parameters for the decoded <b>audio</b> <b>frame</b> from the bitstream and wherein the predictive decoder comprises a synthesis filter device for producing the decoded <b>audio</b> <b>frame</b> by synthesizing {{the one or more}} audio parameters for the decoded audio frame; a memory device comprising one or more memories, wherein each of the memories is configured to store a memory state for the decoded <b>audio</b> <b>frame,</b> wherein the memory state for the decoded <b>audio</b> <b>frame</b> of the one or more memories is used by the synthesis filter device for synthesizing the one or more audio parameters for the decoded audio frame; and a memory state resampling device configured to determine the memory state for synthesizing the one or more audio parameters for the decoded <b>audio</b> <b>frame,</b> which has a sampling rate, for one or more of said memories by resampling a preceding memory state for synthesizing one or more audio parameters for a preceding decoded <b>audio</b> <b>frame,</b> which has a preceding sampling rate being different from the sampling rate of the decoded <b>audio</b> <b>frame,</b> for one or more of said memories and to store the memory state for synthesizing of the one or more audio parameters for the decoded <b>audio</b> <b>frame</b> for one or more of said memories into the respective memory...|$|E
30|$|Low-level audio MPEG- 7 {{descriptors}} {{for every}} <b>audio</b> <b>frame.</b>|$|E
25|$|Time signals, on the contrary, are not {{a problem}} in a {{well-defined}} network with a fixed delay. The DAB multiplexer adds the proper offset to the distributed time information. The time information is also independent from the (possibly varying) audio decoding delay in receivers since the time is not embedded inside the <b>audio</b> <b>frames.</b> This means that built in clocks in receivers will be spot on.|$|R
40|$|International audienceTV {{represents}} a huge source of data. Even if a TV stream exhibits a strong structure to the viewer, {{in terms of}} programs and breaks, this structure is completely implicit in the stream, which is a simple sequence of images and <b>audio</b> <b>frames.</b> This paper presents recent works achieved to recover the structure of such a stream. 4 categories of works are presented, {{as well as their}} results and respective requirements in term of annotation. The paper ends by outlining the challenges to be solved in this largely opened field of research...|$|R
40|$|This paper {{investigates the}} {{benefits}} of streaming autonomous audio objects over error-prone channels instead of encoded <b>audio</b> <b>frames.</b> Due {{to the nature of}} autonomous audio objects such a scheme is error resilient and has a fine-grain scalable bitrate, but also has the additional benefit of being able to disguise packet loss in the reconstructed signal. This paper proposes objectpacking algorithms which will be shown to be able to disguise the presence of long bursts of packet loss, removing the need for complex error-concealment schemes at the decoder. 1...|$|R
40|$|Techniques for {{watermarking}} digital representations such as MPEG audio frames {{that spread}} the watermark information {{across the entire}} <b>audio</b> <b>frame.</b> The techniques work in conjunction with lossy compression techniques and are compatible with the perception models that are often used with lossy compression techniques. The watermark information is spread by means of transformations between the space/time domain and the frequency domain. When a MPEG <b>audio</b> <b>frame</b> is being watermarked, the compressed <b>audio</b> <b>frame</b> as it is produced by the quantizer is transformed from the frequency domain to the time domain; the time domain transformation is then randomized using a key and the randomized time domain transformation is transformed into the frequency domain. The watermark information is added at a predetermined frequency in the frequency domain transformation and the sequence of transformations is done in reverse order, with the randomization and derandomization serving to distribute the watermark information across the frequency domain representation of the watermarked <b>audio</b> <b>frame...</b>|$|E
3000|$|... : {{the quality}} impact of video- or audio-transmission errors, that is, video packet or <b>audio</b> <b>frame</b> loss.|$|E
3000|$|... {{are stored}} {{for later use}} in the {{extraction}} process. This makes the proposed watermarking algorithm semi-blind, as the whole original <b>audio</b> <b>frame</b> is not required in the extraction process.|$|E
30|$|Partition the <b>audio</b> signal into <b>frames</b> of size 4, 096 samples.|$|R
2500|$|... {{a website}} that publishes books, articles, and <b>audio</b> by John <b>Frame</b> and Poythress ...|$|R
40|$|Audio {{signals are}} highly {{structured}} from a low, signal level to high cognitive aspects. We investigate how {{to exploit the}} common sparse structure between similar <b>audio</b> <b>frames</b> in order to reconstruct missing data in audio signals. While joint sparse models and related algorithms have been widely studied, one important challenge is to locate such similar frames : the search must be adapted to the joint-sparse model and should be fast and one must deal with missing data in the frames. We propose, compare and discuss several similarity measures dedicated to this task. We then show how this strategy can lead to better reconstruction of missing data in audio signals...|$|R
3000|$|... {{completely}} {{characterizes the}} signal spectral properties. In the general case, the error signal (or residual signal) {{will not have}} white noise statistics and thus cannot be ignored. In this general case, the all-pole model that results from the LP analysis gives only an approximation of the signal spectrum, and more specifically the spectral envelope. For the particular case of audio signals, the spectrum contains only the frequency components that correspond to the fundamental frequencies of the recorded instruments and all their harmonics. (For simplicity, {{at this point we}} consider only harmonic sounds. The proposed model is tested for complex music signals in Section 5.) The AR filter for an <b>audio</b> <b>frame</b> will capture its spectral envelope. The error signal {{is the result of the}} <b>audio</b> <b>frame</b> filtered with the inverse of its spectral envelope. Thus we conclude that the error signal will contain the same harmonics as the <b>audio</b> <b>frame,</b> but their amplitudes will now have significantly flatter shape in the frequency spectrum.|$|E
40|$|An {{audio decoder}} for {{providing}} a decoded representation of an audio content {{on the basis}} of an encoded representation of the audio content comprises a linear-prediction-domain decoder core configured to provide a time-domain representation of an <b>audio</b> <b>frame</b> {{on the basis of}} a set of linear-prediction domain parameters associated with the <b>audio</b> <b>frame</b> and a frequency-domain decoder core configured to provide a time-domain representation of an <b>audio</b> <b>frame</b> {{on the basis of a}} set of frequency-domain parameters, taking into account a transform window out of a set comprising a plurality of different transform windows. The audio decoder comprises a signal combiner configured to overlap-and-add-time-domain representations of subsequent audio frames encoded in different domains, in order to smoothen a transition between the time-domain representations of the subsequent frames. The set of transform windows comprises one or more windows specifically adapted for a transition between a frequency-domain core mode and a linear-prediction-domain core mode...|$|E
30|$|In the {{proposed}} method the <b>audio</b> <b>frame</b> is marked as voiced frame when the STE {{is high and}} ZCC is low. In contrast, when the STE is low and ZCC is high, the frame is marked as unvoiced frame [35].|$|E
50|$|The MTV video format (no {{relation}} to the cable network) consists of a 512-byte file header that operates by displaying a series of raw image frames during MP3 playback. During this process, <b>audio</b> <b>frames</b> are passed to the chipset's decoder, while the memory pointer of the display's hardware is adjusted to the next image within the video stream. This method does not require additional hardware for decoding, though {{it will lead to}} a higher amount of memory consumption. For that reason, the storage capacity of an MP4 player that uses MTV files is effectively less than that of a player that decompresses files on the fly.|$|R
30|$|Segment {{the host}} <b>audio</b> signal into <b>frames,</b> each of 4, 096 samples in length.|$|R
5000|$|Light {{carrying}} the data signal through the Lightpipe is {{turned into an}} electronic data stream going to an IC chip commonly referred to at Alesis as [...] "the 1-K chip". From there the <b>audio</b> data <b>frame</b> is routed to processing IC's.|$|R
