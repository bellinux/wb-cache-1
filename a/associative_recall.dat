77|38|Public
25|$|Neural Turing {{machines}} couple LSTM {{networks to}} external memory resources, {{with which they}} can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms, such as copying, sorting and <b>associative</b> <b>recall</b> from input and output examples.|$|E
25|$|Neural Turing {{machines}} (NTM) {{extend the}} capabilities of deep neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that NTMs can infer simple algorithms such as copying, sorting and <b>associative</b> <b>recall</b> from input and output examples.|$|E
50|$|Neural Turing {{machines}} couple LSTM {{networks to}} external memory resources, {{with which they}} can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and <b>associative</b> <b>recall</b> from input and output examples.|$|E
5000|$|Nakazawa, K., Quirk, M. C., Chitwood, R. A., Watanabe, M., Yeckel, M. F., Sun, L. D., Kato, A., Carr, C.A., Johnston, D., Wilson, M.A., & Tonegawa, S. (2002). Requirement for hippocampal CA3 NMDA receptors in <b>associative</b> memory <b>recall.</b> Science, 297(5579), 211-218.|$|R
40|$|Asynchronous {{nonlinear}} fractal operators {{were considered}} {{in order to}} implement fractal applications by massive parallel asynchronous computing system as recurrent neural networks. Assuming the convergence of the synchronous fractal operator to an element ~ f which is an approximation of the original image f, it is proved that any asynchronous deterministic realization of local fractal operators is convergent to ~ f and the stochastic realization converges to ~ f with probability one. Beside compression, applications of such stochastic fractal operators for object recognition and image association are described. This new approach {{to the concept of}} associative memory, exhibits correct <b>associative</b> <b>recalls</b> for highly noised images and also for images with small 3 D distortions (on Olivietti face database it results in 100 % rate of face recognition) ...|$|R
40|$|We {{examine the}} number of cells and {{execution}} time taken to correctly recognize rotated patterns in two models: a rotation-invariant neocognitron (RNeocognitron) and a neocognitron-type model (TDR -Neocognitron) which recognizes rotated patterns by use of an <b>associative</b> <b>recalled</b> pattern. In numerical simulations handwritten patterns in CEDER database are used for training and evaluation of recognition rate. We show that TD-R-Neocognitron needs less cells than R-Neocognitron if {{the number of}} pattern classes is large. Execution time by TDR -Neocognitron is about three times as much as that of R-Neocognitron, but TD-R-Neocognitron is more e#ective and e#cient for patterns including many classes like Japanese characters. 1 Introduction Various visual models for pattern recognition based on the Neocognitron[1] have been proposed. We have proposed a rotation-invariant Neocognitron (referred to R-Neocognitron, see [2]), which is based on the Neocognitron. It can recognize also rotated patter [...] ...|$|R
50|$|Neural turing {{machines}} (NTM) {{extend the}} capabilities of deep neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that NTMs can infer simple algorithms such as copying, sorting and <b>associative</b> <b>recall</b> from input and output examples.|$|E
50|$|A Neural Turing machine (NTMs) is a {{recurrent}} {{neural network model}} published by Alex Graves et. al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, {{making it possible to}} optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and <b>associative</b> <b>recall</b> from input and output examples.|$|E
5000|$|Mitra is {{a leading}} proponent of Minimally {{invasive}} education. He has a PhD in Physics but is credited with more than 25 inventions {{in the area of}} cognitive science and education technology. [...] He was conferred the Dewang Mehta Award for Innovation in Information Technology in the year 2005. In September 2012 Mitra won the Leonardo European Corporate Learning Award in the [...] "Crossing Border" [...] category. He argued that broken connections in simulated neural networks are a model for Alzheimer's disease (The effect of synaptic disconnection on bi-directional <b>associative</b> <b>recall.</b> S. Mitra, Proc. IEEE/SMC Conf., Vol.1, 989, 1994 USA).|$|E
40|$|This paper {{discusses}} {{an approach}} to constructing an artificial quantum associative memory (QuAM). The QuAM makes use of two quantum computational algorithms, one for pattern storage {{and the other for}} pattern recall. The result is an exponential increase in the capacity of the memory when compared to traditional associative memories such as the Hopfield network. Further, the paper argues for considering pattern recall as a non-unitary process and demonstrates the utility of non-unitary operators for improving the pattern recall performance of the QuAM. 1. Introduction The field of artificial neural networks (ANN) seeks, among other things, to develop algorithms for imitating in some sense the functionality of the brain. One particular area of interest is that of <b>associative</b> pattern <b>recall,</b> with perhaps the most well known approach being the Hopfield network [7]. Such ANN approaches to the pattern completion problem allow for <b>associative</b> pattern <b>recall,</b> but suffer severe storage restri [...] ...|$|R
40|$|This paper {{proposes a}} general model for {{bidirectional}} associative memories that associate patterns between the X-space and the Y-space. The general model {{does not require}} the usual assumption that the interconnection weight from a neuron in the X-space to a neuron in the Yspace {{is the same as the}} one from the Y-space to the X-space. We start by defining a supporting function to measure how well a state supports another state in a general bidirectional associative memory (GBAM). We then use the supporting function to formulate the <b>associative</b> <b>recalling</b> process as a dynamic system, explore its stability and asymptotic stability conditions, and develop an algorithm for learning the asymptotic stability conditions using the Rosenblatt perceptron rule. The effectiveness of the proposed model for recognition of noisy patterns and the performance of the model in terms of storage capacity, attraction, and spurious memories are demonstrated by some outstanding experimental results. Keywords [...] - [...] ...|$|R
40|$|We {{propose a}} new {{hypothesis}} concerning the lateralization of prefrontal cortex (PFC) activity during verbal episodic memory retrieval. The hypothesis {{states that the}} left PFC is differentially more involved in semantically guided information production than is the right PFC, and that the right PFC is differentially more involved in monitoring and verification than is the left PFC. This "production-monitoring hypothesis" differs from the existing "systematic [...] heuristic hypothesis," which proposes that the left PFC is primarily involved in systematic retrieval operations, and the right PFC in heuristic retrieval operations. To compare the two hypotheses, we measured PFC activity using positron emission tomography (PET) during the performance of four episodic retrieval tasks: stem cued <b>recall,</b> <b>associative</b> cued <b>recall,</b> context recognition (source memory), and item recognition. Recall tasks emphasized production processes, whereas recognition tasks emphasized monitoring processes. Stem cued recall and context-recognition tasks underscored systematic operations, whereas <b>associative</b> cued <b>recall</b> and item-recognition tasks underscored heuristic operations. Consistent with the production-monitoring hypothesis, the left PFC was more activated for recall than for recognition tasks and the right PFC was more activated for recognition than for recall tasks. Inconsistent with the systematic [...] heuristic hypothesis, the left PFC was more activated for heuristic than for systematic tasks and the right PFC showed the converse result. Additionally, the study yielded activation differences outside the PFC. In agreement with a previous recall/ recognition PET study, anterior cingulate, cerebellar, and striatal regions were more activated for recall than for recognition tasks, and the co [...] ...|$|R
5000|$|The Oscillator Based <b>Associative</b> <b>Recall</b> (OSCAR) Model was {{proposed}} by Brown, Preece and Hulme in 2000 [...] The OSCAR Model is another cue driven model of memory. In this model, the cues {{work as a}} pointer to a memory’s position in the mind. Memories themselves are stored as context vectors on what Brown calls the oscillator part of the theory. While a memory is not being used in short term memory,the context vector oscillates further away from the starting position. When a cue becomes present, the context vector that the cue points to gets oscillated back to the starting point, where the memory can then be used in short term memory. There is no theory of decay in this model, so to account for memory loss or fading the theory states that memories move further and further from the starting point, and retrieval becomes more difficult the further the memory is from the starting position. Furthermore, some memories oscillate faster than others. The theory behind this is that memories frequently accessed and used will move slower away from the starting point since the probability of the memory being retrieved is relatively high.|$|E
40|$|Two {{unipolar}} mathematical {{models of}} electronic neural network functioning as terminal-attractor-based associative memory (TABAM) developed. Models comprise sets of equations describing interactions between time-varying {{inputs and outputs}} of neural-network memory, regarded as dynamical system. Simplifies design and operation of optoelectronic processor to implement TABAM performing <b>associative</b> <b>recall</b> of images. TABAM concept described in "Optoelectronic Terminal-Attractor-Based Associative Memory" (NPO- 18790). Experimental optoelectronic apparatus that performed <b>associative</b> <b>recall</b> of binary images described in "Optoelectronic Inner-Product Neural Associative Memory" (NPO- 18491) ...|$|E
40|$|Abstract: 2 ̆ 2 Many recent connectionist {{models can}} be {{categorized}} as associative memories or pattern classifiers. Viewed at the right level of abstraction, the two are the same. Connectionists sometimes appear {{to be trying to}} squeeze all of cognition into the associative memory paradigm, perhaps because it 2 ̆ 7 s the only thing they know how to implement with gradient descent learning algorithms. But the combinatorial structure of thought and language indicates that the answer to 2 ̆ 7 How can slow components think so fast 2 ̆ 7 lies beyond mere <b>associative</b> <b>recall.</b> We must search for additional cognitive primitives that can be implemented in parallel hardware. One modest successor to <b>associative</b> <b>recall</b> is considered here. 2 ̆...|$|E
40|$|In [4] we {{introduce}} the <b>associative</b> algebras Q_n,k(,τ). <b>Recall</b> the definition. These algebras are labeled by discrete parameters n,k; n,k are integers n>k> 0 and n and k have not common divisors. Then, is an elliptic curve and τ {{is a point}} in. We identify with /Γ, where Γ is a lattice. Comment: 26 pages, plain TeX Submitted by request of the authors; no comments are availabl...|$|R
40|$|Pattern completion, {{the ability}} to {{retrieve}} complete memories initiated by partial cues, is a critical feature of the memory process. However, little is known regarding the molecular and cellular mechanisms underlying this process. To study the role of dopamine in memory recall, we have analyzed dopamine transporter heterozygous knockout mice (DAT +/ 2), and found that while these mice possess normal learning, consolidation, and memory recall under full cue conditions, they exhibit specific deficits in pattern completion under partial cue condition. This form of memory recall deficit in the dopamine transporter heterozygous knockout mice can be reversed by a low dose of the dopamine antagonist haloperidol, further confirming that the inability to retrieve memory patterns {{is a result of}} dopamine imbalance. Therefore, our results reveal that a delicate control of the brain’s dopamine level is critical for pattern completion during <b>associative</b> memory <b>recall...</b>|$|R
40|$|This article {{addresses}} {{the relation between}} item recognition and <b>associative</b> (cued) <b>recall.</b> Going beyond measures of performance on each task, the analysis focuses on {{the degree to which}} the contingency between successful recognition and successful recall of a studied item reflects the commonality of memory processes underlying the recognition and recall tasks. Specifically, 4 classes of distributed memory models are assessed for their ability to account for the relatively invariant correlation (. 5) between successive recognition and recall. Basic versions of each model either under- or overpredict the intertask correlation. Introducing variability in goodness-of-encoding and response criteria, as well as output encoding, enabled all 4 models to reproduce the moderate intertask correlation and the increase in correlation observed in 2 mixed-list experiments. This model-based analysis provides a general theoretical framework for interpreting contingencies between successive memory tests...|$|R
40|$|Optoelectronic {{apparatus}} acts as artificial {{neural network}} performing <b>associative</b> <b>recall</b> of binary images. Recall process is iterative one involving optical computation of inner products between binary input vector and one or more reference binary vectors in memory. Inner-product method requires far less memory space than matrix-vector method...|$|E
40|$|Report {{presents}} {{theoretical and}} {{experimental study of}} optically and electronically addressable optical implementation of artificial neural network that performs <b>associative</b> <b>recall.</b> Shows by computer simulation that terminal-attractor-based associative memory can have perfect convergence in associative retrieval and increased storage capacity. Spurious states reduced by exploiting terminal attractors...|$|E
40|$|A simple {{neural network}} is studied, which has sparse, random, plastic, {{excitatory}} connections and also feedback loops between sensory cells and correlator cells. Time {{is limited to}} several discrete instants, where firing is synchronous. For parameter values within biological ranges, the system exhibits a capacity for <b>associative</b> <b>recall,</b> with a controlled amount of extraneous firing, following Hebb-like synaptic changes. ...|$|E
40|$|Research {{has shown}} that {{generating}} words in a study phase (e. g. "shoe" from foot-s [...] e) usually yields better memory performance compared to reading words (e. g. foot-shoe), and this advantage for generated words relative to read words is called the generation effect. One particularly interesting version of the generation effect was studied by Jacoby (1978). In two conditions of his study, subjects generated a word once (e. g. foot-s [...] e) or generated a word immediately after reading it (e. g. foot-s [...] e preceded by foot-shoe). Jacoby (1978) found that generating a word once produced better cued recall (e. g. foot-? ? ? ?) than reading a word, then generating it. One {{purpose of the present}} experiments was to investigate if Jacoby's (1978) results would generalize to other memory tests. The experiments reported manipulated the number of study presentations, the modality of study presentation, the difficulty of generating a word, and the match between perceptual cues (e. g. s [...] e) presented at study and test, and examined performance on four tests (<b>associative</b> cued <b>recall,</b> recognition, word fragment cued recall, and word fragment completion). Jacoby's (1978) results were replicated for <b>associative</b> cued <b>recall,</b> but other patterns of data were obtained on the other tests. A new finding for a word fragment cued recall test occurred based on the match of perceptual cues between study and test. Presenting the same word fragment (e. g. s [...] e) at study and test resulted in better memory performance than did presenting different word fragments at study and test cue (e. g. s [...] e at study but s-o- at test). Dissociations between word fragment cued recall and the word fragment completion ruled out the possibility that generation effects obtained for the word fragment completion test were due to subjects using intentional retrieval strategies. Most of the results were explainable with the transfer appropriate processing framework, which states that the greater the overlap in mental processes engaged at study and test, the better test performance will be...|$|R
40|$|In {{this paper}} a binary {{associative}} network model with minimal number of connections is examined and its microscopic dynamics exactly studied. The knowledge {{of its time}} behavior allows us to determine a learning rule which realizes a one-step <b>recalling</b> <b>associative</b> memory. Its storage capacity is also analyzed with randomly distributed patterns and is proved to be O(log n) in the worst case, n being the number of neurons and connections, but to increase considerably when the patterns to be memorized are correlated. Spurious states are also investigated...|$|R
40|$|An {{essential}} part of image analysis is the location and identification of objects within the image. Noise and clutter make this identification problematic, {{and the size of}} the image may present a computational problem. To overcome these problems, we use a window onto the image to focus onto small areas. Conventionally we still need to know the size of the object we are searching for in order to select a window of the correct size. We describe a method for object location and classification which enables us to use a small window to identify large objects in the image. The window focusses on features in the image, and an <b>associative</b> memory <b>recalls</b> evidence for objects from these features, avoiding the necessity of knowing the dimensions of the objects to be detected. INTRODUCTION The motivation behind this work was the desire to analyse large images, with a view to their classification. In particular, we are looking at the classification of document images according to their content. In [...] ...|$|R
40|$|A new nonrecurrent {{associative}} memory model is proposed. This model {{is composed of}} a nonlinear transformation in the spectral domain followed by the association. The MoorePenrose pseudoinverse is employed to obtain the least-squares optimal solution. Computer simulations are done to evaluate {{the performance of the}} model. The simulations use one-dimensional speech signals and two-dimensional head/shoulder images. Comparison of the proposed model with the classical optimal linear {{associative memory}} and an optimal nonlinear associative memory is presented. I. INTRODUCTION <b>ASSOCIATIVE</b> <b>recall</b> (memory) may be understood as an operation or transformation with a set of input signals or other items considered as keys, and some sort of outcome which constitutes the recall. In the simplest form both the key and the recall are spatial patterns of simultaneous signal values. <b>Associative</b> <b>recall</b> can be realized in electrical networks, optical filters, and as realizations of neural networks. The most [...] ...|$|E
40|$|This paper {{argues that}} {{immunological}} memory {{is in the}} same class of associative memories as Kanerva's Sparse Distributed Memory, Albus's Cerebellar Model Arithmetic Computer, and Marr's Theory of the Cerebellar Cortex. This class of memories derives its associative and robust nature from a sparse sampling of a huge input space by recognition units (B and T cells in the immune system) and a distribution of the memory among many independent units (B and T cells in the memory population in the immune system). Keywords: Immunological Memory, Associative Memory, Cross-Reactive Memory, Original Antigenic Sin, Sparse Distributed Memory. 1 Introduction Cowpox vaccination, used to protect humans from smallpox, was the first known deliberate use of <b>associative</b> <b>recall</b> in the immune response (Jenner, 1798). The modern investigation of <b>associative</b> <b>recall</b> began with the observation that antibodies induced during an influenza infection often have greater affinity to prior strains of influenza than t [...] ...|$|E
40|$|A new {{associative}} memory model is proposed {{on the basis}} of a nonlinear transformation in the Fourier domain of the data. The Moore-Penrose pseudoinverse is used to compute the optimal leastsquares solution. Computer simulations, using onedimensional speech and two-dimensional images, are presented. Comparison of the new model with the classical optimal linear {{associative memory}} and an optimal nonlinear (polynomial) memory is presented. 1 Introduction <b>Associative</b> <b>recall</b> (memory) may be understood as an operation or transformation with a set of input signals or other items considered as keys, and some sort of outcome which constitutes the recall. The most characteristic property of <b>associative</b> <b>recall</b> is the following: if the totality of the input signal, is stored as such in a memory, it is retrievable by a part of the key. A recall is autoassociative if the pattern is retrievable on a basis a fragment of the key. In the heteroassociative mode of operation, an outcome which structural [...] ...|$|E
40|$|We apply genetic {{algorithms}} to fully connected Hopfield associative memory networks. Previously, we {{reported that a}} genetic algorithm can evolve networks with random synaptic weights to store some number of patterns by pruning some of its synapses. The associative memory capacity obtained in that experiment was around 16 % {{of the number of}} neurons. However the size of basin of attraction was rather small compared to the original Hebb-rule associative memory. In this paper, we present {{a new version of the}} previous method trying to control the basin size. As far as we know, this is the first attempt to address the size of basin of attraction of associative memory by evolutionary processes. 1 Introduction <b>Associative</b> memory <b>recall</b> is a process in which an incomplete or a noisy input of a stored pattern results in the retrieval of its complete pattern. The error-correcting capability is due to their distributed storage of patterns among neurons, though they have a limited capacity. We ar [...] ...|$|R
40|$|Abstract: Most {{models of}} Bidirectional {{associative}} memories intend {{to achieve that}} all trained pattern correspond to stable states; however, {{this has not been}} possible. Also, none of the former models has been able to recall all the trained patterns. In this work we introduce a new model of bidirectional associative memory which is not iterative and has no stability problems. It is based on the Alpha-Beta associative memories. This model allows perfect recall of all trained patterns, with no ambiguity and no conditions. An example of fingerprint recognition is presented. Keywords: Bidirectional associative memories, Alpha-Beta <b>associative</b> memories, perfect <b>recall...</b>|$|R
40|$|Bidirectional Associative Memories (BAM) {{based on}} Kosko’s model are {{implemented}} through iterative algorithms and present stability problems. Also, these models {{along with other}} models based on different methods, {{have not been able}} to perfectly recall all trained patterns. In this paper we present an English-Spanish / Spanish-English translator based on a new BAM model denominated Alpha-Beta BAM, whose process is non iterative and does not require to find stable states. The translator recalls the whole set of learned patterns, even when the presented word is incomplete. Key words: Bidirectional Associative Memories, Alpha-Beta <b>Associative</b> Memories, perfect <b>recall,</b> transaltor...|$|R
40|$|We {{extend the}} {{capabilities}} of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and <b>associative</b> <b>recall</b> from input and output examples. ...|$|E
40|$|To {{investigate}} the neural correlates of episodic recollection the ERP correlates of memory for new associations (recently studied novel word pairs) were investigated using two tasks, associative recognition and <b>associative</b> <b>recall.</b> For the recognition task subjects discriminated old from new word pairs and, for pairs judged old, reported whether the pairs were intact or recombined (compared to at study). For the recall task, subjects discriminated old from new words and, for each word judged old, reported its study associate. ERPs were recorded at test from 25 scalp electrodes, with a 1944 -ms recording epoch. In Experiment 1, the tasks were randomly interleaved. Consistent with previous findings, {{relative to the}} ERPs for correctly classified new items, the ERP correlates of successful associative recognition consisted of a sustained left parietal positivity, and two frontal positivities, one early and bilateral, the other occurring later and showing a right-sided maximum. In contrast to previous findings, successful <b>associative</b> <b>recall</b> elicited similar effects to those found for recognition. Topographic analyses revealed that the distribution of these retrieval-related ERP effects were similar across the two tasks, suggesting that the recognition and recall of associative information gives rise to activity in overlapping, if not the same, neural populations. In Experiment 2 the tasks were blocked. In contrast {{to the findings of}} Experiment 1, successful <b>associative</b> <b>recall</b> elicited left parietal and late onsetting right frontal positivities, {{in the absence of the}} early bilateral frontal positivity. This finding suggests that frontally-distributed memory-related ERP effects are both neurally and functionally dissociable. Specifically, we argue that the functional significance of the early frontally distributed ERP effect cannot be accounted for by the 'post-retrieval processing' hypothesis that is taken to account for the late right frontal effect, suggesting that episodic recollection itself is neither neurally nor functionally homogenous...|$|E
40|$|In {{this paper}} we {{describe}} the VLSI design and testing of a high capacity associative memory which we call the exponential correlation associative memory (ECAM). The prototype 3 µ-CMOS programmable chip is capable of storing 32 memory patterns of 24 bits each. The high capacity of the ECAM {{is partly due to}} the use of special exponentiation neurons, which are implemented via sub-threshold MOS transistors in this design. The prototype chip is capable of performing one <b>associative</b> <b>recall</b> in 3 µS...|$|E
40|$|Noise, {{traditionally}} {{defined as}} an unwanted signal or disturbance, {{has been shown to}} play an important constructive role in many information processing systems and algorithms. This noise enhancement has been observed and employed in many physical, biological, and engineered systems. Indeed stochastic facilitation (SF) has been found critical for certain biological information functions such as detection of weak, subthreshold stimuli or suprathreshold signals through both experimental verification and analytical model simulations. In this paper, we present a systematic noise-enhanced information processing framework to analyze and optimize the performance of engineered systems. System performance is evaluated {{not only in terms of}} signal-to-noise ratio but also in terms of other more relevant metrics such as probability of error for signal detection or mean square error for parameter estimation. As an important new instance of SF, we also discuss the constructive effect of noise in <b>associative</b> memory <b>recall.</b> Potential enhancement of image processing systems via the addition of noise is discussed with important applications in biomedical image enhancement, image denoising, and classification...|$|R
40|$|Abstract: Key-binding {{mechanisms}} {{allow to}} use one’s biometric features as a univer-sal digital key without having them ever stored anywhere. Security, {{ease of use}} and privacy concerns are addressed in one stroke. The best known proposal for such a mechanism, the Fingerprint Vault, treats biometric data as projections of a polynomial encoding the key. Its security {{is based on the}} difficulty of polynomial reconstruction. Here I propose a new key-binding mechanism based on <b>associative</b> pattern <b>recall</b> and making use of a totally different security principle, that of the difficulty of energy opti-mization of spin glasses. The idea is to exploit the mixed ferromagnetic and spin glass phase of the Hopfield neural network to encode the key as a local minimum config-uration of the energy functional, ”lost ” amidst the exponentially growing number of valleys and minima representing the spin glass. The correct fingerprint will be able to retrieve the key by dynamical evolution to the nearest attractor. Other fingerprints will be driven far away from the key. Known vulnerabilities of the Fingerprint Vault are eliminated by this new security principle...|$|R
40|$|The postsynaptic {{potentials}} of pyramidal neurons have a non-Gaussian {{amplitude distribution}} {{with a heavy}} tail in both hippocampus and neocortex. Such distributions of synaptic weights were recently shown to generate spontaneous internal noise optimal for spike propagation in recurrent cortical circuits. However, whether this internal noise generation by heavy-tailed weight distributions is possible for and beneficial to other computational functions remains unknown. To clarify this point, we construct an associative memory network model of spiking neurons that stores multiple memory patterns in a connection matrix with a lognormal weight distribution. In associative memory networks, non-retrieved memory patterns generate a cross-talk noise that severely disturbs memory recall. We demonstrate that neurons encoding a retrieved memory pattern and those encoding non-retrieved memory patterns have different subthreshold membrane-potential distributions in our model. Consequently, the probability of responding to inputs at strong synapses increases for the encoding neurons, whereas it decreases for the non-encoding neurons. Our results imply that heavy-tailed distributions of connection weights can generate noise useful for <b>associative</b> memory <b>recall...</b>|$|R
