15|10000|Public
40|$|An {{accurate}} {{method for}} determining the effective stress component of the flow stress in two-phase materials is described. Values of the effective stress, reproducible to within ± 1 Nmm, are obtained by using {{a variation of the}} incremental unloading technique in which the primary variable, strain, is monitored <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> during the test. This method is additionally favoured because of the fewer theoretical assumptions implicit {{in the use of the}} incremental unloading technique...|$|E
40|$|The {{accidental}} {{contamination of}} Salmonella in raw and processed foods {{is a major}} problem for the food and feed industries worldwide. Rapid detection methods for monitoring and identification are required to solve the health and safety problems related to these pathogenic bacteria. Current detection methods require extensive sample preparation and prolonged assay procedures, thus, this research project focused on developing rapid methods which are capable of sensing these microorganisms <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> level. Cont/d...|$|E
40|$|A very {{important}} research {{effort has been}} made in the last decade in the field of high precision measurement with laser instrumentation. The development of a space borne gradiometer operating <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> level using laser measurement of the distance between proof mass over a short base line of order one meter is discussed. Two aspects of laser technology make it a promising tool for gradiometry measurements: quantum limited accuracy and absolute distance measurements. The quantum limit associated with laser instrumentation was formulated. The relevant quantum and classical sources of errors in laser measurements were reviewed and corresponding laser performance requirements for gradient measurements were evaluated. Some mechanical quality factor measurements were made on simple resonant monocrystalline silicon suspensions. It was discovered that the use of zero derivative restoring forces to position the gradiometer test masses will dramatically reduce the gradiometer temperature control requirements. A laser beam side injection scheme was discovered which permits rejection of common mode accelerations. These concepts are briefly discussed...|$|E
5000|$|New {{approaches}} {{retrieve data}} such as [...] content of past atmospheres from fossil leaf stomata and isotope composition, measuring cellular CO2 concentrations. A 2014 study {{was able to}} use the carbon-13 isotope ratios to estimate the CO2 amounts of the past 400 million years, the findings hint <b>at</b> <b>a</b> <b>higher</b> climate <b>sensitivity</b> to CO2 concentrations.|$|R
50|$|WISE {{surveyed}} {{the sky in}} the infrared band <b>at</b> <b>a</b> very <b>high</b> <b>sensitivity.</b> Asteroids that absorb solar radiation can be observed through the infrared band. It was used to detect NEOs, in addition to performing its science goals. It is projected that WISE could detect 400 NEOs (roughly {{two percent of the}} estimated NEO population of interest) within the one-year mission.|$|R
40|$|Molecular {{recognition}} {{is at the}} centre of many areas of chemistry. Examples are analytical chemistry (analyte-sensor), catalysis (transition state-catalyst), medicinal chemistry (drug-biotarget) and advanced materials chemistry (building block A-building block B). Methodology that allows the rapid and precise detection of molecular recognition events is essential in all these fields. Traditionally, molecular recognition has been studied based on a rational design approach involving many iterative optimisation loops, which makes it an energy- and time consuming process. Additionally, it requires detailed knowledge about the target and the recognition process itself, information which is not always available. Currently, combinatorial methods are increasingly being used for detecting molecular recognition events, allowing the simultaneous screening of {{a vast amount of}} chemical compounds enabling a much larger part of chemical space to be explored. Dynamic covalent capture extends on the combinatorial approach for detecting molecular recognition events, but <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> level compared to conventional methodologies and with the novelty of self-selection by the target. The essential point of dynamic covalent capture is that a molecular recognition event is followed by the formation of a reversible covalent bond between the two molecule...|$|R
40|$|The Equivalence Principle is {{accepted}} {{as one of}} the most fundamental principles in modern Physics. However, theories towards the unification of the four forces typically predict violations of this principle. Testing it <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> is expected to make a breakthrough in the current understanding of Physics. A space-based project, STEP (Satellite Test of the Equivalence Principle), aims at testing the principle to the level of 10 ^− 18. This corresponds to an improvement of the current limits, established by ground-based experiments, by approximately five orders of magnitudes. To achieve the sensitivity, imperfections in STEP test masses, such as density inhomogeneity and thermal distortion, could be a problem. This thesis presents preliminary work on the verification of STEP test masses. We have measured density inhomogeneities in materials intended to be used as STEP test masses (beryllium and niobium). In addition, we have developed a device to measure differential thermal expansion of samples that cannot be machined, by using a capacitive sensing method. It is shown that the device has a precision of approximately 0. 3...|$|E
40|$|AbstractBACKGROUND: Conventional {{frequency}} quantitative ultrasound {{in conjunction}} with textural analysis techniques was investigated to monitor noninvasively the effects of cancer therapies in an in vivo preclinical model. METHODS: Conventional low-frequency (∼ 7 MHz) and high-frequency (∼ 20 MHz) ultrasound was used with spectral analysis, coupled with textural analysis on spectral parametric maps, obtained from xenograft tumor-bearing animals (n = 20) treated with chemotherapy to extract noninvasive biomarkers of treatment response. RESULTS: Results indicated statistically significant differences in quantitative ultrasound-based biomarkers in both low- and high-frequency ranges between untreated and treated tumors 12 to 24 hours after treatment. Results of regression analysis indicated {{a high level of}} correlation between quantitative ultrasound-based biomarkers and tumor cell death estimates from histologic analysis. Applying textural characterization to the spectral parametric maps resulted in an even stronger correlation (r 2 = 0. 97). CONCLUSION: The results obtained in this research demonstrate that quantitative ultrasound at a clinically relevant frequency can monitor tissue changes in vivo in response to cancer treatment administration. Using higher order textural information extracted from quantitative ultrasound spectral parametric maps provides more information <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> related to tumor cell death...|$|E
40|$|Background: In heavy-ion {{collisions}} at intermediate energies {{the production}} of light charged particles and intermediate mass fragments (IMFs) is due to different reaction mechanisms and different time scales, ranging from fast dynamical processes to statistical emission from the equilibrated system. Purpose: We compare the IMF statistical and dynamical emission probabilities in collisions of a neutron-rich ^{ 124 }Sn + ^{ 64 }Ni system and a neutron-poor ^{ 112 }Sn + ^{ 58 }Ni system at the laboratory energy of 35 A MeV. Method: The IMFs production mechanism in semiperipheral reactions has been investigated in our previous works. In this paper, the analysis is expanded for {{the same set of}} data and production cross sections have been evaluated for dynamical and statistical emission in a coherent way for light and heavy fragments. Results: The data analysis has evidenced a strong competition between dynamical and statistical emission mechanisms. Probability of the dynamical emission of IMFs is strongly influenced by the (N/Z) ratio of the colliding system. Conclusions: It is demonstrated that the statistical emission is equally probable for the two systems, while the dynamical emission is enhanced for the neutron-rich system, especially for heavy fragments (Z≥ 6). The observed effect points <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> of the prompt-dynamical emission to the (N/Z) ratio of a given system...|$|E
40|$|Current {{homology}} modeling {{methods for}} predicting protein–protein interactions (PPIs) have difficulty in the “twilight zone” (< 40 %) of sequence identities. Threading methods extend coverage {{further into the}} twilight zone by aligning primary sequences {{for a pair of}} proteins to a best-fit template complex to predict an entire three-dimensional structure. We introduce a threading approach, iWRAP, which focuses only on the protein interface. Our approach combines a novel linear programming formulation for interface alignment with a boosting classifier for interaction prediction. We demonstrate its efficacy on SCOPPI, a classification of PPIs in the Protein Databank, and on the entire yeast genome. iWRAP provides significantly improved prediction of PPIs and their interfaces in stringent cross-validation on SCOPPI. Furthermore, by combining our predictions with a full-complex threader, we achieve a coverage of 13 % for the yeast PPIs, which is close to a 50 % increase over previous methods <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity.</b> <b>As</b> an application, we effectively combine iWRAP with genomic data to identify novel cancer-related genes involved in chromatin remodeling, nucleosome organization, and ribonuclear complex assembly. iWRAP is available at [URL] Institutes of Health (U. S.) (Grant 1 R 01 GM 081871...|$|R
40|$|Though {{the areas}} of secure {{multicast}} group architecture, key distribution, and sender authentication are under scrutiny, one topic {{that has not been}} explored is how to integrate these with multilevel security. Multilevel security is the ability to distinguish subjects according to classification levels, which determines to what degree they can access confidential objects. In the case of groups, this means that some members can exchange messages <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> level than others. The Bell-La Padula model [1] outlines the rules of these multilevel accesses. In multicast groups that employ multilevel security, some of these rules are not desirable so a modified set of rules is developed in this paper and is termed differential security. Also, this paper proposes three methods to set up a differentially secure multicast group: (1) Naive approach, (2) Multiple tree differential security (DiffSec) approach, and (3) Single DiffSec tree approach. Our simulation studies show that both Single and Multiple DiffSec tree approaches offer similar performance in terms of bandwidth consumption, which is significantly better than that of the Naive approach. We also discuss the suitability of the schemes taking into account scalability and implementation issues...|$|R
40|$|AbstractThe {{addition}} of a single N-acetylglucosamine residue O-linked to serine and threonine residues of nuclear and cytoplasmic proteins is a widespread modification throughout all eukaryotes. The conventional method for detecting and locating sites of modification is a multi-step radioactivity-based protocol. In this paper we show that using quadrupole time-of-flight (Q-TOF) mass spectrometry, modification sites can be identified <b>at</b> <b>a</b> significantly <b>higher</b> <b>sensitivity</b> than previous approaches. This is the first demonstration that sites of O-GlcNAcylation can be identified directly using mass spectrometry...|$|R
40|$|International audienceBackground: In heavy-ion {{collisions}} at intermediate energies {{the production}} of light charged particles andintermediate mass fragments (IMFs) is due to different reaction mechanisms and different time scales, rangingfrom fast dynamical processes to statistical emission from the equilibrated system. Purpose: We compare the IMF statistical and dynamical emission probabilities in collisions of a neutron-rich$^{ 124 }$Sn + $^{ 64 }$Ni system and a neutron-poor $^{ 112 }$Sn + $^{ 58 }$Ni system at the laboratory energy of 35 A MeV. Method: The IMFs production mechanism in semiperipheral reactions has been investigated in our previousworks. In this paper, the analysis is expanded for {{the same set of}} data and production cross sections have beenevaluated for dynamical and statistical emission in a coherent way for light and heavy fragments. Results: The data analysis has evidenced a strong competition between dynamical and statistical emissionmechanisms. Probability of the dynamical emission of IMFs is strongly influenced by the (N/Z) ratio of thecolliding system. Conclusions: It is demonstrated that the statistical emission is equally probable for the two systems, whilethe dynamical emission is enhanced for the neutron-rich system, especially for heavy fragments (Z> 6). Theobserved effect points <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> of the prompt-dynamical emission to the (N/Z) ratio of a givensystem...|$|E
40|$|Because of {{repeated}} claims that A-DNA cannot exist without aggregation or condensation, {{the state of}} DNA restriction fragments with 84 - 859 bp has been analyzed in aqueous solutions upon reduction of the water activity. Rotational diffusion times tau (d) measured by electric dichroism at different water activities with a wide variation of viscosities are normalized to values tau (c) at the viscosity of water, which indicate DNA structures <b>at</b> <b>a</b> <b>high</b> <b>sensitivity.</b> For short helices (chain lengths [Formula: see text]≤persistence length p), cooperative formation of A-DNA is reflected by the expected reduction of the hydrodynamic length; {{the transition to the}} A-form is without aggregation or condensation upon addition of ethanol at monovalent salt ≤ 1 mM. The aggregation boundary, indicated by a strong increase of tau (c), is shifted to higher monovalent salt (≥ 4 mM) when ethanol is replaced by trifluoroethanol. The BA transition is not indicated anymore by a cooperative change of tau (c) for [Formula: see text]p; tau (c) values for these long chains decrease upon reduction of the water activity continuously over the full range, including the BA transition interval. This suggests a non-cooperative BC transition, which induces DNA curvature. The resulting wide distribution of global structures hides changes of local length during the BA transition. Free A-DNA without aggregation/condensation is found at low-salt concentrations where aggregation is inhibited and/or very slow. In an intermediate range of solvent conditions, where the A-form starts to aggregate, a time window remains {{that can be used for}} analysis of free A-DNA in a quasi-equilibrium state...|$|E
40|$|In {{view of the}} {{evolving}} indications for device therapy in atrial arrhythmia the accurate detection of high rate atrial events is a necessity. In a sheep model of atrial fibrillation (AF) we observed a contradictory behavior of the Thera DR pacemaker. The pacemakers were programmed to deliver burst pacing on detection of sinus rhythm (SR). Paradoxically, progressively more inappropriate bursts were delivered during AF, at a higher sensitivity. This implied that the pacemaker interpreted AF as SR. We assessed the atrial detection of the Thera DR, Diamond, Saphir, and Marathon pacemakers during AF, in a sheep model and in vitro using a waveform generator. By counting the annotated atrial-sensed events reported by the pacemaker we charted {{the behavior of the}} pacemakers at different sensitivities. At a higher sensitivity both the Thera DR and the Diamond paradoxically reported fewer atrial events during AF. This behavior led to inappropriate mode switching and incorrect diagnostic data collection. It could be reproduced in vitro. The Marathon did not show this paradoxical undersensing in vivo or in vitro. This paradoxical undersensing <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> is dependent on the amplitude of the input signal and could not be explained by an overlap of programmable timing parameters. It is caused by ringing of the atrial sensing amplifier. At high atrial rates and a high sensitivity the ringing of the repetitive atrial signal input results in blanking of the atrial amplifier. This observation may be relevant in the device management of patients with paroxysmal AF. status: publishe...|$|E
40|$|The next {{generation}} of particle edm searches will be <b>at</b> such <b>a</b> <b>high</b> <b>sensitivity</b> {{that it will be}} possible for the results to be contaminated by a systematic error resulting from the interaction of the motional (E x v/c) magnetic field with stray field gradients. In this paper we extend previous work to present an analytic form for the frequency shift {{in the case of a}} rectangular storage vessel and discuss the implications of the result for the neutron edm experiment which will be installed at the SNS (Spallation Neutron Source) by the LANL collaborationComment: 10 pages, 7 figures New version: improved description of physicalidea, updated author list, replaced defective figure file...|$|R
40|$|This work is {{supported}} by the European Union (FPVII (2007 - 2013) under Grant Agreement No. 318287 LANDAUER) An AlGaAs/GaAs double barrier resonant tunneling diode (RTD) with a nearby lattice-matched GaInNAs absorption layer was integrated into an optical cavity consisting of five and seven GaAs/AlAs layers to demonstrate cavity enhanced photodetection at the telecommunication wavelength 1. 3 [*]μm. The samples were grown by molecular beam epitaxy and RTD-mesas with ring-shaped contacts were fabricated. Electrical and optical properties were investigated at room temperature. The detector shows maximum photocurrent for the optical resonance <b>at</b> <b>a</b> wavelength of 1. 29 [*]μm. <b>At</b> resonance <b>a</b> <b>high</b> <b>sensitivity</b> of 3. 1 × 10 4 A/W and a response up to several pA per photon at room temperature were found. Publisher PDFPeer reviewe...|$|R
40|$|Background: Gastric Emptying Scintigraphy (GES) at {{intervals}} over 4 {{hours after a}} standardized radio-labeled meal is commonly regarded as {{the gold standard for}} diagnosing gastroparesis. The objectives of this study were: 1) to investigate the best time point and the best combination of multiple time points for diagnosing gastroparesis with repeated GES measures, and 2) to contrast and cross-validate Fisher’s Linear Discriminant Analysis (LDA), a rank based Distribution Free (DF) approach, and the Classification And Regression Tree (CART) model. Methods: A total of 320 patients with GES measures at 1, 2, 3, and 4 hour (h) after a standard meal using a standardized method were retrospectively collected. Area under the Receiver Operating Characteristic (ROC) curve and the rate of false classification through jackknife cross-validation were used for model comparison. Results: Due to strong correlation and an abnormality in data distribution, no substantial improvement in diagnostic power was found with the best linear combination by LDA approach even with data transformation. With DF method, the linear combination of 4 -h and 3 -h increased the Area Under the Curve (AUC) and decreased the number of false classifications (0. 87; 15. 0 %) over individual time points (0. 83, 0. 82; 15. 6 %, 25. 3 %, for 4 -h and 3 -h, respectively) <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> level (sensitivity = 0. 9). The CART model using 4 hourly GES measurements along with patient’s age was the most accurate diagnostic tool (AUC = 0. 88, false classification = 13. 8 %). Patient...|$|R
40|$|Abstract Background High-density {{oligonucleotide}} arrays {{are effective}} tools for genotyping numerous loci simultaneously. In small genome species (genome size: Results We compared the single feature polymorphism (SFP) detection performance of whole-genome and transcript hybridizations using the Affymetrix GeneChip ® Rice Genome Array, using the rice cultivars with full genome sequence, japonica cultivar Nipponbare and indica cultivar 93 - 11. Both genomes were surveyed for all probe target sequences. Only completely matched 25 -mer single copy probes of the Nipponbare genome were extracted, and SFPs {{between them and}} 93 - 11 sequences were predicted. We investigated optimum conditions for SFP detection in both whole genome and transcript hybridization using differences between perfect match and mismatch probe intensities of non-polymorphic targets, assuming that these differences are representative of those between mismatch and perfect targets. Several statistical methods of SFP detection by whole-genome hybridization were compared under the optimized conditions. Causes of false positives and negatives in SFP detection in both types of hybridization were investigated. Conclusions The optimizations allowed a more than 20 % increase in true SFP detection in whole-genome hybridization and a large improvement of SFP detection performance in transcript hybridization. Significance analysis of the microarray for log-transformed raw intensities of PM probes gave the best performance in whole genome hybridization, and 22, 936 true SFPs were detected with 23. 58 % false positives by whole genome hybridization. For transcript hybridization, stable SFP detection was achieved for highly expressed genes, and about 3, 500 SFPs were detected <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> (> 50 %) in both shoot and young panicle transcripts. High SFP detection performances of both genome and transcript hybridizations indicated that microarrays of a complex genome (e. g., of Oryza sativa) can be effectively utilized for whole genome genotyping to conduct mutant mapping and analysis of quantitative traits such as gene expression levels. </p...|$|E
40|$|In {{the last}} decades low-field {{magnetic}} susceptibility measurements have become an increasingly attractive method for geological studies which use the scalar values (bulk susceptibility) {{as well as the}} directional information, the anisotropy of magnetic susceptibility (AMS). Because of the potential for detecting weak fabric anisotropies, AMS has become a routine method for assessing flow directions in magmatic bodies. Sources of AMS in ferrimagnetic basaltic rocks are mainly titanomagnetites. After Jackson et al. (1998) and de Wall (2000), the magnetic susceptibility (MS) of titanomagnetite varies strongly with mineral composition and is in the low-field range strongly depending on the field amplitude of the inducing magnetic field. Here we present a systematic study to record the effects of field dependence on AMS of dykes, sills and lava flows. Variation in MS characteristics have been found indicative for lava emplacement and flow dynamics (Cañón-Tapia et al. 1997, Cañón-Tapia & Pinkerton 2000). The contribution of the effect of field dependence on MS and AMS in titanomagnetite-bearing volcanic rocks needs to be assessed for a reliable interpretation of AMS variations. The key study has been carried out at the Ság-hegy volcanic complex in the Little Hungarian Plain. It is composed of a phreatomagmatic tuff ring, formed during the pliocene-miocene period. After meteoric water supply ended, the phreatomagmatic eruptive style changed into an effusive behaviour and the tephra ring was filled with a lava lake and a dyke-sill complex transected the pyroclastic successions. We report AMS characteristics of sills, dykes and lavas from the lake interior and outflowing lava deposits. Furthermore we discriminated samples that represent the transition from dykes to sills and from intrusive (dyke) to effusive (lava flow) emplacement, respectively. The MS has been measured by a KLY- 4 S kappabridge (AGICO, Brno) which allows a record of the AMS <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> and in various field amplitudes (2 to 450 A/m) ...|$|E
40|$|NSF-China [41330634, 41374016]The {{discrimination}} of {{oil spills}} and look-alike phenomena (e. g., low wind area, wind front area and natural slicks) on Synthetic Aperture Radar (SAR) images {{is a crucial}} task in marine oil spill detection. Many classification techniques can be employed for this purpose. In {{order to make the}} best use of the large variety of statistical and machine learning classification methods, it is necessary to assess their performance differences and make recommendations for classifier selection and improvement. The objective {{of this paper is to}} compare different classification techniques for oil-spill detection in RADARSAT- 1 imagery. The data of this study consists of 15 features of 192 oil spills and look-alikes identified by Canadian Ice Service between 2004 and 2008 off Canada's east and west coastal areas. The studied classifiers include the Support Vector Machine (SVM), Artificial Neural Network (ANN), tree-based ensemble classifiers (bagging, bundling and boosting), Generalized Additive Model (GAM) and Penalized Linear Discriminant Analysis (PLDA). Two performance measures, the specificity at fixed sensitivity (80 %) and the area under the Receiver Operating Characteristic (ROC) curve (AUC), were estimated using cross-validation to evaluate the performance of classifiers <b>at</b> <b>a</b> <b>high</b> <b>sensitivity.</b> Overall, the bundling technique which achieved a median specificity of 90. 7 %, at sensitivity of 80 %, significantly outperformed the second best (i. e. bagging) by 1. 5 percentage points, and the worst (i. e. ANN) by 15 percentage points. The median values of AUC measure indicated consistent results. Bundling and bagging achieved comparable median AUC values of about 92 %, followed by GAM and PLDA, with ANN yielding the smallest. Most classifiers (SVM, bundling and especially PLDA and ANN) performed significantly better on datasets pre-processed by log-transformation and standardization than on the original dataset. These results demonstrate the importance and benefit of selecting the optimal classifiers for oil spill classification, and configuring the classifiers by proper feature construction techniques. (C) 2013 Elsevier Inc. All rights reserved...|$|E
40|$|IntroductionRecently, {{analysis}} of DNA methylation of the SHOX 2 locus {{was shown to}} reliably identify lung cancer in bronchial aspirates of patients with disease. As a plasma-based assay would expand the possible applications of the SHOX 2 biomarker, this study aimed to develop a modified SHOX 2 assay {{for use in a}} blood-based test and to analyze the performance of this optimized SHOX 2 methylation assay in plasma. MethodsQuantitative real-time polymerase chain reaction was used to analyze DNA methylation of SHOX 2 in plasma samples from 411 individuals. A training study (20 stage IV patients with lung cancer and 20 controls) was performed to show the feasibility of detecting the SHOX 2 biomarker in blood and to determine a methylation cutoff for patient classification. The resulting cutoff was verified in a testing study composed of 371 plasma samples from patients with lung cancer and controls. ResultsDNA methylation of SHOX 2 {{could be used as a}} biomarker to distinguish between malignant lung disease and controls <b>at</b> <b>a</b> sensitivity of 60 % (95 % confidence interval: 53 – 67 %) and a specificity of 90 % (95 % confidence interval: 84 – 94 %). Cancer in patients with stages II (72 %), III (55 %), and IV (83 %) was detected <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> when compared with stage I patients. Small cell lung cancer (80 %) and squamous cell carcinoma (63 %) were detected at the <b>highest</b> <b>sensitivity</b> when compared with adenocarcinomas. ConclusionsSHOX 2 DNA methylation is a biomarker for detecting the presence of malignant lung disease in blood plasma from patients with lung cancer...|$|R
40|$|Abstract Background With {{the advent}} of {{high-throughput}} proteomic experiments such as arrays of purified proteins comes the need to analyse sets of proteins as an ensemble, {{as opposed to the}} traditional one-protein-at-a-time approach. Although there are several publicly available tools that facilitate the analysis of protein sets, they do not display integrated results in an easily-interpreted image or do not allow the user to specify the proteins to be analysed. Results We developed a novel computational approach to analyse the annotation of sets of molecules. As proof of principle, we analysed two sets of proteins identified in published protein array screens. The distance between any two proteins was measured as the graph similarity between their Gene Ontology (GO) annotations. These distances were then clustered to highlight subsets of proteins sharing related GO annotation. In the first set of proteins found to bind small molecule inhibitors of rapamycin, we identified three subsets containing four or five proteins each that may help to elucidate how rapamycin affects cell growth whereas the original authors chose only one novel protein from the array results for further study. In a set of phosphoinositide-binding proteins, we identified subsets of proteins associated with different intracellular structures that were not highlighted by the analysis performed in the original publication. Conclusion By determining the distances between annotations, our methodology reveals trends and enrichment of proteins of particular functions within high-throughput datasets <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> than perusal of end-point annotations. In an era of increasingly complex datasets, such tools will help in the formulation of new, testable hypotheses from high-throughput experimental data. </p...|$|R
40|$|Though {{the areas}} of secure {{multicast}} group architecture, key distribution and sender authentication are under scrutiny, one topic {{that has not been}} explored is how to integrate these with multi-level security. Multi-level security is the ability to distinguish subjects according to classification levels, which determines to what degree they can access confidential objects. In the case of groups, this means that some members can exchange messages <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> level than others. The Bell-La Padula model [BL 76] outlines the rules of these multi-level accesses. In multicast groups that employ multi-level security, some of these rules are not desirable so a modified set of rules is developed in this paper and is termed differential security. Also, this paper proposes three methods to set up a differentially secure multicast group: (i) Naïve approach, (ii) multiple tree differential security (DiffSec) approach, and (iii) single DiffSec tree approach. In order to evaluate the performances (in {{terms of the number of}} links used per packet transmitted) of these approaches, extensive simulation experiments were conducted by varying the network connectivity and group size for both uniform and non-uniform membership distribution across security levels. Our studies show that the multiple tree and single DiffSec tree approaches perform much better than the Naïve approach. While the multiple tree approach could be implemented using current technology, this scheme consumes many times more addresses and network resources than the single DiffSec tree approach. From our studies, we conclude that the single DiffSec tree is a viable option for supporting multi-level security as it maximizes the resource utilization and is also scalable. 1...|$|R
40|$|We {{examined}} {{the absorption of}} Zn, Cr and Mn into Brassica campestris L. cultivated in Brown Lowland soil (BL soil), Andosol and Brown Forest soil (BF soil) after five years 2 ̆ 7 applications of chemical fertilizer (CHF), cattle farmyard manure (CFM) and pig farmyard manure (PFM). The effects of animal manure applications on the absorption of Zn, Cr and Mn by B. campestris were estimated by comparing the result in the CFM- and PFM-applied plots with that in CHF-applied plot. Zinc, Cr and Mn in B. campestris were measured by the instrumental neutron activation analysis (INAA) to perform total simultaneous analysis <b>at</b> <b>a</b> <b>high</b> <b>sensitivity.</b> The results obtained {{in this study were}} summarized as follows: 1) For Zn and Cr in B. campestris, the results obtained from INAA were compared with a nitric acid-digestion method (NADM). Results obtained from the INAA and NADM were fully in agreement for Zn, but not in agreement for Cr. The Cr concentration in B. campestris determined using NADM was only 13 - 2727777671520 f that determined by INAA. From the results above, it is suggested that INAA could be used to analyze the correct amounts of trace elements removed by B. campestris. 2) CFM application increased Cr concentration in B. campestris cultivated in BL soil and Zn concentration cultivated in BF soil, while decreased Cr and Mn concentrations cultivated in Andsol and Cr and Mn cultivated in BF soil. PFM application led to a significant increase on the Zn concentration in B. campestris cultivated in BL soil, Zn and Mn cultivated in Andsol and Zn, Cr and Mn cultivated in BF soil. In BL soil, PFM application tended to decrease the Cr concentration in B. campestris. For Zn, Cr and Mn in CFM-applied plot of BF soil, Cr and Mn in CFM-applied plot of Andsol and Cr in CFM- and PFM-applied plots of BL soil, the change of availability by B. campestris of Zn, Cr and Mn in animal manure may provide an important influence on the uptakes of these elements by B. campestris. 3) For Zn, Cr and Mn, the removed amounts by cropping were smaller than their amounts added by the animal manure. Since the 5 -year application of animal manure did not result in a considerable increase in Cr and Mn concentrations in soils, we have to continue soil survey to collect data on the soil accumulation of Cr and Mn contained in animal manure. 牛ふんコンポスト,豚ぷんコンポストおよび化学肥料を各種畑土壌(褐色低地土,多腐植質黒ボク土,褐色森林土) にそれぞれ 5 年間連用した場合に栽培されたコマツナについて地上部の亜鉛,クロム,マンガンの含有率について調べた. 亜鉛,クロム,マンガンの測定には,高感度な全量同時分析を行うために機器中性子放射化分析法(INAA) を用いた. その結果,以下の知見が得られた. 1) INAAによるコマツナ地上部の亜鉛とクロムの含有率を硝酸分解法による測定値と比較した結果,亜鉛では硝酸分解法による測定値はINAAによる測定値とほぼ同じであったが,硝酸分解法によるコマツナ地上部のクロム含有率はINAAの場合の 13 〜 27...|$|E
40|$|For years, E. faecium only {{sporadically}} caused {{opportunistic infections}} in humans {{and was considered}} a relatively harmless commensal. In the last two decades, however, a specific polyclonal E. faecium subpopulation has rapidly become a prominent cause of nosocomial infections, which are often difficult to treat due to acquisition of resistance traits against multiple antimicrobials. These so-called hospital-derived E. faecium clones, that are characterized by ampicillin resistance, {{are responsible for the}} vast majority of hospital outbreaks and infections worldwide and are enriched with over a hundred specific genes, including (putative) virulence genes. The studies described in this thesis focus on the different forces that drive the epidemic rise of hospital-derived E. faecium. These studies demonstrated that on hospital wards where ampicillin-resistant E. faecium (ARE) colonization is endemic, ARE epidemiology is characterized by high admission, high patient-to-patient transmission and high environmental contamination rates and, as a consequence, by high infection attack rates. In particular patients of whom the intestinal flora is disrupted by the use of antibiotics are at high risk of acquiring colonization when they are exposed to ARE, either via a (re) admitted carrier or via the contaminated environment. ARE acquisition typically leads to high density intestinal colonization with E. faecium, which is probably one of the factors that contribute to successful transmission of ARE to other patients, by facilitating colonization of the skin and the inanimate environment. Colonization with ARE is only sporadically present in healthy humans in the community. Among healthy dogs and cats, however, ARE colonization was found to be prevalent. Part of the canine and feline strains were genetically indistinguishable from ARE clones that are frequently observed in nosocomial infections and outbreaks. This suggests that transfer of ARE between the animal and hospital reservoir has occurred in the past and that dogs and cats may still form an occasional source for nosocomial spread of ARE. To prevent infections it is important to control the spread of ARE. Mathematical modelling provided theoretical evidence that screening and pre-emptive isolation of patients at high risk of colonization on admission can be an effective measure to reduce endemic levels of nosocomial ARE carriage, in various different situations. It is important, though, to aim <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> of the screening strategy, since this will yield the best efficacy at the lowest costs in the long term. In an intervention study in hospitalized patients, daily intake of probiotics, defined as representatives of the normal intestinal flora, to increase colonization resistance, unfortunately failed to prevent acquisition of ARE. The results of this thesis may form an important contribution to the development and design of infection control measures for ARE and similar nosocomial pathogen...|$|E
40|$|Permanent {{dielectric}} {{materials are}} {{integral to the}} fabrication of microelectronic devices and packaging. Dielectrics are used throughout devices to electrically and mechanically isolate conductive components. As such, {{they are required to}} have low electrical permittivity and robust mechanical properties. For packaging applications, dielectrics can be directly photo-definable. Dielectrics need to have excellent lithographic properties. These properties are pivotal for enabling high yield and low cost fabrication of reliable, energy efficient devices. The aim of this work was to develop new positive tone dielectrics which have improved or application-specific lithographic, mechanical, and electrical properties. To this end, several new dielectric polymers and chemistries were evaluated and characterized. Initially, it was desired to develop a positive tone, polynorbornene (PNB) dielectric that utilizes diazonaphthoquinone (DNQ) photochemistry. Cross-linking was achieved with epoxy cross-linkers during a thermal cure. Several DNQ-containing compounds were evaluated, but only one had good miscibility with PNB. The dissolution characteristics of PNB were measured with respect to polymer composition, DNQ loading, and cross-linker loading. PNB films exhibited unique dissolution properties, and these measurements allowed for an optimum formulation to be developed. A formulation with 20 pphr DNQ and 10 pphr epoxy cross-linker had sufficient inhibition in unexposed regions and fast dissolution in exposed regions. The resulting dielectric was the first positive tone, DNQ-based PNB dielectric. After achieving photo-definability, the cross-linking of the cured dielectric was evaluated by characterizing the mechanical properties. It was discovered that DNQ acted as a cross-linker in these films, and this insight was key to achieving good curing of the dielectric. Several experiments were performed to support this conclusions, and the reaction kinetics of this cross-linking reaction were evaluated. This effort produced a functional, positive tone dielectric with a sensitivity of 408 mJ cm- 2 and contrast of 2. 3. The modulus was 2. 0 to 2. 6 GPa and the dielectric constant of 3. 7 to 3. 9, depending on the curing conditions. The DNQ cross-linking results led to the investigation of other cross-linking chemistries for positive tone dielectrics. A chemically amplified (CA) photochemistry was utilized along with a Fischer esterification cross-linking reaction. Patterning and cross-linking were demonstrated with a methacrylate polymer. Successful positive tone lithography was demonstrated <b>at</b> <b>a</b> <b>high</b> <b>sensitivity</b> of 32. 4 mJ cm- 2 and contrast of 5. 2. Cross-linking was achieved at 120 °C as shown by residual stress and solubility measurements. The CA photochemistry and Fischer esterification cross-linking were also demonstrated using a PNB dielectric, which was shown to have improved lithographic properties: a sensitivity of 8. 09 mJ cm- 2 and contrast of ≥ 14. 2. Work was performed to evaluate the effect of the polymer composition on the mechanical and electrical properties. A polymer with 60 mol% hexafluoroisopropanol norbornene and 40 mol% tert-butyl ester norbornene exhibited a dielectric constant of 2. 78, which is lower than existing positive tone dielectrics. It also outperformed existing dielectrics in several other categories, including dark erosion, volume change, cure temperature, and in-plane coefficient of thermal expansion. However, a limitation of this dielectric was cracking in thick films. The final study was to improve the mechanical properties of this CA PNB dielectric specifically to enable 5 µm thick films. First, a terpolymer was tested that included a non-functional third monomer. The dielectric constant increased to 3. 48 with 24 mol% of the third monomer. Second, low molecular weight additives were used to lower the modulus. Only one of the five tested additives enabled high quality, thick films. This additive did not significantly affect the dielectric constant at low loadings. An optimized formulation was made, and processing parameters were studied. The additive decreased the lithographic properties, lowering the sensitivity to 175 mJ cm- 2 and lowering the contrast to 4. 36. In all, this work produced three functional dielectrics with positive tone photo-definability and good lithographic properties. Each dielectric can serve a variety of purposes in microelectronics packaging. Ph. D...|$|E
40|$|The {{influence}} of the Ar/O 2 gas ratio during radio frequency (RF) sputtering of the RuO 2 sensing electrode on the pH sensing performance is investigated. The developed pH sensor consists in an RF sputtered ruthenium oxide thin-film sensing electrode, in conjunction with an electroplated Ag/AgCl reference electrode. The performance and characterization of the developed pH sensors in terms of sensitivity, response time, stability, reversibility, and hysteresis are investigated. Experimental {{results show that the}} pH sensor exhibits super-Nernstian slopes in the range of 64. 33 - 73. 83 mV/pH for Ar/O 2 gas ratio between 10 / 0 - 7 / 3. In particular, the best pH sensing performance, in terms of sensitivity, response time, reversibility and hysteresis, is achieved when the Ar/O 2 gas ratio is 8 / 2, <b>at</b> which <b>a</b> <b>high</b> <b>sensitivity,</b> <b>a</b> low hysteresis and a short response time are attained simultaneously...|$|R
5000|$|WISE {{surveyed}} {{the sky in}} four wavelengths of the infrared band, <b>at</b> <b>a</b> very <b>high</b> <b>sensitivity.</b> Its design specified as goals that the full sky atlas of stacked images it produced have 5-sigma sensitivity limits of 120, 160, 650, and 2600 microjanskies (µJy) at 3.3, 4.7, 12, and 23 micrometers (aka microns). [...] WISE achieved at least 68, 98, 860, and 5400 µJy 5-sigma sensitivity at 3.4, 4.6, 12, and 22 micrometers for the WISE All-Sky data release. This is a factor of 1,000 times better sensitivity than the survey completed in 1983 by the IRAS satellite in the 12 and 23 micrometers (micron) bands, and a factor of 500,000 times better than the 1990s survey by the Cosmic Background Explorer (COBE) satellite at 3.3 and 4.7 micrometers. On the other hand, IRAS could also observe 60 and 100 micron wavelengths.|$|R
40|$|Normal conductor-insulator-superconductor (NIS) {{junctions}} {{promise to}} be interesting for x-ray and phonon sensing applications, in particular due to the expected self-cooling of the N electrode by the tunneling current. Such cooling would enable {{the operation of the}} active element of the sensor below the cryostat temperature and <b>at</b> <b>a</b> correspondingly <b>higher</b> <b>sensitivity.</b> It would also allow the use of MS junctions as microcoolers. At present, this cooling has not been realized in large area junctions (suitable for a number of detector applications). In this article, we discuss a detailed modeling of the heat flow in such junctions; we show how the heat flow into the normal electrode by quasiparticle back-tunneling and phonon absorption from quasiparticle pair recombination can overcompensate the cooling power. This provides a microscopic explanation of the self-heating effects we observe in our large area NIS junctions. The model suggests a number of possible solutions...|$|R
40|$|The nuclear {{shell model}} {{predicts that the}} next doubly magic shell-closure beyond " 2 " 0 " 8 Pb is <b>at</b> <b>a</b> proton number between Z = 114 and 126 and <b>at</b> <b>a</b> neutron number N = 184. The {{outstanding}} aim of experimental investigations is the exploration of this region of spherical 'superheavy elements' (SHEs). This article describes the experimental methods that allowed for the identification of elements 107 to 112 at GSI, Darmstadt. Excitation functions were measured for the one neutron evaporation channel of cold fusion reactions using lead and bismuth targets. The maximum cross section was measured at beam energies well below a fusion barrier estimated in one dimension. These studies indicate that the transfer of nucleons is an important process for the initiation of fusion. The recent efforts at JINR, Dubna, to investigate the hot fusion reaction {{for the production of}} SHEs using actinide targets are also presented. First results were obtained on the synthesis of neutron-rich isotopes of element 112 and 114. However, the most surprising result was achieved in the year 1999 at LBNL, Berkeley. In a study of the reaction " 8 " 6 Kr + " 2 " 0 " 8 Pb #-># " 2 " 9 " 4118 "*, three decay chains were measured and assigned to the superheavy nucleus " 2 " 9 " 3118. The decay data reveal that for the heaviest elements, the dominant decay mode is alpha emission, not fission. The results are discussed in the framework of theoretical models. This article also presents plans for the further development of the experimental setup and the application of new techniques. <b>At</b> <b>a</b> <b>higher</b> <b>sensitivity,</b> the exploration of the region of spherical SHEs now seems to become feasible, more than thirty years after its prediction. (orig.) 210 refs. SIGLEAvailable from TIB Hannover: RO 801 (2000 - 02) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|The {{architecture}} of the instrument system for the SETI (Search for Extraterrestrial Intelligence) program is briefly described and the development approach used to implement the operational instruments is discussed. The {{two versions of the}} instrument system include a target survey instrument to observe <b>at</b> <b>a</b> very <b>high</b> <b>sensitivity</b> <b>a</b> selected set of interesting stars that have particular a priori promise, and a sky survey instrument to observe the entire celestial sphere <b>at</b> <b>a</b> lower sensitivity. The targeted survey utilizes the 305 meter antenna at Arecibo, Puerto Rico, a 64 meter DSN antenna, and other large radio telescopes. The Arecibo instrument provides the <b>highest</b> <b>sensitivity</b> by virtue of the antenna gain. The antenna line feeds cover an instantaneous frequency range of 50 MHz (tunable over 100 MHz), while the multichannel spectrum analyzer/signal detector is capable of analyzing a frequency segment 16 MHz wide with a maximum resolution of 1 Hz. The sky survey employs a listen-only, 34 meter antenna. The SETI breadboard development is also described...|$|R
40|$|Abstract Background Gastric Emptying Scintigraphy (GES) at {{intervals}} over 4 {{hours after a}} standardized radio-labeled meal is commonly regarded as {{the gold standard for}} diagnosing gastroparesis. The objectives of this study were: 1) to investigate the best time point and the best combination of multiple time points for diagnosing gastroparesis with repeated GES measures, and 2) to contrast and cross-validate Fisher's Linear Discriminant Analysis (LDA), a rank based Distribution Free (DF) approach, and the Classification And Regression Tree (CART) model. Methods A total of 320 patients with GES measures at 1, 2, 3, and 4 hour (h) after a standard meal using a standardized method were retrospectively collected. Area under the Receiver Operating Characteristic (ROC) curve and the rate of false classification through jackknife cross-validation were used for model comparison. Results Due to strong correlation and an abnormality in data distribution, no substantial improvement in diagnostic power was found with the best linear combination by LDA approach even with data transformation. With DF method, the linear combination of 4 -h and 3 -h increased the Area Under the Curve (AUC) and decreased the number of false classifications (0. 87; 15. 0 %) over individual time points (0. 83, 0. 82; 15. 6 %, 25. 3 %, for 4 -h and 3 -h, respectively) <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity</b> level (sensitivity = 0. 9). The CART model using 4 hourly GES measurements along with patient's age was the most accurate diagnostic tool (AUC = 0. 88, false classification = 13. 8 %). Patients having a 4 -h gastric retention value > 10 % were 5 {{times more likely to have}} gastroparesis (179 / 207 = 86. 5 %) than those with ≤ 10 % (18 / 113 = 15. 9 %). Conclusions With a mixed group of patients either referred with suspected gastroparesis or investigated for other reasons, the CART model is more robust than the LDA and DF approaches, capable of accommodating covariate effects and can be generalized for cross institutional applications, but could be unstable if sample size is limited. </p...|$|R
40|$|AbstractBackgroundIn {{occupational}} safety research, narrative text analysis has been combined with coded surveillance, data to improve identification {{and understanding of}} injuries and their circumstances. Injury data give, information about incidence and the direct cause of an injury, while near-miss data enable the, identification of various hazards within an organization or industry. Further, near-miss data provide an, opportunity for surveillance and risk reduction. The National Firefighter Near-Miss Reporting System, (NFFNMRS) is a voluntary reporting system that collects narrative text data on near-miss and injurious, events within the fire and emergency services industry. In recent research, autocoding techniques, using Bayesian models {{have been used to}} categorize/code injury narratives with up to 90 % accuracy, thereby reducing the amount of human effort required to manually code large datasets. Autocoding, techniques have not yet been applied to near-miss narrative data. MethodsWe manually assigned mechanism of injury codes to previously un-coded narratives from the, NFFNMRS and used this as a training set to develop two Bayesian autocoding models, Fuzzy and Naïve. We calculated sensitivity, specificity and positive predictive value for both models. We also evaluated, the effect of training set size on prediction sensitivity and compared the models’ predictive ability as, related to injury outcome. We cross-validated a subset of the prediction set for accuracy of the model, predictions. ResultsOverall, the Fuzzy model performed better than Naïve, with a sensitivity of 0. 74 compared to 0. 678., Where Fuzzy and Naïve shared the same prediction, the cross-validation showed a sensitivity of 0. 602., As the number of records in the training set increased, the models performed <b>at</b> <b>a</b> <b>higher</b> <b>sensitivity,</b> suggesting that both the Fuzzy and Naïve models were essentially “learning”. Injury records were, predicted with greater sensitivity than near-miss records. ConclusionWe conclude that the application of Bayesian autocoding methods can successfully code both near misses, and injuries in longer-than-average narratives with non-specific prompts regarding injury. Such, coding allowed for the creation of two new quantitative data elements for injury outcome and injury, mechanism...|$|R
40|$|We {{present the}} {{conceptual}} design of TENOR (Tracking and Emulsions for Neutrino Oscillation Research), a "short-baseline" experiment aiming <b>at</b> <b>a</b> new <b>high</b> <b>sensitivity</b> search for ν_μ-ν_τ oscillations in the CERN Wide Band Neutrino Beam. We show that by exploiting the {{advances in the}} technology of nuclear emulsions, one can conceive a hybrid apparatus with an emulsion target a factor six heavier than the one presently used in the CHORUS experiment. The main features of the experiment are discussed, {{as well as the}} issues of data selection, background reduction, and sensitivity in the determination of sin^ 22 θ_μτ. The present estimates on the experimental backgrounds are extrapolated from those anticipated for the CHORUS experiment; a more thorough study is still needed. The expected sensitivity of the experiment on the oscillation mixing angle is more than <b>a</b> factor 15 <b>higher</b> than those of the present explorations. In the case of a negative search, and for large Δ m^ 2, the 90 % CL upper limit in the mixing angle would be sin^ 22 θ_μτ < 9 × 10 ^- 6...|$|R
40|$|This work {{is aimed}} <b>at</b> {{developing}} <b>a</b> <b>high</b> <b>sensitivity</b> salinity (conductivity) sensor for marine applications. The principle of sensing {{involves the use}} of parallel plate capacitors, which minimizes the proximity effects associated with inductive measurement techniques. The barrier properties of two different materials, AZ 5214 and Honeywell 2 ̆ 7 s ACCUFLO T 3027, were investigated for use as the insulation layer for the sensor. Impedance analysis performed on the two coatings using Agilent 2 ̆ 7 s 4924 A Precision Impedance Analyzer served to prove that ACCUFLO was a better dielectric material for this application when compared to AZ 5214. Two separate detection circuits have been proposed for the salinity sensor. In the Twin-T filter method, a variation in capacitance tends to shift the resonant frequency of a Twin-T oscillator, comprising the sensor. Simulations of the oscillator circuit were performed using Pspice. Experiments were performed on calibrated ocean water samples of 34. 996 psu and a shift of 410 Hz/psu was obtained. To avoid the problems associated with the frequency drift in the oscillator, an alternate detection scheme is proposed which employs frequency-to-voltage converters. The sensitivity of this detection scheme was observed to be 10 mV/psu...|$|R
