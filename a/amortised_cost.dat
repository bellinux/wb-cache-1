16|18|Public
50|$|Measurement at <b>amortised</b> <b>cost</b> {{uses the}} {{effective}} interest method {{to provide a}} constant {{rate of return on}} an asset or liability until maturity (IAS39.9).|$|E
5000|$|... {{they have}} fixed or determinable {{maturity}} periods {{and are expected}} to be held to maturity, in which case they are stated at <b>amortised</b> <b>cost,</b> providing a constant rate of return until maturity; ...|$|E
50|$|Borrowing is stated at <b>amortised</b> <b>cost</b> {{using the}} {{effective}} interest method. This {{requires that the}} costs of arranging the borrowing are deducted from the principal value of debt and are amortised over the period of the debt (IAS39.46).|$|E
40|$|Abstract. The {{core of our}} {{resource}} analysis for the embedded systems language Hume is a resource-generic, type-based inference engine that employs the concept of <b>amortised</b> <b>costs</b> to statically infer resource bounds. In this paper we present extensions and improvements of this {{resource analysis}} in several ways. We develop and assess a call count analysis for higher-order programs, as a specific instance of our inference engine. We address usability aspects in general and in particular discuss an improved presentation of the inferred resource bounds together {{with the possibility of}} interactively tuning these bounds. Finally, we demonstrate improvements in the performance of our analysis. ...|$|R
5000|$|Son of the Guardsman {{is based}} on the Robin Hood legends, {{although}} it does not include Robin Hood, just the period and Sherwood Forest setting. [...] The serial was made to use costumes and sets left over from feature films, <b>amortising</b> the <b>costs</b> of all the productions involved. Costume drama serials were rare productions for any producer. The serial's subitle was [...] "Gallant Fighter of the Greenwood." ...|$|R
50|$|Unlike Laker's {{established}} {{rivals in}} the trooping business, the Yorks Air Charter proposed {{to use on}} trooping flights offered unprecedented safety standards and a hitherto unknown level of comfort, including rear-facing seats with headrests. These were industry-firsts in the trooping business. In addition, Air Charter proposed to place spare parts along the routes to be flown — another industry-first. These innovations secured Air Charter its first trooping contract. For Laker, Air Charter's owner, the new contract was a goldmine. The £90 per flying hour the War Office was contracted to pay Air Charter made flying troops in cheap, second-hand planes a very profitable business, {{given the fact that}} Laker could easily <b>amortise</b> the <b>cost</b> of each York for about £5,000 {{as a result of the}} massive spares stockpile he had purchased at rock-bottom prices in government surplus sales on behalf of Aviation Traders.|$|R
50|$|In {{contrast}} to fund valuation, {{the assets of}} a company will generally be valued {{for the purpose of}} a NAV calculation using the book value, the historical cost or the <b>amortised</b> <b>cost</b> of the company's assets, or an appropriate combination of the three.|$|E
50|$|Receivables and {{payables}} {{are recorded}} initially {{at fair value}} (IAS39.43). Subsequent measurement is stated at <b>amortised</b> <b>cost</b> (IAS39.46 and 47). In most cases, trade receivables and trade payables can be stated at the amount expected to be received or paid; however, {{it is necessary to}} discount a receivable or payable with a substantial credit period (see for example IAS18.11 for accounting for revenue).|$|E
50|$|The {{issue of}} {{impairment}} of financial instruments exposed {{deficiencies in the}} IAS 36 framework during the 2008 financial crisis, and the IASB issued an exposure draft in November 2009 that proposed an impairment model based on expected losses rather than incurred losses for all financial assets recorded at <b>amortised</b> <b>cost.</b> The IASB and FASB undertook joint efforts to devise a common impairment model, but the FASB eventually decided to propose an alternative scheme in January 2011. The IASB issued a new exposure draft in January 2013, which later led {{to the adoption of}} IFRS 9 in July 2014, effective for annual periods beginning on or after January 1, 2018. The FASB is still considering the matter.|$|E
5000|$|Up {{until the}} early 1990s, a number of local {{authorities}} had been engaged in interest rate swap transactions as part of managing their debt portfolios. Under the Local Authorities Act 1972, the local authorities had power to borrow in order to <b>amortise</b> their <b>costs</b> of capital projects {{over a longer period}} of time. In connection with that borrowing, certain local authorities sought to enter into swap transactions to hedge their exposure to fluctuations in interest rates. There were some doubts as to the ability of local authorities to enter into such transactions, but the local authorities sought the opinion of Anthony Scrivener QC, a leading commercial silk, who had advised that if a [...] "rate swap is undertaken as part of the proper management of the council's fund then ... the swap will be intra vires" [...] within the powers of the council.|$|R
30|$|The {{analysis}} {{considered the}} Sequoia Acuson S 2000 ultrasound system (Siemens Mountan View, CA), {{for which the}} <b>cost</b> to be <b>amortised</b> (i.e., purchase <b>cost</b> minus residual value, here set at 0) was € 117, 120.00. The annual cost over 10  years’ amortisation was € 11, 712 calculated according to the equipment life span. Considering 300  days of practical annual capacity, a single daily 6 -h shift in the ultrasound section and an average CEUS examination time of 17.4  min (measured on 157 patients imaged with CEUS), the equipment costs amounted to € 1.88 per examination.|$|R
40|$|Random search {{trees have}} the {{property}} that their depth {{depends on the}} order in which they are built. They have to be balanced in order to obtain a more efficient storage-and-retrieval data structure. Balancing a search tree is time consuming. This explains the popularity of data structures which approximate a balanced tree but have lower <b>amortised</b> balancing <b>costs,</b> such as AVL trees, Fibonacci trees and 2 - 3 trees. The algorithms for maintaining these data structures efficiently are complex and hard to derive. This observation has led to insertion algorithms that perform local balancing around the newly inserted node, without backtracking on the search path. This is also called a fringe heuristic. The resulting class of trees is referred to as 1 -locally balanced trees, in this note referred to as hairy trees. In this note a simple analysis of their behaviour is povided. Keywords: search trees, heuristic balancing, local balancing, fringe heuristic, hairy trees. Classification: AMS 68 P 05, [...] ...|$|R
40|$|How {{does the}} {{management}} and resolution of the current crisis compare with {{the response of the}} Nordic countries in the early 1990 s, widely regarded as exemplary? We argue that, while intervention has been prompter, the measures taken so far remain less comprehensive and in-depth. In particular, the cleansing of balance sheets has proceeded more slowly, and less {{attention has been paid to}} reducing excess capacity and avoiding competitive distortions. In general, policymakers have given higher priority to sustaining aggregate demand in the short term than to encouraging adjustment in the financial sector and containing moral hazard. We argue that three factors largely explain this outcome: the more international nature of the crisis; the complexity of the instruments involved; and, hardly appreciated so far, the effect of accounting practices on the dynamics of the events, reflecting in particular the prominent role of fair value accounting (and mark to market losses) in relation to <b>amortised</b> <b>cost</b> accounting for loan books. There is a risk that the policies followed so far may delay the establishment of the basis for a sustainably profitable and less risk-prone financial sector. Crisis management and resolution, principles for successful resolution, Nordic countries, fair value and <b>amortised</b> <b>cost</b> accounting, mark to market losses...|$|E
40|$|This paper {{discusses}} {{about the}} adoption of International Financial Reporting Standards (IFRS) by the Nigerian financial institutions. Nigeria have been using domestic accounting standard (NGAAP) for banks and non-banks financial institutions known as Statement of Accounting Standards (SAS 10 Part 1 and SAS 15 Part 2) issued in 1990 and 1997 respectively for financial reporting. These domestic standards were adopted from International Accounting Standards (IAS 30) but have not been updated like IAS 30 {{as reported by the}} Report on Observance of Standard Codes (ROSC) of Nigeria in 2004 and 2011. The change in accounting regulations is {{as a result of the}} weaknesses of NGAAP and low disclosure requirements. IFRS reporting has more disclosures than NGAAP especially for financial institutions. Under NGAAP financial instruments have not been classified as in IFRS. For instance, financial instruments have been classified into four under IAS 39 as; (i) recognised fair value on gain or loss in profit or loss, (ii) are measured at <b>amortised</b> <b>cost</b> for investments held-to-maturity, (iii) measured at <b>amortised</b> <b>cost</b> for loans and receivables, (iv) measured at fair value gain or loss for available-for-sale financial assets recognised in other comprehensive income. Additionally, financial liabilities have been categories into two namely; (i) measured at amortised fair value on financial liabilities through profit or loss and, (ii) measured at amortised other liabilities. Now with the mandatory adoptions of reporting under IFRS by all listed financial institutions, will the accounting disclosures be more valu...|$|E
40|$|The d 2 {{distance}} {{function is}} commonly used in the clustering of DNA sequences such as expressed sequence tags (ESTs), an important biological application. The use of d 2 allows approximate string matching to be performed with a good balance between selectivity and sensitivity. The computational challenges of EST clustering make the efficient evaluation of the d 2 function an imperative. The paper presents a new incremental algorithm which requires <b>amortised</b> <b>cost</b> of O(m) per evaluation on realistic data sets (where m is {{the average length of}} an EST). In addition, two filtering heuristics are presented which improve clustering performance by estimating upper bounds on the d 2 scores...|$|E
40|$|We {{address the}} {{fundamental}} problem of permuting the elements of an array of n elements according to some given permutation. Our goal is to perform the permutation quickly using only a polylogarithmic number of bits of extra storage. The main result is an algorithm whose worst case running time is O(n log n) and that uses O(log n) additional log n-bit words of memory. A simpler method is presented for the case in which both the permutation and its inverse can be computed at (<b>amortised)</b> unit <b>cost.</b> This algorithm requires O(n log n) time and O(1) words in the worst case. These results are extended {{to the situation in}} which we are to apply a power of the permutation. A linear time, O(1) word method is presented for the special case in which the data values are all distinct and are either initially in sorted order or will be when permuted. 1...|$|R
5000|$|... 100% is a {{television}} game show that {{was shown in}} the United Kingdom from 31 March, 1997, {{the day after the}} inception of its host television station Channel 5, till 24 December, 2001. A Reg Grundy production, it was often billed as [...] "The game show without a host" [...] although it had a presenter, Robin Houston. He read the questions off-screen throughout the show and was never seen by the viewers. The maximum number of shows that were produced in one day was twelve, although the normal recording day saw ten shows being produced. The reason for such a high number was to <b>amortise</b> the production <b>costs.</b> In the spinoff called 100% Gold it was presented by Melinda Walker.|$|R
40|$|The {{possibility}} that future solid polymer {{fuel cell vehicles}} will be fuelled by methanol has been suggested. If this is the case, it will have significant implications for the future structure of the methanol supply industry, and methanol supply and availability may {{have an impact on}} the take-up of these SPFC vehicles. In this study, a model assessing the possible future penetration of methanol SPFC vehicles was constructed. This suggested that it would be possible for SPFC vehicles to achieve rapid market penetration after an initially slow start. A further model indicated that methanol supply would be adequate for vehicle demand until about 2013, when significant new capacity would be required. The cost of this new capacity was estimated, along with the cost of providing refuelling infrastructure such as road tankers, storage, and suitable fuelling stations. <b>Amortising</b> the <b>cost</b> over a short period (to 2013) could double the pre-tax price of methanol as a fuel, while over a longer timeframe (to 2029) it would add less than 10 % to this value. The model suggests that methanol capacity need not be a constraint to the future introduction of SPFC vehicles using it as a fuel, but that other factors such as fuel purity and safety must be carefully considered before real costs can be calculated...|$|R
40|$|Combined {{electrical}} and structural models of five types of permanent magnet linear electrical machines suitable for direct-drive power take-off on wave energy applications are presented. Electromagnetic models were developed using polynomial approximation to {{finite element analysis}} results. The structural models are based on simple beam theory, other classical techniques, and automated finite element analysis formulations. The machine models have been integrated with a time-domain model of a wave energy converter based on a heaving buoy. They have then been optimised using a genetic algorithm approach, using a score based primarily on the <b>amortised</b> <b>cost</b> per unit of energy production. The optimised designs have then been used for {{a comparison of the}} economic performance of the generator types...|$|E
40|$|Objectives To {{evaluate}} and compare surgical parameters and {{costs associated with}} robotic and laparoscopic surgery for colorectal cancer over 5 -years (2009 - 2014) in a single Italian centre. Methods From 2009 to 2014 data about laparoscopic (LAP) and robotic (ROB) assisted colorectal cancer procedures performed in General Surgery, University Hospital of Pisa were collected. Operating time and length of stay were evaluated as surgical parameters and costs included fixed and variable items. <b>Amortised</b> <b>cost</b> of the robot and laparoscopic instrument and <b>amortised</b> <b>cost</b> of robot maintenance per intervention were considered among fixed costs. Variable costs included costs of dispensable equipment, cost of operating room personnel per time and costs of length-of-stay. Over the 5 -years period surgical parameters and costs associated with LAP and ROB were analysed using descriptive statistics and compared between groups by independent T-test and Mann-Whitney test. Generalized linear models {{were used to assess}} the independent effect of type of surgery and time. Results A total of 25 laparoscopic (LAP) and 50 robotic (ROB) procedures were evaluated. Overall median operating time was significantly higher in LAP (270 min vs 312. 5 min, P= 0. 006) and regression analysis showed a borderline significant interaction effect between type of surgery and years (P= 0. 058) suggesting a significant reduction of operating time in ROB but not in LAP. Hospital-stay did not differ between groups (P= 0. 567). Overall mean costs associated to LAP varied between 8, 612 ± 2, 733 Euros in 2009 to 10, 827 ± 5, 038 Euros in 2013. For ROB mean costs were 12, 966 ± 1, 115 Euros in 2010 to 11, 933 ± 1, 753 Euros in 2014. Regression analysis outlined significant greater costs for ROB (P-value< 0. 001) without differences over time. Conclusions Robotic surgery for colorectal cancer carries out significant greater costs and operating time compared to laparoscopic surgery. There is weak evidence of improvement in the exploitation of ROB procedures without significant consequences on costs at presen...|$|E
40|$|We {{describe}} a new automatic static analysis for determining upper-bound functions {{on the use}} of quantitative resources for strict, higher-order, polymorphic, recursive programs dealing with possibly-aliased data. Our analysis is a variant of Tarjan’s manual <b>amortised</b> <b>cost</b> analysis technique. We use a type-based approach, exploiting linearity to allow inference, and place a new emphasis on the number of references to a data object. The bounds we infer depend on the sizes of the various inputs to a program. They thus expose the impact of specific inputs on the overall cost behaviour. The key novel aspect of our work is that it deals directly with polymorphic higher-order functions without requiring source-level transformations that could alter resource usage. We thus obtain safe and accurate compile-time bounds. Our work is generic in that it deals with a variety of quantitative resources. We illustrate our approach with reference to dynamic memory allocations/deallocations, stack usage, and worst-case execution time, using metrics taken from a real implementation on a simple micro-controller platform that is used in safety-critical automotive applications...|$|E
30|$|We {{envisage}} our cell-based broad-phase microarchitectures fabricated {{as part of}} an IC {{that would}} also execute the remainder of the interactive application program loop. Using a single platform would allow for the elimination of data transfer overheads. This concept has been successfully adopted to integrate a CPU and GPU within some Intel Core processors [42] as well as AMD accelerated processing units (APUs) [43], such as those in the PlayStation 4. Within the spectrum of platforms readily available today, our microarchitectures could naturally reside within the fixed-function logic of GPUs, as there is already a significant focus on relocating many elements of the interactive application program loop to these platforms [44]. The remainder of the program loop could utilise the programmable elements of the GPU. Adding one of the microarchitectures would not compromise GPU programmability, as all GPUs include some fixed-function logic such as rasterisation. This is unlikely to change due to power-density limits as well as the lacklustre throughput achieved when traditionally fixed-function elements have been reimplemented using the programmable elements of a GPU [45]. Moreover, it is not prohibitively expensive to include one of the microarchitectures, as the large production volumes of commodity platforms <b>amortises</b> the <b>cost</b> [27]. Therefore, there exists sufficient motivation for the fabrication of our logic as part of a future GPU.|$|R
40|$|The primary aim of {{this report}} {{was to assess the}} {{efficacy}} and safety of robot-assisted surgery, compared to laparoscopic and open surgery for partial nephrectomy, adrenalectomy, radical prostatectomy, radical cystectomy, hysterectomy and ovariectomy. Furthermore, the costs (for acquisition, maintenance, etc.) of robotic surgery from the perspective of hospitals were calculated and the future of robotic surgery was evaluated. In addition to a literature search, the respective manufacturers were contacted. Furthermore, for evaluating the future of robotic surgery, a survey was sent to Austrian experts. None of the 26 identified studies have shown a significant patient-relevant benefit of robot-assisted surgery – at least not for nephrectomy, adrenalectomy, prostatectomy, cystectomy or hysterectomy. There are probably some benefits of robotic-assisted surgery for outcomes such as length of hospital stay or potency after prostatectomy. On average the acquisition costs of a da Vinci® surgical system are around 1. 5 million Euros, with those for maintenance amounting to approx. 150, 000 Euros per year and nearly 1600 Euros operational cost per surgery. The amortisation of a da Vinci® surgical system is only possible when a high number of cases is guaranteed and a high amount from the generated lump sums is provided to <b>amortise</b> the <b>costs.</b> Overall, the experts prognosticated that robotic surgery in Austria will further grow, at least in certain fields. Due to an increasing competition pressure, the costs for robotic surgery could decrease in the future...|$|R
40|$|Random search {{trees have}} the {{property}} that their depth {{depends on the}} order in which they are built. They have to be balanced in order to obtain a more ecient storage-and-retrieval datastructure. Balancing a search tree is time consuming. This explains the popularity of datastructures which approximate a balanced tree but have lower <b>amortised</b> balancing <b>costs,</b> such as AVL trees, Fibonacci trees and 2 - 3 trees. The algorithms for maintaining these datastructures eciently are very complex and hard to derive. This complexity follows from the fact that even partial balancing upon insertion needs extensive rearrangement of the tree, not con ned to the path from the root of the tree to the place of insertion. This observation led us to consider insertion algorithms that perform local balancing around the newly inserted node, without backtracking on the search path. In this note, we investigate the implementation and properties of hairy search trees. Keywords: search trees, heuristic balancing, local balancing, hairy trees. 1 Hairy Trees Hairy trees are a class of binary trees with the property: is hairy (t) 8 node v 2 [v has single son s) s is leaf] The intuition behind this condition is that it prevents trees from having list-like substructures longer than two nodes (twigs"). Some examples of hairy trees are presented in gure 1. The class of hairy trees can be described by the following recursive de nition: J J J J Figure 1 : Two hairy trees and a non-hairy one 1. is hairy (") 2. If t is a singleton tree and x some key value, then is hairy(t), is hairy(Tree:(x; t; ")) and is hairy(Tree:(x; "; t)) ...|$|R
40|$|Energy {{consumption}} {{is a major}} concern in multicore systems. Perhaps the simplest strategy for reducing energy costs is to use only as many cores as necessary while still being able to deliver a desired quality of service. Motivated by earlier work on a dynamic (heterogeneous) core allocation scheme for H. 264 video decoding that reduces energy costs while delivering desired frame rates, we formulate operationally the general problem of executing a sequence of actions on a reconfigurable machine while meeting a corresponding sequence of absolute deadlines, with the objective of reducing cost. Using a transition system framework that associates costs (e. g., time, energy) with executing an action on a particular resource configuration, we use the notion of <b>amortised</b> <b>cost</b> to formulate in terms of simulation relations appropriate notions for comparing deadline-conformant executions. We believe these notions can provide the basis for an operational theory of optimal cost executions and performance guarantees for approximate solutions, in particular relating the notion of simulation from transition systems to that of competitive analysis used for, e. g., online algorithms. Comment: In Proceedings PLACES 2017, arXiv: 1704. 0241...|$|E
40|$|For many {{algorithmic}} problems, traditional algorithms that optimise on {{the number}} of instructions executed prove expensive on I/Os. Novel and very different design techniques, when applied to these problems, can produce algorithms that are I/O efficient. This thesis adds to the growing chorus of such results. The computational models we use are the external memory model and the W-Stream model. On the external memory model, we obtain the following results. (1) An I/O efficient algorithm for computing minimum spanning trees of graphs that improves on the performance of the best known algorithm. (2) The first external memory version of soft heap, an approximate meldable priority queue. (3) Hard heap, the first meldable external memory priority queue that matches the amortised I/O performance of the known external memory priority queues, while allowing a meld operation at the same <b>amortised</b> <b>cost.</b> (4) I/O efficient exact, approximate and randomised algorithms for the minimum cut problem, which has not been explored before on the external memory model. (5) Some lower and upper bounds on I/Os for interval graphs. On the W-Stream model, we obtain the following results. (1) Algorithms for various tree problems and list ranking that match the performance of the best known algorithms and are easier to implement than them. (2) Pass efficient algorithms for sorting, and the maximal independent set problems, that improve on the best known al gorithms. (3) Pass efficient algorithms for the graphs problems of finding vertex-colouring, approximate single source shortest paths, maximal matching, and approximate weighted vertex cover. (4) Lower bounds on passes for list ranking and maximal matching. We propose two variants of the W-Stream model, and design algorithms for the maximal independent set, vertex-colouring, and planar graph single source shortest paths problems on those models...|$|E
40|$|The {{implementation}} of International Accounting Standard 32 Financial Instruments: Disclosure and Presentation (lAS 32), International Accounting Standard 39 Financial Instruments: Recognition and Measurement (lAS 39) and International Financial Reporting Standard 7 Financial Instruments: Disclosures (IFRS 7) by developing countries has been met with mixed reactions {{largely due to}} the extensive use of the fair value concept by the three accounting standards. The use of the fair value concept in developing countries {{has proved to be a}} significant challenge due to either a Jack of formal capital market systems or very thinly traded capital markets. This study investigates the obstacles to determining fair values of equity share investments, government bonds and corporate bonds, treasury bills and loan advances in Mozambique. The study was done through a combination of literature review and empirical research using a questionnaire. The trading statistics of the financial instruments on the Mozambique Stock Exchange and the prospectuses of bonds were used. The empirical research was carried out using a type of non-probability sampling technique called purposive sampling. A subcategory of purposive sampling called expert sampling was used to select the eventual sample which was composed of people with specialised knowledge on the capital market system in Mozambique. The results of the empirical research were analysed using pie charts to summarise the responses. The research concluded that the Mozambique Stock Exchange is an inactive market for financial instruments characterised by thin trading in both equity shares and bonds. The estimation of fair values evidenced by observable market transactions is therefore impossible. The absence of credit rating agencies in Mozambique presents a significant challenge in assigning credit risk and pricing financial instruments such as bonds. The research also noted that significant volatility of the main economic indicators such as treasury bills interest rates and inflation made it difficult to determine fair values of financial instruments using financial modelling techniques. Due to the above obstacles to determining fair values of certain financial instruments in Mozambique, the best alternatives are to value these financial instruments at either cost or <b>amortised</b> <b>cost.</b> Financial AccountingM. Com. (Accounting...|$|E
40|$|International audiencehe direct {{transplantation}} or {{imitation of}} lean production {{has led to}} difficulties of applying a number of lean principles and practices. Thus, diffusion {{of one of the}} main lean principles, just-in-time production, which refers to producing only 'what is really needed, when it is needed, and in the amount needed', seems to be limited. To date, some of the companies produce more than their customers really need. This method of production enables them not only to <b>amortise</b> the high-changeover <b>costs</b> over a large number of products, but also to benefit from commercial opportunities. However, these companies are exposed to financial losses related to storage costs and risks of non-sale. To provide decision elements for determining the best production strategy, we have developed a model for calculating the optimal quantity to be produced. Moreover, we suggest using a fuzzy aggregation system to optimise the consideration of the risk of non-sale. This new approach defines the limits not to exceed by taking into consideration the drawbacks linked to the risks of non-sale...|$|R
40|$|Growing {{pressures}} on healthcare costs are spurring development of lightweight bodyworn sensors for real-time and continuous physiological monitoring. Data from these sensors is streamed wirelessly to a handheld device {{such as a}} smartphone or tablet, and then archived in the cloud by personal health record services. Authenticating the data these devices generate is vital to ensure proper diagnosis, traceability, and validation of claims. Digital signatures at the packet-level are too resource-intensive for bodyworn devices, while block-level signatures are not robust to loss. In this paper we propose, analyse, and validate a practical, lightweight robust authentication scheme suitable for health-monitoring. We make three specific contributions: (a) We develop an authentication scheme that is both low-cost (using a Merkle hash tree to <b>amortise</b> digital signature <b>costs),</b> and loss-resilient (using network coding to recover strategic nodes within the tree). (b) We design a framework for optimising placement of network coding within the tree to maximise data verifiability for a given overhead and loss environment. (c) We validate our scheme using experimental traces of typical operating conditions to show that i...|$|R
40|$|Abstract—Growing {{pressures}} on healthcare costs are spurring development of lightweight bodyworn sensors for real-time and continuous physiological monitoring. Data from these sensors is streamed wirelessly to a handheld device {{such as a}} mobile phone, and then archived over the Internet at a central database. Authenticating the data is vital to ensure proper diagnosis, traceability, and validation of claims. Digital signatures at the packet-level are too resource-intensive for bodyworn devices, while block-level signatures are not robust to loss. In this paper we propose, analyse, and validate a practical, lightweight robust authentication scheme suitable for health-monitoring. We make three specific contributions: (a) We develop an authentication scheme that is both low-cost (using a Merkle hash tree to <b>amortise</b> digital signature <b>costs),</b> and loss-resilient (using network coding to recover strategic nodes within the tree). (b) We develop a framework for optimising placement of network coding within the tree to maximise data verifiability for a given overhead and loss environment. (c) We validate our scheme using experimental traces of typical operating conditions {{to show that it}} achieves high success (over 99 % of the medical data can be authenticated) at very low overheads (as low as 5 % extra transmissions) and at very low cost (the bodyworn device has to perform a digital signature operation no more than once per hour). We believe our novel authentication scheme can be a key ingredient in the integration of wearable medical monitoring devices into current healthcare systems. I...|$|R
40|$|As {{software}} systems rise in {{size and}} complexity, the need for verifying some of their properties increases. One important property to be verified is the resource usage, i. e. how many resources the program will need for its execution, where resources include execution time, memory, power, etc. Resource usage analysis is important in many areas, in particular embedded systems and cloud computing. Thus, resource analysis has been widely researched and some different approaches to this have been proposed based in particular on recurrence solving, abstract interpretation and amortised analysis. In the amortised analysis technique, a nonnegative number, called potential, is assigned to a data structure. The <b>amortised</b> <b>cost</b> of operations is then defined by its actual cost plus the difference in potential of the data structure before and after performing the operation. Amortised analysis {{has been used for}} automatic resource analysis of functional and object-oriented programs. The potentials are defined using refined types and typing rules then ensure that potential and actual resource usage is accounted for correctly. The automatic inference of the potential functions can then be achieved by type inference. In the case of functional programs, the structure of the types is known. Thus, type inference can be reduced to solving linear arithmetic constraints. For object-oriented programs, however, the refined types are more complicated because of the general nature of objects: they can be used to define any data structure. Thus, the type inference must discover not only the potential functions for the data structure but also the data structures themselves. Other features of object-oriented programs that complicate the analysis are aliasing and imperative update. Hofmann and Jost presented in 2006 a type system for amortised heap-space analysis of object-oriented programs, called Resource Aware JAva (RAJA). However, they left the problem of type inference open. In this thesis we present a type inference algorithm for the RAJA system. We were able to reduce the type inference problem to the novel problem of satisfiability of arithmetic constraints over infinite trees and we developed a heuristic algorithm for satisfiability of these constraints. We proved the soundness of the type inference algorithm and developed an OCaml implementation and experimental evaluation that shows that we can compute linear upper-bounds to the heap-space requirements of many programs, including sorting algorithms for lists such as insertion sort and merge sort and also programs that contain different interacting objects that describe real-life scenarios like a bank account. Another contribution of this thesis is a type checking algorithm for the RAJA system that is useful for verifying the types discovered by the type inference by using the proof carrying code technology...|$|E
40|$|Abstract Background Population-based {{registries}} {{are increasingly}} used to recruit patient samples for research, however, they have several limitations including low consent and participation rates, and potential selection bias. To improve access to samples for research, {{the utility of}} {{a new model of}} recruitment termed the ‘Consumer Register’, that allows for direct patient recruitment from hospitals, was examined. This paper reports: (i) consent rates onto the register; (ii) preferred methods and frequency of contact; and (iii) the feasibility of establishing the register, including: (a) cost per person recruited to the register; (b) the differential cost and consent rates of volunteer versus paid data collectors; and (c) participant completion rates. Methods A cross-sectional survey was conducted in five outpatient clinics in Australia. Patients were approached by volunteers or paid data collectors and asked to complete a touch-screen electronic survey. Consenting individuals were asked to indicate their willingness and preferences for enrolment onto a research register. Descriptive statistics were used to examine patient preferences and linear regression used to model the success of volunteer versus paid data collectors. The opportunity and financial costs of establishing the register were calculated. Results A total of 1947 patients (80. 6  %) consented to complete the survey, of which, 1486 (76. 3  %) completed the questionnaire. Of the completers, the majority (69. 4  %, or 1032 participants) were willing to be listed on the register and preferred to be contacted by email (50. 3  %). Almost 39  % of completers were willing to be contacted three or more times in a 12  month period. The annual opportunity cost of resources consumed by the register was valued at $ 37, 187, giving an opportunity cost per person recruited to the register of $ 36. After <b>amortising</b> fixed <b>costs,</b> the annual financial outlay was $ 23, 004 or $ 22 per person recruited to the register. Use of volunteer data collectors contributed to an annual saving of $ 14, 183, however paid data collectors achieved significantly higher consent rates. Successful enrolment onto the register was completed for 42 % of the sample. Conclusions A Consumer Register is a promising and feasible alternative to population-based registries, with the majority of participants willing to be contacted multiple times via low-resource methods such as email. There is an effectiveness/cost trade off in the use of paid versus volunteer data collectors...|$|R
40|$|Maintaining {{security}} and reliability in the electricity supply {{is fundamental to}} the functioning of a modern society and drives the need for adequate transmission capacity for both market participants and customers. Planning the investment in transmission {{has always been a}} complicated undertaking due to the high development costs and long lead times. Furthermore, to anticipate the future needs of customers is a task as difficult as that of cost-effective planning and construction of new facilities. Trying to find treatments for some of these issues represents a major motivation for this thesis. This thesis investigates the problem of how much reinforcement a transmission system requires when a significant proportion of wind generation is integrated into an existing transmission system. A multi-period transmission planning model is developed for determining optimal transmission capacity by balancing <b>amortised</b> transmission investment <b>costs</b> and annual generation costs subject to network security constraints, The model employs the security-constrained DC optimal power flow formulation and applies a solver (DashXpress) to obtain the results of the remaining linear large-scale optimisation problem. This thesis begins by exploring the impact of wind generation on the determination of appropriate levels of system capacity on the transmission network starting from the premise that it is no longer cost effective to invest in sufficient network capacity to accommodate simultaneous peaks from all generators. As such, a significant finding {{of this study is that}} conventional and wind generation should share network capacity. Given the acknowledged increase in uncertainty to security of supply due to difficulties in wind generation forecast this thesis also explores the optimal sourcing of generation reserve, and investigates investment in transmission capacity to exploit the cost benefits offered by standing reserve. Finally, the thesis presents and evaluates an alternative associated with transmission operation and investment level of risk and uncertainty by introducing more flexibility to the way the transmission system is operated. Application of Quadrature Boosters and Demand Side as model of corrective control, brings savings in operating costs without jeopardizing the level of system security, enables better utilisation of existing facilities and reduces the demand for new transmission investment. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

