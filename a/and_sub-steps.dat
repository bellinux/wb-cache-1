10|26|Public
30|$|Elicit {{the major}} tasks and subtasks. The major steps <b>and</b> <b>sub-steps</b> needed to {{accomplish}} the training task are collected by administering a “think aloud” protocol to the SMEs as they perform the training task. A human patient simulator maybe as used a reference point to help them recount how the task is performed.|$|E
40|$|Work {{discusses}} {{about the}} test station in NXP Semiconductors Company in Rožnov pod Radhoštěm. It describes first the test station {{itself and its}} possibilities in software libraries testing. Second it describes automatic selftest of this station <b>and</b> <b>sub-steps</b> of this selftest. This work is also used as a documentation for company needs. KEYWORD...|$|E
30|$|While {{the task}} flows include task {{procedures}} {{that correspond to}} necessary steps <b>and</b> <b>sub-steps</b> as well as steps that may be taken under certain conditions, they do not document additional information collected during the task analysis that is encoded into the task models. This includes information like optimal paths, skills required in performing each step, instructional strategies (e.g. cues & feedbacks) and effected assessment metrics.|$|E
30|$|Each step <b>and</b> <b>sub-step</b> {{should be}} {{thoroughly}} described.|$|R
50|$|The {{ecotourism}} pan-European routes finalized {{the layout}} of the five routes across 36 countries, 230 regions, 154 stages <b>and</b> 180 <b>sub-steps.</b>|$|R
40|$|The {{article of}} record as {{published}} {{may be found}} at [URL] extend in this paper an optimal family of three-step eighth-order methods developed by Džunić et al. (2011) with higher-order weight functions employed in the second <b>and</b> third <b>sub-steps</b> <b>and</b> investigate their dynamics under the relevant extraneous fixed points among which purely imaginary ones are specially treated {{for the analysis of}} the rich dynamics. Their theoretical and computational properties are fully investigated along with a main theorem describing the order of convergence and the asymptotic error constant as well as proper choices of special cases. A wide variety of relevant numerical examples are illustrated to confirm the underlying theoretical development. In addition, this paper investigates the dynamics of selected existing optimal eighth-order iterative maps with the help of illustrative basins of attraction for various polynomials. Research Fund of Dankook Universit...|$|R
30|$|For {{the first}} mode and all modes with {{cumulative}} mass participation factor exceeding 90  %, Rayleigh equivalent damping {{was defined as}} 5  %. The strain–stress hysteresis behavior of steel was modelled as a bilinear with a strain hardening ratio of 3  %. To perform NTHA, time steps <b>and</b> <b>sub-steps</b> have been considered to be 0.005  s and 1000, respectively. In all analysis, P-Delta effect has been included. To model plastic hinge {{at the end of}} elements, inelastic force-based element with fiber-distributed plasticity was selected. To do nonlinear dynamic analysis by means of Opensees, each element was divided into ten parts (McKenna et al. 2000). In these parts, the stresses and strains were derived from three points at the top, middle, and bottom corners of the section.|$|E
40|$|AbstractWhenever we open a {{household}} appliance (or any other mechanical or electrical product, {{such as a}} toy) {{we can find a}} user's manual; this can be a leaflet or an 80 -page book. In any case, we agree to name it instruction/user's manual and we treat it as a unit. This communication presents a research done with the purpose of establishing the rhetorical structure of instruction manuals for household appliances and to contrast the differences in two languages, Spanish and English. With this commitment we have complied a 52 -text bilingual corpus divided into two comparable subcorpora following Sinclair's (1991) and Leech's (1996) ideas on corpus linguistics as well as other scholars. The theoretical framework for this contrastive study is based on macro-linguistics. We have taken the notion of genre to approach the corpus and considered instruction manuals as one more of them. This macro-linguistic perspective is based on Swales (1993), Halliday and Hasan (1989), Werlich (1983) and Bhatia's (1993, 2004) notions. Then, as the communicative process (Shannon, 1948) and the pragmatic relations are similar in every case, {{we have been able to}} preform a rhetoric analysis from a twofold perspective: qualitative and quantitative. On the qualitative side of the analysis, we have determined the prototypical structure, following and adapting Swales model (1993) to the instructional genre, and according to it we have tagged the corpus into moves, steps <b>and</b> <b>sub-steps.</b> On the quantitative side of the analysis, we have looked for the significance of the different moves, steps <b>and</b> <b>sub-steps</b> in numerical terms and percentages. This raw information needs interpretation, and the conclusion will extract it from the differences and similarities in terms of priorities in one language and the other...|$|E
40|$|Avoiding {{inappropriate}} polypharmacy {{has become}} increasingly recognised as a safety imperative for older patient care. Deprescribing is an active process of reviewing all medications being used by individual patients that prompts clinicians to consider which medications have unfavourable risk–benefit trade-offs {{in the context of}} illness severity, advanced age, multi-morbidity, physical and emotional capacity, life expectancy, care goals and personal preferences. Structured guides to deprescribing include algorithms, flow charts or tables which are patient-directed and aim to guide the clinician through sequential steps in deciding which medications should be targeted for discontinuation. In this narrative review, we describe seven structured deprescribing guides whose stated purpose included the reduction of polypharmacy, their use was not restricted to a single drug or drug class and they had undergone some form of efficacy testing. There was considerable heterogeneity in guide design and content, with some guides constituting little more than a set of principles while others entail detailed processes <b>and</b> <b>sub-steps</b> which addressed multiple determinants of drug appropriateness. Evidence of effectiveness for each guide was limited in that none have been evaluated in randomised controlled trials, that pilot or feasibility studies have involved relatively small patient samples, that intra-rater and inter-rater reliabilities have not been determined and that most have been studied in hospital settings. Which is most useful to clinicians is unknown in the absence of head-to-head comparisons. While most guides have face validity, more research is needed for determining effectiveness and ease of use in routine clinical practice, especially in primary care settings...|$|E
40|$|This paper {{presents}} the new classification system which identifies and designates rock masses {{based on their}} fundamental engineering characteristics. The system encompasses the stepwise procedural classification with three steps <b>and</b> a <b>sub-step.</b> The first-step is to classify rock mass into two types; (1) hard rock mass and/or its weathered or altered rock mass, and (2) soft rock mass that is not lithified enough. The second and third steps are to further classify the rock mass based on their inherent structura...|$|R
40|$|Parsek 2 D-MLMD is a semi-implicit Multi Level Multi Domain Particle-in-Cell (PIC) {{code for}} the {{simulation}} of astrophysical and space plasmas. In this Whitepaper, {{we report on}} improvements on Parsek 2 D-MLMD {{carried out in the}} course of the PRACE preparatory access project 2010 PA 1802. Through algorithmic enhancements – in particular the implementation of smoothing <b>and</b> temporal <b>sub-stepping</b> – as well as through performance tuning using HPCToolkit, the efficiency of the code has been improved significantly. For representative benchmark cases, we consistently achieved a total speedup of a factor 10 and higher. status: publishe...|$|R
40|$|ABSTRACT Introduction As urology {{training}} shifts toward competency-based frameworks, {{the need}} for tools for high stakes assessment of trainees is crucial. Validated assessment metrics are lacking for many robot-assisted radical prostatectomy (RARP). As it is quickly becoming {{the gold standard for}} treatment of localized prostate cancer, the development and validation of a RARP assessment tool for training is timely. Materials and methods We recruited 13 expert RARP surgeons from the United States and Canada to serve as our Delphi panel. Using an initial inventory developed via a modified Delphi process with urology residents, fellows, and staff at our institution, panelists iteratively rated each step <b>and</b> <b>sub-step</b> on a 5 -point Likert scale of agreement for inclusion in the final assessment tool. Qualitative feedback was elicited for each item to determine proper step placement, wording, and suggestions. Results Panelist’s responses were compiled and the inventory was edited through three iterations, after which 100 % consensus was achieved. The initial inventory steps were decreased by 13 % and a skip pattern was incorporated. The final RARP stepwise inventory was comprised of 13 critical steps with 52 sub-steps. There was no attrition throughout the Delphi process. Conclusions Our Delphi study resulted in a comprehensive inventory of intraoperative RARP steps with excellent consensus. This final inventory will be used to develop a valid and psychometrically sound intraoperative assessment tool for use during RARP training and evaluation, with the aim of increasing competency of all trainees...|$|R
40|$|A {{new method}} for pest risk {{assessment}} and {{the identification and}} evaluation of risk-reducing options is currently under development by the European Food Safety Authority (EFSA) Plant Health Panel. The draft method has been tested on pests of concern to the European Union (EU). The method is adaptable and can focus either on all the steps <b>and</b> <b>sub-steps</b> of the assessment process or on specific parts if necessary. It is based on assessing changes in pest population abundance as the major driver of the impact on cultivated plants and on the environment. Like other pest risk assessment systems the method asks questions about the likelihood and magnitude of {{factors that contribute to}} risk. Responses can be based on data or expert judgment. Crucially, the approach is quantitative, and it captures uncertainty through the provision by risk assessors of quantile estimates of the probability distributions for the assessed variables and parameters. The assessment is based on comparisons between different scenarios, and the method integrates risk-reducing options where they apply to a scenario, for example current regulation against a scenario where risk-reducing options are not applied. A strategy has been developed to communicate the results of the risk assessment in a clear, comparable and transparent way, with the aim of providing the requestor of the risk assessment with a useful answer to the question(s) posed to the EFSA Plant Health Panel. The method has been applied to four case studies, two fungi, Ceratocystis platani and Cryphonectria parasitica, the nematode Ditylenchus destructor and the Grapevine flavescence dorée phytoplasma. Selected results from these case studies illustrate the types of output that the method can deliver. </p...|$|E
40|$|Includes bibliographical {{references}} (p. 47 - 50). This thesis {{presents a}} genre-based study of U. S. law school lectures attended by foreign lawyers. With {{the practice of}} law becoming more international, an increasing number of working professionals in the field of law are enrolling in U. S. law schools to gain knowledge of the U. S. legal system. Previous research within English for Academic Legal Purposes (EALP) has mainly been concerned with developing and improving reading and writing skills primarily of native speakers. Very few studies have focused on analyzing the spoken interactions in a law school classroom and addressing the needs of foreign lawyers that attend U. S. law schools. This study reports on an analysis of aural and oral needs of foreign lawyers enrolled in a certificate program at a U. S. law school by focusing on the rhetorical structure of lectures and students' role in classroom interaction. The analysis showed that law school lectures constitute a genre made up of moves, steps <b>and</b> <b>sub-steps</b> comprised within three larger units of organization: Content Introduction, Content Development, and Session Closing. Each of these rhetorical functions is signaled by particular lexical items and grammatical choices. The analysis revealed that not all moves and steps are obligatory. Furthermore, instead of following a linear structure, the moves are highly recursive, which is a notable feature of spoken discourse. In addition, the bulk of the lecture is composed of move cycles which resemble the structure of the analytical and organizational tool widely known in the field of law as IRAC (Issue, Rule, Analysis, Conclusion). The analysis of classroom interaction revealed variation in interactivity across larger units and moves. Turn-taking is most frequently initiated by students' questions fulfilling a number of functions, such as seeking information, confirming understanding, seeking clarification, and applying a legal concept to hypothetical situations. In addition to the functions, the forms of questions were also explored. The thesis concludes with suggestions for employing {{the results of this study}} in an EALP course. In particular, the use of lecture transcripts for developing students' aural and oral skills is advocated...|$|E
40|$|Thesis (MBA) [...] Stellenbosch University, 2014. ENGLISH ABSTRACT: Legal {{costs in}} South Africa are {{generally}} regarded as being too high. This leads to numerous problems. For example, {{a person with a}} valid dispute who cannot afford to have the dispute resolved in court, has limited access to justice, which is a constitutional right. The two methods that are most commonly used by attorneys engaged in litigation in South Africa to account to their clients are: Hourly billing, where clients are billed for the time spent on a matter or for the volume and number of documents created, and contingency fees, also known as “no win no fee” arrangements. Both these billing systems have disadvantages, including the incidence of risk during the litigation process and the incentive to the attorney to act in the client’s best interest. It is submitted that a fixed fee structure would resolve many of the problems experienced by the traditional billing methods. The challenge is to arrive at a fixed fee structure which is based on a proper analysis of the amount of work involved in the legal process. In order to calculate such a fixed fee structure, the legal process has to be broken into a number of steps <b>and</b> <b>sub-steps,</b> and each of these steps should then be analysed to ascertain how much work it entails. The aim {{of this study is to}} arrive at a model for a fixed fee structure which can be implemented at other law firms that are also engaged in litigation work. Primary billing data obtained from the author’s law firm was analysed to ascertain the various steps in the legal process, and to calculate the expected amount of work involved in each step. This data was then used to develop a fixed fee structure model which can be adopted by any litigation law firm, by merely multiplying its own current hourly billing fee structure with the values provided in the model. Certain practical problems which may be encountered during the implementation of the fixed fee structure are also discussed and possible alternative solutions are provided...|$|E
40|$|AbstractDue to shorter product lifecycles and {{a rising}} {{complexity}} of the products more and more enterprises consider using the Digital Factory. The Digital Factory is an IT system capable of digitally planning, controlling and optimizing all resources and activities related to a product which are performed beginning with product development and ending in the order processing – {{prior to the start}} of the real production of the product. Today the companies’ system landscapes within the Digital Factory domain are usually very heterogeneous with limited interoperability of the applications in use. However data integration is crucial for a successful implementation of the Digital Factory. This article provides a reference model based framework consisting of three main steps <b>and</b> ten <b>sub-steps.</b> It can be applied by companies in order to optimize and integrate such heterogeneous system landscapes...|$|R
40|$|Moving mesh partial {{differential}} equations (MMPDEs) {{are used in}} the MMPDE moving mesh method to generate adaptive moving meshes for the numerical solution of time dependent problems. How MMPDEs are formulated and solved is crucial to the efficiency and robustness of the method. In this paper, several practical aspects of formulating and solving MMPDEs are studied. They include spatial balance, scaling invariance, bounds on time steps, multiple <b>sub-steps,</b> <b>and</b> two-level mesh movement. Numerical results are also given...|$|R
40|$|Abstract. Being able {{to access}} and provide Internet {{services}} anonymously is an important mechanism to ensure freedom of speech in vast parts of the world. Offering location-hidden services on the Internet requires complex redirection protocols to obscure the locations and identities of communication partners. The anonymity system Tor supports such a protocol for providing and accessing TCP-based services anonymously. The complexity of the hidden service protocol results in significantly higher response times which is, however, a crucial barrier to user acceptance. This communication overhead becomes even more evident when using limited access networks like cellular phone networks. We provide comprehensive measurements and statistical analysis of the bootstrapping of client processes <b>and</b> different <b>sub-steps</b> of the Tor hidden service protocol {{under the influence of}} limited access networks. Thereby, we are able to identify bottlenecks for low-bandwidth access networks and to suggest improvements regarding these networks. ...|$|R
40|$|Consultable des del TDXTítol obtingut de la portada digitalitzadaEl objetivo general de esta investigación fue describir y analizar las estrategias de lectura aplicadas por {{profesores}} universitarios e investigadores, en calidad de comité tribunal, durante la lectura de un trabajo de investigación, acerca del cual también elaboraron un informe. Los lectores son expertos en la temática de dicho trabajo así como en la utilización de procesos de lectura y escritura académica. Los objetivos específicos de la investigación fueron identificar y analizar: a) la representación de la tarea de los lectores; b) los objetivos planteados; c) las operaciones estratégicas que vinculan procesos de lectura y escritura; d) los tipos de problemas planteados y soluciones; y e) el Abordaje Estratégico Global (AEG) aplicado por los lectores. La investigación se llevó a cabo con una metodología cualitativa en la modalidad de estudio de caso. Las técnicas de obtención de información fueron: 1) el protocolo verbal o think-aloud que los lectores desarrollaron en las sesiones de lectura y que grabaron en vídeo; 2) las entrevistas semi-estructuradas; y 3) el registro de observación de los vídeos de los protocolos verbales. Las unidades de análisis fueron unidades estratégicas que se estructuraron con planteamientos de problemas, procesos de solución y producciones realizados por los lectores durante la revisión del trabajo de investigación. Posteriormente, la información obtenida se organizó y analizó mediante el programa para análisis cualitativo Atlas/ti versión 5. 1. Los resultados individuales indicaron que los lectores desarrollaron distintos abordajes estratégicos de la lectura: el caso A estuvo centrado en evaluar el escrito; el caso B en la búsqueda de información; el caso C en complementar los contenidos del texto; el caso D en prever la información que sería abordada en el escrito y determinar el tipo de lectura en función de ésta; y el caso E en obtener una comprensión profunda del contenido. Los resultados comparativos demostraron que se efectuaron un mayor número de procesos estratégico en el apartado 1 del texto revisado, así también que los lectores se centraron en problemas temáticos del contenido, que efectuaron más procesos inferenciales como procesos de solución y que las producciones realizadas con más frecuencia fueron las de obtención y corrección de información. A partir de los resultados obtenidos, en función de los objetivos específicos que planteamos, concluimos que: a) existió una congruencia entre los criterios de evaluación del trabajo de investigación de los revisores y el procesamiento del contenido del texto y que la toma de consciencia sobre criterios de evaluación de textos académicos {{puede ser}} una herramienta de ayuda para un mayor control de procesos de lectura académica en investigadores noveles; b) algunos lectores llevaron a cabo una lectura por etapas y esto, aplicado por investigadores noveles, puede ser de utilidad para lograr una mayor toma de consciencia sobre los pasos y sub-pasos, así como objetivos que forman parte de la tarea de la lectura, y con ello alcanzar un mayor control sobre el procesamiento del texto; c) la distinción entre planificaciones de tipo local y planificaciones globales puede aportar criterios de evaluación de estrategias de lectura; d) las anotaciones fueron llevadas a cabo de un manera multifuncional; e) los problemas planteados por los lectores pueden servir como criterios de evaluación de textos de investigación; f) las unidades estratégicas utilizadas en esta investigación pueden resultar una herramienta metodológica para el estudio de estrategias de lectura. The {{objective of this}} investigation was to analyze and describe the strategies of reading that five university and investigating professors applied, as committee court, during the reading of a work of investigation, in which also they elaborated a report on this text. The reviewers are expert in the development of processes of reading and academic writing, {{as well as in the}} thematic one tried in the work of investigation that read. The specific objectives of the investigation were to identify and to analyze: a) the representation of the task of the readers; b) the large-scale operations that they tie to processes of reading and writing; c) the objectives raised by the readers; d) the kind of posed problems and the type of found solutions; and e) the global strategic boarding applied by the readers. The investigation was carried out with a qualitative methodology in the modality of case study. The techniques of obtaining of information were: the 1) verbal protocol or think-aloud that the readers developed in the reading sessions and that recorded in video; the 2) semi-structured interviews; and 3) the observation log of the videos of the verbal protocols. The analysis units were strategic units that problems expositions of, process of solution and production realized by the readers were structured with during the revision of the work of investigation. Later, the obtained data were organized and analyzed by means of the program for qualitative analysis Atlas/ti version 5. 1. The individual results indicated that the readers developed different strategic boardings from the reading: the case to was focused in evaluating the text; case B in the information search; case C in complementing the contents of the text; case D in anticipating the information that would be boarded in the writing and determining the type of reading based on this one; and the case E in obtaining a deep understanding of the content. The comparative results demonstrated that a greater strategic number of processes took place in section 1 of the reviewed text, also that the readers concentrated in thematic problems of the content, that carried out more inferencial processes like solution processes and that the productions realized with more frequency were those of obtaining and correction of information. From the obtained results, based on the specific objectives that we raised, we concluded that: a) Existed a congruence between the criteria of evaluation of the text and the processing of the content of the text, and taking conscience on criteria of academic text evaluation can be a tool of aid for a greater process control of academic reading in beginner investigators. b) Some readers carried out a reading by stages and this, applied by beginner investigators, can be useful to obtain a greater taking of conscience on the steps <b>and</b> <b>sub-steps,</b> as well as objectives that comprise of the task of the reading, and in this way reach a greater control on the processing of the text. c) The distinction between global planning of local type can contribute like criteria of evaluation of reading strategies; d) the notes were carried out of a multifunctional way. e) The problems posed by the readers can serve like criteria of evaluation of investigation texts. f) The strategic units used in this investigation can be a methodological tool for the study of reading strategies...|$|E
40|$|Up to {{now there}} are few {{examples}} of highly porous fibers, which have advanta-geous properties. Still nano-porous fibers for textile-relevant applications have not been synthesized successfully. Sol-gel analogous syntheses routes as well as production ways for cellulose aerogel fibers have been worked out. Under-standing how processes work and how structures influence properties has been achieved. Methods related to textile technology, machine engineering, chemical knowledge and material science have been combined to work out cellulose aer-ogel multifilament fibers, which show textile relevant properties including fila-ment diameters smaller than 30 µm, filament strengths in the dimension of wool fibers (up to 15 cN/tex) and specific surfaces up to 500 m 2 /g. Machines <b>and</b> pro-cess <b>sub-steps,</b> such as ion-exchange and solid-phase-extraction procedures have been developed. This development has led to small nonwovens and bob-bins of flexible aerogel fibers. Doors for possible application fields have there-fore been opened...|$|R
40|$|AbstractThis paper {{provides}} new formulations {{to derive}} the impulse response matrix, {{which is then}} used in the problem of load identification with application to wind induced vibration. The applied loads are inversely identified {{based on the measured}} structural responses by solving the associated discrete ill-posed problem. To this end — based on an existing parametric structural model — the impulse response functions of acceleration, velocity and displacement have been computed. Time discretization of convolution integral has been implemented according to an existing and a newly proposed procedure, which differ in the numerical integration methods. The former was evaluated based on a constant rectangular approximation of the sampled data and impulse response function in a number of steps corresponding to the sampling rate, while the latter interpolates the sampled data in an arbitrary number of <b>sub-steps</b> <b>and</b> then integrates over the <b>sub-steps</b> <b>and</b> steps. The identification procedure was implemented for a simulation example as well as an experimental laboratory case. The ill-conditioning of the impulse response matrix made it necessary to use Tikhonov regularization to recover the applied force from noise polluted measured response. The optimal regularization parameter has been obtained by L-curve and GCV method. The results of simulation represent good agreement between identified and measured force. In the experiments the identification results based on the measured displacement as well as acceleration are provided. Further it is shown that the accuracy of experimentally identified load depends on the sensitivity of measurement instruments over the different frequency ranges...|$|R
40|$|Abstract: A meshless {{simulation}} {{system is}} presented for elastic deformation driven by skeleton in this paper. In this system, we propose a new method for calculating node rotation while applying a similar technique with stiffness warping {{to tackle the}} nonlinear large deformation. In our method, all node rotations are evaluated from sampling points in attached skeleton by constructing and solving the diffusion partial differential equation. The experiments indicated that the method can enhance {{the stability of the}} dynamics <b>and</b> avoid fussy <b>sub-step</b> calculation in static deformation edition. Moreover, rational deformation results for the area around the skeleton joints can be simulated without user interaction by adopting the simplified technique...|$|R
40|$|Describing the ConnectinGEO {{methodology}} to {{be developed}} and tested during the project and to be promoted beyond the end of it. The main aim of the ConnectinGEO methodology is to find gaps in EO networks and systems (mainly in-situ or non-space), determine remedies and recommendpriorities to solve these gaps. The gap analysis phase follows five different threads: Top-Down thread 1 : Identification {{of a collection of}} observation requirements Top-Down thread 2 : Research programs aims and targets Bottom-up thread 1 : Consultation process Bottom-up thread 2 : GEOSS Discovery and Access Broker analysis Bottom-up thread 3 : industry-driven challenges. This deliverable describes each thread <b>and</b> enumerates <b>sub-steps</b> for each one. It defines a common data model for gap description that all threads will need to follow and respect to communicate and concentrate gaps in a single list. Then a review process will start and external and internal user feedback will be gathered. To end the review process the feedback will be examined and moderated and some gaps will be discarded but other will be confirmed and profiled. Then, a period for reviewing the gaps and identifying remedies will start. The quantifications of the impact, the feasibility and the costs will permit the application of a semi automatic priority calculation. Once the gap table is sorted by priorities, gaps will be classified and some recommendations will be formulated for the funding agencies...|$|R
40|$|International audienceWe are {{interested}} in the simulation of blood flows in large arteries. In the frame of domain decomposition standard explicit coupling schemes can be viewed as one iteration in a Dirichlet-Neumann algorithm with a convenient first guess. For this class of problems, such coupling schemes are unstable due to the added mass effect. Implicit coupling schemes based on a Dirichlet-Neumann algorithm avoid these instabilities but are computationaly time consuming. Robin-Neumann schemes, recently introduced, guarantee stability and optimal first-order accuracy for a linear Stokes/thin-solid coupled system. In this explicit coupling schemes, the displacement is extrapolated in the fluid <b>sub-step</b> <b>and</b> then it is corrected in the solid. These algorithms will be presented in a fully non-linear framework for the coupling of the Navier-Stokes equations (in ALE form) with a non-linear shell structure or a three dimensional structure. Comparisons in terms of accuracy and efficiency with more standard schemes such as non-incremental explicit schemes and implicit schemes will be discussed...|$|R
40|$|Gesture was {{the first}} mode of {{communication}} for the primitive cave men. Later on human civilization has developed the verbal communication very well. But still nonverbal communication has not lost its weightage. Such non – verbal communication are being used {{not only for the}} physically challenged people, but also for different applications in diversified areas, such as aviation, surveying, music direction etc. It is the best method to interact with the computer without using other peripheral devices, such as keyboard, mouse. Researchers around the world are actively engaged in development of robust and efficient gesture recognition system, more specially, hand gesture recognition system for various applications. The major steps associated with the hand gesture recognition system are; data acquisition, gesture modeling, feature extraction and hand gesture recognition. There are several <b>sub-steps</b> <b>and</b> methodologies associated with the above steps. Different researchers have followed different algorithm or sometimes have devised their own algorithm. The current research work reviews the work carried out in last twenty years and a brief comparison has been performed to analyze the difficulties encountered by these systems, as well as the limitation. Finally the desired characteristics of a robust and efficient hand gesture recognition system have been described...|$|R
40|$|The {{parallel}} full approximation {{scheme in}} space and time (PFASST) introduced by Emmett and Minion in 2012 is an iterative strategy for the temporal parallelization of ODEs and discretized PDEs. As the name suggests, PFASST is similar in spirit to a space-time FAS multigrid method performed over multiple time-steps in parallel. However, since the original focus of PFASST has been on the performance of the method in terms of time parallelism, the solution of any spatial system arising from the use of implicit or semi-implicit temporal methods within PFASST have simply been assumed to be solved to some desired accuracy completely at each <b>sub-step</b> <b>and</b> each iteration by some unspecified procedure. It hence is natural to investigate how iterative solvers in the spatial dimensions can be interwoven with the PFASST iterations and whether this strategy leads to a more efficient overall approach. This paper presents an initial investigation on the relative performance of different strategies for coupling PFASST iterations with multigrid methods for the implicit treatment of diffusion terms in PDEs. In particular, we compare full accuracy multigrid solves at each sub-step with a small fixed number of multigrid V-cycles. This reduces the cost of each PFASST iteration at the possible expense of a corresponding {{increase in the number of}} PFASST iterations needed for convergence. Parallel efficiency of the resulting methods is explored through numerical examples...|$|R
40|$|Abstract. The {{parallel}} full approximation {{scheme in}} space and time (PFASST) introduced by Emmett and Minion in 2012 is an iterative strategy for the temporal parallelization of ODEs and discretized PDEs. As the name suggests, PFASST is similar in spirit to a space-time FAS multigrid method performed over multiple time-steps in parallel. However, since the original focus of PFASST has been on the performance of the method in terms of time parallelism, the solution of any spatial system arising from the use of implicit or semi-implicit temporal methods within PFASST have simply been assumed to be solved to some desired accuracy completely at each <b>sub-step</b> <b>and</b> each iteration by some unspecified procedure. It hence is natural to investigate how iterative solvers in the spatial dimensions can be interwoven with the PFASST iterations and whether this strategy leads to a more efficient overall approach. This paper presents an initial investigation on the relative performance of different strategies for coupling PFASST iterations with multigrid methods for the implicit treatment of diffusion terms in PDEs. In particular, we compare full accuracy multigrid solves at each sub-step with a small fixed number of multigrid V-cycles. This reduces the cost of each PFASST iteration at the possible expense of a corresponding {{increase in the number of}} PFASST iterations needed for convergence. Parallel efficiency of the resulting methods is explored through numerical examples. Key words. Parallel in time, PFASST, multigrid 1. Introduction. Th...|$|R
40|$|Gibbs {{sampling}} is a Bayesian {{inference technique}} {{that is used}} in various scientific domains to generate samples from a certain posterior probability density function, given experimental data. Several software implementations of Gibbs sampling exist, which generally adopt very different approaches, {{because it is not}} easy to make a Gibbs sampling implementation exactly correspond to the theoretical approach. In particular, these implementations may use different approximation algorithms to <b>and</b> solutions to <b>sub-steps</b> of the Gibbs sampling process. Scientists working in different domains often use Gibbs sampling software without knowing the details of the implementation. Nevertheless, it is our experience that understanding the implementation can be crucial to enhance the performance of a model, because a software configuration conceived to help the underlying implementation may end in better approximation of the estimated probabilities functions. JAGS (Just Another Gibbs Sampler) is a widely used open-source implementation of Gibbs sampling. Its installation and user 2 ̆ 7 s guide are accurate, but do not indicate how the software really implements Gibbs sampling and {{it is not easy to}} infer this information from the source code. The aim of this paper is to give a high-level overview of the JAGS algorithms and its extensions that implement Gibbs sampling. Our target reader is a scientist who may want to understand the basic concepts underlying Bayesian inference and Gibbs sampling and who want to be aware of what happens behind the scenes when building a model...|$|R
40|$|We {{present a}} new energy-stable open {{boundary}} condition, and an associated numerical algorithm, for simulating incompressible flows with outflow/open boundaries. This open boundary condition ensures the energy {{stability of the}} system, even when strong vortices or backflows occur at the outflow boundary. Under certain situations it {{can be reduced to}} a form that can be analogized to the usual convective boundary condition. One prominent feature of this boundary condition is that it provides a control over the velocity on the outflow/open boundary. This is not available with the other energy-stable open boundary conditions from previous works. Our numerical algorithm treats the proposed open boundary condition based on a rotational velocity-correction type strategy. It gives rise to a Robin-type condition for the discrete pressure and a Robin-type condition for the discrete velocity on the outflow/open boundary, respectively at the pressure <b>and</b> the velocity <b>sub-steps.</b> We present extensive numerical experiments on a canonical wake flow and a jet flow in open domain to test the effectiveness and performance of the method developed herein. Simulation results are compared with the experimental data as well as with other previous simulations to demonstrate the accuracy of the current method. Long-time simulations are performed for a range of Reynolds numbers, at which strong vortices and backflows occur at the outflow/open boundaries. The results show that our method is effective in overcoming the backflow instability, and that it allows for the vortices to discharge from the domain in a fairly natural fashion even at high Reynolds numbers. Comment: 38 pages, 11 figures, 4 table...|$|R
40|$|This paper {{deals with}} a new critica lstate-based {{constitutive}} model for soft rocks and with its application {{to the analysis of}} the response of a pyroclastic rock during insitu plate load tests. The model, formulated in the single-surface plasticity framework,is characterised by the following mainfeatures:(i) a gen- eralised three-invariant yield surface capable of reproducing a wide set of well-known criteria, (ii) the dependency of the elastic stiffness on the current stress state by means of a hyperelastic formulation and (iii) the ability of simulating the plastic strain driven structure degradation processes by a set of ap- propriate isotropic hardening laws. The constitutive model was implemented in a commercial Finite Element code by means of an explicit modified Euler scheme with automatic <b>sub-stepping</b> <b>and</b> error control. The procedure does not require any form of stress correction to prevent drift from the yield surface. The model was applied to simulate the response of a pyroclastic rock, the Neapolitan Yellow Tuff, to in-situ plate load tests conducted by 500 mm and 300 mm circular plates. In particular,in each lo- cation a first test was carried out adopting thelarge plate, applying a loading and unloading cycle; this was followed by asecond loading stage performed on the same portion of rock by the smaller plate up to larger stress levels. Test results pointed out some specific features of the rock response under such loading conditions, including non-linear elastic behaviour and structure degradation, this latter high- lighted by the overall reduction of theshear strength parameters. The numeric alanalyses showed a fairly good agreement with the in-situ experimental data, substantiating therelevance of theselected con- stitutive assumptions for the soft rock under investigation...|$|R
40|$|A second order {{explicit}} one-step numerical {{method for}} the initial value problem of the general ordinary differential equation is proposed. It is obtained by natural modifications of the well-known leapfrog method, which is a second order, two-step, explicit method. According to the latter method, the input data for an integration step are two system states, which refer to different times. The usage of two states instead of a single one {{can be seen as}} the reason for the robustness of the method. Since the time step size thus is part of the step input data, it is complicated to change this size during the computation of a discrete trajectory. This is a serious drawback when one needs to implement automatic time step control. The proposed modification transforms one of the two input states into a velocity and thus gets rid of the time step dependency in the step input data. For these new step input data, the leapfrog method gives a unique prescription how to evolve them stepwise. The stability properties of this modified method are the same as for the original one: the set of absolute stability is the interval [-i,+i] on the imaginary axis. This implies exponential growth of trajectories in situations where the exact trajectory has an asymptote. By considering new evolution steps that are composed of two consecutive old evolution steps we can average over the velocities of the <b>sub-steps</b> <b>and</b> get an integrator with a much larger set of absolute stability, which is immune to the asymptote problem. The method is exemplified with the equation of motion of a one-dimensional non-linear oscillator describing the radial motion in the Kepler problem. Comment: 41 pages, 25 figure...|$|R
40|$|Finding the key {{determinants}} of Picture Archivingand Communication Systems (PACS) performance in hospitals {{has been a}} conundrumfor decades. This research provides a method toassess the strategic alignment of PACS in hospitalsin order to find these key determinants. PACS touches upon every single part of the imaging workflow chain <b>and</b> associated <b>sub-steps</b> that affects the quality of imaging services and clinical outcomes. Achieving the strategic alignment of PACS and pursuingits intended goals and objectives within hospitals {{seem to be an}} intricate and poorlyexamined process. We therefore highlight the value of a PACS evaluation and maturityframework that enables hospitals to reflect holistically on their PACS performance and supports the process of maturing PACS into hospital operations. This dissertation consists of three interrelated parts. The first part explores the impacts of PACS on hospital workflow using a holistic approach that provides fundamental features for assessing PACS from various perspectives. The second part synthesizes the PACS literature on maturity and evolvability in hospitals and defines the PACS Maturity Model (PMM). The PMM describes five levels of PACS maturity and the corresponding process focus. This is subsequently extended as a strategic planning method for PACS deployment, based on the elaboration of the strategic alignment concept and the maturity growth path concepts for the PACS domain. Finally, the third part develops a rigorous perspective and method that supports the process of situationally aligning PACS in hospitals. We develop such an integral model to assess empirically the maturity and organizational alignment of PACS and its impact upon PACS performances – defined as the multifactorial impacts and benefits produced by the application of PACS in terms of hospital efficiency and clinical effectiveness with respect to PACS workflow and patients’ clinical journeys – in hospitals. A key conclusion of this dissertation is that our conceptual model – validated through data from 64 hospitals within the Netherlands – shows a significant positive impact for the PACS alignment construct on perceived PACS performance. Thus, our expectation that the alignment of PACS {{had a significant impact on}} the multifactorial performance of PACS was confirmed. Based on various sources (case studies, conceptual design, meta-analyses and survey research) a PACS evaluation and maturity framework is developed that enables hospitals to reflect holistically on their PACS performance and likewise has predictive power to explain PACS performance within hospitals’ operations. The framework supports a strategic and systematic deployment of PACS to enhance its subsequent evolvability. Based on the outcomes of the various studies included in this dissertation, we believe that hospitals should follow a strategic PACS maturity planning perspective that drives a continuous process of change and adaptation as well as the co-evolvement and alignment of PACS. Adaptability and changeability should be integral properties, next to traditional and deliberate PACS strategic planning. This research extends the body of knowledge concerning PACS maturity and its evaluation from a business informatics perspective. This is important since the filmless facilities of PACS can be exploited much further, to achieve higher productivity levels and operational efficiency gains in hospitals...|$|R
40|$|Employee {{health and}} safety are a top {{priority}} in aerospace manufacturing. As companies increase their production systems capacity in preparation for upcoming rate targets, new opportunities for continuous improvement start becoming evident and time critical. A strong collaboration of Health and Safety, Quality, Manufacturing and Research and Technology groups is paramount to ensure that adequate technologies are developed and deployed in the right stages of the manufacturing system {{in a way that}} is compliant with both technology readiness and the business needs. The integration of collaborative automation on ergo-motivated continuous improvement projects pose two major challenges in this aerospace manufacturing process. Firstly, the availability of resources to measure the current state, i. e. the identification and prioritization of the <b>sub-steps</b> <b>and</b> specific tasks in the process that require technological intervention. Secondly, the potential incompatibility of production systems, continuous improvement and technology development road maps that limit the speed at which new technologies flow to the shop floor. By leveraging the existence of historical safety performance and labor-tracking data, the proposed methodology offers an immediate approximation of occupational risk of the current state. This allows a "first gate" deliverable for any given continuous improvement project for the Occupational Health and Safety group with minimal use of resources, a framework for the R&D organizations to create and prioritize ergonomically-driven projects and ultimately complement business cases to propel technologies towards final deployment. The methodology results in a statistical risk profile that highlights the manual sub-steps of a product line that show better candidacy for collaborative automation. Continuous improvement and conventional Lean/Six Sigma tools where furthermore applied to demonstrate process capability and move a collaborative robot through the production system implementation roadmap in record timing. by Guillermo Pamanes Castillo. Thesis: S. M., Massachusetts Institute of Technology, Department of Mechanical Engineering, 2016. In conjunction with the Leaders for Global Operations Program at MIT. Thesis: M. B. A., Massachusetts Institute of Technology, Sloan School of Management, 2016. In conjunction with the Leaders for Global Operations Program at MIT. Cataloged from PDF version of thesis. Includes bibliographical references...|$|R
40|$|The Escherichia coli twin-arginine {{translocation}} (Tat) system transports fully {{folded and}} assembled proteins across the inner membrane into the periplasmic space. The E. coli Tat machinery minimally {{consists of three}} integral membrane proteins: TatA, TatB and TatC. A popular model of Tat translocation is that cargo first interacts with a substrate binding complex composed of TatB and TatC and then is transported across the inner membrane through a channel comprised primarily of TatA. The most common method for observing the kinetics of Tat transport, a protease protection assay, lacks the ability to distinguish between individual transport <b>sub-steps</b> <b>and</b> {{is limited by the}} inability to observe translocation in real-time. Therefore, a real-time FRET based assay was developed to observe interactions between the cargo protein pre-SufI, and its initial binding site, the TatBC complex. The cargo was found to first associate with the TatBC complex, and then, {{in the presence of a}} membrane potential (?psi), migrate away from the initial binding site after a 20 - 45 second delay. Since cargo migration away from the TatBC complex was not directly promoted by the presence of a ?psi, the delay likely represents some preparatory step that results in a transport competent translocon. In addition, the Tat system has long been identified as a potential biotechnological tool for protein production. However, much is still unknown about which cargos are suitable for transport by the Tat system. To probe the Tat system?s ability to transport substrates of different sizes and shapes, 18 different cargos were generated using the natural Tat substrate pre-SufI as a base. Transport efficiencies for these cargos indicate that not only is the Tat machinery?s ability to transport substrates determined by the protein?s molecular weight, as well as by its dimensions. In total, these results suggest a dynamic translocon that undergoes functionally significant, ?psi-dependent changes during translocation. Moreover, not every protein cargo can be directed through the Tat translocon by a Tat signal peptide, and this selectivity is not only related to the overall size of the protein, but also dependent on shape...|$|R
40|$|In this {{dissertation}} {{one family}} of second-order and two families of higher-order time integration algorithms are newly developed. For {{the development of}} a new family of second-order time integration algorithms, the original equation of structural dynamics is rewritten as two first order differential equations and one algebraic equation. These equations are called mixed formulations, because they include three different kinds of dependent variables (i. e., the displacement, velocity, and acceleration vectors). Equal linear (for the first <b>sub-step)</b> <b>and</b> quadratic (for the second sub-step) Lagrange type interpolation functions in time are used to approximate all three variables involved in the mixed formulations, then the time finite element method and the collocation method are applied to the velocity-displacement and velocity-acceleration relations of the mixed formulations to obtain one- and two-step time integration schemes, respectively. Newly developed one- and two-step time integration schemes are combined as one complete algorithm to achieve enhanced computational features. Two collocation parameters, which are included in the complete algorithm, are optimized and restated in terms of the spectral radius in the high frequency limit (also called the ultimate spectral radius) for the practical control of algorithmic dissipation. Both linear and nonlinear numerical examples are analyzed by using the new algorithm to demonstrate enhanced performance of it. The newly developed second-order algorithm can include the Baig and Bathe method and the non-dissipative case as special cases of its family. For the development of the first family of higher-order time integration algorithms, the displacement vector is approximated over the time interval by using the Hermite interpolation functions in time. The residual vector is defined by substituting the approximated displacement vector into the equation of structural dynamics. The modified weighted residual method is applied to the residual vector. The weight parameters are used to restate the integral forms of the weighted residual statements as algebraic forms, then these parameters are optimized by using the single-degree-of- freedom problem and its exact solution to achieve improved accuracy and unconditional stability. Numerical examples are used to verify performances of the new algorithms. For the development of the second family of implicit higher-order time integration algorithms, the mixed formulations that include three time dependent variables (i. e., the displacement, velocity and acceleration vectors) are used. The equal degree Lagrange type interpolation functions in time are used to approximate the dependent variables involved in the mixed formulations, and the time finite element method and the modified weighted residual method are applied to the velocity-displacement and velocity-acceleration relations of the mixed formulations. Weight parameters are used and optimized to achieve preferable attributes of time integration algorithms. Specially considered numerical examples are used to discuss some fundamental limitations of well-known second-order accurate algorithms and to demonstrate advantages of using newly developed higher-order algorithms...|$|R
40|$|Valmets own foundry in Karlstad has {{during a}} long period of time had {{problems}} being profitable. The goal today is to turn this around. Today there are three different productionlines; engine blocks, Yankeecylinders and components for the wind turbine industry. This master thesis has focused on examining if value stream mapping is a useful tool to use in a foundry, and therefore outside the typical manufacturing industry. The target has also been to evaluate if there is a possibility to improve the flow in the foundry and also the results. During the 20 weeks this work has been done at the foundry data has been collected and analyzed. It has been possible to confirm irregularities in the production at several places. Due to limited time the focus of this theses has been at three different areas: The sand core manufacturing, the forming of concrete for Yankee cylinders and also the molds for the castings of engine blocks. Together with the measured times and values a new proposition of the sand core manufacturing has been developed. There has also been a request in examining the possibilities of changing from three shifts to two shifts at the sand core manufacturing. This has been examined and economical calculations has been made. Simulations have been done in a program called Flexsim. The Flexsim model was a simplification of the real situation which therefore affects the reliability of the result. To continue improving the model is something that is recommended for future thesises. As a complement to the simulations made in Flexim manually simulations was made too, to examine the possibilities of reducing the number of shifts from three to two. The result was clear, it was possible to go down to two shifts. It would though cost approximately 800. 000 kr more per year. But with this investment it is possible to produce 5 motor blocks a week steadily for a long time which helps with the payoff of the investment. Every sold motor block results in a profit of approximately 150. 000 kr. It was also found that it is problematic to make use of value stream mapping in a process that is large, contains a large number of <b>sub-steps</b> <b>and</b> also is very irregular. However, the value flow analysis could be used as a complement to the other simulations. For the production of Yankee cylinders, it was decided that the focus should be on the introduction of a new cement mixer that Valmet bought into. The problem has been how it will be able to reach all three pits, as these are large and extends over a large area. After discussion, it was decided to proceed with a concept that means that the entire cement mixer will be mobile. Valmet themselves then took the decision that the results shall be recorded and then transferred to Valmet since the introduction of the mixer extends too big for this thesis. In summary, it has been noted that there are a lot of things to do. The foundry is in great need of improvements in productivity, standardization, 5 S and so on, and there is a huge potential in the future to improve profitability. Even outside the three areas that this thesis has focused on...|$|R
