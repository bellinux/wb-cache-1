986|112|Public
25|$|Several studies over {{recent decades}} {{have looked at}} the neural {{mechanisms}} underlying birdsong learning by performing lesions to relevant brain structures involved in the production or maintenance of song or by deafening birds before and/or after song crystallization. Another recent experimental approach was recording the bird's song and then playing it back while the bird is singing, causing perturbed <b>auditory</b> <b>feedback</b> (the bird hears the superposition of its own song and a fragmented portion of a previous song syllable). After Nordeen & Nordeen made a landmark discovery as they demonstrated that <b>auditory</b> <b>feedback</b> was necessary for the maintenance of song in adult birds with crystallized song, Leonardo & Konishi (1999) designed an <b>auditory</b> <b>feedback</b> perturbation protocol in order to explore the role of <b>auditory</b> <b>feedback</b> in adult song maintenance further, to investigate how adult songs deteriorate after extended exposure to perturbed <b>auditory</b> <b>feedback,</b> and to examine the degree to which adult birds could recover crystallized song over time after being removed from perturbed feedback exposure. This study offered further support for role of <b>auditory</b> <b>feedback</b> in maintaining adult song stability and demonstrated how adult maintenance of crystallized birdsong is dynamic rather than static.|$|E
25|$|During singing, the {{activation}} of LMAN neurons {{will depend on}} the match between <b>auditory</b> <b>feedback</b> from the song produced by the bird and the stored song template. If this is true, then the firing rates of LMAN neurons will be sensitive to changes in <b>auditory</b> <b>feedback.</b>|$|E
25|$|Altered <b>auditory</b> <b>feedback,</b> so {{that people}} who stutter hear their voice differently, {{has been used for}} over 50 years in the {{treatment}} of stuttering. Altered <b>auditory</b> <b>feedback</b> effect can be produced by speaking in chorus with another person, by blocking out the person who stutters' voice while talking (masking), by delaying slightly the voice of the person who stutters (delayed <b>auditory</b> <b>feedback)</b> or by altering the frequency of the feedback (frequency altered feedback). Studies of these techniques have had mixed results, with some people who stutter showing substantial reductions in stuttering, while others improved only slightly or not at all. In a 2006 review of the efficacy of stuttering treatments, none of the studies on altered <b>auditory</b> <b>feedback</b> met the criteria for experimental quality, such as the presence of control groups.|$|E
40|$|This paper {{presents}} {{the creation of}} an assembly simulation environment with multisensory <b>feedback</b> (<b>auditory</b> and visual), and the evaluation of the effects of <b>auditory</b> and visual <b>feedback</b> on the task performance in the context of assembly simulation in a virtual environment (VE). This VE experimental system platform brings together complex technologies such as constraint-based assembly simulation, optical motion tracking technology, and real time 3 D sound generation technology around a virtual reality workbench and a common software platform. A peg-in-a-hole and a Sener electronic box assembly task have been used as the task cases to conduct the human factor experiment, using sixteen participants. Both objective performance data (i. e., task completion time, TCT; and human performance error rate, HPER) and subjective opinions (i. e., questionnaires) on the utilization of <b>auditory</b> and visual <b>feedback</b> in a virtual assembly environment (VAE) have been gathered from the experiment. Results showed that the introduction of <b>auditory</b> and/or visual <b>feedback</b> into VAE did improve the assembly task performance. They also indicated that integrated <b>feedback</b> (<b>auditory</b> plus visual) offered better assembly task performance than either feedback used in isolation. Most participants preferred integrated feedback to either individual <b>feedback</b> (<b>auditory</b> or visual) or no feedback. The participants’ comments demonstrated that nonrealistic or inappropriate feedback had a negative effect on the task performance, and easily made them frustrated...|$|R
40|$|This paper {{focuses on}} developing, testing, and {{examining}} the Proagents multimodal learning environment to support blind children's explorative {{learning in the}} area of astronomy. We utilize haptic, auditive, and visual interaction. Haptic and <b>auditory</b> <b>feedbacks</b> make the system accessible to blind children. The system is used as an exploration tool for children's spontaneous and question-driven explorations. High-level interaction and play are essential with environments for young children. Proactive agents support and guide children to deepen their explorations and discover the central concepts and relations in phenomena. It has been challenging to integrate together in a pedagogically relevant way the explorative learning approach, proactive agents' actions, haptic perception's possibilities, and the selected astronomical phenomena. Our tests have shown that children are very interested in using the system and the operations of the agents. Hindawi Open acces...|$|R
50|$|The use of simulation-based {{learning}} in the medical field has many benefits, including patient safety, accelerating diagnostic and therapeutic procedures, unfulfilled demand for medical personnel, medical cost reduction and lowering of medical errors that amount to loss of life and associated costs. The use of current technologies allow for very high fidelity simulations. These include Immersive Virtual Environments (IVEs)- computer based 3D environments known as serious games, and other very highly immersive virtual environments, such as Cave Automatic Virtual environment(CAVE),in which the student sits in a projection room wearing goggles and gloves equipped with sensors. This haptic technology activates the sense of touch, allowing the trainee to interface with a simulated patient, {{as well as to}} receive visual and <b>auditory</b> <b>feedbacks,</b> making the simulated learning experience very realistic.|$|R
25|$|Auditory {{processing}} deficits {{have also}} been proposed {{as a cause of}} stuttering. Stuttering is less prevalent in deaf and hard-of-hearing individuals, and stuttering may be reduced when <b>auditory</b> <b>feedback</b> is altered, such as by masking, delayed <b>auditory</b> <b>feedback</b> (DAF), or frequency altered feedback. There is some evidence that the functional organization of the auditory cortex may be different in people who stutter.|$|E
25|$|While <b>auditory</b> <b>feedback</b> is most {{important}} during speech acquisition, it may be activated less if the model has learned a proper feedforward motor command for each speech unit. But {{it has been shown}} that <b>auditory</b> <b>feedback</b> needs to be strongly coactivated in the case of auditory perturbation (e.g. shifting a formant frequency, Tourville et al. 2005). This is comparable to the strong influence of visual feedback on reaching movements during visual perturbation (e.g. shifting the location of objects by viewing through a prism).|$|E
25|$|In a {{comparable}} way to <b>auditory</b> <b>feedback,</b> also somatosensory feedback can be strongly coactivated during speech production, e.g. {{in the case}} of unexpected blocking of the jaw (Tourville et al. 2005).|$|E
50|$|The {{user-interface}} {{may also}} provide Context sensitive feedback, such as changing {{the appearance of}} the mouse pointer or cursor, changing the menu color, or with applicable <b>auditory</b> or tactile <b>feedback.</b>|$|R
30|$|We focus now on {{designing}} the lowest-order virtual model {{that can provide}} a human with high quality force, <b>auditory,</b> and visual <b>feedback.</b> The simplest design involves making the virtual robot incorporate only one neural oscillator--in this case, the robot is the neural oscillator.|$|R
40|$|Objective This study {{investigated}} the effect of multimodal (visual and <b>auditory)</b> continuous <b>feedback</b> with information about {{the uncertainty of the}} input signal on motor imagery based BCI performance. A liquid floating through a visualization of a funnel (funnel feedback) provided enriched visual or enriched multimodal feedback. Methods In a between subject design 30 healthy SMR-BCI naive participants were provided with either conventional bar feedback (CB), or visual funnel feedback (UF), or multimodal (visual and <b>auditory)</b> funnel <b>feedback</b> (MF). Subjects were required to imagine left and right hand movement and were trained to control the SMR based BCI for five sessions on separate days. Results Feedback accuracy varied largely between participants. The MF feedback lead to a significantly better performance in session 1 as compared to the CB feedback and could significantly enhance motivation and minimize frustration in BCI use across the five training sessions. Conclusion The present study demonstrates that the BCI funnel feedback allows participants to modulate sensorimotor EEG rhythms. Participants were able to control the BCI with the funnel feedback with better performance during the initial session and less frustration compared to the CB feedback. Significance The multimodal funnel feedback provides an alternative to the conventional cursorbar feedback for training subjects to modulate their sensorimotor rhythms...|$|R
25|$|Recently, a {{study was}} done showing that verbal fluency test results can differ {{depending}} on the mental focus of the subject. In this study, mental focus on physical speech production mechanisms caused speech production times to suffer, whereas mental focus on <b>auditory</b> <b>feedback</b> improved these times.|$|E
25|$|The Model M's {{buckling}} spring key design {{gives it}} a unique feel and sound. Unlike more common and cheaper dome switch designs, buckling springs give users unmistakable tactile and <b>auditory</b> <b>feedback.</b> Because of its more defined touch, some users report they can type faster and more accurately on the Model M than on other keyboards.|$|E
25|$|An efference copy of {{the motor}} command for song {{production}} {{is the basis of}} the real-time error-correction signal. During singing, activation of LMAN neurons will depend on the motor signal used to generate the song, and the learned prediction of expected <b>auditory</b> <b>feedback</b> based on that motor command. Error correction would occur more rapidly in this model.|$|E
30|$|The {{auditory}} evaluation {{was divided}} into two parts, the first dealt with the auditory interaction for interventions and the second with the <b>auditory</b> interaction for <b>feedback.</b> Simple commands were issued to AALFI to show participants how to initiate the interaction process and hear the intervention and feedback responses.|$|R
40|$|Driving {{distraction}} {{is a vital}} issue within driving research. This paper discusses ongoing {{research in}} applying auditory cues to enhance song-searching abilities on a portable music player or smartphone while driving. Previous research related to this area has revealed issues with using these devices while driving but some research has shown significant benefits in using audio cues. Categories and Subject Descriptors H. 5. 2 [Information Interfaces And Presentation (e. g., HCI) ]: User Interfaces – <b>Auditory</b> (non-speech) <b>feedback,</b> graphical user interfaces (GUI), interaction styles (e. g., commands, menus, forms, direct manipulation), user-centered design, voice I/...|$|R
40|$|Usability is an {{important}} aspect of Virtual assembly environment (VAE). This paper presents the evaluation of the usability of our developed multi-sensory VAE. The usability evaluation is in terms of its three attributes: (a) efficiency of use; (b) user????s satisfaction; and (c) reliability in use. These are addressed by using task completion times (TCTs), questionnaires, and human performance error rates (HPERs), respectively. A pegin- a-hole and a Sener electronic box assembly task cases have been used to perform the experiments, using sixteen participants. The outcomes showed that the introduction of 3 D <b>auditory</b> and/or visual <b>feedback</b> could improve the usability. They also indicated that the integrated <b>feedback</b> (visual plus <b>auditory)</b> offered better usability than either feedback used in isolation. Most participants preferred the integrated feedback to either <b>feedback</b> (visual or <b>auditory)</b> or no <b>feedback.</b> The possible reasons behind the outcomes are also analyse...|$|R
25|$|Assistive {{technology}} {{in this area}} is broken down into low, mid, and high tech categories. Low tech encompasses equipment that is often low cost and does not include batteries or requires charging. Examples include adapted paper and pencil grips for writing or masks and color overlays for reading. Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing. High tech supports involve the use of tablet devices and computers with accompanying software. Software supports for writing include the use of <b>auditory</b> <b>feedback</b> while keyboarding, word prediction for spelling, and speech to text. Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text. Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and <b>auditory</b> <b>feedback</b> of more complex equations using MathML and Daisy.|$|E
25|$|Because mirror neurons exhibit both sensory {{and motor}} activity, some {{researchers}} have suggested that mirror neurons may serve to map sensory experience onto motor structures. This has implications for birdsong learning– many birds rely on <b>auditory</b> <b>feedback</b> to acquire and maintain their songs. Mirror neurons may be mediating this comparison of what the bird hears, how it compares to a memorized song template, and what he produces.|$|E
25|$|In rare cases, {{stuttering}} may {{be acquired}} in adulthood {{as the result}} of a neurological event such as a head injury, tumour, stroke, or drug use. The stuttering has different characteristics from its developmental equivalent: it tends to be limited to part-word or sound repetitions, and is associated with a relative lack of anxiety and secondary stuttering behaviors. Techniques such as altered <b>auditory</b> <b>feedback</b> (see below), which may promote decreasing disfluency in people who stutter with the developmental condition, are not effective with the acquired type.|$|E
40|$|There is {{established}} efficacy {{for the treatment}} for apraxia of speech (AOS); however, the number of efficacious treatments is few and each {{is in need of}} replication. This single-subject multiple-baseline across-behaviors experimental study assessed the effects of on-line visual kinematic and clinician supplied <b>auditory</b> perceptual <b>feedback</b> on the acquisition, generalization and maintenance of speech production in a person with mild-to-moderate severity AOS. Intervention yielded immediate acquisition of nine sequentially treated behaviors and generalization to numerous untreated targets within and outside of the phoneme classes. Differential acquisition effects were realized for the frequency (50 % versus 100 %) of feedback administratio...|$|R
40|$|We present {{techniques}} to enable users to interact on foot with simulated natural ground surfaces in immersive virtual environments. Position and force estimates from in-floor force sensors {{are used to}} enable interaction with deformable ground surfaces, such as soil or ice, presented in a virtual environment and accompanied by plausible <b>auditory,</b> and vi-brotactile <b>feedback.</b> ...|$|R
40|$|We {{describe}} {{a study on}} how <b>auditory</b> and visual <b>feedback</b> affects eye typing. Results show that the feedback method influences both text entry speed and error rate. In addition, a proper feedback mode facilitates eye typing by reducing the user's need to switch her gaze between the on-screen keyboard and the typed text field...|$|R
25|$|Articulatory and {{acoustic}} feedback signals {{are used for}} generating somatosensory and <b>auditory</b> <b>feedback</b> information via the sensory preprocessing modules, which is forwarded towards the auditory and somatosensory map. At {{the level of the}} sensory-phonetic processing modules, auditory and somatosensory information is stored in short-term memory and the external sensory signal (ES, Fig. 5, which are activated via the sensory feedback loop) can be compared with the already trained sensory signals (TS, Fig. 5, which are activated via the phonetic map). Auditory and somatosensory error signals can be generated if external and intended (trained) sensory signals are noticeably different (cf. DIVA model).|$|E
25|$|Brainard & Doupe (2000) posit a {{model in}} which LMAN (of the {{anterior}} forebrain) plays a primary role in error correction, as it detects differences between the song produced by the bird and its memorized song template and then sends an instructive error signal to structures in the vocal production pathway in order to correct or modify the motor program for song production. In their study, Brainard & Doupe (2000) showed that while deafening adult birds led {{to the loss of}} song stereotypy due to altered <b>auditory</b> <b>feedback</b> and non-adaptive modification of the motor program, lesioning LMAN in the anterior forebrain pathway of adult birds that had been deafened led to the stabilization of song (LMAN lesions in deafened birds prevented any further deterioration in syllable production and song structure).|$|E
25|$|Information in the {{anterior}} forebrain pathway is projected from HVC to Area X (basal ganglia), then from Area X to the DLM (thalamus), and from DLM to LMAN, which then links the vocal learning and vocal production pathways through connections {{back to the}} RA. Some investigators have posited a model in which the connection between LMAN and RA carries an instructive signal based on evaluation of <b>auditory</b> <b>feedback</b> (comparing the bird's own song to the memorized song template), which adaptively alters the motor program for song output. The generation of this instructive signal could be facilitated by auditory neurons in Area X and LMAN that show selectivity for the temporal qualities of the bird's own song (BOS) and its tutor song, providing a platform for comparing the BOS and the memorized tutor song.|$|E
40|$|We present {{techniques}} to enable users to interact on foot with simulated natural ground surfaces, such as soil or ice, in immersive virtual environments. Position and force estimates from in-floor force sensors {{are used to}} synthesize plausible <b>auditory</b> and vibrotactile <b>feedback</b> in response. Relevant rendering techniques {{are discussed in the}} context of walking on a virtual frozen pond. ...|$|R
40|$|We {{explore the}} design space of {{freehand}} pointing and clicking interaction with very large high resolution displays from a distance. Three techniques for gestural pointing and two for clicking are developed and evaluated. In addition, we present subtle <b>auditory</b> and visual <b>feedback</b> techniques {{to compensate for}} the lack of kinesthetic feedback in freehand interaction, and to promote learning and use of appropriate postures...|$|R
40|$|This paper {{presents}} {{the development of}} a portable device for the translation of embossed Braille to text. The device optically scans a Braille page and outputs the equivalent text output in real time, thus acting as a written communications gateway between sighted and vision impaired persons [...] Categories and Subject Descriptors H. 5. 1 [Multimedia Information Systems]: – audio input/output, evaluation/methodology. H. 5. 2 [U s e r I n t e r f a c e s]:- <b>auditory</b> (non-speech) <b>feedback,</b> evaluation/methodology, theory and methods, user-centred design. H. 5. 3 [Group and Organization Interfaces]:-collaborative computing, computer supported cooperative work, evaluation / methodology. K. 4. 2 [Social Issues]:-assistive technologies for persons with disabilities, handicapped persons/special needs...|$|R
25|$|Hearing {{plays an}} {{important}} part in both speech generation and comprehension. When speaking, the person can hear their speech, and the brain uses what it hears as a feedback mechanism to fix speech errors. If a single feedback correction occurs multiple times, the brain will begin to incorporate the correction to all future speech, making it a feed forward mechanism. This is apparent in some deaf people. Deafness, as well as other, smaller deficiencies in hearing, can greatly affect one's ability to comprehend spoken language, as well as to speak it. However, if the person loses hearing ability later in life, most can still maintain a normal level of verbal intelligence. This is thought to be because of the brain's feed forward mechanism still helping to fix speech errors, {{even in the absence of}} <b>auditory</b> <b>feedback.</b>|$|E
25|$|Similarly to the ventral stream's {{auditory}} processing, information {{enters the}} ear {{and then into}} the superior temporal gyrus and then finally the superior temporal sulcus. From there the information moves {{to the beginning of the}} dorsal pathway, which is located at the boundary of the temporal and parietal lobes near the Sylvian fissure. The first step of the dorsal pathway begins in the sensorimotor interface, located in the left Sylvian parietal temporal (Spt) (within the Sylvian fissure at the parietal-temporal boundary). The spt is important for perceiving and reproducing sounds. This is evident because its ability to acquire new vocabulary, be disrupted by lesions and <b>auditory</b> <b>feedback</b> on speech production, articulatory decline in late-onset deafness and the non-phonological residue of Wernicke's aphasia; deficient self-monitoring. It is also important for the basic neuronal mechanisms for phonological short-term memory. Without the Spt, language acquisition is impaired. The information then moves onto the articulatory network, which is divided into two separate parts. The articulatory network 1, which processes motor syllable programs, is located in the left posterior inferior temporal gyrus and Brodmann's area 44 (pIFG-BA44). The articulatory network 2 is for motor phoneme programs and is located in the left M1-vBA6.|$|E
2500|$|Leonardo [...] tested {{these models}} {{directly}} by recording spike rates in single LMAN neurons of adult zebra finches during singing in conditions with normal and perturbed <b>auditory</b> <b>feedback.</b> His results {{did not support}} the BOS-tuned error correction model, as the firing rates of LMAN neurons were unaffected by changes in <b>auditory</b> <b>feedback</b> and therefore, the error signal generated by LMAN appeared unrelated to <b>auditory</b> <b>feedback.</b> Moreover, the results from this study supported the predictions of the efference copy model, in which LMAN neurons are activated during singing by the efference copy of the motor signal (and its predictions of expected <b>auditory</b> <b>feedback),</b> allowing the neurons to be more precisely time-locked to changes in <b>auditory</b> <b>feedback.</b>|$|E
40|$|Virtual {{assembly}} environment (VAE) {{technology has}} the {{great potential for}} benefiting the manufacturing applications in industry. Usability is {{an important aspect of}} the VAE. This paper presents the usability evaluation of a developed multi-sensory VAE. The evaluation is conducted by using its three attributes: (a) efficiency of use; (b) user satisfaction; and (c) reliability. These are addressed by using task completion times (TCTs), questionnaires, and human performance error rates (HPERs), respectively. A peg-in-a-hole and a Sener electronic box assembly task have been used to perform the experiments, using sixteen participants. The outcomes showed that the introduction of 3 D <b>auditory</b> and/or visual <b>feedback</b> could improve the usability. They also indicated that the integrated <b>feedback</b> (visual plus <b>auditory)</b> offered better usability than either feedback used in isolation. Most participants preferred the integrated feedback to either <b>feedback</b> (visual or <b>auditory)</b> or no <b>feedback.</b> The participants’ comments demonstrated that nonrealistic or inappropriate feedback had negative effects on the usability, and easily made them feel frustrated. The possible reasons behind the outcomes are also analysed. </p...|$|R
40|$|Abstract. The MicroTactus is {{a family}} of {{instruments}} that we have de-signed to detect signals arising from the interaction of a tip with soft or hard objects and to magnify them for haptic and auditory reproduction. We constructed an enhanced arthroscopic surgical probe and tested it in detecting surface defects of a cartilage-like material. Elastomeric sam-ples were cut at different depths and mixed with blank samples. Subjects were asked to detect the cuts under four conditions: no amplification, with haptic feedback, with sound feedback, and with passive touch. We found that both haptic and <b>auditory</b> and <b>feedback</b> significantly improved detection performance, which demonstrated that an enhanced arthro-scopic probe provided useful information {{for the detection of}} small cuts in tissue-like materials. ...|$|R
40|$|SummarySpeech {{production}} {{is dependent on}} both <b>auditory</b> and somatosensory <b>feedback</b> [1 – 3]. Although audition {{may appear to be}} the dominant sensory modality in speech production, somatosensory information plays a role that extends from brainstem responses to cortical control [4 – 6]. Accordingly, the motor commands that underlie speech movements may have somatosensory as well as auditory goals [7]. Here we provide evidence that, independent of the acoustics, somatosensory information is central to achieving the precision requirements of speech movements. We were able to dissociate <b>auditory</b> and somatosensory <b>feedback</b> by using a robotic device that altered the jaw's motion path, and hence proprioception, without affecting speech acoustics. The loads were designed to target either the consonant- or vowel-related portion of an utterance because these are the major sound categories in speech. We found that, {{even in the absence of}} any effect on the acoustics, with learning subjects corrected to an equal extent for both kinds of loads. This finding suggests that there are comparable somatosensory precision requirements for both kinds of speech sounds. We provide experimental evidence that the neural control of stiffness or impedance—the resistance to displacement—provides for somatosensory precision in speech production [8 – 10]...|$|R
