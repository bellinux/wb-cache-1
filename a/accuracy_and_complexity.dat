403|10000|Public
5|$|Production of {{components}} would be split between Breguet and BAC, and the aircraft themselves would be assembled on two production lines; {{one in the}} UK and one in France, To avoid any duplication of work, each aircraft component had only one source. The British light strike/tactical support versions were the most demanding design, requiring supersonic performance, superior avionics, a cutting edge nav/attack system of more <b>accuracy</b> <b>and</b> <b>complexity</b> than the French version, moving map display, laser range-finder and marked-target seeker (LRMTS). As a result, the initial Br.121 design needed a thinner wing, redesigned fuselage, a higher rear cockpit, and after-burning engines. While putting on smiling faces for the public, maintaining {{the illusion of a}} shared design, the British design de facto departed from the French sub-sonic Breguet 121 {{to such a degree that}} it was for all intents and purposes a new design.|$|E
6000|$|... 133. Next to {{them come}} {{the men of}} the Renaissance schools, headed by Dürer, who, less careful of the beauty and {{refinement}} of the line, delight in its vigor, <b>accuracy,</b> <b>and</b> <b>complexity.</b> And the essential difference between these men and the moderns is that these central masters cut their line for the most part with a single furrow, giving it depth by force of hand or wrist, and retouching, not in the furrow itself, but with others beside it.[AD] Such work can only be done well on copper, and it can display all faculty of hand or wrist, precision of eye, and accuracy of knowledge, which a human creature can possess. But the dotted or hatched line is not used in this central style, and the higher conditions of beauty never thought of.|$|E
5000|$|Array access {{analysis}} can be largely categorized into exact (or reference-list-based) and summary methods for different tradeoffs of <b>accuracy</b> <b>and</b> <b>complexity.</b> Exact methods are precise but very costly {{in terms of}} computation and space storage, while summary methods are approximate but can be computed quickly and economically.|$|E
5000|$|Targeting <b>accuracy,</b> fluency <b>and</b> <b>complexity.</b> English Teaching Professional,16, 2000, 3-6 ...|$|R
3000|$|..., {{which would}} complicate the {{estimation}} process. There {{should be a}} good balance between the estimation <b>accuracy</b> <b>and</b> the <b>complexity</b> regarding which technique is selected to calculate F [...]...|$|R
30|$|Ranging {{techniques}} have significant effects on location <b>accuracy</b> <b>and</b> system <b>complexity</b> [12, 15]. This section outlines the previous work related to two ranging techniques whose performance was individually evaluated: RSS-based and RTT-based ranging methods.|$|R
5000|$|... for quasi-LPV (qLPV) control theory. It {{transforms}} {{a function}} (which {{can be given}} via closed formulas or neural networks, fuzzy logic, etc.) into TP function form if such a transformation is possible. If an exact transformation is not possible, then the method determines a TP function that is an approximation of the given function. Hence, the TP model transformation can provide a trade-off between approximation <b>accuracy</b> <b>and</b> <b>complexity.</b>|$|E
50|$|The {{preferred}} obesity metric {{in scholarly}} circles {{is the body}} fat percentage (BF%) - {{the ratio of the}} total weight of person's fat to his or her body weight, and BMI is viewed merely as a way to approximate BF%. Levels in excess of 32% for women and 25% for men are generally considered to indicate obesity. However, accurate measurement of body fat percentage is much more difficult than measurement of BMI. Several methods of varying <b>accuracy</b> <b>and</b> <b>complexity</b> exist.|$|E
50|$|In {{the last}} decade multi-objective {{optimization}} of fuzzy rule based systems has attracted wide interest within the research community and practitioners. It {{is based on the}} use of stochastic algorithms for Multi-objective optimization to search for the Pareto efficiency in a multiple objectives scenario. For instance, the objectives to simultaneously optimize can be <b>accuracy</b> <b>and</b> <b>complexity,</b> or accuracy and interpretability. A recent review of the field is provided in the work of Fazzolari et al. (2013). In addition, 1 provides an up-to-date and continuously growing list of references on the subject.|$|E
30|$|For all the {{existing}} methods, {{the accuracy of}} approximation is improved {{at the expense of}} the computational complexity. This paper presents an affine approximation method for multiplication, which achieves better trade-off between <b>accuracy</b> <b>and</b> computational <b>complexity.</b>|$|R
40|$|Accurate next {{web page}} {{prediction}} benefits many applications, e-business in particular. The {{most widely used}} techniques for this purpose are Markov Model, association rules and clustering. However, each of these techniques has its own limitations, {{especially when it comes}} to <b>accuracy</b> <b>and</b> space <b>complexity.</b> This paper presents an improved prediction <b>accuracy</b> <b>and</b> state space <b>complexity</b> by using novel approaches that combine clustering, association rules and Markov Models. The three techniques are integrated together to maximise their strengths. The integration model has been shown to achieve better prediction <b>accuracy</b> than individual <b>and</b> other integrated models...|$|R
40|$|Owing to {{low cost}} and {{relatively}} accurate range measurement, ultrasonic sensors {{are widely used}} in various simultaneous localisation and mapping (SLAM) applications. In spite of the abundance of ultrasonic sensor based SLAM applications, a simple and efficient algorithm for an ultrasonic sensor based positioning system with good <b>accuracy</b> <b>and</b> low computational <b>complexity</b> has not yet emerged. The major difficulty is the trade-off between localisation <b>accuracy</b> <b>and</b> computational <b>complexity</b> in most SLAM algorithms, such as extended Kalman filter (EKF) and particle filter. Typically, they improve localisation accuracy by increasing {{the density of the}} landmarks, as a result leading to high computational <b>complexity</b> of algorithms <b>and</b> limiting the utilisation of algorithms into online SLAM systems. This study addresses an improved particle filter algorithm to solve ultrasonic sensor based 2 D range-only SLAM problem with relatively good <b>accuracy</b> <b>and</b> low computational <b>complexity.</b> This algorithm uses a simple four fixed features based system model to limit the density of the landmarks. A technique called map adjustment is proposed to increase the <b>accuracy</b> <b>and</b> efficiency of the algorithm. Using map adjustment, the proposed particle filter algorithm can improve localisation <b>accuracy</b> <b>and</b> lower computational <b>complexity.</b> The experiment results demonstrate that this algorithm has a better performance than conventional particle filter localisation algorithm...|$|R
50|$|The {{information}} bottleneck {{method is}} a technique in information theory introduced by Naftali Tishby, Fernando C. Pereira, and William Bialek. It is designed for finding the best tradeoff between <b>accuracy</b> <b>and</b> <b>complexity</b> (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution p(X,Y) between X and an observed relevant variable Y. Other applications include distributional clustering, and dimension reduction. In a well defined sense it generalized the classical notion of minimal sufficient statistics from parametric statistics to arbitrary distributions, not necessarily of exponential form. It does so by relaxing the sufficiency condition to capture some fraction of the mutual information with the relevant variable Y.|$|E
50|$|He made {{original}} contributions on system identification, {{including the}} estimation of the orders, time-delays and parameters of stochastic systems. He gave a criterion for time-delay estimate, with which one can get a strong consistent time-delay estimate. He with co-authors initiated the research on the parameter identification and adaptive control of the systems with quantized observations, and investigated the optimal adaptive control and identification errors, time complexity, optimal input design, and impact of disturbances and unmodeled dynamics on identification <b>accuracy</b> <b>and</b> <b>complexity</b> in both stochastic and deterministic frameworks. With a series of significant results, he has established a solid framework for the identification and adaptive control of uncertainty systems with quantized information. This is of great importance for many practical systems, especially, when digital communications are needed.|$|E
50|$|Production of {{components}} would be split between Breguet and BAC, and the aircraft themselves would be assembled on two production lines; {{one in the}} UK and one in France, To avoid any duplication of work, each aircraft component had only one source. The British light strike/tactical support versions were the most demanding design, requiring supersonic performance, superior avionics, a cutting edge nav/attack system of more <b>accuracy</b> <b>and</b> <b>complexity</b> than the French version, moving map display, laser range-finder and marked-target seeker (LRMTS). As a result, the initial Br.121 design needed a thinner wing, redesigned fuselage, a higher rear cockpit, and after-burning engines. While putting on smiling faces for the public, maintaining {{the illusion of a}} shared design, the British design de facto departed from the French sub-sonic Breguet 121 {{to such a degree that}} it was for all intents and purposes a new design.|$|E
40|$|Task {{structure}} as a task characteristic {{is believed to}} have a considerable effect on task performance in terms of <b>accuracy,</b> fluency <b>and</b> <b>complexity.</b> Studies in the literature focused on investigating task structure impact on oral out-put; however, this study examines how written task performance is affected by task structure. Thirty two EFL learners were selected for data collection. Each participant, then, narrated two tasks which differed with regard to their inherent structure (tight vs. loose). Collected data were coded and checked for measuring their <b>accuracy,</b> fluency <b>and</b> <b>complexity.</b> Findings of the statistical analysis revealed that structured tasks produced more fluent and complex performances while accuracy remained unaffected. Results have practical implications for language teaching, research and syllabus design...|$|R
30|$|The {{estimation}} <b>accuracy</b> <b>and</b> computational <b>complexity</b> {{are both}} {{proportional to the}} maxCycle and N. So, in this paper, we select maxCycle to be 10 and N to be 7 to provide a good trade-off between the image quality and time cost.|$|R
30|$|In {{order to}} achieve a good {{tradeoff}} between estimation <b>accuracy</b> <b>and</b> the <b>complexity</b> of measurement, an appropriate choice for the sampling set of link gains is important. Unfortunately, this {{is beyond the scope}} of classical sampling theory. Therefore, we need to resort to some heuristics.|$|R
50|$|As production-based {{emissions}} accounting is currently favoured in policy terms, its methodology is well established. Emissions are calculated not directly but indirectly from fossil fuel usage and other relevant {{processes such as}} industry and agriculture according to 2006 guidelines issued by the IPCC for GHG reporting. The guidelines span numerous methodologies dependent {{on the level of}} sophistication (Tiers 1-3 in Table 2). The simplest methodology combines the extent of a human activity with a coefficient quantifying the emissions from that activity, known as an ‘emission factor’. For example to estimate emissions from the energy sector (typically contributing over 90% of CO2 emissions and 75% of all GHG emissions in developed countries) the quantity of fuels combusted is combined with an emission factor - the level of sophistication increasing with the <b>accuracy</b> <b>and</b> <b>complexity</b> of the emission factor. Table 2 outlines how the UK implements these guidelines to estimate some of its emissions-producing activities.|$|E
3000|$|Are there {{significant}} differences in the writing fluency, <b>accuracy</b> <b>and</b> <b>complexity</b> before and after the adoption of BNCweb-assisted DDL activities? [...]...|$|E
40|$|A {{measure of}} {{efficiency}} for influence diagram models with continuous decision variables that considers both the <b>accuracy</b> <b>and</b> <b>complexity</b> of the representation and solution tech-nique is presented. Accuracy {{is determined by}} calculating the mean squared error between influence diagram decision rules and an analytical solution. Complexity is assessed by cal-culating {{the size of the}} functions in the numerical representation at each stage of the solution to reflect both storage requirements and potential computational complexity of downstream mathematical operations. The resulting efficiency score considers the pref-erences of an individual decision maker for <b>accuracy</b> <b>and</b> <b>complexity.</b> Three influence diagram models proposed for use with continuous decision variables are compared using the efficiency measurement. The best model for a given problem may vary based on a decision maker’s willingness to substitute <b>accuracy</b> <b>and</b> <b>complexity.</b> ...|$|E
40|$|From Milsom's equations, which {{describe}} {{the geometry of}} ray-path hops reflected from the ionospheric F-layer, algorithms for the simplified estimation of mirror-reflection height are developed. These allow for hop length {{and the effects of}} variations in underlying ionisation (via the ratio of the F 2 - and E-layer critical frequencies) and F 2 -layer peak height (via the M(3000) F 2 -factor). Separate algorithms are presented which are applicable to a range of signal frequencies about the FOT and to propagation at the MUF. The <b>accuracies</b> <b>and</b> <b>complexities</b> of the algorithms are compared with those inherent in the use of a procedure based on an equation developed by Shimazaki...|$|R
40|$|Kinetic {{equations}} {{bridge the}} gap between a microscopic description and a macroscopic description of the physical reality. Due to the high dimensionality the construction of numerical methods represents a challenge and requires a careful balance between <b>accuracy</b> <b>and</b> computational <b>complexity.</b> Comment: Springer "Encyclopedia of Applied and Computational Mathematics", 201...|$|R
40|$|Abstract — Reliability and its {{modeling}} {{have become}} critical issues for nanotechnology-based circuits. This paper considers {{the use of}} probabilistic models of unreliable logic gates to estimate the reliability of nanoelectronic circuits and derive fundamental error bounds for logic gates. Two methods are experimentally contrasted with respect to <b>accuracy</b> <b>and</b> computational <b>complexity...</b>|$|R
40|$|The {{present study}} {{compared}} {{the effects of}} reading input flooding and listening input flooding techniques on the <b>accuracy</b> <b>and</b> <b>complexity</b> of Iranian EFL learners’ speaking skill. Participants were 66 homogeneous intermediate EFL learners who were randomly divided into three groups of 22 : Reading input flooding group, listening input flooding group, and control group. The reading flooded input group was exposed to the numerous examples of the target structures through reading. In the same phase, the listening group was given relatively the same task, through listening. The participants’ monologues in the posttest were separately recorded, and later transcribed and coded in terms of <b>accuracy</b> <b>and</b> <b>complexity</b> through Bygate’s (2001) standard coding system. The results of ANCOVA indicated the outperformance of reading input flooding group. The study also supported the trade-off effects (Skehan, 1998, 2009) between <b>accuracy</b> <b>and</b> <b>complexity...</b>|$|E
3000|$|Which type of corpora, BNCweb or {{the search}} engine Baidu, is more useful for Chinese EFL {{learners}} in DDL activities {{to facilitate the}} writing development in terms of fluency, <b>accuracy</b> <b>and</b> <b>complexity?</b> [...]...|$|E
40|$|Over the years, {{researchers}} {{have been trying to}} help learners to approach a task in an effective way to promote their second language (L 2) production and development. Researchers have found that giving planning time to learners can improve their L 2 development (Foster & Skehan, 1996, Wendel, 1997). However, a few current findings about task planning are non comprehensive and nonsystematic. To fill this gap, this study examined the effects of both planning conditions and gender on learners’ oral performance in terms of <b>accuracy</b> <b>and</b> <b>complexity</b> among 40 Iranian students at an intermediate level. A total number of 40 participants based on an interview were selected for this study. Based on planning conditions, the participants were assigned to planning and non planning groups. Secondly, they were divided into male and female groups. The findings revealed that 10 -minute planning, in comparison with no planning time, improved the <b>accuracy</b> <b>and</b> <b>complexity</b> of participants’ oral performance. Also, results regarding the effects of gender clarified that gender did not have any effect on the participants’ oral performance in terms of <b>accuracy</b> <b>and</b> <b>complexity...</b>|$|E
40|$|Abstract. The {{decision}} tree {{is a popular}} and widely-used classification model. The two main objectives in {{decision tree}} induction are accurate predictions for unseen instances and human comprehensibility. In this paper, we use multiobjective optimization {{for the evolution of}} decision tree classifiers that are efficient both with respect to classification <b>accuracy</b> <b>and</b> classifier <b>complexity.</b> Simpler decision trees are generally more comprehensible to humans at the expense of accuracy. We employ the Multiobjective Particle Swarm Optimization using Crowding Distance (MOPSO-CD) algorithm to evolve a population of decision trees that are optimal on two objectives: classification <b>accuracy</b> <b>and</b> classifier <b>complexity</b> based on the Minimum Description Length Principle. The validity and performance of this approach is evaluated on several commonly-used benchmark datasets. The results show that our approach is indeed effective in inducing multiple decision trees that are accurate and simple. ...|$|R
30|$|In this work, we {{consider}} as benchmarks the TS approach [1] and the SRM algorithm [7] for the mean AoA and AS estimation, and the TR approach [13] and the ACF-based algorithm [8] for the maximum DS estimation. These recent works were chosen because they currently offer best trade-off between estimation <b>accuracy</b> <b>and</b> computational <b>complexity.</b>|$|R
40|$|The signed k-distance {{transformation}} (k-DT) computes the k nearest prototypes {{from each}} location on a discrete regular grid {{within a given}} D dimensional volume. We propose a new k-DT algorithm that divides the problem into D 1 -dimensional problems <b>and</b> compare its <b>accuracy</b> <b>and</b> computational <b>complexity</b> to the existing raster-scanning and propagation approaches. 1...|$|R
3000|$|... [...]) are the radial basis {{function}} and, less frequently, the polynomial kernel. The generalization {{performance of}} a SVM depends on parameters regulating the trade-off between <b>accuracy</b> <b>and</b> <b>complexity</b> in the training process [32] and the kernel-specific parameters.|$|E
30|$|Accuracy and {{convergence}} of the memetic computing GA-SQP is found better {{in each case}} of the simulation study and effectiveness of the scheme is further established through results of statistics based on different performance indices for <b>accuracy</b> <b>and</b> <b>complexity.</b>|$|E
40|$|In this manuscript, a {{new method}} for calculating the {{artificial}} holograms of three dimensional objects is described. This approximate but fast method is compared with previously defined methods {{in terms of}} <b>accuracy</b> <b>and</b> <b>complexity.</b> In addition, an example application of the method to vibration mode shape analysis is presented...|$|E
40|$|To {{estimate}} {{the states and}} uncertainty of the under-actuated ship, the paper proposed a modified UKF algorithm-Singular Value Decomposition UKF based on acceleration, which simplifies the system reference equation, reduces the calculation <b>complexity</b> <b>and</b> increases the <b>accuracy</b> of the algorithm. The <b>accuracy</b> <b>and</b> calculation <b>complexity</b> of the algorithm are analyzed in detai 1. Numeric simulation demonstrates {{the effectiveness of the}} algorithm. University of Kentucky Lexingto...|$|R
40|$|We {{present a}} {{complete}} modeling technique for inductive parasitics, {{based on the}} vector potential equivalent circuit (VPEC) topology. Novel algorithms for layout extraction and sparsification are introduced. Examples are {{discussed in terms of}} CPU time, <b>accuracy,</b> <b>and</b> model <b>complexity.</b> Finally, extensions for high frequency applications are presented, in-cluding models for skin effect and full wave simulation. ...|$|R
40|$|For my project, I {{modified}} the SBB code {{to reduce its}} training time and achieve cache constancy. Performance of the modified algorithm was evaluated on three datasets from the 2003 NIPS competition and benchmarked against the original SBB algorithm. The results show {{a significant reduction in}} training time while maintaining or improving classification <b>accuracy</b> <b>and</b> solution <b>complexity...</b>|$|R
