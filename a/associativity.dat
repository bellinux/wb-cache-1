2012|24|Public
5|$|NMDA receptor-dependent LTP {{exhibits}} several properties, including input specificity, <b>associativity,</b> cooperativity, and persistence.|$|E
5|$|The last {{equality}} {{follows from}} the above-mentioned <b>associativity</b> of matrix multiplication.|$|E
5|$|<b>Associativity</b> {{refers to}} the {{observation}} that when weak stimulation of a single pathway is insufficient for the induction of LTP, simultaneous strong stimulation of another pathway will induce LTP at both pathways.|$|E
50|$|The use of {{operator}} precedence classes and <b>associativities</b> {{is just one}} way. However, {{it is not the}} most general way: this model cannot give an operator more precedence when competing with '−' than it can when competing with '+', while still giving '+' and '−' equivalent precedences and <b>associativities.</b> A generalized version of this model (in which each operator can be given independent left and right precedences) can be found at http://compilers.iecc.com/comparch/article/01-07-068.|$|R
40|$|International audienceOpen-ended {{questions}} {{are commonly used}} in sensory analyses, and are usually dealt with by correspondence analysis (CA) of the term-respondent matrix. CA is apt in detecting strong associations between terms and groups of respondents, but less so when the {{questions are}} interpreted differently among respondents and, thus, seem to open a polysemic space for the answers. Also, CA offers little flexibility in filtering out irrelevant textual structure, or in controlling the relative contribution of rare versus frequent terms in the overall analysis. This contribution presents methodological extensions of CA together with application on a survey of 1900 responses bearing upon {{the understanding of the}} term "minerality" in wine, whose ambiguity is well attested. Clusters of terms, associated to different meanings of "minerality", are successfully retrieved and visualized. Technically, term-respondent matrix generates a weighted undirected network of positive definite edge weights between terms, interpretable as Markov <b>associativities</b> between terms, whose marginals define term weights. Its eigen-structure is intimately related to spectral clustering, as well as to K-means clustering and MDS visualization of chi 2 dissimilarities between terms. The <b>associativities</b> can be renormalized by multiplying edge weights by powers of term weights, enabling the analyst to control the contribution of term weights. Also, modularity maximisation, popular for its efficient yet arguably instable clustering properties, is shown to correspond to a variant of spectral clustering for some power of renormalized <b>associativities...</b>|$|R
40|$|Abstract — While higher <b>associativities</b> {{are common}} at L- 2 or Last-Level cache hierarchies, direct-mapped and low {{associative}} caches are still used at L- 1 level. Lower <b>associativities</b> result in higher miss rates, but have fast access times on hits. Another issue that inhibits cache performance is the non-uniformity of accesses exhibited by most applications: some sets are underutilized while others receive {{the majority of}} accesses. Higher associative caches mitigate access non-uniformities, but do not eliminate them. This implies that increasing the size of caches or <b>associativities</b> may not lead to proportionally improved cache hit rates. Several solutions have been proposed in the literature {{over the past decade}} to address the non-uniformity of accesses; and each proposal independently claims improvements. However, because the published results use different benchmarks and different experimental setups, {{it is not easy to}} compare them. In this paper we report a side-by-side comparison of these techniques. The conclusion of our work is that, each application may benefit from a different technique and no single scheme works universally well for all applications. Our research is investigating the use of multiple techniques within a processor core and across cores in multicore system to improve the performance of cache memory hierarchies. The study reported in this paper allows us to select best possible solutions for each running application. In this paper, we have included some preliminary results of using multiple solutions simultaneously when running multiple threads...|$|R
5|$|LTP can {{be induced}} either by strong tetanic {{stimulation}} {{of a single}} pathway to a synapse, or cooperatively via the weaker stimulation of many. When one pathway into a synapse is stimulated weakly, it produces insufficient postsynaptic depolarization to induce LTP. In contrast, when weak stimuli are applied to many pathways that converge on a single patch of postsynaptic membrane, the individual postsynaptic depolarizations generated may collectively depolarize the postsynaptic cell enough to induce LTP cooperatively. Synaptic tagging, discussed later, may be a common mechanism underlying <b>associativity</b> and cooperativity. Bruce McNaughton argues that any difference between <b>associativity</b> and cooperativity is strictly semantic.|$|E
5|$|The {{commutativity}} and <b>associativity</b> {{of rational}} addition {{is an easy}} consequence {{of the laws of}} integer arithmetic. For a more rigorous and general discussion, see field of fractions.|$|E
5|$|The {{commutativity}} and <b>associativity</b> of real addition are immediate; {{defining the}} real number 0 {{to be the}} set of negative rationals, it is easily seen to be the additive identity. Probably the trickiest part of this construction pertaining to addition {{is the definition of}} additive inverses.|$|E
50|$|Another useful metric to {{test the}} {{performance}} is Power law of cache misses. It gives you the number of misses when you change {{the size of the}} cache, given that the number of misses for one of the cache sizes is known. Similarly, when you want {{to test the}} performance of the cache in terms of misses across different <b>associativities,</b> Stack distance profiling is used.|$|R
40|$|International audienceOver {{the past}} 20 years, the word "minerality" has been {{increasingly}} {{used in the}} description of wines. However, a precise definition of the concept of minerality appears to be inexistent, and no consensual meaning, even among wine professionals, can be identified. Although this word usage seems to spread out from wine professionals to consumers, research on what consumers assume about minerality is scarce. This paper aims to study the various concepts about minerality held-by consumers by using an open-ended questionnaire. A total of 1697 French-speaking consumers responded to an, online survey and their free answers were analysed using statistical textual methods. The clustering around latent variables (CLV) method was used, taking into account both the lexicon used and the personal characteristics of consumers to classify them. Word <b>associativities</b> were then computed by means of renormalized Markov <b>associativities,</b> generating textual networks associated to each group, as well as to personal characteristics of the consumers. Typically, the most inexperienced consumers confess to have never heard about minerality in wine. Then, young women, also endowed with little wine competences, mainly associate minerality to mineral ions as those found in bottled water. Slightly older consumers embed the concept of minerality into the idea of terroir. Finally, the most experienced consumers refer to sensory perceptions such as gunflint or acidity. Those findings are consistent with a lexical innovation process, diffusing from wine professionals to consumers, referring to the mineral kingdom (as opposed to animal or vegetal), and aiming to stress that the style of their wines has changed-towards more subtlety. Beyond the specific minerality issue investigated in this paper, the methodology (CLV approach used in conjunction with renormalized Markov <b>associativities)</b> demonstrates its ability to generate informative clusters of textual networks, highlighting the cores of prototypical sentences, and apt to investigate the meaning of new concepts...|$|R
40|$|Caches in FPGAs {{can improve}} the {{performance}} of soft processors and other applications beset by slow storage components. In this paper we present a cache generator which can produce caches {{with a variety of}} <b>associativities,</b> latencies, and dimensions. This tool allows system designers to effortlessly create, and investigate different caches in order to better meet the needs of their target system. The effect of these three parameters on the area and speed of the caches is also examined and we show that the designs can meet a wide range of specifications and are in general fast and compact. 1...|$|R
5|$|The {{synaptic}} tag hypothesis {{may also}} account for LTP's <b>associativity</b> and cooperativity. <b>Associativity</b> (see Properties) is observed when one synapse is excited with LTP-inducing stimulation while a separate synapse is only weakly stimulated. Whereas {{one might expect}} only the strongly stimulated synapse to undergo LTP (since weak stimulation alone is insufficient to induce LTP at either synapse), both synapses will in fact undergo LTP. While weak stimuli are unable to induce protein synthesis in the cell body, they may prompt the synthesis of a synaptic tag. Simultaneous strong stimulation of a separate pathway, capable of inducing cell body protein synthesis, then may prompt the production of plasticity-related proteins, which are shipped cell-wide. With both synapses expressing the synaptic tag, both would capture the protein products resulting in the expression of LTP in both the strongly stimulated and weakly stimulated pathways.|$|E
5|$|A set of {{elements}} under two binary operations, + and −, {{is called a}} Euclidean domain if it forms a commutative ring R and, roughly speaking, if a generalized Euclidean algorithm can be performed on them. The two operations of such a ring need not be the addition and multiplication of ordinary arithmetic; rather, they can be more general, such as the operations of a mathematical group or monoid. Nevertheless, these general operations should respect many of the laws governing ordinary arithmetic, such as commutativity, <b>associativity</b> and distributivity.|$|E
5|$|One {{reason for}} the {{popularity}} of the local protein synthesis hypothesis is that it provides a possible mechanism for the specificity associated with LTP. Specifically, if indeed local protein synthesis underlies L-LTP, only dendritic spines receiving LTP-inducing stimuli will undergo LTP; the potentiation will not be propagated to adjacent synapses. By contrast, global protein synthesis that occurs in the cell body requires that proteins be shipped out to every area of the cell, including synapses that have not received LTP-inducing stimuli. Whereas local protein synthesis provides a mechanism for specificity, global protein synthesis would seem to directly compromise it. However, as discussed later, the synaptic tagging hypothesis successfully reconciles global protein synthesis, synapse specificity, and <b>associativity.</b>|$|E
40|$|Several schemes {{have been}} {{proposed}} that incorporate an auxiliary buffer to improve {{the performance of a}} given size cache. Victim caching, aims to reduce the impact of conflict misses in direct-mapped caches. Victim offers competitive performance benefits, but requires a costly data path for swaps and saves between the main cache and the added buffer. Several multilateral schemes (e. g. NTS, PCS) offer competitive performance with Victim across a wide range of <b>associativities,</b> but require no swap/save data path. While these schemes perform well overall, their overall performance lags that of Victim when the main cache is direct-mapped. Furthermore, they also require costly hardware support, but in the form of history tables for maintaining allocation decision information...|$|R
40|$|Soft Processors, {{which are}} {{processors}} {{implemented in the}} programmable fabric on FPGAs, are finding a multitude of applications in modern systems. An important part of processor design are the caches {{that have been used}} to alleviate the degradation in performance caused by accessing slow memory. In this paper we present a cache generator which can produce caches with a variety of <b>associativities,</b> latencies, and dimensions. This tool allows processor system designers to effortlessly create, and investigate different caches in order to best meet the needs of their target system. The effect of these three parameters on the area and speed of the caches is also examined and we show that the designs can meet a wide range of specifications and are in general fast and compact. 1...|$|R
40|$|By using graded (super) Lie algebras, we can {{construct}} noncommutative superspace on curved homogeneous manifolds. In this paper, {{we take a}} flat {{limit to}} obtain flat noncommutative superspace. We particularly consider $d= 2 $ and $d= 4 $ superspaces based on the graded Lie algebras $osp(1 | 2) $, $su(2 | 1) $ and $psu(2 | 2) $. Jacobi identities of supersymmetry algebras and <b>associativities</b> of star products are automatically satisfied. Covariant derivatives which commute with supersymmetry generators are obtained and chiral constraints can be imposed. We also discuss that these noncommutative superspaces {{can be understood as}} constrained systems analogous to the lowest Landau level system. Comment: 29 pages, Latex. A subsection is added to explain the Seiberg's noncommutative superspace as a constrained syste...|$|R
5|$|A finite {{field is}} a set of numbers with four {{generalized}} operations. The operations are called addition, subtraction, multiplication and division and have their usual properties, such as commutativity, <b>associativity</b> and distributivity. An example of a finite field is the set of 13 numbers {0,1,2,…,12} using modular arithmetic. In this field, the results of any mathematical operation (addition, subtraction, multiplication, or division) is reduced modulo 13; that is, multiples of 13 are added or subtracted until the result is brought within the range 0–12. For example, the result of 5×7=35mod13=9. Such finite fields can be defined for any prime p; using more sophisticated definitions, they can also be defined for any power m of a prime p'm. Finite fields are often called Galois fields, and are abbreviated as GF(p) or GF(p'm).|$|E
25|$|Thus {{the group}} {{operation}} is associative. On the other hand, Pascal's theorem {{follows from the}} above <b>associativity</b> formula, and thus from the <b>associativity</b> of the group operation of elliptic curves by way of continuity.|$|E
25|$|<b>Associativity</b> so {{feedback}} can be {{sent from}} manufacturing back to design.|$|E
40|$|Many schemes {{have been}} {{proposed}} that incorporate an auxiliary buffer to improve {{the performance of a}} given size cache. One of the most thoroughly evaluated of these schemes, Victim caching, aims to reduce the impact of conflict misses in direct-mapped caches. While Victim has shown large performance benefits, its competitive advantage is limited to direct-mapped caches, whereas today's caches are increasingly associative. Furthermore, it requires a costly data path for swaps and saves between the cache and the buffer. Several other schemes attempt to obtain the performance improvements of Victim, but across a wide range of <b>associativities</b> and without the costly data path for swaps and saves. While these schemes have been shown to perform well overall, their performance still lags that of Victim when the main cache is direct-mapped. Furthermore, they also require costly hardware support, but in the form of history tables for maintaining allocation decision information. This [...] ...|$|R
40|$|Increasing {{the speed}} of cache {{simulation}} to obtain hit/miss rates en- ables performance estimation, cache exploration for embedded sys- tems and energy estimation. Previously, such simulations, particu- larly exact approaches, have been exclusively for caches which uti- lize the least recently used (LRU) replacement policy. In this paper, we propose a new, fast and exact cache simulation method for the First In First Out(FIFO) replacement policy. This method, called DEW, is able to simulate multiple level 1 cache configurations (dif- ferent set sizes, <b>associativities,</b> and block sizes) with FIFO replace- ment policy. DEW utilizes a binomial tree based representation of cache configurations and a novel searching method to speed up sim- ulation over single cache simulators like Dinero IV. Depending on different cache block sizes and benchmark applications, DEW oper- ates around 8 to 40 times faster than Dinero IV. Dinero IV compares 2. 17 to 19. 42 times more cache ways than DEW to determine accu- rate miss rates...|$|R
40|$|It {{is shown}} that the {{category}} of presheaves of symmetric spectra on a small Grothendieck site C admits a proper closed simplicial model structure so that the associated homotopy category is adjoint equivalent to the stable category associated to presheaves of spectra on C. Introduction The main theorems of the Hovey-Shipley-Smith paper [4] {{say that there is}} a proper closed simplicial model category structure on the category Spt Σ of symmetric spectra, and that the associated homotopy category Ho(Spt Σ) is equivalent to the stable category. This paper shows how to generalize these results to the category PreSpt Σ (C) of presheaves of symmetric spectra on an arbitrary small Grothendieck site C. The results of this paper give an underlying symmetric monoidal model for the stable category of presheaves of spectra. We therefore have a foundation for the discussion of higher <b>associativities</b> and symmetries of the smash product for presheaves and sheaves of spectra in [...] ...|$|R
25|$|An n-ary quasigroup with an n-ary {{version of}} <b>associativity</b> is called an n-ary group.|$|E
25|$|Operation laws like (+) and (*) <b>associativity</b> and {{addition}} conmutativity are {{not related}} with the typeclass, but rather proofs to be checked on the instances.|$|E
25|$|Whereas Cayley–Dickson and split-complex {{constructs}} {{with eight}} or more dimensions are not associative {{with respect to}} multiplication, Clifford algebras retain <b>associativity</b> at any number of dimensions.|$|E
40|$|Fetching {{instructions}} from a set-associative cache in an embedded processor can consume {{a large amount}} of energy due to the tag checks performed. Recent proposals to address this issue involve predicting or memoizing the correct way to access. However, they also require significant hardware storage which negates much of the energy saving. This paper proposes way-placement to save instruction cache energy. The compiler places the most frequently executed instructions {{at the start of the}} binary and at runtime these are mapped to explicit ways within the cache. We compare with a state-of-the-art hardware technique and show that our scheme saves almost 50 % of the instruction cache energy compared to 32 % for the hardware approach. We report results on a variety of cache sizes and <b>associativities,</b> achieving 59 % instruction cache energy savings and an ED product of 0. 80 in the best configuration with negligible hardware overhead and no ISA changes. ...|$|R
40|$|International audienceCurrent compilers cannot {{generate}} {{code that}} {{can compete with}} hand-tuned code in efficiency, even for a simple kernel like matrix–matrix multiplication (MMM). A key step in program optimization is the estimation of optimal values for parameters such as tile sizes and number of levels of tiling. The scheduling parameter values selection {{is a very difficult}} and time-consuming task, since parameter values depend on each other; this is why they are found by using searching methods and empirical techniques. To overcome this problem, the scheduling sub-problems must be optimized together, as one problem and not separately. In this paper, an MMM methodology is presented where the optimum scheduling parameters are found by decreasing the search space theoretically, while the major scheduling sub-problems are addressed together as one problem and not separately according to the hardware architecture parameters and input size; for different hardware architecture parameters and/or input sizes, a different implementation is produced. This is achieved by fully exploiting the software characteristics (e. g., data reuse) and hardware architecture parameters (e. g., data caches sizes and <b>associativities),</b> giving high-quality solutions and a smaller search space. This methodology refers {{to a wide range of}} CPU and GPU architectures...|$|R
40|$|Abstract In this paper, {{we propose}} a {{methodology}} for energy reduction and performance improvement. The target system comprises of an instruction scratchpad memory {{instead of an}} instruction cache. Highly utilized code segments are copied into the scratchpad memory, and are executed from the scratchpad. The copying of code segments from main memory to the scratchpad is performed during runtime. A custom hardware controller is used to manage the copying process. The hardware controller is activated by strategically placed custom instructions within the executing program. These custom instructions inform the hardware controller when to copy during program execution. Novel heuristic algorithms are implemented to determine locations within the program to insert these custom instructions, {{as well as to}} choose the best sets of code segments to be copied to the scratchpad memory. For a set of realistic benchmarks, experimental results indicate the method uses 50. 7 % lower energy (on average) and improves performance by 53. 2 % (on average) when compared to a traditional cache system which is identical in size. Cache systems compared had sizes ranging from 256 to 16 K bytes and <b>associativities</b> ranging from 1 to 32. 1...|$|R
25|$|To {{check that}} the {{symmetric}} group on a set X is indeed a group, {{it is necessary to}} verify the group axioms of closure, <b>associativity,</b> identity, and inverses.|$|E
25|$|An {{associative}} quasigroup {{is either}} empty or is a group, since {{if there is}} at least one element, the existence of inverses and <b>associativity</b> imply the existence of an identity.|$|E
25|$|An element a in E {{is called}} {{invertible}} {{if it is}} invertible in C. Power <b>associativity</b> shows that L(a) and L(a−1) commute. Moreover, a−1 is invertible with inverse a.|$|E
40|$|The {{register}} file is {{a critical}} component in a modern superscalar processor. It must {{be large enough to}} accommodate the results of all in-flight instructions. It must also have enough ports to allow simultaneous issue and writeback of many values each cycle. However, this makes {{it one of the most}} energy-consuming structures within the processor with a high access latency. As technology scales, there comes a point where register accesses are the bottleneck to performance and so must be pipelined over several cycles. This increases the pipeline depth, lowering performance. To overcome these challenges, we propose a novel use of compiler analysis to aid register caching. Adding a register cache allows us to preserve single-cycle register accesses, maintaining performance and reducing energy consumption. We do this by passing information to the processor using free bits in a real ISA, allowing us to cache only the most important registers. Evaluating the register cache over a variety of sizes and <b>associativities</b> and varying the read ports into the cache, our best scheme achieves an energy-delay-squared (EDD) product of 0. 81, with a performance increase of 11 %. Another configuration saves 13 % of register system energy. Using four register cache read ports brings both performance gains and energy savings, consistently outperformin...|$|R
40|$|This is the Accepted Manuscript {{version of}} the {{following}} article: V. Kelefouras, A Kritikakou I. Mporas, V. Kolonias, ???A high-performance matrix???matrix multiplication methodology for CPU and GPU architectures???, The Journal of Supercomputing, Vol. 72 (3) : 804 - 844, January 2016. The final published version is available at: [URL] ?? Springer Science+Business Media New York 2016 Current compilers cannot generate code that can compete with hand-tuned code in efficiency, even for a simple kernel like matrix???matrix multiplication (MMM). A key step in program optimization is the estimation of optimal values for parameters such as tile sizes and number of levels of tiling. The scheduling parameter values selection {{is a very difficult}} and time-consuming task, since parameter values depend on each other; this is why they are found by using searching methods and empirical techniques. To overcome this problem, the scheduling sub-problems must be optimized together, as one problem and not separately. In this paper, an MMM methodology is presented where the optimum scheduling parameters are found by decreasing the search space theoretically, while the major scheduling sub-problems are addressed together as one problem and not separately according to the hardware architecture parameters and input size; for different hardware architecture parameters and/or input sizes, a different implementation is produced. This is achieved by fully exploiting the software characteristics (e. g., data reuse) and hardware architecture parameters (e. g., data caches sizes and <b>associativities),</b> giving high-quality solutions and a smaller search space. This methodology refers {{to a wide range of}} CPU and GPU architectures...|$|R
40|$|Modern CPUs {{often use}} large physically-indexed caches that are direct-mapped or have low <b>associativities.</b> Such caches do not {{interact}} well with virtual memory systems. An improperly placed physical page {{will end up}} in a wrong place in the cache, causing excessive conflicts with other cached pages. Page coloring has been proposed to reduce the conflict misses by carefully placing pages in the physical memory. While page coloring works well for some applications, many factors limit its performance. Page coloring limits the freedom of the page placement system and may increase swapping traffic. In this paper, we propose a novel and simple architecture called color-indexed, physically tagged caches that can significantly reduce the conflict misses. With some simple modifications to the TLB (Translation Look-aside Buffer), the new architecture decouples the addresses of the cache from the addresses of the main memory. Since the cache addresses do not depend on the the physical memory addresses anymore, the system can freely place data in any cache page to minimize the conflict misses, without affecting the paging system. Extensive trace-driven simulation results show that our design performs much better than traditional page coloring techniques. The new scheme enables a direct-mapped cache to achieve hit ratios 1 very close to or better than those of a two-way set associative cache. Moreover, the architecture does not increase cache access latency, which is a drawback of set associative caches. The hardware overhead is minimal. We show that our scheme can reduce the cache size by 50 % without sacrificing performance. A two-way set-associative cache that uses this strategy can perform very close to a fully-associative cache. ...|$|R
