1498|3613|Public
5|$|A comet-impact {{mission was}} first {{proposed}} to NASA in 1996, {{but at the}} time, NASA engineers were skeptical that the target could be hit. In 1999, a revised and technologically upgraded mission proposal, dubbed Deep Impact, was accepted and funded as part of NASA's Discovery Program of low-cost spacecraft. The two spacecraft (Impactor and Flyby) and the three main instruments were built and integrated by Ball Aerospace & Technologies Corp. in Boulder, Colorado. Developing the software for the spacecraft took 18 months and the <b>application</b> <b>code</b> consisted of 20,000 lines and 19 different application threads. The total cost of developing the spacecraft and completing its mission reached US$330million.|$|E
25|$|CPUs {{themselves}} have gained increasingly wide SIMD units (driven by video and gaming workloads) {{and increased the}} number of cores in a bid to eliminate the need for another accelerator, as well as for accelerating <b>application</b> <b>code.</b> These tend to support packed low precision data types.|$|E
25|$|For {{hardware}} {{functions such as}} {{input and}} output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the <b>application</b> <b>code</b> is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers.|$|E
50|$|Below is a table {{indicating}} {{the most common}} <b>application</b> <b>codes.</b>|$|R
50|$|Version 4.0 {{files are}} dated November 11, 2003. 4.0 runtime fully {{supports}} <b>applications</b> <b>coded</b> for 3.0 and 2.0.|$|R
50|$|Version 5.1 {{files are}} dated March 9, 2006. 5.1 runtime {{does not support}} <b>applications</b> <b>coded</b> for 4.0 or before.|$|R
25|$|A Business Process Management {{system is}} quite {{different}} from BPA. However, it is possible to build automation {{on the back of a}} BPM implementation. The actual tools to achieve this vary, from writing custom <b>application</b> <b>code</b> to using specialist BPA tools, as described above. The advantages and disadvantages of this approach are inextricably linked – the BPM implementation provides an architecture for all processes in the business to be mapped, but this in itself delays the automation of individual processes and so benefits may be lost in the meantime.|$|E
25|$|These {{limitations}} {{mean that}} supporting the multitasking {{of more than}} one program at a time would be difficult, without rewriting all of this operating system and <b>application</b> <b>code.</b> Yet doing so, would mean the system would run unacceptably slow on existing hardware. Instead, Apple adopted a system known as MultiFinder in 1987 which keeps the running application in control of the computer as before but allows an application to be rapidly switched to another, normally simply by clicking on its window. Programs which are not in the foreground are periodically given short bits of time to run, but as before, the entire process is controlled by the applications, not the operating system.|$|E
500|$|In December 2011, Rhythm Zone {{confirmed}} that [...] "No Man's Land" [...] {{would be included}} on Japonesque, and it appeared as the thirteenth track on all formats from Japonesque. [...] "No Man's Land", alongside Japonesque tracks: [...] "So Nice", [...] "Slow" [...] featuring American recording artist Omarion, [...] "Brave", and [...] "Escalate", served as the album's lead promotional digital singles on January 18, 2012, which was served through online retailers including Mora, Mu-Mo, music.jp and Recochoku. That same year, [...] "No Man's Land", [...] "So Nice", [...] "Slow", [...] "Brave", and [...] "Escalate" [...] were then re-released on January 25 through Japanese airplay stations. This was the same release date as the album Japonesque A special <b>application</b> <b>code</b> was uploaded onto Koda's official website, which allowed users to access the song from Recochoku and download a full ringtone for free. At the end of January 2012, British production team StarRock promoted the single on Star Rock television; the song was circulated throughout UK radio airplay shows. The single artwork uses the CD and double DVD cover of Japonesque, which was used exclusively through Recochoku stores.|$|E
50|$|In 2009, CSCS and the University of Lugano jointly {{launched}} the platform HP2C {{with the goal}} to prepare the <b>application</b> <b>codes</b> of Swiss researchers for upcoming supercomputer architectures.|$|R
40|$|Abstract—Current {{workflows}} {{have limited}} reusability and shelf life because they specify particular application software {{to be run}} at each step, which can become obsolete or no longer run. We describe semantic workflow representations composed of domain tasks, which the system then automatically maps to the <b>application</b> <b>codes</b> available in the particular execution environment. We have implemented this approach in the WINGS semantic workflow system, and demonstrate the mapping of workflows to different sets of <b>application</b> <b>codes</b> depending on the workflow execution engine and execution environment selected. workflows; workflow reuse I...|$|R
5000|$|An {{existence}} check {{can sometimes}} involve a [...] "brute force" [...] approach of checking all records {{for a given}} identifier, as in this Microsoft Excel Visual Basic for <b>Applications</b> <b>code</b> for detecting whether a worksheet exists: ...|$|R
500|$|Wipeout 2048 was {{developed}} by Sony Studio Liverpool, known as Psygnosis during {{the development of the}} early Wipeout games. Studio Liverpool's technical director, Stuart Lovegrove, affirmed that the game {{was developed}} in parallel with the PlayStation Vita and was a testbed for the console. Lovegrove was aware that the next Wipeout game would be a launch title, and said that it was something Studio Liverpool had done before. Chris Roberts, the game's director of graphics, tools and technologies, said that Sony Computer Entertainment involved the Liverpudlian studio early {{in the development of the}} PlayStation Vita and had a [...] "fairly good idea" [...] of the console's capability. Jon Eggleton, former senior artist of the Wipeout series, said in an interview that Studio Liverpool influenced the Vita's design. When staff were given development kits for what was called a [...] "next-generation portable", a group was formed to brainstorm hardware details; proposals included a touchscreen device, not yet conceived by Sony. Eggleton speculated that the console was released with two analogue sticks solely because [...] "Studio Liverpool said it needed two sticks". During early development of Wipeout 2048 and the PlayStation Vita, the studio provided Sony with feedback on the hardware and libraries and sent updated <b>application</b> <b>code</b> to Sony's firmware staff for testing their compilers. Both Lovegrove and Roberts were impressed with the simplicity of the Vita's firmware, in contrast to the architecture [...] of the PlayStation 3.|$|E
2500|$|Until version 5.0, Android used Dalvik as {{a process}} virtual machine with trace-based {{just-in-time}} (JIT) compilation to run Dalvik [...] "dex-code" [...] (Dalvik Executable), which is usually translated from the Java bytecode. [...] Following the trace-based JIT principle, in addition to interpreting the majority of <b>application</b> <b>code,</b> Dalvik performs the compilation and native execution of select frequently executed code segments ("traces") each time an application is launched. Android4.4 introduced Android Runtime (ART) as a new runtime environment, which uses ahead-of-time (AOT) compilation to entirely compile the application bytecode into machine code upon the installation of an application. [...] In Android4.4, ART was an experimental feature and not enabled by default; it became the only runtime option in the next major version of Android, 5.0.|$|E
2500|$|Specialist {{companies}} are bringing toolsets to market that are purpose-built for {{the function of}} BPA. Toolsets vary in sophistication, {{but there is an}} increasing trend towards the use of artificial intelligence technologies that can understand natural language and unstructured datasets, interact with human beings, and adapt to new types of problems without human-guided training. BPA providers tend to focus on different industry sectors but their underlying approach tends to be similar in that they will attempt to provide the shortest route to automation by exploiting the user interface layer rather than going deeply into the <b>application</b> <b>code</b> or databases sitting behind them. They also simplify their own interface to the extent that these tools can be used directly by non-technically qualified staff. The main advantage of these toolsets is therefore their speed of deployment, the drawback is that it brings yet another IT supplier to the organization.|$|E
40|$|Parallel eigensolver {{operations}} {{are at the}} computational core of many large-scale scientific and engineering <b>application</b> <b>codes.</b> This project analyses parallel performance of established and newly developed parallel dense symmetric eigensolver numerical library routines on PRACE Tier- 0 systems using real datasets from large-scale <b>application</b> <b>codes.</b> This whitepaper builds upon the research report ‘Dense Linear Algebra Performance Analysis on the Fujitsu BX 900 OPL Machine†’ (from the same author) {{which can be found}} at: [URL] / 2011 _Dense_Linear_Algebra_Performance_Analysis_on_the_Fujitsu_BX 900 _OPL_Machine. pdf. This version of the report is updated with results from ScaLAPACK and with the recently released ELPA library routines on the PRACE Tier- 0 machines CURIE and JUGENE...|$|R
40|$|Application {{performance}} on a high performance, parallel platform depends {{on a variety of}} factors, the most important being the performance of the high speed interconnect and the compute node processor. The performance of the compute processor depends on how well the compiler optimizes for a given processor architecture, and how well it optimizes the <b>applications</b> source <b>code.</b> An analysis of uni-processor and parallel performance using different AMD Opteron compilers on key SNL <b>application</b> <b>codes</b> is presented. ...|$|R
40|$|Constitutive {{modeling}} is {{an important}} aspect of computational solid mechanics. Sandia National Laboratories has always had a considerable effort in the development of constitutive models for complex material behavior. However, for this development to be of use the models need to be implemented in our solid mechanics <b>application</b> <b>codes.</b> In support of this important role, the Library of Advanced Materials for Engineering (LAME) has been developed in Engineering Sciences. The library allows for simple implementation of constitutive models by model developers and access to these models by <b>application</b> <b>codes.</b> The library is written in C++ and has a very simple object oriented programming structure. This report summarizes the current status of LAME...|$|R
2500|$|An {{operating}} system (OS) is software (programs and data) {{that runs on}} computers and manages the computer hardware and provides common services for efficient execution of various application software. For hardware functions such as input and output and memory allocation, the {{operating system}} acts as an intermediary between application programs and the computer hardware, although the <b>application</b> <b>code</b> is usually executed directly by the hardware, but will frequently call the OS or be interrupted by it. Operating systems are found on almost any device that contains a computer—from cellular phones and video game consoles to supercomputers and web servers. The GM-NAA I/O, created by Owen Mock and Bob Patrick of General Motors Research Laboratories in early 1956 (or late 1955) for their IBM 701 mainframe computer is {{generally considered to be}} the first [...] "batch processing" [...] operating system and possibly the first [...] "real" [...] operating system. Rudimentary forms of operating systems existed before batch processing, the Input/Output Control System (IOCS) being one example. However, what specifically differentiated and made the GM-NAA I/O as {{the first of its kind}} was that instead of having a human operator manually load each program as what previous systems were only capable of doing, computerized software as used on GM-NAA I/O, thereafter handled the scheduling, management, and multi-tasking of all computer applications.|$|E
2500|$|David Watt's 2004 {{textbook}} also analyzes {{exception handling}} {{in the framework}} of sequencers (introduced in this article in the section on early exits from loops). Watt notes that an abnormal situation, generally exemplified with arithmetic overflows or input/output failures like file not found, is a kind of error that [...] "is detected in some low-level program unit, but [...] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where [...] "the <b>application</b> <b>code</b> tends to get cluttered by tests of status flags" [...] and that [...] "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" [...] Watt notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding explicit code to ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers aren't as suitable as a dedicated exception sequencer with the semantics discussed above.|$|E
50|$|Sample <b>application</b> <b>code.</b>|$|E
40|$|The parallelization of {{real-world}} compute intensive Fortran <b>application</b> <b>codes</b> {{is generally}} not a trivial task. If the time to complete the parallelization is to be significantly reduced then an environment is needed that will assist the programmer in the various tasks of code parallelization. In this paper the authors present a code parallelization environment where a number of tools that address the main tasks such as code parallelization, debugging and optimization are available. The ParaWise and CAPO parallelization tools are discussed which enable the near automatic parallelization of real-world scientific <b>application</b> <b>codes</b> for shared and distributed memory-based parallel systems. As user involvement in the parallelization process can introduce errors, a relative debugging tool (P 2 d 2) is also available {{and can be used}} to perform nearly automatic relative debugging of a program that has been parallelized using the tools. A high quality interprocedural dependence analysis as well as user-tool interaction are also highlighted and are vital to the generation of efficient parallel code and in the optimization of the backtracking and speculation process used in relative debugging. Results of benchmark and real-world <b>application</b> <b>codes</b> parallelized are presented and show the benefits of using the environmen...|$|R
40|$|This White Paper {{reports on}} the {{identification}} of major socio-economic challenges and associated key community <b>application</b> <b>codes</b> {{as a part of}} application support within PRACE Third Implementation Phase project. Description of the sources of input, identification process, guidelines and the final selection outcome is included...|$|R
40|$|EPG-sim is a newly-developed set {{of tools}} that {{performs}} execution-driven critical path simulation, trace generation, and simulation for serial, optimistically parallelized, and parallel <b>application</b> <b>codes.</b> These capabilities are integrated within a single framework {{through the use of}} intelligent source-level instrumentation. The ability to perform execution-driven simulations driven by optimistically parallelized codes, the ability to execute these simulations on parallel hosts, the use of source-level instrumentation, and the integration of the capabilities provided by EPG-sim are among the novel contributions of this work. EPG-sim has important uses in studying parallel architectures, parallelizing compilers, and parallel applications. 1. Introduction Execution-driven simulation and tracing techniques have received considerable attention in recent years. Execution-driven techniques modify <b>application</b> <b>codes</b> by inserting additional instrumentation code. The resulting instrumented code [...] ...|$|R
5000|$|Diverts to reroute traffic without modifying <b>application</b> <b>code</b> ...|$|E
5000|$|Automated <b>application</b> <b>code</b> {{production}} (Java, C#, C++, SQL, …) ...|$|E
5000|$|... #Subtitle level 3: Improving the {{modularity}} of <b>application</b> <b>code</b> ...|$|E
40|$|This manual {{describes}} {{the use of}} PETSc 2. 0 for the numerical solution of partial differential equations and related problems on high-performance computers. The Portable, Extensible Toolkit for Scientific Computation (PETSc) is a suite of data structures and routines that provide the building blocks {{for the implementation of}} large-scale <b>application</b> <b>codes</b> on parallel (and serial) computers. PETSc 2. 0 uses the MPI standard for all message-passing communication. PETSc includes an expanding suite of parallel linear and nonlinear equation solvers that may be used in <b>application</b> <b>codes</b> written in Fortran, C, and C++. PETSc provides many of the mechanisms needed thin parallel <b>application</b> <b>codes,</b> such as simple parallel matrix and vector assembly routines that allow the overlap of communication and computation. In addition, PETSc includes growing support for distributed arrays. The library is organized hierarchically, enabling users to employ the level of abstraction that is most appropriate for a particular problem. By using techniques of object-oriented programming, PETSc provides enormous flexibility for users. PETSc is a sophisticated set of software tools; as such, for some users it initially has a much steeper learning curve than a simple subroutine library. In particular, for individuals without some computer science background or experience programming in C, Pascal, or C++, it may require a large amount of time {{to take full advantage of}} the features that enable efficient software use. However, the power of the PETSc design and the algorithms it incorporates make the efficient implementation of many <b>application</b> <b>codes</b> much simpler than rolling them yourself. For many simple tasks a package such as Matlab is often the best tool; PETSc is not intended for the classes of problems for which effective Matlab code can be written. Since PETSc is still under development, small changes in usage and calling sequences of PETSc routines will continue to occur...|$|R
50|$|On May 22, 2008 SDLBasic was ported to the Sony PlayStation Portable. It {{allows you}} to play games and use <b>applications</b> <b>coded</b> in SDLBasic. Download from the link in references. Recently someone on the qj forums {{released}} an update version 0.7 which included video output on the slim.|$|R
40|$|This paper {{presents}} {{a novel approach}} for extracting knowledge from web-based <b>application</b> source <b>code</b> in supplementing and assisting ontology development from database schemas. The structure of web-based <b>application</b> source <b>code</b> is defined in order to distinguish different kinds of knowledge within the source code for ontology development. The connections between the relevant parts of web <b>application</b> source <b>code</b> and the backend database schema with their various forms are explicitly specified in detail. A knowledge processing and integration model for extracting and integrating the knowledge embedded in the source code for ontology development is then proposed...|$|R
5000|$|<b>Application</b> <b>code</b> {{generators}} for persistent layer, programming interface, web services: ...|$|E
50|$|The {{conciseness}} of DASL {{can be seen}} also {{in terms}} of the content of the two representations (DASL vs. the generated <b>application</b> <b>code</b> in Java/XML/SQL etc.). Most of the DASL code describes business logic and business processes specific to the application, independent of the deployment middleware, frameworks, and presentation mechanisms. This core business logic typically represents only 2-5% of the generated <b>application</b> <b>code.</b> Thus, writing, understanding, and maintaining the <b>application</b> <b>code</b> is much easier at the DASL level than it is {{at the level of the}} generated code, in which the business logic is scattered within various implementation artifacts.|$|E
5000|$|A repository, {{allowing}} decision {{logic to}} be externalized from core <b>application</b> <b>code</b> ...|$|E
40|$|In {{most cases}} of {{distributed}} memory computations, node programs are executed on processors {{according to the}} owner computes rule. However, owner computes rule is not best suited for irregular <b>application</b> <b>codes.</b> In irregular <b>application</b> <b>codes,</b> use of indirection in accessing left hand side array {{makes it difficult to}} partition the loop iterations, and because of use of indirection in accessing right hand side elements, we may reduce total communication by using heuristics other than owner computes rule. In this paper, we propose a communication cost reduction computes rule for irregular loop partitioning, called least communication computes rule. We partition a loop iteration to a processor on which the minimal communication cost is ensured when executing that iteration. The experimental results show that, in most cases, our approaches achieved better performance than other loop partitioning rules...|$|R
5000|$|EWG-rated {{motive power}} & torque outputs, <b>application,</b> ID <b>codes</b> ...|$|R
50|$|DEISA {{produced}} a benchmark suite to help computer scientists assess {{the performance of}} parallel supercomputer systems. The benchmark comprises a number of real <b>applications</b> <b>codes</b> taken {{from a wide range}} of scientific disciplines. A structured framework allows compilation, execution and analysis to be configured and carried out via standard input files.|$|R
