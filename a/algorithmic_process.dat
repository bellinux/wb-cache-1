113|162|Public
25|$|BLAT {{is used to}} find {{regions in}} a target genomic {{database}} which are similar to a query sequence under examination. The general <b>algorithmic</b> <b>process</b> followed by BLAT is similar to BLAST's in that it first searches for short segments in the database and query sequences which have {{a certain number of}} matching elements. These alignment seeds are then extended in both directions of the sequences in order to form high-scoring pairs. However, BLAT uses a different indexing approach from BLAST, which allows it to rapidly scan very large genomic and protein databases for similarities to a query sequence. It does this by keeping an indexed list (hash table) of the target database in memory, which significantly reduces the time required for the comparison of the query sequences with the target database. This index is built by taking the coordinates of all the non-overlapping k-mers (words with k letters) in the target database, except for highly repeated k-mers. BLAT then builds a list of all overlapping k-mers from the query sequence and searches for these in the target database, building up a list of hits where there are matches between the sequences (Figure 1 illustrates this process).|$|E
2500|$|Dennett sees {{evolution}} {{by natural}} selection as an <b>algorithmic</b> <b>process</b> (though he spells out that algorithms as simple as long division often incorporate a significant degree of randomness). This idea is {{in conflict with the}} evolutionary philosophy of paleontologist Stephen Jay Gould, who preferred to stress the [...] "pluralism" [...] of evolution (i.e., its dependence on many crucial factors, of which natural selection is only one).|$|E
5000|$|... #Subtitle level 2: 1995 - Daniel Dennett: {{evolution}} as an <b>algorithmic</b> <b>process</b> ...|$|E
50|$|Another {{part of his}} {{research}} on representation examines the cultural work of algorithms. T{{his research}} focuses on regime change in representation, as norms shift from the modern era’s fixation with the stability of algorismic certainties to our current engagement with dynamic and multi-perspectival <b>algorithmic</b> <b>processes.</b> The <b>algorithmic</b> <b>processes</b> behind such collaborative cultural forms as Photosynth, Google, Wikipedia - and increasingly financial and cartographic practices - serve {{as the focus of}} this research.|$|R
50|$|In the {{cognitive}} steering model, a conscious state emerges from effortful associative simulation, required to align novel data accurately with remote memory, via later <b>algorithmic</b> <b>processes.</b> By contrast, fast unconscious automaticity is constituted by unregulated simulatory biases, which induce errors in subsequent <b>algorithmic</b> <b>processes.</b> The phrase 'rubbish in, rubbish out' {{is used to}} explain errorful steering cognition processing: errors will always occur if the accuracy of initial retrieval and location of data is poorly self-regulated.|$|R
50|$|The {{evolution}} of meaning is then discussed, and Dennett uses {{a series of}} thought experiments to persuade the reader that meaning {{is the product of}} meaningless, <b>algorithmic</b> <b>processes.</b>|$|R
50|$|Dis-unification, in {{computer}} science and logic, is an <b>algorithmic</b> <b>process</b> of solving inequations between symbolic expressions.|$|E
50|$|In {{logic and}} {{computer}} science, unification is an <b>algorithmic</b> <b>process</b> of solving equations between symbolic expressions.|$|E
5000|$|Philosopher Daniel Dennett {{analyses}} {{the importance}} of evolution as an <b>algorithmic</b> <b>process</b> in his 1995 book Darwin's Dangerous Idea. Dennett identifies three key features of an algorithm: ...|$|E
50|$|His musical {{writing is}} {{influenced}} by processes of graphical representation, by a multi-parametric conception of sound and musical structure, by intuitive-based <b>algorithmic</b> <b>processes</b> and by experimentation as a fundamental method of construction.|$|R
5000|$|... "This {{heuristic}} fact, as well {{as certain}} reflections {{on the nature of}} symbolic <b>algorithmic</b> <b>processes,</b> led Church to state the following thesis22. The same thesis is implicitly in Turing's description of computing machines23.|$|R
5000|$|The {{discipline}} of computing is {{the systematic study}} of <b>algorithmic</b> <b>processes</b> that describe and transform information: their theory, analysis, design, efficiency, implementation, and application. The fundamental question underlying all the computing is 'What can be (efficiently) automated?' ...|$|R
50|$|Analytical {{jurisprudence}} {{is not to}} {{be mistaken}} for legal formalism (the idea that legal reasoning is or can be modelled as a mechanical, <b>algorithmic</b> <b>process).</b> Indeed, it was the analytical jurists who first pointed out that legal formalism is fundamentally mistaken as a theory of law.|$|E
50|$|An {{information}} processor or information processing system, {{as its name}} suggests, is a system (be it electrical, mechanical or biological) which takes information (a sequence of enumerated symbols or states) in one form and processes (transforms) it into another form, e.g. to statistics, by an <b>algorithmic</b> <b>process.</b>|$|E
5000|$|Dennett sees {{evolution}} {{by natural}} selection as an <b>algorithmic</b> <b>process</b> (though he spells out that algorithms as simple as long division often incorporate a significant degree of randomness). This idea is {{in conflict with the}} evolutionary philosophy of paleontologist Stephen Jay Gould, who preferred to stress the [...] "pluralism" [...] of evolution (i.e., its dependence on many crucial factors, of which natural selection is only one).|$|E
50|$|Harris Wulfson (18 July 1974 - 23 July 2008) was an American Jewish composer, instrumentalist and {{software}} engineer in Brooklyn, New York. His work employed <b>algorithmic</b> <b>processes</b> and gestural controllers {{to explore the}} boundary where humans encounter their machines.|$|R
3000|$|... the opacity of {{automated}} {{decision making}} are multiple: first, algorithms might use enormous and very complex data sets that are uninterpretable to regulators [25], who frequently lack the required computer science knowledge to understand <b>algorithmic</b> <b>processes</b> [73]; second, automatic decision making might intrinsically transcend human comprehension since algorithms {{do not make}} use of theories or contexts as in regular human based decision-making [58]; and finally, <b>algorithmic</b> <b>processes</b> of firms or companies might be subject to intellectual property rights or covered by trade secret provisions [35]. If there is no transparent information on how algorithms and processes work {{it is almost impossible}} to [44] evaluate the fairness of the algorithms or discover discriminatory patterns in the system [45].|$|R
5000|$|According to Walker, system 1 {{functions}} as a serial cognitive steering processor for system 2, rather than a parallel system. In large-scale repeated studies with school students, Walker tested how students adjusted their imagined self-operation in different curriculum subjects of maths, science and English. He showed that students consistently adjust the biases of their heuristic self-representation to specific states for the different curriculum subjects. [...] The model of cognitive steering proposes that, in order to process epistemically varied environmental data, a heuristic orientation system is required to align varied, incoming environmental data with existing neural <b>algorithmic</b> <b>processes.</b> The brain's associative simulation capacity, centered around the imagination, plays an integrator role to perform this function. Evidence for early-stage concept formation and future self-operation within the hippocampus supports the model,. In the cognitive steering model, a conscious state emerges from effortful associative simulation, required to align novel data accurately with remote memory, via later <b>algorithmic</b> <b>processes.</b> By contrast, fast unconscious automaticity is constituted by unregulated simulatory biases, which induce errors in subsequent <b>algorithmic</b> <b>processes.</b> The phrase ‘rubbish in, rubbish out' is used to explain errorful heuristic processing: errors will always occur if the accuracy of initial retrieval and location of data is poorly self-regulated.|$|R
5000|$|Underlying mindlessness: {{no matter}} how {{complicated}} the end-product of the <b>algorithmic</b> <b>process</b> may be, each step in the algorithm is sufficiently simple to be performed by a non-sentient, mechanical device. The algorithm {{does not require a}} [...] "brain" [...] to maintain or operate it. [...] "The standard textbook analogy notes that algorithms are recipes of sorts, designed to be followed by novice cooks."(p. 51) ...|$|E
50|$|In {{computational}} linguistics, lemmatisation is the <b>algorithmic</b> <b>process</b> {{of determining}} the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research.|$|E
50|$|In {{computational}} biology, de novo {{protein structure}} prediction {{refers to an}} <b>algorithmic</b> <b>process</b> by which protein tertiary structure is predicted from its amino acid primary sequence. The problem itself has occupied leading scientists for decades while still remaining unsolved. According to Science, the problem {{remains one of the}} top 125 outstanding issues in modern science. At present, some of the most successful methods have a reasonable probability of predicting the folds of small, single-domain proteins within 1.5 angstroms over the entire structure.|$|E
5000|$|The {{field of}} study called {{information}} systems encompasses {{a variety of topics}} including systems analysis and design, computer networking, information security, database management and decision support systems. Information management deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support. Communications and networking deals with the telecommunication technologies.Information systems bridges business and computer science using the theoretical foundations of information and computation to study various business models and related <b>algorithmic</b> <b>processes</b> [...] on building the IT systems [...] within a computer science discipline. Computer information system(s) (CIS) is a field studying computers and <b>algorithmic</b> <b>processes,</b> including their principles, their software and hardware designs, their applications, and their impact on society, whereas IS emphasizes functionality over design.|$|R
40|$|Abstract. In this paper, {{the authors}} {{want to show}} a method that allows {{customizing}} energy efficient buildings to the very task and to the very site by linking environmental data and design strategies through <b>algorithmic</b> <b>processes.</b> An optimum solution for the energy efficiency of a building can then be found by running an evolutionary solver...|$|R
40|$|We {{establish}} bounds for {{the coefficient}} e̅_ 1 (I) of the Hilbert {{function of the}} integral closure filtration of equimultiple ideals. These values are shown to help control all <b>algorithmic</b> <b>processes</b> of normalization that make use of extensions satisfying the condition S_ 2 of Serre. Comment: 16 pages. to appear in Math. Res. Letter...|$|R
5000|$|... "But then [...] [...] {{are there}} any limits at all on what may be {{considered}} an <b>algorithmic</b> <b>process?</b> I guess the answer is NO; if you wanted to, you can treat any process at the abstract level as an algorithmic process[...] [...] If what strikes you as puzzling is the uniformity of the ocean's sand grains or {{the strength of the}} tempered-steel blade, an algorithmic explanation is what will satisfy your curiosity -- and it will be the truth[...] [...] [...]|$|E
50|$|Using a {{proprietary}} <b>algorithmic</b> <b>process,</b> MyVetwork provides service members {{and those who}} care about them with a means to connect with and support each other in ways that range from the lighthearted and entertaining to deep and meaningful connections. In the process, it is building an information exchange where veterans of earlier conflicts can provide information, career advice, mentoring and other work-related resources to recently separated vets transitioning from the military experience to civilian work or education.MyVetwork is also able to apply its profiling and matching capability to connect veterans with appropriate civilian jobs.|$|E
5000|$|Darwin {{provided}} {{just such}} an alternative: evolution. Besides providing evidence of common descent, he introduced a mechanism to explain it: natural selection. According to Dennett, natural selection is a mindless, mechanical and <b>algorithmic</b> <b>process</b> - Darwin's dangerous idea. The third chapter introduces the concept of [...] "skyhooks" [...] and [...] "cranes" [...] (see below). He suggests that resistance to Darwinism {{is based on a}} desire for skyhooks, which do not really exist. According to Dennett, good reductionists explain apparent design without skyhooks; greedy reductionists try to explain it without cranes.|$|E
40|$|Abstract. The {{theory of}} optimal <b>algorithmic</b> <b>processes</b> {{is part of}} {{computational}} complexity. This paper deals with analytic computational complexity. The relation between the goodness of an itera-tion algorithm and its new function evaluation and memory requirements are analyzed. A new con-jecture is stated. Key words, computational complexity, optimal algorithm, optimal iteratiola, numerical mathe-matics, iteration theory. 1. Introduction. Computationa...|$|R
5000|$|Noll used {{a digital}} {{computer}} to create artistic patterns and formalized {{the use of}} random and <b>algorithmic</b> <b>processes</b> {{in the creation of}} visual arts. [...] His initial digital computer art was programmed in the summer of 1962 at Bell Telephone Laboratories in Murray Hill, NJ, making him one of the early innovators of digital computer art.|$|R
2500|$|In a September 1997 {{interview}} with Space Age Bachelor magazine, James said he composed ambient techno music at age 13, had [...] "over 100 hours" [...] of unreleased music and had invented music-composition software consisting of <b>algorithmic</b> <b>processes</b> which automatically generated rhythm and melody. In the interview, he also {{claimed to have}} experienced synesthesia and incorporated lucid dreaming into his compositions.|$|R
50|$|Newton came to {{calculus}} {{as part of}} his {{investigations in}} physics and geometry. He viewed calculus as the scientific description of the generation of motion and magnitudes. In comparison, Leibniz focused on the tangent problem and came to believe that calculus was a metaphysical explanation of change. Importantly, the core of their insight was the formalization of the inverse properties between the integral and the differential of a function. This insight had been anticipated by their predecessors, but they were the first to conceive calculus as a system in which new rhetoric and descriptive terms were created. Their unique discoveries lay not only in their imagination, but also in their ability to synthesize the insights around them into a universal <b>algorithmic</b> <b>process,</b> thereby forming a new mathematical system.|$|E
5000|$|A {{more general}} method, [...] "irreversible {{algorithmic}} cooling", {{makes use of}} irreversible transfer of heat outside {{of the system and}} into the environment (and therefore may bypass the Shannon bound). Such an environment can be a heat bath, and the family of algorithms which use it is named [...] "heat-bath algorithmic cooling". In this <b>algorithmic</b> <b>process</b> entropy is transferred reversibly to specific qubits (named reset spins) that are coupled with the environment much more strongly than others. After a sequence of reversible steps that let the entropy of these reset qubits increase they become hotter than the environment. Then the strong coupling results in a heat transfer (irreversibly) from these reset spins to the environment. The entire process may be repeated and may be applied recursively to reach low temperatures for some qubits.|$|E
50|$|In {{application}} of homological algebra techniques to algebraic geometry, {{it has been}} traditional since David Hilbert (though modern terminology is different) to apply free resolutions of R, considered as a graded module over the polynomial ring. This yields information about syzygies, namely relations between generators of the ideal I. In a classical perspective, such generators are simply the equations one writes down to define V. If V is a hypersurface there need only be one equation, and for complete intersections the number of equations can be taken as the codimension; but the general projective variety has no defining set of equations that is so transparent. Detailed studies, for example of canonical curves and the equations defining abelian varieties, show the geometric interest of systematic techniques to handle these cases. The subject also grew out of elimination theory in its classical form, in which reduction modulo I is supposed to become an <b>algorithmic</b> <b>process</b> (now handled by Gröbner bases in practice).|$|E
50|$|Dennett says, for example, that by {{claiming}} that minds cannot be reduced to purely <b>algorithmic</b> <b>processes,</b> many of his eminent contemporaries are claiming that miracles can occur. These assertions have generated {{a great deal of}} debate and discussion in the general public. The book was a finalist for the 1995 National Book Award in non-fiction and the 1996 Pulitzer Prize for General Non-Fiction.|$|R
40|$|Since {{the late}} 1990 's, {{architectural}} form making has investigated advanced computation {{at the earliest}} stages of design through inductive analytic and <b>algorithmic</b> <b>processes.</b> This paper proposes a relational or contextual organization by analyzing existing networked models. It firstly presents a literature review regarding the development of networked models and then outlines the requirements for a conceptual prototype for future design applications...|$|R
5000|$|In a September 1997 {{interview}} with Space Age Bachelor magazine, James said he composed ambient techno music at age 13, had [...] "over 100 hours" [...] of unreleased music and had invented music-composition software consisting of <b>algorithmic</b> <b>processes</b> which automatically generated rhythm and melody. In the interview, he also {{claimed to have}} experienced synesthesia and incorporated lucid dreaming into his compositions.|$|R
