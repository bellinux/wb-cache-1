10|16|Public
40|$|A {{fiber optic}} {{connector}} polishing fixture assembly for sup- porting a terminus of a {{fiber optic cable}} before a polishing surface. The assembly comprises: a fiber optic polishing fixture adapted to support the terminus before the polishing surface; a fixture support connected to the fixture for sup- porting the fixture before the polishing surface; and an adjustable connection between the fixture and the fixture support having user accessible adjustment controls for allowing a user to operate the controls to shift the fixture and fixture support relative to one another for substantially eliminating an <b>apex</b> <b>offset</b> of the terminus {{with respect to the}} polishing surface...|$|E
40|$|Effects of {{different}} nanoscale abrasive suspensions on the geometrical quality including fiber and ferrule surface roughness, fiber height, {{radius of curvature}} and <b>apex</b> <b>offset,</b> and the optical performance in terms of return and insertion losses in polishing of fiber connectors were investigated using a commercial fiber-connector polishing machine. Significant differences in geometrical and optical qualities were found among the suspensions investigated. The results indicate that a colloidal silica suspension of grit size of 50 nm gave a much better geometrical quality than conventionally used alcohol, consequently yielding a more attractive optical performance than the designated return loss of 45 dB and insertion loss of 0. 3 dB...|$|E
40|$|ABSTRACT * The {{recognition}} of facial action units (AUs) in image sequences is a challenging problem. AU detectors achieve good recognition rates, but {{virtually all of}} them deal only with frontal-view face images and cannot handle temporal dynamics of AUs. In this work we report on a system for automatic {{recognition of}} temporal models of AUs from long, profile-view face image sequences. We exploit particle filtering to track 15 facial points in an input faceprofile video sequence and we introduce facial-behavior temporaldynamics recognition from continuous video input using temporal rules. The utilized algorithm performs both automatic segmentation and recognition of temporal segments (i. e., onset, <b>apex,</b> <b>offset)</b> of 23 AUs occurring alone or in a combination in an input face-profile video sequence. A recognition rate of 88 % is achieved. 1...|$|E
40|$|We {{consider}} {{the problem of}} automated recognition of temporal segments (neutral, onset, <b>apex</b> and <b>offset)</b> of Facial Action Units. To this end, we propose the Laplacian-regularized Kernel Conditional Ordinal Random Field model. In contrast to standard modeling approaches to recognition of AUs’ temporal segments, which treat each segment as an independent class, the proposed model takes into account ordinal relations between the segments. The experimental results evidence the effectiveness of such an approach...|$|R
30|$|Ventricular repolarization {{duration}} (VRD) {{is affected}} by heart rate and autonomic control, and thus VRD varies in time {{in a similar way}} as heart rate. VRD variability is commonly assessed by determining the time differences between successive R- and T-waves, that is, RT intervals. Traditional methods for RT interval detection necessitate the detection of either T-wave <b>apexes</b> or <b>offsets.</b> In this paper, we propose a principal-component-regression- (PCR-) based method for estimating RT variability. The main benefit of the method {{is that it does not}} necessitate T-wave detection. The proposed method is compared with traditional RT interval measures, and as a result, it is observed to estimate RT variability accurately and to be less sensitive to noise than the traditional methods. As a specific application, the method is applied to exercise electrocardiogram (ECG) recordings.|$|R
40|$|The {{analysis}} of facial expression temporal dynamics {{is of great}} importance for many real-world applications. Being able to automatically analyse facial muscle actions (Action Units, AUs) in terms of recognising their neutral, onset, <b>apex</b> and <b>offset</b> phases would greatly benefit application areas as diverse as medicine, gaming and security. The base system in this paper uses Support Vector Machines (SVMs) {{and a set of}} simple geometrical features derived from automatically detected and tracked facial feature point data to segment a facial action into its temporal phases. We propose here two methods to improve on this base system in terms of classification accuracy. The first technique describes the original time-independent set of features {{over a period of time}} using polynomial parametrisation. The second technique replaces the SVM with a hybrid SVM/Hidden Markov Model (HMM) classifier to model time in the classifier. Our results show that both techniques contribute to an improved classification accuracy. Modeling the temporal dynamics by the hybrid SVM-HMM classifier attained a statistically significant increase of recall and precision by 4. 5 % and 7. 0 %, respectively...|$|R
40|$|Abstract – The {{recognition}} of facial expressions in image sequences {{is a difficult}} problem with many applications in human-machine interaction. Facial expression analyzers achieve good recognition rates, but virtually all of them deal only with prototypic facial expressions of emotions and cannot handle temporal dynamics of facial displays. The method presented here attempts to handle a large range of human facial behavior by recognizing facial action units (AUs) and their temporal segments (i. e., onset, <b>apex,</b> <b>offset)</b> that produce expressions. We exploit particle filtering to track 20 facial points in an input face video and we introduce AU-dynamics recognition using temporal rules. When tested on Cohn-Kanade and MMI facial expression databases, the proposed method achieved a recognition rate of 90 % when detecting 27 AUs occurring alone or in a combination in an input face image sequence...|$|E
40|$|Abstract- This study {{reports the}} {{development}} of high efficiency polishing protocols of fibre connectors, by replacing the presents three step polishing with two step polishing protocols, and by using various abrasives as polishing mediums, to achieve necessary geometrical and optical qualities for physical contact transmission using a commercially available connector polisher. The study focuses on surface integrity of the fibre and ferrule {{as well as other}} geometry of the polished connector end faces covering the fibre height or undercut, the <b>apex</b> <b>offset,</b> and the radius of curvature. Optical performances of the polished connectors were evaluated by return and insertion losses. Finally, relationship between geometrical qualities and optical performances was established. This study shows that the efficiency for polishing fibre connectors can be raised by at least 30 % by selection of suitable abrasives and grit sizes. Keywords: Fibre optic connectors, Polishing...|$|E
40|$|Automatic {{analysis}} of facial expressions {{is a complex}} area of pattern recognition and computer vision with many un- resolved problems, {{one of which is}} the distinction between posed and spontaneous expressions of emotions. Previous psychology research indicates that the temporal dynamics in the face are essential for distinguishing between posed and spontaneous smiles. There are six temporal characteristics which are important: morphology, apex overlap, symmetry, total duration, speed of onset and speed of offset. In this work, we propose to distinguish between posed and spon- taneous expressions by using Dynamic Bayesian networks (DBN) to model the temporal dynamics. The DBN provides a suitable framework to represent probabilistic relationships between and within the various types of temporal dynamics. Based on the temporal phases of four different Action Units (onset, <b>apex</b> <b>offset</b> and neutral of facial actions) and the six temporal characteristics from the psychology research, we build several DBN models to distinguish between posed and spontaneous expressions. We present experimental results from 50 videos displaying posed and spontaneous smiles. When the DBNs trained on the temporal characteristics are combined to provide a joint classification, we attain an AUC of 0. 97. Computer ScienceElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|Past work on {{automatic}} analysis of facial expressions has focused mostly on detecting prototypic expressions of basic emotions like happiness and anger. The method proposed here enables {{the detection of}} a much larger range of facial behavior by recognizing facial muscle actions [action units (AUs) ] that compound expressions. AUs are agnostic, leaving the inference about conveyed intent to higher order decision making (e. g., emotion recognition). The proposed fully automatic method not only allows the recognition of 22 AUs but also explicitly models their temporal characteristics (i. e., sequences of temporal segments: neutral, onset, <b>apex,</b> and <b>offset).</b> To do so, it uses a facial point detector based on Gabor-feature-based boosted classifiers to automatically localize 20 facial fiducial points. These points are tracked through a sequence of images using a method called particle filtering with factorized likelihoods. To encode AUs and their temporal activation models based on the tracking data, it applies a combination of GentleBoost, support vector machines, and hidden Markov models. We attain an average AU recognition rate of 95. 3 % when tested on a benchmark set of deliberately displayed facial expressions and 72 % when tested on spontaneous expressions...|$|R
40|$|Both the {{configuration}} and {{the dynamics of}} facial expressions are crucial for the interpretation of human facial behavior. Yet to date, {{the vast majority of}} reported efforts in the field either do not take the dynamics of facial expressions into account, or focus only on prototypic facial expressions of six basic emotions. Facial dynamics can be explicitly analyzed by detecting the constituent temporal segments in Facial Action Coding System (FACS) Action Units (AUs) -onset, <b>apex,</b> and <b>offset.</b> In this paper, we present a novel approach to explicit analysis of temporal dynamics of facial actions using the dynamic appearance descriptor Local Phase Quantization from Three Orthogonal Planes (LPQ-TOP). Temporal segments are detected by combining a discriminative classifier for detecting the temporal segments on a frame-by-frame basis with Markov Models that enforce temporal consistency over the whole episode. The system is evaluated in detail over the MMI facial expression database, the UNBC-McMaster pain database, the SAL database, the GEMEP-FERA dataset in database-dependent experiments, in cross-database experiments using the Cohn-Kanade, and the SEMAINE databases. The comparison with other state-of-the-art methods shows that the proposed LPQ-TOP method outperforms the other approaches for the problem of AU temporal segment detection, and that overall AU activation detection benefits from dynamic appearance information...|$|R
40|$|Abstract—Past work on {{automatic}} analysis of facial expressions has focused mostly on detecting prototypic expressions of basic emotions like happiness and anger. The method proposed here enables {{the detection of}} a much larger range of facial behavior by recognizing facial muscle actions [action units (AUs) ] that compound expressions. AUs are agnostic, leaving the inference about conveyed intent to higher order decision making (e. g., emotion recognition). The proposed fully automatic method not only allows the recognition of 22 AUs but also explicitly models their temporal characteristics (i. e., sequences of temporal segments: neutral, onset, <b>apex,</b> and <b>offset).</b> To do so, it uses a facial point detector based on Gabor-feature-based boosted classifiers to automatically localize 20 facial fiducial points. These points are tracked through a sequence of images using a method called particle filtering with factorized likelihoods. To encode AUs and their temporal activation models based on the tracking data, it applies a combination of GentleBoost, support vector machines, and hidden Markov models. We attain an average AU recognition rate of 95. 3 % when tested on a benchmark set of deliberately displayed facial expressions and 72 % when tested on spontaneous expressions. Index Terms—Facial expression analysis, GentleBoost, particle filtering, spatiotemporal facial behavior analysis, support vector machine (SVM). I...|$|R
40|$|The {{polishing}} {{characteristics of}} end faces of fiber optic connectors {{consisting of the}} combined structure of silica inlaid zirconia were investigated using a connector polisher and polishing papers with different abrasive types and grit sizes. The geometrical quality was measured using optical interferometry and fiber optical microscopy. The quality parameters included fiber and ferrule surface roughness, end face morphology, fiber height, radius of curvature, and <b>apex</b> <b>offset.</b> The optical performance was evaluated in terms of return and insertion losses using a loss tester. The surface roughness smaller than 20 nm R-a in the fiber area, the fiber heights smaller than 75 nm, the end face curvature radii of about 20 - 50 mm, and the apex offsets smaller than 100 mum were obtained in a polishing cycle of 1 min. The corresponding average return and insertion losses of - 48 and 0. 15 dB, respectively, were also achieved. The polishing efficiency was raised by at least 30 % compared to the commercially applied processes. Finally, the polishing mechanisms of the fiber optic connectors were investigated regarding material response to abrasive machining. (C) 2003 Elsevier Ltd. All rights reserved...|$|E
40|$|Abstract—Automatic {{analysis}} of human facial expression is a challenging problem with many applications. Most {{of the existing}} automated systems for facial expression analysis attempt to recognize a few prototypic emotional expressions, such as anger and happiness. Instead of representing another approach to machine {{analysis of}} prototypic facial expressions of emotion, the method {{presented in this paper}} attempts to handle a large range of human facial behavior by recognizing facial muscle actions that produce expressions. Virtually all of the existing vision systems for facial muscle action detection deal only with frontal-view face images and cannot handle temporal dynamics of facial actions. In this paper, we present a system for automatic recognition of facial action units (AUs) and their temporal models from long, profile-view face image sequences. We exploit particle filtering to track 15 facial points in an input face-profile sequence, and we introduce facial-action-dynamics recognition from continuous video input using temporal rules. The algorithm performs both automatic segmentation of an input video into facial expressions pictured and recognition of temporal segments (i. e., onset, <b>apex,</b> <b>offset)</b> of 27 AUs occurring alone or in a combination in the input face-profile video. A recognition rate of 87 % is achieved. Index Terms—Computer vision, facial action units, facial expression analysis, facial expression dynamics analysis, particle filtering, rule-based reasoning, spatial reasoning, temporal reasoning. I...|$|E
40|$|A {{study was}} {{undertaken}} to characterize surface temperatures of mounds of imported fire ant, Solenopsis invicta Buren (Hymenoptera: Formicidae) and S. richteri Forel, and their hybrid, {{as it relates}} to sun position and shape of the mounds, to better understand factors that affect absorption of solar radiation by the nest mound and to test feasibility of using thermal infrared imagery to remotely sense mounds. Mean mound surface temperature peaked shortly after solar noon and exceeded mean surface temperature of the surrounding surface. Temperature range for mounds and their surroundings peaked near solar noon, and the temperature range of the mound surface exceeded that of the surrounding area. The temperature difference between mounds and their surroundings peaked around solar noon and ranged from about 2 to 10 °C. Quadratic trends relating temperature measurements to time of day (expressed as percentage of daylight hours from apparent sunrise to apparent sunset) explained 77 to 88 % of the variation in the data. Mounds were asymmetrical, with the <b>apex</b> <b>offset</b> on average 81. 5 ± 1. 2 mm {{to the north of the}} average center. South facing aspects were about 20 % larger than north facing aspects. Mound surface aspect and slope affected surface temperature; this affect was greatly influenced by time of day. Thermal infrared imagery was used to illustrate the effect of mound shape on surface temperature. These results indicate that the temperature differences between mounds and their surroundings are sufficient for detection using thermal infrared remote sensing, and predictable temporal changes in surface temperature may be useful for classifying mounds in images...|$|E
40|$|In {{this work}} we propose a dynamic-texture-based {{approach}} to the recognition of facial Action Units (AUs, atomic facial gestures) and their temporal models (i. e., sequences of temporal segments: neutral, onset, <b>apex,</b> and <b>offset)</b> in near-frontal-view face videos. Two approaches to modelling the dynamics and the appearance in the face region of an input video are compared: an extended version of Motion History Images and a novel method based on Non-rigid Registration using Free-Form Deformations (FFDs). The extracted motion representation is used to derive motion orientation histogram descriptors in both the spatial and temporal domain. Per AU, a combination of discriminative, frame-based GentleBoost ensemble learners and dynamic, generative Hidden Markov Models detects {{the presence of the}} AU in question and its temporal segments in an input image sequence. When tested for recognition of all 27 lower and upper face AUs, occurring alone or in combination in 264 sequences from the MMI facial expression database, the proposed method achieved an average event recognition accuracy of 89. 2 % for the MHI method and of 94. 3 % for the FFD method. The generalization performance of the FFD method has been tested using the Cohn-Kanade database. Finally, we also explored the performance on spontaneous expressions in the Sensitive Artificial Listener dataset...|$|R
40|$|In {{this paper}} we propose a method that {{exploits}} 3 D motion-based features between frames of 3 D facial geometry sequences for dynamic facial expression recognition. An expressive sequence is modeled to contain an onset followed by an <b>apex</b> and an <b>offset.</b> Feature selection methods are applied in order to extract features for each of the onset and offset segments of the expression. These features are then used to train a Hidden Markov Model in order to model the full temporal dynamics of the expression. The proposed fully automatic system was tested in a subset of the BU- 4 DFE database for the recognition of happiness, anger and surprise. Comparisons with a similar system based on the motion extracted from facial intensity images was also performed. The attained results suggest that the use of the 3 D information does indeed improve the recognition accuracy when compared to the 2 D data...|$|R
40|$|Facial micro-expression {{analysis}} has {{attracted much attention}} from the computer vision and psychology communities due to its viability in {{a broad range of}} applications, including medical diagnosis, police interrogation, national security, business negotiation, and social interactions. However, the micro and subtle occurrence that appears on the face poses a major challenge to the development of an efficient automated micro-expression recognition system. Therefore, to date, the annotation of the ground-truths (i. e., emotion label, onset, <b>apex</b> and <b>offset</b> frame indices) are still performed manually by psychologists or trained experts. This thesis briefly reviews the conventional automatic facial microexpression recognition methods and their related works. In general, an automatic facial micro-expression recognition system consists of three basic steps, namely: image preprocessing, feature extraction, and emotion classification. This thesis mainly focuses on the enhancement of the first two steps over conventional methods in the literature. Specifically, a hybrid facial regions selection for pre-processing is proposed. This method is able to eliminate some parts of the face that are irrelevant to any facial emotions. Then, an effective feature descriptor, namely, optical strain, is utilized to capture the variations in characteristics and properties of the micro-expressions in the video. Next, a feature descriptor is developed to encode the essential expressiveness of the apex frame because the information of a single apex frame exhibits the highest variation of motion intensity, which is adequate to represent the emotion of the entire video. Finally, this thesis is concluded by highlighting its contributions and limitations, as well as suggesting possible future directions related to micro-expression recognition system...|$|R
40|$|Past {{research}} on automatic facial expression analysis has focused {{mostly on the}} recognition of prototypic expressions of discrete emotions {{rather than on the}} analysis of dynamic changes over time, although the importance of temporal dynamics of facial expressions for interpretation of the observed facial behavior has been acknowledged for over 20 years. For instance, {{it has been shown that}} the temporal dynamics of spontaneous and volitional smiles are fundamentally different from each other. In this work, we argue that the same holds for the temporal dynamics of brow actions and show that velocity, duration, and order of occurrence of brow actions are highly relevant parameters for distinguishing posed from spontaneous brow actions. The proposed system for discrimination between volitional and spontaneous brow actions is based on automatic detection of Action Units (AUs) and their temporal segments (onset, <b>apex,</b> <b>offset)</b> produced by movements of the eyebrows. For each temporal segment of an activated AU, we compute a number of mid-level feature parameters including the maximal intensity, duration, and order of occurrence. We use Gentle Boost to select the most important of these parameters. The selected parameters are used further to train Relevance Vector Machines to determine per temporal segment of an activated AU whether the action was displayed spontaneously or volitionally. Finally, a probabilistic decision function determines the class (spontaneous or posed) for the entire brow action. When tested on 189 samples taken from three different sets of spontaneous and volitional facial data, we attain a 90. 7 % correct recognition rate. Categories and Subject Descriptors I. 2. 10 [Vision and Scene Understanding]: motion, modeling and recovery of physical attribute...|$|E
40|$|In {{this work}} {{we report on}} the {{progress}} of building a system that enables fully automated fast and robust facial expression recognition from face video. We analyse subtle changes in facial expression by recognizing facial muscle action units (AUs) and analysing their temporal behavior. By detecting AUs from face video we enable the analysis of various facial communicative signals including facial expressions of emotion, attitude and mood. For an input video picturing a facial expression we detect per frame whether any of 15 different AUs is activated, whether that facial action is in the onset, <b>apex,</b> or <b>offset</b> phase, and what the total duration of the activation in question is. We base this process upon a set of spatio-temporal features calculated from tracking data for 20 facial fiducial points. To detect these 20 points of interest in the first frame of an input face video, we utilize a fully automatic, facial point localization method that uses individual feature GentleBoost templates built from Gabor wavelet features. Then, we exploit a particle filtering scheme that uses factorized likelihoods and a novel observation model that combines a rigid and a morphological model to track the facial points. The AUs displayed in the input video and their temporal segments are recognized finally by Support Vector Machines trained on a subset of most informative spatio-temporal features selected by AdaBoost. For Cohn-Kanade and MMI databases, the proposed system classifies 15 AUs occurring alone or in combination with other AUs with a mean agreement rate of 90. 2 % with human FACS coders. 1...|$|R
40|$|We present optical {{images of}} {{the nucleus of the}} nearby radio galaxy M 84 (NGC 4374 = 3 C 272. 1) {{obtained}} with the Wide Field/Planetary Camera 2 (WFPC 2) aboard the Hubble Space Telescope (HST). Our three images cover the Hα + [N II] emission lines as well as the V and I continuum bands. Analysis of these images confirms that the Hα + [N II] emission in the central 5 " (410 pc) is elongated along position angle (P. A.) ≈ 72, which is roughly parallel to two nuclear dust lanes. Our high-resolution images reveal that the Hα + [N II] emission has three components, namely a nuclear gas disk,an `ionization cone', and outer filaments. The nuclear disk of ionized gas has diameter ≈ 1 " = 82 pc and major axis P. A. ≈ 58 ± 6. On an angular scale of 05, the major axis of this nuclear gas disk is consistent with that of the dust. However, the minor axis of the gas disk (P. A. ≈ 148) is tilted with respect to that of the filamentary Hα + [N II] emission at distances > 2 " from the nucleus; the minor axis of this larger scale gas is roughly aligned with the axis of the kpc-scale radio jets (P. A. ≈ 170). The ionization cone (whose <b>apex</b> is <b>offset</b> by ≈ 03 south of the nucleus) extends 2 " from the nucleus along the axis of the southern radio jet. This feature is similar to the ionization cones seen in some Seyfert nuclei, which are also aligned with the radio axes. Comment: 11 pages plus 4 figure...|$|R
40|$|A {{limitation}} of much past research on facial expression of emotion is {{its focus on}} static facial images. The research reported in the present dissertation was designed to examine {{the role played by}} dynamic information in the interpretation of facial expressions, particularly with respect to their perceived authenticity. In a first set of studies, the dynamic properties (i. e., onset, <b>apex,</b> and <b>offset</b> durations) of smiles were manipulated in the context of two social settings. Using a simulated job interview situation, the studies reported in Chapter 2 show that temporal aspects of smiles significantly influenced judgements made about interviewees. Comparable effects were found for synthetic and human faces. In the studies reported in Chapter 3, the impact of dynamic aspects of smiles was investigated in the context of two trust games with financial stakes. Choice of counterpart and decisions to cooperate with another person in the game were influenced by the dynamic quality of counterparts' smiles. These effects of facial dynamics on cooperative behaviour were shown to be mediated by the perceived trustworthiness of the other player. Focusing on real smiles, the research in Chapter 4 explored the role of the Duchenne smile in the expression and perception of spontaneous and posed smiles. In comparison to dynamic aspects, the signal value of the Duchenne marker was found to be limited and significant only for ratings of the upper face and of static displays. The study reported in Chapter 5 examined the role of smiles with different temporal dynamics in moderating judgements of emotional utterances. Smiles significantly influenced perceptions of emotional state evoked by the utterances and led to different attributions depending on whether anger or disgust was conveyed verbally. In sum, the findings illustrate that dynamic properties convey important information that is detected accurately and decoded meaningfully by perceivers...|$|R
30|$|In Fig.  8, {{we present}} current {{estimates}} using three {{different combinations of}} magnetic coordinates, all {{based on the same}} dataset: Swarm A data, with B_z < - 1  nT, and the sunlight terminator crossing the noon-midnight meridian poleward of ± 75 ^∘. The field-aligned and total horizontal currents are shown. In the left column, we show estimated currents in a pure dipole coordinate representation, which is used in some studies where external and internal magnetic fields are co-estimated (e.g., Sabaka et al. 2004; Lesur et al. 2008). This is an orthogonal coordinate system, and so the mathematical treatment is exact. Nevertheless, the currents in Fig.  8 appear smeared out, because dipole coordinates do not organize magnetic disturbances sufficiently well. In the middle plot, we show the results using a combination of dipole components and apex positions, i.e., a similar approach as Juusola et al. (2014) used. Here, we see prominent features in the polar cap field-aligned currents, which are probably not realistic. These features appear because the dipole pole and <b>apex</b> poles are <b>offset</b> with respect to each other. To the right, we show estimates using the technique presented in this paper. Here the questionable polar cap features are absent. The peak currents are also stronger with a consistent treatment of apex coordinates. This leads us to conclude that our technique gives improved estimates of currents, compared to previous studies.|$|R
40|$|This thesis {{presents}} a fully automatic facial expression analysis {{system based on}} the Facial Action Coding System (FACS). FACS is the best known and {{the most commonly used}} system to describe facial activity in terms of facial muscle actions (i. e., action units, AUs). We will present our research on the analysis of the morphological, spatio-temporal and behavioural aspects of facial expressions. In contrast with most other researchers in the field who use appearance based techniques, we use a geometric feature based approach. We will argue that that approach is more suitable for analysing facial expression temporal dynamics. Our system is capable of explicitly exploring the temporal aspects of facial expressions from an input colour video in terms of their onset (start), <b>apex</b> (peak) and <b>offset</b> (end). The fully automatic system presented here detects 20 facial points in the first frame and tracks them throughout the video. From the tracked points we compute geometry-based features which serve as the input to the remainder of our systems. The AU activation detection system uses GentleBoost feature selection and a Support Vector Machine (SVM) classifier to find which AUs were present in an expression. Temporal dynamics of active AUs are recognised by a hybrid GentleBoost-SVM-Hidden Markov model classifier. The system is capable of analysing 23 out of 27 existing AUs with high accuracy. The main contributions of the work presented in this thesis are the following: we have created a method for fully automatic AU analysis with state-of-the-art recognition results. We have proposed {{for the first time a}} method for recognition of the four temporal phases of an AU. We have build the largest comprehensive database of facial expressions to date. We also present for the first time in the literature two studies for automatic distinction between posed and spontaneous expressions. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

