0|152|Public
30|$|<b>Automatic</b> <b>sound</b> event {{detection}} aims at {{processing the}} continuous acoustic signal and converting it into such symbolic {{descriptions of the}} corresponding sound events present at the auditory scene. The research field studying this process is called computational auditory scene analysis [2]. <b>Automatic</b> <b>sound</b> event detection can be utilized {{in a variety of}} applications, including context-based indexing and retrieval in multimedia databases [3, 4], unobtrusive monitoring in health care [5], surveillance [6], and military applications [7]. The symbolic information about the sound events can be used in other research areas, e.g., audio context recognition [8, 9], automatic tagging [10], and audio segmentation [11].|$|R
30|$|<b>Automatic</b> <b>sound</b> event {{detection}} {{systems are}} usually designed for specific tasks or specific environments. There {{are a number}} of challenges in extending the detection system to handle multiple environments and a large set of events. Event categories and variance within each category make the <b>automatic</b> <b>sound</b> event recognition problem difficult even with well-represented categories when having clean and undistorted signals. The overlapping sound events that constitute a natural auditory scene create an acoustic mixture signal that is more difficult to handle. Another challenge is the presence of certain sound events in multiple contexts (e.g., footsteps present in contexts like street, hallway, beach) calling for rules in modeling of the contexts. Some events are context specific (e.g., keyboard sounds present in the office context) and their variability is lower, as they always appear in similar conditions.|$|R
40|$|This project {{examined}} various {{methods for}} detection of the threatened pouched frog, Assa darlingtoni in south-east Queensland. By utilising <b>automatic</b> <b>sound</b> recording devices {{the project has}} provided valuable information on the calling behaviour of the species that will greatly aid in developing effective monitoring programs for this species and contribute to its conservation in the future...|$|R
40|$|A central {{problem in}} <b>automatic</b> <b>sound</b> {{recognition}} is the mapping between low-level audio features and the meaningful content of an auditory scene. We propose a dynamic network model to perform this mapping. In acoustics, much research {{is devoted to}} low-level per-ceptual abilities such as audio feature extraction and grouping, which are translated into successful signal processing techniques. However, little work is done on modeling knowl-edge and context in sound recognition, although this information is necessary to identify a sound event rather than to separate its components from a scene. We first investigate the role of context in human sound identification in a simple experiment. Then we show {{that the use of}} knowledge in a dynamic network model can improve <b>automatic</b> <b>sound</b> identification by reducing the search space of the low-level audio features. Furthermore, context information dissolves ambiguities that arise from multiple interpretations of one sound event...|$|R
40|$|When extracting {{information}} from simultaneous sound sources, listeners successfully exploit many different factors spanning spatial location and source characteristics. I {{will argue that}} detailed constraints on the form of particular source signals are being employed, and that this therefore is an important direction for research into <b>automatic</b> <b>sound</b> organization systems, in applications ranging from speech separation to environmental sound classification to music understanding...|$|R
50|$|Visit with a GPS guide (Montfort-sur-Meu)The {{visit with}} a GPS guide allows to visit Montfort-sur-Meu through an {{interactive}} walk. The tool allows to visit on your own, in family or {{in a small}} group, thanks to <b>automatic</b> <b>sound</b> and visual activities.The circuit lasts an hour and takes you to discover the medieval town in a fun and a simple way.|$|R
50|$|The {{menu and}} story system also seemed {{more suited to}} an early-1990s {{shareware}} game and not a commercially released title, even lacking an <b>automatic</b> <b>sound</b> card configuration system, which was considered standard by 1995. All in all, the game improved little over older genre simulation games such as 4D Sports: Driving, Test Drive 3 or Continuum, only more demanding system-wise.|$|R
40|$|<b>Automatic</b> <b>sound</b> source {{localization}} {{has recently}} gained interest {{due to its}} various applications that range from surveillance to hearing aids, and teleconferencing to human computer interaction. <b>Automatic</b> <b>sound</b> source localization may refer {{to the process of}} determining only the direction of a sound source, which is known as the direction-of-arrival estimation, or also its distance in order to obtain its coordinates. Various methods have previously been proposed for this purpose. Many of these methods use the time and level differences between the signals captured by each element of a microphone array. An overview of these conventional array processing methods is given and the factors that affect their performance are discussed. The limitations of these methods affecting real-time implementation are highlighted. An emerging source localization method based on acoustic intensity is explained. A theoretical evaluation of different microphone array geometries is given. Two well-known problems, localization of multiple sources and localization of acoustic reflections, are addressed...|$|R
40|$|We {{present a}} novel {{approach}} for synthesizing liquid sounds directly from visual simulations of fluid dynamics. The sound generated by liquid is mainly due to the vibration of resonating bubbles in the medium. Our approach couples physically-based equations for bubble resonance with a real-time shallow-water fluid simulator {{as well as an}} hybrid SPH-grid-based simulator to perform <b>automatic</b> <b>sound</b> synthesis. Our system has been effectively demonstrated on several benchmarks. ...|$|R
40|$|We {{describe}} a methodology for reasoning about realistic concurrent programs. Our methodology allows two-state invariants that span multiple objects without sacrificing thread- or data-modularity, {{as well as}} the derived construction of first-class objects that capture knowledge about the system state. The methodology has been implemented in an <b>automatic</b> <b>sound</b> verifier for concurrent C programs being used to verify the code of the Microsoft Hypervisor, the virtualization kernel of Hyper-V. D. 2. 4 [Software Engineer...|$|R
50|$|Computer-aided {{auscultation}} {{aimed at}} detecting and characterizing heart murmurs is called computer-aided heart auscultation (also known as <b>automatic</b> heart <b>sound</b> analysis).|$|R
40|$|We {{present a}} novel {{approach}} for synthesizing liquid sounds directly from visual simulation of fluid dynamics. Our approach {{takes advantage of the}} fact that the sound generated by liquid is mainly due to the vibration of resonating bubbles in the medium and performs <b>automatic</b> <b>sound</b> synthesis by coupling physically-based equations for bubble resonance with multiple fluid simulators. We effectively demonstrate our system on several benchmarks using a real-time shallow-water fluid simulator as well as a hybrid grid-SPH simulator...|$|R
40|$|We {{describe}} a practical method for reasoning about realistic concurrent programs. Our method allows global two-state invariants that restrict update of shared state. We provide simple, sufficient conditions for checking those global invariants modularly. The method has been implemented in VCC 3, an <b>automatic,</b> <b>sound,</b> modular verifier for concurrent C programs. VCC {{has been used}} to verify functional correctness {{of tens of thousands of}} lines of Microsoft’s Hyper-V virtualization platform 4 and of SYSGO’s embedded real-time operating system PikeOS...|$|R
40|$|This is an Open Access article {{published}} by World Scientific Publishing Company. It is distributed {{under the terms}} of the Creative Commons Attribution 4. 0 (CC-BY) License. Further distribution of this work is permitted, provided the original work is properly cited. T. Theodorou, I. Mpoas, A. Lazaridis, N. Fakotakis, 'Data-Driven Audio Feature Space Clustering for <b>Automatic</b> <b>Sound</b> Recognition in Radio Broadcast News', International Journal on Artificial Intelligence Tools, Vol. 26 (2), April 2017, 1750005 (13 pages), DOI: 10. 1142 /S 021821301750005. ?? The Author(s). In this paper we describe an <b>automatic</b> <b>sound</b> recognition scheme for radio broadcast news based on principal component clustering with respect to the discrimination ability of the principal components. Specifically, streams of broadcast news transmissions, labeled based on the audio event, are decomposed using a large set of audio descriptors and project into the principal component space. A data-driven algorithm clusters the relevance of the components. The component subspaces are used by sound type classifier. This methodology showed that the k-nearest neighbor and the artificial intelligent network provide good results. Also, this methodology showed that discarding unnecessary dimension works in favor on the outcome, as it hardly deteriorates the effectiveness of the algorithms...|$|R
40|$|Abstract. We {{describe}} a practical method for reasoning about realistic concurrent programs. Our method allows global two-state invariants that restrict update of shared state. We provide simple, sufficient conditions for checking those global invariants modularly. The method has been implemented in VCC 3, an <b>automatic,</b> <b>sound,</b> modular verifier for concurrent C programs. VCC {{has been used}} to verify functional correctness {{of tens of thousands of}} lines of Microsoft’s Hyper-V virtualization platform 4 and of SYSGO’s embedded real-time operating system PikeOS. ...|$|R
50|$|Starting in 1971, he {{directed}} {{research and development}} of sound and video systems technology at the Radio and Television Research Centre. He helped develop the delta stereophony system—the first true directional and distance sound reinforcement system for large halls and television productions—as well as a home processor for multichannel audio. Today, the sound reinforcement system is used both in Germany and abroad. Multichannel stereo ambiophonics (the first four-channel system) and <b>automatic</b> <b>sound</b> studio technical equipment were also developed under his leadership.|$|R
40|$|Audio {{classification}} has {{applications in}} a variety of contexts, such as <b>automatic</b> <b>sound</b> analysis, supervised audio segmentation and in audio information search and retrieval. Extended Baum-Welch (EBW) transformations are most commonly used as a discriminative technique for estimating parameters of Gaussian mixtures, though recently they have been applied in unsupervised audio segmentation. In this paper, we extend the use of these transformations to derive an audio classification algorithm. We find that our method outperforms both the Support Vector Machine (SVM) and Gaussian Mixture Model (GMM) likelihood classification methods. Index Terms: audio classification, gradient methods 1...|$|R
40|$|In {{ubiquitous}} environments, {{analysis and}} classification of sound plays {{a critical role}} in various acoustic-based recognition systems. This work aims to contribute towards building an <b>automatic</b> <b>sound</b> recognition system that can understand the surrounding environment by the audio information. In this paper, an acoustic signal based context awareness system is proposed for detecting sound events in five different real-world environment. This approach is based on Back Propagation Neural Network (BPNN) classifier using a new feature set from frequency-domain features. The experiments on various categories illustrate that the results of recognition are significant and effective...|$|R
40|$|Title varies slightly: Light list. Volume I, St. Croix River, Maine to Shrewsbury River, New Jersey, 1996 -Title varies slightly: Light list. Volume I, Atlantic Coast, St. Croix River, Maine to Toms River, New Jersey. "This {{publication}} {{contains a}} list of lights, <b>sound</b> signals, <b>buoys,</b> daybeacons [...] . "Mode of access: Internet. Kept up to date by: Notice to mariners; and: Local notice to mariners...|$|R
30|$|Early {{research}} related to the classification of sounds for everyday life has been concentrating on problems with specific sounds. Examples include gunshots [16], vehicles [17], machines [18], and birds [19]. In addition to this, usually a low number of sound categories {{are involved in the}} studies, specifically chosen to minimize overlapping between different categories, and evaluations are carried out with one or very small set of audio contexts (kitchen [20], bathroom [21], meeting room [22], office and canteen [23]). Many of these previously presented methods are not applicable as such for the <b>automatic</b> <b>sound</b> event detection for continuous audio in real-world situations.|$|R
40|$|Audio {{segmentation}} has {{applications in}} a variety of contexts, such as audio information retrieval, <b>automatic</b> <b>sound</b> analysis, and as a pre-processing step in speech recognition. Extended Baum-Welch (EBW) transformations are most commonly used as a discriminative technique for estimating parameters of Gaussian mixtures. In this paper, we derive an unsupervised audio segmentation approach using these transformations. We find that our algorithm outperforms both the Bayesian Information Criterion (BIC) and Cumulative Sum (CUSUM) segmentation methods. In particular, our EBW segmentation algorithm provides improvements over the baseline approaches in detecting landmarks of short duration and minimizing landmark oversegmentation. In addition, we show that the EBW approach provides faster computation compared to the baseline methods...|$|R
30|$|The work {{presented}} in this article studies how the context information can be used in the <b>automatic</b> <b>sound</b> event detection process, and how the detection system can benefit from such information. Humans are using context information to make more accurate predictions about the sound events and ruling out unlikely events given the context. We propose a similar utilization of context information in the <b>automatic</b> <b>sound</b> event detection process. The proposed approach is composed of two stages: automatic context recognition stage and sound event detection stage. Contexts are modeled using Gaussian mixture models and sound events are modeled using three-state left-to-right hidden Markov models. In the first stage, audio context of the tested signal is recognized. Based on the recognized context, a context-specific set of sound event classes is selected for the sound event detection stage. The event detection stage also uses context-dependent acoustic models and count-based event priors. Two alternative event detection approaches are studied. In the first one, a monophonic event sequence is outputted by detecting the most prominent sound event at each time instance using Viterbi decoding. The second approach introduces a new method for producing polyphonic event sequence by detecting multiple overlapping sound events using multiple restricted Viterbi passes. A new metric is introduced to evaluate the sound event detection performance with various level of polyphony. This combines the detection accuracy and coarse time-resolution error into one metric, making the comparison of the performance of detection algorithms simpler. The two-step approach was found to improve the results substantially compared to the context-independent baseline system. In the block-level, the detection accuracy can be almost doubled by using the proposed context-dependent event detection.|$|R
25|$|On 13 June 1842, Clio {{anchored}} off Woosung. On 16 June, {{after the}} defences {{at the mouth}} of the river were <b>sounded</b> and <b>buoyed,</b> the British bombarded the works on both sides of the river as part of the commencement of operations against Shanghai. She then participated in the expedition up the Yangtze River, to the end of hostilities and signing of the Treaty of Nanking on 29 August. Troubridge's replacement as captain of Clio from 30 December 1842 was Commander James Fitzjames.|$|R
40|$|One of {{the most}} {{important}} contributions that any decision 	support system can make to achieve wide acceptance among any community 	is to be able to justify its own suggestions. When dealing with highly 	technical and scientifically advanced practitioners like medical 	doctors or any other related clinical workers, the ability to justify 	itself using the domain specialist usual terminology and technicalities 	is imperative. In this article we demonstrate the use of an ontological 	framework as inferencing basis for <b>automatic</b> <b>sound</b> clinical suggestions 	providing. Our work has two main contributions, consolidating the 	use of {OGCP} (Ontology for General Clinical Practice) as foundation 	and providing controlled English justifications of the extracted 	suggestions. We found that clinical practitioners feel as acceptable 	the Attempto Controlled English justifications generated from the 	knowledge base...|$|R
40|$|<b>Automatic</b> <b>sound</b> IF {{standard}} detection Fully programmable 28 -bit {{audio processor}} for enhanced ATV sound—default TV audio flow loaded on reset Implements Analog Devices and third-party branded audio algorithms Adjustable digital delay line for audio/video Synchronization {{for up to}} 200 ms stereo delay High performance 24 -bit ADC and DAC 94 dB DNR performance on DAC channels 95 dB DNR performance on ADC channels Dual headphone outputs with integrated amplifiers High performance pulse-width modulation (PWM) digital outputs Multichannel digital baseband I/O 4 stereo synchronous digital I 2 S input channels One 6 -channel sample rate converter (SRC) and one stereo SRC supporting input sample rates from 5 kHz to 50 kHz One stereo synchronous digital I 2 S output S/PDIF output with S/PDIF input mux capabilit...|$|R
40|$|AbstractOne of {{the most}} {{important}} contributions that any decision support system can make to achieve wide acceptance among any community {{is to be able to}} justify its own suggestions. When dealing with highly technical and scientifically advanced practitioners like medical doctors or any other related clinical workers, the ability to justify itself using the domain specialist usual terminology and technicalities is imperative. In this article we demonstrate the use of an ontological framework as inferencing basis for <b>automatic</b> <b>sound</b> clinical suggestions providing. Our work has two main contributions, consolidating the use of OGCP (Ontology for General Clinical Practice) as foundation and providing controlled English justifications of the extracted suggestions. We found that clinical practitioners feel as acceptable the Attempto Controlled English justifications generated from the knowledge base...|$|R
40|$|CUIDADO is a {{new project}} (European I. S. T. Project) which aims at {{providing}} content-based music applications (Vinet, Herrera, and Pachet 2002). Among these applications is an authoring tool for managing sample databases including search by similarity, search by textual attributes but also a system allowing <b>automatic</b> <b>sound</b> classification based on predefined taxonomies but also allowing user to define its own taxonomies. This last point raises a crucial issue concerning-the design of the classifier but also-the choice of the appropriate signal descriptors in order to perform the classification. This paper concentrates {{on the design of}} CUIDADO classifier and on two algorithms for automatically selecting the most appropriate signal descriptors for a given taxonomy: the discriminant analysis and the mutual information...|$|R
25|$|On 11 August Dryad formed {{part of a}} {{squadron}} of frigates directed to <b>sound</b> and <b>buoy</b> the Sloe Strait {{in preparation for the}} attack on Flushing, which fell on 15 August. In forcing the West Scheld, the British ships were under fire from shore batteries for two hours, during which time they suffered two men killed and nine wounded; Dryad had no casualties. Prize money was paid in 1812. The expedition ended in failure, mostly due to malaria decimating the expeditionary force, which withdrew by September.|$|R
40|$|Tropical tunas {{associate}} with objects floating {{at the surface}} of the ocean, a behavior widely exploited by fishers. However, the respective roles played by environmental variables and behavioral processes (e. g. social behavior) in the formation of these aggregations remain elusive. To investigate the role of social behavior in the dynamics of such aggregations, we used the binary choice approach. The experimental design comprised two close and identical anchored fish aggregating devices (FADs) equipped with an echo <b>sounder</b> <b>buoy</b> to monitor the aggregated biomass of tuna under each device. Analysis of the results entailed characterizing whether the aggregated biomass is distributed asymmetrically (indicative of social behavior playing a role in the dynamics) or symmetrically between the two close and identical FADs, and comparing the results with theoretical distributions based on different definitions of basic units (individual fish or small schools). The results suggest that social interactions underlie aggregation processes, which represents a major advance in our understanding of these aggregations, a priority for science-based fishery management. While recognizing the logistical and technical constraints, we encourage the development of experimental studies (e. g. in which animals are presented with controlled situations) to enhance our understanding of the behavior of large pelagic fish. © 2013 Elsevier B. V. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|<b>Automatic</b> <b>sound</b> {{recognition}} (ASR) {{has attracted}} increased and wide ranging interests in recent years. In this paper, {{we carry out}} a review of some important contributions in ASR techniques, mainly over the last {{one and a half}} decades. Similar to speech recognition systems, the robustness of an ASR system largely depends on the choice of feature(s) and classifier(s). We take a wider perspective in providing an overview of the features and classifiers used in ASR systems starting from early works in content-based audio classification to more recent developments in applications such as sound event recognition, audio surveillance, and environmental sound recognition. We also review techniques that have been utilized in noise robust sound recognition systems and feature optimization methods. Finally, some of the less commonly known applications of ASR are discussed...|$|R
40|$|A {{computationally}} efficient {{approach to}} the automatic segmentation (labeling) of noise disturbed speech is presented. The segmentation algorithm employs short term spectrum based feature vectors and a subspace representation of the sound classes. The two sound classes of vowels and unvoiced fricatives are trained with the TIMIT acoustic phonetic continuous speech corpus. The sound class detector is applied in a speech enhancement system and for the automatic segment duration measurement. INTRODUCTION Originally this <b>automatic</b> <b>sound</b> class detection algorithm was developed as an improved replacement for the speech pause detector of a speech enhancement system [Wokurek 94]. Clearly this application requires noise robustness. Furthermore, a solution with low computational effort was sought to allow real time implementation. Representing the sound classes by subspaces meets both goals. The speech signal {{is transformed into a}} sequence of feature vectors. This transformation controls which [...] ...|$|R
40|$|Sounds are {{essential}} to how humans perceive and interact with the world and are captured in recordings and shared on the Internet on a minute-by-minute basis. These recordings, which are predominantly videos, constitute the largest archive of sounds we know. However, most of these recordings have undescribed content making necessary methods for <b>automatic</b> <b>sound</b> analysis, indexing and retrieval. These methods have to address multiple challenges, such as the relation between sounds and language, numerous and diverse sound classes, and large-scale evaluation. We propose a system that continuously learns from the web relations between sounds and language, improves sound recognition models over time and evaluates its learning competency in the large-scale without references. We introduce the Never-Ending Learner of Sounds (NELS), a project for continuously learning of sounds and their associated knowledge, available on line in nels. cs. cmu. ed...|$|R
50|$|In 1979, Lowrance {{introduced}} the LDD-1800, the world's first sonar unit guaranteed NOT to find fish. The LDD-1800 {{was one of}} the first digital depth sounders to be controlled by a tiny computer built inside. It would show only the bottom depth in large digital numbers and required no controls. This was Lowrance's first completely <b>automatic</b> depth <b>sounder.</b>|$|R
30|$|The <b>automatic</b> <b>sound</b> event {{classification}} (SEC) {{has attracted}} a growing attention in recent years. Feature extraction is {{a critical factor in}} SEC system, and the deep neural network (DNN) algorithms have achieved the state-of-the-art performance for SEC. The extreme learning machine-based auto-encoder (ELM-AE) is a new deep learning algorithm, which has both an excellent representation performance and very fast training procedure. However, ELM-AE suffers from the problem of unstability. In this work, a bilinear multi-column ELM-AE (B-MC-ELM-AE) algorithm is proposed to improve the robustness, stability, and feature representation of the original ELM-AE, which is then applied to learn feature representation of sound signals. Moreover, a B-MC-ELM-AE and two-stage ensemble learning (TsEL)-based feature learning and classification framework is then developed to perform the robust and effective SEC. The experimental results on the Real World Computing Partnership Sound Scene Database show that the proposed SEC framework outperforms the state-of-the-art DNN algorithm.|$|R
40|$|In this paper, {{we propose}} {{the use of}} spatial and {{harmonic}} features in combination with long short term memory (LSTM) recurrent neural network (RNN) for <b>automatic</b> <b>sound</b> event detection (SED) task. Real life sound recordings typically have many overlapping sound events, {{making it hard to}} recognize with just mono channel audio. Human listeners have been successfully recognizing the mixture of overlapping sound events using pitch cues and exploiting the stereo (multichannel) audio signal available at their ears to spatially localize these events. Traditionally SED systems have only been using mono channel audio, motivated by the human listener we propose to extend them to use multichannel audio. The proposed SED system is compared against {{the state of the art}} mono channel method on the development subset of TUT sound events detection 2016 database. The usage of spatial and harmonic features are shown to improve the performance of SED...|$|R
40|$|Orthoptera {{songs are}} widely used for the {{description}} and diagnosis of new species. Most of the corresponding sound recordings are in analogue format (tapes), widely scattered among institutions, and {{only a small fraction}} is accessible as an organized collection (‘phonothek’). Approximately 12, 000 Orthoptera sound recordings, representing about 4, 000 species from all biogeographic regions, have been digitized and stored in a database during the DORSA project (Digital Orthoptera Specimen Access – www. dorsa. de). Together with images and collection data of voucher specimens, the DORSA serves as a ’Virtual Museum‘, summarizing distributed collections and phonotheks from several German researchers and institutions. A subset of recordings was used to develop <b>automatic</b> <b>sound</b> recognition tools, by using neural networks fed by acoustic parameters. Relevant parameters, such as carrier frequency and pulse repetition rate, were determined by a special software module, which could then be used to extract those features from all cricket songs hitherto available in the DORSA database. These parameter...|$|R
