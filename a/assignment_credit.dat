1|349|Public
40|$|This paper aims {{to compare}} the {{interest}} rates charged by credit unions and banks (commercial and multiple) in order to check {{whether there are differences}} between the rates charged for loans without personal assignment and the effects of this difference in interest rates charged by banks in similar operations. The results show that although the rates charged by credit unions are significantly lower {{there is no evidence that}} competition from credit unions make sufficient pressure to reduce the rates charged by banks on the personal <b>assignment</b> <b>credit</b> lines. We also found results that show that credit unions tend to be installed in rural areas with lower homicide rates, lower population density, high quality of the judiciary system and a great number of households with income between half and three minimum wages per capita, i. e., places where there are signs of high social capital. ...|$|E
40|$|Abstract. Reinforcement {{learning}} (RL) algorithms {{attempt to}} assign {{the credit for}} rewards to the actions {{that contributed to the}} reward. Thus far, <b>credit</b> <b>assignment</b> has been done in one of two ways: uniformly, or using a discounting model that assigns exponentially more credit to recent actions. This paper demonstrates an alternative approach to temporal <b>credit</b> <b>assignment,</b> taking advantage of exact or approximate prior information about correct <b>credit</b> <b>assignment.</b> Infinite impulse response (IIR) filters are used to model <b>credit</b> <b>assignment</b> information. IIR filters generalise exponentially discounting eligibility traces to arbitrary <b>credit</b> <b>assignment</b> models. This approach can be applied to any RL algorithm that employs an eligibility trace. The use of IIR <b>credit</b> <b>assignment</b> filters is explored using both the GPOMDP policy-gradient algorithm and the Sarsa(λ) temporal-difference algorithm. A drop in bias and variance of value or gradient estimates is demonstrated, resulting in faster convergence to better policies. ...|$|R
40|$|Single-agent {{reinforcement}} {{learners in}} time-extended domains and multi-agent systems {{share a common}} difficulty known as the <b>credit</b> <b>assignment</b> problem. Multiagent systems have the structural <b>credit</b> <b>assignment</b> problem of determining the contributions of a particular agent to a common task. Instead, time-extended single-agent systems have the temporal <b>credit</b> <b>assignment</b> problem of determining the contribution of a particular action {{to the quality of}} the full sequence of actions. Traditionally these two problems are considered different and are handled in separate ways. In this article we show how these two forms of the <b>credit</b> <b>assignment</b> problem are equivalent. In this unified framework, a single-agent Markov decision process can be broken down into a single-time-step multiagent process. Furthermore we show that Monte Carlo estimation or Q-learning (depending on whether the values of resulting actions in the episode are known at the time of learning) are equivalent to different agent utility functions in a multi-agent system. This equivalence shows how an often neglected issue in multi-agent systems is equivalent to a well-known deficiency in multi-timestep learning and lays the basis for solving time-extended multi-agent problems, where both <b>credit</b> <b>assignment</b> problems are present. 1...|$|R
30|$|The <b>credit</b> <b>assignment</b> scheme {{required}} to convert feedback information into a suitable reward.|$|R
40|$|Abstract. Multiagent {{systems have}} had a {{powerful}} impact on the real world. Many of the systems it studies (air traffic, satellite coordination, rover explo-ration) are inherently multi-objective, but they are often treated as single-objective problems within the research. A very important concept within multiagent sys-tems is that of credit assignment: clearly quantifying an individual agent’s impact on the overall system performance. In this work we extend the concept of <b>credit</b> <b>assignment</b> into multi-objective problems, broadening the traditional multiagent learning framework to account for multiple objectives. We show in two domains that by leveraging established <b>credit</b> <b>assignment</b> principles in a multi-objective setting, we can improve performance by (i) increasing learning speed by up to 10 x (ii) reducing sensitivity to unmodeled disturbances by up to 98. 4 % and (iii) producing solutions that dominate all solutions discovered by a traditional team-based <b>credit</b> <b>assignment</b> schema. Our results suggest that in a multiagent multi-objective problem, proper <b>credit</b> <b>assignment</b> is as important to performance as the choice of multi-objective algorithm. ...|$|R
40|$|International audienceAdaptive Operator Selection (AOS) {{turns the}} impacts of the {{applications}} of variation operators into Operator Selection through a <b>Credit</b> <b>Assignment</b> mechanism. However, most <b>Credit</b> <b>Assignment</b> schemes make direct use of the fitness gain between parent and offspring. A first issue is that the Operator Selection technique that uses such kind of <b>Credit</b> <b>Assignment</b> {{is likely to be}} highly dependent on the a priori unknown bounds of the fitness function. Additionally, these bounds are likely to change along evolution, as fitness gains tend to get smaller as convergence occurs. Furthermore, and maybe more importantly, a fitness-based <b>credit</b> <b>assignment</b> forbid any invariance by monotonous transformation of the fitness, what is a usual source of robustness for comparison-based Evolutionary Algorithms. In this context, this paper proposes two new <b>Credit</b> <b>Assignment</b> mechanisms, one inspired by the Area Under the Curve paradigm, and the other close to the Sum of Ranks. Using fitness improvement as raw reward, and directly coupled to a Multi-Armed Bandit Operator Selection Rule, the resulting AOS obtain very good performances on both the OneMax problem and some artificial scenarios, while demonstrating their robustness with respect to hyper-parameter and fitness transformations. Furthermore, using fitness ranks as raw reward results in a fully comparison-based AOS with reasonable performances...|$|R
40|$|Coordinating {{multiple}} {{agents that}} need to perform a sequence of actions to maximize a system level reward requires solving two distinct <b>credit</b> <b>assignment</b> problems. First, <b>credit</b> must be assigned for an action taken at time step t that results in a reward at time step t ′> t. Second, credit must be assigned for the contribution of agent i to the overall system performance. The first <b>credit</b> <b>assignment</b> problem is typically addressed with temporal difference methods such as Q-learning. The second <b>credit</b> <b>assignment</b> problem is typically addressed by creating custom reward functions. To address both <b>credit</b> <b>assignment</b> problems simultaneously, we propose the “Q Updates with Immediate Counterfactual Rewards-learning” (QUICR-learning) designed to improve both the convergence properties and performance of Q-learning in large multi-agent problems. QUICR-learning is based on previous work on single-time-step counterfactual rewards described by the collectives framework. Results on a traffic congestion problem shows that QUICR-learning is significantly better than a Q-learner using collectives-based (single-time-step counterfactual) rewards. In addition QUICR-learning provides significant gains over conventional and local Q-learning. Additional results on a multi-agent grid-world problem show that the improvements due to QUICR-learning are not domain specific and can provide up to a ten fold increase in performance over existing methods...|$|R
5000|$|Pro Amateur | Luckyiam.PSC - Extra <b>Credit</b> <b>Assignment</b> #1 [...] "The Vehicle" [...] (2002) Outhouse, Revenge Entertainment ...|$|R
5000|$|Another <b>assignment</b> <b>credited</b> to {{her firm}} {{is known as}} [...] "manufactured home" [...] or {{prefabricated}} homes that are transported and erected at sites in urban areas, which was implemented in 1997 for the Manufactured Housing Institute. These houses are two storied and single storied buildings built at affordable cost which were erected in Wilkinsburg, Washington, D.C. and Louisville. These houses {{did not differ in}} their looks from other in situ-built houses in the neighborhood. William Maxman had died in 1997 and in 2001 she remarried. Her new husband, Rolf Sauer, was an architect from Philadelphia and had one daughter by an earlier marriage. Thus, Maxman has seven children, three of her own and four step children. She has 15 grandchildren.|$|R
5000|$|<b>Credit</b> <b>assignment</b> path (CAP) - A {{chain of}} transformations from input to output. CAPs {{describe}} potentially causal connections between input and output.|$|R
5000|$|The MPEG negotiates {{collective}} bargaining agreements (union contracts) with producers and major motion picture movie studios and enforces existing agreements with employers involved in post-production. The MPEG provides assistance for securing better working conditions, including but salary, medical benefits, safety (particularly [...] "turnaround time") and artistic (<b>assignment</b> of <b>credit)</b> concerns.|$|R
40|$|Abstract: One of {{the basic}} issues in {{navigation}} of autonomous mobile robots is the obstacle avoidance task that is commonly achieved using reactive control paradigm where a local mapping from perceived states to actions is acquired. A control strategy with learning capabilities in an unknown environment can be obtained using reinforcement learning where the learning agent is given only sparse reward information. This <b>credit</b> <b>assignment</b> problem includes both temporal and structural aspects. While the temporal <b>credit</b> <b>assignment</b> problem is solved using core elements of reinforcement learning agent, solution of the structural <b>credit</b> <b>assignment</b> problem requires an appropriate internal state space representation of the environment. In this paper a discrete coding of the input space using a neural network structure is presented {{as opposed to the}} commonly used continuous internal representation. This enables a faster and more efficient convergence of the reinforcement learning process...|$|R
40|$|Decomposition of {{learning}} problems {{is important in}} order to make learning in large state spaces tractable. One approach to learning problem decomposition is to represent the knowledge that will be learned as a collection of smaller, more individually manageable pieces. However, such an approach requires the design of more complex knowledge structures over which structural <b>credit</b> <b>assignment</b> must be performed during learning. The specific knowledge organization scheme chosen has a major impact on the characteristics of the structural <b>credit</b> <b>assignment</b> problem that arises. In this paper, we present an organizational scheme called Externally Verifiable Decomposition designed to facilitate <b>credit</b> <b>assignment</b> over composite knowledge representations. We also describe an experiment in an interactive strategy game that shows that a learner making use of EVD is able to improve performance on the studied task more rapidly than by using pure reinforcement learning. ...|$|R
50|$|Big staffs: Similarly, {{the number}} of {{practitioners}} of science on any one project grew as well, creating difficulty, and often controversy, in the <b>assignment</b> of <b>credit</b> for scientific discoveries (the Nobel Prize system, for example, allows awarding only three individuals in any one topic per year, based on a 19th-century model of the scientific enterprise).|$|R
40|$|Multi-agent {{learning}} in Markov Decisions Problems is challenging {{because of the}} presence of two <b>credit</b> <b>assignment</b> problems: 1) How to credit an action taken at time step t for rewards received at t ′> t; and 2) How to credit an action taken by agent i considering the system reward is a function of the actions of all the agents. The first <b>credit</b> <b>assignment</b> problem is typically addressed with temporal difference methods such as Q-learning or TD(λ). The second <b>credit</b> <b>assignment</b> problem is typically addressed either by hand-crafting reward functions that assign proper credit to an agent, or by making certain independence assumptions about an agent’s state-space and reward function. To address both <b>credit</b> <b>assignment</b> problems simultaneously, we propose the “Q Updates with Immediate Counterfactual Rewards-learning ” (QUICRlearning) designed to improve both the convergence properties and performance of Q-{{learning in}} large multi-agent problems. Instead of assuming that an agent’s value function can be made independent of other agents, this method suppresses the impact of other agents using counterfactual rewards. Results on multi-agent grid-world problems over multiple topologies show that QUICR-learning can achieve up to thirty fold improvements in performance over both conventional and local Q-learning in the largest tested systems. ...|$|R
30|$|<b>Credit</b> <b>assignment</b> is used {{to convert}} a {{feedback}} indicator into a form supported by the adaptation mechanism (i.e. selection rule) and/or for aggregating multiple feedback indicators. The normalization of a feedback indicator {{can be regarded as}} a simple <b>credit</b> <b>assignment</b> scheme. In fact, normalization is helpful in reducing dependency related to feedback indicators built from raw values [22]. The concept of ranking has also been proposed as a way of eliminating the concern regarding raw values [22]. Reference [23] focuses on rare but substantial improvements based on the probability of producing exceptionally good solutions.|$|R
40|$|The use of activity-based <b>credit</b> <b>assignment</b> (ACA) for the {{automatic}} evaluation and selection of candidate com-ponents of systems is considered here. The whole process {{consists of a}} precise automatic structured specification of systems. Mathematical definitions and algorithms are pro-vided. ACA converges on good components/compositions faster than repository-based random search. As systems constitute a vast class of problems to be specified by a mod-eler, this automatic composition of systems opens new re-search perspectives. The paper also places ACA {{within the context of}} existing approaches to <b>credit</b> <b>assignment</b> in clas-sifier systems. ...|$|R
5000|$|Gilmore {{works at}} Mount John University Observatory and Department of Physics and Astronomy, University of Canterbury, Christchurch, New Zealand. He {{is also a}} member of the Organizing Committee of IAU Commission 6, which oversees the {{dissemination}} of information and the <b>assignment</b> of <b>credit</b> for astronomical discoveries. The Commission still bears the name [...] "Astronomical Telegrams", even though telegrams are no longer used.|$|R
40|$|We {{often need}} to learn how to move based on a single {{performance}} measure that reflects the overall success of our movements. However, movements have many properties, such as their trajectories, speeds and timing of end-points, thus the brain needs to decide which properties of movements should be improved; it needs to solve the <b>credit</b> <b>assignment</b> problem. Currently, little is known about how humans solve <b>credit</b> <b>assignment</b> problems in the context of reinforcement learning. Here we tested how human participants solve such problems during a trajectory-learning task. Without an explicitly-defined target movement, participants made hand reaches and received monetary rewards as feedback on a trial-by-trial basis. The curvature and direction of the attempted reach trajectories determined the monetary rewards received in a manner that can be manipulated experimentally. Based on the history of action-reward pairs, participants quickly solved the <b>credit</b> <b>assignment</b> problem and learned the implicit payoff function. A Bayesian credit-assignment model with built-i...|$|R
40|$|Appendix A: Worksheet for Institutions on the <b>Assignment</b> of <b>Credit</b> Hours and Clock Hours {{completed}} by Office of the Registrar provides the peer review {{team with a}} single {{source of information about}} the institution’s calendar, credit hour policies and total credit hour generation related to the courses for which it provides instruction, and an overview of the institution’s pattern of distribution of <b>credit</b> hour <b>assignments...</b>|$|R
40|$|Following the {{specific}} {{characteristics of the}} factoring agreement, its close link with the <b>assignment</b> of <b>credit</b> claims and their different functions, this paper seeks to approach their implications for the debtor’s legal position. How can the debtor’s legal position be changed due to the acts adopted by himself? What is the relevance of {{the specific}} purposes of factoring for the definition of the debtor’s legal position...|$|R
40|$|<b>Credit</b> <b>assignment</b> is a {{fundamental}} issue for the Learning Classifier Systems literature. We engage in a detailed investigation of <b>credit</b> <b>assignment</b> in one recent system called UCS, {{and in the process}} uncover two previously undocumented features. We draw on techniques from the classical pattern recognition literature, showing how to analytically derive an optimal <b>credit</b> <b>assignment</b> system, given certain assumptions. Our aim is not primarily to improve accuracy, but to better understand the system and put it on a more solid theoretical foundation. Nonetheless, empirical results on benign data demonstrate our new system, called UCSpv (UCS with principled voting), can match or exceed the original UCS. Further, its fitness function is principled, and, unlike that of UCS, requires no tuning. However, on more difficult data it seems UCSpv does need some form of tuning or correction. We believe the framework we adopt offers a promising new direction for LCS research, providing principled methods for action selection and bringing LCS closer to the mainstream pattern recognition literature. Categories and Subject Descriptors I. 2. 6 [Artificial Intelligence]: Learning—Concept learning...|$|R
40|$|This {{research}} has aspired {{to build a}} system which is capable of solving problems by means of its past experience, especially an autonomous agent that can learn from trial and error sequences. To achieve this, connectionist neural network architectures are combined with the reinforcement learning methods. And the <b>credit</b> <b>assignment</b> problem in multi layer perceptron (MLP) architectures is altered. In classical <b>credit</b> <b>assignment</b> problems, actual output {{of the system and}} the previously known data in which the system tries to approximate are compared and the discrepancy between them is attempted to be minimized. However, temporal difference <b>credit</b> <b>assignment</b> depends on the temporary successive outputs. By this new method, it is more feasible to find the relation between each event rather than their consequences. Also in this thesis k-means algorithm is modified. Moreover MLP architectures is written in C++ environment, like Backpropagation, Radial Basis Function Networks, Radial Basis Function Link Net, Self-organized neural network, k-means algorithm. And with their combination for the Reinforcement learning, temporal difference learning, and Q-learning architectures were realized, all these algorithms are simulated...|$|R
50|$|Collect all <b>credits,</b> <b>assignment,</b> report, and thesis defence; {{students}} must pass French and English exam. Dual degree programs are also applying with some specializes and some universities in Europe such INSA Rène, INP Toulouse, etc.|$|R
40|$|Classification Abstract — Structured {{matching}} {{captures the}} bottom-up {{pattern of the}} hierarchical classification techniques used in many knowledge systems: subsets of features describing {{the state of the}} world are progressively aggregated into equivalence classes in an abstraction network, until the required decision is made at the root node. In this paper, we describe a supervised learning technique for automatic repair of abstraction hierarchies in structured matching. Note that the task of repairing an abstraction network involves the structural <b>credit</b> <b>assignment</b> problem. We describe the property of empirical determinability, and show that the design of an abstraction network according to this property enables structural <b>credit</b> <b>assignment.</b> I...|$|R
5000|$|... 1994 - The Technical University of Budapest {{is among}} the first {{universities}} in Hungary to introduce the credit system. The university applies the <b>credit</b> <b>assignment</b> according to the European Credit Transfer System (ECTS) in its accredited academic programs.|$|R
5000|$|In the novel, Mr. Neck {{gives the}} extra <b>credit</b> <b>{{assignment}}</b> {{as an option}} for the entire class. In the film, Mr. Neck gives the assignment to Melinda after her parents make her ask him how she can raise her grade.|$|R
3000|$|... is the {{temporal}} <b>credit</b> <b>assignment</b> coefficient for the expectation-boosting method. The {{number of states}} was fixed to 3, since our exploratory experiment, described in Section 5, indicated {{that it was the}} optimal value by a large margin. For each method, the hyperparameters [...]...|$|R
40|$|A {{new way of}} {{measuring}} generalization in unsupervised learning is presented. The measure {{is based on an}} exclusive allocation, or <b>credit</b> <b>assignment,</b> criterion. In a classifier that satisfies the criterion, input patterns are parsed so that the credit for each input feature is assigned exclusively to one of multiple, possibly overlapping, output categories. Such a classifier achieves context-sensitive, global representations of pattern data. Two additional constraints, sequence masking and uncertainty multiplexing, are described; these can be used to refine the measure of generalization. The generalization performance of EXIN networks, winner-take-all competitive learning networks, linear decorrelator networks, and Nigrin's SONNET [...] 2 network is compared. Keywords Generalization, Exclusive allocation, <b>Credit</b> <b>assignment,</b> Binding, Unsupervised learning, Pattern classification, Distributed coding, EXIN (excitatory+inhibitory) learning, Sparse coding, Rule extraction, Regularization, Blind [...] ...|$|R
40|$|Neuronal {{systems that}} are {{involved}} in reinforcement learning must solve the temporal <b>credit</b> <b>assignment</b> pro-blem, i. e., how is a stimulus associated with a reward that is delayed in time? Theoretical studies [1 - 3] have postulated that neural activity underlying learning ‘tags’ synapses with an ‘eligibility trace’, and that the subse-quent arrival of a reward converts the eligibility traces into actual modification of synaptic efficacies. While eligibility traces provide one simple solution to the tem-poral <b>credit</b> <b>assignment</b> problem, they alone do not con-stitute a stable learning rule because there is no other mechanism indicating when learning should cease. In order to attain stability, rules involving eligibility traces often assume that once the association is learned, further learning is prevented via an inhibition of th...|$|R
40|$|Many {{applications}} of Reinforcement Learning (RL) and Evolutionary Computation (EC) are addressing the same problem, namely, to maximize some agent's fitness {{function in a}} potentially unknown environment. The most challenging open issues in such applications include partial observability of the agent's environment, hierarchical {{and other types of}} abstract <b>credit</b> <b>assignment,</b> and the learning of <b>credit</b> <b>assignment</b> algorithms. I will summarize why EC provides a more natural framework for addressing these issues than RL based on value functions and dynamic programming. Then I will point out fundamental drawbacks of traditional EC methods in case of stochastic environments, stochastic policies, and unknown temporal delays between actions and observable effects. I will discuss a remedy called the success-story algorithm which combines aspects of RL and EC...|$|R
40|$|Abstract—In this paper, {{we define}} the {{generalization}} problem, summarize various approaches in generalization, identify the <b>credit</b> <b>assignment</b> problem, and present {{the problem and}} some solutions in measuring generalizability. We discuss anomalies in the ordering of hypotheses in a subdomain when performance is normalized and averaged, and show conditions under which anomalies can be eliminated. To generalize performance across subdomains, we present a measure called probability of win that measures the probability whether one hypothesis is better than another. Finally, we discuss some limitations in using probabilities of win and illustrate their application in finding new parameter values for TimberWolf, a package for VLSI cell placement and routing. Index Terms—Anomalies in generalization, <b>credit</b> <b>assignment</b> problem generalization, machine learning, subdomains, probability of win, VLSI cell placement and routing. ...|$|R
40|$|Abstract. <b>Credit</b> <b>Assignment</b> is an {{important}} ingredient of several proposals {{that have been made}} for Adaptive Operator Selection. Instead of the average fitness improvement of newborn offspring, this paper proposes to use some empirical order statistics of those improvements, arguing that rare but highly beneficial jumps matter as much or more than frequent but small improvements. An extreme value based <b>Credit</b> <b>Assignment</b> is thus proposed, rewarding each operator with the best fitness improvement observed in a sliding window for this operator. This mechanism, combined with existing Adaptive Operator Selection rules, is investigated in an EC-like setting. First results show that the proposed method allows both the Adaptive Pursuit and the Dynamic Multi-Armed Bandit selection rules to actually track the best operators along evolution. ...|$|R
40|$|International audienceThe paper {{reviews the}} {{literature}} on disciplinary <b>credit</b> <b>assignment</b> practices, and {{presents the results of}} a longitudinal study of <b>credit</b> <b>assignment</b> practices in the fields of economics, high energy physics, and information science. The practice of alphabetization of authorship is demonstrated to vary significantly between the fields. A slight increase is found to have taken place in economics during the last 30 years (1978 - 2007). A substantial decrease is found to have taken place in information science during the same period. High energy physics is found to be characterised by a high and stable share of alphabetized multi-authorships during the investigated period (1990 - 2007). It is important to be aware of such disciplinary differences when conducting bibliometric analyses...|$|R
40|$|Abstract — Hand-design of an {{intelligent}} agent’s behaviors and their hierarchy {{is a very}} hard task. One {{of the most important}} steps toward creating intelligent agents is providing them with capability to learn the required behaviors and their architecture. Architecture learning in a behavior-based agent with Subsumption architecture is considered in this paper. Overall value function is decomposed into easily calculate-able parts in order to learn the behavior hierarchy. Using probabilistic formulations, two different decomposition methods are discussed: storing the estimated value of each behavior in each layer, and storing the ordering of behaviors in the architecture. Using defined decompositions, two appropriate <b>credit</b> <b>assignment</b> methods are designed. Finally, the proposed methods are tested in a multi-robot object-lifting task that results in satisfactory performance. Keywords- reinforcement learning, architecture learning, <b>credit</b> <b>assignment,</b> value decomposition, subsumption architecture. I...|$|R
40|$|Many {{algorithms}} {{have been}} recently {{reported for the}} training of analog multi-layer perceptron. Most of these algorithms were evaluated either from a computational or simulation view point. This paper applies several of these algorithms to the training of an analog multi-layer perceptron chip. The advantages and shortcomings of these algorithms in terms of training and generalisation performance and their capabilities in a limited precision environment are discussed. Extensive experiments demonstrate that a trade-off exists between the parallelisation of perturbations and the efficiency of <b>credit</b> <b>assignment.</b> Two semi-parallelisation heuristics are presented and are shown to provide advantages in terms of efficient exploration of the solution space and fewer <b>credit</b> <b>assignment</b> confusions. 1 INTRODUCTION Analog microelectronic implementations of the multi-layer perceptron (MLP) offer a number of attractive aspects: they provide an inherent parallelism since computational element [...] ...|$|R
