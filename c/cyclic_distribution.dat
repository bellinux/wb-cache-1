40|59|Public
50|$|Landscheidt, T. 1987. <b>Cyclic</b> <b>Distribution</b> Of Energetic X-Ray Flares. Solar Physics 107 (1): 195-199.|$|E
40|$|Abstract. The paper {{presents}} a heterogeneous distribution of computa-tions while solving dense linear algebra problems on heterogeneous net-works of computers. The distribution {{is based on}} heterogeneous block <b>cyclic</b> <b>distribution</b> which is extension of the traditional homogeneous block <b>cyclic</b> <b>distribution</b> taking into account differences in the processor performances. The mpC language, specially designed for parallel pro-gramming heterogeneous networks is briefly introduced. An mpC apli-cation carring out Cholesky factorization on a heterogeneous network of workstations is used {{to demonstrate that the}} heterogeneous distribution have an essential advantage over the traditional homogeneous distribu-tion. ...|$|E
40|$|Block <b>cyclic</b> <b>distribution</b> {{seems to}} suit well for most linear algebra {{algorithms}} and {{this type of}} data distribution was chosen for the ScaLAPACK library {{as well as for}} the HPF language. But one has to choose a good compromise for the size of the blocks (to achieve a good computation and communication efficiency and a good load balancing). This choice heavily depends on each operation, so it is essential to be able to go from one block <b>cyclic</b> <b>distribution</b> to another very quickly. Moreover, it is also essential to be able to choose the right number of processors and the best grid shape for a given operation. We present here the data redistribution algorithms we implemented in the ScaLAPACK library in order to go from one block <b>cyclic</b> <b>distribution</b> on a grid to another one on another grid. A complexity study is made that shows the efficiency of our solution. Timing results on the Intel Paragon and the Cray T 3 D corroborate our results. 1 Introduction This paper describes the solution of the [...] ...|$|E
40|$|Distributing data {{is one of}} the key {{problems}} in implementing efficient distributed-memory parallel programs. The problem is especially difficult in programs where (1) data redistribution between computational phases is considered or (2) the participating processors (nodes) executing a parallel application are not dedicated. In either case, the commonly used BLOCK and <b>CYCLIC</b> <b>distributions</b> no longer suffice. We have investigated this problem [...] ...|$|R
40|$|In {{this paper}} {{we present a}} {{multiple}} phase I/O collective operation for generic block <b>cyclic</b> <b>distributions.</b> The communication pattern is automatically generated by an inspector phase and the communication and file access phase are performed by an executor phase. The inspector phase can be amortized over several accesses. We show that our method outperforms other techniques used for parallel I/O optimizations for small access granularities. ...|$|R
40|$|Generating {{code for}} the array {{assignment}} statement of High Performance Fortran(HPF) {{in the presence}} of block-cyclic distributions of data arrays is considered difficult, and several algorithms have been published to solve this problem. We present a comprehensive study of the run-time performance of the code these algorithms generate. We classify these algorithms into several families, identify several issues of interest in the generated code, and present experimental performance data for the various algorithms. We demonstrate that the code generated for block-cyclic distributions runs almost as efficiently as that generated for block or <b>cyclic</b> <b>distributions...</b>|$|R
40|$|We {{present a}} method for message {{aggregation}} for affine loops in the Chapel programming language. Our optimization is based on modulo unrolling, a method to improve memory parallelism for tiled architectures. We adapt the technique {{to the problem of}} compiling for message passing architectures. Our method results in performance improvements in both runtime and communication compared to the non-optimized Chapel compiler. For our Chapel implementation, we modify the leader and follower iterators in the distribution modules, instead of creating a com-piler transformation. Our results compare the performance of optimized programs and programs using existing Chapel data distributions. Data show that our method used with Chapel’s <b>Cyclic</b> <b>distribution</b> results in 64 percent fewer messages and a 36 percent decrease in runtime for our suite of benchmarks; it results in 72 percent fewer messages and a 53 percent decrease in runtime for the Block <b>Cyclic</b> <b>distribution...</b>|$|E
40|$|This work {{presents}} modulo unrolling without unrolling (modulo unrolling WU), {{a method}} for message aggregation for parallel loops in message passing programs that use affine array accesses in Chapel, a Partitioned Global Address Space (PGAS) parallel programming language. Messages incur a non-trivial run time overhead, a significant component of which is independent {{of the size of}} the message. Therefore, aggregating messages improves performance. Our optimization for message aggregation is based on a technique known as modulo unrolling, pioneered by Barua [1] whose purpose was to ensure a statically predictable single tile number for each memory reference for tiled architectures, such as the MIT Raw Machine [2]. Modulo unrolling WU applies to data that is distributed in a cyclic or block-cyclic manner. In this paper, we adapt the aforementioned modulo unrolling technique to the difficult problem of efficiently compiling PGAS languages to message passing architectures. When applied to loops and data distributed cyclically or block-cyclically, modulo unrolling WU can decide when to aggregate messages thereby reducing the overall message count and runtime for a particular loop. Compared to other methods, modulo unrolling WU greatly simplifies the complex problem of automatic code generation of message passing code. It also results in substantial performance improvements in both runtime and communication compared to the non-optimized Chapel compiler. To implement this optimization in Chapel, we modify the <b>Cyclic</b> <b>distribution</b> module's follower iterator and the Block <b>Cyclic</b> <b>distribution</b> module's leader and follower iterators, as opposed to creating a traditional compiler transformation. Results were collected that compare the performance of Chapel programs optimized with modulo unrolling WU and Chapel programs using the existing Chapel data distributions. Data collected on a ten-locale cluster show that on average, modulo unrolling WU used with Chapel's <b>Cyclic</b> <b>distribution</b> results in 64 percent fewer messages and a 36 percent decrease in runtime for our suite of benchmarks. Similarly, modulo unrolling WU used with Chapel's Block <b>Cyclic</b> <b>distribution</b> results in 72 percent fewer messages and a 53 percent decrease in runtime. Finally, the results from three different scaling experiments suggest that the greatest improvements from modulo unrolling WU occur when parallel follower iterator chunks of work contain the greatest number of data elements...|$|E
40|$|Optimizing {{communication}} {{is a key}} issue in compiling data-parallel languages for distributed memory architectures. We examine here the <b>cyclic</b> <b>distribution,</b> and we derive symbolic expressions for communication sets under the only assumption that the initial parallel loop is defined by affine expressions of the indices. This technique relys on unimodular changes of basis. Analysis of the properties of communications leads to a tiling of the local memory addresses that provides maximal message vectorization...|$|E
40|$|This {{article is}} devoted to the {{run-time}} redistribution of one-dimensional arrays that are distributed in a block-cyclic fashion over a processor grid. In a previous paper [2], we have reported how to derive optimal schedules made up of successive communication-steps. In this paper we assume that successive steps may overlap. We show how to obtain an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P-processor grid to a <b>CYCLIC(s)</b> <b>distribution</b> on a Q-processor grid, for arbitrary values of the redistribution parameters P, Q, r, and s. We use graph-theoretic algorithms, and modular algebra techniques to derive these optimal schedulings...|$|R
40|$|In multicomputers, an {{appropriate}} data distribution {{is crucial for}} reducing communication overhead and therefore the overall performance. For this reason, data parallel languages provide programmers with primitives, such as BLOCK and CYCLIC {{that can be used}} to distribute data across the distributed memory. However, the languages do not aid the programmer as to how the distribution should be performed to maximize the performance. Therefore, this paper presents an analysis of data distribution methods for overlapping computation and communication in the Gaussian elimination algorithm. The analysis indicates that both BLOCK and <b>CYCLIC</b> <b>distributions</b> have their own merit; however, BLOCK_CYCLIC with its hybrid characteristic consistently out performs its counterparts. ...|$|R
40|$|International audienceThis {{article is}} devoted to the {{run-time}} redistribution of one-dimensional arrays that are distributed in a block-cyclic fashion over a processor grid. In a previous paper, we have reported how to derive optimal schedules made up of successive communication-steps. In this paper we assume that successive steps may overlap. We show how to obtain an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P-processor grid to a <b>CYCLIC(s)</b> <b>distribution</b> on a Q-processor grid, for arbitrary values of the redistribution parameters P, Q, r, and s. We use graph-theoretic algorithms, and modular algebra techniques to derive these optimal schedulings...|$|R
40|$|Jack Dongarra z We {{present a}} new {{parallel}} {{implementation of a}} divide and conquer algo-rithm for computing the spectral decomposition of a symmetric tridiagonal matrix on distributed memory architectures. The implementation we de-velop di ers from other implementations in that we use a two dimensional block <b>cyclic</b> <b>distribution</b> of the data, we use the Lowner theorem approach to compute orthogonal eigenvectors, and we introduce permutations before the back transformation of each rank-one update {{in order to make}} goo...|$|E
40|$|National audienceIn this paper, we {{deal with}} {{algorithmic}} issues on heterogeneous platforms. We concentrate on dense linear algebra kernels, such as matrix multiplication or LU decomposition. Block <b>cyclic</b> <b>distribution</b> techniques used in ScaLAPACK are no longer sufficient to balance the load among processors running at different speeds. The main result {{of this paper is}} to provide a static data distribution scheme that leads to an asymptotically perfect load balancing for LU decomposition, thereby providing solid foundations toward the design of a cluster-oriented version of ScaLAPACK...|$|E
40|$|International audienceWe {{describe}} recent {{progress of}} an ongoing research programme aimed at producing computational science software that can exploit high performance architectures in the atomic physics application domain. We examine the computational bottleneck of matrix construction in a suite of two-dimensional R-matrix propagation programs, 2 DRMP, that are aimed at creating virtual electron collision experiments on HPC architectures. We build on Ixaru's extended frequency dependent quadrature rules (EFDQR) for Slater integrals and examine the challenge of constructing Hamiltonian matrices in parallel across an m-processor compute node in a block <b>cyclic</b> <b>distribution</b> for subsequent diagonalization by ScaLAPACK...|$|E
40|$|A {{significant}} part of scientific codes consist of sparse matrix computations. In this work we propose two new pseudoregular data distributions for sparse matrices. The Multiple Recursive Decomposition (MRD) partitions the data using the prime factors of the dimensions of a multiprocessor network with mesh topology. Furthermore, we introduce a new storage scheme, storage-by-row-of-blocks, that significantly increases {{the efficiency of the}} Scatter distribution. We will name Block Row Scatter (BRS) distribution this new variant. The MRD and BRS methods achieve results that improve those obtained by other analyzed methods, being their implementation easier. In fact, the data distributions resulting from the MRD and BRS methods are a generalization of the Block and <b>Cyclic</b> <b>distributions</b> used in dense matrices. 1...|$|R
30|$|In {{addition}} to careful performance analysis and comparisons, we have provided some new ideas for PAPR control with DDST, for modeling {{the effects of}} symbol level limiter in channel estimation, and for modeling the <b>cyclic</b> mean <b>distribution</b> based on multinomial distribution or its Gaussian approximation.|$|R
40|$|International audienceThis {{article is}} devoted to the {{run-time}} redistribution of one-dimensional arrays that are distributed in a block-cyclic fashion over a processor grid. While previous studies have concentrated on efficiently generating the communication messages to be exchanged by the processors involved in the redistribution, we focus on the scheduling of those messages: how to organize the message exchanges into "structured" communication steps that minimize contention. We build upon results of Walker and Otto, who solved a particular instance of the problem, and we derive an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P-processor grid to a <b>CYCLIC(s)</b> <b>distribution</b> on a Q-processor grid, for arbitrary values of the redistribution parameters P, Q, r, and...|$|R
40|$|In {{this paper}} {{we present a}} new {{parallel}} radix FFT algorithm based on the BSP model Our parallel algorithm uses the groupcyclic distribution family which makes it simple to understand and easy to implement We show how to reduce the com munication cost of the algorithm {{by a factor of}} three in the case that the inputoutput vector is in the <b>cyclic</b> <b>distribution</b> We also show how to reduce computation time on computers with a cachebased architecture We present performance results on a Cray TE with up to processors obtaining reasonable eciency levels for local problem sizes as small as and very good eciency levels for sizes larger tha...|$|E
40|$|In this paper, {{we present}} a new {{parallel}} radix- 4 FFT algorithm based on the BSP model. Our parallel algorithm uses the group-cyclic distribution family, which makes it simple to understand and easy to implement. We show how to reduce the communication cost of the algorithm {{by a factor of}} three, in the case that the input/output vector is in the <b>cyclic</b> <b>distribution.</b> We also show how to reduce computation time on computers with a cache-based architecture. We present performance results on a Cray T 3 E with up to 64 processors, obtaining reasonable efficiency levels for local problem sizes as small as 256 and very good efficiency levels for sizes larger than 2048...|$|E
40|$|We {{present a}} new {{parallel}} radix- 4 FFT algorithm {{based on the}} BSP model. Our parallel algorithm uses the group-cyclic distribution family, which makes it simple to understand and easy to implement. We show how to reduce the communication cost of the algorithm {{by a factor of}} three, in the case that the input/output vector is in the <b>cyclic</b> <b>distribution.</b> We also show how to reduce computation time on computers with a cache-based architecture. We present performance results on a Cray T 3 E with up to 64 processors, obtaining reasonable efficiency levels for local problem sizes as small as 256 and very good efficiency levels for local sizes larger than 2048. Key words: fast Fourier transform, bulk synchronous paralle...|$|E
40|$|This {{article is}} devoted to the {{run-time}} redistribution of arrays that are distributed in a blockcyclic fashion over a multidimensional processor grid. While previous studies have concentrated on e ciently generating the communication messages to be exchanged by the processors involved in the redistribution, we focus on the scheduling of those messages: how to organize the message exchanges into &quot; communication steps that minimize contention. We build upon results of Walker and Otto, who solved a particular instance of the problem, and we derive an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P-processor grid to a <b>CYCLIC(s)</b> <b>distribution</b> on a Q-processor grid, for arbitrary values of the redistribution parameters P, Q, r, and s. Key-words: distributed arrays, redistribution, block-cyclic distribution, scheduling, MPI, HPF...|$|R
40|$|This {{article is}} devoted to the {{run-time}} redistribution of one-dimensional arrays that are distributed in a block-cyclic fashion over a processor grid. While previous studies have concentrated on efficiently generating the communication messages to be exchanged by the processors involved in the redistribution, we focus on the scheduling of those messages: how to organize the message exchanges into "structured" communication steps that minimize contention. We build upon results of Walker and Otto, who solved a particular instance of the problem, and we derive an optimal scheduling for the most general case, namely, moving from a CYCLIC(r) distribution on a P -processor grid to a <b>CYCLIC(s)</b> <b>distribution</b> on a Q-processor grid, for arbitrary values of the redistribution parameters P, Q, r, and s. Key-words: distributed arrays, redistribution, block-cyclic distribution, scheduling, MPI, HPF. This work {{was supported in part by}} the National Science Foundation Grant No. ASC- 9005933; [...] ...|$|R
40|$|Parallel prefix and suffix {{functions}} {{are very important}} intrinsic functions in HPF (High Performance Fortran) language’s runtime-system libraries. Several algorithms have been published {{to deal with this}} parallel prefix / suffix problem. But there are some specifications should be considered in HPF to implement this kind of functions with high performance. In HPF, there are different kinds of array data distribution models, such as BLOCK, CYCLIC, BLOCK-CYCLIC, which makes it difficult to implement these functions efficiently, especially for BLOCK-CYCLIC case. In this paper, we will survey all these kinds of data distribution models, and introduce a new algorithm for every ditribution model to implement parallel prefix and suffix functions with high performance on distributed memory systems based on message passing communication. From the algorithm we can see, we can deal with the difficult BLOCK-CYCLIC distribution as efficiently as the BLOCK and <b>CYCLIC</b> <b>distributions.</b> Keywords...|$|R
40|$|We {{present a}} new {{parallel}} {{implementation of a}} divide and conquer algorithm for computing the spectral decomposition of a symmetric tridiagonal matrix on distributed memory architectures. The implementation we develop differs from other implementations in that we use a two dimensional block <b>cyclic</b> <b>distribution</b> of the data, we use the Lowner theorem approach to compute orthogonal eigenvectors, and we introduce permutations before the back transformation of each rank-one update {{in order to make}} good use of deflation. This algorithm yields the first scalable, portable and numerically stable parallel divide and conquer eigensolver. Numerical results confirm the effectiveness of our algorithm. We compare performance of the algorithm with that of the QR algorithm and of bisection followed by inverse iteration on an IBM SP 2 and a cluster of Pentium PII's...|$|E
40|$|This paper {{presents}} and analyzes two different strategies of heterogeneous distribution of computations solving dense linear algebra problems on heter-ogeneous networks of computers. The first strategy {{is based on}} heterogeneous distribution of processes over processors and homogeneous block cyclic distri-bution of data over the processes. The second is based on homogeneous distribu-tion of processes over processors and heterogeneous block <b>cyclic</b> <b>distribution</b> of data over the processes. Both strategies were implemented in the mpC languagea dedicated parallel extension of ANSI C for efficient and portable programming of heterogeneous networks of computers. The first strategy was implemented using calls to ScaLAPACK; the second strategy was implemented with calls to LAPACK and BLAS. Cholesky factorization on a heterogeneous network of workstations is used {{to demonstrate that the}} heterogeneous distribu-tions have an advantage over the traditional homogeneous distribution...|$|E
40|$|Array {{statements}} {{are often used}} to express data-parallelism in scientific languages such as Fortran 90 and High Performance Fortran. In compiling array statements for a distributed-memory machine, efficient generation of communication sets and local index sets is important. We show that for arrays distributed block-cyclically on multiple processors, the local memory access sequence and communication sets can be efficiently enumerated as closed forms using regular sections. First, closed form solutions are presented for arrays that are distributed using block or cyclic distributions. These closed forms are then used with a virtual processor approach to give an efficient solution for arrays with block-cyclic distributions. This approach is based on viewing a block-cyclic distribution as a block (or <b>cyclic)</b> <b>distribution</b> {{on a set of}} virtual processors, which are cyclically (or block-wise) mapped to physical processors. These views are referred to as virtual-block or virtual-cyclic [...] ...|$|E
40|$|The {{stated goal}} of High Performance Fortran (HPF) was to "address the {{problems}} of writing data parallel programs where the distribution of data affects performance". After examining {{the current version of}} the language we are led to the conclusion that HPF has not fully achieved this goal. While the basic distribution functions offered by the language [...] regular block, cyclic, and block <b>cyclic</b> <b>distributions</b> [...] can support regular numerical algorithms, advanced applications such as particle-in-cell codes or unstructured mesh solvers cannot be expressed adequately. We believe that this is a major weakness of HPF, significantly reducing its chances of becoming accepted in the numerical community. The paper discusses the data distribution and alignment issues in detail, points out some flaws in the basic language, and outlines possible future paths of development. Furthermore, we briefly deal with the issue of task parallelism and its integration with the data parallel paradigm of [...] ...|$|R
40|$|Sparse matrix vector {{multiplication}} (SpMxV) {{is often}} {{one of the}} core components of many scientific applications. Many authors have proposed methods for its data distribution in distributed memory multiprocessors. We can classify these into four groups: Scatter, D-Way Strip, Recursive and Miscellaneous. In this work we propose a new method (Multiple Recursive Decomposition (MRD)), which partitions the data using the prime factors of the dimensions of a multiprocessor network with mesh topology. Furthermore, we introduce a new storage scheme, storage-by-row-of-blocks, that significantly increases {{the efficiency of the}} Scatter method. We will name Block Row Scatter (BRS) method this new variant. The MRD and BRS methods achieve results that improve those obtained by other analyzed methods, being their implementation easier. In fact, the data distributions resulting from the MRD and BRS methods are a generalization of the Block and <b>Cyclic</b> <b>distributions</b> used in dense matrices. Keywords. Spar [...] ...|$|R
40|$|The <b>distribution</b> of <b>cyclic</b> {{oligomer}} concentrations (examined {{up to the}} cyclic pentamer) in {{the process}} of step-growth polymerisation decreases, in concentrated solution, with i(- 2. 5) if the chains follow Gaussian statistics (i: degree of polymerization). This picture is analogous to that shown by <b>cyclic</b> oligomer <b>distributions</b> obtained under thermodynamic control and is in contrast with that shown by <b>cyclic</b> oligomer <b>distributions</b> obtained by irreversible chain-growth polymerisation which is characterized by a gradient of - 1. 5. This result has been initially obtained by numerical integration of the differential rate equations pertinent to a kinetic model relative to the reaction of a bifunctional reactant A-B under batchwise conditions, and then confirmed by analytical integration of the differential rate equations under the condition that propagation is much faster than cyclisation. The case in which the monomeric ring is strained has been also investigated. Also in this case the obtained distribution is analogous to that shown under equilibrium conditions. From an examination of the distributions of the ring yields, the best conditions for the preparation of either the cyclic monomer or the cyclic dimer are suggested...|$|R
40|$|This paper {{presents}} a novel solution approach for planning <b>cyclic</b> <b>distribution</b> {{from a single}} depot to multiple customers with constant, deterministic demand rates. The objective is to minimize the total cost rate consisting of fleet costs, distribution costs from the depot to the customers and inventory holding costs at the customers. A solution is built in two phases: designing routes and composing the fleet. When designing vehicle routes in the first phase, the route cycle times are chosen such that the distribution and inventory holding costs are minimized. When assigning routes to vehicles in the second phase, the routes remain unchanged, but their cycle times can be adjusted to minimize the required number of vehicles. The building blocks of this two-phase solution approach are embedded in a metaheuristic framework. Computational experiments show that the resulting solution framework outperforms existing solution approaches...|$|E
40|$|This paper {{studies the}} {{benefits}} of compiling dataparallel languages onto a multithreaded runtime environment providing dynamic thread migration facilities. Each abstract process is mapped onto a thread, so that dynamic load balancing {{can be achieved by}} migrating threads among the processing nodes. We describe and evaluate an implementation of this idea in the Adaptor HPF compiler. We show that no deep modification of the compiler are needed, and that the overhead of managing threads can be kept small. As an experimental validation, we report on an HPF implementation of the Gauss Partial Pivoting algorithm. We show that using an initial BLOCK data distribution with our dynamic load balancing scheme can reach the performance of the optimal <b>CYCLIC</b> <b>distribution.</b> Introduction Data-parallel languages are now recognized as major tools for high performance computing. Considerable effort has been put in designing sophisticated methods to compile them efficiently onto a variety of architectures [...] ...|$|E
40|$|We {{consider}} {{the problem of}} sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and opti-mality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in column-blocks and only involves elementwise operations and parallel matrix multiplica-tions. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block <b>cyclic</b> <b>distribution</b> of data and parameters to achieve load balance and improve the efficiency {{in the use of}} memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores. ...|$|E
40|$|The {{capacity}} of serially-ordered auditory-verbal short-term memory (AVSTM) {{is sensitive to}} {{the timing of the}} material to be stored, and both temporal processing and AVSTM capacity are implicated in the development of language. We developed a novel “rehearsal-probe” task to investigate the relationship between temporal precision and the capacity to remember serial order. Participants listened to a sub-span sequence of spoken digits and silently rehearsed the items and their timing during an unfilled retention interval. After an unpredictable delay, a tone prompted report of the item being rehearsed at that moment. An initial experiment showed <b>cyclic</b> <b>distributions</b> of item responses over time, with peaks preserving serial order and broad, overlapping tails. The spread of the response distributions increased with additional memory load and correlated negatively with participants’ auditory digit spans. A second study replicated the negative correlation and demonstrated its specificity to AVSTM by controlling for differences in visuo-spatial STM and nonverbal IQ. The results are consistent with the idea that a common resource underpins both the temporal precision and {{capacity of}} AVSTM. The rehearsal-probe task may provide a valuable tool for investigating links between temporal processing and AVSTM capacity in the context of speech and language abilities...|$|R
40|$|This paper {{describes}} {{the design of}} ScaLAPACK, a scalable software library for performing dense and banded linear algebra computations on distributed memory concurrent computers. The specification of the data distribution has important consequences for interprocessor communication and load balance, and hence {{is a major factor}} in determining performance and scalability of the library routines. The block <b>cyclic</b> data <b>distribution</b> is adopted as a simple, yet general-purpose, way of decomposing block-partitioned matrices. Distributed memory versions of the Level 3 BLAS provide an easy and convenient way of implementing the ScaLAPACK routines...|$|R
40|$|The {{effect of}} interference-fit on {{fretting}} fatigue crack initiation and K was studied numerically for available experimental {{results in a}} single pinned plate in Al-alloy 7075 -T 6. The role of interference ratio was investigated alongside friction coefficient through finite element. <b>Cyclic</b> stress <b>distributions</b> in the plate ligament and fretting stresses on the contact interface were evaluated using 3 -D elastic-plastic finite element models. Additionally a 3 -D elastic finite element model was utilized to discuss K of cracks emanating from interference fitted holes. Results demonstrate that fretting was {{the main reason for}} crack nucleation, and furthermore, the location was precisely predicted and fatigue life enhancement was explained...|$|R
