152|16|Public
2500|$|The Privacy Policy lists {{many types}} of {{information}} they collect and store, including other software running on a user's computer ("programs installed or in use"). Malwarebytes says they collect [...] "name, email address, mailing address, or phone number... company name, company size, business type... Internet protocol (IP) addresses, browser type, Internet service provider (ISP), referring/exit pages, the files viewed on our site ... operating system, date/time stamp, and/or <b>clickstream</b> <b>data</b> ... type of device you use, operating system version, and the unique device identifier... language... 32- or 64-bit... Information from the Windows Security/Action Center, including security settings and programs installed or in use... license... If it represents a console system, the number of seats being managed by that installation of the console Endpoint domain information... organization to which the IP address is licensed, if any".|$|E
50|$|Unauthorized <b>clickstream</b> <b>data</b> {{collection}} {{is considered to}} be spyware. However, authorized <b>clickstream</b> <b>data</b> collection comes from organizations that use opt-in panels to generate market research using panelists who agree to share their <b>clickstream</b> <b>data</b> with other companies by downloading and installing specialized clickstream collection agents.|$|E
5000|$|I-Hsien Ting, Chris Kimble, Daniel Kudenko (2006) [...] "UBB Mining: Finding Unexpected Browsing Behaviour in <b>Clickstream</b> <b>Data</b> {{to improve}} a Web Site’s Design" ...|$|E
25|$|Use <b>clickstream</b> {{analysis}} and <b>data</b> mining to detect fraudulent behavior.|$|R
40|$|Knowing how {{consumers}} navigate {{online shopping}} web sites enables retailers {{to not only}} better design their sites for navigation but also place buying recommendations at strategic points and personalise the flow of content. Frequent navigation paths {{can be derived from}} browsing histories or <b>clickstreams</b> with sequence-oriented <b>data</b> mining techniques. In this working paper, we highlight, with examples, the relevance of frequent navigation paths to online shopping behaviour research and review some relevant data mining techniques...|$|R
40|$|With the {{continued}} growth and proliferation of e-commerce, Web services, and Web-based information systems, the volumes of <b>clickstream,</b> transaction <b>data,</b> and user profile {{data collected by}} Web-based organizations in their daily operations has reached astronomical proportions. Analyzing such data can help these organizations determine the life-time value of clients, design cross-marketing strategies across products and services, {{evaluate the effectiveness of}} promotional campaigns, optimize the functionality of Web-based applications, provide more personalized content to visitors, and find the most effective logical structure for their Web space. This type of analysis involves the automatic discovery of meaningful patterns and relationships from a large collection of primarily semi-structured data, often stored in Web and applications server access logs, as well as in related operational data sources. Web usage mining refers to the automatic discovery and analysis of patterns in clickstreams, user transactions and other associated dat...|$|R
50|$|Use of <b>clickstream</b> <b>data</b> {{can raise}} privacy concerns, {{especially}} since some Internet service providers {{have resorted to}} selling users' <b>clickstream</b> <b>data</b> {{as a way to}} enhance revenue. There are 10-12 companies that purchase this data, typically for about $0.40/month per user. While this practice may not directly identify individual users, it is often possible to indirectly identify specific users, an example being the AOL search data scandal. Most consumers are unaware of this practice, and its potential for compromising their privacy. In addition, few ISPs publicly admit to this practice.|$|E
5000|$|Lillian Clark, I-Hsien Ting, Chris Kimble, Peter Wright, Daniel Kudenko (2006)"Combining ethnographic and <b>clickstream</b> <b>data</b> to {{identify}} user Web browsing strategies" [...] Journal of Information Research, Vol. 11 No. 2, January 2006 ...|$|E
5000|$|Targeted networks: Sometimes called [...] "next generation" [...] or [...] "2.0" [...] ad networks, these {{focus on}} {{specific}} targeting {{technologies such as}} behavioral or contextual, that have been built into an ad server. Targeted networks specialize in using consumer <b>clickstream</b> <b>data</b> to enhance {{the value of the}} inventory they purchase. further specialized targeted networks include social graph technologies which attempt to enhance the value of inventory using connections in social networks.|$|E
40|$|Web usage mining: {{automatic}} {{discovery of}} patterns in <b>clickstreams</b> and associated <b>data</b> collected or generated {{as a result}} of user interactions with one or more Web sites. This paper describes web usage mining for our college log files to analyze the behavioral patterns and profiles of users interacting with a Web site. The discovered patterns are represented as clusters that are frequently accessed by groups of visitors with common interests. In this paper, the visitors and hits were forecasted to predict the further access statistics...|$|R
30|$|According to Borne [49], {{from the}} {{beginning}} of written language until 2003, humanity had produced about five exabytes of data. In 2011, the same amount of data is created every 2  days. In 2013, the same amount of data is created every 10  min. This is because today organizations collect data from different sources, such as machine <b>data,</b> application logs, <b>clickstream</b> logs, whether <b>data,</b> emails, contracts, geographic information systems and geospatial data, survey data, reports, spreadsheets, and social media [41, 50]. The ability to compute massive quantity of information is the key feature of big data analytics.|$|R
40|$|Abstract—Identifying {{correlations}} between context data, user behavior, and semantic information {{can lead to}} new services {{that are able to}} adapt to different situations. This “personalization” process can be based on recommendations on content. To better support service developers in focusing mainly on the creation of their service logic, these recommendations should be provided by a generic multipurpose recommender. Therefore, this paper proposes a generic framework that delivers “contextual recommendations ” that are based on the combination of previously gathered user feedback data (i. e. ratings and <b>clickstream</b> history), context <b>data,</b> and ontology-based content categorization schemes. This paper provides a detailed overview of the specification, a short description of a possible usage scenario, and a discussion of the results...|$|R
50|$|Targeted at the {{enterprise}} market, DataSage {{was a leading}} provider of e-marketing and personalization applications that help organizations create a comprehensive single enterprise-wide view of their customers. DataSage developed innovative software solutions to enhance customer relationship management (CRM), helping organizations to grow and retain high-value customer relationships. DataSage offered a complete software solution that combined world-class consulting services with data mining technology to analyze vast amounts of customer transaction and <b>clickstream</b> <b>data</b> to measure and inform decision-makers to the impact their business decisions had on customer behavior.|$|E
5000|$|Microsoft's {{response}} to this issue, coming from a company spokesperson, was: [...] "We do not copy Google's results." [...] Bing's Vice President, Harry Shum, later reiterated that the search result data Google claimed that Bing copied had in fact come from Bing's very own users. Shum wrote that [...] "we use over 1,000 different signals and features in our ranking algorithm. A small piece of that is <b>clickstream</b> <b>data</b> we get from some of our customers, who opt into sharing anonymous data as they navigate the web {{in order to help}} us improve the experience for all users." ...|$|E
5000|$|The Privacy Policy lists {{many types}} of {{information}} they collect and store, including other software running on a user's computer ("programs installed or in use"). Malwarebytes says they collect [...] "name, email address, mailing address, or phone number... company name, company size, business type... Internet protocol (IP) addresses, browser type, Internet service provider (ISP), referring/exit pages, the files viewed on our site ... operating system, date/time stamp, and/or <b>clickstream</b> <b>data</b> ... type of device you use, operating system version, and the unique device identifier... language... 32- or 64-bit... Information from the Windows Security/Action Center, including security settings and programs installed or in use... license... If it represents a console system, the number of seats being managed by that installation of the console, Endpoint domain information... organization to which the IP address is licensed, if any".|$|E
40|$|The {{main goal}} of this masters thesis is to analyze current {{marketing}} communication of a specific leadership development company active and the Czech market and its effectiveness. Based on the results recommend measures for improvement using appropriate channels. In the first part theoretical background of online communication, strategic framework and measurement of performance is introduced. Statistics, expert surveys and insight from opinion leaders from the field are included to provide more practical and comprehensive understanding of approach to individual channels, current trends and potential future development. Second part is dedicated to analysis of online communication strategy of chosen leadership development company. Analysis are {{based on interviews with}} relevant target group, comparison of online activities of competition and <b>clickstream</b> analysis of <b>data</b> from Google Analytics of various communication channels...|$|R
30|$|Traditional {{information}} systems rely on very structured data. Inputs {{have to be}} accurately entered into the system {{in order to produce}} meaningful outputs (lists, reports, forecasts, etc.). Scholars often discuss the dimension of data quality as accuracy, relevancy, completeness and timeliness [62]. Heterogeneous environments frequently need to use data cleaning techniques to solve the “garbage in, garbage out” problem [63]. Variety of data from different sources like sensor <b>data,</b> application logs, <b>clickstream</b> logs, whether <b>data,</b> emails, contracts, geographic data, reports, spreadsheets and social media is difficult to handle. That is, data from different domains are virtually impossible to translate into a structured form [41]. In fact, big data is diverse and unstructured, therefore it requires special tools and techniques that go beyond traditional information system and relational databases solutions.|$|R
40|$|We analyze {{preferences}} and the reading flow of users {{of a popular}} Austrian online newspaper. Unlike traditional news filtering approaches, we postulate that a user’s preference for particular articles depends {{not only on the}} topic and on propositional contents, but also on the user’s current context and on more subtle attributes. Our assumption is motivated by the observation that many people read newspapers because they actually enjoy the process. Such sentiments depend on a complex variety of factors. The present study is part of an ongoing effort to bring more advanced personalization to online media. Towards this end, we present a systematic evaluation of the merit of contextual and non-propositional features based on real-life <b>clickstream</b> and postings <b>data.</b> Furthermore, we assess the impact of different recommendation strategies on the learning performance of our system...|$|R
40|$|<b>Clickstream</b> <b>data</b> from {{e-commerce}} customers is {{a principal}} resource for evaluation of a website design. Many tools have been proposed for the visualization of <b>Clickstream</b> <b>data</b> [1, 2]. However, they do neither point to website design weaknesses nor translate into the improvements. Usually, an expert needs to go through an often lengthy an...|$|E
40|$|In this paper, I use {{a survey}} of 57 {{individuals}} to inform future analysis of <b>clickstream</b> <b>data.</b> Respondents performed four search tasks and answered several questions about their Internet habits. I use their responses {{to determine how to}} interpret the raw <b>clickstream</b> <b>data</b> in other papers. This paper was written as a chapter of my doctoral dissertation and is intended as a supplement for other papers, rather than as a stand-alone work. In particular, it is relevant to “Analyzing Website Choice Using <b>Clickstream</b> <b>Data</b> ” and “Using Household-Specific Regressions to Estimate True State Dependence at Internet Portals” The growth of the Internet has provided economists, marketers, and statisticians with a mountain of new data to analyze. One prevalent but relatively underused example of such data is <b>clickstream</b> <b>data.</b> This data format consists of each website visited by a panel of users and the order in which they arrive at these websites. It is often accompanied by the time of arrival at and departure from the website as well as the degree of activity at the website and the demographic characteristics of the users. Examples of companies that collect <b>clickstream</b> <b>data</b> ar...|$|E
40|$|The authors discuss {{research}} {{progress and}} future opportunities for modeling consumer choice on the Internet using <b>clickstream</b> <b>data</b> (the electronic records of Internet usage recorded by company web servers and syndicated data services). The authors compare {{the nature of}} Internet choice (as captured by <b>clickstream</b> <b>data)</b> with supermarket choice (as captured by UPC scanner panel data), highlighting the differences relevant to choice modelers. Though the application of choice models to <b>clickstream</b> <b>data</b> is relatively new, the authors review existing early work and provide a two-by-two categorization of the applications studied to date (delineating search versus purchase {{on the one hand}} and within-site versus across-site choices on the other). The paper offers directions for further research in these areas and discusses additional opportunities afforded by clickstream information, including personalization, data mining, automation, and customer valuation. Notwithstanding the numerous challenges associated with <b>clickstream</b> <b>data</b> research, the authors conclude that the detailed nature of the information tracked about Internet usage and e-commerce transactions presents an enormous opportunity for empirical modelers to enhance the understanding and prediction of choice behavior. 246 BUCKLIN ET AL. Key words: internet, choice models, <b>clickstream</b> <b>data</b> 1...|$|E
40|$|Big {{data from}} massive open online courses (MOOCs) have enabled {{researchers}} to examine learning processes at almost infinite levels of granularity. Yet, such data sets {{do not track}} every important element in the learning process. Many strategies that MOOC learners use to overcome learning challenges are not captured in <b>clickstream</b> and log <b>data.</b> In this study, we interviewed 92 MOOC learners to better understand their worlds, investigate possible mechanisms of student attrition, and extend conversations {{about the use of}} big data in education. Findings reveal three important domains of the experience of MOOC students that are absent from MOOC tracking logs: the practices at learners’ workstations, learners’ activities online but off-platform, and the wider social context of their lives beyond the MOOC. These findings enrich our understanding of learner agency in MOOCs, clarify the spaces in-between recorded tracking log events, and challenge the view that MOOC learners are disembodied autodidacts...|$|R
40|$|We {{show that}} the {{e-commerce}} domain can provide all the right ingredients for successful data mining and claim {{that it is a}} killer domain for data mining. We describe an integrated architecture, based on our experience at Blue Martini Software, for supporting this integration. The architecture can dramatically reduce the pre-processing, cleaning, and data understanding effort often documented to take 80 % of the time in knowledge discovery projects. We emphasize the need for data collection at the application server layer (not the web server) in order to support logging of data and metadata that is essential to the discovery process. We describe the data transformation bridges required from the transaction processing systems and customer event streams (e. g., <b>clickstreams)</b> to the <b>data</b> warehouse. We detail the mining workbench, which needs to provide multiple views of the data through reporting, data mining algorithms, visualization, and OLAP. We conclude with a set of challenges. ...|$|R
40|$|Firms are {{increasingly}} using <b>clickstream</b> and transactional <b>data</b> to tailor product offerings to visitors at their site. Ecommerce websites have the opportunity, at each interaction, to offer multiple items (referred to as an offer set) {{that might be}} of interest to a visitor. We consider a scenario where a firm is interested in maximizing the expected payoff when composing an offer set. We develop a methodology that considers possible future offer sets based on the current choices of the user and identifies an offer set that will maximize expected payoffs for an entire session. Our framework considers both the items viewed and purchased by a visitor and models the probability of an item being viewed and purchased separately when calculating expected payoffs. The possibility of a user backtracking and viewing a previously offered item is also explicitly modelled. We show that identifying the optimal offer set is a difficult problem when the number of candidate items is large and the offer set consists of several items even for short time horizons. We develop an efficient heuristic for the one period look-ahead case and show that even by considering such a short horizon the approach is much superior to alternative benchmark approaches. Proposed methodology demonstrates how the appropriate use of information technologies can help e-commerce sites improve their profitability...|$|R
40|$|Marketing on the Internet is {{the next}} big field in {{marketing}} research. <b>Clickstream</b> <b>data</b> is a great contribution to analyze the effects of advocacy based marketing strategies. Handling <b>Clickstream</b> <b>data</b> becomes a big issue. This paper {{will look at the}} problems caused by <b>Clickstream</b> <b>data</b> from a database perspective and consider several theories to alleviate the difficulties. Applications of modern database optimization techniques will be discussed and this paper will detail the implementation of these techniques for the Intel and GM project. by Yufei Wang. Thesis (M. Eng. and S. B.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2005. Includes bibliographical references (leaves 82 - 83) ...|$|E
40|$|<b>Clickstream</b> <b>data</b> {{analysis}} is considered {{as the process}} of collecting, analysing and reporting the aggregate data about the web pages a visitor clicks. Visualizing the <b>clickstream</b> <b>data</b> has gained significant importance in many applications like web marketing, customer prediction, product management, etc. Most existing works employ different tools for visualizing along with techniques like Markov chain modelling. However {{the accuracy of the}} methods can be improved when the shortcomings are resolved. Markov chain modelling has problems of occlusion and unable to provide clear display of data visualizing. These issues can be resolved by improving the Markov chain model by introducing a heuristic method of Kolmogorov– Smirnov distance and maximum likelihood estimator for visualizing. These concepts are employed between the underlying distribution states to minimize the Markov distribution. The proposed model named as WebClickviz is performed in Hadoop Apache Flume which is a highly advanced tool. The <b>clickstream</b> <b>data</b> visualization accuracy can be improved when Apache Flume tools are used. The performance evaluation are made on a specific website <b>clickstream</b> <b>data</b> which shows the proposed model of visualization has better performance than existing models like VizClick...|$|E
40|$|This paper {{estimates}} {{demand for}} Internet portals using a <b>clickstream</b> <b>data</b> panel of 2654 users. It shows that familiar econometric methodologies {{used to study}} grocery store scanner data {{can be applied to}} analyze advertising-supported Internet markets using <b>clickstream</b> <b>data.</b> In particular, it applies the methodology of Guadagni and Little (1983) to better understand households ’ Internet portal choices. The methodology has reasonable out of sample predictive power and can be used to simulate changes in company strategy. (JEL classification numbers: M 31, C 25) 1...|$|E
40|$|Thesis (Ph. D.) [...] University of Washington, 2012 More so {{than ever}} before, large {{datasets}} are being collected and analyzed throughout {{a variety of}} disciplines. Examples include social networking data, software logs, scientific <b>data,</b> web <b>clickstreams,</b> sensor network <b>data,</b> and more. As such, there are {{a wide range of}} users interacting with these large datasets, ranging from scientists, to data analysts, to sociologists, to market researchers. These users are experts in their domain and understand their data extensively, but are not database experts. Database systems are scalable and efficient, but are notoriously difficult to use. In this work, we aim to address this challenge, by leveraging usage history. From usage history, we can extract knowledge about the multitude of users' experiences with the database. Consequently, this knowledge allows us to build smarter systems that better cater to the users' needs. We address different aspects of the database usability problem and develop three complementary systems. First, we aim to ease the query formulation process. We build the SnipSuggest system, which is an autocompletion tool for SQL queries. It provides on-the-go, context-aware assistance in the query composition process. The second challenge we address is that of query debugging. Query debugging is a painful process in part because executing queries directly over a large database is slow while manually creating small test databases is burdensome to users. We present the second contribution of this dissertation: SIQ (Sample-based Interactive Querying). SIQ is a system for automatically selecting a `good' small sample of the underlying input database to allow queries to execute in realtime, thus supporting interactive query debugging. Third, once a user has successfully constructed the right query, they must execute it. However, executing and understanding the performance of a query on a large-scale, parallel database system can be difficult even for experts. Our third contribution, PerfXplain, is a tool for explaining the performance of a MapReduce job running on a shared-nothing cluster. Namely, it aims to answer the question of why one job was slower than another. PerfXplain analyzes the MapReduce log files from past runs to better understand the correlation between different properties of pairs of job and their relative runtimes...|$|R
40|$|Examining {{first year}} students’ {{discussion}} forum participation: Does SES matter? As universities move increasingly towards online learning environments, online teaching tools are frequently {{used to support}} diverse students in their learning. Discussion forums are a common feature of online and blended course design which enable students to interact with peers and teachers about course issues and content, and provide opportunities for giving and receiving feedback. However, existing research indicates that students {{may be reluctant to}} participate in online discussion forums for a range of reasons (Deng & Tavares, 2013). It remains unclear if students who belong to equity groups (e. g., low SES, non-English speaking background, Indigenous, regional/remote) may be more or less likely to engage with this important tool and why. This study examined how low SES students utilised this common online tool and understood their role in its use. The study involved two first-year courses from different faculties (Nursing and Business and Law) in a regional university. All nursing students studied by online, distance mode (n= 156), while the Business and Law course students studied in both distance (n= 164) and face-to-face (n= 200) modes. Approximately 1 / 3 of enrolled students in both courses were identified as low-SES. Each course was considered a unique case (Yin, 2009), with the data set including 92 surveys, 259 forum threads and over 700 responses, demographic <b>data,</b> and <b>clickstream</b> forum use <b>data.</b> Content analysis (Silverman, 2006) was used to identify the types of interactions and responses on the forum. In both cases, survey data revealed that students across all SES groups valued discussion forums, but that they sometimes found other students’ posts confusing or repetitive. Low SES students used the discussion forums to find information and seek clarification, posting higher percentages of student-led threads than other SES groups. However, they did not post responses to others’ threads with the same frequency and primarily commented, rather than providing answers. These findings demonstrate that more consideration must be given to the ways diverse students position themselves in the construction of knowledge, so that low SES students see themselves as capable of providing useful feedback to peers. Deng, L., & Tavares, N. J. (2013). From Moodle to Facebook: Exploring students’ motivation and experiences in online communities. Computers & Education, 68, 167 - 176. Silverman, D. (2006). Interpreting qualitative data (3 rd ed.). London: Sage. Yin, R. K. (2009). Case study research: Design and methods (4 th ed.). Los Angeles: Sage Publications...|$|R
40|$|<b>Clickstream</b> <b>data</b> {{collected}} at any web site (site-centric data) is inherently incomplete, since {{it does not}} capture users ' browsing behavior across sites (user-centric data). Hence, models learned from such data may be subject o limitations, the nature of {{which has not been}} well studied. Understanding the limitations is particularly important since most current personalization techniques are based on site-centric data only. In this paper, we empirically examine the implications of learning from incomplete data in the context of two specific problems: (a) predicting if the remainder of any given session will result in a purchase and (b) predicting if a given user will make a purchase at any future session. For each of these problems we present new algorithms for fast and accurate data preprocessing of <b>clickstream</b> <b>data.</b> Based on a comprehensive xperiment on user-level <b>clickstream</b> <b>data</b> gathered from 20, 000 users ' browsing behavior, we demonstrate that models built on user-centfic data outperform odels built on site-centric data for both prediction tasks...|$|E
40|$|<b>Clickstream</b> <b>data</b> {{offers an}} {{unobtrusive}} data source for understanding web users’ information behavior beyond searching. However, it remains underutilized {{due to the}} lack of structured analysis procedures. This paper provides an integrated framework for information scientists to employ in their exploitation of <b>clickstream</b> <b>data,</b> which could contribute to more comprehensive research on users’information behavior. Our proposed framework consists of two major components, i. e., data preparation and data investigation. Data preparation is the process of collecting, cleaning, parsing, and coding data, whereas data investigation includes examining data at three different granularity levels, namely, footprint, movement, and pathway. To clearly present our data analysis process with the analysis framework, we draw examples from an empirical analysis of <b>clickstream</b> <b>data</b> of OPAC users’ behavior. Overall, this integrated analysis framework is designed to be independent of any specific research settings so that it can be easily adopted by future researchers for their own clickstream datasets and research questions...|$|E
40|$|Acknowledgements: The authors thank Tulin Erdem and Russ Winer for {{organizing}} the U. C. Berkeley 5 th Annual Invitational Choice Symposium, {{during which time}} the authors discussed the ideas presented in this paper. CHOICE AND THE INTERNET: FROM CLICKSTREAM TO RESEARCH STREAM The authors discuss research progress and future opportunities for modeling consumer choice on the Internet using <b>clickstream</b> <b>data</b> (the electronic records of Internet usage recorded by company web servers and syndicated data services). The authors compare the nature of Internet choice (as captured by <b>clickstream</b> <b>data)</b> with supermarket choice (as captured by UPC scanner panel data), highlighting the differences relevant to choice modelers. Though the application of choice models to <b>clickstream</b> <b>data</b> is relatively new, the authors review existing early work and provide a two-by-two categorization of the applications studied to date (delineating search versus purchase {{on the one hand}} and within-site versus across-site choices on the other). The paper offers directions for further research in these areas and discusses additional opportunities afforded by clickstream information, including personalization, data mining, automation, and customer valuation. Notwithstanding the numerous challenges associated with <b>clickstream</b> <b>data</b> research, the authors conclude that the detailed nature of the information tracked about Internet usage and e-commerce transactions presents an enormous opportunity for empirical modelers to enhance the understanding and prediction of choice behavior...|$|E
30|$|The {{available}} usage mining {{techniques are}} based on <b>clickstream</b> <b>data.</b> A clickstream {{is a series of}} page requests triggered by user clicks on webpages, and hence, it represents a record of a user’s activity on the Internet. It could include every website and every page that the user visits, how long the user was on a page or site, along with other possible information. Clickstreams can be collected from server log files, but there, human and machine traffic are not usually differentiated. They can also be collected from browsers through Javascript code; which use tracking cookies to generate information sent from browsers to servers. In both approaches, the traffic information is centered around a particular website or a user. Therefore, such <b>clickstream</b> <b>data</b> is limited, and does not totally satisfy the e-commerce requirements since the way such data are available restrict their benefit in applications beyond simple web analysis [5]. The main reason behind the non-utility of the <b>clickstream</b> <b>data</b> remains is the incompleteness of the data, and the huge size of the logged data.|$|E
40|$|This paper uses <b>clickstream</b> <b>data</b> from Plurimus Corp. (formerly Foveon Corp.) {{to analyze}} user choice of Internet portals. It {{will show that}} {{commonly}} used econometric models for examining grocery scanner data {{can be applied to}} <b>clickstream</b> <b>data</b> on advertising-based online markets. Developing a framework to study consumer choices of free websites is an essential step to better understanding user behavior on the Internet. The main data for this study is a <b>clickstream</b> <b>data</b> set consisting of every website visited by 2654 users from December 27 1999 to March 31 2000. Using this data, I construct several variables including search success, time spent searching, and whether a website is an individual's starting page. I also have advertising and media mentions data. This study helps to increase understanding of user behavior on the Internet. It explores some key determinants of website choice and simulates market responses to changes in the online environment. By better understanding website choice, we can better evaluate the implications of policy decisions. Comment: TPRC conference, 26 pages, 9 tables, 8 figure...|$|E
40|$|Translating the <b>clickstream</b> <b>data</b> in Web server {{log files}} into useable {{information}} about {{the characteristics of the}} consumers who visit Web sites has been a major challenge. This book describes two novel contributions to addressing this challenge. The first is the application of top-down grammatical analysis to <b>clickstream</b> <b>data</b> to infer consumer characteristics. Like top-down approaches to the grammatical analysis of language, this approach specifies sequential patterns to look for in the data, as the patterns marketers are interested in (e. g., purchasing) may be rare, and therefore difficult to detect using bottom-up, data-driven methods. The second contribution is the introduction of a new construct, the Web ad schema, which is a visitor?s mental representation of the structure of a Web site. If this representation is incongruent with the actual structure of a site, a consumer will generate a misleading clickstream. This book should be especially useful to Web site marketers interested in understanding who their customers are and optimizing the information they deliver to their customers, and to researchers developing new applications to analyze <b>clickstream</b> <b>data...</b>|$|E
40|$|This paper {{addresses}} {{the collection of}} 2 ̆ 2 <b>clickstream</b> <b>data,</b> 2 ̆ 2 and sets forth a theory about the legal rules that should govern it. At the outset, I propose a typology for categorizing privacy invasions. A given state of informational privacy may be represented by: the observed behavior, the collecting agent, and the searching agent. Using this typology, I identify the specific sources of concern about collection of <b>clickstream</b> <b>data.</b> Then, based on expected levels of utility and expected transaction costs of 2 ̆ 2 flipping 2 ̆ 2 to a different rule, I argue for {{a particular set of}} privacy defaults for data mining...|$|E
