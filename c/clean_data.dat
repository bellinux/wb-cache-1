323|645|Public
25|$|However, with <b>clean</b> <b>data</b> or in {{theoretical}} settings, {{they can}} sometimes prove very good estimators, particularly for platykurtic distributions, where for small data sets the mid-range {{is the most}} efficient estimator.|$|E
5000|$|Multiple SSDs in a cache set only dirty data (for the {{write-back}} policy) and metadata {{would be}} mirrored, without wasting SSD {{space for the}} <b>clean</b> <b>data</b> and read caches ...|$|E
50|$|However, with <b>clean</b> <b>data</b> or in {{theoretical}} settings, {{they can}} sometimes prove very good estimators, particularly for platykurtic distributions, where for small data sets the mid-range {{is the most}} efficient estimator.|$|E
30|$|Consistent with Delorme et al. [25], we {{find that}} {{extended}} Infomax performs best, {{both in terms of}} the dipolarity score proposed in Delorme et al. [25] and in terms of our ERD peak score. However, the ERD peak score, which heuristically quantifies an oscillatory phenomenon in the <b>cleaned</b> <b>data,</b> is not as sensitive as the dipolarity measure, which is computed on the source signals. This is to be expected, since the source signals are an intermediate step within the artifact reduction, and differences on this intermediate level may not necessarily translate into strong differences in the <b>cleaned</b> <b>data.</b> Indeed, observed ERD differences between the methods are rather small, which suggests that the choice of the decomposition method may often not result in strong differences in data quality.|$|R
30|$|While certain {{sources are}} {{expected}} to provide real-time streams of data (I 6), the scheduled transfer of data in bulk from back-end servers, repositories, and applications may also occur in a manufacturing setting (I 7). In fact, the <b>cleaned</b> <b>data</b> may {{also need to be}} transferred back to some of these same databases for transactional operations.|$|R
5000|$|... #Caption: Figure 4 Here, {{shown are}} {{the stages of}} the software: first, take a noisy {{trajectory}} and clean it; then, find the RDF from the <b>cleaned</b> <b>data.</b> Finally, suggest a set of kinetic schemes that may be related with the found RDF. The software should also present the possibility of numerical simulations of noisy time trajectories.|$|R
50|$|If few beads {{are shown}} in the frame, because every bead moving randomly, {{averaging}} over the position of them for every frame should give us the drift (it should subtracted from the data for having <b>clean</b> <b>data).</b>|$|E
50|$|Just as earlier data {{transmission}} systems {{suffered from the}} lack of an 8-bit <b>clean</b> <b>data</b> path, modern transmission systems often lack support for 16-bit or 32-bit data paths for character data. This has led to character encoding systems such as UTF-8 that can use multiple bytes to encode a value that is too large for a single 8-bit symbol.|$|E
5000|$|These robust {{statistics}} are particularly used as estimators of a scale parameter, {{and have the}} advantages of both robustness and superior efficiency on contaminated data, {{at the cost of}} inferior efficiency on <b>clean</b> <b>data</b> from distributions such as the normal distribution. To illustrate robustness, the standard deviation can be made arbitrarily large by increasing exactly one observation (it has a breakdown point of 0, as it can be contaminated by a single point), a defect that is not shared by robust statistics.|$|E
30|$|Finally, 1 of the 38 {{pipelines}} (2.6 %) {{does not}} use a tool for this stage. This pipeline is described in [70]. This paper focuses on establishing a pipeline for ingestion, communication, and storage. Plans exist to hand off the stored and <b>cleaned</b> <b>data</b> to an analysis component in the future. However, the analysis setup is not specified as of yet.|$|R
40|$|Compress and <b>clean</b> raw <b>data</b> {{file for}} {{permanent}} storage We have identified various error conditions/types and developed algorithms {{to get rid}} of these errors/noises, including the more complicated noise in the newer data sets. (status = 100 % complete). Internet access of compacted raw data. It is now possible to access the raw data via our web site, [URL] The software to read and plot the compacted raw data is also available from the same web site. The users can now download the raw data, read, plot, or manipulate the data as they wish on their own computer. The users are able to access the <b>cleaned</b> <b>data</b> sets. Internet access of the color spectrograms. This task has also been completed. It is now possible to access the spectrograms from the web site mentioned above. Improve the particle precipitation region classification. The algorithm for doing this task has been developed and implemented. As a result, the accuracies improved. Now the web site routinely distributes the results of applying the new algorithm to the <b>cleaned</b> <b>data</b> set. Mark the classification region on the spectrograms. The software to mark the classification region in the spectrograms has been completed. This is also available from our web site...|$|R
40|$|Quality of data is {{critical}} for making data driven business decisions. Enhancing the quality of data enables companies to make better decisions and prevent business losses. Systems similar to Extract Transform and Load (ETL) are often used to clean and {{improve the quality of}} data. Currently, businesses tend to collect a massive amount of customer data, store it in the cloud, and analyze the data to gain statistical inferences about their products, services, and customers. Cheaper storage, constantly improving approaches to data privacy and security provided by cloud vendors, such as Microsoft Azure, Amazon Web Service, seem to be the key driving forces behind this process. This thesis implements Azure Data Factory based ETL system that serves the purpose of data quality management in the Microsoft Azure Cloud platform. In addition to Azure Data Factory, there are four other key components in the system: (1) Azure Storage for storing raw, and semi cleaned data; (2) HDInsight for processing raw and semi <b>cleaned</b> <b>data</b> using Hadoop clusters and Hive queries; (3) Azure ML Studio for processing raw and semi <b>cleaned</b> <b>data</b> using R scripts and other machine learning algorithms; (4) Azure SQL database for storing the <b>cleaned</b> <b>data.</b> This thesis shows that using Azure Data factory as the core component offers many benefits because it helps in scheduling jobs, and monitoring the whole data transformation processes. Thus, it makes data intake process more timely, guarantees data reliability, simplifies data auditing. The developed system was tested and validated using sample raw data...|$|R
50|$|While {{character}} strings {{are very}} common uses of strings, a string {{in computer science}} may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, {{may be used to}} represent non-textual binary data retrieved from a communications medium. This data {{may or may not be}} represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit <b>clean,</b> <b>data</b> corruption may ensue.|$|E
50|$|Unlike the Internet {{protocols}} {{used for}} the exchange of email, the format {{used for the}} storage of email has never been formally defined through the RFC standardization mechanism and has been entirely left to the developer of an email client.However, the POSIX standard defined a loose frame {{in conjunction with the}} mailx program.In 2005 finally, the application/mbox media type was standardized as RFC 4155, and hints that mbox stores mailbox messages in their original Internet Message (RFC 2822) format, except for the used newline character, seven-bit <b>clean</b> <b>data</b> storage, and the requirement that each newly added message is terminated with a completely empty line within the mbox database.|$|E
50|$|While {{efficiency}} is a desirable quality of an estimator, {{it must be}} weighed against other considerations, and an estimator that is efficient for certain distributions may well be inefficient for other distributions. Most significantly, estimators that are efficient for <b>clean</b> <b>data</b> from a simple distribution, such as the normal distribution (which is symmetric, unimodal, and has thin tails) may not be robust to contamination by outliers, and may be inefficient for more complicated distributions. In robust statistics, more importance is placed on robustness and applicability {{to a wide variety}} of distributions, rather than efficiency on a single distribution. M-estimators are a general class of solutions motivated by these concerns, yielding both robustness and high relative efficiency, though possibly lower efficiency than traditional estimators for some cases. These are potentially very computationally complicated, however.|$|E
30|$|In this paper, our {{aim is to}} {{evaluate}} {{the quality of the}} <b>cleaned</b> <b>data</b> (in a sense the end product of the artifact reduction), which goes beyond the quality of the source signals (an intermediate product of the artifact reduction). A validation of the complete artifact reduction pipeline for muscle artifacts in real EEG data was carried out by McMenamin et al. [2]. However, different ICA algorithms were not compared.|$|R
40|$|This report {{presents}} an algorithm for removing artifacts from EEG signal, {{which is based}} on the method of independent component analysis utilizing the signal nonstationarity or sparsity of the artifacts. The algorithm is computationally very fast, enables online processing of long data records with excellent separation accuracy. The algorithm also incorporates using wavelet denoising of the artifact components, recently proposed by Castellanos and Makarov, which reduces distortion of the <b>cleaned</b> <b>data...</b>|$|R
40|$|A {{new method}} to extract {{information}} from color-magnitude diagrams (CMDs) is presented. We employ a Richardson-Lucy (RL) algorithm {{to the analysis}} of the observational uncertainties. The resulting reconstruction is a <b>cleaned</b> <b>data</b> set with which to perform analyses of the star formation history. For this task, synthetic stellar populations are built and compared with the data CMD. As an application, the star formation rate in the solar vicinity is recovered. © 2007 International Astronomical Union...|$|R
50|$|The actual {{process of}} data {{cleansing}} may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address {{that does not}} have a valid postal code) or fuzzy (such as correcting records that partially match existing, known records). Some data cleansing solutions will <b>clean</b> <b>data</b> by cross checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve activities like, harmonization of data, and standardization of data. For example, harmonization of short codes (st, rd, etc.) to actual words (street, road, etcetera). Standardization of data is a means of changing a reference data set to a new standard, ex, use of standard codes.|$|E
5000|$|Step 1 Identification: Searching and {{identifying}} the right {{source of information}} for analytical purposes. Step 2 Extraction: Once a reliable and mineable source of social media data are identified, next comes extraction of the data through APIs or manually. Step 3 Cleaning: This step involves removing the unwanted data from the automatically extracted data Step 4 Analyzing: Next, the <b>clean</b> <b>data</b> is analyzed for business insights. Depending on the layer of social media analytics under consideration and the tools and algorithm employed, the steps and approach to take will greatly vary. Step 5 Visualization: Depending {{on the type of}} data, the analysis part will lead to relevant visualizations for effective communication of results. Step 6 Interpretation or Consumption: This step relies on human judgments to interpret valuable knowledge from the visual data. Meaningful interpretation is of particular importance when we are dealing with descriptive analytics that leaves room for different interpretations ...|$|E
50|$|Clean data: Omniscien Technologies {{focuses on}} <b>clean</b> <b>data</b> in {{contrast}} to the traditional approach that leverages content found on the web in corporate sites, news articles and other similar sources where the same content is available in multiple languages, but does not guarantee high quality data. To ensure that data is as clean and as accurate as possible, Omniscien Technologies has put effort into machine and human resources in this area. The company's data is sourced from high-quality translations provided by book publishers and translation companies, and is aligned at the segment level (usually sentences) and converted into a consistent format in order to be processed by the learning software. This step includes extracting segments from files and documents if they are not in a TMX format. Then the extracted sequence are aligned—and processed by machines, with humans used to validate the accuracy. The data is converted to a base UTF-8 encoding for training the SMT system, small subsets are extracted to guide training, and finally the data is reviewed, cleaned, and analyzed.|$|E
5000|$|Integrate - Integrations {{are a key}} {{requirement}} of a TIP. Data from the platform needs {{to find a way}} back into the security tools and products used by an organization. Full-featured TIPs enable the flow of information collected and analyzed from feeds, etc. and disseminate and integrate the <b>cleaned</b> <b>data</b> to other network tools including SIEMs, internal ticketing systems, firewalls, intrusion detection systems, and more. Furthermore, APIs allow for the automation of actions without direct user involvement.|$|R
40|$|Dynamic {{conductance}} spectra {{are taken}} from Au/CeCoIn_ 5 point contacts in the Sharvin limit along the (001) and (110) directions. Our conductance spectra, reproducibly obtained over wide ranges of temperature, constitute the <b>cleanest</b> <b>data</b> sets ever reported for HFSs. A signature for the emerging heavy-fermion liquid {{is evidenced by the}} development of the asymmetry in the background in the normal state. Below T_c, an enhancement of the sub-gap conductance arising from Andreev reflection is observed, with the magnitude of ∼ 13. 3...|$|R
40|$|It is a popularly held {{belief that}} {{preprocessing}} of data generally improves the classification efficiency of data mining algorithms. We study {{the effects of}} preprocess by utilizing an algorithm to cluster points in a data set based upon each attribute independently, resulting in additional information about the data points with respect to each of its dimensions. Noise, data boundaries are identified and the <b>cleaned</b> <b>data</b> subset is used to study the performance of CLONALG data mining algorithm against unprocessed dataset...|$|R
30|$|Raw data (raw reads) of fastq format were firstly {{processed}} through in-house perl scripts. In this step, <b>clean</b> <b>data</b> (clean reads) {{were obtained}} by removing reads containing adapter, reads containing ploy-N and low quality reads from raw data. At the same time, Q 20, Q 30, GC-content and sequence duplication level of the <b>clean</b> <b>data</b> were calculated. All the downstream analyses were based on <b>clean</b> <b>data</b> with high quality.|$|E
30|$|The <b>clean</b> <b>data</b> {{with high}} quality {{in this study}} have been {{deposited}} into the NCBI Sequence Read Archive under the accession number of SRP 127997.|$|E
3000|$|... cIn {{addition}} to the MI analysis, we performed an empirical test regarding use of 0.25 CPO filter. An experiment was run on <b>clean</b> <b>data</b> with Ω [...]...|$|E
30|$|Observatory hourly mean {{data were}} taken from 148 {{observatories}} between 2009.0 and 2014.7 held at the World Data Centre for Geomagnetism. Known jumps, identified by the operators of the observatories, were corrected for and some poor-quality data were eliminated manually. These <b>cleaned</b> <b>data</b> (without any further selection) are regularly updated and made available by BGS to ESA as the Swarm ‘AUX_OBS’ product (Macmillan and Olsen 2013) {{for use by the}} wider community. The data were then selected according to the criteria in Table 1.|$|R
40|$|We {{report the}} results of a pilot study {{designed}} to investigate the feasibility of collecting information about user actions over the Web. By logging simple events (queries, document views, redisplay of query results) and noting their relative timing, we hoped to be able to predict relevance of viewed documents. Although design problems cast doubt on the accuracy of our results, analysis of the <b>cleanest</b> <b>data</b> reveals that clickthroughs are not very predictive of relevance, but that viewing times, when normalized by document length, are somewhat predictive...|$|R
40|$|This paper {{presents}} a modified K-means algorithm {{that can be}} used for removing noise in multicolor motion capture image sequences. These images have been produced using the Illuminated Line Segment based Marker System described in [1]. The proposed algorithm takes into account the nature of the motion capture images in terms of the number of data pixels normally clustered together and the acceptable degree of compactness of a <b>data</b> cluster. The <b>cleaned</b> <b>data</b> can be used for accurate and effective tracking of the captured motion...|$|R
30|$|Postprocess Blacklisting, {{where we}} use the <b>clean</b> <b>data</b> to {{identify}} which entities we should drop due to ambiguities {{that could not be}} ruled out in the first phase.|$|E
30|$|Raw data (raw reads) of fastq format were firstly {{processed}} through in-house perl scripts. In this step, the <b>clean</b> <b>data</b> (clean reads) {{were obtained}} by removing reads containing adapter, reads containing poly-N and low quality reads from raw data. At the same time, quality parameters of <b>clean</b> <b>data</b> including Q 20, Q 30, GC-content and sequence duplication level were used for data filtering. All the succeeding analyses were carried out using high quality <b>clean</b> <b>data.</b> Reference genome and gene model annotation files were downloaded from The MSU Rice Genome Annotation Project Database website at [URL] An index of the reference genome was built using Bowtie 2 v 2.2. 5 (Langmead et al. 2009) and paired-end clean reads were aligned to the reference genome using TopHat v 2.0. 14 (Trapnell et al. 2010). TopHat {{was chosen as the}} mapping tool, because it can generate a database of splice junctions based on the gene model annotation file, and thus give a better mapping result than other non-splice mapping tools.|$|E
30|$|Lastly, since FDW {{provides}} great improvements on <b>clean</b> <b>data,</b> the approximation performed {{seems not}} to be robust to noise. With the VODIS corpus, the weight reestimate is always better with MLE or FD than with FDW.|$|E
40|$|The Sudbury Neutrino Observatory (SNO) {{has been}} {{collecting}} data since November 1999. The study {{of whether or}} not the D 2 O circulation affects the data is an important part of understanding how the SNO detector behaves. This report looks at several characteristics of the data to determine to what extent the D 2 O circulation affects the data. We found that there is no evidence for any dependence of event rates in the <b>cleaned</b> <b>data</b> sets on the state of D 2 O circulation...|$|R
40|$|This project {{collects}} {{quantitative data}} from 39 pilot liaison and diversion {{services that are}} designed to identify the needs of individuals to prevent them entering the criminal justice system. Services identify, intervene and signpost clients. Each site completes an excel spreadsheet every two weeks. This is sent to Northumbria and Manchester Universities to collate, quality check and analyse. <b>Cleaned</b> <b>data</b> is sent to the Department of Health approximately every month. The data is designed to inform the Department Health's developing business case for liaison and diversion services...|$|R
40|$|The data volumes {{produced}} by new generation multibeam systems are very large, especially for shallow water systems. Results from recent multibeam surveys {{indicate that the}} ratio of the field survey time, to the time used in interactive editing through graphical editing tools, is about 1 : 1. An important reason for the large amount of processing time is that users subjectively decide which soundings are outliers. There is an apparent need for an automated approach for detecting outliers that would reduce the extensive labor and obtain consistent results from the multibeam data cleaning process, independent of the individual that has processed the data. The proposed automated algorithm for cleaning multibeam soundings was tested using the SAX- 99 (Destin FL) multibeam survey data [2]. Eight days of survey data (6. 9 Gigabyte) were cleaned in 2. 5 hours on an SGI platform. A comparison of the automatically <b>cleaned</b> <b>data</b> with the subjective, interactively <b>cleaned</b> <b>data</b> indicates that the proposed method is, if not better, at least equivalent to interactive editing as used on the SAX- 99 multibeam data. Furthermore, the ratio of acquisition to processing time is considerably improved since the time required for cleaning the data was decreased from 192 hours to 2. 5 hours (an improvement by a factor of 77) ...|$|R
