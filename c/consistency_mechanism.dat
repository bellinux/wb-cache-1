40|143|Public
5000|$|Provides a time-machine {{inspired}} rollback capability using selectively granular consistency points. The <b>consistency</b> <b>mechanism</b> {{can cover}} many LUNs at a time. Rollback requires a FlashCopy license and the Spectrum Control Snapshot software.|$|E
3000|$|Mutual {{membership}} checks provide (i) {{a decrease}} in the probability that an inconsistent aggregation session occurs (Check 1) and (ii) detection of false positives (Check 2). This section introduces a <b>consistency</b> <b>mechanism</b> of aggregation sessions for accurate aggregates. This mechanism is based on two nested mutual membership checks between the bloom filters of an aggregator A [...]...|$|E
40|$|In this work, a Multilevel Pre-fetching (MLP) {{algorithm}} is proposed in conjuction with the weak cache <b>consistency</b> <b>mechanism</b> to enhance the frequency of updating cached objects. It is an extension to client polling mechanism and responsible for validating the cached objects without waiting for the requests from the clients. It is executed whenever the cache-server channel is under utilized...|$|E
40|$|Currently, the {{internet}} supports weak cache <b>consistency</b> <b>mechanisms.</b> We {{argue that in}} order to support growing diversity in application requirements, the weak <b>consistency</b> <b>mechanisms</b> must be augmented by strong <b>consistency</b> <b>mechanisms.</b> Existing strong <b>consistency</b> <b>mechanisms</b> are not appealing for the web environments due to large state space or control message overhead. We present the leases approach that balances these tradeoffs and present the policies for determining the optimal lease duration. We present extensions to the http protocol to incorporate the leases and then implement our technique in the Squid Web proxy and Apache Web server. Our experimentation reveals that the overhead of leases implementation is comparable to existing weak <b>consistency</b> <b>mechanisms.</b> v Contents...|$|R
40|$|Abstract — In this paper, {{we argue}} that weak cache <b>consistency</b> <b>mechanisms</b> {{supported}} by existing web proxy caches must be augmented by strong <b>consistency</b> <b>mechanisms</b> to support the growing diversity in application requirements. Existing strong <b>consistency</b> <b>mechanisms</b> are not appealing for web environments due to their large state space or control message overhead. We focus on the lease approach that balances these tradeoffs and present analytical models and policies for determining the optimal lease duration. We present extensions to th...|$|R
40|$|In this paper, {{we argue}} that weak cache <b>consistency</b> <b>mechanisms</b> {{supported}} by existing web proxy caches must be augmented by strong <b>consistency</b> <b>mechanisms</b> to support the growing diversity in application requirements. Existing strong <b>consistency</b> <b>mechanisms</b> are not appealing for web environments due to their large state space or control message overhead. We focus on the lease approach that balances these tradeoffs and present analytical models and policies for determining the optimal lease duration. We present extensions to the http protocol to incorporate leases and then implement our techniques in the Squid proxy cache and the Apache web server. Our experimental evaluation of the leases approach shows that: (i) our techniques impose modest overheads even for long leases (a lease duration of 1 hour requires state to be maintained for 1030 leases and imposes an per-object overhead of a control message every 33 minutes); (ii) leases yields a 138 - 425 % improvement over existing strong consist [...] ...|$|R
40|$|Wireless sensor {{networks}} (WSNs) {{have been}} organized to determine or detect physical and environmental events or any significant parameters {{of a given}} situation. Applications usually send queries to sensor nodes to get back the values periodically from the measurements or detections. In Wireless Sensor Networks (WSNs), Message Queuing Telemetry Transport (MQTT-S) and Constrained Application Protocol (CoAP) are two protocols which are capable to use the publish/subscribe model. The elevated scalability provided by the publish/subscribe model may acquire a high packet loss and therefore requires a proficient <b>consistency</b> <b>mechanism</b> to handle with this situation. The <b>consistency</b> <b>mechanism</b> of MQTT-S and CoAP employs a method which defines a permanent value for the retransmission timeout (RTO). This article argues that this method is not resourceful for deploying publish/subscribe in WSN, because it may be not capable to improve a packet, therefore resulting in a lower packet delivery ratio (PDR) at the subscriber nodes. An adaptive RTO method is proposed and which consists in using a Smooth Round-trip Time and multiplying it by a constant parameter (K). The <b>consistency</b> <b>mechanism</b> of MQTT-S and CoAP {{would be able to}} respond properly to packet loss and would also be trivial for sensor nodes where these resources are critical. A comprehensive evaluation {{of the effects of the}} K value on the calculation of the adaptive RTO method is proposed here. We also set up the setting for obtaining the uppermost PDR on the subscriber nodes for single-hop and multi-hop scenarios. </p...|$|E
40|$|Abstract: Caching {{mechanism}} {{was introduced}} to achieve performance, scalability, and availability in the continuously growing Internet applications, such as the WWW. However, replication introduces the overhead of keeping the caches consistent. The degree to which these caches are kept consistent is called a consistency model. There {{are two types of}} cache consistency mechanisms, Strong cache consistency and Weak cache consistency. The weak cache <b>consistency</b> <b>mechanism</b> requires fewer control messages than the strong cache <b>consistency</b> <b>mechanism.</b> This paper presents a survey of contemporary weak cache consistency mechanisms in use on the Internet today. A weak cache consistency protocol reduces network bandwidth consumption and server load more than either time-to-live fields or an invalidation protocol or proxy cache and can be tuned to return stale data less than 5 % of the time. In this paper, we examine the different approaches to weak cache consistency. An ideal cache consistency solution will provide a reduction in network bandwidth and server load at very low cost...|$|E
40|$|We {{describe}} an efficient software cache <b>consistency</b> <b>mechanism</b> for shared memory multiprocessors that supports multiple writers and works for cache lines of any size. Our mechanism {{relies on the}} fact that � for a correct program � only the global memory needs a consistent view of the shared data between synchronization points. Our delayed <b>consistency</b> <b>mechanism</b> allows arbitrary use of data blocks between synchronizations. In contrast to other mechanisms � our mechanism needs no modification to the processor hardware or any assistance from the programmer or compiler � the processors can use normal cache management policies. Since no special action is needed to use the shared data � the processors are free to act almost as if they are all running out of a single cache. The global memory units are nearly identical to those on currently available machines. We need to add only a small amount of hardware and/or software to implement our mechanism. The mechanism can even be implemented using network connected workstations...|$|E
5000|$|Since April 2015, {{there is}} ongoing work on porting kGraft {{to the common}} live {{patching}} core provided by the Linux kernel mainline. [...] However, implementation of the required function-level <b>consistency</b> <b>mechanisms</b> has been delayed because the call stacks provided by the Linux kernel may be unreliable in situations that involve assembly code without proper stack frames; as a result, the porting work remains in progress [...] In an attempt to improve the reliability of kernel's call stacks, a specialized sanity-check [...] userspace utility has also been developed.|$|R
3000|$|The above basic {{working of}} a DHT {{provides}} the {{foundation on which}} richer storage services can be built, with improved performance and <b>consistency</b> <b>mechanisms</b> tailored to {{the specific needs of}} individual systems. One first important evolution away from DHTs was the introduction of one-hop routing [11], to meet the stringent latency requirements of deployed systems. This is achieved in systems such as Cassandra [12] or Dynamo [11] (and its Erlang counterpart Riak 1), by replicating the full routing information on each node as a speed-up mechanism over the typical O(l [...]...|$|R
40|$|Developments in {{wireless}} networking {{will soon}} enable high bandwidth, low-latency, wireless wide area networks. In addition the rollout of mobile IPv 6 across these wireless technologies will greatly challenge traditionally held ideas regarding support for mobile applications. This paper describes {{the development of}} a novel application which has led to the proposal of a new approach to support interactive mobile applications based on peer to peer networks. This approach offers support for real-time interaction between heterogeneous mobile devices and considers a dynamic peer-to-peer topology with shared state and <b>consistency</b> <b>mechanisms...</b>|$|R
40|$|We {{describe}} hardware that improves on {{the performance}} of Data Merging, an efficient software cache <b>consistency</b> <b>mechanism</b> for shared memory multiprocessors. The hardware support consists of an extra bit attached to each datum in a cache line. These bits are used to control the merge operation rather than a bit mask held in the global memory. The bits {{can also be used to}} reduce the amountofnetwork traffic by sending only modified words to memory...|$|E
40|$|International audienceMost of the {{multiplayer}} games {{available online}} {{are based on}} a client-server architecture because this architecture gives better administration control to the game providers. Besides controlling the account and payment information of the players, this architecture also prevents players from cheating as all the game logic is executing on the centralized server. We proposed a server assisted approach for mobile games in [2]. However, because of the varying and high latency of wireless networks and of the changing consistency requirements during the game play, it is difficult to keep the user experience highly interactive in client-server architecture. This paper presents an adaptive hybrid client-server architecture which changes its behavior according to network and game environment variations to improve game state consistency across different mobile terminals. The server applies <b>consistency</b> <b>mechanism</b> on its side, as in the traditional client-server architecture and dynamically switches to apply a client side <b>consistency</b> <b>mechanism</b> when inconsistencies occur at the client side because of the change in network conditions and/or game requirements. We have evaluated our approach on a car racing game. The results show that we can obtain an improved global consistency under a high and varying latency network using our dynamically adaptable approach...|$|E
40|$|Abstract—Currently {{the authors}} are {{studying}} on CSCW in distributed 3 D virtual spaces. A problem is {{how to keep the}} same state among several computers for consistency. A log-ical clock idea seems available to solve this problem because it can perform user operation events in order according to their happen times. The authors introduced the logical clock idea into IntelligentBox, which is a constructive software de-velopment system for interactive 3 D graphics applications. Furthermore, the authors evaluated its performances. This paper describes the <b>consistency</b> <b>mechanism</b> and its appli-cation examples, and also presents its performance evalua-tions...|$|E
40|$|In this paper, {{we examine}} {{a number of}} SQL and {{so-called}} “NoSQL ” data stores designed to scale simple OLTP-style application loads over many servers. Originally motivated by Web 2. 0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses. We contrast the new systems on their data model, <b>consistency</b> <b>mechanisms,</b> storage mechanisms, durability guarantees, availability, query support, and other dimensions. These systems typically sacrifice some of these dimensions, e. g. database-wide transaction consistency, {{in order to achieve}} others, e. g. higher availability and scalability...|$|R
30|$|In cloud {{computing}} environments guarantees, <b>consistency</b> <b>mechanisms,</b> (shared) state and transactions are frequently traded for robustness, scalability and performance. Based on this challenge we present CMQ, a UDP-based inherently asynchronous message queue to orchestrate messages, events and {{processes in the}} cloud. CMQ’s inherently asynchronous design is shown to perform especially well in modern Layer 2 switches in data center networks, {{as well as in}} the presence of errors. CMQ’s lightweight edge-to-edge design, which is somewhat similar to Unix Pipes, makes it very composable. By presenting our work, we hope to initiate discussion on how to implement lightweight messaging paradigms that are aligned with the overall architectures and goals of {{cloud computing}}.|$|R
40|$|Caching is a {{standard}} {{solution to the problem}} of insufficient bandwidth caused by the rapid increase of information circulation across the Internet. Cache <b>consistency</b> <b>mechanisms</b> areacrucialcomponent of each cache scheme influencing the cache usefulness and reliability. This paper presents a model for optimizing Internet cache content by the use of a genetic algorithm and examines the model by trace-driven experiments. Cached data are considered as a population evolving over simulated time by a number of successive cache "generations". The model is testedbythe use of traces providedbyaSquidproxy cache server. Using trace-driven caching, we show that the proposed evolutionary mechanisms improve cache nonstaleness and consistency and result in an updated cache content...|$|R
40|$|This paper investigates some {{fundamental}} properties and performance {{issues of the}} expiration-based caching systems. We focus on the hierarchical caching systems based on the time-to-live (TTL) expiration mechanism, and present a basic model for such systems. By analyzing the intrinsic timing behavior in this model, we derive some important performance metrics {{from the perspectives of}} caching systems and end users, respectively. We use network simulation results to further substantiate the efficacy of our analysis. Our results show some basic properties and trade-offs for a hierarchical caching system based on the weak <b>consistency</b> <b>mechanism...</b>|$|E
40|$|Two {{component}} architectures for MIN-connected multiprocessors: the Piled Banyan Switching Fabrics (PBSF) and MINC (MIN with Cache <b>consistency</b> <b>mechanism)</b> {{are evaluated}} {{with a real}} machine SNAIL- 2 and an instruction level simulator. The PBSF is a high bandwidth MIN with three dimensional structure, and the MINC is a mechanism for controlling the consistency of private cache modules provided between processors and the MIN. Empirical implementation and simulation {{results show that the}} performance improvement of cache controlled by the MINC is significant, and throughput of the PBSF is sufficient if the cache is provided...|$|E
30|$|NoSQL {{systems have}} some {{limitations}} {{when they are}} used in the mission critical applications which require strong data consistency. For example, asynchronous replication and the eventual <b>consistency</b> <b>mechanism</b> provided by NoSQL are not applicable for the bank systems. If the delay of inconsistency window is too long and the primary crashes in this delay period, then the last update information may be lost because the committed update transactions have not been synchronized to the backups. In this procedure, it is possible that a customer performs a withdraw operation, but the final balance of the account is not reduced accordingly.|$|E
40|$|In cloud {{computing}} environments guarantees, <b>consistency</b> <b>mechanisms,</b> (shared) state and transactions are frequently traded for robustness, scalability and performance. Based on this challenge we present CMQ, a UDP-based inherently asynchronous message queue to orchestrate messages, events and {{processes in the}} cloud. CMQ’s inherently asynchronous design is shown to perform especially well in modern Layer 2 switches in data center networks, {{as well as in}} the presence of errors. CMQ’s lightweight edge-to-edge design, which is somewhat similar to Unix Pipes, makes it very composable. By presenting our work, we hope to initiate discussion on how to implement lightweight messaging paradigms that are aligned with the overall architectures and goals of {{cloud computing}}...|$|R
40|$|Hierarchical Cache Consistency (HCC) is a {{scalable}} cache-consistency architecture for chip multiprocessors {{in which}} caches are shared hierarchically. HCC’s cache-consistency protocol {{is embedded in}} the message-routing network that interconnects the caches, providing a distributed and scalable alternative to bus-based and directory-based <b>consistency</b> <b>mechanisms.</b> The HCC <b>consistency</b> protocol is “progressive ” in that every message makes monotonic progress without timeouts, retries, negative acknowledgments, or retreating in any way. The latency is at most proportional to the diameter of the network. For HCC with a binary fat-tree network, the protocol requires at most 13 bits of additional state per cache line, no matter how large the system. We prove that the HCC protocol is deadlock free and provides sequential consistency...|$|R
40|$|The {{bandwidth}} {{demands of}} the World Wide Web continue to grow at a hyper-exponential rate. Given this rocketing growth, caching of web objects {{as a means to}} reduce network bandwidth consumption {{is likely to be a}} necessity in the very near future. Unfortunately, many Web caches do not satisfactorily maintain cache consistency. This paper presents a survey of contemporary cache <b>consistency</b> <b>mechanisms</b> in use on the Internet today and examines recent research in Web cache consistency. Using trace-driven simulation, we show that a weak cache consistency protocol (the one used in the Alex ftp cache) reduces network bandwidth consumption and server load more than either time-to-live fields or an invalidation protocol and ca...|$|R
40|$|File caching is {{essential}} to good performance in a distributed system, especially as processor speeds and memory sizes continue to improve rapidly while disk latencies do not. Stateless-server systems, such as NFS, cannot properly manage client file caches. Stateful systems, such as Sprite, can use explicit cache consistency protocols to improve both cache consistency and overall performance. By modifying NFS to use the Sprite cache consistency protocols, we isolate {{the effects of the}} <b>consistency</b> <b>mechanism</b> from the other features of Sprite. We find dramatic improvements on some, although not all, benchmarks, suggesting that an explicit cache consistency protocol is necessary for both correctness and good performance. 1...|$|E
40|$|This paper {{describes}} the agentTool Process Editor (APE), an Eclipse plug-in {{based on the}} Eclipse Process Framework. The aim of APE is to facilitate the design, verification, and management of custom agent-oriented software development processes. APE provides five basic structures. The Library is a repository of agent-oriented method fragments. A Process Editor allows the management of tailored processes. Task Constraints help process engineers specify guidelines to constrain how tasks can be assembled, while a Process <b>Consistency</b> <b>mechanism</b> verifies the consistency of tailored processes against those constraints. Finally, the Process Management integrates APE with the agentTool III development environment and provides a way to measur...|$|E
40|$|We make a {{case for}} {{ensuring}} semantic consistency of data at the disk-level. With the additional knowledge of pointers inside a block-based disk, we show that strong meta-data consistency semantics can be provided by the disk. Today’s consistency mechanisms operate at the software-level making disks totally oblivious to the consistent state of the data. Knowledge of consistency at the disk level enables interesting functionality which cannot be provided by traditional disks. In this paper we present a disk-level consistency enforcement mechanism and evaluate it by a prototype implementation where we provide consistency transparently underneath the Linux Ext 2 file system. We show that our <b>consistency</b> <b>mechanism</b> has small overheads, and the modifications required at the software-level are minimal. ...|$|E
40|$|Aspect-oriented {{middleware}} is {{a promising}} {{technology for the}} realisation of dynamic reconfiguration in heterogeneous distributed systems. However, like other dynamic reconfiguration approaches, AO-middleware-based reconfiguration requires that {{the consistency of the}} system is maintained across reconfigurations. AO-middleware-based reconfiguration is an ongoing research topic and several consistency approaches have been proposed. However, most of these approaches tend to be targeted at specific contexts, whereas for distributed systems it is crucial to cover a wide range of operating conditions. In this paper we propose an approach that offers distributed, dynamic reconfiguration in a consistent manner, and features a flexible framework-based consistency management approach to cover a wide range of operating conditions. We evaluate our approach by investigating the configurability and transparency of our approach and also quantify the performance overheads of the associated <b>consistency</b> <b>mechanisms...</b>|$|R
40|$|Honesty Based Selling Price Concept: Achieving Gain Reaching Goodness. This article aims to {{reconstruct}} honesty-based selling price concept in Islam. The research involved owners {{and management of}} YDT, Kopontren DT in Bandung, Charni’s Productions in Yogyakarta, as well as Kedai Assalamu’alaikum and Bismillah Restaurant in Malang. Analysis was conducted by using bayani, burhani and irfani epistemology. Honesty-based selling price concept {{could be classified as}} cost-plus pricing <b>consistency,</b> conditioned market <b>mechanism,</b> and balanced/equibrium market mechanism according to bayani, burhani and irfani epistemology respectively. Generally, honesty-based selling price concept in Islam is <b>consistency</b> market <b>mechanism</b> which is the determination of price at the very beginning until the complete consumption of products...|$|R
40|$|Parallel and {{distributed}} systems architectures support parallel I/O components. Caching {{has been applied}} to distributed I/O subsystems as a standard solution to the problems of fastening data accessibility and increasing data reliability. Cache <b>consistency</b> <b>mechanisms</b> have been implementedin order to influence the cache usefulness in a positive way. This paper presents a new caching technique based on the genetic algorithm idea and examines the effect of this technique on the parallel I/O cache consistency and updating process. Cached data blocks on parallel disks are considered as a population evolving over simulated time and are updated at regular intervals towards an improved cache content. The proposed cache update scheme is compared with the LRU caching scheme which has been widely adopted. The proposed technique shows improved performance compared to conventional caching under simulation runs for various workloads...|$|R
40|$|Web, {{workload}} characterization, performance, servers, caching, World Cup © Copyright Hewlett-Packard Company 1999 This paper {{presents a}} detailed workload characterization {{study of the}} 1998 World Cup Web site. Measurements from this site were collected over a three month period. During this time the site received 1. 35 billion requests, making this the largest Web workload analyzed to date. By examining this extremely busy site and through comparison with existing characterization studies {{we are able to}} determine how Web server workloads are evolving. We find that improvements in the caching architecture of the World-Wide Web are changing the workloads of Web servers, but that major improvements to that architecture are still necessary. In particular, we uncover evidence that a better <b>consistency</b> <b>mechanism</b> is required for World-Wide Web caches...|$|E
40|$|AbstractA {{method is}} {{proposed}} to transform automatically some satisfiable clause sets S into clause sets S′ having a unique Herbrand model (w. r. t. a given signature) which also satisfies S. These clause sets {{with only one}} model {{can be used as}} representations of (in general infinite) Herbrand models of the initial set of clauses. Existing theorem provers may be used to evaluate literals and clauses in the models thus represented (using the standard proof by <b>consistency</b> <b>mechanism).</b> We also prove that for some classes of clauses, the extracted model can also be represented by a tree automaton on tuples of finite trees. This entails in particular that the evaluation of arbitrary function-free formulae in the represented models is decidable...|$|E
40|$|Constraints {{are widely}} {{recognized}} as {{a useful tool for}} user interface construction. Through constraints, relationships among user interface components can be defined declaratively, leaving the task of relationship management to a constraint solver. Multi-way constraint solvers supporting constraint hierarchies provide a means to specify preferential constraint relationships with a dynamically changing computation flow, making them especially well suited to interactive user interfaces. However, previous such constraint solvers lack the ability to enforce inequalities or to effectively handle cyclic constraint relationships. These deficiencies limit the problems that could be solved using a constraint-based approach. This paper presents a new algorithm called UltraBlue for solving hierarchies of multi-way constraints and discusses its application to the architecture of the EUPHORIA user interface management system. Contributions of UltraBlue include a value <b>consistency</b> <b>mechanism</b> for main [...] ...|$|E
5000|$|Since April 2015, {{there is}} ongoing work on porting kpatch and kGraft {{to the common}} live {{patching}} core provided by the Linux kernel mainline. However, implementation of the function-level <b>consistency</b> <b>mechanisms,</b> required for safe transitions between the original and patched versions of functions, has been delayed because the call stacks provided by the Linux kernel may be unreliable in situations that involve assembly code without proper stack frames; as a result, the porting work remains in progress [...] In an attempt to improve the reliability of kernel's call stacks, a specialized sanity-check [...] userspace utility has also been developed {{with the purpose of}} checking kernel's compile-time object files and ensuring that the call stack is always maintained; it also opens up a possibility for achieving more reliable call stacks as part of the kernel oops messages.|$|R
40|$|Despite {{the ever}} {{increasing}} popularity of handheld, networked gaming consoles, fully interactive real-time, multiplayer games designed for these platforms {{have yet to}} become a reality. This is primarily caused by the reliance of these handheld devices on telecoms networks such as GPRS and 3 G to enable communications between players, introducing network latencies beyond the capacity of existing games and supporting <b>consistency</b> <b>mechanisms</b> to operate successfully. This paper motivates {{the need for a}} new approach to consistency in mobile multiplayer gaming environments, and introduces Rendezvous, a novel solution to the high latency consistency problem, based on the concept of elegant recovery from an inconsistent shared state, rather than the more traditional approach of prevention of the initial inconsistency. The paper goes on to present preliminary design, implementation and evaluation of Rendezvous, with respect to its application to a mobile real-time multiplayer game written for the smart phone platform called Knockabout...|$|R
40|$|False sharing can be {{a source}} of {{significant}} overhead on shared-memory multiprocessors. Several program restructuring techniques to reduce false sharing have been proposed in past work. In this paper, we propose an approach for elimination of false sharing based solely on selection of runtime schedule parameters for parallel loops. This approach leads to more portable code since only the schedule parameters need to be changed to target different multiprocessors. Also, the guarantee of elimination (rather than reduction) of false sharing in a parallel loop can significantly reduce the bookkeeping overhead in some memory <b>consistency</b> <b>mechanisms.</b> We present some preliminary experimental results for this approach. 1 Introduction False sharing occurs when two processors attempt to concurrently write to distinct memory locations in the same cache line. The false sharing is self-variable if the two memory locations belong to the same (array or structure) variable; otherwise, it is cross-var [...] ...|$|R
