10000|10000|Public
5|$|Residual {{inert gas}} can be <b>computed</b> <b>for</b> all modeled tissues, but {{repetitive}} group designations in decompression tables are generally based on only the one tissue, {{considered by the}} table designers {{to be the most}} limiting tissue for likely applications. In the case of the US Navy Air Tables (1956) this is the 120 minute tissue, while the Bühlmann tables use the 80 minute tissue.|$|E
5|$|Mathematicians became {{interested}} in mirror symmetry around 1990 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that mirror symmetry {{could be used to}} solve problems in enumerative geometry that had resisted solution for decades or more. These results were presented to mathematicians at a conference at the Mathematical Sciences Research Institute (MSRI) in Berkeley, California in May 1991. During this conference, it was noticed that one of the numbers Candelas had <b>computed</b> <b>for</b> the counting of rational curves disagreed with the number obtained by Norwegian mathematicians Geir Ellingsrud and Stein Arild Strømme using ostensibly more rigorous techniques. Many mathematicians at the conference assumed that Candelas's work contained a mistake since it was not based on rigorous mathematical arguments. However, after examining their solution, Ellingsrud and Strømme discovered an error in their computer code and, upon fixing the code, they got an answer that agreed with the one obtained by Candelas and his collaborators.|$|E
25|$|For instance, to {{find the}} maximum clique in a cograph, compute in {{bottom-up}} order the maximum clique in each subgraph represented by a subtree of the cotree. For a node labeled 0, the maximum clique is the maximum among the cliques <b>computed</b> <b>for</b> that node's children. For a node labeled 1, the maximum clique is the union of the cliques <b>computed</b> <b>for</b> that node's children, and has size equal to {{the sum of the}} children's clique sizes. Thus, by alternately maximizing and summing values stored at each node of the cotree, we may compute the maximum clique size, and by alternately maximizing and taking unions, we may construct the maximum clique itself. Similar bottom-up tree computations allow the maximum independent set, vertex coloring number, maximum clique cover, and Hamiltonicity (that is the existence of a Hamiltonian cycle) to be computed in linear time from a cotree representation of a cograph. Because cographs have bounded clique-width, Courcelle's theorem may be used to test any property in the monadic second-order logic of graphs (MSO1) on cographs in linear time.|$|E
50|$|By 2008, DGSCA has {{established}} many branches throughout Mexico City {{and a few}} dependencies throughout the Republic, having several main divisions including D.C.I. (<b>Computing</b> <b>for</b> Research Division), D.C.D. (<b>Computing</b> <b>for</b> Academic aide Division), D.T. (Telecommunications Division) and the Systems Division.|$|R
5000|$|Reversible <b>computing</b> (<b>for</b> {{relations}} between information and energy) ...|$|R
5000|$|Pipeline (<b>computing)</b> <b>for</b> other computer-related {{versions}} of the concept.|$|R
25|$|With a curve {{given by}} such an {{implicit}} equation, the first problems are {{to determine the}} shape of the curve and to draw it. These problems are not as easy to solve {{as in the case of}} the graph of a function, for which y may easily be <b>computed</b> <b>for</b> various values of x. The fact that the defining equation is a polynomial implies that the curve has some structural properties that may help in solving these problems.|$|E
25|$|First, a {{time series}} is <b>{{computed}}</b> <b>for</b> about 50 million common queries entered weekly within the United States from 2003 to 2008. A query's time series is computed separately for each state and normalized into a fraction {{by dividing the}} number of each query {{by the number of}} all queries in that state. By identifying the IP address associated with each search, the state in which this query was entered can be determined.|$|E
25|$|Feature models view {{semantic}} categories {{as being}} composed of relatively unstructured sets of features. The semantic feature-comparison model, proposed by Smith, Shoben, and Rips (1974), describes memory as being composed of feature lists for different concepts. According to this view, {{the relations between}} categories would not be directly retrieved, they would be indirectly <b>computed.</b> <b>For</b> example, subjects might verify a sentence by comparing the feature sets that represent its subject and predicate concepts. Such computational feature-comparison models include the ones proposed by Meyer (1970), Rips (1975), Smith, et al. (1974).|$|E
5000|$|... #Subtitle level 3: <b>Computing</b> <b>for</b> One Million people, Madagascar ...|$|R
5000|$|WikiTrust <b>computes,</b> <b>for</b> each word, {{three pieces}} of information: ...|$|R
500|$|Pen <b>computing</b> <b>for</b> a broad {{history of}} gesture-based user {{interfaces}} ...|$|R
25|$|The {{computational}} {{cost of a}} water simulation {{increases with}} the number of interaction sites in the water model. The CPU time is approximately proportional to the number of interatomic distances that need to be <b>computed.</b> <b>For</b> the 3-site model, 9 distances are required for each pair of water molecules (every atom of one molecule against every atom of the other molecule, or 3 × 3). For the 4-site model, 10 distances are required (every charged site with every charged site, plus the O–O interaction, or 3 × 3 + 1). For the 5-site model, 17 distances are required (4 × 4 + 1). Finally, for the 6-site model, 26 distances are required (5 × 5 + 1).|$|E
25|$|Research on {{decompression}} continues. Data is {{not generally}} {{available on the}} specifics, however Divers Alert Network (DAN) has an ongoing citizen science based programme run by DAN (Europe) which gathers data from volunteer recreational divers for analysis by DAN research staff and other researchers. This research is funded by subscription fees of DAN Europe members. The Diving Safety Laboratory is a database to which members can upload dive profiles {{from a wide range}} of dive computers converted to a standard format and other data about the dive. Data on hundreds of thousands of real dives is analysed to investigate aspects of diving safety. The large amounts of data gathered is used for probabilistic analysis of decompression risk. The data donors can get immediate feedback in the form of a simple risk analysis of their dive profiles rated as one of three nominal levels of risk (high, medium and low) based on comparison with Bühlmann ZH16c M-values <b>computed</b> <b>for</b> the same profile.|$|E
25|$|When {{a problem}} shows optimal {{substructures}} – meaning the optimal {{solution to a}} problem can be constructed from optimal solutions to subproblems – and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been <b>computed.</b> <b>For</b> example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.|$|E
5000|$|... "Parallel <b>Computing</b> <b>for</b> Graphics." [...] Advances in Computer Graphics, 1990:113-140.|$|R
5000|$|Hyphens in <b>computing,</b> <b>for</b> {{information}} about hard and non-breaking hyphens ...|$|R
5000|$|The Rise of Exotic <b>Computing,</b> <b>for</b> sinfonietta and laptop (2013) ...|$|R
25|$|When using RFLP, the {{theoretical}} {{risk of a}} coincidental match is 1 in 100 billion (100,000,000,000), although the practical risk is actually 1 in 1000 because monozygotic twins are 0.2% of the human population. Moreover, the rate of laboratory error is almost certainly higher than this, and often actual laboratory procedures {{do not reflect the}} theory under which the coincidence probabilities were <b>computed.</b> <b>For</b> example, the coincidence probabilities may be calculated based on the probabilities that markers in two samples have bands in precisely the same location, but a laboratory worker may conclude that similar—but not precisely identical—band patterns result from identical genetic samples with some imperfection in the agarose gel. However, in this case, the laboratory worker increases the coincidence risk by expanding the criteria for declaring a match. Recent studies have quoted relatively high error rates, which may be cause for concern. In the early days of genetic fingerprinting, the necessary population data to accurately compute a match probability was sometimes unavailable. Between 1992 and 1996, arbitrary low ceilings were controversially put on match probabilities used in RFLP analysis rather than the higher theoretically computed ones. Today, RFLP has become widely disused due to the advent of more discriminating, sensitive and easier technologies.|$|E
25|$|In {{the second}} book, Hipparchus starts from the {{opposite}} extreme assumption: he assigns a (minimum) distance to the Sun of 490 Earth radii. This would correspond to a parallax of 7', which is apparently the greatest parallax that Hipparchus thought would not be noticed (for comparison: the typical resolution of the human eye is about 2'; Tycho Brahe made naked eye observation with an accuracy down to 1'). In this case, {{the shadow of the}} Earth is a cone rather than a cylinder as under the first assumption. Hipparchus observed (at lunar eclipses) that at the mean distance of the Moon, the diameter of the shadow cone is 2+½ lunar diameters. That apparent diameter is, as he had observed, 360/650 degrees. With these values and simple geometry, Hipparchus could determine the mean distance; because it was <b>computed</b> <b>for</b> a minimum distance of the Sun, it is the maximum mean distance possible for the Moon. With his value for the eccentricity of the orbit, he could compute the least and greatest distances of the Moon too. According to Pappus, he found a least distance of 62, a mean of 67+1/3, and consequently a greatest distance of 72+2/3 Earth radii. With this method, as the parallax of the Sun decreases (i.e., its distance increases), the minimum limit for the mean distance is 59 Earth radii – exactly the mean distance that Ptolemy later derived.|$|E
500|$|Higher {{order terms}} can be straightforwardly <b>computed</b> <b>for</b> the {{evolution}} operator but these terms display diagrams containing the following simpler ones ...|$|E
5000|$|Pen <b>computing</b> <b>for</b> a broad {{history of}} gesture-based user {{interfaces}} ...|$|R
5000|$|OpenCL {{cross-platform}} API <b>for</b> general-purpose <b>computing</b> <b>for</b> CPUs & GPUs ...|$|R
5000|$|Reading, Writing, Listening, Speaking and <b>computing</b> <b>for</b> {{a variety}} of {{purposes}} ...|$|R
500|$|In {{each step}} k of the Euclidean algorithm, the {{quotient}} q'k and remainder r'k are <b>computed</b> <b>for</b> a given pair of integers r'k−2 and r'k−1 ...|$|E
500|$|The {{question}} of computing the homotopy group πn+k(S'n) for positive k {{turned out to}} be a central question in algebraic topology that has contributed to development of many of its fundamental techniques and has served as a stimulating focus of research. One of the main discoveries is that the homotopy groups πn+k(S'n) are independent of n for n≥k+2. These are called the stable homotopy groups of spheres and have been <b>computed</b> <b>for</b> values of k up to 64. The stable homotopy groups form the coefficient ring of [...] an extraordinary cohomology theory, called stable cohomotopy theory. The unstable homotopy groups (for n< k + 2) are more erratic; nevertheless, they have been tabulated for [...] k < 20. Most modern computations use spectral sequences, a technique first applied to homotopy groups of spheres by Jean-Pierre Serre. Several important patterns have been established, yet much remains unknown and unexplained.|$|E
2500|$|Explicit {{expressions}} for {{the curvature}} and torsion may be <b>computed.</b> <b>For</b> example, ...|$|E
50|$|Numecent {{was named}} a Gartner Cool Vendor in Cloud <b>Computing</b> <b>for</b> 2013.|$|R
5000|$|C-M. Pintea, 2014, Advances in Bio-inspired <b>Computing</b> <b>for</b> Combinatorial Optimization Problem, Springer ...|$|R
50|$|In 2008, he co-founded the <b>Computing</b> <b>for</b> Good (C4G) {{program at}} Georgia Tech.|$|R
2500|$|A running sum of weights must be <b>computed</b> <b>for</b> each k from 1 to n: ...|$|E
2500|$|From these, the {{algebraic}} expressions for all multiples of 3° can be <b>computed.</b> <b>For</b> example: ...|$|E
2500|$|Step 3: The summary {{statistic}} {{is being}} <b>computed</b> <b>for</b> each sequence of simulated data, [...] (Table 1, column 4).|$|E
5000|$|... #Caption: Hewitt Crane {{demonstrates}} pen-input <b>computing</b> <b>for</b> writing Chinese characters at SRI International ...|$|R
5000|$|... 2007: The Edsger W. Dijkstra Prize in Distributed <b>Computing</b> <b>for</b> {{the paper}} [...]|$|R
5000|$|... $50 {{million to}} the Institute <b>for</b> Quantum <b>Computing</b> <b>for</b> a new {{research}} facility.|$|R
