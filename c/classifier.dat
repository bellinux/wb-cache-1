10000|10000|Public
5|$|Northern dialects tend to {{have fewer}} {{classifiers}} than southern ones. 個 ge is the only <b>classifier</b> found in the Dungan language. All nouns could have just one <b>classifier</b> in some dialects, such as Shanghainese (Wu), the Mandarin dialect of Shanxi, and Shandong dialects. Some dialects such as Northern Min, certain Xiang dialects, Hakka dialects, and some Yue dialects use 隻 for the noun referring to people, rather than 個.|$|E
5|$|Most count nouns are inflected for plural number {{through the}} use of the plural suffix -s, but a few nouns have {{irregular}} plural forms. Mass nouns can only be pluralised {{through the use}} of a count noun <b>classifier,</b> e.g. one loaf of bread, two loaves of bread.|$|E
25|$|The auto encoder idea is {{motivated}} by {{the concept of a}} good representation. For example, for a <b>classifier,</b> a good representation can be defined as one that yields a better-performing <b>classifier.</b>|$|E
40|$|This paper {{introduces}} and investigates large iterative multitier ensemble (LIME) <b>classifiers</b> specifically tailored for big data. These <b>classifiers</b> {{are very}} large, but are quite easy to generate and use. They {{can be so}} large that {{it makes sense to}} use them only for big data. They are generated automatically as a result of several iterations in applying ensemble meta <b>classifiers.</b> They incorporate diverse ensemble meta <b>classifiers</b> into several tiers simultaneously and combine them into one automatically generated iterative system so that many ensemble meta <b>classifiers</b> function as integral parts of other ensemble meta <b>classifiers</b> at higher tiers. In this paper, we carry out a comprehensive investigation of the performance of LIME <b>classifiers</b> for a problem concerning security of big data. Our experiments compare LIME <b>classifiers</b> with various base <b>classifiers</b> and standard ordinary ensemble meta <b>classifiers.</b> The results obtained demonstrate that LIME <b>classifiers</b> can significantly increase the accuracy of classifications. LIME <b>classifiers</b> performed better than the base <b>classifiers</b> and standard ensemble meta <b>classifiers...</b>|$|R
40|$|This paper compares today’s {{most common}} {{frame-based}} <b>classifiers.</b> These <b>classifiers</b> {{can be divided}} into the two main groups – generic <b>classifiers</b> which creates the most probable model based on the training data (for example GMM) and discriminative <b>classifiers</b> which focues on creating decision hyperplane. A lot of research has been done with the GMM <b>classifiers</b> and therefore this paper will be mainly focused on the frame-based <b>classifiers.</b> Two discriminative <b>classifiers</b> will be presented. These <b>classifiers</b> implements a hieararchical tree root structure over the input phoneme group which shown to be an effective. Based on these <b>classifiers,</b> two efficient training algorithms will be presented. We demonstrate advantages of our training algorithms by evaluating all <b>classifiers</b> over the TIMIT speech corpus. </em...|$|R
40|$|This {{research}} {{investigated the}} development of <b>classifiers</b> of Hong Kong normal preschoolers and their acquisition strategies. Participants were 59 normal kindergarten children aged from 2. 5 to 5. 5. Findings revealed that as age increases, the amount and diversity of <b>classifiers</b> will also increase. Mixed <b>classifiers,</b> shape <b>classifiers</b> and function <b>classifiers</b> develop in sequence. Generalization of mixed <b>classifiers</b> {{was one of the}} most important and earliest strategies used in the production of <b>classifiers,</b> followed by generalization of shape <b>classifiers</b> to function <b>classifiers.</b> Interestingly, developmental trends of different <b>classifiers</b> showed a similar pattern in that at the age of 4, progress tends to reduce in speed or shows some regression. Similar acquisition patterns were also found in other research on children's comprehension. Therefore, it is suspected that 4 years of age is a critical point for language development. ...|$|R
25|$|A {{classification}} model (<b>classifier</b> or diagnosis) is {{a mapping}} of instances between certain classes/groups. The <b>classifier</b> or diagnosis {{result can be}} a real value (continuous output), in which case the <b>classifier</b> boundary between classes must be determined by a threshold value (for instance, to determine whether a person has hypertension based on a blood pressure measure). Or it can be a discrete class label, indicating one of the classes.|$|E
25|$|The {{singular}} emphatic demonstrative modifiers {{are formed}} by suffixing the non-emphatic singular forms to appropriate numeral <b>classifier</b> for the noun, such as men- for animate nouns. The plural forms are always constructed by suffixing the non-emphatic plural form to pwu- {{regardless of the}} singular <b>classifier.</b>|$|E
25|$|A simple {{computational}} experiment {{illustrates this}} idea. Two instances of a <b>classifier</b> {{were trained to}} distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One {{can see that the}} second <b>classifier</b> performed quite well even after receiving a single example from each category, while performance of the first <b>classifier</b> was close to random guess even after seeing 20 examples.|$|E
25|$|The {{construction}} of possessive <b>classifiers</b> depends on ownership, temporality, degrees of control, locative associations, and status. In addition to status-rising and status-lowering possessive <b>classifiers,</b> {{there are also}} common (non-status marked) possessive <b>classifiers.</b> Status-rising and status-lowering possessive <b>classifiers</b> have different properties of control and temporality. Common possessive <b>classifiers</b> are divided into three main categories – relatives, personal items, and food/drink.|$|R
30|$|A {{collection}} of <b>classifiers</b> {{has been identified}} in every model with this training information. Calculate a distance measure from these policy <b>classifiers</b> in a model from all policy <b>classifiers</b> in every other model using the SEC measure (distance as reciprocal of similarity value) concept for paths. Calculate a weight measure associated with the identified <b>classifiers</b> in every model based on these similarity measures, where <b>classifiers</b> more similar to others have stronger weights. This weight can also be determined from application of Eqs.  15 and 16. These identified <b>classifiers</b> which provide cover for all the <b>classifiers</b> [33] are considered for model merging.|$|R
50|$|<b>Classifiers</b> {{are often}} derived from nouns (or {{occasionally}} {{other parts of}} speech), which have become specialized as <b>classifiers,</b> or may retain other uses besides their use as <b>classifiers.</b> <b>Classifiers,</b> like other words, are sometimes borrowed from other languages. A language {{may be said to}} have dozens or even hundreds of different <b>classifiers.</b> However, such enumerations often also include measure words.|$|R
25|$|In topos theory, the (codomain of the) subobject <b>classifier</b> of an {{elementary}} topos.|$|E
25|$|In 2015, Zhou et al. {{suggested}} to apply naive Bayes <b>classifier</b> to detect pathological brains.|$|E
25|$|The {{standard}} {{word order}} of quantified words is: quantified noun + numeral adjective + <b>classifier,</b> except in round numbers (numbers that end in zero), {{in which the}} word order is flipped, where the quantified noun precedes the classifier: quantified noun + <b>classifier</b> + numeral adjective. The only exception to this rule is the number 10, which follows the standard word order.|$|E
40|$|Margin-based <b>classifiers</b> {{have been}} popular in both machine {{learning}} and statistics for classification problems. Among numerous <b>classifiers,</b> some are hard <b>classifiers</b> while some are soft ones. Soft <b>classifiers</b> explicitly estimate the class conditional probabilities and then perform classification based on estimated probabilities. In contrast, hard <b>classifiers</b> directly target on the classification decision boundary without producing the probability estimation. These {{two types of}} <b>classifiers</b> are based on different philosophies and {{each has its own}} merits. In this paper, we propose a novel family of large-margin <b>classifiers,</b> namely large-margin unified machines (LUMs), which covers a broad range of margin-based <b>classifiers</b> including both hard and soft ones. By offering a natural bridge from soft to hard classification, the LUM provides a unified algorithm to fit various <b>classifiers</b> and hence a convenient platform to compare hard and soft classification. Both theoretical consistency and numerical performance of LUMs are explored. Our numerical study sheds some light on the choice between hard and soft <b>classifiers</b> in various classification problems...|$|R
3000|$|... -nearest neighbors (KNN) <b>classifiers,</b> {{we propose}} two novel ERP-based KNN <b>classifiers.</b> Taking {{advantage}} of the metric property of ERP, we first develop an ERP-induced inner product and a Gaussian ERP kernel, then embed them into difference-weighted KNN <b>classifiers,</b> and finally develop two novel <b>classifiers</b> for pulse waveform classification. The experimental {{results show that the}} proposed <b>classifiers</b> are effective for accurate classification of pulse waveform.|$|R
40|$|In this paper, {{we present}} a class of combinatorial-logical <b>classifiers</b> called test feature <b>classifiers.</b> These <b>classifiers</b> allow us to avoid many of the above drawbacks. <b>Classifiers</b> aregen:O 685 {{directly}} fromtrainDD samplesusin so-called tests, sets of features that are sufficient to distinH 825 pattern from different classes o...|$|R
25|$|Yan, C., Dobbs, D., and Honavar, V. A Two-Stage <b>Classifier</b> for Identification of Protein-Protein Interface Residues. Bioinformatics. Vol. 20. pp. i371-378, 2004.|$|E
25|$|In 2011, Wu and Wang {{proposed}} using DWT for feature extraction, PCA for feature reduction, and FNN with scaled chaotic artificial {{bee colony}} (SCABC) as <b>classifier.</b>|$|E
25|$|Sign {{languages}} {{tend to be}} incorporating <b>classifier</b> languages, where a <b>classifier</b> handshape {{representing the}} object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages {{can be said to}} follow an ergative pattern.|$|E
3000|$|... θsub Subsumption-related parameter. Determines {{how long}} <b>classifiers</b> must {{exist in the}} {{population}} before they can subsume other <b>classifiers.</b> If set too high, subsumption takes too long and thus superfluous <b>classifiers</b> persist. If set too low, overly-general <b>classifiers</b> may out compete less-general but higher performing <b>classifiers.</b> Butz and Wilson (2001) suggest an approximate value of 20. In practice, for longer multi-step problems, a value higher than this may be better.|$|R
40|$|Abstract—We propose {{and design}} {{two classes of}} robust {{subspace}} <b>classifiers</b> for classification of multidimensional signals. Our <b>classifiers</b> are based on robust-estimators and the least-median-ofsquares principle, and we show {{that they may be}} unified as iterated reweighted oblique subspace <b>classifiers.</b> The performance of the proposed <b>classifiers</b> are demonstrated by two examples: noncoherent detection of space-time frequency-shift keying signals, and shape classification of partially occluded two-dimensional (2 -D) _ objects. In both cases, the proposed robust subspace <b>classifiers</b> outperform the conventional subspace <b>classifiers.</b> Index Terms—Noncoherent receivers, robust estimation, shape recognition, subspace classification. I...|$|R
40|$|This work {{proposes to}} learn linguistically-derived sub-unit <b>classifiers</b> for sign language. The {{responses}} of these <b>classifiers</b> {{can be combined}} by Markov models, producing efficient sign-level recognition. Tracking is used to create vectors of hand positions per frame as inputs for sub-unit <b>classifiers</b> learnt using AdaBoost. Grid-like <b>classifiers</b> are built around specific elements of the tracking vector to model {{the placement of the}} hands. Comparative <b>classifiers</b> encode the positional relationship between the hands. Finally, binary-pattern <b>classifiers</b> are applied over the tracking vectors of multiple frames to describe the motion of the hands. Results for the sub-unit <b>classifiers</b> in isolation are presented, reaching averages over 90 %. Using a simple Markov model to combine the sub-unit <b>classifiers</b> allows sign level classification giving an average of 63 %, over a 164 sign lexicon, with no grammatical constraints. 1...|$|R
25|$|The {{frequency}} of <b>classifier</b> use depends greatly on genre, occurring {{at a rate}} of 17.7% in narratives but only 1.1% in casual speech and 0.9% in formal speech.|$|E
25|$|A <b>classifier</b> is a {{generalization}} of a {{finite state machine}} that, similar to an acceptor, produces a single output on termination but has more than two terminal states.|$|E
25|$|In statistics, a {{receiver}} operating characteristic curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary <b>classifier</b> system as its discrimination threshold is varied.|$|E
40|$|This study aims to {{describe}} forms of noun <b>classifiers</b> and their uses by Indonesian speakers in transactions in traditional central markets in Jakarta and Surabaya. The {{data were collected}} through listening and interviewing. They were analyzed by the intralingual correspondence technique. The research findings are as follows. First, there are 43 forms of noun <b>classifiers.</b> Second, they can be classified into three categories, i. e. : (a) individual noun <b>classifiers,</b> (b) collective noun <b>classifiers,</b> and (c) noun <b>classifiers</b> {{in the form of}} measurement units. Third, the dominant noun <b>classifiers</b> in traditional central markets in Jakarta and Surabaya are noun <b>classifiers</b> in the form of measurement units. This {{is due to the fact}} that transactions need precise measurements...|$|R
40|$|A novel {{method is}} {{described}} for obtaining superior classification performance over a variable range of classification costs. By {{analysis of a}} set of existing <b>classifiers</b> using a receiver operating characteristic (###) curve,a set of new realisable <b>classifiers</b> may be obtained by a random combination of two of the existing <b>classifiers.</b> These <b>classifiers</b> lie on the convex hull that contains the original ### points for the existing <b>classifiers.</b> This hull is the maximum realisable ### (#####) ...|$|R
40|$|Abstract- This paper proposes an Automated Learning Method (ALM) {{based on}} Real-Coded Genetic Algorithm (RCGA) to infer the Multi-Criteria <b>Classifiers</b> (MCC) parameters. The Multi-Criteria <b>Classifiers</b> (or Multi-Criteria Classification Methods) {{considered}} {{are based on}} concordance and discordance concepts. A military database of 2545 Forward Looking Infra-Red (FLIR) images representing eight different classes of ships is therefore {{used to test the}} performance of these <b>classifiers.</b> The empirical results of MCC are compared with those obtained by other <b>classifiers</b> (e. g. Bayes and Dempster–Shafer <b>classifiers).</b> In this paper, we show the benefits of cross-fertilization of multi-criteria <b>classifiers</b> and information fusion algorithms...|$|R
25|$|Honorific {{speech is}} usually {{performed}} through {{the choice of}} verbs and possessive <b>classifier.</b> There are only status-raising nouns but none for status lowering; there are only status-lowering pronouns but none for status-raising.|$|E
25|$|Once the stacked auto encoder is trained, its output {{can be used}} as {{the input}} to a {{supervised}} learning algorithm such as support vector machine <b>classifier</b> or a multi-class logistic regression.|$|E
25|$|Kang, D-K., Silvescu, A., Zhang, J. and Honavar, V. Generation of Attribute Value Taxonomies from Data for Accurate and Compact <b>Classifier</b> Construction. IEEE International Conference on Data Mining, IEEE Press. pp.130–137, 2004.|$|E
40|$|Abstract. This paper {{describes}} {{a method for}} fusing a collection of <b>classifiers</b> where the fusion can compensate for some positive correlation among the <b>classifiers.</b> Specifically, {{it does not require}} the assumption of evidential independence of the <b>classifiers</b> to be fused (such as Dempster Shafer’s fusion rule). The proposed method is associative, which allows fusing three or more <b>classifiers</b> irrespective of the order. The fusion is accomplished using a generalized intersection operator (T-norm) that better represents the possible correlation between the <b>classifiers.</b> In addition, a confidence measure is produced that takes advantage of the consensus and conflict between <b>classifiers.</b> ...|$|R
40|$|In {{this paper}} we propose a meta-evolutionary {{approach}} to improve on the performance of individual <b>classifiers.</b> In the proposed system, individual <b>classifiers</b> evolve, competing to correctly classify test points, and are given extra rewards for getting difficult points right. Ensembles consisting of multiple <b>classifiers</b> also compete for member <b>classifiers,</b> and are rewarded based on their predictive performance. In this way we aim to build small-sized optimal ensembles rather than form large-sized ensembles of individually-optimized <b>classifiers.</b> Experimental results on 15 data sets suggest that our algorithms can generate ensembles that are more effective than single <b>classifiers</b> and traditional ensemble methods...|$|R
3000|$|To {{speed up}} classification, Viola and Jones [9] {{proposed}} a cascade structure where several strong <b>classifiers</b> are associated into successive levels. The {{idea is that}} the first strong <b>classifiers</b> reject most of the negative examples, while the last strong <b>classifiers</b> try to discriminate positive examples from hard negative examples. In such cascades, strong <b>classifiers</b> are slightly changed into [...]...|$|R
