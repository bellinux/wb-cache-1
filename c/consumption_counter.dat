0|16|Public
40|$|Encouraging use {{of public}} transit is one {{important}} way to reduce energy <b>consumption</b> and <b>counter</b> climate change. In my research {{on the design of}} Swedish travel centers, I have studied the transit stations for the planned “West Link,” an underground railway tunnel through Gothenburg that will increase the capacity for commuter traffic. I ask how one might design public-transport spaces that incorporate safety, comfort, unambigu-ous orientation, and aesthetic values. A thorough answer to this question might help de-signers and planners to create more sustainable, user-friendly urban spaces...|$|R
50|$|On June 27, 1989, Watkins {{announced}} the Ten-Point Plan to strengthen {{environmental protection and}} waste management activities at the United States Department of Energy's production, research, and testing facilities. In September 1989, he established the Modernization Review Committee to review the assumptions and recommendations of the 2010 Report. On November 9, 1989, Watkins established the Office of Environmental Restoration and Waste Management within the Department of Energy. On August 15, 1990, Secretary Watkins announced plans to increase oil production and decrease <b>consumption</b> to <b>counter</b> Iraqi-Kuwaiti oil losses caused by the Iraqi Invasion of Kuwait. On March 4, 1991, he transmitted the Administration's energy bill to the House and Senate. On May 10, 1992, in testimony before the Senate Armed Services Committee he reported that, {{for the first time}} since 1945, the United States was not building any nuclear weapons.|$|R
50|$|Skeptics of {{the alleged}} {{sustainability}} of hydraulic fracturing have also voiced concerns. While natural gas may burn cleaner than energy sources such as coal, {{there is still a}} danger for the extraction process to release methane into the atmosphere, which is more potent than carbon dioxide. Opponents also contend that there is not enough infrastructure for natural gas extraction in South Africa to replace coal, meaning that natural gas will be exported and coal will remain the source of energy of choice domestically. Proponents are more optimistic that hydraulic fracturing will act as a bridge towards the development of other sources of renewable energy, decreasing the need for coal and oil <b>consumption.</b> Opponents <b>counter</b> that while this strategy would decrease natural gas prices and therefore carbon dioxide emissions, the lowered prices of natural gas would deter further investment into the renewable energy sector.|$|R
40|$|The {{forecasting}} {{power of}} consumer confidence indexes for <b>consumption</b> spending runs <b>counter</b> to {{the predictions of}} the permanent income hypothesis (PIH). This paper resolves this discrepancy by developing a “confidence augmented” permanent income hypothesis (CAPIH). While it does not radically alter the estimated extent of permanent income consumption, the CAPIH model predicts a significantly smaller intertemporal elasticity of substitution than a standard PIH model. In addition, the results are largely invariant to the measure of consumer confidence used and the choice of instrumental variables. ...|$|R
40|$|The {{scale of}} the {{ambition}} to decouple emissions growth from energy <b>consumption</b> runs <b>counter</b> to decades of debates and literatures on the limits of government. Transport biofuels are an early and influential case of the policy capacity challenge in the transition to low-carbon economies. The case stands analytically for the policymaker’s dilemma of maintaining long-term policy goals as credible commitments, despite the flexibility and adaptability in policy-making required to achieve them under high political, technological and market uncertainty. This paper compares US and EU biofuels policy processes in these terms. It reveals an intertemporal choice embedded in biofuels policies which tests the capacity to account for future benefits from a low carbon future in current policy processes; if the pathway to their achievement is uncertain and politically contested in the implementation phase, then future benefits may be heavily discounted, shortening policy-maker horizons and rendering the transition process politically vulnerable...|$|R
40|$|Ocean {{warming is}} {{anticipated}} {{to strengthen the}} persistence of turf-forming habitat, yet the concomitant elevation of grazer metabolic rates may accelerate per capita rates of <b>consumption</b> to <b>counter</b> turf predominance. Whilst this possibility of strong top-down control {{is supported by the}} metabolic theory of ecology (MTE), it assumes that consumer metabolism and consumption keep pace with increasing production. This assumption was tested by quantifying the metabolic rates of turfs and herbivorous gastropods under a series of elevated temperatures in which the ensuing production and consumption were observed. We discovered that as temperature increases towards near-future levels (year 2100), consumption rates of gastropods peak earlier than the rate of growth of producers. Hence, turfs have greater capacity to persist under near-future temperatures than the capacity for herbivores to counter their growth. These results suggest that whilst MTE predicts stronger top-down control, understanding whether consumer-producer responses are synchronous is key to assessing the future strength of top-down control. Nicole L. Mertens, Bayden D. Russell, Sean D. Connel...|$|R
40|$|Estimating power {{consumption}} {{is critical for}} OS process scheduling and for software/hardware developers. However, obtaining processor and system {{power consumption}} is non-trivial and when using simulators time consuming and prone to error. We analytically derive functions for realtime estimation of processor and system power <b>consumption</b> using performance <b>counter</b> data on real hardware. We develop our model based on data gathered from microbenchmarks custom written to capture possible application behavior. The model is independent of our test benchmarks, and therefore, well-suited for future applications. We target chip multi-processors, analyzing the effects of shared resources and temperature on power estimation. We leverage this information for a power-aware thread scheduler. ...|$|R
5000|$|The {{effects of}} {{weightlessness}} on human body were studied by NASA in the 1960s. Experiments demonstrated that weightlessness {{leads to a}} rapid bone depletion, so various remedies were sought to counter that. A number of pharmaceutical companies were asked to develop calcium supplements, but apparently {{none of them were}} as effective as clay. The special clay that was used in this case was Terramin, a reddish clay found in California. Dr. Benjamin Ershoff of the California Polytechnic Institute demonstrated that the <b>consumption</b> of clay <b>counters</b> the effects of weightlessness. He reported that [...] "the calcium in clay ...is absorbed more efficiently ... clay contains some factor or factors other than calcium which promotes improved calcium utilization and/or bone formation." [...] He added, [...] "Little or no benefit was noted when calcium alone was added to the diet." ...|$|R
40|$|Objective: Examine the {{influence}} of altering the size of snack food (ie, small vs large cookies) on short-term energy intake. Methods: First- and sixth-graders (n = 77) participated in a between-subjects experimental design. All participants were offered the same gram weight of cookies during an afternoon tea at their school. For half of the participants, food was cut in 2 to make the small item size. Food intake (number of cookies, gram weight, and energy intake) was examined using ANOVA. Results: Decreasing the item size of food led to a decrease of 25 % in gram weight intake, corresponding to 68 kcal. Appetitive ratings and subject and food characteristics had no moderating effect. Conclusions and Implications: Reducing the item size of food could prove a useful dietary prevention strategy based on decreased <b>consumption,</b> aimed at <b>countering</b> obesity-promoting eating behaviors favored by the easy availability of large food portions...|$|R
40|$|We {{consider}} the quantitative group testing problem where {{the objective is}} to identify defective items in a given population based on results of tests performed on subsets of the population. Under the quantitative group testing model, the result of each test reveals the number of defective items in the tested group. The minimum number of tests achievable by nested test plans was established by Aigner and Schughart in 1985 within a minimax framework. The optimal nested test plan offering this performance, however, was not obtained. In this work, we establish the optimal nested test plan in closed form. This optimal nested test plan is also order optimal among all test plans as the population size approaches infinity. Using heavy-hitter detection as a case study, we show via simulation examples orders of magnitude improvement of the group testing approach over two prevailing sampling-based approaches in detection accuracy and <b>counter</b> <b>consumption.</b> Other applications include anomaly detection and wideband spectrum sensing in cognitive radio systems...|$|R
40|$|Abstract—Energy cost {{becomes a}} major part of data center {{operational}} cost. Computer system consume more power when it runs under high workload. Many past studies focused on how to predict power <b>consumption</b> by performance <b>counters.</b> Some models retrieve performance counters from chips. Some models query performance counters from OS. Most of these researches were verified on several machines and claimed their models were accurate under the test. We found different servers have different energy consumption characters even with same CPU. In this paper, we present BFEPM, a best fit energy prediction model. It choose best model based on the power consumption benchmark result. We illustrate how to use benchmark result to find a best fit model. Then we validate the viability and effectiveness of model on all published results. At last, we apply the best fit model on two different machines to estimate the real-time energy consumption. The results show our model can get better results than single model. Keywords-energy consumption model; green computing; data center management; I...|$|R
50|$|The above table {{shows the}} flow of funds between {{different}} sectors for a closed economy with no explicit financial sector. The minus (-) sign in the table represents that the sector has paid out while the plus (+) sign indicates the receipts of that sector, e.g, -C for the household sector shows that the household has paid for their <b>consumption</b> whereas the <b>counter</b> party of this transaction is the firm which receives +C. This implies that the firms have received the payments from the households. Similarly, all the respective flows in the economy are reported in {{the flow of}} funds. More advanced SFC models consist of a financial sector including banks and is further extended to an open economy by introducing the Rest of World sector. Introducing the financial sector enables in tracing the flow of loans between the sectors, which in turn helps in determining the level of debt every sector holds. These models become more complicated as new sectors and assets are added to the system.|$|R
40|$|Copyright © Cambridge University Press 2015 The current {{generation}} of older people who are approaching or recently experiencing retirement form part of a unique generational habitus who have experienced a cultural shift into consumerism. These baby boomers are often portrayed as engaging in excessive levels of <b>consumption</b> which are <b>counter</b> to notions of sustainable living and to intergenerational harmony. This paper focuses on {{an exploration of the}} mechanisms underpinning the consumption patterns of baby boomers as they retire. We achieve this through an understanding of the everyday practices of grocery shopping which have the potential to give greater clarity to patterns of consumption than the more unusual or ‘extraordinary’ forms of consumption such as global travel. In-depth interviews with 40 older men and women in four locations across England and Scotland were conducted at three points in time across the period of retirement. We suggest that the grocery shopping practices of these older men and women were influenced by two factors: (a) parental values and upbringing leading to the reification of thrift and frugality as virtues, alongside aspirations for self-actualisation such as undertaking global travel, and (b) the influence of household context, and caring roles, on consumption choices. We conclude with some tentative observations concerning the implications of the ways baby boomers consume in terms of increasing calls for people to live in more sustainable ways...|$|R
40|$|Society’s {{increasing}} {{dependence on}} information technology {{has resulted in}} the deployment of vast compute resources. The energy costs of operating these resources coupled with environmental concerns have made power-aware computing one of the primary challenges for the IT sector. Making energy-efficient computing a rule rather than an exception requires that researchers and system designers use the right set of techniques and tools. These involve measuring, modeling, and characterizing the energy consumption of computers at varying degrees of granularity. In this thesis, we present techniques to measure power consumption of computer systems at various levels. We compare them for accuracy and sensitivity and discuss their effectiveness. We test Intel’s hardware power model for estimation accuracy and show that it is fairly accurate for estimating energy consumption when sampled at the temporal granularity of more than tens of milliseconds. We present a methodology to estimate per-core processor power <b>consumption</b> using performance <b>counter</b> and temperature-based power modeling and validate it across multiple platforms. We show our model exhibits negligible computation overhead, and the median estimation errors ranges from 0. 3 % to 10. 1 % for applications from SPEC 2006, SPEC-OMP and NAS benchmarks. We test the usefulness of the model in a meta-scheduler to enforce power constraint on a system. Finally, we perform a detailed performance and energy characterization of Intel’s Restricted Transactional Memory (RTM). We use TinySTM software transactional memory (STM) system to benchmark RTM’s performance against competing STM alternatives. We use microbenchmarks and STAMP benchmark suite to compare RTM versus STM performance and energy behavior. We quantify the RTM hardware limitations that affect its success rate. We show that RTM performs better than TinySTM when working-set fits inside the cache and that RTM is better at handling high contention workloads...|$|R
40|$|Diminishing {{performance}} {{returns and}} increasing power consumption of single-threaded processors have made chip multiprocessors (CMPs) an industry imperative. Unfortunately, low power efficiency and bottlenecks in shared hardware structures can prevent optimal use when running multiple sequential programs. Furthermore, for multithreaded programs, adding a core may harm performance and increase power consumption. To better use otherwise limitedly beneficial cores, software components such as hypervisors and operating {{systems can be}} provided with estimates of application performance and power consumption. They can use this information to improve system-wide performance and reliability. Estimating power consumption can also be useful for hardware and software developers. However, obtaining processor and system power consumption information can be nontrivial. First, we present a predictive approach for real-time, per-core power estimation on a CMP. We analytically derive functions for real-time estimation of processor and system power <b>consumption</b> using performance <b>counter</b> and temperature data on real hardware. Our model uses data gathered from microbenchmarks that capture potential application behavior. The model is independent of our test benchmarks, and thus we {{expect it to be}} well suited for future applications. For chip multiprocessors, we achieve median error of 3. 8 % on an AMD quad-core CMP, 2. 0 % on an Intel quad-core CMP, and 2. 8 % on an Intel eight-core CMP. We implement the same approach inside an Intel XScale simulator and achieve median error of 1. 3 %. Next, we introduce and evaluate an approach to throttling concurrency in parallel programs dynamically. We throttle concurrency to levels with higher predicted efficiency using artificial neural networks (ANNs). One advantage of using ANNs over similar techniques previously explored is that the training phase is greatly simplified, thereby reducing the burden on the end user. We effectively identify energy efficient concurrency levels in multithreaded scientific applications on an Intel quad-core CMP. We improve the energy efficiency for many of our applications by predicting more favorable number and placement of threads at runtime, and improve the average ED 2 by 17. 2 % and 22. 6 % on an Intel quad-core and an Intel eight-core CMP, respectively. Last, we propose a framework that combines both approaches. With the impending shift to many-core architectures, systems need information on power and energy for more energy-efficient use of all cores. Any approach utilizing this framework also needs to be scalable to many cores. We implement an infrastructure that can schedule for power efficiency for a given power envelope, and/or a given thermal envelope. We expect the framework to scale well with number of cores. We perform experiments on quad-core and eight-core platforms. We schedule for better power efficiency by suspending or slowing down (via DVFS) single-threaded programs, or throttling concurrency for multithreaded programs. We utilize the per-core power predictor to schedule applications to remain under a given power envelope. We modify the scheduler policies to take advantage of all power saving options to enforce the power envelope, while minimizing performance loss...|$|R
40|$|Lilja (2005) {{states that}} “In {{the field of}} {{computer}} science and engineering there is surprisingly little agreement on how to measure something as fun- damental as {{the performance of a}} computer system. ”. The field lacks of the most fundamental element for sharing measures and results: an appropriate metric to express performance. Since the introduction of laptops and mobile devices, there has been a strong research focus towards the energy efficiency of hardware. Many papers, both from academia and industrial research labs, focus on methods and ideas to lower power consumption in order to lengthen the battery life of portable device components. Much less effort has been spent on defining the responsibility of software in the overall computational system energy consumption. Some {{attempts have been made to}} describe the energy behaviour of software, but none of them abstract from the physical machine where the measurements were taken. In our opinion this is a strong drawback because results can not be generalized. In this work we attempt to bridge the gap between characterization and prediction, of both hardware and software, of performance and energy, in a single unified model. We propose a model designed to be as simple as possible, generic enough to be abstract from the specific resource being described or predicted (applying to both time, memory and energy), but also concrete and practical, allowing useful and precise performance and energy predictions. The model applies to the broadest set of resource possible. We focus mainly on time and memory (hence bridging hardware benchmarking and classical algorithms time complexity), and energy consumption. To ensure a wide applicability of the model in real world scenario, the model is completely black-box, it does not require any information about the source code of the program, and only relies on external metrics, like completion time, energy <b>consumption,</b> or performance <b>counters.</b> Extending the benchmarking model, we define the notion of experimental computational complexity, as the characterization of how the resource usage changes as the input size grows. Finally, we define a high-level energy model capable of characterizing the power consumption of computers and clusters, in terms of the usage of resources as defined by our benchmarking model. We tested our model in four experiments: Expressiveness: we show the close relationship between energy and clas- sical theoretical complexity, also showing that our experimental com- putational complexity is expressive enough to capture interesting be- haviour of programs simply analysing their resource usage. Performance prediction we use the large database of performance mea- sures available on the CPU SPEC website to train our model and predict the performance of the CPU SPEC suite on randomly selected computers. Energy profiling: we tested our model to characterize and predict the power usage of a cluster running OpenFOAM, changing the number of active nodes and cores. Scheduling: on heterogeneous systems applying our performance pre- diction model to features of programs extracted at runtime, we predict the device where is most convenient to execute the programs, in an heterogeneous system...|$|R

