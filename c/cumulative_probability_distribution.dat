193|10000|Public
2500|$|Let [...] and [...] be {{respectively}} the <b>cumulative</b> <b>probability</b> <b>distribution</b> function and the probability density {{function of the}} N(0,1) distribution.|$|E
2500|$|The {{central limit theorem}} applies in {{particular}} to sums of independent and identically distributed discrete random variables. [...] A sum of discrete random variables is still a discrete random variable, so that we are confronted with a sequence of discrete random variables whose <b>cumulative</b> <b>probability</b> <b>distribution</b> function converges towards a <b>cumulative</b> <b>probability</b> <b>distribution</b> function corresponding to a continuous variable (namely that of the normal distribution). [...] This means that if we build a histogram of the realisations of the sum of [...] independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as [...] approaches infinity, this relation is known as de Moivre–Laplace theorem. The binomial distribution article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.|$|E
2500|$|More abstractly, {{given two}} <b>cumulative</b> <b>probability</b> <b>distribution</b> {{functions}} [...] and , with associated quantile functions [...] and [...] (the inverse {{function of the}} CDF is the quantile function), the Q–Q plot draws the -th quantile of [...] against the -th quantile of [...] {{for a range of}} values of [...] [...] Thus, the Q–Q plot is a parametric curve indexed over [...] with values in the real plane [...]|$|E
30|$|The all-RTT <b>cumulative</b> <b>probability</b> <b>distributions</b> for Wi-Fi, Wi-Fi Direct, and Bluetooth {{are shown}} in Figs.  4,  5, and  6, respectively.|$|R
5000|$|... #Caption: Different <b>cumulative</b> normal <b>probability</b> <b>distributions</b> {{with their}} {{parameters}} ...|$|R
3000|$|... [...]. The <b>cumulative</b> <b>probability</b> density <b>distribution</b> {{represented}} by Eq. (16) {{is referred to}} as a fragility function.|$|R
5000|$|... #Caption: Histogram {{derived from}} the adapted <b>cumulative</b> <b>probability</b> <b>distribution</b> ...|$|E
5000|$|... calculates a <b>cumulative</b> <b>probability</b> <b>distribution</b> of {{the given}} numbers.|$|E
50|$|N(d) is <b>cumulative</b> <b>probability</b> <b>distribution</b> {{function}} for a standard normal distribution.|$|E
40|$|This paper reports work in {{progress}} on the analysis of radio propagation data measured at 28. 5 GHz and 29. 5 GHz in a suburban-type residential area in Ottawa. Both CW and wideband measurements were made in several radio propagation surveys. Issues discussed include: propagation loss in cluttered environments; <b>cumulative</b> <b>probability</b> <b>distributions</b> (CDFs) for excess loss; cross polarisation isolation, and temporal dispersion caused by multipath propagation...|$|R
40|$|Rice {{production}}, as {{with any}} other production activity in agriculture, is risky. Rice producers in Argentina apply several risk management strategies to decrease the impact of risk. However, {{the effects of these}} strategies on reducing risk have not yet been quantified. This investigation aims to provide a firmer basis for analysing the risky agricultural environment, farmers' risk perceptions, and management responses to control risk. The specific objectives are to: 1) ascertain the rice farmers' perceptions about the relative importance of different sources of risk; 2) establish the risk responses that rice farmers implement; 3) determine the influence of the size of farms on farmers' perceptions of the risk sources and on the nature of their risk responses; and 4) evaluate the risk reducing effects of the farmers' responses on the gross margins of the rice enterprise. In this research, 12 rice farmers in the counties of Mercedes and Curuzu Cuatia in the province of Corrientes were interviewed to ascertain their perceptions of the sources of risk affecting their businesses and their responses to risks. The second part of the interview sought to establish the costs of production, prices, and yields possibly obtained by implementing five marketing and production strategies in order to develop a gross margin stochastic model. Gross margin stochastic models for each strategy and for each farm were simulated. The outputs of the simulations comprised two <b>cumulative</b> <b>probability</b> <b>distributions</b> for each strategy, one representing the <b>cumulative</b> <b>probability</b> of obtaining different gross margins using a specific risk management strategy and the other representing the <b>cumulative</b> <b>probability</b> of obtaining different gross margins not using that specific strategy. Each pair of these <b>cumulative</b> <b>probability</b> <b>distributions</b> was compared by using visual stochastic dominance analysis, thus determining which <b>cumulative</b> <b>probability</b> <b>distributions</b> were more efficient in reducing risk. Efficient <b>cumulative</b> <b>probability</b> <b>distributions</b> were then compared, obtaining the efficient set of cumulative distributions for each farm. This research demonstrated that 'changes in the economic and political situation in Brazil' is the most important source of risk for farmers in the study area and that the importance of this source holds regardless the size of the rice farms. This research also suggested that farmers' perceptions of the sources of risk differ according {{to the size of the}} farms. For example, rice farmers' level of concern with production risks increases as the farm size decreases. Farmer's responses to risks also vary according to farm size; for instance, the number of strategies used by rice farmers decreases as the area of the farms falls. This research indicated that many strategic responses reduced risk, some of these responses reduced risk effects more than others...|$|R
40|$|AbstractWe apply belief {{functions}} {{to an analysis}} of future climate change. It is shown that the lower envelope of a set of <b>probabilities</b> bounded by <b>cumulative</b> <b>probability</b> <b>distributions</b> is a belief function. The large uncertainty about natural and socio-economic factors influencing estimates of future climate change is quantified in terms of bounds on <b>cumulative</b> <b>probability.</b> This information is used to construct a belief function for a simple climate change model, which then is projected onto an estimate of global mean warming in the 21 st century. Results show that warming estimates on this basis can generate very imprecise uncertainty models...|$|R
5000|$|... #Caption: Cumulative {{frequency}} distribution, adapted <b>cumulative</b> <b>probability</b> <b>distribution,</b> {{and confidence}} intervals ...|$|E
5000|$|Let [...] and [...] be {{respectively}} the <b>cumulative</b> <b>probability</b> <b>distribution</b> function and the probability density {{function of the}} N(0,1) distribution.|$|E
5000|$|... be the <b>cumulative</b> <b>probability</b> <b>distribution</b> {{function}} of the minimum value of the [...] function on interval [...] conditioned by the value ...|$|E
50|$|Probability bounds {{analysis}} (PBA) is {{a collection}} of methods of uncertainty propagation for making qualitative and quantitative calculations in the face of uncertainties of various kinds. It is used to project partial information about random variables and other quantities through mathematical expressions. For instance, it computes sure bounds on the distribution of a sum, product, or more complex function, given only sure bounds on the distributions of the inputs. Such bounds are called probability boxes, and constrain <b>cumulative</b> <b>probability</b> <b>distributions</b> (rather than densities or mass functions).|$|R
40|$|The assumed {{situation}} is as follows: two cumulative distribution functions F and G are in constant (unknown) ratio θ, for {{values of the}} random variables below t* (assumed known); F and G may behave entirely independently above t*. Observations on a sample from F are then used to obtain unbiased maximum likelihood estimates of and of other unknown parameters of G, denoted by λ. The estimators have application in problems of stochastic dominance {{in the context of}} portfolio analysis and in other situations of intertwining <b>cumulative</b> <b>probability</b> <b>distributions.</b> ...|$|R
40|$|A {{financial}} accounting model that incorporates physical and institutional uncertainties {{was developed for}} geothermal projects. Among the uncertainties it can handle are well depth, flow rate, fluid temperature, and permit and construction times. The outputs of the model are <b>cumulative</b> <b>probability</b> <b>distributions</b> of financial measures such as capital cost, levelized cost, and profit. These outputs are well suited for use in an investment decision incorporating risk. The model has the powerful feature that conditional <b>probability</b> <b>distribution</b> {{can be used to}} account for correlations among any of the input variables. The model has been applied to a geothermal reservoir at Heber, California, for a 45 -MW binary electric plant. Under the assumptions made, the reservoir appears to be economically viable...|$|R
5000|$|In {{the case}} when the {{variables}} [...] are independent and identically distributed with <b>cumulative</b> <b>probability</b> <b>distribution</b> function [...] for all i the theorem reduces to ...|$|E
5000|$|Regardless of continuity-versus-discreteness {{and related}} issues, if {{one knows the}} <b>cumulative</b> <b>probability</b> <b>distribution</b> {{function}} FX (but not Fg(X)), then the expected value of g(X) is given by a Riemann - Stieltjes integral ...|$|E
50|$|The crucial idea of rank-dependent {{expected}} utility was to overweigh only unlikely extreme outcomes, {{rather than}} all unlikely events. Formalising this insight required transformations {{to be applied}} to the <b>cumulative</b> <b>probability</b> <b>distribution</b> function, rather than to individual probabilities (Quiggin, 1982, 1993).|$|E
40|$|<b>Cumulative</b> <b>probability</b> <b>distributions</b> {{of income}} for {{management}} scenarios involving four pre-harvest marketing strategies are subjected to stochastic dominance analysis to determine risk-efficient sets of strategies for different groups of farmers in North Florida. Results indicate that farmers should behave differently in their choice of marketing strategies according to their risk attitudes. Highly risk-averse farmers should prefer some forward contracting while low risk-averse and risk loving farmers should prefer cash sales at harvest. Use of the futures markets leads to both higher income and greater risk than forward contracting but lower income and risk than cash sales. Agribusiness,...|$|R
40|$|Entropy is a {{fundamental}} measure of information content which has been applied {{in a wide variety}} of fields. We present three applications of entropy in the industrial engineering field: dispatching of Automatic Guided Vehicles (AGV), ranking and selection of simulated systems based on the mean performance measure, and comparison between random variables based on <b>cumulative</b> <b>probability</b> <b>distributions.</b> The first application proposes three entropy-based AGV dispatching algorithms. We contribute to the body of knowledge by considering the consequence of potential AGV moves on the load balance of the factory before AGVs are dispatched. Kullback-Leibler directed divergence is applied to measure the divergence between load distribution after each potential move and load distribution of a balanced factory. Simulation experiment...|$|R
40|$|The {{properties}} of ceramic matrix composites (CMC’s) {{are known to}} display {{a considerable amount of}} scatter due to variations in fiber/matrix properties, interphase properties, interphase bonding, amount of matrix voids, and many geometry- or fabrication-related parameters, such as ply thickness and ply orientation. This paper summarizes preliminary studies in which formal probabilistic descriptions of the material-behavior- and fabrication-related parameters were incorporated into micromechanics and macromechanics for CMC’s. In this process two existing methodologies, namely CMC micromechanics and macromechanics analysis and a fast probability integration (FPI) technique are synergistically coupled to obtain the probabilistic composite behavior or response. Preliminary results in the form of <b>cumulative</b> <b>probability</b> <b>distributions</b> and information on the probability sensitivitie...|$|R
5000|$|... holds if g is any <b>cumulative</b> <b>probability</b> <b>distribution</b> {{function}} on {{the real}} line, no matter how ill-behaved. In particular, no matter how ill-behaved the cumulative distribution function g of a random variable X, if the moment E(Xn) exists, then it is equal to ...|$|E
5000|$|M/M/1 Model (Single Queue Single Server/ Markovian): In this model, {{elements}} of queue are {{served on a}} first-come, first-served basis. Given the mean arrival and service rates, then actual rates vary around these average values randomly and hence have to be determined using a <b>cumulative</b> <b>probability</b> <b>distribution</b> function.|$|E
50|$|P-boxes are {{specified}} by {{left and right}} bounds on the <b>cumulative</b> <b>probability</b> <b>distribution</b> function (or, equivalently, the survival function) of a quantity and, optionally, additional information about the quantity’s mean, variance and distributional shape (family, unimodality, symmetry, etc.). A p-box represents a class of probability distributions consistent with these constraints.|$|E
40|$|Recent {{literature}} in the last Maximum Entropy workshop introduced an analogy between <b>cumulative</b> <b>probability</b> <b>distributions</b> and normalized utility functions. Based on this analogy, a utility density function can de defined as the derivative of a normalized utility function. A utility density function is non-negative and integrates to unity. These two properties {{form the basis of}} a correspondence between utility and probability. A natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values. Maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment, and helps assign utility values based on partial preference information. This paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility...|$|R
40|$|Empirical {{distribution}} {{functions for}} one-minute average rain-rate values were compiled for three station year of observations at Nigeria environmental and climatic observatory (NECOP) propagation experiment terminal sites. The empirical distribution functions {{were compared with}} cumulative distribution functions generated using four different rain-rate distribution models. It is found that although each of the models shows similar qualitative features at lower exceedance of time, the characteristic at higher time percentages shows quantitative difference from the experimental data except the improved version of Moupfouma model. The results further show that the rainfall rate and the microwave propagation characteristics in this region are out of accord with International Telecommunication Union predictions. This information is vital for predicting rain fading <b>cumulative</b> <b>probability</b> <b>distributions</b> over this region...|$|R
40|$|Cumulative-binomial {{computer}} program, CROSSER, one of set {{of three}} programs, calculates <b>cumulative</b> binomial <b>probability</b> <b>distributions</b> for arbitrary inputs. CROSSER, CUMBIN (NPO- 17555), and NEWTONP (NPO- 17556), used independently of one another. Point of equality between reliability of system and common reliability of components found. Used by statisticians and users of statistical procedures, test planners, designers, and numerical analysts. Program written in C...|$|R
5000|$|If g is the <b>cumulative</b> <b>probability</b> <b>distribution</b> {{function}} of a random variable X that has a probability density function with respect to Lebesgue measure, and f is any function for which the expected value E(|f(X)|) is finite, then the probability density {{function of}} X is the derivative of g and we have ...|$|E
5000|$|The {{central limit theorem}} applies in {{particular}} to sums of independent and identically distributed discrete random variables. A sum of discrete random variables is still a discrete random variable, so that we are confronted with a sequence of discrete random variables whose <b>cumulative</b> <b>probability</b> <b>distribution</b> function converges towards a <b>cumulative</b> <b>probability</b> <b>distribution</b> function corresponding to a continuous variable (namely that of the normal distribution). This means that if we build a histogram of the realisations of the sum of [...] independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as [...] approaches infinity, this relation is known as de Moivre-Laplace theorem. The binomial distribution article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.|$|E
5000|$|If f is a {{probability}} density function, then {{the value of}} the integral above is called the -th moment of the probability distribution. More generally, if F is a <b>cumulative</b> <b>probability</b> <b>distribution</b> function of any probability distribution, which may not have a density function, then the -th moment of the probability distribution is given by the Riemann-Stieltjes integral ...|$|E
40|$|A {{procedure}} is presented that expresses pollutant movement in groundwater as a <b>probability</b> <b>distribution.</b> <b>Probability</b> is {{introduced into the}} pollutant movement solution process by using the Turning Bands Method (TBM), a spectral analysis model. The TBM generates a selected number of spatially-varying, correlated hydraulic conductivity fields, {{all of which have}} the same descriptive statistics, i. e., mean, standard deviation and correlation function. A groundwater flow-mass transport model developed by the Illinois State Water Survey is used to predict pollution concentration as a function of space and time for a specified groundwater contamination event. The groundwater flow-mass transport model develops a set of pollution movement estimates, one for each hydraulic conductivity field generated by the TBM. These realizations of pollutant movement are described first as frequency distributions and eventually as <b>cumulative</b> exceedance <b>probability</b> distributions;This {{procedure is}} applied to a documented field situation at the Idaho National Engineering Laboratory (INEL). Disposal of tritium in low-level radioactive and chemical wastes from the Idaho Chemical Processing Plant (ICPP) into the Snake River Plain aquifer through a deep injection well was recorded from 1976 through 1982. The movement of the tritium plume has been observed at several monitoring sites. TBM and groundwater flow-mass transport model parameters are determined in a calibration process with the recorded INEL data. One hundred realizations of a tritium disposal event during 1979 and 1980 at the INEL are used to generate <b>cumulative</b> exceedance <b>probabilities</b> of time of arrival and tritium concentration versus duration of exposure at an observation well downstream of the ICPP injection well;The observed <b>cumulative</b> exceedance <b>probability</b> <b>distributions</b> of time of arrival and conditional and marginal <b>cumulative</b> exceedance <b>probability</b> <b>distributions</b> of each tritium concentration and duration of exposure are fitted to the Pearson Type III distribution. A logit transformation is used to obtain a simple mathematical equation representing the joint <b>cumulative</b> exceedance <b>probability</b> <b>distribution</b> of tritium concentration and duration of exposure. Finally, the pertinence of these types of probability results to toxicity studies and benefit-cost analysis is discussed...|$|R
40|$|A circuit is {{described}} for {{the measurement of}} the <b>cumulative</b> amplitude <b>probability</b> <b>distribution</b> of electrical signals. Details are given {{of the use of}} the apparatus in noise and aerodynamic turbulence measurements. The input level is 0. 6 volts peak-to-peak (100 mV R. M. S. with a Gaussian <b>probability</b> <b>distribution),</b> and the frequency response is approximately 10 c/s to 70 kc/s. The output signal is obtained as a train of 3 volt negative-going pulses which are integrated on a galvanometer...|$|R
30|$|The <b>cumulative</b> <b>probability</b> <b>distributions</b> of {{the wind}} speed at all the study {{locations}} (Figure 1 b) show a similar trend. The cumulative distribution function {{can be used for}} estimating the time for which wind speed is within a certain speed interval. For wind speeds greater or equal to 2.5 m/s cut-in wind speed, Enugu, Owerri and Onitsha have frequencies of about 96.9 %, 86.5 % and 86.9 %, respectively, while the same locations respectively have frequencies of about 88.4 %, 44.7 % and 55.3 % for wind speed of 3.5 m/s cut-in wind speed. According to Ojosu and Salawu [3], if a wind turbine system with a design cut-in wind speed of 2.2 m/s is used in these sites for wind energy resource for electricity generation, all the sites will have frequencies of more than 92 %.|$|R
