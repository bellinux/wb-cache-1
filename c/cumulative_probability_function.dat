37|10000|Public
40|$|A Multivariate Weibull Distribution A multivariate {{survival}} function of Weibull Distribution is developed by expanding the theorem by Lu and Bhattacharyya. From the {{survival function}}, the probability density function, the <b>cumulative</b> <b>probability</b> <b>function,</b> the determinant of the Jacobian Matrix, {{and the general}} moment are derived...|$|E
40|$|A multivariate {{survival}} function of Weibull Distribution is developed by expanding the theorem by Lu and Bhattacharyya (1990). From the {{survival function}}, the probability density function, the <b>cumulative</b> <b>probability</b> <b>function,</b> the determinant of the Jacobian Matrix, {{and the general}} moment are derived. The proposed model is also applied to the tumor appearance data of female rats...|$|E
40|$|We {{study the}} volume {{distribution}} of nodal domains of random band-limited functions on generic manifolds, {{and find that}} in the high energy limit a typical instance obeys a deterministic universal law, independent of the manifold. Some of the basic qualitative properties of this law, such as its support, monotonicity and continuity of the <b>cumulative</b> <b>probability</b> <b>function,</b> are established. Comment: 32 page...|$|E
50|$|N(d) is <b>cumulative</b> <b>probability</b> {{distribution}} <b>function</b> for {{a standard}} normal distribution.|$|R
2500|$|Let [...] and [...] be {{respectively}} the <b>cumulative</b> <b>probability</b> distribution <b>function</b> and the <b>probability</b> density <b>function</b> of the N(0,1) distribution.|$|R
5000|$|... be the <b>cumulative</b> <b>probability</b> {{distribution}} <b>function</b> of {{the minimum}} value of the [...] function on interval [...] conditioned by the value ...|$|R
40|$|In {{this note}} we are {{concerned}} with the sums S=Y 1 +Y 2 + [...] . +Yn, where every constituent follows the negative binomial distribution with arbitrary parameters. We derive the exact probability mass function and the <b>cumulative</b> <b>probability</b> <b>function</b> of S. We also show that one can relate to the distribution of S as a mixture negative binomial distribution. Negative binomial distributions Series representations...|$|E
40|$|The paper {{presents}} {{a study on}} water quality concerning nutrients {{in relation to the}} hydrological and meteorological conditions in three agricultural catchments in Latvia. Statistical analysis, i. e., descriptive statistics, Kolmogorov–Smirnov test, Mann-Whitney U test, Spearman’s Rank–Order correlation and <b>cumulative</b> <b>probability</b> <b>function</b> has been used to quantify relationships between variables. Results of the study could be used to evaluate the impact of agricultural intensity on water quality...|$|E
40|$|In this paper, {{we propose}} {{a method for}} the {{parameter}} estimation of a Weibull distribution. It {{is based on the}} estimates of the <b>cumulative</b> <b>probability</b> <b>function.</b> The estimates, obtained by applying the proposed methodology are compared with the results obtained by several literature estimators. In particular, we then enlarge the method to samples showing outliers. Such a method could be employed in mechanical applications or time series analysis...|$|E
50|$|Mathematically {{it is the}} <b>cumulative</b> <b>probability</b> density <b>function</b> of {{the surface}} profile's height and can be {{calculated}} by integrating the profile trace.|$|R
5000|$|In {{the case}} when the {{variables}} [...] are independent and identically distributed with <b>cumulative</b> <b>probability</b> distribution <b>function</b> [...] for all i the theorem reduces to ...|$|R
5000|$|Regardless of continuity-versus-discreteness {{and related}} issues, if {{one knows the}} <b>cumulative</b> <b>probability</b> {{distribution}} <b>function</b> FX (but not Fg(X)), then the expected value of g(X) is given by a Riemann - Stieltjes integral ...|$|R
40|$|Due to the {{complexity}} of system and lack of expertise, epistemic uncertainties may present in the experts’ judgment on the importance of certain indices during group decision-making. A novel combination weighting method is proposed to solve the index weighting problem when various uncertainties are present in expert comments. Based on the idea of evidence theory, various types of uncertain evaluation information are uniformly expressed through interval evidence structures. Similarity matrix between interval evidences is constructed, and expert’s information is fused. Comment grades are quantified using the interval number, and <b>cumulative</b> <b>probability</b> <b>function</b> for evaluating the importance of indices is constructed based on the fused information. Finally, index weights are obtained by Monte Carlo random sampling. The method can process expert’s information with varying degrees of uncertainties, which possesses good compatibility. Difficulty in effectively fusing high-conflict group decision-making information and large information loss after fusion is avertible. Original expert judgments are retained rather objectively throughout the processing procedure. <b>Cumulative</b> <b>probability</b> <b>function</b> constructing and random sampling processes do not require any human intervention or judgment. It can be implemented by computer programs easily, thus having an apparent advantage in evaluation practices of fairly huge index systems...|$|E
40|$|It is {{well known}} that the {{classical}} measures of skewness are not reliable and their sampling distributions are not known for small samples. Therefore, we consider the modified measure of skewness that is defined in terms of <b>cumulative</b> <b>probability</b> <b>function.</b> The main advantage of this measure is that its sampling distribution is derived from sample data as the sum of dependent Bernoulli random variables. Moreover, its variance and confidence interval are obtained based on multiplicative binomial distribution. Comparison with classical measures using simulation and an application to actual data set are given...|$|E
40|$|Abstract:- In {{this paper}} {{a method for}} the {{parameter}} estimation of a Weibull distribution is explained. It {{is based on the}} estimates of the <b>cumulative</b> <b>probability</b> <b>function</b> performed by a wavelet method. The estimates, obtained by the application of the proposed method, are compared with the results based on other literature estimators. In particular, it is shown that the estimates are stable also when samples show outliers. Such a method could be utilised in mechanical applications (e. g. transmissions, rolling organs, kinematics couples effort subjected) or time series analysis. Key-Words:- Wavelet analysis, numerical analysis, Weibull distribution, parameter estimation, mechanical lifetime. ...|$|E
3000|$|... where x [...] t [...] and x [...] t+ 1 are {{the upper}} and lower bounds of the t th time interval, respectively; ψ [...] t [...] is the <b>cumulative</b> <b>probability</b> of the t th time interval; F(·) and f(·) are the <b>cumulative</b> <b>probability</b> density <b>function</b> and the <b>probability</b> density <b>function,</b> respectively.|$|R
5000|$|If g is the <b>cumulative</b> <b>probability</b> {{distribution}} <b>function</b> of {{a random}} variable X {{that has a}} <b>probability</b> density <b>function</b> with respect to Lebesgue measure, and f is any function for which the expected value E(|f(X)|) is finite, then the <b>probability</b> density <b>function</b> of X is the derivative of g and we have ...|$|R
50|$|The crucial idea of rank-dependent {{expected}} utility was to overweigh only unlikely extreme outcomes, {{rather than}} all unlikely events. Formalising this insight required transformations {{to be applied}} to the <b>cumulative</b> <b>probability</b> distribution <b>function,</b> rather than to individual probabilities (Quiggin, 1982, 1993).|$|R
40|$|We are {{focusing}} on three alternative techniques {{that can be used}} to empirically select predictors for failure prediction purposes. The selected techniques have all different assumptions about the relationships between the independent variables. Linear discriminant analysis is based on linear combination of independent variables, logit analysis uses the logistic <b>cumulative</b> <b>probability</b> <b>function</b> and genetic algorithms is a global search procedure based on the mechanics of natural selection and natural genetics. Our aim is to study if these essential differences between the methods (1) affect the empirical selection of independent variables to the model and (2) lead to significant differences in failure prediction accuracy...|$|E
30|$|The {{uncertainty}} is quantified {{through a large}} number of equiprobable reservoir stochastic images (realizations) with different sets of dynamic data to obtain a flow response distribution through the <b>Cumulative</b> <b>Probability</b> <b>Function</b> (CPF). In uncertainty assessment, nine quantiles, which represent a range of realizations from less-likely to most-likely, are collected based on a probability distribution to generate the same range of the calculated field cumulative oil production and/or net present value (Liu et al. 2001). As a result, uncertainty quantification includes three main steps in integrated reservoir simulation studies through experimental design approaches: sensitivity analysis for parameters screening, reservoir modeling, and Monte Carlo simulation (Carpenter 2013).|$|E
40|$|A b s t r a c t We are {{focusing}} on three alternative techniques {{that can be used}} to empirically select predictors for failure prediction purposes. The selected techniques have all different assumptions about the relationships between the independent variables. Linear discriminant analysis is based on linear combination of independent variables, logit analysis uses the logistic <b>cumulative</b> <b>probability</b> <b>function</b> and genetic algorithms is a global search procedure based on the mechanics of natural selection and natural genetics. Our aim is to study if these essential differences between the methods (1) affect the empirical selection of independent variables to the model and (2) lead to significant differences in failure prediction accuracy...|$|E
40|$|In this work, {{we develop}} a {{statistical}} methodology to characterize solar radiation and ambient temperature from real measurements. These parameters determine {{the behavior of}} solar energy in a particular location and they are essential to establish the performance of different systems that use this type of energy, such as photovoltaic or solar-thermal systems. Since these ambient parameters have a random behavior and they cannot be controlled by human intervention over the earth's surface, a methodology to obtain a probability density distribution for twelve (12) hours a day (from 6 : 00 a. m to 6 : 00 p. m.) was built so as to predict solar energy behavior. From these density <b>functions,</b> corresponding <b>cumulative</b> <b>probability</b> <b>functions</b> are calculated. In the cases where variables cannot be deterministically determined, a numerical-best polynomial representation is found. The <b>cumulative</b> <b>probability</b> <b>functions</b> obtained in this work {{can be used as}} a basis to constructing statically reliable predictions of performance for different solar energy-based systems. These systems may serve as a first energy resource by using stochastic methods like Montecarlo simulations. As an example, in this paper we take data from measurement campaigns conducted in Bogotá to explain the aforementioned methodology. Finally, expressions that characterize the corresponding radiation and ambient temperature for each hour (from a statistically reliable random number between 0 and 1) are shown...|$|R
5000|$|... holds if g is any <b>cumulative</b> <b>probability</b> {{distribution}} <b>function</b> on {{the real}} line, no matter how ill-behaved. In particular, no matter how ill-behaved the cumulative distribution function g of a random variable X, if the moment E(Xn) exists, then it is equal to ...|$|R
40|$|Raftery (1993) has {{suggested}} that project cost estimates be presented {{in the form of}} <b>cumulative</b> <b>probability</b> <b>functions</b> (Raftery Curves) rather than the current practice of single point estimates. This paper compares the Raftery Curves derived empirically for ten data sets gathered throughout the world. The most consistent feature of all the curves was found to be the shift associated with to the number of bidders entering bids for contracts. This is examined both in terms of bias and consistency. Contrary to some previous studies, no evidence was found of any trends related to the value size of projects...|$|R
40|$|We {{discuss the}} {{formation}} of large amplitude waves for sea states characterized by JONSWAP spectra with random phases. In this context we discuss experimental results performed {{in one of the}} largest wave tank facilities in the world. We present experimental evidence that the tail of the <b>cumulative</b> <b>probability</b> <b>function</b> of the wave heights for random waves strongly depends on the ratio between the wave steepness and the spectral bandwidth. When this ratio, called the Benjamin-Feir Index, is large the Rayleigh distribution clearly underestimates the occurrence of large amplitude waves. Our experimental results are also successfully compared with previously performed numerical simulations of the Dysthe equation. Comment: 10 pages, 9 figure...|$|E
40|$|We {{investigate}} the translocation {{of a single}} stranded DNA (ssDNA) through a pore which fluctuates between two conformations, using coupled master equations (ME). The probability density function (pdf) of the first passage times (FPT) of the translocation process is calculated, displaying a triple, double or mono-peaked behavior, depending on the interconversion rates between the conformations, the applied electric field, and the initial conditions. The <b>cumulative</b> <b>probability</b> <b>function</b> (cpf) of the FPT, in a field-free environment, is shown to have two regimes, characterized by fast and slow timescales. An analytical expression for the mean first passage time (MFPT) of the translocation process is derived, and provides, {{in addition to the}} interconversion rates, an extensive characterization of the translocation process...|$|E
40|$|The recent {{paper by}} Yacoub et al. [1] {{introduces}} what {{is referred to}} as the η – κ distribution to describe the statistical variation of the envelope in a fast fading environment. The paper discusses several properties of the distribution. Two of the properties discussed are the nth moment, E(Pn), and the <b>cumulative</b> <b>probability</b> <b>function</b> (cpf), FPP (•), where P is a random variable representing the normalized envelope. The expression given for E(Pn) (see equation (10) in Yacoub et al. [1]) is a doubly infinite sum of the Gauss hypergeometric function (which, itself, is an infinite sum). That given for FP (•) (see equation (11) in Yacoub et al. [1]) is a triple sum of the incomplete gamma function...|$|E
5000|$|If f is a <b>probability</b> density <b>function,</b> {{then the}} value of the {{integral}} above is called the -th moment of the probability distribution. More generally, if F is a <b>cumulative</b> <b>probability</b> distribution <b>function</b> of any <b>probability</b> distribution, which may not have a density function, then the -th moment of the probability distribution is given by the Riemann-Stieltjes integral ...|$|R
5000|$|M/M/1 Model (Single Queue Single Server/ Markovian): In this model, {{elements}} of queue are {{served on a}} first-come, first-served basis. Given the mean arrival and service rates, then actual rates vary around these average values randomly and hence have to be determined using a <b>cumulative</b> <b>probability</b> distribution <b>function.</b>|$|R
50|$|P-boxes are {{specified}} by {{left and right}} bounds on the <b>cumulative</b> <b>probability</b> distribution <b>function</b> (or, equivalently, the survival function) of a quantity and, optionally, additional information about the quantity’s mean, variance and distributional shape (family, unimodality, symmetry, etc.). A p-box represents a class of probability distributions consistent with these constraints.|$|R
30|$|Typical {{values of}} spatial {{correlation}} lengths, Θ, for soils as given e.g. by Li et al. [19] {{are in the}} range 0.1 to 5  m for Θy and from 2 to 30  m for Θx. Based on the subsurface condition in the project and some specific regulations in the region of interest (e.g., the minimum distance between the boreholes must be at least 15  m based on the uniformity level of subsurface soils), the average spatial correlation length for this study is assumed to be 10  m. The characteristic length, L, was taken as 30  m, which is based on analyses investigating potential failure mechanisms for this problem. Because of considering spatial correlation (Fig.  3), the discrete <b>cumulative</b> <b>probability</b> <b>function</b> becomes steeper.|$|E
40|$|ABSTRACT We {{investigate}} the translocation of a single-stranded DNA through a pore which fluctuates between two conformations, using coupled master equations. The probability density {{function of the}} first passage times of the translocation process is calculated, displaying a triple-, double-, or monopeaked behavior, depending on the interconversion rates between the conformations, the applied electric field, and the initial conditions. The <b>cumulative</b> <b>probability</b> <b>function</b> of the first passage times, in a field-free environment, is shown to have two regimes, characterized by fast and slow timescales. An analytical expression for the mean first passage time of the translocation process is derived, and provides, {{in addition to the}} interconversion rates, an extensive characterization of the translocation process. Relationships to experimental observations are discussed...|$|E
40|$|We {{investigate}} the translocation {{of a single}} stranded DNA through a pore which fluctuates between two conformations, using coupled master equations. The probability density function of the first passage times (FPT) of the translocation process is calculated, displaying a triple, double or mono peaked behavior, depending on the interconversion rates between the conformations, the applied electric field, and the initial conditions. The <b>cumulative</b> <b>probability</b> <b>function</b> of the FPT, in a field-free environment, is shown to have two regimes, characterized by fast and slow timescales. An analytical expression for the mean first passage time of the translocation process is derived, and provides, {{in addition to the}} interconversion rates, an extensive characterization of the translocation process. Relationships to experimental observations are discussed. Comment: 8 pages, 5 figures, Biophys. J., in pres...|$|E
40|$|Floodplain {{deposition}} is {{an essential}} part of the Holocene sediment dynamics of many catchments and a thorough dating control of these floodplain deposits is therefore essential to understand the driving forces of these sediment dynamics. In this paper we date floodplain and colluvial deposition in the Belgian Dijle catchment using accelerator mass spectrometric radiocarbon and optical stimulated luminescence dating. Relative mass accumulation curves for the Holocene were constructed for three colluvial sites and 12 alluvial sites. A database was constructed of all available radiocarbon ages of the catchment and this database was analysed using relative sediment mass accumulation rates and <b>cumulative</b> <b>probability</b> <b>functions</b> of ages and site-specific sedimentation curves. <b>Cumulative</b> <b>probability</b> <b>functions</b> of ages were split into different depositional environments representing stable phases and phases of accelerated clastic deposition. The results indicate that there is an important variation between the different dated sites. After an initial stable early and middle Holocene phase with mainly peat growth in the floodplains, clastic sedimentation rates increased from 4000 BC on. This first phase was more pronounced and started somewhat earlier for colluvial deposits then for alluvial deposits. The main part of the Holocene deposits, both in colluvial and alluvial valleys, was deposited during the last 1 ka. The sedimentation pattern of the individual dated sites and the catchment-wide pattern indicate that land use changes are responsible for the main variations in the Holocene sediment dynamics of this catchment, while the field data do not provide indications for a climatological influence on the sediment dynamics. status: publishe...|$|R
5000|$|The {{central limit theorem}} applies in {{particular}} to sums of independent and identically distributed discrete random variables. A sum of discrete random variables is still a discrete random variable, so that we are confronted with a sequence of discrete random variables whose <b>cumulative</b> <b>probability</b> distribution <b>function</b> converges towards a <b>cumulative</b> <b>probability</b> distribution <b>function</b> corresponding to a continuous variable (namely that of the normal distribution). This means that if we build a histogram of the realisations of the sum of [...] independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as [...] approaches infinity, this relation is known as de Moivre-Laplace theorem. The binomial distribution article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.|$|R
40|$|Abstract-We derive an {{infinite}} series {{expression of the}} average signal-to-noise ratio (SNR) gain for a dual selection combining system under Nakagamiln fading using a series representation of the bivariate <b>cumulative</b> <b>probability</b> distribution <b>function.</b> The average SNR gain expression is shown {{to relate to the}} Nakagami shape factor m, the power correlation and the power allocation ratio. 1...|$|R
