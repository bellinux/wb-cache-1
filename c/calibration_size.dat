4|136|Public
40|$|In {{view of the}} {{discrepancies}} in published crater frequency data, effects modifying production size distributions of impact craters are discussed. The resulting criteria are applied to {{the determination of the}} size distributions of unmodified impact crater populations in selected lunar regions of different ages. The measured cumulative crater frequencies are used to obtain a general <b>calibration</b> <b>size</b> distribution curve by a normalization procedure. Deviations of measured size distributions from the calibration distribution are strongly suggestive of the existence of processes having modified the primary impact crater population...|$|E
40|$|Several RPT sensors {{have been}} {{developed}} to acquire objective and quantitative pulse waves. These sensors offer improved performance with respect to pressure <b>calibration,</b> <b>size</b> and sensor deployment, but not temperature. Since most pressure sensors are sensitive to temperature, various temperature compensation techniques {{have been developed}}, but these techniques are largely inapplicable to RPT sensors due to the size restrictions of the sensor, and incompatibility between the compensation techniques and the RPT sensor. Consequently, in this paper a new RPT sensor comprising six piezoresistive pressure sensors and one thermistor has been developed through finite element analysis and then a suitable temperature compensation technique has been proposed. This technique compensates for temperature variations by using the thermistor and simple compensation equations. As verification of the proposed compensation technique, pulse waves of all types were successfully compensated for temperature changes...|$|E
30|$|In Traditional Chinese Medicine (TCM) and Korean Medicine (KM), four {{diagnostic}} {{methods of}} observation (listening, smelling, inquiring, and palpation) {{are used to}} diagnose diseases in patients. Pulse diagnosis is the representative diagnostic method belonging to the palpation diagnostic methods (Kim and Kang 2008). The purpose of pulse diagnosis is to determine evolution of a disease, causes of a disease, position of a disease, and a cure for the disease. Pulse diagnosis is traditional and venerable. However, {{it is difficult to}} become proficient in measurement of pulse waves, and the measurement and analysis of pulse waves is subjective. Therefore, a pulse diagnosis can vary with different doctors. A number of studies have attempted to objectify and quantify pulse waves to overcome this problem (Ryu et al. 2007; Kim et al. 1999) by using pulse diagnosis sensors, pulse wave simulators, and pulse diagnosis instruments (or pulse taking devices) (Fu and Lai 1989 Jeon et al. 2008; Kim et al. 2009 a; Luo et al. 2012; Shin et al. 2010; Shin et al. 2011; Yoo et al. 2013). A pulse diagnosis sensor has been developed for pressure <b>calibration,</b> <b>size,</b> temperature and deployment, while a pulse wave simulator has been developed for an objective standard for pulse analysis. These advancements mainly focus on development of hardware to acquire objective and quantitative pulse waves. In the present study, we developed a software-based pulse wave measurement method.|$|E
40|$|We {{investigated}} {{the effect of}} both the <b>calibration</b> set <b>size</b> (number of samples) and the calibration sampling strategy {{on the performance of}} vis–NIR models to predict clay content and exchangeable Ca (Ca++). We evaluated the following calibration sampling algorithms: Kenard–Stone (KSS), conditioned Latin hypercube (cLHS) and fuzzy c-means (FCMS), which are commonly used in spectroscopy and digital soil mapping. These algorithmswere tested separately using a field-scale dataset and a regional scale dataset. For each datasetwe randomly selected a validation subset and the remaining samples were used as candidates for calibration sampling. The accuracy of vis–NIRmodels of clay content and Ca++were compared {{on the basis of the}} sampling algorithms used for selecting the calibration samples. We also tested 38 different <b>calibration</b> set <b>sizes</b> varying from 10 to 380 samples. The vis–NIR models were calibrated by using the support vector regression machine (SVM) algorithm. The training root mean square error (RMSE), the normalized RMSE and the prediction RMSE were used to evaluate the sensitivity of the models to both the sampling algorithmand the <b>calibration</b> set <b>size.</b> In addition, we {{investigated the}} sample representativeness of each algorithm and we suggest a novel and simple methodology to identify an adequate <b>calibration</b> set <b>size</b> based only on the vis–NIR data (i. e. without prior knowledge of the response variables). As expected, our results showthat the error of the soil vis–NIRmodels depends on the <b>calibration</b> set <b>size.</b> When the number of calibration samples is relatively small the sampling algorithm may play an important role on the accuracy of the vis–NIRmodels. On the other hand, if the <b>calibration</b> set <b>size</b> is large enough, the samplingmethod is not a critical issue. Concerning the sample representativeness, we found for all the algorithms that the original distribution of the vis–NIR data can be better replicated by increasing the <b>calibration</b> set <b>size.</b> The results indicate that the calibration samples selected by the cLHS and by the FCMSalgorithms better replicate the original vis–NIR distribution of all the samples, in comparison to those samples selected by the KSS algorithm...|$|R
3000|$|In {{the present}} work, <b>calibration</b> and <b>size</b> {{correction}} were not performed, so the zeta-potential values reported {{are of a}} relative nature. In the experimental setup using ESA technique, two different sets of concentrated ZrO 2 nanosuspensions were prepared, as follow: [...]...|$|R
40|$|A {{simulation}} {{study of}} the effects of varying the item <b>calibration</b> sample <b>size</b> on varying size item pools was run for the maximum information adap-tive test. Items were calibrated on the three-pa-rameter logistic model on sample sizes of 500, 1, 000, and 2, 000. Item pools of 100, 200, or 300 items were developed from the three <b>calibration</b> sample <b>sizes.</b> Fixed-length adaptive tests of 10, 15, 20, 25, 30, and 35 items were given to a different group of 500 simulated subjects for each combina-tion of item pool <b>size</b> and <b>calibration</b> sample <b>size.</b> Results indicated that high correlations between ability and estimated ability would be obtained in any testing if a sufficient number of items were ad-ministered. The reduction of absolute error of abil-ity estimation was found to require at least 200 items calibrated on 2, 000 subjects. Adaptive, or tailored, testing refers to a series of techniques for finding and scoring the most useful set of items to be administered to an indi-vidual. Adaptive testing techniques range from strictly mechanical (Weiss, 1973) to mathemati-cally elegant models (Owen, 1969). In all these techniques, the basic components are pa-rameters representing the items, some method for ability estimation, a method for item selec-tion, and rules for the initiation and termination of testing. Development of item pools can b...|$|R
40|$|Spiking is {{a useful}} {{approach}} to improve the accuracy of regional or national calibrations when {{they are used to}} predict at local scales. To do this, a small subset of local samples (spiking subset) is added to recalibrate the initial calibration. If the spiking subset is small in comparison with the size of the initial calibration set, then it could have little noticeable effect and a small improvement can be expected. For these reasons, we hypothesized that the accuracy of the spiked calibrations can be improved when the spiking subset is extra-weighted. We also hypothesized that the spiking subset selection and the initial <b>calibration</b> <b>size</b> could affect the accuracy of the recalibrated models. To test these hypotheses, we evaluated different strategies to select the best spiking subset, with and without extra-weighting, to spike three different-sized initial calibrations. These calibrations were used to predict the soil organic carbon (SOC) content in samples from four target sites. Our results confirmed that spiking improved the prediction accuracy of the initial calibrations, with any differences depending on the spiking subset used. The best results were obtained when the spiking subset contained local samples evenly distributed in the spectral space, regardless of the initial calibration's characteristics. The accuracy was improved significantly when the spiking subset was extra-weighted. For medium- and large-sized initial calibrations, the improvement from extra-weighting was larger than that caused by the increase in spiking subset size. Similar accuracies were obtained using small- and large-sized calibrations, suggesting that incipient spectral libraries could be useful if the spiking subset is properly selected and extra-weighted. When small-sized spiking subsets were used, the predictions were more accurate than those obtained with ‘geographically-local’ models. Overall, our results indicate that we can minimize the efforts needed to use near-infrared (NIR) spectroscopy effectively for SOC assessment at local scales...|$|E
40|$|The {{objective}} {{of the present study}} was the application of RF to binary disease traits recorded in a cow calibration group. Major tasks addressed the impact of the cow <b>calibration</b> group <b>size,</b> the frequency of diseased cows in the reference population, and the genomic architecture on accuracies of GEBV. Results from RF applications were compared with GBLUP methodology. Peer reviewe...|$|R
40|$|This work {{describes}} calibration {{methods for}} the particle sizing and particle concentration {{systems of the}} passive cavity aerosol spectrometer probe (PCASP). Laboratory calibrations conducted over six years, {{in support of the}} deployment of a PCASP on a cloud physics research aircraft, are analyzed. Instead of using the many <b>calibration</b> <b>sizes</b> recommended by the PCASP manufacturer, a relationship between particle diameter and scattered light intensity is established using three sizes of mobility-selected polystyrene latex particles, one for each amplifier gain stage. In addition, studies of two factors influencing the PCASP's determination of the particle size distribution – amplifier baseline and particle shape – are conducted. It is shown that the PCASP-derived size distribution is sensitive to adjustments of the sizing system's baseline voltage, and that for aggregates of spheres, a PCASP-derived particle size and a sphere-equivalent particle size agree within uncertainty dictated by the PCASP's sizing resolution. Robust determinations of aerosol concentration, and size distribution, also require calibration of the PCASP's aerosol flowrate sensor. Sensor calibrations, calibration drift, and the sensor's non-linear response are documented...|$|R
40|$|Modulation-frequency-encoded {{fluorescence}} excitation {{enables the}} identification of end-labeled DNA samples of different genetic origin during their electrophoretic separation, opening perspectives for intrinsic <b>size</b> <b>calibration,</b> malign / healthy sample comparison, and exploitation of multiplex ligation-dependent probe amplification...|$|R
40|$|A reiterative {{analysis}} {{was applied to}} determine the optimum number of storms required to generate site mean concentrations (SMC), using total phosphorus data from 17 urban catchments. For each analysed catchment, event mean concentration data were randomly placed into various <b>calibration</b> set <b>sizes,</b> ranging from 1 to N (where N was equal to {{the total number of}} storms available at the given catchment). Geometric mean estimates of SMCs associated with each <b>calibration</b> set <b>size</b> were then calculated, and verified using all available data from the catchment of interest. This process was repeated 10, 000 times for each catchment. Average errors associated with each sample size were then plotted and used to estimate the optimum number of storms required to derive SMCs at each catchment. The optimum was derived by evaluating the balance between cost and uncertainty, whereby the minimum number of storms producing a relatively accurate estimate of SMC was accepted. Overall, it was found that between five and seven storm events were sufficient. In addition, it was deduced that sampling only six storm events would be approximately 40 % cheaper than sampling 12 events...|$|R
30|$|Quantification {{requires}} {{the conversion of}} the recorded counts per pixel into activity per volume unit. This is usually obtained through a calibration step where a source of known activity is scanned. One study has presented the use of a point source and of planar acquisitions to obtain the conversion factor [21]. However, most of the other studies copied the extensively validated PET procedure where a large source of known activity and volume are scanned [4, 21]. This last methodology was adopted in this work, but the influence of the <b>calibration</b> phantom <b>size</b> was also investigated. The reason behind this was twofold. The first point was that using a calibration phantom of a size similar to the test phantom is too fair for the whole procedure and does not correspond to what would be possible with patients. The second point was that large phantoms are not easy to handle. Therefore, any reduction in the <b>calibration</b> phantom <b>size</b> would ease the calibration procedure. This would be particularly desirable if the procedure has to be repeated frequently. The largest calibration phantom used (XL) had sizes comparable to the NEMA and contrast phantoms. The two other calibration phantoms (L and M) had reduced sizes while the cylindrical shape was maintained.|$|R
40|$|In {{the machine}} {{learning}} literature, {{it is commonly}} accepted as fact that as <b>calibration</b> sample <b>sizes</b> increase, Na ve Bayes classifiers initially outperform Logistic Regression classifiers in terms of classification accuracy. Applied to subtests from an on-line final examination and from a highly regarded certification examination, this study shows that the conclusion also applies to the probabilities estimated from short subtests of mental abilities and that small samples can yield excellent accuracy. The calculated Bayes probabilities {{can be used to}} provide meaningful examinee feedback regardless of whether the test was originally designed to be unidimensional...|$|R
40|$|We {{demonstrate}} {{the technique of}} multipoint viscosity measurements incorporating the accurate <b>calibration</b> of micron <b>sized</b> particles. We describe {{the use of a}} high-speed camera to measure the residual motion of particles trapped in holographic optical tweezers, enabling us to calculate the fluid viscosity at multiple points across the field-of-view of the microscope within a microfluidic system...|$|R
40|$|Knowledge of {{the times}} when crop and forest {{vegetation}} experience seasonally related changes in development is important in understanding growth and yield relationships. This article describes how densitometry of {{earth resources technology satellite}} (ERTS- 1) multispectral scanner (MSS) imagery can be used to identify such phenological events. Adjustments for instrument <b>calibration,</b> aperture <b>size,</b> gray-scale differences between overpasses, and normalization of changing solar elevation are considered in detail. Seasonal vegetation differences can be identified by densitometry of band 5 (0. 6 - 0. 7 microns) and band 7 (0. 8 - 1. 1 microns) MSS imagery. Band-to-band ratios of the densities depicted the changes more graphically than the individual band readings...|$|R
40|$|The {{iterative}} and convergent {{nature of}} ensemble learning algorithms provides potential for improving classification of complex landscapes. This study performs land-cover classification in a heterogeneous Massachusetts landscape by comparing three ensemble learning techniques (bagging, boosting, and random forests) and a non-ensemble learning algorithm (classification trees) using multiple criteria related to algorithm and training data characteristics. The ensemble learning algorithms had comparably high accuracy (Kappa range: 0. 76 - 0. 78), which was 11 % {{higher than that}} of classification trees. Ensemble learning techniques were not influenced by calibration data variability, were robust to one-fifth calibration data noise, and insensitive to a 50 % reduction in <b>calibration</b> data <b>size...</b>|$|R
40|$|An {{accurately}} calibrated {{item bank}} {{is essential for}} a valid computerized adaptive test. However, in some settings, such as occupational testing, there is limited access to examinees for calibration. As {{a result of the}} limited access to possible examinees, collecting data to accurately calibrate an item bank in an occupational setting is usually difficult. In such a setting, the item bank can be calibrated online in an operational setting. This study explored three possible automatic online calibration strategies, with the intent of calibrating items accurately while estimating ability precisely and fairly. That is, the item bank is calibrated in a situation where examinees are processed and the scores they obtain have consequences. A simulation study was used to identify the optimal calibration strategy. The outcome measure was the mean absolute error of the ability estimates of the examinees participating in the calibration phase. Manipulated variables were the <b>calibration</b> strategy, the <b>size</b> of the <b>calibration</b> sample, the <b>size</b> of the item bank, and the item response model...|$|R
40|$|Genetic {{analysis}} of wood chemical composition is often {{limited by the}} cost and throughput of direct analytical methods. The speed and low cost of Fourier transform near infrared (FT-NIR) overcomes many of these limitations, {{but it is an}} indirect method relying on calibration models that are typically developed and validated with small sample sets. In this study, we used > 1500 young greenhouse grown trees from a clonally propagated single Populus family, grown at low and high nitrogen, and compared FT-NIR <b>calibration</b> sample <b>sizes</b> of 150, 250, 500 and 750 on calibration and prediction model statistics, and heritability estimates developed with pyrolysis molecular beam mass spectrometry (pyMBMS) wood chemical composition. As <b>calibration</b> sample <b>size</b> increased from 150 to 750, predictive model statistics improved slightly. Overall, stronger calibration and prediction statistics were obtained with lignin, S-lignin, S/G ratio, and m/z 144 (an ion from cellulose), than with C 5 and C 6 carbohydrates, and m/z 114 (an ion from xylan). Although small differences in model statistics were observed between the 250 and 500 sample calibration sets, when predicted values were used for calculating genetic control, the 500 sample set gave substantially more similar results to those obtained with the pyMBMS data. With the 500 sample calibration models, genetic correlations obtained with FT-NIR and pyMBMS methods were similar. Quantitative trait loci (QTL) analysis with pyMBMS and FT-NIR predictions identified only three common loci for lignin traits. FT-NIR identified four QTLs that were not found with pyMBMS data, and these QTLs were for the less well predicted carbohydrate traits...|$|R
3000|$|We are {{interested}} in examining the over-time and cross-region observed values of occupational groups and productivity in Spain, under the lens of our three occupational choices model. For this purpose, we calibrate {{the values of the}} parameters of the model with data from the Spanish economy for the years 2000 – 2005. This <b>calibration</b> sets the <b>size</b> of each occupational group at values of 6.5 [...]...|$|R
40|$|In this article, {{evaluation}} of a vision-based measuring system for parallel machine-tool calibration is per-formed. The simultaneous measurement of the 6 pose components enables one to perform calibration using the efficient inverse kinematic method. The system is composed of a single camera and a calibration board generated on a LCD monitor. <b>Calibration</b> board <b>size</b> {{can be adapted to}} the camera field of view, and spe-cific points of interest can be generated, in order to improve pose measurement. Based on a single indus-trial camera, the measuring system is low-cost and easy-to-use. An experimental {{evaluation of}} the sys-tem is performed on a machine-tool axis. Measure-ment bias and precision are estimated by compari-son with laser interferometry, and influence of foca...|$|R
40|$|Novel ultra-compact, {{electrically}} switchable, time-structured/pulsed, ~ 1 - 14 MeV-level neutron and photon generators have application embedded {{into large}} detector systems, especially calorimeters, for energy and operational <b>calibration.</b> The small <b>sizes</b> are applicable to permanent in-situ deployment, {{or able to}} be conveniently inserted into large high energy physics detector systems. For bench- testing of prototypes, or for detector module production testing, these compact n and gamma generators offer advantages...|$|R
40|$|We review {{here the}} {{advances}} {{that have been}} made in methodology using the analytical ultracentrifuge for characterising polydisperse polymer systems with a quasi-continuous distribution of molecular weight. These advances include improved ways of defining the weight and z-average molecular weights of a distribution from sedimentation equilibrium experiments and hence the ratio Mz=Mw, off-line <b>calibration</b> of <b>size</b> exclusion chromatography by sedimentation equilibrium and conversion of a sedimentation coefficient distribution into a molecular weight distribution. Although these methods are applicable to other polymeric systems, we focus on polysaccharides, mucins and other glycoconjugates, systems which particularly benefit from the availability of recently available long optical path length cells for the minimisation of complications through thermodynamic non-ideality. Â© 2010 Wiley-VCH Verlag GmbH & Co. KGaA...|$|R
40|$|The {{majority}} of camera calibration methods, including the Gold Standard algorithm, use point based information and simultaneously estimate all calibration parameters. In contrast, we propose a novel calibration method that exploits line orientation information and decouples the problem into two simpler stages. We formulate {{the problem as}} minimisation of the lateral displacement between single projected image lines and their vanishing points. Unlike previous vanishing point methods, parallel line pairs are not required. Additionally, the invariance properties of vanishing points mean that multiple images related by pure translation {{can be used to}} increase the <b>calibration</b> dataset <b>size</b> without increasing the number of estimated parameters. We compare the method with vanishing point methods and the Gold Standard algorithm and demonstrate that it has comparable performance...|$|R
40|$|Chantier qualité GAGenomic {{selection}} {{refers to}} the use of genotypic information for predicting breeding values of selection candidates. A prediction formula is calibrated with the genotypes and phenotypes of reference individuals constituting the <b>calibration</b> set. The <b>size</b> and the composition of this set are essential parameters affecting the prediction reliabilities. The objective {{of this study was to}} maximize reliabilities by optimizing the calibration set. Different criteria based on the diversity or on the prediction error variance (PEV) derived from the realized additive relationship matrix-best linear unbiased predictions model (RA-BLUP) were used to select the reference individuals. For the latter, we considered the mean of the PEV of the contrasts between each selection candidate and the mean of the population (PEVmean) and the mean of the expected reliabilities of the same contrasts (CDmean). These criteria were tested with phenotypic data collected on two diversity panels of maize (Zea mays L.) genotyped with a 50 k SNPs array. In the two panels, samples chosen based on CDmean gave higher reliabilities than random samples for various <b>calibration</b> set <b>sizes.</b> CDmean also appeared superior to PEVmean, which can be explained by the fact that it takes into account the reduction of variance due to the relatedness between individuals. Selected samples were close to optimality for a wide range of trait heritabilities, which suggests that the strategy presented here can efficiently sample subsets in panels of inbred lines. A script to optimize reference samples based on CDmean is available on request...|$|R
40|$|The {{efficiency}} of TSP samplers for large particle {{is determined by}} inertial effects; instead of using Stokes number for characterizing the instrument, the particle stop distance is chosen. Polydisperse glass beads with aerodynamic diameters between 20 and 120 mym were used as <b>calibration</b> aerosol. The <b>size</b> distribution sampled under in situ conditions {{as compared to the}} original size distribution gives the aspiration efficiency, provided that the smallest particle in the test aerosol are sampled with sufficient efficiency...|$|R
40|$|Introduction: Preliminary Examination (PE) of the Stardust cometary {{collector}} revealed many {{tracks in}} the silica aerogel and impact craters on aluminium (Al) foil, from which Wild 2 dust particle fluence and size distribution were determined. Laboratory light gas gun (LGG) shots provided impactor <b>size</b> <b>calibrations.</b> Analogue impacts of diverse mineral compositions and aggregate particles aided interpretation of dust composition and structure. We now describe our recent impact experiments on foil by organic materials, which reveal distinctive crater surface textures, and even preserved residue...|$|R
40|$|Recent {{advances}} in active particle selection in the Heidelberg Van de r Graaf (VdG) dust accelerator {{have led to}} high-fidelity, low-backgro und <b>calibrations</b> of track <b>sizes</b> in aerogel {{as a function of}} particle size and velocity in the difficult regime above 10 km sec [...] 1 and sub micron sizes. To the extent that the VdG shots are analogs for inters tellar dust (ISD) impacts, these new measurements enable us to place preliminary constraints on the ISD flux based on Stardust@home data...|$|R
40|$|Scanning force {{microscopy}} (SFM) reveals surface topography by scanning a sharp tip {{in close}} proximity to the sample. Due to tip–sample interaction, artificial broadening of the real surface structure with the tip geometry occurs. One approach for image reconstruction is the use of calibration standards, preferably in the size range of the samples. In the present study, an image reconstruction method based on colloidal gold as a geometric standard was used to reconstruct SFM images of biomolecules. Sample and <b>calibration</b> standard <b>size</b> were in the nanometer range, and the standards were coadsorbed with the specimen. Raw and reconstructed images of the biomolecules were compared, and the reconstruction was characterized by difference images as well as determination of the difference volume. The application of image reconstruction based on colloidal gold as a calibration standard for SFM of biomolecules is discussed...|$|R
30|$|In {{addition}} to the count data, list data are also produced. This data type contains the information of the APD’s pulse height {{as well as the}} ESA energy step for each incoming particle and is mainly used for the in-flight <b>calibration.</b> The <b>size</b> of a data packet is 6  bytes per event (see Table  2 for the contents of the packet). Considering the expected maximum count rate is 5000  counts/s for one azimuthal channel, it is not reasonable to produce list data for all events (in that case, the data product rate is 7.5  kB per SV step, far beyond the capacity of the system data recorder and the down link rate). For this reason, the production of the list data packets is restricted to 10 events per SV step on a first-come basis (then the size of the list data is 960  bytes per spin phase).|$|R
40|$|Plenoptic {{cameras are}} gaining {{attention}} for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification pro-cedure for lenselet-based plenoptic cameras {{appropriate for a}} range of computer vision applications. We derive a novel physically based 4 D intrinsic matrix relating each recorded pixel to its corresponding ray in 3 D space. We further propose a radial distortion model and a practical objec-tive function based on ray reprojection. Our 15 -parameter camera model is of much lower dimensionality than cam-era array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three <b>calibration</b> grid <b>sizes</b> over five datasets. Typical RMS ray reprojection er-rors are 0. 0628, 0. 105 and 0. 363 mm for 3. 61, 7. 22 and 35. 1 mm calibration grids, respectively. Rectification ex-amples include calibration targets and real-world imagery. 1...|$|R
40|$|Abstract. In the {{conformal}} prediction literature, {{it appears}} axiomatic that transductive conformal classifiers possess a higher predictive effi-ciency than inductive conformal classifiers, however, this {{depends on whether}} or not the nonconformity function tends to overfit misclassi-fied test examples. With the conformal prediction framework’s increasing popularity, it thus becomes necessary to clarify the settings in which this claim holds true. In this paper, the efficiency of transductive conformal classifiers based on decision tree, random forest and support vector ma-chine classification models is compared to the efficiency of corresponding inductive conformal classifiers. The results show that the efficiency of conformal classifiers based on standard decision trees or random forests is substantially improved when used in the inductive mode, while con-formal classifiers based on support vector machines are more efficient in the transductive mode. In addition, an analysis is presented that dis-cusses the effects of <b>calibration</b> set <b>size</b> on inductive conformal classifier efficiency. ...|$|R
40|$|A {{method of}} {{identifying}} the transition rule, encapsulated in a modified cellular automata (CA) model, is demonstrated using experimentally observed evolution of dendritic crystal growth patterns in NH 4 Br crystals. The {{influence of the}} factors, such as experimental set-up and image pre-processing, colour and <b>size</b> <b>calibrations,</b> on the method of identification are discussed in detail. A noise reduction parameter and the diffusion velocity of the crystal boundary are also considered. The {{results show that the}} proposed method can in principle provide a good representation of the dendritic growth anisotropy of any system...|$|R
40|$|With musical {{applications}} in mind, this paper {{reports on the}} level of noise observed in two commercial infrared marker-based motion capture systems: one high-end (Qual-isys) and one affordable (OptiTrack). We have tested how various features (<b>calibration</b> volume, marker <b>size,</b> sampling frequency, etc.) influence the noise level of markers lying still, and fixed to subjects standing still. The conclusion is that the motion observed in humans standing still is usu-ally considerably higher than the noise level of the systems. Dependent on the system and its calibration, however, the signal-to-noise-ratio may in some cases be problematic. 1...|$|R
40|$|International audienceWe {{discuss the}} {{self-assembly}} of monatomic chains composed of atoms of two kinds on a periodic substrate. We {{assume that there}} exists small positive misfit between the average atomic diameter of the deposited atoms and the substrate lattice parameter. This may cause the chain <b>size</b> <b>calibration.</b> Our model calculations show that if the interatomic interactions are of ordering type, at low temperature the distribution of chain lengths at thermal equilibrium is more strongly peaked around the optimum length than {{in the case of}} chains composed from identical atoms...|$|R
40|$|We {{characterize}} the evolutionary radiation of planktic foraminifera by the test size distributions of entire assemblages {{in more than}} 500 Cenozoic marine sediment samples, including more than 1 million tests. <b>Calibration</b> of Holocene <b>size</b> patterns with environmental parameters and comparisons with Cenozoic paleoproxy data show a consistently positive correlation between test size and surface-water stratification intensity. We infer that the observed macroevolutionary increase in test size of planktic foraminifera through the Cenozoic was an adaptive response to intensifying surface-water stratification in low latitudes, which was driven by polar cooling...|$|R
40|$|International audience"Two main issues {{regarding}} stormwater quality {{models have been}} investigated: i) the effect of <b>calibration</b> dataset <b>size</b> and characteristics on calibration and validation results; ii) the optimal split of available data into calibration and validation subsets. Data from 13 catchments {{have been used for}} three pollutants: BOD, COD and SS. Three multiple regression models were calibrated and validated. The use of different data sets and different models allows viewing general trends. It was found mainly that multiple regression models are case sensitive to calibration data. Few data used for calibration infers bad predictions despite good calibration results. It was also found that the random split of available data into halves for calibration and validation is not optimal. More data should be allocated to calibration. The proportion of data to be used for validation increases with the number of available data (N) and reaches about 35 % for N around 55 measured events. ...|$|R
