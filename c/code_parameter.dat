35|1444|Public
5000|$|... error <b>code</b> (<b>{{parameter}}</b> 1, parameter 2, parameter 3, parameter 4) error name ...|$|E
5000|$|The Asteroid Lightcurve Database (LCDB) of the Collaborative Asteroid Lightcurve Link (CALL) uses a {{numeric code}} {{to assess the}} quality of a period {{solution}} for minor planet light curves (it does not necessarily assess the actual underlying data). Its quality <b>code</b> <b>parameter</b> [...] "U" [...] ranges from 0 (incorrect) to 3 (well-defined): ...|$|E
50|$|Outside the range, P {{tends to}} follow a {{geometric}} distribution on each side (p. 3).It is encoded using a Rice code with parameters chosen based on previous choices.For each Δ and each possible Rice <b>code</b> <b>parameter</b> k, the algorithm keeps track {{of the total number}} of bits that would have been used to encode pixels outside the range.Then for each pixel, it chooses the Rice code with the based on Δ at the pixel.|$|E
3000|$|... [...]. This {{designed}} {{family has}} more flexible <b>code</b> <b>parameters</b> {{when compared to}} the family of Hamming <b>codes</b> having the <b>parameters</b> [...]...|$|R
3000|$|... [...])=l). Thus, {{the value}} of the rank {{deficiency}} depends on <b>code</b> <b>parameters</b> (k and n). Indeed, only two consecutive rank deficiencies are necessary to determine all <b>code</b> <b>parameters.</b> The <b>code</b> word length n can be determined by the difference between two values of l corresponding to two consecutive rank deficiencies of R [...]...|$|R
30|$|The {{overhead}} {{reduces the}} efficiency of the design (overall code rate) especially when designing for large <b>code</b> <b>parameters.</b> However, as the packets size increases, the efficiency improves. Therefore, the design is applicable to packets of any size provided that they are not very small. For moderate <b>code</b> <b>parameters,</b> packets of few hundred bits (all network standards requires even more than this) are good enough that will not affect {{the efficiency of}} the code very much. For large <b>code</b> <b>parameters,</b> the efficiency can be improved by increasing the packet size and/or by utilizing the mentioned ways of reducing the overhead.|$|R
5000|$|Given an {{alphabet}} of two symbols, or {{a set of}} two events, P and Q, with probabilities p and (1 &minus; p) respectively, where p ≥ 1/2, Golomb coding can be used {{to encode}} runs of zero or more Ps separated by single Qs. In this application, the best setting of the parameter M is the nearest integer to [...] When p = 1/2, M = 1, and the Golomb code corresponds to unary (n ≥ 0 Ps followed by a Q is encoded as n ones followed by a zero). If a simpler code is desired, one can assign Golomb-Rice parameter [...] (i.e., Golomb parameter [...] ) to the integer nearest to although not always the best parameter, it is usually the best Rice parameter and its compression performance is quite close to the optimal Golomb code. (Rice himself proposed using various codes for the same data to figure out which was best. A later JPL researcher proposed various methods of optimizing or estimating the <b>code</b> <b>parameter.)</b> ...|$|E
3000|$|... value, and the LP based <b>code</b> <b>parameter</b> {{optimization}} are all {{included in}} a single repeat-until loop. This indicates that the code parameters are also changed in the EBSA framework.|$|E
3000|$|... ● The {{transfer}} speed between CEC and VM cannot be neglected, {{especially if the}} internet connection is slower and large data sets must be transmitted (e.g., input data, <b>code,</b> <b>parameter).</b>|$|E
5000|$|Small {{memory dump}} {{contains}} various info {{such as the}} stop <b>code,</b> <b>parameters,</b> list of loaded device drivers, etc.|$|R
40|$|The recent {{growth in}} {{personal}} wireless communication devices {{being used for}} image transmission, have poised a challenge to protect this data against loss over mobile radio channels. The paper addresses this problem investigating error protection schemes {{in the context of}} JPEG 2000 compressed imagery. More particularly, the results reported in this paper provide coding guidelines concerning the selection of appropriate Turbo <b>code</b> <b>parameters</b> along with JPEG 2000 <b>coding</b> <b>parameters</b> mentioned in [2]. 1...|$|R
50|$|Persistent Rice adaptation, using a Rice <b>coding</b> <b>parameter</b> {{derivation}} for {{entropy coding}} that has memory that persists across transform coefficient sub-block boundaries.|$|R
3000|$|... is also determined, {{which reduces}} the {{broadcast}} phase design to optimization of the EW RLC <b>code</b> <b>parameter</b> Γ(BS)(ξ) {{such that the}} average received video quality D is maximized after the target system delay T.|$|E
40|$|In {{this paper}} we propose the {{application}} of a new transform-based coding method[1] in conjunction with Golomb-Rice () codes to lower significantly the complexity, which can be used in various applications, e. g. the Multiple Description coding[2]. The theoretical evaluations predict no important loss in compression performance, while the complexity is considerably reduced. Since codes are very fast and well suited for exponentially decaying distributions, they were implemented during the last decade in image and audio compressors. In all these schemes, the selection of the <b>code</b> <b>parameter</b> is performed presuming Laplacian distribution of prediction errors. We derive the selection method for the <b>code</b> <b>parameter</b> also for the case of Gaussian inputs. 1...|$|E
40|$|We {{examine the}} use of Golomb-power-of- 2 (GPO 2) codes to {{efficiently}} and loss-lessly encode sequences of nonnegative integers from a discrete source. Specifically, we’re interested in the problem of selecting which GPO 2 code to use for a source or a block of samples; this problem {{is at the heart}} of the well-known Rice entropy cod-ing algorithm. We’re particularly concerned with the case where the mean sample value is known or can be estimated. We derive bounds on the optimum <b>code</b> <b>parameter</b> as a function of the mean sample value. These bounds establish that no more than three possible code choices can be optimum for a given mean sample value. We derive a simple method to select the optimum GPO 2 code for a geometrically distributed source given the mean value. We also devise a simpler code selection procedure that generalizes previously known methods. Both code selection methods can be implemented using only integer arithmetic and table look-ups, and require no divisions. We show that, for any source with known mean, the GPO 2 <b>code</b> <b>parameter</b> selected under this simple procedure is always within one of the optimum <b>code</b> <b>parameter</b> for the source, and that the added cost due to suboptimum parameter selection under this procedure is never more than 1 / 2 bit per sample and no more than about 13 percent inefficiency. We investigate {{the use of}} both code selection procedures as lower complexity alternatives to Rice coding within the emerging Consultative Committee for Space Data Systems (CCSDS) image compression standard. For the images tested, both code selection methods produce negligible added rate compared to optimal code selection as in Rice coding. I...|$|E
40|$|We assign binary and ternary error-correcting {{codes to}} the data of {{syntactic}} structures of world languages and we study the distribution of code points {{in the space of}} <b>code</b> <b>parameters.</b> We show that, while most codes populate the lower region approximating a superposition of Thomae functions, there is a substantial presence of codes above the Gilbert-Varshamov bound and even above the asymptotic bound and the Plotkin bound. We investigate the dynamics induced on the space of <b>code</b> <b>parameters</b> by spin glass models of language change, and show that, in the presence of entailment relations between syntactic parameters the dynamics can sometimes improve the code. For large sets of languages and syntactic data, one can gain information on the spin glass dynamics from the induced dynamics in the space of <b>code</b> <b>parameters.</b> Comment: 14 pages, LaTeX, 12 png figure...|$|R
3000|$|... • Following the {{approach}} of [22], we use Bernstein’s inequality and Bennett’s inequality to upper bound the false-positive and false-negative error probability, respectively. From these bounds, we derive conditions on the <b>code</b> <b>parameters</b> (<b>code</b> length, cutoff, threshold) such that the error probabilities are sufficiently low.|$|R
40|$|Electronic travel aids (ETAs) can {{potentially}} increase {{the safety and}} comfort of blind users by detecting and displaying obstacles outside {{the range of the}} white cane. In a series of experiments, we aim to balance the amount of information displayed and the comprehensibility of the information taking into account the risk of information overload. In Experiment 1, we investigate perception of compound signals displayed on a tactile vest while walking. The results confirm that the threat of information overload is clear and present. Tactile <b>coding</b> <b>parameters</b> that are sufficiently discriminable in isolation may not be so in compound signals and while walking and using the white cane. Horizontal tactor location is a strong <b>coding</b> <b>parameter,</b> and temporal pattern is the preferred secondary <b>coding</b> <b>parameter.</b> Vertical location is also possible as <b>coding</b> <b>parameter</b> but it requires additional tactors and makes the display hardware more complex and expensive and less user friendly. In Experiment 2, we investigate how we can off-load the tactile modality by mitigating part of the information to an auditory display. Off-loading the tactile modality through auditory presentation is possible, but this off-loading is limited and may result in a new threat of auditory overload. In addition, taxing the auditory channel may in turn interfere with other auditory cues from the environment. In Experiment 3, we off-load the tactile sense by reducing the amount of displayed information using several filter rules. The resulting design was evaluated in Experiment 4 with visually impaired users. Although they acknowledge the potential of the display, the added of the ETA as a whole also depends on its sensor and object recognition capabilities. We recommend to use not more than two <b>coding</b> <b>parameters</b> in a tactile compound message and apply filter rules {{to reduce the amount of}} obstacles to be displayed in an obstacle avoidance ETA...|$|R
40|$|The {{problem of}} the study and {{measurement}} of web applications including architectural features of CMS / CMF Drupal is considered. The experimental model to determine the modes of web applications is built. The requirements for mathematical model of <b>code</b> <b>parameter</b> estimation that is executed for process control optimization are described. The model of estimation of the resource consumption by the web application is developed. Software implementation of the system using PHP and Python languages is made</p...|$|E
40|$|In this paper, {{we propose}} {{a new class}} of regular {{fractional}} repetition (FR) codes constructed from perfect difference families and quasi-perfect difference families to store big data in distributed storage systems. The main advantage of the proposed construction method is that it supports a wide range of <b>code</b> <b>parameter</b> values compared to existing ones, which is an important feature to be adopted in practical systems. When using one instance of the proposed codes for a given parameter set, we show that the amount of stored data is very close to that of an existing state-of-the-art optimal FR code...|$|E
40|$|Abstract. The local cyclic elastic-plastic {{stress-strain}} {{responses were}} simulated using the incremental plasticity procedures of ABAQUS finite element code. It is shown {{a better understanding}} on the evolutions of the local cyclic stress-strain and the strong interactions between the most stressed material element and its neighboring material elements in the plastic deformations and stress redistributions. Based on the stress/strain states of the stabilized cycle, a new damage parameter, proposed on modification of the ASME <b>code</b> <b>parameter,</b> is applied and improved correlations between the predicted and the experimental fatigue lives are shown. It is concluded that the improvement of fatigue life prediction depends {{not only on the}} fatigue damage models, but also on the accurate evaluations of the cyclic elasto-plastic stress/strain responses...|$|E
30|$|In future work, we {{will extend}} this {{approach}} to other <b>coding</b> <b>parameter</b> configurations that take transmission effects and user environments into consideration. Moreover, we will also explore ways to reduce computational complexity.|$|R
40|$|Multiple {{description}} {{motion compensation}} (MDMC) is a multiple description video coding scheme that has shown good error resilience performance. MDMC enables one to vary <b>coding</b> <b>parameters</b> {{according to the}} desired trade-off between coding efficiency and error resilience. To fully utilize this advantage, one needs to establish a set of models, relating the rate, encoder distortion, and the end-to-end distortion after transmission, with the encoder parameters and channel parameters. Using these models, one can find the optimal <b>coding</b> <b>parameters</b> for given channel parameters and rate (or distortion) constraints. In this paper, we formulate and validate the rate and encoder distortion models...|$|R
3000|$|In this article, we {{proposed}} a parameters’ optimization scheme in order to eliminate uncertainties when selecting <b>coding</b> <b>parameters.</b> Compare with existing methods, our approach builds a relationship map of the quantization parameter QP [...]...|$|R
40|$|A {{recently}} developed data-compression method is an adaptive technique for coding quantized wavelet-transformed data, nominally {{as part of}} a complete image-data compressor. Unlike some other approaches, this method admits a simple implementation and does not rely on the use of large code tables. A common data compression approach, particularly for images, is to perform a wavelet transform on the input data, and then losslessly compress a quantized version of the wavelet-transformed data. Under this compression approach, it is common for the quantized data to include long sequences, or runs, of zeros. The new coding method uses prefixfree codes for the nonnegative integers {{as part of a}}n adaptive algorithm for compressing the quantized wavelet-transformed data by run-length coding. In the form of run-length coding used here, the data sequence to be encoded is parsed into strings consisting of some number (possibly 0) of zeros, followed by a nonzero value. The nonzero value and the length of the run of zeros are encoded. For a data stream that contains a sufficiently high frequency of zeros, this method is known to be more effective than using a single variable length code to encode each symbol. The specific prefix-free codes used are from two classes of variable-length codes: a class known as Golomb codes, and a class known as exponential-Golomb codes. The codes within each class are indexed by a single integer parameter. The present method uses exponential-Golomb codes for the lengths of the runs of zeros, and Golomb codes for the nonzero values. The code parameters within each code class are determined adaptively on the fly as compression proceeds, on the basis of statistics from previously encoded values. In particular, a simple adaptive method has been devised to select the parameter identifying the particular exponential-Golomb code to use. The method tracks the average number of bits used to encode recent runlengths, and takes the difference between this average length and the <b>code</b> <b>parameter.</b> When this difference falls outside a fixed range, the <b>code</b> <b>parameter</b> is updated (increased or decreased). The Golomb <b>code</b> <b>parameter</b> is selected based on the average magnitude of recently encoded nonzero samples. The coding method requires no floating- point operations, and more readily adapts to local statistics than other methods. The method can also accommodate arbitrarily large input values and arbitrarily long runs of zeros. In practice, this means that changes in the dynamic range or size of the input data set would not require a change to the compressor. The algorithm has been tested in computational experiments on test images. A comparison with a previously developed algorithm that uses large code tables (generated via Huffman coding on training data) suggests that the data-compression effectiveness of the present algorithm is comparable to the best performance achievable by the previously developed algorithm...|$|E
40|$|Modern {{wireless}} communication standard varies {{a lot from}} each other and is evolving rapidly. Flexibility becomes the dominate consideration of software defined radio (SDR) system design. Reconfigurable platform is preferred in the SDR due to the reuse of hardware. Convolutional code is widely adopted in many wireless protocols but the <b>code</b> <b>parameter</b> differs. In order to support multi-standard service, a decoder compatible for different protocols is needed. In this paper, we designed a flexible Viterbi decoder which is compatible with WiMAX, UMB and LTE’s channel coding scheme. High efficient cascaded Addcompare-select unit architecture and sliding window method for trace back are presented. Meanwhile, conflict free memory access model is also given. FPGA prototype shows our design is highly flexible while maintaining a high throughput with low area cost compared to others...|$|E
40|$|The {{application}} of turbo codes in modern communication systems makes decoding {{a time and}} power consuming task. It is known that analog decoder are superior to digital decoder designs in terms of speed and power consumption. The fact that a modification of a single <b>code</b> <b>parameter</b> like the block length requires a new analog decoder implementation, in combination with a complexity, which grows linearly with the block length, so far prohibits a real world application in e. g. UMTS. This paper introduces a novel mixed signal turbo decoder for different block lengths and arbitrary interleaver structures. For the prominent case of the UMTS turbo code {{the complexity of the}} component decoder is reduced by a factor of up to 91 compared to an all analog implementation with a performance loss of only 0. 05 dB...|$|E
40|$|A {{method of}} blind {{recognition}} of the <b>coding</b> <b>parameters</b> for binary Bose-Chaudhuri-Hocquenghem (BCH) codes is proposed in this paper. We consider an intelligent communication receiver which can blindly recognize the <b>coding</b> <b>parameters</b> of the received data stream. The only knowledge is that the stream is encoded using binary BCH codes, while the <b>coding</b> <b>parameters</b> are unknown. The problem can be addressed on {{the context of the}} non-cooperative communications or adaptive coding and modulations (ACM) for cognitive radio networks. The recognition processing includes two major procedures: code length estimation and generator polynomial reconstruction. A hard decision method has been proposed in a previous literature. In this paper we propose the recognition approach in soft decision situations with Binary-Phase-Shift-Key modulations and Additive-White-Gaussian-Noise (AWGN) channels. The code length is estimated by maximizing the root information dispersion entropy function. And then we search for the code roots to reconstruct the primitive and generator polynomials. By utilizing the soft output of the channel, the recognition performance is improved and the simulations show the efficiency of the proposed algorithm...|$|R
30|$|In this section, {{we discuss}} some {{specific}} cases {{for the design}} of erasure codes proposed in this paper. We demonstrate the PLR reduction capability of these codes through analytical calculations and simulation results for different <b>code</b> <b>parameters.</b>|$|R
5000|$|The ATR {{starts with}} a header of 32 bits {{organized}} into 4 bytes, denoted H1 to H4. H1 codes the protocol (with [...] and [...] being invalid), H2 <b>codes</b> <b>parameters</b> of the protocol. Little more is standardized.|$|R
40|$|International audienceBlind {{recognition}} of communication parameters {{is a research}} topic of high importance for both military and civilian communication systems. Numerous studies about carrier frequency estimation, modulation recognition as well as channel identification are available in literature. This paper deals with the blind {{recognition of}} the space–time block coding (STBC) scheme used in multiple input–multiple-output (MIMO) communication systems. Assuming there is perfect synchronization at the receiver side, this paper proposes three maximum-likelihood (ML) -based approaches for STBC classification: the optimal classifier, the second-order statistic (SOS) classifier, and the <b>code</b> <b>parameter</b> (CP) classifier. While the optimal and the SOS approaches require ideal conditions, the CP classifier is well suited for the blind context where the communication parameters are unknown at the receiver side. Our simulations show that this blind classifier is more easily implemented and yields better performance than those available in literature...|$|E
30|$|Several {{techniques}} {{have been proposed}} to determine optimal labeling pattern for the modulation (bit pattern vector allocated to each constellation point). The ideas of binary switching algorithm (BSA), which aims at labeling costs optimization, are presented in [6, 11]. However, the BSA based labeling optimization evaluates the labeling cost assuming that full a priori information is available. Hence, this approach only aims at lifting up as much the rightmost point of the demapper EXIT curve as possible. Yang et al. [12] introduce adaptive binary switching algorithm (ABSA) to obtain optimal labeling pattern, where optimality is defined by {{taking into account the}} labeling costs at multiple a priori MI points. Hence, ABSA changes the shape of the demapper EXIT curves more flexibly than BSA. However, the optimal labeling obtained in ABSA is on given code-basis since the <b>code</b> <b>parameter</b> optimization is not included in the ABSA iterations.|$|E
3000|$|... [...]. In this case, {{the unequal}} {{recovery}} time (URT) property enables the central point to decode user layers sequentially over time, {{starting from the}} BL onwards [18]. Thus the central point is able to produce encoded packets {{as soon as the}} BL of the message x(BS)is decoded and include additional layers as soon as they become available while updating the broadcast EW RLC <b>code</b> <b>parameter</b> Γ(BS)(ξ) “on the fly,” as illustrated in Example 2. We note that this scenario introduces a trade-off between increase in the upload delays of higher layers and decrease {{in the beginning of the}} broadcast phase, which has to be balanced by the optimal solution. Unfortunately, the distortion optimized system design for this scenario would result in tedious optimization problem, which is why we leave it out of consideration. However, we note that expected delay analysis, similar to the one presented in Table 2, could be used as a simple approximation for the layer-by-layer decode-and-broadcast system design.|$|E
40|$|We {{demonstrate}} propagation {{rules of}} subsystem code constructions by extending, shortening and combining given subsystem codes. Given an [[n,k,r,d]]_q subsystem code, we drive new subsystem <b>codes</b> with <b>parameters</b> [[n+ 1,k,r,≥ d]]_q, [[n- 1,k+ 1,r,≥ d- 1]]_q, [[n,k- 1,r+ 1,d]]_q. The interested readers shall consult our companion papers for {{upper and lower}} bounds on subsystem <b>codes</b> <b>parameters,</b> and introduction, trading dimensions, families, and references on subsystem codes [1][2][3] and references therein. Comment: Private comments are welcom...|$|R
5000|$|NICAM's second role - {{transmission}} to {{the public}} - {{was developed in the}} 80s by the BBC. This variant was known as NICAM-728, after the 728 kbit/s bitstream it is sent over. It uses the same audio <b>coding</b> <b>parameters</b> as NICAM-3.|$|R
40|$|This article {{presents}} the results of a study on the noise immunity of DVB channels when higher-order M-ary APSK modulation schemes and concatenated BCH-LDPC codes are used. Dependencies to determine the probability at the decoder output are given taking into consideration the BCH and LDPC <b>code</b> <b>parameters</b> and the error probability in the communication channel. The influence of the BCH packets length, the BCH code rate, the number of maximum iteration and the parameters of LDPC parity-check matrix on the code efficiency is analyzed. Research of the influence of the concatenated LDPC-BCH <b>code</b> <b>parameters</b> on the radio channel noise immunity is conducted and dependencies to determine the required CNR at the input of the satellite receiver are given...|$|R
