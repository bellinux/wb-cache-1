25|201|Public
40|$|CETTE THESE A POUR OBJET L'ETUDE DE LA STABILITE D'UN SYSTEME AEROELASTIQUE COMPOSE D'UNE STRUCTURE NON LINEAIRE SOUMISE A UN ECOULEMENT TRANSSONIQUE DE FLUIDE PARFAIT. LA PRESENCE DE NON-LINEARITES STRUCTURALES CONCENTREES, <b>COMME</b> <b>PAR</b> <b>EXEMPLE</b> UN JEU DANS LE MOUVEMENT DE ROTATION ENTRE UNE AILE D'AVION ET SA GOUVERNE, ENTRAINE L'APPARITION DE CYCLES LIMITES D'OSCILLATION. CES CYCLES LIMITES SONT ISSUS D'UNE BIFURCATION DE HOPF DES EQUATIONS MODELISANT LE SYSTEME DYNAMIQUE FLUIDE-STRUCTURE. UNE FORMULATION REDUITE DES EQUATIONS AEROELASTIQUES EN UTILISANT UN MODELE D'ETAT POUR LES FORCES AERODYNAMIQUES EST PRESENTE DANS LE CADRE BIDIMENSIONNEL ET TRIDIMENSIONNEL. LES FORCES AERODYNAMIQUES SONT DETERMINEES PAR L'INTERMEDIAIRE DES EQUATIONS D'EULER LINEARISEES AUTOUR D'UNE CONFIGURATION STATIONNAIRE DE L'ECOULEMENT. DANS LE CADRE TRIDIMENSIONNEL, UNE REDUCTION DU SYSTEME STRUCTURAL EST REALISEE PAR SOUS-STRUCTURATION DYNAMIQUE. LES BIFURCATIONS DE HOPF DU SYSTEME DYNAMIQUE NON LINEAIRE SONT ALORS DETERMINEES NUMERIQUEMENT PAR L'ALGORITHME DE ROOSE ET HLAVACEK EN FONCTION DU PARAMETRE PRESSION GENERATRICE DE L'ECOULEMENT. LES BRANCHES DE CYCLES LIMITES ISSUES DE CES BIFURCATIONS SONT CALCULEES PAR UNE TECHNIQUE DE CONTINUATION DE L'ABSCISSE CURVILIGNE. LES RESULTATS, AINSI OBTENUS, SONT COMPARES DANS LE CADRE BIDIMENSIONNEL AVEC LES SOLUTIONS, BEAUCOUP PLUS COUTEUSES, ISSUES DU COUPLAGE DIRECT DES EQUATIONS D'EULER ET DU MOUVEMENT DE LA STRUCTURE. UNE BONNE CONCORDANCE EST OBTENUE ENTRE LES DEUX METHODES. DEUX APPLICATIONS ONT ETE EFFECTUEES DANS CE CADRE : UN PROFIL D'AILE A DEUX DEGRES DE LIBERTE AVEC UN JEU DANS LE MOUVEMENT DE ROTATION ET UN PROFIL D'AILE A TROIS DEGRES DE LIBERTE COMPORTANT UN JEU DANS LE MOUVEMENT DE ROTATION DE LA GOUVERNE. DANS LE CADRE TRIDIMENSIONNEL L'EXPERIENCE NUMERIQUE PORTE SUR UNE AILE REELLE COMPORTANT EGALEMENT UN JEU SUR LE MOUVEMENT DE ROTATION DE LA GOUVERNE. PARIS-BIUSJ-Thèses (751052125) / SudocPARIS-BIUSJ-Lab. Mécanique the (751055218) / SudocCentre Technique Livre Ens. Sup. (774682301) / SudocSudocFranceF...|$|E
40|$|In thermal {{transformation}} {{processes for}} the production of metal parts of complex shape, shaping tools are frequently used. In die forging as an example, the tools are exposed to high mechanical and thermal loads and therefore subject to heavy wear. For fast feedback directly in the production process the accuracy to size of metal parts produced in such a way is at present frequently checked with groping measuring tools. This is unsatisfactory because of the complex forms, the broad product range and the thermal conditions. A 3 D – measurement system for highest possible flexibility and industrial environment, conceived for this application and based on a 3 D- sensor attached to an industrial robot for positioning is described here. The Structured – Light approach is used, i. e. the sensor consists of one or several CCD- cameras, combined with a programmable line projector. Two different calibration procedures of the line projector system are compared, on the one hand based on a polynomial model with a large number of coefficients, on the other hand based on a model targeted towards the actual optical / physical conditions with a significantly smaller number of coefficients. Finally a procedure for the solution of the Sensor to Hand calibration of the measuring robot is shown. RESUME: Dans les processus de transformation thermiques à l'aide d'outils formants visant à la fabrication de pièces métalliques de formes complexes, <b>comme</b> <b>par</b> <b>exemple</b> l'estampage, les outils sont soumis à des efforts mécaniques et thermiques élevés et par conséquent sont sujets à une forte usure. De manière à avoir un rapide retour d'information directement au niveau de la production, la précision des pièces produites doit être fréquemment examinée. La complexité des formes, la large gamme de produits et les contrainte...|$|E
40|$|This note follows our {{previous}} works on games with randomly arriving players [3] and [5]. Contrary {{to these two}} articles, here we seek a dynamic equilibrium, using the tools of piecewise deterministic control systems The resulting discrete Isaacs equation obtained is rather involved. As usual, it yields an explicit algorithm in the finite horizon, linear-quadratic case via a kind of discrete Riccati equation. The infinite horizon problem is briefly considered. It seems to be manageable only if one limits the number of players present in the game. In that case, the linear quadratic problem seems solvable via essentially the same algorithm, although we have no convergence proof, but only very convincing numerical evidence. We extend the solution to more general entry processes, and more importantly, to cases where the players may leave the game, investigating several stochastic exit mechanisms. We then consider the continuous time case, with a Poisson arrival process. While the general Isaacs equation is as involved as in the discrete time case, the linear quadratic case is simpler, and, provided again that we bound {{the maximum number of}} players allowed in the game, it yields an explicit algorithm with a convergence proof to the solution of the infinite horizon case, subject to a condition reminiscent of that found in [20]. As in the discrete time case, we examine the case where players may leave the game, investigating several possible stochastic exit mechanisms. MSC: 91 A 25, 91 A 06, 91 A 20, 91 A 23, 91 A 50, 91 A 60, 49 N 10, 93 E 03. Foreword This report is a version of the article [2] where players minimize instead of maximizing, and the linear-quadratic examples are somewhat different. On détermine les stratégies d'équilibre dans un jeu dynamique où des joueurs identiques arrivent de façon aléatoire, <b>comme,</b> <b>par</b> <b>exemple,</b> des congénères arrivant sur une même ressource. On considère aussi divers mécanismes de sortie aléatoire. On obtient des théorèmes d'existence et des algorithmes de calcul, plus explicites dans le cas particulier linéaire quadratique. Toute l'étude est conduite en horizon fini et en horizon infini, et en temps discret et en temps continu. Ce rapport est une version du working paper CRESE des mêmes auteurs (en économie mathématique), référence [2], mais où les joueurs minimisent au lieu de maximiser, et les exemples linéaires quadratiques sont un peu différents...|$|E
40|$|International audience[The fear of {{overpopulation}} is recurrent in {{the course}} of history as shown, for example, by the reading of Plato or Aristotle. But is this fear confirmed or reversed by the long history of humanity, and then by contemporary history?][La crainte d’une surpopulation est récurrente au fil de l’histoire <b>comme</b> le montre <b>par</b> <b>exemple</b> la lecture de Platon ou d’Aristote. Mais cette crainte est-elle confirmée ou infirmée par la longue histoire de l’humanité, puis par l’histoire contemporaine ?...|$|R
40|$|Les pages 394 et 395 sont manquantesInternational audience[The fear of {{overpopulation}} is recurrent in {{the course}} of history as shown, for example, by the reading of Plato or Aristotle. But is this fear confirmed or reversed by the long history of humanity, and then by contemporary history?][La crainte d’une surpopulation est récurrente au fil de l’histoire <b>comme</b> le montre <b>par</b> <b>exemple</b> la lecture de Platon ou d’Aristote. Mais cette crainte est-elle confirmée ou infirmée par la longue histoire de l’humanité, puis par l’histoire contemporaine ?...|$|R
5000|$|April 2013 French label Grand Palais/Modulor {{released}} the single L'Air Entre Nous, {{taken from the}} European version of new album Stories For Watering Skies which features additional French language title Vivre. The album has been released in France and Europe the 16 April 2013 receiving rave reviews in the French press from Rock & Folk, Rock First and Magic. Rock & Folk's Nicolas Ungemuth {{had this to say}} (emphasis added): [...] "...De loin, cette expatriee sort ce que Lou Doillon, Robi, Paradis et les autres ne sauront jamais faire. Noblesse oblige..un morceau <b>comme</b> 'Stepping Stones' <b>par</b> <b>exemple.</b> Qui en est capable? Comment cette jeune femme a la connaissance erudite et aux gouts impeccables, parvient-elle a se renouveler ainsi, renaitre de ses cendres, et signer a nouveau, tant d'aquarelles pop avec cet accent adorable? Peu de gens le savent et c'est tant mieux. Les miracles sont inexplicables, c'est pour cela qu'ils fascinent.” Nicolas Ungemuth ...|$|R
40|$|This {{article is}} a tribute to the memory of Professor Enzo Martinelli, with deep respect and reconesance. Nicolae Teleman. The index formula is a local {{statement}} made on global and local data; for this reason we introduce local Alexander - Spanier co-homology, local periodic cyclic homology, local Chern character and local T^∗-theory. Index theory should be done: Case 1 : for arbitrary rings, Case 2 : for rings of functions over topo- logical manifolds. Case 1 produces general index theorems, as for example, over pseudo-manifolds. Case 2 gives a general treatment of classical and non- commutative index theorems. All existing index theorems belong to the second category. The tools of the theory would contain: local T^∗ -theory, local peri- odic cyclic homology, local Chern character. These tools are extended to non- commutative topology. The index formula has three stages : Stage I is done in T^loc_i-theory, Stage II is done in the local periodic cyclic homology and Stage III involves products of distributions, or restriction to the diagonal. For each stage there corresponds a topological index and an analytical index. The construction of T^∗-theory involves the T-completion. It involves also the need to work with half integers; this should have important consequences. La formule de l’indice est une formule locale faite avec les donnes globales et locales; pour cette raison, nous introduisons local Alexander - Spanier co- homologie, homologie cyclique priodique locale, caractere de Chern locale et la theorie local T^∗. Dans la theorie de l’indice: Case 1 : pour anneaux ar- bitraires, Cas 2 : pour anneaux de fonctions sur les varietes topologiques. Cas 1 est le cas des theormes de l’indice general, <b>comme</b> <b>par</b> <b>exemple,</b> par example pseudo-varietees. Cas 2 donne un traitement general des theoremes d’index clas- siques et de la geometrie non-commutatives. Tous les theoremes d’index existants appartiennent la deuxime catgorie. Le outils de la theorie contiendra: local T^∗ -theorie, homologie cyclique priodique locale, caractere de Chern locale. Ces outils sont tendus topologie non-commutative. La formule de l’indice a ha trois tapes: Etape I est fait dans T^loc_i-theorie, Etape II est fait dans l’homologie cyclique periodique local et Etape III implique des produits de distributions, ou la re- striction la diagonale. Pour chaque tape correspond un index topologique et une index analytique. La construction de T^∗-theorie utilises le T-completion. Elle implique la necessite de travailler avec demi entiers; cela devrait avoir des consquences importantes...|$|E
40|$|Orange (the {{brand name}} of France T el ecom) {{provides}} mobile network, television and Internet services. In order to o er these services, Orange has developed {{equipment such as}} the Livebox and the Set Top Box. However, to maintain the quality of service in these equipment, the company researches methods that allow the integration of new features in its equipments, {{as well as of}} services developed by third parties. To reach this purpose, Orange studies the possibility of introducing in their equipment an OSGi Service Platform, a dynamic module system for JavaTM, that supports the dynamic and transparent installation of components. This platform would allow the development of applications in a modular fashion, while o ering strong isolation mechanisms. This is necessary to open the Orange Platform for the software developed by third-party providers. In the meantime, recent studies have demonstrated that the OSGi Service Platform possesses a considerable number of security weaknesses. These vulnerabilities, if ignored, can be exploited by the malicious components to block or interfere with Orange services, or to obtain sensitive information from the Orange customers. Something that cannot be neglected by the enterprise. To avoid these problems, a validation platform will be developed that will analyze all the applications developed for Orange equipment, in order to ensure security properties, such as a non-malicious behavior. For example, the validation platform needs to detect a situation where the application wants to occupy a considerable part of the computational resources such as the processor, the RAM, the hard drive, among others. If the constructed platform built does not detect any malicious behavior, Orange will give a certi cation to the third-party application. Orange (le nom de marque de France T el ecom) fournit des services de r eseaux mobiles, t el evision et Internet. A n d'o rir ces services, Orange a d evelopp e des equipements tels que la Livebox et la Set Top Box. Cependant, pour maintenir la qualit e des services de ces equipements, l'entreprise recherche une m ethode qui permet l'int egration de nouvelles fonctionnalit es, et aussi de services d evelopp es par des tiers. Pour atteindre cet objectif, Orange etudie la possibilit e d'introduire dans ses equipements une plate-forme de services OSGi, un syst eme de modules dynamiques pour JavaTMqui o re la possibilit e de faire des installations dynamiques et transparentes des composants. Cette plate-forme permettra le d eveloppement modulaire des applications, qualit es n ecessaires a des developeurs tiers. Cependant, des etudes r ecentes ont d emontr e que la plate-forme OSGi poss ede un nombre consid erable de failles de s ecurit e. Ces vuln erabilit es, si elles sont ignor ees, peuvent ^etre utilise es par les composants malveillants pour bloquer ou interf erer avec les services d'Orange, ou pour obtenir des informations sensibles des clients Orange. Pour eviter ces probl emes, une plate-forme de validation sera d evelopp ee pour analyser toutes les applications d evelopp ees pour les equipements d'Orange a n d'assurer des propri et es de s ecurit e, comme un comportement non malveillant. Par exemple, la plateforme de validation doit d etecter une situation o u l'application veut occuper une partie consid erable des ressources de calcul <b>comme</b> <b>par</b> <b>exemple</b> le processeur, la m emoire vive, le disque dur, etc. Si la plate-forme construite ne trouve pas un comportement malveillant, Orange donnera une certi cation pour l'application...|$|E
40|$|This PhD work {{is devoted}} to {{developing}} an experimental framework to investigate chemical spatiotemporal organization through mechanisms that could be at play during pattern formation in development. We introduce new tools to increase the versatility of DNA-based networks as pattern-forming systems. The emergence of organization in living systems is a longstanding fundamental question in biology. The two most influential ideas in developmental biology used to explain chemical pattern formation are Wolpert's positional information and Turing's reaction-diffusion self-organization. In the case of positional information, the pattern emerges from a pre-existing morphogen gradient across space that provides positional values as in a coordinate system. Whereas, the Turing mechanism relies on self-organization by driving a system of an initially homogeneous distribution of chemicals into an inhomogeneous pattern of concentration by a process that involves solely reaction and diffusion. Although numerical simulations and mathematical analysis corroborate the incredible potential of reaction-diffusion mechanisms to generate patterns, their experimental implementation is not trivial. And despite of the exceptional achievements in pattern formation with Belousov–Zhabotinsky systems, these are difficult to engineer, thus limiting their experimental implementation to few available mechanisms. In order to engineer reaction-diffusion systems that display spatiotemporal dynamics the following three key elements must be controlled: (i) the topology of the network (how reactions are linked to each other, i. e. in a positive or negative feedback manner), (ii) the reaction rates and (iii) the diffusion coefficients. Recently, using nucleic acids as a substrate to make programmable dynamic chemical systems together with the lessons from synthetic biology and DNA nanotechnology has appeared as an attractive approach due to the simplicity to control reaction rates and network topology by the sequence. Our experimental framework {{is based on the}} PEN-DNA toolbox, which involves DNA hybridization and enzymatic reactions that can be maintained out of equilibrium in a closed system for long periods of time. The programmability and biocompatibility of the PEN-DNA toolbox open new perspectives for the engineering of the reaction-diffusion chemical synthesis, in particular in two directions. Firstly, to study biologically-inspired pattern-forming mechanisms in simplified, yet relevant, experimental conditions. Secondly to build new materials that would self-build by a process inspired from embryo morphogenesis. We worked towards the goal of meeting the two requirements of Turing patterning, transferring chemical spatiotemporal behavior into material patterns, and imposing boundary conditions to spatiotemporal patterns. Therefore, the structure of this document is divided into four specific objectives resulting in four chapters. In chapter 1 we worked on testing a DNA-based reaction network with an inhibitor-activator topology. In chapter 2 we focused on developing a strategy to tune the diffusion coefficient of activator DNA strands. In chapter 3 we explored how chemical patterns determine the shape of a material. Finally, in chapter 4 we addressed the issue of controlling the geometry over a DNA-based reaction-diffusion system. Overall, we have expanded the number of available tools to study chemical and material pattern formation and advance towards Turing patterns with DNA. Cette thèse porte sur la mise en place et le développement d’une approche expérimentale pour l’étude de la dynamique spatio-temporelle de réseaux de réactions à base d’ADN. Nos résultats démontrent la capacité des réseaux d’ADN à se spatialiser sous la forme d’ondes progressives. Nous avons également pu obtenir des motifs stationnaires à base d’ADN et d’assemblages de billes. Ce travail contribue donc à la conception de motifs spatio-temporels de réactions chimiques et de matériaux par le biais de réseaux réactionnels biochimiques programmables. Nous apportons également de nouvelles données sur l’émergence d’ordre spatio-temporel à partir de processus de réaction-diffusion. De ce fait, cette étude contribue à une meilleure compréhension des principes fondamentaux qui régissent l’apparition d’une auto-organisation moléculaire dans un système chimique hors-équilibre. De plus, la combinaison de réseaux synthétiques d’ADN, du contrôle du coefficient de diffusion de plusieurs espèces d’ADN et de la micro-fluidique peut donner lieu à des motifs spatiaux stables, <b>comme</b> <b>par</b> <b>exemple,</b> les fameuses structures de Turing, ce qui tend à confirmer le rôle de celles-ci dans la morphogénèse...|$|E
50|$|Le Mariage <b>par</b> <b>exemple</b> ou les Époux à l’épreuve, one-act comedy.|$|R
5000|$|... 2010 : Hitchcock <b>par</b> <b>exemple,</b> {{illustrations}} by Florent Chavouet, Editions Naïve.|$|R
50|$|<b>Par</b> <b>exemple</b> le nombre 12496 {{engendre}} une période de 4 termes, le nombre 14316 une période de 28 termes.|$|R
40|$|La modélisation des systèmes dynamiques requiert la prise en compte d’incertitudes liées à l’existence inévitable de bruits (bruits de mesure, bruits sur la dynamique), à la méconnaissance de certains phénomènes perturbateurs mais également aux incertitudes sur la valeur des paramètres (spécification de tolérances, phénomène de vieillissement). Alors que certaines de ces incertitudes se prêtent bien à une modélisation de type statistique <b>comme</b> <b>par</b> <b>exemple</b> ! les bruits de mesure, d’autres se caractérisent mieux pa ! r des bornes, sans autre attribut. Dans ce travail de thèse, motivés par les {{observations}} ci-dessus, nous traitons le problème de l’intégration d’incertitudes statistiques et à erreurs bornées pour les systèmes linéaires à temps discret. Partant du filtre de Kalman Intervalle (noté IKF) développé dans [Chen 1997], nous proposons des améliorations significatives basées sur des techniques récentes de propagation de contraintes et d’inversion ensembliste qui, contrairement aux mécanismes mis en jeu par l’IKF, permettent d’obtenir un résultat garanti tout en contrôlant le pessimisme de l’analyse par intervalles. Cet algorithme est noté iIKF. Le filtre iIKF a la même structure récursive que le filtre de Kalman classique et délivre un encadrement de tous les estimés optimaux et des matrices de covariance possibles. L’algorithme IKF précédent évite quant à lui le problème de l’inversion des matrices intervalles, ce qui lui vaut de perdre des solutions possibles. Pour l’iIKF, nous proposons une méthode originale garantie pour l’inversion des matrices intervalle qui couple l’algorithme SIVIA (Set Inversion via Interval Analysis) et {{un ensemble}} de problèmes de propagation de contraintes. Par ailleurs, plusieurs mécanismes basés sur la propagation de contraintes sont également mis en œuvre pour limiter l’effet de surestimation due à la propagation d’intervalles dans la structure récursive du filtre. Un algorithme de détection de défauts basé sur iIKF est proposé en mettant en œuvre une stratégie de boucle semi-fermée {{qui permet de}} ne pas réalimenter le filtre avec des mesures corrompues par le défaut dès que celui-ci est détecté. A travers différents exemples, les avantages du filtre iIKF sont exposés et l’efficacité de l’algorithme de détection de défauts est démontré. ABSTRACT : In this thesis, {{a new approach to}} estimation problems under the presence of bounded uncertain parameters and statistical noise has been presented. The objective is to use the uncertainty model which appears as the most appropriate for every kind of uncertainty. This leads to the need to consider uncertain stochastic systems and to study how the two types of uncertainty combine : statistical noise is modeled as the centered gaussian variable and the unknown but bounded parameters are approximated by intervals. This results in an estimation problem that demands the development of mixed filters and a set-theoretic strategy. The attention is drawn on set inversion problems and constraint satisfaction problems. The former is the foundation of a method for solving interval equations, and the latter can significantly improve the speed of interval based arithmetic and algorithms. An important contribution of this work consists in proposing an interval matrix inversion method which couples the algorithm SIVIA with the construction of a list of constraint propagation problems. The system model is formalized as an uncertain stochastic system. Starting with the interval Kalman filtering algorithm proposed in [Chen 1997] and that we name the IKF, an improved interval Kalman filtering algorithm (iIKF) is proposed. This algorithm is based on interval conditional expectation for interval linear systems. The iIKF has the same structure as the conventional Kalman filter while achieving guaranteed statistical optimality. The recursive computational scheme is developed in the set-membership context. Our improvements achieve guaranteed interval inversion whereas the original version IKF [Chen 1997] uses an instance (the upper bound) of the interval matrix to avoid the possible singularity problems. This point of view leads to a sub-optimal solution that does not preserve guaranteed results, some solutions being lost. On the contrary, in the presence of unknown-but-bounded parameters and measurement statistical errors, our estimation approach {{in the form of the}} iIKF provides guaranteed estimates, while maintaining a computational burden comparable to that of classic statistical approaches. Several constraint based techniques have also been implemented to limit the overestimation effect due to interval propagation within the interval Kalman filter recursive structure. The results have shown that the iIKF out puts bounded estimates that enclose all the solutions consistent with bounded errors and achieves good overestimation control. iIKF is used to propose a fault detection algorithm which makes use of a Semi-Closed Loop strategy which does not correct the state estimate with the measure as soon as a fault is detected. Two methods for generating fault indicators are proposed : they use the a priori state estimate and a threshold based on the a posteriori and a priori covariance matrix, respectively, and check the consistency against the measured output. Through different examples, the advantages of the iIKF with respect to previous versions are exhibited and the efficiency of the iIKF based Semi-Closed Loop fault detection algorithm is clearly demonstrated...|$|E
40|$|Modern {{real-time}} systems {{tend to be}} mixed-critical, in {{the sense}} that they integrate on the same computational platform applications at different levels of criticality. Integration gives the advantages of reduced cost, weight and power consumption, which can be crucial for modern applications like Unmanned Aerial Vehicles (UAVs). On the other hand, this leads to major complications in system design. Moreover, such systems are subject to certification, and different criticality levels needs to be certified at different level of assurance. Among other aspects, the real-time scheduling of certifiable mixed critical systems has been recognized to be a challenging problem. Traditional techniques require complete isolation between criticality levels or global certification to the highest level of assurance, which leads to resource waste, thus loosing the advantage of integration. This led to a novel wave of research in the real-time community, and many solutions were proposed. Among those, one of the most popular methods used to schedule such systems is Audsley approach. However this method has some limitations, which we discuss in this thesis. These limitations are more pronounced in the case of multiprocessor scheduling. In this case priority-based scheduling looses some important properties. For this reason scheduling algorithms for multiprocessor mixed-critical systems are not as numerous in literature as the single processor ones, and usually are built on restrictive assumptions. This is particularly problematic since industrial real-time systems strive to migrate from single-core to multi-core and many-core platforms. Therefore we motivate and study a different approach that can overcome these problems. A restriction of practical usability of many mixed-critical and multiprocessor scheduling algorithms is assumption that jobs are independent. In reality they often have precedence constraints. In the thesis we show the mixed-critical variant of the problem formulation and extend the system load metrics to the case of precedence-constraint task graphs. We also show that our proposed methodology and scheduling algorithm MCPI can be extended to the case of dependent jobs without major modification and showing similar performance with respect to the independent jobs case. Another topic we treated in this thesis is time-triggered scheduling. This class of schedulers is important because they considerably reduce the uncertainty of job execution intervals thus simplifying the safety-critical system certification. They also simplify any auxiliary timing-based analyses that may be required to validate important extra-functional properties in embedded systems, such as interference on shared buses and caches, peak power dissipation, electromagnetic interference etc [...] The trivial method of obtaining a time-triggered schedule is simulation of the worst-case scenario in event-triggered algorithm. However, when applied directly, this method is not efficient for mixed-critical systems, as instead of one worst-case scenario they have multiple corner-case scenarios. For this reason, it was proposed in the literature to treat all scenarios into just a few tables, one per criticality mode. We call this scheduling approach Single Time Table per Mode (STTM) and propose a contribution in this context. In fact we introduce a method that transforms practically any scheduling algorithm into an STTM one. It works optimally on single core and shows good experimental results for multi-cores. Finally we studied the problem of the practical realization of mixed critical systems. Our effort in this direction is a design flow that we propose for multicore mixed critical systems. In this design flow, as the model of computation we propose a network of deterministic multi-periodic synchronous processes. Our approach is demonstrated using a publicly available toolset, an industrial application use case and a multi-core platform. Les systèmes temps-réels modernes ont tendance à obtenir la criticité mixte, dans le sens où ils intègrent sur une même plateforme de calcul plusieurs applications avec différents niveaux de criticités. D'un côté, cette intégration permet de réduire le coût, le poids et la consommation d'énergie. Ces exigences sont importantes pour des systèmes modernes <b>comme</b> <b>par</b> <b>exemple</b> les drones (UAV). De l'autre, elle conduit à des complications majeures lors de leur conception. Ces systèmes doivent être certifiés en prenant en compte ces différents niveaux de criticités. L'ordonnancement temps réel des systèmes avec différents niveaux de criticités est connu comme étant l’un des plus grand défi dans le domaine. Les techniques traditionnelles nécessitent une isolation complète entre les niveaux de criticité ou bien une certification globale au plus haut niveau. Une telle solution conduit à un gaspillage des ressources, et à la perte de l’avantage de cette intégration. Ce problème a suscité une nouvelle vague de recherche dans la communauté du temps réel, et de nombreuses solutions ont été proposées. Parmi elles, l'une des méthodes la plus utilisée pour ordonnancer de tels systèmes est celle d'Audsley. Malheureusement, elle a un certain nombre de limitations, dont nous parlerons dans cette thèse. Ces limitations sont encore beaucoup plus accentuées dans le cas de l'ordonnancement multiprocesseur. Dans ce cas précis, l'ordonnancement basé sur la priorité perd des propriétés importantes. C’est la raison pour laquelle, les algorithmes d'ordonnancement avec différents niveaux de criticités pour des architectures multiprocesseurs ne sont que très peu étudiés et ceux qu’on trouve dans la littérature sont généralement construits sur des hypothèses restrictives. Cela est particulièrement problématique car les systèmes industriels temps réel cherchent à migrer vers plates-formes multi-cœurs. Dans ce travail nous proposons une approche différente pour résoudre ces problèmes...|$|E
40|$|En général, les résultats des bioessais d’écotoxicologie sont étudiés par des méthodes statistiques et les paramètres estimés n’ont pas de signification biologique. La modélisation est apparue plus récemment en écotoxicologie et bénéficie même ces temps derniers d’un regain d’intérêt. Son développement s’effectue actuellement dans deux {{directions}} complémentaires que nous avons voulu présenter ici en en montrant les principaux apports. D’une part les effets sur les individus font l’objet d’efforts de modélisation afin de donner un sens biologique aux paramètres des tests de toxicité pour pouvoir intégrer des facteurs confondant au cours des tests <b>comme</b> <b>par</b> <b>exemple</b> des variations de la concentration d’exposition ou pour pouvoir déterminer les modes d’action des composés. D’autre part, l’écosystème étant l’objet d’étude {{par excellence}} de l’écotoxicologie, la modélisation est utilisée pour déduire les effets au niveau des populations à partir d’essais réalisés sur les individus. Jusqu’à présent, des approches classiques, qui se fondent sur l’équation d’Euler ou la diagonalisation de matrices de Leslie, ont été utilisées et ont permis une meilleure définition des paramètres à rechercher au niveau des tests de toxicité. D’autres approches sont à développer pour gagner en pertinence vis-à-vis du terrain (notamment hétérogénéité spatiale de la pollution et des habitats). Traditional analysis of toxicity tests provides toxicity parameters that are estimated with purely statistical methods. Consequently, these parameters {{do not have}} any intrinsic biological meaning and these methods provide no information about the mode of action of the tested chemicals. It is also difficult for these methods to change scale from the individual level to the population level, or to account for temporal and spatial heterogeneity. Modelling is an important tool in ecotoxicology and recently it appears to have gained more interest. Developments in modelling are currently expanding in two directions, modelling effects at the individual level and applying toxicity data obtained at the individual level to responses at the population level. The objective of the current study was to present these two complementary modelling approaches together with the opportunities they offer. Modelling at the individual level provides parameters that are biologically relevant. Modelling also facilitates the formulation and the testing of hypotheses concerning toxicity processes (physiological mode of action and kinetics). Confounding factors such as time, varying exposure concentrations, or feeding can also be incorporated into models. In this paper, two kinds of models were examined: biochemistry-based models (Hill models) and energy-based models (Dynamic Energy Budget models). In the Hill approach, effects are modelled as the interaction between chemicals and receptors in the organisms, which leads to a relationship between concentration and effects close to the logistic equation often used in toxicity test analysis. In the energy-based approach, models are built on the dynamic energy budget theory, in which energy derived from food is used for maintenance, growth and reproduction. The effect of compounds is then described as a change in one of the parameters describing these physiological functions. Kinetics are taken into account by a one-compartment model. The uptake rate is proportional to the exposure concentration, whereas the elimination rate is proportional to the concentration in the tissue. This model is simple but is relevant for many organisms and compounds (KOOIJMAN and BEDAUX, 1996). As time is taken into account through kinetic modelling, the estimation of the other parameters, such as the No Effect Concentration, does not depend on the exposure duration. An energy relevant model has many advantages. First, observed effect profiles are more in agreement with expectations (KOOIJMAN and BEDAUX, 1996). Second, it becomes possible to account for the fact that an effect on survival increases the amount of food consumed per surviving organisms, which in turn partly compensates for the negative effects of pollutants. Third, it allows for the examination of effects at the population level on density and biomass, complementary to the usual study of population growth rate. Most of the recent modelling research is related to deriving effects at the population level from effects at the individual level, because ecosystems are the target of ecotoxicology. Until recently, classical approaches, like the Euler equation or Leslie matrices, were used with population growth rates as endpoints. They provide interesting tools to determine the impact of life cycle parameters at the population level and to assess which level of effects has to be assessed. Even a simple approach such as that proposed by CALOW et al. (1997), separating the population into two different classes, juveniles and adults, can produce very interesting results. For instance, the authors showed that in populations for which females die just after reproduction, juvenile survival had much more importance than for populations where females can reproduce several times during their lifetime. The opposite is true concerning adult survival. However, these approaches do have some limits that make complementary approaches necessary to fully understand the effects of pollutants at the population level. First, they do not account for effects on the carrying capacity. SIBLY (1999) pointed out {{that there is a need}} for ecological studies on the effects of pollutants that measure their effects on density dependence and carrying capacity. Indeed an effect on population growth rate only accounts for a risk of disappearance for the population, but cannot help in the understanding of effects on biomass or density. Effects on the carrying capacity can have substantial effects at the ecosystem level, especially when studying species that constitute a food resource for other species. Second, more complex tools have to be developed to take into account spatial heterogeneity of pollution and habitats in order to be relevant from an ecosystem point of view. Indeed, it has been shown that uncontaminated sites can be significantly disturbed if they are connected, through the migration of organisms, with contaminated sites (SPROMBERG et al., 1998) ...|$|E
5000|$|Dans le cadre de ces concerts, Le Divan du Monde accueillera également des danseurs tel que la hip-hoppeuse Bintou Dembélé <b>par</b> <b>exemple</b> ...|$|R
5000|$|Désaccordée <b>comme</b> <b>par</b> de la neige, Tübingen, 22 May 1986, 1989.|$|R
5000|$|... 2010 Prix Gémeaux {{nomination}} for [...] "Best Original Theme" [...] (<b>Comme</b> <b>Par</b> Magie) ...|$|R
40|$|The European Nitrates Directive 91 / 6 / 76 /EEC aims {{to ensure}} water quality by {{preventing}} pollution of surface and groundwater induced by nitrates originating from agricultural sources and by promoting agronomical good practices. While {{the implementation of}} this Directive seems effective, it appears however {{that the use of}} nitrogen has still increased by 6 % {{over the last four years}} in 27 European countries. Furthermore, agricultural sources would be still at the origin of 50 % of the total amount of nitrogen discharged into surface waters ([URL] In Wallonia (Belgium), the Nitrates Directive has been transposed under the Sustainable Nitrogen Management in Agriculture Program (PGDA). Launched in 2002, it involves different sets of actions, like rules definitions concerning fertilizers application, specific and appropriate crop management in vulnerable areas, the control of potentially leachable nitrogen (APL) levels in soils, etc. This is the global context in which lies the present thesis. The main aim is to optimise the nitrogen fertiliser practices to ensure that the needs of a winter wheat culture (Triticum aestivum L.) could be met while reducing the environmental pressure. It relies on the use of crop models, which describe the growth and the development of a culture interacting with its environment, namely the soil and the atmosphere. The major difficulty while working with crop models and model-based decision support tools lies in the fact that different sources of uncertainties have an impact on the modelled phenomena. Indeed, crop models are constituted by a consequent number of differential non-linear equations, involving a lot of parameters which need to be determined as accurately as possible in order to match as close as possible observed sequences of measurements. The first source of uncertainty is thus constituted by the parameters definition. Once the model has been correctly and robustly calibrated it can be used to perform predictions. However, in an agronomical context, the time-delay between sowing and harvest is consequent. As the end-season yield is often the expected output, the uncertainty linked to the non-knowledge of the future implies for the modeller to refer to different hypothesis concerning upcoming climatic scenarios. Finally, moving from models to decision systems dealing with N management involves a last source of uncertainty. Indeed the main problem is that the impact of a given practice is delayed in time from its realisation. In addition to the uncertainty linked to climatic projections themselves, it is highly important to consider the interactions between the practices and the climate. Furthermore, in a decision-making process, it could be highly relevant to know the uncertainty's estimation that could be tolerated on the decision [...] Therefore, the present thesis aims to study these different sources of uncertainty in order to design an efficient decision support system. It is divided into five parts. In the first part, a Bayesian sampling algorithm, known as DREAM (DiffeRential Evolution Adaptative Metropolis) will be presented. It was successfully coupled with the STICS soil-crop model used in this study. The a posteriori probability density function of many parameters was sampled in order to improve the simulations of the growth of a winter wheat culture (Triticum aestivum L.). The DREAM algorithm offers different advantages in comparison to usual methods. Among these, it is possible to study i) the most probable a posteriori parameters distributions, ii) the parameters correlations, and iii) the uncertainties impacted on model outputs. Furthermore, a new version of the likelihood function was proposed, making an explicit use of the coefficient of variation. Results showed that it allowed the noise existing on measurements to be considered, but also the heteroscedasticity phenomenon usually encountered in biological growth processes. In parallel, assimilation data is another way to improve models simulations. These techniques allow considering measurements performed in real-time (e. g. remote measures of LAI or soil water content) in order to correct and adjust the possible drift of model simulations. In particular, a recently developed algorithm, known as variational filter, was evaluated. Its superiority, both in term of state variables simulations improvement and parameter resampling, was demonstrated. The third part of the research focuses on the real-time end season yield prediction. It involves building climate matrix ensembles, combining different time ranges of projected mean data and real measured weather originating from the historical records. As the crop growing season progresses, the effects of real monitored data plays a greater role and the prediction reliability increases. Our results demonstrated that a reliable predictive delay of 3 - 4 weeks before harvest could be obtained. Finally, using real-time data acquired with a micrometeorological station enabled to (i) predict, daily, potential yield at the local level, (ii) detect stress occurrence, and (iii) quantify yield losses (or gains). Being based on projected seasonal norms, this methodology is in opposition to another technique that consists to offer a panel of solution for what concerns the future. Such probabilistic technique relies on the use of stochastic weather generator (LARS-WG in this case). However, in the fourth part of this thesis, on the basis of the convergence in law theorem, it was demonstrated that in 90 % of the climatic situations, both approaches were equivalent, exhibiting RRMSE and normalised deviation criteria inferior to 10 %. Furthermore the two approaches offered similar predictive delay-time. The main difference between techniques lies in the finality. The first allows to quickly simulate the remaining yield potential, while the second aims to quantify the uncertainty level associated to the predictions. In the fifth and last part of this thesis, in order to quantify the uncertainty level associated to different modalities of N applications, the STICS model answers were studied under stochastic climatic realisations. It was demonstrated that, if no N was applied, under our temperate climatic conditions, the yield distribution could be considered as normal. However, with increasing N practices, the asymmetry level was found itself increasing. As soon as N was applied, not only were the yields higher, but also was the probability to achieve yields that were at least superior to the mean of the distribution. This undoubtedly reduced the risk for the farmer to achieve low yields levels. To summary all the researches conducted in this thesis, a N strategic decision support system was developed. In a general way, for what concerns the Hesbaye Region, the superiority of three fractions N protocols was demonstrated. In addition, the three rates fertilisation management based on the systematic applications of 60 kgN. ha- 1 at tillering and stem extension stages and offering the possibility to adapt the flag-leaf fraction in real-time appeared as an optimal strategy. Within this tool, the uncertainty associated to climatic variability could be finely characterised, and the risk encountered by the farmer was quantified for different investigated practices. But far more important, it was demonstrated that N management could be optimised in real-time. In a general way, the research should be pursued by studying more fundamentally and systematically a wide range of different agro-environmental situations. In particular, it would be interesting to study of the Genotype × Environment × Cultural practices interactions to ensure food security in a climatic changing world. La directive Européenne 91 / 6 / 76 /EEC vise à protéger la qualité de l’eau en prévenant la pollution des eaux souterraines et superficielles par les nitrates provenant de sources agricoles et en promouvant l’usage de bonnes pratiques. Si la mise en œuvre de cette directive s’avère efficace, il apparaît cependant que l’utilisation d’azote a augmenté de 6 % au cours des quatre dernières années dans les 27 Etats membres et que l’agriculture est toujours à l’origine de plus de 50 % de la quantité totale d’azote déversée dans les eaux superficielles ([URL] En Wallonie, la directive «Nitrates» est transposée sous la forme du Programme de Gestion Durable de l'Azote (PGDA). Entré en vigueur en 2002, celui-ci comporte un ensemble de mesures <b>comme,</b> <b>par</b> <b>exemple,</b> la fixation de règles au niveau de l’épandage des fertilisants, la mise en place de modes de gestion spécifiques des cultures en zones vulnérables, la mesure des teneurs en azote potentiellement lessivable (APL) dans les sols, etc. La thèse de doctorat s'inscrit dans ce contexte général qui vise à optimiser la quantité de fertilisants azotés à apporter à une culture de blé d’hiver (Triticum aestivum L.). Elle s’appuie sur un modèle de culture ou modèle écophysiologique, qui décrit la croissance et le développement d’une culture en interaction avec le sol et l’atmosphère. La difficulté majeure dans l’utilisation d’un tel modèle en tant qu'outil d’aide à la décision est liée aux nombreuses incertitudes qui interviennent à différents niveaux. En effet, les modèles de culture dynamiques sont constitués d’un grand nombre d’équations différentielles non-linéaires, comportant de nombreux paramètres à estimer pour que les sorties se rapprochent le plus possible de séquences observées. Une première source d’incertitude existe donc au niveau des paramètres du modèle. Une fois le modèle robustement calibré, il peut être utilisé à des fins prédictives. Toutefois, dans un contexte agronomique, le délai entre la date de semis et celle de récolte est important. Comme il est souhaitable de prédire le rendement final d’un point de vue quantitatif et qualitatif, différentes hypothèses sur les scénarios climatiques qui interviennent pendant la saison culturale doivent être posées. Une deuxième source d’incertitude est donc liée au climat. Enfin, le passage d'un modèle à un outil fonctionnel de gestion des fertilisants azotés nécessite de franchir une étape supplémentaire car l’impact réel des fertilisants n’est connu qu'en fin de saison. Une troisième source d’incertitude est donc liée à la quantification du niveau d'erreur toléré sur la gestion des fertilisants, compte tenu des interactions existant entre la pratique envisagée et la réalisation climatique. Cette thèse de doctorat a pour objectif d'étudier les différentes sources d'incertitudes afin d’aboutir à un outil d'aide à la décision efficace. Elle comporte cinq parties. Dans la première partie, une approche Bayésienne d'identification des paramètres du modèle de culture STICS est présentée. Elle repose sur l'algorithme DREAM (DiffeRential Evolution Adaptative Metropolis). Les distributions a posteriori de plusieurs paramètres du modèle STICS sont échantillonnées en vue d'améliorer les simulations de la croissance et du développement du blé d'hiver. Par rapport aux algorithmes de type Simplex habituellement utilisés dans les modèles de culture, le couplage de STICS avec DREAM fournit une approximation de la distribution a posteriori des paramètres, une évaluation des corrélations qui existent entre eux et surtout quantifie l'incertitude dans l'estimation des rendements. Par ailleurs, une fonction de vraisemblance faisant explicitement mention du coefficient de variation est proposée. Les résultats montrent que cette dernière permet de prendre en compte le bruit sur les mesures et l'hétéroscédasticité régulièrement rencontré dans les modèles de culture. En parallèle ou en complément aux approches d’identification paramétrique, des techniques de filtrage permettent d’améliorer les simulations des modèles de culture. Au moyen d'algorithmes couplés au modèle de culture, ces techniques visent à intégrer dans le modèle des mesures réalisées en cours de saison, <b>comme</b> <b>par</b> <b>exemple</b> l'indice de développement foliaire LAI ou la teneur en eau du sol, de manière à corriger la trajectoire modélisée de l'évolution des variables d'état. La capacité d'un algorithme récemment développé et connu sous le nom de filtre variationnel est évaluée et sa supériorité en termes d’estimation paramétrique et de réduction des incertitudes sur les variables de sortie est démontrée par rapport à celle de méthodes plus conventionnelles, basées sur le filtre de Kalman. La troisième partie de cette étude se focalise sur la prédiction du rendement à l'aide du modèle STICS en s’attachant aux incertitudes liées aux entrées climatiques. La méthodologie utilisée consiste à construire des 'ensembles de matrices' climatiques évoluant au cours de la saison, constitués d'une part de données climatiques réelles mesurées au fur et à mesure de la croissance de la culture, et d'autre part de données climatiques moyennes calculées au départ d'une base de données locale historique. Au fur et à mesure de l'avancement de la saison, la part des premières données devient prépondérante par rapport à celle des secondes. Il est montré que cette méthodologie, que nous appellerons ‘climat moyen’ permet de prédire les rendements environ un mois avant la date de récolte avec un intervalle de confiance de 10 %. En outre, cette approche permet de quantifier les potentialités restantes de rendement d'une culture donnée dans des conditions agro-environnementales données et de détecter l'occurrence de stress lors de la croissance des cultures. Dans la quatrième partie, une méthode reposant sur la génération d'un grand nombre de réalisations climatiques stochastiques (LARS-Weather Generator) est mise en œuvre. En se basant sur le théorème de la convergence en loi, il est démontré que les approches stochastiques et ‘climat moyen’ présentent des critères statistiques comme la RRMSE et la déviation normalisée inférieurs à 10 % et sont dans 90 % des situations climatiques équivalentes en termes de simulation du rendement. Elles offrent par ailleurs le même délai prédictif. La distinction entre les méthodes se situe surtout au niveau de la finalité visée, l'une permettant de simuler rapidement le potentiel restant à la culture, l'autre assurant la quantification du niveau d'incertitude sur la prédiction. Dans la cinquième et dernière partie, la réponse du modèle STICS est étudiée sous des réalisations climatiques stochastiques en vue de quantifier le niveau d'incertitude associé à différentes modalités d'apports de fertilisants azotés. Il est montré que, sous nos latitudes, en l’absence de fertilisant azoté, les distributions de rendements sont normales. Avec l'accroissement des niveaux de fertilisation, un degré d'asymétrie croissant est observé. Dès l'instant où un fertilisant azoté est appliqué, non seulement les rendements augmentent, mais la fréquence des rendements au moins supérieurs à la moyenne croit également. Ceci réduit inéluctablement le risque pour l'agriculteur d'obtenir des rendements bas. Pour synthétiser les études menées au long de cette thèse, un outil de gestion stratégique de l'azote, c'est-à-dire sans connaissance a priori des conditions climatiques à venir, est mis au point. D'une manière générale, pour ce qui concerne nos régions limoneuses, la supériorité des fertilisations en trois fractions est mise en évidence. Un système de gestion de l'azote, basé sur un apport de deux modalités de 60 kilos d'azote par hectare appliqués aux stades tallage et redressement et offrant la possibilité de moduler la fraction de dernière feuille, semble optimal. L'incertitude associée aux prédictions et issue de la variabilité climatique peut être caractérisée finement et il est possible de quantifier le risque encouru par l'agriculteur qui envisage différentes pratiques afin de procéder à une optimisation de celles-ci. D’une manière générale, les recherches pourraient être poursuivies en étudiant de façon plus fondamentale et plus systématique différentes situations agro-environnementales. En particulier, il serait intéressant de développer l’étude de l'interaction Genotype × Environnement × Pratiques culturales pour garantir la sécurité alimentaire dans un contexte de changement climatique...|$|E
40|$|A {{wide range}} of {{different}} image modalities can be found today in medical imaging. These modalities allow the physician to obtain a non-invasive view of the internal organs {{of the human body}}, such as the brain. All these three dimensional images are of extreme importance in several domains of medicine, for example, to detect pathologies, follow the evolution of these pathologies, prepare and realize surgical planning with, or without, the help of robot systems or for statistical studies. Among all the medical image modalities, Magnetic Resonance (MR) imaging has become of great interest in many research areas due to its great spatial and contrast image resolution. It is therefore perfectly suited for anatomic visualization of the human body such as deep structures and tissues of the brain. Medical image analysis is a complex task because medical images usually involve a large amount of data and they sometimes present some undesirable artifacts, as for instance the noise. However, the use of a priori knowledge in the analysis of these images can greatly simplify this task. This prior information is usually represented by the reference images or atlases. Modern brain atlases are derived from high resolution cryosections or in vivo images, single subject-based or population-based, and they provide detailed images that may be interactively and easily examined in their digital format in computer assisted diagnosis or intervention. Then, in order to efficiently combine all this information, a battery of registration techniques is emerging based on transformations that bring two medical images into voxel-to-voxel correspondence. One of the main aims of this thesis is to outline the importance of including prior knowledge in the medical image analysis framework and the indispensable role of registration techniques in this task. In order to do that, several applications using atlas information are presented. First, the atlas-based segmentation in normal anatomy is shown as it is a key application of medical image analysis using prior knowledge. It consists of registering the brain images derived from different subjects and modalities within the atlas coordinate system to improve the localization and delineation of the structures of interest. However, the use of an atlas can be problematic in some particular cases where some structures, for instance a tumor or a sulcus, exists in the subject and not in the atlas. In order to solve this limitation of the atlases, a new atlas-based segmentation method for pathological brains is proposed in this thesis as well as a validation method to assess this new approach. Results show that deep structures of the brain can still be efficiently segmented using an anatomic atlas even if they are largely deformed because of a lesion. The importance of including a priori knowledge is also presented in the application of brain tissue classification. The prior information represented by the tissue templates can be included in a brain tissue segmentation approach thanks to the registration techniques. This is another important issue presented in this thesis and it is analyzed through a comparative study of several non-supervised classification techniques. These methods are selected to represent the whole range of prior information {{that can be used in}} the classification process: the image intensity, the local spatial model, and the anatomical priors. Results show that the registration between the subject and the tissue templates allows the use of prior information but the accuracy of both the prior information and the registration highly influence the performance of the classification techniques. Another aim of this thesis is to present the concept of dynamic medical image analysis, in which the prior knowledge and the registration techniques are also of main importance. Actually, many medical image applications have the objective of statically analyzing one single image, as for instance in the case of atlas-based segmentation or brain tissue classification. But in other cases the implicit idea of changes detection is present. Intuitively, since the human body is changing continuously, we would like to do the image analysis from a dynamic point of view by detecting these changes, and by comparing them afterwards with templates to know if they are normal. The need of such approaches is even more evident in the case of many brain pathologies such as tumors, multiple sclerosis or degenerative diseases. In these cases, the key point is not only to detect but also to quantify and even characterize the evolving pathology. The evaluation of lesion variations over time can be very useful, for instance in the pharmaceutical research and clinical follow up. Of course, a sequence of images is needed in order to do such an analysis. Two approaches dealing with the idea of change detection are proposed as the last (but not least) issue presented in this work. The first one consists of performing a static analysis of each image forming the data set and, then, of comparing them. The second one consists of analyzing the non-rigid transformation between the sequence images instead of the images itself. Finally, both static and dynamic approaches are illustrated with a potential application: the cortical degeneration study is done using brain tissue segmentation, and the study of multiple sclerosis lesion evolution is performed by non-rigid deformation analysis. In conclusion, the importance of including a priori information encoded in the brain atlases in medical image analysis has been put in evidence with a {{wide range of}} possible applications. In the same way, the key role of registration techniques is shown not only as an efficient way to combine all the medical image modalities but also as a main element in the dynamic medical image analysis. A wide range of different image modalities can be found today in medical imaging. These modalities allow the physician to obtain a non-invasive view of the internal organs of the human body, such as the brain. All these three dimensional images are of extreme importance in several domains of medicine, for example, to detect pathologies, follow the evolution of these pathologies, prepare and realize surgical planning with, or without, the help of robot systems or for statistical studies. Among all the medical image modalities, Magnetic Resonance (MR) imaging has become of great interest in many research areas due to its great spatial and contrast image resolution. It is therefore perfectly suited for anatomic visualization of the human body such as deep structures and tissues of the brain. Medical image analysis is a complex task because medical images usually involve a large amount of data and they sometimes present some undesirable artifacts, as for instance the noise. However, the use of a priori knowledge in the analysis of these images can greatly simplify this task. This prior information is usually represented by the reference images or atlases. Modern brain atlases are derived from high resolution cryosections or in vivo images, single subject-based or population-based, and they provide detailed images that may be interactively and easily examined in their digital format in computer assisted diagnosis or intervention. Then, in order to efficiently combine all this information, a battery of registration techniques is emerging based on transformations that bring two medical images into voxel-to-voxel correspondence. One of the main aims of this thesis is to outline the importance of including prior knowledge in the medical image analysis framework and the indispensable role of registration techniques in this task. In order to do that, several applications using atlas information are presented. First, the atlas-based segmentation in normal anatomy is shown as it is a key application of medical image analysis using prior knowledge. It consists of registering the brain images derived from different subjects and modalities within the atlas coordinate system to improve the localization and delineation of the structures of interest. However, the use of an atlas can be problematic in some particular cases where some structures, for instance a tumor or a sulcus, exists in the subject and not in the atlas. In order to solve this limitation of the atlases, a new atlas-based segmentation method for pathological brains is proposed in this thesis as well as a validation method to assess this new approach. Results show that deep structures of the brain can still be efficiently segmented using an anatomic atlas even if they are largely deformed because of a lesion. The importance of including a priori knowledge is also presented in the application of brain tissue classification. The prior information represented by the tissue templates can be included in a brain tissue segmentation approach thanks to the registration techniques. This is another important issue presented in this thesis and it is analyzed through a comparative study of several non-supervised classification techniques. These methods are selected to represent the whole range of prior information that can be used in the classification process: the image intensity, the local spatial model, and the anatomical priors. Results show that the registration between the subject and the tissue templates allows the use of prior information but the accuracy of both the prior information and the registration highly influence the performance of the classification techniques. Another aim of this thesis is to present the concept of dynamic medical image analysis, in which the prior knowledge and the registration techniques are also of main importance. Actually, many medical image applications have the objective of statically analyzing one single image, as for instance in the case of atlas-based segmentation or brain tissue classification. But in other cases the implicit idea of changes detection is present. Intuitively, since the human body is changing continuously, we would like to do the image analysis from a dynamic point of view by detecting these changes, and by comparing them afterwards with templates to know if they are normal. The need of such approaches is even more evident in the case of many brain pathologies such as tumors, multiple sclerosis or degenerative diseases. In these cases, the key point is not only to detect but also to quantify and even characterize the evolving pathology. The evaluation of lesion variations over time can be very useful, for instance in the pharmaceutical research and clinical follow up. Of course, a sequence of images is needed in order to do such an analysis. Two approaches dealing with the idea of change detection are proposed as the last (but not least) issue presented in this work. The first one consists of performing a static analysis of each image forming the data set and, then, of comparing them. The second one consists of analyzing the non-rigid transformation between the sequence images instead of the images itself. Finally, both static and dynamic approaches are illustrated with a potential application: the cortical degeneration study is done using brain tissue segmentation, and the study of multiple sclerosis lesion evolution is performed by non-rigid deformation analysis. In conclusion, the importance of including a priori information encoded in the brain atlases in medical image analysis has been put in evidence with a wide range of possible applications. In the same way, the key role of registration techniques is shown not only as an efficient way to combine all the medical image modalities but also as a main element in the dynamic medical image analysis. Resumen Hoy en día existen muchas modalidades de imágenes médicas digitales que permiten a los médicos el estudio in vivo de los órganos del cuerpo humano, como por ejemplo del cerebro. Estas imágenes son muy útiles en muchos campos de la medicina como por ejemplo en la detección, seguimiento y estudio de patologías, en la preparación y realización de operaciones quirúrgicas asistidas por ordenador o en estudios estadísticos. De entre todos los tipos de imágenes medicas, destaca la imagen de Resonancia Magnética (RM) por su alta resolución espacial, su gran variedad de posibles contrastes y su inocuidad al no utilizar radiación ionizante. Estas características hacen que la imagen por RM sea muy adecuada para la visualización anatómica del cuerpo humano, por ejemplo para visualizar las estructuras y los tejidos del cerebro. El análisis de imágenes médicas es una tarea compleja ya que normalmente estas imágenes consituyen grandes volúmenes de datos y, además, presentan ruido y otros artefactos de la imagen como los cambios de iluminación. Sin embargo, la inclusión de información a priori en el análisis de estas imágenes puede facilitar mucho su estudio. La información a priori está normalmente representada por las imágenes de referencia o atlas que determinan un espacio concreto en el cual se describe la anatomía, por ejemplo, del cerebro humano. Actualmente los atlas del cerebro (creados a partir de secciones criogénicas o de imágenes in vivo, basados en un solo sujeto o en toda una población) proporcionan imágenes digitales muy detalladas que pueden ser examinadas interactiva y fácilmente en el diagnostico de tratamientos y planificación de los mismos por ordenador. En consecuencia, para poder combinar de manera eficiente toda la información contenida en los distintos tipos de imágenes médicas surgen las técnicas de registro* que proporcionan las transformaciones geométricas que sitúan dos imágenes en correspondencia anatómica voxel a voxel. Uno de los objetivos principales de esta tesis es remarcar la importancia de incluir información a priori en el proceso de análisis de imágenes médicas así como resaltar el papel indispensable de los métodos de registro en este proceso. Para demostrarlo, presentamos distintas aplicaciones que utilizan atlas. Primero, presentamos la aplicación de segmentación basada en atlas en sujetos con anatomía normal ya que es una de las aplicaciones principales que incluyen información a priori. La segmentación basada en atlas consiste en registrar una o varias imágenes del cerebro en el sistema de referencia del atlas para facilitar la localización y segmentación de las estructuras de interés. Sin embargo, el uso del atlas está limitado en algunos casos donde puede haber estructuras, como un tumor o un sulcus, que estén presentes en el paciente pero no en el atlas. Para solventar este problema, se propone un nuevo método de segmentación basado en atlas para cerebros patológicos así como un método para su validación. Los resultados obtenidos demuestran que las estructuras de interés del cerebro se pueden segmentar utilizando la información contenida en un atlas aunque estén muy deformadas debido a una lesión. La importancia de la utilización de la información a priori se demuestra también en la clasificación de los distintos tejidos del cerebro. La información a priori contenida en los atlas de tejidos del cerebro puede ser utilizada por los métodos de clasificación gracias al registro de imágenes. ésta es también una aplicación importante y se presenta a través del estudio comparativo de varias técnicas de clasificación no supervisadas. Los métodos de clasificación analizados han sido elegidos de manera que representen la diversidad de información a priori que se puede utilizar, es decir, la intensidad de la imagen, la información local espacial y la información global contenida en los atlas. Los resultados obtenidos demuestran que el uso de atlas es posible gracias a las técnicas de registro pero que la calidad de la clasificación depende mucho de la precisión del método de registro y de la calidad de la información a priori utilizados. El tercer objetivo de esta tesis es presentar el concepto de análisis dinámico de las imágenes médicas, en el cual, la información a priori y los métodos de registro siguen siendo de mucha importancia. En realidad, muchas aplicaciones de las imágenes medicas tienen como objetivo el análisis estático de una imagen como, por ejemplo, en el caso de la segmentación basada en atlas o en la clasificación de tejidos del cerebro. Pero en otros casos la idea de detección de cambios es implícita. Intuitivamente, ya que en el cuerpo se producen cambios continuamente, podríamos analizar las imágenes medicas desde un punto de vista dinámico, es decir, detectando los cambios que se producen y comparándolos con modelos de cambios para determinar si son normales. La necesidad de detección de cambios es aún más evidente en el estudio de ciertas patologías del cerebro como por ejemplo tumores, esclerosis múltiple o enfermedades degenerativas. En estos casos, la clave está no sólo en detectar sino también en cuantificar e incluso caracterizar la evolución de la lesión. Este tipo de estudios pueden ser muy útiles por ejemplo en la investigación farmacéutica o en el seguimiento clínico. Evidentemente, para realizar este tipo de estudios evolutivos se considera que se dispone de una secuencia de imágenes a distintos intervalos de tiempo. Dos métodos distintos que lidian con la idea de detección de cambios son presentados en esta tesis. El primero consiste en realizar el análisis estático de cada una de las imágenes que forman la secuencia y luego comparar los resultados. El segundo método consiste en realizar el análisis de la transformación obtenida entre las imágenes de la secuencia, en vez de realizar el análisis de cada imagen. Finalmente, presentamos una aplicación potencial de cada uno de los métodos como ejemplo: el estudio de la degeneración cortical del cerebro que esta hecho a partir de la clasificación de tejidos y el estudio de la evolución de esclerosis múltiple que esta hecha a partir del análisis de la transformación obtenida por registro. En conclusión, se ha puesto en evidencia la importancia de considerar la información a priori de los atlas anatómicos del cerebro en el análisis de imágenes médicas en una gran variedad de aplicaciones. De la misma manera, el papel decisivo de los métodos de registro ha sido presentado no sólo como una manera eficiente de combinar las distintas modalidades de imágenes médicas sino también como un elemento importante en el análisis dinámico de las mismas. [...] * Anglicismo de registration. Dans le domaine de l'imagerie médicale il existe une grande variété de modalités d'images 3 D qui permettent aux médecins d'obtenir une visualisation non invasive des organes du corps humain, <b>comme</b> <b>par</b> <b>exemple</b> du cerveau. Toutes ces modalités d'images sont très importantes dans divers domaines de la médicine <b>comme</b> <b>par</b> <b>exemple</b> pour détecter certaines pathologies, pour suivre l'évolution des ces pathologies, pour préparer et pour réaliser des opérations chirurgicales avec ou sans l'aide de systèmes robotiques ou même pour des études statistiques. Parmi toutes les modalités d'images médicales, l'Imagerie par Résonance Magnétique (IRM) est devenue très importante grâce à sa grande résolution spatiale et son fort contraste pour les tissues mous. L'IRM est donc très bien adaptée pour la visualisation anatomique du corps humain, par exemple des structures profondes ou des tissus du cerveau. L'analyse des images médicales est très complexe car ces images sont représentées par de grandes quantités de données et elles présentent parfois des effets non désirables comme le bruit. Cependant l'utilisation d'information a priori pendant le traitement d'images peut faciliter beaucoup leur analyse. Normalement, cette information a priori est représentée par les images dites de référence ou atlas, qui déterminent un espace commun ou l'anatomie humaine peut être précisément représent ée comme c'est le cas du cerveau. Aujourd'hui les atlas sont dérivés des images cryosectionées de grande résolution ou des images in vivo et ils sont basés sur un seul individu ou sur une population d'individus. Dans tous les cas, elles fournissent des images très détaillées qui peuvent être facilement analysées dans leur format digital pour des applications comme la vision et l'aide au diagnostic par ordinateur. Finalement, il existe une grande variété des techniques de recalage basées sur des transformations qui donnent une correspondance voxel-a-voxel des images et qui permettent de combiner très efficacement toutes les informations contenues dans les images médicales. Un des principaux objectifs de cette thèse c'est de souligner l'importance d'inclure dans l'analyse des images médicales l'information connue a priori et le rôle indispensable des techniques de recalage. Différentes applications qui utilisent l'information contenue dans des atlas sont présentées. Tout d'abord, la segmentation basée sur un atlas est présentée car c'est une application de pointe dans l'utilisation d'information a priori. Il s'agit de recaler des images du cerveau dérivées des différents individus ou modalités d'image avec un atlas qui permettra d'améliorer la localisation et segmentation des structures d'intérêt. Cependant, l'utilisation d'atlas et parfois limitée dans certains cas ou quelques structures, par exemple un sulcus ou une tumeur, sont présents dans le patient mais ne sont pas présents dans l'atlas. On propose dans ce travail une nouvelle méthode de segmentation basée sur un atlas dans les cas de cerveaux pathologiques ainsi qu'une méthode pour sa validation. Les résultats montrent que les structures profondes du cerveau peuvent encore être segmentées efficacement à l'aide d'un atlas même si elles ont été largement déformées par une lésion. La pertinence d'inclure l'information a priori est aussi présentée dans le cadre de la segmentation des tissus principaux du cerveau. L'information con...|$|E
40|$|Due to its {{prominent}} directionality and strength, H-bonds {{are ones}} {{of the most}} widely used non-covalent interactions in supramolecular chemistry. Despite its relative high strength (energy of an H-bond in the gas phase typically ranges between 0 − 5 Kcal mol− 1) in comparison with other non-covalent interactions, association of two molecules by means of a single H-bond leads to complexes displaying low thermodynamical stabilities, thus limiting their exploitation in the non-covalent synthesis of functional materials for real-world applications. Thereby, when stronger interactions are required, the general engineering approach focuses on the covalent synthesis of rigid planar molecular scaffolding in which several H-bonding donating (D) and accepting (A) moieties are arranged into a so-called ‘H-bonding array’. Due to the selective recognition processes and to the tunability of their association strength, multiple H-bonding arrays have become an indispensable molecular module in the tool-box of supramolecular chemists, allowing, through selective self-assembly and/or self-organization processes, the bottom-up preparation of functional materials such as liquid crystals, patterned surfaces and supramolecular polymers. In principle, the stability of H-bonded supramolecular complexes could be modulated in an indefinite number of ways. For example, when stronger interactions (e. g., higher association constant values) are required, the increase of the number of the H-bonding sites represents one of the efficient strategy to reinforce the stability of the ultimate assembly. Nevertheless, a strong Ka value is not always requested. In fact, whilst highly stable complexes are required in the field of supramolecular polymers, whose properties at the molecular level (such as degree of polymerization, Dp, and viscosity) result linearly correlated to the Ka values, these may instead be detrimental for the construction of more sophisticated hierarchized nano-architectures, arising from a delicate interplay between internal (e. g. ii stacking, solvophobic/solvophilic interactions) and external (e. g. time, temperature, concentration, etc.) factors. The aim of this thesis is to design and synthesize novel triple H-bonding arrays (DAD, ADD and DDD) based on five-membered heteroaromatic rings. The proposed use of thiolyl, oxolyl, azolyl, and triazolyl scaffoldings for recognition systems, it is intended as a mean to better achieve the control on the binding properties and selectivity of triple H-bondind recognition arrays, allowing an easy tunability of the binding motifs. With the variation of the substituents and the heteroatom onto the hetero-aromatic rings, it has been intended to create a selection of versatile, structurally similar, host-guest pairs complexes that display different association constants (Ka) in order to better match the requirements of different supramolecular applications. Focusing on the most relevant factors that influence the association constants of hydrogen bonded complexes, {{in the first part of}} Chapter 1 the reader is introduced on how specific H-bonding arrays, featuring wide ranges of Ka values (spanning among eight orders of magnitude) can be designed. Subsequently, the second part is focused on the physical and chemical properties of a large variety of H-bonding assembled molecular modules that upon self-assembly and self-organization processes opened new ways towards novel fascinating applications. Figure 1 Designed H-bonding arrays based on 5 -membered heterocycles. Chapter 2 deals with the description of the synthetic efforts undertaken towards the preparation of the DAD and DDD H-bonding arrays. The first two subsections (2. 1 - 2) describe the rethrosynthetic approaches and the results of the unsuccessful methodological routes (through Buchwald-Hartwig amidation cross-coupling reactions, reduction of azido-derivatives and nucleophilic addition of organo-metallic reagents to isocyanate derivatives as produced through Curtius rearrangement) tackled to introduce amidic and/or ureidic functions at the 2 -position of five-membered heteroaromatic rings. Several DAD H-bonding arrays based on thiolyl scaffolding were successfully synthesized (sections 2. 3). Figure 2 Synthesized thiolyl DAD H-bonding arrays. In section 2. 4 are presented the synthetic step undertaken in the attempt to generate DAD arrays based on oxalyl derivatives. Unfortunately the introduction of electron-donating groups such as amidic or carbamic functions to the ring led to very unstable intermediates, and thus the amido-oxolyl derivatives capable of recognition mediated by triple H bonding were never isolated. Figure 3 Synthesized oxolyl-protected DAD H-bond array. The synthetic strategies towards the synthesis of DDD arrays based on of azolyl scaffolding are described in section 2. 5. Protected azolyl module 182 (see Figure 4) was synthesized in thirteen steps starting from the pyrrole module. Unfortunately, due to the complications encountered in the cleavage of the N-azolyl protecting group, the synthesis of the azolyl DDD H-bonding arrays based could not be finally accomplished. Figure 4 Synthesized azolyl-protected DDD H-bond array. Section 2. 6 presents the synthesis of newly designed self-adapting ADD/DDD H-bonding array based on ureido-triazolyl scaffoldings. Exploiting the prototropic equilibrium of the triazole nucleus the modules synthesized are expected to show an ADD or a DDD arrangement of the binding sites depending on the H-bonding functionalities of the complementary guest used for the complexation. Figure 5 Synthesized triazolyl-based ureido H-bonding arrays. Prototropic self-adapting properties: from a DDD to a ADD H-bonding array. Due to solubility limitations in common organic solvents (e. g., CDCl 3 and CD 2 Cl 2), the molecular recognition ability in solution could not be studied and further modifications of the molecular structural properties are required. Grâce à leur directionnalité proéminente ainsi qu’à leur force, les ponts hydrogène sont l’une des interactions non-covalentes les plus exploitées en chimie supramoléculaire. Malgré leur force relativement élevée en comparaison avec d’autres interactions non-covalentes (l’énergie d’un pont hydrogène en phase gazeuse se situe typiquement entre 0 et 5 Kcal mol- 1), l’association de deux molécules au moyen d’un pont hydrogène simple mène à des complexes présentant des stabilités thermodynamiques faibles, limitant ainsi leur exploitation dans la synthèse non-covalente de matériaux fonctionnels menant à de réelles applications. Dès lors, lorsque des interactions plus fortes sont nécessaires, l’approche d’ingénierie habituelle se concentre sur la synthèse covalente d’échafaudages moléculaires rigides et planaires dans lesquels plusieurs fonctionnalités donneurs (D) et accepteurs (A) de ponts hydrogènes sont arrangées dans ce que l’on appelle un ‘réseau de ponts hydrogène’. Grâce aux procédés de reconnaissances spécifiques et au contrôle de leur force d’association, les réseaux de ponts hydrogène multiples sont devenus des modules moléculaires indispensables dans la boîte à outils des chimistes supramoléculaires, permettant, via des procédés sélectifs d’auto-assemblage et/ou auto-organisation, la préparation bottom-up de matériaux fonctionnels tels que des cristaux liquides, des surfaces à motifs et des polymères supramoléculaires. En principe, la stabilité des complexes supramoléculaires à base de ponts hydrogène peut être modulée d’un nombre infini de manières. Par exemple, lorsque des interactions plus fortes (e. g., des valeurs de constante d’association élevées) sont requises, augmenter le nombre de sites de ponts hydrogène représente une stratégie efficace pour renforcer la stabilité de l’assemblage final. Néanmoins, une valeur élevée de Ka n’est pas toujours nécessaire. En effet, alors que des complexes hautement stabilisés sont requis dans le domaine des polymères supramoléculaires, dont les propriétés à l’échelle moléculaire (tel que le degré de polymérisation Dp, et la viscosité) corrèlent linéairement aux valeurs des Ka, ces dernières peuvent également être néfastes à la construction de nano-architectures hiérarchisées plus sophistiquées, résultant d’une interaction délicate entre des facteurs internes (<b>comme</b> <b>par</b> <b>exemple</b> du π−π stacking, des interactions solvophobiques/solvophiliques) et externes (tels que le temps, la température, la concentration etc.). L’objectif de cette thèse est l’élaboration et la synthèse de nouveaux réseaux de ponts hydrogène (DAD, ADD, DDD) basés sur des cycles hétéroatomiques à cinq chaînons. Nous proposons l’utilisation d’échafaudages thiolyl, oxolyl, azolyl, et triazolyl comme systèmes de reconnaissance, afin de réaliser un meilleur contrôle des propriétés de liaison et de la sélectivité des réseaux de reconnaissance à ponts hydrogène triples, permettant ainsi une transformation aisée des motifs de liaison. La variation des substituants et de l’hétéroatome sur les cycles hétéroaromatiques permettrait de créer une sélection de complexes de paires host-guest versatiles et de structure similaire, présentant ainsi différentes constantes d’association (Ka) pour mieux répondre aux différentes demandes d’applications supramoléculaires. Se concentrant sur les facteurs les plus pertinents qui influencent les constantes d’association des complexes à base de ponts hydrogène, le lecteur est introduit dans la première partie du Chapitre 1 sur la façon dont peuvent être élaborés des réseaux spécifiques à base de ponts hydrogène présentant une large gamme de valeurs de Ka (s’étendant sur un ordre de huit de magnitude). Ensuite, la deuxième partie traite des propriétés physiques et chimiques d’une grande variété d’assemblages par ponts hydrogène de modules moléculaires qui après auto-assemblage et auto-organisation ont ouvert de nouvelles voies à l’élaboration de nouvelles applications fascinantes. Figure 1 Réseaux de ponts hydrogène élaborés sur base d’hétérocycles à 5 membres. 	Le Chapitre 2 présente la description des efforts synthétiques réalisés pour la préparation des réseaux de ponts hydrogène DAD et DDD. Les deux premières sous-sections (2. 1 - 2) décrivent les approches rétrosynthétiques et les résultats des voies méthodologiques non fructueuses (via des réactions de cross-coupling d’amidation de Buchwald-Hartwig, réduction de dérivés azido et addition nucléophile de réactifs organométalliques à des dérivés isocyanate comme produit par le réarrangement de Curtius) entreprises pour introduire les fonctions amidiques et/ou uréidiques en position 2 des cycles hétéroaromatiques à cinq chaînons. 	Plusieurs réseaux de ponts hydrogène DAD à base de thiolyl ont été synthétisés avec succès (section 2. 3). Figure 2 Réseaux de ponts hydrogène DAD synthétisés, protégés par du thiolyl. Dans la section 2. 4 sont présentées les étapes synthétiques réalisées pour générer les réseaux DAD à base de dérivés oxalyl. Malheureusement, l’introduction sur le cycle de groupements électrodonneurs tels que des fonctions amidiques ou carbamiques a mené à des intermédiaires très instables. Dès lors, les dérivés amido-oxolyl capables de reconnaissance via des ponts hydrogène triples n’ont jamais été isolés. Figure 3 Réseau de ponts hydrogène DAD oxolyl-protégé synthétisé. Les stratégies synthétiques menant à la synthèse des réseaux DDD à base d’azolyl sont décrites dans la section 2. 5. Le module protégé par le groupemement azolyl 182 (voir Figure 4) a été synthétisé en treize étapes à partir du module pyrrole. Malheureusement, à cause de complications rencontrées lors du clivage du groupement protecteur N-azolyl, la synthèse des réseaux de ponts hydrogènes DDD protégés par l’azolyl n’a finalement pu être réalisée. Figure 4 Réseau de ponts hydrogène DDD azolyl-protégé synthétisé. La Section 2. 6 présente la synthèse de réseaux de ponts hydrogène auto-adaptant ADD/DDD, nouvellement élaborés à partir d’échafaudages ureido-triazolyl. Grâce à l’équilibre prototropique du noyau triazole, nous nous attendons à ce que les modules synthétisés présentent un arrangement des sites de liaisons ADD ou DDD, selon les fonctionnalités ponts hydrogène des guest complémentaires utilisés pour la complexation. Figure 5 Réseaux de ponts hydrogène ureido synthétisés à base de triazole. Propriétés auto-adaptantes prototropiques: d’un réseau ponts hydrogène DDD à ADD. 	Dû aux limitations de solubilité dans les solvants organiques communs (tels que CDCl 3 et CD 2 Cl 2), la capacité de reconnaissance moléculaire en solution n’a pu être étudiée. De plus amples modifications des propriétés moléculaires structurelles sont dès lors nécessaires. (DOCSC 02) [...] FUNDP, 201...|$|E
6000|$|... "Well, of your not yourselves being so--and of YOUR not in particular. I haven't {{the least}} doubt in the world, <b>par</b> <b>exemple,</b> {{that she thinks}} you too meek." ...|$|R
5000|$|Suite de l'Académie françoise, en laquelle il est traicté de l'homme et <b>comme</b> <b>par</b> une histoire naturelle du corps et de l'âme (1580) ...|$|R
6000|$|... "But I should fail. I {{only know}} three phrases of English, {{and a few}} words: <b>par</b> <b>exemple,</b> de sonn, de mone, de stares--est-ce bien dit? My opinion {{is that it would}} be better to give up the thing altogether: to have no English examination, eh?" ...|$|R
40|$|Résumé Comprenant 26000 âmes selon le recensement marocain de 1960, les Ait Sukhman dans la {{province}} centrale marocaine de Béni Mellal chevauchent les chaînes du Moyen- Atlas et du Haut-Atlas Central. Typiques en beaucoup d'aspects socio-culturels des tribus transhumantes des Imazighen dont ils forment un groupe, ils sont néanmoins atypiques en beaucoup d'autres : 1) Dans le fait qu'au moins deux de leurs clans-clefs réclament leur descendance d'un esclave noir de Mawlay 'Abd al-Qadir al-Jilali (et ceci dans une région où les noirs et les Haratin sont vus traditionnellement par des Berbères blancs comme leurs inférieurs sociaux), bien qu'il ne s'y manifeste aucune trace visible d'ascendance noire; 2) dans le bizarre système économique d'un de leurs groupements-clés localisés, les Ait 'Abdi du Plateau du Kusar (Koucer), qui gardent des chameaux à une altitude de presque 3 000 mètres, où ils se trouvent sous la neige pendant neuf mois de l'année, pénétrés de l'idée qu'ils en ont besoin pour le transport des grains; 3) dans l'existence, sur leur territoire (et dans ce cas, chez les Ait *Abdi d'Aghbala) d'un grenier collectif dont l'entrée est gardée par des serpents venimeux, dans une montagne presque inaccessible; 4) par le fait que même s'ils ont cinq clans, lesquels pour la plupart sont discontinus et redoublés au point de vue territorial, ici comme ailleurs dans la région, ils n'ont aucune notion du principe de khams khmas ou « cinq cinquièmes » qui apparaît avec une certaine régularité comme un facteur assez majeur dans la composition et l'organisation des tribus marocaines; et finalement 5) dans le système inutilement embrouillé d'élections annuelles pour le chef local, encore chez les Ait *Abdi du Kusar. En d'autres aspects traditionnels socioculturels <b>comme</b> <b>par</b> <b>exemple</b> le mariage préférentiel entre cousins parallèles patrilatéraux, l'organisation et la profondeur des lignages, la segmentarité, le droit coutumier, les serments collectifs, la vengeance, le prix du sang et la guerre traditionnelle (surtout avec les Ait Hadiddu) ainsi que s'agissant des arbres et des lieux saints, des « marabouts », ou encore de leur mentalité reconnue par tous leurs voisins et par eux-mêmes comme «arriérée» et «sauvage», ils ressemblent étroitement aux tribus qui les entourent; mais dans leurs aspects déjà énumérés ils s'en détachent suffisamment pour fournir une exception qui prouve la règle. The article {{which follows}} {{is based on}} approximately two months of anthropological fieldwork among the Ait Sukhman during the summers of 1959, I 960 and 1961. The fïeldwork {{was carried out in}} connection with a wider overall project on the sociocultural anthropology of Moroccan Berber-speaking tribes which the author worked on from 1959 to 1967, and which was kindly funded by the American Museum of Natural History in New York, to which institution he extends his thanks. His thanks must go equally to Professor Ernest Gellner of the London School of Economics for having introduced him to the highland Ait Sukhman at Anargi (Anergui) in the Moroccan Central Atlas in 1959. The Ait Sukhman are a Tamazight Berber-speaking tribal group of Central Morocco, consisting largely of transhumants and located in a single territorial bloc straddling the Middle Atlas range to the north and that of the bigger and higher Central High Atlas to the south. They are bordered on the northwest by the Ait r-Rba' and the Ait Sri, on the north by the Ait Ishaq and Ishqirn, on the east by the Ait Yihya and the Ait Hadiddu, both of the Ait Yafalman confederacy (with the latter having traditionally been among their principal enemies), on the south by the northernmost wing of the supertribe of the Ait 'Atta, the Ait Bu Iknifen n-Tlmast, {{as well as by the}} local groups of the Ait 'Atta at Usikis and Msimrir, and on the southwest by the Ihansalen and by the Ait Ishha of the Ait Massad confederacy (Maroc, Carte des Tribus: 1. 500, 000 e, Rabat, 1958 and 1962). From an administrative standpoint they fall entirely into the province of Bni Mallal (Beni Mellal circle of 1 -Qsiba and three rural communes of Aghbala, Fum 1 -Ansar and Tizi n-Isli, circle of Wawizakht and two communes of Anargi and Tagalft, and circle of Azilal and commune of Zawiya Ahansal, with a population in 1 960, by our interpolation of the Moroccan census of that year, of 6, 614 nuclear families and 26, 182 total population (Royaume du Maroc, Service Central des Statistiques 1962 : 162, 166, 167 - 8, 169 - 70, 176, 178 - 9) (1). Figures derived from any subsequent censuses have not been available to us. There is no question but that the Ait Sukhman, or their ancestors, as well as their neighbors, or the latters' ancestors, were integrally involved in the great and disorganized push of Berber tribal groups from the Jbil Saghru and southeastern Morocco northwest across the Central High Atlas and the Middle Atlas toward the Atlantic coastal plains, from the sixteenth century (or earlier) through the nineteenth and early twentieth centuries, what Terrasse has described as « la poussée (montagnarde) venue du Sud, fait essentiel de l'histoire marocaine» (Terrasse 1949 - 50, cited by Couvreur 1968 : 13; also Brignon et al., 1967 : 259 - 62). According to Couvreur, the Ait Sukhman may have been established at Anargi in the Central High Atlas, which is generally regarded as their point of origin and where they were first divided up, as early as the thirteenth century (though in our view this date seems too early), and certainly by the sixteenth (1968 : 13 - 14); and two clans which were possibly Ait Sukhman or had become so, the Ait Hamama and the Ait Sa'id (w-'Ali ?), figure among those who sold land to Sidi Lahsin w-'Atman of the Ihansalen in the latter's famous land deed of 1598 A. D. / 1006 A. H. (Ithier 1947, unpublished), hence buttressing the sixteenth century date. Couvreur also notes that in the first instance there is a contradiction which a legend tries to resolve : Dawud w-'Ali, the ancestor of one of the Ait Sukhman clans, had a first-born son from the region but then adopted a second son who had come from the south. In this fact we may have a memory of the mixture of two groups of different origin (ibid. 14, n. 13). At any rate, Couvreur adds that apart from the Zawiya Ahansal land deed of 1598, an additional Arabic manuscript from another Ihansalen zawiya at Askar has them chasing out the last stranger groups from Anargi and consolidating their own position in the Anargi area around 1650 - 70 (ibid : 15). In any event, as Tarrit already noted in colonial times (Tarrit 1 923 : 53 1), it has come to be that if one speaks of the Ait Sukhman in the area of Bni Mallal and Wawizakht, it is generally admitted that one is dealing with these same Ait Dawud w-'Ali, while the Ait Sa'id w-'Ali, the Ait'Abdi and the Ait Hamama are all specified as such. The Ait Hamama came to occupy the right bank of the Asif n-Wirin, after they were pushed out of the Azagharfal by he Ait'Abdi, while the latter occupy its left bank up to and including the crests of Ijbartan, Imghal, the Amalu n- 1 -Kusar and the Amalu n-Zaimuzen, the twin massifs of the Kusard and the Zaimuzen. (It is no accident that the Kusar massif and plateau, at a height of almost 3, 000 meters, was the scene of the magnificent last-ditch resistance of the Ait Sukhman to the French before their final surrender on September 3, 1933, the date which marked the end of the French 'pacification' of the Central Atlas). The Ait Dawud w-'Ali control both banks of the Wad 1 -Abid River from Bu Tfarda to the Jbil Sghat, as well as being in control of the Anargi Pass, the Tizi n-w-Anargi, while the Ait Sa id w-Ali, the lowest-lying clan, form a territorial triangle the high point of which is supported by the Wad 1 -Abid and the base of which is at Fum 1 -Ansar (Tm. Imi n- 1 -Ansart) and at Bni Mallal itself, virtually on the edge of the flatlands where the massive wall of the Central Atlas begins to rise up. The reasons behind these present-day Ait Sukhman clan locations — and we shall consider the clan structure presently — can probably be summed up by the two socioeconomic and ideological imperatives of pastureland and politics. But before we take up these issues, we discuss that thrown up by the name Ait Sukhman itself; and that this is but the first of a considerable number of anomalies of a sociocultural, socioeconomic and sociopolitical kind — which serve to set the Ait Sukhman off from their other Tamazight- speaking Berber tribal neighbors - will soon become apparent. The etymology of the name Ait Sukhman is in no doubt, but its attitudinal repercussions are of some interest : for although the question may not be one which bothers or puzzles the tribesmen themselves, it certainly throws up a degree of ambiguity in the mind of the oustide observer. The name Sukhman is derived from Tamazight ismakh (pi. isimghari), or "black", and the Ait Sukhman are hence "people/ descendants of a black". Now this etymology, which is indisputable, is very curious on two counts. The first is that it seems to be known and invoked by only the members of the two highest and most inaccessible Ait Sukhman clans, the Ait Dawud w-'Ali of Anargi (also known as the Ait w-Anargi) and the Ait'Abdi and their close kinsmen the Ait Bindaq of the Kusar and the Zaimuzen; and not to or by any of the others which inhabit lower-lying areas where under normal circumstances a rather greater number of blacks might be expected to be found. The second point is that even though Sukhman, the putative black ancestor and tribal point of definition, is locally held to have been a slave of the great saint Mawlay 'Abd al-Qadir al-Jilali (whose connections with Morocco and its people are legion, even though he himself never lived there and is buried in Baghdad), and even though the Ait Sukhman themselves tend, as Central Atlas Berbers go, to be rather dark-complexioned — a fact already noted by de la Chapelle (1 93 1 : 48, n. 1 0) — they do not look in the least negroid, or at least not to this observer, but rather as fully Caucasoid and Mediterranean as any of theirHart David Montgomery. The Ait Sukhmann of the moroccan central Atlas : an ethnographie survey and a case study in Sociocultural Anomaly. In: Revue de l'Occident musulman et de la Méditerranée, n° 38, 1984. pp. 137 - 152...|$|E
40|$|A strong {{interest}} {{has been focused}} from several years on the algae pathway for energy production, especially for transportation fuels called third generation biofuel or G 3 biofuel, and mainly from microalgae route, considering {{it could be a}} high potential alternative strategy for renewable energy and fuel production. Algae, and especially microalgae, present significant advantages compared with land resources, such as much higher productivity and lack of competition with food applications. Nevertheless, based on current knowledge, the production of an algae biomass for energy remains a difficult target to reach, due to the numerous existing hurdles such as the energetic yield and the economic positioning, without neglecting the environmental and societal aspect. Unlike first generation (GI) and a few second generation biofuel (G 2) processes, G 3 biofuel processes are far from the industrialization step. In 2010, under the initiative of IFP Energies nouvelles, Airbus, Safran, EADS IW and the “Académie des Technologies”, launched a French national study of the potential of the algae sector as resources for the so called G 3 biofuel production. This study was called “Algogroup” and led by IFP Energies nouvelles. The objective was to obtain a shared vision of the deployment possibilities. It {{led to the creation of}} this Algogroup task force with the previous partners, adding Sofiprotéol, INRA 1, IFREMER 1, CEVA 1 and the Agrimip pole to combine all available knowledge and determine the responses which could be given to the existing questions. The Algogroup objective was to facilitate vision sharing between participating organisations and industrials on the technical improvements, the probabilities of success, the R&D needs and the development perspectives, while paying close attention to the obstacles which have to be alleviated to improve the positioning of the algae pathway. To reach this target, Algogroup has explored several axes, which enabled a thorough analysis of the potentials and limits of the technology: from the species selection to the harvesting (lipid extraction/recovery), including environmental and economical aspects. This paper focuses on some main aspects of the Algogroup study related to economical positioning and environmental terms, specially Life Cycle Analysis (LCA). A large share of the work was dedicated to microalgae, but since it was also considered important to examine the potential role of macroalgae, a specific analysis was conducted on this aspect. It has enabled the group to issue some recommendations such as a need for an integrated approach, need for tools to run comprehensive technico-economic assessments, including co products valorisation. Despite the limited amount of reliable information currently available on the algofuel sectors, especially in terms of environmental balance, numerous challenges still remain to be taken up to make these sectors credible and profitable, both technically, economically and environmentally. On the economic aspect the estimated costs for future microalgae biofuels remain in a very broad range from $ 2 /Gal to $ 7 /Gal. There remains great potential to decrease microalgae oil production costs, but this has to be considered very carefully given the large amount of underlying assumptions. Moreover, as yet underlined, microalgae biofuels are not currently being produced at a commercial scale, thus these are only potential scenarios, which will have to be confirmed. And finally, several technologies can be used to produce microalgae oil and location possibilities are proposed. Another key point is that, in a large majority of scenarios, the economic viability of the pathway relies on the valorisation of what one usually calls co-products. Valorisation of co-products is not considered a valid option in the long-term as no market identified today could absorb the quantities associated to a new fuel market. Besides, environmental studies have demonstrated that the energetic balance was not favourable at present, based on current processes, but the variation range of the results let some space for significant improvements. The balance of greenhouse gas emissions was favourable, and there also the variation range was very wide. As regards the other environmental impact categories, however, the uncertainties are too great to draw any conclusions. Because of the heterogeneity of approaches and results for the development of the algaepathway, we must bear in mind that without reliable and robust assessments of these sectors it will not be possible to direct their technical development sustainably. Macroalgae as a resource for biofuels production are very far from being a commercial reality, but do present some advantages such, for green algae, exhibiting several similarities with current GI and G 2 feedstock, being producers of starch and unlignified cellulose. Nevertheless, they also contain other specific compounds. Red and brown macroalgae are currently the most produced species, but their composition calls for the development of new transformation processes. Although technically feasible at lab-scale, the economic viability of such processes is being endangered by the complexity of the processes involved and the numerous steps required as well as by non-technical issues such as competition with other markets like green chemistry. To have a true share of the future fuel mix, macroalgae production needs to be increase by a dozen- time fold. This increase should not be done without social acceptance or at the expenses of the environment. This issue was adressed for microalgae, but data on macroalgae are currently lacking to be able to conduct Life Cycle Assessment (LCA) on this very specific environment. There are also additional problems to be taken into account, such as the lack of legislation or conflicts of usage with existing sea activities for example. Potential for high tonnage production seems real, but the challenge is to federate existing actors and new ones to build a new agro-industry. As a conclusion, no true leveraging option, leading to significant breakthroughs has really emerged as a short term solution, but wide spaces for significant improvement could be envisaged and more laboratory and pilot works have to be achieved before being able to move to a higher scale, leading to the first step toward industrial production. Depuis quelques années, un intérêt croissant pour la production d’algues, notamment les micro-algues, pour la production d’énergie a été observé, spécialement pour la production de biocarburants pour le transport routier et aérien, filière que l’on a coutume de qualifier de troisième génération. Les algues et spécialement les micro-algues affichent de nombreux avantages comparés aux ressources terrestres, <b>comme</b> <b>par</b> <b>exemple</b> une productivité nettement plus élevée et l’absence de compétition avec les filières alimentaires. Néanmoins, l’état actuel des connaissances ne conduit pas à penser qu’un développement de la culture de micro-algues pour la production d’énergie soit possible à court-moyen terme en raison de nombreux écueils à lever comme la balance énergétique, le positionnement économique sans oublier les aspects sociétaux et environnementaux. Contrairement aux filières de première génération et certaines filières de seconde génération, les biocarburants de troisième génération sont encore loin de l’industrialisation mais la nécessité de disposer d’une analyse commune et partagée par l’ensemble des acteurs de la filière est nécessaire. Ainsi, en 2010, à l’initiative d’IFP Energies nouvelles, Airbus, Safran, EADS IW, et l’Académie des Technologies ont mis en place un groupe d’étude national dédié à l’étude du potentiel de la filière micro-algues pour la production de biocarburants G 3. Ce groupe, nommé Algogroup, piloté par IFP Energies nouvelles a eu comme objectif d’aboutir à une vision partagée d’un possible déploiement de la filière G 3. Outre les membres fondateurs, Algogroup a aussi intégré les expertises dans le domaine, de Sofiprotéol, de l’INRA 1, de IFREMER 1, du CEVA 1, de Agrimip ainsi que de nombreux autres laboratoires et industriels. Les travaux menés au sein d’Algogroup ont donc permis de collecter un ensemble de données sur le potentiel et les limites de la filière, la position des industriels et des laboratoires, sur les axes de recherches nécessaires à mettre en oeuvre pour permettre à la filière de se développer. La réflexion a été structurée selon différents thèmes. Les aspects technologiques : quelles souches, quel mode de culture, de récolte, les aspects économiques ainsi que les aspects environnementaux. Ce papier met l’accent sur les résultats d’Algogroup sur le positionnement économique et environnemental des micro-algues. En parallèle, une réflexion sur le potentiel des macro-algues a aussi été conduite au sein d’Algogroup. A ce jour, uniquement un nombre limité de données est accessible pour le secteur des “algocarburants” et s’engager dans la construction d’une telle filière est encore prématuré. Ainsi les résultats provenant d’Algogroup seront de précieuses contributions à l’élaboration d’une feuille de route Algocarburants. Sur un plan économique, les coûts estimés des futurs biocarburants fabriqués à partir de micro-algues s’étaleront dans une fourchette de 2 à 7 $/Gal. Cette situation laisse à penser qu’un large champ de possibilités est envisageable pour réduire le coût de production des huiles algales mais il faut toutefois rester très prudent car les scenarii conduisant à ces diminutions reposent sur des hypothèses qu’il faudra démontrer. En effet, ces scenarii considèrent des technologies et des localisations de production très variables. Un autre point clé des modèles économiques analysés est que dans une large majorité de ces scenarii, la viabilité économique repose sur la valorisation des coproduits. De telles options ne sont pas considérées comme acceptables sur le long terme en raison de l’incertitude qui règne sur les capacités d’absorption par le marché de ces produits lorsqu’ils seront liés à une production en grosses quantités de biocarburants. Considérant le volet environnemental, le travail a démontré que la balance énergétique n’était pas favorable, en se référant aux procédés explorés disponibles. Toutefois, les variations enregistrées laissent la place à des possibilités d’amélioration. Concernant les émissions de gaz à effet de serre, le bilan apparait favorable mais là aussi avec une plage de variations très large. Pour les autres aspects environnementaux, les incertitudes sont trop grandes pour conclure. Par ailleurs, en raison des très grandes hétérogénéités des approches et des résultats publiés pour le développement d’une filière micro-algues, il apparait que sans évaluation fiable et robuste du secteur, il n’est pas possible de considérer à ce jour ledéveloppement des techniques comme totalement compatibles avec les critères de durabilité. Pour les macro-algues, nous sommes encore très loin de pouvoir les considérer comme une ressources pour la production de biocarburants mais celles-ci présentent des avantages, comme pour le cas des algues vertes des similitudes avec les ressources utilisées pour les filières G 1 ou G 2. Concernant les algues brunes et les algues rouges qui sont aujourd’hui les espèces les plus produites, leurs compositions demandent le développement de nouveaux procédés pour leur valorisation. Bien que ces procédés soient faisables à l’échelle du laboratoire, la viabilité économique à grande échelle est à démontrer en raison de la complexité et du nombre d’étapes requis par ces procédés tout comme la compétition de cette filière avec les autres marchés, comme celui de la chimie verte. Par ailleurs, pour que les macro-algues puissent avoir un réel devenir dans le mix biocarburants, leur production doit être considérablement augmentée. Ce point ne peut être considéré sans avoir évalué l’impact sociétal et environnemental et aujourd’hui peu de données sont accessibles pour bien apprécier ces 2 volets. Enfin, il faudrait bien analyser tous les aspects législatifs liés au développement de culture à gros tonnages en mer. En dépit de ces aspects, le potentiel de production de grosses quantités semble réel. En conclusion, le travail effectué par Algogroup n’a pas fait émerger de réelles ruptures permettant d’envisager un développement à court moyen terme de la filière algocarburants mais des possibilités d’amélioration peuvent être envisagées. Ceci demande de poursuivre les travaux au niveau du laboratoire et à l’échelle du pilote avant de passer à une échelle préindustrielle...|$|E
40|$|International audienceNew {{media and}} {{transnational}} culture in Tunisia : {{what are the}} socio-cultural issues? From time immemorial means and techniques of communication have been considered remarkably useful tools {{in the quest for}} individual and collective emancipation and also as real catalysts for social change. The transnationalisation of networks and cultural exchanges may be seen as bringing this about, notably in Southern societies. Latterly, news items and even expert opinion have supported this idealistic vision of Communication Technology by reducing the ''Arab Spring'' to a technological performance of social networking (Castells, 2011). In reality, in Maghreb and in the Arab World, the mass use of new media and the Internet may be hailed as the forerunner of a new political outlook (Mohsen-Finan, 2009; Gonzalez-Quijano & Guaaybess, 2009). In Tunisia, dissenting voices published on the Web by cyber-activists may be seen to have lead to a transformation of the public sphere in a country where an authoritarian state monopolised political expression and was opposed to any form of autonomy in civil society (Lecomte, 2009). These different findings highlight the dynamic ''libertarian'' effect of the new media made possible by the technology of the World Wide Web. From its inception the Web was destined to become a network open to all, short circuiting systems of state control and carrying with it hopes of social reconstruction from ''the base upwards'' (Cardon, 2010, p. 13). Without calling into question any of the socio-cultural and technological strengths of ''cultural globalisation'', it is important for us, nevertheless, to question the way individuals see and envisage their integration into global society. The impact their social and cultural practices have on the political agenda, for example, is an important measure of what is socio-politically at stake in ''globalisation''. In fact one can see how the use of ICTs can become a powerful catalyst in social protests, {{as was the case in}} the revolutionary uprising in Tunisia. However, in order for the public sphere to be completely transformed other factors need to be present, such as, for example, a determination to overthrow the establishment and an ability to mobilise the masses. When examining current changes in Tunisian society, (without claiming to provide an in depth analysis), we are lead into an analysis of certain Internet practices, namely how individual accounts are used by the blogosphere before the fall of ex president Ben Ali. By studying personal on-line testimonies we are able to establish the way in which individuals make the most of transnationalisation, demonstrate their social and cultural choices and pronounce on their individual to public rights. Instead of taking a holistic view of the public sphere it is preferable to use Bernard Miège's notion of ''partial public space'' (Miège, 2010) and Peter Dahlgren's notion of specific features in the domain of ''the public sphere on-line'' (Dahlgren, 2005). The space opened up by Tunisian bloggers can, of course, be considered as a movement of competing public interests (Fraser, 2001, p. 139) but at the same time as one which has its own specific structural features, whether they be political, social or cultural. One possible objective of the movement being to open up discursive space. But as Dominique Cardon points out, in this space ''the dividing line between private sociability and public debate is blurred by a new sensitivity which leads individuals to reveal more about themselves and create links between their private lives and public issues'' (Cardon, 2010, p. 11). In the light of these elements, digital space provides a new platform for public exchanges. Reading and studying the accounts published on the Net by bloggers in Tunisia shows us that digital practices are determined by the assertion of self and one's values with a view to instigating individual and public action; action which may appear less like traditional political militancy and more like the liberation of personal expression. Whilst aiming at achieving citizenship, these practices focus on notions of individuality and autonomy in one's personal life. In this way emergent declarations from a sphere made up of people who are ''autonomous'', ''amateurs'' (Flichy, 2010) are put into perspective and we can demonstrate to what extent the global network of the Internet has underpinned civic activism in a society struggling for freedom of expression. Our hypothesis is that affirming one's individuality on a blog goes hand in hand with civic practices in the sense that it can be seen as active participation in the city state. The use Tunisian internet users made of digital freedom basically illustrates a willingness on their behalf to see themselves as private citizens, ones that political authorities had constantly sought to marginalise. Even though constrained by digital boundaries, the mobilisation of certain bloggers embodies, in reality, this sense of individual emancipation and brings with it elements of a new societal project, which once again refutes culturalist theories about holistic society. Through the study of certain Tunisian blogs in which self assertion combines with civic culture, our questioning focuses on the form and content of digital texts setting themselves up as new culture : what are the formal aspects highlighted by Web users in their blogs? How do bloggers associate their individual need for autonomy, by claiming freedom of expression on the Net, with collective action setting its sights on a new model of society? Would this place of public exchange be a joint extension of the private sphere and its problems? Our enquiry is based on a socio-discursive analysis of a sample of blogs which have been added to for several years and which have a wide audience in spite of censorship. It strives to guide thinking about this new culture of participation afforded by the Net in Tunisia by illustrating how complex the issue is. A complexity which comes, in particular, from the political and social constraints which, in some respects, govern individual expression, but also from the determination Tunisian bloggers have shown, throughout the different phases of mobilisation, to be victorious in the field of on-line discussion. Longtemps les moyens et techniques de communication ont été considérés comme de formidables outils d'émancipation individuelle et collective et comme de véritables accélérateurs de changements sociaux. La transnationalisation des réseaux et les échanges culturels viendraient consacrer cet état de fait notamment dans les sociétés du Sud. Plus récemment, les discours journalistiques et même savants charrient cette vision idéalisée des Technologies de la communication en réduisant " les révolutions arabes " à la performance technologique des réseaux de communication (Castells, 2011). Concrètement, au Maghreb et dans le Monde arabe, la massification des usages des nouveaux médias et la progression des pratiques d'Internet présageraient de l'avènement d'un nouvel espace politique (Mohsen-Finan, 2009;Gonzalez-Quijano & Guaaybess, 2009). En Tunisie, les prises de parole contestataires publiées sur le Net par des cyber-activistes attesteraient d'une reconfiguration de l'espace public dans un pays où l'État autoritaire monopolisait l'expression politique et s'opposait à toute forme d'autonomie de la société civile (Lecomte, 2009). Ces différentes constatations mettent en perspective la dynamique " libertaire " des nouveaux médias rendue possible par les spécificités techniques des réseaux de communication planétaire comme Internet. Dès l'origine, le web était destiné à devenir un réseau ouvert à tous, court-circuitant les systèmes de contrôle étatique et portant les aspirations d'une construction sociale " par le bas " (Cardon, 2010, p. 13). Sans remettre en question certains atouts technologiques et socio-culturels de la " mondialisation culturelle ", il importe pour notre part d'interroger la manière dont les individus imaginent et assurent leur intégration à la société globale. L'impact de leurs pratiques sociales et culturelles sur l'agenda politique, par exemple, est un repère important pour mesurer les enjeux socio-politiques de la " mondialisation ". En effet, on peut constater que l'usage des TIC peut devenir un puissant catalyseur des mouvements sociaux, comme cela a été le cas lors du soulèvement révolutionnaire en Tunisie. Toutefois, pour que l'espace public soit complètement transformé, d'autres paramètres devraient être nécessairement présents <b>comme,</b> <b>par</b> <b>exemple,</b> le potentiel de mobilisation de la rue et sa détermination à renverser l'ordre dominant. En se penchant sur les mutations actuelles de la société tunisienne, sans pour autant prétendre apporter des explications exhaustives, notre proposition s'efforce d'analyser certaines pratiques d'Internet, en l'occurrence les récits individuels déployés par la blogosphère avant la chute de l'ex-président Ben Ali. En étudiant les paroles personnelles mises en ligne, il s'agit pour nous de montrer la manière dont les individus tirent profit de la transnationalisation, manifestent leurs choix culturels et sociaux et articulent leurs libertés individuelles aux libertés publiques. Contrairement à une vision holistique de la sphère publique, nous empruntons à Bernard Miège la notion d'" espace public partiel " (Miège, 2010) et à Peter Dahlgren les spécificités de la " sphère publique en ligne " (Dahlgren, 2005) en considérant l'espace ouvert par les blogueurs tunisiens, certes, comme un mouvement de publics concurrents (Fraser, 2001, p. 139) mais possédant ses propres spécificités structurelles qu'elles soient politiques, sociales ou culturelles. L'un des enjeux de ce mouvement étant d'élargir éventuellement l'espace discursif. Mais, comme le précise Dominique Cardon, dans cet espace," la ligne de partage entre sociabilité privée et débat public est trouée par une nouvelle sensibilité qui conduit les individus à s'exposer et à tisser, devant les autres, les fils entre leur vie personnelle et les enjeux publics " (Cardon, 2010, p. 11). Au vu de ces éléments, l'espace numérique offre une nouvelle modalité de l'échange public. L'observation et la lecture des récits publiés sur le Net par les blogueurs en Tunisie nous montrent que les pratiques numériques sont déterminées par des manifestations d'affirmation de soi et de ses valeurs, en vue d'un agir individuel et collectif qui prendrait moins le portrait d'un militantisme politique traditionnel qu'une forme de libération de la parole personnelle. Tout en visant la conquête de la citoyenneté, ces pratiques mettent en avant l'individualité et l'autonomie de la vie personnelle. Nous mettrons ainsi en perspective les paroles émergentes d'une sphère formée de personnes " autonomes ", " des amateurs " (Flichy, 2010), en vue de montrer en quoi le réseau planétaire qui est Internet a été le support d'une lutte civique et citoyenne pour l'épanouissement d'une subjectivité muselée. Notre hypothèse est que l'affirmation de son individualité sur le blog va de paire avec les pratiques de la citoyenneté au sens d'une participation active à la cité. La revendication des internautes tunisiens des libertés numériques constituent au fond une volonté de reconnaître en eux les individus-citoyens que l'autorité politique au sens large cherchait constamment à marginaliser. Bien que circonscrit dans l'espace numérique, la mobilisation de certains blogueurs incarne en réalité cette émancipation individuelle, qui réfute encore une fois les thèses culturalistes à propos de la société holiste, et qui porte les éléments d'un nouveau projet sociétal. A travers l'étude de certains blogs tunisiens où la réalisation de soi se conjugue avec la culture citoyenne, notre questionnement s'articule atour de la forme et du contenu des écrits numériques s'érigeant en une nouvelle culture : quels sont les aspects formels mis en évidence par les internautes dans leurs blogs? Comment les blogueurs articulent-ils leurs problématiques individuelles d'autonomie à travers la revendication de la liberté d'expression sur le Net, avec un agir collectif visant un nouveau modèle de société? Cet espace d'échange public serait-il une coextension de la sphère privée et de ses problématiques? Notre travail s'appuie sur une analyse socio-discursive d'un échantillon de blogs qui sont à l'œuvre depuis quelques années et qui jouissent d'une bonne audience malgré la censure. Il s'efforce de conduire une réflexion sur cette nouvelle culture de participation qu'offre le Net en Tunisie en montrant sa complexité, notamment à travers les contraintes politiques et sociales qui déterminent à certains égards les expressions individuelles, mais aussi la détermination des blogueurs tunisiens dans les différentes phases de mobilisation pour la conquête de la parole en ligne...|$|E
6000|$|We had {{a little}} {{difficulty}} here in getting along with the French; and our German (in which, by the way, some of the party are rather expert) had been acquired in Saxony, and was taken for base coin here. The innkeeper was an attentive host, and wished to express every thing that was kind and attentive; all of which he succeeded in doing wonderfully well, by a constant use of the two words, [...] "par exemple." [...] As a specimen of his skill, I asked him if an extra horse could be had at Einsiedeln, and his answer was, [...] "Par <b>exemple,</b> monsieur; <b>par</b> <b>exemple,</b> oui; c'est-à-dire, <b>par</b> exemple." [...] So we took the other horse, <b>par</b> <b>exemple,</b> and proceeded.|$|R
6000|$|But {{my master}} seized the wax taper. [...] "Pardon me, my lord," [...] says he. [...] "What! a servant do it, when {{your son is}} in the room? Ah, <b>par</b> <b>exemple,</b> my dear father," [...] said he, laughing, [...] "you {{think there is no}} politeness left among us." [...] And he led the way out.|$|R
5000|$|On attribue souvent à Gaston Jèze la formule suivante : L'impôt est une prestation pécuniaire requise des particuliers par voie d'autorité, à titre définitif et sans contrepartie, en vue de la {{couverture}} des {{charges publiques}} (v. <b>par</b> <b>exemple,</b> Encyclopedia Universalis, 1996, v° Impôt, vol. 11, p. 1001). En réalité, cette définition est due à Georges Vedel8.|$|R
40|$|International audienceUnveiling, {{self-representation}} and mediation. New {{norms of}} sociability on the socialweb This paper, which follows {{on from the}} research work conducted for several years, aims at reassessing the nature of social control {{and the way it}} is exercised {{at a time when the}} use of digital information and communication technologies, characterized by their hyper-connectivity, spreads among the population. First, we showed that the filing of information on individuals and populations, after it had fed Social Control (SC) to the point that it had become the dominant paradigm, had somewhat frozen it. Indeed, an important part of the information collected on individuals remained invisible and therefore escaped "infocontrol". It then appeared to us that it was urgent to consider that phenomenon. The conditions of acquisition as well as the nature of the nominative information had changed and had modified the way to interfere with the individual. Thus, a drastic change in perspective was required to better take into account the question of social control in the early 21 st century (Carré, Panico, 2011 -a). Then we studied the way a priori filing had shifted to self-displaying, which is deliberately chosen - more particularly at least by this part of society that we call digital natives or the " Y Generation " - so as to propose a few elements for a communication approach of social control (Carré, Panico, 2012). Finally, we wished to study the way - between strategies of anonymity and self-mediation - this new feeling of liberty leads individuals to stage and display themselves - in specific circumstances, namely through groups that are sometimes very short-lived (Carré, Panico, 2011 -b). This enabled us to escape the dilemma according to which, for some, you can only read such an original phenomenon in two ways: either an unprecedented emancipation of the individual or a no less unprecedented control. We have proposed a different interpretation, away from such a dilemma, based on the anthropological evolution of the relation between the individual and society. We wish to pursue here our reflection by questioning, with others, the development - seemingly impulsive, or even regressive, of behaviors that can sometimes look unbridled, but also meet a demand for visibility, a visibility whose legitimacy stems from the fact that it is provided by the individuals themselves. Let us make it clear that this study targets more particularly the younger generations (digital natives, or " Y Generation ", even though Proulx (2011), who works on these issues, seems to consider that this category is still too vaguely outlined and needs to be defined). These " youths " would become the prescribers of a new communication norm, affecting the building of one's identity, the relation to authority, the status of privacy and self-image, a norm that would express itself through a dense visual communication, a sign of the omnipresence of these "collective-individuals" in the public sphere. Didn't R. Senett (1979) say 30 years ago that the public sphere was no longer the place to find fulfillment for one's social being but for one's personality"? That could look like a premonition totally confirmed today [...] . except that the collective-individual we have here is endowed with a " basic personality "[1], and a community-based rather than a social conscience. In that context of non-individualism, we can understand that the power to act is thereby increased [...] . Thus, we contend that observable communication practices - very visual for the most part -posting online, sharing photos, videos [...] . - which are developing on the Web and mark a shift from the anonymous relation between individual and society to the collective, and the unveiling it entails, are the sign of a new ethics of the link (that we call social through an abuse of language) based on a pragmatic "acting with [...] . " rather than a more and more theoretical "living together". Through the question of "the Care of the Self" in Foucault's words (1984) as in his eponymous work, we wish to examine the significance of such unveiling, self-representation practices which spread on the Socialweb[2] and which, according to the author, are related to the subject's truth, knowing that these practices still follow on from the mainstream media model, i. e. the TV model, while going beyond that. Such practices now tend to establish themselves more and more as norms of communication acts. Thus, in the first part of this paper, we will try, in reference to Foucault, to question the notion of "individual" as opposed to the "subject" figure, that we find more efficient to explain the way unveiling and visibility practices spread and organize themselves to make sense, on the "part of the Internet that the SocialWeb constitutes". In the second part, we will see how, innovative practices of the link that are all inscribed in the ethics of unveiling could eventually lead to a new form of sociability, among other things a re-composition, a redefinition, a renewal in the use of traditional forms of visibility that were highly organized (like in institutions, associations, trade unions, etc.), and could also lead to new ways to exercise power. Finally, we will question the change that the development of the Socialweb heralds, asking ourselves how the underlying media/communication relation, and the ethics driving it, holds within or not an original society model, a model of values in which the individual tends to yield to the collective - a little bit in the same way as the hackers in the Anonymous group - and in which, as a consequence, the usual sociability norms could spread in new ways. [1] In the sense of Linton's anthropology[2] We will use the neologism Socialweb (in one word) to designate a certain part of the Internet which, owing to its simplicity of use, its non-discriminatory access from an economic point of view, its appeal for what is related to the community and the contribution, places at the heart of the sociotechnical system an " individual-actor " emancipated from traditional authoritiess and simultaneously produces new forms of sociability. La communication proposée s'inscrit dans la continuité des travaux que nous menons depuis plusieurs années et se donne pour objet de réévaluer la nature et les modalités d'exercice du contrôle social à l'heure où se généralise au sein de la population l'usage des techniques numériques d'info-communication caractérisées par leur hyperconnectivité. Dans un premier temps nous avons montré que le fichage des individus et des populations, après avoir nourri ce contrôle social (CS) au point d'en apparaître comme le paradigme dominant, l'avait quelque peu figé. Et ce, du fait qu'une part importante de l'information recueillie sur les individus restait invisible et par conséquent échappait à cet " infocontrôle ". Il nous est apparu alors qu'il était urgent de considérer ce phénomène, tant les modalités d'acquisition et la nature de l'information nominative avaient changé et avaient modifié la manière d'intervenir sur l'individu. Ainsi un revirement de perspective s'imposait afin de mieux prendre en compte la question du contrôle social en ce début de 21 e siècle (Carré, Panico, 2011 -a). Puis dans un deuxième temps nous avons étudié de quelle manière l'on était passé du fichage a priori subi à l'affichage de soi, quant à lui fortement choisi - plus particulièrement par cette frange au moins de la société que l'on nomme les digital natives ou encore la " Génération Y " - afin de proposer quelques éléments pour une approche communicationnelle du contrôle social (Carré, Panico, 2012). Nous avons dans un troisième temps souhaité étudier la manière selon laquelle est vécu, entre stratégies d'anonymat et médiatisation de soi, ce sentiment nouveau de liberté qui amène les individus - certes dans des conditions précises, notamment à travers des collectifs parfois très éphémères - à se mettre en scène et s'afficher eux-mêmes (Carré, Panico, 2011 -b). Cela nous a permis de sortir du dilemme qui voudrait, selon certains, qu'il n'y ait que deux lectures possibles de ce phénomène original : soit une émancipation sans précédent de l'individu; soit un contrôle lui aussi sans précédent. Nous avons proposé une lecture différente qui échappe à ce dilemme et qui se fonde sur l'évolution à caractère anthropologique du rapport individu/société. Nous voudrions ici poursuivre notre réflexion et pour cela interroger à côté d'autres le développement en apparence impulsif, voire régressif, de comportements aux allures parfois débridées, certes, mais qui toutefois répondent à une injonction de visibilité qui, elle, tire sa légitimité d'être portée par les individus eux-mêmes. Précisons d'emblée que notre propos cible plus particulièrement les jeunes générations (les digital natives, ou " Génération Y ", quoique Proulx (2011) qui travaille sur ces questions semble indiquer que cette catégorie aux contours un peu lâches reste à constituer). Ces " jeunes " deviendraient prescripteurs d'une nouvelle norme communicationnelle, norme qui touche à la construction identitaire, à la relation à l'autorité, au statut de l'intime et à l'image de soi, et qui s'exprime à travers une communication visuelle dense qui signe l'omniprésence de ces " individus-collectifs " dans l'espace public. R. Senett (1979) ne disait-il pas il y a 30 ans que l'espace public n'est plus le lieu de réalisation de son être social, mais de sa personnalité " ? Cela pourrait sembler prémonitoire et aujourd'hui pleinement établi [...] . sauf à se dire que l'individu-collectif auquel on a à faire ici est davantage doté d'une " personnalité de base "[1], et d'une conscience communautaire plutôt que sociale. Dans ce contexte non individualiste on peut comprendre qu'augmente du coup sa puissance d'agir [...] . Ainsi, selon-nous, les pratiques communicationnelles observables - souvent très visuelles : mise en ligne, partage de photos, vidéos, clips [...] . - qui se développent sur le Web et qui marquent le déplacement du rapport anonyme individu/société au profit du collectif, et de son corollaire le dévoilement, sont-elles le signe d'une éthique nouvelle du lien (que, par abus de langage, nous dirons social) davantage fondée sur un pragmatique " agir avec [...] . " que sur un " vivre ensemble " qui lui, semble de plus en plus théorique. A travers la question du " souci de soi " formulée par Foucault (1984) dans l'ouvrage éponyme, nous souhaiterions nous pencher sur le sens à donner à ces pratiques de dévoilement, de mise en scène de soi, qui se déploient sur le Websocial[2] et qui touchent selon l'auteur à la vérité du sujet, conscients que ces pratiques s'inscrivent pour une part encore dans le prolongement du modèle médiatique dominant, i. e. télévisuel tout en le dépassant. Pratiques qui tendent néanmoins aujourd'hui à s'instituer de plus en plus en normes d'action communicationnelle. C'est donc ce que nous ferons dans la première partie de cette communication où, nous référant à Foucault, nous tenterons de questionner la notion d'" individu " en lui opposant la figure du " sujet " plus apte selon nous à expliquer la manière selon laquelle les pratiques de dévoilement, de mise en visibilité sur " la part de l'Internet que constitue le Websocial ", se déploient et s'ordonnent pour faire sens. Dans une deuxième partie nous verrons comment à partir de pratiques innovantes du lien qui toutes s'inscrivent dans une éthique du dévoilement, on pourrait en venir à une nouvelle sociabilité, c'est-à-dire entre autre à une recomposition, à une redéfinition et à un usage rénové des formes de visibilité traditionnelles en comparaison fortement organisées (<b>comme</b> <b>par</b> <b>exemple,</b> les institutions, les associations, les syndicats, etc.), ainsi qu'à de nouvelles modalités d'exercice des pouvoirs Nous questionnerons enfin le changement que laisse augurer le développement du Websocial, nous demandant en quoi le rapport médiatique/communicationnel qui le sous-tend, et l'éthique qui l'anime, font qu'il est ou pas dépositaire d'un modèle sociétal original, modèle de valeurs où l'individu tendrait à céder la place au collectif - un peu dans l'esprit et la manière des hackers du collectif des Anonymous - et où de manière corollaire se redéploieraient les normes de sociabilité en usage. [1] Au sens de l'anthropologie de Linton[2] Nous utiliserons le néologisme de Websocial (tout attaché) pour désigner une certaine instance de l'Internet qui, de par sa simplicité d'utilisation, son accès peu discriminant d'un point de vue économique, son appétence pour le communautaire et le contributif, viendrait mettre au cœur du dispositif sociotechnique un " individu acteur " émancipé de ses traditionnelles tutelles et simultanément produirait des sociabilités inédites...|$|E
40|$|With recent {{progress}} in computing, algorithmics and telecommunications, 3 D models are increasingly used in various multimedia applications. Examples include visualization, gaming, entertainment and virtual reality. In the multimedia domain 3 D {{models have been}} traditionally represented as polygonal meshes. This piecewise planar representation {{can be thought of}} as the analogy of bitmap images for 3 D surfaces. As bitmap images, they enjoy great flexibility and are particularly well suited to describing information captured from the real world, through, for instance, scanning processes. They suffer, however, from the same shortcomings, namely limited resolution and large storage size. The compression of polygonal meshes has been a very active field of research in the last decade and rather efficient compression algorithms have been proposed in the literature that greatly mitigate the high storage costs. However, such a low level description of a 3 D shape has a bounded performance. More efficient compression should be reachable through the use of higher level primitives. This idea has been explored to a great extent in the context of model based coding of visual information. In such an approach, when compressing the visual information a higher level representation (e. g., 3 D model of a talking head) is obtained through analysis methods. This can be seen as an inverse projection problem. Once this task is fullled, the resulting parameters of the model are coded instead of the original information. It is believed that if the analysis module is efficient enough, the total cost of coding (in a rate distortion sense) will be greatly reduced. The relatively poor performance and high complexity of currently available analysis methods (except for specific cases where a priori knowledge about the nature of the objects is available), has refrained a large deployment of coding techniques based on such an approach. Progress in computer graphics has however changed this situation. In fact, nowadays, an increasing number of pictures, video and 3 D content are generated by synthesis processing rather than coming from a capture device such as a camera or a scanner. This means that the underlying model in the synthesis stage can be used for their efficient coding without the need for a complex analysis module. In other words it would be a mistake to attempt to compress a low level description (e. g., a polygonal mesh) when a higher level one is available from the synthesis process (e. g., a parametric surface). This is, however, what is usually done in the multimedia domain, where higher level 3 D model descriptions are converted to polygonal meshes, if anything by the lack of standard coded formats for the former. On a parallel but related path, the way we consume audio-visual information is changing. As opposed to recent past and a large part of today's applications, interactivity is becoming a key element in the way we consume information. In the context of interest in this dissertation, this means that when coding visual information (an image or a video for instance), previously obvious considerations such as decision on sampling parameters are not so obvious anymore. In fact, as in an interactive environment the effective display resolution can be controlled by the user through zooming, there is no clear optimal setting for the sampling period. This means that because of interactivity, the representation used to code the scene should allow the display of objects in a variety of resolutions, and ideally up to infinity. One way to resolve this problem would be by extensive over-sampling. But this approach is unrealistic and too expensive to implement in many situations. The alternative would be to use a resolution independent representation. In the realm of 3 D modeling, such representations are usually available when the models are created by an artist on a computer. The scope of this dissertation is precisely the compression of 3 D models in higher level forms. The direct coding in such a form should yield improved rate-distortion performance while providing a large degree of resolution independence. There has not been, so far, any major attempt to efficiently compress these representations, such as parametric surfaces. This thesis proposes a solution to overcome this gap. A variety of higher level 3 D representations exist, of which parametric surfaces are a popular choice among designers. Within parametric surfaces, Non-Uniform Rational B-Splines (NURBS) enjoy great popularity as a wide range of NURBS based modeling tools are readily available. Recently, NURBS has been included in the Virtual Reality Modeling Language (VRML) and its next generation descendant eXtensible 3 D (X 3 D). The nice properties of NURBS and their widespread use has lead us to choose them as the form we use for the coded representation. The primary goal of this dissertation is the definition of a system for coding 3 D NURBS models with guaranteed distortion. The basis of the system is entropy coded differential pulse coded modulation (DPCM). In the case of NURBS, guaranteeing the distortion is not trivial, as some of its parameters (e. g., knots) have a complicated influence on the overall surface distortion. To this end, a detailed distortion analysis is performed. In particular, previously unknown relations between the distortion of knots and the resulting surface distortion are demonstrated. Compression efficiency is pursued at every stage and simple yet efficient entropy coder realizations are defined. The special case of degenerate and closed surfaces with duplicate control points is addressed and an efficient yet simple coding is proposed to compress the duplicate relationships. Encoder aspects are also analyzed. Optimal predictors are found that perform well across a wide class of models. Simplification techniques are also considered for improved compression efficiency at negligible distortion cost. Transmission over error prone channels is also considered and an error resilient extension defined. The data stream is partitioned by independently coding small groups of surfaces and inserting the necessary resynchronization markers. Simple strategies for achieving the desired level of protection are proposed. The same extension also serves the purpose of random access and on-the-fly reordering of the data stream. Avec les récents progrès de l'informatique et des télécommunications, les modèles 3 D sont de plus en plus utilisés dans les applications multimédia. La visualisation, les jeux, le divertissement et la réalité virtuelle comptent parmi les exemples les plus répandus. Dans le domaine du multimédia les modèles 3 D ont été traditionnellement représentés comme des maillages polygonaux. Cette représentation plane par morceaux, peut être vue comme l'analogue des images bitmap pour les surfaces 3 D. Comme les images bitmap, ils jouissent d'une grande flexibilité et sont particulièrement bien adaptés pour décrire des informations acquises depuis le monde réel, <b>comme</b> <b>par</b> <b>exemple,</b> lors d'un processus de balayage. Ils souffrent, cependant, des mêmes limitations, notamment une résolution limitée et un grand espace de stockage. La compression de maillages polygonaux est un domaine de recherche très actif depuis une décennie et des algorithmes de compression efficaces permettant de réduire fortement les besoins en place de stockage, ont été proposés dans la littérature. Cependant, cette description bas-niveau de formes 3 D a une performance limitée. Une compression plus efficace devrait être possible avec l'usage de primitives de plus haut niveau. Cette idée a été extensivement explorée dans le contexte du codage à base de modèles de l'information visuelle. Dans une telle approche, lors de la compression de l'information visuelle une représentation de plus haut niveau (par ex. un modèle 3 D d'une tête parlante) est obtenue par analyse. Ceci peut être vu comme un problème de projection inverse. Une fois cette tâche accomplie, les paramètres du modèle résultants sont codés à la place de l'information originale. Il est communément admis que si le module d'analyse est suffisamment efficace le coût total de codage (dans le sens débit distorsion) en sera largement réduit. La performance relativement basse et la haute complexité des méthodes d'analyse existantes (mis à part des cas spécifiques où une connaissance a priori de la nature des objets est disponible), a empêché un large déploiement des techniques de codage basées sur une telle approche. Le progrès dans le domaine de l'infographie (computer graphics) a néanmoins changé cette situation. En effet, de nos jours, un nombre croissant d'images, vidéos et contenu 3 D sont générés par procédés de synthèse au lieu de provenir d'un appareil de capture, tels une caméra ou un scanner. Cela signifie que le modèle sous-jacent dans le stade de synthèse peut être utilisé pour améliorer les performances de codage sans avoir besoin de recourir à un module d'analyse hautement complexe. En d'autres mots, ce serait une erreur que de vouloir essayer de compresser une description bas-niveau (par ex. un maillage polygonal) alors qu'une description de plus haut niveau est disponible dans le processus de synthèse (par ex. une surface paramétrique). Cela est, cependant, ce qui est couramment fait dans le domaine du multimédia, où des descriptions de modèles 3 D de haut niveau sont convertis en maillage polygonaux, ne serait-ce que par manque d'un format standard pour le codage de ceux-ci. Par ailleurs, la façon dont nous consommons l'information audiovisuelle est en train de changer. A l'opposé des anciennes applications et une grande partie des actuelles, l'interactivité est en train de devenir un élément clé de la manière dont nous consommons l'information. Dans le cadre de la présente dissertation, cela signifie que, lorsque nous codons une information visuelle (par ex. une image ou une vidéo), des considérations évidentes par le passé telles que la sélection des paramètres d'échantillonnage n'est plus aussi évidente qu'avant. En effet, à l'instar d'un environnement interactif où la résolution d'affichage effective peut être contrôlée par l'utilisateur à travers un zoom, il n'y a pas de choix optimal clairement défini pour les paramètres d'échantillonnage. Cela signifie qu'à cause de l'interactivité, la représentation utilisée pour coder la scène devrait permettre l'affichage des objets dans une large gamme de résolutions et, idéalement, jusqu'à l'infini. Une façon de résoudre ce problème serait l'utilisation d'un suréchantillonnage extensif. Néanmoins, cette approche est irréaliste et trop coûteuse à implanter dans beaucoup de situations. L'alternative serait d'utiliser une représentation indépendante de la résolution. Dans le domaine du modelage 3 D, lesdites représentations sont couramment disponibles lorsque les modèles sont crées par un artiste sur un ordinateur. Le sujet de cette dissertation est précisément la compression de modèles 3 D dans une forme de plus haut niveau. Le codage direct sous une telle forme devrait délivrer une performance débit distorsion améliorée tout en procurant un large degré d'indépendance de résolution. Il n'y a pas eu, jusqu'à ce jour, de travaux majeurs sur la compression efficace de telles représentations, telles que les surfaces paramétriques. Cette thèse propose une solution pour combler ce vide. Une variété de représentations 3 D de haut niveau existe, parmi lesquelles les surfaces paramétriques sont un choix répandu parmi les designers. Dans la famille des surfaces paramétriques, les B-Splines rationnelles non-uniformes (NURBS) jouissent d'une grande popularité étant donné qu'une large gamme d'outils basés sur les NURBS sont couramment disponibles. Récemment, les NURBS ont été ajoutées dans le Virtual Reality Modeling Language (VRML) ainsi que son descendant de nouvelle génération le eXtensible 3 D (X 3 D). Les bonnes propriétés des NURBS et leur utilisation largement répandue nous ont conduit à les choisir comme forme sous laquelle les modèles seront codés. Le but principal de cette dissertation est la définition d'un système de codage des modèles 3 D NURBS avec une distorsion garantie. La base du système est la modulation par impulsion et codage différentiel (DPCM) codée entropiquement. Dans le cas de NURBS, garantir la distorsion n'est pas évident, dès lors que certains de ses paramètres (par ex. noeuds) ont une influence compliquée sur la distorsion totale de la surface. A cette fin, une analyse détaillé de la distorsion est effectuée. En particulier, des relations jusqu'alors inconnues entre la distorsion des noeuds et la distorsion de la surface résultante est démontrée. L'efficacité de la compression est recherchée à chaque stade et des codeurs entropiques simples mais néanmoins efficaces sont définis. Le cas particulier de surfaces fermées et dégénérées avec des point de contrôle dupliqués est adressé et un codage simple et efficace est proposé pour compresser les relations de duplication. Les aspects de l'encodeur sont aussi analysés. Des prédicteurs optimaux ayant une bonne performance sur une large classe de modèles sont trouvés. Des techniques de simplification, ayant un coût négligeable sur la distorsion, sont aussi considérées pour une meilleure efficacité de compression. La transmission sur des canaux présentant un taux d'erreur non négligeable est aussi considérée est une extension de résilience aux erreurs est définie. Le train de données est morcelé en codant indépendamment des petits groupes de surfaces et en insérant les marqueurs de resynchronisation nécessaires. Des stratégies simples pour atteindre le niveau désiré de protection sont proposées. La même extension sert aussi pour l'accès aléatoire et le réordonnancement à la demande du train de données...|$|E
40|$|Video {{signals are}} {{sequences}} of natural images, where images are often modeled as piecewise-smooth signals. Hence, video {{can be seen}} as a 3 D piecewise-smooth signal made of piecewise-smooth regions that move through time. Based on the piecewise-smooth model and on related theoretical work on rate-distortion performance of wavelet and oracle based coding schemes, one can better analyze the appropriate coding strategies that adaptive video codecs need to implement in order to be efficient. Efficient video representations for coding purposes require the use of adaptive signal decompositions able to capture appropriately the structure and redundancy appearing in video signals. Adaptivity needs to be such that it allows for proper modeling of signals in order to represent these with the lowest possible coding cost. Video is a very structured signal with high geometric content. This includes temporal geometry (normally represented by motion information) as well as spatial geometry. Clearly, most of past and present strategies used to represent video signals do not exploit properly its spatial geometry. Similarly to the case of images, a very interesting approach seems to be the decomposition of video using large over-complete libraries of basis functions able to represent salient geometric features of the signal. In the framework of video, these features should model 2 D geometric video components as well as their temporal evolution, forming spatio-temporal 3 D geometric primitives. Through this PhD dissertation, different aspects on the use of adaptivity in video representation are studied looking toward exploiting both aspects of video: its piecewise nature and the geometry. The first part of this work studies the use of localized temporal adaptivity in subband video coding. This is done considering two transformation schemes used for video coding: 3 D wavelet representations and motion compensated temporal filtering. A theoretical R-D analysis as well as empirical results demonstrate how temporal adaptivity improves coding performance of moving edges in 3 D transform (without motion compensation) based video coding. Adaptivity allows, at the same time, to equally exploit redundancy in non-moving video areas. The analogy between motion compensated video and 1 D piecewise-smooth signals is studied as well. This motivates the introduction of local length adaptivity within frame-adaptive motion compensated lifted wavelet decompositions. This allows an optimal rate-distortion performance when video motion trajectories are shorter than the transformation "Group Of Pictures", or when efficient motion compensation can not be ensured. After studying temporal adaptivity, the second part of this thesis is dedicated to understand the fundamentals of how can temporal and spatial geometry be jointly exploited. This work builds on some previous results that considered the representation of spatial geometry in video (but not temporal, i. e, without motion). In order to obtain flexible and efficient (sparse) signal representations, using redundant dictionaries, the use of highly non-linear decomposition algorithms, like Matching Pursuit, is required. General signal representation using these techniques is still quite unexplored. For this reason, previous to the study of video representation, some aspects of non-linear decomposition algorithms and the efficient decomposition of images using Matching Pursuits and a geometric dictionary are investigated. A part of this investigation concerns the study on the influence of using a priori models within approximation non-linear algorithms. Dictionaries with a high internal coherence have some problems to obtain optimally sparse signal representations when used with Matching Pursuits. It is proved, theoretically and empirically, that inserting in this algorithm a priori models allows to improve the capacity to obtain sparse signal approximations, mainly when coherent dictionaries are used. Another point discussed in this preliminary study, on the use of Matching Pursuits, concerns the approach used in this work for the decompositions of video frames and images. The technique proposed in this thesis improves a previous work, where authors had to recur to sub-optimal Matching Pursuit strategies (using Genetic Algorithms), given the size of the functions library. In this work the use of full search strategies is made possible, at the same time that approximation efficiency is significantly improved and computational complexity is reduced. Finally, a priori based Matching Pursuit geometric decompositions are investigated for geometric video representations. Regularity constraints are taken into account to recover the temporal evolution of spatial geometric signal components. The results obtained for coding and multi-modal (audio-visual) signal analysis, clarify many unknowns and show to be promising, encouraging to prosecute research on the subject. Video signals are sequences of natural images, where images are often modeled as piecewise-smooth signals. Hence, video {{can be seen as}} a 3 D piecewise-smooth signal made of piecewise-smooth regions that move through time. Based on the piecewise-smooth model and on related theoretical work on rate-distortion performance of wavelet and oracle based coding schemes, one can better analyze the appropriate coding strategies that adaptive video codecs need to implement in order to be efficient. Efficient video representations for coding purposes require the use of adaptive signal decompositions able to capture appropriately the structure and redundancy appearing in video signals. Adaptivity needs to be such that it allows for proper modeling of signals in order to represent these with the lowest possible coding cost. Video is a very structured signal with high geometric content. This includes temporal geometry (normally represented by motion information) as well as spatial geometry. Clearly, most of past and present strategies used to represent video signals do not exploit properly its spatial geometry. Similarly to the case of images, a very interesting approach seems to be the decomposition of video using large over-complete libraries of basis functions able to represent salient geometric features of the signal. In the framework of video, these features should model 2 D geometric video components as well as their temporal evolution, forming spatio-temporal 3 D geometric primitives. Through this PhD dissertation, different aspects on the use of adaptivity in video representation are studied looking toward exploiting both aspects of video: its piecewise nature and the geometry. The first part of this work studies the use of localized temporal adaptivity in subband video coding. This is done considering two transformation schemes used for video coding: 3 D wavelet representations and motion compensated temporal filtering. A theoretical R-D analysis as well as empirical results demonstrate how temporal adaptivity improves coding performance of moving edges in 3 D transform (without motion compensation) based video coding. Adaptivity allows, at the same time, to equally exploit redundancy in non-moving video areas. The analogy between motion compensated video and 1 D piecewise-smooth signals is studied as well. This motivates the introduction of local length adaptivity within frame-adaptive motion compensated lifted wavelet decompositions. This allows an optimal rate-distortion performance when video motion trajectories are shorter than the transformation "Group Of Pictures", or when efficient motion compensation can not be ensured. After studying temporal adaptivity, the second part of this thesis is dedicated to understand the fundamentals of how can temporal and spatial geometry be jointly exploited. This work builds on some previous results that considered the representation of spatial geometry in video (but not temporal, i. e, without motion). In order to obtain flexible and efficient (sparse) signal representations, using redundant dictionaries, the use of highly non-linear decomposition algorithms, like Matching Pursuit, is required. General signal representation using these techniques is still quite unexplored. For this reason, previous to the study of video representation, some aspects of non-linear decomposition algorithms and the efficient decomposition of images using Matching Pursuits and a geometric dictionary are investigated. A part of this investigation concerns the study on the influence of using a priori models within approximation non-linear algorithms. Dictionaries with a high internal coherence have some problems to obtain optimally sparse signal representations when used with Matching Pursuits. It is proved, theoretically and empirically, that inserting in this algorithm a priori models allows to improve the capacity to obtain sparse signal approximations, mainly when coherent dictionaries are used. Another point discussed in this preliminary study, on the use of Matching Pursuits, concerns the approach used in this work for the decompositions of video frames and images. The technique proposed in this thesis improves a previous work, where authors had to recur to sub-optimal Matching Pursuit strategies (using Genetic Algorithms), given the size of the functions library. In this work the use of full search strategies is made possible, at the same time that approximation efficiency is significantly improved and computational complexity is reduced. Finally, a priori based Matching Pursuit geometric decompositions are investigated for geometric video representations. Regularity constraints are taken into account to recover the temporal evolution of spatial geometric signal components. The results obtained for coding and multi-modal (audio-visual) signal analysis, clarify many unknowns and show to be promising, encouraging to prosecute research on the subject. Le signal vidéo est une séquence d'images en mouvement, dont les images sont souvent modelées comme des signaux réguliers par morceaux. Ainsi, le signal vidéo peut être considéré comme un signal 3 D régulier par morceaux, et composé de régions qui suivent un certain mouvement a travers le temps. La modélisation du signal vidéo par morceaux permet d'analyser en détail le comportement de différentes stratégies de codage, et ainsi de déterminer quelles sont les approches les plus appropriées pour maximiser le taux de compression. Afin de permettre un codage efficace de la vidéo, il est nécessaire d'utiliser des méthodes adaptatives de décomposition du signal. Cette adaptabilité doit être optimisée pour garantir une modélisation du signal avec un coût de codage minimum. La nature du signal vidéo est fortement liée à sa structure, avec une forte composante géométrique. Celle-ci inclut tant la géométrie temporelle (normalement représentée par l'information du mouvement) que la géométrie spatiale. La plupart des méthodes utilisées pour la représentation du signal vidéo ne tient pas compte de sa géométrie spatiale. De même que dans le cas des images, une stratégie prometteuse pour exploiter conjointement la structure géométrique spatio-temporelle est celle qui utilise des dictionnaires redondants avec une forte composante géométrique. Dans le contexte de la vidéo, les primitives géométriques 2 D doivent suivre une évolution temporelle en formant des complexes primitives 3 D qui ont la fonction de représenter, en même temps, les composantes géométriques spatiales et temporelles du signal. Dans cette thèse de doctorat, plusieurs aspects concernant l'utilisation de méthodes adaptatives pour la modélisation du signal vidéo sont traités. Cette thèse traite particulièrement des aspects structurels de la vidéo ainsi que de sa nature géométrique. La première partie du présent travail porte sur l'étude de l'utilisation de décompositions temporelles adaptatives dans des approches basées sur la décomposition de la vidéo en sous-bandes. L'influence de l'adaptabilité est notamment discutée pour deux stratégies de codage: les transformées en ondelettes 3 D et le filtrage temporel avec compensation de mouvement. Les avantages de l'utilisation d'adaptabilité dans des représentations basées sur la transformée en ondelettes 3 D sont démontrés à l'aide d'une étude théorique R-D ainsi que par des résultats expérimentaux. L'utilisation de l'adaptabilité dans le cadre du filtrage temporel avec compensation du mouvement est aussi étudiée en faisant une analogie entre le signal vidéo, avec le mouvement compensé, et les signaux réguliers par morceaux 1 D. Cette analogie suggère l'introduction des transformées de longitude localement variable dans des schémas de décomposition par ondelettes bases sur des lifting steps. Cette modification permet une plus forte compression du signal grâce à une meilleure adaptation de la représentation des trajectoires de mouvement avec une longueur inférieure à celle du Groupe d'Images (GOP - en anglais -) ou quand l'erreur due à la compensation de mouvement est trop élevé. Après l'étude d'adaptabilité temporelle, une deuxième partie de cette thèse se concentre également sur l'étude et la compréhension des concepts de base pour exploiter, conjointement, la structure géométrique spatio-temporelle du signal. Cette recherche se base sur des études précédentes qui tenaient compte de la géométrie spatiale de la vidéo, sans considérer son évolution temporelle (mouvement). Afin d'obtenir des représentations flexibles et efficaces (parcimonieuses), avec des dictionnaires redondants, il faut utiliser des algorithmes de décomposition hautement non linéaires, tels que les algorithmes gloutons (Greedy algorithms et Matching Pursuits en anglais). L'utilisation de ces techniques est encore peu explorée. Pour cette raison, avant d'étudier de telles représentations, certains aspects liés à l'utilisation de ces algorithmes conjointement avec des dictionnaires cohérents pour l'approximation des images et de la vidéo sont étudiés. Une partie de cette étude présente l'utilisation des modèles à priori dans des algorithmes non-linéaires comme les Matching Pursuits. En fonction du dictionnaire utilisé, et du signal, les Matching Pursuits peuvent avoir des grandes difficultés pour arriver à obtenir des expansions parcimonieuses optimales. Basé sur ce résultat, il peut être démontré, de manière théorique et expérimentale, que l'utilisation des modèles à priori (<b>comme</b> <b>par</b> <b>exemple</b> des modèles probabilistes) peut contribuer très significativement à l'amélioration des performances de ces algorithmes. Une autre partie de l'étude préliminaire, pour l'utilisation des Matching Pursuits, concerne l'approche utilisée dans cette thèse pour la décomposition du signal vidéo et des images. L'approche proposé dans cette thèse améliore une méthode existante de Matching Pursuits utilisée dans le passé dans un but similaire. Cette méthode était basée, pour des raisons de complexité calculatoire, sur l'utilisation d'algorithmes génétiques. La méthode proposée dans cette thèse rend possible, d'une manière plus rapide et efficace, la substitution de l'algorithme génétique par une recherche exhaustive. Finalement, l'utilisation des Matching Pursuits avec des modèles à priori pour la décomposition du signal vidéo est étudiée. Des critères de régularité ont étés imposés afin de capturer l'évolution temporelle des composantes géométriques 2 D. Les résultats obtenus pour le codage de ces représentations, ainsi que les résultats issus des analyses multimodales (audio/vidéo) d'une séquence, permettent d'éclaircir une grand partie des points incompris jusqu'alors sur l'utilisation des dictionnaires redondants avec des Matching Pursuits pour les représentations géométriques adaptatives en espace et en temps du signal vidéo...|$|E
5000|$|... "la chaleur, tant dans la terre que dans l’air ne peut régulièrement venir que des rayons du soleil. J’ose dire {{pourtant}} que j’ai été assez heureux pour l’imiter en petit à l’égard de quelques petits fruits : j’en ai fait mûrir cinq et six semaines devant le temps, <b>par</b> <b>exemple</b> des fraises à la fin mars, des précoces, et des pois en avril, des figues en juin, des asperges et des laitues pommées en décembre, janvier ..." ...|$|R
5000|$|The metteur en scène, Jean-Marie Villégier {{named his}} company the Illustre Théâtre in {{reference}} to Molière : « On sait <b>par</b> <b>exemple</b> quil a joué Corneille, Tristan LHermite, Rotrou. Mon idée était de travailler ce répertoire, tel que la trouvé Molière avant de devenir lécrivain que lon connaît. » ("We know, for example, that he played Corneille, Tristan LHermite, Rotrou. My idea was to work this repertoire, as Molière found it before becoming the writer that we know.") ...|$|R
50|$|Art. 16. S. M. le Roi d'Annam continuera, <b>comme</b> <b>par</b> le passé, à diriger l'administration intérieure de ses États, sauf les {{restrictions}} qui résultent de la présente convention.Art. 17. Les dettes actuelles de l'Annam vis-à-vis de la France seront acquittées au moyen de paiements dont le mode sera ultérieurement déterminé. S. M. le Roi d'Annam s’interdit de contracter aucun emprunt à l'étranger sans l'autorisation du Gouvernement français.|$|R
