1447|2270|Public
5|$|Temp: One of two <b>crawlers</b> (cockroaches) to {{join the}} quest. He {{is one of the}} first <b>crawlers</b> who {{encounters}} Boots and her brother and befriends the two.|$|E
5|$|Two fliers, two <b>crawlers,</b> two spinners assent.|$|E
5|$|Boots (Margaret): Boots is Gregor's two-year-old sister. She accidentally {{discovers the}} {{entrance}} to the Underland. She befriends the <b>crawlers</b> immediately and is the only reason they decide to come on the quest.|$|E
40|$|Current nonparallel focused image <b>crawler</b> {{is limited}} {{largely by the}} network {{bandwidth}} which will cause low efficiency of the <b>crawler.</b> In order to overcome such disadvantage, this paper proposes a design of a focused image <b>crawler</b> based on mobile agent technology. The <b>crawler</b> uses a parallel processing architecture to archive the parallel search. It deals with URL information in the main <b>crawler</b> system, and crawls on web pages in the <b>crawler</b> subsystem. It changes the working mode of traditional <b>crawler,</b> and solves {{the problems in the}} internet such as heterogeneousness and the instability of the network. Also, it reduces the requirement of the network robustness. At the end, some critical technologies of the <b>crawler</b> system are proposed, which further increases the efficiency of the <b>crawler...</b>|$|R
25|$|FAST <b>Crawler</b> is a {{distributed}} <b>crawler.</b>|$|R
30|$|When the one-class SVM {{identifies}} a <b>crawler,</b> {{we use the}} multi-class SVM named C-SVC {{to further}} classify the <b>crawler</b> using the long session information. We number three classes as 1, 2, and 3 in the training set of C-SVC for depth-first <b>crawler,</b> width-first <b>crawler,</b> and random-like <b>crawler,</b> respectively. We also output the specific class number as results into the file for the monitoring program to update the “User Information Table”.|$|R
5|$|An {{updated version}} of the {{original}} film, titled , was made in celebration {{for the release of}} The Sky <b>Crawlers</b> in 2008. The Ghost in the Shell 2.0 release features replacements of the original animations with the latest digital film and animation technologies, such as 3D-CGI. It includes a new opening, digital screens and holographic displays, and omits several brief scenes.|$|E
5|$|On October 3, 2012, {{mainstream}} media reported that Cook {{had been killed}} in Winston-Salem, North Carolina after being struck by a car. Web <b>crawlers</b> picked up the story and the rumors went nationwide. The story was later confirmed to be that of a David Lee Cook, a North Carolina Department of Transportation worker who had been killed while removing a tree from the road during hazardous conditions. The original news organization released an explanation story after finding out of their mistake.|$|E
5|$|Over 2,400 {{species of}} mantis in about 430 genera are recognized. They are {{predominantly}} found in tropical regions, but some live in temperate areas. The systematics of mantises {{have long been}} disputed. Mantises, along with stick insects (Phasmatodea), were once placed in the order Orthoptera with the cockroaches (now Blattodea) and rock <b>crawlers</b> (now Grylloblattodea). Kristensen (1991) combined the Mantodea with the cockroaches and termites into the order Dictyoptera, suborder Mantodea.|$|E
50|$|The {{initial list}} of URLs {{contained}} in the <b>crawler</b> frontier are known as seeds. The web <b>crawler</b> will constantly ask the frontier what pages to visit. As the <b>crawler</b> visits each of those pages, it will inform the frontier with the response of each page. The <b>crawler</b> will also update the <b>crawler</b> frontier with any new hyperlinks contained in those pages it has visited. These hyperlinks {{are added to the}} frontier and will visit those new web pages based on the policies of the <b>crawler</b> frontier. This process continues recursively until all URLs in the crawl frontier are visited.|$|R
5000|$|... #Caption: A {{tethered}} pipeline ILI <b>crawler</b> manufactured {{and operated}} by Diakont. Technicians use the socket {{on the front of}} this <b>crawler</b> to attach modules using different inspection technologies; this <b>crawler</b> is shown with an EMAT inspection module.|$|R
40|$|Abstract. A multi-body {{dynamics}} {{model and}} ground {{model of a}} small <b>crawler</b> chassis was built on software RecurDyn/Track(LM). The simulations of the small <b>crawler</b> chassis turning on hard and soft terrain are implemented respectively, {{and the results are}} analyzed and compared. The steering properties of <b>crawler</b> chassis turning on soft terrain are emphasized. The simulation results can provide some theoretical guidance for <b>crawler</b> chassis steering performance...|$|R
5|$|The Mysterium Tech (or MysTech) {{system allows}} players to use in-game objects collectively known as MysTech, {{and create new}} MysTech by using a {{configuration}} screen accessed through Elementor Host items. MysTech cannot be used until they are awakened after a certain story event. Eight basic colors of MysTech exist, representing different elements; for example, green represents poison. Players can use MysTech to inflict damage upon enemies, plague them with certain status effects (such as freezing them in place), or heal party members. Casting status effect-MysTech on party members will cure them if afflicted by enemy status spells. MysTech slabs and Elementor Hosts can be found as treasure in the game world or bought from shops. To create MysTech, players place colored bugs (found on small hills in several game locations) in empty slots on an Elementor Host. The color of bugs placed in the function slot determines the color of MysTech, while other slots modify the power and/or range of the spell. Players can add special bugs known as Cobalt <b>Crawlers</b> to make a spell target all enemies instead of one; a Host filled with eight <b>Crawlers</b> unlocks a secret spell. The effect of bugs can be amplified by feeding them petals from Lifeflowers, {{which can be found}} scattered throughout the world of Anachronox. Special types of Hosts with two or three different functions allow players to pick which MysTech function to use in battle.|$|E
5|$|The {{bulk of the}} {{adventure}} takes place in two locations: the upper level fortress of the hill giants' lair, and the dungeon level beneath it. In the upper level there are halls, barracks and common rooms. These rooms house Chief Nosnra and other hill giants, ogres, and servants. The dungeon level consists of slave quarters, torture chambers, and caverns. These house troglodytes, bugbears, and carrion <b>crawlers.</b> The majority of the treasure can be found by searching the dungeons. The chief's treasure room contains {{a map of the}} glacial rift from Glacial Rift of the Frost Giant Jarl, and a magic chain that automatically transports the party there.|$|E
25|$|Web <b>crawlers</b> {{typically}} {{identify themselves}} to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which <b>crawlers</b> have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out {{more information about the}} crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web <b>crawlers.</b> Spambots and other malicious Web <b>crawlers</b> are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.|$|E
40|$|The {{functions}} of Web <b>crawler</b> download information from web for search engine. Web pages changed without any notice. Web <b>crawler</b> has to revisit web site to download updated and new web pages. It is estimated 40 % of current web traffic {{is generated by}} web <b>crawler.</b> This paper proposes query based approach to inform updates on web site to web <b>crawler</b> using Dynamic web page and HTTP GET Request. Dynamic web page generates HTML based response having list of updates on web site after <b>crawler</b> last visit. Web <b>crawler</b> only visits updated web pages instead of visiting full web sites for updates. Proposed scheme is tested & results show {{that it is very}} promising...|$|R
40|$|In this paper, {{a passive}} {{bilateral}} tele-operation of a pneumatic powered robotic <b>crawler</b> is presented. The <b>crawler</b> {{is designed to}} be four legged with three degrees of freedom along each leg. The front two legs of the <b>crawler</b> are remotely maneuvered by means of a Phantom haptic interface, with hind legs tracing the path generated by the front legs. The <b>crawler</b> dynamics are obtained in the joint space of the <b>crawler.</b> A simplified model of Phantom dynamics is used in the controller design for achieving the required tele-operation. A passive controller is designed in the <b>crawler</b> joint space to achieve bilateral tele-operation. In this paper, simulation results of tele-operation of only a single leg of the <b>crawler</b> are provided. Results show good tracking performance of the controller. Preliminary implementation results along a single actuator have been encouraging...|$|R
2500|$|In {{addition}} to the specific <b>crawler</b> architectures listed above, there are general <b>crawler</b> architectures published by Cho ...|$|R
25|$|<b>Crawlers</b> can {{retrieve}} data much quicker and in greater depth than human searchers, {{so they can}} have a crippling impact on the performance of a site. Needless to say, if a single crawler is performing multiple requests per second and/or downloading large files, a server {{would have a hard time}} keeping up with requests from multiple <b>crawlers.</b>|$|E
25|$|The {{importance}} of a page for a crawler can also be expressed {{as a function of}} the similarity of a page to a given query. Web <b>crawlers</b> that attempt to download pages that are similar to each other are called focused crawler or topical <b>crawlers.</b> The concepts of topical and focused crawling were first introduced by Filippo Menczer and by Soumen Chakrabarti et al.|$|E
25|$|StormCrawler, a {{collection}} of resources for building low-latency, scalable web <b>crawlers</b> on Apache Storm (Apache License).|$|E
50|$|The carrion <b>crawler</b> {{appears in}} the Monster Manual for this edition (2008), {{including}} the enormous carrion <b>crawler.</b>|$|R
50|$|In total, 5 Hornsby {{caterpillar}} {{machines were}} made—3 oil powered engines for gun haulage; one small Schneider auto; and a steam <b>crawler.</b> One oil <b>crawler</b> survives in operational condition at The Tank Museum, Bovington, while {{the tracks of}} the steam <b>crawler</b> survive.|$|R
50|$|In {{addition}} to the specific <b>crawler</b> architectures listed above, there are general <b>crawler</b> architectures published by Choand Chakrabarti.|$|R
25|$|An {{example of}} the focused <b>crawlers</b> are {{academic}} <b>crawlers,</b> which crawls free-access academic related documents, such as the citeseerxbot, which is the crawler of CiteSeerX search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open source <b>crawlers,</b> such as Heritrix, must be customized to filter out other MIME types, or a middleware is used to extract these documents out and import them to the focused crawl database and repository. Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents takes {{only a small fraction}} in the entire web pages, a good seed selection are important in boosting the efficiencies of these web <b>crawlers.</b> Other academic <b>crawlers</b> may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads.|$|E
25|$|<b>Crawlers</b> can {{validate}} hyperlinks and HTML code. They {{can also}} be used for web scraping (see also data-driven programming).|$|E
25|$|It is {{important}} for Web <b>crawlers</b> to identify themselves so that Web site administrators can contact the owner if needed. In some cases, <b>crawlers</b> may be accidentally trapped in a crawler trap {{or they may be}} overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine.|$|E
50|$|As {{can be seen}} in the {{architecture}} of a web <b>crawler,</b> the <b>crawler</b> frontier, depicted as the scheduler, will contains the list of URLs to visit, the Page downloader downloads the pages from the Web and the Web repository contains the pages from a <b>crawler.</b>|$|R
5000|$|Slesser {{then joined}} Back Street <b>Crawler</b> and its {{successor}} band, <b>Crawler,</b> {{for the balance}} of the 1970s. Following the demise of <b>Crawler,</b> Slesser joined the band Geordie in 1980, after its lead singer, Brian Johnson, joined AC/DC, subsequent to the death of Bon Scott.|$|R
5000|$|Jimmy and the <b>Crawler</b> (2013) - a novella {{replacing}} the cancelled novels Krondor: The <b>Crawler</b> and Krondor: The Dark Mage.|$|R
25|$|Pages {{built on}} AJAX {{are among those}} causing {{problems}} to web <b>crawlers.</b> Google has proposed a format of AJAX calls that their bot can recognize and index.|$|E
25|$|A {{recent study}} {{based on a}} large scale {{analysis}} of robots.txt files showed that certain web <b>crawlers</b> were preferred over others, with Googlebot being the most preferred web crawler.|$|E
25|$|For {{those using}} Web <b>crawlers</b> for {{research}} purposes, {{a more detailed}} cost-benefit analysis is needed and ethical considerations {{should be taken into}} account when deciding where to crawl and how fast to crawl.|$|E
40|$|In this paper, we {{describe}} the WIRE (Web Information Retrieval Environment) project and focus on some details of its <b>crawler</b> component. The WIRE <b>crawler</b> is a scalable, highly configurable, high performance, open-source Web <b>crawler</b> which we have used to study the characteristics of large Web collections...|$|R
50|$|One is the <b>Crawler</b> unit, which {{transports}} {{the shuttle}} to a designated location. The <b>Crawler</b> unit is {{operated by the}} G.I. Joe member Hardtop, and features dual surface-clearing laser cannons and 7.62mm machine guns. The <b>Crawler</b> also contains a crank to place the booster/shuttle into launch position.|$|R
50|$|In the Dungeons & Dragons fantasy roleplaying game, the carrion <b>crawler</b> a type {{of fictional}} monster. A carrion <b>crawler</b> is {{described}} as a large yellow and green caterpillar-like aberration. The carrion <b>crawler</b> was introduced in the game's first supplement, Greyhawk, in 1975. The carrion <b>crawler</b> subsequently appeared in the first edition Advanced Dungeons & Dragons game's original Monster Manual sourcebook, and then continued to appear in the game's second edition, third edition, and fourth edition.|$|R
