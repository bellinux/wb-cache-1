20|9|Public
40|$|Abstract. In {{a seminal}} paper, Huet {{introduced}} abstract properties of term rewriting systems, and the <b>confluence</b> <b>analysis</b> of terminating term rewriting systems by critical pairs computation. In this paper, we provide an abstract notion of critical pair for arbitrary binary relations and context operators. We show how this notion {{applies to the}} <b>confluence</b> <b>analysis</b> of various transition systems, ranging from classical term rewriting systems to production rules with constraints and partial control strategies, such as the Constraint Handling Rules language CHR. Interestingly, we show in all these cases that some classical critical pairs can be disregarded. The crux of these analyses {{is the ability to}} compute critical pairs between states built with general context operators, on which a bounded, not necessarily well-founded, ordering is assumed...|$|E
40|$|Concurrent {{access to}} shared state {{results in a}} {{fundamental}} trade-off between coordination—surfacing {{in the form of}} reduced availabil-ity, decreased scalability, and increased latency—and application-level consistency, or semantic guarantees for end users. Traditional mechanisms such as serializable transactions are sufficient to en-sure application-level consistency but require synchronous coor-dination, while weaker mechanisms may sacrifice consistency for less coordination and greater scalability. In this paper, we identify a necessary and sufficient condition for achieving coordination-free execution without violating application-level consistency: invari-ant confluence. By explicitly considering application-level invari-ants, invariant <b>confluence</b> <b>analysis</b> allows databases to coordinate between operations only when anomalies that might violate invari-ants are possible. This provides a formal basis for coordination-avoiding database systems, which coordinate only when it is nec-essary to do so. We demonstrate the utility of invariant <b>confluence</b> <b>analysis</b> on a subset of SQL and via a coordination-avoiding proof-of-concept database prototype that scales linearly to over a million TPC-C New-Order transactions per second on 100 servers. 1...|$|E
40|$|Information about {{functional}} dependencies is used {{by modern}} CHR compilers for both optimisation and for further program analysis (e. g. <b>confluence</b> <b>analysis).</b> Before this work, CHR compilers relied on an ad hoc analysis for functional dependencies based on searching for rules of a particular form and the results from late storage analysis. We present a more formal functional dependency analysis of CHRs based on abstract interpretation. We show, by example, that the new analysis is more accurate than the existing ad hoc analysis. status: publishe...|$|E
40|$|In this paper, {{the mixing}} layer {{between the two}} {{incoming}} flows in a 90 degree, asymmetrical open channel confluence is investigated. Specific attention is given towards looking into the effect of bed roughness on the flow patterns in the <b>confluence.</b> This <b>analysis</b> is performed in a Serret-Frenet type axis system, in order {{to come up with}} a more convenient way of representing the measurement data. The effects on the velocity distribution, velocity difference over the mixing layer, width and location of the mixing layer are studied, and found to be influenced...|$|R
30|$|Nonlinear {{analysis}} {{is a remarkable}} <b>confluence</b> of topology, <b>analysis</b> and applied mathematics. Indeed, the fixed-point theory {{is one of the}} most rapidly growing topic of nonlinear functional analysis. It is a vast and interdisciplinary subject whose study belongs to several mathematical domains such as: classical analysis, functional analysis, operator theory, topology and algebraic topology, etc. This topic has grown very rapidly perhaps due to its interesting applications in various fields within and out side the mathematics such as: integral equations, initial and boundary value problems for ordinary and partial differential equations, game theory, optimal control, nonlinear oscillations, variational inequalities, complementary problems, economics and others.|$|R
40|$|Abstract. Regular tree {{languages}} are a popular device for reachability analysis over term rewrite systems, with many applications like analysis of cryptographic protocols, or <b>confluence</b> and termination <b>analysis.</b> At {{the heart of}} this approach lies tree automata completion, first introduced by Genet for left-linear rewrite systems. Korp and Middeldorp introduced so-called quasi-deterministic automata to extend the technique to non-left-linear systems. In this paper, we introduce the simpler notion of state-compatible automata, which are slightly more general than quasi-deterministic, compatible automata. This notion also allows us to decide whether a regular tree language is closed under rewriting, a problem which was not known to be decidable before. Several of our results have been formalized in the theorem prover Is-abelle/HOL. This allows to certify automatically generated non-confluence and termination proofs that are using tree automata techniques. ...|$|R
40|$|Abstract. Analyzing {{confluence}} of CHR programs manually {{can be an}} impractical and time consuming task. Based on a new theorem for state equivalence, this work presents the first tool for testing equivalence of CHR states. As state equivalence is {{an essential component of}} <b>confluence</b> <b>analysis,</b> we apply this tool {{in the development of a}} confluence checker that overcomes limitations of existing checkers. We further provide evaluation results for both tools and detail their modular design, which allows for extensions and reuse in future implementations of CHR tools. ...|$|E
30|$|Although {{the use of}} the log {{growth rate}} in {{analyzing}} network effects is due to stationarity concerns log differencing makes each variables noisier. Moreover sloppy reporting by small and medium-sized firms also contaminates the variable with additional measurement errors. Estimation of true regression parameters when all measurements have additional noise was studied by Frisch in the 1930 s under the rubric of statistical <b>confluence</b> <b>analysis</b> (Frisch 1934; Hendry and Morgan 1989). Similar to its modern descendant, partial identification (Manski 2009; Tamer 2010), our results show that estimation of the structural parameters ignoring measurement error provides lower bounds on estimates of the true structural parameters.|$|E
40|$|In 1945, {{the year}} Richard Stone first {{published}} on demand analysis, {{there was no}} consensus on how economic data should be analyzed; but over the decade 1945 - 54 least squares regression emerged. This process {{may be seen in}} Stone's work, but Stone also had a major influence on the emergence and formalization of the methodology. He was influenced by the Cowles Commission work, but did not see simultaneity as a major practical problem. His abandonment of <b>confluence</b> <b>analysis</b> was in part due its lack of statistical foundations, but also to his perception of residual serial correlation as a dominant problem in time-series econometrics. Copyright 1991 by Royal Economic Society. ...|$|E
40|$|In {{this paper}} {{we present a}} simple {{deterministic}} model of a biological tissue growing within a porous scaffold. By neglecting the effects of nutrient limitations and intercellular pressure on cell growth, and by using Darcy's law to model the cells' movement through the scaffold, our model is formulated as a moving boundary problem. Due to the difficulty in solving the resulting system, we reformulate it as a linear complementarity problem using the Baiocchi transformation, and give both one-dimensional analytical solutions and two-dimensional numerical ones. We then focus on the behaviour of the moving boundary as the colony approaches <b>confluence,</b> using asymptotic <b>analysis</b> to derive the time of confluence and {{the shape of the}} moving boundary; we show in particular that the moving boundary evolves to an ellipse. We also show that pressures increase considerably in the tissue shortly before the scaffold is filled, and identify the potential problem for tissue engineers of a "slit" being left devoid of cells. © World Scientific Publishing Company...|$|R
40|$|This paper {{contributes}} {{to the development of}} the theoretical understanding in the field of religious tourism management by considering the Pilgrimage of the Dew as an alternative paradigm of analysis, which has been constituted in a destination able to host a very large number of travellers (pilgrims), in a very short period of time (3 days), without damaging the environment, ecologic and social, due to the own anthropologic characteristics of this form of travelling that gives priority to the human experience more than the market consumption. Thus, the model researched in this paper through the concept of “social capital” is opportune due to the <b>confluence</b> in the <b>analysis</b> of different social sciences (anthropology, economy and sociology) and from the theory and technique of tourism, concluding that the maintenance of the authenticity and sustainability of this form of religious tourism should be planned not on the basis of a real state model as currently, but on the unseasonality and a much better and regu-lated quality of the services provided...|$|R
40|$|The {{fusion of}} myogenic cells has been {{examined}} on the fine-structural level in muscle cell cultures of embryonic Japanese Coturnix quail. Cells, selected by light microscopy, were serially sectioned normal to their long axis. In this plane, oblique sections of cell membranes are rare and plasmalemmal profiles {{are more easily}} traced between adjacent cells. In seven cases, pairs of cells, apparently fixed {{in the process of}} fusion, are joined by a single cytoplasmic bridge. Since obliquely sectioned membranes often suggest cytoplasmic <b>confluence,</b> tilting stage <b>analysis</b> was employed to resolve cell membranes in suspect cases. In contrast to such artifacts of superposition, however, the observed intercommunicating pores are contained within a pair of culs-de-sac formed by the fused membranes of both cells. These blind pouches can be traced back between the cells to the external space. The confluent regions are clearly demarcated and they are not simply areas between vesicular profiles. The results of this analysis suggest that (a) at no time is there any loss of integrity of the cellular envelope, and (b) fusion is most probably initiated at single sites between pairs of cells, the pore enlarging, leaving first vestiges and eventually no trace of the original intervening membranes...|$|R
40|$|Constraint Handling Rules is {{a logical}} {{concurrent}} committedchoice rule-based language. Recently it was shown that the classical union-find algorithm can be implemented in CHR with optimal time complexity. Here we investigate if a parallel implementation of this algorithm is also possible in CHR. The problem is hard for several reasons:- Up to now, no parallel computation model for CHR was defined. - Tarjan’s optimal union-find {{is known to be}} hard to parallelize. - The parallel code should be {{as close as possible to}} the sequential one. It turns out that <b>confluence</b> <b>analysis</b> of the sequential implementation gives almost all the information needed to parallelize the union-find algorithm under a rather general parallel computation model for CHR...|$|E
40|$|AbstractRegular tree {{languages}} are a popular device for reachability analysis over term rewrite systems, with many applications like analysis of cryptographic protocols, or confluence and termination analysis. At {{the heart of}} this approach lies tree automata completion, first introduced by Genet for left-linear rewrite systems. Korp and Middeldorp introduced so-called quasi-deterministic automata to extend the technique to non-left-linear systems. In this paper, we introduce the simpler notion of state-compatible automata, which are slightly more general than quasi-deterministic, compatible automata. This notion also allows us to decide whether a regular tree language is closed under rewriting, a problem which was not known to be decidable before. The improved precision has a positive impact in applications which are based on reachability analysis, namely termination and <b>confluence</b> <b>analysis.</b> Our results have been formalized in the theorem prover Isabelle/HOL. This allows to certify automatically generated proofs that are using tree automata techniques...|$|E
40|$|Abstract. E-government {{services}} usually process {{large amounts}} of confidential data. Therefore, security requirements for the communication between components have to be adhered in a strict way. Hence, it is of main interest that developers can analyze their modularized models of actual systems {{and that they can}} detect critical patterns. For this purpose, we present a general and formal framework for critical pattern detection and user-driven correction as well as possibilities for automatic analysis and verification at meta-model level. The technique is based on the formal theory of graph transformation, which we extend to transformations of type graphs with inheritance within a type graph hierarchy. We apply the framework to specify relevant security requirements. The extended theory is shown to fulfil the conditions of a weak adhesive HLR category allowing us to transfer analysis techniques and results shown for this abstract framework of graph transformation. In particular, we discuss how <b>confluence</b> <b>analysis</b> and parallelization can be used to enable parallel critical pattern detection and elimination. ...|$|E
40|$|We {{investigate}} experimentally the depositions of two contiguous debris flows {{flowing into}} a main river reach. The {{aim of the}} present experimental research is to analyze the geometry and the mutual interactions of debris flow deposits conveyed by these tributaries in the main channel. A set of 19 experiments has been conducted considering three values of the confluence angle, two slopes of the tributary, and three different triggering conditions (debris flows occurring simultaneously in the tributaries, or occurring first either in the upstream or in the downstream tributary). The flow rate along the main channel was always kept constant. During each experiment the two tributaries had the same slope and <b>confluence</b> angle. The <b>analysis</b> of the data collected during the experimental tests indicates that {{the volume of the}} debris fan is mainly controlled by the slope angle, as expected, while the shape of the debris deposit is strongly influenced by the confluence angle. Moreover, in the case of multiple debris flows, the deposit shape is sensitive to the triggering conditions. Critical index for damming formation available in literature has been considered and applied to the present case, and, {{on the basis of the}} collected data, considerations about possible extension of such indexes to the case of multiple confluences are finally proposed...|$|R
40|$|This study {{analyzes}} {{and discusses}} atmospheric boundary layer vertical profiles of potential temperature, specific humidity, and wind speed {{at each of}} the sides of the Brazil-Malvinas Confluence in the southwestern Atlantic Ocean. Such confluence is characterized by the meeting of water masses with very different characteristics: the southern waters of the Malvinas current can be several degrees colder and appreciably less salty than the northern Brazil current waters. At the same time, a synoptic cycle can be identified at the region, marked by the successive passages of frontal systems and extratropical cyclones. The different phases of the synoptic cycle lead to different thermal advections at the confluence, causing respective different patterns of atmospheric boundary layer adjustment to the surface heterogeneity induced by the confluence. In the present study, this adjustment along the synoptic cycle is analyzed using data from five experiments performed across the confluence from 2003 to 2008. In each of the campaigns a number of soundings were launched from a ship at both sides of the <b>confluence.</b> A climatological <b>analysis</b> with respect to the closest frontal passage is presented, and it suggests that the observations collected {{at each of the}} years analyzed are referent to a different day of the synoptic cycle. The average profiles at each side of the confluence are in agreement with previous modeling studies of warm and cold thermal advection patterns over an oceanic front. Furthermore, our study shows that peculiar transitional characteristics are also observed between the conditions of well-established warm and cold advection. At many phases of the synoptic cycle a strongly stratified boundary layer occurs at one or both sides of the confluence. Some of the observed characteristics, such as a large moisture accumulation near the surface, suggest that existing sensible and latent heat fluxes parameterizations fail under very strong stratifications, and the consequences of this deficiency are analyzed. Pages: 1 - 1...|$|R
40|$|Look around you. What unassuming {{skills or}} assets are just {{bursting}} with potential? Meet the peer economy, where people monetize skills and assets {{they already have}} using online, peer-to-peer marketplaces. Lyft, Shapeways, Etsy, Skillshare [...] . these platforms enable strangers to transact confidently. Instead of education, reskilling or being network rich, new marketplaces emerge everyday to reconfigure people's existing assets and skills into income generating opportunities. Airbnb, TaskRabbit, KitchenSurfing, Postmates [...] . From small-scale manufacturing to space sharing to personal services, amateurs and professionals alike can easily jump in. As an alternative to full-time employment with benefits-a 20 century model worn thin-the peer economy (sometimes called the "sharing economy") is setting imaginations on fire. At its best, the peer economy can reintegrate people who are defined out of the traditional workplace and, therefore, the traditional economy (the elderly, homemakers, those with varying physical and mental ableness, those at risk for human trafficking, etc.). At its worst, it exploits human labor and degrades human dignity. Between positive and negative speculations, I have identified five particularly sticky issues: 1. Can peer economy opportunities comprise a livable work lifestyle? 2. Who is accountable when something goes wrong? 3. Do legal classifications override social relationships? 4. Can providers cultivate a collective voice? 5. How do peer economy actors historically contextualize the model? The thesis begins with a historical overview of how we have arrived at this moment of possibility. The second act brings readers {{up to speed on}} conversation among investors, startups, cities, policy makers, entrenched interests, media, scholars and critics, and labor advocates. As antecedents to the peer economy, I introduce marginalized movements in the third chapter that could inform how the peer economy develops; I believe that this space can be a distributed network that matchmakces providers' needs with capacity across the sector. From 2013 - 2014, I conducted ethnographic field research to suss out emergent needs among peer economy providers, and I summarize the results in chapter four before finally tying together why the peer economy-regardless of speculation-has been so captivating. This thesis is a <b>confluence</b> of historical <b>analysis,</b> economic theory, sociology, rhetorical analysis, qualitative and ethnographic fieldwork, and legal precedents that culminates in interventions for the peer economy. First and foremost, it considers whether the peer economy is a livable work lifestyle. The peer economy is a charismatic and rapidly spreading concept that is fundamentally transforming the way many people think about employment. by Denise Fung Cheng. Thesis: S. M., Massachusetts Institute of Technology, Department of Comparative Media Studies, 2014. Cataloged from PDF version of thesis. Includes bibliographical references (pages 107 - 114) ...|$|R
40|$|E-government {{services}} usually process {{large amounts}} of confidential data, but simultaneously they shall provide simple and userfriendly graphical interfaces. Therefore, security requirements for the communication between components have to be adhered in a very strict way. Hence it is of main interest that developers can analyze their modularized models of actual systems {{and that they can}} detect critical patterns. For this purpose, we present a general and formal framework for critical pattern detection and user-driven correction as well as possibilities for automatic analysis and verification of security requirements on the meta model level. The technique is based on the formal theory of graph transformation, which we extend to transformations of type graphs with inheritance within a type graph hierarchy in order to enable the specification of relevant security requirements in this scenario. The extended theory is shown to fulfil the conditions of a weak adhesive HLR category allowing us to transfer analysis techniques and results shown for this abstract framework of graph transformation. In particular, we discuss how <b>confluence</b> <b>analysis</b> and parallelization can be used to enable distributed critical pattern detection...|$|E
40|$|Minimizing coordination, or {{blocking}} {{communication between}} con-currently executing operations, {{is key to}} maximizing scalability, availability, and high performance in database systems. However, uninhibited coordination-free execution can compromise applica-tion correctness, or consistency. When is coordination necessary for correctness? The classic use of serializable transactions is sufficient to maintain correctness but is not necessary for all applications, sacrificing potential scalability. In this paper, we develop a formal framework, invariant confluence, that determines whether an appli-cation requires coordination for correct execution. By operating on application-level invariants over database states (e. g., integrity constraints), invariant <b>confluence</b> <b>analysis</b> provides a necessary and sufficient condition for safe, coordination-free execution. When programmers specify their application invariants, this analysis al-lows databases to coordinate only when anomalies that might violate invariants are possible. We analyze the invariant confluence of com-mon invariants and operations from real-world database systems (i. e., integrity constraints) and applications and show that many are invariant confluent and therefore achievable without coordination. We apply these results to a proof-of-concept coordination-avoiding database prototype and demonstrate sizable performance gains com-pared to serializable execution, notably a 25 -fold improvement over prior TPC-C New-Order performance on a 200 server cluster. 1...|$|E
40|$|Dendritic channel {{patterns}} {{have the}} potential to isolate populations within drainages, depending on the relative position within the stream hierarchy of the populations. We investigated the extent of genetic subdivision in the Australian freshwater fish Pseudomugil signifer (Kner) (Pseudomugilidae) from two drainages in northern Queensland, Australia, using allozyme techniques. The drainages were adjacent and had similar channel patterns each with two major subcatchments coalesced to an estuarine <b>confluence.</b> <b>Analysis</b> of 30 sites across the two drainages revealed that although there was significant genetic variation among sites in both drainages, this was not between the two subcatchments in either case. This result did not support predictions of the stream hierarchy model (SHM), which would predict higher levels of variation among subcatchments than within them, nor did it suggest that estuarine conditions represent a significant barrier to dispersal in this species. More variation was among sites within each subcatchment. Multidimensional scaling plots revealed that, although most sites within a drainage were similar to one another, outlier sites occurred in each drainage, so correlations between genetic distance and geographic distance were weak. We suggest that the distance between sites and the probability of connectivity between sites may better explain the observed distribution of genetic diversity. Griffith Sciences, Griffith School of EnvironmentFull Tex...|$|E
40|$|Sediment routing {{fundamentally}} influences channel morphology and {{the propagation}} of disturbances such as debris flows. The transport and storage of bedload particles across headwater channel confluences, which may be significant nodes of the channel network in terms of sediment routing, morphology, and habitat, are poorly understood, however. We investigated patterns and processes of sediment routing through headwater confluences by comparing them to published results from lower-gradient confluences and by comparing the dispersive behavior of coarse bedload particles between headwater confluence and non-confluence reaches. We addressed these questions with a field tracer experiment using passive-integrated transponder and radio-frequency identification technology in the East Fork Bitterroot River basin, Montana, USA. Within the confluence zone, tracers tended to be deposited towards scour-hole and channel margins, suggesting narrow, efficient transport corridors that mirror those observed in prior studies, {{many of which are}} from finer-grained systems. Coarse particles in some confluence reaches experienced reduced depositional probabilities within the confluence relative to upstream and downstream of the <b>confluence.</b> <b>Analysis</b> of particle transport data suggests that variation in the spatial distribution of coarse-sediment particles may be enhanced by passing through confluences, though further study is needed to evaluate confluence effects on dispersive regimes and sediment routing on broader spatial and temporal scales...|$|E
40|$|Aspect-oriented {{software}} development aims at improving separation of concerns {{at all levels}} in the {{software development}} life-cycle, from architecture to code implementation. In this thesis we strive to develop verification methods specifically for aspect-oriented programming languages. For this purpose, we model the behaviour of these languages through an operational semantics. We use graph transformations to specify these semantics. Graph transformation has mathematical foundation, and provides an intuitive way to describe component-based systems, such as software systems. In addition, graph transformations have an executable nature, {{and can be used}} to generate the execution state space of programs. We use these state spaces for the verification of programs with aspects. We start by defining an improvement of specification using rule-based systems. Pure rule-based systems typically consist of a single, unstructured set of rules. We propose so-called control automata, which can be added on top of pure rule-based systems. Then, we specify the runtime semantics of a number of aspect-oriented languages, namely of (1) Composition Filters, (2) Featherweight Java with assignments with an aspectual extension, and (3) a subset of multithreaded Java extended with a subset of AspectJ. We show that such semantics can be used to simulate a (partial) program and we expose the steps involved in the execution of such a program and that the resulting labelled transition systems can be used for existing verification methods. Then, we propose two novel approaches that address complications caused by the use of aspect-oriented programming. The approaches are based on the given semantics. The first approach allows the detection of aspect interference on shared join points by performing a <b>confluence</b> <b>analysis</b> on the resulting state space. The second approach allows the verification of run-time properties that require tracking of individual objects over time. This is achieved by extending the semantics with special rules for adding tracking information to the graphs. We show that the approach can be used for both object-oriented and aspect-oriented implementations...|$|E
40|$|Foulds {{introduced}} six {{rules of}} tumor progression {{based on his}} observations of spontaneous mammary cancer in mice and generalized them to all forms of neoplasia [Foulds, L. (1954) Cancer Res. 14, 327 - 339 and Foulds, L. (1969) Neoplastic Development (Academic, New York), Vol. 1, preface and pp. 72 - 74. ] Rules III, IV, and V are considered controversial, and research in animals seems inadequate to resolve the controversies. A subline of NIH 3 T 3 cells undergoes progressive transformation to produce foci of increasing population density when repeatedly constrained by sequential rounds of growth to and maintenance at <b>confluence.</b> <b>Analysis</b> of the results provides a cellular basis for rules III, IV, and V. Rule III states that progression is independent {{of the growth of}} the tumor and occurs in tumors that are arrested. Cell culture shows that progression is actually favored by constraint of growth, a result inconsistent with a major role for point mutations in progression. Indeed, there is a suggestion that the transformation may arise from chromatin changes preceding apoptosis. Rule IV states that progression can be gradual or abrupt but the latter conclusion has been frequently criticized. Cell culture exhibits both forms of progression but, in particular, eliminates the doubt about the abrupt form. Rule V, which is in a sense an extension of rule IV, states that progression follows one of alternative paths of development. The results in culture indicate that every independent transforming event gives rise to foci of unique morphology. Thus, even for the single characteristic of transformed focus morphology, many alternative paths to neoplasia are available to cells. In addition to clarifying the rules of progression, a method is described for pinpointing the time of the occurrence of events that are only expressed as dense foci after a variable lag time. The results in culture reinforce Foulds' conclusion that neoplastic development is primarily an epigenetically driven process and identify some of the cellular interactions that underlie that process...|$|E
40|$|Graph {{transformation}} {{with its}} formal foundations and its {{broad range of}} theoretical results, on the one hand, and competitive tool support, on the other hand, constitutes an effective framework for model-driven software development. Within the last decade, the theory of algebraic graph transformations has been developed towards a comprehensive formal framework including several sophisticated results on modeling, analysing, and verifying graph transformation systems. Prominent theoretical results are the static verification of consistency constraints as well as static conflict detection and conflict resolution techniques. Consistency constraints provide means to declaratively define global assertions that must remain true. Conflict detection and resolution techniques provide means to statically discover potential unintended interactions of graph transformations. Based on the framework for algebraic graph transformations several model transformation tools were developed over the last years. However, {{in order to become}} suitable for the practical needs in every-day software engineering, these tool oriented graph transformation approaches integrate language concepts that go beyond the simple manipulation of plain graphs. An important concept is the treatment of data values such as integers, booleans, and strings. The integration of primitive data attributes within the graph structure is indispensable to model almost all realistic systems, since they combine the structural aspects of a system with data aspects such as computations of values. While in the last years, many advanced language concepts were adapted from the tool oriented approaches and integrated within the theory of algebraic graph transformations, there is currently no theoretical approach that appropriately reflects the de-facto data attribute handling approach of practical implementations. Thus, the main body of theoretical results does not immediately apply to those implemented approaches. As a result, current tool support for analysis and verification techniques of attributed graph transformation systems is rather limited. This thesis attempts to close this gap. To this end, a framework for attributed graph transformation systems is proposed. In contrast to existing approaches, the proposed framework reflects more closely the attribute handling of current state of the art graph transformation implementations. We show that our proposed approach preserves the fundamental theoretical results of the algebraic approach for graph transformations. Additionally, we verify the well-known results for the static verification of consistency constraints, conflict detection, and conflict resolution by <b>confluence</b> <b>analysis</b> within our framework. Finally, a prototypical implementation is provided to show that the theoretical concepts can be realized. Moreover, to assess its potential for analyzing real world applications, the prototype is applied to analyze a case study from the enterprise modeling domain...|$|E
40|$|Aspect-oriented {{software}} development aims at improving separation of concerns {{at all levels}} in the {{software development}} life-cycle, from architecture to code implementation. In particular, aspect-oriented programming addresses separation of concerns at the implementation level, by allowing the modular expression of crosscutting concerns. In this thesis we strive to develop verication methods specifically for aspectoriented languages. For this purpose, we model the behaviour of these languages through an operational semantics. We use graph transformations to specify these semantics. Graph transformation has mathematical foundation, and provides an intuitive way to describe component-based systems, such as software systems. In addition, graph transformations have an executable nature, {{and can be used}} to generate the execution state space of programs. We use these state spaces for the verification of programs with aspects. We start by defining an improvement of specification by rule-based systems. Pure rule-based systems typically consist of a single, unstructured set of rules. The behaviour of such systems is that all rules are applicable in every state. Rules can then only be forced into a certain order of application by adding special elements to the states, which are tested for within the rules. In other words, control over rule applicability is not explicit but has to be encoded in the state, which reduces understandability and maintainability of the rule-based system as a whole. We propose so-called control automata, which can be added on top of pure rule-based systems. The resulting behaviour is defined as the product of the original state space and the control automaton. Our control automata include so-called failure transitions, representing the observation of the non-applicability of one or more rules. The result is a reactive semantics for control expressions, which is distinct from the usual input-output semantics. Control automata may introduce articial non-determinism into the behaviour, which is an undesirable effect. We introduce guarded control automata {{to get rid of this}} effect, and we define a semantics preserving transformation from ordinary control automata to guarded ones. In the next part of the thesis, we specify the run-time semantics of a number of aspect-oriented languages, namely of (1) Composition Filters, (2) Featherweight Java with assignments with an aspectual extension, and (3) a subset of multithreaded Java extended with a subset of AspectJ. We illustrate how such a graph-based semantics can aid in understanding the run-time behaviour of a language. Moreover, we show that such a semantics can be used to simulate a (partial) program and we expose the steps involved in the execution of such a program. We illustrate that this executable nature benefits the rigour of the specification method; mistakes are easily detected by simply testing the simulation. Finally, we show that the resulting labelled transition systems can be used for existing verification methods. Then, we propose two novel approaches that address complications caused by the use of aspect-oriented programming. The first approach addresses a problem that can occur in many aspect-oriented languages. Aspects that in isolation behave correctly may interact when being combined. When interaction changes an aspect's behaviour or disables an aspect, we call this interference. One particular type of interference occurs when aspects are applied to shared join points, since then the ordering of the aspects can also influence the behaviour of the composition. We present an approach to detect aspect interference at shared join points. The approach is based on simulation of all orderings of the advices that are scheduled for execution at a shared joinpoint. A <b>confluence</b> <b>analysis</b> is performed on the resulting state space to detect whether the execution order has aected the resulting states. The second approach addresses the problem of verification of dynamic properties of systems. These properties can only be verified by simulating the execution of the system: an execution semantics is required. More specifically, we deal with properties that require tracking of individual objects over time. We stress the need for automatic verification of properties (or constraints) for aspect-oriented programming because of the obliviousness properties of such languages: a developer cannot tell from looking at the base code that aspects are executed. When software evolves, existing functionality may break unintentionally. We propose to augment an existing graph-based execution semantics with special verification rules. These rules can, when needed, add information to the graphs for tracking of objects. The properties are specified as events (or interactions) related to roles. Once these roles are identified in the syntax, the program can be verified regardless of implementation details. We show that the approach can be applied to both object-oriented and aspect-oriented implementations...|$|E
40|$|Multiple {{strategic}} discontinuities of the {{constantly changing}} business environment are driving organizations, {{both large and}} small to seek new ways of conducting business to create wealth. The only way organizations can cope with these strategic discontinuities, in their pursuit of strategic self-renewal, is to develop and maintain strategic flexibility whereby new sources of wealth can be created through new combinations of resources. It is through such a strategic flexibility that any organization can endeavor to achieve a dynamic fit between the organization and its environment. Dynamic capabilities and their proxies are the means by which organizations can explore and exploit various resource configurations across different possible functional, cross-functional and cross-unit activities and thereby nurture their strategic flexibility in achieving this objective of strategic self-renewal, on a sustained basis. A critical literature review pertaining to resources, dynamic capabilities, competences and strategic renewal has pointed to a dearth of holistic quantitative studies of strategic renewal from a dynamic contingency perspective or a coevolutionary perspective, particularly in the Indian context. There has not been adequate literature in striking a blend of dynamic capabilities and their proxies in terms of a judicious mix of inside-out and outside-in approaches to strategy. The empirical and conceptual gaps in literature have provided us the impetus to take upon this study. On the conceptual front, there has not been any theoretical model of integrated resource leverage that links dynamic capabilities and their proxies, in the context of strategic renewal of organizations. On the empirical front, studies on dynamic capabilities till date have been predominantly qualitative anecdotal or episodic in nature and hence firm-specific or industry-specific in their outlook. It is imperative to synthesize the conceptual debates and the diverse empirical findings towards a more integrated understanding of resources and dynamic capabilities in the context of strategic self-renewal of organizations. The present study attempts to bridge these research gaps, first by developing a conceptual model and then testing it for subsequent statistical validation. It endeavors to understand the dynamics between different resource leverages and strategic renewal initiatives and their impact on organizational performance, on the dual fronts of market performance and financial performance. It also proposes a definitional framework encompassing the hierarchy of concepts viz., resources, dynamic capabilities and dynamic competences, whose definitions from the extant literature were duly synthesized for a greater and cohesive understanding of these concepts. The objectives of the current study were: •To compare and contrast the extent of implementation of various resource leverage practices and strategic renewal initiatives in different organizations of varying age, size and ownership. •To study the dynamic interactions between various resource leverages and strategic renewal initiatives and their impact on organizational performance. •To make appropriate suggestions for enhanced leverage of resources for augmented prospects of strategic renewal of organizations. A conceptual model was developed based on support from literature and a preliminary study. The model comprises several path links (hypotheses), each of which connects two variables (constructs with pertinent indicators) and their directionality. Besides establishing rationale for each of the hypotheses, the constructs were operationalized based on an existing survey instrument (Volberda, 1998) and other allied literature. All in all, a set of 38 hypotheses was identified and formulated for the current study and has been tested using Variance-Based Structural Equation Modeling (VBSEM). The conceptual model links three major aspects viz., strategic renewal initiatives, resource leverages and organizational performance. The resource leverages are the result of constant application and development of dynamic capabilities of an organization while the strategic renewal initiatives, by and large act as proxies of these dynamic capabilities. On one hand, the model focuses on different strategic renewal initiatives that tend to jack up the absorptive capacity of an organization from time to time, with critical inputs of information from the multiple perspectives of customers, employees, competitors and business environment in general. On the other hand, it encompasses multitudinous resource leverage practices that explore and exploit different resource configurations across various possible functional, cross-functional and cross-unit activities by virtue of dynamic capabilities. Thus, a blend of inside-out and outside-in approaches to strategy is captured in this model. Based on a critical literature review and a preliminary study entailing in-depth interviews with strategy experts and a pilot study, two complementary questionnaires were developed. The first questionnaire encompasses various resource leverage practices in terms of a firm’s dynamic capabilities, and it also includes measures of organizational performance while the second questionnaire entails strategic renewal initiatives in terms of proxies of dynamic capabilities. The first questionnaire was personally administered to the CEO while the second one to a senior level manager in each organization and their responses were garnered. The sample covered 80 multi-unit organizations (80 matched pairs of respondent executives). The first phase of data analysis comprised of computation of means and standard deviations and statistical tests of comparison for each item. Kruskal-Wallis test was used for each item, across different groups of organizations, based on age-wise, size-wise and ownership-wise modes of classification. Based on the results, i. e., wherever the results rejected the null hypothesis of the foregoing Kruskal-Wallis test, subsequently, multiple testing procedure entailing pair-wise t-tests (with Bonferroni p-value adjustment method) was adopted to further explore differences, if any, between different pairs of groups. Based on the statistical significance of these tests, conclusions were drawn as to whether nature of ownership, age and size of organizations can explain the variations in resource leverage practices, strategic renewal initiatives and firm performance across organizations. Next, the survey data were analyzed at an aggregate level to test the hypothesized conceptual model by means of a multivariate technique viz., Variance-Based Structural Equation Modeling (VBSEM). Our rationale for choosing VBSEM stems from various soft prerequisites of VBSEM such as minimal sample requirements, scope for flexible interplay of data and theory, least distributional assumptions (nonparametric approach), model complexity, theory-building and scope for accommodating both latent and emergent constructs in the model. In accordance with the recommendations of Falk and Miller (1992), model trimming was carried out based on widely accepted threshold values for loadings (for reflective indicators), path coefficients and Rvalues for endogenous constructs. In addition to this, formative indicators (at the outer model level) and exogenous constructs (independent LVs at the inner model level) were handled by duly checking for multicollinearity, using a holistic approach similar to forward stepwise regression proposed by A. Koutsoyuannis (1977) and which is a revised version of the Frisch’s <b>Confluence</b> <b>Analysis.</b> After obtaining PLS estimates for all the parameters, bootstrapping for 1000 resamples was carried out to obtain stable estimates for all these parameters of PLS. Thus, bootstrap estimates were found for all the parameters of PLS viz., weights for the formative indicators and loadings for the reflective indicators in the measurement model; and path coefficients in the structural model. Pertinent F-tests were used both for Rvalues and their respective effect sizes (f values) to test their statistical significance. Direct and indirect effects of exogenous variables on the endogenous variables in the structural model were computed. Besides these, Tenenhaus’ goodness-of-fit index for the overall model was computed and this was found to be adequate, given the complexity of the model. Concomitant to the foregoing quantitative analyses, three detailed case studies in manufacturing, service and process industries, one in each of these industries, were undertaken for the purpose of triangulation. All the three case studies were carried out and analyzed on a common theme viz., dynamic interactions between different resource leverages and strategic renewal initiatives and their impact on organizational performance. These case studies also represent three categories of ownership (one listed organization, one closely held organization and one Indian multinational) and three different age groups, and thereby provide sufficient contrast in contexts. Major conclusions of the study pointed to unexplored or under-explored or under-exploited resource leverage practices and strategic renewal initiatives in the Indian context. Some of the major ones include leverage of strategic alliances, leverage of IT, leverage of Product-Market Combinations (PMCs), modularity orientation in product design, inter-industry benchmarks, resource accumulation and resource conservation strategies in general. Based on these conclusions, appropriate suggestions were made. Major suggestions to organizations include exploration of new business opportunities via strategic alliances, exploration of white space opportunities via technological boundary-spanning activities, exploration of alternative business models in different product markets, augmentation of IT integration with business processes, enhancement of collective learning via cross-functional and cross-unit activities via practices such as job rotation and proper management of knowledge portals and increased encouragement to employees (particularly technological gatekeepers and entrepreneurial boundary-spanners) in all brainstorming sessions for generation of New Product Development (NPD) ideas. Major empirical contributions of this study include establishment of significance of different strategic variables in the Indian context, assessment of the dynamic interactions between various resource leverages and strategic renewal initiatives and their impact on organizational performance on the dual fronts of market performance and financial performance. Major conceptual contributions of this study comprise refinement, integration and operationalization of theoretical concepts drawn from the diverse yet complementary sources of literature such as resources, dynamic capabilities, competences and strategic renewal from a dynamic contingency perspective or a co-evolutionary perspective concomitant to development of a comprehensive model of integrated resource leverage via dynamic capabilities and their proxies for strategic self-renewal of multi-unit organizations Directions to future researchers include longitudinal studies for greater insights of causality in the model with feedback effects, empirical studies with multiple respondents per organization, inclusion of objective measures of performance or / and industry-specific studies with a larger sample size for gaining an enhanced understanding of resource leverage practices for strategic organizational renewal...|$|E

