104|265|Public
3000|$|... ϕ was {{assisted}} with the <b>conventional</b> <b>error</b> rate, a sensitivity (SE), specificity (SP) {{and the area}} under the receiver operating characteristic curve (AUC) on the 25 % performance-set.|$|E
30|$|Analysis of {{encoding}} and decoding {{processing power}} requirement and energy consumption at the nodes in the multi-hop wireless sensor network, when implementing <b>conventional</b> <b>error</b> control coding algorithms (Bose, Chaudhuri, and Hocquenghem (BCH), Reed-Solomon and convolutional codes).|$|E
40|$|Abstract: A Physical Unclonable Function (PUF) uniquely {{identifies}} identically manufactured silicon devices. To derive keys, a stability {{algorithm is}} required. Unlike <b>conventional</b> <b>error</b> correction used in communication systems, a PUF stability algorithm has a dual mandate of accounting for environmental noise while minimally disclosing keying material; the latter, security, aspect {{is generally not}} a concern for <b>conventional</b> <b>error</b> correction use cases. For the purpose of comparison, we classify PUF stability algorithms into three Syndrome coding methods: Code-Offset; Index-Based Syndrome; Pattern Vector. We analyze and compare these methods {{with a focus on}} security and reliability properties, including a comparison of relevant security assumptions as well as a comparison of relevant ASIC PUF reliability data...|$|E
40|$|A {{technique}} is presented {{by which a}} geopotential base height between 100 and 10 mbar may be estimated {{to serve as a}} reference for the construction of stratospheric height fields from remotely sensed temperatures. The scheme employs conventional data obtained below 100 mbar coupled with remotely sensed observations above this level to derive an accurate estimate of the height field by the minimization of the error at the tie-on level, the pressure level of the base height. In a series of numerical realizations, it is demonstrated that the technique yields an unbiased and optimal estimate of the height at levels where <b>conventional</b> <b>errors</b> are typically large, particularly where the <b>conventional</b> height <b>error</b> grows rapidly in the vertical...|$|R
60|$|One {{speciality}} of Fitzpiers's was {{respected by}} Grace {{as much as}} ever--his professional skill. In this she was right. Had his persistence equalled his insight, instead of being the spasmodic and fitful thing it was, fame and fortune need never have remained a wish with him. His freedom from <b>conventional</b> <b>errors</b> and crusted prejudices had, indeed, been such as to retard rather than accelerate his advance in Hintock and its neighborhood, where people {{could not believe that}} nature herself effected cures, and that the doctor's business was only to smooth the way.|$|R
40|$|It {{is shown}} that the noise process in quantum {{computation}} can be described by spatially correlated decoherence and dissipation. We demonstrate that the <b>conventional</b> quantum <b>error</b> correcting codes correcting for single-qubit errors are applicable for reducing spatially correlated noise. Comment: 8 pages, late...|$|R
40|$|Static Random Access Memory (SRAM) {{cells in}} {{ultra-low}} power Integrated Circuits (ICs) based on nanoscale Complementary Metal Oxide Semiconductor (CMOS) devices {{are likely to}} be the most vulnerable to large-scale soft errors. <b>Conventional</b> <b>error</b> correction circuits {{may not be able to}} handle the distributed nature of such errors and are susceptible to soft errors themselves. In this thesis, a distributed error correction circuit called Self-Healing Cellular Automata (SHCA) that can repair itself is presented. A possible way to deploy a SHCA in a system of SRAM-based embedded program memories (ePM) for one type of chip multi-processors is also discussed. The SHCA is compared with <b>conventional</b> <b>error</b> correction approaches and its strengths and limitations are analyzed...|$|E
40|$|Iterative {{processing}} for linear matrix channels, aka turbo equalization, turbo demodulation, or turbo CDMA, {{has traditionally}} been studied as the concatenation of <b>conventional</b> <b>error</b> control codes with the linear (matrix) channel. However, in several situation, such as CDMA, the channel itself contains inherent redundancy, such as a repetition code contained in the direct-spread signature sequences of CDMA. For such systems, iterative demodulation of the linear channel exploiting the inherent code structure, followed by feed-forward <b>conventional</b> <b>error</b> control coding provides an efficient and powerful alternative, and outperform the more complex turbo CDMA methods for equal power modes (users). However, such equal-power systems are spectrally limited. In this paper optimized power distributions are derived and studied which allow arbitrary spectral efficiencies to be achieved with simple two-stage receivers. I...|$|E
40|$|Predictive video codecs such as MPEG and H. 26 x {{are very}} {{susceptible}} to prediction mismatch between encoder and decoder or “drift” {{when there are}} packet losses, leading to {{a significant reduction in}} the decoded quality. Targeting backward compatibility with standard predictive video codecs, we propose a method that is conceptually different from <b>conventional</b> <b>error</b> protection schemes and inspired instead by principles from distributed source coding. Extra information is sent over a lower-rate auxiliary channel to correct for errors in the decoded frame and mitigate the drift. We present extensive simulation results to demonstrate that significant gains in performance (of the order of 3 dB in PSNR over FEC-based solutions and 5 dB in PSNR over baseline solutions for typical scenarios) can be achieved over <b>conventional</b> <b>error</b> protection schemes such as Forward Error Corrections Codes under reasonable latency constraints...|$|E
40|$|By {{taking into}} account the {{physical}} nature of quantum errors it is possible to improve the efficiency of quantum error correction. Here we consider an optimisation to <b>conventional</b> quantum <b>error</b> correction which involves exploiting asymmetries in the rates of X and Z errors by reducing the rate of X correction. As an example, we apply this optimisation to the [[7, 1, 3]] code and make a comparison with <b>conventional</b> quantum <b>error</b> correction. After two levels of concatenated error correction we demonstrate a circuit depth reduction of at least 43 % and reduction in failure rate of at least 67 %. This improvement requires no additional resources and the required error asymmetry is likely to be present in most physical quantum computer architectures. Comment: 8 pages, 4 figure...|$|R
6000|$|... 'All the Primate {{had taught}} me--what heathenism is, {{how to deal}} with it, the simple truisms about the [...] "common sin, common redemption," [...] the {{capacity}} latent in every man, because he is a man, and not a fallen angel nor a brute beast, the many <b>conventional</b> <b>errors</b> on Mission (rather) ministerial work--many, many things I spoke of very fully and frequently. I felt it was a great responsibility. How strange that I forgot all my nervous dread, and only wished there could be thousands more present, for I knew that I was speaking words of truth, of hope, and love; and God did mercifully bless much that He enabled, me to say, and men's hearts were struck within them, though, indeed, I made no effort to excite them.|$|R
40|$|The {{third of}} three studies {{involving}} a comparative analysis of transactional and narrative writing of selected samples {{of students in}} two Ontario districts, this study addressed questions arising from the earlier studies and examined tit writing skills of students in grades 5, 8, and 12. Writing samples were examined for conformity to the norms of story structure and argumentation, and {{the subject matter of}} the transactional writing was analyzed. The level of affective development manifested in subsamples of both the stories and the arguments was rated. All the papers in the sample were analyzed for syntactic complexity scores and for mechanical and <b>conventional</b> <b>errors.</b> Comparisons of the first and second versions of the papers were made to assess skills in revising and editing. Findings indicated the need of students to learn how to write an argument and to revise a paper. Other iinplications were (1) tha...|$|R
30|$|Turbo codes have a {{superior}} performance over the <b>conventional</b> <b>error</b> control codes, which inspired Zhong et al. [17] to suggest using a power efficient single-hop sensor network instead of multi-hop sensor network by deploying turbo codes {{to extend the}} communication range. The high power consumption of the turbo decoder prevents the authors in [16] and [17] from implementing turbo codes on multi-hopWSNs.|$|E
40|$|Summary In {{this note}} we {{introduce}} {{a method for}} handling general boundary conditions based on an approach suggested by Nitsche (1971) for the approximation of Dirichlet boundary conditions. We use Poisson’s equations as a model problem and present the a priori and the a posteriori error estimates. Also, we show that <b>conventional</b> <b>error</b> estimates for Dirichlet and Neumann boundary conditions are a special case of the proposed error estimates...|$|E
40|$|Multiscale error {{diffusion}} (MED) {{is superior}} to <b>conventional</b> <b>error</b> diffusion algorithms as it can eliminate directional hysteresis completely. However, the complexity of this frame-oriented process is much higher and makes it not suitable for real-time applications. In this paper, a fast MED algorithm is proposed. The complexity of this algorithm is remarkably reduced as compared with conventional MED algorithms. It also supports parallel processing. Department of Electronic and Information EngineeringRefereed conference pape...|$|E
40|$|This paper {{points out}} that the {{predictability}} analysis of conventional time series may in general be invalid for long-range dependent (LRD) series since the <b>conventional</b> mean-square <b>error</b> (MSE) may generally not exist for predicting LRD series. To make the MSE of LRD series prediction exist, we introduce a generalized MSE. With that, the proof of the predictability of LRD series is presented in Hilbert space...|$|R
40|$|A new battery {{modelling}} {{method is}} presented {{based on the}} simulation error minimization criterion rather than the <b>conventional</b> prediction <b>error</b> criterion. A new integrated optimization method to optimize the model parameters is proposed. This new method is validated {{on a set of}} Li ion battery test data, and the results confirm the advantages of the proposed method in terms of the model generalization performance and long-term prediction accuracy. ...|$|R
40|$|Abstract—A novel {{family of}} low-density parity-check codes is {{proposed}} based on MacNeish–Mann theorem {{for construction of}} mutually orthogonal Latin squares. Codes from this family have high code rate, girth at least six, large minimum distance, and significantly outperform <b>conventional</b> forward <b>error</b> correction schemes based on Reed-Solomon (RS) and turbo codes. Index Terms—Forward error correction (FEC), low-density parity-check (LDPC) codes, MacNeish–Mann theorem, mutually orthogonal Latin squares (MOLS), optical communications. I...|$|R
40|$|Abstract: A turbo {{equalizer}} is {{modified to}} allow its operation in a blind manner, i. e., without resorting to training sequences or to channel identification steps. It exploits a recent variant of the constant modulus algorithm, in collaboration with differential encoding, for which the decoder is linked in an iterative scheme with a <b>conventional</b> <b>error</b> correction coder. A characterization of stationary points is obtained, and conditions for proximity to a maximum likelihood decoding rule are identified. 1...|$|E
40|$|In this paper, {{we propose}} “cross layer ” design, such as swift-OFDM, low-latency packet-awareness coder and {{adaptive}} noise filtering design for wireless multimedia delivery. Unlike the <b>conventional</b> <b>error</b> correction code, {{we take the}} bursty nature of wireless Internet into our coder design consideration. Furthermore, we adaptively filter out noise based on image content {{so that we can}} reduce noise for least visual distortion. The experiment results show that the proposed design results in better video quality. I...|$|E
40|$|In this paper, {{we present}} a new {{statistical}} modeling technique called “updating mixture of principal components ” (UMPC). UMPC specifically captures the non-stationary {{as well as the}} multi-modal characteristics of the data. Real-world data such as video data typically have these two characteristics. The video content changes over time and has a multi-modal probability distribution. We apply UMPC to perform error concealment for video data transmitted over networks with losses, and show that UMPC outperforms <b>conventional</b> <b>error</b> concealment methods. 1...|$|E
3000|$|It {{is still}} {{worth noting that}} the {{proposed}} PRC is always better than the <b>conventional</b> equal <b>error</b> protection strategy we define as worst case. Using such strategy, the source node protects all the stream layers with the same coding rate equal to γ and all receiver nodes can recover the equal number of stream layers. Thus, the worst node with highest PLR should be considered, which determines the truncated layer L [...]...|$|R
40|$|The {{purpose of}} this paper is twofold: We derive some {{fundamental}} properties of log-likelihood ratio (LLR) values and propose two novel "soft-decision" Monte Carlo simulation techniques based on probabilities or LLR values. Specifically, we prove that the pdf of LLR values is exponential symmetric and demonstrate that soft-decision simulation outperforms <b>conventional</b> bit <b>error</b> rate simulations with respect to accuracy and/or simulation time. Keywords: Monte Carlo simulation, log-likelihood values, a posteriori probability decoding. 1...|$|R
40|$|This {{study was}} made to {{determine}} the most prevalent errors, areas of weakness, and their frequency {{in the writing of}} letters so that a course in business communications dasses at Kapiolani Community College (Hawaii) could be prepared that would help students learn to write effectively. The 55 participating students were divided into two groups that met two 1 -hour-and- 20 -minute sessions a week for 18 weeks. Twenty-one different inaccuracies were compiled and rnked to frequency. In comparing weaknesses pertaining to reasoning with <b>conventional</b> <b>errors</b> in usage, the ratio was slightly more than 2 to 1. It was concluded that having successfully completed prerequisite courses and review work on style does not prepare students to write effective business communications. Recommendations were made that (1) students must be informed c.) : the importance of one idea over another and given the ability to construct them in order to effect this relationship, (2) students should develop their power to reason effectively by analyzing problems, judging reader response, weighing values, and arriving at sot [...] ind solutions, and (3) the teacher should enlist the help of other business teachers in obtaining various letter samples, newe...|$|R
40|$|Phase field {{equations}} {{are commonly}} used as a regularized model, where bulk phases are separated by interface regions that have a thickness of the order γ. Their numerical analysis is well established for a fixed parameter size γ, but <b>conventional</b> <b>error</b> estimates depend exponentially on γ− 1 and thus become useless in the relevant case if γ → 0. Technical applications include e. g. the simulation of Sn-Cu alloys {{for the production of}} lead free solder or Ni-Al alloys used for rotor blade surfaces...|$|E
40|$|Inspired by Knill's {{scheme for}} message passing error detection, here we develop a scheme for message passing error {{correction}} for the nine-qubit Bacon-Shor code. We show that for {{two levels of}} concatenated error correction, where classical information obtained at the first level is used to help interpret the syndrome at the second level, our scheme will correct all cases with four physical errors. This results in a reduction of the logical failure rate relative to <b>conventional</b> <b>error</b> correction by a factor proportional to the reciprocal of the physical error rate. Comment: 8 pages, 9 figure...|$|E
40|$|<b>Conventional</b> <b>error</b> {{analyses}} norms in {{the finite}} element method {{are based on the}} percentage error or its equivalent in some computed value as compared to the theoretically predicted value. In this paper, the author develops a simple basis for classifying 'errors of the second kind' which arise due to the 'spurious constraints' imposed by certain formulations. He shows how simple error models based on this error estimate {{may be related to the}} geometrical/structural parameters that cause a severe deterioration of results when these parameters take large values in a limiting penalty function sense...|$|E
40|$|Optical {{communications}} {{system based on}} direct detection of photons rather than heterodyning of carrier with local oscillator. Direct-detection system uses single laser source, pulse-position modulation, and Reed-Solomon coding to protect against burst <b>errors.</b> <b>Conventional</b> photomultiplier tube is receiver. Technology applicable to terrestrial communications...|$|R
40|$|Abstract. This paper proposes (apparently) novel {{standard}} error formulas for the density-weighted average derivative estimator of Powell, Stock, and Stoker (1989). Asymptotic {{validity of the}} {{standard error}}s developed in this paper {{does not require the}} use of higher-order kernels and the standard errors are “robust ” {{in the sense that they}} accommodate (but do not require) bandwidths that are smaller than those for which <b>conventional</b> standard <b>errors</b> are valid. Moreover, the results of a Monte Carlo experiment suggest that th...|$|R
40|$|We {{study how}} the non-adiabatic effect causes the {{observable}} fluctuation in the "geometric phase" for a two-level system, which {{is defined as}} the experimentally measurable quantity in the adiabatic limit. From the Rabi's exact solution to this model, we give a reasonable explanation to the experimental discovery of phase fluctuation in the superconducting circuit system [P. J. Leek, et al., Science 318, 1889 (2007) ], which seemed to be regarded as the <b>conventional</b> experimental <b>error.</b> Comment: 4 pages, 3 figure...|$|R
40|$|The {{forecast}} {{performance of}} the fractionally integrated error correction model is investigated against several competing models for the prediction of the Nikkei stock average index. The competing models include the martingale model, the vector autoregressive model and the <b>conventional</b> <b>error</b> correction model. Models are considered with and without conditional heteroscedasticity. For forecast horizons of over twenty days, the best forecasting performance is obtained for the model when fractional cointegration is combined with conditional heteroscedasticity. The results reinforce the notion that cointegration and fractional cointegration are important for long-horizon prediction...|$|E
40|$|A new {{configuration}} {{is introduced}} to integrate diplexers and power dividers. The proposed configuration {{is based on}} coupling matrix. The design of the lumped element network is based on addition of an extra term to the <b>conventional</b> <b>error</b> function of the coupling matrix to decouple the two ports of the power divider. An optimized lumped element network is implemented successfully on an EBG based guiding technology known as ridge gap waveguide. The optimization of the physical structure is done efficiently by dividing the diplexer-power divider into many sub-circuits and analyzing the corrected delay response of them...|$|E
40|$|Abstract- Jenkins-Yuhas {{network has}} been applied {{successfully}} to solve the trailer-truck backing up problem. Genetic algorithms (CA) is used to train the Jenkins-Yuhas network controller. The integration of genetic algorithms and neural networks training avoids complicated formulation of derivative function that required in <b>conventional</b> <b>error</b> back propagation techniques (gradient descent). In addition, it can avoid trapping in local minimum point. The performance of CA trained neural network controller is evaluated via simulation studies. It has been demonstrated that the controller can successfully control trailer-truck for different initial parking conditions, i. e. with same set of trained weights, to the loading dock. 1...|$|E
40|$|A digital halftoning {{method is}} {{proposed}} to diffuse error {{with a more}} symmetric error distribution by making usc {{of the concept of}} multiscale error diffusion. The method can improve the diffusion performance by effectively reducing directional hysteresis. The diffusion is row-oriented rather than frame-oriented and hence can reduce the latency and computational effort as compared with <b>conventional</b> multiscale <b>error</b> diffusion schemes. This makes it possible for real-time applications. Department of Electronic and Information EngineeringCentre for Multimedia Signal Processing, Department of Electronic and Information Engineerin...|$|R
40|$|We {{review the}} {{statistical}} models applied {{to test for}} heterogeneous treatment effects in the recent empirical literature, with a particular focus on data from randomized field experiments. We show that testing for heterogeneous treatment effects is highly common, and {{likely to result in}} a large number of false discoveries when <b>conventional</b> standard <b>errors</b> are applied. We demonstrate that applying correction procedures developed in the statistics literature can fully address this issue, and discuss the implications of multiple testing adjustments for power calculations and experimental design. ...|$|R
40|$|Applications of {{wireless}} multimedia sensor networks (WMSNs) require high data rates to enable delivery of quality video at any channel conditions. In {{order to improve}} the performance of video transmission over ultra-wideband (UWB) WMSNs, we first propose an optimum group-of picture bit-stream partitioning algorithm for unequal error protection (UEP) of video signals. To further enhance the proposed UEP scheme an adaptive equal-unequal error protection is also proposed. Performance results in terms of peak signal-to-noise ratio (PSNR) reveal that the proposed schemes outperform the <b>conventional</b> equal <b>error</b> protection and UEP schemes by up to 7 dB...|$|R
