20|1088|Public
50|$|Interscript was a {{rich text}} {{document}} markup language designed by Xerox {{to act as a}} <b>common</b> <b>interchange</b> <b>format</b> between disparate document formats. It was part of a system that included the Xerox Character Code Standard (XCCS) and the InterPress page description representation.|$|E
50|$|CIF (Common Intermediate Format or <b>Common</b> <b>Interchange</b> <b>Format),</b> {{also known}} as FCIF (Full Common Intermediate Format), is a format used to {{standardize}} the horizontal and vertical resolutions in pixels of YCbCr sequences in video signals, commonly used in video teleconferencing systems. It was first proposed in the H.261 standard.|$|E
40|$|Current {{research}} in software reengineering offers {{a great amount}} of tools specialized on certain reengineering tasks. The definition of a powerful <b>common</b> <b>interchange</b> <b>format</b> is a key issue to provide interoperability between these tools. This paper discusses aspects of data interchange formats for exchanging reengineering related data. It proposes a graph-based format to exchange both application specific concepts and data by XML documents...|$|E
40|$|A query {{language}} for xml should be {{flexible enough to}} cover the whole range of information sources that can be labelled by xml, including databases and web documents. In this article we present a comparative analysis of several {{query language}}s that have been created for xml. We study sets of desirable traits both {{from the point of view}} of the semistructured data community (that emphasise especially very large databases, the integration of heterogeneous sources and data transformation into <b>common</b> <b>interchange</b> <b>formats),</b> and from the point of view of the information retrieval community (that emphasise especially full-text searches, manipulation of the sets of results, inclusion relationships, distances, and ranking of the result documents) ...|$|R
50|$|The {{foundation}} of the ontology {{is a set of}} primitive concepts (object, activity, activity_occurrence, timepoint), constants (inf+, inf-), functions (beginof, endof), and relations (occurrence_of, participates_in, between, before, exists_at, is_occurring_at). This core ontology is then used to describe more complex concepts. The ontology uses the <b>Common</b> Logic <b>Interchange</b> <b>Format</b> (CLIF) to represent the concepts, constants, functions, and relations.|$|R
5000|$|In CGIF, {{brackets}} enclose {{the information}} inside the concept nodes, and parentheses enclose the information inside the relation nodes. The letters x and y, which are called coreference labels, {{show how the}} concept and relation nodes are connected. In the <b>Common</b> Logic <b>Interchange</b> <b>Format</b> (CLIF), those letters are mapped to variables, as in the following statement: ...|$|R
40|$|This paper {{describes}} {{the addition of}} bidirectional flow export to the IPFIX protocol, {{and the impact of}} this effort on security-related flow analysis. Along the way, it examines the application of bidirectional flow measurement to common security analysis tasks and the positive impact the adoption of IPFIX as a <b>common</b> <b>interchange</b> <b>format</b> could, and will, have on the community using flow measurement for security purposes...|$|E
40|$|We {{present a}} storage format for placing multiple, dynamic 2. 5 D point sample streams (e. g. RGBZ video) or color video {{projections}} into {{the context of}} a common world space. The approach utilizes the concept of projective geometry to let virtual projectors "cast" the data from where it was originally recorded by the involved cameras. We exemplify the data format's versatility by demonstrating how several moving cameras reproduce 3 D geometry exported from modeling software, and outline the extension to real-world acquisition. We also explain how this format can act as <b>common</b> <b>interchange</b> <b>format</b> for the camera parameters of lightfield/multi-view video footage...|$|E
40|$|International audienceMost {{software}} system properties {{can be quantified}} with the application of measurement processes. OMG's Structured Metrics Meta-Model (SMM) supports the meta-model agnostic method to define these measurements. This paper introduces the first building block of the MEASURE (Measuring Software Engineering) [5] project tool chain: the Modelio modeling tool together with the SMM Module developed based in Modelio's open source distribution. These tools permit the formal specification of metrics and a <b>common</b> <b>interchange</b> <b>format</b> enabling interoperability between the project's tools. This paper presents this new approach of metrics specification that simplifies their combination and their integration into compliant platforms and tool...|$|E
50|$|The CL Standard {{includes}} {{specifications for}} three dialects, the <b>Common</b> Logic <b>Interchange</b> <b>Format</b> (CLIF) (Annex A), the Conceptual Graph <b>Interchange</b> <b>Format</b> (CGIF) (Annex B), and an XML-based notation for Common Logic (XCL) (Annex C).The semantics of these dialects are {{defined in the}} Standard by their translation to the abstract syntax and semantics of Common Logic.Many other logic-based languages could also be defined as subsets of CL by means of similar translations; among them are the RDF and OWL languages, which have been defined by the W3C.|$|R
30|$|A {{possible}} {{outcome of}} the increasing complexity of the cloud computing sector is the demise of all-in-one monitoring tools. Many of the tools surveyed here are not complete solutions, rather they provide part of the monitoring process. With the diversification of cloud providers, APIs, applications and other factors it will become increasingly difficult to develop tools with encompass {{all areas of the}} cloud computing stack. It is therefore likely that future monitoring solutions will be comprised of several tools which can be integrated and alternated to provide comprehensive monitoring. This trend is already coming to fruition in the open source domain where collectd, statsd, Graphite, Riemann {{and a variety of other}} tools which have <b>common</b> <b>interchange</b> <b>formats</b> can be orchestrated to provide comprehensive monitoring. This trend is slowly gaining traction amongst industrial tools such as many of the monitoring as a service tools which provide interoperability and complimenting feature sets. CopperEgg for example, is interoperable with a number of data sources including Amazon CloudWatch and the two tools provide contrasting feature sets and different levels of granularity. The rise of a ’no silver bullet’ mindset would hasten the long predicted demise of conventional enterprise monitoring tools and see significant diversification of the cloud monitoring sector.|$|R
40|$|Abstract. This {{document}} {{describes a}} strawman specification for an Argument <b>Interchange</b> <b>Format</b> (AIF) {{that might be}} used for data exchange between Argumentation tools or communication in Multi-Agent Systems (MAS). The document started life as a skeleton for contributions from participants in the Technical Forum Group meeting in Budapest in September 2005, receiving also input from third parties. The results were subsequentely improved and added to by online discussion to form a more substantial. In its current form, this document {{is intended to be}} a strawman model which serves as a point of discussion for the community rather than an attempt at a definitive, all encompassing model. The hope is that it could provide a useful input to ArgMAS discussion in paricular on the utility of <b>common</b> Argumentation <b>Interchange</b> <b>Formats,</b> what form they might take and a potential research / development agenda to help realise them. ...|$|R
3000|$|... [*]H. 264 /SVC {{reference}} software JSVM 9.18 [46] {{is adopted}} as the video encoder. The generated video streaming is encoded at 30 {{frames per second}} and a GOP consists of 8 frames. The test video sequences are Foreman, Mother & Daughter, Hall, and Container in QCIF (quarter <b>common</b> <b>interchange</b> <b>format)</b> with 300 frames. Each of the sequences features a different pattern of temporal motion and spatial characteristics which is reflected in their corresponding video quality versus encoding rate dependencies. We concatenate them 10 times to be 3, 000 frames long {{in order to obtain}} statistically meaningful results. The loss requirement (Δ) and delay constraint ([...] [...]...|$|E
40|$|The MT {{engine of}} the Janus speech-to-speech {{translation}} system is designed around four main principles: 1) an interlingua approach that allows the e cient addition of new languages, 2) the use of semantic grammars that yield low cost high quality translations for limited domains, 3) modular grammars that support easy expansion into new domains, and 4) e cient integration of multiple grammars using multi-domain parse lattices and domain re-scoring. Within {{the framework of the}} C-STAR-II speech-to-speech translation effort, these principles are tested against the challenge of providing translation for a number of domains and language pairs with the additional restriction of a <b>common</b> <b>interchange</b> <b>format...</b>|$|E
40|$|Numerous {{architectural}} description languages (ADLs) {{have been}} developed, each providing complementary capabilities for architectural development and analysis. Unfortunately, each ADL and supporting toolset operates in isolation, making it di cult to integrate those tools and share architectural descriptions. Acme {{is being developed}} as a joint e ort of the software architecture research community as a <b>common</b> <b>interchange</b> <b>format</b> for architecture design tools. Acme provides a structural framework for characterizing architectures, together with annotation facilities for additional ADLspeci c information. This scheme permits subsets of ADL tools to share architectural information that is jointly understood, while tolerating the presence of information that falls outside their common vocabulary. In this paper we describe Acme's key features, rationale, and technical innovations. ...|$|E
40|$|Of {{the common}} {{commutative}} binary logical connectives, only and and {{or may be}} used as operators that take arbitrary numbers of arguments with order and multiplicity being irrelevant, that is, as connectives that take sets of arguments. This is especially evident in the <b>Common</b> Logic <b>Interchange</b> <b>Format,</b> in which it is easy for operators to be given arbitrary numbers of arguments. The reason is that and and or are associative and idempotent, as well as commutative. We extend the ability of taking sets of arguments to the other common commutative connectives by defining generalized versions of nand, nor, xor, and iff, as well as the additional, parameterized connectives andor and thresh. We prove that andor is expressively complete—all the other connectives may be considered abbreviations of it...|$|R
40|$|This paper {{describes}} the KRAFT architecture which supports {{the fusion of}} knowledge from multiple, distributed, heterogeneous sources. The architecture uses constraints as a <b>common</b> knowledge <b>interchange</b> <b>format,</b> expressed against a common ontology. Knowledge held in local sources can be tranformed into the common constraint language, and fused with knowledge from other sources. The fused knowledge is then used to solve some problem or deliver some information to a user. Problem-solving in KRAFT typically exploits pre-existing constraint solvers. KRAFT uses an open and flexible agent architecture in which knowledge sources, knowledge fusing entities, and users are all represented by independent KRAFT agents, communicating using a messaging protocol. Facilitator agents perform matchmaking and brokerage services between the various kinds of agent. KRAFT i...|$|R
40|$|A {{behavioral}} biometric such as keystroke dynamics {{which makes}} use of the typing cadence of an Individual {{can be used to}} strengthen existing security techniques effectively and cheaply. Due to the ballistic (semi-autonomous) nature of the typing behavior it is difficult to impersonate, making it useful as a biometric. Therefore in this paper, we provide a basic background of the behavioural basis behind the use of keystroke dynamics. We also discuss the data acquisition methods, approaches and the performance of the methods used by researchers on standard computer keyboards. In this survey, we find that the use and acceptance of this biometric could be increased by development of standardized databases, assignment of nomenclature for features, development of <b>common</b> data <b>interchange</b> <b>formats,</b> establishment of protocols for evaluating methods, and resolution of privacy issues...|$|R
40|$|Exchanging {{simulation}} data among simulation practitioners is to a {{great extent}} hindered by the use of different kinds of data formats in simulation software packages. The purpose of the CostGlue project is to facilitate the exchange of simulation data in the field of telecommunications. We propose a common data interchange format and a data exchange model for raw simulation data, metadata and post-processing data. Based on this model, we additionally propose a framework, CostGlue, designed for packaging simulation output data into the <b>common</b> <b>interchange</b> <b>format,</b> launching post-processing plugins and exporting data into input formats for various third party tools. As a proof of concept we have implemented the framework as a software package and released it as free software. 1...|$|E
40|$|AbstractThis paper {{addresses}} {{the problem of}} exchanging uncertainty assessments in multi-agent systems. Since {{it is assumed that}} each agent might completely ignore the internal representation of its partners, a <b>common</b> <b>interchange</b> <b>format</b> is needed. We analyze the case of an interchange format defined by means of imprecise probabilities, pointing out the reasons of this choice. A core problem with the interchange format concerns transformations from imprecise probabilities into other formalisms (in particular, precise probabilities, possibilities, belief functions). We discuss this so far little investigated question, analyzing how previous proposals, mostly regarding special instances of imprecise probabilities, would fit into this problem. We then propose some general transformation procedures, which take also account of the fact that information can be partial, i. e. may concern an arbitrary (finite) set of events...|$|E
40|$|This {{work is to}} be {{considered}} {{as an extension of}} the integration mediator [20] approach to heterogenous databases. Former approaches towards an integration of different knowledge sources through a <b>common</b> <b>interchange</b> <b>format,</b> KIF [14] seem to be dead [10] because of the unability to incorporate future developments in knowledge representation. We would like therefore to perform the integration of already existing knowledge sources, such as databases (relational and object-oriented), knowledge-based systems, spreadsheets and statistical programs by the use of a deductive database based upon annotated logic [16, 18, 19] and constraint programming. The use of annotated logic allows a proper integration of inconsistent, temporal and uncertain knowledge, the use of multiple constraint domains enables the amalgamation of semantically different knowledge representations. Annotated logic furthermore enables the use of methods for performance improvement from classical logic such as magic sets sin [...] ...|$|E
40|$|Abstract. This paper {{proposes a}} {{framework}} for transforming and integrating heterogeneous XML data sources, making use of known correspondences from them to ontologies expressed {{in the form of}} RDFS schemas. The paper first illustrates how correspondences to a single ontology can be exploited. The approach is then extended to the case where correspondences may refer to multiple ontologies, themselves interconnected via schema transformation rules. The contribution of this research is an XML-specific approach to the automatic transformation and integration of XML data, making use of RDFS ontologies as a `semantic bridge'. 1 Introduction The emergence of several new standards from the World Wide Web Consortium has provided a strong basis for enhancing the interoperability of web-based applications. XML is established as the <b>common</b> data <b>interchange</b> <b>format,</b> while RDF, RDFS and OWL provide {{a framework for}} assigning meaning to heterogenous web-based data...|$|R
40|$|This paper {{describes}} the Knowledge Reuse And Fusion/Transformation (KRAFT) architecture which supports {{the fusion of}} knowledge from multiple, distributed, heterogeneous sources. The architecture uses constraints as a <b>common</b> knowledge <b>interchange</b> <b>format,</b> expressed against a common ontology. Knowledge held in local sources can {{be transformed into a}} common constraint language, and fused with knowledge from other sources. The fused knowledge is then used to solve some problem or deliver some information to a user. Problem solving in KRAFT typically exploits pre-existing constraint solvers. KRAFT uses an open and flexible agent architecture in which knowledge sources, knowledge fusing entities and users are all represented by independent KRAFT agents, communicating using a messaging protocol. Facilitator agents perform matchmaking and brokerage services between the various kinds of agent. KRAFT is being applied to an example application in the domain of network data services design...|$|R
40|$|Dependence on {{computers}} to store and process sensitive information {{has made it}} necessary to secure them from intruders. A behavioral biometric such as keystroke dynamics which makes use of the typing cadence of an individual {{can be used to}} strengthen existing security techniques effectively and cheaply. Due to the ballistic (semi-autonomous) nature of the typing behavior it is difficult to impersonate, making it useful as a biometric. Therefore in this paper, we provide a basic background of the psychological basis behind the use of keystroke dynamics. We also discuss the data acquisition methods, approaches and the performance of the methods used by researchers on standard computer keyboards. In this survey, we find that the use and acceptance of this biometric could be increased by devel-opment of standardized databases, assignment of nomenclature for features, development of <b>common</b> data <b>interchange</b> <b>formats,</b> establishment of protocols for evaluating methods, and resolution of privacy issues...|$|R
40|$|Current {{research}} in software reengineering offers {{a great amount}} of tools specialized on certain reengineering tasks. The definition of a powerful <b>common</b> <b>interchange</b> <b>format</b> is a key issue to provide interoperability between these tools. This paper discusses aspects of data interchange formats for exchanging reengineering related data. It proposes a graph-based format to exchange both application specific concepts and data by XML documents. Keywords: reengineering, data interchange format, tool interoperability, graph technology 1. Motivation An important topic at WCRE' 98 were talks [29, 30] and discussions on representation formats used in different reengineering toolsets. As a result of these discussions a general and powerful format allowing the exchange of reengineering data between different toolsets was required. This paper aims at motivating and presenting an approach to such an interchange format in order to continue the discussions from WCRE' 98. Current activities in softwa [...] ...|$|E
40|$|The {{continuously}} increasing {{complexity of}} computer animations makes {{it necessary to}} rely on the knowledge of various experts to cover the different areas of computer graphics and animation. This fact, which can be noted in many areas of scientific working, leads to increasing effort being put into research concerning cooperative working over the internet. However, it still requires substantialeffort and time to combine different animation techniques in a common virtual environment. When trying to perform collaborative animation over a network, we often face the problem of having to combine animation systems and applications based on different software and hardware and using incompatible data structures. We present an approach, based on a client-server architecture and employing a VRML-based language as <b>common</b> <b>interchange</b> <b>format,</b> that allows inhomogeneous systems to be easily incorporated into a collaborative animation. The applications can be freed from employing plug-ins or libraries [...] ...|$|E
40|$|Reverse {{engineering}} systems hold {{great promise}} in aiding developers regain control over long-lived software projects whose architecture {{has been allowed}} to "drift". However, {{it is well known that}} these systems have relative strengths and weaknesses, and to date relatively little work has been done on integrating various subtools within other reverse engineering systems. The design of a <b>common</b> <b>interchange</b> <b>format</b> for data used by reverse engineering tools is therefore of critical importance. In this position paper, we describe some of our previous work with TAXFORM (Tuple Attribute eXchange FORMat) [2, 6], and in integrating various "fact extractors" into the PBS reverse engineering system. For example, we have recently created translation mechanisms that enable the Acacia system's C and C++ extractors to be used within PBS, and we have used these mechanisms to create software architecture models of two large software systems: the Mozilla web browser (2. 2 MLOC of C++ and C) and the VIM t [...] ...|$|E
40|$|Structural health {{monitoring}} systems {{on the one}} side and solutions for building management and automation, including safety and security, on the other side are currently treated as separate systems. Both systems rely on input from sensor data while measuring completely different physical parameters. In this paper, a novel approach for sensor data management in building applications that combines structural {{health monitoring}} with safety and security applications is presented. This approach is called a unified data management system and enables the exchange of sensor data between all participating subsystems. For example, a system for dynamic escape route planning, besides using smoke detection data could benefit significantly from complementary sensor data from a structural health monitoring system. The key element of a unified data management solution is a <b>common</b> data <b>interchange</b> <b>format.</b> In this approach, existing standards and common internet technology are used where possible. This allows easy development and the use of COTS hardware. Finally, the implementation of this system in hardware and software is described...|$|R
40|$|Currently the {{landscape}} of software ecosystem modelling methods and languages is like Babel {{after the fall of}} the tower: there are many methods and languages available and interchanging data between researchers and organizations that actively govern their ecosystem, is practically impossible. The lack of a universally accepted set of modelling methods is hampering the advancement of software ecosystems research. Using a literature study and a set of interviews amongst peers, we aim to establish a set of understandings and requirements for a universally accepted set of software ecosystem modelling methods. The work is an initial push in a larger research initiative that has the goal of advancing the maturity of (software) ecosystems modelling. The success of such an initiative will be found in the availability of <b>common</b> databases, better <b>interchange</b> <b>formats</b> between researchers, and more capable software ecosystem modelling tools...|$|R
40|$|The {{development}} {{of the ability to}} incorporate pictures into screen displays may well have been the main factor in making the World Wide Web what is commonly described as the Internet’s ‘killer application’. While the production of computer graphic images can require specialised skills and equipment, much can be achieved with basic computer literacy and standard hardware. Some fundamental graphic art software operations are described, including conversion to the most <b>common</b> Internet graphics <b>interchange</b> <b>format</b> (GIF) and Joint Photographic Experts Group (JPG or JPEG) file formats. Public domain sources of graphics, icons and clip-art are also noted. Hypertext mark-up language (HTML) features related to image display are discussed. Some more advanced uses of Web graphics include click-able image-maps, and animated images. An emerging portable network graphics (PNG) standard seems likely to replace some existing file formats, for both technical and legal reasons...|$|R
40|$|The {{purpose of}} this thesis is to {{describe}} what Building Information Modeling, BIM, could mean for landscape architects, and also hopefully {{to make sense of}} the possibilities and problems of the system. BIM is about virtual design and is based on all consultancy groups in the design team modeling in 3 D. Intelligent objects and metadata, such as price and material, are used. The project group has a <b>common</b> <b>interchange</b> <b>format</b> for simple sharing of information. To get an idea of how the situation with BIM for landscape architects looks like today, I collected information mostly through interviews. Landscape architects, clients and contractors were interviewed. Today there is some uncertainty in the industry around which BIM software to use as a landscape architect and there are no obvious choices. To provide an overview of the software situation, I made a compilation of the software mentioned by my respondents. The discussion revolves around how the work process of landscape architects is affected when working in 3 D and BIM, and how that process can be improved...|$|E
40|$|International audienceThe {{continuously}} increasing {{complexity of}} computer animationsmakes {{it necessary to}} rely on the knowledge of various experts to cover the different areas of computer graphics and animation. This fact, which can be noted in many areas of scientific working, leads to increasing effort being put into research concerning cooperative working over the internet. However, it still requires substantialeffort and time to combine different animation techniques in a common virtual environment. When trying to perform collaborative animation over a network, we often face the problem of having to combine animation systems and applications based on different software and hardware and using incompatible data structures. We present an approach, based on a client-server architecture and employing a VRML-based language as <b>common</b> <b>interchange</b> <b>format,</b> that allows inhomogeneous systems to be easily incorporated into a collaborative animation. The applications can be freed from employing plug-ins or libraries to link into a common animation platform; they keep a local copy of the global scene and only need the ability to export the internal data representation into the so called ”PaVRML” language, the language we use use to exchange data and synchronize the clients. This approach does not only allow a number of practitioners to share their know-how within a common animation without requiring the huge amount of work necessary to port their application to a common platform. It also makes it often possible in the first place to combine the capabilities of different animation systems into a single complex animation. Additionally, we investigate solutions to optimize the network load for real-time applications. In this paper we present preliminary results and discuss the future developments of this ongoing work...|$|E
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (2000). All Rights Reserved. This document describes a file format suitable for describing directory information or modifications made to directory information. The file format, known as LDIF, for LDAP Data Interchange Format, is typically used to import and export directory information between LDAP-based directory servers, or to describe a set of changes which are {{to be applied to}} a directory. Background and Intended Usage There are a number of situations where a <b>common</b> <b>interchange</b> <b>format</b> is desirable. For example, one might wish to export a copy of the contents of a directory server to a file, move that file to a different machine, and import the contents into a second directory server. Additionally, by using a well-defined interchange format, development of data import tools from legacy systems is facilitated. A fairly simple set of tools written in awk or perl can, for example, convert a database of personnel information into an LDIF file. This file can then be imported into a directory server, regardless of the internal database representation the target directory server uses. The LDIF format was originally developed and used in the University of Michigan LDAP implementation. The first use of LDIF was in describing directory entries. Later, the format was expanded to allow representation of changes to directory entries. Good Standards Track [Page 1] RFC 2849 LDAP Data Interchange Format June 2000 Relationship to the application/directory MIME content-type: The application/directory MIME content-type [1] is a general framework and format for conveying directory information, and is independent of any particular directory service. The LDIF format is a simpler format which is perhaps easier to create, and may also be used, as noted, to describe a set of changes to be applied to a directory...|$|E
40|$|In audio–graphic scenes, {{visual and}} audio {{modalities}} are synchronized {{in time and}} space, and their behaviour is determined by a common process. We present here a novel way of modeling audio– graphic content for interactive 3 D scenes {{with the concept of}} sound processes and their activation through 2 D or 3 D profiles. Many 3 D applications today support both graphical and audio effects to provide a more realistic user experience; however a <b>common</b> model and <b>interchange</b> <b>format</b> for interactive audio–graphic scenes is still lacking. X 3 D {{is one of the most}} promising formats for 3 D scene representation. It is extensible and supports simple spatial audio representation and almost all basic and advanced 3 D computer graphics techniques. We therefore propose an extension of the X 3 D standard to represent the sound process and activation profile model for providing a rich audio–graphic description in X 3 D...|$|R
40|$|Abstract: This paper {{addresses}} {{heterogeneity of}} business process metamodels and related <b>interchange</b> <b>formats.</b> The different approaches towards <b>interchange</b> <b>format</b> design {{and effects of}} <b>interchange</b> <b>format</b> specification are presented first. In particular completeness is identified as an important design criterion for <b>interchange</b> <b>formats.</b> Afterwards the superset of metamodel concepts is extracted from 15 currently available XML-based specifications for business process modelling. Furthermore, these concepts are used {{as a framework for}} comparing the completeness of 15 specifications. ...|$|R
40|$|This paper {{addresses}} {{heterogeneity of}} business process metamodels and related <b>interchange</b> <b>formats.</b> We present different approaches towards <b>interchange</b> <b>format</b> design {{and effects of}} <b>interchange</b> <b>format</b> specification first. Moreover, we derive the superset of metamodel concepts from 15 currently available XML-based specifications for business process modelling. These concepts are used {{as a framework for}} comparing the 15 specifications. 1...|$|R
