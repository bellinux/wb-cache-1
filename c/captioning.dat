1164|10000|Public
5|$|On 27 January 2014, {{along with}} the Nine Network, NBN {{switched}} from the Supertext logo to Nine's Closed <b>Captioning</b> logo.|$|E
5|$|The rental {{release of}} the film to Netflix, Blockbuster, and Redbox was {{controversial}} since it failed to include closed <b>captioning.</b> Disney faced a consumer backlash over this and quickly released a statement that this removal was an unfortunate error {{and that it was}} moving to correct the issue.|$|E
5|$|In {{the scene}} where Tracy is on his bacon run, every mention of January 17th is either shot so the actor's mouth is hidden or dubbed over. This can be seen most clearly when Cerie says January 17th. It seems the {{original}} was February 17th but later changed to match the release date of the episode. Closed <b>captioning</b> still shows February 17th.|$|E
40|$|<b>Captions</b> in videos provide rich {{semantic}} {{information the}} contents of the video can be clearly understand with help of the <b>captions.</b> Without even seeing the video the person can understand about the video by knowing the <b>captions</b> in the video. By considering these reasons <b>captions</b> extraction in videos become an important prerequisite. In this paper we use a stroke filter, the stroke filter identifies the strokes in the video and usually <b>caption</b> regions have strokes such that the strokes identified belongs to <b>captions</b> by which the <b>captions</b> are detected. Then the locations of the <b>captions</b> are localized. In the next Step the <b>caption</b> pixels are separated from the background pixels. At last a post processing step is performed to check whether the correct <b>caption</b> pixels are extracted. In this paper a step by step sequential procedure to extract <b>captions</b> from videos is proposed. Keywords-stroke filter, <b>caption</b> detection, <b>caption</b> extraction,caption pixels I...|$|R
50|$|As <b>caption</b> {{writing is}} an art of words, it is rarely {{possible}} to evaluate the superiority of one <b>caption</b> over others accurately and objectively. Consequently, the judging of submitted <b>captions</b> is usually quite subjective. Sometimes competition rules limit <b>caption</b> size, but usually the <b>caption</b> is no longer than one sentence (10 - 20 words). As an exception, the <b>caption</b> can consist of several sentences. Usually it {{is not allowed to}} use profane or offensive language in submitted <b>captions.</b>|$|R
40|$|The ccaption package enables {{restyling}} of <b>captions</b> {{and provides}} for ‘continuation’ <b>captions,</b> unnumbered <b>captions,</b> bilingual <b>captions,</b> and an ‘anonymous’ <b>caption</b> (a legend) {{that can be}} used in any environment. It also provides commands to define <b>captions</b> {{that can be used}} outside float environments as well as a mechanism for creating new types of float environments and subfloats. The package has been tested in conjunction with the tocloft rotating, <b>caption</b> 2, sidecap, subfigure, endfloat and hyperref packages...|$|R
5|$|The Search for Spock was {{released}} on home video in 1985. The initial retail offerings included VHS, Betamax, LaserDisc and CED formats with closed <b>captioning.</b> As {{part of a plan}} to support its push of 8mm video cassette, Sony partnered with Paramount Home Video to bring titles like The Search for Spock to the platform in 1986.|$|E
5|$|AMI-tv is a Canadian, English-language, {{digital cable}} {{specialty}} channel {{owned by the}} non-profit organization Accessible Media. AMI-tv broadcasts a selection of general entertainment programming with accommodations {{for those who are}} visually or hearing impaired, with audio descriptions on the primary audio track and closed <b>captioning</b> available across all programming. Along with acquired content, AMI-tv also broadcasts original series on accessibility- and disability-related topics, and has occasionally broadcast simulcasts of news and sporting events in its open described video format—including, since 2012, the Paralympic Games, an offshoot of the Olympic Games for athletes with disabilities.|$|E
5|$|R. Rathnavelu {{was hired}} as the cinematographer after Ravi K. Chandran, Nirav Shah and Thiru were considered. Anthony was the film's editor. Yuen Woo Ping, known {{for his work in}} The Matrix trilogy and the Kill Bill films, was {{selected}} to be the stunt co-ordinator, while Legacy Effects, a visual effects studio based in the United States, were in charge of the prosthetic make-up and animatronics in the film. Munich-based film technical company, Panther, were responsible for the crane shots. The film's subtitle <b>captioning</b> was done by Rekha Haricharan.|$|E
50|$|Their current staff {{includes}} James Sparling, Percussion <b>caption</b> head, Noah Bellamy, visual <b>caption</b> head, John Hartwick, brass <b>caption</b> head, Lucas Schmidt, Color Guard <b>caption</b> head, Kevin J, Mike Wells, Assistant Director.|$|R
50|$|Along the {{traditional}} <b>caption</b> contests, there have appeared {{different forms of}} this genre such as anti-caption contests for worst submitted <b>caption</b> and reverse <b>caption</b> contests where the most suitable picture must be submitted for a given <b>caption.</b> There are also <b>caption</b> contests devoted to particular themes such as history or football.|$|R
40|$|Table 1 : 46 words (6 lines × 7. 6 words/line) Figure 1 : 338 words (double column, 18 -word <b>caption)</b> Figure 2 : 344 words (double column, 24 -word <b>caption)</b> Figure 3 : 340 words (double column, 20 -word <b>caption)</b> Figure 4 : 170 words (single column, 10 -word <b>caption)</b> Figure 5 : 173 words (single column, 13 -word <b>caption)</b> Figure 6 : 171 words (single column, 11 -word <b>caption...</b>|$|R
5|$|The {{technology}} company Akamai reported that 5,401,250web users logged on news sites {{in less than}} one minute, the fifth highest peak among news websites since the company started tracking data in 2005. During at-peak usage, news websites served seven million simultaneous video streams, which was {{the highest number of}} simultaneous video streams in Akamai's history. The Obama inaugural ceremony not only achieved the highest Internet viewership for a U.S.presidential inauguration, the inaugural event was the first to feature a live audio description of a swearing-in ceremony and the first to include closed <b>captioning</b> in the live webcast of the event.|$|E
5|$|National {{legislation}} {{requires that}} all television programmes broadcasts in Croatia {{are made in}} Croatian language or with appropriate translations either using dubbing or subtitling. In general, all foreign programming is subtitled, except for cartoons and narrated parts of documentaries and similar programmes. An attempt to change this was made by Nova TV in 2006, when a soap opera was dubbed, but the move provoked negative response from viewers and critics, causing the experiment to be abandoned. The legislation does not provide for mutually intelligible languages. That led to formal requests made by the Electronic Media Council demanding language localisation of television programmes made in Serbian language. Ultimately, that issue was resolved through subtitling using teletext service normally used for closed <b>captioning.</b>|$|E
5|$|Nikolas Tomas Stauskas (born October 7, 1993) is a Canadian {{professional}} basketball player for the Philadelphia 76ers of the National Basketball Association (NBA). A native of Mississauga, Ontario, Stauskas played two seasons of National Collegiate Athletic Association (NCAA) {{competition for the}} Michigan Wolverines ending with the 2013–14 team before declaring for the NBA draft. Stauskas was drafted eighth overall in the 2014 NBA draft by the Sacramento Kings, for which he began his NBA career. Towards {{the end of his}} rookie season, Stauskas was tagged with the nickname Sauce Castillo after a closed <b>captioning</b> error resulted in a social media meme. Stauskas, whose family is of Lithuanian heritage, {{is a member of the}} Canadian national basketball team.|$|E
50|$|Web-based <b>captioned</b> {{telephone}} enables {{telephone calls}} {{to be placed}} with <b>captions,</b> by utilizing the World Wide Web browser window of a computer or smart phone. It {{is similar to a}} traditional <b>captioned</b> phone call except the user's own telephone equipment is used, whilst the <b>captions</b> are viewed online instead of in the <b>captioned</b> telephone display screen.|$|R
30|$|In the {{original}} publication, part of Fig.  3 <b>caption</b> has been incorrectly added to Fig.  4 <b>caption.</b> The corrected figures <b>captions</b> are given here.|$|R
40|$|Humor is an {{integral}} aspect of the human experience. Mo-tivated {{by the prospect of}} creating computational models of humor, we study the influence of the language of cartoon <b>captions</b> on the perceived humorousness of the cartoons. Our studies are based on a large corpus of crowdsourced cartoon <b>captions</b> that were submitted to a contest hosted by the New Yorker. Having access to thousands of cap-tions submitted for the same image allows us to analyze the breadth of responses of people to the same visual stimulus. We first describe how we acquire judgments about the humorousness of different <b>captions.</b> Then, we detail the construction of a corpus where <b>captions</b> deemed funnier are paired with less-funny <b>captions</b> for the same cartoon. We analyze the <b>caption</b> pairs and find significant differences be-tween the funnier and less-funny <b>captions.</b> Next, we build a classifier to identify funnier <b>captions</b> automatically. Given two <b>captions</b> and a cartoon, our classifier picks the funnier one 69 % of the time for <b>captions</b> hinging on the same joke, and 64 % of the time for any pair of <b>captions.</b> Finally, we use the classifier to find the best <b>captions</b> and study how its predictions could be used to significantly reduce the load on the cartoon contest’s judges...|$|R
5|$|Since the 1990s the IMA has {{continually}} improved accessibility for visitors; {{the initiative}} was {{a contributing factor}} to the museum receiving the National Medal for Museum and Library Service in 2009. The IMA provides <b>captioning</b> on videos produced by the museum, large print binders for exhibits, accessible seating and sign language interpretation in Tobias Theater, and wheelchair-accessible trails in 100 Acres. The museum also maintains partnerships with the Indiana School for the Deaf and the Indiana School for the Blind. In 1993 the IMA opened the Garden for Everyone, a wheelchair-accessible garden designed to emphasize multiple senses. The garden includes varieties of fragrant and textured plants {{as well as a}} number of sculptures, including La Hermana del Hombre Boveda by Pablo Serrano.|$|E
5|$|The HDMI {{standard}} {{was not designed}} to pass closed caption data (for example, subtitles) to the television for decoding. As such, any closed caption stream must be decoded and included as an image in the video stream(s) prior to transmission over an HDMI cable to be viewed on the DTV. This limits the caption style (even for digital captions) to only that decoded at the source prior to HDMI transmission. This also prevents closed captions when transmission over HDMI is required for upconversion. For example, a DVD player that sends an upscaled 720p/1080i format via HDMI to an HDTV has no way to pass Closed <b>Captioning</b> data so that the HDTV can decode it, {{as there is no}} line 21 VBI in that format.|$|E
5|$|All YouTube {{users can}} upload videos up to 15 minutes each in duration. Users {{who have a}} good track record of {{complying}} with the site's Community Guidelines may be offered the ability to upload videos up to 12 hours in length, as well as live streams, which requires verifying the account, normally through a mobile phone. When YouTube was launched in 2005, {{it was possible to}} upload longer videos, but a ten-minute limit was introduced in March 2006 after YouTube found that the majority of videos exceeding this length were unauthorized uploads of television shows and films. The 10-minute limit was increased to 15 minutes in July 2010. In the past, it was possible to upload videos longer than 12 hours. Videos can be at most 128 GB in size. Video captions are made using speech recognition technology when uploaded. Such <b>captioning</b> is usually not perfectly accurate, so YouTube provides several options for manually entering the captions for greater accuracy.|$|E
40|$|A {{series of}} 24 short, 30 -second video {{segments}} <b>captioned</b> at different speeds {{were shown to}} 578 people. The subjects used a five-point scale (Too Fast, Fast, OK, Slow, Too Slow) to make an assessment of each segment’s <b>caption</b> speed. The “OK ” speed, defined as {{the speed at which}} “Caption speed is comfortable to me, ” was found to be about 145 words per minute (wpm). Most subjects did not seem to have significant trouble with the <b>captions</b> until the rate was at least 170 wpm. People who could hear wanted slightly slower <b>captions.</b> However, this seemed to relate to how often people watched <b>captioned</b> television. Frequent viewers were comfortable with somewhat faster <b>captions.</b> Age and sex were not related to the <b>caption</b> speeds people were comfortable with. Education had no relation to <b>caption</b> speed except that people who had attended graduate school might prefer slightly faster <b>captions...</b>|$|R
25|$|A <b>captioned</b> {{telephone}} is {{a telephone}} that displays real-time <b>captions</b> {{of the current}} conversation. The <b>captions</b> are typically displayed on a screen embedded into the telephone base.|$|R
50|$|Once the {{timeslot}} for <b>caption</b> {{writing is}} complete, a voting round ensues, where players pick their favourite {{from the list}} of <b>captions</b> displayed. Players cannot vote for their own <b>caption!</b> Also, if a player does not vote within the specified countdown time limit, they will score no points for this round. This is to stop people cheating by writing <b>captions</b> and then not voting for the <b>captions</b> of others.|$|R
25|$|Some stadiums utilize on-site captioners {{while others}} {{outsource}} them to external providers who caption remotely. A prominent provider of in-arena <b>captioning</b> systems is Good Sport <b>Captioning,</b> founded by Patti White of St. Louis. White {{had worked as}} a stenographer at a courthouse near where Busch Stadium was being constructed and reached a deal with the team to provide in-stadium <b>captioning</b> upon the stadium's 2006 opening-conducting her activity from her home. Patti later formed Good Sport <b>Captioning</b> to provide remote <b>captioning</b> for other teams and venues.|$|E
25|$|Real-time <b>captioning,</b> {{a process}} for <b>captioning</b> live broadcasts, was {{developed}} by the National <b>Captioning</b> Institute in 1982. In real-time <b>captioning,</b> court reporters trained to write at speeds of over 225 words per minute give viewers instantaneous access to live news, sports, and entertainment. As a result, the viewer sees the captions within two to three seconds of the words being spoken.|$|E
25|$|In 2010, Vegas Pro, the {{professional}} non-linear editor, was updated to support importing, editing, and delivering CEA-608 closed captions. Vegas Pro 10, released on October 11, 2010, added several enhancements to the closed <b>captioning</b> support. TV-like CEA-608 closed <b>captioning</b> {{can now be}} displayed as an overlay when played back in the Preview and Trimmer windows, {{making it easy to}} check placement, edits, and timing of CC information. CEA708 style Closed <b>Captioning</b> is automatically created when the CEA-608 data is created. Line 21 closed <b>captioning</b> is now supported, as well as HD-SDI closed <b>captioning</b> capture and print from AJA and Blackmagic Design cards. Line 21 support provides a workflow for existing legacy media. Other improvements include increased support for multiple closed <b>captioning</b> file types, as well as the ability to export closed caption data for DVD Architect, YouTube, RealPlayer, QuickTime, and Windows Media Player.|$|E
40|$|The <b>caption</b> package {{consists}} of two parts – the kernel (<b>caption</b> 3. sty) and the main package (<b>caption.</b> sty). The kernel provides all the user commands and internal macros which are necessary for typesetting <b>captions</b> and setting parameters regarding these. While the standard LATEX document classes provide an internal command called makecaption and no options to control its behavior (except the vertical skips {{above and below the}} <b>caption</b> itself), we provide similar commands called @make and @@make, but with a lot of options which can be selected with. Loading the kernel part do not change the output of a LATEX document – it just provides functionality which can be used by LATEX 2 ε packages which typesets <b>captions,</b> for example the <b>caption</b> and subfig packages. The <b>caption</b> package redefines the LATEX commands...|$|R
50|$|<b>Caption</b> That is {{an online}} {{multiplayer}} <b>caption</b> writing game where players write <b>captions</b> {{for a given}} image and then vote for their favourite from the list others have written.|$|R
40|$|<b>Captions</b> include {{information}} which {{relates to the}} images. In order to obtain {{the information in the}} <b>captions,</b> text extraction methods from images have been developed. However, most existing methods can be applied to <b>captions</b> with a fixed height or stroke width using fixed pixel-size or block-size operators which are derived from morphological supposition. We propose an edge connected components based method that can extract Korean <b>captions</b> that are composed of various sizes and fonts. We analyze the properties of edge connected components embedding <b>captions</b> and build a decision tree which discriminates edge connected components which include <b>captions</b> from ones which do not. The images for the experiment are collected from broadcast programs such as documentaries and news programs which include <b>captions</b> with various heights and fonts. We evaluate our proposed method by comparing the performance of the latent <b>caption</b> area extraction. The experiment shows that the proposed method can efficiently extract various sizes of Korean <b>captions...</b>|$|R
25|$|In 2004, retired Canadian Senator Jean-Robert Gauthier, a hard-of-hearing person, filed a {{complaint}} with the Canadian Human Rights Commission against Radio-Canada concerning <b>captioning,</b> particularly the absence of real-time <b>captioning</b> on newscasts and other live programming. As part of the settlement process, Radio-Canada agreed to submit a report on the state of <b>captioning,</b> especially real-time <b>captioning,</b> on Radio-Canada and RDI. The report, which was the subject of some criticism, proposed an arrangement with Cité Collégiale, a college in Ottawa, to train more French-language real-time captioners.|$|E
25|$|Offline <b>captioning</b> {{involves}} a five-step design and editing process, and does {{much more than}} simply display the text of a program. Offline <b>captioning</b> helps the viewer follow a story line, become aware of mood and feeling, and allows them to fully enjoy the entire viewing experience. Offline <b>captioning</b> is the preferred presentation style for entertainment-type programming.|$|E
25|$|The {{government}} of Australia provided seed funding in 1981 {{for the establishment}} of the Australian Caption Centre (ACC) and the purchase of equipment. <b>Captioning</b> by the ACC commenced in 1982 and a further grant from the Australian government enabled the ACC to achieve and maintain financial self-sufficiency. The ACC, now known as Media Access Australia, sold its commercial <b>captioning</b> division to Red Bee Media in December 2005. Red Bee Media continues to provide <b>captioning</b> services in Australia today.|$|E
25|$|Some {{programs}} may be prepared {{in their entirety}} several hours before broadcast, but with insufficient time to prepare a timecoded <b>caption</b> file for automatic play-out. Pre-prepared <b>captions</b> look similar to offline <b>captions,</b> although the accuracy of cueing may be compromised slightly as the <b>captions</b> are not locked to program timecode.|$|R
40|$|Title from NCLC <b>caption</b> card. Attribution to Hine {{based on}} provenance. In album: Street trades. Hine no. 3927. Title derived from similar photograph, Hine no. 3921. <b>Caption</b> on <b>caption</b> card: "Exhib. panel. "No {{location}} or date recorded on <b>caption</b> card; date estimate based on cards for photos with neighboring numbers...|$|R
50|$|Some {{programs}} may be prepared {{in their entirety}} several hours before broadcast, but with insufficient time to prepare a timecoded <b>caption</b> file for automatic play-out. Pre-prepared <b>captions</b> look similar to offline <b>captions,</b> although the accuracy of cueing may be compromised slightly as the <b>captions</b> are not locked to program timecode.|$|R
