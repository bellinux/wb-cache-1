172|481|Public
25|$|Tadeo is a {{communication}} platform for use between hearing, deaf and hard-of-hearing persons {{in a professional}} environment; it offers a French Sign Language interpretation service, in <b>Cued</b> <b>Speech,</b> and live Transcription, remotely and real-time.|$|E
2500|$|R. Orin Cornett– American physicist, {{university}} professor, and administrator. [...] Dr. Cornett was {{the inventor}} of a literacy system for the deaf, known as <b>Cued</b> <b>Speech.</b>|$|E
2500|$|An {{individual}} who communicates by American Sign Language, or another mode of manual communication, such as Signing Exact English, contact signing (Pidgin Signed English), <b>Cued</b> <b>Speech,</b> or Linguistics of Visual English, uses a videophone or other video device, {{such as a}} webcam, to connect via broadband Internet to a Video Relay Service; ...|$|E
40|$|The {{purpose of}} this study was to {{identify}} the achievement in Malay Language among hearing-impaired students who uses <b>cue</b> <b>speech</b> as communication tools. Observations and interviews were used throughout the research. Samples involved in this study consisted of 6 students who have hearing impairment that used <b>cue</b> <b>speech</b> as a tools of communication, 4 parents of students either mother or father, a teacher who teach Malay Language and 2 teachers who teach other than Malay Language such as Mathematics and Music. Besides that, the headmaster of the school was also interviewed. All six students who were involved in this study were students from primary school. Two students were from standard four, two students were from standard five and two students were from standard 6. All of these students were randomly chosen with the purpose to view their achievement as a whole. Suggestions for future researches were also proposed to increase the level of language achievement for students with hearing-impairment that uses <b>cue</b> <b>speech</b> as communication tools...|$|R
40|$|Two {{experiments}} {{were conducted to}} compare the effectiveness of teaching rhythm reading using a traditional approach versus a simplified <b>speech</b> <b>cue</b> method. Results of the first experiment indicated {{the superiority of the}} <b>speech</b> <b>cue</b> method. Experiment 2 was conducted to determine whether the <b>speech</b> <b>cue</b> method could be used effectively with minimal teacher training. A 23 -item rhythm-reading test was administered to 107 third graders before and after rhythm instruction by each method. In both experiments, the <b>speech</b> <b>cue</b> group made significantly greater gains than the traditional group. The difference between pretest and posttest scores was significant for both treatment groups. The findings have general application for rhythm-reading instruction using the <b>speech</b> <b>cue</b> method for early training...|$|R
50|$|Replace stimuli: for example, {{subtitles}} {{or closed}} captioning, audio <b>cues,</b> sonification, <b>speech</b> synthesis or haptic cues.|$|R
2500|$|R. Orin Cornett (1913 – 2002), physicist, {{was born}} in Driftwood. He earned a {{doctorate}} of physics and applied mathematics from the University of Texas in 1940, and invented the communication system for the hearing impaired known as <b>Cued</b> <b>Speech.</b> [...] He taught at Oklahoma Baptist University, Penn State, and Harvard University. [...] He {{also served as a}} vice president at Oklahoma Baptist and as the Vice President of Long Range Planning for Gallaudet University.|$|E
50|$|Though to {{a hearing}} person, <b>Cued</b> <b>Speech</b> may look similar to signing, <b>Cued</b> <b>Speech</b> {{is not a}} sign language; {{nor is it a}} Manually Coded Sign System for a spoken language. Rather, <b>Cued</b> <b>Speech</b> is a manual {{modality}} of communication for representing any language at the phonological level (phonetics).|$|E
50|$|<b>Cued</b> <b>Speech</b> uses lipreading with {{accompanying}} hand {{shapes that}} disambiguate the visemic (consonant) lipshape. <b>Cued</b> <b>speech</b> {{is said to}} be easier for hearing parents to learn than a sign language, and studies, primarily from Belgium, show that a deaf child exposed to <b>cued</b> <b>speech</b> in infancy can make more efficient progress in learning a spoken language than from lipreading alone. The use of <b>cued</b> <b>speech</b> in cochlear implantation for deafness is likely to be positive. A similar approach, involving the use of handshapes accompanying seen speech, is Visual Phonics, which is used by some educators to support the learning of written and spoken language.|$|E
25|$|The {{process that}} allows infants to use {{prosodic}} <b>cues</b> in <b>speech</b> input {{to learn about}} language structure has been termed “prosodic bootstrapping”.|$|R
40|$|Gestures {{and visible}} <b>speech</b> <b>cues</b> are often {{available}} to listeners to aid their {{comprehension of the}} speaker’s meaning. However, visual communication cues are not always beneficial over-and-above the audible <b>speech</b> <b>cues.</b> My goal is to outline several types of constraints which operate in the human cognitive processing system that bear on this question: When do visual language <b>cues</b> (visible <b>speech</b> and gestures) provide an aid to comprehension, and when do they not? Research on visual-spoken language comprehension carried out in my lab over recent years is described and recommendations will be made concerning the design of multi-modal interfaces...|$|R
40|$|This paper {{describes}} an automatically annotated multimodal corpus of multi-party meetings. The corpus provides {{for each subject}} involved in the experimental sessions information on her/his social behavior and personality traits, as well as audiovisual <b>cues</b> (<b>speech</b> rate, pitch and energy, head orientation, head, hand and body fidgeting). The corpus {{is based on the}} audio and video recordings of thirteen sessions, which took place in a lab setting equipped with cameras and microphones. Our main concern in collecting this corpus was to investigate the possibility of creating a system capable of automatically analyzing social behaviors and predicting personality traits using audio-visual cues. Categories and Subject Descriptor...|$|R
5000|$|Since <b>cued</b> <b>speech</b> {{is based}} on making sounds visible to the hearing impaired, <b>cued</b> <b>speech</b> {{is not limited to}} use in English {{speaking}} nations. Because of the demand for use in other languages/countries, by 1994 Cornett had adapted cueing to 25 other languages and dialects. [...] Originally designed to represent American English, the system was adapted to French in 1977. , <b>Cued</b> <b>Speech</b> has been adapted to approximately 60 languages and dialects, including six dialects of English. For tonal languages such as Thai, the tone is indicated by inclination and movement of the hand. For English, <b>Cued</b> <b>Speech</b> uses eight different hand shapes and four different positions around the mouth.|$|E
5000|$|The {{editor of}} the <b>Cued</b> <b>Speech</b> Journal reports that [...] "Research {{indicating}} that <b>Cued</b> <b>Speech</b> does greatly improve the reception of spoken language by profoundly deaf children was reported in 1979 by Gaye Nicholls, and in 1982 by Nicholls and Ling." ...|$|E
5000|$|During his career, Cornett {{wrote and}} {{published}} hundreds of articles {{as well as}} several books on mathematics, physics, higher education, deaf education, <b>Cued</b> <b>Speech</b> and other subjects. He also served as editor of several publications, including the parental guidebook <b>Cued</b> <b>Speech</b> Resource Book for Parents of Deaf Children (...) [...]|$|E
40|$|This paper {{discusses}} issues arising when {{applying the}} IBM Audio-Indexing System to retrieval of video. Issues discussed include {{the relationship between}} speech transcription accuracy and retrieval performance, query processing schemes and the critical problem of mapping between <b>cues</b> in <b>speech</b> and the relevant video shots. The temporal relationship between the occurrence of <b>cues</b> in <b>speech</b> transcripts and relevant shots is quantified and then simple schemes for performing this mapping are described and evaluated. Experiments demonstrate the promise of more sophisticated schemes involving up-front video ranking and one possible implementation is discussed. Techniques are evaluated using the TREC- 2002 Video Track queries and corpus, comprising a total of 68. 45 hours of video. 1...|$|R
40|$|A {{cochlear}} implant vocoder {{was used to}} evaluate relative contributions of spectral and binaural temporal fine-structure <b>cues</b> to <b>speech</b> intelligibility. In Study I, stimuli were vocoded, and then convolved through head related transfer functions (HRTFs) to remove speech temporal fine structure but preserve the binaural temporal fine-structure cues. In Study II, the order of processing was reversed to remove both speech and binaural temporal fine-structure <b>cues.</b> <b>Speech</b> reception thresholds (SRTs) were measured adaptively in quiet, and with interfering speech, for unprocessed and vocoded speech (16, 8, and 4 frequency bands), under binaural or monaural (right-ear) conditions. Under binaural conditions, as the number of bands decreased, SRTs increased. With decreasing number of frequency bands, greater benefit from spatial separation of target and interferer was observed, especially in the 8 -band condition. The present results demonstrate a strong role of the binaural cues in spectrally degraded speech, when the target and interfering speech {{are more likely to be}} confused. The nearly normal binaural benefits under present simulation conditions and the lack of order of processing effect further suggest that preservation of binaural cues is likely to improve performance in bilaterally implanted recipients...|$|R
2500|$|The {{speech sound}} signal {{contains}} {{a number of}} acoustic cues {{that are used in}} <b>speech</b> perception. The <b>cues</b> differentiate <b>speech</b> sounds belonging to different phonetic categories. For example, one of the most studied <b>cues</b> in <b>speech</b> is voice onset time or VOT. VOT is a primary cue signaling the difference between voiced and voiceless plosives, such as [...] "b" [...] and [...] "p". Other cues differentiate sounds that are produced at different places of articulation or manners of articulation. The speech system must also combine these cues to determine the category of a specific speech sound. This is often thought of in terms of abstract representations of phonemes. These representations can then be combined for use in word recognition and other language processes.|$|R
50|$|Within the United States, {{proponents of}} <b>Cued</b> <b>Speech</b> often discuss {{the system as}} an {{alternative}} to ASL and similar sign languages, although others note that it can be learned in addition to such languages. For the ASL using community, <b>Cued</b> <b>Speech</b> is a unique potential component for learning English as a second language. Within Bilingual-Bicultural models, <b>Cued</b> <b>Speech</b> does not borrow or invent signs from ASL, nor does CS attempt to change ASL syntax or grammar. Rather, CS provides an unambiguous model for language learning that leaves ASL intact.|$|E
5000|$|From 1975 to 1984, {{he became}} Research Professor and {{director}} of <b>Cued</b> <b>Speech</b> Programs. From 1981 to 1983, he also served as Gallaudet's Chairman of the Center for Studies in Language and Communication. [...] During Cornett's tenure there, he adapted his <b>Cued</b> <b>Speech</b> system to 52 languages and major dialects, while writing and publishing audiocassette lessons in 34 of those languages and dialects. Upon his retirement in 1984, Gallaudet University awarded him the status of Professor emeritus. [...] He continued working with the international <b>Cued</b> <b>Speech</b> community from his Maryland home after his retirement.|$|E
5000|$|<b>Cued</b> <b>Speech</b> {{was invented}} in 1966 by R. Orin Cornett at Gallaudet College, Washington, D.C. After {{discovering}} that children with prelingual and profound hearing impairments typically have poor reading comprehension, he developed the system {{with the aim of}} improving the reading abilities of such children through better comprehension of the phonemes of English. At the time, some were arguing that deaf children were earning these lower marks because they had to learn two different systems: American Sign Language (ASL) for person-to-person communication and English for reading and writing. As many sounds look identical on the lips (such as [...] and [...] ), the hand signals introduce a visual contrast in place of the formerly acoustic contrast. <b>Cued</b> <b>Speech</b> may also help people hearing incomplete or distorted sound—according to the National <b>Cued</b> <b>Speech</b> Association at cuedspeech.org, [...] "cochlear implants and <b>Cued</b> <b>Speech</b> are powerful partners".|$|E
40|$|The {{ability to}} {{accurately}} perceive emotions {{is crucial for}} effective social interaction. Many questions remain regarding how different sources of emotional <b>cues</b> in <b>speech</b> (e. g., prosody, semantic information) are processed during emotional communication. Using a cross-modal emotional priming paradigm (Facial affect decision task), we compared the relative contributions of processing utterances with single-channel (prosody-only) versus multi-channel (prosody and semantic) cues on the perception of happy, sad, and angry emotional expressions. Our data show that emotional <b>speech</b> <b>cues</b> produce robust congruency effects on decisions about an emotionally related face target, although no processing advantage occurred when prime stimuli contained multi-channel as opposed to single-channel <b>speech</b> <b>cues.</b> Our data suggest that utterances with prosodic cues alone and utterances with combined prosody and semantic cues both activate knowledge that leads to emotional congruency (priming) effects, but that the convergence of these two information sources does not always heighten access to this knowledge during emotional speech processing...|$|R
40|$|Purpose: Improved speech {{recognition}} in binaurally combined acoustic–electric stimulation (otherwise known as bimodal hearing) could arise when listeners integrate <b>speech</b> <b>cues</b> from the acoustic and electric hearing. The aims {{of this study}} were (a) to identify <b>speech</b> <b>cues</b> extracted in electric hearing and residual acoustic hearing in the low-frequency region and (b) to investigate cochlear implant (CI) users' ability to integrate <b>speech</b> <b>cues</b> across frequencies. Method: Normal-hearing (NH) and CI subjects participated in consonant and vowel identification tasks. Each subject was tested in 3 listening conditions: CI alone (vocoder speech for NH), hearing aid (HA) alone (low-pass filtered speech for NH), and both. Integration ability for each subject was evaluated using a model of optimal integration—the PreLabeling integration model (Braida, 1991). Results: Only a few CI listeners demonstrated bimodal benefit for phoneme identification in quiet. <b>Speech</b> <b>cues</b> extracted from the CI and the HA were highly redundant for consonants but were complementary for vowels. CI listeners also exhibited reduced integration ability for both consonant and vowel identification compared with their NH counterparts. Conclusion: These findings suggest that reduced bimodal benefits in CI listeners are due to insufficient complementary <b>speech</b> <b>cues</b> across ears, a decrease in integration ability, or both. National Organization for Hearing ResearchNational Institute on Deafness and Other Communication Disorders (U. S.) (Grant R 03 DC 009684 - 01) National Institute on Deafness and Other Communication Disorders (U. S.) (Grant R 01 DC 007152 - 02...|$|R
40|$|In recent {{research}} efforts, {{the integration of}} visual <b>cues</b> into <b>speech</b> analysis systems has been proposed with favorable response. This paper introduces a novel approach for lip activity and visual speech detection. We argue that the large deviation and increased values {{of the number of}} pixels with low intensities that the mouth region of a speaking person demonstrates can be used as visual <b>cues</b> for detecting <b>speech.</b> We describe a statistical algorithm, based on detection theory, for the efficient characterization of speaking and silent intervals in video sequences. The proposed system has been tested into a number of video sequences with encouraging experimental results. Potential applications include speech intent detection, speaker determination and semantic video annotation. 1...|$|R
50|$|<b>Cued</b> <b>Speech</b> {{is unique}} among forms of MCE {{in that it}} does not use {{borrowed}} or invented signs in an attempt to convey English. Instead, the American version of <b>Cued</b> <b>Speech</b> uses eight hand shapes - none of which are derived from sign languages - to represent consonant phonemes, and four hand placements around the face to represent vowel phonemes. R. Orin Cornett, who developed <b>Cued</b> <b>Speech</b> in 1966 at Gallaudet University, sought to combat poor reading skills among deaf college students by providing deaf children with a solid linguistic background. <b>Cued</b> <b>Speech</b> must be combined with mouthing (associated with the speaking of a language), as the hand shape, hand placement, and information on the mouth combine as unique feature bundles to represent phonemic values. Cues are not intended to be understood without mouthing, however, many deaf native cuers are able to decipher the cues alone without the use of the mouth. Similarly they tend to be able to perform well at deciphering the information on the mouth without the use of the hand (which is commonly referred to as lip reading). <b>Cued</b> <b>Speech</b> has been adapted for languages and dialects around the world.|$|E
5000|$|<b>Cued</b> <b>Speech</b> is {{a visual}} system of {{communication}} used with and among deaf or hard-of-hearing people. It is a phonemic-based system which makes traditionally spoken languages accessible {{by using a}} small number of handshapes, known as cues (representing consonants), in different locations near the mouth (representing vowels), as a supplement to speechreading. The National <b>Cued</b> <b>Speech</b> Association defines <b>Cued</b> <b>Speech</b> as [...] "...a visual mode of communication that uses hand shapes and placements in combination with the mouth movements and speech to make the phonemes of spoken language look different from each other." [...] It adds information about the phonology of the word that is not visible on the lips. This allows people with hearing or language difficulties to visually access the fundamental properties of language. It is now used with people with a variety of language, speech, communication, and learning needs. It is different from American Sign Language (ASL), which is a separate language from English. <b>Cued</b> <b>Speech</b> is considered a communication modality, but {{can be used as a}} strategy to support auditory rehabilitation, speech articulation, and literacy development.|$|E
50|$|There is also {{a visual}} model of spoken {{language}} called <b>cued</b> <b>speech.</b> Learning to lip read is very difficult because many sounds look the same on the lips. <b>Cued</b> <b>speech</b> enables young children with hearing loss to clearly see what is being said, and learn spoken languages with normal grammar and vocabulary. It clarifies lip reading using 8 hand shapes in 4 positions and usually takes less than 20 hours to learn the entire system.|$|E
40|$|International audienceVisual speech {{information}} helps listeners perceive {{speech in}} noise. The cues underpinning this visual advantage {{appear to be}} global and distributed, and previous research hasn't succeeded in pinning down simple dimensions to explain the effect. In this study {{we focus on the}} temporal aspects of visual <b>speech</b> <b>cues.</b> In comparison to a baseline of auditory only sentences mixed with noise, we tested the effect of making available a visual speech signal that carries the rhythm of the spoken sentence, through a temporal visual mask function linked to the times of the auditory p-centers, as quantified by stressed syllable onsets. We systematically varied the relative alignment of the peaks of the maximum exposure of visual <b>speech</b> <b>cues</b> with the presumed anchors of sentence rhythm and contrasted these <b>speech</b> <b>cues</b> against an abstract visual condition, whereby the visual signal consisted of a stylised moving curve with its dynamics determined by the mask function. We found that both visual signal types provided a significant benefit to speech recognition in noise, with the <b>speech</b> <b>cues</b> providing the largest benefit. The benefit was largely independent of the amount of delay in relation to the auditory p-centers. Taken together, the results call for further inquiry into temporal dynamics of visual and auditory speech...|$|R
40|$|A {{difference}} in fundamental frequency (ΔF 0) and a {{difference in}} spatial location (ΔSL) are two cues known to provide masking releases when multiple speakers talk at once in a room. Situations were examined in which reverberation should {{have no effect on}} the mechanisms underlying the release from energetic masking produced by these two <b>cues.</b> <b>Speech</b> reception thresholds using both unpredictable target sentences and the coordinate response measure followed a similar pattern. Both ΔF 0 s and ΔSLs provided masking releases in the presence of non-speech maskers (matched in excitation pattern and temporal envelope to speech maskers) which, as intended, were robust to reverberation. Larger masking releases were obtained for speech maskers, but critically, they were affected by reverberation. The results suggest that reverberation either limits the amount of informational masking there is to begin with, or affects its release by ΔF 0 s or ΔSLs...|$|R
40|$|A {{recent study}} by Zeng et al (2005) [PNAS, 102, 2293 - 2298] {{demonstrated}} the importance of FM <b>cues</b> for auditory <b>speech</b> identification in a competing noise environment. The current speech identification study investigated this finding for both an Auditory Only (AO) and an Auditory-Visual (AV) speech in noise identification task. The results demonstrated an FM advantage (compared to AM only) for both the AO and AV speech conditions. This finding shows the need cochlear implant speech processors to encode both AM and FM information even if visual <b>speech</b> <b>cues</b> are present. 1...|$|R
5000|$|In {{her paper}} [...] "The Relationship Between Phonological Coding And Reading Achievement In Deaf Children: Is <b>Cued</b> <b>Speech</b> A Special Case?" [...] (1998), Ostrander notes, [...] "Research has {{consistently}} shown {{a link between}} lack of phonological awareness and reading disorders (Jenkins & Bowen, 1994)" [...] and discusses the research basis for teaching <b>cued</b> <b>speech</b> {{as an aid to}} phonological awareness and literacy. Ostrander concludes that further research into these areas is needed and well justified.|$|E
50|$|<b>Cued</b> <b>speech</b> {{has been}} adapted {{to more than}} fifty {{languages}} and dialects. However, {{it is not clear}} how many of them are actually in use.|$|E
50|$|A very {{different}} form of manually coded language is <b>cued</b> <b>speech,</b> {{an aid to}} lipreading which has been developed for Afrikaans, South African English, and Setswana.|$|E
25|$|The McGurk {{effect is}} not as {{pronounced}} in schizophrenic individuals as in normal individuals. However, it is not significantly different in adults. Schizophrenia slows down the development of audiovisual integration and does not allow it to reach its developmental peak. However, no degradation is observed. Schizophrenics {{are more likely to}} rely on auditory cues than visual <b>cues</b> in <b>speech</b> perception.|$|R
40|$|In many {{practical}} situations, {{a desirable}} user interface {{to a computer}} system should have a model of where a person is looking at and what he/she is paying attention to. This is particularly important if a system is providing multimodal communication <b>cues,</b> <b>speech,</b> gesture, lipreading, etc., [2, 3, 8] and the system must identify, whether the cues are aimed at it, or at {{someone else in the}} room. This paper describes a system that identifies user focus of attention by visually determining where a person is looking. While other attempts at gaze tracking usually assume a fixed or limited location of a person 's face, the approach presented here allows for complete freedom of movement in a room. The gaze-tracking system, uses several connectionist modules, that track a person's face using a software controlled pan-tilt camera with zoom and identifies the focus of attention from the orientation and direction of the face. 1 Introduction One major impediment to user acceptance of speech inte [...] ...|$|R
30|$|The model {{described}} in the previous sections proposes multiple cases for each typographic cue (three cases for each font size and two cases for each font style). Using many prosodic <b>cues</b> in <b>speech</b> and mapping the same font attribute in different ways can be confusing to the listener. The following experiment has been designed in order to select the optimum acoustic rendition of text font cues.|$|R
