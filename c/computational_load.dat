2925|337|Public
25|$|Applications {{which use}} the Windows Presentation Foundation for the display {{elements}} can directly print to the XPS print path {{without the need}} for image or colorspace conversion. The XPS format used in the spool file, represents advanced graphics effects such as 3D images, glow effects, and gradients as Windows Presentation Foundation primitives, which are processed by the printer drivers without rasterization, preventing rendering artifacts and reducing <b>computational</b> <b>load.</b> When the legacy GDI Print Path is used, the XPS spool file is used for processing before it is converted to a GDI image to minimize the processing done at raster level.|$|E
500|$|According to Miyamoto, Pilotwings 64 was {{designed}} to allow gamers to experience free flight in realistic 3D environments on the Nintendo 64. Prior to the game's conception, Paradigm had worked on military vehicle and flight simulators, but not video games. Dave Gatchel of Paradigm disclosed that with regard to creating the game, they began with a [...] "physics-based approach", but deviated from this {{in order to gain}} a balance between accuracy and fun for players. He indicated that there was never an issue as to whether Pilotwings 64 should be more of an arcade game or a simulation, as their goal was to [...] "always have a more arcade feel". The technical team studied the original Pilotwings extensively during development. Pilotwings on the SNES makes use {{of the power of the}} 16-bit console, principally its Mode 7 capability. Similarly, Pilotwings 64 prominently demonstrates the graphical features of its own console. Gatchel suggested that just as design elements present in the game generated its production requirements, these same elements were influenced by the Nintendo 64's technology during development. The large islands within the game were created using Paradigm's own 3D development tool Vega UltraVision. Navigation of these environments is relatively smooth thanks to Pilotwings 64 taking advantage of several key Nintendo 64 hardware features. Conventional level of detail and mipmapping were used to reduce the <b>computational</b> <b>load</b> of distant landscape objects and terrains when they were rendered. The processes respectively substitute simpler geometrical shapes for more complex ones and less detailed textures for more detailed ones, lowering the polygon count and 3D rendering time for a given frame and thus putting less demand on the geometric engine. Pilotwings 64 also applies z-buffering, which keeps track of an object's depth and tells the graphics processor which portions of the object to render and which to hide. This, along with texture filtering and anti-aliasing, makes the object appear solid and smooth along its edges rather than pixelated.|$|E
2500|$|Although {{the sensor}} unit was {{originally}} planned {{to contain a}} microprocessor that would perform operations such as the system's skeletal mapping, it was revealed in January 2010 that the sensor would no longer feature a dedicated processor. Instead, processing would be handled {{by one of the}} processor cores of Xbox 360's Xenon CPU. According to Alex Kipman, Kinect system consumes about 10-15% of Xbox 360's computing resources. However, in November, Alex Kipman made a statement that [...] "the new motion control tech now only uses a single-digit percentage of Xbox 360's processing power, down from the previously stated 10 to 15 percent." [...] A number of observers commented that the <b>computational</b> <b>load</b> required for Kinect makes the addition of Kinect functionality to pre-existing games through software updates even less likely, with concepts specific to Kinect more likely to be the focus for developers using the platform.|$|E
40|$|Abstract. The {{modified}} propagator method(MPM) {{improves the}} performances of DOA estimation, but compared with the propagator method (PM), the <b>computational</b> <b>loads</b> increase, especially under large number of antenna array elements condition. This paper studies the principle of using unitary transform to rearrange the propagator, and proposes unitary transformed MPM(UMPM) in order to decrease the <b>computational</b> <b>loads</b> by real-valued computation. The simulation {{results show that the}} proposed algorithm can achieve superior precision and lower <b>computational</b> <b>loads</b> than PM, MUSIC and MPM. I...|$|R
40|$|It is {{demonstrated}} that supra-linear (greater than linear) speedup {{is possible in}} processing distributed divisible <b>computational</b> <b>loads</b> where computation time is a nonlinear function of load size. This result is radically di#erent from the traditional distributed processing of <b>computational</b> <b>loads</b> with linear processing complexity appearing in over 50 journal papers...|$|R
40|$|Abstract: In this paper, {{we study}} the {{scheduling}} problem for polynomial time complexity <b>computational</b> <b>loads</b> {{in a single}} level tree network with collective communication model. The problem of minimizing the processing time is investigated when the <b>computational</b> <b>loads</b> require polynomial order of processing time which {{is proportional to the}} size of load fraction. In divisible load theory framework, the presence of polynomial time complexity <b>computational</b> <b>loads</b> leads to solving higherorder algebraic equations to find the optimal load fractions assigned to the processors in the network. The problem of finding optimal load fraction is a computationally intensive task. Using a mild assumption on the ratio of communication time to com- 1 putation time, we present a closed-form solution for near optimal load fractions and processing time for the entire load fractions. Finally, we also present a closed-form solution for scheduling polynomial loads with start-up delay in communication and computation. The numerical speedup results obtained using closed-form solution clearly show that super-linear speedup is possible for the polynomial <b>computational</b> <b>loads.</b> Index Terms: Nonlinear divisible loads, broadcast communication or simultaneously load distribution model, overhead delays, single-level tree network. ...|$|R
2500|$|Whereas the {{original}} {{problem may be}} stated in a finite dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that {{the original}} finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the <b>computational</b> <b>load</b> reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily {{in terms of the}} variables in the original space, by defining them in terms of a kernel function [...] selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters [...] of images of feature vectors [...] that occur in the data base. With this choice of a hyperplane, the points [...] in the feature space that are mapped into the hyperplane are defined by the relation: [...] Note that if [...] becomes small as [...] grows further away from , each term in the sum measures the degree of closeness of the test point [...] to the corresponding data base point [...] In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in {{one or the other of}} the sets to be discriminated. Note the fact that the set of points [...] mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets which are not convex at all in the original space.|$|E
5000|$|ZHL-8: A version using {{a reduced}} number of tissue {{compartments}} {{to reduce the}} <b>computational</b> <b>load</b> for personal dive computers.|$|E
50|$|Surveillance Performance Index (SPI) is a {{benchmarking}} {{system that is}} used to rate IP-based security systems for a particular bandwidth or <b>computational</b> <b>load.</b>|$|E
3000|$|...,i= 1,…,M, {{which prevent}} {{deriving}} an analytical solution to (13). To avoid high <b>computational</b> <b>loads</b> {{of using the}} global searching algorithm, a relaxed version of (13) is proposed here.|$|R
40|$|Scheduling {{divisible}} loads {{with the}} nonlinear computational complexity is a challenging task as the recursive equations are nonlinear {{and it is}} difficult to find closed-form expression for processing time and load fractions. In this study, we attempt to address a divisible load scheduling problem for <b>computational</b> <b>loads</b> having second order computational complexity in a masterslave paradigm with non-blocking mode of communication. First, we develop algebraic means of determining the optimal size of load fractions assigned to the processors in the network using a mild assumption on communication to computation speed ratio. We use numerical simulation to verify the closeness of the proposed solution. Like in earlier works which consider processing loads with first order computational complexity, we study the conditions for optimal sequence and arrangements using the closed-form expression for optimal processing time. Our finding revels that the condition for optimal sequence and arrangements for second order <b>computational</b> <b>loads</b> are the same as that of linear <b>computational</b> <b>loads.</b> This scheduling algorithm can be used for aerospace applications such as Hough transform for image processing and pattern recognition using hidden Markov model...|$|R
30|$|Various {{beamforming}} algorithms for OFDMA communications {{have been}} investigated [2, 3]. However, {{most of the research}} focuses on beamforming per subcarrier using the conventional single-carrier beamforming algorithm. This approach causes high <b>computational</b> <b>loads</b> and increases system complexity.|$|R
5000|$|Hardware GNSS {{receivers}} are {{in general}} more efficient {{from the point}} of view of both <b>computational</b> <b>load</b> and power consumption since they have been designed in a highly specialized way with the only purpose of implementing the GNSS processing.|$|E
5000|$|Pierre Rautenbach is a {{computer}} scientist {{best known for his}} work in the field of real-time shadow rendering [...] and for constructing a system that dynamically swaps rendering algorithms - based on performance - as scene complexity (and thus <b>computational</b> <b>load)</b> increases. In this renderer, if needed and appropriate, some of the <b>computational</b> <b>load</b> is shifted from the GPU to the CPU. The net effect is a “satisficing” system that guarantees smooth motion through a scene and rendering quality that gracefully degrades in the face of increasing scene complexity and then recovers as conditions improve. Rautenbach {{is also the author of}} the book, 3D Game Programming Using DirectX 10 and OpenGL published by Cengage Learning.|$|E
50|$|The Viterbi {{algorithm}} is the optimum algorithm used to decode convolutional codes. There are simplifications {{to reduce the}} <b>computational</b> <b>load.</b> They rely on searching only the most likely paths. Although not optimum, they have generally been found to give good results in low noise environments.|$|E
30|$|Many {{of these}} estimators make {{assumptions}} on {{the shape of}} the signal distribution, assume narrow spatial spreads, and eigen-decompose the full-rank covariance matrix into a pseudo-signal subspace and a pseudo-noise subspace. Most often they result into a multi-dimensional optimization problem, implying high <b>computational</b> <b>loads.</b>|$|R
40|$|In {{this paper}} a {{weighted}} generalized predictive controller and a simplified adaptive control algorithm using a multi-step cost-function with polynomial weighting are presented. A simulation example and {{a comparison of}} <b>computational</b> <b>loads</b> are also given {{in order to show}} the performance of the controller...|$|R
40|$|Abstract — We {{present a}} hybrid energy {{management}} technique that exploits {{the variability of}} and correlations among the <b>computational</b> <b>loads</b> of tasks in a real-time application with soft timing constraints. In our technique, task load variability and correlations are captured in stochastic models that incorporate certain salient features and essential characteristics of the application. We use the stochastic models in formulating and solving the energy management problem for applications with soft timing constraints running on multiprocessor systems with dynamic voltage scaling (DVS). We present a novel optimization formulation for minimizing average energy consumption while providing a probabilistic guarantee for satisfying timing constraints. We compare our stochastic models and energy management scheme with other models and schemes that do not capture/exploit either the variability of or the correlations among the <b>computational</b> <b>loads</b> of tasks. I...|$|R
50|$|G.719 is an ITU-T {{standard}} audio {{coding format}} providing high quality, moderate bit rate (32 to 128 kbit/s) wideband (20 Hz - 20 kHz audio bandwidth, 48 kHz audio sample rate) audio coding at low <b>computational</b> <b>load.</b> It was produced through {{a collaboration between}} Polycom and Ericsson.|$|E
50|$|In this {{important}} step, the latest track prediction {{is combined with}} the associated plot to provide a new, improved estimate of the target state {{as well as a}} revised estimate of the errors in this prediction. There is a wide variety of algorithms, of differing complexity and <b>computational</b> <b>load,</b> {{that can be used for}} this process.|$|E
50|$|The {{normalized}} {{form of the}} LRLS has fewer recursions and variables. It can be calculated by applying a normalization to the internal variables of the algorithm which will keep their magnitude bounded by one. This is generally not used in real-time applications {{because of the number}} of division and square-root operations which comes with a high <b>computational</b> <b>load.</b>|$|E
40|$|It is {{interesting}} to compare the efficiency of two methods when their <b>computational</b> <b>loads</b> in each iteration are equal. In this paper, two classes of contraction methods for monotone variational inequalities are studied in a unified framework. The methods of both classes {{can be viewed as}} prediction-correction methods, which generate the same test vector in the prediction step and adopt the same step-size rule in the correction step. The only difference is that they use different search directions. The <b>computational</b> <b>loads</b> of each iteration of the different classes are equal. Our analysis explains theoretically why one class of the contraction methods usually outperforms the other class. It is demonstrated that many known methods belong to these two classes of methods. Finally, the presented numerical results demonstrate the validity of our analysis...|$|R
40|$|The Finite Element Method (FEM) and Boundary Element Method (BEM) are {{commonly}} used as numerical prediction techniques for vibro-acoustic analysis and design. Due to the large model sizes and subsequent <b>computational</b> <b>loads,</b> the practical use of these techniques is restricted to low-frequency applications. However, {{there is a strong}} industrial need to extend vibro-acoustic analyse...|$|R
40|$|Multiplication and {{division}} have in general {{been much more}} difficult toperform than addition and subtraction. Perhaps, if we could find some device for reducing multiplication {{and division}} to addition and subtraction, <b>computational</b> <b>loads</b> could be lightened. One such device is that of loga-rithms of course. This note outlines another such device with a brief outlin...|$|R
50|$|XPS {{supports}} HD Photo images natively for raster images. The XPS format used in {{the spool}} file represents advanced graphics effects such as 3D images, glow effects, and gradients as Windows Presentation Foundation primitives, which printer drivers could offload their rasterization to the printer {{in order to reduce}} <b>computational</b> <b>load</b> if the printer is capable of rasterizing those primitives.|$|E
50|$|SMPTE RP 187 further {{attempted}} to standardize the pixel aspect ratio values for 480i and 576i. It designated 177:160 for 480i or 1035:1132 for 576i. However, due to significant difference with practices in effect by {{industry and the}} <b>computational</b> <b>load</b> that they imposed upon the involved hardware, SMPTE RP 187 was simply ignored. SMPTE RP 187 information annex A.4 further suggested the use of 10:11 for 480i.|$|E
5000|$|Hagit Borer is a {{professor}} of linguistics at Queen Mary University of London. Her research falls within the area of Generative Grammar. Her theoretical approach shifts the <b>computational</b> <b>load</b> from words to syntactic structure, and pursues the consequences of this shift in morphosyntax, in language acquisition, in the syntax-semantics interface, and in syntactic inter-language variation. She initiated the eXoSkeletal framework in Morphology, which implements this idea.|$|E
40|$|MLUTs used {{in color}} {{characterization}} require significant memory for embedded systems. Most tools that create de-vice characterization maps, use MLUTs. For this reason, there is interest in approximating MLUT color characteri-zation mappings with more compressed function approxi-mation methods. In this document, {{artificial neural network}} approximations for MLUTs are investigated. Color accu-racy, <b>computational</b> <b>loads,</b> and memory requirements are discussed. 1...|$|R
50|$|Application {{checkpointing}} {{can be used}} {{to restore}} a given state of the system when a node fails during a long multi-node computation. This is essential in large clusters, given that as the number of nodes increases, so does the likelihood of node failure under heavy <b>computational</b> <b>loads.</b> Checkpointing can restore the system to a stable state so that processing can resume without having to recompute results.|$|R
3000|$|... [...]. In other words, {{when the}} value of variances or {{sampling}} times becomes high, it diminishes the location accuracy of the MT. As compared with the traditional KF-based tracking scheme, the proposed FOSB-based tracking scheme can achieve acceptable location performance and can reduce the computational burden with a slightly large corner effect. Therefore, {{the approach of the}} proposed tracking algorithm can work well under the large <b>computational</b> <b>loading</b> conditions.|$|R
50|$|Kimeme {{is an open}} {{platform}} for multi-objective optimization and multidisciplinary design optimization. It {{is intended to be}} coupled with external numerical software such as Computer Aided Design (CAD), Finite Element Analysis (FEM), Structural analysis and Computational Fluid Dynamics tools. It has been developed by Cyber Dyne Srl and provides both a design environment for problem definition and analysis and a software network infrastructure to distribute the <b>computational</b> <b>load.</b>|$|E
50|$|The use of sea {{skimming}} {{increases the}} risk of water impact with the missile before reaching the target, due to weather conditions, rogue waves, software bugs and other factors. Sea skimming also hinders target acquisition, as many of the principles that hinder the target's detection of the missile also hinder the missile's detection of the target. Furthermore, sea skimming involves a significant <b>computational</b> <b>load,</b> increasing the required processing power and cost.|$|E
50|$|The Cray CX1 is a deskside {{high-performance}} workstation {{designed by}} Cray Inc., {{based on the}} x86-64 processor architecture. It was launched on September 16, 2008, and was discontinued in early 2012. It comprises a single chassis blade server design that supports a maximum of eight modular single-width blades, giving up to 96 processor cores. <b>Computational</b> <b>load</b> can be run independently on each blade and/or combined using clustering techniques.|$|E
40|$|Abstract. We show a {{two-phase}} {{approach for}} minimizing various communication-cost metrics in fine-grain partitioning of sparse matrices for parallel processing. In the first phase, we obtain a partitioning {{with the existing}} tools on the matrix to determine <b>computational</b> <b>loads</b> of the processor. In the second phase, we try to minimize the communicationcost metrics. For this purpose, we develop communication-hypergraph and partitioning models. We experimentally evaluate the contributions on a PC cluster. ...|$|R
40|$|In {{the last}} decade, robot dynanic conputational {{algorithm}} have been steadily improved. Lagrangian [1] and Newton-Euler [2, 3] recursive nunerical algorithns, Horak’s mixed calculation [4], Lisp based symbolic derivation of the robot closed forn equations [5, 6] and recently, costumized conputing algorithns [7 – 9] {{have been important}} stages towards nore efficient procedures, Nevertheless, the still existing high <b>computational</b> <b>loads,</b> have prevented the industrial application of such algorithns. info:eu-repo/semantics/publishedVersio...|$|R
40|$|We show a {{two-phase}} {{approach for}} minimizing various communication-cost metrics in fine-grain partitioning of sparse matrices for parallel processing. In the first phase, we obtain a partitioning {{with the existing}} tools on the matrix to determine <b>computational</b> <b>loads</b> of the processor. In the second phase, we try to minimize the communication-cost metrics. For this purpose, we develop communication-hypergraph and partitioning models. We experimentally evaluate the contributions on a PC cluster. © Springer-Verlag Berlin Heidelberg 2003...|$|R
