47|10000|Public
40|$|Consider multi-dimensional root finding {{when the}} {{equations}} are available only implicitly via a Monte Carlo simulation oracle that for any solution returns a vector of point estimates. We develop DARTS, a stochasticapproximation algorithm that makes quasi-Newton {{moves to a}} new solution whenever the <b>current</b> <b>sample</b> <b>size</b> is large compared to the estimated quality of the current solution and estimated sampling error. We show that DARTS converges in a certain precise sense, and discuss reasons to expect substantial computational efficiencies over traditional stochastic approximation variations. ...|$|E
30|$|To {{ensure that}} the <b>current</b> <b>sample</b> <b>size</b> of 100 was enough for all models to obtain {{adequate}} power and precision of parameter estimates, we conducted Monte Carlo studies following Muthén and Muthén (2002; see In’nami and Koizumi 2013, for concrete procedures). The {{results showed that the}} sample size was sufficient, except for the TEAP higher-order and TOEFL iBT unitary model (Model 3), which concerns Research Question 2. The covariance matrix of this model was not positive definite as will be reported below, which prevented us from calculating the power and precision of the parameter estimates.|$|E
30|$|As aforementioned, the use {{of linear}} and {{non-linear}} regression for {{the determination of the}} required correction has been rejected as an unsafe method in this case. However, a bigger sample might likely lead to different results and would allow the investigation of the statistical significance of the results, which with the <b>current</b> <b>sample</b> <b>size</b> was not possible. In specific, in order to apply a one-sample t-test (one-tailed), which would be applicable here, at least 27 persons per comparison (and consequently per driver group) would be required (for a power of 0, 8 and Cohen medium effect size of 0, 50). Definitely, priority for more trials with bigger sample should be given to the cases that already present a strong tendency, meaning those that have resulted in transfer algorithms.|$|E
30|$|Future {{studies are}} being planned with {{increased}} <b>sample</b> <b>size</b> and comparisons with gender- and age-matched control sample {{in order to}} overcome the <b>current</b> study <b>sample</b> <b>size</b> limitation.|$|R
30|$|MGC- 0109 {{given at}} reperfusion reduced infarct size by 40 %. Due to the <b>current</b> small <b>sample</b> <b>size,</b> {{these values are}} not {{statistically}} significant.|$|R
30|$|Power {{analysis}} indicated that <b>sample</b> <b>sizes</b> were not sufficient to detect any long term changes that occurred, likely due to high spatial (i.e., among plots) variability in pre-fire conditions and post-fire response. Conversely, <b>current</b> <b>sample</b> <b>sizes</b> were large enough to detect short term changes following second entry fires. The lack of significant findings following first entry fire may also be due to pseudoreplication of plots, which is inherent in prescribed fire analysis with multiple burn units over multiple years (van Mantgem et al. 1999), or to the overall low burn severity found in the monitoring plots (Figure 1, Table 1).|$|R
30|$|Data {{were tested}} for {{normality}} using the Kolmogorov–Smirnov test. A repeated-measures {{analysis of variance}} was performed. Fischer’s exact test was used for the categorical data. Independent t test was used to compare the continuous variables in the two groups. The Mann–Whitney U test was performed to compare the nonparametric values of the two groups. Data were expressed as median (interquartile range (IQR) [range]), number (proportion), or mean (SD) as appropriate. The volume of cases was not enough to allow a priori power analysis. However, a post hoc power analysis indicated that the <b>current</b> <b>sample</b> <b>size</b> of 35 patients is powered to detect 35 % absolute difference in mortality rate, with a type I error of 0.05 and a power of 80 %. A value of P <  0.05 was considered statistically significant.|$|E
40|$|One of the {{fundamental}} hypotheses in observational cosmology is {{the validity of the}} so-called cosmic distance-duality relation (CDDR). In this paper, we perform Monte Carlo simulations based on the method developed in Holanda, Goncalves & Alcaniz (2012) [JCAP 1206 (2012) 022] to answer the following question: what is the number of galaxy clusters observations N_crit needed to check the validity of this relation at a given confidence level? At 2 σ, we find that N_crit should be increased at least by a factor of 5 relative to the <b>current</b> <b>sample</b> <b>size</b> if we assume the current observational uncertainty σ_obs. Reducing this latter quantity by a factor of 2, we show that the present number of data would be already enough to check the validity of the CDDR at 2 σ. Comment: 5 pages, 3 figures, late...|$|E
30|$|There {{are several}} {{limitations}} to this study. The backgrounds, such as disease and supportive therapy except for hypouricemic agents, were varied. In addition some {{patients who were}} defined as having a low TLS risk might not have needed febuxostat or allopurinol. Although the superiority or even non-inferiority of febuxostat at 40  mg/day to allopurinol at 300  mg/day was not proven with the <b>current</b> <b>sample</b> <b>size,</b> our results suggest that at least the equality or non-inferiority of febuxostat might be demonstrated if the sample size is increased. If so, we could conclude that febuxostat is more useful than allopurinol, given {{the difference in the}} necessity for dose reduction. Thus, prospective randomized controlled trials in a larger cohort are warranted. Moreover, if we had evaluated serum and urine levels of purine precursors, it might have provided a better insight, because both febuxostat and allopurinol increase xanthine levels, and xanthinuria may lead to kidney injury (Hande et al. 1981).|$|E
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. In recent years, {{there has been}} a considerable amount of research on the use of regularization methods for inference and prediction in quantitative genetics. Such researchmostly focuses on selection ofmarkers and shrinkage of their effects. In this review paper, the use of ridge regression for prediction in quantitative genetics using single-nucleotide polymorphism data is discussed. In particular, we consider (i) the theoretical foundations of ridge regression, (ii) its link to commonly used methods in animal breeding, (iii) the computational feasibility, and (iv) the scope for constructing prediction models with nonlinear effects (e. g., dominance and epistasis). Based on a simulation study we gauge the current and future potential of ridge regression for prediction of human traits using genome-wide SNP data. We conclude that, for outcomes with a relatively simple genetic architecture, given <b>current</b> <b>sample</b> <b>sizes</b> in most cohorts (i. e. ...|$|R
40|$|The main {{objective}} {{of this paper is}} to investigate the relationship between the <b>size</b> of training <b>sample</b> and the predictive power of well-known classification techniques. We first display this relationship using the results of some empirical studies and then propose a general mathematical model which can explain this relationship. Next, we validate this model on some real data sets and found that the model provides a good fit to the data. This model also allow a more objective determination of optimum training <b>sample</b> <b>size</b> in contrast to <b>current</b> training <b>sample</b> <b>size</b> selection approaches which tend to be ad hoc or subjective...|$|R
40|$|Genetic {{prediction}} {{based on}} either identity by state (IBS) sharing or pedigree {{information has been}} investigated extensively with best linear unbiased prediction (BLUP) methods. Such methods were pioneered in plant and animal-breeding literature and have since been applied to predict human traits, {{with the aim of}} eventual clinical utility. However, methods to combine IBS sharing and pedigree information for genetic prediction in humans have not been explored. We introduce a two-variance-component model for genetic prediction: one component for IBS sharing and one for approximate pedigree structure, both estimated with genetic markers. In simulations using real genotypes from the Candidate-gene Association Resource (CARe) and Framingham Heart Study (FHS) family cohorts, we demonstrate that the two-variance-component model achieves gains in prediction r 2 over standard BLUP at <b>current</b> <b>sample</b> <b>sizes,</b> and we project, based on simulations, that these gains will continue to hold at larger <b>sample</b> <b>sizes.</b> Accordingly, in analyses of four quantitative phenotypes from CARe and two quantitative phenotypes from FHS, the two-variance-component model significantly improves prediction r 2 in each case, with up to a 20 % relative improvement. We also find that standard mixed-model association tests can produce inflated test statistics in datasets with related individuals, whereas the two-variance-component model corrects for inflation...|$|R
40|$|The fortuitous {{occurrence}} {{of a type}} II-Plateau (IIP) supernova, SN 2014 bc, in a galaxy for which distance estimates {{from a number of}} primary distance indicators are available provides a means with which to cross-calibrate the standardised candle method (SCM) for type IIP SNe. By applying calibrations from the literature we find distance estimates in line with the most precise measurement to NGC 4258 based on the Keplerian motion of masers (7. 6 ± 0. 23 Mpc), albeit with significant scatter. We provide an alternative local SCM calibration by only considering type IIP SNe that have occurred in galaxies for which a Cepheid distance estimate is available. We find a considerable reduction in scatter (σ_I = 0. 16 mag.), but note that the <b>current</b> <b>sample</b> <b>size</b> is limited. Applying this calibration, we estimate a distance to NGC 4258 of 7. 08 ± 0. 86 Mpc. Comment: 7 pages, 4 figures, 4 tables, published in A&A Letters. Corrected for missing reference in caption of Fig. ...|$|E
40|$|Abstract: To {{explore the}} {{relative}} impact of genetic and nongenetics factors on human brain anatomy dur-ing {{childhood and adolescence}} development, a collaborative team from the Child Psychiatry Branch of the National Institute of Mental Health and Virginia Commonwealth University is applying structural equation modeling to brain morphometric data acquired via magnetic resonance imaging from a large sample of monozygotic and dizygotic pediatric subjects. In this report, we discuss methodologic issues related to pe-diatric neuroimaging twin studies and synthesize results to date from the project. <b>Current</b> <b>sample</b> <b>size</b> from the ongoing longitudinal study is approximately 150 twin pairs. Consistent themes are: (1) heritability is high and shared environmental effects low for most brain morphometric measures; (2) the cerebellum has a distinct heritability profile; (3) genetic and environmental factors {{contribute to the development}} of the cortex in a regional and age specific manner; and (4) shared genetic effects account for more of the var-iance than structure specific effects. Understanding of influences on trajectories of brain development may shed light on the emergence of psychopathology during childhood and adolescence and ultimately ma...|$|E
40|$|In {{cases of}} {{potential}} child abuse, parents may provide hearsay testimony {{on behalf of}} a child, retelling events from the child’s perspective. However, according to the limited research that exists, parents may {{have a negative impact on}} their child’s memory of an event (Principe, DiPuppo, & Gammel, 2013). In order to gain a better understanding of parental hearsay, parents’ descriptions of information children provided in recorded parent-child discussions were compared to the actual information the children provided in the initial discussion and in a 1 -week follow-up interview. Children interviewed by parents were also compared to children interviewed by a trained interviewer. To date, 11 children between the ages of 6 - 9 years have been assessed. While the <b>current</b> <b>sample</b> <b>size</b> was too small to yield many significant results, graphs and effect sizes suggest there are differences in memory accuracy and completeness between parents and children and across children’s interview condition. Whether hearsay testimony or children’s testimony is preferable may depend on how suggestive the initial parent-child discussion is...|$|E
40|$|In recent years, {{there has}} been a {{considerable}} amount of research on the use of regularization methods for inference and prediction in quantitative genetics. Such research mostly focuses on selection of markers and shrinkage of their effects. In this review paper, the use of ridge regression for prediction in quantitative genetics using single-nucleotide polymorphism data is discussed. In particular, we consider (i) the theoretical foundations of ridge regression, (ii) its link to commonly used methods in animal breeding, (iii) the computational feasibility, and (iv) the scope for constructing prediction models with nonlinear effects (e. g., dominance and epistasis). Based on a simulation study we gauge the current and future potential of ridge regression for prediction of human traits using genome-wide SNP data. We conclude that, for outcomes with a relatively simple genetic architecture, given <b>current</b> <b>sample</b> <b>sizes</b> in most cohorts (i. e., N < 10, 000) the predictive accuracy of ridge regression is slightly higher than the classical genome-wide association study approach of repeated simple regression (i. e., one regression per SNP). However, both capture only a small proportion of the heritability. Nevertheless, we find evidence that for large-scale initiatives, such as biobanks, <b>sample</b> <b>sizes</b> can be achieved where ridge regression compared to the classical approach improves predictive accuracy substantially...|$|R
40|$|For {{more than}} 100 years, {{some form of}} road has cut through the forest east of Snoqualmie Pass. This has presumably {{separated}} the red-backed vole (Myodes gapperi) into two populations {{north and south of}} the current I- 90. With the long-term separation of populations, phenotypic divergence can occur. I tested for differences in size between these populations using specimens that had previously been collected from pitfall traps on either side of I- 90 near Keechelus Dam. I measured total body length, tail length, hind foot length, ear length, and weight for both sexes. Preliminary data summary shows a slight difference in average length of 4. 5 millimeters, and a weight difference of 1. 6 grams with <b>current</b> <b>sample</b> <b>sizes</b> of 11 and 17 for the south and north populations, respectively. I will continue to collect data and analyze the measurements taken from both populations to determine if any of their characteristics have diverged significantly. Any differences in the populations now can be compared with differences in the future after wildlife crossing structures have reconnected these populations...|$|R
40|$|The {{substantial}} heritability of {{most complex}} diseases suggests that genetic data could provide useful risk prediction. To date {{the performance of}} genetic risk scores has fallen short of the potential implied by heritability, but this {{can be explained by}} insufficient <b>sample</b> <b>sizes</b> for estimating highly polygenic models. When risk predictors already exist based on environment or lifestyle, two key questions are to what extent can they be improved by adding genetic information, and what is the ultimate potential of combined genetic and environmental risk scores? Here, we extend previous work on the predictive accuracy of polygenic scores to allow for an environmental score that may be correlated with the polygenic score, for example when the environmental factors mediate the genetic risk. We derive common measures of predictive accuracy and improvement as functions of the training <b>sample</b> <b>size,</b> chip heritabilities of disease and environmental score, and genetic correlation between disease and environmental risk factors. We consider simple addition of the two scores and a weighted sum that accounts for their correlation. Using examples from studies of cardiovascular disease and breast cancer, we show that improvements in discrimination are generally small but reasonable degrees of reclassification could be obtained with <b>current</b> <b>sample</b> <b>sizes.</b> Correlation between genetic and environmental scores has only minor effects on numerical results in realistic scenarios. In the longer term, as the accuracy of polygenic scores improves they will come to dominate the predictive accuracy compared to environmental scores...|$|R
40|$|The FMAP 2006 is a pan-European forest map, {{recently}} {{produced at}} the Joint Research Centre of the European Commission. The fine spatial resolution {{at which the}} pixels have been mapped (25 m) has important benefits for derived products, e. g., analyzing landscape fragmentation and spatial pattern changes. This validation study showed another advantage. A unique dataset of detailed field reference plots was available through a collaboration with National Forest Inventories (NFIs) in Europe. However, most existing continental or global land cover maps have a coarse spatial resolution that is not matched to the small area of the sampling units applied by the NFIs, which makes them not suited {{for this type of}} reference data. The fine spatial resolution of the FMAP 2006 facilitated the label assignment of the field reference plots to the pixels for the validation process. More than one million reference plots were available for validation, distributed across 17 countries in Europe. In contrast to previous validations of land cover products at this scale, the <b>current</b> <b>sample</b> <b>size</b> of the reference data allowed for a reliable assessment at local, regional and pan-European level. JRC. H. 3 -Forest Resources and Climat...|$|E
40|$|Off-site {{recreational}} fishery surveys, {{when compared to}} on-site surveys, allows fisheries managers to contact a larger sample over a wider spatial scale at a lower cost. However, off-site surveys are prone to nonresponse bias. Nonresponse bias {{is known to have}} adverse effects on sample estimates and can erode the leverage of benefits provided by off-site surveys. I explored nonresponse bias in an off-site survey administered to estimate annual total effort and catch in British Columbia’s lower and middle Fraser River white sturgeon (Acipenser transmontanus) {{recreational fishery}}. I explored biases associated with survey mode and response rate. I further used¬¬ simulation modeling to determine how sample size affects both survey costs and estimates’ accuracy. I found that nonresponse bias arose from anglers’ participation rate {{and to a lesser extent}} from anglers’ catch. Anglers who did not fish were less likely to respond. Simulation modelling showed that sample size in the first phase of contact could be reduced by 40 %, while holding the follow-up contact at <b>current</b> <b>sample</b> <b>size,</b> and still produce accurate results. Generally, results show that nonresponse bias affected off-site survey estimates even in a relatively small group of specialized anglers...|$|E
40|$|Ministry of Agriculture (MINAGRI) {{maintained}} a comprehensive database of agricultural statistics. The DSA {{was responsible for}} providing information on agricultural policy based on annual surveys of rural households. These surveys were conducted {{under the auspices of}} the Enquete National Agricole (ENA). These surveys, which were interrupted in 1994 were resumed in 1999 by the Food Security Research Project (FSRP) and the Agricultural Statistics Division (DSA) of the MINAGRI. The FSRP/DSA began conducting agricultural surveys in 1999 using a national sample of 1584 households. The FSRP/DSA collects land use (area) and production data on a seasonal basis (twice a year). The FSRP/DSA has 11 enumerators (one per province) as compared to 78 enumerators that the ENA had before 1994. The <b>current</b> <b>sample</b> <b>size</b> is also 26 % bigger than the one the ENA used. Since the FSRP did not have as much financial resources as those available to the ENA, it had to find a less costly but also accurate method to conduct the surveys. The most time consuming and therefore expensive activity of data collection is area/field measurement. After considering various area measurement methodologies, the FSRP/DSA selected the P 2 /...|$|E
40|$|RMP or Program) {{evaluates the}} {{concentration}} of pollutants {{in a wide range}} of matrices. Trace metals and organic compounds are monitored in water, sediments, bivalves, and sport fish as part of the Status and Trends element of the RMP (SFEI 2005, 2006 b). Periodically, the cost-effectiveness and statistical power of the major elements of the RMP are evaluated using power analysis in combination with an evaluation of the information needs and priorities of water quality managers. This evaluation of the Program was motivated by new understanding of bay processes (e. g., presence of phytoplankton blooms), changes in the regulatory focus from the water column to biota (e. g., bird eggs, sport fish and small fish), and significant management actions that may impact the Bay (e. g., large-scale wetland restorations). This report describes methods and selected findings for power analyses conducted in 2006, and documents the rationale for Technical Review Committee (TRC) and Steering Committee (SC) decisions regarding the design of RMP Status and Trends monitoring. The objective of the power analysis was to evaluate whether <b>current</b> <b>sampling</b> <b>size</b> and frequency are appropriate for meeting the needs of RMP stakeholders, including th...|$|R
40|$|In {{species of}} {{conservation}} concern {{it is often}} difficult to be certain that population diversity and structure have been adequately characterised by genetic sampling. Since practical and financial constraints tend to be associated with increasing <b>sample</b> <b>sizes</b> in many conservation genetic studies, it is important to consider the potential for sampling error and bias due to inadequate samples or spatio-temporal structure within populations. We analysed sequence data from the mitochondrial DNA control region in a large sample (n = 245) of green sea turtles Chelonia mydas collected at the globally important rookery of Ascension Island, South Atlantic. We examined genetic diversity and structure among 10 sampling sites, 4 beach clusters and 4 nesting seasons, and evaluated the genetic composition of Ascension against other Atlantic nesting populations, including the well-studied rookery at Tortuguero (Costa Rica). Finally, we used rarefaction and GENESAMP analyses to assess the ability of different <b>sample</b> <b>sizes</b> to provide acceptable genetic representations of a population, using Ascension and Tortuguero as models. On Ascension, we found 13 haplotypes, of which only 3 had been previously observed in the rookery, and 5 previously undescribed. We detected no differentiation among beach clusters or sampling seasons, and only weak differentiation among the 3 primary nesting sites. The increased <b>sample</b> <b>size</b> for Ascension provided higher resolution and statistical power in describing genetic structure among all other known Atlantic rookeries. Our extrapolations showed that a maximum of 18 and 6 haplotypes are expected to occur in Ascension and Tortuguero, respectively, and that <b>current</b> <b>sample</b> <b>sizes</b> are sufficient to describe most of the variation. We recommend using rarefaction and GENESAMP analyses on a rookery-by-rookery basis to evaluate whether a sample set adequately describes mitochondrial DNA diversity, thus strengthening subsequent phylogeographic and mixed stock analyses, and management recommendations for conservation...|$|R
40|$|Additional Supporting Information may {{be found}} online in the {{supporting}} information tab for this article. The substantial heritability of most complex diseases suggests that genetic data could provide useful risk prediction. To date the performance of genetic risk scores has fallen short of the potential implied by heritability, but this {{can be explained by}} insufficient <b>sample</b> <b>sizes</b> for estimating highly polygenic models. When risk predictors already exist based on environment or lifestyle, two key questions are to what extent can they be improved by adding genetic information, and what is the ultimate potential of combined genetic and environmental risk scores? Here, we extend previous work on the predictive accuracy of polygenic scores to allow for an environmental score that may be correlated with the polygenic score, for example when the environmental factors mediate the genetic risk. We derive common measures of predictive accuracy and improvement as functions of the training <b>sample</b> <b>size,</b> chip heritabilities of disease and environmental score, and genetic correlation between disease and environmental risk factors. We consider simple addition of the two scores and a weighted sum that accounts for their correlation. Using examples from studies of cardiovascular disease and breast cancer, we show that improvements in discrimination are generally small but reasonable degrees of reclassification could be obtained with <b>current</b> <b>sample</b> <b>sizes.</b> Correlation between genetic and environmental scores has only minor effects on numerical results in realistic scenarios. In the longer term, as the accuracy of polygenic scores improves they will come to dominate the predictive accuracy compared to environmental scores. This work is supported by the MRC (MR/K 006215 / 1, MR/M 026175 / 1). Peer-reviewedPublisher Versio...|$|R
40|$|Given {{the growing}} {{attention}} to quality improvement, comparative effectiveness research, and pragmatic trials embedded within learning health systems, {{the use of the}} cluster randomization design is bound to increase. The number of clusters available for randomization is often limited in such trials. Designs that incorporate pre-intervention measurements (e. g. cluster cross-over, repeated parallel arm, and stepped wedge designs) can substantially reduce the required numbers of clusters by decreasing between-cluster sources of variation. However, there are substantial risks associated with few clusters, including increased probability of chance imbalances and type I and type II error, limited perceived or actual generalizability, and fewer options for statistical analysis. Furthermore, <b>current</b> <b>sample</b> <b>size</b> methods for the stepped wedge design make a strong underlying assumption with respect to the correlation structure-in particular, that the intracluster and inter-period correlations are equal. This is in contrast with methods for the cluster cross-over design that explicitly allow for a smaller inter-period correlation. Failing to similarly allow for the inter-period correlation in the design of a stepped wedge trial may yield perilously low sample sizes. Further methodological and empirical work is required to inform sample size methods and guidance for the stepped wedge trial and to provide minimum thresholds for this design...|$|E
40|$|We have {{initiated}} {{a project to}} classify stellar spectra automatically from high-dispersion objective prism plates. The automated technique presented here is a simple backpropagation neural network, {{and is based on}} the visual classification work of Houk. The plate material (Houk's) is currently being digitized, and contains ≈ 105 stars down to V ≈ 11 at ≈ 2 -Å resolution from ≈ 3850 to 5150 Å. For this first paper in the series we report on the results of 575 stars digitized from 6 plates. We find that even with the limited data set now in hand we can determine the temperature classification to better than 1. 7 spectral subtypes from B 3 to M 4. Our <b>current</b> <b>sample</b> <b>size</b> provides insufficient training set material to generate luminosity and metallicity classifications. Our eventual aims in this project are (1) to create a large and homogeneous digital stellar spectral library; (2) to create a well-understood and robust automatic classification algorithm which can determine temperatures, luminosities and metallicities {{for a wide variety of}} spectral types; (3) to use these data, supplemented by deeper plate material, for the study of Galactic structure and chemical evolution; and (4) to find unusual or new classes of objects...|$|E
40|$|International audienceGenetic risk {{prediction}} {{has several}} potential applications in medical research and clinical practice {{and could be}} used, for example, to stratify a heterogeneous population of patients by their predicted genetic risk. However, for polygenic traits, such as psychiatric disorders, the accuracy of risk prediction is low. Here we use a multivariate linear mixed model and apply multi-trait genomic best linear unbiased prediction for genetic risk prediction. This method exploits correlations between disorders and simultaneously evaluates individual risk for each disorder. We show that the multivariate approach significantly increases the prediction accuracy for schizophrenia, bipolar disorder, and major depressive disorder in the discovery {{as well as in}} independent validation datasets. By grouping SNPs based on genome annotation and fitting multiple random effects, we show that the prediction accuracy could be further improved. The gain in prediction accuracy of the multivariate approach is equivalent to an increase in sample size of 34 % for schizophrenia, 68 % for bipolar disorder, and 76 % for major depressive disorders using single trait models. Because our approach can be readily applied to any number of GWAS datasets of correlated traits, it is a flexible and powerful tool to maximize prediction accuracy. With <b>current</b> <b>sample</b> <b>size,</b> risk predictors are not useful in a clinical setting but already are a valuable research tool, for example in experimental designs comparing cases with high and low polygenic risk...|$|E
40|$|CONECOFOR is the Italian {{program for}} the {{intensive}} monitoring of forest ecosystems sponsored by the Ministry for Agricultural and Forest Policy and the European Commission. It is based upon a series of investigations carried out on 20 (27 from 1999) plots located throughout Italy. The investigations collect data on various ecosystem compartments (soil, ground vegetation, trees, atmosphere) and processes (atmospheric inputs, tree nutrition and growth). A major benefit of intensive monitoring programs is the opportunity they provide to organize integrated studies aimed at understanding the driving forces acting at the ecosystem level. In Italy, the Integrated and Combined (I&C) evaluation system within the CONECOFOR program involves three major approaches: (i) the evaluation of risk status in relation to air pollution, (ii) the quantification of the ecosystem’s status and changes, and (iii) {{the evaluation of the}} relationship between pressure and status indicators through time. The I&C project involves scientists from many institutions; major emphasis is placed on the evaluation of data quality and precision. These are regarded as the basic steps in the whole project and a likely source of information about the suitability of the <b>current</b> <b>sample</b> <b>sizes</b> for providing a proper estimation of the parameters under consideration. The paper will provide information on the conceptual and methodological background of the I&C projec...|$|R
40|$|AbstractBackgroundThe {{impact of}} both cancer and its {{treatment}} on bone {{is an essential}} component of oncological practice. Bone oncology not only affects patients with both early stage and metastatic disease but also covers the entire spectrum of tumour types. We therefore decided to review and summarise bone oncology-related trials that are currently being conducted in Canada. MethodWe assessed ongoing and recently completed trials in Canada. We used available North American and Canadian cancer trial websites and also contacted known investigators in this field for their input. ResultsTwenty seven clinical trials were identified. Seven pertained to local treatment of bone metastasis from any solid tumour type. Seven were systemic treatment trials, five focused on bone biology and predictive factors, three evaluated safety of bone-targeted agents, three were adjuvant trials and two trials investigated impact of cancer therapy on bone health. The majority of trials were related to systemic treatment and bone biology in breast cancer. Most were small, single centre, grant-funded studies. Not surprisingly the larger safety and adjuvant studies were pharmaceutical company driven. DiscussionDespite the widespread interest in bone-targeted therapies our survey would suggest that most studies are single centre and breast cancer focused. If major advances in bone oncology are to be made then collaborative strategies are needed to not only increase <b>current</b> <b>sample</b> <b>sizes</b> but to also expand these studies into non-breast cancer populations...|$|R
40|$|Includes bibliographical {{references}} (leaves 43 - 45) The {{longer period}} pre-main-sequence (PMS) and T Tauri stars present a puzzle to the expected angular momentum evolution of young stars, additional photometric observations to increase <b>current</b> <b>sample</b> <b>sizes</b> and probe longer period systems. Here I present {{a sample of}} previously identified PMS and T Tauri stars observed with SuperWASP-North in La Palma during the 2004 - 2009 observing seasons. The SuperWASP temporal coverage ranges from 30 to over 500 days, up to a factor of 15 to 20 greater than those of previous surveys. Our goals were twofold: to search for periods longer than 8 days using the extended baseline of SuperWASP observations, and 2) to search for year-to-year changes in period that may help {{in the understanding of}} angular momentum losses in young PMS stars. In total, 115 sources are included, with periods ranging from 1. 231 to 11. 675 Days. The period distribution is bimodal, with values clustered between 1 to 2 days and 6 to 8 days, as previously found by Herbst and colleagues. Surprisingly we only found 2 sources with periods between 10 and 16 days. We speculate these may also be a result of the disk locking mechanism and further study is needed. There is a slight trend that the majority of slower rotating stars have an IR excess (measured with H-K) as compared to the faster rotators...|$|R
40|$|Adolescent {{girls have}} been shown to be at {{increased}} risk for experiencing depression (Atwater, 1996), emotional distress (Covey 2 ̆ 6 Feltz, 1991), lower self-esteem (Bush 2 ̆ 6 Simmons, 1987), and dissatisfied body image (Covey 2 ̆ 6 Feltz, 1991). Female athletes are more likely to attribute successful outcomes to factors independent of themselves (e. g., luck rather than skill) (Hendy 2 ̆ 6 Boyer, 1993). Such cognitive distortions provide an unrealistic framework from which girls evaluate themselves and situations in which they are involved. 2 ̆ 2 Thinking for Success 2 ̆ 2 is an intervention designed to provide adolescent girls with active strategies to overcome self-defeating cognitions. The intervention was conducted with a sample of 32 female-adolescent athletes on a cross-country team. Results suggested that the intervention improved participants 2 ̆ 7 levels of self-efficacy for improving their body image, coping with daily life stress, managing cross-country and physical-fitness abilities, and implementing workshop skills. There was a suggestive, but nonsignificant, trend for lowered trait anxiety. Skills taught throughout the intervention, and level of efficacy for practicing said skills, significantly predicted this decline in trait anxiety. Contrary to expectations, athletic performance was not affected by intervention participation. Power analyses indicated that the <b>current</b> <b>sample</b> <b>size</b> was sufficient to detect only large effects. A larger sample size would have been required to detect small or medium effects...|$|E
40|$|Attention Deficit/Hyperactivity Disorder (ADHD) {{is one of}} {{the most}} common childhood-onset neuropsychiatric disorders. Despite high {{heritability}} estimates, genome-wide association studies (GWAS) have failed to find significant genetic associations, likely due to the polygenic character of ADHD. Nevertheless, genetic studies suggested the involvement of several processes important for synaptic function. Therefore, we applied a functional gene-set analysis to formally test whether synaptic functions are associated with ADHD. Gene-set analysis tests the joint effect of multiple genetic variants in groups of functionally related genes. This method provides increased statistical power compared to conventional GWAS. We used data from the Psychiatric Genomics Consortium including 896 ADHD cases and 2455 controls, and 2064 parent-affected offspring trios, providing sufficient statistical power to detect gene sets representing a genotype relative risk of at least 1. 17. Although all synaptic genes together showed a significant association with ADHD, this association was not stronger than that of randomly generated gene sets matched for same number of genes. Further analyses showed no association of specific synaptic function categories with ADHD after correction for multiple testing. Given <b>current</b> <b>sample</b> <b>size</b> and gene sets based on current knowledge of genes related to synaptic function, our results do not support a major role for common genetic variants in synaptic genes in the etiology of ADHD. © 2014 by the authors; licensee MDPI, Basel, Switzerland...|$|E
40|$|Genetic risk {{prediction}} {{has several}} potential applications in medical research and clinical practice {{and could be}} used, for example, to stratify a heterogeneous population of patients by their predicted genetic risk. However, for polygenic traits, such as psychiatric disorders, the accuracy of risk prediction is low. Here we use a multivariate linear mixed model and apply multi-trait genomic best linear unbiased prediction for genetic risk prediction. This method exploits correlations between disorders and simultaneously evaluates individual risk for each disorder. We show that the multivariate approach significantly increases the prediction accuracy for schizophrenia, bipolar disorder, and major depressive disorder in the discovery {{as well as in}} independent validation datasets. By grouping SNPs based on genome annotation and fitting multiple random effects, we show that the prediction accuracy could be further improved. The gain in prediction accuracy of the multivariate approach is equivalent to an increase in sample size of 34 % for schizophrenia, 68 % for bipolar disorder, and 76 % for major depressive disorders using single trait models. Because our approach can be readily applied to any number of GWAS datasets of correlated traits, it is a flexible and powerful tool to maximize prediction accuracy. With <b>current</b> <b>sample</b> <b>size,</b> risk predictors are not useful in a clinical setting but already are a valuable research tool, for example in experimental designs comparing cases with high and low polygenic ris...|$|E
40|$|Abstract Background Molecular {{signatures}} are sets of genes, proteins, genetic variants {{or other}} variables {{that can be}} used as markers for a particular phenotype. Reliable signature discovery methods could yield valuable insight into cell biology and mechanisms of human disease. However, it is currently not clear how to control error rates such as the false discovery rate (FDR) in signature discovery. Moreover, signatures for cancer gene expression {{have been shown to be}} unstable, that is, difficult to replicate in independent studies, casting doubts on their reliability. Results We demonstrate that with modern prediction methods, signatures that yield accurate predictions may still have a high FDR. Further, we show that even signatures with low FDR may fail to replicate in independent studies due to limited statistical power. Thus, neither stability nor predictive accuracy are relevant when FDR control is the primary goal. We therefore develop a general statistical hypothesis testing framework that for the first time provides FDR control for signature discovery. Our method is demonstrated to be correct in simulation studies. When applied to five cancer data sets, the method was able to discover molecular signatures with 5 % FDR in three cases, while two data sets yielded no significant findings. Conclusion Our approach enables reliable discovery of molecular signatures from genome-wide data with <b>current</b> <b>sample</b> <b>sizes.</b> The statistical framework developed herein is potentially applicable to a wide range of prediction problems in bioinformatics. </p...|$|R
40|$|The {{increasing}} size of {{the data}} samples recorded by the CDF and DO experiments at the Tevatron enables studies {{of a wide range}} of processes involving the electroweak bosons W and Z. Single boson production is now looked at in terms of differential cross sections such as rapidity or transverse momentum dependence. Diboson production cross-sections are several orders of magnitude smaller than single boson production cross-sections, but all combinations Wgamma, Zgamma, WW and WZ have been observed. ZZ production is expected at a rate just below the observation threshold with <b>current</b> data <b>sample</b> <b>sizes,</b> but this channel is expected to be accessible to the Tevatron experiments soon. Comment: 6 pages, 2 figures, to appear in the proceedings of Les Rencontres de Physique de La Vallee d'Aoste, La Thuile, 4 - 10 March 200...|$|R
40|$|Background: Confocal Laser Scanning Microscopy (CLSM) {{provides}} the opportunity to perform 3 D DNA content measurements on intact cells in thick histological sections. So far, <b>sample</b> <b>size</b> has been limited by the time consuming nature of the technology. Since the power of DNA histograms to resolve different stemlines depends on both the <b>sample</b> <b>size</b> and the coefficient of variation (CV) of histogram peaks, interpretation of 3 D CLSM DNA histograms might be hampered by both a small <b>sample</b> <b>size</b> and a large CV. The {{aim of this study}} was to analyze the required CV for 3 D CLSM DNA histograms given a realistic <b>sample</b> <b>size.</b> Methods: By computer simulation, virtual histograms were composed for <b>sample</b> <b>sizes</b> of 20000, 10000, 5000, 1000, and 273 cells and CVs of 30, 25, 20, 15, 10 and 5 %. By visual inspection, the histogram quality with respect to resolution of G 0 / 1 and G 2 /M peaks of a diploid stemline was assessed. Results: As expected, the interpretability of DNA histograms deteriorated with decreasing <b>sample</b> <b>sizes</b> and higher CVs. For CVs of 15 % and lower, a clearly bimodal peak pattern with well distinguishable G 0 / 1 and G 2 /M peaks were still seen at a <b>sample</b> <b>size</b> of 273 cells, which is our <b>current</b> average <b>sample</b> <b>size</b> with 3 D CLSM DNA cytometry. Conclusions: For unambiguous interpretation of DNA histograms obtained using 3 D CLSM, a CV of at most 15 % is tolerable at currently achievable <b>sample</b> <b>sizes.</b> To resolve smaller near diploid stemlines, a CV of 10 % or better should be aimed at. With currently available 3 D imaging technology, this CV is achievable...|$|R
