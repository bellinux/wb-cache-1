1|10000|Public
40|$|Mathematical {{models of}} the {{composite}} surface winding process are considered in the paper aiming at the development of methods, algorithms and a program complex for the geometrical modelling of the composite surface winding process and for the parameter calculation of an envelope, formed from composite materials. As a result the geometrical model {{of the process of}} the composite strip laying on the composite surface of the mandrell, having non-smooth ioints of compartments has been developed as well as methods of the winding parameter <b>calculation.</b> <b>Estimation</b> <b>methods</b> ologiey of the strip winding and fit have been also developed. Mathematical {{models of the}} winding process for composite surfaces, making it possible to form envelopes with complex technical surfaces have been elaborated. The software technical surfaces has been introduced into operation in the design office of composite materials in Ulan-Ude the algorithms may be used as the special mathematical and program module in systems of the design work automation. Results may find their field of application in enterprises, engaging in the automated design and production of composite material constructionsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Growth {{competition}} assays {{have been}} developed to quantify the relative fitnesses of human immunodeficiency virus (HIV- 1) mutants. In this article we develop mathematical models to describe viral/cellular dynamic interactions in the assay experiment, from which new competitive fitness indices or parameters are defined. These indices include the log fitness ratio (LFR), the log relative fitness (LRF), and the production rate ratio (PRR). From the population genetics perspective, we clarify the confusion and correct the inconsistency in the definition of relative fitness in the literature of HIV- 1 viral fitness. The LFR and LRF are easier to estimate from the experimental data than the PRR, which was misleadingly defined as the relative fitness in recent HIV- 1 research literature. <b>Calculation</b> and <b>estimation</b> <b>methods</b> based on two data points and multiple data points were proposed and were carefully studied. In particular, we suggest using both standard linear regression (method of least squares) and a measurement error model approach for more-accurate estimates of competitive fitness parameters from multiple data points. The developed methodologies are generally applicable to any growth competition assays. A user-friendly computational tool also has been developed and is publicly available on the World Wide Web at [URL]...|$|R
30|$|In this paper, {{we propose}} an <b>estimation</b> <b>method</b> that {{requires}} simple profilers or <b>calculations</b> for complexity <b>estimation</b> {{as well as}} a DVFS method that prevents frame drop and buffering, which is in contrast with the methods proposed in [2, 3]. Our proposed <b>estimation</b> <b>method</b> does complexity <b>estimation</b> with simple profilers or calculations by using the characteristic of multimedia content requiring the repeated processing of similar calculations.|$|R
5000|$|Eliminate mental <b>calculations,</b> <b>estimations,</b> comparisons, and {{unnecessary}} thinking.|$|R
40|$|The need to {{determine}} soil-air partitioning coefficients (K-SA) of low-volatility organic chemicals {{as a measure of}} their distribution in the soil surface after release into the environment resulted {{in the development of a}} novel chamber system, which has been filed for patent. A major advantage of this pseudo-static system is that sufficient time can be factored into the experiment to ensure that the system has achieved equilibrium. In a highly precise method, the air is collected in adsorption tubes and subsequently liberated in a thermodesorption system for the quantitation of the adsorbed compound. The precision of the method is great enough that even the effects of temperature and soil moisture on the soil-air partitioning of very low-volatility compounds can be quantified. Because of analytical detection limits, quantitation of these influences has not been possible to date. Functionality of the setup was illustrated by measurements on the fungicide fenpropimorph. K-SA values of fenpropimorph displayed a negative relationship with temperature and soil moisture. The type of application (spraying or incorporation) and the use of formulated compounds was shown to have a major impact on the measured K-SA values. Comparison with <b>calculations</b> using an <b>estimation</b> <b>method</b> revealed that the use of experimentally determined K-SA values will facilitate a more adequate consideration of volatilization in recent model approaches...|$|R
40|$|The {{objective}} {{of this study is}} to compare interval <b>estimation</b> <b>methods</b> for population means of positively skewed distributions. The <b>estimation</b> <b>methods</b> are the interval <b>estimation</b> <b>method</b> with student-t statistics, the interval <b>estimation</b> <b>method</b> with Johnson’s statistics, the interval <b>estimation</b> <b>method</b> with Hall’s statistics and the interval <b>estimation</b> <b>method</b> with Chen’s statistics. Log-normal distribution and Weibull distribution are considered. The measures of skewness under the consideration are 1. 0, 3. 0, 5. 0, respectively. The sample sizes are 10, 30, 50 and the confidence levels are 0. 95. The consideration has two steps. First, the confidence level of interval <b>estimation</b> <b>methods</b> are not lower than the determined confidence level value. The second is the comparision of mean of lower confidence limit, mean of upper confidence limit and mean of confidence interval length. The experimental data are generated by the Monte Carlo Simulation technique. The confidence level of interval <b>estimation</b> <b>method</b> with Bootstrap is higher than the non-boot-strap. The interval <b>estimation</b> <b>method</b> with Johnson’s statistics is the optimum <b>estimation</b> <b>method</b> for the upper confidence interval and two-tailed confidence interval. The interval <b>estimation</b> <b>method</b> with Chen’s statistics is the optimum <b>estimation</b> <b>method</b> for the lower confidence interval. Commonly, the confidence level of interval <b>estimation</b> <b>methods</b> for upper confidence interval are varied by the measure of skewness butthe confidence level of interval <b>estimation</b> <b>methods</b> for lower confidence interval and two-tailed confidence interval are converted by the measure of skewness. The mean of lower confidence limit is varied by the sample size, on the other hand, the mean of upper confidence limit and mean of confidence interval length are converted by the sample size...|$|R
40|$|Motion {{segmentation}} is {{a classic}} and on-going research topic which is an important pre-stage for many video processes. The reliability of the motion field calculation directly determines either {{success or failure of}} such segmentation. Due to the aperture property of optical flow <b>calculation,</b> motion <b>estimation</b> at moving object boundary remains a challenging ill-posed problem. In this paper, we propose a reliable opticalflow <b>estimation</b> <b>method</b> from the view of missing data reconstruction. Furthermore, to overcome the occlusion problem at motion boundaries, we apply a motion segmentation scheme which integrates with spatial segmentation. Experimental results of proposed method and other’s are presented...|$|R
30|$|In this section, we {{describe}} the widely used 2 -D channel <b>estimation</b> <b>method</b> and the general idea of DD channel <b>estimation</b> <b>method.</b>|$|R
40|$|ALIS. ?????????? ???????? ?????????? ???, ?? ????????? ????? ???????????, ?? ?? ??????? ???? ????????????? ??? ????????? 10 ??. A {{method for}} calculating the maximum {{detectable}} range (MDR) of thermal imager, which observes the object {{located on a}} background with uneven brightness, was developed. The model with the object on an uneven background is studied, which is equivalent to influence of variable flow on detector. The flow is considered as the sum of two components: a constant component that degrades the detector sensitivity and a variable component. The variable component, together with the intrinsic noise interferes with detector and determines the threshold flux of the thermal imager. If the variable component due to the fluctuations of the background luminance exceeds the detector intrinsic noise, it limits the MDR. Variable background flow that perceived by the detector is formed by the depth of modulation, which depends on the {{instantaneous field of view}} spatial integration and background variance. A general equation for calculating the MDR was obtained. This equation depends on the difference between object and background fluxes and does not depend on the area of the entrance pupil of thermal imager lens. Thermal imager ALIS was considered as an example of applying the MDR <b>calculation</b> <b>method.</b> <b>Estimation</b> of MDR with the calculating method under the given conditions of observation shows that MDR is 10 km away. ?????????? ????? ??????? ???????????? ????????? ??????????? (???) ???????????? ??????? ??????????, ?????????????? ?? ????, ??????? ????? ????????????? ??????? ? ???? ?????? ???????????. ??????????? ??????, ????? ???, ?? ??????? ????????????? ??????, ?????????????, ??? ??????????? ???????? ?? ???????? ????????? (??) ??????????? ??????????? ??????. ???? ????? ??????????????? ??? ????? ???? ????????????: ?????????? ????????????, ??????? ???????? ???????????????? ??, ? ?????????? ????????????. ?????????? ???????????? ?????? ? ???????????? ?????? ?? ??????? ??????, ??????? ?????????? ????????? ????? ???????????. ???? ?????????? ????????????, ????????????? ???????????? ??????? ????, ????????? ??????????? ???? ??, ?? ??? ???????????? ???. ?????????? ?????, ?????????????? ?? ?? ????, ????????? ???????? ?????????, ??????? ??????? ?? ????????????????? ?????????????? ?????????? ????? ?????? ???????????, ? ?????????? ????. ???????? ????? ????????? ??? ??????? ???, ????????? ?? ???????? ??????? ?? ??????? ? ????, ? ?? ????????? ?? ??????? ???????? ?????? ????????? ???????????. ? ???????? ??????? ?????????? ????????????? ?????? ??????? ??? ?????????? ?????????? ALIS. ?????????? ???????? ??????? ???, ? ??????? ???????? ???????????, ??? ??? ???????? ???????? ?????????? ??? ?????????? 10 ??...|$|R
3000|$|... is {{independent}} of channel <b>estimation</b> <b>methods.</b> Thus, {{if we want to}} decrease BER, we have to choose a channel <b>estimation</b> <b>method</b> that simultaneously maximizes |μ [...]...|$|R
40|$|In this study, {{compressed}} channel <b>estimation</b> <b>method</b> for sparse multipath two-way relay networks is investigated. Conventional <b>estimation</b> <b>methods,</b> e. g., Least Square (LS) and Minimum Mean Square Error (MMSE), {{are based}} on the dense assumption of relay channel and cannot exploit channel sparsity which has been verified by lots of channel measurements. Unlike the previous methods, we propose a compressed channel <b>estimation</b> <b>method</b> by using bi-sparse constraint which can exploit the sparsity and hence provide significant improvements in MSE performance when compared with conventional LS-based <b>estimation</b> <b>method.</b> Simulation results confirm the superiority of proposed method...|$|R
40|$|This paper {{provides}} a simple <b>estimation</b> <b>method</b> for an error component regression model with general MA(q) remainder disturbances. The <b>estimation</b> <b>method</b> utilizes the transformation derived by Baltagi and Li [3] for an error component model with autoregressive remainder disturbances, {{and a standard}} orthogonalizing algorithm for the general MA(q) model. This <b>estimation</b> <b>method</b> is computationally simple utilizing only least-squares regressions. This is important for panel data regressions where brute force GLS is in many cases not feasible. This <b>estimation</b> <b>method</b> performs well relative to true GLS in Monte-Carlo experiments. ...|$|R
40|$|The mutual {{information}} is useful {{measure of a}} random vector component dependence. It is important in many technical applications. The <b>estimation</b> <b>methods</b> are often based on the well known relation between the {{mutual information}} and the appropriate entropies. In 1999 Darbellay and Vajda proposed a direct <b>estimation</b> <b>methods.</b> In this paper we compare some available <b>estimation</b> <b>methods</b> using different 2 -D random distributions...|$|R
30|$|It is {{well known}} that the DD channel <b>estimation</b> <b>method</b> {{performs}} well in OFDM systems. Though a derivative of the OFDM, the OFDMA used in the WiMAX DL system is different in that subcarriers are partitioned into different groups destined for different MSs. To account for this difference, we propose an allocation method for WiMAX DL data bursts so that the destination MSs can apply DD channel estimation to these bursts without demodulating data bursts intended for other MSs. In addition, the DD channel <b>estimation</b> <b>method</b> must be adapted to account for the WiMAX frame structure. To this end, three DD channel <b>estimation</b> <b>methods</b> based on the proposed allocation method are suggested. Theoretical analysis strongly suggests that, among the three proposed <b>estimation</b> <b>methods,</b> the adaptive weighted-average <b>estimation</b> <b>method</b> may achieve the best performance. Simulation results confirm this for the four channel models considered. Moreover, in the worst case scenario where the channel suffers from a large delay spread, it is found that the proposed adaptive weighted-average <b>estimation</b> <b>method</b> significantly outperforms the widely used 2 -D <b>estimation</b> <b>method.</b> For the consideration of implementation, we also discuss various ways to reduce the complexity of the proposed adaptive algorithm. Overall, the proposed allocation method and the adaptive weighted-average <b>estimation</b> <b>method</b> {{can be applied to the}} WiMAX DL transmission to improve the BER performance for error-sensitive data, such as HARQ.|$|R
40|$|In {{confirmatory factor}} {{analysis}} (CFA), which is used quite often for scale development and adaptation studies, the selected <b>estimation</b> <b>method,</b> affects the results obtained from the data. Because of the selected <b>estimation</b> <b>method,</b> the model parameters and their standard errors, and the model data fit values may alter the results substantially. So that, {{the purpose of this}} research is to compare the performance of different <b>estimation</b> <b>methods</b> for CFA. Maximum likelihood (ML), unweighted least squares (ULS) and diagonally weighted least squares (DWLS) are used in this research as <b>estimation</b> <b>methods.</b> These methods are applied in data sets and regression coefficients and their standard errors, t values, fit indexes and iteration numbers obtained from these <b>estimation</b> <b>methods</b> are examined. As a result, ULS method can converge with the minimum number iterations {{and it seems to be}} the more accurate method for estimating the parameters...|$|R
40|$|Should we {{be using}} {{relative}} <b>estimation</b> <b>methods</b> in software effort estimation? This thesis looks into {{an aspect of}} agile estimating by comparing the effects of using relative <b>estimation</b> <b>methods</b> with absolute <b>estimation</b> <b>methods</b> for software development effort estimates. The thesis describes a study conducted on a software development project where estimates in story points (relative) and ideal time (absolute) are provided using planning poker...|$|R
40|$|Abstract. Robust <b>estimation</b> <b>method</b> in {{generalized}} Gaussian {{distribution of}} observations under obedience can effectively eliminate or reduce {{the influence of}} gross errors, however, peculiarity of different <b>estimation</b> <b>methods</b> are not the same. In this paper, it’s used simulation method, the commonly used 13 kinds of robust features robust <b>estimation</b> <b>methods</b> were compared. The results showed that: L 1 method, Danish method, German-McClure method and IGGIII program is more efficient robust <b>estimation</b> <b>methods</b> in Observations to obey generalized gaussian distribution, which method {{is more effective than}} other commonly used to eliminate the impact of robust estimation of gross errors or weaken...|$|R
40|$|This paper {{focuses on}} several <b>estimation</b> <b>methods</b> for SAR- models {{in case of}} missing {{observations}} in the dependent variable. First, we show with an example and then in general, how missing observations can change the model and thus resulting in {{the failure of the}} 'traditional' <b>estimation</b> <b>methods.</b> To estimate the SAR- model with missings we propose different <b>estimation</b> <b>methods,</b> like GMM, NLS and OLS. We will suggest to derive some of the estimators based on a model approximation. A Monte Carlo Simulation is conducted to compare the different <b>estimation</b> <b>methods</b> in their diverse numerical and sample size aspects...|$|R
40|$|This article studies <b>estimation</b> <b>methods</b> for the {{location}} parameter. We consider several robust location estimators {{as well as}} several <b>estimation</b> <b>methods</b> based on a phase I analysis, i. e., the use of a control chart to study a historical dataset retrospectively to identify disturbances. In addition, we propose a new type of phase I analysis. The <b>estimation</b> <b>methods</b> are evaluated in terms of their mean-squared errors and their effect on the X-bar control charts used for real-time process monitoring (phase II). It turns out that the phase I control chart based on the trimmed trimean far outperforms the existing <b>estimation</b> <b>methods.</b> This method has therefore proven to be very suitable for determining X-bar phase II control chart limits...|$|R
3000|$|This paper {{considered}} {{the detection of}} deterministic SCI in a baseband OFDM architecture. The detection performance of a time-domain correlation method is investigated and compared against the conventional ML <b>estimation</b> <b>method.</b> A key benefit of the time-domain <b>estimation</b> <b>method</b> is that it requires no channel estimation at the receiver. The detection performance of the time-domain <b>estimation</b> <b>method</b> {{is found to be}} largely dependent on the value of N [...]...|$|R
40|$|This paper {{presents}} a novel motion <b>estimation</b> <b>method</b> for mesh-based video motion tracking. The proposed method {{has been called}} mesh-based square-matching (MB-SM) motion <b>estimation</b> <b>method.</b> The MB-SM method outperforms the commonly used motion <b>estimation</b> <b>methods</b> in terms of computational cost reduction, efficiency and image quality (i. e. peak {{signal to noise ratio}} (PSNR)). It yields comparable PSNR values with the hexagonal matching method [1] while it has lower computation complexity. 1...|$|R
40|$|Abstract: In this study, {{compressed}} channel <b>estimation</b> <b>method</b> for sparse multipath two-way relay networks is investigated. Conventional <b>estimation</b> <b>methods,</b> e. g., Least Square (LS) and Minimum Mean Square Error (MMSE), {{are based}} on the dense assumption of relay channel and cannot exploit channel sparsity which has been verified by lots of channel measurements. Unlike the previous methods, we propose a compressed channel <b>estimation</b> <b>method</b> by using bi-sparse constraint which can exploit the sparsity and hence provide significant improvements in MSE performance when compared with conventional LS-based <b>estimation</b> <b>method.</b> Simulation results confirm the superiority of proposed method. Keywords: Channel State Information (CSI), compressive channel estimation, compressive sensing, spars...|$|R
50|$|Bayesian <b>estimation</b> <b>methods.</b>|$|R
3000|$|... (2)Reference [6] {{proposed}} a secure <b>estimation</b> <b>method</b> for steganographic capacity {{based on the}} DCT domain. It only proves the influence of image complexity on payload by doing some experiments but has not worked out the specific capacity <b>estimation</b> <b>method.</b>|$|R
30|$|The {{most widely}} used <b>method</b> for the <b>estimation</b> of {{parameters}} of distribution is the maximum likelihood <b>estimation</b> <b>method</b> (MLE) and the moment method. We employ the maximum likelihood <b>estimation</b> <b>method</b> MLE to estimate the unknown parameter of BBX distribution.|$|R
40|$|The {{increasing}} interest in applying small area <b>estimation</b> <b>methods</b> urges the needs for training in small area estimation. To {{better understand the}} behaviour of small area estimators in practice, simulations are a feasible way for evaluating and teaching properties of the estimators of interest. By designing such simulation studies, students gain {{a deeper understanding of}} small area <b>estimation</b> <b>methods.</b> Thus, we encourage to use appropriate simulations as an additional interactive tool in teaching small area <b>estimation</b> <b>methods...</b>|$|R
40|$|Abstract — In this paper, {{we present}} a {{distributed}} <b>estimation</b> <b>method</b> in wireless sensor networks (WSNs) based on decisions transmitted over Rayleigh fading channels. The fusion centre can uses either coherent receiver or non-coherent receiver to acquire decisions transmitted over Rayleigh fading channels. The <b>estimation</b> <b>method</b> using coherent receiver and the <b>estimation</b> <b>method</b> using non-coherent receiver are presented and the Cramer-Rao lower bounds (CRLBs) are derived. Simulation results showed that in ideal situations, the RMS errors given by the distributed <b>estimation</b> <b>method</b> were close to the CRLB. Moreover, simulation results highlighted {{the importance of the}} number of sensors, channel SNR, and accurate channel SNR information known to the fusion centre on estimation performance...|$|R
40|$|Conventional multiclass {{conditional}} probability <b>estimation</b> <b>methods,</b> such as Fisher's discriminate analysis and logistic regression, often require restrictive distributional model assumption. In this paper, a model-free <b>estimation</b> <b>method</b> is proposed to estimate multiclass {{conditional probability}} {{through a series}} of conditional quantile regression functions. Specifically, the conditional class probability is formulated as difference of corresponding cumulative distribution functions, where the cumulative distribution functions can be converted from the estimated conditional quantile regression functions. The proposed <b>estimation</b> <b>method</b> is also efficient as its computation cost does not increase exponentially with the number of classes. The theoretical and numerical studies demonstrate that the proposed <b>estimation</b> <b>method</b> is highly competitive against the existing competitors, especially when the number of classes is relatively large...|$|R
40|$|This paper {{describes}} a multiple time interval (“multi-interval”) parameter <b>estimation</b> <b>method.</b> The multi-interval parameter <b>estimation</b> <b>method</b> estimates a parameter {{from a new}} multi-interval prediction error polynomial that can simultaneously consider multiple time intervals. The root of the multi-interval prediction error polynomial includes the effect on each time interval, and the important mode can be estimated by solving one polynomial for multiple time intervals or signals. The algorithm of the multi-interval parameter <b>estimation</b> <b>method</b> proposed in this paper {{is applied to the}} test function and the data measured from a PMU (phasor measurement unit) installed in the KEPCO (Korea Electric Power Corporation) system. The results confirm that the proposed multi-interval parameter <b>estimation</b> <b>method</b> accurately and reliably estimates important parameters...|$|R
40|$|The {{paper is}} {{dedicated}} to the <b>estimation</b> <b>method</b> in accounting, especially to its basis, representatives and practical use. The paper is written from the theoretical definition of the generally accepted accounting principles, characterizing the accrual basis, the going concern and the prudence principle as a background of the <b>estimation</b> <b>method.</b> Furthermore, this work characterizes accruals and allowances as manifestations of the <b>estimation</b> <b>method.</b> Both expressions are first defined in general, than in the Czech legislation and there is outlined the approach in IAS / IFRS. Subsequently, the thesis analyzes the issue of accruals and allowances in the existing enterprise Pražské vodovody a kanalizace, a. s and focuses {{on the use of the}} <b>estimation</b> <b>method</b> in practice...|$|R
40|$|Background: Despite {{decades of}} research, {{there is no}} {{consensus}} on which software effort <b>estimation</b> <b>methods</b> produce the most accurate models. Aim: Prior work has reported that, given M <b>estimation</b> <b>methods,</b> no single method consistently outperforms all others. Perhaps rather than recommending one <b>estimation</b> <b>method</b> as best, it is wiser to generate estimates from ensembles of multiple <b>estimation</b> <b>methods.</b> Method: 9 learners were combined with 10 pre-processing options to generate 9 × 10 = 90 solo-methods. These were applied to 20 data sets and evaluated using 7 error measures. This identified the best n (in our case n = 13) solo-methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8 and 13 solo-methods were then combined to generate 12 multi-methods, which were then compared to the solo-methods. Results: (i) The top 10 (out of 12) multi-methods significantly out-performed all 90 solo-methods. (ii) The error rates of the multimethods were significantly less than the solo-methods. (iii) The ranking of the best multi-method was remarkably stable. Conclusion: While there is no best single effort <b>estimation</b> <b>method,</b> there exist best combinations of such effort <b>estimation</b> <b>methods...</b>|$|R
30|$|The maximum {{likelihood}} <b>estimation</b> <b>method</b> {{was used in}} this study for the parameter estimation of the optimal distribution. The basic principle of this method is as follows: assuming the known population distribution and an unknown parameter θ, one value θ̂ is chosen from all possible values, which can result in the maximal probability of the observed results. θ̂ is then defined as the {{maximum likelihood}} estimation value of θ, and the parameter <b>estimation</b> <b>method</b> was named as maximum likelihood <b>estimation</b> <b>method</b> [17].|$|R
40|$|This paper compares various {{estimation}} {{techniques used}} to determine the impact of distance and borders on international trade. The results consistently confirm the significantly negative distance effect, while the border effect, measured by evaluating whether intra-continental trade exceeds inter-continental trade, appears to be ambiguous and dependent on the <b>estimation</b> <b>method.</b> In addition, also the size of both effects varies substantially across <b>estimation</b> <b>methods.</b> Finally, the authors generally find that the estimations are in line with the respective weighting schemes of each <b>estimation</b> <b>method...</b>|$|R
40|$|Semiparametric minimum-distance <b>estimation</b> <b>methods</b> are {{introduced}} for {{the estimation of}} parametric or semiparametric econometric models. The semiparametric minimum-distance <b>estimation</b> <b>methods</b> share some familiar properties of the classical minimum-distance <b>estimation</b> <b>method.</b> However, they {{can be applied to}} the estimation of models with disagregated data. Asymptotic properties of the estimators are analyzed. Some goodness-of-fit test statistics {{are introduced}}. For the estimation of some econometric models, weighted minimum-distance estimators can be asymptotically efficient. The minimum-distance estimators are asympototically invariant with respect to some transformations...|$|R
40|$|Determining {{the fatigue}} {{properties}} (Manson-Coffin and Ramberg-Osgood parameters) for a steel material requires time consuming and expensive testing. In {{the early stages}} of a design process, it is not feasible to perform this testing. To help solve this problem numerous researchers have developed <b>estimation</b> <b>methods</b> to estimate the Manson-Coffin parameters from monotonic properties data. Additionally, other researchers have compared the results from these various <b>estimation</b> <b>methods</b> for large material classifications. However, a comprehensive comparison of these <b>estimation</b> <b>methods</b> has not been made for steels in different heat treatment states. More accurate results for the best <b>estimation</b> <b>method</b> can be made with smaller classifications, which have more consistent properties. In this research, best <b>estimation</b> <b>methods</b> are determined for six steel heat treatments. In addition to looking at steel heat treatment classifications, the estimation of the Ramberg-Osgood parameters is also examined through the compatibility conditions. Without them, the approach of estimating the fatigue properties using the <b>estimation</b> <b>methods</b> would not be practically useful. Finally, in the comparison of the <b>estimation</b> <b>methods,</b> an appropriate statistical comparison methodology is utilized; multiple contrasts comparison. This methodology is implemented into the comparison of the different <b>estimation</b> <b>methods,</b> by comparing the estimated lives and the experimental lives as a regression so that the entire life range can be considered. The <b>estimation</b> <b>methods</b> can also be utilized to get estimates of the variability of the fatigue properties given the variability of the monotonic properties data, since there is a functional relationship developed between the two sets of material properties. This variability is necessary for a stochastic design process, in order to obtain a more optimally designed component or structure. Overall the <b>estimation</b> <b>methods</b> have a number of practical applications within a fatigue design process. Their use and implementation needs to be supplemented by the appropriate knowledge of their limitations and for what classifications they give the best results. An expert system is developed to summarize this knowledge to assist an engineer. This research aims to provide this knowledge and expands their use to account for variability in fatigue properties for stochastic analysis. 1 yea...|$|R
40|$|Thermodynamics {{establishes}} equilibrium {{relations among}} thermodynamic parameters (“properties”) and delineates {{the effects of}} variation of the thermodynamic functions (typically temperature and pressure) on those parameters. However, classical thermodynamics does not provide values for the necessary thermodynamic properties, which must be established by extra-thermodynamic means such as experiment, theoretical <b>calculation,</b> or empirical <b>estimation.</b> While many values {{may be found in}} the numerous collected tables in the literature, these are necessarily incomplete because either the experimental measurements have not been made or the materials may be hypothetical. The current paper presents a number of simple and relible <b>estimation</b> <b>methods</b> for thermodynamic properties, principally for ionic materials. The results may also be used as a check for obvious errors in published values. The <b>estimation</b> <b>methods</b> described are typically based on addition of properties of individual ions, or sums of properties of neutral ion groups (such as “double” salts, in the Simple Salt Approximation), or based upon correlations such as with formula unit volumes (Volume-Based Thermodynamics) ...|$|R
