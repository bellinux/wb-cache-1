11|126|Public
50|$|In {{the early}} 1990s, Bronk Ramsey became {{interested}} in the application of Bayesian statistics to the analysis of radiocarbon data. In 1994, he authored OxCal, an online radiocarbon calibration program.Bronk Ramsey has made significant contributions to various chronological issues, including the Minoan eruption of Thera, the British Neolithic, the dispersal of modern humans out of Africa and the Egyptian chronology. His research interests also include the improvement of the radiocarbon <b>calibration</b> <b>record.</b> He {{is a member of the}} International Calibration (IntCal) group. His recent work has focused on improving the radiocarbon <b>calibration</b> <b>record</b> and synthesizing radiocarbon data with other chronometric information. In October 2012, Bronk Ramsey published the first wholly terrestrial radiocarbon <b>calibration</b> <b>record</b> extending back to the limit of the technique.|$|E
40|$|This work descripes the {{principles}} of RF spektrum analyzers and generators, introduces the technical specifications of specific instruments and describes the design of computer-controlled measuring system using generator N 9310 A and N 9320 A spectrum analyzer for measuring the value of transmission of two-port blocks. The practical part {{is focused on the}} realization of the program for the operation and measurement automation allowing to set various parameters, system <b>calibration,</b> <b>record,</b> display measured values and verification functions of different equivalent circuit...|$|E
40|$|This paper {{introduces}} a new method to calibrate data collected using Electro-Magnetic Articulography (EMA) into an appropriate articulatory space. A bite plate {{record for a}} target subject is used to define the maxillary occlusal and midsagittal planes, and then a single quaternion rotation is derived to transform the dataset into the new anatomically referenced space. The choice of specific rotation solution is discussed relative to the corresponding anatomical assumptions regarding the original sensor placement and coordinate system. Data were collected using NDI Wave Speech Research System for one pilot subject, and calibration results and consistency throughout the <b>calibration</b> <b>record</b> reviewed. The {{results show that the}} rotation solution can accurately and consistently transform all sensors ’ positions into an articulatory space in which sensor movements and orientations can be easily analyzed. This preliminary study enables the investigation of articulatory kinematics and relationship to acoustics. Index Terms — Electro-Magnetic Articulography, articulatory space, quaternion representation 1...|$|E
5000|$|Checking Aids When {{there are}} special tools for {{checking}} parts, this section shows {{a picture of}} the tool and <b>calibration</b> <b>records,</b> including dimensional report of the tool.|$|R
3000|$|Initially, the {{application}} [...] "Karkendamm.exe" [...] (Figure 1, ((FNI) FI 2013), (Crémer 2013)) reads configuration files, where camera settings and recording parameters were specified, and started animal identification, <b>calibration,</b> <b>recording,</b> and preprocessing.|$|R
5000|$|... #Subtitle level 3: Long-term <b>calibration</b> and <b>record</b> {{continuity}} ...|$|R
40|$|Radiocarbon dating is {{the most}} {{commonly}} used chronological tool in archaeological and environmental sciences dealing with the past 50, 000 years, making the radiocarbon calibration curve {{one of the most important}} records in paleosciences. For the past 12, 560 years, the radiocarbon calibration curve is constrained by high quality tree-ring data. Prior to this, however, its uncertainties increase rapidly due to the absence of suitable tree-ring 14 C data. Here, we present new high-resolution 14 C measurements from 3 floating tree-ring chronologies from the last deglaciation. By using combined information from the current radiocarbon calibration curve and ice core 10 Be records, we are able to absolutely date these chronologies at high confidence. We show that our data imply large 14 C-age variations during the Bølling chronozone (Greenland Interstadial 1 e) - a period that is currently characterized by a long 14 C-age plateau in the most recent IntCal 13 <b>calibration</b> <b>record.</b> We demonstrate that this lack of structure in IntCal 13 may currently lead to erroneous calibrated ages by up to 500 years...|$|E
40|$|An {{expanded}} Cariaco Basin 14 C chronology {{is tied to}} 230 Th-dated Hulu Cave speleothem {{records in}} order to provide detailed marine-based 14 C calibration for the past 50, 000 years. The revised, high-resolution Cariaco 14 C <b>calibration</b> <b>record</b> agrees well with data from 230 Th-dated fossil corals back to 33 ka, with continued agreement despite increased scatter back to 50 ka, suggesting that the record provides accurate calibration back to the limits of radiocarbon dating. The calibration data document highly elevated Delta 14 C during the Glacial period. Carbon cycle box model simulations show that the majority of observed Delta 14 C change can be explained by increased 14 C production. However, from 45 to 15 ka, Delta 14 C remains anomalously high, indicating that the distribution of radiocarbon between surface and deep ocean reservoirs was different than it is today. Additional observations of the magnitude, spatial extent and timing of deep ocean Delta 14 C shifts are critical for a complete understanding of observed Glacial Delta 14 C variability...|$|E
40|$|We {{observed}} cycles {{presented in}} a luminescent solar insolation proxy record from a speleothem from Jewel Cave, South Dakota, US. We found cycles of orbital precession with periods of 23 and 19 ka and of obliquity of 41 ka and many others from non- orbital origin in this sample. We determined the Solar origin of the cycles with durations of 11500, 4400, 3950, 2770, 2500, 2090, 1960, 1670, 1460, 1280, 1195, 1145, 1034, 935, 835, 750 and 610 years. It was done by their detection both in proxy records of speleothem luminescence, D 14 C {{and the intensity of}} the geomagnetic dipole. It is well known that the main variations in the last two records are produced by the solar wind. The most intensive cycle discovered in this record has duration of 11. 5 ka. It is not of orbital origin. It was found previously to be the most intensive cycle in the D 14 C <b>calibration</b> <b>record</b> and has been interpreted to be of terrestrial origin because “it is too strong tobe of solar origin”. Our studies suggest that it should be a solar cycle modulating the geomagnetic field and 14 C reversed production as the other solar cycles do...|$|E
5000|$|The Crown {{is under}} a {{constitutional}} duty to disclose evidence in its possession which relates to a criminal charge, {{as part of the}} accused's right to make full answer and defence, guaranteed by s 7 of the Charter. [...] This duty of disclosure requires the Crown to provide copies of maintenance and <b>calibration</b> <b>records</b> for the screening device ...|$|R
40|$|Despite several {{approaches}} to realize subject-to-subject transfer of pre-trained classifiers, the full {{performance of a}} Brain-Computer Interface (BCI) for a novel user can only be reached by presenting the BCI system with data from the novel user. In typical state-of-the-art BCI systems with a supervised classifier, the labeled data is collected during a <b>calibration</b> <b>recording,</b> in which the user is asked to perform a specific task. Based on the known labels of this recording, the BCI’s classifier can learn to decode the individual’s brain signals. Unfortunately, this <b>calibration</b> <b>recording</b> consumes valuable time. Furthermore, it is unproductive {{with respect to the}} final BCI application, e. g. text entry. Therefore, the calibration period must be reduced to a minimum, which is especially important for patients with a limited concentration ability. The main contribution of this manuscript is an online study on unsupervised learning in an auditory event-related potential (ERP) paradigm. Our results demonstrate that the <b>calibration</b> <b>recording</b> can be bypassed by utilizing an unsupervised trained classifier, that is initialized randomly and updated during usage. Initially, the unsupervised classifier tends to make decoding mistakes, as the classifier might not have seen enough data to build a reliable model. Using a constant re-analysis of the previously spelled symbols, these initially misspelled symbols can be rectified posthoc when the classifier has learned to decode the signals. We compare the spelling performance of our unsupervised approach and of the unsupervised posthoc approach to the standard supervised calibration-based dogma for n = 10 healthy users. To assess the learning behavior o...|$|R
50|$|Methods for {{extending}} the <b>calibration</b> and <b>record</b> continuity also {{make use of}} similar calibration activities et al., 2010.|$|R
40|$|The Landsat- 4 Thematic Mapper {{collected}} {{imagery of}} the Earth's surface from 1982 to 1993. Although largely overshadowed by Landsat 5, which was launched in 1984, Landsat 4 TM imagery extends the Thematic Mapper-based record of the Earth back to 1982 and also substantially supplements the image archive collected by Landsat 5. To provide a consistent <b>calibration</b> <b>record</b> for the TM instruments, Landsat 4 TM was cross-calibrated to Landsat 5 using nearly simultaneous overpass imagery of pseudo-invariant calibration sites (PICS) in the time period of 1988 through 1990. To determine if the radiometric gain of Landsat 4 had changed over its lifetime, time series from two PICS locations, a Saharan site known as Libya 4 and a site in southwest North America, {{commonly referred to as}} the Sonoran Desert PICS, were developed. Results indicated that Landsat 4 had been very stable over its lifetime with no discernible degradation in sensor performance in all the reflective bands except band 1. In contrast, band 1 exhibited a 12 % decay in responsivity over the lifetime of the instrument. Results from this work have been implemented at USGS EROS, which enables users of Landsat TM data sets to obtain consistently calibrated data from Landsat 4 and 5 TM as well as Landsat 7 ETM+ instruments...|$|E
40|$|This {{investigation}} {{is motivated by}} the current need for a detailed post launch calibration of the Thematic Mapper (TM) thermal band (Band 6), aboard NASA’s Landsat 5 spacecraft. The historical calibration spans the period from 1984 to 2007. It is through fusion of environmental data sources (i. e. buoy observations, surface observations, and radiosonde observations) that a vicarious calibration approach will be implemented to construct the complete <b>calibration</b> <b>record</b> of the Landsat 5 TM thermal band. The vicarious calibration process {{takes advantage of the}} long standing National Data Buoy Center (NDBC) moored buoy fleet to acquire historic ground truth measurements needed over the lifetime of Landsat 5. These measurements are propagated to the sensor through the use of physics based models to establish a predicted at sensor radiance. Through comparison of the predicted at sensor radiance and the actual sensor observed radiance, a calibration metric is established. Results indicate the Landsat 5 TM thermal band, originally planned for a 3 year mission, has fluctuated only slightly (1 K) over the 24 + years in orbit. The calibration curve developed in this study is consistent with previous results from campaigns preformed in 1985 and post 1999. The data indicated that the sensor exhibited a clear gain issue (i. e. over estimates low radiance targets and under estimates high radiance targets) found to be approximately consistent over time. Additionally, an event occurring either prior to or during 1999, caused a discernible fluctuation in sensor performance (i. e. dominant cold bias) for all data post 1999. It is the recommendation of this vicarious calibration I II campaign that a linear (Dual: slope 2 ̆ 6 intercept) correction be applied to the Landsat 5 data archive. As a result of the correction, the Landsat 5 TM Band 6 is radiometrically calibrated to within ± 0. 488 K, in reference to a 300 K blackbody. This result was verified through an extensive error propagation analysis, which found the proposed methodology to have an expected error of 0. 454 K. The proposed methodology was also verified by a comparison study to the traditional approach (i. e. non buoy derived ground truth) using the closely monitored and trusted Landsat 7 data calibrated using the traditional approach. The comparison found the two methods were not statistically different, which offered the confidence that this methodology could be applied successfully over the domain of this study. This comparison not only validates the <b>calibration</b> <b>record</b> of Landsat 5, but also demonstrates the utility of the method in future efforts. This work has demonstrated that a successful historical vicarious calibration campaign can be conducted using exclusively free and easily accessible data. It has been established that the proposed methodology can be implemented to achieve a high level of radiometric integrity, which includes both historic and future efforts, in the calibration of remote thermal infrared systems...|$|E
40|$|Sub-seasonal streamflow {{forecasts}} {{provide useful}} information {{for a range}} of water resource management and planning applications. This work has focused on improving forecasts for one such application: the management of water available in an open channel drainage network to maximise environmental and social outcomes in a region in southern Australia. Conceptual rainfall-runoff models with a postprocessor error model for uncertainty analysis were applied to provide forecasts of monthly streamflow. Two aspects were considered to improve the accuracy of the forecasts: 1) state updating to force the models to match observations {{from the start of the}} forecast period, and 2) selection of a calibration period representative of the forecast period. Five metrics were used to assess forecast performance, representing the reliability, precision, bias and skill of the forecasts produced, using both observed and forecast climate data. The results indicate that assimilating observed streamflow data into the model, by updating the storage level at the start of a forecast period, improved the performance of the forecasts across the metrics when compared to an approach that “warmed up” the storage levels using historical climate data. The shorter calibration period improved the performance of the forecasts, particularly for a catchment that was expected to have experienced a change in the rainfall-runoff relationship in the past. The results highlight the importance of identifying a <b>calibration</b> <b>record</b> representative of the expected forecast conditions, and if this step is ignored degradation of predictive performance can result. Matthew S. Gibbs, David McInerney, Greer Humphrey, Mark A. Thyer, Holger R. Maier, Graeme C. Dandy and Dmitri Kavetsk...|$|E
40|$|The {{smoothness}} of {{the pavement}} surface approaching a Weigh-In-Motion (WIM) scale directly affects the device’s ability to accurately estimate static loads from measured dynamic forces. Lack of smoothness creates difficulties in obtaining an acceptable weighing error. Efforts {{were made in}} this study to determine appropriate threshold values of profile length and roughness approaching the WIM. Roughness data for 19 WIM sites across Texas were collected, and the corresponding <b>calibration</b> <b>records</b> from those WIM sites were employed to verify the smoothness threshold values proposed by AASHTO MP 14 and ASTM E 1318. Since there are multiple lanes at each WIM site, a total number of 70 profiles was surveyed. Based on the <b>calibration</b> <b>records</b> from the WIM sites and field roughness measurements, {{it was found that}} the upper threshold value of the Long Range Index (LRI) of AASHTO method MP 14 needs to be adjusted in order to screen out the locations that yield unacceptable weighing error. The lower threshold value was found to be unnecessary because more than 90 % of WIM sites yielded acceptable weighing error even when exceeding the lower threshold value. The LRI is preferable to the Short Range Index (SRI) as it matched well with the field WIM calibration results. Based on the 70 profile...|$|R
50|$|This is a {{simplified}} example. The mathematics of the example can be challenged. It {{is important that}} whatever thinking guided this process in an actual <b>calibration</b> be <b>recorded</b> and accessible. Informality contributes to tolerance stacks and other difficult to diagnose post calibration problems.|$|R
40|$|The primary job of an Instrumentation and Data Acquisition System (DAS) Engineer is to {{properly}} measure physical phenomenon of hardware using appropriate instrumentation and DAS equipment designed to record data during a specified {{test of the}} hardware. A DAS system includes a CPU or processor, a data storage device such as a hard drive, a data communication bus such as Universal Serial Bus, software to control the DAS system processes like <b>calibrations,</b> <b>recording</b> of data and processing of data. It also includes signal conditioning amplifiers, and certain sensors for specified measurements. My internship responsibilities have included testing and adjusting Pacific Instruments Model 9355 signal conditioning amplifiers, writing and performing checkout procedures, writing and performing calibration procedures while learning the basics of instrumentation...|$|R
40|$|The unique {{position}} of the Deep Space Climate Observatory (DSCOVR) Earth Polychromatic Imaging Camera (EPIC) at the Lagrange 1 point makes an important addition to the data from currently operating low Earth orbit observing instruments. EPIC instrument {{does not have an}} onboard calibration facility. One approach to its calibration is to compare EPIC observations to the measurements from polar-orbiting radiometers. Moderate Resolution Imaging Spectroradiometer (MODIS) is a natural choice for such comparison due to its well-established <b>calibration</b> <b>record</b> and wide use in remote sensing. We use MODIS Aqua and Terra L 1 B 1  km reflectances to infer calibration coefficients for four EPIC visible and NIR channels: 443, 551, 680 and 780  nm. MODIS and EPIC measurements made between June 2015 and 2016 are employed for comparison. We first identify favorable MODIS pixels with scattering angle matching temporarily collocated EPIC observations. Each EPIC pixel is then spatially collocated to a subset of the favorable MODIS pixels within 25  km radius. Standard deviation of the selected MODIS pixels {{as well as of the}} adjacent EPIC pixels is used to find the most homogeneous scenes. These scenes are then used to determine calibration coefficients using a linear regression between EPIC counts s − 1 and reflectances in the close MODIS spectral channels. We present thus inferred EPIC calibration coefficients and discuss sources of uncertainties. The lunar EPIC observations are used to calibrate EPIC O 2 absorbing channels (688 and 764  nm), assuming that there is a small difference between moon reflectances separated by [*]∼[*]  10  nm in wavelength and provided the calibration factors of the red (680  nm) and NIR (780  nm) are known from comparison between EPIC and MODIS...|$|E
40|$|In this study, {{we sought}} {{to address the issue}} of how to derive an {{extended}} synthetic record of fire incidence and timing at regional scale that would be representative of a short remotely-sensed <b>calibration</b> <b>record.</b> We used annual rainfall and simulated annual ground stratum growth to develop multiple regression relationships for prediction of annual fire probability and proportion of late (August-November) fires from AVHRR NDVI fire footprint data across Australian tropical savannas. Relationships were examined using spatial averaging in moving windows varying from 3 X 3 to 61 X 61 pixels in size. Model fits as measured by R 2 improved as window size increased, but output layers became smoother and less representative of natural heterogeneity. A 25 X 25 pixel window was selected as the best compromise between model fit and smoothing. A 113 -year synthetic record of annual fire probability and proportion of late fires was generated using the spatially explicit layers of model coefficients. The statistical properties of the synthetic fire probabilities were compared with those derived from the available fire footprint record, using a simple vegetation classification based on ground stratum type for spatial stratification. The two data sets showed a strong correspondence for both burned area and fire probability; spatial variation in mean and coefficient of variation of fire probability was representative of that observed in the historical record. There was significant temporal variation in the synthetic annual fire probability for different vegetation zones across the tropical savanna region for the full 113 -year length of record. This simple approach could readily be applied to other areas of the world provided rainfall data are available and annual ground stratum growth can be simulated with a suitable model or estimated with remote sensing. Crown Copyright (c) 2005 Published by Elsevier Ltd. All rights reserved...|$|E
40|$|The paper {{describes}} a procedure for accurately and speedily calibrating tanks {{used for the}} chemical processing of nuclear materials. The procedure features the use of (1) precalibrated vessels certified to deliver known volumes of liquid, (2) calibrated linear measuring devices, and (3) a digital computer for manipulating data and producing printed <b>calibration</b> information. <b>Calibration</b> <b>records</b> of the standards are traceable to primary standards. Logic is incorporated in the computer program to accomplish curve fitting and perform the tests to accept or to reject the calibration, based on statistical, empirical, and report requirements. This logic {{is believed to be}} unique. "U. S. Atomic Energy Commission Contract AT(29 - 1) - 1106. ""RFP- 488; UC- 38 Engineering and Equipment; TID- 4500 (40 th Ed.). "" 1 July 65. "Includes bibliographical references (page 23). The paper {{describes a}} procedure for accurately and speedily calibrating tanks used for the chemical processing of nuclear materials. The procedure features the use of (1) precalibrated vessels certified to deliver known volumes of liquid, (2) calibrated linear measuring devices, and (3) a digital computer for manipulating data and producing printed <b>calibration</b> information. <b>Calibration</b> <b>records</b> of the standards are traceable to primary standards. Logic is incorporated in the computer program to accomplish curve fitting and perform the tests to accept or to reject the calibration, based on statistical, empirical, and report requirements. This logic is believed to be unique. Mode of access: Internet. This bibliographic record is available under the Creative Commons CC 0 "No Rights Reserved" license. The University of Florida Libraries, as creator of this bibliographic record, has waived all rights to it worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law...|$|R
40|$|Dieser Beitrag ist mit Zustimmung des Rechteinhabers aufgrund einer (DFG geförderten) Allianz- bzw. Nationallizenz frei zugänglich. This {{publication}} is {{with permission}} of the rights owner freely accessible due to an Alliance licence and a national licence (funded by the DFG, German Research Foundation) respectively. This contribution reviews how usability in Brain- Computer Interfaces (BCI) can be enhanced. As an example, an unsupervised signal processing approach is presented, which tackles usability by an algorithmic improvement {{from the field of}} machine learning. The approach completely omits the necessity of a <b>calibration</b> <b>recording</b> for BCIs based on event-related potential (ERP) paradigms. The positive effect is twofold - first, the experimental time is shortened and the productive online use of the BCI system starts as early as possible. Second, the unsupervised session avoids the usual paradigmatic break between calibration phase and online phase, which is known to introduce data-analytic problems related to non-stationarity...|$|R
40|$|There {{is a step}} of {{significant}} difficulty experienced by brain-computer interface (BCI) users when going from the <b>calibration</b> <b>recording</b> to the feedback application. This effect has been previously studied and a supervised adaptation solution has been proposed. In this paper, we suggest a simple unsupervised adaptation method of the linear discriminant analysis (LDA) classifier that effectively solves this problem by counteracting the harmful effect of nonclass-related nonstationarities in electroencephalography (EEG) during BCI sessions performed with motor imagery tasks. For this, we first introduce three types of adaptation procedures and investigate them in an offline study with 19 datasets. Then, we select one of the proposed methods and analyze it further. The chosen classifier is offline tested in data from 80 healthy users and four high spinal cord injury patients. Finally, {{for the first time}} in BCI literature, we apply this unsupervised classifier in online expe riments. Additionally, we show that its performance is significantly better than the state-of-the-art supervised approach...|$|R
5000|$|Audiometry (hearing test). This must be {{undertaken}} using equipment that {{conforms to the}} appropriate standards and guidelines specified with regular <b>recorded</b> <b>calibration</b> and maintenance. The subject should meet specific standards.|$|R
40|$|Designing a {{self-paced}} brain computer interface (BCI) {{that works}} reliably across human subjects {{has been a}} challenge. Of particular interest are simple BCIs that enable detection of an Intentional Control (IC) state {{against a background of}} No Control (NC) state. Such BCIs are known as brain switches or BCI switches. One of the possible methods to build a BCI switch is based on the consistent increase in the alpha component of the EEG spectrum when subjects close their eyes. The present work proposes a simple approach to achieve automatic user customization with just one minute of <b>calibration</b> <b>recording.</b> This design is evaluated based on thirty trials on each of the seven healthy subjects. The results validate the robustness of the proposed IC detection approach to variations across sessions and subjects. The average time required to activate the switch is typically two to three seconds after eye closure. These results suggest that any user with the ability to modulate their alpha rhythm by eye closure can effectively utilize the proposed BCI switch as an assistive technology to gain increased control over their environment...|$|R
50|$|On October 8, 2007 the EP Omar Rodriguez-Lopez & Lydia Lunch, a {{collaboration}} with spoken word poet Lydia Lunch, was released. The Apocalypse Inside of an Orange {{is a double}} LP featuring the original quintet and was released on vinyl November 20, 2007. It was also released for digital download. <b>Calibration,</b> a <b>record</b> that Rodríguez-López recorded during his stay in Amsterdam, was released February 5, 2008. It was described as being influenced by electronic music and acid-jazz.|$|R
40|$|At {{the request}} of the Minnesota Power, Inc., the Energy & Environmental Research Center (EERC) sampled for lead at the stack (or duct {{directly}} leading to the stack) for three units at the Boswell Energy Center. All sampling was done in triplicate using U. S. Environmental Protection Agency (EPA) Method 12, with sampling procedures following EPA Methods 1 through 4. During the test program, lead sampling was done using EPA Method 12 in the duct at the outlet of the baghouse serving Unit 2 and the duct at the outlet of the wet particulate scrubber serving Unit 3. For Unit 4, lead sampling was done at the stack. The specific objective for the project was to determine the concentration of lead in the flue gas being emitted into the atmosphere from the Boswell Energy Center. The test program was performed during the period of May 8 through 11, 2000. This report presents the test data, sample calculations, and results, and a discussion of the lead sampling performed at the Boswell Energy Center. The detailed test data and test results, raw test data, process data, laboratory reports, and equipment <b>calibration</b> <b>records</b> are provided in Appendices A, B, and C...|$|R
40|$|We {{present a}} novel {{algorithm}} for efficient removal of rolling shutter distortions in uncalibrated streaming videos. Our proposed method is calibration free {{as it does}} not need any knowledge of the camera used, nor does it require <b>calibration</b> using specially <b>recorded</b> <b>calibration</b> sequences. Our algorithm can perform rolling shutter removal under varying focal lengths, as in videos from CMOS cameras equipped with an optical zoom. We evaluate our approach across {{a broad range of}} cameras and video sequences demonstrating robustness, scaleability, and repeatability. We also conducted a user study, which demonstrates preference for the output of our algorithm over other state-of-the art methods. Our algorithm is computationally efficient, easy to parallelize, and robust to challenging artifacts introduced by various cameras with differing technologies. 1. ...|$|R
40|$|Multiple {{channels}} of electromyogram activity are frequently transduced via electrodes, then combined electronically to form one electrophysiologic recording, e. g. bipolar, linear double difference and Laplacian montages. For high quality recordings, precise gain and frequency response matching {{of the individual}} electrode potentials is achieved in hardware (e. g., an instrumentation amplifier for bipolar recordings). This technique works well {{when the number of}} derived signals is small and the montages are pre-determined. However, for array electrodes employing a variety of montages, hardware channel matching can be expensive and tedious, and limits the number of derived signals monitored. This report describes a method for channel matching based on the concept of equalization filters. Monopolar potentials are recorded from each site without precise hardware matching. During a calibration phase, a time-varying linear chirp voltage is applied simultaneously to each site and recorded. Based on the <b>calibration</b> <b>recording,</b> each monopolar channel is digitally filtered to “correct ” for (equalize) differences in the individual channels, and then any derived montages subsequently created. In a hardware demonstration system, the common mode rejection ratio (at 60 Hz) of bipolar montages improved from 35. 2 ± 5. 0 dB (prior to channel equalization) to 69. 0 ± 5. 0 dB (after equalization) ...|$|R
40|$|In 12 {{chapters}} (Part I) this {{extension to}} the two 'The Sound of Silence' editions covers the development, calculation, construction and measurement of the fully differential (= balanced) phono-amp solution 'RIAA Phono-Amp Engine II'. Additionally, the balanced measurement amplifiers & measurement tools, the discussion on BJT gain stages, the 1 /f noise calculation methods for BJTs, the calculation of fully-differential amplifiers, the numerous Mathcad worksheets, and the presentation of test and <b>calibration</b> <b>records</b> fill a further 10 chapters of Part II with essential knowhow that will equip the reader to develop his/her own phono-amp solution in the most balanced way - and of course, as low-noise as possible. Engine II offers eight different amplifying paths from the Engine's input to its output - solid-state as well as valve driven. To expand the input possibilities via an additional external input, any kind of other linear input amplifiers can be connected. Further, a selection of six highly diverse external input amplifier examples are discussed - BJT, valve, transformer and JFET-driven variants. Although the book primarily focuses on MC cartridge amplification, MM cartridge considerations are not neglected: two further chapters in Part II address methods for increasing the signal-to-noise ratios of MM phono-amps. ...|$|R
40|$|While {{the concept}} of the pH {{measurement}} that was first published 100 years ago remains essentially the same, significant technological improvements in glass formulation and construction and reference electrode design have increased pH electrode life and performance. Considerable experience has been gained in terms of electrode installation methods for difficult process conditions as the scope of pH applications has expanded Yet much of this practical expertise has not been documented and the ability of embedded diagnostics and <b>calibration</b> <b>records</b> and wireless devices to eliminate common problems and provide smarter and more effective maintenance has not been explored. This paper addresses the essential aspects of each major phase of a successful pH measurement implementation from selection, installation, calibration, to performance monitoring. The relative merits of various measurement types and principles, application strong points and watch-outs, installation practices, factory and field calibration methods, wiring and grounding problems, troubleshooting, and online diagnostics are concisely discussed. The examples focus on demanding industrial applications that require 0. 01 to 0. 02 pH accuracies despite harsh conditions from sterilization-in-place in biopharmaceutical processes and high reaction temperatures, salt, polymer, and crystal concentrations in chemical processes. The paper concludes with recent test results of wireless pH measurements in a bioreactor and looks forward into future improvements...|$|R
40|$|American Sign Language (ASL) {{generation}} {{software can}} improve the accessibility of information and services for deaf individuals with low English literacy. The understandability of current ASL systems is limited; they have been constructed {{without the benefit of}} annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, <b>calibration,</b> and <b>recording</b> protocol, we conducted an evaluation study with native ASL signers. ...|$|R
40|$|PM 10 {{monitoring}} {{networks are}} equipped with heterogeneous samplers. Some of these samplers are known to underestimate true levels of concentrations (non-reference samplers). In this {{paper we propose a}} hierarchical spatio-temporal Bayesian model for the <b>calibration</b> of measurements <b>recorded</b> using non-reference samplers, by borrowing strength from non co-located reference sampler measurements...|$|R
40|$|The {{airborne}} {{gamma ray}} survey recorded more than 40, 000 scintillation spectra and 20, 000 spectra from semiconductor detectors. The vehicular survey produced a further 1346 and 763 spectral sets respectively. The installation, <b>calibration,</b> <b>recording</b> and analysis followed SURRC procedures {{which have been}} developed and validated over many years and are fully documented. Pre flight checks on detector performance for energy calibration, energy resolution and sensitivity were performed on a daily basis. Background readings over water were taken on a daily basis. All data were registered and backed up in duplicate to form a digital archive of the survey. Subsequent analysis and mapping has used a combination of standard procedures established over many years, and new techniques developed to analyse the low energy spectra. All results have been retained to facilitate traceability and further analysis in the future. The sensitivity of the aircraft and vehicle were also checked at Greenham Common by collecting a set of 31 core samples for independent laboratory analysis. The key points arising from the airborne survey of the entire area show {{that there has been}} sufficient sensitivity to record variations in the natural background. The levels of 137 Cs are consistent with weapons' testing fallout, and are substantially lower than {{in other parts of the}} UK and Europe. The average levels of K (0. 5...|$|R
40|$|The C- 14 dating {{method is}} the {{cornerstone}} for inferring age estimates for natural archives covering the last 50 000 yrs. However, C- 14 age calibration for {{the last ice age}} relies mostly on records that only indirectly reflect the atmospheric C- 14 concentrations. In consequence, calendar age estimates are significantly more uncertain for the period of the last ice age compared to the past 14000 yrs where tree-ring based <b>calibration</b> <b>records</b> exist. Here we connect a C- 14 tree-ring chronology from Kauri trees in New Zealand to ice core Be- 10 records via the common signal in the galactic cosmic ray flux around the period of the Laschamp geomagnetic field minimum (ca. 41 000 yrs BP). Synchronous changes of modelled C- 14 and C- 14 inferred from U/Th-dated speleothems support the ice core chronology independently and suggest that the published ice core time scale errors are rather conservative for this period. Our analysis puts C- 14 age determinations directly into the context of ice core climate records and it shows that the C- 14 records underlying the C- 14 calibration curve overestimate the atmospheric C- 14 concentration by more than 200 parts per thousand. Consequently, C- 14 age calibration presently yields too old calendar age estimates by about 1200 yrs for this period. (C) 2014 Elsevier B. V. All rights reserved...|$|R
40|$|Abstract: PM 10 {{monitoring}} {{networks are}} equipped with heterogeneous samplers. Some of these samplers are known to underestimate true levels of concentrations (non-reference samplers). In this {{paper we propose a}} hierarchical spatio-temporal Bayesian model for the <b>calibration</b> of measurements <b>recorded</b> by non-reference samplers by borrowing strength from non co-located reference sampler measurements...|$|R
