578|1|Public
5|$|In recent years, {{this latter}} {{argument}} has been fortified by {{the theory of}} <b>connectionism.</b> Many connectionist models of the brain have been developed in which the processes of language learning {{and other forms of}} representation are highly distributed and parallel. This would tend to indicate that {{there is no need for}} such discrete and semantically endowed entities as beliefs and desires.|$|E
5|$|Fodor {{also has}} {{positive}} arguments {{in favour of}} the reality of mental representations in terms of the LOT. He maintains that if language is the expression of thoughts and language is systematic, then thoughts must also be systematic. Fodor draws on the work of Noam Chomsky to both model his theory of the mind and to refute alternative architectures such as <b>connectionism.</b> Systematicity in natural languages was explained by Chomsky in terms of two more basic concepts: productivity and compositionality.|$|E
5|$|In the 70s, AI {{was subject}} to critiques and {{financial}} setbacks. AI researchers had failed to appreciate {{the difficulty of the}} problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the field of <b>connectionism</b> (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons.|$|E
25|$|In the mid-1980s, {{parallel}} {{distributed processing}} became popular under the name <b>connectionism.</b> Rumelhart and McClelland (1986) described the use of <b>connectionism</b> to simulate neural processes.|$|E
25|$|Theoretical {{approaches}} {{to explain how}} mind emerges from the brain include <b>connectionism,</b> computationalism and Bayesian brain.|$|E
25|$|<b>Connectionism</b> {{is the use}} of {{artificial}} neural networks to model specific cognitive processes using what are considered to be simplified but plausible models of how neurons operate. Once trained to perform a specific cognitive task these networks are often damaged or 'lesioned' to simulate brain injury or impairment in an attempt to understand and compare the results to the effects of brain injury in humans.|$|E
25|$|Computational {{modeling}} {{is a tool}} used in mathematical {{psychology and}} cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. <b>Connectionism</b> uses neural networks to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.|$|E
25|$|Research in AI is {{concerned}} with producing machines to automate tasks requiring intelligent behavior. Examples include control, planning and scheduling, the ability to answer diagnostic and consumer questions, handwriting, natural language, speech and facial recognition. As such, the study of AI has also become an engineering discipline, focused on providing solutions to real life problems, knowledge mining, software applications, strategy games like computer chess and other video games. One of the biggest limitations of AI is {{in the domain of}} actual machine comprehension. Consequentially natural language understanding and <b>connectionism</b> (where behavior of neural networks is investigated) are areas of active research and development.|$|E
500|$|There {{are several}} {{varieties}} of eliminative materialism, but all maintain that our common-sense [...] "folk psychology" [...] badly misrepresents {{the nature of}} some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or <b>connectionism</b> {{will prove to be}} a much more accurate account of how the brain works.|$|E
500|$|In 1982, {{physicist}} John Hopfield {{was able}} to prove that a form of neural network (now called a [...] "Hopfield net") could learn and process information in a completely new way. Around the same time, David Rumelhart popularized a new method for training neural networks called [...] "backpropagation" [...] (discovered years earlier by Paul Werbos). These two discoveries revived the field of <b>connectionism</b> which had been largely abandoned since 1970.|$|E
500|$|In the 1980s {{a form of}} AI {{program called}} [...] "expert systems" [...] was adopted by {{corporations}} {{around the world and}} knowledge became the focus of mainstream AI research. In those same years, the Japanese government aggressively funded AI with its fifth generation computer project. Another encouraging event in the early 1980s was the revival of <b>connectionism</b> in the work of John Hopfield and David Rumelhart. Once again, AI had achieved success.|$|E
500|$|A {{perceptron}} {{was a form}} {{of neural}} network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that [...] "perceptron may eventually be able to learn, make decisions, and translate languages." [...] An active research program into the paradigm was carried out throughout the 60s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in <b>connectionism</b> for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.|$|E
500|$|In 1988 Pinker and Alan Prince {{published}} an influential critique of a connectionist {{model of the}} acquisition of the past tense (a textbook problem in language acquisition), followed {{by a series of}} studies of how people use and acquire the past tense. This included a monograph on children's regularization of irregular forms and his popular 1999 book, [...] Pinker argued that language depends on two things, the associative remembering of sounds and their meanings in words, and the use of rules to manipulate symbols for grammar. He presented evidence against <b>connectionism,</b> where a child would have to learn all forms of all words and would simply retrieve each needed form from memory, in favour of the older alternative theory, the use of words and rules combined by generative phonology. He showed that mistakes made by children indicate the use of default rules to add suffixes such as [...] "-ed": for instance 'breaked' and 'comed' for 'broke' and 'came'. He argued that this shows that irregular verb-forms in English have to be learnt and retrieved from memory individually, and that the children making these errors were predicting the regular [...] "-ed" [...] ending in an open-ended way by applying a mental rule. This rule for combining verb stems and the usual suffix can be expressed as ...|$|E
2500|$|Computational {{neuroscience}} {{is distinct}} from psychological <b>connectionism</b> and from learning theories of disciplines such as machine learning, neural networks, and computational learning theory in that it emphasizes descriptions of functional and biologically realistic neurons (and neural systems) and their physiology and dynamics. [...] These models capture the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, proteins, and chemical coupling to network oscillations, columnar and topographic architecture, and learning and memory.|$|E
2500|$|Artificial neural {{networks}} (ANNs), {{a form of}} <b>connectionism,</b> are computing systems inspired by the biological {{neural networks}} that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as [...] "cat" [...] or [...] "no cat" [...] and using the analytic results to identify cats in other images. They have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming.|$|E
2500|$|Part of {{the problem}} was the kind of {{philosophy}} that Dreyfus used in his critique. Dreyfus was an expert in modern European philosophers (like Heidegger and Merleau-Ponty). AI researchers of the 1960s, by contrast, based their understanding of the human mind on engineering principles and efficient problem solving techniques related to management science. On a fundamental level, they spoke a different language. Edward Feigenbaum complained, [...] "What does he offer us? Phenomenology! That ball of fluff. That cotton candy!" [...] In 1965, there was simply too huge a gap between European philosophy and artificial intelligence, a gap that has since been filled by cognitive science, <b>connectionism</b> and robotics research. It would take many years before artificial intelligence researchers were able to address the issues that were important to continental philosophy, such as situatedness, embodiment, perception and gestalt.|$|E
50|$|In the mid-1980s, {{parallel}} {{distributed processing}} became popular under the name <b>connectionism.</b> Rumelhart and McClelland (1986) described the use of <b>connectionism</b> to simulate neural processes.|$|E
50|$|These {{findings}} also relate to <b>Connectionism.</b> <b>Connectionism</b> attempts {{to model the}} cognitive language processing of the human brain, using computer architectures that make associations between elements of language, based on frequency of co-occurrence in the language input. Frequency {{has been found to}} be a factor in various linguistic domains of language learning. <b>Connectionism</b> posits that learners form mental connections between items that co-occur, using exemplars found in language input. From this input, learners extract the rules of the language through cognitive processes common to other areas of cognitive skill acquisition. Since <b>connectionism</b> denies both innate rules and the existence of any innate language-learning module, L2 input is of greater importance than it is in processing models based on innate approaches, since, in <b>connectionism,</b> input is the source of both the units and the rules of language.|$|E
50|$|In {{his first}} book, The Algebraic Mind: Integrating <b>Connectionism</b> and Cognitive Science (MIT Press, 2001), Marcus {{challenged}} {{the idea that}} the mind might consist of largely undifferentiated neural networks. He argued that understanding the mind would require integrating <b>connectionism</b> with classical ideas about symbol-manipulation.|$|E
50|$|As <b>connectionism</b> became {{increasingly}} {{popular in the}} late 1980s, some researchers (including Jerry Fodor, Steven Pinker and others) reacted against it. They argued that <b>connectionism,</b> as then developing, threatened to obliterate {{what they saw as}} the progress being made in the fields of cognitive science and psychology by the classical approach of computationalism. Computationalism is a specific form of cognitivism that argues that mental activity is computational, that is, that the mind operates by performing purely formal operations on symbols, like a Turing machine. Some researchers argued that the trend in <b>connectionism</b> represented a reversion toward associationism and the abandonment of the idea of a language of thought, something they saw as mistaken. In contrast, those very tendencies made <b>connectionism</b> attractive for other researchers.|$|E
5000|$|The {{neural network}} branch of <b>connectionism</b> {{suggests}} {{that the study of}} mental activity is really the study of neural systems. This links <b>connectionism</b> to neuroscience, and models involve varying degrees of biological realism. Connectionist work in general need not be biologically realistic, but some neural network researchers, computational neuroscientists, try to model the biological aspects of natural neural systems very closely in so-called [...] "neuromorphic networks". Many authors find the clear link between neural activity and cognition to be an appealing aspect of <b>connectionism.</b>|$|E
5000|$|... #Subtitle level 2: <b>Connectionism</b> and second-language {{acquisition}} ...|$|E
50|$|A {{representational}} theory like SIT seems {{opposite to}} dynamic systems theory (DST), while <b>connectionism</b> {{can be seen}} as something in between. That is, <b>connectionism</b> flirts with DST {{when it comes to the}} usage of differential equations and flirts with theories like SIT when it comes to the representation of information. In fact, the different operating bases of SIT, <b>connectionism,</b> and DST, correspond to what Marr called the computational, the algorithmic, and the implementational levels of description, respectively. According to Marr, these levels of description are complementary rather than opposite, thus reflecting epistemological pluralism.|$|E
5000|$|... #Subtitle level 2: SIT versus <b>connectionism</b> {{and dynamic}} systems theory ...|$|E
5000|$|... #Subtitle level 3: Perceptrons and {{the dark}} age of <b>connectionism</b> ...|$|E
50|$|Another {{approach}} {{which deals}} {{more with the}} semantic content of cognitive science is <b>connectionism</b> or neural network modeling. <b>Connectionism</b> relies {{on the idea that}} the brain consists of simple units or nodes and the behavioral response comes primarily from the layers of connections between the nodes and not from the environmental stimulus itself.|$|E
50|$|Marcus, G. F. (1998). Can <b>connectionism</b> save constructivism? Cognition, 66, 153-182.|$|E
50|$|Marcus, G. F. (1998). Rethinking eliminative <b>connectionism.</b> Cognitive Psychology, 37, 243-282.|$|E
5000|$|<b>Connectionism</b> and computationalism {{need not}} be at odds, but {{the debate in the}} late 1980s and early 1990s led to {{opposition}} between the two approaches. Throughout the debate, some researchers have argued that <b>connectionism</b> and computationalism are fully compatible, though full consensus on this issue has not been reached. Differences between the two approaches include the following: ...|$|E
5000|$|Todd, P. and Loy, Gareth, eds., Music and <b>Connectionism,</b> Cambridge: MIT Press, 1991.|$|E
50|$|<b>Connectionism</b> and the Mind: Parallel Processing, Dynamics, and Evolution in Networks. Basil Blackwell. 2002.|$|E
50|$|<b>Connectionism</b> {{is a set}} of {{approaches}} in the fields of artificial intelligence, cognitive psychology, cognitive science, neuroscience, and philosophy of mind, that models mental or behavioral phenomena as the emergent processes of interconnected networks of simple units. The term was introduced by Donald Hebb in the 1940s. There are many forms of <b>connectionism,</b> but the most common forms use neural network models.|$|E
50|$|Marcus, G. F. (2001). The Algebraic Mind: Integrating <b>Connectionism</b> and Cognitive Science. Cambridge, MA: MIT Press.|$|E
50|$|Theoretical {{approaches}} {{to explain how}} mind emerges from the brain include <b>connectionism,</b> computationalism and Bayesian brain.|$|E
50|$|R. Sun, Integrating Rules and <b>Connectionism</b> for Robust Commonsense Reasoning. John Wiley and Sons, New York. 1994.|$|E
50|$|Though PDP is the {{dominant}} form of <b>connectionism,</b> other theoretical work should also be classified as connectionist.|$|E
