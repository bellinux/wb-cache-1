4|14|Public
50|$|Helix Software Company pioneered {{virtual memory}} {{compression}} in 1992, filing a patent application {{for the process}} in October of that year. In 1994 and 1995, Helix refined the process using test-compression and secondary memory caches on video cards and other devices. However, Helix did not release a product incorporating virtual memory compression until July 1996 {{and the release of}} Hurricane 2.0, which used the Stac Electronics Lempel-Ziv-Stac compression algorithm and also used off-screen video RAM as a <b>compression</b> <b>buffer</b> to gain performance benefits.|$|E
5000|$|MySQL Archive is {{a storage}} engine for the MySQL {{relational}} database management system. Users {{can use this}} analytic storage engine to create a table that is “archive” only. Data cannot be deleted from this table, only added. The Archive engine uses a compression strategy based on the zlib library and it packs the rows using a bit header to represent nulls and removes all whitespace for character type fields. When completed, the row is inserted into the <b>compression</b> <b>buffer</b> and flushed to disk by an explicit flush table, a read, or {{the closing of the}} table.|$|E
50|$|The animal {{extracellular}} matrix includes the interstitial matrix and the basement membrane. Interstitial matrix is present between various animal cells (i.e., in the intercellular spaces). Gels of polysaccharides and fibrous proteins fill the interstitial space {{and act as}} a <b>compression</b> <b>buffer</b> against the stress placed on the ECM. Basement membranes are sheet-like depositions of ECM on which various epithelial cells rest. Each type of connective tissue in animals has a type of ECM: collagen fibers and bone mineral comprise the ECM of bone tissue; reticular fibers and ground substance comprise the ECM of loose connective tissue; and blood plasma is the ECM of blood.|$|E
40|$|Performance {{improvements}} are always needed in computer graphics. Better performance frees up computational resources {{which can be}} used to increase the level of realism in the rendered images. Despite a very rapid development of graphics hardware, we are still far from the point where photo-realistic rendering can be done in real time. Therefore, better algorithms must be developed in order to advance the field. In this thesis, we present several new algorithms targeted for the rasterization pipeline, which is, at the time of writing, the de-facto standard rendering algorithm used in graphics hardware. We focus on three areas in the pipeline. The first is the rasterization step where we present efficient sampling strategies which can improve performance, or enable new algorithms to be implemented. This includes an investigation of very inexpensive, but relatively high quality, sampling schemes, an algorithm for conservative rasterization through area sampling, and an algorithm for efficiently rasterizing multiple views which can be useful for holographic displays, and other multi-view displays. The second area of focus is on compression algorithms. Compression is frequently used in graphics hardware in order to minimize memory traffic and thereby increase performance. We present efficient algorithms for high dynamic range texture <b>compression,</b> depth <b>buffer</b> <b>compression,</b> and color <b>buffer</b> <b>compression.</b> The inner workings of <b>buffer</b> <b>compression</b> have been kept secret by hardware vendors, and the only available information is available through patents. Therefore, we also present surveys of the prior art in <b>buffer</b> <b>compression.</b> The final area we focus on is programmable culling. Culling is an old concept in computer graphics, and it is often necessary to include some form of culling to make an algorithm truly efficient. However, current graphics hardware only has a few fixed function culling algorithms implemented. This often makes it impossible, or impractical, to implement culling in new rendering algorithms that rely on graphics hardware. We present a methodology for automatically deriving culling algorithms from shader programs, and we show how this can be used to perform culling both on a pixel level, and on a geometry or vertex level. In both cases, we show that our automatic culling can greatly improve rendering performance {{for a wide variety of}} scenes...|$|R
40|$|In this paper, {{we first}} present {{a survey of}} {{existing}} color <b>buffer</b> <b>compression</b> algorithms. After that, we introduce a new scheme based on an exactly reversible color transform, simple prediction, and Golomb-Rice encoding. In addition to this, we introduce an error control mechanism, {{which can be used}} for approximate (lossy) color <b>buffer</b> <b>compression.</b> In this way, the introduced error is kept under strict control. To the best of our knowledge, this has not been explored before in the literature. Our results indicate superior compression ratios compared to existing algorithms, and we believe that approximate compression can be important for mobile GPU...|$|R
50|$|The Compression Analysis Tool is a Windows {{application}} that enables end users to benchmark the performance characteristics of streaming implementations of LZF4, DEFLATE, ZLIB, GZIP, BZIP2 and LZMA {{using their own}} data. It produces measurements and charts with which users can compare the compression speed, decompression speed and compression ratio of the different compression methods and to examine how the <b>compression</b> level, <b>buffer</b> size and flushing operations affect the results.|$|R
40|$|This paper {{describes}} the design, implementation, and experimental {{evaluation of a}} reconfigurable network processor that can do on-the-fly content adaptation of IP packets for wired or wireless networks. In our demonstration application, FPGA technology is {{used in conjunction with}} traditional RISC microprocessors to perform IP packet compression in hardware, using a CAM-based hardware implementation of the Lempel-Ziv (LZ) compression algorithm. The experimental evaluation considers HTTP Web browsing traffic using the TCP/IP protocols. The measurement results show that the proposed LZ compression architecture can reduce network byte traffic volume by 5 - 38 %. Furthermore, the hardware-based approach provides consistent throughput performance as the <b>compression</b> <b>buffer</b> size is increased...|$|E
40|$|Despite {{the limited}} power {{available}} in a batteryoperated hand-held device, a display system must still have an enough resolution and sufficient color depth to deliver the necessary information. We introduce some methodologies for frame <b>buffer</b> <b>compression</b> that efficiently reduce the power consumption of display systems and thus distinctly extend battery life for hand-held applications. Our algorithm {{is based on a}} run-length encoding for on-the-fly compression, with a negligible burden in resources and time. We present an adaptive and incremental re-compression technique to maintain efficiency under frequent partial frame buffer updates. We save about 30 % to 90 % frame buffer activity on average for various hand-held applications. We have implemented an LCD controller with frame <b>buffer</b> <b>compression</b> occupying 1, 026 slices and 960 flip-flops in a Xilinx Sprantan-II FPGA, which has an equivalent gate count of 65, 000 gates. It consumes 30 mW more power and 10 % additional silicon space than an LCD controller without frame <b>buffer</b> <b>compression,</b> but reduces the power consumption of the frame buffer memory by 400 mW...|$|R
40|$|This paper {{presents}} what {{we believe}} are the ﬁrst (public) algorithms for ﬂoating-point (fp) color and fp depth <b>buffer</b> <b>compression.</b> The depth codec is also available in an integer version. The codecs are harmonized, meaning that they share basic technology, {{making it easier to}} use the same hardware unit for both types of compression. We further suggest to use these codecs in a uniﬁed codec architecture, meaning that compression/decompression units previously only used for color- and depth <b>buffer</b> <b>compression</b> can be used also during texture accesses. Finally, we investigate the bandwidth implication of using this in a uniﬁed cache architecture. The proposed fp 16 color buffer codec compresses data down to 40 % of the original, and the fp 16 depth codec allows compression down to 4. 5 bpp, compared to 5. 3 for the state-of-the-art int 24 depth compression method. If used in a uniﬁed codec and cache architecture, bandwidth reductions of about 50 % are possible, which is signiﬁcant...|$|R
40|$|Depth buffer {{performance}} {{is crucial to}} modern graphics hardware. This {{has led to a}} large number of algorithms for reducing the depth buffer bandwidth. Unfortunately, these have mostly remained documented only in the form of patents. Therefore, we present a survey on the design space of efficient depth buffer implementations. In addition, we describe our novel depth <b>buffer</b> <b>compression</b> algorithm, which gives very high compression ratios. Categories and Subject Descriptors (according to ACM CCS) : I. 3. 3 [Picture/Image Generation]: framebuffer operations 1...|$|R
40|$|Achieving the {{throughput}} of one wafer {{per minute}} per layer with a direct-write maskless lithography system, using 25 nm pixels for 50 nm feature sizes, requires data rates of about 10 Tb/s. In previous work, {{we have shown}} that lossless binary compression {{plays a key role}} in the system architecture for such a maskless writing system. Recently, we developed a new compression technique Context-Copy-Combinatorial-Code (C 4) specifically tailored to lithography data which exceeds the compression efficiency of all other existing techniques including BZIP 2, 2 D-LZ, and LZ 77. The decoder for any chosen compression scheme must be replicated in hardware tens of thousands of times in any practical direct write lithography system utilizing compression. As such, decode implementation complexity has a significant impact on overall complexity. In this paper, we explore the tradeoff between the compression ratio, and decoder buffer size for C 4. Specifically, we present a number of techniques to reduce the complexity for C 4 <b>compression.</b> First, <b>buffer</b> <b>compression</b> is introduced as a method to reduce decoder buffer size by an order of magnitude without sacrificing compression efficiency. Second, linear prediction is used as a low-complexity alternative to both context-based prediction and binarization. Finally, we allow for copy errors, which improve the compression efficiency of C 4 at small buffer sizes. With these techniques in place, for a fixed buffer size, C 4 achieves a significantly higher compression ratio than those of existing compression algorithms. We also present a detailed functional block diagram of the C 4 decoding algorithm as a first step towards a hardware realization...|$|R
50|$|The {{driving wheels}} of the S6 had the rather unusual {{diameter}} of 2,100 mm {{in order to keep}} the rpm down and to guarantee the smooth running of the driving gear. Originally a diameter of 2,200 mm had been planned, but in the end it was reduced by 100 mm. The heavy balancing masses of the driving gear were badly affected by weight savings. That resulted in serious jerkiness when running. A remedy was achieved by coupling the tender closer to the locomotive and thus raising the <b>compression</b> of the <b>buffer</b> springs.|$|R
40|$|In this paper, {{we present}} a new color <b>buffer</b> <b>compression</b> {{algorithm}} for floating-point buffers. It can operate in either an approximate (lossy) mode or in an exact (lossless) mode. The approximate mode is error-bounded {{and the amount of}} introduced accumulated error is controlled via a few parameters. The core of the algorithm lies in an efficient representation and color space transform, followed by a hierarchical quadtree decomposition, and then hierarchical prediction and Golomb-Rice encoding. We believe this is the first lossy compression algorithm for floating-point buffers, and our results indicate significantly reduced color buffer bandwidths and negligible visible artifacts...|$|R
40|$|Modern {{hand-held}} multimedia terminals consume significant {{power for}} their quality display devices. Due to 60 Hz or higher LCD refresh operations, frame buffer memory and related buses become dominant power consumers. In this paper, we introduce an efficient frame <b>buffer</b> <b>compression</b> scheme that uses differential Huffman coding and its hardware implementation. The compression and decompression must be simple and not incur distinct power overhead involving no CPU operations. We have achieved both on-thefly compression and high compression efficiency devising a limited-size code book, color-difference reduction techniques and an adaptive code book update scheme. On the MobileMark 2002 benchmark, our techniques reduce the frame buffer activity by 52 % to 90 %, saving up to 86 mW including the overhead...|$|R
40|$|A burst {{compression}} and expansion technique is described for asynchronously interconnecting variable-data-rate users with cost-efficient ground terminals in a satellite-switched, time-division-multiple-access (SS/TDMA) network. <b>Compression</b> and expansion <b>buffers</b> in each ground terminal convert between lower rate, asynchronous, continuous-user data streams and higher-rate TDMA bursts synchronized with the satellite-switched timing. The technique described uses a first-in, first-out (FIFO) memory approach which enables {{the use of}} inexpensive clock sources by both the users and the ground terminals and obviates the need for elaborate user clock synchronization processes. A continuous range of data rates from kilobits per second to that approaching the modulator burst rate (hundreds of megabits per second) can be accommodated. The technique was developed for use in the NASA Lewis Research Center System Integration, Test, and Evaluation (SITE) facility. Some key features of the technique have also been implemented in the ground terminals developed at NASA Lewis for use in on-orbit evaluation of the Advanced Communications Technology Satellite (ACTS) high burst rate (HBR) system...|$|R
40|$|Abstract—We propose {{an optimal}} <b>buffered</b> <b>compression</b> {{algorithm}} for shape coding {{as defined in}} the forthcoming MPEG- 4 international standard. The MPEG- 4 shape coding scheme consists of two steps: first, distortion is introduced by down and up scaling; then, context-based arithmetic encoding is applied. Since arithmetic coding is “lossless, ” the down up scaling step is considered as a virtual quantizer. We first formulate the buffer-constrained adaptive quantization problem for shape coding, and then propose an algorithm for the optimal solution under buffer constraints. Recently {{the fact that a}} conversion ratio (CR) of 1 4 makes coded image irritating to human observers for QCIF size was reported for MPEG- 4 shape coding. Therefore, a careful consideration for small size images such as QCIF should be given to prevent coded images from being unacceptable. To this end, a low bit rate tuned algorithm is proposed in this paper as well. Experimental results are given using an MPEG- 4 shape codec. I...|$|R
40|$|AbstractPast {{years have}} {{witnessed}} {{the rapid growth of}} computer-based social software. Despite the increasing popularity of mobile devices, the choices of social software on these devices are still limited to non-real-time email and social media systems. Real-time social software on mobile devices is virtually non-existent due to the device characteristics such as small screen real estate, limited battery talk time, scarce network resources, and inherent need for personalization, which present challenges to the design and implementation of effective and useful real-time mobile social software. In this article, we present a technical solution to these challenges using a smartphone-based real-time collaborative note-taking system as an example. The solution allows for personalized multi-user view through flexible layout of multiple windows, maximally utilizing the available screen real estate, personalized content synchronization through synchronization protocols and algorithms based on the operational transformation technique and a <b>buffer</b> <b>compression</b> algorithm based on the operational merging technique, maximally utilizing the available battery talk time and network resources, and personalized content retrieval through customizable search methods...|$|R
40|$|The {{quality of}} VoIP {{communication}} relies significantly {{on the network}} that transports the voice packets because this network does not usually guarantee the available bandwidth, delay, and loss that are critical for real-time voice traffic. The solution proposed here is to manage the voice-over-IP stream dynamically, changing parameters as needed to assure quality. The main objective of this dissertation is to develop an adaptive speech encoding system {{that can be applied}} to conventional (telephony-grade) and wideband voice communications. This comprehensive study includes the investigation and development of three key components of the system. First, to manage VoIP quality dynamically, a tool is needed to measure real-time changes in quality. The E-model, which exists for narrowband communication, is extended to a single computational technique that measures speech quality for narrowband and wideband VoIP codecs. This part of the dissertation also develops important theoretical work in the area of wideband telephony. The second system component is a variable speech-encoding algorithm. Although VoIP performance is affected by multiple codecs and network-based factors, only three factors can be managed dynamically: voice payload size, speech <b>compression</b> and jitter <b>buffer</b> management. Using an existing adaptive jitter-buffer algorithm, voice packet-size and compression variation are studied as they affect speech quality under different network conditions. This study explains the relationships among multiple parameters as they affect speech transmission and its resulting quality. Then, based on these two components, the third system component is a novel adaptive-rate control algorithm that establishes the interaction between a VoIP sender and receiver, and manages voice quality in real-time. Simulations demonstrate that the system provides better average voice quality than traditional VoIP...|$|R

