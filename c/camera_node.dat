48|127|Public
40|$|Using image {{stitching}} {{technology to}} find out overlap areas in the <b>camera</b> <b>node</b> capture image, inform the <b>camera</b> <b>node,</b> <b>camera</b> <b>node</b> will filter redundancy image data which is overlap with adjacent nodes, and then cluster nodes rebuild images which has been received, stitch together, form a panorama. Through this process, it can reduce image redundancy data affectivity in densely deployed wireless camera image sensor network, the network life cycle can be greatly improved. This paper proposes a new calculation method. ...|$|E
40|$|This paper {{proposes a}} {{distributed}} algorithm for object track-ing in a camera sensor network. At each <b>camera</b> <b>node,</b> an efficient online multiple instance learning algorithm is inte-grated with particle filter for camera’s image plane tracking. To improve the tracking accuracy, each <b>camera</b> <b>node</b> shares its particle states {{with others and}} fuses multi-camera information locally. In particular, particle weights are updated according to the fused information. The effectiveness of the proposed algorithm is demonstrated on human tracking in challenging environments...|$|E
40|$|Abstract—In {{this work}} we {{consider}} an event-driven Wireless Visual Sensor Network (WVSN) where each <b>camera</b> <b>node</b> transmits a frame to the cluster-head only if {{an event of}} interest was captured in the frame for energy and bandwidth conservation. Specifically, we consider the scenario where each <b>camera</b> <b>node</b> receives decision support from an independent but possibly attacked (and hence error-prone) scalar-sensor regarding {{the presence or absence}} of an event. We study the overall detection performance achieved by various techniques that utilize the scalar and image-based decisions. We conclude that in image sequences involving extraneous lighting and background changes (such as in the case of outdoor surveillance), the combination techniques generally achieve a lower total probability of error. I...|$|E
40|$|Robotic {{vision is}} limited by line of sight and onboard camera capabilities. Robots can acquire video or images from remote cameras, but {{processing}} additional data has a computational burden. This paper applies the Distributed Robotic Vision Service, DRVS, to robot path planning using data outside line-of-sight of the robot. DRVS implements a distributed visual object detection service to distributes the computation to remote <b>camera</b> <b>nodes</b> with processing capabilities. Robots request task-specific object detection from DRVS by specifying a geographic region of interest and object type. The remote <b>camera</b> <b>nodes</b> perform the visual processing and send the high-level object information to the robot. Additionally, DRVS relieves robots of sensor discovery by dynamically distributing object detection requests to remote <b>camera</b> <b>nodes.</b> Tested over two different indoor path planning tasks DRVS showed dramatic reduction in mobile robot compute load and wireless network utilization...|$|R
40|$|We {{analyze the}} {{performance}} of TCP and UDP transport protocols when applied to image retrieval or object recognition in wireless visual sensor networks (VSN). We focus on two different paradigms for image analysis, namely compress-then-analyze (CTA) and analyze-then-compress (ATC). The former entails the transmission of JPEG encoded images from <b>camera</b> <b>nodes</b> to a server, where the analysis takes place. The latter consists in extracting and compressing local visual features on board <b>camera</b> <b>nodes,</b> before transmission to a remote location. The presented analysis is useful to assess the best coupling between application and transport layers under delay and accuracy constraints for different networking conditions...|$|R
30|$|The {{uncertainty}} in QoS, cannot {{be solved by}} merely provisioning a statically high QoS at all the <b>camera</b> <b>nodes</b> {{and it will be}} an un-necessary waste, as at many times the subject tracking may not be required at that camera.|$|R
40|$|This demo showcases some of {{the results}} {{obtained}} by the GreenEyes project, whose main objective is to enable visual analysis on resource-constrained multimedia sensor networks. The demo features a multi-hop visual sensor network operated by BeagleBones Linux computers with IEEE 802. 15. 4 communication capabilities, and capable of recognizing and tracking objects according to two different visual paradigms. In the traditional compress-then-analyze (CTA) paradigm, JPEG compressed images are transmitted through the network from a <b>camera</b> <b>node</b> to a central controller, where the analysis takes place. In the alternative analyze-then-compress (ATC) paradigm, the <b>camera</b> <b>node</b> extracts and compresses local binary visual features from the acquired images (either locally or in a distributed fashion) and transmits them to the central controller, where {{they are used to}} perform object recognition/tracking. We show that, in a bandwidth constrained scenario, the latter paradigm allows to reach better results in terms of application frame rates, still ensuring excellent analysis performance...|$|E
40|$|International audienceThe {{problem of}} energy {{conservation}} in wireless image sensor networks is addressed, and a fast zonal discrete cosine transform (DCT) design {{which aims to}} decrease the complexity of JPEG baseline compression is presented. Energy consumption measurements have been made on a real wireless <b>camera</b> <b>node</b> in order to evaluate {{the amount of energy}} which can be saved during the image compression process without loss of visual quality. Although the gain depends on the compression rate, such a DCT design is a simple and effective way to prolong the lifetime of the camera nodes, and thereby the network lifetime...|$|E
40|$|This paper {{discusses}} the target localization problem of wireless visual sensor networks. Specifically, each node with a low-resolution camera extracts multiple feature points {{to represent the}} target at the sensor node level. A statistical method of merging the position information of different sensor nodes to select the most correlated feature point pair at the base station is presented. This method releases {{the influence of the}} accuracy of target extraction on the accuracy of target localization in universal coordinate system. Simulations show that, compared with other relative approach, our proposed method can generate more desirable target localization's accuracy, and it has a better trade-off between <b>camera</b> <b>node</b> usage and localization accuracy...|$|E
40|$|We compare two {{paradigms}} for {{image analysis}} in vi- sual sensor networks (VSN). In the compress-then-analyze (CTA) paradigm, images acquired from <b>camera</b> <b>nodes</b> are compressed {{and sent to}} a central controller for further analysis. Conversely, in the analyze-then-compress (ATC) approach, <b>camera</b> <b>nodes</b> perform visual feature extraction and transmit a compressed version of these features to a central controller. We focus on state-of-the-art binary features which are particularly suitable for resource-constrained VSNs, and we show that the ”winning” paradigm depends primarily on the network conditions. Indeed, while the ATC approach {{might be the only}} possible way to perform analysis at low available bitrates, the CTA approach reaches the best results when the available bandwidth enables the transmission of high-quality images...|$|R
40|$|In this paper, {{we propose}} an {{effective}} and robust decentralized tracking scheme based on the square root cubature information filter (SRCIF) to balance the energy consumption and tracking accuracy in wireless camera sensor networks (WCNs). More specifically, regarding the characteristics and constraints of <b>camera</b> <b>nodes</b> in WCNs, some special mechanisms are put forward and integrated in this tracking scheme. First, a decentralized tracking approach is adopted so that the tracking can be implemented energy-efficiently and steadily. Subsequently, task cluster nodes are dynamically selected by adopting a greedy on-line decision approach based on the defined contribution decision (CD) considering the limited energy of <b>camera</b> <b>nodes.</b> Additionally, we design an efficient cluster head (CH) selection mechanism that casts such selection problem as an optimization problem based on the remaining energy and distance-to-target. Finally, we also perform analysis on the target detection probability when selecting the task cluster nodes and their CH, owing to the directional sensing and observation limitations in field of view (FOV) of <b>camera</b> <b>nodes</b> in WCNs. From simulation results, the proposed tracking scheme shows an obvious improvement in balancing the energy consumption and tracking accuracy over the existing methods...|$|R
40|$|In Wireless {{multimedia}} sensor networks (WMSNs), the <b>camera</b> <b>nodes</b> {{connected in}} the vision graph share overlapped field of views (FOVs) and {{they depend on}} the densely deployed relay nodes in the communication network graph {{to communicate with each}} other. Given a uniformly deployed camera sensor network with relay nodes, the problem is to find the number of hops for the vision-graph-neighbor-searching messages to construct the vision graph in an energy efficient way. In this paper, mathematical models are developed to analyze the FOV overlap of the <b>camera</b> <b>nodes</b> and the multi-hop communications in two dimensional topologies, which are utilized to analyze the relation between vision graph construction and maximum hop count. In addition, simulations are conducted to verify the models...|$|R
40|$|Abstract—This paper {{presents}} a vision-based system for cooperative object detection, localization and tracking using Wireless Sensor Networks (WSNs). The proposed system exploits the distributed sensing capabilities, communication infrastructure and parallel computing {{capabilities of the}} WSN. To reduce the bandwidth requirements, the images captured are processed at each <b>camera</b> <b>node</b> {{with the objective of}} extracting the location of the object on each image plane, which is transmitted to the WSN. The measures from all the camera nodes are processed by means of sensor fusion techniques such as Maximum Likelihood (ML) and Extended Kalman Filter (EKF). The paper describes hardware and software aspects and presents some experimental results. Keywords-cooperative perception, Wireless Sensor Networks, Extended Kalman Filte...|$|E
40|$|This paper {{presents}} a lightweight method for localizing and counting people in indoor spaces using motion and size criteria. A histogram designed to filter moving objects within a specified size range, can operate directly on frame difference output to localize human-sized moving entities {{in the field}} of view of each <b>camera</b> <b>node.</b> Our method targets a custom, ultra-low power imager architecture operating on address-event representation, aiming to implement the proposed algorithm on silicon. In this paper we describe the details of our design and experimentally determine suitable parameters for the proposed histogram. The resulting histogram and counting algorithm are implemented and tested on a set of iMote 2 camera sensor nodes deployed in our lab. 1...|$|E
40|$|This work studies how visual {{analysis}} tasks {{based on}} feature extraction can be speeded {{up in the}} context of Visual Sensor Networks. The main catch is for the <b>camera</b> <b>node</b> to leverage the presence of neighboring sensor nodes and offload the task, thus parallelizing its execution. We propose two mathematical programming formulations for the optimal visual task offloading problem: the first one targets the minimization of the overall task completion time while enforcing energy consumption constraints onto the nodes; the second maximizes the overall sensor network lifetime subject to a temporal constraint on the task completion time. The aforementioned formulations are used to characterize the achievable speed-up and consequent energy consumption in representative visual sensor network topologies...|$|E
40|$|The paper {{develops}} {{an ad hoc}} {{network of}} active pan/tilt/zoom (PTZ) and passive wide field-of-view (FOV) cameras capable of carrying out observation tasks autonomously. The network {{is assumed to be}} uncalibrated, lacks a central controller, and relies upon local decision making at each node and inter-node negotiations for its overall behavior. To this end, we develop intelligent <b>camera</b> <b>nodes</b> (both active and passive) that can perform multiple observation tasks simultaneously. We also present a negotiation protocol that allows <b>cameras</b> <b>nodes</b> to setup collaborative tasks in a purely distributed manner. Camera assignments conflicts that invariably arise in such networks are naturally and gracefully handled through at-node processing and inter-node negotiations. We expect the proposed camera network to be highly scalable {{due to the lack of}} any centralized control...|$|R
40|$|This paper {{describes}} a novel decentralized target tracking scheme for distributed smart cameras. This approach {{is built on}} top of a distributed localization protocol which allows the smart <b>camera</b> <b>nodes</b> to automatically identify neighboring sensors with overlapping fields of regard and establish a communication graph which reflects how the nodes will interact to fuse measurements in the network. The new protocol distributes the detection and tracking problems evenly throughout the network accounting for sensor handoffs in a seamless manner. The approach also distributes knowledge about the state of tracked objects amongst the nodes in the network. This information can then be harvested through distributed queries which allow network participants to subscribe to different kinds of events that they may be interested in. The proposed scheme has been used to track targets in real time using a collection of custom designed smart <b>camera</b> <b>nodes.</b> Results from these experiments are presented. 1...|$|R
40|$|We {{report on}} a home {{wireless}} sensor network deployment that utilizes cameras to collect activity information. This paper describes the design choices and the general experience of maintaining such a data-heavy network. The system includes <b>camera</b> <b>nodes,</b> a classifier program to distill the packets into appropriate database tables, and a client-side user interface for diagnosing and interacting with the network. 1...|$|R
40|$|Abstract- A video sensor {{platform}} {{consisting of}} a smart image sensor with focal plane motion processing is presented. The <b>camera</b> <b>node</b> is intended for ultra-low bandwidth ad-hoc wireless networks with data rates of less than 2 kB/sec. To meet the targets of low power consumption and low complexity, the CMOS image sensor (consuming 4. 2 mW of power at 30 fps) autonomously monitors for scene changes, outputting full image data only when relevant. For this sensor node, several different operating configurations leveraging the analog processing and storage capability of the imager, including a full motion DCT based video compression algorithm are shown. The entire sensor module can operate from 3 AA batteries and consumes 225 mW at full operation. I...|$|E
40|$|Establishing correspondences among object {{instances}} {{is still}} challenging in multi-camera surveillance systems, {{especially when the}} cameras’ fields of view are non-overlapping. Spatiotemporal constraints can help in solving the correspondence problem but still leave a wide margin of uncertainty. One way to reduce this uncertainty is to use ap- pearance information about the moving objects in the site. In this paper we present the preliminary results of a new method that can capture salient appearance characteristics at each <b>camera</b> <b>node</b> in the network. A Latent Dirichlet Allocation (LDA) model is created and maintained at each node in the camera network. Each object is encoded {{in terms of the}} LDA bag-of-words model for appearance. The encoded appearance is then used to establish probable matching across cameras. Preliminary experiments are conducted on a dataset of 20 individuals and comparison against Madden’s I-MCHR is reported...|$|E
40|$|In {{this work}} {{we present a}} generic {{architecture}} for interfacing various input devices to VRML browsers. Concentrating on the aspect of navigation, our system supports {{the full range of}} potential input devices from conventional haptic devices like keyboard and mouse over special Virtual-Reality devices like spacemouse and joystick to, as a special feature, semantically higher level input like speech and gesture recognition. The communication between the individual components of modeling of the various devices and handling both discrete and continuous navigation information. Two new node extensions support the VRML author in creating highly customizable 3 D applications: The DeviceSensor node allows grabbing arbitrary user input in a systematic way and the <b>Camera</b> <b>node</b> gives full control over the scene view by specifying velocity vectors and thus enabling arbitrary navigation modes. Finally, the proof of concept is given by a prototypical implementation in VRML...|$|E
40|$|We {{introduce}} Meerkats, {{a wireless}} network of batteryoperated <b>camera</b> <b>nodes</b> {{that can be}} used for monitoring and surveillance of wide areas. One distinguishing feature of Meerkats (when compared, for example, with systems like Cyclops [10]) is that our nodes are equipped with sufficient processing and storage capabilities to be able to run relatively sophisticated vision algorithms (e. g., motion estimation) locally and/or collaboratively. In previous work [9, 8, 7] we analyzed the energy consumption characteristics of the Meerkats nodes under different duty cycles, involving different power states of the system’s components. In this paper we present an analysis of the performance of the surveillance system as a function of the image acquisition rate and of the synchronization between <b>cameras</b> <b>nodes.</b> Our ultimate goal is to optimally balance the trade-off between application-specific performance requirements (e. g., event miss rate) and network lifetime (as a function of the energy consumption characteristics of each node). ...|$|R
40|$|Abstract — Event-driven visual sensor {{networks}} {{consist of}} col- {{order to determine}} their relevance to the surveillance task [6]. laborating <b>camera</b> <b>nodes</b> and scalar sensors which aid in the Figure 1 depicts one possible scenario where each camera detection of events {{of interest in the}} environment. This collab-node receives decision support from a scalar sensor before oration is significant since the <b>camera</b> <b>nodes</b> generally utilize lightweight image processing in order to determine if a frame is transmitting frames to the cluster head. relevant to the given application. The reliability of the supporting In comparison with other approaches, the event-driven scalar sensor however may be compromised by an actuation scalar-assisted approach may be considered a “push ” approach attack which perturbs the sensor’s measurements. In this work in that the visual nodes transmit selected frames to the cluster-we examine the achievable actuation of hostile nodes that are not head when such frames become available. In contrast, in a globally coordinated and that may be selfish or untrustworthy in their preferences. We compare our findings with existing research “pull ” approach, the cluster-head advertises what features are which assumes that all the hostile nodes are coordinated to of interest in the application [7]. The feature advertising thus actuate with the same parameter. We determine that given certain pulls data that matches the search from the visual nodes. The conditions, local optimization may actually result in a stronger appeal of the push-based approach in the limited-resource stealthy attack than the global coordination case. regime of VSNs is that the <b>camera</b> <b>nodes</b> are able to “rid...|$|R
40|$|This paper {{presents}} {{an overview of}} our demonstration of a low-bandwidth, wireless camera network where image compression is undertaken at each node. We briefly introduce the Fleck hardware platform we have developed as well as describe the image compression algorithm which runs on individual nodes. The demo will show real-time image data coming back to base as individual <b>camera</b> <b>nodes</b> {{are added to the}} network. Copyright 2007 ACM...|$|R
30|$|We {{discuss how}} {{to obtain the}} {{accurate}} and globally consistent self-calibration of a distributed camera network, in which camera nodes with no centralized processor may be spread over a wide geographical area. We present a distributed calibration algorithm based on belief propagation, in which each <b>camera</b> <b>node</b> communicates only with its neighbors that image {{a sufficient number of}} scene points. The natural geometry of the system and the formulation of the estimation problem give rise to statistical dependencies that can be efficiently leveraged in a probabilistic framework. The camera calibration problem poses several challenges to information fusion, including overdetermined parameterizations and nonaligned coordinate systems. We suggest practical approaches to overcome these difficulties, and demonstrate the accurate and consistent performance of the algorithm using a simulated 30 -node camera network with varying levels of noise in the correspondences used for calibration, as well as an experiment with 15 real images.|$|E
30|$|For the {{symbiosis}} {{of humans}} and machines, various kinds of sensing devices will be either implicitly or explicitly embedded, networked, and cooperatively function in our future living environment [1 – 3]. To cover wider areas of interest, multiple cameras {{will have to be}} deployed. In general, gesture recognizing systems that function in real world must operate in real-time, including the time needed for event detection, tracking, and recognition. Since the number of cameras can be very large, distributed processings of incoming images at each <b>camera</b> <b>node</b> are inevitable in order to satisfy the real-time requirement. Also, improvements in recognition performance can be expected by integrating responses from each distributed processing component. But it is usually not evident how the responses should be integrated. Furthermore, since a gesture is such a dynamic and complex motion, single-view observation does not necessary guarantee better recognition performance. One needs to know from which camera views a gesture should be observed in order to quantitatively determine the optimal camera configuration and views.|$|E
40|$|A Generic User Interface Framework Current VRML browser {{implementations}} {{lack the}} flexibility of adaptable user interfaces and navigation modes, in massive contradiction to the primary motivation for using 3 D, namely to give the user a more natural {{understanding of how to}} interact with computer systems and, in return, to make computer work more time-efficient. Multimodal interaction, {{such as the use of}} gesture and speech recognition, promises further improvement of the usability of 3 D applications but can only be seen in dedicated applications thus far. We present a user interface framework including the definition of a node set and the interfaces to other devices: As a key feature, we propose a DeviceSensor node that allows grabbing arbitrary user input, and a <b>Camera</b> <b>node</b> to realize arbitrary navigation modes. The interface is based on the abstract formalism of a context-free grammar providing the representation of domain- and device-independent multimodal information contents. By changing the grammer the behaviour of the system can easily be modified...|$|E
40|$|Abstract Wireless Image Sensor Networks (WISNs) {{consisting}} of untethered <b>camera</b> <b>nodes</b> and sensors may be deployed {{in a variety}} of unattended and possibly hostile environments to obtain surveillance data. In such settings, the WISN nodes must perform reliable event acquisition to limit the energy, computation and delay drains associated with forwarding large volumes of image data wirelessly to a sink node. In this work we investigate the event acquisition properties of WISNs that employ various techniques at the <b>camera</b> <b>nodes</b> to distinguish between event and non-event frames in uncertain environments that may include attacks. These techniques include lightweight image processing, decisions from n sensors with/without cluster head fault and attack detection, and a combination approach relying on both lightweight image processing and sensor decisions. We analyze the relative merits and limitations of each approach in terms of the resulting probability of event detection and false alarm in the face of occasional errors, attacks and stealthy attacks. Keywords security Image sensor networks · Lightweight event acquisition · Sensor network...|$|R
40|$|Event-driven visual sensor {{networks}} (VSNs) rely on {{a combination}} of <b>camera</b> <b>nodes</b> and scalar sensors to determine if a frame contains an event of interest that should be transmitted to the cluster head. The appeal of eventdriven VSNs stems from the possibility of eliminating nonrelevant frames at the source thus implicitly minimizing the amount of energy required for coding and transmission. The challenges of the event-driven paradigm result from the vulnerability of scalar sensors to attack or error and from the lightweight image processing available to the <b>camera</b> <b>nodes</b> due to resource constraints. In this work we focus on the reliability issues of VSNs in the case of global actuation attacks on the scalar sensors. We study the extent to which various utility functions enable an attacker to increase the average expected number of affected nodes with a relatively small penalty in the loss of stealth. We then discuss tradeoffs between different attack detection strategies in terms of the cost of processing and the required information at the cluster head and nodes. 1...|$|R
40|$|Visual sensor {{networks}} (VSN) are {{networks of}} smart cameras capable of local image processing and data communication. Different from traditional camera-based surveillance network where cameras simply stream their image data to a centralized server for processing, cameras in VSNs form a distributed system, performing information extraction and collaborating on application-specific tasks. Due to the resource {{constraints of the}} <b>camera</b> <b>nodes</b> such as restricted computation capacity, remaining battery power and available bandwidth, VSNs usually need to {{limit the amount of}} data being exchanged among the <b>camera</b> <b>nodes</b> and the complexity of algorithms running on the <b>camera</b> <b>nodes.</b> This thesis work shows how complex vision tasks can be integrated with networking requirements in two different VSN contexts. The first context is large-scale ad-hoc wireless smart cameras working on battery power which resembles the architecture of general wireless sensor networks. In this context we build geographic hash table (GHT) based network protocols adaptive to the nature of image sensors. These protocols decouple the events from the camera locations. Simulation results show that these protocols allow efficient distributed camera calibration and event-based constraint processing. The second context is smaller-scale static wired smart cameras with constant power supplies which can be found in public spaces that need surveillance such as airports and casinos. We avoid heavy-weight processing by employing only basic feature sets and simple vision algorithms. Simple vision algorithms are fallible by themselves, but can be fused to produce credible results comparable to richer features with more resource-hungry algorithms. We build a real camera network with resource management and exploit the spatial-temporal relationships among cameras based on their overlapping or nearby field-of-views to allow efficient camera collaborations and maximize the effectiveness of data exchange. 1...|$|R
40|$|Wireless Multi-media Sensor Networks (WMSNs) {{have become}} {{increasingly}} popular in recent years, driven {{in part by the}} increasing commoditization of small, low-cost CMOS sensors. As such, the challenge of automatically calibrating these types of cameras nodes has become an important research problem, especially for the case when a large quantity of these type of devices are deployed. This paper presents a method for automatically calibrating a wireless <b>camera</b> <b>node</b> with the ability to rotate around one axis. The method involves capturing images as the camera is rotated and computing the homographies between the images. The camera parameters, including focal length, principal point and the angle and axis of rotation can then recovered from two or more homographies. The homography computation algorithm is designed to deal with the limited resources of the wireless sensor and to minimize energy con- sumption. In this paper, a modified RANdom SAmple Consensus (RANSAC) algorithm is proposed to effectively increase the efficiency and reliability of the calibration procedure...|$|E
40|$|Abstract—Wide-area {{wireless}} camera {{networks are}} being in-creasingly deployed in many urban scenarios. The {{large amount of}} data generated from these cameras pose significant information processing challenges. In this work, we focus on representation, search and retrieval of moving objects in the scene, with emphasis on local <b>camera</b> <b>node</b> video analysis. We develop a graph model that captures the relationships among objects without the need to identify global trajectories. Specifically, two types of edges are defined in the graph: object edges linking the same object across the whole network and context edges linking different objects within a spatial-temporal proximity. We propose a manifold ranking method with a greedy diversification step to order the relevant items based on similarity as well as diversity within the database. Detailed experimental results using video data from a 10 -camera network covering bike paths are presented. Index Terms—Distributed camera network, diverse and relevant ranking, graph-based modeling, information search and retrieval. I...|$|E
40|$|Visual sensor {{networks}} (VSNs) {{consist of}} image sensors, embedded processors and wireless transceivers which are powered by batteries. Since {{the energy and}} bandwidth resources are limited, setting up a tracking system in VSNs is a challenging problem. In this paper, we present a framework for human tracking in VSN environments. The traditional approach of sending compressed images to a central node has certain disadvantages such as decreasing the performance of further processing (i. e., tracking) because of low quality images. Instead, in our framework, each <b>camera</b> <b>node</b> performs feature extraction and obtains likelihood functions. By transforming to an appropriate domain and taking only the signicant coefficients, these likelihood functions are compressed and this new representation {{is sent to the}} fusion node. An appropriate domain is selected by performing a comparison between well-known transforms. We have applied our method for indoor people tracking and demonstrated the superiority of our system over the traditional approach of sending compressed images by comparing the tracking results and communication loads...|$|E
40|$|We {{describe}} a wide area camera network on a campus setting, the SCALLOPSNet (Scalable Large Optical Sensor Network). It covers with about 100 stationary cameras an expansive area {{that can be}} divided into three distinct regions: inside a building, along urban paths, and in a remote natural reserve. Some of these regions lack connections for power and communications, and, therefore, necessitate wireless, battery-powered <b>camera</b> <b>nodes.</b> In our exploration of available solutions, we found existing smart cameras to be insufficient for this task, and instead designed our own battery-powered <b>camera</b> <b>nodes</b> that communicate using 802. 11 b. The camera network uses the Internet Protocol on either wired or wireless networks to communicate with our central cluster, which runs cluster and cloud computing infrastructure. These frameworks like Apache Hadoop are well suited for large distributed and parallel tasks such as many computer vision algorithms. We discuss the design and implementation details of this network, together with the challenges faced in deploying such a large scale network on a research campus. We plan to make the datasets available for researchers in the computer vision community in the near future. 1...|$|R
40|$|Visual sensor {{networks}} {{have emerged as}} an important class of sensor-based distributed intelligent systems, with unique performance, complexity, and quality of service challenges. Consisting {{of a large number}} of low-power <b>camera</b> <b>nodes,</b> visual sensor networks support a great number of novel vision-based applications. The <b>camera</b> <b>nodes</b> provide information from a monitored site, performing distributed and collaborative processing of their collected data. Using multiple cameras in the network provides different views of the scene, which enhances the reliability of the captured events. However, the large amount of image data produced by the cameras combined with the network’s resource constraints require exploring new means for data processing, communication, and sensor management. Meeting these challenges of visual sensor networks requires interdisciplinary approaches, utilizing vision processing, communications and networking, and embedded processing. In this paper, we provide an overview of the current state-of-the-art in the field of visual sensor networks, by exploring several relevant research directions. Our goal is to provide a better understanding of current research problems in the different research fields of visual sensor networks, and to show how these different research fields should interact to solve the many challenges of visual sensor networks...|$|R
40|$|Abstract—We {{introduce}} {{an ad hoc}} {{network of}} active pan/tilt/zoom and passive cameras capable of carrying out collaborative tasks through strictly local interactions. Camera interactions are modeled as negotiations between two or more cameras. Through negotiations camera agree upon how best to carry out an observation tasks. Our smart <b>camera</b> <b>nodes</b> are modeled as behavior-based agents and are capable of engaging in multiple negotiations and performing multiple tasks simulta-neously. We expect the proposed camera network to be highly scalable {{due to the lack}} of any centralized control...|$|R
