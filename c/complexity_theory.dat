4144|687|Public
5|$|This proof, {{published}} by Gabriel Lamé in 1844, represents {{the beginning of}} computational <b>complexity</b> <b>theory,</b> and also the first practical application of the Fibonacci numbers.|$|E
5|$|As older {{problems}} are solved, more complexities are being introduced, like adding additional parts to an engine. Interaction {{of these new}} parts, or niches, create emergent properties, like time-saving office devices (e.g. email, mobile computers, etc.) make communication more efficient but also expand the network of contacts and {{increase the amount of}} time spent on such duties. Homer-Dixon relates this to <b>complexity</b> <b>theory</b> explaining that as new niches are filled there is a synergistic burst of simplicity. However, this can also lead to less control or freedom as emergent properties are created, like a new government program leading to a sprawling bureaucracy. Information theory is touched upon relating the amount of information required to describe a system and the degree of that system's complexity. Chaos theory is used to describe how small changes can lead to widely varying results and path dependence.|$|E
5|$|In {{computational}} <b>complexity</b> <b>theory,</b> 2-satisfiability {{provides an}} example of an NL-complete problem, one that can be solved non-deterministically using a logarithmic amount of storage and that is among the hardest of the problems solvable in this resource bound. The set of all solutions to a 2-satisfiability instance can be given the structure of a median graph, but counting these solutions is #P-complete and therefore not expected to have a polynomial-time solution. Random instances undergo a sharp phase transition from solvable to unsolvable instances as the ratio of constraints to variables increases past 1, a phenomenon conjectured but unproven for more complicated forms of the satisfiability problem. A computationally difficult variation of 2-satisfiability, finding a truth assignment that maximizes the number of satisfied constraints, has an approximation algorithm whose optimality depends on the unique games conjecture, and another difficult variation, finding a satisfying assignment minimizing the number of true variables, is an important test case for parameterized complexity.|$|E
5000|$|... #Subtitle level 3: From <b>complexity</b> <b>theories</b> to {{systemic}} design ...|$|R
40|$|It is {{becoming}} increasingly important for organizations to gain competitive advantage {{by being able to}} manage and survive change. This paper presents two theoretical paradigms (systems and <b>complexity</b> <b>theories)</b> through which organizational change processes can be fruitfully examined. Systems and <b>complexity</b> <b>theories</b> are two valuable perspectives that can equip organizational leaders with the requisite knowledge and understanding of how to respond and adapt to the uncertainties and demands of global change. These two paradigms are particularly useful in the areas of organizational intelligence, organizational design, knowledge management, and corporate strategy, to mention but a few...|$|R
40|$|Unlike {{more stable}} industries, {{high-tech}} firms must constantly be in a strategy development phase. These companies are {{in desperate need}} of assistance in strategy formulation. Chaos and <b>Complexity</b> <b>theories</b> can provide a powerful approach to support the development of business strategies to deal with these fast-moving environments. This paper analyzes the different schemes provided by Chaos and <b>Complexity</b> <b>theories</b> and their possible applications to study cycles of products, processes, and organizational innovations in the high-tech industries. This analysis includes the definitions and previous work accomplished in similar areas. In addition, a case study is selected (the disk drive industry) and preliminary work using attractors, phase diagrams and neural networks is discussed...|$|R
25|$|The Blum axioms {{can be used}} {{to define}} an {{abstract}} computational <b>complexity</b> <b>theory</b> on the set of computable functions. In computational <b>complexity</b> <b>theory,</b> the problem of determining the complexity of a computable function is known as a function problem.|$|E
25|$|Byrne, David (1998). <b>Complexity</b> <b>Theory</b> and the Social Sciences. London: Routledge.|$|E
25|$|The set of all {{recursive}} functions {{is known}} as R in computational <b>complexity</b> <b>theory.</b>|$|E
50|$|Gianluca Bocchi participated and {{organized}} {{a number of}} international interdisciplinary conferences both as coordinator {{and member of the}} scientific committee and he translated and edited the Italian edition of many books on epistemology, philosophy of science, evolutionary studies and <b>complexity</b> <b>theories</b> (Ilya Prigogine, Edgar Morin, Niles Eldredge, Ervin Laszlo).|$|R
50|$|In {{computational}} {{complexity and}} communication <b>complexity</b> <b>theories</b> the decision tree {{model is the}} model of computation or communication in which an algorithm or communication process {{is considered to be}} basically a decision tree, i.e., a sequence of branching operations based on comparisons of some quantities, the comparisons being assigned the unit computational cost.|$|R
40|$|Abstract: This article {{traces the}} {{development}} of <b>complexity</b> <b>theories</b> and proposes a Complexity Representation Model (CRM) for management processes. The purpose here was to translate key elements of <b>complexities</b> <b>theories</b> (e. g. self organisation, adaption, co-evolution, chaos) into a recognisable form and relate these to management practice (particularly knowledge management and learning). A further Complexity Application Model (CAM) is offered that shows {{the relationship between the}} formal and informal aspects of the management environment and the CRM. It models an active environment that should learn and adapt to minor perturbations and major schisms. It is a conceptual guide as to the “ideal ” management system, one that selforganises, learns, adapts and evolves with its environment. The application of the CAM is discussed in terms of practical methods and processes {{that can be used to}} manage and encourage managers to feel they are in control of a complex adaptive management system...|$|R
25|$|In {{previous}} speeches, Crichton criticized {{environmental groups}} {{for failing to}} incorporate <b>complexity</b> <b>theory.</b> Here he explains in detail why <b>complexity</b> <b>theory</b> is essential to environmental management, using the history of Yellowstone Park {{as an example of}} what not to do. The speech was delivered to the Washington Center for Complexity and Public Policy in Washington, D.C. on November 6, 2005.|$|E
25|$|Post's {{main area}} of {{scholarly}} interest is {{intellectual property law}} {{and the relationship of}} <b>complexity</b> <b>theory</b> to the law.|$|E
25|$|The {{problem of}} finding a minimum vertex cover is a {{classical}} optimization problem in computer science and is a typical example of an NP-hard optimization problem that has an approximation algorithm. Its decision version, the vertex cover problem, was one of Karp's 21 NP-complete problems and is therefore a classical NP-complete problem in computational <b>complexity</b> <b>theory.</b> Furthermore, the vertex cover problem is fixed-parameter tractable and a central problem in parameterized <b>complexity</b> <b>theory.</b>|$|E
40|$|In {{addition}} to qualitative methods presented in chaos and <b>complexity</b> <b>theories</b> in educational research, this article addresses quantitative methods that may show potential {{for future research}} studies. Although much {{in the social and}} behavioral sciences literature has focused on computer simulations, this article explores current chaos and complexity methods that have the potential to bridge the divide between qualitative and quantitative, as well as theoretical and applied, human research studies. These methods include multiple linear regression, nonlinear regression, stochastics, Monte Carlo methods, Markov Chains, and Lyapunov exponents. A postulate for post hoc regression analysis is then presented as an example of an emergent, recursive, and iterative quantitative method when dealing with interaction effects and collinearity among variables. This postulate also highlights the power of both qualitative and quantitative chaos and <b>complexity</b> <b>theories</b> in order to observe and describe both the micro and macro levels of systemic emergence...|$|R
40|$|This article {{deals with}} the {{adoption}} process {{of one of the}} most popular performance measurement systems, activity based costing and management (ABCM). The research object of the article is performance measurement system analyzed from organizational changes point of view disclosing the role of organizational values. Theoretical framework of this study is based on two approaches, namely contingency and <b>complexity</b> <b>theories...</b>|$|R
40|$|What is {{information}}? What role does {{information entropy}} {{play in this}} information exploding age, especially in understanding emergent behaviors of complex systems? To answer these questions, we discuss the origin of information entropy, the difference between information entropy and thermodynamic entropy, the role of information entropy in <b>complexity</b> <b>theories,</b> including chaos theory and fractal theory, and speculate new fields in which information entropy may play important roles...|$|R
25|$|The {{concept of}} {{polynomial}} time leads to several complexity classes in computational <b>complexity</b> <b>theory.</b> Some important classes defined using polynomial time are the following.|$|E
25|$|Later, Kolmogorov {{focused his}} {{research}} on turbulence, where his publications (beginning in 1941) significantly influenced the field. In classical mechanics, he {{is best known for}} the Kolmogorov–Arnold–Moser theorem, first presented in 1954 at the International Congress of Mathematicians. In 1957, working jointly with his student V. I. Arnold, he solved a particular interpretation of Hilbert's thirteenth problem. Around this time he also began to develop, and was considered a founder of, algorithmic <b>complexity</b> <b>theory</b> – often referred to as Kolmogorov <b>complexity</b> <b>theory.</b>|$|E
25|$|The {{first one}} (chronologically) {{is used in}} {{analytic}} number theory, {{and the other one}} in computational <b>complexity</b> <b>theory.</b> When the two subjects meet, this situation is bound to generate confusion.|$|E
40|$|Abstract. Propositional proof {{complexity}} is {{the study}} of the sizes of propositional proofs, and more generally, the resources necessary to certify propositional tautologies. Questions about proof sizes have connections with computational <b>complexity,</b> <b>theories</b> of arithmetic, and satisfiability algorithms. This is article includes a broad survey of the field, and a technical exposition of some recently developed techniques for proving lower bounds on proof sizes. Content...|$|R
5000|$|In contrast, {{cognitive}} <b>complexity</b> <b>theories</b> {{as proposed}} by Beiri (1961), attempt to identify individuals who are more complex in their approach to problem-solving against those who are simpler. The instruments used to measure this concept of [...] "cognitive style" [...] are either Driver's Decision Style Exercise (DDSE) (Carey, 1991) or the Complexity Self-Test Description Instrument, which are somewhat ad hoc and so are little used at present.|$|R
40|$|The {{development}} of strategy remains a debate for academics and {{a concern for}} practitioners. Published {{research has focused on}} producing models for strategy development and on studying how strategy is developed in organisations. The Operational Research literature has highlighted the importance of considering complexity within strategic decision making; but little has been done to link strategy development with <b>complexity</b> <b>theories,</b> despite organisations and organisational environments becoming increasingly more complex. We review the dominant streams of strategy development and <b>complexity</b> <b>theories.</b> Our theoretical investigation results in the first conceptual framework which links an established Strategic Operational Research model, the Strategy Development Process model, with complexity via Complex Adaptive Systems theory. We present preliminary findings from the use of this conceptual framework applied to a longitudinal, in-depth case study, to demonstrate the advantages of using this integrated conceptual model. Our research shows that the conceptual model proposed provides rich data and allows for a more holistic examination of the strategy development process. © 2012 Operational Research Society Ltd. All rights reserved...|$|R
25|$|The problem often {{arises in}} {{resource}} allocation {{where there are}} financial constraints and is studied in fields such as combinatorics, computer science, <b>complexity</b> <b>theory,</b> cryptography, applied mathematics, and daily fantasy sports.|$|E
25|$|The vertex cover {{problem is}} an NP-complete problem: {{it was one}} of Karp's 21 NP-complete problems. It is often used in {{computational}} <b>complexity</b> <b>theory</b> {{as a starting point for}} NP-hardness proofs.|$|E
25|$|Descriptive <b>complexity</b> <b>theory</b> relates logics to {{computational}} complexity. The {{first significant}} result in this area, Fagin's theorem (1974) established that NP {{is precisely the}} set of languages expressible by sentences of existential second-order logic.|$|E
40|$|We {{present an}} {{application}} of miniKanren {{with respect to the}} needs of Proof and <b>Complexity</b> <b>Theories.</b> We implement an Interactive Proof System for deciding the truth of quantified boolean formulae, which completes the proof that the complexity classes IP (class of problems solvable by an interactive proof system) and PSPACE (class of decision problems requiring polynomial space, but may need exponential time) are equal. We experiment on the reversibility capabilities of miniKanren and discover...|$|R
40|$|In {{the last}} 15 years, {{terrestrial}} geomorphology has been revolutionized by {{the theories of}} chaotic systems, fractals, self-organization, and selforganized criticality. Except {{for the application of}} fractal theory to the analysis of lava flows and rampart craters on Mars, these theories have not yet been applied to problems of Martian landscape evolution. These <b>complexity</b> <b>theories</b> are elucidated below, along with the methods used to relate these theories to the realities of Martian fluvial systems...|$|R
50|$|Ecohealth {{approaches}} {{as currently}} practiced are participatory, systems-based approaches to understanding and promoting health and wellbeing {{in the context}} of social and ecological interactions. What differentiates these approaches from earlier integrative attempts is a firm grounding in <b>complexity</b> <b>theories</b> and post-normal science (Waltner-Toews, 2004; Waltner-Toews et al., 2008). While a variety of organizations promote integrative approaches such as One Health, the primary funder and promoter of ecohealth in particular, world-wide, is the International Development Research Centre in Ottawa (http://www.idrc.ca/ecohealth/).|$|R
25|$|Atomix {{has been}} the subject of {{scientific}} research in computational <b>complexity</b> <b>theory.</b> When generalized to puzzles of arbitrary sizes, the problem of determining whether an Atomix puzzle has a solution is PSPACE-complete. Some heuristic approaches have been considered.|$|E
25|$|The School of Technology and Computer Science {{grew out}} of early {{activities}} carried out at TIFR for building digital computers. Today, its activities cover areas such as Algorithms, <b>Complexity</b> <b>Theory,</b> Formal Method, Applied Probability, Mathematical Finance, Information Theory, Communications, etc.|$|E
25|$|Closely related {{fields in}} {{theoretical}} computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational <b>complexity</b> <b>theory</b> {{is that the}} former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms {{that could be used}} to solve the same problem. More precisely, computational <b>complexity</b> <b>theory</b> tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.|$|E
50|$|Her current {{research}} is situated within {{the tension between}} general social theory and specific forms of inequality, especially gender. Over the years this led her from theories of patriarchy to a current concern to mainstream difference into social theory. She {{has an interest in}} economic matters, a fascination with new political forms, and concern with marginalised groups. Today, all of these issues are framed by globalisation, the understanding of which requires new forms of social <b>theory,</b> especially <b>complexity</b> <b>theories.</b>|$|R
40|$|Since {{the early}} 1970 s, {{the notions of}} space and place have been located on {{the two sides of}} a {{barricade}} that divides what has been described as science’s two great cultures. Space is located among the ‘hard’ sciences as a central term in the attempt of geography to transform the discipline from a descriptive into a quantitative, analytic, and thus scientific, enterprise. Place, on the other hand, is located among the ‘soft’ humanities and social philosophy oriented social sciences as an important notion in the post- 1970 attempt to transform geography from a positivistic into a humanistic, structuralist, hermeneutic, critical science. More recently, the place-oriented geographies have adopted postmodern, poststructuralist, and deconstruction approaches, while the quantitative spatial geographies have been strongly influenced by theories of self-organization and complexity. In this paper I first point to, and then explore, structural similarities between <b>complexity</b> <b>theories</b> and theories oriented toward social philosophy. I then elaborate the thesis that, in consequence, <b>complexity</b> <b>theories</b> have the potential to bridge the geographies of space and place and, by implication, the two cultures of science. Finally, discuss in some detail conceptual and methodological implications. ...|$|R
40|$|As the {{neuronal}} (and cognitive) processes are, after all, so well known, {{they offer}} a nice testbench for <b>complexity</b> <b>theories,</b> {{and they are a}} nice prototype for understanding behaviors in cybernetic systems in general. It turns out that when applying the framework of Hebbian neurons, many observed brain functionalities can be attacked, including sparse coding on the low level, and chunking on the higher one. Even the mysteries of causality and mind vs. matter can perhaps be given new perspectives. ...|$|R
