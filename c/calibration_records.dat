15|121|Public
5000|$|Checking Aids When {{there are}} special tools for {{checking}} parts, this section shows {{a picture of}} the tool and <b>calibration</b> <b>records,</b> including dimensional report of the tool.|$|E
5000|$|The Crown {{is under}} a {{constitutional}} duty to disclose evidence in its possession which relates to a criminal charge, {{as part of the}} accused's right to make full answer and defence, guaranteed by s 7 of the Charter. [...] This duty of disclosure requires the Crown to provide copies of maintenance and <b>calibration</b> <b>records</b> for the screening device ...|$|E
40|$|The {{smoothness}} of {{the pavement}} surface approaching a Weigh-In-Motion (WIM) scale directly affects the device’s ability to accurately estimate static loads from measured dynamic forces. Lack of smoothness creates difficulties in obtaining an acceptable weighing error. Efforts {{were made in}} this study to determine appropriate threshold values of profile length and roughness approaching the WIM. Roughness data for 19 WIM sites across Texas were collected, and the corresponding <b>calibration</b> <b>records</b> from those WIM sites were employed to verify the smoothness threshold values proposed by AASHTO MP 14 and ASTM E 1318. Since there are multiple lanes at each WIM site, a total number of 70 profiles was surveyed. Based on the <b>calibration</b> <b>records</b> from the WIM sites and field roughness measurements, {{it was found that}} the upper threshold value of the Long Range Index (LRI) of AASHTO method MP 14 needs to be adjusted in order to screen out the locations that yield unacceptable weighing error. The lower threshold value was found to be unnecessary because more than 90 % of WIM sites yielded acceptable weighing error even when exceeding the lower threshold value. The LRI is preferable to the Short Range Index (SRI) as it matched well with the field WIM calibration results. Based on the 70 profile...|$|E
50|$|In {{the early}} 1990s, Bronk Ramsey became {{interested}} in the application of Bayesian statistics to the analysis of radiocarbon data. In 1994, he authored OxCal, an online radiocarbon calibration program.Bronk Ramsey has made significant contributions to various chronological issues, including the Minoan eruption of Thera, the British Neolithic, the dispersal of modern humans out of Africa and the Egyptian chronology. His research interests also include the improvement of the radiocarbon <b>calibration</b> <b>record.</b> He {{is a member of the}} International Calibration (IntCal) group. His recent work has focused on improving the radiocarbon <b>calibration</b> <b>record</b> and synthesizing radiocarbon data with other chronometric information. In October 2012, Bronk Ramsey published the first wholly terrestrial radiocarbon <b>calibration</b> <b>record</b> extending back to the limit of the technique.|$|R
3000|$|Initially, the {{application}} [...] "Karkendamm.exe" [...] (Figure 1, ((FNI) FI 2013), (Crémer 2013)) reads configuration files, where camera settings and recording parameters were specified, and started animal identification, <b>calibration,</b> <b>recording,</b> and preprocessing.|$|R
5000|$|... #Subtitle level 3: Long-term <b>calibration</b> and <b>record</b> {{continuity}} ...|$|R
40|$|The paper {{describes}} a procedure for accurately and speedily calibrating tanks {{used for the}} chemical processing of nuclear materials. The procedure features the use of (1) precalibrated vessels certified to deliver known volumes of liquid, (2) calibrated linear measuring devices, and (3) a digital computer for manipulating data and producing printed calibration information. <b>Calibration</b> <b>records</b> of the standards are traceable to primary standards. Logic is incorporated in the computer program to accomplish curve fitting and perform the tests to accept or to reject the calibration, based on statistical, empirical, and report requirements. This logic {{is believed to be}} unique. "U. S. Atomic Energy Commission Contract AT(29 - 1) - 1106. ""RFP- 488; UC- 38 Engineering and Equipment; TID- 4500 (40 th Ed.). "" 1 July 65. "Includes bibliographical references (page 23). The paper {{describes a}} procedure for accurately and speedily calibrating tanks used for the chemical processing of nuclear materials. The procedure features the use of (1) precalibrated vessels certified to deliver known volumes of liquid, (2) calibrated linear measuring devices, and (3) a digital computer for manipulating data and producing printed calibration information. <b>Calibration</b> <b>records</b> of the standards are traceable to primary standards. Logic is incorporated in the computer program to accomplish curve fitting and perform the tests to accept or to reject the calibration, based on statistical, empirical, and report requirements. This logic is believed to be unique. Mode of access: Internet. This bibliographic record is available under the Creative Commons CC 0 "No Rights Reserved" license. The University of Florida Libraries, as creator of this bibliographic record, has waived all rights to it worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law...|$|E
40|$|At {{the request}} of the Minnesota Power, Inc., the Energy & Environmental Research Center (EERC) sampled for lead at the stack (or duct {{directly}} leading to the stack) for three units at the Boswell Energy Center. All sampling was done in triplicate using U. S. Environmental Protection Agency (EPA) Method 12, with sampling procedures following EPA Methods 1 through 4. During the test program, lead sampling was done using EPA Method 12 in the duct at the outlet of the baghouse serving Unit 2 and the duct at the outlet of the wet particulate scrubber serving Unit 3. For Unit 4, lead sampling was done at the stack. The specific objective for the project was to determine the concentration of lead in the flue gas being emitted into the atmosphere from the Boswell Energy Center. The test program was performed during the period of May 8 through 11, 2000. This report presents the test data, sample calculations, and results, and a discussion of the lead sampling performed at the Boswell Energy Center. The detailed test data and test results, raw test data, process data, laboratory reports, and equipment <b>calibration</b> <b>records</b> are provided in Appendices A, B, and C...|$|E
40|$|In 12 {{chapters}} (Part I) this {{extension to}} the two 'The Sound of Silence' editions covers the development, calculation, construction and measurement of the fully differential (= balanced) phono-amp solution 'RIAA Phono-Amp Engine II'. Additionally, the balanced measurement amplifiers & measurement tools, the discussion on BJT gain stages, the 1 /f noise calculation methods for BJTs, the calculation of fully-differential amplifiers, the numerous Mathcad worksheets, and the presentation of test and <b>calibration</b> <b>records</b> fill a further 10 chapters of Part II with essential knowhow that will equip the reader to develop his/her own phono-amp solution in the most balanced way - and of course, as low-noise as possible. Engine II offers eight different amplifying paths from the Engine's input to its output - solid-state as well as valve driven. To expand the input possibilities via an additional external input, any kind of other linear input amplifiers can be connected. Further, a selection of six highly diverse external input amplifier examples are discussed - BJT, valve, transformer and JFET-driven variants. Although the book primarily focuses on MC cartridge amplification, MM cartridge considerations are not neglected: two further chapters in Part II address methods for increasing the signal-to-noise ratios of MM phono-amps. ...|$|E
40|$|Despite several {{approaches}} to realize subject-to-subject transfer of pre-trained classifiers, the full {{performance of a}} Brain-Computer Interface (BCI) for a novel user can only be reached by presenting the BCI system with data from the novel user. In typical state-of-the-art BCI systems with a supervised classifier, the labeled data is collected during a <b>calibration</b> <b>recording,</b> in which the user is asked to perform a specific task. Based on the known labels of this recording, the BCI’s classifier can learn to decode the individual’s brain signals. Unfortunately, this <b>calibration</b> <b>recording</b> consumes valuable time. Furthermore, it is unproductive {{with respect to the}} final BCI application, e. g. text entry. Therefore, the calibration period must be reduced to a minimum, which is especially important for patients with a limited concentration ability. The main contribution of this manuscript is an online study on unsupervised learning in an auditory event-related potential (ERP) paradigm. Our results demonstrate that the <b>calibration</b> <b>recording</b> can be bypassed by utilizing an unsupervised trained classifier, that is initialized randomly and updated during usage. Initially, the unsupervised classifier tends to make decoding mistakes, as the classifier might not have seen enough data to build a reliable model. Using a constant re-analysis of the previously spelled symbols, these initially misspelled symbols can be rectified posthoc when the classifier has learned to decode the signals. We compare the spelling performance of our unsupervised approach and of the unsupervised posthoc approach to the standard supervised calibration-based dogma for n = 10 healthy users. To assess the learning behavior o...|$|R
50|$|Methods for {{extending}} the <b>calibration</b> and <b>record</b> continuity also {{make use of}} similar calibration activities et al., 2010.|$|R
40|$|This work descripes the {{principles}} of RF spektrum analyzers and generators, introduces the technical specifications of specific instruments and describes the design of computer-controlled measuring system using generator N 9310 A and N 9320 A spectrum analyzer for measuring the value of transmission of two-port blocks. The practical part {{is focused on the}} realization of the program for the operation and measurement automation allowing to set various parameters, system <b>calibration,</b> <b>record,</b> display measured values and verification functions of different equivalent circuit...|$|R
40|$|While {{the concept}} of the pH {{measurement}} that was first published 100 years ago remains essentially the same, significant technological improvements in glass formulation and construction and reference electrode design have increased pH electrode life and performance. Considerable experience has been gained in terms of electrode installation methods for difficult process conditions as the scope of pH applications has expanded Yet much of this practical expertise has not been documented and the ability of embedded diagnostics and <b>calibration</b> <b>records</b> and wireless devices to eliminate common problems and provide smarter and more effective maintenance has not been explored. This paper addresses the essential aspects of each major phase of a successful pH measurement implementation from selection, installation, calibration, to performance monitoring. The relative merits of various measurement types and principles, application strong points and watch-outs, installation practices, factory and field calibration methods, wiring and grounding problems, troubleshooting, and online diagnostics are concisely discussed. The examples focus on demanding industrial applications that require 0. 01 to 0. 02 pH accuracies despite harsh conditions from sterilization-in-place in biopharmaceutical processes and high reaction temperatures, salt, polymer, and crystal concentrations in chemical processes. The paper concludes with recent test results of wireless pH measurements in a bioreactor and looks forward into future improvements...|$|E
40|$|The C- 14 dating {{method is}} the {{cornerstone}} for inferring age estimates for natural archives covering the last 50 000 yrs. However, C- 14 age calibration for {{the last ice age}} relies mostly on records that only indirectly reflect the atmospheric C- 14 concentrations. In consequence, calendar age estimates are significantly more uncertain for the period of the last ice age compared to the past 14000 yrs where tree-ring based <b>calibration</b> <b>records</b> exist. Here we connect a C- 14 tree-ring chronology from Kauri trees in New Zealand to ice core Be- 10 records via the common signal in the galactic cosmic ray flux around the period of the Laschamp geomagnetic field minimum (ca. 41 000 yrs BP). Synchronous changes of modelled C- 14 and C- 14 inferred from U/Th-dated speleothems support the ice core chronology independently and suggest that the published ice core time scale errors are rather conservative for this period. Our analysis puts C- 14 age determinations directly into the context of ice core climate records and it shows that the C- 14 records underlying the C- 14 calibration curve overestimate the atmospheric C- 14 concentration by more than 200 parts per thousand. Consequently, C- 14 age calibration presently yields too old calendar age estimates by about 1200 yrs for this period. (C) 2014 Elsevier B. V. All rights reserved...|$|E
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "The Department of the Interior's (Interior) Minerals Management Service (MMS) collected the equivalent of over $ 9 billion in oil and gas royalties in fiscal year 2007, more than $ 5 billion of which it deposited in the U. S. Treasury; it dispersed the remaining approximately $ 4 billion to other federal, state, and tribal accounts. These royalties [...] payments made {{to the federal government}} for the right to produce oil and gas from federal lands and waters [...] represent one of the country's largest nontax sources of revenue. The amount of oil and gas royalties MMS collects may increase if the price of energy increases and industry's demand to drill on lands and in waters controlled by the federal government continues to trend upward. Companies that develop and produce oil and gas resources from federal lands and waters do so under leases obtained from and administered by Interior [...] BLM for onshore leases and MMS's OEMM for offshore leases. Together, BLM and OEMM are responsible for ongoing oversight of oil and gas operations on more than 28, 000 producing leases to help ensure that oil and gas companies comply with applicable laws, regulations, and agency policies. Among other things, BLM (BLM) and OEMM (OEMM) staff inspect leases to verify that oil and gas production is accounted for as required by the Federal Oil and Gas Royalty Management Act of 1982 and agency regulations and policies. These inspections typically include an examination of the meters and their <b>calibration</b> <b>records.</b> ...|$|E
50|$|This is a {{simplified}} example. The mathematics of the example can be challenged. It {{is important that}} whatever thinking guided this process in an actual <b>calibration</b> be <b>recorded</b> and accessible. Informality contributes to tolerance stacks and other difficult to diagnose post calibration problems.|$|R
40|$|The primary job of an Instrumentation and Data Acquisition System (DAS) Engineer is to {{properly}} measure physical phenomenon of hardware using appropriate instrumentation and DAS equipment designed to record data during a specified {{test of the}} hardware. A DAS system includes a CPU or processor, a data storage device such as a hard drive, a data communication bus such as Universal Serial Bus, software to control the DAS system processes like <b>calibrations,</b> <b>recording</b> of data and processing of data. It also includes signal conditioning amplifiers, and certain sensors for specified measurements. My internship responsibilities have included testing and adjusting Pacific Instruments Model 9355 signal conditioning amplifiers, writing and performing checkout procedures, writing and performing calibration procedures while learning the basics of instrumentation...|$|R
40|$|Dieser Beitrag ist mit Zustimmung des Rechteinhabers aufgrund einer (DFG geförderten) Allianz- bzw. Nationallizenz frei zugänglich. This {{publication}} is {{with permission}} of the rights owner freely accessible due to an Alliance licence and a national licence (funded by the DFG, German Research Foundation) respectively. This contribution reviews how usability in Brain- Computer Interfaces (BCI) can be enhanced. As an example, an unsupervised signal processing approach is presented, which tackles usability by an algorithmic improvement {{from the field of}} machine learning. The approach completely omits the necessity of a <b>calibration</b> <b>recording</b> for BCIs based on event-related potential (ERP) paradigms. The positive effect is twofold - first, the experimental time is shortened and the productive online use of the BCI system starts as early as possible. Second, the unsupervised session avoids the usual paradigmatic break between calibration phase and online phase, which is known to introduce data-analytic problems related to non-stationarity...|$|R
40|$|The Bicron Surveyor MX is a {{portable}} radiation monitoring instrument {{used by the}} Office of Radiation Protection at Oak Ridge National Laboratory. This instrument must be calibrated in order to assure reliable operation. A manual calibration procedure was developed, but it was time consuming and repetitive. Therefore, an automated tester station {{that would allow the}} technicians to calibrate the instruments faster and more reliably was developed. With the automated tester station, <b>calibration</b> <b>records</b> and accountability could be generated and maintained automatically. This allows the technicians to concentrate on repairing defective units. The Automated Bicron Tester consists of an operator interface, an analog board, and a digital controller board. The panel is the user interface that allows the technician to communicate with the tester. The analog board has an analog-to-digital converter (ADC) that converts the signals from the instrument into digital data that the tester can manipulate. The digital controller board contains the circuitry to perform the test and to communicate the results to the host personal computer (PC). The tester station is connected to the unit under test through a special test harness that attaches to a header on the Bicron. The tester sends pulse trains to the Bicron and measures the resulting meter output. This is done to determine if the unit is functioning properly. The testers are connected to the host PC through an RS- 485 serial line. The host PC polls all the tester stations that are connected to it and collects data from those that have completed a calibration. It logs these data and stores the record in a format ready for export to the Maintenance, Accountability, Jobs, and Inventory Control (MAJIC) database. It also prints a report. The programs for the Automated Bicron Tester and the host are written in the C language...|$|E
40|$|The {{objective}} of the WMO-Global Atmosphere Watch program (GAW) is to coordinate the monitoring and research of the changing composition of the atmosphere and is purpose is to understand the impact of atmospheric chemistry on climatic and to evaluate the influence of atmospheric chemistry on the environment. To fullfill ist mission, all data produced in GAW must be of known quality and adequate for their intended use. This {{is a prerequisite for}} their use in environmental decision making. WMO-GAW recognised the importance of quality assurance (QA) and called for the establishment of QA/SAC's to assume this QA-responsibility within the GAW program. The first such center was formed at the Institute for Atmospheric Environmental Research (IFU), Garmisch-Partenkirchen with funds from the BMBF. It was charged with the design and implementation of comprehensive QA procedures covering all GAW measurement parameters (greenhouse gases, ozone, reactive gases, aerosols, aerosol optical depth, precipitation chemistry, meteorology and radiation incl. UV-B), operator training and capacity building. QA/SAC Germany called upon international experts to develop a comprehensive QA-plan which incorporates the elements of scientific quality assurance, namely (1) definition of data quality objectives, (2) quality control at the GAW sites incl. training and (3) quality assessment through system- and performance audits performed {{under the auspices of the}} QA/SAC. All necessary documentation were developed, tested and are now in the implementation phase. The QA/SAC has established so-called World Calibration Facilities in charge of providing traceable <b>calibration</b> <b>records</b> for all GAW parameters and executing the performance audits. In order to provide global coverage, the QA/SAC Germany has been instrumental in setting up two more QA/SAC's, namely in the USA (funded by NOAA, EPA and DOE) and Japan (funded by JMA). (orig.) SIGLEAvailable from TIB Hannover: DtF QN 1 (56, 22) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|E
40|$|Closed Issues: 505 - 0. 00 {{units of}} insulin on enchilada 553 MQTT Client Id {{needs to be}} more unique 552 Calibration Pushover doesn't use DISPLAY_UNITS 523 Use the {{selected}} time format for tool tips 513 When alarm is cleared from another device the BG turns gray bug 501 Merge SVG and Sensor entries received via MQTT 499 Ignore sensor noise level when Dex sends special value SGV 464 dead code 449 Events in 24 hr format are not shown in graph duplicate 418 Improvement: Sound an alarm when data hasn't been received in n minutes 407 24 hour time format is not working for lower graph bug 396 Pebble MMOL settings issues 361 show <b>calibration</b> <b>records</b> in pebble enhancement 272 MQTT Merged PRs: 566 update logos and favicon 560 Clean up some entries code and increase coverage 558 Better default MQTT clientid 554 Client side alarm for stale data 539 use new sysTime field to prevent dupes caused by jitter 536 use selected time format for tooltips and x axises 534 Show raw during startup when noise changes to 1 before there is a BG 518 fix a bug that caused a treatment's glucose value to be ignored 514 Treatment hemispheres position doesn't always use entered BG bug 511 enable is optional 506 display of noise level and rawbg value when rawbg is enabled 504 allow API to search things better 498 Smoother opening/closing of the settings and treatment drawers 497 better future data warning 496 context range fix 492 Compression and pebble. js mongo fixes 489 does compression interfere with socket. io? 488 respect potential I/O errors in pebble response 484 Display uploader battery next to time ago 483 Support for an adjustable focus range 482 use an updated font to include an hourglass glyph 481 better treatment BG unit handling 480 fixed issue with delta always being 0 in /pebble 476 Always return the last Cal entry (don't worry about how old it might be) 471 No more % based font-sizes!!! 466 simplify boolean assignment 465 rm dead code 452 Add an option to enable IOB in the mainline 439 Port profile api from wip/iob-cob 190 MQT...|$|E
40|$|There {{is a step}} of {{significant}} difficulty experienced by brain-computer interface (BCI) users when going from the <b>calibration</b> <b>recording</b> to the feedback application. This effect has been previously studied and a supervised adaptation solution has been proposed. In this paper, we suggest a simple unsupervised adaptation method of the linear discriminant analysis (LDA) classifier that effectively solves this problem by counteracting the harmful effect of nonclass-related nonstationarities in electroencephalography (EEG) during BCI sessions performed with motor imagery tasks. For this, we first introduce three types of adaptation procedures and investigate them in an offline study with 19 datasets. Then, we select one of the proposed methods and analyze it further. The chosen classifier is offline tested in data from 80 healthy users and four high spinal cord injury patients. Finally, {{for the first time}} in BCI literature, we apply this unsupervised classifier in online expe riments. Additionally, we show that its performance is significantly better than the state-of-the-art supervised approach...|$|R
5000|$|Audiometry (hearing test). This must be {{undertaken}} using equipment that {{conforms to the}} appropriate standards and guidelines specified with regular <b>recorded</b> <b>calibration</b> and maintenance. The subject should meet specific standards.|$|R
40|$|Radiocarbon dating is {{the most}} {{commonly}} used chronological tool in archaeological and environmental sciences dealing with the past 50, 000 years, making the radiocarbon calibration curve {{one of the most important}} records in paleosciences. For the past 12, 560 years, the radiocarbon calibration curve is constrained by high quality tree-ring data. Prior to this, however, its uncertainties increase rapidly due to the absence of suitable tree-ring 14 C data. Here, we present new high-resolution 14 C measurements from 3 floating tree-ring chronologies from the last deglaciation. By using combined information from the current radiocarbon calibration curve and ice core 10 Be records, we are able to absolutely date these chronologies at high confidence. We show that our data imply large 14 C-age variations during the Bølling chronozone (Greenland Interstadial 1 e) - a period that is currently characterized by a long 14 C-age plateau in the most recent IntCal 13 <b>calibration</b> <b>record.</b> We demonstrate that this lack of structure in IntCal 13 may currently lead to erroneous calibrated ages by up to 500 years...|$|R
40|$|Maintaining {{consistent}} traceability of high-precision {{measurements of}} CO 2 isotopes {{is critical in}} order to obtain accurate atmospheric trends of δ 13 C and δ 18 O (in CO 2). Although a number of laboratories/organizations around the world have been conducting baseline measurements of atmospheric CO 2 isotopes for several decades, reports on the traceability and maintenance are rare. In this paper, a principle and an approach for maintaining consistent traceability in high-precision isotope measurements (δ 13 C and δ 18 O) of atmospheric CO 2 are described. The concept of Big Delta is introduced and its role in maintaining traceability of the isotope measurements is described and discussed extensively. The uncertainties of the traceability have been estimated based on annual <b>calibration</b> <b>records</b> over the last 10 yr. The overall uncertainties of CO 2 isotope measurements for individual ambient samples analyzed by the program at Environment Canada have been estimated (excluding these associated with the sampling). The values are 0. 02 and 0. 05 ‰ in δ 13 C and δ 18 O, respectively, which are close to the World Meteorological Organization (WMO) targets for data compatibility. The annual rates of change in δ 13 C and δ 18 O of the primary anchor (which links the flask measurements back to the VPDB-CO 2 scale) are close to zero (− 0. 0016 ± 0. 0012 ‰, and − 0. 006 ± 0. 003 ‰ per year, respectively) over a period of 10 yr (2001 – 2011). The average annual changes of δ 13 C and δ 18 O in air CO 2 at Alert GAW station over the period from 1999 to 2010 have been evaluated and confirmed; they are − 0. 025 ± 0. 003 ‰ and 0. 000 ± 0. 010 ‰, respectively. The results are consistent with a continuous contribution of fossil fuel CO 2 to the atmosphere, having a trend toward more negative in δ 13 C, whereas the lack of change in δ 18 O likely reflects the influence from the global hydrologic cycle. The total change of δ 13 C and δ 18 O during this period is ~ 0. 27 ‰ and ~ 0. 00 ‰, respectively. Finally, the challenges and recommendations as strategies to maintain a consistent traceability are described...|$|E
40|$|M. A. University of Hawaii at Manoa 2011. Includes bibliographical references. The {{amount of}} solar {{radiation}} that reaches the Earth's surface is modulated by fluctuations in atmospheric gases including water vapor, {{the density of}} aerosols and cloud optical depth. Regional trends in solar radiation have been observed in various places around the Earth, but {{little research has been}} done on solar radiation trends in Hawaiʻi. To address this, data obtained from upper elevation climate stations within the HaleNet climate network on the Island of Maui, Hawaiʻi were evaluated for trends. First, solar radiation was modeled using the SPECTRAL 2 clear day radiation model and tested against observations. Solar radiation was modeled using a series of model parameterization scenarios at different temporal resolutions, to determine the scenario that most accurately represented clear sky radiation at the points of observations. Second, the modeled solar radiation was compared with observations to identify and correct instrument response changes over time. Thirdly, a long-term record of solar radiation was evaluated for trends at three upper elevation stations. Solar radiation was modeled at three HaleNet locations along a 1630 -m elevation gradient on the slopes of Haleakalā. The model was parameterized using daily, monthly, and annual mean input scenarios. The model tended to overestimate clear sky solar radiation at all stations under three input scenarios for a 1 -year test period, but nevertheless produced satisfactory results with regression slopes all within 0. 03 of unity and RMSE values in the range of 18. 9 [...] 38. 0 W m- 2. The model performed best at these stations when input parameters were held constant at their annual mean values. Clear day solar radiation was modeled using a fixed parameterization scenario that was determined to give the best simulation of clear sky radiation at the HaleNet stations. A clear day ratio (CDR) of the observed to modeled clear day radiation was established for the ~ 21 year record at each station. Historical <b>calibration</b> <b>records</b> were used to assess time periods where sensors were either recalibrated or replaced. Apparent shifts in sensor response were assessed using a linear regression analysis including the affects of autocorrelation. Using the mean CDR for a given time period or a linear regression equation (when a significant trend in CDR was identified), solar radiation was adjusted to give the CDR during a time when instrument calibration was deemed to be the most reliable. A long-term record of solar radiation at three HaleNet stations was analyzed for trends. Annual anomalies were calculated as departures from the respective long-term mean and a clear day index was established as the ratio of monthly averaged solar radiation to calculated monthly extraterrestrial radiation {{at the top of the}} atmosphere. Trends were assessed for all available monthly data, and two 6 -month seasonal periods, which roughly correspond to the wet and dry seasons in Hawaiʻi. Positive trends in annual anomalies were found at all three experimental stations (0. 29 to 0. 9 W m- 2 per yr.), however, these trends were not statistically significant. During the June to November (dry) seasonal period, statistically significant positive trends were shown (1. 1 to 1. 7 W m- 2 per yr.), and maximum positive anomaly was observed in 2010. Clear day index results were consistent with these findings, indicating that cloud cover has decreased over time at high elevations on Maui...|$|E
40|$|Laboratory {{identification}} of radionuclides at environmental concentrations {{can easily be}} mistaken, because many energy interferences (coincident or overlapping spectral peaks) are possible. Conventional laboratory quality control measurements are typically not designed to test interferences found in real samples. In order to evaluate the occurrence of radiological false positives in environmental soil and groundwater samples collected at the Savannah River Site, instrument printouts, <b>calibration</b> <b>records,</b> and procedure manuals were examined between 1997 and 2001 at five commercial radiological laboratories. False positives of many radionuclides {{were found to be}} routinely reported at all five laboratories; causes vary. Magnitudes were generally between 0. 1 and 3 pCi/g in soils, and between 2 and 40 pCi/L in groundwater, within the range of possible concern to regulators. The frequency of false positives varied, but for several nuclides listed below, nearly every detection reported in SRS environmental samples during the study period was judged to be false. Gamma spectroscopy: Low-level false positives of Mn- 54, Zr- 95, Eu- 155, and Np- 239 were reported in many soil samples from four laboratories, due to interference from naturally occurring Tl- 208, Pb- 212, and Ac- 228. There were two causes. First, laboratories did not include low abundance (less than 2 per cent) peaks of Tl- 208, Pb- 212, and Ac- 228 in the libraries used by instrument software for interference correction. Second, even when instrument software rejected the identifications, incorrectly identified nuclides were still often reported as detected, because some labs' data management systems were not able to distinguish between software-accepted nuclides and software-rejected nuclides. Magnitudes of these false positives are usually below 1 pCi/g. Alpha spectroscopy: Incomplete chemical separation and breakthrough of natural radionuclides and/or daughters of laboratory tracers generate false positives in soils up to about 5 pCi/g. Examples include natural Th- 228 causing false positives of Am- 241 and Pu- 238; natural U- 234 being mistaken for Np- 237; and natural Ra- 224 generating false positives of Cm- 243 / 244. Peak taildown of tracers or other nuclides into neighboring peak windows can also generate false positives in both soils and groundwater. Examples include taildown of Th- 229 tracer into the Th- 230 spectral window, and U- 234 taildown causing false positives of U- 235. Liquid Scintillation: Incomplete chemical separation may generate false positives of virtually any Liquid Scintillation nuclide. High tritium content in Savannah River Site groundwater causes false positives of C- 14. Actinides produce false positives of Pm- 147. Most false positives in alpha spectroscopy and liquid scintillation counting at commercial labs are due to incomplete separation of the target nuclide from interferors. However, quality control measurements such as matrix spikes and chemical yield are of ten unable to identify cases where interfering nuclides break through into the final preparation. In addition, laboratory personnel often fail to manually interpret sample spectra, and thus do not notice peak interference when it occurs. In fact, some laboratories are unable to produce energy spectra for liquid scintillation or alpha spectroscopy, making recognition of peak interference virtually impossible. Gamma spectroscopy of environmental samples usually does not involve chemical separation, so peak interference may occur in every sample. Interference correction software appears to be ineffective at the concentrations seen in environmental samples. At all labs, careful manual review of printouts appeared to be lacking when the study was performed. Some labs have improved their data review processes since then...|$|E
40|$|An {{expanded}} Cariaco Basin 14 C chronology {{is tied to}} 230 Th-dated Hulu Cave speleothem {{records in}} order to provide detailed marine-based 14 C calibration for the past 50, 000 years. The revised, high-resolution Cariaco 14 C <b>calibration</b> <b>record</b> agrees well with data from 230 Th-dated fossil corals back to 33 ka, with continued agreement despite increased scatter back to 50 ka, suggesting that the <b>record</b> provides accurate <b>calibration</b> back to the limits of radiocarbon dating. The calibration data document highly elevated Delta 14 C during the Glacial period. Carbon cycle box model simulations show that the majority of observed Delta 14 C change can be explained by increased 14 C production. However, from 45 to 15 ka, Delta 14 C remains anomalously high, indicating that the distribution of radiocarbon between surface and deep ocean reservoirs was different than it is today. Additional observations of the magnitude, spatial extent and timing of deep ocean Delta 14 C shifts are critical for a complete understanding of observed Glacial Delta 14 C variability...|$|R
40|$|Designing a {{self-paced}} brain computer interface (BCI) {{that works}} reliably across human subjects {{has been a}} challenge. Of particular interest are simple BCIs that enable detection of an Intentional Control (IC) state {{against a background of}} No Control (NC) state. Such BCIs are known as brain switches or BCI switches. One of the possible methods to build a BCI switch is based on the consistent increase in the alpha component of the EEG spectrum when subjects close their eyes. The present work proposes a simple approach to achieve automatic user customization with just one minute of <b>calibration</b> <b>recording.</b> This design is evaluated based on thirty trials on each of the seven healthy subjects. The results validate the robustness of the proposed IC detection approach to variations across sessions and subjects. The average time required to activate the switch is typically two to three seconds after eye closure. These results suggest that any user with the ability to modulate their alpha rhythm by eye closure can effectively utilize the proposed BCI switch as an assistive technology to gain increased control over their environment...|$|R
50|$|On October 8, 2007 the EP Omar Rodriguez-Lopez & Lydia Lunch, a {{collaboration}} with spoken word poet Lydia Lunch, was released. The Apocalypse Inside of an Orange {{is a double}} LP featuring the original quintet and was released on vinyl November 20, 2007. It was also released for digital download. <b>Calibration,</b> a <b>record</b> that Rodríguez-López recorded during his stay in Amsterdam, was released February 5, 2008. It was described as being influenced by electronic music and acid-jazz.|$|R
40|$|We {{present a}} novel {{algorithm}} for efficient removal of rolling shutter distortions in uncalibrated streaming videos. Our proposed method is calibration free {{as it does}} not need any knowledge of the camera used, nor does it require <b>calibration</b> using specially <b>recorded</b> <b>calibration</b> sequences. Our algorithm can perform rolling shutter removal under varying focal lengths, as in videos from CMOS cameras equipped with an optical zoom. We evaluate our approach across {{a broad range of}} cameras and video sequences demonstrating robustness, scaleability, and repeatability. We also conducted a user study, which demonstrates preference for the output of our algorithm over other state-of-the art methods. Our algorithm is computationally efficient, easy to parallelize, and robust to challenging artifacts introduced by various cameras with differing technologies. 1. ...|$|R
40|$|We {{observed}} cycles {{presented in}} a luminescent solar insolation proxy record from a speleothem from Jewel Cave, South Dakota, US. We found cycles of orbital precession with periods of 23 and 19 ka and of obliquity of 41 ka and many others from non- orbital origin in this sample. We determined the Solar origin of the cycles with durations of 11500, 4400, 3950, 2770, 2500, 2090, 1960, 1670, 1460, 1280, 1195, 1145, 1034, 935, 835, 750 and 610 years. It was done by their detection both in proxy records of speleothem luminescence, D 14 C {{and the intensity of}} the geomagnetic dipole. It is well known that the main variations in the last two records are produced by the solar wind. The most intensive cycle discovered in this record has duration of 11. 5 ka. It is not of orbital origin. It was found previously to be the most intensive cycle in the D 14 C <b>calibration</b> <b>record</b> and has been interpreted to be of terrestrial origin because “it is too strong tobe of solar origin”. Our studies suggest that it should be a solar cycle modulating the geomagnetic field and 14 C reversed production as the other solar cycles do...|$|R
40|$|Multiple {{channels}} of electromyogram activity are frequently transduced via electrodes, then combined electronically to form one electrophysiologic recording, e. g. bipolar, linear double difference and Laplacian montages. For high quality recordings, precise gain and frequency response matching {{of the individual}} electrode potentials is achieved in hardware (e. g., an instrumentation amplifier for bipolar recordings). This technique works well {{when the number of}} derived signals is small and the montages are pre-determined. However, for array electrodes employing a variety of montages, hardware channel matching can be expensive and tedious, and limits the number of derived signals monitored. This report describes a method for channel matching based on the concept of equalization filters. Monopolar potentials are recorded from each site without precise hardware matching. During a calibration phase, a time-varying linear chirp voltage is applied simultaneously to each site and recorded. Based on the <b>calibration</b> <b>recording,</b> each monopolar channel is digitally filtered to “correct ” for (equalize) differences in the individual channels, and then any derived montages subsequently created. In a hardware demonstration system, the common mode rejection ratio (at 60 Hz) of bipolar montages improved from 35. 2 ± 5. 0 dB (prior to channel equalization) to 69. 0 ± 5. 0 dB (after equalization) ...|$|R
40|$|The Landsat- 4 Thematic Mapper {{collected}} {{imagery of}} the Earth's surface from 1982 to 1993. Although largely overshadowed by Landsat 5, which was launched in 1984, Landsat 4 TM imagery extends the Thematic Mapper-based record of the Earth back to 1982 and also substantially supplements the image archive collected by Landsat 5. To provide a consistent <b>calibration</b> <b>record</b> for the TM instruments, Landsat 4 TM was cross-calibrated to Landsat 5 using nearly simultaneous overpass imagery of pseudo-invariant calibration sites (PICS) in the time period of 1988 through 1990. To determine if the radiometric gain of Landsat 4 had changed over its lifetime, time series from two PICS locations, a Saharan site known as Libya 4 and a site in southwest North America, {{commonly referred to as}} the Sonoran Desert PICS, were developed. Results indicated that Landsat 4 had been very stable over its lifetime with no discernible degradation in sensor performance in all the reflective bands except band 1. In contrast, band 1 exhibited a 12 % decay in responsivity over the lifetime of the instrument. Results from this work have been implemented at USGS EROS, which enables users of Landsat TM data sets to obtain consistently calibrated data from Landsat 4 and 5 TM as well as Landsat 7 ETM+ instruments...|$|R
40|$|American Sign Language (ASL) {{generation}} {{software can}} improve the accessibility of information and services for deaf individuals with low English literacy. The understandability of current ASL systems is limited; they have been constructed {{without the benefit of}} annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, <b>calibration,</b> and <b>recording</b> protocol, we conducted an evaluation study with native ASL signers. ...|$|R
40|$|PM 10 {{monitoring}} {{networks are}} equipped with heterogeneous samplers. Some of these samplers are known to underestimate true levels of concentrations (non-reference samplers). In this {{paper we propose a}} hierarchical spatio-temporal Bayesian model for the <b>calibration</b> of measurements <b>recorded</b> using non-reference samplers, by borrowing strength from non co-located reference sampler measurements...|$|R
