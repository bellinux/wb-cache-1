8|10000|Public
2500|$|Since joining The X Factor and Australia's Got Talent in 2007, Minogue {{has become}} a Style Icon in Australia, Ireland and the United Kingdom, {{receiving}} critical acclaim from various fashion designers such as Victoria Beckham and wearing dresses from J'Aton Couture, Antonio Berardi, Dolce & Gabbana, Marchesa, Philip Armstrong, Carla Zampatti, Gucci and Aurelio Costarella and has featured on fashion magazines like Cosmopolitan, InStyle and Vogue. The press in Britain have especially taken notice of her sense of fashion and different hair styles since Cheryl Cole joined The X Factor in 2008 often <b>comparing</b> <b>both</b> <b>of</b> <b>them.</b> The praise Minogue got from the tabloids on The X Factor lead her to set up her own line called Project D along with a fragrance. The first line from Project D by Dannii and Tabitha was sold exclusively by Selfridges in the United Kingdom, the Spring / Summer line was showcased by Minogue during the live first Sunday night show of The X Factor Season 7, wearing her Jingle prom-style dress. In August 2012 the label was rebranded and relaunched as Project D London. On 5 July 2013 Minogue announced that she {{would no longer be}} associated with Project D London, ending a three-year collaboration with Tabitha Somerset-Webb.|$|E
50|$|There are {{differences}} between TSE and ESE. There are survey results <b>comparing</b> <b>both</b> <b>of</b> <b>them.</b> The survey result shown that TSE and ESE is complementary and interdependent {{with each other}} which ESE has a higher rating while TSE {{could also be a}} hidden element for ESE. So the combination of TSE and ESE will be ideal for an enterprise in this generation.|$|E
5000|$|Since joining The X Factor and Australia's Got Talent in 2007, Minogue {{has become}} a Style Icon in Australia, Ireland and the United Kingdom, {{receiving}} critical acclaim from various fashion designers such as Victoria Beckham and wearing dresses from J'Aton Couture, Antonio Berardi, Dolce & Gabbana, Marchesa, Philip Armstrong, Carla Zampatti, Gucci and Aurelio Costarella and has featured on fashion magazines like Cosmopolitan, InStyle and Vogue. The press in Britain have especially taken notice of her sense of fashion and different hair styles since Cheryl Cole joined The X Factor in 2008 often <b>comparing</b> <b>both</b> <b>of</b> <b>them.</b> The praise Minogue got from the tabloids on The X Factor lead her to set up her own line called Project D along with a fragrance. The first line from Project D by Dannii and Tabitha was sold exclusively by Selfridges in the United Kingdom, the Spring / Summer line was showcased by Minogue during the live first Sunday night show of The X Factor Season 7, wearing her Jingle prom-style dress. In August 2012 the label was rebranded and relaunched as Project D London. On 5 July 2013 Minogue announced that she {{would no longer be}} associated with Project D London, ending a three-year collaboration with Tabitha Somerset-Webb.|$|E
40|$|We {{study the}} total inelastic gamma gamma {{cross-section}} and discuss predictions from different models, {{with a special}} attention to their dependence on the input parameters. In particular we examine the results from a simple extension of the Regge Pomeron exchange model and those from the eikonalized mini-jet model. We then <b>compare</b> <b>both</b> <b>of</b> <b>them</b> with recent LEP data...|$|R
40|$|Dream {{and love}} {{represent}} experiences which, {{under certain conditions}} and within certain limits, can be mutually <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> reflect, in fact, a structure of depossession and derealization, in which subject and the present find themselves deprived of their primacy. Dream and love represent, in the last analysis, a secret both banal and ordinary, universal and public, that continues to put in question {{the very nature of}} what we call truth...|$|R
40|$|In {{this paper}} two {{different}} password databases are <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> had only Hungarian human-generated passwords. The first database contained user passwords {{relating to the}} authorization mechanism of online banking transactions, while the second one contained passwords relating to the authorization of opening newsletters. The first aim {{of this paper was}} to analyze both databases and find differences and similarities between them. The second aim was to develop a method of decrypting the most Hungarian hashed passwords in a given time unit...|$|R
40|$|ABSTRACT The {{research}} about Thermoluminicent Dosimeter (TLD) response analyze on Cs- 137 source and X-ray with different variation, there are 0, 2 mSv, 0, 5 mSv, 1, 0 mSv was conducted {{for determining the}} variation response and <b>comparing</b> <b>both</b> <b>of</b> <b>them.</b> Before the radiation was used, the TLD was anneling for cleaning the TLD from remaining electron. The radiation result using Cs- 137 showed the average error about 8, 247 %, while using X-ray showed the average about 16, 94 %. The obtained error from both of them showed us that the average error is less then 20 %, which is the definite standart. The TLD was used with LiF materials and Mg, Cu, P actifators. This TLD have the very high sensitivity...|$|E
40|$|As {{using the}} {{classical}} quasi-steady state (QSS) model {{could not be}} able to accurately simulate the dynamic characteristics of DC transmission and its controlling systems in electromechanical transient stability simulation, when asymmetric fault occurs in AC system, a modified quasi-steady state model (MQSS) is proposed. The model firstly analyzes the calculation error induced by classical QSS model under asymmetric commutation voltage, which is mainly caused by the commutation voltage zero offset thus making inaccurate calculation of the average DC voltage and the inverter extinction advance angle. The new MQSS model calculates the average DC voltage according to the actual half-cycle voltage waveform on the DC terminal after fault occurrence, and the extinction advance angle is also derived accordingly, so as to avoid the negative effect of the asymmetric commutation voltage. Simulation experiments show that the new MQSS model proposed in this paper has higher simulation precision than the classical QSS model when asymmetric fault occurs in the AC system, by <b>comparing</b> <b>both</b> <b>of</b> <b>them</b> with the results of detailed electromagnetic transient (EMT) model of the DC transmission and its controlling system...|$|E
40|$|This paper {{appraised}} {{and compared}} concisely the Ethiopian Value added tax (VAT) and Sales Tax. No extensive {{research has been}} done in these essential and critical features of the comparison of sales tax with VAT in the Ethiopian context. And this paper aims at <b>comparing</b> <b>both</b> <b>of</b> <b>them</b> by focusing on the general context of tax base, tax rate, the methods applied to implement and administer them based on the proclamations and regulations available for each. Replacing the outdated sales tax, VAT is still the most fashionable taxing system in Ethiopia. The paper revealed that there are some common features and different structures among VAT and sales tax. Both of them are consumption tax and revenue is collected from some entity other than the entity that actually bears the tax cost. While sales tax levied on the total value of an economic exchange, VAT comprehensively applied on all exchanges used in the production, distribution and consumption process. In addition, while sales tax has reduced but no zero rates, VAT has zero rates without reduced rate. As compared to sales tax, VAT contributed positively for the economy of one country if it is implemented and administered appropriately. Otherwise, the problem of sales tax continues in the VAT system too...|$|E
40|$|The present work {{presents}} a preliminary {{study on the}} effect of the filler distribution on the elastic modulus of a nanoparticle filled polymer. To this end, two different theoretical approaches are implemented and <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> account for an interphase layer embedding the nanoparticle, with mechanical properties {{different from those of the}} matrix. Conversely, only one <b>of</b> <b>them</b> accounts for the variation of the interparticle distance. The comparison between these models allows to draw some conclusions {{on the effect of}} the filler distribution...|$|R
30|$|When a node blocks or fails, SDR {{starts to}} become a {{decreasing}} trend. The SDR of TMRJam is {{lower than that of}} TMR. Meanwhile, SDR in NCMRJam is higher than that of NCMR. And the question is about how it can be evaluated, and which scheme can effectively resist such kinds <b>of</b> interferences. To <b>compare</b> <b>both</b> <b>of</b> <b>them</b> fairly, rate <b>of</b> decline is employed and considered as a criterion to evaluate the anti-jamming abilities. The rate of decline of TRMJam is equal to (TMR-TMRJam)/TMR, and the rate of decline of NCMRJam can be obtained with similar method.|$|R
40|$|APE {{smearing}} and overlap-Dirac operator {{are combined}} to filter QCD vacuum configurations. The results obtained from overlap fermions and improved 5 Li cooling are <b>compared,</b> <b>both</b> <b>of</b> <b>them</b> exhibit structures <b>of</b> dilute liquid of instanton. Finally the overlap fermions, improved 5 Li cooling and APE smearing are combined {{to calculate the}} topological charge and identify the structure of QCD vacuum. The results suggest dilute liquid of instanton dominance of topological charge fluctuations in quenched lattice QCD. Comment: 5 pages, 3 figures, 2 tables talk presented on the SINO-Japan nuclear physics collaboration meeting, May 15 - 20, Shanghai, Chin...|$|R
40|$|Treball de Final de Màster Universitari en Professor/a d'Educació Secundària Obligatòria i Batxillerat, Formació Professional i Ensenyaments d'Idiomes. Codi: SAP 419. Curs acadèmic 2016 / 2017 New {{technologies}} {{have become an}} important part of our lives. Nowadays, we are surrounded by mobile phones, computers, tablets that have changed our daily habits. We check our Whatsapp every day, we can communicate via video with physically distanced people instead of only as a phone call, we use them for specific purposes with the creation of applications that meet our interests {{and so on and so}} forth. One of the fields in which technologies play a relevant role is education, and within it language learning can benefit most specially through digital tools because languages and technologies are about communication and interaction. The paper here presented is intended to explore that relationship by means of three tools (i. e. Glogster, Kahoot and Spreaker), and in particular how they can be used in a secondary school, so that technologies play in favour of language learning in this stage of education. The modality opted for with this paper is the one called “Didactic materials” following the present regulation for final projects in the Máster Universitario en Profesor/a de Educación Secundaria Obligatoria y Bachillerato, Formación Profesional y Enseñanza de Idiomas. Main results of this study suggest that e-tools are interesting for secondary students, although not enough manageable for them and thus there is a need to make them more intuitive than they are, at least as far as Glogster is concerned. With another tool, Kahoot, results prove that it is more easily matching students’ needs. Therefore, it is likely that simplification in e-tools works better in high schools rather than others that require more time to learn how they work, which is the main conclusion reached <b>comparing</b> <b>both</b> <b>of</b> <b>them...</b>|$|E
40|$|Maritime {{communication}} {{plays an}} important role in the marine activities. However, insufficient knowledge of radio channel characteristics over the sea is a limiting factor in the development of wideband wireless communication systems for maritime applications. Therefore, in order to increase understanding of the radio channel over sea, two land-toship (L 2 S) radio channel measurement campaigns have been performed in Trondheim, Norway. Two scenarios are investigated in the MarCom project to cover a variety of maritime environments: 1) short-distance open sea environment. 2) long-distance (≈ 45 km) open sea environment. Channel characteristics such as path-loss, spatial channel correlation, power delay profile, mean excess delay and RMS delay spread obtained from the channel measurements are analyzed. The received signal levels (RSL) have been compared with the Okumura-Hata model, the COST 231 -Hata model and the ITU-R P. 1546 - 2 model. It has been found that the ITU-R model for open cold sea corresponding to field-strength value exceeded at 50 % of the locations gives the best fit with themeasured results. By using two receiver antennas with 1. 85 and 2. 9 meters spacing for the two scenarios respectively, the radio channel is shown to be partly correlated (spatial correlation coefficient for the received signals from the two RX antennas varies from 0. 3 to 0. 9) when the distance between the transmitter (TX) and receiver (RX) is within 2 km, while the two channels are found to be highly correlated when the boat is far away from the shore. The ship turning and the shadowing effect caused by other ships might, however, reduce the spatial correlation between the two RX channels. Power delay profile (PDP) is studied by classifying the channels into two groups: frequency-selective channel close to the harbor and non frequency-selective channel at a long TX-RX distance. The mean excess delay and RMS delay spread have been investigated. The PDF of the mean excess delay and the RMS delay spread for the short-distance measurement can be modeled by using a three-term Gaussian model. Few reflected rays are found except for the reflected rays from the sea surface with a small difference from the LOS in delay, which means that there is no need to model the PDFs for the long-distance measurement due to fewer reflections from the shore. Based on measured results on RSL and classic path-loss models, two path-loss models for the open sea environments are proposed: a) Round Earth Loss (REL) model and b) Quasi-Deterministic (QD) model. The REL model has been proposed base on a geometrical model of round Earth together with effects on the radio link that are the Free Space Loss (FSL), effective reflection from sea roughness, divergence and diffraction loss. The Quasi-deterministic model has been invented by utilizing the merits of both the ITU-R model and the Plane Earth Loss (PEL) model after <b>comparing</b> <b>both</b> <b>of</b> <b>them</b> with the longdistance path loss measurement results. Both models match the measured results very well.   PhD i elektronikk og telekommunikasjonPhD in Electronics and Telecommunicatio...|$|E
40|$|There {{are some}} method to design {{flexible}} pavement, two <b>of</b> <b>them</b> are Bina Marga Component Analysis and NCSA method. <b>Both</b> <b>of</b> these methods really have difference. Thus, writer was interested {{to have a}} research. The aim {{of this research is}} <b>compare</b> <b>both</b> <b>of</b> <b>them</b> on the depth of pavement design. On this paper <b>both</b> <b>of</b> this method used to design Trisakti–Liang Anggang trip by A. Yani higway. On this study, NCSA method resulted D 1 = 5 cm, D 2 = 10 cm, D 3 = 17, 5 cm and Bina Marga Component Analysis method resulted D 1 = 5 cm, D 2 = 10 cm, D 3 = 27 cm...|$|R
5000|$|... #Caption: The mythological {{creature}} Phoenix, to whom Moltres {{has been}} <b>compared</b> due to <b>both</b> <b>of</b> <b>them</b> being fire birds..|$|R
40|$|AbstractThis paper {{presents}} {{the process to}} carry out performance calibration of Articulated Arm Coordinate Measuring Machines according to ASME B 89. 4. 22 Standard. The growing use of this class of measurement equipment {{has been accompanied by}} an absence of authorized laboratories to provide calibration certificates for AACMM's performance. Due to ASME B 89. 4. 22 and VDI 2617 - 9 are nowadays the unique standards in the field of AACMM verification, IK 4 Tekniker has <b>compared</b> <b>both</b> <b>of</b> <b>them</b> in order to develop internal test procedures to yield reliable performance calibration results. IK 4 Tekniker has achieved ENAC accreditation in the field of AACMM calibration. Internal test procedures and uncertainty evaluation analysis have been developed, as well as ENAC certificated reference test equipments have been acquired...|$|R
40|$|Psychological {{acceptance}} (acceptance) {{and emotional}} intelligence (EI) are two relatively new individual characteristics that are hypothesised to affect well-being and performance at work. This study <b>compares</b> <b>both</b> <b>of</b> <b>them,</b> in terms <b>of</b> {{their ability to}} predict various well-being outcomes (i. e. general mental health, physical well-being, and job satisfaction). In making this comparison, the effects of job control are accounted for; this is a work organisation variable that is consistently associated with occupational health and performance. Results from 290 United Kingdom workers showed that EI {{did not significantly predict}} any of the well-being outcomes, after accounting for acceptance and job control. Acceptance predicted general mental health and physical well-being but not job satisfaction, and job control was associated with job satisfaction only. Discussion focuses on the theoretical and applied implications of these findings. These include support for the suggestion that not controlling one's thoughts and feelings (as advocated by acceptance) may have greater benefits for mental well-being than attempting consciously to regulate them (as EI suggests) ...|$|R
40|$|BACKGROUND AND PURPOSE: Intensity-modulated {{radiation}} therapy (IMRT) has shown its superiority to three-dimensional conformal radiotherapy {{in the treatment}} of prostate cancer. Different optimization algorithms are available: algorithms which first optimize the fluence followed by a sequencing (IM), and algorithms which involve the machine parameters directly in the optimization process (DSS). The aim of this treatment-planning study is to <b>compare</b> <b>both</b> <b>of</b> <b>them</b> regarding dose distribution and treatment time. PATIENTS AND METHODS: Ten consecutive patients with localized prostate cancer were enrolled for the planning study. The planning target volume and the rectum volume, urinary bladder and femoral heads as organs at risk were delineated. Average doses, the target dose homogeneity H, D(5), D(95), monitor units per fraction, and the number of segments were evaluated. RESULTS: While there is only a small difference in the mean doses at rectum and bladder, there is a significant advantage for the target dose homogeneity in the DSS-optimized plans compared to the IM-optimized ones. Differences in the monitor units (nearly 10...|$|R
40|$|Tuning of {{parallel}} applications requires {{the use of}} effective tools for detecting performance bottlenecks. Many of the current program analysis tools provide only statistical summaries. This may hide individual situations of performance loss that occur along a parallel program execution. We believe that exhaustive and time [...] aware event collection at fine [...] grain level is essential to capture this kind of situations. This paper presents a tracing mechanism based on code interposition and a low [...] overhead tracing library implementation for the usual compiler [...] directed code injection. It also <b>compares</b> <b>both</b> <b>of</b> <b>them</b> and evaluates the overhead introduced in the execution of the application, distinguishing by the one introduced by the tracing tools and the one introduced by system mechanisms needed for some specific performance analysis (i. e. hardware counters). Both tracing mechanisms are used to collect detailed traces that feed an analysis and visualization tool (Paraver). The whole environ [...] ...|$|R
40|$|Two di erent {{methods for}} {{establishing}} a space-like Coulomb sum rule for the relativistic Fermi gas are <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> divide the charge response by a normalizing factor {{such that the}} reduced response thus obtained ful lls the sum rule at large momentum transfer. To determine the factor, in the rst approach one exploits the scaling property of the longitudinal response function, while in the second one enforces the completeness of {{the states in the}} space-like domain via the Foldy-Wouthuysen transformation. The energy-weighted and the squared-energy-weighted sum rules for the reduced responses are explored as well and the extension to momentum distributions that are more general than a step-function is also considered. The two methods yield reduced responses and Coulomb sum rules that saturate in the non-Pauli-blocked region, which can hardly be distinguished for Fermi momenta appropriate to atomic nuclei. Notably the sum rule obtained in the Foldy-Wouthuysen approach coincides with the well known non-relativisti...|$|R
40|$|Generally any {{real-world}} {{problem is not}} always solvable, because in that not only a percentage of uncertainty is present, but also, {{a certain percentage of}} indeterminacy is present. The presence of uncertainty has been analyzed using fuzzy logic. In this book the amount of indeterminacy is being analyzed using neutrosophic logic. Most of these models use the concept of matrices. Matrices have certain limitation; when the models are time-dependent and any two experts' opinions are being studied simultaneously, one cannot <b>compare</b> <b>both</b> <b>of</b> <b>them</b> at each stage. The new concept of bimatrices would certainly cater to these needs. A bimatrix AB = A 1 U A 2, where A 1 and A 2 are distinct matrices of arbitrary order. This book introduces the concept of bimatrices, and studies several notions like bieigen values, bieigen vectors, characteristic bipolynomials, bitransformations, bioperators and bidiagonalization. Further, we introduce and explore the concepts like fuzzy bimatrices, neutrosophic bimatrices and fuzzy neutrosophic bimatrices, which will find its application in fuzzy and neutrosophic logic. Comment: 181 page...|$|R
40|$|We compute RR fields by K-theory for T-dual {{versions}} of Type II superstring theories on a T 9 −p torus with orientifold planes Op ±. The RR charges at cohomology level are also computed. This {{is done by}} wrapping D(q + n) branes on compact non-trivial homological n-cycles of the transverse space RPn to the orientifold {{in order to build}} Dq-branes on top of Op ± −planes. We <b>compare</b> <b>both</b> <b>of</b> <b>them</b> by K-theory through the Atiyah-Hirzebruch Spectral Sequence and some new correlations between branes on orientifold planes Op ± and obstructions to the existence of some branes are found. In the procedure we find a topological condition that fixes the number of the T-dual {{versions of}} the non-BPS ̂ D 4 - and ̂ D 3 -branes in Type USp(32) string theory on compact spaces, that is actually the topological condition needed to cancel global gauge anomalies in lower dimensional systems. Finally, for RR fields not related to any source and classified by K-theory, there is an apparent T-duality violation. We show that, with a correct interpretation, this is not the case...|$|R
40|$|We {{formulate}} {{and study}} computationally the fluctuating compressible Navier-Stokes equations for reactive multi-species fluid mixtures. We contrast two different expressions for the covariance of the stochastic chemical production {{rate in the}} Langevin formulation of stochastic chemistry, and <b>compare</b> <b>both</b> <b>of</b> <b>them</b> to predictions <b>of</b> the chemical Master Equation for homogeneous well-mixed systems close to and far from thermodynamic equilibrium. We develop a numerical scheme for inhomogeneous reactive flows, based on our previous methods for non-reactive mixtures [K. Balakrishnan, A. L. Garcia, A. Donev and J. B. Bell, Phys. Rev. E 89 : 013017, 2014]. We study the suppression of non-equilibrium long-ranged correlations of concentration fluctuations by chemical reactions, {{as well as the}} enhancement of pattern formation by spontaneous fluctuations. Good agreement with available theory demonstrates that the formulation is robust and a useful tool in the study of fluctuations in reactive multi-species fluids. At the same time, several problems with Langevin formulations of stochastic chemistry are identified, suggesting that future work should examine combining Langevin and Master Equation descriptions of hydrodynamic and chemical fluctuations. Comment: Submitted to J. Chem. Phys., see ArXiv: 1310. 0494 for non-reactive mixture...|$|R
40|$|The neutron-induced flssion {{cross section}} for Am- 242 m was {{measured}} with time-of-°ight(TOF) method from 0. 003 eV to 30 eV. We already measured the cross section using Kyoto University lead slowing-down spectrometer(KULS) from 0. 1 eV to 10 keV and using the standard thermal neutron fleld (D 2 O facility) at 0. 025 eV. The present result was <b>compared</b> with <b>both</b> <b>of</b> <b>them.</b> Although the JENDL- 3. 2 and ENDF/B-VI were slightly higher in the energy region lower than » 1 eV, the present result agreed with the KULS result and the D 2 O result within their experimental error. ...|$|R
40|$|We {{study the}} {{numerical}} solutions of the Dicke Hamiltonian, which describes {{a system of}} many two level atoms interacting with a monochromatic radiation field into a cavity. The Dicke model {{is an example of}} a quantum collective behavior which shows superradiant quantum phase transitions in the thermodynamic limit. Results obtained employing two different bases are <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> use the pseudospin basis to describe the atomic states. For the photon states we use in one case Fock states, while in the other case we use a basis built over a particular coherent state, associated to each atomic state. It is shown that, when the number of atoms increases, the description of the ground state of the system in the superradiant phase requires an equivalent number of photons to be included. This imposes a strong limit to the states that can be calculated using Fock states, while the dimensionality needed to obtain convergent results in the other basis decreases when the atomic number increases, allowing calculations that are very difficult in the Fock basis. Naturally, it reduces also the computing time, economizing computing resources. We show results for the energy, the photon number and the number of excited atoms, for the ground and the first excited state. Comment: 8 pages, 9 figures, 1 table, Proc. Quantum Optics V, Cozumel, Mexico, November 15 - 19, 201...|$|R
40|$|Due to its causal semantics, Bayesian {{networks}} (BN) {{have been}} widely employed to discover the underlying data relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables, BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly optimize the classification performance of the GBNs. The {{advantages and disadvantages of}} the two frameworks are discussed and experimentally <b>compared.</b> <b>Both</b> <b>of</b> <b>them</b> demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN. Comment: 16 pages and 5 figures for the article (excluding appendix...|$|R
40|$|This {{bachelor}} thesis analyses {{the possibilities}} of dynamic web content development by the help of technologies PHP and ASP. NET, attempting to <b>compare</b> <b>both</b> <b>of</b> <b>them</b> and establishes basic differences. All work is divided into the four main chapters. First, introductory part, treat of dynamic web content development, used technologies and initiatress reader to the given problems. The second chapter is already about analysing and comparing the two technologies mentioned above. This part begins by the basic introduction and history of technologies, follows practical part „ installation, setting, configuration", where is reader familiarized with needed development tools for starting each of technology. Further treat {{of the way of}} writing server code including syntax of languages, in which is PHP compared with language C# and describes the object orientation of languages. Chapter includes the samples of data access by <b>both</b> <b>of</b> the technologies, whether it uses functions or objects, special libraries etc. Summarises the support of platforms for mull over technologies and describes development environments {{that can be used to}} realize applications by the help of these technologies. Third chapter is engaged in design and practical realization of simple editorial system web application in both above described technologies. The editorial system is using appropriate database systems and includes basic content management interface, which is administred and managed through the web interface. Last chapter contains final estimation and summarization of ascertained conclusions, recapitulate downright findings and tries to answer the question, in which cases give priority to ASP. NET before PHP and on the contrary...|$|R
30|$|The {{results of}} our study suggest that asthma disease can be {{classified}} with an accuracy of approximately 90 % by combining discrete wavelet transformation with different machine learning methods. There was not any performance difference between AdaBoosted Random Forest and Random Forest classifiers. However, when <b>compared</b> to MLPNN, <b>both</b> <b>of</b> <b>them</b> perform better. Slight performance improvements were observed for all classification algorithms after FEV and FVC 1 clinical values {{were added to the}} feature vector.|$|R
40|$|Cost {{control is}} one of the {{important}} factors for competition through managing the controllable costs as low as possible, it will lowest the basic price of production, so that selling price will be compete with anothers companies which are cost efficient. The determination cost of goods sold with activity based costing method will prevents cost distortion. Beside that, ini managing stock supplies will be on time, so it will minimize the raw materials. Based on the problems, this reseach is aimed to evaluate the calculation cost of goods sold which implemented by a company, it also analyze activity based costing, and <b>compare</b> <b>both</b> <b>of</b> <b>them.</b> The implications <b>of</b> differences will be recommended to the management. Reffering to the research resaut, that is calculating overhead cost allocation which howadays applied by a company, it is only using one cost driver, called machine hour. Meanwhile, activity based costing, there are 5 cost drivers : machine hour, kilowatthour, kilogram gas, working hour, and space of usage of factory. The comparison of calculating cost of goods sold there are 7 products are under valued and 4 product are over valued. The other benefits by implementing activity based costing are able to identify cost efficency level for any product. Then we will know which activities consume high resources, so that it will increase overhead cost. Considering the benefits of activity based costing method, it will recommended to a company. Implementation this activity based costing method needs strong commitment from the whole employees, supported by adequate resources, and computerized. ...|$|R
40|$|AbstractBackgroundPreanesthetic {{medication}} in pediatrics is {{very helpful}} in relieving anxiety, fear, and psychological trauma due to maternal deprivation. Many drugs used in different routes aiming to alleviate stress and prevent psychological trauma. Of these drugs midazolam and ketamine are commonly used. We aimed in this work to <b>compare</b> <b>both</b> <b>of</b> <b>them</b> with dexmedetomidine which is α 2 -agonist when used intranasally in children undergoing bone marrow biopsy and aspirate in sedation and premedication. Methods 96 children aged 2 – 8 years with ASA physical status II scheduled for bone marrow biopsy and aspirate were divided into three groups 32 child in each one: (M group) who were premedicated with intranasal midazolam 0. 2 mg/kg, (D group) who were premedicated with intranasal dexmedetomidine 1 μg/kg, and (K group) who were premedicated with intranasal ketamine 5 mg/kg. The degree of sedation was assessed every 5 min for 30 min by using a 4 point sedation scale. Also, child–parent separation was assessed and graded according to a 4 point scale at 30 min. ResultsWe found that dexmedetomidine group achieved a faster sedation score less than 3 {{at the point of}} 10 min, then all groups achieved a comparable sedation score till point <b>of</b> 25 min, <b>both</b> dexmedetomidine and midazolam groups had better sedation score than ketamine group at 30 min. Children achieved child–parents separation score grade 1 was significantly higher in dexmedetomidine group than midazolam and ketamine groups. ConclusionsMidazolam, ketamine and dexmedetomidine produced adequate sedation with little side effects. So, we prefer to use midazolam due its efficacy and safety as well as availability and its low price in comparison to ketamine and dexmedetomidine...|$|R
40|$|I devoted my diploma thesis to the {{research}} {{on the influence of}} dysfunctional families, including processes within it as well as structures that assist in the development of emotional and behavioural disorders (EBD) of an adolescent. The term EBD, the characteristics of children with EBD and some important classifications, are defined in the theoretical part. Then I focused on the development of EBD in connection with family. I introduced the characteristics of functional and dysfunctional families and <b>compared</b> <b>both</b> <b>of</b> <b>them.</b> Further on I described risky factors causing the development of EBD, unsuitable parental roles, styles of upbringing and their consequences on the adolescent. I defined the term of emotional parenting and the consequences caused by lack of suitable parenting. Furthermore, I emphasized the characteristics of socioeconomic position of families with adolescents who are situated in residential treatment institutions. In addition, I presented the strategies that are used by adolescents to substitute the unfulfilled needs for love, security and belonging. The diploma work introduces the kinds of help which are provided for adolescents and their parents at the residential treatment institutions. The empirical part includes interviews with two expert workers at Zavod za vzgojo in izobraževanje Logatec, pointing out their opinion and experiences with family influence concerning EBD. Connections to the theoretical basis and strong influence of ignorance, chaos, lack of warmth and love, and above all, conflicts among parents, are stated. All these factors have huge influence on the development of EBD. I found out that many adolescents are in conflict with their inner insecurity which they try to mask in order to appear fearless on the outside. Quite often they socialize with similar peers and escape into the world of alcohol and drugs. ...|$|R
40|$|The {{main goal}} of this {{document}} is {{the presentation of the}} recent EUROCONTOL BADA family 4 document and the changes and improvements respect its predecessor at the trajectory prediction of some aerial vehicles. In order to do this, first we will describe the previous families used, included in the BADA family 3. First, {{a brief description of the}} situation, the model used is presented and the main parameterizations are described. After that, we will make a detailed study of the successor, BADA family 4, in which we will be able to observe which are the main changes respect the previous version and which new parameters can be calculated that we could not calculate before. Showing also the functions so it is possible to <b>compare</b> <b>both</b> families. Once we have read and learnt these documents, it is time to compare them. First this is done in a more descriptive way, observing the main differences in the formulas used and which is the right process and order to use them. After that, in order to see the differences in an analytical view, we will do the same simulation taking into account both ways to calculate the trajectory parameters. As BADA family 4 is a very recent document, there are no premade programs that use its formulas, due to this situation, we had to prepare a MatLab program in order to <b>compare</b> <b>both</b> <b>of</b> <b>them.</b> Another issue due to be recent is that is restricted, so we can only simulate a determined plane in a determined conditions. In order to know if our program works properly, EUROCONTROL have given us the result of how should be the parameters depending on altitude, atmosphere and engine type. At the end, we will <b>compare</b> <b>both</b> documents analytically using those programs. First, it is shown that the based on BADA 4 give the expected results taking into account different environments with different input parameters. After this, the program that simulates BADA 3 will be used and the output given by both documents is studied and analysed...|$|R
40|$|In this paper, several {{numbers of}} {{biometric}} images are compressed {{in order to}} reduce the number of bits needed in representing an image with conservation of image quality. Biometric images compression is important to solve the problem of efficiently transmitting data and storing large number of biometric images in low capacity of memory device. Biometric images are compressed using two techniques which are DCT and Quantization. The compression algorithm is implemented on general purpose computer and DSP processor in order to <b>compare</b> between <b>both</b> <b>of</b> <b>them</b> in terms processing time and evaluate the performance of this technique by measuring the difference between the original image and reconstructed image using PSNR, SSIM and MSE. Experimental results show DCT algorithm produces a high quality for reconstructed images with acceptable compression rate in terms of quality level is more than 50 %. Furthermore, implementing the proposed algorithm using DSP board achieves better performance in terms of processing time compared with PC based...|$|R
40|$|Two {{different}} estimation {{techniques for}} {{the spectrum of}} a nonstationary time series are <b>compared</b> empirically. <b>Both</b> <b>of</b> <b>them</b> are assuming a time-dependent autoregressive (AR-) model for the data. The fifirst estimation technique used is the Frequency State Dependent Model (FSDM-) technique (Schmitz and Urfer, 1997), a modification of the well known Kalman-filter approach. The FSD-Model is based on Priestleys SD-Models {{for the analysis of}} nonstationary time series (e. g.,Priestley, 1988). An alternative approach for estimating AR-parameters of nonstationary time series was proposed by Tsatsannis and Giannkis (1993). The basic idea is to directly decompose the time-dependent autoregressive parameters into their wavelet representation and to select suitable wavelet coefficients for reconstruction. In either case, Kitagawa's (1983) "instantaneous spectrum" is calculated to obtain the actual spectral estimates. Applied to empirical data, both approaches lead to similar spectral estimates. However, simulations show how crucial the selection of wavelet coefficients is when applying the latter technique...|$|R
40|$|The paper {{presents}} {{a method of}} analysis of bone remodelling {{in the vicinity of}} implants. The authors aimed at building a model and numerical procedures which may be used as a tool in the prosthesis design process. The model proposed by the authors is based on the theory of adaptive elasticity and the lazy zone concept. It takes into consideration not only changes of the internal structure of the tissue (described by apparent density) but also surface remodelling and changes caused by the effects revealing some features of “creep”. Finite element analysis of a lumbar spinal segment with an artificial intervertebral disc was performed by means of the Ansys system with custom APDL code. The algorithms were in two variants: the so-called siteindependent and site-specific. Resultant density distribution and modified shape of the vertebra are <b>compared</b> for <b>both</b> <b>of</b> <b>them.</b> It is shown that this two approaches predict the bone remodelling in different ways. A comparison with available clinical outcomes is also presented and similarities to the numerical results are pointed out...|$|R
