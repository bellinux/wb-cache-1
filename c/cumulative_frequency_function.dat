2|4418|Public
30|$|Burr [2] {{developed}} {{the system of}} Burr distributions. The Burr system of distributions includes 12 types of cumulative distribution functions which yield a variety of density shapes. The attractiveness of this relatively unknown family of distributions for model fitting is that it combines a simple mathematical expression for <b>cumulative</b> <b>frequency</b> <b>function</b> with coverage in the skewness–kurtosis plane. Many standard theoretical distributions, including the Weibull, exponential, logistic, generalized logistic, Gompertz, normal, extreme value, and uniform distributions, are special cases or limiting cases of the Burr system of distributions (see [11]). Family of Burr-type distributions is a very popular distribution family for modelling lifetime data and for modelling phenomenon with monotone and unimodal failure rates (see, for example, [13, 18]).|$|E
40|$|Abstract: The paper {{presents}} a multidimensional analysis of mineral processing feeds consisting of different amounts of different size and density fractions. The considered feed was coal which was screened into size fractions which were subsequently separated into density fractions and their weights determined. The feed material was characterized with commonly used size and density frequency and cumulative distribution plots and next approximated with the Weibull (size) and logistic (density) mathe-matical functions. Having {{the contribution of}} each particle size and density fraction in the feed a two– dimensional analysis of the feed size/density properties was performed using two methods. The first one {{is based on the}} best chosen <b>cumulative</b> <b>frequency</b> <b>function</b> for two random variables and the second uses the so–called Morgenstern family functions. In the paper the undependability of the particles size and density was investigated using statistical approach based on the so–called 2 test, and the correlation between these parameters using the so–called F–Snedecor statistical test. In both cases it was found that particles size and density of the investigated coal particles were dependent what means that with growth of particle size its density grew too and there was correlation between them regardless of significance level assumed for the analysis...|$|E
40|$|An {{asymptote}} {{is derived}} from Turing's local reestimation formula for population frequencies, and a local reestimation formula {{is derived from}} Zipf's law for the asymptotic behavior of population frequencies. The two are shown to be qualitatively different asymptotically, but nevertheless to be instances of a common class of reestimation-formula-asymptote pairs, in which they constitute {{the upper and lower}} bounds of the convergence region of the <b>cumulative</b> of the <b>frequency</b> <b>function,</b> as rank tends to infinity. The results demonstrate that Turing's formula is qualitatively different from the various extensions to Zipf's law, and suggest that it smooths the frequency estimates towards a geometric distribution. Comment: 9 pages, uuencoded, gzipped PostScript; some typos remove...|$|R
40|$|Top-k query {{processing}} is {{a widespread}} field of research. Its application {{can be used in}} many fields like wireless sensor networks, mobile ad-hoc networks, peer-to-peer networks and many more. The basic problem in top-k query processing is that, a single algorithm cannot be used as {{a solution to the problem}} of top-k query processing because there are many types of top-k query processing. The algorithm has to be based on the situation, the classification and the type of database and query model. The research method used in this project provides a solution to the problem of generating the probable top-k query results. The algorithm also provides the probability that the results are more likely in the top-k result set. The most prominent algorithms in the field of top-k query processing techniques are FA (Fagin Algorithm), TA (Threshold Algorithm). The algorithm used in this project is built upon the Threshold Algorithm making it applicable for a wider range of top-k query processing problems with better efficiency. In this project the algorithm is implemented on sorted data. The confidence for the results is also assigned. The confidence is calculated by using the <b>cumulative</b> <b>frequency</b> distribution <b>functions</b> and Central Limit Theorem. Histograms implemented in the Threshold Algorithm for the dat...|$|R
40|$|Random {{variables}} represent outcomes {{from random}} phenomena. They are specified by two objects. The range R of possible {{values and the}} frequency f(x) with which values from within the range can occur. When the range is a discrete set we have a discrete random variable and when the range is continuous we have a continuous random variable. See several examples below. For notational convenience we often extend the range to a larger set where outcomes outside the original range are assigned zero frequency. The terms: a random variable X, the distribution f(x), or the Range R, is discrete (respectively continuous) are equivalent. The <b>cumulative</b> <b>frequency</b> or distribution <b>function,</b> of a random variable X, is defined as F (x) = {∑x − ∞ f(y) if X is discrete, ∫ x − ∞ f(y) dy if X is continuous. 2. Expectation and moments The expected value (or first moment) of a random variable X is a constant E(X) defined b...|$|R
5000|$|The <b>cumulative</b> <b>frequencies</b> are plotted, and PSM advocates claim interpretive qualities {{exist for}} any {{intersecting}} of the <b>cumulative</b> <b>frequencies</b> {{for each of}} the four price categories. Note that the standard method requires that two of the four <b>cumulative</b> <b>frequencies</b> must be inverted in order to have the possibility of four intersecting points. Conventional practice inverts the <b>cumulative</b> <b>frequencies</b> for [...] "too cheap" [...] and [...] "cheap".|$|R
40|$|A {{new data}} structure, namely “cumulative {{frequency}} matrix (CFM) ”, is proposed here for maintaining <b>cumulative</b> <b>frequencies.</b> For an order- 0 model having 256 symbols, CFM is a 2 -D array of 16 rows and 16 columns. Two nibbles, say L for left and R for right, of a byte symbol represents {{row and column}} dimensions respectively. Matrix element (L, R) represents <b>cumulative</b> <b>frequency</b> of symbol with right nibble as R among symbols with left nibble as L. Within row, it stores <b>cumulative</b> <b>frequency</b> of symbols with right nibble varying from 0 to 15. Adaptive arithmetic coding is a lossless data compression method. It needs to update <b>cumulative</b> <b>frequencies</b> at runtime. Various algorithms for maintaining <b>cumulative</b> <b>frequencies,</b> computing <b>cumulative</b> <b>frequency</b> interval etc. are discussed here. Practical implementation shows that proposed data structure is simpler as well as efficient as compared to other data structures in use...|$|R
30|$|Here, we {{investigated}} records of geomagnetic events observed at three magnetic observatories {{operated by the}} Japan Meteorological Agency (JMA): Kakioka, Memambetsu, and Kanoya. In this article, we provide the <b>cumulative</b> <b>frequency</b> distributions of sudden geomagnetic change phenomena, that is, sudden impulses (SI) and storm sudden commencements (SSC), and show latitude dependence of slopes of the <b>frequency</b> distribution <b>functions</b> in a low to middle magnetic latitude.|$|R
40|$|This article {{presents}} {{a method for}} producing graphical displays of cumulative distribu-tion functions. Step-by-step {{instructions on how to}} produce these graphs are provided using a concrete example. By following the step-by-step instructions, researchers will be able to produce the plots for themselves using SPSS. Thus, they will have an alternative way to analyze and report their data. Researchers, in an effort to better understand and report their data, often produce different kinds of tables and charts. The importance of graphic tools is increasingly being recognized in the social sciences (cf. Cleveland, 1995; Tukey, 1977; Wilkinson, 1999). As the recent recommendations of the APA Task Force on Statistical Inference so strongly emphasized, There are [...] . [many] ways to include data or distributions in graphics, includ-ing box plots and stem-and-leaf plots [...] . and kernel density estimates [...] Many of these procedures are found in modern statistical packages. It is time for au-thors to take advantage of them and for editors and reviewers to urge authors to do so. (Wilkinson & APA Task Force on Statistical Inference, 1999, p. 602) Simply put, many thoughtful researchers feel that a picture of their data gives them more insight than do just tables and charts. The {{purpose of this article is}} to present a series of steps that would enable the researcher to produce graphical displays of <b>cumulative</b> <b>frequency</b> distri-bution <b>functions.</b> To do so, data sets derived by Tanguma and Willson (1999) will be used. However, due to space limitations, sample sizes have been reduced to 20 data points instead of the original 200 data points. Correspondence concerning this article should be addressed to Jesus Tanguma, Texas A&...|$|R
50|$|<b>Cumulative</b> <b>frequency</b> {{analysis}} is {{the analysis of}} the frequency of occurrence of values of a phenomenon less than a reference value. The phenomenon may be time- or space-dependent. <b>Cumulative</b> <b>frequency</b> is also called frequency of non-exceedance.|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> {{distribution}} with a discontinuity ...|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> {{figure for}} GFMT (short version).|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> {{distribution}} (lognormal) of {{hydraulic conductivity}} (X-data) ...|$|R
50|$|Find the <b>frequencies,</b> {{relative}} <b>frequency,</b> <b>cumulative</b> <b>frequency</b> etc. as required.|$|R
50|$|To {{present the}} <b>cumulative</b> <b>frequency</b> {{distribution}} {{as a continuous}} mathematical equation instead of a discrete set of data, one may try to fit the <b>cumulative</b> <b>frequency</b> distribution to a known cumulative probability distribution,. If successful, the known equation is enough to report the frequency distribution and a table of data will not be required. Further, the equation helps interpolation and extrapolation.However, care should be taken with extrapolating a <b>cumulative</b> <b>frequency</b> distribution, because {{this may be a}} source of errors. One possible error is that the frequency distribution does not follow the selected probability distribution any more beyond the range of the observed data.|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> distribution, adapted <b>cumulative</b> {{probability}} distribution, {{and confidence}} intervals ...|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> of {{simplified}} Chinese {{characters in}} Modern Chinese text ...|$|R
30|$|The bias {{correction}} technique {{adopted in}} this work is the “quantile mapping” one: mean and variability of the simulated values are corrected using the anomaly of the modeled <b>cumulative</b> <b>frequency</b> distribution compared to the observed <b>cumulative</b> <b>frequency</b> distribution. The algorithm systematically removes the median differences to zero and adopts the model output variance characteristics equal to the observed one.|$|R
50|$|One {{way is to}} use the {{relative}} <b>cumulative</b> <b>frequency</b> Fc as an estimate.|$|R
50|$|Probability {{theory can}} help to {{estimate}} the range in which the random error may be.In the case of <b>cumulative</b> <b>frequency</b> {{there are only two}} possibilities: a certain reference value X is exceeded or it is not exceeded. The sum of frequency of exceedance and <b>cumulative</b> <b>frequency</b> is 1 or 100%. Therefore, the binomial distribution can be used in estimating the range of the random error.|$|R
50|$|Further {{it gives}} the option to see the Q-Q plot in terms of {{calculated}} and observed <b>cumulative</b> <b>frequencies.</b>|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> {{analysis}} of the variable annual discharge of a river. Data analyzed with the CumFreq program ...|$|R
40|$|Abstract Background Disparities {{in health}} {{outcomes}} across communities are a central concern {{in public health}} and epidemiology. Health disparities research often links differences in health outcomes to other social factors like income. Choropleth maps of health outcome rates show the geographical distribution of health outcomes. This paper illustrates the use of <b>cumulative</b> <b>frequency</b> map legends for visualizing how the health events are distributed in relation to social characteristics of community populations. The approach uses two graphs in the <b>cumulative</b> <b>frequency</b> legend to highlight {{the difference between the}} raw count of the health events and the raw count of the social characteristic like low income in the geographical areas of the map. The approach is applied to mapping publicly available data on low birth weight by town in Connecticut and Lyme disease incidence by town in Connecticut in relation to income. The steps involved in creating these legends are described in detail so that health analysts can adopt this approach. Results The different health problems, low birth weight and Lyme disease, have different <b>cumulative</b> <b>frequency</b> signatures. Graphing poverty population on the <b>cumulative</b> <b>frequency</b> legends revealed that the poverty population is distributed differently with respect to the two different health problems mapped here. Conclusion <b>Cumulative</b> <b>frequency</b> legends can be useful supplements for choropleth maps. These legends can be constructed using readily available software. They contain all of the information found in standard choropleth map legends, and they can be used with any choropleth map classification scheme. <b>Cumulative</b> <b>frequency</b> legends effectively communicate the proportion of areas, the proportion of health events, and/or the proportion of the denominator population in which the health events occurred that falls within each class interval. They illuminate the context of disease through graphing associations with other variables. </p...|$|R
5000|$|... #Caption: <b>Cumulative</b> <b>frequency</b> {{distribution}} of the annual average river discharge, showing a large variation. The figure was made with the CumFreq program ...|$|R
5000|$|CumFreq {{uses the}} {{plotting}} position approach {{to estimate the}} <b>cumulative</b> <b>frequency</b> {{of each of the}} observed magnitudes in a data series of the variable.|$|R
40|$|The {{question}} of whether age-of-acquisition(AoA), frequency, and repetition priming effects occur at a common stage or {{at different stages of}} processing is addressed. Two single-stage accounts (i. e., <b>cumulative</b> <b>frequency</b> and a neural-network simulation) are considered in regard to their predictions concerning the interactions between AoA and frequencywith aging and priming effects. A repetition-priming face-classification task was conducted on both older and younger participants to test these predictions. Consistent with the predictions of the neural-network simulation, AoA had an effect on reaction times that could not be explained by <b>cumulative</b> <b>frequency</b> alone. Also, as predicted by the simulation, the size of the priming effect was determined by the <b>cumulative</b> <b>frequency</b> of the item. It is discussed how this evidence is supportive of the notion that AoA, frequency, and priming all have effects at a common and single stage during face processing...|$|R
50|$|The <b>cumulative</b> <b>frequency</b> is {{the total}} of the {{absolute}} frequencies of all events at or below {{a certain point in}} an ordered list of events.|$|R
50|$|The {{confidence}} belt around an experimental <b>cumulative</b> <b>frequency</b> or return period curve gives {{an impression of}} the region in which the true distribution may be found.|$|R
5000|$|The <b>cumulative</b> <b>frequency</b> MXr [...] of a {{reference}} value Xr is the frequency {{by which the}} observed values X are {{less than or equal}} to Xr.|$|R
50|$|In {{statistics}} and data analysis the application software CumFreq {{is a free}} and user-friendly tool for <b>cumulative</b> <b>frequency</b> analysis of a single variable and for probability distribution fitting.|$|R
40|$|Texto completo. Acesso restrito. p. 349 – 359 The {{nature of}} geochemical {{anomalies}} is discussed on a speculative basis. Anomalies {{are classified as}} independent or as additive, depending on their spatial relation to the background population. The additive anomalies, which are more commonly encountered in geochemical exploration, cannot {{be separated from the}} background, in a composite population, by extracting straight lines from curved <b>cumulative</b> <b>frequency</b> graphs, because the <b>cumulative</b> <b>frequency</b> of the distribution of the additive component of an anomaly plots as a curved line on normal and lognormal probability paper...|$|R
5000|$|The {{cumulative}} probability Pc of X {{to be smaller}} {{than or equal to}} Xr can be estimated in several ways {{on the basis of the}} <b>cumulative</b> <b>frequency</b> M [...]|$|R
50|$|An {{alternative}} {{method is to}} plot the log of the particle size against the <b>cumulative</b> <b>frequency.</b> This graph will usually consist two reasonably straight lines with a connecting line corresponding to the antimode.|$|R
40|$|LiqueMap is a {{proposed}} map for real-time distribution {{over the internet}} that estimates the spatial distribution of liquefaction probability in the epicentral area immediately after an earthquake. As proposed here, {{it is based on}} the methodology for liquefaction hazard mapping that uses field-determined <b>cumulative</b> <b>frequency</b> distributions of the liquefaction potential index (LPI) of surficial geologic units to estimate the probability of surface manifestations of liquefaction at a given level of shaking and earthquake magnitude. The requirements for creating a LiqueMap after an earthquake are: (1) a map of PGA; (2) a map of the surficial geology; and (3) predetermined empirical LPI <b>cumulative</b> <b>frequency</b> distributions of the mapped surficial geologic units. LiqueMap as proposed here is generated by using a post-earthquake map of PGA and the LPI <b>cumulative</b> <b>frequency</b> distributions to map the probability of surface manifestations of liquefaction within the strongly shaken area. LiqueMap is potentially useful to utility and transportation agencies for assessing potential damage to lifelines from liquefaction-induced permanent ground deformation. When overlain on maps of vulnerable pipelines and other lifelines, LiqueMap can help set priorities for post-earthquake inspections...|$|R
3000|$|Data cleaning—remove missing data, <b>cumulative</b> <b>frequency</b> {{not between}} 99 and 101, blends where a single coal {{represented}} more than 90 % of the blend (i.e., only blends where {{used in this}} stage of the process) [...]...|$|R
5000|$|In hydrology, the Fréchet {{distribution}} {{is applied to}} extreme events such as annually maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Fréchet distribution to ranked annually maximum one-day rainfalls in Oman showing also the 90% confidence belt based on the binomial distribution. The <b>cumulative</b> <b>frequencies</b> of the rainfall data are represented by plotting positions {{as part of the}} <b>cumulative</b> <b>frequency</b> analysis. However, in most hydrological applications, the distribution fitting is via the generalized extreme value distribution as this avoids imposing the assumption that the distribution does not have a lower bound (as required by the Frechet distribution).|$|R
