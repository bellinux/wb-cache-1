23|5|Public
25|$|The {{image on}} the right {{illustrates}} an example of fitting the log-normal distribution to ranked annually maximum one-day rainfalls showing also the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of a}} cumulative frequency analysis.|$|E
25|$|The blue picture {{illustrates}} {{an example}} of fitting the log-logistic distribution to ranked maximum one-day October rainfalls and it shows the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by the plotting position r/(n+1) {{as part of the}} cumulative frequency analysis.|$|E
25|$|In {{hydrology}} the Pareto {{distribution is}} applied to extreme events such as annually maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Pareto distribution to ranked annually maximum one-day rainfalls showing also the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis.|$|E
5000|$|... #Caption: Uncertainty {{analysis}} with <b>confidence</b> <b>belts</b> using the binomial distribution ...|$|R
50|$|SegReg recognizes {{many types}} of {{relations}} and selects the ultimate type {{on the basis of}} statistical criteria like the significance of the regression coefficients. The SegReg output provides statistical <b>confidence</b> <b>belts</b> of the regression lines and a confidence block for the breakpoint. The confidence level can be selected as 90%, 95% and 98% of certainty.|$|R
40|$|An {{improved}} {{method of}} gas-liquid chromatography (GLC) for routine estimation of adrenal steroids in urine is presented. It {{was applied to}} the exploration of steroid output in 120 normal prepuberal children to both sexes aged 3 mth to 12 yr. Urinary data were compared to plasmatic values of adrenal hormones estimated by radioimmune assays (RIA). Analysis of the relationships between steroids outputs and bone age revealed that the excretion of cortisol catabolites was expressed by a saturation curve whereas that of Δ 4 - 3 keto androgens catabolites were expressed by geometrical progression curves; urine dehydroepiandrosterone (DHEA) and plasma DHEA-sulfate showed outbreaks of high values beyond 6 yr of age which ruled out all curve fitting attempts. The computation of these relationships allowed to determine the <b>confidence</b> <b>belts</b> of the various regressions which appear as a useful tool to estimate the delayed or accelerated character of adrenal activity in prepuberal childhood. Comparison of the various urinary (GLC) and plasmatic (RIA) parameters enabled us to comment on adrenal puberty. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
25|$|In {{hydrology}} {{the distribution}} of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem. The blue picture illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis.|$|E
5000|$|... #Caption: Return {{periods and}} <b>confidence</b> <b>belt.</b> The {{curve of the}} return periods {{increases}} exponentially.|$|E
50|$|The {{software}} {{employs the}} binomial distribution {{to determine the}} <b>confidence</b> <b>belt</b> of the corresponding cumulative distribution function.|$|E
40|$|In {{high energy}} physics, a widely used method to treat {{systematic}} uncertainties in confidence interval calculations {{is based on}} combining a frequentist construction of <b>confidence</b> <b>belts</b> with a Bayesian treatment of systematic uncertainties. In this note we present {{a study of the}} coverage of this method for the standard Likelihood Ratio (aka Feldman & Cousins) construction for a Poisson process with known background and Gaussian or log-Normal distributed uncertainties in the background or signal efficiency. For uncertainties in the signal efficiency of upto 40 % we find over-coverage on the level of 2 to 4 % {{depending on the size of}} uncertainties and the region in signal space. Uncertainties in the background generally have smaller effect on the coverage. A considerable smoothing of the coverage curves is observed. A software package is presented which allows fast calculation of the confidence intervals for a variety of assumptions on shape and size of systematic uncertainties for different nuisance parameters. The calculation speed allows experimenters to test the coverage for their specific conditions. Comment: 19 pages, 7 figures, version to match the one accepted by NI...|$|R
40|$|The {{need for}} {{quantitative}} {{analysis of the}} major mineral constituents in carbonate rocks resulted from studies of carbonate aggregates u ed in highway concrete. Three quantitative X-ray methods for calcite and dolomite are investigated. The first, a compara-tively simple X-ray method, determines the relative percent of the carbonate fraction that is calcite and dolomite by measuring the intensity ratios of the largest diffraction maxima of the respective minerals. The sample preparation and X-ray procedures were changed and the altered method experimentally shown to be reliable. The second, a combination of chemical and X-ray procedures, involves the X-ray determination f the structural formulas of calcite and dolomite and an EDTA titration procedure for determining the proportion of calcium and magnesium ions in the sample. Two sources of error were found: (1) non-carbonate mlnera[s in the sample liberating "extra " calcium and magnesium ions dur-ing the EDTA titration procedure, and (2) incorrect structural formulas of dolomite due to inaccurate X-ray calibration curve. The third method, an internal standard technique, was established and calibration curves with <b>confidence</b> <b>belts</b> constructed. This method gives absolute [...] not relatlve~ values for not only calcite and dolomite bnt also quartz...|$|R
50|$|The <b>confidence</b> <b>belt</b> {{around an}} {{experimental}} cumulative frequency or return period curve gives {{an impression of}} the region in which the true distribution may be found.|$|E
50|$|The {{prediction}} {{of the return}} period, which is of interest in time series, is also accompanied by a <b>confidence</b> <b>belt.</b> The construction of confidence belts is not found in most other software.|$|E
50|$|The blue picture {{illustrates}} {{an example}} of fitting the exponential distribution to ranked annually maximum one-day rainfalls showing also the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis.|$|E
50|$|The blue picture {{illustrates}} {{an example}} of fitting the log-logistic distribution to ranked maximum one-day October rainfalls and it shows the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by the plotting position r/(n+1) {{as part of the}} cumulative frequency analysis.|$|E
5000|$|In {{hydrology}} the Pareto {{distribution is}} applied to extreme events such as annually maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Pareto distribution to ranked annually maximum one-day rainfalls showing also the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis.|$|E
5000|$|In {{hydrology}} {{the distribution}} of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem. The blue picture illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis.|$|E
5000|$|In hydrology, the Fréchet {{distribution}} {{is applied to}} extreme events such as annually maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Fréchet distribution to ranked annually maximum one-day rainfalls in Oman showing also the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The cumulative frequencies of the rainfall data are represented by plotting positions {{as part of the}} cumulative frequency analysis. However, in most hydrological applications, the distribution fitting is via the generalized extreme value distribution as this avoids imposing the assumption that the distribution does not have a lower bound (as required by the Frechet distribution).|$|E
50|$|In {{hydrology}} {{the distribution}} of long duration river discharge and rainfall (e.g., monthly and yearly totals, consisting of the sum of 30 respectively 360 daily values) is often thought to be almost normal according to the central limit theorem. The normal distribution, however, needs a numeric approximation. As the logistic distribution, which can be solved analytically, {{is similar to the}} normal distribution, it can be used instead. The blue picture illustrates an example of fitting the logistic distribution to ranked October rainfalls—that are almost normally distributed—and it shows the 90% <b>confidence</b> <b>belt</b> based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.|$|E
40|$|Roe and Woodroofe (RW) have {{suggested}} that certain conditional probabilities {{be incorporated into the}} ``unified approach'' for constructing confidence intervals, previously described by Feldman and Cousins (FC). RW illustrated this conditioning technique using one of the two prototype problems in the FC paper, that of Poisson processes with background. The main effect was on the upper curve in the <b>confidence</b> <b>belt.</b> In this paper, we attempt to apply this style of conditioning to the other prototype problem, that of Gaussian errors with a bounded physical region. We find that the lower curve on the <b>confidence</b> <b>belt</b> is also moved significantly, in an undesirable manner. Comment: 14 pages including figure...|$|E
40|$|The {{original}} frequentist {{approach for}} computing confidence intervals involves {{the construction of}} the <b>confidence</b> <b>belt</b> which provides a mapping of the observation in data into a subset of values for the parameter. There are different prescriptions for constructing the <b>confidence</b> <b>belt,</b> here we use the one provided by Feldman and Cousins. Alternative methods based on the frequentist idea exist, including the delta likelihood method, the $CL_s$ method and a method here referred to as the $p$-value method, which have all been commonly used in high energy experiments. The {{purpose of this article is}} to draw attention to a series of potential problems when applying these alternative methods to the important case where the predicted signal depends quadratically on the parameter of interest, a situation which is common in high energy physics as it covers scenarios encountered in effective theories. These include anomalous Higgs couplings and anomalous trilinear and quartic gauge couplings. It is found that the alternative methods, contrary to the original method using the <b>confidence</b> <b>belt,</b> encode the goodness-of-fit into the confidence intervals and potentially over-constrain the parameter. Comment: 13 pages, 15 figures, the text is edited and more figures are added to improve readabilit...|$|E
40|$|The {{formalism}} {{that allows}} {{to take into}} account the error sigma_b of the expected mean background b in the statistical analysis of a Poisson process with the frequentistic method is presented. It is shown that the error sigma_b cannot be neglected if it is not much smaller than sqrt(b). The resulting <b>confidence</b> <b>belt</b> is larger that the one for sigma_b= 0, leading to larger confidence intervals for the mean mu of signal events. Comment: 15 pages including 2 figures, RevTeX. Final version published in Phys. Rev. D 59 (1999) 11300...|$|E
40|$|We {{connect the}} power of Confidence Intervals in {{different}} Frequentist methods to their reliability. We show {{that in the case}} of a bounded parameter a biased method which near the boundary has large power in testing the parameter against larger alternatives and small power in testing the parameter against smaller alternatives is desirable. Considering the recently proposed methods with correct coverage, we show that the Maximum Likelihood Estimator method [1, 2] has optimal bias. It is well known that the most important property of Frequentist Confidence Intervals is coverage: a 100 (1 − α) % Confidence Interval belong to a set of intervals that cover the true value of the measured quantity µ with Frequentist probability 1 − α. Neyman’s method obtains Confidence Intervals with correct coverage through the construction for each possible value of µ of an acceptance interval with probability 1 −α for an estimator µ of µ. The union of all acceptance intervals in the µ–µ plane is called the <b>Confidence</b> <b>Belt.</b> The Confidence Interval for µ resulting from a measurement µobs of the estimator is the set of all values of µ whose acceptance interval for µ include µobs...|$|E
40|$|We give a {{classical}} <b>confidence</b> <b>belt</b> construction which unifies {{the treatment of}} upper confidence limits for null results and two-sided confidence intervals for non-null results. The unified treatment solves a problem (apparently not previously recognized) that the choice of upper limit or two-sided intervals leads to intervals which are not confidence intervals if the choice {{is based on the}} data. We apply the construction to two related problems which have recently been a battle-ground between classical and Bayesian statistics: Poisson processes with background, and Gaussian errors with a bounded physical region. In contrast with the usual classical construction for upper limits, our construction avoids unphysical confidence intervals. In contrast with some popular Bayesian intervals, our intervals eliminate conservatism (frequentist coverage greater than the stated confidence) in the Gaussian case and reduce it to a level dictated by discreteness in the Poisson case. We generalize the method in order to apply it to analysis of experiments searching for neutrino oscillations. We show that this technique both gives correct coverage and is powerful, while other classical techniques that have been used by neutrino oscillation search experiments fail one or both of these criteria. Comment: 40 pages, 15 figures. Changes 15 -Dec- 99 to agree more closely with published version. A few small changes, plus the two substantive changes we made in proof back in 1998 : 1) The definition of "sensitivity" in Sec. V(C). It was inconsistent with our actual definition in Sec. VI. 2) "Note added in proof" at end of the Conclusio...|$|E
40|$|A {{measurement}} of ZZ anomalous trilinear gauge couplings, f_i^V where i= 4, 5 and V=Z,γ, in LHC proton-proton collisions at √(s) = 8 TeV has been performed using data collected with the ATLAS detector in 2012, with {{electrons and muons}} in the final state. The dataset corresponds to an integrated luminosity of 20. 3 ± 0. 6 fb^- 1. In total, 272 candidate ZZ events are observed where both lepton pairs have an invariant mass in the range 66 - 116 GeV with an expected number of events from SM ZZ of 227. 3 ± 0. 3 and a background expectation of 10. 3 ± 2. 0. The sensitivity {{of a range of}} observables has been evaluated and the transverse momentum of the leading lepton is found to be the more sensitive observable and thus used for the aTGC measurement. Even though there is an excess in the total event count in data, the distribution of events in the observable does not favour a non-zero value of aTGCs since the excess is located at low transverse momentum while a small deficit is observed at high transverse momentum where the sensitivity is largest. A binned maximum likelihood fit finds that the preferred values of aTGCs are f_i^V= 0. Different statistical methods for deriving confidence intervals and contours have been investigated and it is concluded that among the three methods tested which are the method of profile likelihood, a method here denoted the p-value method and the Neyman construction, the first two systematically over-constrain the parameters in scenarios where less events are observed than what is expected from the SM, while the Neyman construction by definition gives the correct coverage. For the present observation in data, the confidence intervals and contours follow this pattern. The <b>confidence</b> <b>belt</b> gives the following confidence intervals: - 0. 0104 < f_ 4 ^γ < 0. 0104, - 0. 0088 < f_ 4 ^Z < 0. 0086, - 0. 0104 < f_ 5 ^γ < 0. 0104,- 0. 0088 < f_ 5 ^Z < 0. 0090. Compared to previous results, the present analysis improves the former best limits set by the CMS experiment using an integrated luminosity of 5. 0 fb^- 1 by ∼ 30 %...|$|E
40|$|Graduation date: 1965 The {{study of}} highlead logging {{operations}} {{through the use}} of statistical models is investigated, and their potential use for estimating and control of operations examined. During the preliminary phases of the study 16 work elements and 26 influencing variables are identified and measurement criteria developed for each. A coding system is also developed to aid recording the variables previously identified in a manner understandable to an electronic computer. Based on the variables and elements to be measured and recorded, data sheets and field procedures are developed and tested. It was proven necessary to use a two man field team to insure that all of the data on each turn, or cycle, was recorded accurately. The procedures necessary to convert the field data into computer input are explained and illustrated with a set of data sheets and summary sheet. The computing methods used in a stepwise regression analysis are discussed in general and its effects on selected lists of variables for each element are shown and discussed. Statistical models of each of the ten regular elements are computed from data taken on 590 turns, or cycles. The resulting models are then analyzed as to their effect on total cycle time and their relation to the form expected by logging estimators and supervisors. The majority of the deviations from expected form are explained but several remain unresolved, and pose a problem for future study. The frequency, mean duration, and proportion of total time consumed by the six irregular elements are computed and their relation to usual job efficiency experienced on similar operations is examined and found to be of the same order. Ten statistical models for the regular elements are recombined into gross element models to provide more convenient data for field use. The gross element and total cycle models were recomputed from the original field data using the variables from the appropriate element models. The resulting gross element models are then tabularized and sample sheets included to illustrate the general procedure for determining cycle times from the tabular data and correction factors. The reliability of the existing models is discussed and it is shown {{that it is impossible to}} develop statistical measures of spread or deviation from the computed regression line from the existing data. The measures of the probability of observing and computing a model on only a chance variation are very small, all are less than 2. 5 percent. Because of this the conclusion is drawn that causal relationships do exist in highlead logging. To calibrate the existing models an independent data sample is needed. The multivariate calibration procedures necessary are identified and a proposed computing method to reduce the size of the <b>confidence</b> <b>belt</b> is discussed. The study pointed out several areas for future study. They are: 1. The present element and gross element models should have confidence limits placed on them. 2. Several of the models indicate higher than expected standard error. These should be studied for possible improvements. 3. The range of the variables should be enlarged as soon as possible to reflect conditions in other logging areas. 4. A cause analysis study be instituted to study machine breakdown history and causes. 5. Development of a training manual for future investigators to reduce the instruction time and enable them to better answer questions from the men in the field. 6. A comparison study of the results predicted by the models with the estimates and historical records of logging. performance. 7. Locate or develop the necessary computer programs to allow full computer estimation of highlead performance using the models, topographic maps and timber cruise data...|$|E

