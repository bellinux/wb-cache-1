2147|194|Public
25|$|The {{ability to}} {{appropriately}} generalize to whole classes of yet unseen words, {{coupled with the}} abilities to parse <b>continuous</b> <b>speech</b> and keep track of word-ordering regularities, may be the critical skills necessary to develop proficiency with and knowledge of syntax and grammar.|$|E
25|$|Along {{the lines}} of {{probabilistic}} frequencies, the C/V hypothesis basically states all language hearers use consonantal frequencies to distinguish between words (lexical distinctions) in <b>continuous</b> <b>speech</b> strings, in comparison to vowels. Vowels are more pertinent to rhythmic identification. Several follow-up studies revealed this finding, as they showed that vowels are processed independently of their local statistical distribution.|$|E
25|$|Romansh {{comprises}} a {{group of}} closely related dialects, which are most commonly divided into five different varieties, {{each of which has}} developed a standardized form. These standardized regional standards are referred to as idioms in Romansh to distinguish them from the local vernaculars, which are referred to as dialects. These dialects form a dialect continuum without clear-cut divisions. Historically a <b>continuous</b> <b>speech</b> area, this continuum has now been ruptured by the spread of German, so that Romansh is now geographically divided into at least two non-adjacent parts.|$|E
5000|$|Voice Dictation - {{high-level}} {{objects for}} <b>continuous</b> dictation <b>speech</b> recognition ...|$|R
5000|$|<b>Speech</b> Recognition (multilingual, <b>continuous,</b> {{emotional}} <b>speech,</b> handicapped speaker, out-of-vocabulary words, alternative way of feature extraction, {{new models}} for acoustic and language modelling) ...|$|R
40|$|This study {{examined}} <b>continuous</b> automated <b>speech</b> {{recognition in the}} university lecture theatre. The participants were both native speakers of English (L 1) and English {{as a second language}} students (L 2) enrolled in an information systems course (Total N= 160). After an initial training period, an L 2 lecturer in information systems delivered three 2 -hour lectures over a three-week period to the participants and other students. Student self reports indicated that {{there were a number of}} perceived benefits associated with the use of <b>continuous</b> automated <b>speech</b> recognition. Compared with L 1 students, a significantly greater number of L 2 students and special needs students reported that the system had potential as an instructional support mechanism. However, a greater accuracy in the system’s recognition of lecture text vocabulary needs to be achieved. The implications are that lecturers need an extensive training period before delivering lectures using <b>continuous</b> automated <b>speech</b> recognition...|$|R
25|$|The Language Comprehension Department, {{headed by}} Anne Cutler, {{undertakes}} empirical investigation and computational modeling of {{the understanding of}} spoken language. Until 2009, the work within the department was largely divided between two research projects: decoding <b>continuous</b> <b>speech</b> and phonological learning for speech perception. From 2009 onwards, {{most of the work}} of the department goes into the project called Mechanisms and Representations in Comprehending Speech. This project focuses on core theoretical issues in speech comprehension such as on how episodic memories - such as hearing someone speak in an unfamiliar dialect - influence the speech perception system, or how is prior knowledge about one's language (phonotactic probabilities, lexical knowledge, frequent versus infrequent word combinations) used during perception.|$|E
25|$|To {{look more}} closely at this issue, Saffran Aslin and Newport {{conducted}} another study in which infants underwent the same training with the artificial grammar but then were presented with either words or part-words rather than words or nonwords. The part-words were syllable sequences composed of the last syllable from one word and the first two syllables from another (such as kupado). Because the part-words had been heard during the time when children were listening to the artificial grammar, preferential listening to these part-words would indicate that children were learning not only serial-order information, but also the statistical likelihood of hearing particular syllable sequences. Again, infants showed greater listening times to the novel (part-) words, indicating that 8-month-old infants were able to extract these statistical regularities from a <b>continuous</b> <b>speech</b> stream.|$|E
25|$|After {{completing}} his graduate studies, Jelinek, who had developed {{an interest in}} linguistics, had plans to work with Charles F. Hockett at Cornell University. However these fell through and during {{the next ten years}} he continued to study information theory. Having previously worked at IBM during a sabbatical, he began full-time work there in 1972at first on leave for Cornell, but permanently from 1974. He remained there for over twenty years. Although at first he had been offered a regular research job, upon his arrival he learned that Josef Raviv had recently been promoted to head of the newly opened IBM Haifa Research Laboratory, and became head of the <b>Continuous</b> <b>Speech</b> Recognition group at the Thomas J. Watson Research Center. Despite his team's successes in this area, Jelinek's work remained little known in his home country because Czech scientists were not allowed to participate in key conferences.|$|E
30|$|AM_CZ&es {{denotes the}} {{acoustic}} model consisting of 133 h 24 min of Czech recordings, 12 h 52 min of <b>continuous</b> Spanish <b>speech</b> (whole Albayzin corpus), and 69 min of isolated words uttered by disabled people.|$|R
40|$|Parameter-tying (or sharing) {{is widely}} used in hidden Markov models (HMM) for speech {{recognition}} because {{of its ability to}} enhance recognition accuracy and robustness. This paper tries to make best use of phonetic structure of the Chinese language to optimize parameter-tying in building acoustic models for an HMM-based <b>continuous</b> Chinese <b>speech</b> recognition system. Phonetic context based parameter-tying schemes are studied and compared with conventional parametertying schemes through experiments on a very large vocabulary, speaker-independent, <b>continuous</b> Chinese <b>speech</b> recognition system. The test of four male and four female subjects shows that the proposed parametertying scheme gives substantial improvement over the conventional ways of parametertying, without significantly increasing the number of system parameters. I...|$|R
40|$|Chinese Mandarin is a tonal language. Tone {{perception}} {{ability of}} people with sensorineural hearing loss (SNHL) is often weaker than normal people. To help the SNHL people better perceive and distinguish tone information in Chinese speech, we focus on real-time tone enhancement method for mandarin <b>continuous</b> <b>speeches.</b> In this paper, based on the experimental investigation on the acoustic features most related to tone perception, we pro-pose a practical tone enhancing model which employs the unified features independent of Chinese tonal patterns. Using this model, we further implement a real-time tone enhancement method which can avoid syllable segmentation and tonal pattern recogni-tion. By the tone identification test for the normal and SNHL people under both quiet and noisy backgrounds, {{it is found that}} the enhanced speeches with the proposed method gains an aver-age 5 % higher correct rate compared to original speeches. And the time delay of the enhancement method can be controlled within 800 ms, which can be further used in hearing aids to bene-fit the SNHL people in their daily life. Index Terms — Tone enhancement, sensorineural hearing loss, real-time system, <b>continuous</b> Mandarin <b>speech</b> 1...|$|R
25|$|Reddy's early {{research}} was conducted at the AI labs at Stanford, first {{as a graduate student}} and later as an Assistant Professor, and at CMU since 1969. His AI research concentrated on perceptual and motor aspect of intelligence such as speech, language, vision and robotics. Over a span of five decades, Reddy and his colleagues created several historic demonstrations of spoken language systems, e.g., voice control of a robot, large vocabulary connected speech recognition, speaker independent speech recognition, and unrestricted vocabulary dictation. Reddy and his colleagues have made seminal contributions to Task Oriented Computer Architectures, Analysis of Natural Scenes, Universal Access to Information, and Autonomous Robotic Systems. Hearsay I {{was one of the first}} systems capable of <b>continuous</b> <b>speech</b> recognition. Subsequent systems like Hearsay II, Dragon, Harpy, and Sphinx I/II developed many of the ideas underlying modern commercial speech recognition technology as summarized in his recent historical review of speech recognition with Xuedong Huang and James K. Baker.|$|E
2500|$|Parsing is {{the process}} by which a <b>continuous</b> <b>speech</b> stream is {{segmented}} into its [...] meaningful units, e.g. sentences, words, and syllables. Saffran (1996) represents a singularly seminal study in this line of research. Infants were presented with two minutes of <b>continuous</b> <b>speech</b> of an artificial language from a computerized voice to remove any interference from extraneous variables such as prosody or intonation. After this presentation, infants were able to distinguish words from nonwords, as measured by longer looking times in the second case.|$|E
2500|$|Over the years, {{transcription}} equipment {{has changed}} from manual typewriters to electric typewriters to word processors to computers and from plastic disks and magnetic belts to cassettes and endless loops and digital recordings. Today, speech recognition (SR), also known as <b>continuous</b> <b>speech</b> recognition (CSR), is increasingly being used, with medical transcriptions and or [...] "editors" [...] providing supplemental editorial services, although there are occasional instances where SR fully replaces the MT. Natural-language processing takes [...] "automatic" [...] transcription a step further, providing an interpretive function that speech recognition alone does not provide (although Ms do).|$|E
40|$|In {{this paper}} the upper {{performance}} limits of automatic syllable segmentation algorithms using single or multiple frequency band envelopes {{as their primary}} segmentation feature are explored. Each algorithm is tested against the TIMIT corpus of <b>continuous</b> read <b>speech.</b> The results show that candidate matching rates as high as 99...|$|R
30|$|A {{total of}} 40 utterances of clean <b>continuous</b> digital <b>speech</b> {{were used to}} {{construct}} parallel dictionaries in the NMF-based methods. These utterances were also used to train the GMM in the GMM-based method. Ten randomly selected utterances of clean and noisy <b>continuous</b> digital <b>speech</b> {{were used in the}} evaluation. Table 1 shows the content of the database, dictionary, and test data. We used the noise data from the CENSREC-C- 1 [31] database. The noisy speech was created by adding white noise or a noise signal recorded in a car, airport, restaurant, or subway to the clean speech data. The SNRs were 0, 10, and 20 dB. The noise dictionary was extracted from the before- and after-utterance sections in the evaluation sentence. The average number of noisy frames was 223.|$|R
40|$|This paper {{presents}} {{a novel approach}} to tone recognition in <b>continuous</b> Cantonese <b>speech</b> based on overlapped di-tone Gaussian mixture models (ODGMM). The ODGMM is designed with special consideration {{on the fact that}} Cantonese tone identification relies more on the relative pitch level than on the pitch contour. A di-tone unit covers a group of two consecutive tone occurrences. The tone sequence carried by a Cantonese utterance can be considered as the connection of such di-tone units. Adjacent di-tone units overlap with each other by exactly one tone. For each di-tone unit, a GMM is trained with a 10 -dimensional feature vector that characterizes the F 0 movement within the unit. In particular, the di-tone models capture the relative deviation between the F 0 levels of the two tones. Viterbi decoding algorithm is adopted to search for the optimal tone sequence, under the phonological constraints on syllable-tone combination. Experimental results show the ODGMM approach significantly outperforms the previously proposed methods for tone recognition in <b>continuous</b> Cantonese <b>speech.</b> 1...|$|R
2500|$|The DVI {{system is}} speaker-dependent, {{requiring}} each pilot {{to create a}} template. It is not used for safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used {{for a wide range}} of cockpit functions. Voice commands are confirmed by visual or aural feedback, and serves to reduce pilot workload. [...] All functions are also achievable by means of a conventional button-press or soft-key selections; functions include display management, communications, and management of various systems. EADS Defence and Security in Spain has worked on a new non-template DVI module to allow for <b>continuous</b> <b>speech</b> recognition, speaker voice recognition with common databases (e.g. British English, American English, etc.) and other improvements.|$|E
2500|$|To {{determine}} if young children have these same abilities Saffran Aslin and Newport exposed 8-month-old infants to an artificial grammar. The grammar {{was composed of}} four words, each composed of three nonsense syllables. During the experiment, infants heard a <b>continuous</b> <b>speech</b> stream of these words [...] Importantly, the speech was presented in a monotone with no cues (such as pauses, intonation, etc.) to word boundaries other than the statistical probabilities. Within a word, the transitional probability of two syllable pairs was 1.0: in the word bidaku, for example, the probability of hearing the syllable da immediately after the syllable bi was 100%. Between words, however, the transitional probability of hearing a syllable pair was much lower: After any given word (e.g., bidaku) was presented, one of three words could follow (in this case, padoti, golabu, or tupiro), so the likelihood of hearing any given syllable after ku was only 33%.|$|E
2500|$|It is a {{well-established}} finding that, unlike written language, spoken language {{does not have}} any clear boundaries between words; spoken language is a continuous stream of sound rather than individual words with silences between them. This lack of segmentation between linguistic units presents a problem for young children learning language, who must be able to pick out individual units from the <b>continuous</b> <b>speech</b> streams that they hear. One proposed method of how children are able to solve this problem is that they are attentive to the statistical regularities of the world around them. For example, in the phrase [...] "pretty baby," [...] children are more likely to hear the sounds pre and ty heard together during the entirety of the lexical input around them than they are to hear the sounds ty and ba together. In an artificial grammar learning study with adult participants, Saffran, Newport, and Aslin found that participants were able to locate word boundaries based only on transitional probabilities, suggesting that adults are capable of using statistical regularities in a language-learning task. This is a robust finding that has been widely replicated.|$|E
40|$|We {{present a}} new <b>continuous</b> {{automatic}} <b>speech</b> recognition system where no a priori assumptions on the dependencies between the observed and the hidden speech processes are made. Rather, dependencies are learned form data using the Bayesian networks formalism. This approach guaranties to improve modelling delity {{as compared to}} HMMs. Furthermore, our approach is technically very attractive because all the computational eort {{is made in the}} training phase...|$|R
40|$|Great {{progress}} has been made in the development of recognition systems for <b>continuous</b> read <b>speech</b> but the performance of these systems degrades severely when they are applied to spontaneous speech. This indicates that a different approach in modeling is required to design a system that is better suited to spontaneous speech. Our approach is to combine two advances proposed in previous work: the use of acoustically derive...|$|R
40|$|In this paper, we {{presents}} HyperSausage Neuron {{based on}} the High-Dimension Space(HDS), and proposes a new algorithm for speaker independent <b>continuous</b> digit <b>speech</b> recognition. At last, compared to HMM-based method, the recognition rate of HyperSausage Neuron method is {{higher than that of}} in HMM-based method. IEEE Computat Intelligence, Hong Kong Chapter.; Xidian Univ.; Hong Kong Baptist Univ.; Natl Nat Sci Fdn China.; Guangdong Univ Technol...|$|R
2500|$|According {{to recent}} research, {{there is no}} neural {{evidence}} of statistical language learning in children with autism spectrum disorders. When exposed to a continuous stream of artificial speech, neurotypical children displayed less cortical activity in the dorsolateral frontal cortices (specifically the middle frontal gyrus) as cues for word boundaries increased. However activity in these networks remained unchanged in autistic children, regardless of the verbal cues provided. This evidence, highlighting the importance of proper Frontal Lobe brain function is {{in support of the}} [...] "Executive Functions" [...] Theory, used to explain some of the biologically related causes of Autistic language deficits. [...] With impaired working memory, decision making, planning, and goal setting, which are vital functions of the Frontal Lobe, Autistic children are at loss when it comes to socializing and communication (Ozonoff, et al., 2004). Additionally, researchers have found that the level of communicative impairment in autistic children was inversely correlated with signal increases in these same regions during exposure to artificial languages. Based on this evidence, researchers have concluded that children with autism spectrum disorders don't have the neural architecture to identify word boundaries in <b>continuous</b> <b>speech.</b> Early word segmentation skills have been shown to predict later language development, which could explain why language delay is a hallmark feature of autism spectrum disorders.|$|E
2500|$|The {{incremental}} research techniques developed at IBM eventually became {{dominant in the}} field after DARPA, in the mid-80s, returned to NLP research and imposed that methodology to participating teams, shared common goals, data, and precise evaluation metrics. The <b>Continuous</b> <b>Speech</b> Recognition Group's research, which required large amounts of data to train the algorithms, {{eventually led to the}} creation of the Linguistic Data Consortium. In the 1980s, although the broader problem of speech recognition remained unsolved, they sought to apply the methods developed to other problems; machine translation and stock value prediction were both seen as options. A group of IBM researchers went on to work for Renaissance Technologies. Jelinek wrote, [...] "The performance of the Renaissance fund is legendary, but I have no idea whether any methods we pioneered at IBM have ever been used. My former colleagues will not tell me: theirs is a very hush-hush operation!" [...] Methods very similar to those developed for achieving speech recognition are at the base of most machine translation systems in use today. Observers have said that Pierce's paradigm, according to which engineering achievements in this area would be built on scientific progress, has been inverted, with the achievements in engineering being at the base of a number of scientific findings.|$|E
5000|$|Brain-to-Text: Towards <b>continuous</b> <b>speech</b> as a {{paradigm}} for BCI ...|$|E
40|$|The {{aim of the}} {{experiment}} described in this paper was to devise and test a procedure that would allow identification of a phoneme {{on the basis of}} only tongue-to-palate and labial contacts that accompanied its realization in <b>continuous</b> read <b>speech.</b> The hypothesis underlying this study was that the articulatory correlates of the phonemic distinctive features can be induced statistically from dimensionality-reduced electropalatographic data. 29931...|$|R
50|$|The first {{individual}} generally {{credited with}} developing and deploying a commercialized speech translation system capable of translating <b>continuous</b> free <b>speech</b> is Robert Palmquist, with his release of an English-Spanish large vocabulary system in 1997. This effort was funded {{in part by}} the Office of Naval Research To further develop and deploy speech translation systems, in 2001 he formed SpeechGear, which has broad patents covering speech translation systems.|$|R
40|$|This paper {{presents}} {{the continuation of}} the work completed by Satori and all. [SCH 07] by the realization of an automatic speech recognition system (ASR) for Arabic language based SPHINX 4 system. The previous work was limited to the recognition of the first ten digits, whereas the present work is a remarkable projection consisting in <b>continuous</b> Arabic <b>speech</b> recognition with a rate of recognition of surroundings 96 %...|$|R
50|$|Tonality for the {{distribution}} of <b>continuous</b> <b>speech</b> into tone groups.|$|E
50|$|Prosodic {{bootstrapping}} {{may also}} provide an explanation {{to the problem}} as to how infants segment continuous input. Just like adult speakers, children are exposed to <b>continuous</b> <b>speech.</b> Hearing <b>continuous</b> <b>speech</b> poses a problem for children learning their native language because pauses in speech do not align with word boundaries. As a result, children have to construct word representations from the speech that they hear.|$|E
50|$|Julius is a high-performance, two-pass large {{vocabulary}} <b>continuous</b> <b>speech</b> recognition (LVCSR) decoder {{software for}} speech-related researchers and developers.|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceWe {{present a}} new <b>continuous</b> {{automatic}} <b>speech</b> recognition system where no a priori assumptions on the dependencies between the observed and the hidden speech processes are made. Rather, dependencies are learned form data using the Bayesian networks formalism. This approach guaranties to improve modelling fidelity {{as compared to}} HMMs. Furthermore, our approach is technically very attractive because all the computational effort {{is made in the}} training phase...|$|R
40|$|The {{development}} of a <b>continuous</b> visual <b>speech</b> recognizer for a silent speech interface has been investigated using a visual speech corpus of ultrasound and video images of the tongue and lips. By using high-speed visual data and tied-state cross-word triphone HMMs, and including syntactic information via domain-specific language models, word-level recognition accuracy as high as 72 % was achieved on visual speech. Using the Julius system, it was {{also found that the}} recognition should be possible in nearly real-time...|$|R
40|$|While {{commercial}} {{speech recognition}} systems remain {{limited in their}} capabilities, research systems are now relatively mature, although computationally expensive. Specialized hardware is one solution to this problem, and certainly the only solution at present for mobile applications. We present a queue-based hardware architecture for large-vocabulary, speaker-independent, <b>continuous,</b> real-time <b>speech</b> recognition in the mobile environment, demonstrating better than real-time per-formance. We base our results on simulation of approximately one hour of speech data for a 5, 000 word vocabulary. ...|$|R
