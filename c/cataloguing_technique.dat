0|55|Public
50|$|Ziedan sees {{cataloguing}} as an ars maior, {{that has}} not received the attention it deserves. According to him, cataloguing {{is the key to}} have a panoramic view of a particular manuscript heritage. With this in mind, Ziedan produced some 20 manuscript catalogues using detailed descriptive <b>cataloguing</b> <b>techniques</b> rather than skimpy uninformative bibliographic records. His catalogues are mostly thematic, i.e. they are not general catalogues, they handle each theme of knowledge separately.|$|R
50|$|Scilken’s {{advocacy}} through writing {{would later}} inspire him {{to create his}} own publication the U*N*A*B*A*S*H*E*D Librarian, the how I run my library good letter. Choosing topics {{that focused on the}} practical parts of librarianship, the publication featured a collection of articles, cartoons, letters, and advice all dealing with the world of libraries. Advice was given on how to better improve library services including an assortment of topics from more traditional <b>cataloguing</b> <b>techniques</b> to more practical cleaning techniques.|$|R
40|$|This {{document}} presents <b>catalogue</b> <b>techniques</b> used at network GDAC {{level to}} facilitate the discovery of platforms and data files. Some AtlantOS networks are organized as DAC-GDACs that continuously update a catalogue of metadata on observation datasets and platforms: •	A DAC is a Data Assembly Centre operating at national or regional scale. It manages data and metadata for its area with a direct link to Scientifics and Operators. The DAC pushes observations to the network GDAC. •	A GDAC is a Global Data Assembly Centre. It is designed for a global observation network such as Argo, OceanSITES, DBCP, EGO, Gosud, etc… The GDAC aggregates data and metadata of an observation network, in real-time and delayed mode, provided by DACs...|$|R
5000|$|Most {{scholars}} {{who have written}} anything about Mary Fage's Fames Roule have read it as merely a solicitation for money or gifts. Margaret J. M. Ezell argues that as such, Fames Roule was a rare piece of openly professional writing from a woman during the 17th century. Ezell writes, [...] "Mary Fage lays claim to being, if not the first female professional writer, then the one most blatant in her search for patronage." [...] To do so, Fage calls on not only the tradition of acrostics, but also a tradition of cataloguing. Her appropriation of <b>catalogue</b> <b>techniques</b> for personal gain shows Fage’s skill with language. [...] Yet Travitsky suggests that a monetary analysis of Fage’s motives for writing Fames Roule is limiting. Viewed {{in the context of}} the political climate in which it was published, Fames Roule can be read as Mary Fage’s effort to align herself with the Stuart regime.|$|R
40|$|We have {{conducted}} an optical/near-infrared {{study of the}} environments of radio-loud quasars (RLQs) at redshifts z= 0. 6 – 2. 0. In this paper we discuss the sample selection and observations for the z= 1. 0 – 2. 0 subsample and the reduction and <b>cataloguing</b> <b>techniques</b> used. We discuss technical issues at some length, since few detailed descriptions of near-IR data reduction and multicolor object cataloguing are currently available in single literature references. Our sample of 33 RLQs contains comparable numbers of flat- and steep-radiospectrum sources and sources of various radio morphologies, and spans a similar range of Mabs and Prad, allowing us to disentangle dependence of environment on optical or radio luminosity from redshift evolution. We use the standard “shift-and-stare ” method of creating deep mosaiced images where the exposure time (and thus the RMS noise) at each pixel is not constant across the mosaic. An unusual feature of our reduction procedure {{is the creation of}} images with constant RMS noise from such mosaics. We adopted this procedure to enable use...|$|R
500|$|According to Daniel Hoffman, Whitman [...] "is a poet whose {{hallmark}} is anaphora". Hoffman {{describes the}} use of the anaphoric verse as [...] "a poetry of beginnings" [...] and that Whitman's use of its repetition and similarity at the inception of each line is [...] "so necessary as the norm against which all variations and departures are measured...what follows is varied, the parallels and the ensuing words, phrases, and clauses lending the verse its delicacy, its charm, its power". Further, the device allows Whitman [...] "to vary the tempo or feeling, to build up climaxes or drop off in innuendoes" [...] Scholar Stanley Coffman analyzed Whitman's <b>catalogue</b> <b>technique</b> through the application of Ralph Waldo Emersons comment that such lists are suggestive of the metamorphosis of [...] "an imaginative and excited mind". According to Coffman, Emerson adds that because [...] "the universe is the externalization of the soul, and its objects symbols, manifestations of the one reality behind them, Words which name objects also carry with them the whole sense of nature and are themselves to be understood as symbols. [...] Thus a list of words (objects) will be effective in giving to the mind, under certain conditions, a heightened sense not only of reality but of the variety and abundance of its manifestations." ...|$|R
40|$|Background: The {{advent of}} {{high-throughput}} experimentation in biochemistry {{has led to}} the generation of vast amounts of chemical data, necessitating the development of novel analysis, characterization, and <b>cataloguing</b> <b>techniques</b> and tools. Recently, a movement to publically release such data has advanced biochemical structureactivity relationship research, while providing new challenges, the biggest being the curation, annotation, and classification of this information to facilitate useful biochemical pattern analysis. Unfortunately, the human resources currently employed by the organizations supporting these efforts (e. g. ChEBI) are expanding linearly, while new useful scientific information is being released in a seemingly exponential fashion. Compounding this, currently existing chemical classification and annotation systems are not amenable to automated classification, formal and transparent chemical class definition axiomatization, facile class redefinition, or novel class integration, thus further limiting chemical ontology growth by necessitating human involvement in curation. Clearly, {{there is a need for}} the automation of this process, especially for novel chemical entities of biological interest. Results: To address this, we present a formal framework based on Semantic Web technologies for the automatic design of chemical ontology which can be used for automated classification of novel entities. We demonstrate the automatic self-assembly of a structure-based chemical ontology based on 60 MeSH and 40 ChEBI chemical classes...|$|R
5000|$|According to Daniel Hoffman, Whitman [...] "is a poet whose {{hallmark}} is anaphora". Hoffman {{describes the}} use of the anaphoric verse as [...] "a poetry of beginnings" [...] and that Whitman's use of its repetition and similarity at the inception of each line is [...] "so necessary as the norm against which all variations and departures are measured...what follows is varied, the parallels and the ensuing words, phrases, and clauses lending the verse its delicacy, its charm, its power". Further, the device allows Whitman [...] "to vary the tempo or feeling, to build up climaxes or drop off in innuendoes" [...] Scholar Stanley Coffman analyzed Whitman's <b>catalogue</b> <b>technique</b> through the application of Ralph Waldo Emersons comment that such lists are suggestive of the metamorphosis of [...] "an imaginative and excited mind". According to Coffman, Emerson adds that because [...] "the universe is the externalization of the soul, and its objects symbols, manifestations of the one reality behind them, Words which name objects also carry with them the whole sense of nature and are themselves to be understood as symbols. Thus a list of words (objects) will be effective in giving to the mind, under certain conditions, a heightened sense not only of reality but of the variety and abundance of its manifestations." ...|$|R
5000|$|Löwenthal and Guterman theorize that right-wing {{agitation}} increases social dissatisfaction, {{while simultaneously}} hampering rational responses to it. Much of {{the basis of}} pro-fascist and antisemitic propaganda appears to be irrational in substance, yet Löwenthal's research revealed that it was planned and calculated to achieve a specific response. Paul Apostolidos writes that Prophets in Deceit [...] "precisely <b>catalogues</b> the <b>techniques</b> used by the agitator to promote irrationalism in his audience." [...] Pro-fascist sentiment in America in the 1940s was not spontaneous, more grounded on long held beliefs, which Löwenthal labeled social malaise.|$|R
50|$|Mass {{spectrometry}} is {{a clinical}} application involving diagnosis of bacteria-specific molecules. By using a known database of previously <b>catalogued</b> organisms, similar <b>techniques</b> can produce results that reference {{back to this}} database, resulting in rapid diagnosis of microorganisms. Recent {{studies have suggested that}} these tests can become specific enough to diagnose down to the sub-species level by observing novel biomarkers.|$|R
40|$|AtlantOS WP 7 is {{dedicated}} to improve harmonization of data management procedures, and thereby improve the quality, interoperability and discoverability of data resources in AtlantOS. To improve harmonization, AtlantOS WP 7 works on multiple levels; a) 	WP 7 has identified selected areas, where significant improvements of interoperability can be obtained. This {{has resulted in the}} formulation of a common agreement stating a set of specific minimum standards, which shall ensure cross platform coherence. This includes minimum standards for use of identifiers for platforms and institutions, metadata including vocabularies, quality control and dissemination means. Furthermore, guidelines regarding DOI assignment, <b>catalogue</b> <b>techniques</b> and vocabulary use in AtlantOS have been formulated. b) 	AtlantOS has formulated and installed a Data Management Plan (DMP) setting the framework for handling and dissemination of AtlantOS data. This was the first step towards improved harmonization and includes an overview of the Data Landscape, prioritization of Essential Variables for AtlantOS, regulations regarding open access to data and recommendations on use of standards. c) 	AtlantOS WP 7 is initiating investigations of the use of GEOSS services, both for technical broker solutions to improve harmonization as well as for dissemination of AtlantOS data resources in an interdisciplinary global context. d) 	AtlantOS is also working on improving the transcontinental data sharing. A workshop is planned for in 2017 specifically targeting improvement of transcontinental sharing of data from the Atlantic Ocean. We here present the preliminary incentives for improving the transatlantic collaboration...|$|R
30|$|Since the Foresight Maturity Model {{lacks the}} <b>techniques</b> <b>catalogue</b> or best {{practices}} that would facilitate reaching the desired level of foresight maturity {{in areas such}} as leadership, planning, framing, scanning, forecasting and vision building, some recommendations of management concepts, which would improve foresight maturity in companies in under-developed regions (as the Podlaskie region), are given in the following section of the paper.|$|R
40|$|We have {{conducted}} an optical/near-infrared {{study of the}} environments of radio-loud quasars (RLQs) at redshifts z= 0. 6 - 2. In this paper we discuss the sample selection and observations for the z= 1 - 2 subsample and the reduction and <b>cataloguing</b> <b>techniques</b> used. We discuss technical issues at some length, since few detailed descriptions of near-IR data reduction and multicolor object cataloguing are currently available in single literature references. Our sample of 33 RLQs contains comparable numbers of flat- and steep- radio spectrum sources and sources of various radio morphologies, and spans a similar range of absolute magnitude and radio power, allowing us to disentangle dependence of environment on such properties from redshift evolution. We use the standard ``shift-and-stare'' method of creating deep mosaiced images where the exposure time (and thus the RMS noise) at each pixel is not constant across the mosaic. An unusual feature of our reduction procedure {{is the creation of}} images with constant RMS noise from such mosaics. We adopted this procedure to enable use of the FOCAS detection package over almost the entire mosaic instead of only in the area of deepest observation where the RMS noise is constant, thereby roughly doubling our areal coverage. We correct the object counts in our fields for stellar contamination using the SKY model of Cohen (1995) and compare the galaxy counts to those in random fields. Even after accounting for possible systematic magnitude offsets we find a significant excess of K>= 19 galaxies. Analysis and discussion of this excess population is presented by Hall & Green (1998) ...|$|R
30|$|ML should desirably {{be smaller}} because, as {{previous}} studies have indicated, the making of any RI forecast map should ideally involve as small events as possible (e.g., Rundle et al., 2002, 2003; Tiampo et al, 2002; Holliday et al, 2005; Nanjo et al., 2006 a, b, c). However, small events more often fail to be detected than larger ones. It is therefore necessary to evaluate the completeness magnitude (MC), above which all events can be relied upon to have been <b>catalogued.</b> Various <b>techniques</b> to compute MC and their practical applications were reviewed by Wiemer and Wyss (2002). A method frequently used is to define MC as the point of deviation from the GR distribution (Eq. (1)).|$|R
40|$|This book {{analyses}} the historical, {{economic and}} political context for the current prohibition of particular drugs. The study investigates the problem of drug control providing a systematic analysis {{of the development of}} the international system of regulation. Identifying the political rationalities that provided the basis of that system, it positions these moral justifications for exercising power in relation to the practical programs that put them into practice. The objective of the work is not simply to <b>catalogue</b> the <b>techniques</b> and strategies employed in the process of governing illicit drugs, but also to note the failures, unintended consequences and other difficulties associated with getting such programs to work. Arts, Education & Law Group, School of Criminology and Criminal JusticeNo Full Tex...|$|R
40|$|The main {{contribution}} {{of this work}} is the design of an application framework based on both conversational agents and user profiling technologies {{for the development of}} e-commerce services. User profiles are exploited by conversational agents to help customers in retrieving potentially interesting products from a <b>catalogue.</b> Three <b>techniques</b> were used for collecting data for a usability test: eye-movement tracking, questionnaire, and recording the user-system dialogue. The main outcomes of the experimental sessions are: (1) the dialogue capabilities of the agent facilitate the interaction between the user and the e-commerce site; and, (2) user profiles improve the retrieval capabilities of the agent. Finally, some limitations of the user profiling techniques adopted in the framework are discussed and a more sophisticated content-based profiling technique is proposed...|$|R
5000|$|Marc Restellini {{also puts}} {{technological}} and scientific advances {{at the service}} of Modigliani's work. In 1997, at the invitation of the art dealer Daniel Wildenstein, he began Amedeo Modigliani’s catalogue raisonné, based on scientific standards unique in the world of research on artworks Modigliani’s catalogue raisonné is based on 600 scientific files and systematic analyses for each work. This scientific rigor makes it possible to establish a comparative calibration. The latter is further refined by the integration of new technologies. For example, Marc Restellini is the first to use a comparative pigment analysis protocol for a paintings <b>catalogue.</b> Other <b>techniques,</b> such as the systematic use of infra-red plates, or digital processes of [...] "fake colors", complete this corpus to achieve a degree of precision of analysis really innovative in this field.|$|R
40|$|Lean budgets force {{organizations}} {{in particular the}} libraries who are cost-centres, to devise strategies that make the very most of the resources available. With the abundance of evolving technological innovations and the variety of information that is becoming available to the customer, competitive pressure will continue to intensify for libraries to constantly improve their systems and processes. Development and use of information technology (IT), in particular the latest online <b>cataloguing</b> tools and <b>techniques,</b> enable library professionals in enhancing their delivery systems...|$|R
25|$|In 1923, Oschepkov and Spiridinov collaborated (independently) {{with a team}} {{of other}} experts on a grant from the Soviet {{government}} to improve the Red Army's hand-to-hand combat system. Spiridonov had envisioned integrating the most practical aspects of the world's fighting systems into one comprehensive style that could adapt to any threat. Oschepkov had observed Kano's distillation of Tenjin Shin’yo Ryu jujutsu, Kito Ryu jujutsu and Fusen Ryu jujutsu into judo, and he had developed the insight required to evaluate and integrate combative techniques into a new system. Their developments were supplemented by Anatoly Kharlampiyev and I. V. Vasiliev who also traveled the globe to study the native fighting arts of the world. Ten years in the making, their <b>catalogue</b> of <b>techniques</b> was instrumental in formulating the early framework of the art to be eventually referred to as sambo.|$|R
40|$|Abstract Background The {{advent of}} {{high-throughput}} experimentation in biochemistry {{has led to}} the generation of vast amounts of chemical data, necessitating the development of novel analysis, characterization, and <b>cataloguing</b> <b>techniques</b> and tools. Recently, a movement to publically release such data has advanced biochemical structure-activity relationship research, while providing new challenges, the biggest being the curation, annotation, and classification of this information to facilitate useful biochemical pattern analysis. Unfortunately, the human resources currently employed by the organizations supporting these efforts (e. g. ChEBI) are expanding linearly, while new useful scientific information is being released in a seemingly exponential fashion. Compounding this, currently existing chemical classification and annotation systems are not amenable to automated classification, formal and transparent chemical class definition axiomatization, facile class redefinition, or novel class integration, thus further limiting chemical ontology growth by necessitating human involvement in curation. Clearly, {{there is a need for}} the automation of this process, especially for novel chemical entities of biological interest. Results To address this, we present a formal framework based on Semantic Web technologies for the automatic design of chemical ontology which can be used for automated classification of novel entities. We demonstrate the automatic self-assembly of a structure-based chemical ontology based on 60 MeSH and 40 ChEBI chemical classes. This ontology is then used to classify 200 compounds with an accuracy of 92. 7 %. We extend these structure-based classes with molecular feature information and demonstrate the utility of our framework for classification of functionally relevant chemicals. Finally, we discuss an iterative approach that we envision for future biochemical ontology development. Conclusions We conclude that the proposed methodology can ease the burden of chemical data annotators and dramatically increase their productivity. We anticipate that the use of formal logic in our proposed framework will make chemical classification criteria more transparent to humans and machines alike and will thus facilitate predictive and integrative bioactivity model development. </p...|$|R
40|$|This paper {{discusses}} {{the most relevant}} earth construction techniques that have been employed {{in the province of}} Soria (Spain). The objectives of the research are, first, to locate and <b>catalogue</b> each building <b>techniques</b> with earth within the geographic regions and environmental units and their relation to physical and human environment. Then, the second aim is analyze these techniques and characterize them in terms of material, components and construction processes. Three types are studied: the construction of rammed-earth walls, adobe masonry and timber frame filled with adobe...|$|R
40|$|This thesis {{evaluated}} hazards from subaerial (originating {{above sea}} level) landslide-induced tsunamis in Knight Inlet and Howe Sound. Field assessments were conducted at Adeane Point and Mount Gardner. GIS was used at site and inlet scales to compile existing map data, to map submarine slide deposits, to measure topographic parameters, and to integrate observations. Modelling at Adeane Point employed kinematic, limit equilibrium (SWEDGE) and discrete element (3 DEC) analyses {{in order to}} estimate the volume of a potential landslide. Results suggest that the hazard from subaerial slide-induced waves is high in Knight Inlet, particularly in the area between Adeane Point and Glacier Bay, whereas, when compared with Knight Inlet, the hazard in Howe Sound appears considerably less. Modelling results suggest that topography and discontinuity persistence are the leading controls on failure volume. A preliminary <b>catalogue</b> of <b>techniques</b> for assessing hazards from slide waves was created, and related issues were discussed...|$|R
40|$|On April 24 - 25, 1991, {{people from}} seven DOE {{organizations}} {{participated in the}} annual meeting of the Calorimetry Exchange Program. The meeting featured a review of the statistical analysis of the calorimetry and gamma-ray data submitted to the exchange program during 1990. The meeting also enabled the group to review progress of five projects concerning a tritium exchange program, reprogramming of the database, a <b>catalogue</b> of measurement <b>techniques,</b> additional samples, and recharacterization of the current sample. There were presentations on recent advances in calorimetry and gamma-ray measurements...|$|R
40|$|This paper {{addresses}} {{the topic of}} interoperation of Grey resources. The title should be read as INTERoperation for Exploitation, Science and Technology. It builds on work by the authors published in previous GL conferences. The method is architectural analysis and comparison. The costs of the study are negligible, {{but of course the}} costs of implementing any solution are considerable. The result/conclusion is that CERIF (Common European Research Information Format) is the essential component to meet the requirements and is applicable - {{to a greater or lesser}} degree - in all architectural solutions. Our GL 9 (2007) paper proposed a Grey landscape architecture and identified the need for (1) excellent metadata (to improve discovery and control usage), (2) an institutional document repository of (or including) grey, (3) an institutional CRIS for the contextual research information, (4) linkage between the document repository and the CRIS of an institution and thence (in a controlled manner with formal descriptive and restrictive metadata) to other institutions, (5) an e-research repository of research datasets and software, (6) linkage between the e-research repository and the CRIS of an institution and thence (in a controlled manner with formal descriptive and restrictive metadata) to other institutions, (7) an institutional policy to mandate deposition of the material with appropriate metadata. These very requirements define the components for interoperation of Grey resources, and their interoperation with other resources to provide a holistic support for R&D. Indeed they can be extended (via the CRIS) to interoperation with other management systems of an organisation such as finance, human resources, project management, production control etc. However, the capability for interoperation can be provided in several implemented architectures. This paper discusses the advantages and disadvantages of different solutions including experience of their use. This analysis and experience is then applied to the grey environment. Remote and local wrapping of resources, <b>cataloguing</b> <b>techniques</b> and a full compliant model are discussed as well as harvesting technology. It concludes that - particularly for the grey environment - the optimal architecture involves formal syntax (structure of information) and defined semantics (meaning of information) as defined by CERIF. Includes: Conference preprint, Powerpoint presentation, Abstract and Biographical notes, Pratt student commentaryXAInternationa...|$|R
5000|$|Shimon Bar-Efrat (1929-2010) was an Israeli Old Testament scholar. He was Head of Biblical Studies at the Hebrew University Secondary School in Jerusalem, and is {{best known}} for his book, Narrative Art in the Bible, in which he [...] "provides a <b>catalogue</b> of {{literary}} <b>techniques</b> and devices found in Old Testament narratives." [...] Jeffrey Staley suggests that, along with Robert Alter, Adele Berlin, and Meir Sternberg, Bar-Efrat is a master of [...] "leading the reader through the sudden twists and sharp turns, the steep ridges and dizzying drop-offs that make up the art of ancient Hebrew characterization." ...|$|R
40|$|IT {{technologies}} {{and especially the}} progressive use of Internet have transformed the mechanism of knowledge transmission and its reproduction. We find, on the one hand, a model of knowledge circulation based on contract self-enforcement, through technological protection measures (TPMs). This kind of control is identified in Digital Rights Management (DRM), whose {{goal is to make}} the license terms for access and use of information recognizable by the software and equipment made for the use of information. Through DRM systems an automatic application (in personal computers, mobile phones, televisions, etc.) of contractual rules used for the distribution of digital contents is possible. On the other hand, stands the idea of Creative Commons Language which starts from the need to contrast the risk that a rigid and centralized control might colonize knowledge and above all to enhance the use of information technologies, Network, Web and new intermediaries (institutional archives, Internet search engines such as Google Books Search and Google Scholar, etc.). This latter aspect presents features of particular interest and is worthy of attention in this paper. The circulation of information on the Web becomes a key issue. Indeed, if the main purpose of Creative Commons is to ensure maximum diffusion and reuse of information and if the main space for the circulation of content is represented by the Web, the studies on <b>cataloguing</b> <b>techniques,</b> classification of information and the relevant rights in virtual spaces deserve special care. According to Creative Commons licenses (CCLs), the development and evaluation of knowledge are based on the collaboration of an open community of persons. The CC movement represents a landmark, not only from an ideological and contractual point of view, but also from a technological one: CC licenses, using some system technologies similar to those of DRM, appear to users in a readable form and also in a machine-readable form. The technologies which allow DRM systems to exercise strict control over information are developed by CC to facilitate the diffusion and the use of content, aiming at a flexible and decentralized control. The essay is intended to first provide {{a brief description of the}} IT technologies developed in DRM systems and by Creative Commons and then, for the translation of rules into IT code, to highlight the differences and, especially, the various achieved purposes...|$|R
40|$|This article {{examines}} the literature of statutory drafting. This underappreciated genre is perhaps the last place {{one would expect to}} find a sensitive soul struggling to escape the prison house of language. Yet the economic realities facing today 2 ̆ 7 s graduate students mean that many former lit majors will find their way onto legislative drafting committees, whence they peek out at us from under the sub-sections of their lives. It is time that this genre was formally recognized. Of course, its artists struggle against the challenge of the form and their achievements must be measured in millimeters rather than miles. This article offers a brief <b>catalogue</b> of the <b>technique</b> and art of statutory poetry, with examples...|$|R
50|$|Born in Kōchi Prefecture, Japan; in his youth, {{he was a}} sumo wrestler. He was {{captain of}} the sumo club at Kobe Business School (now Kobe University) and won the All-Kansai Student Sumo Championship. He later became the {{director}} of General Affairs at the Asahi News in Osaka. He was advised to learn the technique of Daito-ryu aiki-jujutsu for self-defense and was introduced to Ueshiba, becoming one of his early prewar students. Later (in 1936) he studied directly under Takeda when the latter came teaching at the Asahi News dojo. He received the Kyoju Dairi (teaching certification) a year later and was awarded the Menkyo kaiden rank in 1939, directly from Takeda. He later {{became one of the}} most prominent teachers of Daito-ryu aiki-jujutsu. In 1959, he established the Kansai Aikido Club to teach the techniques of Ueshiba and Takeda. Hisa is also remembered for compiling a <b>catalogue</b> of <b>techniques</b> from photographs taken at the Asahi News dojo featuring both Ueshiba and Takeda. The catalogue constitutes today an invaluable historical source for the early development of aikido. He also held the rank of 8th dan in sumo and 5th dan in judo.|$|R
40|$|International audienceAll mountainous {{countries}} {{have been exposed to}} slope movements throughout their history, reaching often disastrous dimensions. A significant increase in damage effects had to be recognised within the last two decades. The reasons for this steady increase in damage are often relayed on a probable impact of climate change. But people contribute also to, and even may exacerbate, or modify the hazard. The increase of our living standards, the concentration of people, infrastructure and goods at economically privileged but probably hazardous places, additional settlements in disaster prone regions and the enormous increase in mobility on road and train have to be considered as well. The number of victims and the cost of the damage may be high, depending on the duration, spatial extension and magnitude of the processes, and on the vulnerability of the exposed environment. Even though considerable advances in the last decade have been gained in landslide process understanding (landslide databases and event <b>catalogues,</b> monitoring <b>techniques,</b> investigation tools), there are a series of gaps that should be filled in existing knowledge in order to apply this knowledge for long-term development of the mountain territories and safety of the citizens. This is mainly the consequence of the large kinematic variability of slope movements (slide, flow, fall, spreading), which are dynamic systems, complex in time and space and closely linked to both inherited and current preparatory and triggering controls...|$|R
40|$|ABSTRACT- The {{perspective}} of multimedia object retrieval is the matching of relevant {{object in the}} image collection based on user query in a large database. In the past years, there is a comprehensive enhancement under content-based image retrieval system (CBIR). However, performance of the system need to be faster. Recent work has described various approaches and schemes {{that would have been}} expensive and difficult to arrange. There are several other methods which include query expression according to user needs. From the <b>catalogue</b> of retrieval <b>techniques,</b> Relevance Feedback enriches query refinement process. This paper proposes an algorithm for relevance feedback based on greedy approach to enhance the retrieval method. The greedy algorithm is used to sum up the performance of multimedia object retrieval system by using Relevance feedback technique. The main focus of the paper is to analyze how this greedy aspect of Relevance Feedback can be consolidated into existing retrieval system...|$|R
40|$|We {{investigate}} {{structures in}} the D 1 CFHTLS deep field {{in order to test}} the method that will be applied to generate homogeneous samples of clusters and groups of galaxies in order to constrain cosmology and detailed physics of groups and clusters. Adaptive kernel technique is applied on galaxy <b>catalogues.</b> This <b>technique</b> needs none of the usual a-priori assumptions (luminosity function, density profile, colour of galaxies) made with other methods. Its main drawback (decrease of efficiency with increasing background) is overcame by the use of narrow slices in photometric redshift space. There are two main concerns in structure detection. One is false detection and the second, the evaluation of the selection function in particular if one wants "complete" samples. We deal here with the first concern using random distributions. For the second, comparison with detailed simulations is foreseen but we use here a pragmatic approach with comparing our results to GalICS simulations to check that our detection number is not totally at odds compared to cosmological simulations. We use XMM-LSS survey and secured VVDS redshifts up to z~ 1 to check individual detections. We show that our detection method is basically capable to recover (in the regions in common) 100 % of the C 1 XMM-LSS X-ray detections in the correct redshift range plus several other candidates. Moreover when spectroscopic data are available, we confirm our detections, even those without X-ray data. Comment: 14 pages, 22 additionnal jpeg figures, accepted in A&...|$|R
40|$|Aims. We {{investigate}} {{structures in}} the D 1 CFHTLS deep field to test the method that will be applied to generate homogeneous samples of clusters and groups of galaxies in order to constrain the cosmology and detailed physics of groups and clusters. Methods. An adaptive kernel technique was applied to galaxy <b>catalogues.</b> This <b>technique</b> needs none of the usual a-priori assumptions (luminosity function, density profile, colour of galaxies) made with other methods. Its main drawback (decrease in efficiency with increasing background) is overcome {{by the use of}} narrow slices in photometric redshift space. There are two main concerns in structure detection. One is false detection and the second, the evaluation of the selection function in particular if one wants complete samples. We deal with the first concern using random distributions. For the second, comparison with detailed simulations is foreseen but we used a pragmatic approach by comparing our results to GalICS simulations to check that our detection number is not totally at odds with cosmological simulations. We used the XMM-LSS survey and secured VVDS redshifts up to z similar to 1 to check individual detections. Results. We show that our detection method is basically able to recover 100 % of the C 1 XMM-LSS X-ray detections (in the regions in common) in the correct redshift range plus several other candidates. Moreover, when spectroscopic data are available, we confirm our detections, even those without X-ray data. Peer reviewe...|$|R
40|$|Characterization {{of protein}} {{modification}} by phosphorylation {{is one of}} the major tasks that have to be accomplished in the post-genomic era. Phosphorylation is a key reversible modification occurring mainly on serine, threonine and tyrosine residues that can regulate enzymatic activity, subcellular localization, complex formation and degradation of proteins. The understanding of the regulatory role played by phosphorylation begins with the discovery and identification of phosphoproteins and then by determining how, where and when these phosphorylation events take place. Because phosphorylation is a dynamic process difficult to quantify, we must at first acquire an inventory of phosphoproteins and characterize their phosphorylation sites. Several experimental strategies can be used to explore the phosphorylation status of proteins from individual moieties to phosphoproteomes. In this review, we will examine and <b>catalogue</b> how proteomics <b>techniques</b> can be used to answer specific questions related to protein phosphorylation. Hence, we will discuss the different methods for enrichment of phospho-proteins and -peptides, and then the various technologies for their identification, quantitation and validation...|$|R
40|$|Today’s {{economy is}} a service economy, and an {{increasing}} number of services is electronic, i. e. can be ordered and provisioned online. Examples include Internet access, email and Voice over IP. Just as any other kind of services, e-services often are offered in bundles, and many consumer needs require the construction of e-service bundles. For example, a need to communicate with family abroad, can be satisfied by Voice over IP, which also requires Internet access. The problem is how to compose an e-service bundle so that the needs of the consumer are met optimally and the suppliers can provide the services in the bundle in an economically sustainable way. This is a requirements engineering problem: matching consumer needs to (combinations of) solutions. In this paper, we propose a technique to match a consumer need with a multi-supplier bundle of commercial e-services. The technique is intended to be used by suppliers when they build a service catalogue that describes what they can offer with their technical infrastructure in terms of what consumers can buy. It is also of interest to brokers who match consumer needs to what is offered by various suppliers in their <b>catalogues.</b> The <b>technique</b> is illustrated by means of a case study in which we used the technique to structure part of the catalogue of a Dutch telecommunication company. The technique is related to goal-oriented RE but it starts from consumer values rather than goals, and it matches existing solutions to needs, rather than creating a new solution. 1...|$|R
40|$|Over {{the last}} decade {{there has been a}} growing {{interest}} in the integration of agile software development process (ASDP) and user-centred design (UCD). However, there are no papers that study which usability techniques related to requirements engineering are being adopted in the ASDP, and there are no formalized proposals for their adoption. Objective: Identify which techniques related to requirements engineering activities are being adopted in the ASDP and determine how they are being adopted. Method: We have conducted a systematic mapping study (SMS) to retrieve the literature reporting the application of usability techniques in the ASDP. We analysed these <b>techniques</b> using a <b>catalogue</b> of <b>techniques</b> compiled by software engineering researchers. We then determined {{the manner in which the}} techniques that are being used in the ASDP were adopted. Results: The agile community is very much interested in adopting usability techniques. The most used techniques are Personas, contextual inquiry and prototyping. Conclusions: This research offers an overview of the adoption of usability techniques related to requirements engineering in ASDPs and reports how they are being adopted. We found that some of the techniques are being adapted for adoption. This research was funded by the Spanish Ministry of Education, Culture and Sports FLEXOR and “Realizando Experimentos en la Industria del Software: Comprensión del Paso de Laboratorio a la Realidad” projects (TIN 2014 - 52129 -R and TIN 2014 - 60490 -P, respectively) and the eMadrid-CM “Investigación y Desarrollo de Tecnologías Educativas en la Comunidad de Madrid” project (S 2013 /ICE- 2715) ...|$|R
40|$|Observations {{from the}} Heliospheric Imager (HI) {{instruments}} aboard the twin STEREO spacecraft have enabled the compilation of several catalogues of coronal mass ejections (CMEs), each characterizing {{the propagation of}} CMEs through the inner heliosphere. Three such catalogues are the Rutherford Appleton Laboratory (RAL) -HI event list, the Solar Stormwatch CME catalogue, and, presented here, the J-tracker catalogue. Each catalogue uses a different method to characterize the location of CME fronts in the HI images: manual identification by an expert, the statistical reduction of the manual identifications of many citizen scientists, and an automated algorithm. We provide a quantitative comparison {{of the differences between}} these <b>catalogues</b> and <b>techniques,</b> using 51 CMEs common to each catalogue. The time-elongation profiles of these CME fronts are compared, as are the estimates of the CME kinematics derived from application of three widely used single-spacecraft-fitting techniques. The J-tracker and RAL-HI profiles are most similar, while the Solar Stormwatch profiles display a small systematic offset. Evidence is presented that these differences arise because the RAL-HI and J-tracker profiles follow the sunward edge of CME density enhancements, while Solar Stormwatch profiles track closer to the antisunward (leading) edge. We demonstrate that the method used to produce the time-elongation profile typically introduces more variability into the kinematic estimates than differences between the various single-spacecraft-fitting techniques. This has implications for the repeatability and robustness of these types of analyses, arguably especially so in the context of space weather forecasting, where it could make the results strongly dependent on the methods used by the forecaster...|$|R
