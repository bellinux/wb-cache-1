0|1069|Public
3000|$|... dev, {{from the}} {{development}} database {{are used to}} train the <b>calibration</b> <b>weights,</b> i.e. the intercept and regression coefficient of the logistic regression model, and subsequently these <b>calibration</b> <b>weights</b> can then be used to calibrate scores from the test database. The pooled procedure for calculating the <b>calibration</b> <b>weights</b> was adopted (refer to [19] for details) in this paper. For a detailed tutorial on logistic regression calculation in converting a score to an interpretable likelihood ratio, refer to [12].|$|R
5000|$|Kott, P. (2006). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonresponse and {{coverage}} errors. Survey Methodology, 133-142 ...|$|R
50|$|A mass used to {{calibrate}} a weighing scale {{is sometimes called}} a calibration mass or <b>calibration</b> <b>weight.</b>|$|R
40|$|<b>Calibration</b> <b>weighting</b> is a {{methodology}} under which probability-sample weights are adjusted {{in such a}} way that when applied to survey data they can produce model-unbiased estimators for a number of different target variables. This paper briefly reviews the history of <b>calibration</b> <b>weighting</b> before the term was coined and some major developments since then. A change in the definition of a calibration estimator is recommended. This change expands the class to include such special cases as, 1, randomization-optimal estimators, and, 2, randomization-consistent estimators incorporating local polynomial regression. Although originally developed as a method for reducing sampling errors, <b>calibration</b> <b>weighting</b> has also been applied to adjust for unit nonresponse and for coverage errors. A variant of the jackknife variance estimator proposed here should prove computationally convenient for these applications...|$|R
5000|$|Kott, P., & Chang, T. (2010). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonignorable unit nonresponse. Journal of the American Statistical Association, 105, 1265-1275.|$|R
40|$|In the {{introductory}} chapter {{of this work}} is caught organizational structure of the national metrology system in the Czech Republic and its links to international organizations. There is indicated the basic terminology of metrology, particularly {{in the area of}} classification instruments. The following sections approaching the issue of measurement uncertainties, their classification, sources of uncertainty determined by the type A and B, their specifics and calculation. The above linked area already dealing with themselves calibrations, first of all <b>calibration</b> <b>weights,</b> classification of weights according accuracy classes, established procedures, and finally determining uncertainty in <b>calibration</b> <b>weights.</b> Then, immediately followed by a chapter dealing with calibration balances, performed tests and measurement uncertainties. The main part is of course directed towards the application of acquired knowledge to practical examples, thus performing the <b>calibration</b> <b>weight</b> class F 2 using a high-precision weights, both in the premises of the Technical University in Brno, both in the laboratory weighing the Czech Metrological Institute. Further calibration was performed school balances Ohaus Explorer EX 224...|$|R
50|$|Where γ is the <b>weight</b> <b>density.</b> <b>Weight</b> <b>density</b> {{may also}} be {{described}} as the pressure gradient because it directly determines how much extra pressure will be added by increasing depth of the column of fluid.|$|R
40|$|A {{nondestructive}} {{method for}} measuring local <b>weight</b> <b>density</b> {{has been developed}} which utilizes a hybrid sensor system, i. e., a contactor and an optical sensor. The accuracy of this system {{as defined by the}} correlation coefficient between light intensity and the local <b>weight</b> <b>density</b> is demonstrated to be above 90 %...|$|R
40|$|The work is {{justified}} {{from the standpoint}} of system analysis using multimedia factors to improve rehabilitation patients at different stages of the disease. The first time the formation of the methodology vector priorities specialists in various fields of <b>calibration</b> <b>weight</b> judgments are used according to their professional expertise...|$|R
40|$|In {{the range}} 16 to 29 °C, {{increases}} in temperature caused large (two-to threefold) increases in growth velocity, growth strain rate, and biomass deposition rate in primary roots of maize, Zea mays L. Temperature had small effects on root diameter, fresh <b>weight</b> <b>density,</b> and dry <b>weight</b> <b>density,</b> and negligible effects on {{length of the}} growth zone and growth strain at particular positions...|$|R
5000|$|... 2010 Edward Kresge - Exxon Chief Polymer Scientist who {{developed}} tailored molecular <b>weight</b> <b>density</b> EPDM elastomers ...|$|R
3000|$|... 0 [*]=[*] 1 {{means that}} the {{estimation}} of <b>weight,</b> <b>density,</b> and Young’s modulus by the vibration method with additional mass is perfect.|$|R
40|$|Background and Purpose. The {{purpose of}} this study was to {{evaluate}} the reli-ability cf measurements of weight distribution among the wheels of wheelchairs using a commercial balance testing system. Reliable data may be useful in the wheelchair evaluation and adjustment process. Subjects. Three male full-time manual wheelchair users aged 30, 26, and 2 7 years with cervical spinal cord injuries 7. 5, 65, and 10 years in duration participated. Metbods. <b>Calibration</b> <b>weights,</b> unoccupied wheelchairs, and occupied wheelchairs were repeatedly placed o n the force transducers of the balance testing system to obtain measure-ments o f weight distribution. Results The intraclass correlation coeficients of the measurements were. 99 for <b>calibration</b> <b>weights,.</b> 96 for unoccupied wheelchairs, and. 98 for wheelchairs occupied by the subjects. Conclusion and Discussion. The described use of this instrumentation appears to generate reliable measure-ments o f static weight distribution. With further testing, this system may provide useful information related to manual wheelchair prescription and adjustment...|$|R
50|$|Edward N. Kresge is {{a retired}} Exxon scientist, noted for his {{development}} of ethylene-propylene viscosity index modifiers, polyolefin thermoplastic elastomers, and tailored molecular <b>weight</b> <b>density</b> EPDM elastomers.|$|R
50|$|Crown green bowls come in {{a variety}} of bias strengths, <b>weights,</b> <b>densities,</b> sizes, {{materials}} and colours. The minimum weight is 2lb but there is no maximum weight.|$|R
40|$|An {{explanation}} is {{given for the}} paradoxical fact that, at low signal-to-noise ratios, the systematic feedback encoder results in fewer decoding bit errors than does a nonsystematic feedforward encoder for the same code. The analysis identifies a new code property, the d-distance <b>weight</b> <b>density</b> of the code. For a given d-distance <b>weight</b> <b>density,</b> the decoding bit error probability depends {{on the number of}} taps in the realization of the encoder inverse. Among all encoders for a given convolutional code, the systematic one has the simplest encoder inverse and, hence, gives the smallest bit error probabilit...|$|R
3000|$|The {{ratios of}} <b>weight,</b> <b>density,</b> and Young’s modulus, {{estimated}} by the vibration method with additional mass, to those obtained by the normal method without the concentrated mass (W/W [...]...|$|R
40|$|Fractional {{hot deck}} imputation, {{considered}} in Fuller and Kim (2005), is extended to multivariate missing data. The joint {{distribution of the}} study items is nonparametrically estimated using a discrete approximation, where the discrete transformation also serves to define imputation cells. The procedure first estimates the probabilities for the cells and then imputes real observations for missing items. <b>Calibration</b> <b>weighting</b> is used to reduce the imputation variance. Replication variance estimation is discussed...|$|R
40|$|We {{extend the}} problem of obtaining an {{estimator}} for the finite population mean parameter incorporating complete auxiliary information through calibration estimation in survey sampling but considering a functional data framework. The functional <b>calibration</b> sampling <b>weights</b> of the estimator are obtained by matching the calibration estimation problem with the maximum entropy on the mean principle. In particular, the calibration estimation is viewed as an infinite dimensional linear inverse problem following {{the structure of the}} maximum entropy on the mean approach. We give a precise theoretical setting and estimate the functional <b>calibration</b> <b>weights</b> assuming, as prior measures, the centered Gaussian and compound Poisson random measures. Additionally, through a simple simulation study, we show that our functional calibration estimator improves its accuracy compared with the Horvitz-Thompson estimator...|$|R
40|$|Sometimes {{benchmark}} constraints in a calibration problem {{cannot be}} met {{if there are}} range restric-tions on the <b>calibration</b> <b>weights.</b> There are various approaches to this problem that involve either allow-ing the benchmark constraints to be adjusted within a specified tolerance, or to determine a minimal lin-ear adjustment of the benchmark constraints. In this paper we propose an optimization problem that explicitly incorporates into the objective function {{a measure of the}} amount by which the benchmark con-straints are missed...|$|R
40|$|Tailbiting codes encoded by {{convolutional}} encoders are studied. An {{explanation is}} {{given for the}} fact that, at low signal-to-noise ratios, a systematic feedback encoder results in fewer decoding bit errors than a nonsystematic feedforward encoder for the same tailbiting code. The analysis {{is based on a}} recently introduced code property, namely, the <b>weight</b> <b>density</b> of distance-d codewords. For a given distance-d <b>weight</b> <b>density,</b> the decoding bit error probability depends on an encoder property, viz., the number of taps in the tap-minimal encoder pseudoinverse. Among all convolutional encoders that encode a given tailbiting code, the systematic one has the tap-minimal encoder pseudoinverse with fewest taps and, hence, gives the smallest bit error probability...|$|R
40|$|Given a {{randomly}} drawn sample, <b>calibration</b> <b>weighting</b> {{can provide}} double {{protection against the}} selection bias resulting from unit nonresponse. This means that if either an assumed linear prediction model or an implied unit selection model holds, the resulting estimator will be asymptotically unbiased in some sense. The functional form of the selection model when using linear alibration adjustment is dubious. The authors discuss an alternative, nonlinear calibration-weighting procedure and software that can, among other things, implicitly estimate a logistic-response model. " (author's abstract...|$|R
50|$|Calculating {{the volume}} of fiber ratio in a {{composite}} is relatively simple. The volume fiber fraction can be calculated {{using a combination of}} <b>weights,</b> <b>densities,</b> elastic moduli, stresses in respective directions, poison's ratios, and volumes of the matrix (resin system), fibers, and voids.|$|R
5000|$|The {{pressure}} of the reservoir fluids {{at the bottom of}} the hole is 38MPa. We have a kill fluid with a <b>weight</b> <b>density</b> of 16kN.m−3. What will need to be the height of the hydrostatic head in order to kill the well? ...|$|R
40|$|Weighting {{procedures}} are commonly applied in surveys {{to compensate for}} nonsampling errors such as nonresponse errors and coverage errors. Two types of weight-adjustment {{procedures are}} commonly used {{in the context of}} unit nonresponse: (i) nonresponse propensity <b>weighting</b> followed by <b>calibration,</b> also known as the two-step approach and (ii) nonresponse <b>calibration</b> <b>weighting,</b> also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented...|$|R
40|$|Parametric {{fractional}} imputation {{is proposed}} {{as a general}} tool for missing data analysis. Using fractional weights, the observed likelihood can be approximated by the weighted mean of the imputed data likelihood. Computational efficiency can be achieved using the idea of importance sampling and <b>calibration</b> <b>weighting.</b> The proposed imputation method provides efficient parameter estimates for the model parameters specified in the imputation model and also provides reasonable estimates for parameters that {{are not part of}} the imputation model. Variance estimation is discussed and results from a limited simulation study are presented. Copyright 2011, Oxford University Press. ...|$|R
40|$|We {{examined}} the atomic concentrations and the <b>weight</b> <b>densities</b> of SiC surfaces irradiated with remote nitrogen plasmas. The unique approach {{of this work}} is that we compared the SiC surface irradiated with atomic nitrogen with that irradiated {{with a mixture of}} atomic nitrogen and molecular nitrogen in the metastable A(3) Sigma(+) (u) state. As a result, it was found that molecular nitrogen in the A(3) Sigma(+) (u) state has a higher efficiency than atomic nitrogen in the nitriding of SiC surfaces. The <b>weight</b> <b>density</b> measurements have revealed the removal of Si and C from the SiC surface by the irradiation of remote nitrogen plasma. These results suggest that the formation of volatile molecules is less significant when the SiC surface is irradiated with molecular nitrogen in the metastable A(3) Sigma(+) (u) state...|$|R
40|$|Nylon- 6 is {{synthesised}} from epsilon-caprolactam by an anionic polymerisation method, {{through a}} two-step process using sodium metal {{as a catalyst}} and diisocyanate-caprolactam block initiator. Two different aliphatic diisocyanates are used. The effects of diisocyanate structure and concentration on percentage crystallinity, molecular <b>weight,</b> <b>density</b> and thermal properties of nylon are discussed...|$|R
40|$|Main abstract: Fluctuation scaling {{reports on}} all {{processes}} producing a data set. Some fluctuation scaling relationships, {{such as the}} Horwitz curve, follow exponential dispersion models which have useful properties. The mean-variance method applied to Poisson distributed data is a special case of these properties allowing the gain of a system to be measured. Here, a general method is described for investigating gain (G), dispersion (β), and process (α) in any system whose fluctuation scaling follows a simple exponential dispersion model, a segmented exponential dispersion model, or complex scaling following such a model locally. When gain and dispersion cannot be obtained directly, relative parameters, GR and βR, may be used. The method was demonstrated on data sets conforming to simple, segmented, and complex scaling. These included mass, fluorescence intensity, and absorbance measurements and specifications for classes of <b>calibration</b> <b>weights.</b> Changes in gain, dispersion, and process were observed in the scaling of these data sets in response to instrument parameters, photon fluxes, mathematical processing, and <b>calibration</b> <b>weight</b> class. The process parameter which limits the type of statistical process that can be invoked to explain a data set typically exhibited 0 4 possible. With two exceptions, <b>calibration</b> <b>weight</b> class definitions only affected β. Adjusting photomultiplier voltage while measuring fluorescence intensity changed all three parameters (0 <α< 0. 8; 0 <βR< 3; 0 <GR< 4. 1). The method provides a framework for calibrating and interpreting uncertainty in chemical measurement allowing robust compar ison of specific instruments, conditions, and methods. Supporting information abstract: On first inspection, fluctuation scaling data may appear to approximate a straight line when log transformed. The data presented in figure 5 of the main text gives a reasonable approximation to a straight line and for many purposes this would be sufficient. The {{purpose of the study}} of fluorescence intensity was to determine whether adjusting the voltage of a photomultiplier tube while measuring a fluorescent sample changes the process (α), the dispersion (β) and/or the gain (G). In this regard, the linear model established that PMT setting affects more than the gain. However, a detailed analysis beginning with testing for model mis-specification provides additional information. Specifically, Poisson behavior is only seen over a limited wavelength range in the 600 V and 700 V data sets...|$|R
5000|$|Where P is the {{pressure}} at depth h in the column, g is {{the acceleration of}} gravity and ρ is {{the density of the}} fluid. It is common in the oil industry to use <b>weight</b> <b>density,</b> which is the product of mass density and the acceleration of gravity. This reduces the equation to: ...|$|R
30|$|This work {{examines}} {{the effect of}} a method for generating bending vibration on the accuracy of a nondestructive and simple estimation of <b>weight,</b> <b>density,</b> and Young’s modulus through a vibration test without measuring specimen weight. The resonance frequencies with and without the concentrated mass generated by tapping the RT (radial tangential)-plane under the free-free condition were compared with those generated by the normal free-free bending vibration. The air-dried specimens and wet specimens in a drying process at 20  °C and 65 % relative humidity were used and then their <b>weight,</b> <b>density,</b> and Young’s modulus were estimated by the vibration test. The appropriate resonance frequency of the bending vibration could be obtained by tapping the RT-plane. Generating bending vibration by tapping the RT-plane is effective for the application of the vibration method with additional mass to a drying process.|$|R
50|$|Phillip S. Kott (born 1952) is an American statistician. He {{has worked}} in the field of survey {{statistics}} for more than 25 years, and is regarded as a leader in this field. His areas of expertise include survey sampling design, analysis of survey data, and <b>calibration</b> <b>weighting,</b> among other areas. He revolutionized sampling design and estimation strategies with the Agricultural Resource Management Survey, which uses survey information more efficiently. He has taught at George Mason University, and USDA Graduate School. He is currently an Associate Editor for the Journal of Official Statistics and the scientific journal Survey Methodology.|$|R
40|$|Correlations among {{grain sorghum}} quality factors (proximate composition, {{physical}} properties, and water absorption properties) were evaluated. Samples of 46 commercial hybrids (24 and 22 from crop years 1993 and 1994) were analyzed for starch, protein, crude free fat, test <b>weight,</b> absolute <b>density,</b> 1, 000 kernel weight, percent kernel abraded, water absorption index, initial water absorption rate, and moisture saturation point. Test <b>weight,</b> absolute <b>density,</b> and percent kernel abraded were positively correlated among themselves (r 3 ̆e 0. 5). Protein was {{negatively correlated with}} both test <b>weight</b> and absolute <b>density</b> (r 3 ̆c - 0. 5), while moisture saturation point showed negative correlations with test <b>weight,</b> absolute <b>density,</b> 1, 000 kernel weight, and percent kernel abraded (r 3 ̆c - 0. 4). Principal component factor analysis through the covariance matrix explained 95...|$|R
40|$|Abstract: Through {{double punch}} test, the tensile {{strength}} of compacted loess is determined under different water contents and different dry densities, {{the relationship between}} tensile strength, water content and the dry <b>weight</b> <b>density</b> is discussed, and their relationship is established. Comparing with Brazilian test, it proved the feasibility of determining tensile strength of compacted loess with double punch tests...|$|R
40|$|Although {{survey data}} are {{sometimes}} weighted by their selection weights, {{it is often}} preferable to use auxiliary information available on the whole population to improve estimation. <b>Calibration</b> <b>weighting</b> (Deville and Sarndal, 1992, Journal of the American Statistical Association 87 : 376 - 382) {{is one of the}} most common methods of doing this. This method adjusts the selection weights so that known population totals for the auxiliary variables are reproduced exactly, while ensuring that the calibrated weights are as close as possible to the original sampling weight. The simplest example of calibration is poststratification. This is the special case where the auxiliary variable is a single categorical variable. General calibration extends this to deal with more than one auxiliary variable and allows the user to include both categorical and numerical variables. A typical example might occur in a population survey, where the selection weights could be calibrated to ensure that the sample weighted by the <b>calibration</b> <b>weights</b> has exactly the same distribution as the population on variables such as age, sex, and region. Many packages have routines for calibration. SAS has the macro CALMAR; GenStat has the procedure SVCALIBRATE; and R has the function calibrate. However, no such routine is publicly available in Stata. I will introduce a user-written Stata program for calibration and will also discuss a simple extension to show how it can incorporate a nonresponse correction. I will also briefly discuss the program's strengths and limitations when compared to rival packages. ...|$|R
3000|$|The {{resonance}} frequencies {{with and}} without the concentrated mass generated by tapping the RT-plane under the free-free condition were compared with those generated by the normal free-free bending vibration. Then, the <b>weight,</b> <b>density,</b> and Young’s modulus estimated by the vibration method with additional mass using the bending vibration generated by tapping the RT-plane were examined. The following results were obtained: [...]...|$|R
