68|10000|Public
5000|$|The {{abovementioned}} notations can be {{best described}} by {{the assumption that the}} white buzz is formed when the total amount of positive comments is greater than the sum of neutral and negative ones. The same can be applied {{in the case of the}} black buzz. To obtain more accurate data, it might be prudent to search as much information as possible, as the <b>correctness</b> <b>of</b> <b>data</b> is directly proportional to the amount of the consumers' responses examined. Recent studies have indicated that the impact of negative comments tends to be higher than that of positive ones as they think they are more credible. However, it might depend on customer's reaction on different type of comments. [...] People are also more inclined to share a negative experience than a positive one in order to raise public awareness and prevent their friends and relatives from using the same service.Thus, it might be recommended for a big company to reduce the black buzz around the product or brand rather than increasing the amount of positive comments.|$|E
40|$|Cloud {{computing}} is a internet based computing {{which enables}} sharing of services. Many users place their {{data in the}} cloud, so <b>correctness</b> <b>of</b> <b>data</b> and security is a prime concern. This work studies the problem of ensuring the integrity and security of data storage in Cloud Computing. Security in cloud is achieved by signing the data block before sending to the cloud. Signing is performed using BLS algorithm which is more secure compared to other algorithms. To ensure the <b>correctness</b> <b>of</b> <b>data,</b> we consider the task of allowing a third party auditor (TPA), {{on behalf of the}} cloud client, to verify the integrity of the data stored in the cloud. By utilizing public key based homomorphic authenticator with random masking privacy preserving public auditing can be achieved. The technique of bilinear aggregate signature is used to achieve batch auditing. Batch auditing reduces the computation overhead. As the data in the cloud is used by many industries, modification of data cannot be avoided. Unlike most prior works, the new scheme further supports secure and efficient dynamic operations on data blocks, including: data update, delete and append. We explore the efficient technique for error correction called reed Solomon technique which ensures the <b>correctness</b> <b>of</b> <b>data.</b> Key word...|$|E
40|$|It {{has been}} great {{development}} in cloud computing since past few years. It offers different kinds of services for example Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as Service (IaaS). Cloud computing enables the user and organizations to store their data remotely and enjoy good quality applications on the demand without having any burden associated with local hardware resources and software managements but it possesses a new security risk towards <b>correctness</b> <b>of</b> <b>data</b> stored at cloud. There has been different techniques which in turn provide <b>correctness</b> <b>of</b> <b>data,</b> for example Merkle Hash Tree (MHT), Distributed erasure-coded data & flexible distributed storage integrity auditing mechanism. This work {{is based on the}} survey of different techniques of cloud storage with their benefits, disadvantages and security challenges. This particular study allows you to discover long term research areas and techniques for bettering the current downsides...|$|E
3000|$|... [...]. Further we use ∆ as a {{matching}} algorithm for checking <b>correctness</b> <b>of</b> the biometric <b>data,</b> and the function δ [...]...|$|R
40|$|Cloud {{storage is}} the only {{solution}} to the IT organizations to optimize the escalating storage costs and maintenance. Data outsourcing in to the cloud has become today trending environment for the thin organizations. The entire user’s data is store in large data centers, those which are located remotely and the users don’t have any control on it. on the other hand, this unique feature of the cloud poses many new security challenges which need to be clearly understood and resolved. Even the information is not accessible by the user the data centers should grant {{a way for the}} customer to check the <b>correctness</b> <b>of</b> his <b>data</b> is maintained or is compromise. As a solution this we are proposing providing information correctness to the customers in Cloud Storage. The proposing system gives a information correctness proof, the customer can employ to check the <b>correctness</b> <b>of</b> his <b>data</b> in the cloud. The <b>correctness</b> <b>of</b> the <b>data</b> can be approved upon by both the cloud and the customer and can be included in the Service level agreement (SLA). This scheme ensures that the storage at the client side is minimal which will be beneficial for thin clients...|$|R
40|$|This paper {{presents}} a practical automatic verification procedure for proving linearizability (i. e., atomicity and functional <b>correctness)</b> <b>of</b> concurrent <b>data</b> structure implementations. The procedure employs a novel instrumentation to verify logically pure executions, and is evaluated {{on a number}} of standard concurrent stack, queue and set algorithms. ...|$|R
40|$|Failure is {{unavoidable}} in any computing environment and hence any computing architecture must address recovery issues. Recovery becomes more complicated when sites are distributed, autonomous and heterogeneous. Grid architecture {{is such an}} evolving distributed architecture. Databases operating in Grid architecture have different recovery issues than their other distributed counterparts - distributed and multidatabase. In this paper we focus on maintaining <b>correctness</b> <b>of</b> <b>data</b> in case of site failure in Grid database...|$|E
40|$|The {{quality of}} data can only be {{improved}} by cleaning data prior to loading into the data warehouse as <b>correctness</b> <b>of</b> <b>data</b> is essential for well-informed and reliable decision making. Data warehouse is the only viable solution that can bring that dream into a reality. The quality of the data can only be produced by cleaning data prior to loading into data warehouse. Data Cleaning {{is a very important}} process of the data warehouse. It is not a very easy process as many different types of unclean data can be present. So <b>correctness</b> <b>of</b> <b>data</b> is essential for well-informed and reliable decision making. Also, whether a data is clean or dirty is highly dependent on the nature and source of the raw data. Many attempts have been made till now to clean the data using different types of algorithms. In this paper an attempt has been made to provide a hybrid approach for cleaning data which combines modified versions of PNRS, Transitive closure algorithms and Semantic Data Matching algorithm {{can be applied to the}} data to get better results in data corrections...|$|E
30|$|Data {{collection}} {{for this study}} {{took place in the}} second week of November 2012. Although very time consuming, the content analysis was conducted by the author himself to ensure for consistency. As the author reads eight languages, accuracy of the content analysis was not compromised. For FB pages in Japanese and in some non-Slavic and non-Romanic languages, a colleague was present to help with translation and control the <b>correctness</b> <b>of</b> <b>data</b> collection process.|$|E
40|$|In this {{application}} paper we describe {{a system for}} visualization <b>of</b> spatial <b>data</b> structures, {{that are used to}} accelerate ray shooting in global illumination. The system has been implemented and further used to verify the <b>correctness</b> <b>of</b> spatial <b>data</b> structures implementation, since for large and complex scenes the verification is difficult or even impossible...|$|R
40|$|Summary: PRIDE-NMR is a fast novel {{method to}} relate known protein folds to NMR {{distance}} restraints. It {{can be used}} to obtain a first guess about a structure being determined, as well as to estimate the completeness or verify the <b>correctness</b> <b>of</b> NOE <b>data.</b> Availability: The PRIDE-NMR server is available a...|$|R
50|$|Data {{exploration}} is {{an approach}} similar to initial data analysis, whereby a data analyst uses visual exploration {{to understand what}} is in a dataset and the characteristics <b>of</b> the <b>data,</b> rather than through traditional data management systems. These characteristics can include size or amount <b>of</b> <b>data,</b> completeness <b>of</b> the <b>data,</b> <b>correctness</b> <b>of</b> the <b>data,</b> possible relationships amongst data elements or files/tables in the data.|$|R
40|$|Abstract — This paper {{describes}} a procedure of identifying sensor faults and reconstructing the erroneous measurements. Data mining algorithms are successfully applied for deriving models that estimate {{the value of}} one variable based on correlated others. The estimated values can then be used instead of the recorded ones of a measuring instrument with false reading. The aim is to reassure the <b>correctness</b> <b>of</b> <b>data</b> entered to an optimization software application under development for th...|$|E
40|$|Abstract —  Cloud {{computing}} is {{an internet}} based computing which enables sharing of services. Cloud computing {{allows users to}} use applications without installation any application and access their personal files and application at any computer with internet or intranet access. Many users place their data in the cloud, so <b>correctness</b> <b>of</b> <b>data</b> and security is a prime concern. Cloud Computing is technology for next generation Information and Software enabled work {{that is capable of}} changing the software working environment. It is interconnecting the large-scalecomputing resources to effectively integrate, and to computing resources as a service to users. To ensure the <b>correctness</b> <b>of</b> <b>data,</b> we consider the task of allowing a third party auditor (TPA), on behalf of the cloud client, to verify the integrity of the data stored in the cloud, the auditing process should bring in no new vulnerabilities towards user data privacy, and introduce no additional online burden to user. In this research paper, we propose a secure cloud storage system supporting privacy-preserving public auditing. We further extend our result to enable the TPA to perform audit efficiently with RC 5 Encryption Algorithm. Resulted encrypted method is secure and easy to use...|$|E
40|$|This paper {{presents}} {{the design of}} a communication state transfer protocol to support process migration in a dynamic, distributed computing environment. In our design, processes in distributed computation communicate one another via message passing and are migration-enabled. Due to mobility, mechanisms to maintain reliability and <b>correctness</b> <b>of</b> <b>data</b> communication are needed. Following an event-based approach, such mechanisms are derived to handle various communication situations when a process migrates. These mechanisms collectively preserve the semantics of the communicationand support efficient communication state transfer. ...|$|E
40|$|We {{describe}} {{our experience}} with verifying the scheduler-related functionality of FreeRTOS, a popular open-source embedded real-time operating system. We propose a methodology {{for carrying out}} refinement-based proofs <b>of</b> functional <b>correctness</b> <b>of</b> abstract <b>data</b> types in the popular code-level verifier VCC. We then apply this methodology {{to carry out a}} full machine-checked proof <b>of</b> the functional <b>correctness</b> <b>of</b> the FreeRTOS scheduler. We describe the bugs found during this exercise, the fixes made, and the effort involved...|$|R
40|$|The pi^- p -> omega n {{threshold}} {{cross section}} {{measured in the}} seventies resisted up to now a consistent theoretical description in line with experiment, mainly caused by too large Born contributions. This led to a discussion in the literature about the <b>correctness</b> <b>of</b> the extraction <b>of</b> these <b>data</b> points. We show that the extraction method used in these references is indeed correct {{and that there is}} no reason to doubt the <b>correctness</b> <b>of</b> these <b>data.</b> Comment: 5 pages, discussion extended, conclusions unchange...|$|R
40|$|Abstract. This paper {{draws on}} the {{successful}} experience from domestic and abroad researches, testing repeatedly of powder subgrade with load triaxial tests, analyzing {{the relationship between the}} subgrade soil resilient modulus, deviatoric stress, water content and degree of compaction. At the same time, conducting corresponded roadbed soil resilient modulus forecast model, and qualifying its <b>correctness</b> <b>of</b> test <b>data</b> model...|$|R
30|$|Tracking and {{ensuring}} the <b>correctness</b> <b>of</b> <b>data</b> {{is an important}} part of the execution pipeline. The Nanosurveyor framework provides a module called nscxwrite which allows customized writing of files at different stages of the data acquisition pipeline (raw, filtered, and reconstructed). This capability provides several benefits, such as assurances to users that data move correctly from module to module and are not corrupted along the way, as well as an ability to debug an algorithm that is executed within a complex sequence of events.|$|E
40|$|Semantic {{properties}} of stored {{data can be}} represented by integrity constraints. Repairing violated constraints thus contributes to maintain the <b>correctness</b> <b>of</b> <b>data.</b> Repairing all inconsistencies in databases often is inconvenient and sometimes not feasible. We show {{that it is possible}} to con-ceive partial repairs that reduce the amount of extant in-consistency but do not necessarily eliminate it totally. Thus, such repairs are able to improve the integrity of the stored data, while providing more flexibility than traditional re-pairs that are required to be total. ...|$|E
30|$|Entity-centric: The entity is {{the main}} object in this group, and the trust model focuses on the {{trustworthiness}} of vehicles. To achieve this, the trust model needs sufficient information about the neighbors and sender of the message. But the high mobility of vehicles leads to failure to collect enough information about the neighbors/sender. In addition, the <b>correctness</b> <b>of</b> <b>data</b> is another problem in this group. When an entity received data from a trustworthy sender, {{according to the presence}} of attackers as well as limitation of sensors, data correctness still remains obscure.|$|E
50|$|Individual packets {{may take}} {{different}} routes to the destination and {{arrive at the}} destination out of order. The destination computer verifies the <b>correctness</b> <b>of</b> the <b>data</b> in each packet (using information in the trailer), reassembles the original item using the packet number information in the header, and presents the item to the receiving application or user.|$|R
40|$|AbstractThe {{rapid growth}} of the World Wide Web has {{resulted}} in more data being accessed over the Internet. In turn there {{is an increase in}} the use <b>of</b> semistructured <b>data,</b> which plays a crucial role in many web applications particularly with the introduction of XML and its related technologies. This increase in use makes the design <b>of</b> good semistructured <b>data</b> structures essential. The Object Relationship Attribute model for Semistructured data (ORA-SS) is a graphical notation for designing and representing semistructured data. In this paper, we demonstrate an approach to formally validate the ORA-SS data models in order to enhance the <b>correctness</b> <b>of</b> semistructured <b>data</b> design. A mathematical semantics for the ORA-SS notation is defined using the Z formal language, and further validation processes are carried out to check the <b>correctness</b> <b>of</b> the semistructured <b>data</b> models at both the schema and instance levels...|$|R
40|$|Monitoring the {{development}} process is a complex task due {{to the amount of}} resources required (i. e. time and money), moreover the <b>correctness</b> <b>of</b> collected <b>data</b> is not guaranteed if the required tasks are performed manually. This paper describes an extension of the Eclipse IDE environment to allow developers to collect data regarding {{the development}} process without any effort. 1...|$|R
40|$|AbstractTwo {{methods for}} proving the <b>correctness</b> <b>of</b> <b>data</b> {{representations}} are presented which employ a mathematical {{relation between the}} data values in a representation and those in its abstract model. One method reflects the behavioural equivalence relation of abstract data type theory, {{and the other a}} new “behavioural inclusion” notion that formalizes the idea of a “partial representation” of a data type. These correctness concepts and proof methods are strictly more general than the conventional ones based on abstraction functions, and they are no longer affected by “implementation bias” in specifications...|$|E
40|$|Abstract. Simulation {{relations}} are tools for establishing the <b>correctness</b> <b>of</b> <b>data</b> refinement steps. In the simply-typed lambda calculus, logical {{relations are}} the standard choice for simulation relations, but they suffer from certain shortcomings; these are resolved {{by use of}} the weaker notion of pre-logical relations instead. Developed from a syntactic setting, abstraction barrier-observing simulation relations serve the same purpose, and also handle polymorphic operations. Meanwhile, second-order prelogical relations directly generalise pre-logical relations to polymorphic lambda calculus (System F). We compile the main refinement-pertinent results of these various notions of simulation relation, and try to raise some issues for aiding their comparison and reconciliation. ...|$|E
40|$|AbstractUsing the {{theorem prover}} Isabelle/HOL we have formalized and proved correct and {{executable}} bytecode verifier {{in the style}} of Kildall's algorithm for a significant subset of the Java Virtual Machine (JVM). First an abstract framework for proving <b>correctness</b> <b>of</b> <b>data</b> flow based type inference algorithms for assembly languages is formalized. It is shown that under certain conditions Kildall's algorithm yields a correct bytecode verifier. Then the framework is instantiated with our previous work about the JVM. Finally, we demonstrate the flexibility of the framework by extending our previous JVM model and the executable bytecode verifier with object initialization...|$|E
30|$|When {{affected}} by other markers on the driveway, {{or when the}} lane is damaged or the pollution is serious, {{the image of the}} lane line collected by the system is incomplete and of poor quality, making it difficult for the system to accurately judge and analyze the data, and it is impossible to guarantee the <b>correctness</b> <b>of</b> the <b>data</b> and real-time detection of lane lines.|$|R
50|$|The Chase is {{a simple}} fixed-point {{algorithm}} testing and enforcing implication <b>of</b> <b>data</b> dependencies in database systems. It plays important roles in database theory {{as well as in}} practice.It is used, directly or indirectly, on an everyday basis by people who design databases, and it is used in commercial systems to reason about the consistency and <b>correctness</b> <b>of</b> a <b>data</b> design. New applications of the chase in meta-data management and data exchange are still being discovered.|$|R
5000|$|Data reification (stepwise refinement) {{involves}} {{finding a}} more concrete representation <b>of</b> the abstract <b>data</b> types {{used in a}} specification. There may be several steps before an implementation is reached. Each reification step for an abstract data representation [...] involves proposing a new representation [...] In order {{to show that the}} new representation is accurate, a retrieve function is defined that relates [...] to , i.e[...] The <b>correctness</b> <b>of</b> a <b>data</b> reification depends on proving adequacy, i.e.|$|R
40|$|Simulation {{relations}} are tools for establishing the <b>correctness</b> <b>of</b> <b>data</b> refinement steps. In the simply-typed lambda calculus, logical {{relations are}} the standard choice for simulation relations, but they su#er from certain shortcomings; these are resolved {{by use of}} the weaker notion of pre-logical relations instead. Developed from a syntactic setting, abstraction barrier-observing simulation relations serve the same purpose, and also handle polymorphic operations. Meanwhile, second-order prelogical relations directly generalise pre-logical relations to polymorphic lambda calculus (System F). We compile the main refinement-pertinent results of these various notions of simulation relation, and try to raise some issues for aiding their comparison and reconciliation...|$|E
30|$|Unlike the RMI the sockets do {{not provide}} remote objects and offers only means for data transfer. The {{programmer}} is solely responsible for ensuring the <b>correctness</b> <b>of</b> <b>data</b> flow that {{is making sure that}} both the server and the client expect the same type of data. As the real life communication between a server and a client requires many types of messages being transferred and this may sound prohibitive. Fortunately enough the JVM supports the transfer of any serializable object what makes the task less daunting. Using serialized objects by the socket mechanism {{does not mean that we}} have access to objects methods. Only the data are transferred.|$|E
30|$|In this approach, {{data are}} {{embedded}} in the bitstream resulting from the encoding process and extracted before traversing the decoder side. Since the bitstream is more sensitive to modifications than the original audio signal, the hiding capacity should be kept small to avoid embedded data perceptibility. Furthermore, transcoding can modify embedded data values and therefore could alter {{the integrity of the}} steganographic system. However, one of the positive sides of these methods is the <b>correctness</b> <b>of</b> <b>data</b> retrieval. Hidden message-extraction is done with no loss in tandem-free operations since it is not affected by the encoding process. A general scheme of the three steganography approaches is illustrated in Figure 9.|$|E
30|$|As {{with the}} {{application}} testing, the <b>correctness</b> <b>of</b> the <b>data</b> {{as well as}} the proper authentication were verified. The web application has been created as an add-on and is not fully customized for viewing on a large computer screen, but, despite this fact, the visualization is transparent and works reliably. The server handled a larger number of connected applications using a mobile application or a browser on a computer.|$|R
40|$|Abstract- IEEE 802. 16 {{standard}} {{known as}} WiMAX (Worldwide Interoperability for Microwave Access), {{is one of}} the most promising wireless access technology for next generation all-IP networks. The fundamental requirements for WiMAX to define itself as a possible winning technology are data reliability and the ability to deliver multimedia contents. WiMAX networks face all the problems related to hostile wireless environment, where power constraints make it difficult to provide hard QoS guarantees. This paper addresses the main issues of security and power efficiency, proposing an efficient cross-layered approach for data transmission. Direct transmission consumes more energy. Multihop communication involves formation of groups, where group heads aggregate the data before transmitting it to the Base Station. It uses Chessboard clustering algorithm to perform clustering. If a group head is compromised, then the Base Station cannot ensure the <b>correctness</b> <b>of</b> the <b>data</b> sent to it. A group head is selected at random for forwarding the aggregate. Hence, this paper proposes a novel mechanism for the Base Station to ensure the <b>correctness</b> <b>of</b> the <b>data</b> sent to it...|$|R
40|$|Abstract — In Cloud storage {{with the}} high costs <b>of</b> <b>data</b> storage devices {{as well as the}} rapid rate at which data is being {{generated}} it proves costly for enterprises or individual users to frequently update their hardware. Apart from reduction in storage costs data outsourcing to the cloud it helps in reducing the maintenance. In this storage moves the user’s data to large data centers, which are remotely located, on which user does not have any control. However, this unique feature of the cloud poses many new security challenges which need to be clearly understood and resolved. The important thing to be addressed is to assure the customer of the integrity i. e. <b>correctness</b> <b>of</b> his <b>data</b> in the cloud. As the data is physically not accessible to the user the cloud should provide a way for the user to check if the integrity <b>of</b> his <b>data</b> is maintained or is compromised. In this paper we provide a scheme which gives a proof <b>of</b> <b>data</b> integrity in the cloud which the customer can employ to check the <b>correctness</b> <b>of</b> his <b>data</b> in the cloud. This proof can be agreed upon by both the cloud and the customer and can be incorporated in the Service level agreement (SLA). Keywords:POR,SLA,Encryption,Integrity,Cloud storage,Sentinels,archive. I...|$|R
