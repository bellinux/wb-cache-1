0|36|Public
5000|$|Operation: A {{sequence}} of contraction steps, each {{consisting of a}} rake <b>operation</b> and a <b>compress</b> <b>operation</b> (in any order). The rake operation removes all the leaf nodes in parallel. The <b>compress</b> <b>operation</b> finds an independent set of unary nodes and splice out the selected nodes.|$|R
5000|$|Assume {{the number}} of nodes before the {{contraction}} to be m, and m' after the contraction. By definition, the rake operation deletes all [...] and the <b>compress</b> <b>operation</b> deletes at least 1/4 of [...] in expectation. All [...] remains. Therefore, we can see: ...|$|R
40|$|Multiplex {{expansion}} in point-of-care diagnostics usually requires a linear increase of premium commodities such as reagents or space. Here we demonstrate {{the power of}} binary and molecular encoding to <b>compress</b> device <b>operations.</b> We describe the first colorimetric 7 -segment display on a paper-based biosensor, providing compact and intuitive read-outs for multiplex detections...|$|R
50|$|From about 1935 to 1960, buses by the AEC, Daimler, and {{sometimes}} Leyland and Guy companies offered Preselector gearboxes, either {{as an option}} or as standard. London buses invariably used this transmission, along with other cities. Country area buses still commonly retained manual transmissions as {{they did not have}} the requirement of constant stopping and starting at bus stops. The London specification included <b>compressed</b> air <b>operation</b> of the change-gear pedal, where others used unassisted operation.|$|R
50|$|In {{computer}} science, {{a binary}} decision diagram (BDD) or branching {{program is a}} data structure {{that is used to}} represent a Boolean function. On a more abstract level, BDDs can be considered as a compressed representation of sets or relations. Unlike other <b>compressed</b> representations, <b>operations</b> are performed directly on the compressed representation, i.e. without decompression. Other data structures used to represent a Boolean function include negation normal form (NNF), and propositional directed acyclic graph (PDAG).|$|R
2500|$|In a {{financial}} and sales tailspin, the R. Dakin Company proceeded {{to sell this}} headquarters building to C. Michael Hogan, president of Earth Metrics Inc., an environmental research and consulting firm. Earth Metrics then moved into two floors of the building, as Dakin <b>compressed</b> its <b>operations.</b> At Dakin's request, The Good Guys! were secured as a major tenant in 1992, and the new owner performed substantial tenant improvements of approximately $1,300,000, upgrading the computer room infrastructure and building an auditorium.|$|R
40|$|RC 4 -Based Hash Function {{is a new}} {{proposed}} {{hash function}} based on RC 4 stream cipher for ultra low power devices. In this paper, we analyse {{the security of the}} function against collision attack. It is shown that the attacker can find collision and multi-collision messages with complexity only 6 <b>compress</b> function <b>operations</b> and egligible memory with time complexity 213. In addition,we show the hashing algorithm can be distinguishable from a truly random sequence with probability close to one. 6 page(s...|$|R
40|$|Abstract—In this paper, {{the design}} issues of a coverage-based CRRM {{strategy}} are studied to support voice {{services in a}} heterogeneous UMTS/GSM environment which distribute the traffic load by controlling the effective UMTS coverage through a predefined pathloss threshold. Comparing with the conventional Load Balancing strategy, the proposed coverage-based CRRM gives a better performance in terms of voice capacity {{at the expense of}} extra inter-system handovers. The effects of <b>compress</b> mode <b>operation</b> and mobility on system performance and pathloss threshold selection are also studied in details to refine the CRRM. I...|$|R
40|$|Abstract. Sequence {{representations}} supporting queries access, {{select and}} rank {{are at the}} core of many data structures. There is a considerable gap between different upper bounds, and the few lower bounds, known for such representations, and how they interact with the space used. In this article we prove a strong lower bound for rank, which holds for rather permissive assumptions on the space used, and give matching upper bounds that require only a compressed representation of the sequence. Within this <b>compressed</b> space, <b>operations</b> access and select can be solved within almost-constant time. ...|$|R
40|$|Abstract—Since nation-wide {{coverage}} can’t immediately {{be achieved}} with the recent roll-out of UMTS networks in many countries, network performance and service continuity is highly based on co-using the existing GSM infrastructure. Therefore, seamless inter-working between the two networks is substantial for a successful launch of UMTS services. This article provides a detailed introduction into the mechanisms of cross network measurements and handovers. Based on life network experiences, a novel parameterization method for UMTS <b>compressed</b> mode <b>operation</b> and inter-system handovers is proposed and evaluated by means of dynamic event-driven simulation. Index Terms—Inter-System Handover, Inter-RAT Handover, UMTS-GSM Handover, UMTS, GSM, Compressed Mod...|$|R
40|$|We {{answer the}} {{following}} question: Let P and Q be graded posets having some property and let ◦ be some poset operation. Is {{it true that}} P ◦ Q has also this property? The considered properties are: being Sperner, a symmetric chain order, Peck, LYM, and rank <b>compressed.</b> The studied <b>operations</b> are: direct product, direct sum, ordinal sum, ordinal product, rankwise direct product, and exponentiation...|$|R
40|$|Sequence {{representations}} supporting queries $access$, $select$ and $rank$ are at {{the core}} of many data structures. There is a considerable gap between the various upper bounds and the few lower bounds known for such representations, and how they relate to the space used. In this article we prove a strong lower bound for $rank$, which holds for rather permissive assumptions on the space used, and give matching upper bounds that require only a compressed representation of the sequence. Within this <b>compressed</b> space, <b>operations</b> $access$ and $select$ can be solved in constant or almost-constant time, which is optimal for large alphabets. Our new upper bounds dominate all of the previous work in the time/space map...|$|R
40|$|Damper spring reduces {{deflections}} of omega-cross-section seal, reducing {{probability of}} failure and extending life of seal. Spring is split ring with U-shaped cross section. Placed inside omega seal and inserted with seal into seal cavity. As omega seal compressed into cavity, spring and seal make contact near convolution of seal, and spring becomes <b>compressed</b> also. During <b>operation,</b> when seal dynamically loaded, spring limits deflection of seal, reducing stress on seal...|$|R
5000|$|In a {{financial}} and sales tailspin, the R. Dakin Company proceeded {{to sell this}} headquarters building to C. Michael Hogan, president of Earth Metrics Inc., an environmental research and consulting firm. Earth Metrics then moved into two floors of the building, as Dakin <b>compressed</b> its <b>operations.</b> At Dakin's request, The Good Guys! were secured as a major tenant in 1992, and the new owner performed substantial tenant improvements of approximately $1,300,000, upgrading the computer room infrastructure and building an auditorium. [...] The Foster Company purchased the entire holding in 1995. The Foster Company is the successor entity of T. Jack Foster, a wealthy real estate magnate, for whom Foster City, California is named, since Foster owned and developed most of that San Francisco Peninsula city of population around 30,000.|$|R
40|$|We {{describe}} a non-blocking concurrent hash trie based on shared-memory single-word compare-and-swap instructions. The hash trie supports standard mutable lock-free operations such as insertion, removal, lookup and their conditional variants. To ensure space-efficiency, removal <b>operations</b> <b>compress</b> the trie when necessary. We show how to implement an efficient lock-free snapshot op-eration for concurrent hash tries. The snapshot operation uses a single-word compare-and-swap and avoids copying the data struc-ture eagerly. Snapshots {{are used to}} implement consistent iterators and a linearizable size retrieval. We compare concurrent hash trie performance with other concurrent data structures and evaluate {{the performance of the}} snapshot operation...|$|R
50|$|The Electro-pneumatic {{brake system}} on British railway trains was {{introduced}} in 1950 and remains the primary braking system for multiple units in service today. The Southern Region of British Railways operated a self-contained fleet of electric multiple units for suburban and middle distance passenger trains. From 1950, {{an expansion of the}} fleet was undertaken and the new build adopted a braking system that was novel in the UK, the electro-pneumatic brake in which <b>compressed</b> air brake <b>operation</b> was controlled electrically by the driver. This was a considerable and successful technical advance, enabling a quicker and more sensitive response to the driver’s operation of brake controls.|$|R
40|$|Crushers are {{machines}} {{which use}} a metal surface to break or <b>compress</b> materials mining <b>operations</b> use crushers, commonly classified {{by the degree}} to which they fragment the starting material with primary and secondary crushers handling coarse materials and tertiary and quaternary crushers reducing ore particles to finer gradations. This paper focuses on review of a work carried out by researchers in the field of kinematic & dynamic analysis of the jaw crusher attachment. Kinematic & Dynamic analysis is helpful for understanding and improving the design quality of jaw crusher. There are many researcher work done by researcher in the same field but still there is a scope to develop Kinematic & dynamic analysis to jaw crusher attachment...|$|R
5000|$|The Atlas Diesel merger {{experienced}} significant growth {{during the}} First World War and {{towards the end}} of the war, export dominated 40-50 % of production. The depression years caused significant losses for the company, which led to several financial reconstructions in the 1920s and 1930s. The economy began recovering, demand started growing again in the mid-1930s, and Atlas Diesel experienced a boom in sales, where <b>compressed</b> air <b>operations</b> was the most expansive area.The Second World War remained an active period for the firm and a time when strategic planning for development played a principal role. Manufacturing capabilities were embellished and the purchasing along with the acquisition of manufacturing subsidiaries in Sweden and other countries, was a key component to Atlas Diesel's continued growth after WWII. The [...] "Swedish method" [...] was another war period strategy that strongly influenced the firm's pneumatic program, consisting of lightweight rock drills and drill bits with carbide tips. In 1948 the company terminated its diesel manufacturing and the name [...] "Atlas Diesel" [...] was no longer pertinent. The name Atlas Copco became official in 1955 and was inspired by the Belgian subsidiary Compagnie Pneumatique.|$|R
40|$|ABSTRACT: Truncated {{multipliers}} offers {{significant improvements}} in area, delay, and power. The proposed method finally reduces the number of full adders and half adders during the tree reduction. While using this proposed method experimentally, area can be saved. The output {{is in the form}} of LSB and MSB. Finally the LSB part is <b>compressed</b> by using <b>operations</b> such as deletion, reduction, truncation, rounding and final addition. In previous related papers, to reduce the truncation error by adding error compensation circuits. In this project truncation error is not more than 1 ulp (unit of least position). So there is no need of error compensation circuits, and the final output will be précised. To further extend the work the design is realized in a FIR filter...|$|R
40|$|Abstract — Mining {{frequent}} item sets in a {{data set}} {{is a significant}} problem of data mining {{that can only be}} solved in exponential time. For especially very large data sets, finding frequent item sets in practical run times is extremely important. A market basket data set can be represented by a set of bit vectors, which enables fast computation and low memory requirements. Space requirements can be reduced and better run time results can be obtained if bit vectors can be <b>compressed,</b> and binary <b>operations</b> can be applied on compressed bit vectors. In this paper, we study {{the advantages and disadvantages of}} using compression techniques for finding frequent item sets. Experimental evaluations clearly show that applying special purpose compression techniques has many benefits on a wide range of data sets. 1...|$|R
40|$|Artículo de publicación ISISequence {{representations}} {{supporting the}} queries access, select, and rank {{are at the}} core of many data structures. There is a considerable gap between the various upper bounds and the few lower bounds known for such representations, and how they relate to the space used. In this article, we prove a strong lower bound for rank, which holds for rather permissive assumptions on the space used, and give matching upper bounds that require only a compressed representation of the sequence. Within this <b>compressed</b> space, the <b>operations</b> access and select can be solved in constant or almost-constant time, which is optimal for large alphabets. Our new upper bounds dominate all of the previous work in the time/space map. Fondecyt, Chile 	 1 - 110066 French MAPPI Project 	 ANR- 2010 -COSI- 004 Academy of Finland 	 250345 (Co-ECGR...|$|R
40|$|This paper {{addresses}} {{the problem of}} processing motion-JPEG video data in the <b>compressed</b> domain. The <b>operations</b> covered are those where a pixel in the output image is an arbitrary linear combination of pixels in the input image, which includes convolution, scaling, rotation, translation, morphing, de-interlacing, image composition, and transcoding. This paper further develops an approximation technique called condensation to improve performance and evaluates condensations in terms of processing speed and image quality. Using condensation, motion-JPEG video can be processed at near real-time rates on current generation workstations. 1 Introduction Processing video data is problematic due to the high data rates involved. Television quality video requires approximately 100 GBytes for each hour, or about 27 MBytes for each second. Such data sizes and rates severely stress storage systems and networks and make even the most trivial real-time processing impossible without special purpose ha [...] ...|$|R
40|$|We {{present an}} {{end-to-end}} server–client system for high-resolution video transmission over IP networks. The system supports communication of <b>compressed</b> information, user <b>operations</b> through a fully functional user interface {{as well as}} detection and tracking of moving objects/people. It relies on a communication subsystem which has been developed for real time transmission of MJPEG / MJPEG 2000 compressed video information over erroneous out-of-order packet-based local and wide area networks. The images are at the HDTV standard resolution. The server grabs frames from a camera-link sensor, encodes the raw video, places the encoded bit stream into packets and transmits them over network. The client receives and reorders the packets and then displays the video data. Each user can request different focus windows from the server according to the viewpoint of interest. Each encoded frame is filled with restart markers in order to face up {{the case of a}} frame been lost...|$|R
40|$|During {{the last}} 15 years, {{embedded}} systems {{have grown in}} complexity and performance to rival desktop systems. The architectures of these systems present unique challenges to processor microarchitecture, including instruction encoding and instruction fetch processes. This paper presents new techniques for reducing embedded system code size without reducing functionality. This approach is to extract the pipeline decoder logic for an embedded VLIW processor in software at system development time. The code size reduction is achieved by Huffman compressing or tailor encoding the ISA of the original program. Some interesting results were found. In particular, the degree of compression for the ROM doesn't translate into an improvement in instructions delivered per cycle. Experiments found that when the missprediction penalty of the added Huffman decoder stage was taken into account, a Tailored ISA approach produced higher performance. Methods that <b>compress</b> the entire <b>operation</b> using Huffman [...] ...|$|R
40|$|The {{maintenance}} of large raster images, such as satellite pictures in geographic information systems, {{is still a}} major performance bottleneck. For reasons of storage space, images in a collection are maintained in <b>compressed</b> form. An <b>operation</b> on an image can be performed by first decompressing the compressed version. This, however, can be {{a major source of}} inefficiency, because the entire image needs to be accessed even though a small part of it might suffice to answer the query. We propose to perform spatial queries directly on the compressed version of the image. With current compression algorithms, this cannot be done efficiently. We therefore propose a new compression technique that allows for the subsequent use of a spatial data structure to guide a search. In response to a range query, our algorithm delivers a compressed partial image; this response may be sent over a network as it is, or it may be decompressed for further processing. We have implemented the new algorithm, and w [...] ...|$|R
40|$|Abstract—Multipliers have a {{significant}} impact in the performance of the entire Dsp system. Many high-performance algorithms and architectures have been proposed to accelerate multiplication without increasing the hardware. In previous papers, the truncation error is reduced by adding error compensation circuits. In this paper, truncation error is not more than 1 ULP (unit of least position). So there is no need of error compensation circuits. Truncated multipliers offer significant improvements in area, delay, and power. The modified booth algorithm helps {{to reduce the number of}} partial products to be generated. It is known to be the fastest multiplication algorithm. Partial products bits were <b>compressed</b> by using <b>operations</b> such as deletion, reduction, truncation, rounding and final addition. Thus the resultant multipliers shows a good performance which were used in various high-speed applications. The proposed method reduces the number of full adders and half adders during the tree reduction. The proposed method experimentally shows a reduction of area and power. Keyterms—Truncated multiplier, truncated error, modified booth algorithm, high speed applications, partial products. I...|$|R
40|$|Graduation date: 2001 Air {{compressors}} are {{a significant}} industrial energy user and therefore a prime target for industrial energy audits. The project goals were to develop a software tool, AIRMaster, develop a methodology for performing compressed air system audits, and conduct field audits to refine the methodology and assess savings potential from six common Operation and Maintenance (O&M) measures. AIRMaster and supporting manuals are designed for general auditors or plant personnel to evaluate <b>compressed</b> air system <b>operation</b> with simple instrumentation during a short-term audit. AIRMaster provides a systematic approach to compressed air system audits, analyzing collected data, and reporting results. AIRMaster focuses on inexpensive Operation and Maintenance measures, such as fixing air leaks and improving controls, that can significantly improve performance and reliability of the compressed air system, without significant risk to production. An experienced auditor can perform an audit, analyze collected data, and produce results in 2 - 3 days. AIRMaster reduces {{the cost of an}} audit, thus freeing funds to implement recommendations. AIRMaster proved to be a fast and effective tool. In seven audits AIRMaster identified energy savings of 4, 056, 000 kWh, or 49. 2...|$|R
40|$|We {{answer the}} {{following}} question: Let P and Q be graded posets having some property and let ffi be some poset operation. Is {{it true that}} P ffi Q has also this property ? The considered properties are: being Sperner, a symmetric chain order, Peck, LYM, and rank <b>compressed.</b> The studied <b>operations</b> are: direct product, direct sum, ordinal sum, ordinal product, rankwise direct product, and exponentiation. 1 Introduction and overview Throughout we will consider finite graded partially ordered sets, i. e. finite posets in which every maximal chain has the same length. For such posets P there exists a unique function r : P 7 ! IN (called rank function) and a number m (called rank of P), such that r(x) = 0 (r(x) = m) if x is a minimal (resp., maximal) element of P, and r(y) = r(x) + 1 if y covers x in P (denoted x !Δ y). The set P (i) := fx 2 P : r(x) = ig is called i-th level and its cardinality jP (i) j the i-th Whitney number. If S is a subset of P, let r(S) := P x 2 S r(x), in par [...] ...|$|R
40|$|This state-of-science review {{sets out}} to provide an {{indicative}} assessment of enabling technologies for reducing UK industrial energy demand and carbon emissions to 2050. In the short term, i. e. the period that will rely on current or existing technologies, the road map and priorities are clear. A variety of available technologies will lead to energy demand reduction in industrial processes, boiler <b>operation,</b> <b>compressed</b> air usage, electric motor efficiency, heating and lighting, and ancillary uses such as transport. The prospects for the commercial exploitation of innovative technologies {{by the middle of}} the 21 st century are more speculative. Emphasis is therefore placed on the range of technology assessment methods that are likely to provide policy makers with a guide to progress in the development of high-temperature processes, improved materials, process integration and intensification, and improved industrial process control and monitoring. Key among the appraisal methods applicable to the energy sector is thermodynamic analysis, making use of energy, exergy and 'exergoeconomic' techniques. Technical and economic barriers will limit the improvement potential to perhaps a 30 % cut in industrial energy use, which would make a significant contribution to reducing energy demand and carbon emissions in UK industry. Non-technological drivers for, and barriers to, the take-up of innovative, low-carbon energy technologies for industry are also outlined. Industrial processes R&D challenges Technology assessment methods...|$|R
40|$|The second-generation University of Colorado closed-path tunable-diode laser {{hygrometer}} (CLH- 2) is {{an instrument}} for the airborne in situ measurement of total water content – the sum of vapor-, liquid- and ice-phase water – in clouds. This compact instrument has been flown on the NSF/NCAR Gulfstream-V aircraft in an underwing canister. It operates autonomously and uses fiber-coupled optics to {{eliminate the need for}} a supply of dry <b>compressed</b> gas. In <b>operation,</b> sample air is ingested into a forward-facing sub-isokinetic inlet; this sampling configuration results in particle concentrations that are enhanced relative to ambient and causes greater instrument sensitivity to condensed water particles. Heaters within the inlet vaporize the ingested water particles, and the resulting augmented water vapor mixing ratio is measured by absorption of near-infrared light in a single-pass optical cell. The condensed water content is then determined by subtracting the ambient water vapor content from the total and by accounting for the inertial enhancement of particles into the sampling inlet. The CLH- 2 is calibrated in the laboratory over a range of pressures and water vapor mixing ratios; the uncertainty in CLH- 2 condensed water retrievals is estimated to be 14. 3 % to 16. 1 % (1 -σ). A vapor-only laboratory intercomparison with the first-generation University of Colorado closed-path tunable-diode laser hygrometer (CLH) shows agreement within the 2 -σ uncertainty bounds of both instruments...|$|R
40|$|Thesis (MIng (Mechanical Engineering)) [...] North-West University, Potchefstroom Campus, 2013 Various {{commercial}} {{software packages}} {{are available for}} simulating <b>compressed</b> air network <b>operations.</b> However, none of these software packages are able to dynamically prioritise compressor selection on large compressed air networks in the mining industry. In this dissertation, a dynamic compressor selector (DCS) will be developed that will actively and continuously monitor system demand. The software will ensure that the most suitable compressors, based on efficiency and position in the compressed air network, are always in operation. The study will be conducted at a platinum mine. Compressed air flow and pressure requirements will be maintained without compromising mine safety procedures. Significant energy savings will be realised. DCS will receive shaft pressure profiles {{from each of the}} shafts’ surface compressed air control valves. These parameters will be used to calculate and predict the compressed air demand. All pipe friction losses and leaks will be taken into account to determine the end-point pressure losses at different flow rates. DCS will then prioritise the compressors of the compressed air network based on the overall system requirement. This software combines the benefits of supply-side and demand-side management. Potential energy savings with DCS were proven and compressor cycling reduced. A DCS user-friendly interface was created to easily set up any mine’s compressed air network. Master...|$|R
40|$|An amine-based {{carbon dioxide}} (CO 2) and water vapor sorbent in pressure-swing regenerable beds has been {{developed}} by Hamilton Sundstrand and baselined for the Orion Atmosphere Revitalization System (ARS). In three previous years at this conference, reports were presented on extensive Johnson Space Center (JSC) testing of this technology in a sea-level pressure environment with simulated and real human metabolic loads in both open and closed-loop configurations. The test article design was iterated a third time before the latest series of such tests, which was performed {{in the first half}} of 2009. The new design incorporates a canister configuration modification for overall unit compactness and reduced pressure drop, as well as a new process flow control valve that incorporates both compressed gas purge and dual-end vacuum desorption capabilities. This newest test article is very similar to the flight article designs. Baseline tests of the new unit were performed to compare its performance to that of the previous test articles. Testing of <b>compressed</b> gas purge <b>operations</b> helped refine launchpad operating condition recommendations developed in earlier testing. Operating conditions used in flight program computer models were tested to validate the model projections. Specific operating conditions that were recommended by the JSC test team based on past test results were also tested for validation. The effects of vacuum regeneration line pressure on resulting cabin conditions was studied for high metabolic load periods, and a maximum pressure is recommended...|$|R
40|$|First, {{consider}} the Independent Auditor. Plant management should consider using an independent auditor without ties to equipment sales who has {{experience with a}} variety of manufacturing facilities and can provide strong references documenting proven energy savings. The quality and comprehensiveness of manufacturing facilities, and therefore audits, vary widely and {{it is important to consider}} experience. Compressed Air Technologies has audited nearly 200 manufacturing facilities since our inception in 1996. Compressed Air Technologies is the preferred air energy services provider for Georgia Pacific Corporation, INVISTA Chemicals and a primary energy partner with KRAFT Foods, among others. Compressed Air Technologies has no OEM affiliation of any kind and will provide recommendations that are system-neutral and commercially impartial. Compressed Air Technologies (CAT) is an international energy reduction and system optimization services corporation. CAT is a totally independent business structure with no ties or affiliations with any air compressor manufacturers -we are free of OEM restrictions. Our strict adherence to an independent approach related to compressed air and gas system assessments allows for unbiased commentary and recommendations, regardless of the brand or equipment type currently utilized in your plant. The majority of our competitors have either 1) equipment to sell and/or, 2) existing equipment on the site to protect. In both situations, the interests of the client is not the paramount issue. Independent auditors should have no obvious or hidden agenda. CAT compressed air audit services are in essence, holistic. The analysis of your facility compressed air and gases systems is all-inclusive and consists of an engineered assessment beginning at the air compressor equipment area inside your facility and finishing at point-of-use consumption points. CAT engineers utilize a variety of instruments to collect data from all critical, relevant areas of the compressed air and gas systems. From air compressor blow off loss to the cost of the always present distribution/ piping system leaks, each individual loss or inefficiency is measured, analyzed, quantified and catalogued. CAT engineers then compile all of the collected data to create a total current profile report of your facility operations including an assigned cost to each discovered inefficiency. A typical audit results in identification of 25 to 60 percent or more in total system operational waste, which includes energy costs, maintenance costs, and all matters related to operating your compressed gas systems. It is not unusual to discover half of what you pay for <b>compressed</b> air <b>operations</b> is wasted due to inefficient utilization and operation of this important utility...|$|R
40|$|New {{computational}} {{procedures are}} proposed {{for evaluating the}} exhaust brake specific mass emissions of each pollutant species in internal combustion (IC) engines. The procedures start from the chemical reaction of fuel with combustion air and, {{based on the measured}} exhaust raw emissions THC, CH 4, NOx, CO, O 2, CO 2, calculate the volume fractions of the compounds in the exhaust gases, including those that are not usually measured, such as water, nitrogen and hydrogen. The molecular mass of the exhaust gases is then evaluated and the brake specific emissions can be obtained if the exhaust flow rate and the engine power output are measured. The algorithm can also be applied to the evaluation of air/fuel ratio from measured raw volume emissions of IC engines. The new procedures take the effects of various fuel and combustion air compositions into account, with particular reference to different natural gas blends {{as well as to the}} presence of water vapor, CO 2, Ar and He in the combustion air. In the paper, the algorithms are applied to the evaluation of air-fuel ratio and brake specific mass emissions in an automotive bi-fuel Spark Ignition (SI) engine with multipoint sequential port-fuel injection. The experimental tests were carried out in a wide range of steady-state operating conditions under both gasoline and <b>compressed</b> natural gas <b>operations.</b> The specific emissions calculated from the new procedures are compared to those evaluated by applying Society of Automotive Engineers (SAE) and International Standards Organization (ISO) recommended practices and the air-fuel ratio results are compared to those obtained either from directly measured air and fuel mass flow rates or from Universal Exhaust Gas Oxygen (UEGO) sensor data. The sensitivity of the procedure results to the main engine working parameters, the influence of environmental conditions (in particular the effect of air humidity on NOx formation) and the experimental uncertainties are also determined...|$|R
40|$|Crystallization is {{a widely}} used unit {{operation}} in the pharmaceutical and chemical industries. In {{the last three decades}} in the pharmaceutical industries this has led to a new direction for directly crystallizing Active Pharmaceutical Ingredients (APIs) to have a suitable product size and this is the focus of this work. The model drug used in this study was racemic ibuprofen, commonly known as “ibuprofen” which is a very popular analgesic. Ibuprofen was also chosen as little crystallization kinetics information was available in the published literature and there is potential industrial interest in crystallizing this material. Ibuprofen is mostly supplied in tablet form. However, commercially available ibuprofen particles are needle like with rough surfaces and show poor flowability, poor compaction behavior and a tendency to stick to the tablet punches. To overcome these problems, a suitable size and shape of ibuprofen crystal is desirable that could be directly <b>compressed</b> with fewer <b>operation</b> steps (usually from 6 down to 2) but still having good product stability and therapeutic efficacy. A controlled crystallization process would be useful in producing such particles. This project was aimed to develop a controlled crystallization process for ibuprofen (I) crystallized from absolute ethanol (E) or water - ethanol mixtures (W+E) to explore conditions favorable for industrial crystallization and to gain knowledge of its solubility, metastability, nucleation and crystal growth kinetic data that is essential in the design of industrial crystallizers. The experimental data of this study were all obtained at conditions in line with expected industrial practice which includes temperatures between 10 ºC to 40 ºC. Solubility measurement data shows that the ibuprofen solubility in ethanol increases significantly with temperature from 0. 59 g/g at 10 ºC to 2. 14 g/g at 40 ºC (as mass ratio unit I/E). The data was fitted by a square law relation, I*/E = 0. 497 (± 0. 008) + 0. 001026 (± 0. 000012) T 2, where T is temperature in ºC, I*/E is the solubility mass ratio of ibuprofen to ethanol in solution and the values in brackets are the 95 % confidence uncertainties on the parameters. For aqueous ethanol, adding water initially slightly increases the solubility at 25 and 40 ºC, but then it decreases substantially. At 40 ºC solutions between 34 % to 64 % mass fraction water in solvent W/(W+E) separate into two liquid phases. The upper layer, with the lower density, contains the higher ibuprofen and ethanol contents. The metastable zone width (MSZW) in alcoholic solutions is quite narrow. For ethanol solutions, the 1 hour secondary nucleation threshold MSZW, measured as relative supersaturations, range from 0. 03 to 0. 08. For water - ethanol mixtures it ranges from 0. 001 to 0. 02. The primary nucleation thresholds MSZW are approximately six times greater. These nucleation thresholds were used to select metastable zone operating conditions for the nucleation and crystal growth rate experiments of this thesis. To develop a continuous crystallizer, nucleation kinetics data are necessary. Measurements were made of the secondary nucleation rates for ibuprofen in ethanol and water - ethanol mixtures using a batch technique. The nucleation rate appears to be first order with respect to supersaturation and to be independent of temperature with a nucleation rate constant, kB = 1. 73 × 108 (± 65 %) #/min/(kg slurry) /(unit of ∆(I/E)). It appears to fall with increasing water content in aqueous solutions. Further work is needed in this area. Seeded batch experiments were used to measure ibuprofen crystal growth rates at similar range of conditions. Ibuprofen shows growth rate dispersion (GRD), so size proportional growth (SPG) seeds having a coefficient of variation by volume CVV of 0. 50 (± 0. 03), prepared from a batch nucleation, were used. For the effect of supersaturation s, the data was fitted with a power law expression, G = kG sn. It was found that growth rates were first order, i. e. n = 1. For n = 1, the best fit kG value is 15 (± 10 %) μm/min/unit of (I/E) at 25 ºC for crystals having the volume median size. The error is the 95 % uncertainty. kG increases with temperature with an activation energy of 13. 7 (± 18 %) kJ/mole. Thus growth appears to be kinetics controlled rather than mass transfer controlled. In the chosen supersaturation units, kG increases with increasing water content in aqueous mixtures. Combining the effects gave the correlation for kG = 5. 3 (± 3. 1) *exp{ 0. 024 (± 0. 015) *T}* exp[{ 7. 2 (± 2. 0) – 0. 21 (± 0. 1) *T} *XW]. where T is the temperature in °C, XW is the mass fraction of water to ethanol in solution and the values in brackets are the 95 % confidence uncertainties on the parameters. The shape of the crystals from all solutions were plate like and were of good size. Sample calculations are provided to illustrate how the data obtained can be used in the process design of crystallizers to produce crystals of the desired size...|$|R
40|$|The {{operation}} of supply chains (SCs) has {{for many years}} been focused on efficiency, leanness and responsiveness. This has resulted in reduced slack in <b>operations,</b> <b>compressed</b> cycle times, increased productivity and minimised inventory levels along the SC. Combined with tight tolerance settings for the realisation of logistics and production processes, {{this has led to}} SC performances that are frequently not robust. SCs are becoming increasingly vulnerable to disturbances, which can decrease the competitive power of the entire chain in the market. Moreover, in the case of food SCs non-robust performances may ultimately result in empty shelves in grocery stores and supermarkets. The overall objective of this research is to contribute to Supply Chain Management (SCM) theory by developing a structured approach to assess SC vulnerability, so that robust performances of food SCs can be assured. We also aim to help companies in the food industry to evaluate their current state of vulnerability, and to improve their performance robustness through a better understanding of vulnerability issues. The following research questions (RQs) stem from these objectives: RQ 1 : What are the main research challenges related to (food) SC robustness? RQ 2 : What are the main elements that have to be considered in the design of robust SCs and what are the relationships between these elements? RQ 3 : What is the relationship between the contextual factors of food SCs and the use of disturbance management principles? RQ 4 : How to systematically assess the impact of disturbances in (food) SC processes on the robustness of (food) SC performances? To answer these RQs we used different methodologies, both qualitative and quantitative. For each question, we conducted a literature survey to identify gaps in existing research and define {{the state of the art}} of knowledge on the related topics. For the second and third RQ, we conducted both exploration and testing on selected case studies. Finally, to obtain more detailed answers to the fourth question, we used simulation modelling and scenario analysis for vulnerability assessment. Main findings are summarised as follows. Based on an extensive literature review, we answered RQ 1. The main research challenges were related to the need to define SC robustness more precisely, to identify and classify disturbances and their causes in the context of the specific characteristics of SCs and to make a systematic overview of (re) design strategies that may improve SC robustness. Also, we found that it is useful to be able to discriminate between varying degrees of SC vulnerability and to find a measure that quantifies the extent to which a company or SC shows robust performances when exposed to disturbances. To address RQ 2, we define SC robustness as the degree to which a SC shows an acceptable performance in (each of) its Key Performance Indicators (KPIs) during and after an unexpected event that caused a disturbance in one or more logistics processes. Based on the SCM literature we identified the main elements needed to achieve robust performances and structured them together to form a conceptual framework for the design of robust SCs. We then explained the logic of the framework and elaborate on each of its main elements: the SC scenario, SC disturbances, SC performance, sources of food SC vulnerability, and redesign principles and strategies. Based on three case studies, we answered RQ 3. Our major findings show that the contextual factors have a consistent relationship to Disturbance Management Principles (DMPs). The product and SC environment characteristics are contextual factors that are hard to change and these characteristics initiate the use of specific DMPs as well as constrain the use of potential response actions. The process and the SC network characteristics are contextual factors that are easier to change, and they are affected by the use of the DMPs. We also found a notable relationship between the type of DMP likely to be used and the particular combination of contextual factors present in the observed SC. To address RQ 4, we presented a new method for vulnerability assessments, the VULA method. The VULA method helps to identify how much a company is underperforming on a specific Key Performance Indicator (KPI) in the case of a disturbance, how often this would happen and how long it would last. It ultimately informs the decision maker about whether process redesign is needed and what kind of redesign strategies should be used in order to increase the SC’s robustness. The VULA method is demonstrated in the context of a meat SC using discrete-event simulation. The case findings show that performance robustness can be assessed for any KPI using the VULA method. To sum-up the project, all findings were incorporated within an integrated framework for designing robust SCs. The integrated framework consists of the following steps: 1) Description of the SC scenario and identification of its specific contextual factors; 2) Identification of disturbances that may affect KPIs; 3) Definition of the relevant KPIs and identification of the main disturbances through assessment of the SC performance robustness (i. e. application of the VULA method); 4) Identification of the sources of vulnerability that may (strongly) affect the robustness of performances and eventually increase the vulnerability of the SC; 5) Identification of appropriate preventive or disturbance impact reductive redesign strategies; 6) Alteration of SC scenario elements as required by the selected redesign strategies and repeat VULA method for KPIs, as defined in Step 3. Contributions of this research are listed as follows. First, we have identified emerging research areas - SC robustness, and its counterpart, vulnerability. Second, we have developed a definition of SC robustness, operationalized it, and identified and structured the relevant elements for the design of robust SCs in the form of a research framework. With this research framework, we contribute to a better understanding of the concepts of vulnerability and robustness and related issues in food SCs. Third, we identified the relationship between contextual factors of food SCs and specific DMPs used to maintain robust SC performances: characteristics of the product and the SC environment influence the selection and use of DMPs; processes and SC networks are influenced by DMPs. Fourth, we developed specific metrics for vulnerability assessments, which serve as a basis of a VULA method. The VULA method investigates different measures of the variability of both the duration of impacts from disturbances and the fluctuations in their magnitude. With this project, we also hope to have delivered practical insights into food SC vulnerability. First, the integrated framework for the design of robust SCs can be used to guide food companies in successful disturbance management. Second, empirical findings from case studies lead to the identification of changeable characteristics of SCs that can serve as a basis for assessing where to focus efforts to manage disturbances. Third, the VULA method can help top management to get more reliable information about the “health” of the company. The two most important research opportunities are: First, there is a need to extend and validate our findings related to the research framework and contextual factors through further case studies related to other types of (food) products and other types of SCs. Second, there is a need to further develop and test the VULA method, e. g. : to use other indicators and statistical measures for disturbance detection and SC improvement; to define the most appropriate KPI to represent the robustness of a complete SC. We hope this thesis invites other researchers to pick up these challenges and help us further improve the robustness of (food) SCs.  ...|$|R
