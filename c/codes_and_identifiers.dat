1|10000|Public
40|$|Donald Chvatal is President of Ringgold, Inc. Ringgold {{develops}} {{and maintains}} Identify as a registry of institutional identifiers and webservice for publishers. Our {{mission is to}} classify institutional subscribers and organize metadata about them, thereby helping publishers to sell their products more effectively. Ringgold is working with publishers, agents, and libraries to explore using Identify to improve journal supply chain efficiencies. Ringgold also maintains OpenRFP, an e-Procurement toolkit for suppliers to educate librarians about products and for libraries to make better-informed purchase decisions. Current <b>codes</b> <b>and</b> <b>identifiers</b> for institutions (e. g., ISIL, SAN, DOI, MARC) are inadequate for providing metadata and linkage between institutional components of the journal supply chain. Ringgold, Inc. is collaborating with publishers, agents, aggregators, libraries, to remedy this as an independent and trusted agency to provide identifiers {{that can be used}} for the successful transmission of information between organizations to expedite the delivery of e-content to subscribers. The presentation summarizes the work of a pilot project involving UK subscribers (including British librararies), reporting on how the identifier is working there, its effects on publishers and suppliers, and its evolution to expanding coverage to institutional subscribers in North America...|$|E
5000|$|... #Subtitle level 2: Short <b>codes</b> <b>and</b> service <b>identifiers</b> (prefix) ...|$|R
50|$|LOINC applies {{universal}} <b>code</b> names <b>and</b> <b>identifiers</b> {{to medical}} terminology related to electronic health records. The {{purpose is to}} assist in the electronic exchange and gathering of clinical results (such as laboratory tests, clinical observations, outcomes management and research). LOINC has two main parts: laboratory LOINC and clinical LOINC. Clinical LOINC contains a subdomain of Document Ontology which captures types of clinical reports and documents.|$|R
40|$|Abstract- Nowadays, {{many papers}} are {{developing}} {{to improve the}} software quality control. In our paper {{we are going to}} help the developers to maintain the source <b>code</b> <b>and</b> <b>identifiers</b> <b>and</b> we will show the textual similarity between source <b>code</b> <b>and</b> related high level faults. The developers are improving the source code library. So, if the software development environment provides similarities between the source <b>code</b> <b>and</b> the high level problems then it will be quite easier for the developers to keep the software quality ahead. In our proposing system the candidate identifiers needs to implement in eclipse IDE for that we need a plug-in called COde COmprehension Nurturant Using Traceability (COCONUT). This paper also reports on two controlled experiments performed with master’s and bachelor’s students. The quality of identifiers, comments in the produced source code with or without coconut. The approach presented in this paper relates to approaches aimed at applying IR techniques for traceability recovery and for quality improvement/assessment. So that quality of the source code lexicon will be improved. Thus the usefulness of the coconut is taken as a feature of software development environments...|$|R
50|$|Both the Universal Product <b>Code</b> <b>and</b> EAN-13 <b>{{identifier}}s</b> {{that are}} still found on many trade items can be mapped into a 14-digit GTIN identifier, by padding to the left with zero digits to reach a total of 14 digits. An SGTIN EPC identifier can therefore be constructed by combining the resulting GTIN with a unique serial number and following the encoding rules in the EPCglobal Tag Data Standard.|$|R
40|$|Summary: A {{relational}} database {{has been developed}} {{based on the results}} from the application of the DynDom program to a number of proteins for which multiple X-ray conformers are available. The database is populated via aweb-based tool that allows visitors to the website to run the DynDom program server-side by selecting pairs of X-ray conformers by Protein Data Bank <b>code</b> <b>and</b> chain <b>identifier.</b> Availability: The website can be found at...|$|R
5000|$|AMD {{refers to}} it as Family 10h Processors, {{as it is the}} {{successor}} of the Family 0Fh Processors (codename K8). 10h and 0Fh refer to the main result of the CPUID x86 processor instruction. In hexadecimal numbering, 0Fh (h represents hexadecimal numbering) equals the decimal number 15, and 10h equals decimal 16. (The [...] "K10h" [...] form that sometimes pops up is an improper hybrid of the [...] "K" [...] <b>code</b> <b>and</b> Family <b>identifier</b> number.) ...|$|R
40|$|This study aims to {{investigate}} the change of developed land in three different locations along Highway 4 Road from Phattalung to HatYai. The method involves creating a digitized grid of geographical coordinates covering the study area. The land-use <b>codes</b> <b>and</b> plot <b>identifiers</b> were recorded in database tables indexed by grid coordinates. Logistic regression of land development adjusted for spatial correlation was used to model its change over a 9 -year period using land-use at the previous survey combined with location as a determinant. The results show increasing average percentages of developed land (3...|$|R
5000|$|AGESA {{documentation}} {{was previously}} {{available only to}} AMD partners that had signed a non-disclosure agreement (NDA). A form of AGESA source code scrubbed of [...] "proprietary <b>code,</b> <b>identifiers</b> <b>and</b> concepts" [...] was open-sourced in early 2011 to gain track in coreboot, but these releases were stopped in 2014.|$|R
40|$|This study aims {{to analyze}} land-use change by a digitized-grid method, a simple {{technique}} {{that can be}} used for such analysis. We describe a procedure for restructuring land-use data comprising polygonal “shape files” containing successive (x, y) boundary points of plots for geographic land-use categories as grid-digitized data, and illustrate this method using data from Thailand. The new data comprise a rectangular grid of geographical coordinates with land-use <b>codes</b> <b>and</b> plot <b>identifiers</b> as fields in database tables indexed by the grid coordinates. Having such a database overcomes difficulties land-use researchers face when querying, analyzing and forecasting land-use change...|$|R
40|$|Tool {{support for}} {{refactoring}} code written in mainstream languages such as C and C is currently lacking {{due to the}} complexity introduced by the mandatory preprocessing phase that forms part of the C/C compilation cycle. The defintion and use of macros complicates the notions of scope <b>and</b> of <b>identifier</b> boundaries. The concept of token equivalence classes {{can be used to}} bridge the gap between the language proper semantic analysis and the nonpreprocessed source code. The CScout toolchest uses the developed theory to analyze large interdependent program families. A Web-based interactive front end allows the precise realization of rename and remove refactorings on the original C source code. In addition, CScout can convert programs into a portable obfuscated format or store a complete and accurate representation of the <b>code</b> <b>and</b> its <b>identifiers</b> in a relational database...|$|R
5000|$|It is {{perceived}} by the PC community that after {{the use of the}} codename K8 for the Athlon 64 processor family, AMD no longer uses K-nomenclatures (which originally stood for Kryptonite) since no K-nomenclature naming convention beyond K8 has appeared in official AMD documents and press releases after the beginning of 2005. AMD now refers to the codename K8 processors as the Family 0Fh processors. 10h and 0Fh refer to the main result of the CPUID x86 processor instruction. In hexadecimal numbering, 0F(h) (where the h represents hexadecimal numbering) equals the decimal number 15, and 10(h) equals the decimal number 16. (The [...] "K10h" [...] form that sometimes pops up is an improper hybrid of the [...] "K" [...] <b>code</b> <b>and</b> Family <b>identifier</b> number.) ...|$|R
40|$|The {{main purpose}} of this study is {{presenting}} a conceptual model to integrate Iranian digital libraries. This research had been done in three steps. Documentary research method is used in the first step. All the related documents were made research population and content analysis techniques is used to analyze documents. Analytical survey research method is used in the second step of the research. All the active Iranian digital libraries were made the research population in this step. In fact there are 32 active digital libraries that just 26 of them participated in this study. Researcher-made questionnaire is used to collect data in this step. Finally system analyzing research method is used in the third step. Findings of the study demonstrated that elements such as technological infrastructure, <b>Coding</b> <b>and</b> <b>identifier</b> systems, Metadata standards, Legal and organizational issues, Semantic knowledge-based databases, and Knowledge Organizing Systems are needed in semantic integration. Also methods such as translators and making semantic relations can be used in semantic integration. Moreover tools such as ontology, knowledge organizing system, knowledgebase databases, translators, and descriptive and analytical languages should be used in semantic integration. Other findings related to Question 2 of the study showed that the studied digital libraries are not in suitable situation in covering digital materials and filling metadata fields. Also documenting metadata fields is in not good mood. Therefore there is need to strengthen all the identified weakness in order to make semantic integration ability. Finally proposed model of this study presented in three layers entitled data, inference machine, and application...|$|R
40|$|This paper {{presents}} {{an approach that}} helps developers to maintain source <b>code</b> <b>identifiers</b> <b>and</b> comments dependable with high-level artifact. This approach calculates and shows the textual similarity source <b>code</b> <b>and</b> related artifacts. The assumption is developers are induced to improve the source code lexicon (terms) used in identifiers or comments. The software development environment provides information about the textual similarity between the source <b>code</b> under development <b>and</b> the related high-level artifacts. Proposed approach recommends candidate identifiers build from high-artifacts related to the source code under development. The goal of the experiments is to estimate the similarity of the source and high level artifacts...|$|R
40|$|AbstractWe present two {{randomized}} algorithms, one for {{message passing}} {{and the other}} for shared memory, that, with probability~ 1, schedule multiparty interactions in a strongly fair manner. Both algorithms improve upon a previous result by Joung and Smolka (proposed in a shared-memory model, along with a straightforward conversion to the message-passing paradigm) in the following aspects: first, processes’ speeds as well as communication delays need not be bounded by any predetermined constant. Secondly, our algorithms are completely decentralized, and the shared-memory solution makes use of only single-writer variables. Finally, both algorithms are symmetric in the sense that all processes execute the same <b>code,</b> <b>and</b> no unique <b>identifier</b> is used to distinguish processes...|$|R
40|$|The paper {{presents}} an approach helping developers to maintain source <b>code</b> <b>identifiers</b> <b>and</b> comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source <b>code</b> <b>and</b> related high-level artifacts. Our conjecture is that developers are induced {{to improve the}} source code lexicon, i. e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source <b>code</b> under development <b>and</b> the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source <b>code</b> under development <b>and</b> has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master’s and bachelor’s students. The goal of the experiments is to evaluate the quality of <b>identifiers</b> <b>and</b> comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between <b>code</b> <b>and</b> high-level artifacts helps {{to improve the quality}} of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments...|$|R
40|$|The {{objective}} of this work was to outline and to characterize the sub-basins of the Alto Rio Jamanxim hydrographic basin <b>and</b> to <b>code</b> them using the concept of “otto-basins”, {{as well as to}} quantify the deforestation area and the increase in roads extension in its drainage area from 1999 to 2005. The work was based on data of river’s and road’s network, digital elevation model (DEM), and four TM/Landsat- 5 images from 1999 and 2005. The outline of the boundaries and the hydrologic parameters of the Basin were based on a 1 : 250, 000 DEM. Results have shown that nine sub-basins have been outlined in the study area, coded up to level 5 (otto-basin <b>code</b> system) <b>and</b> <b>identifiers</b> varied from 44291 to 44299. Deforestation by human activities totaled 635 km 2 (11 % of the Basin) up to 1999 and to 1, 257 km 2 up to 2005, therefore, an increase of 622 km 2 or 98 % between these two dates. Regarding the road network, a total extension of 1, 685 km was mapped up to 1999 and 3, 638 km up to July 2005 what corresponds to an increase of 116 % in road construction inside the basin boundaries in six years...|$|R
2500|$|In day-to-day {{business}} it happens that containers do appear {{which do not}} follow the ISO 6346 identification at all; however, they are fully CSC safety approved containers. Usually these are [...] "shippers owned" [...] containers, {{which means that they}} are not owned by the carrier but supplied by the cargo owners (shippers). They may have no registered owner <b>code</b> <b>and</b> no category <b>identifier</b> <b>and</b> have no check digit. It is advisable to follow ISO 6346 as the absence of a compliant identification code causes problems for both carriers and container terminals to correctly identify the equipment and properly deliver the cargo, because computer systems require ISO 6346 conformant naming and as such missing prefixes are invented. For example, YYYY at the carrier and XXXX at the terminal causes the equipment to mismatch. Moreover, since ISO 6346 identification has become a requirement in international Customs conventions (Customs Conventions on Containers and Istanbul Convention), many Customs Administrations have begun validating that containers are marked as per the standard.|$|R
40|$|There is {{a growing}} {{interest}} in creating tools that can assist engineers in all phases of the software life cycle. This assistance requires techniques that go beyond traditional static and dynamic analysis. An {{example of such a}} technique is the application of information retrieval (IR), which exploits information found in a project’s natural language. Such information can be extracted from the source <b>code’s</b> <b>identifiers</b> <b>and</b> comments and in artifacts associated with the project, such as the requirements. The techniques described pertain to the maintenance and evolution phase of the software life cycle and focus on problems such as feature location and impact analysis. These techniques highlight the bright future that IR brings to addressing software engineering problems...|$|R
40|$|Code clones are {{duplicate}} {{fragments of}} code that perform the same task. As software code bases increase in size, {{the number of}} code clones also tends to increase. These code clones, possibly created through copy-and-paste methods or unintentional duplication of effort, increase maintenance cost over the lifespan of the software. Code clone detection tools exist to identify clones where a human search would prove unfeasible, however {{the quality of the}} clones found may vary. I demonstrate that the performance of such tools can be improved by normalizing the source code before usage. I developed Normalizer, a tool to transform C source code to normalized source code where the code is written as consistently as possible. By maintaining the code 2 ̆ 7 s function while enforcing a strict format, the variability of the programmer 2 ̆ 7 s style will be taken out. Thus, code clones may be easier to detect by tools regardless of how it was written. Reordering statements, removing useless <b>code,</b> <b>and</b> renaming <b>identifiers</b> are used to achieve normalized code. Normalizer was used to show that more clones can be found in Introduction to Computer Networks assignments by normalizing the source code versus the original source code using a small variety of code clone detection tools...|$|R
40|$|As {{software}} systems {{continue to}} grow <b>and</b> evolve, locating <b>code</b> for maintenance <b>and</b> reuse tasks becomes increasingly difficult. Existing static code search techniques using natural language queries provide little support to help developers determine whether search results are relevant, and few recommend alternative words to help developers reformulate poor queries. In this paper, we present a novel approach that automatically extracts natural language phrases from source <b>code</b> <b>identifiers</b> <b>and</b> categorizes the phrases and search results in a hierarchy. Our contextual search approach allows developers to explore the word usage {{in a piece of}} software, helping them to quickly identify relevant program elements for investigation or to quickly recognize alternative words for query reformulation. An empirical evaluation of 22 developers reveals that our contextual search approach significantly outperforms the most closely related technique in terms of effort and effectiveness. 1...|$|R
40|$|We present two {{randomized}} algorithms, one for {{message passing}} {{and the other}} for shared memory, that, with probability 1, schedule multiparty interactions in a strongly fair manner. Both algorithms improve upon a previous result by Joung and Smolka (proposed in a shared-memory model, along with a straightforward conversion to the message-passing paradigm) in the following aspects: First, processes' speeds as well as communication delays need not be bounded by any predetermined constant. Secondly, our algorithms are completely decentralized, and the sharedmemory solution makes use of only single-writer variables. Finally, both algorithms are symmetric in the sense that all processes execute the same <b>code,</b> <b>and</b> no unique <b>identifiers</b> are used to distinguish processes. 1 Introduction Since Hoare introduced CSP [13], interactions and nondeterminism have become two fundamental features in many programming languages for distributed computing (e. g., Ada [34], Script [11], Action Systems [3], IP [...] ...|$|R
40|$|After {{discussion}} with the University Registrar and representatives from that office, the LSU Olinde Career Center would like to propose a new designation for internship courses of courses for internships. Doing so will allow the Office of the University Registrar to tag internship courses more appropriately within the course database. The intentional byproduct of this action will be to better field requests from the Department of Labor concerning how many and where LSU students are participating in internships for credit. Additionally, similar requests are also common from SACS for accreditation as well as U. S. News & World Report for college surveys. The purpose of this memo is to only request the method in which courses are listed, not {{to change the way}} the courses themselves are conducted. This process will first begin with identifying courses similar in nature to an internship course. Consent will be requested from the Department Chair to better identify courses and to continue to code future courses as such. Looking forward, we would like to encourage that courses of this type will include the word "internship" in the title. This new <b>code</b> <b>and</b> course <b>identifier</b> will increase visibility to students who are selecting courses thereby making those who participate in such courses more employable upon graduation. Furthermore, potential employers who receive transcripts by request from students will be able t...|$|R
40|$|Abstract—We {{present a}} generalizable formal model of {{software}} readability {{based on a}} human study of 5000 participants. Readability is fundamental to maintenance, but remains poorly understood. Previous models focused on symbol counts of small code snippets. By contrast, we approach code as read on screens by humans and propose to analyze visual, spatial and linguistic features, including structural patterns, sizes of <b>code</b> blocks, <b>and</b> verbal <b>identifier</b> content. We construct a readability metric based on these notions and show that it agrees with human judgments {{as well as they}} agree with each other and better than previous work. We identify universal features of readability and languageor experience-specific ones. Our metric also correlates with an external notion of defect density. We address multiple programming languages and different length samples, and evaluate using an order of magnitude more participants than previous work, all suggesting our model is more likely to generalize. I...|$|R
50|$|Floyd Bennett Field was New York City's first {{municipal}} airport, later a {{naval air}} station, {{and is now}} a park. While no longer used as an operational commercial, military or general aviation airfield, a section is still used as a helicopter base by the New York City Police Department (NYPD). Located in Marine Park, southeast Brooklyn, the field was created by connecting Barren Island and a number of smaller marsh islands to the mainland by filling the channels between them with sand pumped from the bottom of Jamaica Bay. The airport was named after famed aviator and Medal of Honor recipient Floyd Bennett, a Brooklyn resident {{at the time of his}} death. It was dedicated on June 26, 1930, and officially opened on May 23, 1931. The IATA airport <b>code</b> <b>and</b> FAA airfield <b>identifier</b> <b>code</b> was NOP when it was an operational naval air station and later coast guard air station, but now uses the FAA Location Identifier NY22 for the heliport operated there by the NYPD.|$|R
5000|$|Mapping {{from other}} Standard Metadata <b>and</b> <b>Identifiers</b> to EIDR: Other {{metadata}} <b>and</b> <b>identifier</b> {{systems can be}} directly mapped into EIDR: ...|$|R
40|$|Abstract—Comments <b>and</b> <b>identifiers</b> are {{the main}} source of {{documentation}} of source-code and are therefore {{an integral part of the}} development and the maintenance of a program. As English is the world language, most comments <b>and</b> <b>identifiers</b> are written in English. However, if they are in any other language, a developer without knowledge of this language will almost perceive the code to be undocumented or even obfuscated. In absence of industrial data, academia is not aware of the extent of the problem of non-English comments <b>and</b> <b>identifiers</b> in practice. In this paper, we propose an approach for the language identification of source-code comments <b>and</b> <b>identifiers.</b> With the approach, a large-scale study has been conducted of the natural language of source-code comments <b>and</b> <b>identifiers,</b> analyzing multiple open-source and industry systems. The results show that a significant amount of the industry projects contain comments <b>and</b> <b>identifiers</b> in more than one language, whereas none of the analyzed open-source systems has this problem. I...|$|R
50|$|There {{are several}} {{advantages}} to decoupling Location <b>and</b> <b>Identifier,</b> <b>and</b> to LISP specifically.|$|R
50|$|The VOR encodes azimuth (direction {{from the}} station) as the phase {{relationship}} between a reference signal and a variable signal. The omnidirectional signal contains a modulated continuous wave (MCW) 7 wpm Morse <b>code</b> station <b>identifier,</b> <b>and</b> usually contains an amplitude modulated (AM) voice channel. The conventional 30 Hz reference signal is frequency modulated (FM) on a 9,960 Hz subcarrier. The variable amplitude modulated (AM) signal is conventionally derived from the lighthouse-like rotation of a directional antenna array 30 times per second. Although older antennas were mechanically rotated, current installations scan electronically to achieve an equivalent result with no moving parts. This is achieved by a circular array of typically 60 directional antennas, the signal to each one being amplitude modulated by the 30 Hz reference signal delayed in phase to match the azimuthal position of each individual antenna. When the composite signal is received in the aircraft, the AM and FM 30 Hz components are detected and then compared to determine the phase angle between them.|$|R
40|$|Due to the {{progressive}} nature of preventable vision loss, annual examinations {{are necessary to}} address early stages of diseases. While {{studies have focused on}} risk factors leading to preventable vision loss, little work has been done to understand prevalence of vision difficulty in regard to availability of services and factors such as age, health insurance, and poverty. This study demonstrates geographic trends in vision difficulty to broaden the understanding of disparities in vision care accessibility in the United States. American Community Survey 2014 5 -Year Estimate disability data were analyzed alongside Urban Influence <b>Codes</b> <b>and</b> National Provider <b>Identifier</b> registry data for optometrists and ophthalmologists to investigate correlations between accessibility to eye care and prevalence of vision difficulties. Through ArcMap software, ordinary least squares analysis of county-level data of eye care providers and other factors produced the standard residuals for the model used to identify vision care disparities. Vision care disparities were detected in 107 total counties between all twelve Urban Influence Codes classifications using county-level data. This study focuses only on the first of three phases of addressing equal health care access and establishes necessary background material for the next two phases by geographically identifying the locations of vision care disparities...|$|R
50|$|A {{listing of}} more than 900,000 {{assigned}} publisher <b>codes</b> is published, <b>and</b> can be ordered in book form (€1399, US$1959). The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: <b>identifier</b> 0 <b>and</b> <b>identifier</b> 1.|$|R
40|$|UDEF) name <b>and</b> <b>code</b> <b>identifier</b> {{for each}} of the "data content " element names in EIA- 836, i. e., those {{elements}} that are the "leaf " elements in the hierarchical tree, as opposed to the container elements at higher levels whose purpose is to provide context to the leaf elements. Both EIA- 836 Data Element names (column 1 below) and UDEF 2 Names (Column 2) are comprised of one “object class ” word, one “property ” word, and one or more optional “qualifier ” words for the object class and property words. While the EIA- 836 names follow this concept the overriding criteria in their selection is the ease in which CM practitioners can recognize the element name. The EIA- 836 schemas provide the context for each element. The UDEF name on the other hand is a semantically correct, context driven identifier for a data element. It is therefore generally longer and has more qualifiers. The object word and its preceding qualifiers are separated from the property word and its preceding qualifiers. UDEF names, since they use a precise limited vocabulary, can be reduced to a shorter alphanumeric code identifier (Column 3 in the table) ...|$|R
50|$|In addition, Nim {{supports}} a Uniform Call Syntax <b>and</b> <b>identifier</b> equality.|$|R
5000|$|Cards {{retain a}} {{transaction}} list (amount, times <b>and</b> <b>identifier</b> of the terminal) ...|$|R
50|$|For bank {{classification}} values <b>and</b> <b>identifiers</b> for German clearing regions, see Bankleitzahl (in German).|$|R
40|$|Abstract—Questions from Stack Overflow {{provide a}} unique {{opportunity}} to gain insight into what programming concepts are the most confusing. We present a topic modeling analysis that combines question concepts, types, <b>and</b> <b>code.</b> Using topic modeling, we are able to associate programming concepts <b>and</b> <b>identifiers</b> (like the String class) with particular types of questions, such as, “how to perform encoding”. I...|$|R
