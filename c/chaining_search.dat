9|153|Public
5000|$|... {{backward}} <b>chaining</b> <b>search,</b> possibly {{enhanced by}} the use of state constraints (see STRIPS, graphplan) ...|$|E
40|$|In {{this paper}} we discuss {{techniques}} for online generation of macro-actions {{as part of the}} planning process and demonstrate their use in a forward <b>chaining</b> <b>search</b> planning framework. The macroactions learnt are specifically created at places in the search space where the heuristic is not informative. We present results to show that using macro-actions generated during planning can improve planning performance...|$|E
40|$|The Graphplan {{algorithm}} {{exemplifies the}} speed-up achieveable with disjunctive representations that leverage opposing directions of refinement. It {{is in this}} spirit that we introduce Bsrgraphplan, {{a work in progress}} intended to address issues of scalability and expressiveness which are problematic for Graphplan. Specifically, we want to endow the planner with intelligent backtracking and full quantification of action schemata. Since Graphplan employs a backward <b>chaining</b> <b>search,</b> it lacks the necessary state information to support these mechanisms. We hypothesize that alternatively pointing the search {{in the direction of the}} goals provides an sufficient amelioration. Further, we demonstrate that a forward <b>chaining</b> <b>search</b> strategy can be competitive by restricting the search to the intersection of the relevant and applicable actions, and by enforcing ordering constraints on the plan prefix. The former is accomplished by using operators of a plangraph constructed in a top-down fashion to extend the plan prefix. To accomplish the latter we introduce an additional data structure, a constraint tree, which is constructed by a generalization of partial order planning...|$|E
40|$|In this paper, {{we present}} a new concept and its circuit implemen-tation for {{high-speed}} associative memories based on Hamming dis-tance. A <b>chained</b> <b>search</b> logic embedded in a memory cell enables a word-parallel search operation based on Hamming distance. A hierar-chically <b>chained</b> <b>search</b> architecture maintains a high-speed operation with faultless precision in a large input number. We also propose a scalable multi-chip architecture which attains fully chip- and word-parallel Hamming distance search {{by taking advantage of}} the digital associative memories. We have designed and fabricated a prototype chip with 64 bit × 32 word memories using a 0. 18 µm CMOS process. The measurement results show that the operation achieves a speed of 411. 5 MHz at a supply voltage of 1. 8 V. The worst-case search time is 158. 0 ns for 64 -bit 32 -word stored data. In a low-voltage operation, the operation speed achieves 40. 0 MHz at a supply voltage of 0. 75 V...|$|R
40|$|Abstract-Shifting {{bioenergy}} use to be {{a commercial}} fuel {{is an important}} thing for this moment. The fact said that not only conventional energy (fossil based energy) will be lost in around twenty years later, but also energy demand increases significantly that insist to find energy resource alternative to solve this phenomenon. The best energy resource alternative is biomass resource such as a palm oil based biomass. Technology, management, and supply chain issues are some of many useful research areas to keep bioenergy supply and its availability as a whole. The management of supply chain issue is the one of those issues in bioenergy field research. The classic problem in supply chain management, especially bioenergy supply chain management, is an optimum supply <b>chain</b> path <b>search.</b> In this paper we use ant colony algorithm as our main research contribution. This methodology is used to optimize supply <b>chain</b> path <b>search.</b> The result of this research analysis described by using Unified Modeling Language (UML) that consisted of activity diagram, use case diagram, class diagram, state chart diagram, sequence diagram, and collaboration diagram. We prepare some solution for the problem such as electronic mapping based supply chain route analysis, optimization of supply <b>chain</b> path <b>searching</b> and the best supply <b>chain</b> route <b>search...</b>|$|R
5000|$|Forward {{chaining}} {{starts with}} the available data and uses inference rules to extract more data (from an end user, for example) until a goal is reached. An inference engine using forward <b>chaining</b> <b>searches</b> the inference rules until it finds one where the antecedent (If clause) {{is known to be}} true. When such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data. [...] Inference engines will iterate through this process until a goal is reached.|$|R
40|$|The {{research}} {{aimed at}} analyzing {{the behavior of}} search and information use from users of Médiathèque Simone de Beauvoir, the French Alliance library João Pessoa, Paraíba State, from the model developed by David Ellis. The results revealed that the users´ information behavior resembled the categories established in Ellis´ model. The mapping of search behavior and information use of the library media users indicated that they use the internet in a noteworthy way: {{from the beginning to}} the <b>chaining</b> <b>search</b> for information, they use references presented on the Internet, considering that an important source of information and they trust the official websites. It was concluded that the use of the Internet is a characteristic of the users´ information behavior nowadays...|$|E
40|$|We {{present in}} this paper a hybrid {{planning}} systemwhich com-bines constraint satisfaction techniques and planning heuris-tics to produce optimal sequential plans. It integrates its own consistency rules and filtering and decomposition mech-anisms suitable for planning. Given a fixed bound on the plan length, our planner works directly on a structure related to Graphplan’s planning graph. This structure is incrementally built: Each time it is extended, a sequential plan is searched. Different search strategies may be employed. Currently, it is a forward <b>chaining</b> <b>search</b> based on problem decomposition with action sets partitioning. Various techniques are used to reduce the search space, such as memorizing nogood states or estimating goals reachability. In addition, the planner im-plements two different techniques to avoid enumerating some equivalent action sequences. Empirical evaluation shows that our system is very competitive on many problems, especially compared to other optimal sequential planners...|$|E
40|$|Introduction We {{have been}} {{developing}} a learning and revision system, hamlet, that learns multiple target concepts in nonlinear problem solving. hamlet learns control knowledge for individual decisions {{to guide the}} problem solver, by loosely explaining the trace of training episodes, and then refining the learned knowledge through experiencing positive and negative examples. We have described different aspects of hamlet's procedures in [1, 2]. hamlet is an ongoing research project. In this talk, we describe the multiple learning opportunities that hamlet addresses {{in the context of}} nonlinear planning, and the refinement of the learned knowledge. 2 Learning opportunities in nonlinear problem solving As a case study, we will present hamlet, a learning system that uses the prodigy architecture [3, 4]. prodigy, in its latest version prodigy 4. 0, follows a means-ends analysis backward <b>chaining</b> <b>search</b> procedure reasoning about multiple goals and multiple alternative operators relevant to t...|$|E
5000|$|... forward <b>chaining</b> {{state space}} <b>search,</b> {{possibly}} enhanced with heuristics ...|$|R
5000|$|The {{difference}} between searching and associative browsing {{is in the}} [...] "flow" [...] of the events. While in associative browsing you are surfing from site to site following your associative <b>chain,</b> in <b>search</b> mode you are inquiring information which will help you develops an associative chain.|$|R
40|$|A {{computational}} method is proposed for inferring protein interactions from genome sequences {{on the basis}} of the observation that some pairs of interacting proteins have homologs in another organism fused into a single protein <b>chain.</b> <b>Searching</b> sequences from many genomes revealed 6809 such putative proteinprotein interactions in Escherichia coli and 45, 502 in yeast. Many members of these pairs were confirmed as functionally related; computational filtering further enriches for interactions. Some proteins have links to several other proteins; these coupled links appear to represent functional interactions such as complexes or pathways. Experimentally confirmed interacting pairs are documented in a Database of Interacting Proteins. The lives of biological cells are controlled by interacting proteins in metabolic and signaling pathways and in complexes such as the molecular machines that synthesize and use adenosine triphosphate (ATP), replicate an...|$|R
40|$|Planning graphs {{have been}} shown to be a rich source of {{heuristic}} information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states, and the naive technique enumerates the graphs individually. This is equivalent to solving a multiple-source shortest path problem by iterating a single-source algorithm over each source. We introduce a data-structure, the state agnostic planning graph, that directly solves the multiple-source problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in deterministic planning to capture a set of planning graphs used in forward <b>chaining</b> <b>search.</b> A more prominent application of this technique is in belief state space planning, where each search node utilizes a set of planning graphs; an optimization to exploit state overlap between belief states collapses the set of sets of planning graphs to a single set. We describe another extension in probabilistic planning that reuses planning graph samples of probabilistic action outcomes across search nodes to otherwise curb the inherent prediction cost associated with handling probabilistic actions. Our experimental evaluation (using many existing International Planning Competition problems) quantifies each of these performance boosts, and demonstrates that heuristic belief state space progression planning using our technique is competitive with the state of the art...|$|E
40|$|In {{this paper}} we {{describe}} COLIN, a forward-chaining heuristic search planner, capable of reasoning with COntinuous LINear numeric change, {{in addition to}} the full temporal semantics of PDDL 2. 1. Through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning. COLIN combines FF-style forward <b>chaining</b> <b>search,</b> {{with the use of a}} Linear Program (LP) to check the consistency of the interacting temporal and numeric constraints at each state. The LP is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application. In addition, we develop an extension of the Temporal Relaxed Planning Graph heuristic of CRIKEY 3, to support reasoning directly with continuous change. We extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action. Finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. To support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects. We present results for COLIN that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners...|$|E
40|$|AbstractPlanning graphs {{have been}} shown to be a rich source of {{heuristic}} information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states, and the naive technique enumerates the graphs individually. This is equivalent to solving a multiple-source shortest path problem by iterating a single-source algorithm over each source. We introduce a data-structure, the state agnostic planning graph, that directly solves the multiple-source problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in deterministic (classical) planning to capture a set of planning graphs used in forward <b>chaining</b> <b>search.</b> A more prominent application of this technique is in conformant and conditional planning (i. e., search in belief state space), where each search node utilizes a set of planning graphs; an optimization to exploit state overlap between belief states collapses the set of sets of planning graphs to a single set. We describe another extension in conformant probabilistic planning that reuses planning graph samples of probabilistic action outcomes across search nodes to otherwise curb the inherent prediction cost associated with handling probabilistic actions. Finally, we show how to extract a state agnostic relaxed plan that implicitly solves the relaxed planning problem in each of the planning graphs represented by the state agnostic planning graph and reduces each heuristic evaluation to counting the relevant actions in the state agnostic relaxed plan. Our experimental evaluation (using many existing International Planning Competition problems from classical and non-deterministic conformant tracks) quantifies each of these performance boosts, and demonstrates that heuristic belief state space progression planning using our technique is competitive with the state of the art...|$|E
40|$|Abstract Nowadays, {{large scale}} {{optimisation}} problems arise {{as a very}} interesting field of research, because they appear in many real-world problems (bio-computing, data mining, etc.). Thus, scalability becomes an essential requirement for modern optimisation algorithms. In a previous work, we presented memetic algorithms based on local <b>search</b> <b>chains.</b> Local <b>search</b> <b>chain</b> concerns the idea that, at one stage, the local search operator may continue the operation of a previous invocation, starting from the final configuration reached by this one. Using this tech-nique, it was presented a memetic algorithm, MA-CMA-Chains, using the CMA-ES algorithm as its local search component. This proposal obtained very good results for continuous optimisation problems, in particular with medium-size (with up to dimension 50). Unfortunately, CMA-ES scalability is restricted by several costly opera-tions, thus MA-CMA-Chains could not be successfully applied to large scale problems. In this article we study the scalability of memetic algorithms based on local <b>search</b> <b>chains,</b> creating memetic algorithms with different local search methods and comparing them, considering both the error values and the processing cost. We also propose a variation of Solis Wets method, that we call Subgrouping Solis Wets algorithm. This local search method explores, at {{each step of the}} algorithm, only a random subset of the variables. This subset changes after a certain number of evaluations. Finally, we propose a new memetic algorithm based on local <b>search</b> <b>chains</b> for high dimensionality, MA-SSW-Chains, using the Subgrouping Solis Wets’ algorithm as its local search method. This algorithm is compared with MA-CMA-Chains and different reference algorithms, and it is shown that the proposal is fairly scalable and it is statistically very competitive for high-dimensional problems...|$|R
40|$|A defect {{exists in}} the polymer poly(9, 9 -dioctylfluorene) (PFO). With time, {{photo-oxidation}} alters the emissive properties of this polymer, causing light to emit at a lower energy. The usual, pure blue emission of PFO degrades to part blue and part green. The new emission found in the green band (g-band) makes devices such as LEDs that utilize the polymers grow dimmer. To analyze the impurity, the monomers substituent for PFO, fluorene and fluorenone, are substituted and their interactions are tested at different concentrations in solution. The following experiments give further evidence that intermolecular interactions, as opposed to intramolecular interactions, create the g-band defect. Optical microscopy {{is also used to}} examine the alignment of polymer chains of poly(methyl methacrylate) (PMMA) in thin films. Stretching, heating, rubbing, and heated deposition on a substrate all attempted to straighten PMMA <b>chains.</b> <b>Searching</b> for the Origin of the G-band Defect in Poly(9, 9 -dioctylfluorene) with Monomers Searching for the Origin of the G-band Defect in Poly(9, 9 -dioctylfluorene) with Monomer...|$|R
40|$|Typical {{characteristics}} of remote sensing applications are concurrent tasks, {{such as those}} found in disaster rapid response. The existing composition approach to geographical information processing service <b>chain,</b> <b>searches</b> for an optimisation solution and is what can be deemed a "selfish" way. This way leads to problems of conflict amongst concurrent tasks and decreases the performance of all service chains. In this study, a non-cooperative game-based mathematical model to analyse the competitive relationships between tasks, is proposed. A best response function is used, to assure each task maintains utility optimisation by considering composition strategies of other tasks and quantifying conflicts between tasks. Based on this, an iterative algorithm that converges to Nash equilibrium is presented, the aim being to provide good convergence and maximise the utilisation of all tasks under concurrent task conditions. Theoretical analyses and experiments showed that the newly proposed method, when compared to existing service composition methods, has better practical utility in all tasks...|$|R
40|$|This paper {{address a}} new problem in RFID {{authentication}} research for the first time. That is, existing RFID authentication schemes generally assume that the backend server is absolutely secure, however, this assumption is rarely tenable in practical conditions. It disables existing RFID authentication protocols from being safely applied to a reallife scenario in which the backend server is actually vulnerable, compromised or even malicious itself. We propose an RFID authentication scheme against an unsecure backend server. It is based on hash <b>chain,</b> <b>searching</b> over encrypted data, and coprivacy, defending against the privacy revealing to the backend server. The proposed scheme is scalable, resistant to desynchronization attacks, and provides mutual authentication in only three frontend communication steps. Moreover, {{it is the first}} scheme meeting the special security and privacy requirement for a cloud-based RFID authentication scenario in which the backend server is untrustworthy to readers held by cloud clients. Comment: This paper has been withdrawn by the author due to a crucial sign erro...|$|R
40|$|Introduces representations, techniques, and {{architectures}} used {{to build}} applied systems and to account for intelligence from a computational point of view. Applications of rule <b>chaining,</b> heuristic <b>search,</b> constraint propagation, constrained search, inheritance, and other problem-solving paradigms. Applications of identification trees, neural nets, genetic algorithms, and other learning paradigms. Speculations on the contributions of human vision and language systems to human intelligence. Enrollment may be limited...|$|R
40|$|In {{this paper}} an {{approach}} based on Heuristic Semantic Walk (HSW) is presented, where semantic proximity measures among concepts {{are used as}} heuristics in order to guide the concept <b>chain</b> <b>search</b> in the collaborative network of Wikipedia, encoding problem-specific knowledge in a problem-independent way. Collaborative information and multimedia repositories over the Web represent a domain of increasing relevance, since users cooperatively add to the objects tags, label, comments and hyperlinks, which reflect their semantic relationships, with or without an underlying structure. As {{in the case of}} the so called Big Data, methods for path finding in collaborative web repositories require solving major issues such as large dimensions, high connectivity degree and dynamical evolution of online networks, which make the classical approach ineffective. Experiments held on a range of different semantic measures show that HSW lead to better results than state of the art search methods, and points out the relevant features of suitable proximity measures for the Wikipedia concept network. The extracted semantic paths have many relevant applications such as query expansion, synthesis of explanatory arguments, and simulation of user navigation...|$|R
40|$|Sustainability {{oriented}} innovation {{continues to}} garner increasing attention {{as the answer}} to how firms may improve environmental and/or social performance while simultaneously finding competitive advantage. Radically innovating new products and services to replace harmful market incumbents is central to this thesis, yet studies to date have found it to be a highly expensive process with high degrees of uncertainty and risk. Extant research however has largely neglected to examine the details of the actual product innovation process itself and has under appreciated the influence of corporate strategic context. Our paper addresses this gap in the literature through an in-depth case study of a sustainability oriented innovation process for a radical new product within a multinational life sciences company, DSM. Our findings identify five critical organizational practices through which strategic direction has enabled the innovation process: technology super-scouting throughout the value <b>chain,</b> <b>search</b> heuristics that favor radical sustainability solutions, integration of sustainability performance metrics in product development, championing the value chain to build demand for radical sustainability oriented product innovation, and harnessing the benefits of open innovation...|$|R
40|$|Stochastic {{sampling}} based trackers {{have shown}} good performance for abrupt motion tracking {{so that they}} have gained popularity in recent years. However, conventional methods tend to use a two-stage sampling paradigm, in which the search space needs to be uniformly explored with an inefficient preliminary sampling phase. In this paper, we propose a novel sampling-based method in the Bayesian filtering framework to address the problem. Within the framework, nearest neighbor field estimation is utilized to compute the importance proposal probabilities, which guide the Markov <b>chain</b> <b>search</b> towards promising regions and thus enhance the sampling efficiency; given the motion priors, a smoothing stochastic sampling Monte Carlo algorithm is proposed to approximate the posterior distribution through a smoothing weight-updating scheme. Moreover, to track the abrupt and the smooth motions simultaneously, we develop an abrupt-motion detection scheme which can discover the presence of abrupt motions during online tracking. Extensive experiments on challenging image sequences demonstrate the effectiveness and the robustness of our algorithm in handling the abrupt motions. Comment: submitted to Elsevier Neurocomputin...|$|R
40|$|Description This package {{implements}} an algorithm {{family for}} continuous optimization called memetic algorithms with local <b>search</b> <b>chains</b> (MA-LS-Chains). Memetic algorithms are hybridizations of genetic algorithms with local search methods. They are especially suited for continuous optimization. Version 0. 2 -...|$|R
40|$|Abstract. In {{this survey}} paper we give an {{intuitive}} {{treatment of the}} discrete time quantization of classical Markov <b>chains.</b> Grover <b>search</b> and the quantum walk based search algorithms of Ambainis, Szegedy and Magniez et al. will be stated as quantum analogues of classical search procedures. We present a rather detailed description of a somewhat simplified version of the MNRS algorithm. Finally, in the query complexity model, we show how quantum walks {{can be applied to}} the followin...|$|R
40|$|Recently {{a number}} of {{discrete}} quantum walk based algorithms have been produced [1 – 6]. These are closely related to, or are specific cases of, structured search applied using a quantum walk. The formalism of discrete quantum walks has clear classical parallel {{in the form of}} Markov <b>chains.</b> Similarly, <b>search</b> via quantum walk has similar parallels in classical structured search. This report will review the field of search via discrete quantum walk and give a technical overview of the mathematical methods and structure. ...|$|R
40|$|In {{this survey}} paper we give an {{intuitive}} {{treatment of the}} discrete time quantization of classical Markov <b>chains.</b> Grover <b>search</b> and the quantum walk based search algorithms of Ambainis, Szegedy and Magniez et al. will be stated as quantum analogues of classical search procedures. We present a rather detailed description of a somewhat simplified version of the MNRS algorithm. Finally, in the query complexity model, we show how quantum walks {{can be applied to}} the following search problems: Element Distinctness, Matrix Product Verification, Restricted Range Associativity, Triangle, and Group Commutativity. Comment: 16 pages, survey pape...|$|R
40|$|Abstract. The {{purpose of}} this {{research}} is to find and analyze statistical characteristics of a simplex search in a noisy surrounding and drifting aim conditions. The search process is described by using multiple Markov <b>chains.</b> Efficient simplex <b>search</b> algorithms for nonstationary object optimisation can be created on this investigation basis...|$|R
40|$|AbstractTo keep {{ahead of}} the completion, {{organizations}} look for continuous improvement in Supply Chain Management (SCM). Lean paradigm connected to SCM is a strategy based on cost and time reduction to improve the effectiveness. At the operational level, Lean Supply Chain (LSC) is focused on optimizing the processes of all supply <b>chain,</b> <b>searching</b> for simplification, reducing waste and reducing activities that do not add value. A well-defined lean supply chain measurement system increases the chance for success because it enables managers to see areas where supply chain performance can be improved, so they can focus their attention, and obtain higher levels of performance. There {{are a number of}} conceptual frameworks and discussions on supply chain performance measurements in the literature, however {{there is a lack of}} empirical analysis and case studies on performance metrics and measurements in the supply chain environment of Small and Medium Sized Enterprises (SMEs). This research aims to develop a conceptual framework for managing LSC, integrating both financial and non-financial performance dimensions and so it expands the existent knowledge and provides indication of how LSC performance can be assessed and improved in this and other kinds of organizations. The proposed framework has been implemented in a Portuguese SME operating in the food manufacturing sector. A case study was developed to better understand the suitability of this tool...|$|R
40|$|Abstract—The {{field of}} complex biomechanical {{modeling}} {{has begun to}} rely on Monte Carlo techniques to investigate the effects of parameter variability and measurement uncertainty on model outputs, search for optimal parameter combinations, and define model limitations. However, advanced stochastic methods to perform data-driven explorations, such as Markov chain Monte Carlo (MCMC), become necessary {{as the number of}} model parameters increases. Here we demonstrate the feasibility and, what to our knowledge is, the first use of an MCMC approach to improve the fitness of realistically large biomechanical models. We used a Metropolis-Hastings algorithm to search increasingly complex parameter landscapes (3, 8, 24, and 36 dimensions) to uncover underlying distributions of anatomical parameters of a “truth model ” of the human thumb on the basis of simulated kinematic data (thumbnail location, orientation, linear and angular velocities) polluted by zero-mean, uncorrelated multivariate Gaussian “measurement noise. ” Driven by these data, ten Markov <b>chains</b> <b>searched</b> each model parameter space for the subspace that best fit the data (posterior distribution). As expected, the convergence time increased, more local minima were found, and marginal distributions broadened as the parameter space complexity increased. In the 36 -dimensional scenario, some chains found local minima but the majority of chains converged to the true posterior distribution (confirmed using a cross-validation dataset), thus demonstrating the feasibility and utility of these methods for realistically large biomechanical problems. Index Terms—Bayesian statistics, biomechanical model...|$|R
50|$|Backward {{chaining}} {{starts with}} a list of goals (or a hypothesis) and works backwards from the consequent to the antecedent {{to see if there is}} data available that will support any of these consequents. An inference engine using backward <b>chaining</b> would <b>search</b> the inference rules until it finds one which has a consequent (Then clause) that matches a desired goal. If the antecedent (If clause) of that rule is not known to be true, then it is added to the list of goals (in order for one's goal to be confirmed one must also provide data that confirms this new rule).|$|R
6000|$|... (1) {Ah! {{wherefore}} did he {{turn to look}} [...] {I {{know not}} why {{he turned to look}} [...] Since fatal was the gaze he took? [...] So far escaped from death or <b>chain,</b> [...] To <b>search</b> for her and search in vain: [...] Sad proof in peril and in pain [...] How late will Lover's hope remain.|$|R
40|$|In this paper, {{we present}} an {{algorithm}} for parsing natural images into middle level vision representations – regions, curves, and curve groups (parallel curves and trees). The algorithm is targeted for an integrated solution to image segmentation and curve grouping through Bayesian inference. The paper makes the following {{contributions to the}} literature. (1) It studies a layered (or 2. 1 D-sketch) representation integrating both regions and curves models which compete to explain an input image in terms of maximizing a Bayesian Posterior probability. The curve layer occludes the region layer and the curves observe a partial order occlusion relation among themselves. (2) It studies a Markov <b>chain</b> <b>search</b> scheme which consists of many pairs of reversible jumps to traverse the complex solution space. Six Markov chain jumps are designed as Metropolized Gibbs Samplers (MGS). A MGS proposes the next state within the jump scope of the current state according to a conditional probability like a Gibbs sampler and then accepts the proposal with a Metropolis-Hasting step. One pair of jumps is designed by the Swendsen-Wang cut method for curve grouping. The paper discusses systematic design strategies of devising reversible jumps for the complex inference task. (3) The proposal probability ratios in jumps are factorized into ratios of discriminative probabilities. The latter are computed in a bottom-up process and drive the Markov chain dynamics in a Data-Driven Markov Chain Monte Carlo framework. We demonstrate {{the performance of the}} algorithm in experiments with a number of natural images...|$|R
40|$|We {{address a}} family of hard {{benchmark}} instances for the Simple Plant Location Problem (also known as the Uncapacitated Facility Location Problem). The recent attempt by Fischetti et al. to tackle the Körkel-Ghosh instances resulted in seven new optimal solutions and 22 improved upper bounds. We use automated generation of heuristics to obtain a new algorithm for the Simple Plant Location Problem. In our experiments, our new algorithm matched all the previous best known and optimal solutions, and further improved 12 upper bounds, all within shorter time budgets compared to the previous efforts. Our algorithm design process is split into two phases: (i) development of algorithmic components such as local search procedures and mutation operators, and (ii) composition of a metaheuristic from the available components. Phase (i) requires human expertise and often can be completed by implementing several simple domain-specific routines known from the literature. Phase (ii) is entirely automated by employing the Conditional Markov <b>Chain</b> <b>Search</b> (CMCS) framework. In CMCS, a metaheuristic is flexibly defined {{by a set of}} parameters, called configuration. Then the process of composition of a metaheuristic from the algorithmic components is reduced to an optimisation problem seeking the best performing CMCS configuration. We discuss the problem of comparing configurations, and propose a new efficient technique to select the best performing configuration from a large set. To employ this method, we restrict the original CMCS to a simple deterministic case that leaves us with a finite and manageable number of meaningful configurations...|$|R
40|$|We {{study the}} Bipartite Boolean Quadratic Programming Problem (BBQP) {{which is an}} {{extension}} of the well known Boolean Quadratic Programming Problem (BQP). Applications of the BBQP include mining discrete patterns from binary data, approximating matrices by rank-one binary matrices, computing the cut-norm of a matrix, and solving optimisation problems such as maximum weight biclique, bipartite maximum weight cut, maximum weight induced sub-graph of a bipartite graph, etc. For the BBQP, we first present several algorithmic components, specifically, hill climbers and mutations, and then show how to com- bine them in a high-performance metaheuristic. Instead of hand-tuning a standard metaheuristic to test the efficiency of the hybrid of the components, we chose to use an automated generation of a multi- component metaheuristic to save human time, and also improve objectivity in the analysis and compar- isons of components. For this we designed a new metaheuristic schema which we call Conditional Markov <b>Chain</b> <b>Search</b> (CMCS). We show that CMCS is flexible enough to model several standard metaheuristics; this flexibility is controlled by multiple numeric parameters, and so is convenient for automated genera- tion. We study the configurations revealed by our approach and show that the best of them outperforms the previous state-of-the-art BBQP algorithm by several orders of magnitude. In our experiments we use benchmark instances introduced in the preliminary version of this paper and described here, which have already become the de facto standard in the BBQP literature...|$|R
40|$|Worldwide, extreme climate risks cause {{stakeholders}} in food supply <b>chains</b> to <b>search</b> for new risk management tools. In Australia, recently so-called crop yield simulation insurance {{has been introduced}} based on an integrated agrometeorological simulation model. Current uptake is relatively low, possibly because Australian farmers perceive commodity price risk as more important than climate risk. Also, they perceive risk management tools such as water management and diversification as more important than buying crop insurance. Still, opportunities seem to exist for new insurance products, such as crop yield simulation insurance, as indicated by farmers’ interest into such products. Outcomes are useful in worldwide discussions on risk management opportunities in dryland agricultur...|$|R
