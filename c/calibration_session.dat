38|29|Public
40|$|A brain-computer {{interface}} (BCI) enables direct communication {{from the brain}} to devices, bypassing the traditional pathway of peripheral nerves and muscles. Traditional approaches to BCIs require the user to train for weeks or even months to learn to control the BCI. In contrast, BCIs based on machine learning only require a <b>calibration</b> <b>session</b> of {{less than an hour}} before the system can be used, since the machine adapts to the user’s existing brain signals. However, this <b>calibration</b> <b>session</b> has to be repeated before each use of the BCI due to inter-session variability, which makes using a BCI still a time-consuming and an error-prone enterprise. In this work, we present a second-order baselining procedure that reduces these variations, and enables the creation of a BCI that can be applied to new subjects without such a <b>calibration</b> <b>session.</b> The method was validated with a motor-imagery classification task performed by 109 subjects. Results showed that our subjectindependent BCI without calibration performs as well as the popular common spatial patterns (CSP) -based BCI that does use a <b>calibration</b> <b>session...</b>|$|E
40|$|Even {{though the}} P 300 based speller {{has proved to}} be usable by real patients, it is not a {{user-friendly}} system. The necesarry <b>calibration</b> <b>session</b> and slow spelling make the system tedious. We present a machine learning approach to P 300 spelling that enables us to remove the <b>calibration</b> <b>session.</b> We achieve this by a combination of unsupervised training, transfer learning across subjects and language models. On top of that, we can increase the spelling speed by incorporating a dynamic stopping approach. This yields a P 300 speller that works instantly and with high accuracy and spelling speed, even for unseen subjects...|$|E
40|$|Abstract Speech {{recognition}} in car environments {{has been identified}} as a valuable means for reducing driver distraction when operating non-critical in-car systems. Likelihood-maximising (LIMA) frameworks optimise speech enhancement algorithms based on recognised state sequences rather than traditional signal-level criteria such as maximising signal-to-noise ratio. Previously presented LIMA frameworks require calibration utterances to generate optimised enhancement parameters which are used for all subsequent utterances. Sub-optimal recognition performance occurs in noise conditions which are significantly different from that present during the <b>calibration</b> <b>session</b> – a serious problem in rapidly changing noise environments. We propose a dialog-based design which allows regular optimisation iterations in order to track the changing noise conditions. Experiments using Mel-filterbank spectral subtraction are performed to determine the optimisation requirements for vehicular environments and show that minimal optimisation assists real-time operation with improved speech recognition accuracy. It is also shown that the proposed design is able to provide improved recognition performance over frameworks incorporating a <b>calibration</b> <b>session...</b>|$|E
40|$|The brain-computer {{interface}} (BCI) {{system has been}} developed to assist people with motor disability. To make the system more user-friendly, {{it is a challenge}} to reduce the electrode preparation time and have a good reliability. This study aims to find a minimal set of electrodes for an individual stroke subject for motor imagery to control an assistive device using functional electrical stimulation for 20 sessions with accuracy higher than 90 %. The characteristics of this minimal electrode set were evaluated with two popular algorithms: Fisher's criterion and support-vector machine recursive feature elimination (SVM-RFE). The number of <b>calibration</b> <b>sessions</b> for channel selection required for robust control of these 20 sessions was also investigated. Five chronic stroke patients were recruited for the study. Our results suggested that the number of <b>calibration</b> <b>sessions</b> for channel selection did not {{have a significant effect on}} the classification accuracy. A performance index devised in this study showed that one training day with 12 electrodes using the SVM-RFE method achieved the best balance between the number of electrodes and accuracy in the 20 -session data. Generally, 8 - 36 channels were required to maintain accuracy higher than 90 % in 20 BCI training sessions for chronic stroke patients. Department of Health Technology and Informatic...|$|R
40|$|This work {{introduces}} a novel classifier for a P 300 -based speller, which, contrary to common methods, {{can be trained}} entirely unsupervisedly using an Expectation Maximization approach, {{eliminating the need for}} costly dataset collection or tedious <b>calibration</b> <b>sessions.</b> We use publicly available datasets for validation of our method and show that our unsupervised classifier performs competitively with supervised state-of-the-art spellers. Finally, we demonstrate the added value of our method in different experimental settings which reflect realistic usage situations of increasing difficulty and which would be difficult or impossible to tackle with existing supervised or adaptive methods...|$|R
40|$|Current {{state-of-the-art}} in Brain computer Interfacing (BCI) involves tuning classifiers to subject-specific {{training data}} acquired from <b>calibration</b> <b>sessions</b> prior to functional BCI use Using a large database of EEG recordings from 45 subjects, {{who took part}} in movement imagination task experiments. we Construct an ensemble of classifiers derived from subject-specific temporal and spatial filters. The ensemble is then sparsified using quadratic regression with l(1) regularization such that the final classifier generalizes reliably to data of subjects not included in the ensemble Our offline results indicate that BCI-naive users Could start real-time BCI use Without any prior calibration at only very limited loss of performance...|$|R
40|$|This {{clinical}} trial investigates the facilitating effects of combining tDCS with EEG-based motor imagery Brain-Computer Interface (MI-BCI) robotic feedback compared to sham-tDCS for upper limb stroke rehabilitation. 32 hemiparetic stroke patients were recruited and screened {{for their ability}} to use EEG-based MI-BCI. Subsequently, 17 of these patients who passed screening and gave further consent were randomized to receive 20 minutes of tDCS or sham-tDCS prior to 10 sessions of 1 -hour MI-BCI with robotic feedback for 2 weeks. The offline and online accuracies of detecting motor imagery from idle condition for the <b>calibration</b> <b>session</b> and the evaluation part of the 10 rehabiltiation sessions were respectively assessed. The results showed that there were {{no significant difference in the}} accuracies of the <b>calibration</b> <b>session</b> from both groups, but the online accuracies of the evaluation part of 10 rehabilitation sessions of the tDCS group were significantly higher than the sham-tDCS group. Hence the results suggest towards tDCS effect in modulating motor imagery in stroke...|$|E
40|$|Speech {{recognition}} in car environments {{has been identified}} as a valuable means for reducing driver distraction when operating non-critical in-car systems. Likelihood-maximising (LIMA) frameworks optimise speech enhancement algorithms based on recognised state sequences rather than traditional signal-level criteria such as maximising signal-to-noise ratio. Previously presented LIMA frameworks require calibration utterances to generate optimised enhancement parameters which are used for all subsequent utterances. Sub-optimal recognition performance occurs in noise conditions which are significantly different from that present during the <b>calibration</b> <b>session</b> - a serious problem in rapidly changing noise environments. We propose a dialog-based design which allows regular optimisation iterations in order to track the changing noise conditions. Experiments using Mel-filterbank spectral subtraction are performed to determine the optimisation requirements for vehicular environments and show that minimal optimisation assists real-time operation with improved speech recognition accuracy. It is also shown that the proposed design is able to provide improved recognition performance over frameworks incorporating a <b>calibration</b> <b>session...</b>|$|E
40|$|In {{order to}} enhance the {{usability}} of a motor imagery-based brain-computer interface (BCI), it is highly desirable to reduce the calibration time. Due to inter-subject variability, typically a new subject has to undergo a 20 - 30 minutes <b>calibration</b> <b>session</b> to collect sufficient data for training a BCI model based on his/her brain patterns. This paper proposes a new subject-to-subject adaptation algorithm to reliably reduce the calibration time of a new subject to only 3 - 4 minutes. To reduce the calibration time, unlike several past studies, the proposed algorithm {{does not require a}} large pool of historic sessions. In the proposed algorithm, using only a few trials from the new subject, first, the new subject's data is adapted to each available historic session separately. This is done by a linear transformation minimizing the distribution {{difference between the two groups}} of EEG data. Thereafter, among the available historic sessions, the one matched the most to the new subject's adapted data is selected as the <b>calibration</b> <b>session.</b> Consequently, the previously trained model based on the selected historic session is entirely used for the classification of the new subject's data after adaptation. The proposed algorithm is evaluated on a publicly available dataset with 9 subjects. For each subject, the <b>calibration</b> <b>session</b> is selected only from the calibration sessions of the eight other subjects. The experimental results showed that our proposed algorithm not only reduced the calibration time by 85 %, but also performed on average only 1. 7 % less accurate than the subject-dependent calibration results...|$|E
40|$|This paper {{describes}} a recent effort in characterizing frequency stability {{performance of the}} ground system in the NASA Deep Space Network (DSN). Unlike the traditional approach where performance is obtained from special <b>calibration</b> <b>sessions</b> that are both time consuming and require manual setup, the new method taps into the daily spacecraft tracking data. This method significantly increases the amount of data available for analysis, roughly by two orders of magnitude; {{making it possible to}} conduct trend analysis with reasonable confidence. Since the system is monitored daily, any significant variation in performance can be detected timely. This helps the DSN maintain its performance commitment to customers...|$|R
40|$|In {{the last}} 20 years, imaging {{spectrometry}} in the visible and near infrared onboard spacecraft {{has become an}} essential technique to study surfaces and atmospheres of planetary objects, prior {{and in addition to}} in-situ analyses. Remote sensing experiments analyzing the sunlight reflected by planetary surfaces can be used to derive the mineralogical composition and physical properties of the natural surfaces and atmospheres observed. This technique was successfully used on Mars with ISM instrument. As an improvement of ISM, OMEGA onboard MARS-EXPRESS will {{play a key role in}} the mineralogical mapping of the Martian surface. We present the reflectance measurements made with OMEGA during on ground <b>calibration</b> <b>sessions</b> on natural slabs of rocks and minerals part of a database which will be used on similar future American instruments (as CRISM onboard MRO). We discuss the implication on the scientific capabilities of OMEGA instrument...|$|R
40|$|Weighing-in-motion (WIM) {{of trucks}} is a {{relatively}} inexpensive technique for measuring truck axle loads. Unfortunately, WIM measurements are subject to systematic and random errors. A method has been developed to suppress the influence of these errors: During periodic <b>calibration</b> <b>sessions</b> on a WIM site, {{a random sample of}} trucks is taken from the traffic stream. Each truck axle is weighed twice: once by the WIM equipment, then on an accurate (static) scale. By comparing the above pairs of axle loads, the magnitude of the systematic and random errors of the WIM equipment used on the site can be established. With this knowledge {{a large portion of the}} errors can be eliminated, and inaccurate WIM measurements, done in periods between two calibrations, rec fied. Three computer tools have been developed to assist with the calibration, mass data processing, and with the graphical representation of results. 1...|$|R
3000|$|We {{detect the}} {{preamble}} {{by means of}} a replica-correlator detector (equivalent to a matched filter detector [45]). There are two ways to build the replica signal, either by averaging many ERPs during a previous <b>calibration</b> <b>session</b> or synthetically by software. In this study, we decided the second option, thus avoiding calibration sessions and challenging the study under a plug-and-play approach. The replica signal was the expected electrophysiological response to the stimulus signal p [...]...|$|E
40|$|The aim of {{this study}} was to {{evaluate}} both intra- and interoperator reliability of a radiological three-dimensional classification system (KPG index) for the assessment of degree of difficulty for orthodontic treatment of maxillary canine impactions. Cone beam computed tomography (CBCT) scans of fifty impacted canines, obtained using three different scanners (NewTom, Kodak, and Planmeca), were classified using the KPG index by three independent orthodontists. Measurements were repeated one month later. Based on these two sessions, several recommendations on KPG Index scoring were elaborated. After a joint <b>calibration</b> <b>session,</b> these recommendations were explained to nine orthodontists and the two measurement sessions were repeated. There was a moderate intrarater agreement in the precalibration measurement sessions. After the <b>calibration</b> <b>session,</b> both intra- and interrater agreement were almost perfect. Indexes assessed with Kodak Dental Imaging 3 D module software showed a better reliability in z-axis values, whereas indexes assessed with Planmeca Romexis software showed a better reliability in x- and y-axis values. No differences were found between the CBCT scanners used. Taken together, these findings indicate that the application of the instructions elaborated during this study improved KPG index reliability, which was nevertheless variously influenced by the use of different software for images evaluation...|$|E
40|$|Objective. Most BCIs have {{to undergo}} a <b>calibration</b> <b>session</b> in which data is {{recorded}} to train decoders with machine learning. Only recently zero-training methods have become a subject of study. This work proposes a probabilistic framework for BCI applications which exploit event-related potentials (ERPs). For {{the example of a}} visual P 300 speller we show how the framework harvests the structure suitable to solve the decoding task by (a) transfer learning, (b) unsupervised adaptation, (c) language model and (d) dynamic stopping. Approach. A simulation study compares the proposed probabilistic zero framework (using transfer learning and task structure) to a state-of-the-art supervised model on n = 22 subjects. The individual influence of the involved components (a) –(d) are investigated. Main results. Without any need for a <b>calibration</b> <b>session,</b> the probabilistic zero-training framework with inter-subject transfer learning shows excellent performance—competitive to a state-of-the-art supervised method using calibration. Its decoding quality is carried mainly by the effect of transfer learning in combination with continuous unsupervised adaptation. Significance. A high-performing zero-training BCI is within reach {{for one of the most}} popular BCI paradigms: ERP spelling. Recording calibration data for a supervised BCI would require valuable time which is lost for spelling. The time spent on calibration would allow a novel user to spell 29 symbols with our unsupervised approach. It could be of use for various clinical and non-clinical ERP-applications of BCI...|$|E
3000|$|A {{key issue}} for BUPLAS reliability, where {{knowledge}} and skills transfer is a non-negotiable business requirement, has been {{what appears to be}} success in training non-language background raters to assess the quality of English communication at work. Unlike most other LSP assessment tests, SMEs are used for all BUPLAS assessments. Over the last ten years, call centre recruiters and quality assurance personnel have shown acceptable rates of reliability after attending the requisite BUPLAS training workshops, and after multiple and regular <b>calibration</b> <b>sessions.</b> As part of BUPLAS accreditation, all candidate calibration results are recorded so their performance over time is tracked and they are accredited as calibrated once they are consistently within the calibration threshold (. 5 measure on BUPLAS). These findings perhaps contest the common assumption that only language assessors can reliably use language communication scales and descriptors; SMEs with appropriate training and support appear to be doing an equally good job. In the recent OET studies (reported by McNamara in Cambridge ESOL presentation, 2013 [...]...|$|R
40|$|This report {{presents}} {{adaptive control}} algorithms for conventional {{modes of operation}} of MEMS z-axis gyroscopes. In an open-loop mode, an off-line self-calibration scheme is proposed for estimating fabrication imperfections and enhancing {{the performance of a}} gyroscope operating in open-loop mode. This scheme can be implemented during the initial calibration stage when the gyroscope is turned on, or at regular <b>calibration</b> <b>sessions,</b> which may be performed periodically. An adaptive add-on control scheme is also proposed for a closed-loop mode of operation. This scheme is realized by adding an outer loop to a conventional force-balancing scheme that includes a parameter estimation algorithm. This parameter adaptation algorithm estimates the angular rate, identifies and compensates the quadrature error, and may permit on-line automatic mode tuning. The convergence and resolution analysis show that the proposed adaptive add-on control scheme prevents the angular rate estimate from being contaminated by the quadrature error, while keeping ideal resolution performance of a conventional force-balancing scheme. ...|$|R
40|$|Non-stationarities are {{ubiquitous}} in EEG signals. They are especially {{apparent in the}} use of EEG-based brain–computer interfaces (BCIs) : (a) in the differences between the initial calibration measurement and the online operation of a BCI, or (b) caused by changes in the subject’s brain processes during an experiment (e. g. due to fatigue, change of task involvement, etc). In this paper, we quantify for the first time such systematic evidence of statistical differences in data recorded during offline and online sessions. Furthermore, we propose novel techniques of investigating and visualizing data distributions, which are particularly useful for the analysis of (non-) stationarities. Our study shows that the brain signals used for control can change substantially from the offline <b>calibration</b> <b>sessions</b> to online control, and also within a single session. In addition to this general characterization of the signals, we propose several adaptive classification schemes and study their performance on data recorded during online experiments. An encouraging result of our study is that surprisingly simple adaptive methods in combination with an offline feature selection scheme can significantly increase BCI performance...|$|R
40|$|Brain-Computer Interface (BCI) {{systems are}} {{traditionally}} designed by {{taking into account}} user-specific data to enable practical use. More recently, subject independent (SI) classification algorithms have been developed which bypass the subject specific adaptation and enable rapid use of the system. A brain switch is a particular BCI system where the system is required to distinguish from two separate mental tasks corresponding to the on-off commands of a switch. Such applications require a low false positive rate (FPR) while having an acceptable response time (RT) until the switch is activated. In this work, we develop a methodology that produces optimal brain switch behavior through subject specific (SS) adaptation of: a) a multitrial prediction combination model and b) an SI classification model. We propose a statistical model of combining classifier predictions that enables optimal FPR calibration through a short <b>calibration</b> <b>session.</b> We trained an SI classifier on a training synchronous dataset and tested our method on separate holdout synchronous and asynchronous brain switch experiments. Although our SI model obtained similar performance between training and holdout datasets, 86 % and 85 % for the synchronous and 69 % and 66 % for the asynchronous the between subject FPR and TPR variability was high (up to 62 %). The short <b>calibration</b> <b>session</b> was then employed to alleviate that problem and provide decision thresholds that achieve when possible a target FPR= 1 % with good accuracy for both datasets...|$|E
40|$|Abstract. In {{an on-line}} {{adaptive}} test, data for calibrating new items are collected from examinees while they take an operational test. In this paper, we will assume {{a situation where}} a <b>calibration</b> <b>session</b> must collect data about several experimental items simultaneously. While past {{research has focused on}} estimation of one or more two-parameter logistic items, this research focuses on estimating several threeparameter logistic items simultaneously. We consider this problem in terms of constrained optimization over probability distributions. The probability distributions are over a two-by-two contingency table, and the marginal distributions form the constraints. We formulate these constraints for network-flow constraint, and investigate a conjugate-gradient-search algorithm to optimize the determinant of Fisher’s information matrix...|$|E
40|$|Up to {{now even}} {{subjects}} that are {{experts in the}} use of machine learning based BCI systems still have to undergo a <b>calibration</b> <b>session</b> of about 20 - 30 min. From this data their (movement) intentions are so far infered. We now propose a new paradigm that allows to completely omit such calibration and instead transfer knowledge from prior sessions. To achieve this goal we first define normalized CSP features and distances in-between. Second, we derive prototypical features across sessions: (a) by clustering or (b) by feature concatenation methods. Finally, we construct a classifier based on these individualized prototypes and show that, indeed, classifiers can be successfully transferred to a new session for a number of subjects. ...|$|E
40|$|The Tile Calorimeter is a steel-scintillator device which {{constitutes}} the central hadronic {{section of the}} ATLAS Calorimeter. About 12 % of the Tile Calorimeter has been exposed to test beams, at CERN, {{in order to determine}} the electromagnetic scale and to evaluate its uniformity. During the 2003 calibration periods, an online monitoring program has been developed to ease the setup of correct running condition and the assessment of data quality. The program, developed in C++ and based on the ROOT framework, has been built using the Online Software Service provided by the ATLAS Online Software group and is equipped with a graphical user interface. This application has been intensively used throughout the <b>calibration</b> <b>sessions</b> in 2003 and also during the setup and test with cosmic rays carried out, at the beginning of 2004, on a few modules of TileCal during the pre-assembly of the extended barrel. After a brief overview of ATLAS DAQ and Services, the architecture and features of the Tile Calorimeter monitoring program are described in detail. Performances and successive integrations are discussed in the last part of the paper...|$|R
40|$|We {{report the}} stability, accuracy, and {{development}} {{history of a}} new left atrial pressure (LAP) sensing system in ambulatory heart failure (HF) patients. A total of 84 patients with advanced HF underwent percutaneous transseptal implantation of the pressure sensor. Quarterly noninvasive calibration by modified Valsalva maneuver was achieved in all patients, and 96. 5 % of <b>calibration</b> <b>sessions</b> were successful with a reproducibility of 1. 2  mmHg. Absolute sensor drift was maximal after 3  months at 4. 7  mmHg (95 % CI, 3. 2 – 6. 2  mmHg) and remained stable through 48  months. LAP was highly correlated with simultaneous pulmonary wedge pressure at 3 and 12  months (r[*]=[*] 0. 98, average difference of 0. 8 [*]±[*] 4. 0  mmHg). Freedom from device failure was 95 % (n[*]=[*] 37) at 2  years and 88 % (n[*]=[*] 12) at 4  years. Causes of failure were identified and mitigated with 100 % freedom from device failure and less severe anomalies in the last 41 consecutive patients (p[*]=[*] 0. 005). Accurate and reliable LAP measurement using a chronic implanted monitoring system is safe and feasible in patients with advanced heart failure...|$|R
40|$|Recent {{advances}} in signal processing and machine learning techniques have enabled {{the application of}} Brain-Computer Interface (BCI) technologies to fields such as medicine, industry and recreation. However, BCIs still suffer from the requirement of frequent <b>calibration</b> <b>sessions</b> due to the intra- and inter- individual variability of brain-signals, which makes calibration suppression through transfer learning an area of increasing interest {{for the development of}} practical BCI systems. In this paper, we present an unsupervised transfer method (spectral transfer using information geometry, STIG), which ranks and combines unlabeled predictions from an ensemble of information geometry classifiers built on data from individual training subjects. The STIG method is validated in both offline and real-time feedback analysis during a rapid serial visual presentation task (RSVP). For detection of single-trial, event-related potentials (ERPs), the proposed method can significantly outperform existing calibration-free techniques as well as outperform traditional within-subject calibration techniques when limited data is available. This method demonstrates that unsupervised transfer learning for single-trial detection in ERP-based BCIs can be achieved without the requirement of costly training data, representing a step-forward in the overall goal of achieving a practical user-independent BCI system...|$|R
40|$|We propose an {{automatic}} approach to synchronize {{a network of}} uncalibrated and unsynchronized video cameras, and recover the complete calibration of all these cameras. In this paper, we extend recent work on computing the epipolar geometry from dynamic silhouettes, to deal with unsynchronized sequences and find the temporal offset between them. This is used to compute the fundamental matrices and the temporal offsets between many view-pairs in the network. Knowing the time-shifts between enough viewpairs allows us to robustly synchronize the whole network. The calibration of all the cameras is recovered from these fundamental matrices. The dynamic shape of the object can then be recovered using a visual-hull algorithm. Our method is especially useful for multi-camera shape-fromsilhouette systems, as visual hulls can now be reconstructed {{without the need for}} a specific <b>calibration</b> <b>session...</b>|$|E
40|$|The first {{implementation}} of the KM 3 NeT-IT neutrino telescope consists in the installation of 24 Strings and 8 Towers. Focusing on the Towers, {{the idea behind this}} work is to exploit the LED sources mounted in the OMs to develop a complementary system, on shore and before the deployment, for the determination of time delays, aiming at the characterizations of the time response of the different elements of the detector. During the assembling of the first tower, a set of measurements has been carried out; the test set-up and the measurement procedure are described, together with preliminary results of the calibration system. Lesson learnt is quite encouraging: uncertainties of the order of 400 [*]ps are reached with very few cautions taken during the short <b>calibration</b> <b>session,</b> and with large room for improvement, making this system feasible and effective for the KM 3 NeT-IT experiment...|$|E
40|$|International audienceAn {{important}} factor in the usability of a brain-computer interface (BCI) is the setup and calibration time required for the interface to perform accurately. Recently, brain-switches based on the rebound following motor imagery of a single limb effector have been investigated as basic BCIs due to their good performance with limited electrodes, and brief training session requirements. Here, a BCI is proposed which expands the methodology of brain-switches to design an interface composed of multiple brain-controlled buttons. The algorithm is designed as a system paced interface which can recognise 2 intentional-control tasks and a no-control state based on the activity during and following motor imagery in only 3 electroencephalogram channels. An online experiment was performed over 6 subjects to validate the algorithm, and the results show that a working BCI can be trained from a single <b>calibration</b> <b>session</b> and that the post motor imagery features are both informative and robust over multiple sessions...|$|E
40|$|Abstract We {{report the}} stability, accuracy, and {{development}} {{history of a}} new left atrial pressure (LAP) sensing system in ambulatory heart failure (HF) patients. A total of 84 patients with advanced HF underwent percutaneous transseptal implantation of the pressure sensor. Quarterly noninvasive calibration by modified Valsalva maneuver was achieved in all patients, and 96. 5 % of <b>calibration</b> <b>sessions</b> were successful with a reproducibility of 1. 2 mmHg. Absolute sensor drift was maximal after 3 months at 4. 7 mmHg (95 % CI, 3. 2 – 6. 2 mmHg) and remained stable through 48 months. LAP was highly correlated with simultaneous pulmonary wedge pressure at 3 and 12 months (r= 0. 98, average difference of 0. 8 ± 4. 0 mmHg). Freedom from device failure was 95 % (n= 37) at 2 years and 88 % (n= 12) at 4 years. Causes of failure were identified and mitigated with 100 % freedom from device failure and less severe anomalies in the last 41 consecutive patients (p= 0. 005). Accurate and reliable LAP measurement using a chronic implanted monitoring system is safe and feasible in patients with advanced heart failure...|$|R
40|$|The Occupational Requirements Survey (ORS) is an {{establishment}} {{survey conducted by}} the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on vocational preparation and the cognitive and physical requirements of occupations in the U. S. economy, as well as the environmental conditions in which those occupations are performed. Calibration training is a type of refresher training that compares interviewer performance against predetermined standards to assess rating accuracy, inter-rater reliability, and other measures of performance. This paper will review the results of three separate <b>calibration</b> training <b>sessions</b> that focused on a data collector’s ability to identify {{the presence or absence of}} physical demands and environmental conditions based on visual observation (assessed by watching job videos), assign Standard Occupational Classification (SOC) codes, and code Specific Vocational Preparation (SVP), which is a measure of the lapsed time required by a typical worker to reach average performance. Information obtained from these sessions was used to help evaluate training and mentoring programs, as well as to provide input into quality assurance procedures. However, the three <b>calibration</b> training <b>sessions</b> described in this paper generally showed minimal impact on performance measures used in the sessions...|$|R
40|$|International audience— To {{propose a}} {{reliable}} and robust Brain-Computer Interface (BCI), efficient machine learning and signal process-ing methods have to be used. However, it is often necessary to have {{a sufficient number of}} labeled brain responses to create a model. A large database that would represent all of the possible variabilities of the signal is not always possible to obtain, because <b>calibration</b> <b>sessions</b> have to be short. In the case of BCIs based on the detection of event-related potentials (ERPs), we propose to tackle this problem by including additional deformed patterns in the training database {{to increase the number of}} la-beled brain responses. The creation of the additional deformed patterns is based on two approaches: (i) smooth deformation fields, and (ii) right and left shifted signals. The evaluation is performed with data from 10 healthy subjects participating in a P 300 speller experiment. The results show that small shifts of the signal allow a better estimation of both spatial filters, and a linear classifier. The best performance, AUC= 0. 828 ± 0. 061, is obtained by combining the smooth deformation fields and the shifts, after spatial filtering, compared to AUC= 0. 543 ± 0. 025, without additional deformed patterns. The results support the conclusion that adding signals with small deformations can significantly improve the performance of single-trial detection when the amount of training data is limited...|$|R
40|$|Can {{people use}} text-entry based brain-computer {{interface}} (BCI) systems {{and start a}} free spelling mode without any <b>calibration</b> <b>session?</b> Brain activities differ largely across people and across sessions for the same user. Thus, how can the text-entry system classify the desired character among the other characters in the P 300 -based BCI speller matrix? In this paper, we introduce a new unsupervised classifier for a P 300 -based BCI speller, which uses a disjunctive normal form representation to define an energy function involving a logistic sigmoid function for classification. Our proposed classifier updates the initialized random weights performing classification for the P 300 signals from the recorded data exploiting {{the knowledge of the}} sequence of row/column highlights. To verify the effectiveness of the proposed method, we performed an experimental analysis on data from 7 healthy subjects, collected in our laboratory. We compare the proposed unsupervised method to a baseline supervised linear discriminant analysis (LDA) classifier and demonstrate its effectiveness...|$|E
40|$|Laplacian {{filters are}} widely used in neuroscience. In the context of brain-computer interfacing, they might be {{preferred}} to data-driven approaches such as common spatial patterns (CSP) {{in a variety of}} scenarios such as, e. g., when no or few user data are available or a <b>calibration</b> <b>session</b> with a multi-channel recording is not possible, which is the case in various applications. In this paper we propose the use of an ensemble of local CSP patches (CSPP) which can be considered as a compromise between Laplacian filters and CSP. Our CSPP only needs {{a very small number of}} trials to be optimized and significantly outperforms Laplacian filters in all settings studied. Additionally, CSPP also outperforms multi-channel CSP and a regularized version of CSP even when only very few calibration data are available, acting as a CSP regularizer without the need of additional hyperparameters and at a very low cost: 2 - 5 min of data recording, i. e. ten times less than CSP...|$|E
40|$|The {{measurement}} of the difference of the transmit and receive delays of the signals in a Two-Way Satellite Time and Frequency Transfer (TWSTFT) Earth station is crucial for its nanosecond time transfer capability. Also, the monitoring of the change of this delay difference with time, temperature, humidity, or barometric pressure is important for improving the TWSTFT capabilities. An automated system for this purpose has been developed from the initial design at NMi-VSL. It calibrates separately the transmit and receive delays in cables, amplifiers, upconverters and downconverters, and antenna feeds. The obtained results can be applied as corrections to the TWSTFT measurement when, before and after a measurement session, a <b>calibration</b> <b>session</b> is performed. Preliminary results obtained at NMi-VSL will be shown. Also, if available, {{the results of a}} manual version of the system that is planned to be circulated in Sept. 1994 together with a USNO portable station on a calibration trip to European TWSTFT Earth stations...|$|E
40|$|Abstract The {{objective}} {{of this study was}} to describe an interviewer training and calibration method to evaluate oral health literacy using the Brazilian Rapid Estimate of Adult Literacy in Dentistry (BREALD- 30) in epidemiological studies. An experienced researcher (gold standard) conducted all training sessions. The interviewer training and <b>calibration</b> <b>sessions</b> included three different phases: theoretical training, practical training, and calibration. In the calibration phase, six interviewers (dentists) independently assessed 15 videos of individuals who had different levels of oral health literacy. Accuracy and reproducibility were evaluated using the kappa coefficient and the intraclass correlation coefficient (ICC). The percentage of agreement for each word in the instrument was also calculated. After training, the kappa values were higher than 0. 911 and 0. 893 for intra- and inter-rater agreement, respectively. When the results were analyzed separately for the different levels of literacy, the lowest agreement rate was found when evaluating the videos of individuals with low literacy (K = 0. 871), but still within the range considered to be near-perfect agreement. The ICC values were higher than 0. 990 and 0. 975 for intra- and inter-rater agreement, respectively. The lowest percentage of agreement was 86. 6 % for the word &# 8220;hipoplasia&# 8221; (hypoplasia). This interviewer training and calibration method proved to be feasible and effective. Therefore, it can be used as a methodological tool in studies assessing oral health literacy using the BREALD- 30...|$|R
40|$|To {{report on}} the {{progress}} of an ongoing research collaboration on magnetic resonance imaging (MRI) in juvenile idiopathic arthritis (JIA) and describe the proceedings of a meeting, held prior to Outcome Measures in Rheumatology (OMERACT) 12, bringing together the OMERACT MRI in JIA working group and the Health-e-Child radiology group. The goal of the meeting was to establish agreement on scoring definitions, locations, and scales for the assessment of MRI of patients with JIA for both large and small joints. The collaborative work process included premeeting surveys, presentations, group discussions, consensus on scoring methods, pilot scoring, conjoint review, and discussion of a future research agenda. The meeting resulted in preliminary statements on the MR imaging protocol of the JIA knee and wrist and determination of the starting point for development of MRI scoring systems based on previous studies. It was also considered important to be descriptive rather than explanatory in the assessment of MRI in JIA (e. g., "thickening" instead of "hypertrophy"). Further, the group agreed that well-designed <b>calibration</b> <b>sessions</b> were warranted before any future scoring exercises were conducted. The combined efforts of the OMERACT MRI in JIA working group and Health-e-Child included the assessment of currently available material in the literature and determination of the basis from which to start the development of MRI scoring systems for both the knee and wrist. The future research agenda for the knee and wrist will include establishment of MRI scoring systems, an atlas of MR imaging in healthy children, and MRI protocol requisite...|$|R
40|$|A Doctoral Thesis Submitted for the Degree of Doctor of Philosophy. This thesis {{develops}} {{the potential of}} consumer-grade digital cameras for accurate spatial measurement. These cameras are generally considered unstable but their uncertain geometry can be partially resolved by calibration. The validity of calibration data over time should be carefully assessed before subsequent photogrammetric measurement. The use of such digital cameras for photogrammetric measurement is increasingly accepted in many industrial fields but also in a diverse range of fields including medical and forensic science and architectural work. However, the stability of these cameras is less frequently reported in the literature, which {{can be attributed to}} the absence of standards for quantitative analyses of camera stability. The approach used to assess camera stability in this study is based on comparing the accuracy in the reconstructed object space, achieved using sets of interior orientation parameters of a sensor, derived in different <b>calibration</b> <b>sessions.</b> This technique was successfully applied to assess the temporal stability and manufacturing consistency of seven identical Nikon Coolpix 5400 digital cameras. These cameras demonstrated remarkable potential to maintain their internal geometry over a 1 -year period. This study also identified residual systematic error surfaces, discernable in digital elevation models (DEMS) derived from image pairs. These ’domes’ are caused by slightly inaccurately estimated lens distortion parameters. A methodology that uses a mildly convergent image configuration removes the systematic error sources. This result is significant for DEM generation using low-cost digital cameras and a series of case studies demonstrated that this methodology can reduce the need for an accurate lens model and effectively increase the accuracy achievable with non-metric digital sensors...|$|R
