893|3190|Public
5000|$|... #Subtitle level 2: p-adic {{interpretation}} of arithmetic <b>coding</b> <b>algorithm</b> ...|$|E
50|$|MPC-MLQ (Multipulse LPC with Maximum Likelihood Quantization) is {{a speech}} <b>coding</b> <b>algorithm</b> used in G.723.1 speech coding {{standard}} {{working at a}} bit rate of 6.3 kbit/s.|$|E
5000|$|AAC is a {{wideband}} audio <b>coding</b> <b>algorithm</b> that exploits {{two primary}} coding strategies to dramatically {{reduce the amount}} of data needed to represent high-quality digital audio: ...|$|E
40|$|Video <b>coding</b> <b>algorithms</b> are {{important}} for the proliferation of multimedia applications (i. e., those that incorporate video, voice and data) in high speed networks. Video <b>coding</b> <b>algorithms</b> reduce the bandwidth required for video images while maintaining good picture quality. Of particular interest are adaptive video <b>coding</b> <b>algorithms,</b> which can adapt the encoding of a video stream (and thus its bandwidth requirements) dynamically based {{on the amount of}} bandwidth available in the network. Such applications can make efficient use of the available bit rate (ABR) service class proposed for B-ISDN. The goal of this research was to evaluate adaptive video <b>coding</b> <b>algorithms.</b> Therefore there were three main objectives in this research. The first objective was to determine the robustness of adaptive video <b>coding</b> <b>algorithms</b> to changes in network load. The second objective was to identify the domain (network types and load characteristics) in which adaptive video <b>coding</b> <b>algorithms</b> are feasible. T [...] ...|$|R
50|$|Video and Voice <b>coding</b> <b>algorithms.</b>|$|R
5000|$|Technical - Documentation of <b>code,</b> <b>algorithms,</b> interfaces, and APIs.|$|R
5000|$|The MPEG-4 audio <b>coding</b> <b>algorithm</b> family {{spans the}} range from {{low bit rate}} speech {{encoding}} (down to 2 kbit/s) to high-quality audio coding (at 64 kbit/s per channel and higher).|$|E
5000|$|Algebraic code-excited linear {{prediction}} (ACELP) is a patented speech <b>coding</b> <b>algorithm</b> by [...] VoiceAge Corporation {{in which}} a limited set of pulses is distributed as excitation to a linear prediction filter.|$|E
50|$|Code-excited linear {{prediction}} (CELP) is {{a speech}} <b>coding</b> <b>algorithm</b> originally proposed by M. R. Schroeder and B. S. Atal in 1985. At the time, it provided significantly better quality than existing low bit-rate algorithms, such as residual-excited linear prediction and {{linear predictive coding}} vocoders (e.g., FS-1015). Along with its variants, such as algebraic CELP, relaxed CELP, low-delay CELP and vector sum excited linear prediction, it is currently {{the most widely used}} speech <b>coding</b> <b>algorithm.</b> It is also used in MPEG-4 Audio speech coding. CELP is commonly used as a generic term for a class of algorithms and not for a particular codec.|$|E
5000|$|... #Subtitle level 2: An Example of Message Authentication <b>Code</b> <b>Algorithm</b> ...|$|R
30|$|Modern speech <b>coding</b> <b>algorithms</b> {{are based}} on source-filter model, wherein the model {{parameters}} are extracted using linear prediction principles applied in temporal domain [1]. Most popular audio <b>coding</b> <b>algorithms</b> {{are based on}} exploiting psychoacoustic models in the spectral domain [2, 3]. In this work, we explore signal processing methods to code speech and audio signals in a unified approach.|$|R
40|$|This paper {{describes}} {{an overview of}} the MPEG video <b>coding</b> <b>algorithms</b> and standards. MPEG- 1 and MPEG- 2 video <b>coding</b> <b>algorithms</b> are outlined in more detail. MPEG- 4 video coding features and the structure of MPEG- 4 video object plane (VOP) are described. Furthermore the specific properties of the standards related to their applications are presented...|$|R
5000|$|The normal Huffman <b>coding</b> <b>algorithm</b> assigns a {{variable}} length code to every symbol in the alphabet. More frequently used symbols will be assigned a shorter code. For example, suppose {{we have the}} following non-canonical codebook: ...|$|E
50|$|The Gray <b>coding</b> <b>algorithm</b> {{used for}} murray {{integers}} is modified {{from that of}} fixed-based systems. In Cole's version, he replaces di with ri−1−di rather than r−1−di; using the variable radix {{in place of the}} fixed radix.|$|E
50|$|The {{transmitter}} incorporates {{an advanced}} <b>coding</b> <b>algorithm</b> which reduces electromagnetic interference over copper cables and enables robust clock recovery at {{the receiver to}} achieve high skew tolerance for driving longer cables as well as shorter low-cost cables.|$|E
40|$|Background With the {{introduction}} of ICD- 10 throughout Canada, {{it is important to}} ensure that Acute Myocardial Infarction (AMI) comorbidities employed in risk adjustment methods remain valid and robust. Therefore, we developed ICD- 10 <b>coding</b> <b>algorithms</b> for nine AMI comorbidities, examined the validity of the ICD- 10 and ICD- 9 <b>coding</b> <b>algorithms</b> in detection of these comorbidities, and assessed their performance in predicting mortality. The nine comorbidities that we examined were shock, diabetes with complications, congestive heart failure, cancer, cerebrovascular disease, pulmonary edema, acute renal failure, chronic renal failure, and cardiac dysrhythmias. Methods Coders generated a comprehensive list of ICD- 10 codes corresponding to each AMI comorbidity. Physicians independently reviewed and determined the clinical relevance of each item on the list. To ensure that the newly developed ICD- 10 <b>coding</b> <b>algorithms</b> were valid in recording comorbidities, medical charts were reviewed. After assessing ICD- 10 algorithms' validity, both ICD- 10 and ICD- 9 algorithms were applied to a Canadian provincial hospital discharge database to predict in-hospital, 30 -day, and 1 -year mortality. Results Compared to chart review data as a 'criterion standard', ICD- 9 and ICD- 10 data had similar sensitivities (ranging from 7. 1 – 100 %), and specificities (above 93. 6 %) for each of the nine AMI comorbidities studied. The frequencies for the comorbidities were similar between ICD- 9 and ICD- 10 <b>coding</b> <b>algorithms</b> for 49, 861 AMI patients in a Canadian province during 1994 – 2004. The C-statistics for predicting 30 -day and 1 year mortality were the same for ICD- 9 (0. 82) and for ICD- 10 data (0. 81). Conclusion The ICD- 10 <b>coding</b> <b>algorithms</b> developed in this study to define AMI comorbidities performed similarly as past ICD- 9 <b>coding</b> <b>algorithms</b> in detecting conditions and risk-adjustment in our sample. However, the ICD- 10 <b>coding</b> <b>algorithms</b> should be further validated in external databases. </p...|$|R
40|$|Abstract: In this paper, a {{modified}} algorithm for object segmentation of binary images is presented, and denoted as 2 D modified chain <b>code</b> <b>algorithm.</b> The 2 D modified chain <b>code</b> <b>algorithm</b> {{can be applied}} to color images after being binarized. The segmented object is used to derive the chain code in the image. The definition of the 2 D- modified chain <b>code</b> <b>algorithm</b> is valid for shapes composed of triangular, rectangular, and hexagonal cells. The 2 D modified chain code preserves information and allows computing geometric dimension. The results demonstrate that the 2 D modified chain <b>code</b> <b>algorithm</b> could extract the coordinates of the shapes at lower computational cost when compared to the classical chain code. Here, a considerable improvement in accuracy (20. 1 - 57. 2 %) over what is possible with the classical chain code has been achieved at the expense of slight increase in computational cost (10 - 20 %) ...|$|R
50|$|PMAC, {{which stands}} for Parallelizable MAC, is a message {{authentication}} <b>code</b> <b>algorithm.</b> It was created by Phillip Rogaway.|$|R
50|$|Fenwick {{trees are}} {{particularly}} designed {{to implement the}} arithmetic <b>coding</b> <b>algorithm,</b> which maintains counts of each symbol produced and needs to convert those to the cumulative probability of a symbol less than a given symbol. Development of operations it supports were primarily motivated by use in that case.|$|E
50|$|The Navajo I is {{a secure}} {{telephone}} {{built into a}} briefcase that {{was developed by the}} U.S. National Security Agency. According to information on display in 2002 at the NSA's National Cryptologic Museum, 110 units were built in the 1980s for use by senior government officials when traveling. It uses the linear predictive <b>coding</b> <b>algorithm</b> LPC-10 at 2.4 kilobits/second.|$|E
50|$|Huffman coding {{is a more}} {{sophisticated}} technique for constructing variable-length prefix codes. The Huffman <b>coding</b> <b>algorithm</b> takes as input the frequencies that the code words should have, and constructs a prefix code that minimizes the weighted average of the code word lengths. (This {{is closely related to}} minimizing the entropy.) This is a form of lossless data compression based on entropy encoding.|$|E
50|$|A related blob {{encoding}} method is crack <b>code.</b> <b>Algorithms</b> exist to convert between chain code, crack code, and run-length encoding.|$|R
40|$|This paper {{discusses}} a switching method {{which can}} be used to combine two sequential universal source <b>coding</b> <b>algorithms.</b> The switching method treats these two algorithms as black-boxes and can only use their estimates of the probability distributions for the consecutive symbols of the source sequence. Three weighting algorithms based on this switching method are presented. Empirical results show that all three weighting algorithms give a performance better than the performance of the source <b>coding</b> <b>algorithms</b> they combine. ...|$|R
5000|$|The message {{authentication}} <b>code</b> <b>algorithm,</b> e.g. SHA256, {{is used to}} create the message digest, a cryptographic hash of each block of the message stream.|$|R
50|$|Residual-excited linear {{prediction}} (RELP) is an obsolete speech <b>coding</b> <b>algorithm.</b> It {{was originally}} {{proposed in the}} 1970s and {{can be seen as}} an ancestor of code-excited linear prediction (CELP). Unlike CELP however, RELP directly transmits the residual signal. To achieve lower rates, that residual signal is usually down-sampled (e.g. to 1 - 2 kHz). The algorithm is hardly used anymore in audio transmission.|$|E
5000|$|According to {{information}} {{on display in}} 2005 at the NSA's National Cryptologic Museum, the STU-II was in use from the 1980s to the present. It uses the linear predictive <b>coding</b> <b>algorithm</b> LPC-10 at 2.4 kilobits/second to digitize voice, and the [...] "Key Distribution Center" [...] (KDC) for key management. The display also stated that the STU-II B is the standard narrow band secure telephone.|$|E
50|$|The basic {{processing}} {{unit of the}} design is called a macroblock, and H.261 was the first standard in which the macroblock concept appeared. Each macroblock consists of a 16×16 array of luma samples and two corresponding 8×8 arrays of chroma samples, using 4:2:0 sampling and a YCbCr color space. The <b>coding</b> <b>algorithm</b> uses a hybrid of motion-compensated inter-picture prediction and spatial transform coding with scalar quantization, zig-zag scanning and entropy encoding.|$|E
5000|$|Message {{authentication}} <b>code</b> <b>algorithm</b> (...) - The issuer can freely set an algorithm {{to verify}} the signature on the token. However, some supported algorithms are insecure.|$|R
40|$|Embedded {{scalable}} image <b>coding</b> <b>algorithms</b> {{based on}} the wavelet transform have received considerable attention lately in academia and in industry {{in terms of both}} <b>coding</b> <b>algorithms</b> and standards activity. In addition to providing a very good coding performance, the embedded coder has the property that the bit stream can be truncated at any point and still decodes a reasonably good image. In this paper we present some state-of-the-art wavelet-based embedded rate scalable still image coders. In addition, the JPEG 2000 still image compression standard is presented. ...|$|R
40|$|This brief {{presents}} a comprehensive introduction to feature coding, {{which serves as}} a key module for the typical object recognition pipeline. The text offers a rich blend of theory and practice while reflects the recent developments on feature coding, covering the following five aspects: (1) Review the state-of-the-art, analyzing the motivations and mathematical representations of various feature coding methods; (2) Explore how various feature <b>coding</b> <b>algorithms</b> evolve along years; (3) Summarize the main characteristics of typical feature <b>coding</b> <b>algorithms</b> and categorize them accordingly; (4) ...|$|R
50|$|Harmonic Vector Excitation Coding, {{abbreviated}} as HVXC is {{a speech}} <b>coding</b> <b>algorithm</b> specified in MPEG-4 Part 3 (MPEG-4 Audio) standard for very {{low bit rate}} speech coding. HVXC supports bit rates of 2 and 4 kbit/s in the fixed and variable bit rate mode and sampling frequency 8 kHz. It also operates at lower bitrates, such as 1.2 - 1.7 kbit/s, using a variable bit rate technique. The total algorithmic delay for the encoder and decoder is 36 ms.|$|E
50|$|Speex is {{an audio}} {{compression}} format designed for speech {{and also a}} free software speech codec {{that may be used}} on VoIP applications and podcasts. It is based on the CELP speech <b>coding</b> <b>algorithm.</b> Speex claims to be free of any patent restrictions and is licensed under the revised (3-clause) BSD license. It may be used with the Ogg container format or directly transmitted over UDP/RTP. It may also be used with the FLV container format.|$|E
50|$|H.261 was {{originally}} designed for transmission over ISDN lines on which data rates are multiples of 64 kbit/s. The <b>coding</b> <b>algorithm</b> {{was designed to}} be able to operate at video bit rates between 40 kbit/s and 2 Mbit/s. The standard supports two video frame sizes: CIF (352×288 luma with 176×144 chroma) and QCIF (176×144 with 88×72 chroma) using a 4:2:0 sampling scheme. It also has a backward-compatible trick for sending still images with 704×576 luma resolution and 352×288 chroma resolution (which was added in a later revision in 1993).|$|E
40|$|The {{need for}} low {{complexity}} speech <b>coding</b> <b>algorithms</b> has emerged due to application driven requirements. This {{may be attributed}} to the power consumption constraints placed on hand held mobile communication systems and the Electromagnetic Interference emission requirements placed on aU telecommunication products for both home and office use. Electromagnetic Interference emissions are hardware design specific and become prevalent when faster rated hardware is used. Low complexity speech <b>coding</b> <b>algorithms</b> can be implemented on slower DSP processors, thereby making it easier to meet the emission requirements. Slower DSP processors consume less power than faster processors...|$|R
40|$|Implementation of the International Statistical Classification of Diseases and Related Health Problems, Tenth Revision, Canada (ICD- 10 -CA) and the Canadian Classification of Interventions (CCI) {{coding system}} {{presents}} challenges for using Canadian administrative data. Thus, a multistep process {{was conducted to}} develop ICD- 10 -CA/CCI <b>coding</b> <b>algorithms</b> to define nine comorbidities and three procedures. These clinical variables {{have been used in}} ICD- 9 -CM data for risk adjustment in assessment of outcomes after aortic and mitral valve replacement surgery. Among patients included in the ICD- 9 -CM data during 1999 and 2001 and in the ICD- 10 -CA/CCI data during 2002 and 2003 in a Canadian Health Region, frequencies of the nine comorbidities and the three procedures remained generally similar across databases. The newly developed ICD- 10 -CA/CCI and previous ICD- 9 -CM <b>coding</b> <b>algorithms</b> are comparable in detecting these clinical variables. However, performance of ICD- 10 -CA/CCI <b>coding</b> <b>algorithms</b> in risk adjustment should be evaluated in a larger database...|$|R
40|$|The {{switching}} method [4] is {{a scheme}} which combines two universal source <b>coding</b> <b>algorithms.</b> The two universal source <b>coding</b> <b>algorithms</b> both estimate the probability {{distribution of the}} source symbols, and the switching method allows an encoder to choose {{which of the two}} probability distributions it uses for every source symbol. The switching algorithm is an efficient weighting algorithm that uses this switching method. This paper focuses on the companion algorithm, the algorithm running in parallel to the main CTW-algorithm. 1 The switching method: A short introduction The switching method [4] defines a way in which two modeling algorithms can be combined. Consider a source sequence x 1, [...] ., xN. Suppose that two sequential modeling algorithms, A and B, run both along the entire source sequence, and give for every symbol an estimate of its probability distribution. These modeling algorithms could be memoryless estimators, estimators for fixed tree models, or entire universal source <b>coding</b> <b>algorithms</b> on their own. At each moment the encoder in the switching method use...|$|R
