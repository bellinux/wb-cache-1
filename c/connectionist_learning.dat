116|84|Public
50|$|<b>Connectionist</b> <b>Learning</b> with Adaptive Rule Induction On-line (CLARION) is a {{cognitive}} architecture {{that has been}} used to simulate several tasks in cognitive psychology and social psychology, as well as implementing intelligent systems in artificial intelligence applications. An important feature of CLARION is the distinction between implicit and explicit processes and focusing on capturing the interaction between these two types of processes. The system was created by the research group led by Ron Sun.|$|E
50|$|By formalizing {{learning}} {{in such a}} way, connectionists have many tools. A very common strategy in <b>connectionist</b> <b>learning</b> methods is to incorporate gradient descent over an error surface in a space defined by the weight matrix. All gradient descent {{learning in}} connectionist models involves changing each weight by the partial derivative of the error surface {{with respect to the}} weight. Backpropagation (BP), first made popular in the 1980s, is probably the most commonly known connectionist gradient descent algorithm today.|$|E
50|$|She {{hoped that}} the escape from {{the problem of the}} origin of {{semantic}} primitives would lie in either empirical classification procedures operating on actual texts (in the way some now speak of deriving primitives by massive <b>connectionist</b> <b>learning),</b> or by having an adequate formal theory of the structure of thesauri, which she believed to make explicit certain underlying structures of the semantic relations in a natural language: a theory such that “primitives” would emerge naturally as the organizing classification of thesauri. For some years, she and colleagues explored lattice theory as the underlying formal structure of such thesauri.|$|E
5000|$|Computationalists {{in general}} {{focus on the}} {{structure}} of explicit symbols (mental models) and syntactical rules for their internal manipulation, whereas <b>connectionists</b> focus on <b>learning</b> from environmental stimuli and storing this information in a form of connections between neurons.|$|R
40|$|<b>Connectionist</b> network <b>learning</b> {{of context}} free {{languages}} {{has so far}} been applied only to very simple cases and has often made use of an external stack. Learning complex context free languages with a homogeneous neural mechanism looks like a much harder problem. The current paper takes a step toward solving this problem by analyzing context free grammar computation (without addressing learning) in a class of analog computers called Dynamical Automata, which are naturally implemented in connectionist networks. The result is a widely applicable method of using fractal sets to organize infinite state computations in a bounded state space. An appealing consquence is the development of parameter-space maps which locate various complex computers in spatial relationships to one another. An example suggests that such a global perspective on the organization of the parameter space may be helpful for solving the hard problem of getting <b>connectionist</b> networks to <b>learn</b> complex grammars from exam [...] ...|$|R
40|$|Systematicity, {{the ability}} to {{represent}} and process structurally related objects, is a significant and pervasive property of cognitive behaviour, and clearly evident in language. In the case of <b>Connectionist</b> models that <b>learn</b> from examples, systematicity is generalization over examples sharing a common structure. Althoug...|$|R
40|$|The goal of neural-symbolic {{computation}} is {{to integrate}} ro-bust <b>connectionist</b> <b>learning</b> and sound symbolic reasoning. With the {{recent advances in}} <b>connectionist</b> <b>learning,</b> in par-ticular deep neural networks, forms of representation learn-ing have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar. 1...|$|E
40|$|Understanding how the {{structure}} of community interactions is modified by coevolution is vital for understanding system responses to change at all scales. However, in absence of a group selection process, collective community behaviours cannot be organised or adapted in a Darwinian sense. An open question thus persists: are there alternative organising principles that enable us to understand how coevolution of component species creates complex collective behaviours exhibited at the community level? We address this issue using principles from <b>connectionist</b> <b>learning,</b> a discipline with well-developed theories of emergent behaviours in simple networks. We identify conditions where selection on ecological interactions is equivalent to 'unsupervised learning' (a simple type of <b>connectionist</b> <b>learning)</b> and observe that this enables communities to self organize without community-level selection. Despite not being a Darwinian unit, ecological communities can behave like <b>connectionist</b> <b>learning</b> systems, creating internal organisation that habituates to past environmental conditions and actively recalling those conditions. Comment: 37 pages 11 figure...|$|E
40|$|<b>Connectionist</b> <b>learning</b> {{methods for}} {{distributed}} feedforward networks typically result in "catastrophic forgetting. " That is, as new information is learned, old information is rapidly obliterated. This {{contrasts with the}} much more gradual forgetting observed in human memory, thus casting doubt on the cognitive plausibility of <b>connectionist</b> <b>learning</b> methods. It has been shown previously that paired neural networks that are "cross-trained" in a rather intricate manner can address this problem. Here, we show that a simpler single-network approach that makes use only of noise passing through the network can also significantly reduce catastrophic interference. We speculate {{that this kind of}} method might be involved in human learning...|$|E
40|$|Reasoning {{about the}} past is of {{fundamental}} importance in several applications in computer science and artificial intelligence, including reactive systems and planning. In this paper we propose efficient temporal knowledge representation algorithms to reason about and implement past time logical operators in neural-symbolic systems. We do so by extending models of the <b>Connectionist</b> Inductive <b>Learning</b> and Logic Programming System with past operators. This contributes towards integrated learning and reasoning systems considering temporal aspects. We validate the effectiveness of our approach by means of case studies. ...|$|R
40|$|This paper {{introduces}} {{the idea that}} conceptual clustering can be performed using <b>connectionist</b> competitive <b>learning.</b> Competitive learning is used to detect clusters of objects and their corresponding (qualitative) descriptions. A genetic algorithm is employed to choose a subset of these descriptions such that the objects matching them form partitions over the population of objects concerned. Hierarchical classification trees are built by recursing the above two steps (competitive learning 'clustering' and genetic algorithm 'partitioning') over the objects matching the descriptions at each node. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|R
40|$|An {{instance-based}} learning system is presented. SC-net is a fuzzy hybrid <b>connectionist,</b> symbolic <b>learning</b> system. It remembers some examples and makes groups of examples into exemplars. All real-valued attributes are represented as fuzzy sets. The network representation and learning method is described. To illustrate {{this approach to}} learning in fuzzy domains, an example of segmenting magnetic resonance images of the brain is discussed. Clearly, the boundaries between human tissues are ill-defined or fuzzy. Example fuzzy rules for recognition are generated. Segmentations are presented that provide results that radiologists find useful...|$|R
40|$|We {{contribute}} to a taxonomy of data with respect to appropriate selection of machine learning methods. Artificial data sets are carefully designed to expose the biases in a symbolic learning method and two <b>connectionist</b> <b>learning</b> methods. The data sets {{are based on the}} p-type and s-type classifications used in the past. Earlier results with these data sets are confirmed. New results indicate that irrelevant attributes are generally bad for <b>connectionist</b> <b>learning</b> systems while noise in the data set is generally bad for symbolic learning methods, even when using data which is normally well suited to the respective learning method. A third result shows that fewer training examples is a more serious problem for symbolic methods than connectionist methods...|$|E
40|$|Sentence {{production}} {{is the process}} we use to create language-specific sentences that convey particular meanings. In production, there are complex interactions between meaning, words, and syntax {{at different points in}} sentences. Computational models can make these interactions explicit and <b>connectionist</b> <b>learning</b> algorithms have been useful for building such models. Connectionist models use domaingeneral mechanisms to learn internal representations and these mechanisms can also explain evidence of long-term syntactic adaptation in adult speakers. This paper will review work showing that these models can generalize words in novel ways and learn typologically-different languages like English and Japanese. It will also present modeling work which shows that <b>connectionist</b> <b>learning</b> algorithms can account for complex sentence production in children and adult production phenomena like structural priming, heavy NP shift, and conceptual/lexical accessibility...|$|E
40|$|Background: The {{structure}} and organisation of ecological interactions within an ecosystem is modified by the evolution and coevolution {{of the individual}} species it contains. Understanding how historical conditions have shaped this architecture is vital for understanding system responses to change at scales from the microbial upwards. However, {{in the absence of}} a group selection process, the collective behaviours and ecosystem functions exhibited by the whole community cannot be organised or adapted in a Darwinian sense. A long-standing open question thus persists: Are there alternative organising principles that enable us to understand and predict how the coevolution of the component species creates and maintains complex collective behaviours exhibited by the ecosystem as a whole? Results: Here we answer this question by incorporating principles from <b>connectionist</b> <b>learning,</b> a previously unrelated discipline already using well-developed theories on how emergent behaviours arise in simple networks. Specifically, we show conditions where natural selection on ecological interactions is functionally equivalent to a simple type of <b>connectionist</b> <b>learning,</b> ‘unsupervised learning’, well-known in neural-network models of cognitive systems to produce many non-trivial collective behaviours. Accordingly, we find that a community can self-organise in a well-defined and non-trivial sense without selection at the community level; its organisation can be conditioned by past experience in the same sense as <b>connectionist</b> <b>learning</b> models habituate to stimuli. This conditioning drive...|$|E
40|$|This paper {{describes}} a general architecture SCAN for hybrid symbolic connectionist processing of natural language phrases. SCAN's architecture shows how <b>learned</b> <b>connectionist</b> domain-dependent semantic representations {{can be combined}} with encoded symbolic syntactic representations. Within this general architecture we focus on a connectionist model for semantic classification based on a scanning understanding of phrases. We specify strategies at the top-most theory level and we show how these strategies are realized in a recurrent connectionist plausibility network at the underlying representation level. In particular, this model demonstrates that a recurrent <b>connectionist</b> network can <b>learn</b> a semantic memory model for phrase classification based on a scanning understanding. 1 Introduction In the past {{the debate about the}} use of symbolic versus connectionist representations has shown strong arguments for both perspectives. From a strictly symbolic perspective, connectionist representatio [...] ...|$|R
40|$|A new {{adaptive}} {{anomaly detection}} framework, {{based on the}} use of unsupervised evolving connectionist systems, is proposed {{to address the issue of}} concept drift. It is designed to adapt to normal behavior changes while still recognizing anomalies. The evolving <b>connectionist</b> systems <b>learn</b> a subject’s behavior in an online, adaptive fashion without a priori knowledge of the underlying data distributions. Experiments with the KDD Cup 1999 network data and the Windows NT user profiling data show that our adaptive anomaly detection systems, based on Fuzzy Adaptive Resonance Theory (ART) and Evolving Fuzzy Neural Networks (EFuNN), can significantly reduce the false alarm rate while the attack detection rate remains high...|$|R
40|$|The {{overwhelming}} majority of research currently pursued {{within the framework of}} concept-learning concentrates on discrimination-based learning, an inductive learning paradigm that relies on both examples and counter-examples of the concept. This emphasis, however, can present a practical problem: there are real-world engineering problems for which counter-examples are both scarce and difficult to gather. For these problems, recognition-based learning systems are much more appropriate because they do not use counter-examples in the conceptlearning phase. The purpose of this dissertation is to analyze a <b>connectionist</b> recognition-based <b>learning</b> system [...] -autoassociation-based classification [...] -and answer the following questions: ffl What features of the autoassociator make it ca [...] ...|$|R
40|$|The {{success of}} a connectionist model of {{cognitive}} development on the balance scale task is due to manipulations which impede convergence of the backpropagation learning algorithm. The model was trained {{at different levels of}} a biased training environment with exposure to a varied number of training instances. The effects of weight updating method and modifying the network topology were also examined. In all cases in which these manipulations caused a decrease in convergence rate, there was an increase in the proportion of psychologically realistic runs. We conclude that incremental <b>connectionist</b> <b>learning</b> is not sufficient for producing psychologically successful connectionist balance scale models, but must be accompanied by a slowing of convergence. Introduction <b>Connectionist</b> <b>learning</b> algorithms have successfully acted as transition mechanisms in a number of recent models of cognitive developmental phenomena. McClelland (1988) suggested that gradual, incremental error reduction is a [...] ...|$|E
40|$|The present paper compares {{stochastic}} learning (Hidden Markov Models), symbolic learning (Inductive Logic Programming), and <b>connectionist</b> <b>learning</b> (Simple Recurrent Networks using backpropagation) on a single, linguistically {{fairly simple}} task, that of learning enough phonotactics to distinguish words from non-words for a simplified set of Dutch, the monosyllables. The methods are all tested using 10 % reserved data {{as well as}} a comparable number of randomly generated strings. Orthographic and phonetic representations are compared. The results indicate that while stochastic and symbolic methods have little difficulty with the task, connectionist methods do. 1 Introduction This paper describes a study of the application of various learning methods for recognizing the structure of monosyllabic words. The learning methods we compare are taken from three paradigms: stochastic learning (Hidden Markov Models), symbolic learning (Inductive Logic Programming), and <b>connectionist</b> <b>learning</b> (Simpl [...] ...|$|E
40|$|Essentially {{all work}} in <b>connectionist</b> <b>learning</b> {{up to now}} has been {{induction}} from examples (e. g. Hinton 1987), but instruction is as important in symbolic artificial intelligence (e. g. Mostow 1986, Rychener 1986) {{as it is in}} nature. This paper describes an implemented <b>connectionist</b> <b>learning</b> system that transforms an instruction expressed in a description language into an input for a connectionist knowledge representation system, which in turn changes the network in order to integrate new knowledge. Integration is always important when a single new fact causes changes in several parts of the knowledge-base; it is an adjustment which cannot easily be done with learningby -example techniques only. The new, integrated knowledge can be used in conjunction with prior knowledge. The learning method used is recruitment learning, a technique which converts network units from a pool of free units into units which carry meaningful information, i. e. represent generic concepts. 1. Introduction. Th [...] ...|$|E
40|$|This paper {{describes}} Rapture [...] {{a system}} for revising probabilistic knowledge bases that combines <b>connectionist</b> and symbolic <b>learning</b> methods. Rapture uses {{a modified version of}} backpropagation to refine the certainty factors of a probabilistic rule base and it uses ID 3 's information-gain heuristic to add new rules. Results on refining three actual expert knowledge bases demonstrate that this combined approach generally performs better than previous methods...|$|R
40|$|In {{this paper}} {{we take a}} <b>connectionist</b> machine <b>learning</b> {{approach}} {{to the problem of}} metre perception and learning in musical signals. We present a hybrid network consisting of a nonlinear oscillator network and a recurrent neural network. The oscillator network acts as an entrained resonant filter to the musical signal. It ‘perceives’ metre by resonating to the inherent frequencies within the signal. The neural network learns the long-term temporal structures present in the signal. We show that our hybrid network outperforms previous approaches of a single layer recurrent neural network in melody prediction tasks. By perceiving metrical structure, our system is enabled to model more coherent long-term structures, and {{can be used in a}} multitude of analytic and generative scenarios, including live performance applications...|$|R
40|$|This {{purpose of}} this paper is to {{identify}} implications of various learning theories for workplace learning and performance and HRD. It begins with a review of various theoretical positions on learning including behaviorism, Gestalt theory, cognitive theory, schema theory, <b>connectionist</b> theory, social <b>learning</b> or behavior modeling, social perspective theory, and situated cognition theory. Implications are drawn from these various theories. A theoretical framework is then constructed incorporating these implications, along with suggestions for needed research...|$|R
40|$|Abstract. Current work on connectionist models {{has been}} focused largely on arti�cial neural {{networks}} that are inspired by the networks of biological neurons in the human brain. However, there are also other connectionist architectures that differ significantly from this biological exemplar. We proposed a novel <b>connectionist</b> <b>learning</b> architecture inspired by the physics associated with optical coatings of multiple layers of thin-films in a previous paper (Li and Purvis 1999, Annals of Mathematics and Artificial Intelligence, 26 : 1 – 4). The proposed model differs significantly from the widely used neuron-inspired models. With thin-film layer thicknesses serving as adjustable parameters (as compared with connection weights in a neural network) for the learning system, the optical thin-�lm multilayer model (OTFM) is capable of approximating virtually any kind of highly nonlinear mappings. The OTFM is not a physical implementation using optical devices. Instead, it is proposed as a new <b>connectionist</b> <b>learning</b> architecture with its distinct optical properties as compared with neural networks. In this paper we focus on a detailed compariso...|$|E
40|$|Current work on connectionist models {{has been}} focused largely on {{artificial}} neural networks that are inspired by the networks of biological neurons in the human brain. However there are also other connectionist architectures that differ significantly from this biological exemplar. We proposed a novel <b>connectionist</b> <b>learning</b> architecture inspired by the physics associated with optical coatings of multiple layers of thin-films in a previous paper [1]. The proposed model differs significantly from the widely used neuron-inspired models. With thin-film layer thicknesses serving as adjustable parameters (as compared with connection weights in a neural network) for the learning system, the optical thin-film multilayer model (OTFM) is capable of approximating virtually any kind of highly nonlinear mappings. In this paper we focus on a detailed comparison of a typical neural network model and the OTFM. We describe {{the architecture of the}} OTFM and show how it {{can be viewed as a}} <b>connectionist</b> <b>learning</b> model. We then present the experimental results of using the OTFM in solving a classification problem typical of conventional connectionist architectures. ...|$|E
40|$|Standard {{generative}} linguistic theory, {{which uses}} discrete symbolic models of cognition, has some strengths and weaknesses. It is strong on providing {{a network of}} outposts that make scientific travel in the jungles of natural language feasible. It is weak in that it currently depends on the elaborate and unformalized use of intuition to develop critical supporting assumptions about each data point. In this regard, {{it is not in}} a position to characterize natural language systems in the lawful terms that ecological psychologists strive for. <b>Connectionist</b> <b>learning</b> models offer some help: They define lawful relations between linguistic environments and language systems. But our understanding of them is currently weak, especially when it comes to natural language syntax. Fortunately, symbolic linguistic analysis can help connectionism if the two meet via dynamical systems theory. I discuss a case in point: Insights from linguistic explorations of natural language syntax appear to have identified information structures that are particularly relevant to understanding ecologically appealing but analytically mysterious <b>connectionist</b> <b>learning</b> models. This article is concerned with the relation between discrete, symbolic systems of th...|$|E
40|$|Keywords : Control {{of dynamic}} processes, Identification, Connectionist modelling, MultiLayer Perceptrons In this report, we {{investigate}} {{the application of}} connectionist techniques to identification and process control. After a general presentation, we focus on several aspects, mainly: problems related to the application of connectionist networks to the particular field of identification and control, and characteristics and specificities of numerical <b>learning</b> techniques and <b>connectionist</b> networks, as compared to classical techniques. We then classify and present these techniques in four categories: first those concerning identification, then <b>connectionist</b> controller <b>learning</b> through an existing controller, direct identification of the neural controller, and indirect learning approaches. In {{the end of each}} section we present some explanatory applications of the methods presented. Partially supported by the Neuronet Project CEC Network of Excellence N o 8961 2 C. Goutte and C. Led [...] ...|$|R
40|$|AI and <b>connectionist</b> {{approaches}} to <b>learning</b> from examples differ in knowledge-base representation and inductive mechanisms. To explore these differences we experiment {{with a system}} from each paradigm: 1 D 3 and back-propagation. We compare the systems {{on the basis of}} both prediction accuracy and length of training. The systems show distinct performance differences across a variety of domains. We identify aspects of each system that may account for these performance differences. Finally, we suggest paths for cross-paradigm interaction. ...|$|R
40|$|A {{new field}} of {{children}} 2 ̆ 7 s learning is emerging. This new field differs from the old in recognizing that children 2 ̆ 7 s learning includes active as well as passive mechanisms and qualitative as well as quantitative changes. Children 2 ̆ 7 s learning involves substantial variability of representations and strategies within individual {{children as well as}} across different children. The path of learning involves the introduction of new approaches as well as changes in the frequency of prior ones. The rate and the breadth of learning tend to occur at a human scale, intermediate between the extremes depicted by symbolic and <b>connectionist</b> models. <b>Learning</b> has many sources; one that is particularly promising for educational purposes is self-explanations. Overall, contemporary analyses show that learning and development have a great deal in common...|$|R
40|$|Background: The {{structure}} and organisation of ecological interactions within an ecosystem is modified by the evolution and coevolution {{of the individual}} species it contains. Understanding how historical conditions have shaped this architecture is vital for understanding system responses to change at scales from the microbial upwards. However, {{in the absence of}} a group selection process, the collective behaviours and ecosystem functions exhibited by the whole community cannot be organised or adapted in a Darwinian sense. A long-standing open question thus persists: Are there alternative organising principles that enable us to understand and predict how the coevolution of the component species creates and maintains complex collective behaviours exhibited by the ecosystem as a whole? Results: Here we answer this question by incorporating principles from <b>connectionist</b> <b>learning,</b> a previously unrelated discipline already using well-developed theories on how emergent behaviours arise in simple networks. Specifically, we show conditions where natural selection on ecological interactions is functionally equivalent to a simple type of <b>connectionist</b> <b>learning,</b> ‘unsupervised learning’, well-known in neural-network models of cognitive systems to produce many non-trivial collective behaviours. Accordingly, we find that a community can self-organise in a well-defined and non-trivial sense without selection at the community level; its organisation can be conditioned by past experience in the same sense as <b>connectionist</b> <b>learning</b> models habituate to stimuli. This conditioning drives the community to form a distributed ecological memory of multiple past states, causing the community to: a) converge to these states from any random initial composition; b) accurately restore historical compositions from small fragments; c) recover a state composition following disturbance; and d) to correctly classify ambiguous initial compositions according to their similarity to learned compositions. We examine how the formation of alternative stable states alters the community’s response to changing environmental forcing, and we identify conditions under which the ecosystem exhibits hysteresis with potential for catastrophic regime shifts. Conclusions: This work highlights the potential of connectionist theory to expand our understanding of evo-eco dynamics and collective ecological behaviours. Within this framework we find that, despite not being a Darwinian unit, ecological communities can behave like <b>connectionist</b> <b>learning</b> systems, creating internal conditions that habituate to past environmental conditions and actively recalling those conditions. Theoretical ecology, Community assembly, Network structures, Ecological memory, Associative learning, Regime shifts, Community matrix *Correspondence...|$|E
40|$|Previous {{work has}} shown that <b>connectionist</b> <b>learning</b> systems can {{simulate}} {{important aspects of the}} categorization of speech sounds by human and animal listeners. Training is on representations of synthetic, exemplar voiced and unvoiced stop consonants passed through a computational model of the auditory periphery. In this work, we use the modern inductive inference technique of support vector machines (SVMs) as the learning system. Visualization of the SVM's weight vector reveals what has been learned about the voiced/unvoiced distinction...|$|E
40|$|Although the {{detection}} of invariant structure in a given set of input patterns is vital to many recognition tasks, <b>connectionist</b> <b>learning</b> rules {{tend to focus on}} directions of high variance (principal components). The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule. An unsupervised two-layer network implementing this method in a competitive setting learns to extract coherent depth information from random-dot stereograms. ...|$|E
40|$|Multicausal abductive tasks {{appear to}} have {{deliberate}} and implicit components: people generate and modify explanations using a series of recognizable steps, but these steps appear to be guided by an implicit hypothesis evaluation process. This paper proposes a hybrid symbolic-connectionist learning architecture for multicausal abduction. The architecture tightly integrates a symbolic Soar model for generating and modifying hypotheses with Echo, a connectionist model for evaluating hypotheses. The symbolic component uses knowledge compilation to quickly acquire general rules for generating and modifying hypotheses, and for making decisions based on the current best explanation. The <b>connectionist</b> component <b>learns</b> to provide better hypothesis evaluation by implicitly acquiring explanatory strengths based on the frequencies of events during problem solving. 1. Introduction Abduction {{is the process of}} generating a best explanation for a set of observations. Symbolic models of abductive rea [...] ...|$|R
40|$|Overfitting is a {{well-known}} problem {{in the fields of}} symbolic and <b>connectionist</b> machine <b>learning.</b> It describes the deterioration of generalisation performance of a trained model. In this paper, we investigate the ability of a novel artificial neural network, bp-som, to avoid overfitting. bp-som is a hybrid neural network which combines a multi-layered feed-forward network (mfn) with Kohonen's self-organising maps (soms). During training, supervised back-propagation learning and unsupervised som learning cooperate in finding adequate hidden-layer representations. We show that bp-som outperforms standard backpropagation, and also back-propagation with a weight decay when dealing with the problem of overfitting. In addition, we show that bp-som succeeds in preserving generalisation performance under hidden-unit pruning, where both other methods fail. 1 On avoiding overfitting In machine-learning research, the performance of a trained model is often expressed in its generalisation perfo [...] ...|$|R
40|$|The {{majority}} or the <b>connectionist</b> {{theories of}} <b>learning</b> {{are based on}} the Hebbian Learning Rule (Hebb, 1949). According to this rule, connections between neurons presenting correlated activity are strengthened. <b>Connectionist</b> theories of <b>learning</b> are essentially abstract implementations of general features of brain plasticity in architectures of artificial neural networks. Theoretical Background Connectionism provides a framework (Rumelhart, Hinton, & McClelland, 1986) for the study of cognition using Artificial Neural Network models. Neural network models are architectures of simple processing units (artificial neurons) interconnected via weighted connections. An artificial neuron functions as a detector, which produces an output activation value determined by the level of the total input activation and an activation function. As a result, when a neural network is exposed to an environment, encoded as activation patterns in the input units of the network, it responds with activation patterns across the units. In the connectionist framework an artificial neural network model depicts cognition when it is able to respond to its environment with meaningful activation patterns. This can be achieved by modifications of the values of the connection weights, so as to regulate the activation patterns in the network appropriately. Therefore, connectionism suggests that learning involves the shaping of the connection weights. A learnin...|$|R
