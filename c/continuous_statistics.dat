15|43|Public
40|$|We {{consider}} the many-particle quantum mechanics of anyons, i. e. identical particles in two space dimensions with a <b>continuous</b> <b>statistics</b> parameter α∈ [0, 1] ranging from bosons (α= 0) to fermions (α= 1). We prove a (magnetic) Hardy inequality for anyons, {{which in the}} case that α is an odd numerator fraction implies a local exclusion principle for the kinetic energy of such anyons. From this result, and motivated by Dyson and Lenard's original approach to the stability of fermionic matter in three dimensions, we prove a Lieb-Thirring inequality for these types of anyons. Comment: Corrected and accepted version. 30 pages, 4 figure...|$|E
40|$|Many {{data sets}} are sampled on regular lattices in two, {{three or more}} dimensions, and recent work has shown that {{statistical}} properties of these data sets {{must take into account}} the continuity of the underlying physical phenomena. However, the effects of quantization on the statistics have not yet been accounted for. This paper therefore reconciles the previous papers to the underlying mathematical theory, develops a mathematical model of quantized statistics of continuous functions, and proves convergence of geometric approximations to <b>continuous</b> <b>statistics</b> for regular sampling lattices. In addition, the computational cost of various approaches is considered, and recommendations made about when to use each type of statistic...|$|E
40|$|Today’s {{processors}} {{provide a}} rich source of statistical information on program execution characteristics through hardware counters. However, traditionally, operating system (OS) support for and utilization of the hardware counter statistics has been limited and ad hoc. In this paper, we make the case for direct OS management of hardware counter statistics. First, we show the utility of processor counter statistics in CPU scheduling (for improved performance and fairness) and in online workload modeling, both of which require online <b>continuous</b> <b>statistics</b> (as opposed to ad hoc infrequent uses). Second, we show that simultaneous system and user use of hardware counters is possible via time-division multiplexing. Finally, we highlight potential counter misuses to indicate that the OS should address potential security issues in utilizing processor counter statistics. ...|$|E
40|$|Abstract A {{definition}} for the statistical {{significance of a}} signal in an experiment is proposed by establishing a correlation between the observed p-value and the normal distribution integral probability, which is suitable for both counting experiment and <b>continuous</b> test <b>statistics.</b> The explicit expressions to calculate the statistical significance for both cases are given...|$|R
2500|$|... (At {{the end of}} 2013, {{the project}} moved from SourceForge to GitHub, and <b>continuous</b> {{download}} <b>statistics</b> have not been publicly available since then, but in March 2015 a press release stated that MuseScore had been downloaded over eight million times, and in December 2016 the project stated that version 2.0.3 had been downloaded 1.9 million times in the nine months since its release.) ...|$|R
40|$|We {{argue that}} the concept of wavefunction {{reduction}} should be introduced to standard quantum mechanics in any physical processes where effective reduction of wavefunction occurs, {{as well as in the}} measurement processes. With this, we make a conjecture that particles obey some generalized statistics that contains the quantum and classical statistics as special cases, where the degree of overlapping determines the statistics that particles should obey among <b>continuous</b> generalized <b>statistics.</b> We present an example consistent with the conjecture...|$|R
40|$|We {{consider}} {{the problem of}} active steganography, wherein {{the goal is to}} survive benign attacks in addition to maintaining statistical and perceptual transparency. The stego image can be “advertised” in uncompressed format, but they can survive JPEG compression upto a certain design quality factor while still resisting statistical ste-ganalysis. The data is hidden in the discrete cosine transform (DCT) coefficients using the statistical restoration framework: a fraction of the coefficients are used for hiding and the rest are used for restoring the statistics. In order to advertise the images in uncompressed for-mat, we must restore the unquantized or <b>continuous</b> <b>statistics.</b> This paper extends the statistical restoration framework so as to make only integer perturbations to the pixel values while modifying the transform coefficients, thus matching their histogram computed us-ing very small bin-width. We present numerical results confirming the applicability of the presented technique. Index Terms — Steganography, steganalysis, fractional bin-width, optimal pixel perturbations, statistical restoratio...|$|E
40|$|We {{propose a}} simple yet {{efficient}} steganalytic algorithm for watermarks embedded by two state-of-the-art 3 D watermarking algorithms by Cho et al. The main observation {{is that while}} in a clean model the means/variances of Cho et al. ’s normalized histogram bins are expected to follow a Gaussian distribution, in a marked model their distribution will be bimodal. The proposed algorithm estimates the number of bins through an exhaustive search and then {{the presence of a}} watermark is decided by a tailor made normality test or a t-test. We also propose a modification of Cho et al. ’s watermarking algorithms with the watermark embedded by changing the histogram of the radial coordinates of the vertices. Rather than targeting a <b>continuous</b> <b>statistics</b> such as the mean or variance of the values in a bin, the proposed watermarking modifies a discrete statistic, which here is the height of the histogram bin, to achieve watermark embedding. Experimental results demonstrate that the modified algorithm offers not only better resistance against the steganalytic attack we developed, but also an improved robustness/capacity trade-off...|$|E
40|$|This study {{describes}} the land information systems, their evolution, {{and the market}} for land information in Finland. We describe the availability and quality of price information both in urban (residential, office and industrial markets) and rural markets (arable and forest land). In addition to pure land, we also describe what statistics exist concerning of different forms of real estate. In addition to official statistics, we also made a small survey to the earlier Finnish land price studies. Although {{a lot of other}} type of information has been available for long periods of time, <b>continuous</b> <b>statistics</b> on land and real estate prices with national coverage have only been available from the first half of 1980 s. Available statistics are used to describe price and rent developments in different markets. As for land and real estate, changes in their real prices have been remarkable even in international comparison. The boom at the end of 1980 s and the bust thereafter can be seen in all price series. Land Prices, Real Estate Markets, Finland...|$|E
40|$|The false {{discovery}} rate (FDR) is {{a widely}} used error measure in multiple testing. Adaptive FDR procedures, incorporating a conservative estimator of the proportion of true null hypotheses, are usually more powerful than their non-adaptive counterparts. However, their performances depend critically on the test <b>statistics</b> having <b>continuous</b> distributions with very restrictive dependencies. Recent genomic sequencing technologies such as microarray and next-generation sequencing have generated massive data sets from which statistical analyses produce discrete test <b>statistics</b> and <b>continuous</b> test <b>statistics</b> bearing unknown and strong dependency. These two features {{have been known to}} have adverse effects on existing FDR procedures. Towards this end, better adaptive FDR procedures for discrete and for dependent <b>continuous</b> test <b>statistics</b> are needed. For multiple testing based on p-values from discrete test statistics whose null distributions dominate the uniform distribution, new estimators of the proportion of true null hypotheses and of the FDRs of one-step multiple testing procedures (MTPs) are proposed. Further, to better estimate the FDRs of one-step MTPs when testing normal means under dependence, a (uniformly) consistent estimator of the proportion of nonzero normal means, when the test statistics follow a joint normal distribution whose known covariance matrix represents strong dependencies of a certain type, has been developed. Improved performances of these new estimators have been justied both theoretically and empirically. ...|$|R
40|$|Given {{a finite}} {{collection}} of continuous semimartingales, we derive a semimartingale decomposition of the corresponding ranked (order-statistics) processes. We apply the decomposition {{to extend the}} theory of equity portfolios generated by ranked market weights to the case where the stock values admit triple points. Portfolios Portfolio generating functions <b>Continuous</b> semimartingales Order <b>statistics</b> Local times...|$|R
40|$|A {{method is}} given for calculating the strict {{minimum message length}} (SMML) {{estimator}} for 1 -dimensional exponential families with <b>continuous</b> sufficient <b>statistics.</b> A set of $n$ equations are found that the $n$ cut-points of the SMML estimator must satisfy. These equations can be solved using Newton's method and this approach is used to produce new results and to replicate results that C. S. Wallace obtained using his boundary rules for the SMML estimator. A rigorous proof is also given that, despite being composed of step functions, the posterior probability corresponding to the SMML estimator is a continuous function of the data. Comment: 10 pages, 2 tables and 1 figur...|$|R
40|$|The {{defining}} {{properties of}} party identification long {{established for the}} United States fail with some frequency to be replicated in electoral systems abroad. A number of plausible suggestions {{have been made to}} account for this system-level variability: Most of these have some face merit, but none taken alone is adequate to provide a full cross-system explanation. Variation in party system size or fractionalization has recently been discussed as another source of differential dynamics of party loyalties. Unfortunately, the conventional means of assessing party identification properties are subject to rather severe artifacts, typically ignored, when comparisons are made across systems of very different party size. The conceptual stakes underlying key methods options for such comparisons—most notably, between continuous and discrete statistical tools—are examined. The use of <b>continuous</b> <b>statistics</b> for systems of very multiple parties rests on an assumption that voters do in some degree regard these party systems as imbedded in a continuous space. A simple test for this assumption is mounted in four systems and unsurprisingly it shows very clear support. Analysis of residuals beyond this obvious result add several points of less obvious information about the distribution of party affect in such systems...|$|E
40|$|Abstract. This paper {{describes}} a real-time event-based system to distribute and analyze high-velocity sensor {{data collected from}} a soccer game case study used in the DEBS 2013 Grand Challenge. Our approach uses the OMG Data Distribution Service (DDS) for data dissemination and we combine it with algorithms to provide the necessary real-time analytics. We implemented the system using the Real-Time Innovations (RTI) Connext TM DDS implementation, which provides a novel platform for Quality-of-Service (QoS) -aware distribution of data and real-time event processing. We evaluated latency and update rates {{of one of the}} queries in our solution to show the scalability and benefits of configurable QoS provided by DDS. 1 DEBS Grand Challenge Problem The ACM International Conference on Distributed Event-based Systems (DEBS) Grand Challenge problem comprises real-life data and queries in event-based systems. The goal of the DEBS 2013 Grand Challenge is to implement an event-based system for real-time, complex event processing of high velocity sensor data collected from a soccer game [1]. The real-time analytics for the DEBS Grand Challenge comprises <b>continuous</b> <b>statistics</b> gained by processing raw sensor data such as running analysis, ball possession, heat map, and shot on goal. Contemporary approaches to gathering data for soccer matches utilizes a complex system of 1...|$|E
40|$|Versión final disponible en [URL] study {{cell count}} moments up to fifth {{order of the}} {{distributions}} of haloes, of halo substructures {{as a proxy for}} galaxies, and of mass {{in the context of the}} halo model and compare theoretical predictions to the results of numerical simulations. On scales larger than the size of the largest cluster, we present a simple point cluster model in which results depend only on cluster–cluster correlations and on the distribution of the number of objects within a cluster, or cluster occupancy. The point cluster model leads to expressions for moments of galaxy counts in which the volume-averaged moments on large scales approach those of the halo distribution and on smaller scales exhibit hierarchical clustering with amplitudes Sk determined by moments of the occupancy distribution. In this limit, the halo model predictions are purely combinatoric, and have no dependence on halo profile, concentration parameter or potential asphericity. The full halo model introduces only two additional effects: on large scales, haloes of different mass have different clustering strengths, introducing relative bias parameters; and on the smallest scales, halo structure is resolved and details of the halo profile become important, introducing shape-dependent form factors. Because of differences between discrete and <b>continuous</b> <b>statistics,</b> the hierarchical amplitudes for galaxies and for mass behave differently on small scales even if galaxy number is exactly proportional to mass, a difference that is not necessarily well described in terms of bias...|$|E
40|$|The {{concept of}} wavefunction {{reduction}} should {{be introduced to}} standard quantum mechanics in any physical processes where effective reduction of wavefunction occurs, {{as well as in}} the measurement processes. When the overlap is negligible, each particle obey Maxwell-Boltzmann even if the particles are in principle described by totally symmetrized wavefunction [P. R. Holland, The Quantum Theory of Motion, Cambridge University Press, 1993, p 293]. We generalize the conjecture. That is, particles obey some generalized statistics that contains the quantum and classical statistics as special cases, where the level of overlapping determines the statistics that particles should obey among <b>continuous</b> generalized <b>statistics.</b> We present an example consistent with the conjecture...|$|R
40|$|Background The Swedish Dementia Registry (SveDem) was {{developed}} {{with the aim}} {{to improve the quality}} of diagnostic work-up, treatment and care of patients with dementia disorders in Sweden. Methods SveDem is an internet based quality registry where several indicators can be followed over time. It includes information about the diagnostic work-up, medical treatment and communi-ty support (www. svedem. se). The patients are diagnosed and followed-up yearly in special-ist units, primary care centres or in nursing homes. Results The database was initiated in May 2007 and covers almost all of Sweden. There were 28 722 patients registered with a mean age of 79. 3 years during 2007 – 2012. Each participating unit obtains <b>continuous</b> online <b>statistics</b> from its own registrations and they can be com...|$|R
40|$|The {{minimum message length}} {{principle}} is an information theoretic criterion that links data compression with statistical inference. This paper studies the strict minimum message length (SMML) estimator for d-dimensional exponential families with <b>continuous</b> sufficient <b>statistics,</b> for all d > 1. The partition of an SMML estimator is shown to consist of convex polytopes (i. e. convex polygons when d= 2) which can be described explicitly {{in terms of the}} assertions and coding probabilities. While this result is known, we give a new proof based on the calculus of variations, and this approach gives some interesting new inequalities for SMML estimators. We also use this result to construct an SMML estimator for a 2 -dimensional normal random variable with known variance and a normal prior on its mean. Comment: Revised to include new insights and result...|$|R
40|$|We {{present a}} simple and {{accurate}} method to constrain galaxy bias based {{on the distribution of}} counts in cells. The most unique feature of our technique is that it is applicable to non-linear scales, where both dark matter statistics and the nature of galaxy bias are fairly complex. First, we estimate the underlying continuous distribution function from precise counts-in-cells measurements assuming local Poisson sampling. Then a robust, non-parametric inversion of the bias function is recovered from the comparison of the cumulative distributions in simulated dark matter and galaxy catalogs. Obtaining <b>continuous</b> <b>statistics</b> from the discrete counts is the most delicate novel part of our recipe. It corresponds to a deconvolution of a (Poisson) kernel. For this we present two alternatives: a model independent algorithm based on Richardson-Lucy iteration, and a solution using a parametric skewed lognormal model. We find that the latter is an excellent approximation for the dark matter distribution, but the model independent iterative procedure is more suitable for galaxies. Tests based on high resolution dark matter simulations and corresponding mock galaxy catalogs show that we can reconstruct the non-linear bias function down to highly non-linear scales with high precision in the range of - 1 <δ< 5. As far as the stochasticity of the bias, we have found a remarkably simple and accurate formula based on Poisson noise, which provides an excellent approximation for the scatter around the mean non-linear bias function. In addition we have found that redshift distortions have a negligible effect on our bias reconstruction, therefore our recipe can be safely applied to redshift surveys. Comment: 32 pages, 18 figures; submitted to Ap...|$|E
40|$|We {{measure the}} amount of {{absorption}} in the Lyman-alpha forest at 0 < z < 1. 6 in HST FOS spectra of 74 QSOs. At 0 < z < 1. 6 we find that 79 % of the absorption is from the low density intergalactic medium, 12 % from metals and 9 % from the strong H I lines, nearly identical to the percentages (78, 15 and 7) that we measured independently at z= 2 from spectra taken with the Kast spectrograph on the Lick 3 -m. At z= 1 the low density intergalactic medium absorbs 0. 037 +/- 0. 004 of the flux. The error includes some {{but not all of}} the uncertainty in the continuum level. The remaining part gives relative errors of approximately 0. 21 when we report the mean absorption in eight independent redshift intervals, and 0. 047 when we average over all redshifts. We find 1. 46 times more absorption from the low density intergalactic medium than comes from Ly-alpha lines that Bechtold et al. 2002 listed in the same spectra. The amount of absorption increases with z and can be fit by a power law in (1 +z) with index 1. 01. This corresponds to no change in the number of lines, of fixed rest frame equivalent widths, per unit redshift, consistent with the Janknecht et al. 2006 results on the distribution of lines. When we include similar measurements from higher redshifts, we need more degrees of freedom to fit {{the amount of}} absorption at 0 < z < 3. 2. A power law with a break in slope, changing from index 1. 5 at low z to 3. 0 above z ~ 1. 1 is a better but only marginally acceptable fit. We also calculate two other <b>continuous</b> <b>statistics,</b> the flux probability distribution function and the flux autocorrelation function that is non zero out to v ~ 500 km/sec at 0. 5 < z < 1. 5. Comment: 12 pages, submitted to MNRA...|$|E
40|$|From a climatological {{point of}} view, a winter day usually is symbolized with cold weather or a {{snowfall}} day. The {{first and last}} days of snowfall may be considered as winters’ real starting and finishing points. Starting and finishing points of seasons are affected by sun location, which is fixed yearly whereas snowfalls usually begin with cooling and they may happen in any time. Cooling has several reasons, {{the most important of}} which may be approaching and locating of polar vortex trough or isolated cut-off low systems from polar vortex on a region. The current research aims at identifying the role of polar vortex in creating cooling necessary for Iranian first and last snowfalls and finding a way for estimating its occurrence time. Firstly, snowfall data of all synoptic stations is collected from Islamic Republic of Iran Meteorological Organization (IRIMO) in 3 -hourly basis. After statistical review, snowfall data related to 38 stations which have at least 33 years <b>continuous</b> <b>statistics</b> from 1976 to 2008 is considered proper in order {{to be used in the}} research. Then, after determining cold and warm peak times in summers and winters and basing them, the first and last days occur snowfall after warm and cold peak in each year have been respectively identified first and last snowfalls. The first and last snowfall days (calendar series) changed into Julian days. Using Smada 6. 0 software, resulted series have been fitted by different statistical distributions (for example normal, 2 parameter log normal, 3 parameter log normal, pearson type 3, log pearson type 3 and gumball distributions) and most proper distributions have been recognized. Ultimately, using those distributions, occurrence probability of snowfall days has been calculated. Geopotential height data for those selected days is collected from National Center for Environmental Program and National Center for Atmospheric Research (NCEP/NCAR). Using Grads software, related maps have been drawn. Resulted maps have been analyzed through the using of Skew-T Log P diagrams obtained from University of Wyoming. In most of the cases, Iranian first and last snowfall days happen because of cooling resulted from locating of polar vortex troughs or their isolated cut-off low systems on Northwestern of Iran or its neighborhood. Using astrological calendar and getting assistance from probability rules, can be estimate Iranian first and last snowfall occurrence times. </p...|$|E
40|$|By {{explicitly}} {{identifying a}} basis valid {{for any number}} of electrons, we demonstrate that simple multi-quasihole wavefunctions for the ν = 1 / 2 Pfaffian paired Hall state exhibit an exponential degeneracy at fixed positions. Indeed, we conjecture that for 2 n quasiholes the states realize a spinor representation of an expanded (<b>continuous)</b> nonabelian <b>statistics</b> group SO(2 n). In the four quasihole case, this is supported by an explicit calculation of the corresponding conformal blocks in the c = 1 2 + 1 conformal field theory. We present an argument for the universality of this result, which is significant for the foundations of fractional statistics generally. We note, for annular geometry, an amusing analogue to black hole entropy. We predict, as a generic consequence, glassy behavior. Many of our considerations also apply to a form of the (3, 3, 1) state. ...|$|R
40|$|We develop {{statistical}} methods which allow effective visual detection, categorization, and tracking {{of objects in}} complex scenes. Such computer vision systems must be robust to wide variations in object appearance, the often small size of training databases, and ambiguities induced by articulated or partially occluded objects. Graphical models provide a powerful framework for encoding the statistical structure of visual scenes, and developing corresponding learning and inference algorithms. In this thesis, we describe several models which integrate graphical representations with nonparametric {{statistical methods}}. This approach leads to inference algorithms which tractably recover high– dimensional, continuous object pose variations, and learning procedures which transfer knowledge among related recognition tasks. Motivated by visual tracking problems, we first develop a nonparametric extension of the belief propagation (BP) algorithm. Using Monte Carlo methods, we provide general procedures for recursively updating particle–based approximations of <b>continuous</b> sufficient <b>statistics.</b> Efficient multiscale sampling methods then allow this nonparametri...|$|R
40|$|We {{present a}} minimal {{parameter}} kinetic model which describes the corrosion and passivation ofmetal surfaces. Two elementary steps are included, namely, the oxidation of surface metal atoms to produce adsorbed cations, and the subse-quent dissolution of these cations into the electrolyte. Oxide layer formation {{is represented by}} two-dimensional condensa-tion of adsorbed cations to produce a <b>continuous</b> phase; <b>statistics</b> of the condensation process are treated at the mean field level. The structure and magnitude of the i-V curve in the active-passive transition region is calculated for two cases: the limit of maximum passivation hysteresis (pinodal decomposition f the oxide phase), and the limit of zero hysteresis (equil-ibrated phase transition). Methods for analyzing experimental i-V curves to yield the rate constants associated with the model are presented. Passivation {{plays a crucial role}} in the corrosion re-sistance and electrochemical behavior of many metals. The remarkable nature of the passivation effect is shown most clearly by polarization curves (i-V plots) of the active-passive transition region, which reveal that the oxidatmn current passes through a maximu...|$|R
40|$|To {{elucidate}} {{the causes and}} mechanisms of twinning and higher multifetal maternities, we {{have taken advantage of}} the statistical sources of Sweden, where <b>continuous</b> <b>statistics</b> for the whole population are the oldest available. We found strong secular and regional fluctuations. The rates of multiple maternities were the highest during the last three decades of the 18 th century, when the twinning rate was more than 17 per 1, 000, the triplet rate was more than 3 per 10, 000, and the quadruplet rate was almost 7 per 1 million maternities. During 1849 – 1873 the twinning rate in Sweden was 14. 2 per 1, 000, but this rate showed great regional differences, being 18. 0 per 1, 000 on the island of Gotland and 12. 6 per 1, 000 in the county ofA¨ lvsborg. During this period the twinning rate in the countryside in the county of Stockholm was 20. 4, but in the city of Stockholm it was only 14. 1 per 1, 000. In Sweden after the 1930 s there was a marked decrease in the twinning rate, which by the 1960 s had fallen to only about half of what it had been two centuries earlier. The corresponding reductions for triplet and quadruplet rates were about 75 %. The aim of this paper was to study the temporal and regional variations in multiple maternities in Sweden from 1751 to 1960 based on demographic and some socioeconomic data for the counties. We confirmed our earlier studies that maternal age and parity cannot satisfactorily explain the secular and regional differences in the twinning rates. In contrast to studies in France (1901 – 1968), we found no unequivocal association between the twinning rates and the crude birth rates. The correlation coefficients between the twinning rate and the crude birth rate showed statistically significant regional and temporal variations. After eliminating the temporal trends, regional differences in the correlation coefficients remained. The twinning rates for the counties seem to converge toward a common low level, 10 – 12 per 1, 000. The observed convergence toward relatively similar levels may be caused by the increased matrimonial migration distances and decreased endogamy of the citizens as a consequence of better communications. The increased urbanization and industrialization that started in the last decades of the 19 th century broke up the old static agrarian isolates and caused Sweden, within 2 – 3 generations, to develop from a poor nation {{to one of the most}} prosperous in the world. A more urban and affluent lifestyle, a better diet, and increased stress and sedentary occupations may have reduced the physical capacity of mothers to carry gestations with multiple embryos or fetuses to completion...|$|E
40|$|This {{study was}} {{carried in the}} period from 17 / 3 / 2003 to 16 / 7 / 2003 (four months duration) in Khartoum {{teaching}} hospital(K. T. H.), in the orthopaedics and traumatology department. The study objectives were to know the actual rate of postoperative wound infection(POWI), to know the pattern of its presentation and to know the effect of some risk factors in the causation of POWI. An initial questionnaire was filled to all patients who had clean elective orthopaedic procedure during the above mentioned period. The questionnaire was filled {{just prior to the}} performance of the procedure. A second questionnaire was filled for each patient each time he came for follow-up. During this period 319 patients to whom 329 clean elective procedures were performed, were included in the study. Two patients were known to die during the follow up period. From the rest, we were able to follow 153 patients (46. 1 %) [...] Follow up ranged from 9 days to 105 day, with a mean of 42. 7 days and standard deviation of 22. 38. For the patients who were followed up females were 37 (24. 2 %) and males were 116 (75. 8 %). Age ranged from 0. 5 year to 85 years, with the mean age of 35. 3 years, and standard deviation of 21. 7. Half of the patient (49. 7 %) were below 30 years. Femur was the site of operation of almost half of the patients (49. 0 %) and fractures accounted for (79. 7 %). Postoperative wound infection was found in 9 out of 153 patients (5. 9 %). The superficial infection was found in 4 patients (2. 6 %) superficial and deep in 5 patients (3. 3 %). All the patients with superficial infection and one with deep and superficial infection had complete resolution. But 4 out of 5 patients with deep and superficial infection ended in chronic osteomylitis. POWI was found {{to be related to the}} type of operation (P= 0. 004). Plating was a significant factor in causing POWI. Four (4) out of five (5) who had deep POWI, had the operation of plate and screw. The 5 th patients had DHS, which is actually a variant of plate and screw. POWI was also found to be related to the duration of operation(P= 0. 008). All operations that developed deep POWI took more than one hour. Out of the nine patients with postoperative wound infection, cultures were done to six patients. All patients with deep and superficial infection had cultures. Staphylococcus aureus was isolated from four (4) patients, streptococcus group B from one, and, coliform bacteria from the other. The study recommended urgent intervention to reduce this significant high rate of deep POWI, special attention to plating which was in particular associated with POWI and one antibiotic policy to all units. We also recommended a system of <b>continuous</b> <b>statistics</b> concerning POWI...|$|E
40|$|Copulas {{are shown}} in this paper to provide an {{effective}} strategy to describe the statistical dependence between peak flow discharge and flood volume featuring hydrographs forcing a flood control reservoir. A 52 year time series of flow discharges observed in the Panaro River (Northern Italian Apennines) is used to fit an event-based bivariate distribution and to support time-continuous modeling of a flood control reservoir, located online along the river system. With regard to reservoir performances, a method aimed at estimating the bivariate return period is analytically developed, by exploiting the derived distribution theory and a simplified routing scheme. In this approach, the return period {{is that of the}} peak flow discharge released downstream from the reservoir. Therefore, in order to verify the reliability of the proposed method, a nonparametric version of its frequency distribution is assessed by means of <b>continuous</b> simulation <b>statistics.</b> Copula derived and nonparametric distributions of the downstream peak flow discharge are found to be in satisfactory agreement. Finally, a comparison of bivariate return period estimates carried out by using alternative approaches is illustrated...|$|R
40|$|A {{probability}} density function (p. d. f.) f(x; θ) with known parameter θ (or parameters θ =(θ 1, [...] .,θn)) {{enables us to}} predict {{the frequency with which}} random data x will take on a particular value (if discrete) or lie in a given range (if <b>continuous).</b> In <b>statistics</b> we are concerned with the inverse problem, that of making inferences about the parameters from observed data. There are two main approaches to statistical inference, which we may call frequentist and Bayesian. In frequentist statistics, probability is interpreted as the frequency of the outcome of a repeatable experiment. Estimators are used to measure values for unknown parameters, and confidence intervals can be constructed which contain the unknown true value of a parameter with a specified probability. Statistical tests can be constructed which, depending on the outcome of the experiment, accept or reject hypotheses. One does not, however, define a probability for a parameter θ, which is treated as a constant whose value may be unknown. In Bayesian statistics, the interpretation of probability is more general and includes degree of belief. One can then speak of a p. d. f. for a parameter θ, which expresse...|$|R
40|$|Techniques are {{described}} which attempt to synthesise world events based on 15, 000 news reports culled from the Internet each day, and {{to detect and}} visualise Breaking News Stories derived from changes in Flux and clustering analysis Statistical Indicators for individual countries have been defined, and software written which calculates these thematic indicators for all world countries. This work builds on the Europe Media Monitor (EMM) in use by the European Commission ([URL]). EMM has invented a powerful method of rapidly classifying multilingual articles by matching weighted combinations of phrase and word patterns. The articles are cross-classified according to the countries mentioned and to general themes like "Conflict" "FoodSecurity" "Natural Disaster" "Ecology" and so on. The Alert system runs 24 hours per day keeping <b>continuous</b> hourly <b>statistics</b> on article populations. This allows SOTW to provide real-time graphical presentations of "news maps" time series of Indicators, and animations of crisis developments. More detailed geolocation of news based on Town names and Gazetteers, coupled with a "Top News" clustering algorithm results in 24 hourly News distributions. This technique also links clusters through to different language versions and back through time. Geospatial representation of news developments enables decision makers to better sumariseJRC. G. 2 -Support to external securit...|$|R
40|$|Background: The Swedish Dementia Registry (SveDem) was {{developed}} {{with the aim}} {{to improve the quality}} of diagnostic work-up, treatment and care of patients with dementia disorders in Sweden. Methods: SveDem is an internet based quality registry where several indicators can be followed over time. It includes information about the diagnostic work-up, medical treatment and community support (www. svedem. se). The patients are diagnosed and followed-up yearly in specialist units, primary care centres or in nursing homes. Results: The database was initiated in May 2007 and covers almost all of Sweden. There were 28 722 patients registered with a mean age of 79. 3 years during 2007 - 2012. Each participating unit obtains <b>continuous</b> online <b>statistics</b> from its own registrations and they can be compared with regional and national data. A report from SveDem is published yearly to informmedical and care professionals as well as political and administrative decision-makers about the current quality of diagnostics, treatment and care of patients with dementia disorders in Sweden. Conclusion: SveDem provides knowledge about current dementia care in Sweden and serves as a framework for ensuring the quality of diagnostics, treatment and care across the country. It also reflects changes in quality dementia care over time. Data from SveDem can be used to further develop the national guidelines for dementia and to generate new research hypotheses...|$|R
40|$|We {{demonstrate}} that the exact form-factors of the Calogero-Sutherland model which were recently found in [10] in confirmation of the congectures earlier made by Haldane (Ref. [9]) can be reproduced {{in the framework of}} some bosonization procedure in momentum space. This observation implies a possibility of an exact bosonization of this model describing one-dimensional anyons. 1 The Calogero-Sutherland (CS) model [1] of one-dimensional (1 D) spinless particles with long-ranged 1 /x 2 interaction (V (x) = π 2 λ(λ− 1) L 2 (sin πx L) 2 in a finite size L system) provides a remarkable example of an exactly solvable system which {{can be viewed as a}} continuous deformation away from either ideal Fermi or Bose gas. At special values λ = 1 / 2, 1 and 2 the original <b>continuous</b> model describes <b>statistics</b> of eigenvalues of random matrices belonging to one of the three Dyson’s ensembles. Recentl...|$|R
40|$|In this paper, {{the largest}} and the {{smallest}} observations are considered, {{at the time when}} a new record of either kind (upper or lower) occurs based on a sequence of independent random variables with identical <b>continuous</b> distributions. These <b>statistics</b> are referred to as current upper and lower records, respectively, in the statistical literature. We derive expressions for the Pitman closeness of current records to a common population parameter and then apply these results to location-scale families of distributions with a special emphasis on the estimation of quantiles. In the case of symmetric distributions, we show that this criterion possesses some symmetry properties. Exact expressions are derived for the Pitman closeness probabilities in the case of Uniform (- 1, 1) and exponential distributions. Moreover, for the population median, we show that the Pitman closeness probability is distribution-free. Current records Location-scale family Pitman closeness Pitman closer estimator Quantiles...|$|R
30|$|Based on an {{expected}} prevalence of confirmed infection of 75 %, we initially planned to enroll 40 patients (resulting {{in a total}} of 30 patients with confirmed infection). However, recruitment was stopped at 39 cases because of a higher th{{an expected}} incidence of cases with confirmed infections. The general characteristics of the study participants were summarized using descriptive <b>statistics.</b> <b>Continuous</b> data were expressed as medians and ranges and compared with the Mann-Whitney U test. Categorical variables were summarized as frequency counts and percentages. A 2 [*]×[*] 2 contingency table with four diagnostic outcomes (true positive, false positive, true negative, and false negative) was constructed based on the final diagnostic results. Receiver operating characteristic (ROC) curve analysis was applied to investigate the prediction accuracy. Optimal cut-off points that maximized prediction were identified using the Youden’s index. All calculations were performed using the SPSS software (version 22.0; IBM, Armonk, NY, USA). A two-tailed p value <[*] 0.05 was considered statistically significant.|$|R
40|$|In {{this paper}} we exploit the {{principle}} of maximum entropy to gain insight into the process underlying the internal dynamics of a stock market. We first introduce a simplified physical model, the ideally liquid stock, to describe market price evolution and derive an operational definition of price volatility in non-stationary conditions. Using this model we perform {{an analysis of the}} information entropy which we compare with market data of stocks traded on the Italian stock exchange in Milan. This leads us to identify constraints to a procedure of entropy maximization and to describe the bulk of the volatility distribution on very general grounds. The nature of the two constraints we find is discussed and related to information available to all market participants. Finally, a stochastic process able to reproduce these findings is presented and the origin of the two constraints is clarified. Bayesian analysis, Bayesian <b>statistics,</b> <b>Continuous</b> time dynamic finance, Dynamic models, Econophysics,...|$|R
40|$|Statistical {{learning}} – {{implicit learning}} of statistical regularities within sensory input – {{is a way}} of acquiring structure within <b>continuous</b> sensory environments. <b>Statistics</b> computation, initially shown to be involved in word segmentation, has been demonstrated to be a general mechanism that operates across domains, across time and space, and across species. Recently, statistical leaning has been reported to be present even at birth when newborns were tested with a speech stream. The aim {{of the present study was}} to extend this finding, by investigating whether newborns’ ability to extract statistics operates in multiple modalities, as found for older infants and adults. Using the habituation procedure, two experiments were carried out in which visual sequences were presented. Results demonstrate that statistical learning is a general mechanism that extracts statistics across domain since the onset of sensory experience. Intriguingly, present data reveal that newborn learner’s limited cognitive resources constrain the functioning of statistical learning, narrowing the range of what can be learned...|$|R
40|$|In this paper, we {{exploit the}} theory of dense graph limits to provide a new {{framework}} to study the stability of graph partitioning methods, which we call structural consistency. Both stability under perturbation as well as asymptotic consistency (i. e., convergence with probability $ 1 $ as the sample size goes to infinity under a fixed probability model) follow from our notion of structural consistency. By formulating structural consistency as a continuity result on the graphon space, we obtain robust results that are completely independent of the data generating mechanism. In particular, our results apply in settings where observations are not independent, thereby significantly generalizing the common probabilistic approach where data {{are assumed to be}} i. i. d. In order to make precise the notion of structural consistency of graph partitioning, we begin by extending {{the theory of}} graph limits to include vertex colored graphons. We then define <b>continuous</b> node-level <b>statistics</b> and prove that graph partitioning based on such statistics is consistent. Finally, we derive the structural consistency of commonly used clustering algorithms in a general model-free setting. These include clustering based on local graph statistics such as homomorphism densities, as well as the popular spectral clustering using the normalized Laplacian. We posit that proving the continuity of clustering algorithms in the graph limit topology can stand on its own as a more robust form of model-free consistency. We also believe that the mathematical framework developed in this paper goes beyond the study of clustering algorithms, and will guide the development of similar model-free frameworks to analyze other procedures in the broader mathematical sciences. Comment: 33 page...|$|R
