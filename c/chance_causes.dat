12|227|Public
50|$|The major {{developer}} of Stoicism, Chrysippus, took {{the edge off}} strict necessity. Like Democritus, Aristotle, and Epicurus before him, he wanted to strengthen the argument for moral responsibility, in particular defending it from Aristotle's and Epicurus' indeterminate <b>chance</b> <b>causes.</b>|$|E
5000|$|Fisher {{reiterated the}} p = 0.05 {{threshold}} and explained its rationale, stating: It is usual and convenient for experimenters to take 5 per cent {{as a standard}} level of significance, {{in the sense that}} they are prepared to ignore all results which fail to reach this standard, and, by this means, to eliminate from further discussion the greater part of the fluctuations which <b>chance</b> <b>causes</b> have introduced into their experimental results. He also applies this threshold to the design of experiments, noting that had only 6 cups been presented (3 of each), a perfect classification would have only yielded a p-value of [...] which would not have met this level of significance. Fisher also underlined the interpretation of p, as the long-run proportion of values at least as extreme as the data, assuming the null hypothesis is true.|$|E
40|$|Firstly, we {{deal with}} the {{verification}} of normality and other necessary prerequisites needed in this thesis. We also introduce transformations to converse non-normally distributed data to normal and continue with capability analysis. We describe the design of control charts, useful tools to assess process stability. They help us to eliminate assignable causes and leave only <b>chance</b> <b>causes</b> in process. We obtain process in control state. Finally, we introduce both capability and performance ratios for both normal and non-normal data, and analyse some of their properties. At the end of the thesis, we prove acquired knowledge by performing capability analysis of real process...|$|E
5000|$|When traveling, ride your train respectfully, take no {{personal}} <b>chances,</b> <b>cause</b> {{no problems with}} the operating crew or host railroad, act like an extra crew member.|$|R
40|$|Concurrent {{bilateral}} keratoconus and granular dystrophy {{is reported}} in a 32 year old patient with decreased vision. Initially contact lenses were attempted unsuccessfully to treat the conditions. There {{are a handful of}} other reports of these combined pathologies in the literature, and the likelihood of a <b>chance</b> <b>cause</b> or possible genetic linkage between the conditions is discussed...|$|R
50|$|In 2013 he {{was signed}} by the Super Rugby side the Blues. He earned his first Super Rugby start in the Blues {{historic}} 21-28 loss over the Bulls in round four of the competition. Gibson-Park was {{also a part of}} the memorable Blues team to face France, getting his <b>chance</b> <b>cause</b> of injury with All Blacks halfback Piri Weepu.|$|R
40|$|In this paper, {{we argue}} that {{parallels}} can be drawn between information and knowledge systems security (IKSS) concerns of today’s organizations and quality management concerns of early twentieth century organizations. We propose a literature review of some Total Quality Management (TQM) and Lean Management (LM) solutions that may be relevant for information and knowledge systems security. This hybridization of IKSS and TQM/LM domains leads to propose innovative solutions for IKSS, notably by considering assignable causes and <b>chance</b> <b>causes</b> of security failures within organizations. Even if this work is still at an early stage, our approach suggests a shift from “security assurance” to “security control” within information and knowledge systems (IKS) ...|$|E
40|$|Abstract — This paper {{presents}} {{a method for}} analyzing the variations of the RGB spectrum of lesion skin images using the novel segmentation process based on Six Sigma concept. This analysis further contemplates on the incidence and propagation of cancer. It {{is based on the}} underlying principles of Dr. W. A. Shewhart’s Control Charts, which focuses {{on the fact that the}} variability does exist in all repetitive processes. The heterogeneous color variation within the skin is considered as an assignable cause and is due to the secretion of excess melanin. These variations possess greater magnitude as compared to the <b>chance</b> <b>causes</b> due to the color variations found in normal skins. The power of control chart that lies in its ability to separate out this assignable cause, which is one of inherent mnemonics of Malignant Melanoma, is exhibited. The proposed Six Sigma based segmentation identifies the normal skin region from the regions of lesions, besides its fuzzy border. Results show that the method produces a robust segmentation of regions of high color contrast...|$|E
40|$|Control charts {{are widely}} used in {{statistical}} process control to detect changes in a production process and for monitoring a process {{to make sure that}} it is in control. In conventional statistical process control, the pattern of <b>chance</b> <b>causes</b> is often assumed to follow normal distribution. It is well known that the assumption of normality or any specific parametric form for the process distribution is too restrictive. In such situations, distribution-free or nonparametric control charts can serve the purpose better. In this paper, distribution-free control charts are developed based on a class of one-sample test statistics with sub-samples of size two due to Mehra, Prasad and Madhava Rao (Austral. J. Statist. 32 : 373392, 1990). The control charts based on their statistic (-chart) are easy to understand and to use. The performance of the proposed procedures is studied through the average run length, which is the expected number of samples required by the procedure to signal out of control. It is observed that the performance of the proposed chart is better than the existing charts in the literature...|$|E
5000|$|Walter A. Shewhart {{originally}} {{used the}} term <b>chance</b> <b>cause.</b> The term common cause was coined by Harry Alpert in 1947. The Western Electric Company {{used the term}} natural pattern. [...] Shewhart called a process that features only common-cause variation as being in statistical control. This term is deprecated by some modern statisticians who prefer the phrase stable and predictable.|$|R
50|$|Quitting while {{ahead is}} unlikely, though, since a gambler who is winning has little {{incentive}} to quit, and is instead encouraged to continue to gamble by their winning. Once {{in the throes of}} a winning streak, the individual may even become convinced that it is their skill, rather than <b>chance,</b> <b>causing</b> their winnings, or good luck on their side, and thus it seems especially senseless to stop.|$|R
5000|$|Less <b>chance</b> of {{artifacts}} <b>caused</b> by parrafin embedding or freezing ...|$|R
40|$|Control {{limits are}} one of the main {{elements}} of control charts. Generally speaking, a control chart is the graphical display of a statistic regarding the quality characteristic of interest, computed from a sample randomly drawn from a process at different time instances. As natural variability is always present in a process, we expect some variability on the control chart. Excessive variability, owing to special cause events, is referred to as being due to an assignable cause. Otherwise, when only <b>chance</b> <b>causes</b> – also called common causes of variation – are operating, the process is said to be in statistical control. In order {{to make a decision about}} the status of the process, control limits are typically positioned so that under the hypothesis of no deviation in the process, a type I probability error corresponds to an economically acceptable average run length. In this article, we discuss in a general manner how to compute control limits, and give some remarks on how limits should be computed so as to reduce false out of control signals...|$|E
30|$|Statistical {{process control}} (SPC) widely employs various process {{monitoring}} charts for determining whether the process under consideration is performing within the specified limits are not. Process monitoring charts gives a graphical {{description of the}} process performance and it instantly helps the process personnel to differentiate <b>chance</b> <b>causes</b> from assignable causes. Process capability indices are the measure of efficiency of the process to produce the product within the specified dimensional tolerance limits. To be specific, CP is the process potential capability index and CPK is the process performance capability index. CP gives {{a measure of the}} variation and deviation in the process. The higher the CP value, the less the variation and deviation in the process. CPK, on the other hand, is obtained from CPKU and CPKL. As a precaution of safety, the lower value among the CPKU and CPKL is considered to be the value of CPK. Elaborating on this, {{it can be said that}} centering of the process within the specification limits is done by CPK. It provides an indication whether the process is operating at the center of the specified tolerance zone or nearer to the upper or lower specification limits.|$|E
40|$|Graduation date: 1987 In {{production}} processes, {{there are}} two types of variations that affect production quality - - variations produced by <b>chance</b> <b>causes</b> and variations produced by assignable causes. One of the main instruments in quality control used to control quality by distinguishing between variations produced by <b>chance</b> <b>causes</b> and a real process change is the control chart. Each type of control chart has advantages and disadvantages in a specified situation. For example, some control charts fail to detect small shifts, while the others are ineffective to detect large shifts in process mean. In this study, three types of control charts, namely, X[bar], cumulative sum, and geometric moving average control charts were compared on an economic basis. A simulation model was developed to simulate the control chart functions in a typical production process. The simulation was executed in BASIC on an IBM PC/XT. Before comparison, each control chart was matched so that all the control charts have the same characteristics when the process operates in-control for a certain period of time. The effects of the type of control chart, sample size, sampling interval, and the magnitude of shift in process mean on profit per hour were observed and analyzed using Analysis of Variance (ANOVA). The results show that, in general, the cumulative sum control chart has advantage over the other two types of control charts when shift of small magnitude of about 0. 5 σ is present. X[bar]-control chart is ineffective to detect small shifts; however, its effectiveness increases sharply as the magnitude of shift increases to values of 1. 5 σ or beyond. Geometric moving average control chart gives best results at intermediate shift levels of about l. 0 σ. Of the three sample sizes (3, 4 and 5) used in this study, sample size of five yields the highest profit per hour. However, too large a sample size may result in a decrease of profit per hour if the testing causes the destruction of items and the cost of sampling per item is very high. Small sampling interval of one hour yields the highest profit per hour among three sampling intervals (1, 2 and 4 hours) used in this study. Too small sampling interval could yield lower profit per hour if the increased cost of more frequent sampling, more investigations caused by false alarms, and more frequent shut down of the production process exceeds the savings from early detection of the shift, particularly, when the cost of sampling, the cost of searching for an assignable cause, and the income per hour of production are very high...|$|E
50|$|Consider this example: {{your doctor}} knows that a surgery has a 5% <b>chance</b> of <b>causing</b> {{complete}} paralysis {{but does not}} tell you. Without the operation, however, you will very likely die within 12 months.|$|R
50|$|In {{the second}} {{portion of the}} series, Dudley takes the nieces for a world tour on a rented leisure boat, giving Dinky a <b>chance</b> to <b>cause</b> mishap on the {{different}} continents for more adventures.|$|R
50|$|Women {{who agree}} to be {{screened}} have their breasts X-rayed on a specialized X-ray machine. This exposes the woman's breasts to {{a small amount of}} ionizing radiation, which has a very small, but non-zero, <b>chance</b> of <b>causing</b> cancer.|$|R
40|$|The {{outset of}} new technologies, systems and {{applications}} in manufacturing sector {{has no doubt}} lighten up our workload, yet the <b>chance</b> <b>causes</b> of variation in production system cannot be eliminated completely. Every produced/ordered lot may have some fraction of defectives which may vary from process to process. In addition the situation is more susceptible when the items are deteriorating in nature. However, the defective items can be secluded from the good quality lot through a careful inspection process. Thus, a screening process is obligatory in today’s technology driven industry which has the customer satisfaction as its only motto. Moreover, {{in order to survive}} in the current global markets, credit financing has been proven a very influential promotional tool to attract new customers and a good inducement policy for the retailers. Keeping this scenario in mind, the present paper investigates an inventory model for a retailer dealing with imperfect quality deteriorating items under permissible delay in payments. Shortages are allowed and fully backlogged. This model jointly optimizes the order quantity and shortages by maximizing the expected total profit. A mathematical model is developed to depict this scenario. Results have been validated with the help of numerical example. Comprehensive sensitivity analysis has also been presented...|$|E
40|$|Quality {{decisions}} {{are one of}} the major decisions in inventory management. It affects customer’s demand, loyalty and customer satisfaction and also inventory costs. Every manufacturing process is inherent to have some <b>chance</b> <b>causes</b> of variation which may lead to some defectives in the lot. So, in order to cater the customers with faultless products, an inspection process is inevitable, which may also be prone to errors. Thus for an operations manager, maintaining the quality of the lot and the screening process becomes a challenging task, when his objective is to determine the optimal order quantity for the inventory system. Besides these operational tasks, the goal is also to increase the customer base which eventually leads to higher profits. So, as a promotional tool, trade credit is being offered by both the retailer and supplier to their respective customers to encourage more frequent and higher volume purchases. Thus taking into account of these facts, a strategic production model is formulated here to study the combined effects of imperfect quality items, faulty inspection process, rework process, sales return under two level trade credit. The present study is a general framework for many articles and classical EPQ model. An analytical method is employed which jointly optimizes the retailer’s credit period and order quantity, so as to maximize the expected total profit per unit time. To study the behavior and application of the model, a numerical example has been cited and a comprehensive sensitivity analysis has been performed. The model can be widely applicable in manufacturing industries like textile, footwear, plastics, electronics, furniture etc...|$|E
40|$|To produce {{products}} with consistent quality, manufacturing processes {{need to be}} closely monitored for any deviations in the process. Proper analysis of control charts {{that are used to}} determine the state of the process not only requires a thorough knowledge and understanding of the underlying distribution theories associated with control charts, but also the experience of an expert in decision making. The present work proposes a modified backpropagation neural network methodology to identify and interpret various patterns of variations that can occur in a manufacturing process. Control charts primarily in the form of X-bar chart are widely used to identify the situations when control actions will be needed for manufacturing systems. Various types of patterns are observed in control charts. Identification of these control chart patterns (CCPs) can provide clues to potential quality problems in the manufacturing process. Each type of control chart pattern has its own geometric shape and various related features can represent this shape. This project formulates Shewhart mean (X-bar) and range (R) control charts for diagnosis and interpretation by artificial neural networks. Neural networks are trained to discriminate between samples from probability distributions considered within control limits and those which have shifted in both location and variance. Neural networks are also trained to recognize samples and predict future points from processes which exhibit long term or cyclical drift. The advantages and disadvantages of neural control charts compared to traditional statistical process control are iscussed. In processes, the causes of variations may be categorized as chance (unassignable) causes and special (assignable) causes. The variations due to <b>chance</b> <b>causes</b> are inevitable, and difficult to detect and identify. On the other hand, the variations due to special causes prevent the process being a stable and predictable. Such variations should be determined effectively and eliminated from the process by taking the necessary corrective actions to maintain the process in control and improve the quality of the products as well. In this study, a multilayered neural network trained with a back propagation algorithm was applied to pattern recognition on control charts. The neural network was experimented on a set of generated data...|$|E
40|$|An {{improved}} Jones {{sample splitter}} is described which is faster to operate than the conventional type, yet requires no special skill or training. Samples obtained with this instrument are {{free from the}} occasional erratic samples obtained with ordinary Jones sample splitters. Working drawings are given for this improved sample splitter, which can be built with hand tools. The sample splitter was tested with small gravel, granules, and sand in which the percentage of each constituent was accurately known. Three series of 24 samples each were taken and sieved; in each series one detail of technique was varied. The components separated by sieving were returned to the original mixture in each test to keep {{the conditions of the}} experiment nearly constant. Summarized instruction are given for the best technique of operation. The second part of the paper deals with methods used to measure the performance of the improved sample splitter. The concept of a state of control is discussed. The calculations required to detect lack of constancy in the <b>chance</b> <b>cause</b> system, biased percentages, or the presence of a predominating cause of variation are shown. The principle that variability in comparable series of samples tends to diminish as the state of control is approached was found to apply in the present study. These methods of statistical analysis are very general and may be applied to field sampling studies or wherever quantitative measurements are made on a series of samples suspected of coming from an essentially homogeneous unit or constant <b>chance</b> <b>cause</b> system...|$|R
2500|$|Women {{who agree}} to be {{screened}} have their breasts X-rayed on a specialized X-ray machine. [...] This exposes the woman's breasts to {{a small amount of}} ionizing radiation, which has a very small, but non-zero, <b>chance</b> of <b>causing</b> cancer.|$|R
60|$|My {{first impulse}} was to tell her of my love, and then I thought of the {{helplessness}} of her position wherein I alone could lighten the burdens of her captivity, and protect her in my poor way against the thousands of hereditary enemies she must face upon our arrival at Thark. I could not <b>chance</b> <b>causing</b> her additional pain or sorrow by declaring a love which, in all probability she did not return. Should I be so indiscreet, her position {{would be even more}} unbearable than now, and the thought that she might feel that I was taking advantage of her helplessness, to influence her decision was the final argument which sealed my lips.|$|R
5000|$|The primary {{advantage}} of MaterniT21 PLUS {{over the other}} major high accuracy tests for Down syndrome, Amniocentesis and Chorionic villus sampling, is that MaterniT21 PLUS is noninvasive. [...] Because amniocentesis and chorionic villus sampling are invasive, they have a <b>chance</b> of <b>causing</b> miscarriage.|$|R
40|$|Since year 2000, {{scientists}} on artificial and natural intelligences started to study chance discovery - methods for discovering events/situations that significantly affect decision making. Partially because the editors Ohsawa and Abe are teaching at schools of Engineering and of Literature with sharing {{the interest in}} chance discovery, this book reflects interdisciplinary aspects of progress: First, as an interdisciplinary melting pot of cognitive science, computational intelligence, data mining/visualization, collective intelligence, … etc, chance discovery came to reach new application domains e. g. health care, aircraft control, energy plant, management of technologies, product designs, innovations, marketing, finance etc. Second, basic technologies and sciences including sensor technologies, medical sciences, communication technologies etc. joined this field and interacted with cognitive/computational scientists in workshops on chance discovery, to obtain breakthroughs by stimulating each other. Third, “time” came to be introduced explicitly as a significant variable ruling causalities - background situations <b>causing</b> <b>chances</b> and <b>chances</b> <b>causing</b> impacts on events and actions of humans in the future. Readers may urge us to list the fourth, fifth, sixth, … but let us stop here and open this book. ...|$|R
50|$|Somewhat {{beyond the}} path split, the idler photons {{encounter}} beam splitters BSa, BSb, and BSc that {{each have a}} 50% chance of allowing the idler photon to pass through and a 50% <b>chance</b> of <b>causing</b> it to be reflected. Ma and Mb are mirrors.|$|R
2500|$|According to Aristotle, spontaneity and <b>chance</b> are <b>causes</b> of some things, {{distinguishable}} {{from other}} types of <b>cause.</b> <b>Chance</b> as an incidental cause lies {{in the realm of}} accidental things. It is [...] "from what is spontaneous" [...] (but note that what is spontaneous does not come from chance). For a better understanding of Aristotle's conception of [...] "chance" [...] it might be better to think of [...] "coincidence": Something takes place by chance if a person sets out with the intent of having one thing take place, but with the result of another thing (not intended) taking place.|$|R
5000|$|Even though Geralt did {{not believe}} in destiny, he unknowingly demanded the unborn child of princess Pavetta and her husband Duny as a reward for his {{services}} by invoking [...] "the Law of Surprise" [...] (a law that states that if one cannot pay for the services of a witcher, the witcher is entitled to something the debtor does not yet know they have). The child {{turned out to be a}} girl, Ciri (otherwise known as Ziri coming from the elder speech word Zireael meaning Swallow). He did not take her because women cannot be witchers. However, fate or blind <b>chance</b> <b>caused</b> Geralt and Ciri to cross their paths thrice, and after the death of her grandmother, queen Calanthe, Geralt ends up taking care of the girl and loving her as his own daughter.|$|R
50|$|In one form, {{the shell}} bursts {{in the air}} above the tank and a number of shaped charge (HEAT) or HEDP (High Explosive Dual Purpose) bomblets or grenades rain down. Any that hit the tank have a good <b>chance</b> of <b>causing</b> damage, since they are attacking the thin top armor.|$|R
40|$|The {{theoretical}} framework underlying the GRID approach (Component Process Model) {{suggests that the}} results of subjective event appraisal drive the changes in all other components in the emotion episode. Thus one would expect to find a high degree of differentiation in the structure of this component. This is confirmed once the powerful influence of a valence superfactor is partialed out of the features. In addition to the valence appraisal factor, six additional factors are identified: novelty/ <b>chance</b> <b>cause,</b> coping ability, expected/ familiar, goal relevance, norm violation, and self vs other cause. These factors contribute substantially to the overall emotion dimension {{with the exception of the}} POWER dimension. The claim that appraisal is at the basis of emotion differentiation, including meaning, is supported by the result that the seven appraisal factors allow correct classification of 57 % of the cases in a discriminant analysis...|$|R
60|$|Black Paul was a {{powerful}} aid in the preservation of order among the disorderly. Conflicts between factions of the crew were greatly feared by him, for the schemes which happy <b>chance</b> had <b>caused</b> to now revolve themselves in his master mind would have been sadly interfered with by want of concord among {{the men of the}} Revenge.|$|R
40|$|It {{was marked}} {{out that the}} investors should be {{prepared}} for permanent changes of the forms in which the risk can manifest itself from the evolution of market economy grounded in the industrialized countries: operational, financial, strategic risks are also added to the traditional exposition to <b>chance,</b> <b>caused</b> {{by a series of}} factors, as legislation modifications, certain standards and European codes, the risks imposed by the evolution of the new economy, pressures that emerge in the personnel policy domain, the new needs for technical equipment, the costs efficiency, the spontaneous (unexpected, unplanned) economical events. It is interesting to analyze in what measure one can talk about the risk management in the strategies to attract the foreign investors in this context. This supposes an additional effort of the authorities to identify and subsequently eradicate the risk that could emerge in the case of some direct foreign investments in Romania...|$|R
40|$|Several {{effects of}} seismic {{activity}} on the release of radionuclides from a potential repository at Yucca Mountain are quantified. Future seismic events are predicted {{using data from the}} seismic hazard analysis conducted for the Exploratory Studies Facility (ESF). Phenomenological models are developed, including rockfall (thermal-mechanical and seismic) in unbackfilled emplacement drifts, container damage caused by fault displacement within the repository, and flow-path <b>chance</b> <b>caused</b> by changes in strain. Using the composite-porosity flow model (relatively large-scale, regular percolation), seismic events show little effect on total-system releases; using the weeps flow model (episodic pulses of flow in locally saturated fractures), container damage and flow-path changes cause over an order of magnitude increase in releases. In separate calculations using, more realistic representations of faulting, water-table rise caused by seismically induced changes in strain are seen to be higher than previously estimated by others, but not sufficient to reach a potential repository...|$|R
25|$|Donald's {{drifting}} {{from job}} to job was reportedly inspired by Barks' own experiences. So was his usual lack of success. And even in those that he was successful this would be temporary, just until a mistake or <b>chance</b> event <b>caused</b> another failure, another disappointment for the frustrated duck. Barks also reported that this was another thing he was familiar with.|$|R
60|$|It was {{the very}} spot at which I first had seen David Innes. If he had ever raised a cairn above the {{telegraph}} instrument no sign of it remained now. Had {{it not been for}} the <b>chance</b> that <b>caused</b> Cogdon Nestor to throw down his sleeping rug directly over the hidden instrument, it might still be clicking there unheard--and this story still unwritten.|$|R
