22|25|Public
5000|$|By control ability: controllable, {{uncontrollable}} costs. <b>Controllable</b> <b>costs</b> {{are those}} {{which can be}} controlled or influenced by a conscious management action. Uncontrollable costs cannot be controlled or influenced by a conscious management action.|$|E
5000|$|... #Caption: Diminishing Marginal Product {{ensures the}} Rise in Cost from {{producing}} an additional Item (Marginal Cost) is always {{greater than the}} Average Variable (Controllable) Cost at that level of production. Since some costs cannot be controlled in the [...] "Short Run", the Variable (<b>Controllable)</b> <b>Costs</b> will always be lower than the Total Costs in the [...] "Short Run".|$|E
50|$|Red Rock Entertainment {{primarily}} {{works on}} {{projects that are}} at an advanced stage and {{are looking for the}} final tranche of financing. Its focus is on film and TV projects that have commercial appeal, an identifiable audience, <b>controllable</b> <b>costs</b> and a sound financial structure. As an executive producer, Red Rock Entertainment arranges for investors to visit sets during filming, appear as extras and attend private screenings. It also regularly arranges seminars at Elstree Studios, at which high-profile corporate and financial specialists offer advice and insight into the various tax advantages of investing in the UK.|$|E
40|$|Maintenance is {{recognised}} as {{the largest}} <b>controllable</b> <b>cost</b> among direct mining costs (Lewis and Steinberg 2001). Ironically, the ideal architecture for information systems and technologies for optimising maintenance functions has not been extensively studied. While many maintenance management approaches recognise the importance of understanding equipment functionality and performance monitoring, based in an integral range of information, extensive study to attain proper organisation of the information have not been undertaken. Degradation detection and prediction applications found in recent literature are generally for specific applications rather than generic (Lee et al 2006). The current study is intended to address Longwall mining equipment integrated data infrastructure. The expected outcome {{of this study is}} to come up with a centralised information architecture that will utilise all the relevant information in the organisation to optimise asset management functions in a results based framework...|$|R
40|$|This project {{presents}} a strategic analysis to maximize maintenance operations in Alcan Kitimat Works in British Columbia. The project studies {{the role of}} maintenance in improving its overall maintenance performance. It provides strategic alternatives and specific recommendations addressing Kitimat Works key strategic issues and problems. A comprehensive industry and competitive analysis identifies the industry structure and its competitive forces. In the mature aluminium industry, the bargaining power of suppliers is moderate; bargaining power of customers is high; threat of substitute is high; rivalry among competing producers is high; while potential of new entrants is low. The overall industry is extremely competitive. Maintenance is a <b>controllable</b> <b>cost.</b> Maximizing maintenance effectiveness and equipment uptime will result in higher profit margins and low operating cost. Kitimat Works must develop a competitive advantage, given the significance of maintenance in today 2 ̆ 7 s operating environment where excellence in maintenance performance becomes a strategic issue for competitive organizations...|$|R
40|$|Though IP {{multicast}} is resource ef£cient {{in delivering}} data {{to a group}} of members simultaneously, it suffers from scalability problem with the number of concurrently active multicast groups because it requires a router to keep forwarding state for every multicast tree passing through it. To solve this state scalability problem, we proposed a scheme, called aggregated multicast. The key idea is that multiple groups are forced to share a single delivery tree. In our earlier work, we introduced the basic concept of aggregated multicast and presented some initial results to show that multicast state can be reduced. In this paper, we develop a more quantitative assessment of the cost/bene£t trade-offs. We propose an algorithm to assign multicast groups to delivery trees with <b>controllable</b> <b>cost</b> and introduce metrics to measure multicast state and tree management overhead for multicast schemes. We then compare aggregated multicast with conventional multicast schemes, such as source speci£c tree scheme and shared tree scheme. Our extensive simulations show that aggregated multicast can achieve signi£cant routing state and tree management overhead reduction while containing the expense of extra resources (bandwidth waste and tunnelling overhead). We conclude that aggregated multicast is a very cost-effective and promising direction for scalable transit domain multicast provisioning...|$|R
50|$|Red Rock Entertainment Ltd {{is a film}} {{investment}} and production company based at Elstree Film Studios in Borehamwood, Hertfordshire (UK). Many well-known TV shows on British television and is the studio of choice for many successful British films. The company acts as executive producers sourcing {{investment and}} finance for film and TV projects. The CEO of Red Rock Entertainment is Gary Collins. Red Rock Entertainment primarily works on projects that are at an advanced stage and {{are looking for the}} final amount of financing. Their sole focus is on film and TV projects that have a commercial appeal, an identifiable audience, moderately low and <b>controllable</b> <b>costs</b> and a sound financial structure.|$|E
40|$|This study aims to {{evaluate}} how {{the accounting system}} serves {{as a means of}} controlling production costs at PT. Plantation Nusantara X (Persero) PG. Pesantren Baru, Kediri. This type of research is a descriptive study with a case study approach. Sources of data used in the form of primary and secondary data. Data was collected by means of documentation and interviews. Analysis of quantitative data used to analyze the five conditions that exist in the accounting {{of the structure of the}} organization, the separation of <b>controllable</b> <b>costs</b> and uncontrollable costs, budgeting, cost accounting systems, and expense reporting system. The results showed that the application of accounting at. Plantation Nusantara X (Persero) PG. Pesantren Baru, Kediri has not fully met the separation of <b>controllable</b> <b>costs</b> and uncontrollable costs. Based on these conclusions need for the application of accounting to the maximum in order to control the cost of production can be applied to the fullest as well...|$|E
40|$|The {{purpose of}} this paper is to write a {{training}} manual on cost management for operating managers in the Las Vegas nightclub industry. The manual will be designed to be used as a tool to control and manage costs for managers responsible for making decisions that directly affect <b>controllable</b> <b>costs.</b> These decisions include but are not limited to staffing and par levels, price points, purchasing, and staff training and monitoring...|$|E
40|$|A Digital Fabrication Production System (DFPS) is {{a concept}} {{describing}} a set of processes, tools, and resources that {{will be able to}} produce an artifact according to a design, fast, cheap, and easy, independently of location. A DFPS project is a complex assembly of custom parts that is delivered by a network of fabrication and assembly processes. This network is called the value chain. The workflow concept of a DFPS is the following: begin design process with a custom geometric form; decompose it into constructible parts; send the part files for fabrication to various locations; transport all parts at the construction site at the right time; finally, assemble the final artifact. Conceptually it means that based on a well structured value chain we could build anything we want, at anyplace, at <b>controllable</b> <b>cost</b> and quality. The goals of a DFPS are the following: custom shapes, controllable lead time, <b>controllable</b> quality, <b>controllable</b> <b>cost,</b> easiness of fabrication, and easiness of assembly. Simply stated this means to build any form, anywhere, accurately, cheap, fast, and easy. Unfortunately, the reality with current Digital Fabrication (DF) projects is rather disappointing: They take more time than what was planned, they get more expensive than what was expected, they involve great risk and uncertainty, and finally they are too complex to plan, understand, and manage. Moreover, most of these problems are discovered during production when it is already late for correction. However, there is currently no systematic approach to evaluate difficulty of production of DF projects in Architecture. Most of current risk assessment methods are based on experience gathered from previous similar cases. But it is the premise of mass customization that projects can be radically different. Assembly incompatibilities are currently addressed by building physical mockups. But physical mockups cause a significant loss in both time and cost. All these problems suggest that an introduction of a DFPS for mass customization in architecture needs first an integrated theory of assembly and management control. Evaluating feasibility of a DF project has two main problems: first, how to evaluate assemblability of the design; second, how to evaluate performance of the value chain. Assemblability is a system’s structure problem, while performance is a system’s dynamics problem. Structure of systems has been studied in the field of Systems Engineering by Network Analysis methods such as the Design Structure Matrix (DSM) (Steward 1981), and the liaison graph (Whitney 2004), while dynamics of systems have been studied by System Dynamics (Forrester 1961). Can we define a formal method to evaluate the difficulty of production of an artifact if we know the artifact’s design and the production system’s structure? This paper formulates Attribute Process Methodology (APM); a method for assessing feasibility of a DFPS project that combines Network Analysis to evaluate assemblability of the design with System Dynamics to evaluate performance of the value chain...|$|R
40|$|Productivity is a {{significant}} issue in the US auto industry that is often viewed as {{the success or failure}} that a vehicle assembly plant can make or break their production schedule. In other words, productivity is often looked at {{in terms of the number}} of assembled vehicles produced per year. While high production volume is an important indicator in a manufacturing environment, it certainly does not necessarily imply high productivity. By definition, Productivity is the ratio of output (number of vehicles produced) divided by all input resources such as labor, material, capital, overhead, health and energy costs. Improvement in productivity can be achieved in two ways: a reduction of inputs while output remains constant, or an increase in output while inputs remain constant. Energy is the single most <b>controllable</b> <b>cost</b> parameter in the input parameters of the productivity equation. In today’s competitive marketplace, energy efficiency can provide means to improve productivity through reducing the manufacturing energy cost. This paper addresses this missing link between energy and productivity in U. S. auto industry. Furthermore, it outlines a methodological approach for driving improvements in energy usage in a major vehicle assembly plant in US. It is hoped that efforts in this direction would pave the road to better understand role of energy as an important factor in the assembly plant’s vision to productivity improvement...|$|R
40|$|Several {{papers have}} been {{published}} which present simplified models for the aggregate planning problem which do not yield optimal schedules. They are typically supported with computations that suggest that the resulting schedules produce costs within a few percent of optimum. This note suggests that those cost comparisons have been made against inappropriate base costs which include a large fixed cost element. When these models are compared using only the <b>controllable</b> variable <b>costs,</b> costs {{that appeared to be}} only one or two percent over minimum turn out to be as much as twenty to thirty percent over the costs resulting from optimal solutions. planning, inventory/production: production smoothing...|$|R
40|$|Activity-based costing (ABC) {{is given}} much {{attention}} in the literature. Most authors seem to agree that improved cost allocations are useful to indicate the long-term profitability of products. However, a controversy starts when the relevance of ABC for short-term decision-making is discussed. This topic is addressed here {{and some of the}} arguments found in the literature are reviewed. The first objective is to contribute to discussions on the usefulness of ABC for decision-making. In this paper it is suggested that ABC can be relevant for short-term decisions. It is suggested here that particular costs and revenues become controllable by making a particular decision, even if these costs and revenues appear to be uncontrollable a priori. This concept should be considered as a hypothesis which still needs to be tested. The second objective is to contribute to the literature on why managers use cost allocations for decision-making instead of only <b>controllable</b> <b>costs</b> and revenues. It is discussed that uncertainty about <b>controllable</b> <b>costs</b> and revenues will stimulate a manager to use cost allocations for decision-making. Allocated costs provide a reference point which helps a manager to resolve how much risk he or she is willing to take. This has been confirmed in an experiment, reported in Wouters (1993) ...|$|E
40|$|Ensuring the {{measurement}} effectiveness of Cost of Quality (COQ) implementations presents steadily a practical managerial problem. Even {{though there are}} many approaches to measurement COQ optimal methods for this purpose in real conditions often depend on confrontation and evaluation of different calculation models. The paper based on a case study describes a procedure for comparison between so called <b>controllable</b> <b>costs</b> and incurred cost of poor quality. Presented study confirms assumption of a propitious influence of preventive costs on an overall COQ and a change of structure of separate cost of quality items...|$|E
40|$|Abstract: The {{technique}} {{proposed in}} this paper {{can be used to}} improve a given existing time-periodical production schedule for a group of products in a group of workplaces (machines) having limited capacity, performing cost optimisation by lot size alteration. A coefficient, by which all lot sizes are multiplied, is derived which allows {{the creation of a new}} schedule that is feasible under all given constraints, minimising the total <b>controllable</b> <b>costs</b> and preserving the mutual ratio of lot sizes. The presented technique can also be used to solve the optimal lot size problem for a set of product, if the mutual relationship of these lot sizes is given...|$|E
40|$|This paper uses a two-step {{methodology}} {{to examine}} the relationship between managerial cost inefficiency and the takeover of U. S. thrifts during a period of market liberalization and widespread takeover activity, 1994 to 2000. In the first stage using stochastic cost frontiers, we estimate <b>controllable</b> managerial <b>cost</b> inefficiency scores for all stock firms operating each year in 1994 to 2000. In a second stage, we use these scores to examine correlates of takeovers, focusing on cost inefficiency. For takeovers by banks, we find a significant negative relationship between cost inefficiency and takeover, suggesting an exit of more cost efficient firms from the thrift industry during this period. However, takeovers by thrifts are associated with other characteristics. ...|$|R
40|$|We {{propose a}} forward {{resource}} reservation (FRR) scheme {{to reduce the}} data burst delay at edge nodes in optical burst switching (OBS) systems. We also explore algorithms to implement the various intrinsic features of the FRR scheme. Linear predictive filter (LPF) -based methods are investigated and demonstrated to be effective for dynamic burst-length prediction. An aggressive resource reservation algorithm is proposed to deliver a significant performance improvement with <b>controllable</b> bandwidth <b>cost.</b> By reserving resources in an aggressive manner, an FRR system can reduce both the signaling retransmission probability and the bandwidth wastage as compared with a system without the aggressive reservation. An FRR-based QoS strategy is also proposed to achieve burst delay differentiation for different classes of traffic. Theoretical analysis and simulation results verify the feasibility of the proposed algorithms and show that our FRR scheme yields a significant delay reduction for time-critical traffic without incurring a deleterious bandwidth overhead...|$|R
50|$|Firstly, the {{mechanisms}} of DHR can make the system show much uncertainty to the attacker within the <b>controllable</b> and <b>cost</b> acceptable range for the defender, which provides a powerful solution to security threats of the statically deterministic DRS. It can also force the inevitable human attack, different from the fault-error, into a risk control problem of minimum probability, which can normalize the threat of certainty or uncertainty into a reliability problem of stochastic failure according to the physical mechanism of DHR. Secondly, {{it is a big}} challenge to make precise cooperation between homogeneous redundant components according to the related researches. Hence the heterogeneous feature of DHR will make cooperation more difficult for attackers. Furthermore, dynamism and randomness can improve the uncertainty of multi-participatory actions with consistent or synergistic requirements. Last but not least, the multi-mode arbitrator significantly enhances the difficulty of cooperative attack under the condition of non-cooperation.|$|R
40|$|In 1995, TransAlta Utilities and Dairyland Power {{agreed to}} {{participate}} in a project funded by the U. S. Department of Energy to demonstrate a power plant optimization software product developed by Praxis Engineers, Inc. The product, the Plant Environmental and Cost Optimization System (PECOS{trademark}), considers the power plant in its entirety from coal receipts and yard management to solid by-products and emissions. Its basic goal is to minimize the <b>controllable</b> <b>costs</b> of power generation. PECOS does so by performing an on-line analysis of all operations and their co-optimization to achieve a minimum generation cost. The software acts as an advisor to the plant operators and computes settings that achieve this goal. A general schematic of PECOS is given...|$|E
40|$|This {{research}} paper explores alternative bulk lubricant delivery options to create efficiencies within the current structure and alignment with Petro-Canada’s strategy for first quartile operations. Petro-Canada strives for Q 1 operations that allow its cost structure to be competitive. The current bulk lubricant delivery system within British Columbia {{is an opportunity}} to increase efficiencies and lower total costs associated with distribution of bulk lubricants to end users. The complexity of the current system increases inefficiencies and there are many opportunities to resolve this situation to allow for efficiencies and effectiveness throughout the delivery system. This project will review internal operational impact of alternatives proposed as well as the external impact. The overall intent of the project is to simplify and increase efficiency with the current process. It will allow Petro-Canada to improve alignment throughout their entire lubricant division and is a strategic fit with their overall strategy to improve <b>controllable</b> <b>costs...</b>|$|E
40|$|This paper {{investigates the}} concept of {{customers}} 2 ̆ 7 perceived price fairness {{in the context of}} different price increase conditions. Several tourism service industries seem reluctant to systematically vary or occasionally rise prices, mostly because of potential negative consumer responses. Previous studies in behavioral pricing confirm that a price increase may be perceived as highly unfair and, with this, may lead to negative consequences for the firm. However, {{there is some evidence that}} not all price increase events are perceived equally and that consumers 2 ̆ 7 fairness perception depends on the situational conditions of the respective price event. Drawing on the principle of dual entitlement and attribution theory, the results of a standardized survey with 1530 cable car customers in Switzerland reveal that cost-based reasons seem to legitimate a price increase, rather than excess demand conditions. Still, within cost conditions, an increase in internally <b>controllable</b> <b>costs</b> is perceived as a less fair reason for raising prices as opposed to an exogenously caused and uncontrollable cost increase. Interestingly, increasing prices without any communicated reason is perceived as the most unfair condition, indicating the crucial role of price communication...|$|E
40|$|Ofgem {{appointed}} consultants Cambridge Economics Policy Associates (CEPA) {{to produce}} a report examining the important considerations for Ofgem in using benchmarking in the distribution price control review (DPCR 4). The work comprised three areas: A. Ofgem’s approach in DPCR 3 – This focussed on the regression technique (COLS) used to benchmark DNO’s on (normalised) <b>controllable</b> operating <b>costs.</b> CEPA were asked to review the approach in DPCR 3 identifying its strengths and weaknesses. In addition {{they were asked to}} identify significant developments that would require alternative approaches e. g. mergers, the use of the frontier firm as the benchmark etc; B. Alternative benchmarking techniques and methodologies – Given the available data and other restrictions CEPA were asked to advise on the strength and weakness of alternative techniques to those used in DPCR 3. In addition CEPA were asked to advise on the approach to selecting appropriate cost drivers; an...|$|R
40|$|A {{rocket motor}} nozzle thermal {{structural}} test technique that utilizes arc heated nitrogen to simulate a motor burn was developed. The technique {{was used to}} test four heavily instrumented full-scale Star 48 rocket motor 2 D carbon/carbon segments at conditions simulating the predicted thermal-structural environment. All four nozzles survived the tests without catastrophic or other structural failures. The test technique demonstrated promise as a low <b>cost,</b> <b>controllable</b> alternative to rocket motor firing. The technique includes the capability of rapid termination in the event of failure, allowing post-test analysis...|$|R
40|$|Control of {{distributed}} systems has various industrial applications, {{as it is}} often desired to keep complex multi-disciplinary systems in some given state. Definition or parameteri-zation of control space is the first main issue we face when formulating a control problem. Usually, one wishes to keep the parameterization space dimension as small as possible to limit {{the complexity of the}} problem. In addition, for any control approach to be effective, it should be realizable during the time the system is still <b>controllable.</b> Computational <b>cost</b> is therefore another critical issue. Our aim in this paper is to discuss alternative remedies for these two problems. We discuss the behavior of an electrokinetic microchan-nel system where the control variables include both the geometry of the microchannels and the temporal control of potentials. In a real system, the geometric control is achieved by the realization of etched microchannel structures using microlithography techniques. Flow control is accomplished by applying electric potentials along microchannels. We discuss the behavior of our design and control platform for two complementary classes of problems: the situation where the number of controls is small (a potential field) and where the number of controls is large (the geometry of a microchannel turn) ...|$|R
40|$|The dollar flow in United States {{medical care}} has been {{analyzed}} {{in terms of}} a six-level model; this model and the gross 1981 flow data are set forth. Of the estimated $ 310 billion expended in 1981, it is estimated that $ 85 -$ 95 billion was the "surgical stream", i. e., that amount expended to take care of surgical patients at a variety of institutional types and including ambulatory care and surgeons' fees. Some of the determinants of surgical flow are reviewed as well as <b>controllable</b> <b>costs</b> and case mix pressures. Surgical complications, when severe, increase routine operative costs by a factor of 8 to 20. Maintenance of high quality in American surgery, despite new manpower pressures, {{is the single most important}} factor in cost containment. By voluntary or imposed controls on fees, malpractice premiums, case mix selection, and hospital utilization, a saving of $ 2. 0 -$ 4. 0 billion can be seen as reachable and practical. This is five per cent of the surgical stream and is a part of the realistic "achievable" savings of total flow estimated to be about + 15 billion or 5 per cent...|$|E
40|$|Maintenance {{has gained}} credit {{over the past}} decades. The oil and gas {{industry}} requires efficient maintenance programs due to the hazardousness surrounding the industry. Crude oil margins are also dropping and maintenance yields high <b>controllable</b> <b>costs.</b> Therefore, safety and economy are the driving forces of maintenance optimisation. Refineries have to operate when margins are the most profitable so reliability is crucial. Centrifugal pumps are essential features of the refining process. Due to limited resources, maintenance work is prioritised according to the operating context and risks. Criticality analysis is a widespread tool in the industry which supports the resource allocation decision. The criticality level has, thus, to be highly accurate and adapted to the plant situation since it influences the overall maintenance system efficiency. Maintenance plans must be properly designed and implemented to enhance reliability. This paper states maintenance organisation around centrifugal pumps maintenance in a refinery. First, the current criticality assessment methodology {{has been found to be}} inadequate to the prioritisation of work so a new method has been developed. Then, a tailored spare parts strategy has been implemented and discussed. Finally, preventive maintenance plan has been reviewed using pump specialists findings and maintenance optimisation methodologies which has led to several improvement hints proposals...|$|E
40|$|Background: The {{balanced}} scorecard (BSC) {{is considered to}} be a useful tool for management in a variety of business environments. The purpose of this article is to utilize the experimental data produced by the incorporation and implementation of the BSC in hospitals and to investigate the effects of the BSC red light tracking warning system on performance improvement. Methods: This research was designed to be a retrospective follow-up study. The linear mixed model was applied for correcting the correlated errors. The data used in this study were secondary data collected by repeated measurements taken between 2004 and 2010 by 67 first-line medical departments of a public academic medical center in Taipei, Taiwan. The linear mixed model of analysis was applied for multilevel analysis. Results: Improvements were observed with various time lags, from the subsequent month to three months after red light warning. During follow-up, the red light warning system more effectively improved <b>controllable</b> <b>costs,</b> infection rates, and the medical records completion rate. This further suggests that follow-up management promotes an enhancing and supportive effect to the red light warning. Conclusion: The red light follow-up management of BSC is an effective and efficient tool where improvement depends on ongoing and consistent attention in a continuing effort to better administer medical care and control costs...|$|E
40|$|The {{quest for}} {{realistic}} computer generated images continues. An exciting area {{of which is}} real-time rendering. Images need indirect light, soft shadows and color bleeding in order to exhibit realistic global illumination qualities. Achieving real time frame rates requires compromise as high quality methods such as view dependent ray tracing may consume hours instead of the milliseconds available. Today's applications must run on today's computers, the majority of which are still using last years hardware at best. Light-mapping or precomputed light and shadow textures are a well known technique, used in computer and video games, to enhance the realism of statically lit scenes at very low run-time cost. This paper presents a new lighting technique capable of rendering diffuse global illumination for static scenes of high complexity built from arbitrary polygonal meshes. It draws from existing methods such as ray tracing, photon mapping and radiosity to produce worthwhile results with a <b>controllable</b> precomputation <b>cost.</b> The precomputation phase and the final real-time render phase are designed to run on systems with limited memory, and do not require any specific CPU and GPU features beyond floating point calculation and texture mapping. Keywords Global Illumination, Indirect Light, Photon Map, Rendering, Real Tim...|$|R
40|$|Simulation tools play a {{fundamental}} {{role on the}} development of Grid middlewares since they provide a <b>controllable</b> and low <b>cost</b> environment for validating new concepts and implementations that address aspects such as heterogeneity, scalability, security and the dynamism of Grid environments. This paper describes the motivation, design principles, architecture, and implementation of the Opportunistic Grid Simulation Tool- OGST, an object-oriented discrete event simulator whose main objective is to assist developers of opportunistic Grid middlewares on validating new concepts and implementations under different execution environment conditions and scenarios. The preliminary motivation for OGST development was to provide a way for evaluating the behavior of adaptive and autonomic mechanisms for opportunist Grids, such as adaptive scheduling approaches and dynamic re-scheduling of applications. ...|$|R
40|$|With the {{prevalence}} of social media and GPS-enabled devices, a massive amount of geo-textual data has been generated in a stream fashion, leading {{to a variety of}} applications such as location-based recommendation and information dissemination. In this paper, we investigate a novel real-time top-k monitoring problem over sliding window of streaming data; that is, we continuously maintain the top-k most relevant geo-textual messages (e. g., geo-tagged tweets) for a large number of spatial-keyword subscriptions (e. g., registered users interested in local events) simultaneously. To provide the most recent information under <b>controllable</b> memory <b>cost,</b> sliding window model is employed on the streaming geo-textual data. To the best of our knowledge, this is the first work to study top-k spatial-keyword publish/subscribe over sliding window. A novel centralized system, called Skype (Topk Spatial-keyword Publish/Subscribe), is proposed in this paper. In Skype, to continuously maintain top-k results for massive subscriptions, we devise a novel indexing structure upon subscriptions such that each incoming message can be immediately delivered on its arrival. To reduce the expensive top-k re-evaluation cost triggered by message expiration, we develop a novel cost-based k-skyband technique {{to reduce the number of}} re-evaluations in a cost-effective way. Extensive experiments verify the great efficiency and effectiveness of our proposed techniques. Furthermore, to support better scalability and higher throughput, we propose a distributed version of Skype, namely, DSkype, on top of Storm, which is a popular distributed stream processing system. With the help of fine-tuned subscription/message distribution mechanisms, DSkype can achieve orders of magnitude speed-up than its centralized version...|$|R
40|$|For most order quantity/reorder point {{inventory}} systems, the stochastic model, which specifies {{the demands}} as stochastic processes, {{is often more}} accurate than its deterministic counterpart [...] -the EOQ model. However, {{the application of the}} stochastic model has been limited because of the absence of insightful analytical results on the model. This paper analyzes the stochastic order quantity/reorder point model in comparison with a corresponding deterministic EOQ model. Based on simple optimality conditions for the control variables derived in the paper, a sensitivity analysis is carried out, and a number of basic qualitative properties are established for the optimal control parameters. Our main results include the following: (1) in contrast to the deterministic EOQ model, the <b>controllable</b> <b>costs</b> of the stochastic model due to selection of the order quantity (assuming the reorder point is chosen optimally for every order quantity) are actually smaller, while the total costs are clearly larger; the optimal order quantity is larger, but the difference is relatively small when the quantity is large; the cost performance is even less sensitive to choices of the order quantity; (2) the relative increase of the costs incurred by using the quantity determined by the EOQ instead of the optimal from the stochastic model is no more than 1 / 8, and vanishes when the ordering costs are significant relative to other costs. inventory/production, sensitivity analysis, stochastic model...|$|E
40|$|PT DSA is {{a company}} engaged in the {{industry}} which produces plastic ore used as the main ingredient manufacture of household appliances. Problems faced by PT DSA are: the presentation of information in terms of ore production cost of plastic, in which top management as users can not report quickly and accurately obtain the required data in decision-making. And the problem is formulated: "How Implementation Effectiveness Production Cost Control at PT DSA?" In {{the discussion of the}} data analysis section, it can be concluded that the results of research at PT DSA responsibility accounting is used for cost control has not been implemented effectively. It {{can be seen from the}} budgeting process and who is responsible for reporting on the implementation of the work that has been accomplished. Manufacturing cost reporting systems can still be used to assess the performance, efficiency and effectiveness of each department for the report did not provide a comparison between the budget and expenses. And PT DSA in preparing budget reports do not separate the cost of production with a <b>controllable</b> <b>costs</b> uncontrollable. As well as the PT DSA, there are departments that have the dual task of making the department that has the double task becomes ineffective. In order for the accounting responsibility on PT DSA can run effectively, it should have a separation of functions of employees to run operations effectively, and preferably in preparing the production budget to separate between production costs can be controlled with uncontrollable. As well as in preparing the production budget includes an entire head of departments...|$|E
40|$|Cost {{control is}} one of the {{important}} factors for competition through managing the <b>controllable</b> <b>costs</b> as low as possible, it will lowest the basic price of production, so that selling price will be compete with anothers companies which are cost efficient. The determination cost of goods sold with activity based costing method will prevents cost distortion. Beside that, ini managing stock supplies will be on time, so it will minimize the raw materials. Based on the problems, this reseach is aimed to evaluate the calculation cost of goods sold which implemented by a company, it also analyze activity based costing, and compare both of them. The implications of differences will be recommended to the management. Reffering to the research resaut, that is calculating overhead cost allocation which howadays applied by a company, it is only using one cost driver, called machine hour. Meanwhile, activity based costing, there are 5 cost drivers : machine hour, kilowatthour, kilogram gas, working hour, and space of usage of factory. The comparison of calculating cost of goods sold there are 7 products are under valued and 4 product are over valued. The other benefits by implementing activity based costing are able to identify cost efficency level for any product. Then we will know which activities consume high resources, so that it will increase overhead cost. Considering the benefits of activity based costing method, it will recommended to a company. Implementation this activity based costing method needs strong commitment from the whole employees, supported by adequate resources, and computerized. ...|$|E
40|$|Drug-related {{morbidity}} (DRM) {{results when}} drug thera-py {{does not produce}} the intended therapeutic outcome, either due to treatment failure or {{the production of a}} new medical problem. 1 Use of drug therapy may be expected to result in some morbidity; in fact, {{at least half of the}} DRM that occurs may be preventable (PDRM). 2 - 5 As described by Hepler and Strand, 1 DRM may be classified as pre-ventable only if it was preceded by a recognizable drug-re-lated problem for which the causes and adverse outcome or treatment failure must have been foreseeable, identifi-able, and <b>controllable.</b> The <b>costs</b> and consequences resulting from PDRM can be significant. PDRM is reported to account for 3 – 9 % of hospital admissions, and> 50 % of drug-related hospital ad-missions may be considered preventable. 6 A recent Cana-dian study estimated that the annual cost of PDRM in old-er adults is $ 10. 9 billion (CND). 7 Clinical indicators are tools that have been widely used to assess quality issues related to the use of medicines. Several authors have reported on the development and use of indicators of PDRM in different jurisdictions. MacKin-non and Hepler 8 described the development of clinical in-dicators of PDRM in the US that were adapted for use in the UK 9 and further evaluated in another US managed care organization database. 10 Recently, in Nova Scotia, Canada, further development and validation of the original 52 US PDRM indicators was undertaken. 11 These indicators were operationalized retro-spectively, using administrative claims data to determin...|$|R
40|$|Nanofibers {{are very}} thin fibers having diameters lower than 100 nm and their lengths {{might be as}} long as {{possible}} within production limits. The large surface area of nanofibers gives opportunity to functionalize them. Nanofibers have several applications including both applications for industrial production in many sectors and for research studies. Nanofibers find applications in energy devices such as solar cells, fuel cells and nanogenarators; in filtration applications (such as water/oil filtration, fine particle filtration, aerosol filtration, air filtration, nanoparticle filtration) and in several medical applications including antibacterial efficacy, wound healing, drug delivery and scaffolds for tissue engineering. There are several methods to produce nanofibers: Electrospinning, self assembly, phase separation, bacterial cellulose, templating, drawing, extraction, vapor-phase polymerization, kinetically controlled solution synthesis, conventional chemical polymerization for anyline. Electrospinning is the most widely used method to produce nanofibers. In electrospinning, a high electric field, which is in kilovolts, is applied to a polymer solution. The polymer solution is drawn from a syringe to a collector surface. Electrospinning requires usage of appropriate solvent, removal of evaporating solvent, an adequate power supply to overcome the viscosity and surface tension of the polymer solution; while, jet instability and jet control remain as challenges in electrospinning. Nanofiber production methods possess some disadvantages as: higher cost compared to conventional fiber production methods, health hazards such as inhale risk of nanofibers during production and keeping the environment safe from evaporating solvents used during nanofiber production. Up to date, many researches have been conducted on nanofibers and electrospinning; still, more <b>controllable,</b> more <b>cost</b> effective, more environmentally friendly and safer methods are of essential importance to future applications of nanofibers. ...|$|R
40|$|This study {{aimed at}} {{evaluating}} the accounting responsibility {{system as a}} controlling production costs at PT. Multi Sarana Indotani in Mojokerto. This study belonged to descriptive by using case study as the approach. Sources of data were secondary data. Data were collected by using documentation. Quantitative data {{analysis was used to}} analyze the five conditions that exist in accounting responsibility. They were the organizational structure, the separation between <b>controllable</b> and non-controllable <b>costs,</b> budgeting, cost accounting systems, and expense reporting system. The results of the study showed that the application of accounting responsibilities at PT. Multi Sarana Indotani in Mojokerto could not be a controlling production costs yet. It happened because the company could not complete one of the requirements. The company reported the accountability report annually or in each period. Based on the results, the researcher implied that in applying the accounting responsibility as controlling the cost of production, all companies have to require the five requirements so that the planning, monitoring and controlling the production cost are able to run well...|$|R
