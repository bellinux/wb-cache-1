9|1|Public
5000|$|Additional {{information}} {{besides the}} program counter can be recorded at each interval. For example, other hardware performance counters {{can be read}} or the entire function <b>callstack</b> can be recorded. Recording the stack {{is important because it}} allows the developer to know not only where time was spent (or events occurred), but also how that code was called.|$|E
50|$|Process Explorer {{can be used}} {{to track}} down problems. For example, it {{provides}} a means to list or search for named resources that are held by a process or all processes. This {{can be used to}} track down what is holding a file open and preventing its use by another program. As another example, it can show the command lines used to start a program, allowing otherwise identical processes to be distinguished. Like Task Manager, it can show a process that is maxing out the CPU, but unlike Task Manager it can show which thread (with the <b>callstack)</b> is using the CPU - information that is not even available under a debugger.|$|E
40|$|We {{develop a}} machine-learned {{similarity}} metric for Windows failure reports using telemetry data gathered from clients describing the failures. The key feature is a tuned <b>callstack</b> edit distance with learned costs for seven fundamental edits based on <b>callstack</b> frames. We present {{results of a}} failure similarity classifier based on this and other features. We also describe how the model can be deployed to conduct a global search for similar failures across a failure database. ...|$|E
40|$|JavaScript {{developers}} create {{programs by}} calling functions {{and they use}} functions to construct objects. JavaScript development tools need to report to developers about those functions and constructors, for example in debugger <b>callstacks</b> and in object representations. However, most functions are anonymous: developers need not to specify names for functions. Based on our analysis of ten large, widely used JavaScript projects, less than 7 % of JavaScript functions are named by developers. After studying examples from these JavaScript projects, we propose Static Function-Object Consumption, a principled, automated approach based on local source code analysis for providing names to nameless JavaScript functions. We applied our approach to 90000 anonymous functions {{that appeared in the}} analyzed JavaScript project. The approach is successful in naming more than 99 % (91 % are unique within their file) of anonymous functions while the average length of function names is kept less than 37 characters. Categories and Subject Descriptors D. 2. 5 [Testing an...|$|R
40|$|Observing the {{performance}} of an application at runtime requires economy in what performance data is measured and accessed, and flexibility in changing the focus of performance interest. This paper describes {{the performance}} <b>callstack</b> as an efficient performance view of a running program which can be retrieved and controlled by external analysis tools. The performance measurement support {{is provided by the}} TAU profiling library whereas tool-program interaction support is available through the DAQV framework. How these systems are merged to provide dynamic performance <b>callstack</b> sampling is discussed. 1 Introduction The are several motivations for wanting to observe {{the performance of}} a parallel application during its execution [8] (e. g., to terminate a long running job or to steer performance variables). The downside in doing so is the deleterious effect on performance that may result. This trade-off forces consideration of a means to capture just enough performance information to make [...] ...|$|E
40|$|The TAU {{performance}} {{system is}} an integrated performance instrumentation, measurement, and analysis toolkit offering support for profiling and tracing modes of measurement. This paper introduces memory introspection capabilities of TAU {{featured on the}} Cray XT 3 Catamount compute node kernel. TAU supports examining the memory headroom, or the amount of heap memory available, at routine entry, and correlates it to the program’s <b>callstack</b> as an atomic event...|$|E
40|$|This paper {{describes}} a general framework—and its imple-mentation in a tool called EXPLORER–for statically answer-ing {{a class of}} interprocedural control flow queries about Java programs. EXPLORER allows users to formulate queries about feasible <b>callstack</b> configurations using regular expres-sions, and it employs a precise, demand-driven algorithm for answering such queries. Specifically, EXPLORER constructs an automatonA that is iteratively refined until either the lan-guage accepted by A is empty (meaning that the query has been refuted) or until no further refinement is possible based on a precise, context-sensitive abstraction of the program. We evaluate EXPLORER by applying it to three different pro-gram analysis tasks, namely, (1) analysis of the observer de-sign pattern in Java, (2) identification of a class of perfor-mance bugs, and (3) analysis of inter-component commu-nication in Android applications. Our evaluation shows that EXPLORER is both efficient and precise...|$|E
40|$|Recent {{work on the}} Extended Object Tcl (XOTcl) was geared {{towards the}} orthogonality, the ease of use, the productiveness, and the tailorability of the language. The result is an {{innovative}} object-oriented language framework which serves for developing a family of object-oriented Tcl dialects. In this work-in-progress report, we map the background and history of advanced language constructs (i. e., mixin classes, filters, method delegation) and their continued refinement (i. e., transitive mixins, mixin and filter guards). We present the infrastructure for creating derivative Tcl OO dialects (i. e., creating object systems and their structural relations, assembling base object behavior). A canonical model and infrastructure of parametrization of commands, methods, and objects is presented. Important steps of internal re-designing and refactoring (<b>callstack</b> and object life-time management) are discussed. Execution time and call throughput measurements for basic object life-time and method dispatch scenarios are reported, exhibiting substantial improvements over the XOTcl 1. 6. x branch and TclOO 0. 6. ...|$|E
40|$|Abstract—Given limited {{resource}} and time before software release, development-site testing and debugging {{become more and}} more insufficient to ensure satisfactory software performance. As a counterpart for debugging in the large pioneered by the Microsoft Windows Error Reporting (WER) system focusing on crashing/hanging bugs, performance debugging in the large has emerged thanks to available infrastructure support to collect execution traces with performance issues from a huge number of users at the deployment sites. However, performance debugging against these numerous and complex traces remains a significant challenge for performance analysts. In this paper, to enable performance debugging in the large in practice, we propose a novel approach, called StackMine, that mines <b>callstack</b> traces to help performance analysts effectively discover highly impactful performance bugs (e. g., bugs impacting many users with long response delay). As a successful technology-transfer effort, since December 2010, StackMine has been applied in performance-debugging activities at a Microsoft team for performance analysis, especially for a large number of execution traces. Based on real-adoption experiences of StackMine in practice, we conducted an evaluation of StackMine on performance debugging in the large for Microsoft Windows 7. We also conducted another evaluation on a third-party application. The results highlight substantial benefits offered by StackMine in performance debugging in the large for large-scale software systems. I...|$|E
40|$|Most {{programming}} languages {{support a}} call stack in the programming model {{and also in}} the runtime system. We show that for applications targeting low-power embedded microcontrollers (MCUs), RAM usage can be significantly decreased by partially or completely eliminating the runtime <b>callstack.</b> We present flattening, a transformation that absorbs a function into its caller, replacing function invocations and returns with jumps. Unlike inlining, flattening does not duplicate the bodies of functions that have multiple callsites. Applied aggressively, flattening results in stack elimination. Flattening is most useful in conjunction with a lifting transformation that moves global variables into a local scope. Flattening and lifting can save RAM. However, even more benefit can be obtained by adapting the compiler to cope with properties of flattened code. First, we show that flattening adds false paths that confuse a standard live variables analysis. The resulting problems can be mitigated by breaking spurious live-range conflicts between variables using information from the unflattened callgraph. Second, we show that the impact of high register pressure due to flattened and lifted code, and consequent spills out of the register allocator, can be mitigated by improving a compiler’s stack layout optimizations. We have implemented both of these improvements in GCC, and have implemented flattening and lifting as source-tosource transformations. On a collection of applications for the AVR family of 8 -bit MCUs, we show that total RAM usage can be reduced by 20 % by compiling flattened and lifted programs with our improved GCC...|$|E

