94|88|Public
2500|$|Cheyenne is a morphologically polysynthetic {{language}} with a sophisticated, agglutinating verb system contrasting {{a relatively simple}} noun structure. [...] Many Cheyenne verbs can stand alone in sentences, and can be translated by complete English sentences. [...] Aside from its verb structure, Cheyenne has several grammatical features that are typical of Algonquian languages, including an animate/inanimate noun <b>classification</b> <b>paradigm,</b> an obviative third person and distinction of clusivity {{in the first person}} plural pronoun.|$|E
2500|$|Lung cancers are now {{considered}} {{a large and}} extremely heterogeneous family of neoplasms that feature widely varying genetic, biological, and clinical characteristics. About 50 different lung cancer variants are recognized under the 2004 revision of the World Health Organization ("WHO-2004") histological typing system, the most widely recognized and used lung cancer classification scheme. Recent studies have shown beyond doubt that the old <b>classification</b> <b>paradigm</b> of [...] "small cell carcinoma vs. non-small cell carcinoma" [...] is now obsolete, and that the correct [...] "subclassification" [...] of lung cancer cases is necessary to assure that patients receive optimum management.|$|E
5000|$|Cheyenne is a morphologically polysynthetic {{language}} with a sophisticated, agglutinating verb system contrasting {{a relatively simple}} noun structure. [...] Many Cheyenne verbs can stand alone in sentences, and can be translated by complete English sentences. Aside from its verb structure, Cheyenne has several grammatical features that are typical of Algonquian languages, including an animate/inanimate noun <b>classification</b> <b>paradigm,</b> an obviative third person and distinction of clusivity {{in the first person}} plural pronoun.|$|E
40|$|Abstract. Classifier {{combination}} {{falls in}} the so called machine learning area. Its aim is to combine some <b>classification</b> <b>paradigms</b> {{in order to improve}} the individual accuracy of the component classifiers. Classifier hierarchies are an alternative among the several methods of classifier combination. In this paper we present new results about a recently proposed hierarchy construction method. Experiments have been carried out over 42 databases from the UCI repository, showing an improvement over the performance of the base classifiers...|$|R
25|$|The {{historic}} and modern hunting uses of Rhodesian Ridgebacks have included everything from upland game birds to larger 'dangerous game'. While the hunting versatility of the breed has served it {{well in the}} field, it has caused much confusion and contention among ridgeback fanciers about what these dogs are, and are not, as hunting companions. Throughout its history, the Rhodesian ridgeback has been a breed of dog that has somewhat defied the strict interpretation of most conventional group <b>classification</b> <b>paradigms.</b>|$|R
40|$|This paper {{presents}} SHALMANESER, {{a software}} package for shallow semantic parsing, the automatic assignment of semantic classes and roles to free text. SHALMANESER is a toolchain of independent modules communicating through a common XML format. System output can be inspected graphically. SHALMANESER {{can be used}} either as a “black box ” to obtain semantic parses for new datasets (classifiers for English and German frame-semantic analysis are included), or as a research platform that can be extended to new parsers, languages, or <b>classification</b> <b>paradigms.</b> 1...|$|R
5000|$|Lung cancers are now {{considered}} {{a large and}} extremely heterogeneous family of neoplasms that feature widely varying genetic, biological, and clinical characteristics. About 50 different lung cancer variants are recognized under the 2004 revision of the World Health Organization ("WHO-2004") histological typing system, the most widely recognized and used lung cancer classification scheme. Recent studies have shown beyond doubt that the old <b>classification</b> <b>paradigm</b> of [...] "small cell carcinoma vs. non-small cell carcinoma" [...] is now obsolete, and that the correct [...] "subclassification" [...] of lung cancer cases is necessary to assure that patients receive optimum management.|$|E
5000|$|An {{explosion}} of new knowledge, accumulated mainly {{over the last}} 20 years, has proved that lung cancers should be considered an extremely heterogeneous family of neoplasms with widely varying genetic, biological, and clinical characteristics, particularly their responsiveness to {{the large number of}} newer treatment protocols. Well over 50 different histological variants are now recognized under the 2004 revision of the World Health Organization ("WHO-2004") typing system, currently the most widely used lung cancer classification scheme. Recent studies have shown beyond doubt that the old clinical <b>classification</b> <b>paradigm</b> of [...] "SCLC vs. NSCLC" [...] is now obsolete, and that correct [...] "subclassification" [...] of lung cancer cases is necessary to assure that lung cancer patients receive optimum management.|$|E
40|$|Abstract. Extra {{information}} is often readily available but not utilized in a <b>classification</b> <b>paradigm.</b> Here we explore using extra labels (profile faces and rotated faces) {{to aid in}} distinguishing faces versus non-faces. We propose a way to combine simple discriminant classifiers to build a more complex ones and justify the combination in a probabilistic setting. ...|$|E
40|$|A {{family of}} parsimonious shifted {{asymmetric}} Laplace mixture models is introduced. We extend {{the mixture of}} factor analyzers model to the shifted asymmetric Laplace distribution. Imposing constraints on the constitute parts of the resulting decomposed component scale matrices leads to a family of parsimonious models. An explicit two-stage parameter estimation procedure is described, and the Bayesian information criterion and the integrated completed likelihood are compared for model selection. This novel family of models is applied to real data, where it is compared to its Gaussian analogue within clustering and <b>classification</b> <b>paradigms...</b>|$|R
40|$|Abstract. In {{this paper}} an {{empirical}} evaluation of different generative scores for expression microarray data classification is proposed. Score spaces represent a quite recent {{trend in the}} machine learning community, taking {{the best of both}} generative and discriminative <b>classification</b> <b>paradigms.</b> The scores are extracted from topic models, a class of highly interpretable probabilistic tools whose utility in the microarray classification context has been recently assessed. The experimental evaluation, performed on 3 literature datasets and with 7 score spaces, demonstrates the viability of the proposed scheme and, for the first time, it compares pros and cons of each space. ...|$|R
40|$|We {{describe}} {{a novel approach}} for real-time fuzzy classification of spectral data using autonomous agents. The immediate goal {{of this approach is}} to provide an interactive (real-time) image classification middleware as a feedback backbone for a control or monitoring system. The domain of this experiment is a controlled combustion chamber, with a monitoring system that checks the concentration of specified elements in the effuse. The proposed middleware acts as an intelligent peer to provide `timely' and accurate feedback to the control system on real-time classification of the spectral data images. We discuss the results of implementing three <b>classification</b> <b>paradigms</b> for this middleware, and their performances in terms of accuracy and interactivity...|$|R
40|$|We {{describe}} a state-of-the-art automatic {{system that can}} acquire subcategorisation frames from raw text for a free word-order language. We use it to construct a subcate-gorisation lexicon of German verbs from a large Web page corpus. With an automatic verb <b>classification</b> <b>paradigm</b> we evaluate our subcategorisation lexicon against a pre-vious classification of German verbs; the lexicon produced by our system performs better than the best previous results. ...|$|E
40|$|Garner's speeded <b>classification</b> <b>paradigm</b> is {{the tool}} {{of choice for}} gauging the effect of {{irrelevant}} information on the perception of task information. The paradigm consists of three tasks. In Baseline, task-irrelevant stimulus dimensions are held constant and the participant classifies values on the task-relevant dimension. In Filtering, the participant again classifies values on the relevant dimension but values on the task-irrelevant dimension also vary from trial to trial in a random fashion. Finally, in Correlation the task-irrelevant values vary again but now in correspondence with values of the task-relevant dimension. In a series of experiments with the same stimuli, we manipulated the perceptual salience of the task-irrelevant dimension. The results showed that making the irrelevant stimuli salient impaired task performance in Filtering. However, the same manipulation improved task performance in Correlation. We conclude that attention-grabbing irrelevant information is not always detrimental to performance. Whether or not such information disrupts performance depends on {{its relationship with the}} task-information. Garner's speeded <b>classification</b> <b>paradigm</b> is a popular tool in the investigation of selectiv...|$|E
40|$|The hidden {{units in}} {{multi-layer}} perceptrons {{are believed to}} act as feature extractors. In other words, the outputs of the hidden units represent the features in a more traditional statistical <b>classification</b> <b>paradigm.</b> This viewpoint offers a statistical, objective approach to determining the optimal number of hidden units required. This approach {{is based on an}} F-ratio test, and proceeds in an iterative fashion. The method and its application to simulated time-series data are presented...|$|E
40|$|Abstract — We {{address the}} problem of human motion {{recognition}} in this paper. The goal of human motion recognition is to recognize the type of motion recorded in a video clip, which consists of a set of temporarily ordered frames. By defining a Mercer kernel between two video clips directly, we propose in this paper a recognition strategy that can incorporate both the information of each individual frame and the temporal ordering between frames. Combining the proposed kernel with the support vector machine, {{which is one of the}} most effective <b>classification</b> <b>paradigms,</b> the resulting recognition strategy exhibits excellent performance over real data sets. Indexed Terms — human motion recognition, convolution kernels, support vector machines. I...|$|R
40|$|Support Vector Machines is a {{very popular}} machine {{learning}} technique. De-spite of all its theoretical and practical advantages, SVMs could produce sub-optimal results with imbalanced datasets. That is, an SVM classifier trained on an imbalanced dataset can produce suboptimal models which are biased towards the majority class and have low performance on the minority class, {{like most of the}} other <b>classification</b> <b>paradigms.</b> There have been various data preprocessing and algorithmic techniques proposed in the literature to allevi-ate this problem for SVMs. This chapter aims to review these techniques. Support Vector Machines (SVMs) [1, 2, 3, 4, 5, 6, 7] is a popular machine learning technique, which has been successfully applied to many real-world classification problems from various domains. Due to its theoretical and prac...|$|R
50|$|<b>Paradigm</b> <b>classification</b> in {{ontology}} is {{a two-dimensional}} classification scheme, {{such as a}} spreadsheet. It is a subset of faceted classification.|$|R
40|$|Battlefield commanders, {{as well as}} {{autonomous}} vehicle systems, must {{have the ability to}} make near real-time mobility assessments of large terrain regions. A context-dependent terrain <b>classification</b> <b>paradigm</b> is proposed through a terrain mobility model that employs the notion of cooperating mission, agent and situation experts. The model attempts to produce a time-sensitive mobility view of the environment by classifying terrain into homogeneous regions, each with an associated traversal costUS Army Combat Development Experimentation Center Fort Ord, CA 93941 - 5012 [URL] ATEC 46 - 8 N...|$|E
40|$|The goal of {{this paper}} is to o#er a {{framework}} for classification of images and video according to their "type", or "style" [...] a problem which is hard to define, but easy to illustrate; for example, identifying an artist by the style of his/ her painting, or determining the activity in a video sequence. The paper o#ers a simple <b>classification</b> <b>paradigm</b> based on local properties of spatial or spatio-temporal blocks. The learning and classification are based on the naive Bayes classifier. A few experimental results are presented...|$|E
40|$|The current {{bifurcated}} conflict <b>classification</b> <b>paradigm</b> {{for applying}} the Law of Armed Conflict (LOAC) {{has lost its}} usefulness. Regulation of state militaries was originally {{based on the principle}} that the armed forces of a state were acting as the sovereign agents of the state and were granted privileges and given duties based on that grant of agency. These privileges and duties became the bases for the formulation of the modern LOAC. During the twentieth century, the LOAC became bifurcated, with the complete LOAC applying only to armed conflicts between sovereigns and only few provisions of the law applying to armed conflicts that were not between sovereigns. This bifurcation has led to a lack of clarity for the sovereign 2 ̆ 7 s agents in LOAC application and given states the ability to manipulate which law applies to application of force through their agents. The applicability of the LOAC should no longer be based on the manipulable and unclear conflict <b>classification</b> <b>paradigm,</b> but should instead return to its foundations in the sovereign 2 ̆ 7 s grant of agency. Thus, any time a sovereign applies violent force through its armed forces, those armed forces should apply the full LOAC to their actions, regardless of the type or classification of the conflict...|$|E
40|$|Abstract Visual scene {{understanding}} is a fundamen-tal task in computer vision systems. Traditional appear-ance-based <b>classification</b> <b>paradigms</b> struggle {{to cope with}} the view and appearance variations of indoor scenes and functional objects. In this paper, we present a Stochas-tic Scene Grammar (SSG) model to parse indoor im-ages. The grammar is defined on a Function-Geometry-Appearance (FGA) hierarchy based on two observa-tions: i) Functionality is the most essential property to define an indoor object, e. g. “an object to sit on” defines a chair, ii) The geometry (3 D shape) of an ob-ject is designed to serve its function. We formulate the nature of the object functionality and contextual rela-tions into the Stochastic Scene Grammar model, which characterizes a joint distribution over the FGA hierar-chy. This hierarchical structure includes both functiona...|$|R
40|$|Understanding how humans {{search for}} objects in a vi‑ sual scene is a {{fascinating}} yet poorly understood topic. The <b>classification</b> image <b>paradigm,</b> originally developed with 1 ‑D signals for auditory psychophysics and later extended to images (2 ‑D signals) for vision research, in order to study observer strategies in a vernier acuity tas...|$|R
40|$|We {{present an}} {{empirical}} {{investigation of the}} modeling techniques for identifying fault-prone software components early in the software life cycle. Using software complexity measures, the techniques build models which classify components as likely to contain faults or not. The modeling techniques applied in this study cover the main <b>classification</b> <b>paradigms,</b> including principal component analysis, discriminant analysis, logistic regression, logical classification models, layered neural networks, and holographic networks. Experimental results are obtained from 27 academic software projects. We evaluate the models with respect to four criteria: predictive validity, misclassification rate, achieved quality, and verification cost. A surprising result is that no model is able to discriminate between components with faults and components without faults. 1 : Introduction Software complexity metrics are often used as indirect metrics of reliability since they can be obtained relatively early in [...] ...|$|R
40|$|In a {{standard}} supervised <b>classification</b> <b>paradigm,</b> stimuli are presented sequentially, participants make a classification, and feedback follows immediately. In this article, {{we use a}} semisupervised <b>classification</b> <b>paradigm,</b> in which feedback is given after a prespecified percentage of trials only. In Experiment 1, feedback was given in 100 %, 0 %, 25 %, and 50 % of the trials. Previous research reported by Ashby, Queller, and Berretty (1999) indicated that in an information-integration task, perfect accuracy was obtained supervised (100 %) but not unsupervised (0 %). Our results show that in both the 100 % and 50 % conditions, participants were able to achieve maximum accuracy. However, in the 0 % and the 25 % conditions, participants failed to learn. To discover {{the influence of the}} no-feedback trials on the learning process, the 50 % condition was replicated in Experiment 2, substituting unrelated filler trials for the no-feedback trials. The results indicated that accuracy rates were similar, suggesting no impact of the no-feedback trials on the learning process. The possibility of ever learning in a 25 % setting was also researched in Experiment 2. Using twice as many trials, the results showed that all but 2 participants succeeded, suggesting that only the total number of feedback trials is important. The impact of the semisupervised learning results for ALCOVE, COVIS, and SPEED models is discussed...|$|E
40|$|Indoor {{functional}} objects exhibit large {{view and}} appear-ance variations, thus {{are difficult to}} be recognized by the traditional appearance-based <b>classification</b> <b>paradigm.</b> In this paper, we present an algorithm to parse indoor images based on two observations: i) The functionality is the most essential property to define an indoor object, e. g. “a chair to sit on”; ii) The geometry (3 D shape) of an object is designed to serve its function. We formulate {{the nature of the}} object function into a stochastic grammar model. This model char-acterizes a joint distribution over the function-geometry-appearance (FGA) hierarchy. The hierarchical structur...|$|E
40|$|This thesis compares hand-designed {{features}} {{with features}} learned by feature learning methods in video classification. The features learned by Principal Component Analysis whitening, Independent subspace analysis and Sparse Autoencoders {{were tested in}} a standard Bag of Visual Word <b>classification</b> <b>paradigm</b> replacing hand-designed features (e. g. SIFT, HOG, HOF). The classification performance was measured on Human Motion DataBase and YouTube Action Data Set. Learned features showed better performance than the hand-desined features. The combination of hand-designed features and learned features by Multiple Kernel Learning method showed even better performance, including cases when hand-designed features and learned features achieved not so good performance separately...|$|E
40|$|Abstract. Universal Nearest Neighbours (unn) is a {{classifier}} recently proposed, {{which can}} also effectively estimates the posterior probability of each classification act. This algorithm, intrinsically binary, {{requires the use of}} a decomposition method to cope with multiclass problems, thus reducing their complexity in less complex binary subtasks. Then, a reconstruction rule provides the final classification. In this paper we show that the application of unn algorithm in conjunction with a recon-struction rule based on the posterior probabilities provides a classifica-tion scheme robust among different biomedical image datasets. To this aim, we compare unn performance with those achieved by Support Vec-tor Machine with two different kernels and by a k Nearest Neighbours classifier, and applying two different reconstruction rules for each of the aforementioned <b>classification</b> <b>paradigms.</b> The results on one private and five public biomedical datasets show satisfactory performance. ...|$|R
40|$|Abstract: Classifying text {{data has}} been an active area of {{research}} for a long time. Text document is multifaceted object and often inherently ambiguous by nature. Multi-label learning deals with such ambiguous object. Classification of such ambiguous text objects often makes task of classifier difficult while assigning relevant classes to input document. Traditional single label and multi class text <b>classification</b> <b>paradigms</b> cannot efficiently classify such multifaceted text corpus. Through our paper we are proposing a novel label propagation approach based on semi supervised learning for Multi Label Text Classification. Our proposed approach models the relationship between class labels and also effectively represents input text documents. We are using semi supervised learning technique for effective utilization of labeled and unlabeled data for classification. Our proposed approach promises better classification accuracy and handling of complexity and elaborated {{on the basis of}} standard datasets such as Enron, Slashdot and Bibtex...|$|R
40|$|In {{this paper}} we compare the {{performance}} of two image <b>classification</b> <b>paradigms</b> (object- and pixel-based) for creating a land cover map of Asmara, the capital of Eritrea and its surrounding areas using a Landsat ETM+ imagery acquired in January 2000. The image classification methods used were maximum likelihood for the pixel-based approach and Bhattacharyya distance for the object-oriented approach available in, respectively, ArcGIS and SPRING software packages. Advantages and limitations of both approaches are presented and discussed. Classifications outputs were assessed using overall accuracy and Kappa indices. Pixel- and object-based classification methods result in an overall accuracy of 78 % and 85 %, respectively. The Kappa coefficient for pixel- and object-based approaches was 0. 74 and 0. 82, respectively. Although pixel-based approach is {{the most commonly used}} method, assessment and visual interpretation of the results clearly reveal that the object-oriented approach has advantages for this specific case-study...|$|R
40|$|The {{semantic}} congruity effect {{refers to}} the facilitation of judgements (i) when {{the direction of the}} comparison of two items coincides with the relative position of the items along the dimension comparison or (ii) when the relative size of a standard and a target stimulus coincides. For example, people are faster in judging 'which is bigger?' for two large items, than judging 'which is smaller?' for two large items (selection paradigm). Also, people are faster in judging a target stimulus as smaller when compared to a small standard, than when compared to a large standard, and vice versa (<b>classification</b> <b>paradigm).</b> We use the Drift Diffusion Model (DDM) to explain the time course of a semantic congruity effect in a <b>classification</b> <b>paradigm.</b> Formal modelling of semantic congruity allows the time course of the decision process to be described, using an established model of decision making. Moreover, although there have been attempts to explain the semantic congruity effect within evidence accumulation models, two possible accounts for the congruity effect have been proposed but their specific predictions have not been compared directly, using a model that could quantitatively account for both; a shift in the starting point of evidence accumulation or a change in the rate at which evidence is accumulated. With our computational investigation we provide evidence for the latter, while controlling for other possible explanations such as a variation in non-decision time or boundary separation, that have not been taken into account in the explanation of this phenomenon...|$|E
40|$|A new <b>classification</b> <b>paradigm,</b> which {{automatically}} acquires WordNet-Based rules from a corpus, is presented. The {{approach is}} applied to developing an autonomous software agent that can recognize emotions which are expressed in natural language during an interactive human-computer environment. Such an agent could adapt to a user’s emotional state and dynamically adjust its interaction etiquette. Hierarchical concepts of WordNet’s noun and verb hypernymy are the basic building blocks of the classification rules. A greedy learning algorithm automatically determines which hierarchical concepts are best suited for each rule. A corpus of 5000 emotional sentences has been compiled from 502 test subjects and serves as input to the system. ...|$|E
40|$|Abstract — Multiple {{classifier}} systems (MCS) {{have become}} a popular <b>classification</b> <b>paradigm</b> for strong generalization performance. Diversity measures {{play an important role}} in constructing and explaining multiple classifier systems. The first observation concerning Arabian manuscript reveals the complexity of the task, especially for the used ensemble of classifier for recognitions. In this paper we propose a new approach Based on “Overproduce and select Paradigm ” for Arabic handwritten recognition. It combines diversity measures and individual classifiers accuracy for selecting the best classifier sub. We have tested this approach with three fusion methods (Voting, Weighted voting and Bks (Behavior Knowledge Space)). The obtained experimental results are very encouraging...|$|E
40|$|This paper {{proposes to}} {{generate}} {{and to use}} barcodes to annotate medical images and/or their regions of interest such as organs, tumors and tissue types. A multitude of efficient feature-based image retrieval methods already exist that can assign a query image to a certain image class. Visual annotations may help to increase the retrieval accuracy if combined with existing feature-based <b>classification</b> <b>paradigms.</b> Whereas with annotations we usually mean textual descriptions, in this paper barcode annotations are proposed. In particular, Radon barcodes (RBC) are introduced. As well, local binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as barcodes. The IRMA x-ray dataset with 12, 677 training images and 1, 733 test images is used to verify how barcodes could facilitate image retrieval. Comment: To be published in proceedings of The IEEE International Conference on Image Processing (ICIP 2015), September 27 - 30, 2015, Quebec City, Canad...|$|R
40|$|Third International Workshop, MLMI 2012, Held in Conjunction with MICCAI 2012 International audienceUniversal Nearest Neighbours (unn) is a {{classifier}} recently proposed, {{which can}} also effectively estimates the posterior probability of each classification act. This algorithm, intrinsically binary, {{requires the use of}} a decomposition method to cope with multiclass problems, thus reducing their complexity in less complex binary subtasks. Then, a reconstruction rule provides the final classification. In this paper we show that the application of unn algorithm in conjunction with a reconstruction rule based on the posterior probabilities provides a classification scheme robust among different biomedical image datasets. To this aim, we compare unn performance with those achieved by Support Vector Machine with two different kernels and by a k Nearest Neighbours classifier, and applying two different reconstruction rules for each of the aforementioned <b>classification</b> <b>paradigms.</b> The results on one private and five public biomedical datasets show satisfactory performance...|$|R
40|$|The "Westland" set of {{empirical}} accelerometer helicopter data with seeded and labeled faults is analyzed {{with the aim}} of condition monitoring. The autoregressive (AR) coefficients from a simple linear model encapsulate {{a great deal of information}} in a relatively few measurements; and it has also been found that augmentation of these by harmonic and other parameters call improve classification significantly. Several techniques have been explored, among these restricted Coulomb energy (RCE) networks, learning vector quantization (LVQ), Gaussian mixture classifiers and decision trees. A problem with these approaches, and in common with many <b>classification</b> <b>paradigms,</b> is that augmentation of the feature dimension can degrade classification ability. Thus, we also introduce the Bayesian data reduction algorithm (BDRA), which imposes a Dirichlet prior oil training data and is thus able to quantify probability of error in all exact manner, such that features may be discarded or coarsened appropriately...|$|R
