14|42|Public
40|$|Many tools {{have been}} built to analyze source code. Most of these tools do not {{adequately}} support reengineering activities {{because they do not}} allow a software engineer to simultaneously perform queries about both the existing and the desired source structure. This paper introduces the <b>conceptual</b> <b>module</b> approach that overcomes this limitation. A <b>conceptual</b> <b>module</b> is a set of lines of source that are treated as a logical unit. We show how the approach simplifies the gathering of source information for reengineering tasks, and describe how a tool to support the approach was built as a front-end to existing source analysis tools. Keywords Source code analysis, software reuse, code scavenge, software structure, modularization, reverse engineering 1 INTRODUCTION Many reengineering activities performed by software engineers require reasoning about the source code for the system. Part of the reengineering process, for instance, may involve the identification and formation of new software c [...] ...|$|E
40|$|A <b>conceptual</b> <b>module</b> {{capable of}} scene interpreta-tion for {{incident}} detection has been implemented us-ing a relational network approach and {{a concept of}} dynamic grouping to ease the complexity involved in computing multiple object behaviours. This system has been tested on several roundabout scenarios and has given good results using simulated perfect data. Using more realistic data necessitated the handling of multiple hypotheses on the object classes. This was easily handled using the method developed and even though this increased {{the complexity of the}} system, on-line response was maintained. ...|$|E
40|$|Human-centred design: an {{emergent}} <b>conceptual</b> <b>module</b> by Zhang T and Dong H) Understanding {{human needs}} and how design responds to human needs {{are essential for}} human-centred design (HCD). By combining Maslow’s hierarchy of needs model and Küthe’s “design and society” model, this paper proposes a conceptual model of human-centred design which marries psychology and sociology in investigating the relationship between design and human needs. The study reveals a tendency that design evolution responds to the hierarchy of human needs. Nowadays design tends to care for more levels of human needs...|$|E
40|$|Those who assume domain-specificity, or {{conceptual}} modularity, {{must face}} “Fodor's Paradox” (the problem of “combinatorial explosion”). One strategy involves postulating a “metamodule” that evolved {{to take as}} input the output of all other specialized <b>conceptual</b> <b>modules,</b> and then integrate these outputs into cross-domain thoughts. It's difficult to see whether this proposed metamodular capacity stems from language or Theory of Mind...|$|R
40|$|Abstract—Conventional {{distributed}} system courses follow a syllabus {{in which a}} list of topics is discussed independently and {{at different levels of}} abstractions. We propose to use a wireless sensor network environment to pin all topics down to concrete applications and to maintain issues such as fault tolerance and coordination continuously present. We describe a syllabus with eight <b>conceptual</b> <b>modules,</b> each of them associated to a hands-on experience with wireless sensor networks, which may be assigned either as homework or as a hands-on class, depending on the number of classroom hours that are available. I...|$|R
40|$|Aircraft {{conceptual}} design often focuses on unconventional configurations like for example forward swept wings. Assessing {{the characteristics of}} these configurations usually {{requires the use of}} physic based analysis modules. This {{is due to the fact}} that for unconventional configurations no sucient database for historic based analysis modules is available. Nevertheless, physic based models require a lot of input data and their computational cost can be high. Generating input values in a trade study manually is work-intensive and error-prone. <b>Conceptual</b> design <b>modules</b> can be used to generate sucient input data for physic based models and their results can be re-integrated into the {{conceptual design}} phase. In this study a direct link between a <b>conceptual</b> design <b>module</b> and an aerodynamic design module is presented. Geometric information is generated by the <b>conceptual</b> design <b>module</b> and the physic based results, in form of the Oswald factor, are then fed back. Apart from the direct link, an equation for determination of the Oswald factor is derived via a Symbolic Regression Approach...|$|R
40|$|With {{the recent}} {{technological}} advances, {{it is possible}} to monitor vital signs using Bluetooth-enabled biometric mobile devices such as smartphones, tablets or electric wristbands. In this manuscript, we present a system to estimate the risk of cardiovascular diseases in Ambient Assisted Living environments. Cardiovascular disease risk is obtained from the monitoring of the blood pressure by means of mobile devices in combination with other clinical factors, and applying reasoning techniques based on the Systematic Coronary Risk Evaluation Project charts. We have developed an end-to-end software application for patients and physicians and a rule-based reasoning engine. We have also proposed a <b>conceptual</b> <b>module</b> to integrate recommendations to patients in their daily activities based on information proactively inferred through reasoning techniques and context-awareness. To evaluate the platform, we carried out usability experiments and performance benchmarks...|$|E
40|$|AbstractCNC {{technology}} is the key technology of the machine tools, which are the base of industrial unit computerization. CNC machines are operated by controllers, {{each of which has}} a software module inside known as interpreter. The function of interpreter is to extract data from CAM system generated code and convert to controller motion commands. However, with the development of numerical control technology existing CNC systems are limited with the interpreter lacking in expansibility, modularity and openness. In order to overcome these problems open architecture control was introduced. In this paper, a <b>conceptual</b> <b>module</b> of new software system is presented. The developed system is able to interpret ISO 14649 and 6983 code and translate as per internal structure required by the CNC machine. It interprets position, feed rate, tool, spindle etc data and translates to CNC machine. At the mean time it is also able to generate output in text and XML files as per user defined file structure...|$|E
40|$|In {{this paper}} we {{describe}} {{our approach to}} reconstructing the software architecture of J 2 EE web applications. We use the Siemens Four Views approach, separating the architecture into <b>conceptual,</b> <b>module,</b> execution, and code views. We paid {{particular attention to the}} dependencies in the implementation, which led to two results. One is the distinction between a traditional usage dependency and a more superficial “knows” dependency, where one module knows of another by type name but nothing else. Another is the recognition of important but implicit dependencies in web applications, between a client page formatting a request and a module interpreting the request. We make these explicit as “logical interfaces. ” Using separate views improves our ability to describe these architectures, and we provide a scenario showing how it helps a developer understand the impact of changes to the application. Although we have not yet developed tools to automate such a reconstruction, this paper provides a critical first step in describing what should be present in an architecture description, and how this information can be deduced from the implementation. ...|$|E
40|$|This paper {{sketches}} {{a solution}} to a problem which has been emphasized by Fodor. This is {{the problem of how to}} explain distinctively-human flexible cognition in modular terms. There are three aspects to the proposed account. First, it is suggested that natural language sentences might serve to integrate the outputs of a number of <b>conceptual</b> <b>modules.</b> Second, a creative sentence-generator, or supposer, is postulated. And third, it is argued that a set of principles of inference to the best explanation can be constructed from already-extant aspects of linguistic testimony and discourse interpretation. Most importantly, it is suggested that the resulting architecture should be implementable in ways that are computationally tractable...|$|R
40|$|While {{integrating}} {{linguistic knowledge}} {{of any kind}} is becoming an almost implicit practice in natural language understanding systems, the inclusion of cultural or world knowledge in these tools might have been neglected sometimes. However, a NLP system or knowledge base enriched with cultural information is a more robust, better cohesioned instrument for natural language understanding processes. The integration {{of this type of}} knowledge in NLP systems may be proven to contribute to solving some phenomena that occur in natural language, such as anaphor, metaphor and metonymy, ambiguity or co-reference, amongst others. The objective {{of this article is to}} describe the way FunGramKB (a knowledge base) integrates cultural knowledge in its <b>conceptual</b> <b>modules</b> and, in particular, how the information contained in the Onomasticon module of FunGramKB can contribute to maximising the informativeness and completeness of the whole system, thus resolving ambiguity problems in a determined linguistic phenomenon: anaphora...|$|R
40|$|Abstract—We {{describe}} a cognitive architecture (LIDA) that affords attention, action selection and human-like learning {{intended for use}} in controlling cognitive agents that replicate human experiments as well as performing real-world tasks. LIDA combines sophisticated action selection, motivation via emotions, a centrally important attention mechanism, and multimodal instructionalist and selectionist learning. Empirically grounded in cognitive science and cognitive neuroscience, the LIDA architecture employs a variety of modules and processes, {{each with its own}} effective representations and algorithms. LIDA has much to say about motivation, emotion, attention, and autonomous learning in cognitive agents. In this paper we summarize the LIDA model together with its resulting agent architecture, describe its computational implementation, and discuss results of simulations that replicate known experimental data. We also discuss some of LIDA’s <b>conceptual</b> <b>modules,</b> propose non-linear dynamics as a bridge between LIDA’s modules and processes and the underlying neuroscience, and point out some of the differences between LIDA and other cognitive architectures. Finally, we discuss how LIDA addresses some of the open issues in cognitive architecture research. Index Terms—Autonomous agent, Cognitive model...|$|R
40|$|International audienceThis paper {{describes}} {{a simple but}} practical methodology to identify the contribution of primary and secondary air pollutants from the local/regional emission sources to Hong Kong, a highly urbanized city with complex terrain and coastlines. The meteorological model MM 5 coupled with a three-dimensional, mutli-particle trajectory model is used to identify salient aspects of regional air pollutant transport characteristics during some typical meteorological conditions over the Pearl River Delta (PRD) region. Several weighting factors are determined for calculating the air mass/pollutant trajectory and are {{used to evaluate the}} local and regional contribution of primary pollutants over the PRD to Hong Kong pollution. The relationships between emission inventories, physical paths and chemical transformation rates of the pollutants, and observational measurements are formulated. The local and regional contributions of secondary pollutants are obtained by this <b>conceptual</b> <b>module</b> under different weather scenarios. Our results demonstrate that major pollution sources over Hong Kong come from regional transport. In calm-weather situations, 78 % of the respirable suspended particulates (RSP) totals in Hong Kong are contributed by regional transport, and 49 % are contributed by the power plants within the PRD. In normal-day situations, 71 % of the RSP are contributed by regional transport, and 45 % are contributed by the power plants...|$|E
40|$|AbstractFunGramKB is a {{multilingual}} and multipurpose lexico-conceptual knowledge-base {{designed for}} its use in various tasks in Natural Language Processing (NLP), like information extraction and retrieval, machine translation or artificial reasoning (Periñán and Arcas, 2004; Mairal and Periñán, 2009; 2010). Its modular structure reflects three levels of knowledge—lexical, grammatical and ontological— which, though independent, are interrelated through the <b>conceptual</b> <b>module</b> and divided into three others: the Ontology, the Cognicon and the Onomasticon. Moreover, the Ontology represents a hierarchical catalogue of concepts that describe semantic knowledge organized in three subontologies, whose metaconcepts correspond to #ENTITY, #EVENT and #QUALITY, which permit the internal organization of nouns, verbs and adjectives respectively. Each of these subontologies is divided as well into three groups: metaconcepts (they represent cognitive dimensions), basic concepts (which comprise common sense knowledge) and terminal concepts (that provide expert knowledge). In this contribution we analyze some criminal offences prototypical from the legal domain, such as “corruption”, “extortion” and “forgery”, derived from their corresponding verbs. We have chosen these as examples of terminal concepts that attest {{that it is possible}} to integrate expert knowledge in FunGramKB, thanks to the conceptual representation language COREL, common to the three main modules of the conceptual level. The detailed analysis of the above mentioned entities will ascertain not only {{that it is possible to}} reuse the information from the Meaning Postulates (MPs) of the events from which they derive, but also that the information already included in the knowledge base is maximized...|$|E
40|$|We {{show how}} two web-based {{architecture}} trend analysis tools, considering different views and their changes over time, {{contribute to our}} daily effort {{to cope with the}} complexity of a large, software-intensive medical imaging system. 1 Summary of the presentation Philips Medical Systems develops large SW-intensive embedded imaging systems, comprising hundreds of hardware and software components. To be competitive in the market, the development process must meet several demanding and often conflicting requirements: high product quality, short time-to-market and low costs. Success depends, among other things, on an appropriately managed software architecture. Software architecture can be described by four related views [2], i. e. <b>conceptual,</b> <b>module</b> interconnection, code and execution architecture. Software architects take the responsibility for specifying these views correctly and keeping them consistent. To manage the software architecture, architects and designers continuously need up-to-date information about ongoing activities, problems that arise, and recent progress. This information can be related to any of the views mentioned above, and must be structured {{in such a way that}} navigation from general to more detailed information is possible and dependencies between different views can easily be traced. Considering the huge amount of available data, adequate tool support is needed to collect, process and present the required information. We describe first experiences with two trend analysis tools, which are integrated in the daily software development process. One is the Code and Module Architecture Dashboard (CMAD) for monitoring changes of different code, module, and interconnection metrics organized according to the module hierarchy. The other, called Execu...|$|E
40|$|From {{the history}} of mathematics, {{it is clear that}} some {{numerical}} concepts are far more pervasive than others. In a densely multimodular mind, evolved cognitive abilities lie at the basis of human culture and cognition. One possible way to explain the differential spread and survival of cultural concepts based on this assumption is the epidemiology of culture. This approach explains the relative success of cultural concepts as a function of their fit with intuitions provided by <b>conceptual</b> <b>modules.</b> A wealth of recent evidence from animal, infant, and neuroimaging studies suggests that human numerical competence is rooted in an evolved number module. In this study, I adopted an epidemiological perspective to examine the cultural transmission of numerical concepts in {{the history of}} mathematics. Drawing on historical and anthropological data on number concepts, I will demonstrate that positive integers, zero, and negative numbers have divergent cultural evolutionary histories owing to a distinct relationship with the number module. These case studies provide evidence for the claim that science can be explained in terms of evolved cognitive abilities that are universal in Homo sapiens. status: publishe...|$|R
40|$|Abstract:- This paper {{presents}} a preliminary study of {{an approach that}} models programmable logic controllers (PLCs) for their effective deployment in industrial control processes. A working model is developed for automatic allocation of PLCs and also a formal verification of Ladder Diagram representations of control processes using the Symbolic Model Verifier (SMV) tool. Automatic resource allocation is achieved through the proposition of a digraph model for any Ladder Diagram representation of a control process, which is then translated into an XML (Extensible Mark Up Language) model. The required PLC resources needed to implement a control process are extracted from the XML model. These resources are then used by a selection engine to determine, from a PLC database, the most appropriate PLCs or Embedded Controllers (EBCs) that can satisfy the resource requirements. Additionally, information extracted from the XML model is used to generate a formally verifiable SMV code of the system. This paper focuses on the practical implementation, testing, and verification of three <b>conceptual</b> <b>modules</b> applied to a control process. These are, the XML model of the control process, the PLC Database Automatic Resource Allocation, and the XML-to-SMV translator. This work was significantly motivated by the ever increasing number of industries who seek to increase their productivity by automating their processes...|$|R
40|$|We {{describe}} Module Analysis for Multiple Choice Responses (MAMCR), a new {{methodology for}} carrying out network analysis on responses to multiple choice assessments. This method is used to identify modules of non-normative responses which can then be interpreted {{as an alternative to}} factor analysis. MAMCR allows us to identify <b>conceptual</b> <b>modules</b> that are present in student responses that are more specific than the broad categorization of questions that is possible with factor analysis and to incorporate non-normative responses. Thus, this method may prove to have greater utility in helping to modify instruction. In MAMCR the responses to a multiple choice assessment are first treated as a bipartite, student X response, network which is then projected into a response X response network. We then use data reduction and community detection techniques to identify modules of non-normative responses. To illustrate the utility of the method we have analyzed one cohort of postinstruction Force Concept Inventory (FCI) responses. From this analysis, we find nine modules which we then interpret. The first three modules include the following: Impetus Force, More Force Yields More Results, and Force as Competition or Undistinguished Velocity and Acceleration. This method has a variety of potential uses particularly to help classroom instructors in using multiple choice assessments as diagnostic instruments beyond the Force Concept Inventory...|$|R
40|$|Each {{theoretical}} research field has a preferred {{set of data}} structures for modeling its set of conceptual information; for example, some focus on meta-data structures, while others prefer a strictly structured ontological model. This poster takes an analytical perspective on some engineering methods that bridge these, ostensibly quite dissimilar, data structures. It will {{be the result of}} this investigation, to attempt to link knowledge management, cataloging structures, and user perceptions of data models, seeing these as both related and complementary in nature. First, we examine the role of evidence from user-centred studies; for example, card sorting. These demonstrate individual user perceptions in a given area and how these perceptions tailor the collection of input data. Evidence from collaborative studies may also be evaluated regarding development of the underlining data structures. These can be applied to many evidence areas; such as, evaluation of fit between user perception of a task or topic area and the interpretation offered by an automated system. Secondly, we examine evidence from the analysis of the data sets themselves, guided in part by initial user input; this may be used as supporting evidence for evaluation purposes. This type of analysis can provide evidence of how, collaboratively, users' starting perceptions are shaped by past experiences, others' experiences and suggestions by automated systems. Lastly, we consider the evidence in the evaluation of linkages of the data sets from an ontology and the development of the data structures of a <b>conceptual</b> <b>module,</b> feeding into a more theoretical although evidence-led discussion of the relation between language, structure and evidence grounded within grounded data and user perceptions. All of these approaches together can be used as sources of information for eventual development of functional application elements, including ontological and meta-data structures, and data structures in a more general sense...|$|E
40|$|This {{dissertation}} {{focuses on}} investigating the feature and mechanism {{of local and}} regional scale atmospheric circulations and its associated pollution transport and trapping in the Pearl River Delta (PRD) region, which is a highly urbanized area with complex terrain and coastlines. The first step {{of this research was}} to set up a localized fine-scale (1 km) land-surface/urban modeling system coupled with the mesoscale meteorological model MM 5, to provide a more accurate and realistic prediction of meteorological condition over the region. After the model evaluations, a number of numerical experiments have been conducted {{in order to understand the}} impact of urbanization and its associated urban heat islands on meteorology over the PRD. The modeling system was then coupled with a particle trajectory model to demonstrate the unique land-sea breeze circulation features and the temporal evolution, and develop a conceptual model for the air pollution trapping phenomenon in the region. Further sensitivity experiments were used to illustrate the impact of urbanization and large-scale flows on the pollution processes. After that, a new methodology was introduced to identify the contribution of primary and secondary pollutants from the emissions of local/regional area sources and power plants to Hong Kong. Several weighting factors were established to the air mass/pollutant trajectory calculations and used to evaluate the local and regional contribution of primary pollutants to Hong Kong pollution. The relationships between emission inventories, physical paths and chemical transformation rates of the pollutants, and observational measurements were formulated. The local and regional contributions of secondary pollutants were obtained by this <b>conceptual</b> <b>module.</b> Finally, this study challenges the backward trajectory analysis, which is a well accepted analytical method in air quality research in Hong Kong. The result demonstrates large uncertainties in their behavior when comparing calculations using different resolution of wind fields, indicating that care must be taken on choosing the suitable resolution of wind fields when calculating trajectories for diagnostic studies...|$|E
40|$|Trade marks are {{valuable}} intangible intellectual property (IP) assets with potentially high reputational value {{that can be}} protected. Similarity between trade marks may potentially lead to infringement. That similarity is normally assessed based on the visual, conceptual and phonetic aspects of the trade marks in question. Hence, this thesis addresses this issue by proposing a trade mark similarity assessment support system that uses the three main aspects of trade mark similarity as a mechanism to avoid future infringement. A conceptual model of the proposed trade mark similarity assessment support system is first proposed and developed based on the similarity assessment criteria outlined in a trade mark manual. The proposed model is the first contribution of this study, and it consists of visual, conceptual, phonetic and inference engine modules. The second contribution of this work is an algorithm that compares trade marks based on their visual similarity. The algorithm performs a similarity assessment using content-based image retrieval (CBIR) technology and an integrated visual descriptor derived using the low-level image feature, i. e. the shape feature. The performance of the algorithm is then assessed using information retrieval based measures. The obtained result demonstrates better retrieval performance {{in comparison to the}} state of the art algorithm. The conceptual aspect of trade mark similarity is then examined and analysed using a proposed algorithm that employs semantic technology in the <b>conceptual</b> <b>module.</b> This contribution enables the computation of the conceptual similarity between trade marks, with the utilisation of an external knowledge source in the form of a lexical ontology, together with natural language processing and set similarity theory. The proposed algorithm is evaluated using both information VI retrieval and human collective opinion measures. The retrieval result produced by the proposed algorithm outperforms the traditional string similarity comparison algorithm in both measures. The phonetic module examines the phonetic similarity of trade marks using another proposed algorithm that utilises phoneme analysis. This algorithm employs phonological features, which are extracted based on human speech articulation. In addition, the algorithm also provides a mechanism to compare the phonetic aspect of trade marks with typographic characters. The proposed algorithm is the fourth contribution of this study. It is evaluated using an information retrieval based measure. The result shows better retrieval performance in comparison to the traditional string similarity algorithm. The final contribution of this study is a methodology to aggregate the overall similarity score between trade marks. It is motivated by the understanding that trade mark similarity should be assessed holistically; that is, the visual, conceptual and phonetic aspects should be considered together. The proposed method is developed in the inference engine module; it utilises fuzzy logic for the inference process. A set of fuzzy rules, which consists of several membership functions, is also derived in this study based on the trade mark manual and a collection of trade mark disputed cases is analysed. The method is then evaluated using both information retrieval and human collective opinion. The proposed method improves the retrieval accuracy and the experiment also proves that the aggregated similarity score correlates well with the score produced from human collective opinion. The evaluations performed in the course of this study employ the following datasets: the MPEG- 7 shape dataset, the MPEG- 7 trade marks dataset, a collection of 1400 trade marks from real trade mark dispute cases, and a collection of 378, 943 company names...|$|E
50|$|OpenRDK is an {{open-source}} {{software framework}} for robotics for developing loosely coupled modules. It provides transparent concurrency management, inter-process (via sockets) and intra-process (via shared memory) blackboard-based communication and a linking technique {{that allows for}} input/output data ports <b>conceptual</b> system design. <b>Modules</b> for connecting to simulators and generic robot drivers are provided.|$|R
40|$|In {{the present}} study we {{introduce}} a design environ- ment consisting of a framework, a central model and a newly developed <b>conceptual</b> design <b>module.</b> The Common Para- metric Aircraft Configuration Scheme (CPACS) is the stan- dard syntax definition for the exchange of information within preliminary airplane design at DLR. Several higher fidelity analysis modules are already connected to CPACS, includ- ing aerodynamics, primary structures, mission analysis and climate impact. The analysis modules can be interfaced via a distributed framework. To initialize the design processes, capabilities are needed {{to close the gap}} between top-level re- quirements and preliminary design. Additionally, results of a design loop need to be merged to generate inputs for fur- ther iterations and convergence control. For this purpose we developed a <b>conceptual</b> design <b>module</b> based on handbook methods where the focus is set on multi-fidelity. For the up- ward change in level of detail a knowledge-based approach is used for the generation of CPACS models. This includes the geometry generation, as well as additional data such as the mass breakdown and the tool-specific inputs for further anal- yses in higher fidelity modules. The feedback loop is closed downwards by reducing the granularity from the CPACS data set back to the level of conceptual design methods. The con- ceptual design module is object-oriented and concepts, both for parameter and method replacement, are introduced. First results for multi-fidelity calculations are shown...|$|R
40|$|AbstractIn {{recent past}} history of {{computer}} systems industry, for decades, {{the hegemony of}} a few de facto standards dictated by major proprietary commercial products dominated the Operating Systems (OS) field. In such technological context, conso- nantly to this trend, the knowledge objective focused by academical and training courses on OS-related disciplines has often been addressed more from the stand point of essential theoretical background than of the technical skills for actuation on de- sign and development field. Emerging paradigms, nevertheless, have been rapidly changing this scenario. Among them, the establishment of Open Source concept is boosting the growing diversity of new operating systems; concomitantly, evolution of embedded hardware architectures has {{make it possible to}} run sophisticated operating systems where only bare rudimentary, ad hoc system-software were once practical. Aligned along this perspective, this paper introduces a new platform for teaching and training programs on OS development founded on a project-based approach which guides the student throughout the process of designing and programming a sufficiently simple, but yet realistic and fully functional, OS from the scratch. The differential of the present proposal regarding related works is that, instead of either merely inspecting example-code or experimenting with simulators, the apprentice is guided across the challenge of coding an entire new instance of a didactic system specification. A comparison of the companion OS-example with existing alternatives brings out a less complex implementation structure which maps <b>conceptual</b> <b>modules</b> with implementation blocks in an intuitive correspondence and with reduced function cou- pling. Moreover, the learning platform comes with a courseware material consistently linked to the laboratory practices, and aimed at the systemic comprehension of the many related multidisciplinary aspects...|$|R
40|$|Women and Leadership Roles is {{culled from}} {{workshops}} conducted by Prof. Indira Parikh at the IIMA. From 1980 till date programmes exploring issues facing Women in Management are {{offered at the}} Institute. Issues surrounding leadership, work roles and authority are debated. The objectives are to {{explore the influence of}} the transformation of organisations on womens roles in the corporate world; to explore leadership roles and also individual life-spaces; to discover wholesome ways to actualise dreams and chart new career paths. The programmes are divided into two modules, Conceptual and Experiential. The <b>conceptual</b> <b>module</b> explores the impact of transformation in organisations on individual employees, particularly women. In the experiential module, the exploration is around life-spaces and systems where processes of socialisation in both family and work settings are highlighted. How did women who are impacted by these diverse interfaces give shape to their roles? The paper discusses the experiences of the participants at home and at the workplace. Shifts in the mindsets of people and the society have contributed to the acceptance of working women as capable, hardworking and committed professionals and individuals. On the other hand, women commonly feel a constant pressure to perform and prove themselves at the workplace and simultaneously, a persistent feeling of guilt in coping up with the expectations of the family at home. Although several women have been successful in striking a balance between home and work, not many have managed to assume leadership positions in the corporate world, which was still considered a mans domain. An important and interesting issue discussed in the paper is the exploration of womens life spaces, their identity and the roles they take, especially in terms of leadership. The life-space of women vis-�-vis the home and family and also vis-�-vis the workplace is analysed and discussed by the participants. At the home front the dynamics of in-laws, especially the interface of the women with their mother-in-law, their experience of motherhood and the dynamics of relationships with the husband, all contribute to the dilemmas of marriage. The women shared some of their personal experiences related to their entry into the workplace and their interfaces with their superiors, colleagues and subordinates of both genders. The dilemmas faced by women in terms of assuming leadership roles, climbing the corporate ladder and contributing to decision-making processes in the organisation are, anchored in the socio-cultural context {{as well as in the}} maps and definitions they carry from the past. Future scenarios were also painted by the participants. For the first time in recent history women have begun to assume leadership roles in the corporate world and are hopeful of blazing new trails for future generations and creating new role models. Women can look forward to the future with optimism. Women are experienced in managing one of the most complex organisation imaginable - the household, and therefore can apply their skills and experiences in terms of hard work and sensitivity in managing relationships, at the workplace. The authors discuss how women can be successful leaders if they achieve congruity between their inner instincts and their career goals. The Indian Woman today is at a threshold where she is confronting not only herself and her own inner feelings, historical conditioning and fears, but also managing interfaces in the outside world, both at home and workplace. As recent role models demonstrate, women tentatively are crossing this threshold, challenging themselves and blazing a new path for future generations. Indian women and the society as a whole has moved from well-entrenched gender-centric roles, where Man was considered the leader and provider and Woman the idealised deity, submissive and subservient to the wishes of her family. The revolutionary thinking that is emerging, partly due to education and Western influences, recognises roles that are not stereotyped by gender and allow men to recognise their femininity and women their masculinity. This transformation is far from complete; however important beginnings have been made particularly in the metropolitan cities and in tomorrows industries where enabling technologies have brought dramatic changes in terms of creating virtual workspaces. ...|$|E
40|$|In {{this work}} we present M-VIVIE, {{a system for}} video {{sequence}} indexing based on the identity of appearing subjects, which exploits a multithread architecture for fast processing. The system is composed of more <b>conceptual</b> component <b>modules,</b> each performing a specific kind of processing. Each module can possibly be substituted with a different one performing the same task. Special attention is devoted to classification and clustering activities. M-VIVIE presents a number of peculiarities, that distinguish it from existing systems with similar goals. Among these, the account for the concomitant appearance of two identities in the same clip, {{and the use of}} such information in the process of identity mapping. M-VIVIE was tested on different kinds of real video clips to assess both its efficacy and efficiency. (c) 2012 Elsevier B. V. All rights reserved...|$|R
40|$|The report {{provides}} {{details and}} {{background of a}} modeling approach {{to be used for}} identification and evaluation of future air/ground communication scenarios. The model approach incorporates several man-machine-systems with sub-systems connected via digital data link. The <b>conceptual</b> approach, <b>module</b> architecture, information structures and activity sequences, the negotiation process including transfer between different areas of controller responsibility and the implementation environment SLAMSYSTEM are basic elements for the feasibility assessment and detailed evaluation of advanced communication scenarios. A scenario developed for the PHARE-Programme and successfully applied during PHARE Demonstration 1 is chosen as a reference. Model validation issues are discussed, followed by a variation of experimental scenario parameters which underline the capabilities of the model. (orig.) SIGLEAvailable from TIB Hannover: RN 437 (98 - 11) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|A {{conceptual}} design {{study has been}} completed which has shown the feasibility of ultra-low-mass planar solar arrays with specific power of 200 watts/kilogram. The beginning of life (BOL) power output of the array designs would be 10 kW at 1 astronomical unit (AU) and a 55 C deg operating temperature. Two designs were studied: a retractable rollout design and a non-retractable fold-out. The designs employed a flexible low-mass blanket and low-mass structures. The blanket utilized 2 x 2 cm high-efficiency (13. 5 % at 28 C deg AM 0), ultra-thin (50 micron), silicon solar cells protected by thin (75 micron) plastic encapsulants. The structural design utilized the 'V'-stiffened approach which allows a lower mass boom to be used. In conjunction with the <b>conceptual</b> design, <b>modules</b> using the thin cells and plastic encapsulant were designed and fabricated...|$|R
40|$|To help us {{identify}} {{and focus on}} pragmatic and concrete {{issues related to the}} role of software architecture in the design and development of large systems, we conducted a survey of a variety of software systems used in industrial applications. Our premise, which guided the examination of these systems, was that software architecture is concerned with capturing the structures of a system and the relationships among the elements both within and between structures. The structures we found fell into several broad categories: <b>conceptual</b> structure, <b>module</b> structure, code structure, and execution structure. These categories address different engineering concerns. The separation of such concerns, combined with specialized implementation techniques, decreased the complexity of implementation, and improved reuse and reconfiguration. We observed that in practice, software architecture played an important role throughout the development process: specification, design, functional decompo [...] ...|$|R
40|$|The {{design of}} complex {{information}} systems usually involves {{the production of}} a number of separate <b>conceptual</b> design <b>modules.</b> These are subsequently integrated to form a global information schema. The integration of these modules is a well-known problem area in database systems design. With the likely availability of "off-the-shelf" modules in future this has become a crucial problem to solve. This paper presents an approach to the solution of this problem using conceptual graphs. Keywords: conceptual schema, schema integration, schema reuse, database management systems, IRDS, generalisation, analogy. 1 Introduction The large investment in, and complexity of, enterprise information systems compels the development of better tools to manage the design process and integration of enterprise subsystems. In order to facilitate the effective and economic production of such systems, formal modelling tools (schema or modelling languages) and associated methodologies are used. The modelling pro [...] ...|$|R
40|$|The growing {{interest}} in integrating symbolic and subsymbolic computing techniques is manifested by {{the increasing number of}} hybrid systems that employ both methods of processing. This paper presents an analysis of some of these systems with respect to their symbolic/subsymbolic interactions. Then, a general purpose mechanism for linking symbolic and sub symbolic computing is introduced. Through the use of programming abstractions, an intermediary agent called a supervisor is created and bound to each subsymbolic network. The role of a supervisor is to monitor and control the network behavior and interpret its output. Details of the subsymbolic computation are hidden behind a higher level interface, enabling symbolic and subsymbolic components to interact at corresponding <b>conceptual</b> levels. <b>Module</b> level parallelism is achieved because subsymbolic modules execute independently. Methods for construction of hierarchical systems of subsymbolic modules are also provided...|$|R
40|$|Abstract Background Most {{research}} scientists {{working in}} the fields of molecular epidemiology, population and evolutionary genetics are confronted with the management of large volumes of data. Moreover, the data used in studies of infectious diseases are complex and usually derive from different institutions such as hospitals or laboratories. Since no public database scheme incorporating clinical and epidemiological information about patients and molecular information about pathogens is currently available, we have developed an information system, composed by a main database and a web-based interface, which integrates both types of data and satisfies requirements of good organization, simple accessibility, data security and multi-user support. Results From the moment a patient arrives to a hospital or health centre until the processing and analysis of molecular sequences obtained from infectious pathogens in the laboratory, lots of information is collected from different sources. We have divided the most relevant data into 12 <b>conceptual</b> <b>modules</b> around which we have organized the database schema. Our schema is very complete and it covers many aspects of sample sources, samples, laboratory processes, molecular sequences, phylogenetics results, clinical tests and results, clinical information, treatments, pathogens, transmissions, outbreaks and bibliographic information. Communication between end-users and the selected Relational Database Management System (RDMS) is carried out by default through a command-line window or through a user-friendly, web-based interface which provides access and management tools for the data. Conclusion epiPATH is an information system for managing clinical and molecular information from infectious diseases. It facilitates daily work related to infectious pathogens and sequences obtained from them. This software is intended for local installation in order to safeguard private data and provides advanced SQL-users the flexibility to adapt it to their needs. The database schema, tool scripts and web-based interface are free software but data stored in our database server are not publicly available. epiPATH is distributed under the terms of GNU General Public License. More details about epiPATH can be found at [URL]. </p...|$|R
40|$|Recent trends on {{ubiquitous}} computing have created new requirements for discovering {{and using the}} available services in the network. This leads to require semantic interoperability between heterogeneous entities, which means to realize dynamic ontology to exchange semantic information and configure automatically. This thesis mainly concentrates on semantic discovery of service components. A survey {{of the state of}} the art in services discovery is provided, and two key procedures, dynamic adaptation and effective searches are being focused on. Having given a comprehensive survey of all major research being conducted in the area of ontology construction, the thesis proposes a generic approach to the semantic service discovery that allows establishing dynamic adaptive ontology by using RDF/S to facilitate the description of service components. And the prototype also provides the mechanism to combine the ontology infrastructure with <b>conceptual</b> retrieval <b>module</b> which is a powerful reasoning engine to acquire high performance on searching process. Conjunctive with Microsoft web servic...|$|R
40|$|Abstract. Current {{research}} on the VINLEN inductive database system is briefly reviewed and illustrated by selected results. The goal of {{research on}} VINLEN {{is to develop a}} methodology for deeply integrating a wide range of knowledge generation operators with a relational database and a knowledge base. The current system has already integrated an AQ learning system for generating attributional rules in two modes: theory formation, in which generated rules are consistent and complete with regard to data, and pattern discovery, in which generated rules represent strong patterns, not necessarily consistent or complete. It also has integrated a <b>conceptual</b> clustering <b>module</b> for splitting data into conceptual classes, and providing descrip-tions of those classes. Preliminary data management and knowledge visualization operators, such as the intelligent target data generator (ITG) and concept asso-ciation graph display, have also been integrated. To facilitate an easy interaction with the system, a user-oriented visual interface has been implemented. An exam-ple of results from applying VINLEN to a medical problem domain is presented to illustrate VINLEN knowledge discovery and representation capabilities. ...|$|R
40|$|In this study, a {{new design}} of magnetorheological (MR) valve module using annular-radial gaps concept are {{developed}} {{to improve the}} design flexibility and manufacturability towards commercialization process. In commercial perspective, a product with flexible performance capacity is sometimes more preferable than a high performance product but with fixed specifications since frequent design resizing can be inefficient in terms of manufacturing process. This paper proposes a new design of a compact MR valve using annular-radial concept {{as an effort to}} enhance the performance of an MR valve {{while at the same time}} improving the easiness of performance range and the simplicity of manufacturing process using a fully modular valve structure. In order to evaluate the valve performance, the <b>conceptual</b> design <b>module</b> MR valve, the proposed design is evaluated in terms of pressure drop characteristics with respect to the magnetic field strength and current input in the perspective of module performance. The simulation results have shown that the proposed design has successfully improved the achievable pressure drop with additional advantage in performance flexibility through modular valve concept...|$|R
40|$|Current {{research}} on the VINLEN inductive database system is briefly reviewed and illustrated by selected results. The goal of {{research on}} VINLEN {{is to develop a}} methodology for deeply integrating a wide range of knowledge generation operators with a relational database and a knowledge base. The current system has already integrated an AQ learning system for generating attributional rules in two modes: theory formation, in which generated rules are consistent and complete with regard to data, and pattern discovery, in which generated rules represent strong patterns, not necessarily consistent or complete. It also has integrated a <b>conceptual</b> clustering <b>module</b> for splitting data into conceptual classes, and providing descriptions of those classes. Preliminary data management and knowledge visualization operators, such as the intelligent target data generator (ITG) and concept association graph display, have also been integrated. To facilitate an easy interaction with the system, a user-oriented visual interface has been implemented. An example of results from applying VINLEN to a medical problem domain is presented to illustrate VINLEN knowledge discovery and representation capabilities...|$|R
