15|212|Public
50|$|Statistical {{procedures}} for finding differences between groups, along with {{interactions between the}} groups that were included in an experiment or study, can be classified along two dimensions: 1) were the statistical contrasts that will be evaluated decided upon prior to collecting the data (planned) or while {{trying to figure out}} what those data are trying to reveal (post hoc), and 2) does the procedure use a decision-based (i.e., per <b>contrast)</b> <b>error</b> rate or does it instead use an experiment-wise error rate. Rodger’s method, and some others, are classified according to these dimensions in the table below.|$|E
5000|$|The display driver chip has an RGB to RGBW color {{vector space}} {{converter}} and gamut mapping algorithm, followed by metamer and subpixel rendering algorithms. In {{order to maintain}} saturated color quality, to avoid simultaneous <b>contrast</b> <b>error</b> between saturated colors and peak white brightness, while simultaneously reducing backlight power requirements, the display backlight brightness is under control of the PenTile driver engine. [...] When the image is mostly desaturated colors, those near white or grey, the backlight brightness is significantly reduced, often to less than 50% peak, while the LCD levels are increased to compensate. When the image has very bright saturated colors, the backlight brightness is maintained at higher levels. The PenTile RGBW also has an optional high brightness mode that doubles {{the brightness of the}} desaturated color image areas, such as black&white text, for improved outdoor view-ability.|$|E
40|$|We present {{practical}} {{strategies for}} residual-based error control in solving ordinary differential equations by the discontinuous Galerkin method following the general approach proposed by C. Johnson {{and his co-workers}} in [5], [15], and [10]. Conventional strategies for adaptive step-size selection in difference methods {{are based on the}} concepts of 'local truncation error' and 'discrete stability'. In <b>contrast,</b> <b>error</b> control for the finite element Galerkin method relies on a posteriori error estimates in terms of 'local residuals' and 'continuous dual stability'. The essential feature is the 'Galerkin orthogonality' property which together with a 'duality argument' yields asymptotically optimal weighted a posteriori error estimates. The step-size selection aims at equilibrating the local error indicators, yielding almost optimal step-size distributions and corresponding rigorous error bounds. This approach is developed here for the case of non-stiff initial value problems and its performance is illustrated by several simple test problems including the highly unstable Lorenz system. (orig.) Available from TIB Hannover: RR 1606 (96 - 53) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|E
40|$|A blockwise {{distortion}} {{measure is}} proposed {{for evaluating the}} visual quality of compressed images. The proposed measure calculates quantitatively how well important visual properties have been preserved in the distorted image. The method consists of three quality factors detecting <b>contrast</b> <b>errors,</b> structural errors, and quantization errors. The proposed method outperforms PQS {{for a set of}} test images, and is much simpler to implement. The method should also be applicable to color images; properties like color richness and saturation are captured by the quantization and contrast measures respectively...|$|R
40|$|Digital {{subtraction}} radiography (DSR) {{enables the}} detection of subtle early detrimental effects of periodontal disease {{as well as the}} evaluation of the effects of therapy. However, the differences between two radiographs due to alignment and <b>contrast</b> <b>errors</b> must be kept at minimum. In the present in vitro study we test the efficacy of three basic contrast correction methods in the reduction of contrast mismatches which can adversely affect a subtracted image. The ODTF (Optical Density Thickness Function) method, which is based on a function relating grey level values of the aluminium wedge image and the corresponding thickness of the wedge, induced less <b>contrast</b> correction <b>error</b> than the CDF (Cumulative Density Function) and the LSQA (Least Square Quadratic Approximation) methods. Moreover, CDF, ODTF, and LSQA functions obtained from the reference structure density distribution may be applied for objective contrast enhancements and for standardisation of image quality, while the ODTF function allows also bone change volume estimations...|$|R
40|$|We {{show the}} best linear {{unbiased}} predictor (BLUP) {{can be derived}} as the best predictor (under normality) based on all <b>error</b> <b>contrasts</b> (i. e., transformation of data with mean 0). The result reveals an interesting connection between BLUP and REML [...] restricted or residual maximum likelihood [...] estimates. Mixed models <b>Error</b> <b>contrasts</b> REML...|$|R
40|$|Inexpensive “point-and-shoot ” camera {{technology}} has combined with social network technology {{to give the}} gen-eral population a motivation to use face recognition tech-nology. Users expect a lot; they want to snap pictures, shoot videos, upload, and have their friends, family and acquain-tances more-or-less automatically recognized. Despite the apparent simplicity of the problem, face recognition {{in this context is}} hard. Roughly speaking, failure rates in the 4 to 8 out of 10 range are common. In <b>contrast,</b> <b>error</b> rates drop to roughly 1 in 1, 000 for well controlled imagery. To spur advancement in face and person recognition this pa-per introduces the Point and Shoot Face Recognition Chal-lenge (PaSC). The challenge includes 9, 376 still images of 293 people balanced with respect to distance to the cam-era, alternative sensors, frontal versus not-frontal views, and varying location. There are also 2, 802 videos for 265 people: a subset of the 293. Verification results are pre-sented for public baseline algorithms and a commercial al-gorithm for three cases: comparing still images to still im-ages, videos to videos, and still images to videos...|$|E
40|$|A {{range of}} core {{operations}} and planning {{problems for the}} national electrical grid are naturally formulated and solved as stochastic programming problems, which minimize expected costs subject {{to a range of}} uncertain outcomes relating to, for example, uncertain demands or generator output. A critical decision issue relating to such stochastic programs is: How many scenarios are required to ensure a specific error bound on the solution cost? Scenarios are the key mechanism used to sample from the uncertainty space, and the number of scenarios drives computational difficultly. We explore this question {{in the context of a}} long-term grid generation expansion problem, using a bounding procedure introduced by Mak, Morton, and Wood. We discuss experimental results using problem formulations independently minimizing expected cost and down-side risk. Our results indicate that we can use a surprisingly small number of scenarios to yield tight error bounds in the case of expected cost minimization, which has key practical implications. In <b>contrast,</b> <b>error</b> bounds in the case of risk minimization are significantly larger, suggesting more research is required in this area in order to achieve rigorous solutions for decision makers...|$|E
40|$|An {{iterative}} algorithm for {{the reconstruction}} of natural images given only their contrast map is presented. The solution is neuro-physiologically inspired, where the retinal cells, for the most part, transfer only the contrast information to the cortex, which at some stage performs reconstruction for perception. We provide an image reconstruction algorithm based on least squares error minimization using gradient descent {{as well as its}} corresponding Bayesian framework for the underlying problem. Starting from an initial image, we compute its contrast map using the Difference of Gaussians (DoG) operator at each iteration, which is then compared to the contrast map of the original image generating a <b>contrast</b> <b>error</b> map. This contrast map is processed by a non-linearity to deal with saturation effects. Pixel values are then updated proportionally to the resulting contrast errors. Using a least squares error measure, the result is a convex error surface with a single minimum, thus providing consistent convergence. Our experiments show that the algorithm’s convergence is robust to initial conditions but not the performance. A good initial estimate results in faster convergence. Finally, an extension of the algorithm to colour images is presented. We test our algorithm on images from the COREL public image database. The paper provides a novel approach to manipulating an image in its contrast domain. ...|$|E
40|$|It is by {{now well}} known that the {{integral}} Hellmann–Feynman (IHF) theorem has little quantitative utility for chemically interesting problems, although the formalism potentially affords a ready physical interpretation of changes in molecular conformation. In this paper, the IHF theorem is applied to variational and simple LCAO wavefunctions for the H 2 + ground state, which range in quality from crude to essentially exact. The IHF results improve quite dramatically {{with the quality of}} the wavefunctions. This suggests that errors in the IHF formula may be of the same order as those in the wavefunction. (In <b>contrast,</b> <b>errors</b> in variationally determined energies are of second order.) Our results suggest a convenient test which can be applied to any revised IHF formalism developed in the future...|$|R
40|$|In {{this paper}} we {{introduce}} Functional Difference Predictors (FDPs), {{a new class}} of perceptually-based image difference metrics that predict how image errors affect the ability to perform visual tasks using the images. To define the properties of FDPs, we conduct a psychophysical experiment that focuses on two visual tasks: spatial layout and material estimation. In the experiment we introduce errors in the positions and contrasts of objects reflected in glossy surfaces and ask subjects to make layout and material judgments. The results indicate that layout estimation depends only on positional errors in the reflections and material estimation depends only on <b>contrast</b> <b>errors.</b> These results suggest that in many task contexts, large visible image errors may be tolerated without loss in task performance, and that FDPs may be better predictors of the relationship between errors and performance than current Visible Difference Predictors (VDPs) ...|$|R
40|$|Prosody {{awareness}} (the rhythmic patterning of speech) {{accounts for}} unique variance in reading development. However, studies {{have thus far}} focused on early-readers and utilised literacy measures which fail to distinguish between monosyllabic and multisyllabic words. The current study investigated the factors that are specifically associated with multisyllabic word reading {{in a sample of}} fifty children aged between 7 - and 8 -years. Prosodic awareness was the strongest predictor of multisyllabic word reading accuracy, after controlling for phoneme awareness, morphological awareness, vocabulary, and short-term memory. Children also made surprisingly few phonemic <b>errors</b> while, in <b>contrast,</b> <b>errors</b> of stress assignment were commonplace. Prosodic awareness was also the strongest predictor of stress placement errors, although this finding was not significant. Prosodic skills may play an increasingly important role in literacy performance as children encounter more complex reading materials. Once phoneme-level skills are mastered, prosodic awareness is arguably the strongest predictor of single word reading...|$|R
40|$|We {{developed}} {{a method that}} analyzes {{the quality of the}} cultivated cropland class mapped in the USA National Land Cover Database (NLCD) 2006. The method integrates multiple geospatial datasets and a Multi Index Integrated Change Analysis (MIICA) change detection method that captures spectral changes to identify the spatial distribution and magnitude of potential commission and omission errors for the cultivated cropland class in NLCD 2006. The majority of the commission and omission errors in NLCD 2006 are in areas where cultivated cropland is not the most dominant land cover type. The errors are primarily attributed to the less accurate training dataset derived from the National Agricultural Statistics Service Cropland Data Layer dataset. In <b>contrast,</b> <b>error</b> rates are low in areas where cultivated cropland is the dominant land cover. Agreement between model-identified commission errors and independently interpreted reference data was high (79 %). Agreement was low (40 %) for omission error comparison. The majority of the commission errors in the NLCD 2006 cultivated crops were confused with low-intensity developed classes, while the majority of omission errors were from herbaceous and shrub classes. Some errors were caused by inaccurate land cover change from misclassification in NLCD 2001 and the subsequent land cover post-classification process...|$|E
40|$|The paper {{focuses on}} image fusion between multi-spectral images and {{panchromatic}} images using a wavelet analysis method with good signal processing and image processing traits. A new weighting technique is developed based on wavelet transformation for {{the fusion of}} a high spatial resolution image and a low-resolution, multi-spectral image. The method improves a standard wavelet merger for merging the lower frequency components of a multi-spectral image and its high spatial resolution image by means of local deviation rules with weighting average. And then the merged image is reconstructed by an inverse wavelet transform using the fused approximation and details from the high spatial resolution image. Also, a multi-spectral images fusion algorithm is proposed based on wavelet transform characteristic of human vision system. Firstly, perform a wavelet multi-scale transformation of each source image. Then a new fusion regular is presented based on human vision system corresponding high (low) frequency components are divided into several blocks, and <b>contrast</b> <b>error</b> of every block is calculated, an adaptive threshold selection is proposed to decide which {{should be used to}} construct the new high (low) frequency components. Finally, the fused image is obtained by taking inverse wavelet transform. The experimental results show that the new method presented is clearly better in not only preserving spectral and improving spatial presentation, but also avoiding mosaic occurring. 1...|$|E
40|$|With {{the growing}} {{interest}} in using binary phase only filters (BPOF) in optical correlators that are implemented on magnetooptic spatial light modulators, {{an understanding of the}} effect of errors in system alignment and optical components is critical in obtaining optimal system performance. We present simulations of optical correlator performance degradation in the presence of eight errors. We break these eight errors into three groups: 1) alignment errors, 2) errors due to a combination of component imperfections and alignment errors, and 3) errors which result solely from non-ideal components. Under the first group, we simulate errors in the distance from the object to the first principle plane of the transform lens, the distance from the second principle plane of the transform lens to the filter plane, and rotational misalignment of the input mask with the filter mask. Next we consider errors which result from a combination of alignment and component imperfections. These include errors in the transform lens, the phase compensation lens, and the inverse Fourier transform lens. Lastly we have the component errors resulting from the choice of spatial light modulator. These include <b>contrast</b> <b>error</b> and phase errors caused by the non-uniform flatness of the masks. The effects of each individual error are discussed, and the result of combining all eight errors under assumptions of reasonable tolerances and system parameters is also presented. Conclusions are drawn as to which tolerances are most critical for optimal system performance...|$|E
3000|$|... decreases, the {{probability}} of detection decreases, and more scans are required in average to retain a specified number of detections. In <b>contrast,</b> the RMS <b>error</b> of ML([...] [...]...|$|R
40|$|In {{this second}} {{report on the}} {{fluorescence}} overlay antigen mapping (FOAM) technique, we highlight some of the errors that may influence faithful color rendition of slide preparations using triple antigen immunofluorescence staining. Reliable interpretation of multicolor fluorescence images requires that the observer can unambiguously assign each color in these images {{to the presence of}} a specific combination of the labeled antigens, This is possible only when the image fidelity meets certain standards, The present study concentrates on color fidelity which is easily undermined by spectral matching <b>errors,</b> image <b>contrast</b> <b>errors,</b> and exposure time errors. Evaluation of these errors, using the photomicrographic overlay variant of FOAM, showed the potential unreliability of the simultaneous use of multiple fluorophores for immunofluorescence microscopy. The procedures described here may serve as a solid starting point in formulating technical conditions that allow reliable color rendition in multicolor immunofluorescence microscopy. Furthermore, these procedures can be adapted to studies other than the analysis of basement membrane zone antigens, to which they have been first applied...|$|R
40|$|Discourse {{boundaries}} {{have been}} associated with an increased rate of disfluent events. It is hypothesized that the reason for this increase is the heavy processing requirement incurred either in planning the next chunk of discourse or in the introduction of many new or high perplexity entities. In a sample of academic lecture speech, we find that non-error disfluencies (such as filled pauses) occur preferentially shortly after (but not right at) {{the beginning of a new}} discourse segment. This suggests that the processing load may not increase just at the boundary but instead somewhat later, i. e. that the speaker can make use of the results of earlier planning during the first portion of the new segment. In <b>contrast,</b> <b>errors</b> of selection or serial ordering of grammatical elements do not show a boundary-related peak in their distribution across a discourse segment, supporting the hypothesis that this second kind of nonfluent event arises at a different point in the speech production planning process...|$|R
40|$|This work {{seeks to}} advance our {{understanding}} ofa preschool-age {{view of the}} relationship between representations and their referents. Here we have outlined and tested a novel task - the false objects task - designed to mirror its predecessor - the false photographs task (Zaitchik, 1990). In line with previous false photograph work, behavioural measures here have indicated that children perceive representations and their referents to be in accordance with one another, over-endowing referent objects with aspects which are unique to the photograph's own spatiotemporal history. In five initial experiments we have provided a robust demonstration of children's tendency to make errors on this novel task, in contrast to suitable controls and across a range of materials. In the second half of this body of work we sought to investigate the factors contributing to errors on the false objects task, and concluded that these could not be attributed to memory difficulties, cueing or a proclivity to view the situation in accordance with magical casual reasoning. In <b>contrast,</b> <b>error</b> rates reduced when the experimental context encouraged children to view the representation and referent object as separate items. We conclude that these results most strongly support an account of children's difficulty with the dual nature of representations. These findings support and extend our understanding of the repercussions of this difficulty, and indicate that confusion between ''the sign and the thing signified" is evident in children's behaviour as well as in their verbal reports. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Subject of investigation: {{method for}} {{revealing}} scale {{on the surface}} of hot-rolled etched strips and sheets by electron-optical systems of control with the use of transmitters. Purpose of the work: development of an electron-optical control method and algorithms for revaling scale {{on the surface of}} a hot-rolled sheet with the use of transmitters in real-time. The results of the work include the proposal of a method permitting enhancement of the reliability and certainty in revealing scale on the surface of a hot-rolled sheet, the development of an algorithm for detecting scale which forms transmitter output signal correction coefficients, and deriving of a formula of the operators of the transmitter <b>contrast</b> <b>error</b> and prescribed-accuracy correction. The performed investigation served as a basis for determining the illuminator and transmitter angles of inclination with respect to the surface unden control. These angles provide for the maximum contrast of the scale against the background of flawless metal. A device is developed for compensating the dergee of diffusion of the object under check. The scale revealing system is put into practice. A prototype system of control of an evident strip shape has successfully passed approbation. The reliable control of scale on the hot-rolled sheet surface will make it possible to reduce metal losses, decrease the rejected metal and enhance the general efficiency of the rolling equipment. Field of application: ferrous and non-ferrous metallurgy enterprisesAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Para verificar se a pequena influência da heterogeneidade das variâncias, observada em trabalho anterior, sobre os testes de Bonferroni, "t" e Tukey se devia ao fato de ter sido utilizado o erro médio que corresponde ao quadrado médio da interação tratamentos x blocos (Q. M. trat x bloco), os autores estudaram o comportamento do teste "t" com o uso de três tipos de erro: Q. M. trat x bloco, erro do contraste específico e erro ponderado dos contrastes. Foram simulados 200 experimentos, em grupos de 50, para cada uma das 56 combinações de quatro coeficientes de variação, dois efeitos de tratamentos e sete grupos de heterogeneidade, totalizando 11. 200 experimentos. Foi feita a análise da variância de cada ensaio e estudados os seguintes contrastes: m 1 - m 3 e m 2 - m 3, onde m 3 representa a testemunha. "A priori" foi estabelecido que o tratamento 1 seria {{superior}} à testemunha nas quantidades de 8 e 17 % (para os efeitos 1 e 2 respectivamente) e o tratamento 2, nas quantidades de 5 e 12 % (para os mesmos efeitos). Assim, foi empregado o teste t, unilateral, para comparação das médias nos dois contrastes. Na presente pesquisa, são válidas as seguintes conclusões: as diferenças nas porcentagens de rejeição com os três tipos de erro praticamente inexistem; as relações de variâncias com valores maiores que 6 podem ser consideradas heterogêneas, quando utilizado o erro do contraste; a porcentagem de rejeição obtida é indiretamente proporcional ao coeficiente de variação e diretamente proporcional à magnitude dos contrastes, mesmo em condições de heterogeneidade de erros. It {{was observed}} previously a small effect of heterogeneous variances on the Bonferroni's test, Student "t" test and Tukey's test. In this paper, it was tested whether such observation {{was due to}} the fact that the error used for the tests was the treat. x block interaction mean square besides the existence of certain heterogeneous variances. The authors considered the existence of the following situations: a) two groups of variance in each experiment, and b) three kinds of errors for "t" test: treat. x block interaction mean square, specific <b>contrast</b> <b>error</b> for each comparison; and, pooled <b>contrast</b> <b>error.</b> It was simulated two hundred experiments, in groups of 50, for each of 56 combinations of four coefficients of variation, two treatment effects and seven groups of heterogeneous variances, performing a total of 11, 200 simulations. In the analysis of variance, for each experiment, the following contrasts between means were considered: m 1 - m 3 and m 2 - m 3, where m 3 represents the control treatments mean. The two treatment means m 1 and m 2 were considered "a priori" more productive than the control by 8 % and 5 % (effect 1) and by 17 % and 12 % (effect 2), respectively for m 1 and m 2. Thus, the one tail "t" test was applied. From the results observed, it was concluded that: a) non significant differences occurred concerning the percentages of rejections of the null hypothesis using any of the three errors considered; b) the relation among variances greater than 6 was statistically heterogeneous, when the error used was that of specific contrast error; and c) the percentages of rejections obtained were indirectly proportional to the coefficients of variation and directly proportional to the range values of contrasts between means, even in case of heterogeneous error conditions...|$|E
5000|$|A {{systemic}} {{problem is}} a problem due to issues inherent in the overall system,rather than due to a specific, individual, isolated factor. <b>Contrast</b> with pilot <b>error,</b> user error, or [...]|$|R
30|$|Two {{types of}} unsafe {{crossing}} behaviours {{can be distinguished}} according to pedestrian’s intention. The term “violation” is frequently used to distinguish deliberate crossing {{in the presence of}} active controls from unintentional rule breaches that are referred to as “errors”. In observational and other studies, young pedestrians are considered a high risk group of users who deliberately violate rules [9, 15]. Their crossing behaviours have been associated with sensation seeking tendencies (thrill-seeking) or perception of control, compared to elderly for example. Furthermore, male pedestrians are associated with higher risk-taking tendencies than females, however such a trend was only confirmed by one observational study in which male transgressors were identified slightly more often than females (59  %) [7, 14]. Finally, according to Clancy, Dickinson [15] as well as Metaxatos and Sriraj [1], motivations to deliberately transgress are associated with the given journey context (e.g. being in a hurry, avoiding missing the next train, being on time at work/school). In <b>contrast,</b> <b>errors</b> are often associated with elderly pedestrians likely to experience hearing, motor or visual impairments [6, 8, 9, 14] or with distraction [1, 6].|$|R
40|$|AbstractIn {{a recent}} paper of ours [Hess & Field (1993). Vision Research, 33, 2663 – 2670], we claimed {{that there was}} a {{predictable}} relationship between position <b>errors</b> and <b>contrast</b> <b>errors</b> for an undersampled system. In this paper we re-state our main points. We feel that the response to that paper by Levi and Klein in the accompanying article does not require us to produce changes in our original position. We believe that the data support the notion that the principal causes of the positional errors in the normal periphery and in the amblyopic visual system are due to uncalibrated distortions in the local signs of visual neurons. We believe that undersampling {{plays a major role in}} producing positional errors only in the far periphery at, or very near, the acuity limit. We maintain that our initial studies provide strong evidence that undersampling is insufficient as an explanation for the positional errors in the periphery of normals (Hess & Field, 1993) or the central field of amblyopes [Hess & Field (1994). Vision Research, 34, 3397 – 3406. Copyright © 1996 Elsevier Science Ltd...|$|R
40|$|Abstract—This paper {{describes}} a multistage perceptual quality assessment (MPQA) model for compressed images. The {{motivation for the}} development of a perceptual quality assessment is to measure (in) visible differences between original and processed images. The MPQA produces visible distortion maps and quantitative error measures informed by considerations of the human visual system (HVS). Original and decompressed images are decomposed into different spatial frequency bands and orientations modeling the human cortex. Contrast errors are calculated for each frequency and orientation, and masked as a function of contrast sensitivity and background uncertainty. Spatially masked <b>contrast</b> <b>error</b> measurements are then made across frequency bands and orientations to produce a single perceptual distortion visibility map (PDVM). A perceptual quality rating (PQR) is calculated from the PDVM and transformed into a one to five scale, PQR 1 5, for direct comparison with the mean opinion score, generally used in subjective ratings. The proposed MPQA model is based on existing perceptual quality assessment models, while it is differentiated by the inclusion of contrast masking as a function of background uncertainty. A pilot study of clinical experiments on wavelet-compressed digital angiogram has been performed on a sample set of angiogram images to identify diagnostically acceptable reconstruction. Our results show that the PQR 1 5 of diagnostically acceptable lossy image reconstructions have better agreement with cardiologists’ responses than objective error measurement methods, such as peak signal-to-noise ratio A Perceptual thresholding and CSF-based Uniform quantization (PCU) method is also proposed using the vision models presented in this paper. The vision models are implemented in the thresholding and quantization stages of a compression algorithm and shown to produce improved compression ratio performance with less visible distortion than that of the embedded zerotrees wavelet (EZWs). Index Terms—Digital angiography, human visual system, perceptual quality, wavelet compression...|$|E
40|$|Variable length code (VLC), {{also called}} Huffman code [I 1, {{is the most}} popular data {{compression}} technique used for image compression and transmission. But, it is vulnerable to loss of synchronization under transmission in a noisy channel. It will result in large drops in video transmission quality. For stopping error propagation, synchronization codeword (SC) is mostly used to localize error propagation effect. However, most of the SC cannot tolerate noise errors. We propose an iterative and efficient algorithm to construct error-resilient and variablelength SC for any application defined VLC table. They can be used for synchronization of packet (ERVL-SOP). It can be appended at the end of certain number of symbols to form a packet. Thus, channel noise error can be localized. With using constructed SC as SOP, the result shows that it can achieve high signal-to-noise ratio (PSNR= 26 dB) compared to use normal VLC code as SOP at bit error rate (BER) of IO ” environment. In addition, the incorrect ERVL-SOP detection rate can be decreased to less than 0. 001 3 % effectively. (ESC). However, the constructed SC still has one flaw. They cannot tolerate any error. An occurrence of error on SC may either make SC undecipherable or lead to lose synchronization capability. In the <b>contrast,</b> <b>error</b> also may generate invalid SC. In addition, some designs use fix-length SC. But, it will have difficulties and need to sacrifice some performances for combining fix-length SC with VLC bitstream. As a result, development of error-resilient and variable length SC remains an important design issue. In this paper, we propose a novel algorithm to construct errorresilient and variable length SC for using as synchronization of packet (ERVL-SOP). It can tolerate both preceding symbols’ error propagation and single bit error occurrence on SOP itself. Thus, it can localize burst error effectively in worsening channel...|$|E
40|$|Choosing a rainfall-runoff {{model for}} use in flood {{forecasting}} is not a straightforward decision and indeed may involve the selection of more than one. The aim of this Part 1 report {{is to provide a}} literature review of models in order to furnish a basic understanding of the types of model available, highlighting their similarities and differences. A sub-set of those reviewed are selected for more detailed assessment using data from a range of catchments. The results of this model intercomparison are presented in the Part 2 report. Whilst there is a plethora of “brand name” models they involve a relatively small set of model functions which are configured in a variety of different ways. This is illustrated by the models reviewed here. The initial selection of models for review is guided by those already in use for flood forecasting in the UK. To this are added well-known models developed overseas and those with a distributed formulation. From this menu of models are selected the following eight models for intercomparison in Part 2 : the Thames Catchment Model (TCM), the Midlands Catchment Runoff Model (MCRM), the Probability Distributed Moisture (PDM) model, the Isolated Event Model (IEM), the US National Weather Service Sacramento model, the Grid Model, the Transfer Function (TF) model and the Physically Realisable Transfer Function (PRTF) model. The first six are conceptual soil moisture accounting models, with the Grid Model having a distributed formulation, whilst the TF and PRTF are “black box” time-series models. Also selected for review in Part 1 are the Input-Storage-Output or ISO-function model and the NAM model, which are both conceptual approaches. An outline review of some newer, general approaches to forecasting are given which include neural network (NN), fuzzy rule-based and nearest neighbour methods. An important aspect of the use of rainfall-runoff models in a real-time forecasting environment is the ability to incorporate recent observations of flow in order to improve forecast performance. The available methods for forecast updating are reviewed with particular reference to state correction and error prediction techniques. The latter aim to adjust, for example, the water contents of conceptual stores in a model and are usually tailored for a specific model. In <b>contrast,</b> <b>error</b> prediction operates independently of the rainfall-runoff model structure by exploiting the dependence in model errors to predict future ones. Parameter adjustment techniques are considered separately {{in the context of the}} simple TF and PRTF models. The Part 1 report ends with an overview of the models reviewed. This includes consideration of the ease of use of different models in calibration and in an operational forecasting environment. In conclusion, the didactic rather than judgmental approach adopted in the review is justified. It is inherently dangerous to judge the efficacy of a model by the variety of functionality it supports or processes it purports to represent. The Part 2 report presents the results of the intercomparison of models across a range of catchments. These results provide an objective basis on which to make judgements concerning the choice of models. Guidelines on model choice are presented in terms of forecast accuracy for different types of catchment together with other factors, such as ease of calibration and operational use, considered only partially in this Part 1 report. ...|$|E
40|$|The lattice-parameter error {{due to a}} {{relative}} misorientation between the specimen and the flat film in the transmission divergent X-ray beam and Kossel techniques developed for alpha -iron has been studied in detail. It {{has been shown that}} the most frequently encountered case of film misorientation can be theoretically divided into two pure components. The Type I component has been shown to have no influence whatsoever on the lattice parameter measured by this or any other method involving the ratio technique. The lattice-parameter error is thus due exclusively to the Type II component. In addition, {{it has been shown that}} these errors, which can become relatively important, are present in all linearly non-symmetric methods. In <b>contrast,</b> <b>errors</b> due to Type II components in linearly symmetrical methods should be very small if not experimentally undetectable. The geometry involved is such that the lattice parameter error cannot be experimentally determined. However, such film misorientations and their resulting lattice-parameter errors can be reduced to a very low value by means of the reticule method previously proposed and described. Anglai...|$|R
3000|$|... [*]=[*] 3.08 [*]×[*] 10 − 2, respectively. The paraunitarity {{error is}} {{relatively}} high in <b>contrast</b> with decomposition <b>error.</b> This {{is due to}} the difference between the first two singular values and the last singular value.|$|R
40|$|We {{compare and}} <b>contrast</b> the <b>error</b> {{probability}} and fidelity as {{measures of the}} quality of the receiver's measurement strategy for a quantum communications system. The error probability is a measure of the ability to retrieve classical information and the fidelity measures the retrieval of quantum information. We present the optimal measurement strategies for maximising the fidelity given a source that encodes information on the symmetric qubit-states. Comment: 11 pages, RevTeX, 3 figures(EPSF...|$|R
40|$|The {{distributed}} feature {{approach to}} semantic memory organization {{has been supported}} by data from patients with Alzheimer’s disease (AD) (e. g., Gonnerman et al., 1997). This account makes specific predictions about the types of errors one would expect in AD as semantic memory deteriorates, with initially more <b>contrast</b> coordinate <b>errors,</b> followed by superordinates, and finally an increase in unrelated responses. We investigate these predictions using a picture naming task, with both natural kinds and artifacts...|$|R
50|$|These 9 subtests {{generate}} 16 main {{achievement scores}} {{and hundreds of}} optional <b>error,</b> <b>contrast,</b> accuracy, and time-interval scores. As such, use of the computerized scoring assistant (available for purchase from the test publisher) makes scoring the measure less time consuming.|$|R
40|$|Inner {{speech is}} {{typically}} characterized as either the activation of abstract linguistic representations or a de- tailed articulatory simulation that lacks only {{the production of}} sound. We present {{a study of the}} speech errors that occur during the inner recitation of tongue-twister-like phrases. Two forms of inner speech were tested: inner speech without articulatory movements and articulated (mouthed) inner speech. Although mouthing oneâ��s inner speech could reasonably be assumed to require more articulatory planning, prominent theories assume that such planning should not affect the experience of inner speech and, consequently, the errors that are â��heardâ�� during its production. The errors occurring in articulated inner speech exhibited the phonemic similarity effect and the lexical bias effectâ��two speech-error phenomena that, in overt speech, have been localized to an articulatory- feature-processing level and a lexicalâ��phonological level, respectively. In <b>contrast,</b> <b>errors</b> in unarticulated inner speech did not exhibit the phonemic similarity effectâ��just the lexical bias effect. The results are interpreted as support for a flexible abstraction account of inner speech. This conclusion has ramifications for the embodiment of language and speech and for the theories of speech production...|$|R
40|$|Errors trigger {{changes in}} {{behavior}} that help individuals adapt to new situations. The dorsal {{anterior cingulate cortex}} (dACC) {{is thought to be}} central to this response, but more lateral frontal regions are also activated by errors and may make distinct contributions. We investigated error processing by studying 2 distinct error types: commission and timing. Thirty-five subjects performed a version of the Simon Task designed to produce large number of errors. Commission errors were internally recognized and were not accompanied by explicit feedback. In <b>contrast,</b> timing <b>errors</b> were difficult to monitor internally and were explicitly signaled. Both types of error triggered changes in behavior consistent with increased cognitive control. As expected, robust activation within the dACC and bilateral anterior insulae (the Salience Network) was seen for commission <b>errors.</b> In <b>contrast,</b> timing <b>errors</b> were not associated with activation of this network but did activate a bilateral network that included the right ventral attentional system. Common activation for both error types occurred within the pars operculari and angular gyri. These results show that the dACC does not respond to all behaviorally salient errors. Instead, the error-processing system is multifaceted, and control can be triggered independently of the dACC when feedback is unexpected...|$|R
40|$|Description This package {{provides}} {{functions to}} diagnose and make inferences from various linear models, {{such as those}} ob-tained from 'aov','lm', 'glm', 'gls', 'lme', and 'lmer'. Inferences include predicted means and standard <b>errors,</b> <b>contrasts,</b> multiple comparisons, permutation tests and graphs. License GPL (> = 2...|$|R
40|$|The {{fire weather}} of {{south-east}} Australia from 1985 to 2009 has been simulated using the Weather Research and Forecasting (WRF) model. The US National Oceanic and Atmospheric Administration Centers for Environmental Prediction and National Center for Atmospheric Research reanalysis supplied the lateral boundary conditions and initial conditions. The model simulated climate and the reanalysis were evaluated against station-based {{observations of the}} McArthur Forest Fire Danger Index (FFDI) using probability density function skill scores, annual cumulative FFDI and days per year with FFDI above 50. WRF simulated the main features of the FFDI distribution and its spatial variation, with an overall positive bias. Errors in average FFDI were caused mostly by errors {{in the ability of}} WRF to simulate relative humidity. In <b>contrast,</b> <b>errors</b> in extreme FFDI values were driven mainly by WRF errors in wind speed simulation. However, in both cases the quality of the observed data is difficult to ascertain. WRF run with 50 -km grid spacing did not consistently improve upon the reanalysis statistics. Decreasing the grid spacing to 10 km led to fire weather that was generally closer to observations than the reanalysis across the full range of evaluation metrics used here. This suggests it is a very useful tool for modelling fire weather over the entire landscape of south-east Australia...|$|R
