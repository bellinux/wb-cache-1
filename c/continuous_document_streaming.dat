0|291|Public
40|$|We {{propose the}} first known {{solution}} to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L 1 vector space while guaranteeing a (worst-case) upper bound of O(log 2 n log ∗ n) onthe distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and (2) approximate the result of tree-edit-distance similarity joins over <b>continuous</b> XML <b>document</b> <b>streams.</b> Experimental results from an empirical study with both synthetic and real-life XML data trees validate our approach, demonstrating that the average-case behavior of our embedding techniques is much better than what would be predicted from our theoretical worstcase distortion bounds. To the best of our knowledge, these are the first algorithmic results on lowdistortion embeddings for tree-edit distance metrics, and on correlating (e. g., through similarity joins) XML data in the streaming model. Categories and Subject Descriptors: H. 2. 4 [Database Management]: Systems—Query processing; G. 2. 1 [Discrete Mathematics]: Combinatorics—Combinatorial algorithm...|$|R
40|$|On the Internet, {{plain text}} {{documents}} created and viewed by users constitute ever changing <b>document</b> <b>streams.</b> Lots {{of the literature}} is devoted to topic modeling, while the sequential patterns of topics in <b>document</b> <b>streams</b> are ignored. In this paper, {{we deal with the}} problem of mining user related rare sequential patterns of topics in the Internet <b>document</b> <b>streams,</b> which can be used in many fields, such as real-time user behavioral monitoring on the Internet. We propose an approach to discover rare patterns based on the temporal and probabilistic information of topics. Experiments show that the proposed approach can discover user related rare patterns of topic effectively. Copyright 2014 ACM. On the Internet, plain text documents created and viewed by users constitute ever changing <b>document</b> <b>streams.</b> Lots of the literature is devoted to topic modeling, while the sequential patterns of topics in <b>document</b> <b>streams</b> are ignored. In this paper, we deal with the problem of mining user related rare sequential patterns of topics in the Internet <b>document</b> <b>streams,</b> which can be used in many fields, such as real-time user behavioral monitoring on the Internet. We propose an approach to discover rare patterns based on the temporal and probabilistic information of topics. Experiments show that the proposed approach can discover user related rare patterns of topic effectively. Copyright 2014 ACM...|$|R
50|$|The IRSG also {{reviews and}} approves {{documents}} published {{as part of}} the IRTF <b>document</b> <b>stream</b> (RFC 5743).|$|R
30|$|Recently, Zhu et al. [173] {{has made}} an attempt to mine the rare {{sequential}} patterns from <b>document</b> <b>streams</b> which are sequential in nature. The algorithm named Sequential Topic Patterns(STPs) tries to identify the abnormal activities of Internet users over <b>document</b> <b>streams.</b> A similar attempt has been made by Rahman et al. [124] to detect anomalies in SCADA logs. The mining of rare sequential patterns has not been extensively explored and {{there is much room}} for expansion in this regard. The rare pattern mining community needs to develop efficient rare sequential pattern mining techniques maintaining the temporal and sequential relationships among the rare patterns.|$|R
40|$|Abstract: The {{problem of}} {{processing}} streaming XML data is gaining widespread {{attention from the}} research community. In this paper, a novel approach for processing complex Twig Pattern with OR-predicates and AND-predicates over XML <b>documents</b> <b>stream</b> is presented. For {{the improvement of the}} processing performance of Twig Patterns, all the Twig Patterns are combined into a single prefix query tree that represents such queries by sharing their common prefixes. Its OR-predicates and AND-predicates of a node are represented as a separate abstract syntax tree associated with the node. Consequently, all the Twig Patterns are evaluated in a single, document-order pass over the input <b>document</b> <b>stream</b> for avoiding the interim results produced by the post-processing nested paths of YFilter. Compared with the existing approach, experimental results show that it can significantly improve the performance for matching complex Twig Patterns over XML <b>document</b> <b>stream,</b> especially for large size XML documents. Based on the prior works, the optimization of twig patters under DTD (document type definition) by using structural and constraint information of DTD is also addressed, which is static, namely, i...|$|R
30|$|A deep {{detailed}} study {{is required for}} <b>document</b> <b>stream</b> clustering in different aspects. Time series and transaction applications, uncertain data and fuzzy consideration, arbitrary and non-convex shaped clusters {{can be considered for}} the future study.|$|R
40|$|More {{and more}} {{powerful}} computer technology inspires people to investigate information hidden under huge amounts of documents. We are especially interested in documents with relative time order, which we also call <b>document</b> <b>streams.</b> Examples include TV news, forums, emails of company projects, call center telephone logs, etc. To get an insight into these <b>document</b> <b>streams,</b> first we need to detect the events among the <b>document</b> <b>streams.</b> We use a time-sensitive Dirichlet process mixture model to find {{the events in the}} <b>document</b> <b>streams.</b> A time sensitive Dirichlet process mixture model is a generative model, which allows a potentially infinite number of mixture components and uses a Dirichlet compound multinomial model to model the distribution of words in documents. We consider three different time sensitive Dirichlet process mixture models: an exponential decay kernel model, a polynomial decay function kernel Dirichlet process model and a sliding window kernel model. Experiments on the TDT 2 dataset have shown that the time sensitive models performs 18 - 20 % better in terms of accuracy than the Dirichlet process mixture model. The sliding windows kernel and the polynomial kernel is more promising in detecting events. We use ThemeRiver to provide a visualization of the events along the time axis. With the help of ThemeRiver, people can easily get an overall picture of how different events evolve. Besides Themeriver, we investigate using top words as a high-level summarization of each event. Experiment results on TDT 2 dataset suggests that the sliding window kernel is a better choice both in terms of capturing the trend of the events and expressibility. ...|$|R
40|$|A {{fundamental}} problem in {{text data mining}} is to extract meaningful structure from <b>document</b> <b>streams</b> that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity {{for a period of}} time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise [...] - that the appearance of a topic in a <b>document</b> <b>stream</b> is signaled by a "burst of activity," with certain features rising sharply in frequency as the topic emerges. The goal of th...|$|R
30|$|We {{conducted}} Experiment 2 {{for measuring}} the quality and performance of DCSTREAM compared to the two previous methods STREAM and ConStream against the real world datasets. Intrusion Detection Dataset (KDDCUP 99) is chosen as the abrupt instance and <b>document</b> <b>stream</b> as gradual datasets.|$|R
40|$|A {{personal}} information filtering system monitors an incoming <b>document</b> <b>stream</b> {{to find the}} documents that match information needs specified by user profiles. The most challenging aspect in adaptive filtering {{is to develop a}} system to learn user profiles e#ciently and e#ectively from very limited user supervision...|$|R
40|$|Extensive {{experiments}} are conducted {{to evaluate the}} effectiveness PFreeBT and PNLH by using a stream of two-year news stories and three benchmarks. The results showed that the patterns of the bursty features and the bursty topics which are identified by PFreeBT match our expectations, whereas PNLH demonstrates significant improvements over all of the existing heuristics. These favorable results indicated that both PFreeBT and PNLH are highly effective and feasible. For the problem of bursty topics identification, PFreeBT adopts an approach, in which we term it as feature-pivot clustering approach. Given a <b>document</b> <b>stream,</b> PFreeBT first identifies a set of bursty features from there. The identification process is based on computing the probability distributions. According to the patterns of the bursty features and two newly defined concepts (equivalent and map-to), a set of bursty topics can be extracted. For the problem of constructing a reliable classifier, we formulate it as a partially supervised classification problem. In this classification problem, only a few training examples are labeled as positive (P). All other training examples (U) are remained unlabeled. Here, U is mixed with the negative examples (N) and some other positive examples (P'). Existing techniques that tackle this problem all focus on finding N from U. None of them attempts to extract P' from U. In fact, it is difficult to succeed as the topics in U are diverse and the features in there are sparse. In this dissertation, PNLH is proposed for extracting a high quality of P' and N from U. In this dissertation, two heuristics, PFreeBT and PNLH, are proposed to tackle the aforementioned problems. PFreeBT aims at identifying the bursty topics in a <b>document</b> <b>stream,</b> whereas PNLH aims at constructing a reliable classifier for a given bursty topic. It is worth noting that both heuristics are parameter free. Users do not need to provide any parameter explicitly. All of the required variables can be computed base on the given <b>document</b> <b>stream</b> automatically. In this information overwhelming century, information becomes ever more pervasive. A new class of data-intensive application arises where data is modeled best as an open-ended stream. We call such kind of data as data <b>stream.</b> <b>Document</b> <b>stream</b> is a variation of data stream, which consists of a sequence of chronological ordered documents. A fundamental problem of mining <b>document</b> <b>streams</b> is to extract meaningful structure from there, so as to help us to organize the contents systematically. In this dissertation, we focus on such a problem. Specifically, this dissertation studies two problems: to identify the bursty topics in a <b>document</b> <b>stream</b> and to construct a classifiers for the bursty topics. A bursty topic is one of the topics resides in the <b>document</b> <b>stream,</b> such {{that a large number of}} documents would be related to it during a bounded time interval. Fung Pui Cheong Gabriel. "August 2006. "Adviser: Jeffrey Xu Yu. Source: Dissertation Abstracts International, Volume: 68 - 03, Section: B, page: 1720. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2006. Includes bibliographical references (p. 122 - 130). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstracts in English and Chinese. School code: 1307...|$|R
40|$|News and twitter are {{sometimes}} closely correlated, while sometimes {{each of them}} has quite independent flow of information, due to the difference of the concerns of their information sources. In order to ef-fectively capture the nature of those two text streams, {{it is very important}} to model both their correlation and their difference. This paper first models their correlation by applying a time series topic model to the <b>document</b> <b>stream</b> of the mixture of time series news and twitter. Next, we divide news streams and twitter into distinct two series of <b>document</b> <b>streams,</b> and then we apply our model of bursty topic detection based on the Kleinberg’s burst detection model. This approach successfully models the difference of the two time series topic models of news and twitter as each hav-ing independent information source and its own concern. ...|$|R
40|$|International audienceWe propose in {{this paper}} two new models for {{modeling}} topic and word-topic dependencies between consecutive <b>documents</b> in <b>document</b> <b>streams.</b> The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance {{the influence of the}} LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA), both in terms of perplexity and for tracking similar topics in a <b>document</b> <b>stream...</b>|$|R
40|$|The {{efficient}} {{processing of}} <b>document</b> <b>streams</b> plays animportant role in many information filtering systems. Emerging applications,such as news update filtering and social network notifications, demandpresenting end-users {{with the most}} relevant content to their preferences. Inthis work, user preferences are indicated {{by a set of}} keywords. A centralserver monitors the <b>document</b> <b>stream</b> and continuously reports to each user thetop-k documents that are most relevant to her keywords. Our objective is tosupport large numbers of users and high stream rates, while refreshing thetop-k results almost instantaneously. Our solution abandons the traditionalfrequency-ordered indexing approach. Instead, it follows an identifier-orderingparadigm that suits better the nature of the problem. When complemented with anovel, locally adaptive technique, our method offers (i) proven optimalityw. r. t. the number of considered queries per stream event, and (ii) an order ofmagnitude shorter response time (i. e., time to refresh the query results) than thecurrent state-of-the-art...|$|R
40|$|Methods for {{detecting}} and summarizing emergent keywords have been extensively studied since social media and microblogging activities {{have started to}} play an important role in data analysis and decision making. We present a system for monitoring emergent keywords and summarizing a <b>document</b> <b>stream</b> based on the dynamic semantic graphs of <b>streaming</b> <b>documents.</b> We introduce the notion of dynamic eigenvector centrality for ranking emergent keywords, and present an algorithm for summarizing emergent events that is based on the minimum weight set cover. We demonstrate our system with an analysis of streaming Twitter data related to public security events...|$|R
30|$|Document {{formatting}} {{systems like}} roff [23] use pipelines with different commands to process and filter <b>document</b> <b>streams.</b> They {{are similar in}} spirit to the approach used in Clive for processing I/O streams (at least for editing commands). Of course, roff like tools rely on conventions {{to be able to}} cooperate in a pipe-line more than they rely on the mechanisms provided by the underlying system, and they are specific-purpose tools.|$|R
40|$|We study {{a problem}} of {{detecting}} priming events based on a time series index and an evolving <b>document</b> <b>stream.</b> We define a priming event as an event which triggers abnormal movements of the time series index, i. e., the Iraq war {{with respect to the}} president approval index of President Bush. Existing solutions either focus on organizing coherent keywords from a <b>document</b> <b>stream</b> into events or identifying correlated movements between keyword frequency trajectories and the time series index. In this paper, we tackle the problem in two major steps. (1) We identify the elements that form a priming event. The element identified is called influential topic which consists of a set of coherent keywords. And we extract them by looking at the correlation between keyword trajectories and the interested time series index at a global level. (2) We extract priming events by detecting and organizing the bursty influential topics at a micro level. We evaluate our algorithms on a real-world dataset and the result confirms that our method is able to discover the priming events effectively...|$|R
50|$|Since 1951, the Fluß-Station Schlitz ("Schlitz River Station") {{has been}} in Schlitz. This working group from the Max Planck Institute for Limnology is researching {{the ecology of the}} river Breitenbach in the {{neighbouring}} Breitecke Nature Preserve, making the Breitenbach, through many studies and publications, one of the world's best researched and <b>documented</b> <b>streams.</b> In 2006, this research station, with the current scientific leader's retirement, is to be closed by the Max Planck Society.|$|R
40|$|In this poster, {{we develop}} an {{evolutionary}} document summarization system for discovering the changes {{and differences in}} each phase of a disaster evolution. Given a collection of <b>document</b> <b>streams</b> describing an event, our system generates a short summary delivering the main development theme of the event by extracting the most representative and discriminative sentences at each phase. Experimental results on the collection of press releases for Hurricane Wilma in 2005 demonstrate the efficacy of our proposal...|$|R
5000|$|In March 2010, Synchronica {{acquired}} the OMA Instant Messaging and Presence Service business, reseller agreements, and existing worldwide mobile operator customer base, of the Instant Messaging developer Colibria AS. Synchronica integrated Colibria's IM technology into Mobile Gateway. In September {{of the same}} year, Synchronica acquired its Canadian mobile email rival, iseemedia Inc. As part of the deal, Synchronica acquired iseemedia's patent-pending <b>document</b> <b>streaming</b> technology, as well as contracts with three large mobile operators in India and South-East Asia.|$|R
40|$|This paper {{presents}} an evolutionary algorithm for modeling the arrival dates of <b>document</b> <b>streams,</b> which is any time-stamped collection of documents, such as newscasts, e-mails, IRC conversations, scientific journals archives and weblog postings. This algorithm assigns frequencies (number of document arrivals per time unit) to time intervals {{so that it}} produces an optimal fit to the data. The optimization is a trade off between accurately fitting the data and avoiding too many frequency changes; this way the analysis is able to find fits which ignore the noise. Classical dynamic programming algorithms are limited by memory and efficiency requirements, {{which can be a}} problem when dealing with long streams. This suggests to explore alternative search methods which allow for some degree of uncertainty to achieve tractability. Experiments have shown that the designed evolutionary algorithm is able to reach the same solution quality as those classical dynamic programming algorithms in a shorter time. We have also explored different probabilistic models to optimize the fitting of the date streams, and applied these algorithms to infer whether a new arrival increases or decreases interest in the topic the <b>document</b> <b>stream</b> is about. Comment: 22 pages, submitted to Journal of Information Retrieva...|$|R
40|$|This paper {{presents}} an evolutionary algorithm for modeling the arrival dates of <b>document</b> <b>streams,</b> which is any timestamped collection of documents, such as newscasts, e-mails, scientific journals archives and weblog postings. The {{goal is to}} find a frequency curve that fits the data circumventing the unavoidable noise. Classical dynamic programming algorithms are limited by memory and efficiency requirements, which can be a problem when dealing with long streams. This suggests to explore alternative search methods which although do not guarantee optimality, are far more efficient. Experiments have shown that the designed evolutionary algorithm is able to reach high quality solutions in a short time. We have also explored different approaches to infer whether new arrivals increase or decrease interest in the topic the <b>document</b> <b>stream</b> is about. In particular, we present a variant of the evolutionary algorithm, which is able to very quickly fit a stream extended with new data, by taking advantage of the fit obtained for the original substream. These mechanisms can be used for real time detection of changes in the trend of interest in a topic, an important application of this kind of models...|$|R
50|$|Large <b>continuous</b> <b>documents</b> {{might not}} be split into {{separate}} sheets. By continuously folding two single sided printed sheets back-to-back and binding together a stack of continuous form paper along one of the folded edges, {{it is possible to}} flip through the stack like a book of double-sided printed pages. With this technique, the stack is normally flipped top to bottom or bottom to top rather than side to side.|$|R
50|$|The {{corporate}} {{constitution of}} a private company registered under the BVI Business Companies Act consists of the memorandum and articles of association. Although these are technically two separate documents, and the companies legislation contains detailed provisions at various points as to what provisions should appear in the memorandum and which should appear in the articles, {{for all intents and}} purposes the documents are co-joined and filed as a single <b>continuous</b> <b>document.</b>|$|R
40|$|In {{this paper}} we {{introduce}} {{a novel approach}} for incrementally building aspect models, {{and use it to}} dynamically discover underlying themes from <b>document</b> <b>streams.</b> Using the new approach we present an application which we call query-line tracking i. e., we automatically discover and summarize different themes or stories that appear over time, and that relate to a particular query. We present evaluation on news corpora to demonstrate the strength of our method for both query-line tracking, online indexing and clustering...|$|R
40|$|We {{propose a}} simple, yet effective, {{pipeline}} architecture for document classification. The task {{we intend to}} solve is to classify large and content-wise heterogeneous <b>document</b> <b>streams</b> on a layered nine-category system, which distinguishes medical from non-medical texts and sorts medical texts into various subgenres. While the document classification problem is often dealt with using computationally powerful and, hence, costly classifiers (e. g., Bayesian ones), we have gathered empirical evidence that a much simpler approach based on n-gram-statistics achieves a comparable level of classification performance. 1...|$|R
40|$|International audienceIn {{this paper}} {{we aim at}} {{filtering}} documents containing timely relevant information about an entity (e. g., a person, a place, an organization) from a <b>document</b> <b>stream.</b> These <b>documents</b> that we call vital documents provide relevant and fresh information about the entity. The approach we propose leverages the temporal information reflected by the temporal expressions in the document in order to infer its vitality. Experiments carried out on the 2013 TREC Knowledge Base Acceleration (KBA) collection show the effectiveness of our approach compared to state-of-the-art ones...|$|R
40|$|Keeping {{track of}} changes in user {{interests}} from a <b>document</b> <b>stream</b> with a few relevance judgments {{is not an easy}} task. To tackle this problem, we propose a novel method that integrates (1) pseudorelevance feedback mechanism, (2) assumption about the persistence of user interests and (3) incremental method for data clustering. This approach has been empirically evaluated using Reuters- 21578 corpus in a setting for information filtering. The experiment results reveal that it significantly improves the performances of existing user-interest-tracking systems without requiring additional, actual relevance judgments...|$|R
40|$|In {{this paper}} {{we aim at}} {{filtering}} documents containing timely relevant information about an entity (e. g., a person, a place, an organization) from a <b>document</b> <b>stream.</b> These <b>documents</b> that we call vital documents provide relevant and fresh information about the entity. The approach we propose leverages the temporal information reflected by the temporal expressions in the document in order to infer its vitality. Experiments carried out on the 2013 TREC Knowledge Base Acceleration (KBA) collection show the effectiveness of our approach compared to state-of-the-art ones...|$|R
40|$|Summarising, aggregating, and {{querying}} streams is a {{very interesting}} problem, one a lot of the big names (like Mi-crosoft Research) are working on. In this report we are sharing with our experience on evaluation of semantic-based search over XML <b>document</b> <b>streams.</b> We developed a frame-work to compare behaviour of two search heuristics on pro-cessing streamed DBLP data. Our report {{is a work in progress}} as it goes in many research directions and leaves many ques-tions open. We are circulating it now for feedback. 1...|$|R
40|$|An {{adaptive}} information {{filtering system}} monitors a <b>document</b> <b>stream</b> {{to identify the}} documents that match information needs specified by user profiles. As the system filters, it also refines its knowledge about the user’s information needs based on long-term observations of the <b>document</b> <b>stream</b> and periodic feedback(training data) from the user. Low variance profile learning algorithms, such as Rocchio, work well at the early stage of filtering when the system has very few training data. Low bias profile learning algorithms, such as Logistic Regression, work well at the later stage of filtering when the system has accumulated enough training data. However, an empirical system needs to works well consistently at all stages of filtering process. This paper addresses this problem by proposing a new technique to combine different text classification algorithms via a constrained maximum likelihood Bayesian prior. This technique provides a trade off between bias and variance, and the combined classifier may achieve a consistent good performance {{at different stages of}} filtering. We implemented the proposed technique to combine two complementary classification algorithms: Rocchio and logistic regression. The new algorithm is shown to compare favorably with Rocchio, Logistic Regression, and the best methods in the TREC- 9 and TREC- 11 adaptive filtering tracks. 1...|$|R
40|$|Algorithms {{evaluating}} {{a keyword}} query over XML <b>document</b> <b>streams</b> {{have a chance}} to become a core part of a new class of information filtering systems. However, the problem of creating a common benchmark for such systems has not re-ceived much attention in the community yet. We present our initial contribution to create such a benchmark by in-troducing a method of identifying correct result nodes across streams. We practically prove that our method can be used to understand behaviour of a search heuristic, compare ef-fectiveness of various search heuristics and efficiency of pro-cessing algorithms (implementations) employing the same search heuristic...|$|R
40|$|Researches {{carried out}} around 1970 showed that network {{bandwidth}} {{is not sufficient}} for streaming media over the network. In late 1990 s the network bandwidth has improved and it facilitated the utilization of streaming media. From time to time advancements being made to streaming technologies with major focus on audio and video data types and several successful researches are being carried out to use mobile phones as streaming clients. But there were no research outcomes regarding the <b>streaming</b> of <b>documents</b> over the network. This paper addresses this issue and document a successfully developed mobile application to cater <b>document</b> <b>streaming.</b> 1...|$|R
5000|$|Adopt-A-Stream: This ongoing citizen based {{volunteer}} {{monitoring program}} samples aquatic insects and <b>documents</b> present <b>stream</b> {{conditions in the}} spring and fall of each year. Volunteers are led by a trained volunteer team leader.|$|R
40|$|A {{fundamental}} problem in {{text data mining}} is to extract meaning-ful structure from <b>document</b> <b>streams</b> that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity {{for a period of}} time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text min-ing work ih this area is the following intuitive premise [...] that the appearance of a topic in a <b>document</b> <b>stream</b> is signaled by a &quot;burst of activity, &quot; with certain features rising sharply in frequency as the topic emerges. The goal of the present work is to develop a formal approach for modeling such &quot;bursts, &quot; {{in such a way that}} they can be robustly and efficiently identified, and can provide an organizational frame-work for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing the-ory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them. I...|$|R
40|$|Abstract—The goal {{of online}} event {{analysis}} is to detect events and track their associated documents {{in real time}} from a <b>continuous</b> <b>stream</b> of <b>documents</b> generated by multiple information sources. Unlike traditional text categorization methods, event analysis approaches consider the temporal relations among documents. However, such methods suffer from the threshold-dependency problem, so they only perform well for a narrow range of thresholds. In addition, if {{the contents of a}} <b>document</b> <b>stream</b> change, the optimal threshold (that is, the threshold that yields the best performance) often changes as well. In this paper, we propose a threshold-resilient online algorithm, called the Incremental Probabilistic Latent Semantic Indexing (IPLSI) algorithm, which alleviates the threshold-dependency problem and simultaneously maintains the continuity of the latent semantics to better capture the story line development of events. The IPLSI algorithm is theoretically sound and empirically efficient and effective for event analysis. The results of the performance evaluation performed on the Topic Detection and Tracking (TDT) - 4 corpus show that the algorithm reduces the cost of event analysis by as much as 15 percent 20 percent and increases the acceptable threshold range by 200 percent to 300 percent over the baseline. Index Terms—Clustering, online event analysis, probabilistic algorithms, text mining. Ç...|$|R
