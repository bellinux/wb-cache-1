12|52|Public
5000|$|The {{encoding}} {{used for}} the NewStringUTF, GetStringUTFLength, GetStringUTFChars, ReleaseStringUTFChars and GetStringUTFRegion functions is [...] "modified UTF-8", which is not valid UTF-8 for all inputs, but a different encoding really. The null character (U+0000) and codepoints not on the Basic Multilingual Plane (greater {{than or equal to}} U+10000, i.e. those represented as surrogate pairs in UTF-16) are encoded differently in modified UTF-8. Many programs actually use these functions incorrectly and treat the UTF-8 strings returned or passed into the functions as standard UTF-8 strings instead of modified UTF-8 strings. Programs should use the NewString, GetStringLength, GetStringChars, ReleaseStringChars, GetStringRegion, GetStringCritical and ReleaseStringCritical functions, which use UTF-16LE encoding on little-endian architectures and UTF-16BE on big-endian architectures, and then use a UTF-16 to UTF-8 <b>conversion</b> <b>routine.</b>|$|E
40|$|This paper adresses {{the problem}} of {{modelling}} paraphrases in a deep linguistic processing framework where the meaning construction component {{is based on an}} LFG grammar. We present a syntax-based approach to paraphrase extraction that operates on shallow dependency analyses in a parallel corpus. By means of an XLE-based <b>conversion</b> <b>routine,</b> we generate transfer rules for the automatically acquired semantic correspondences. These rules can be used as an additional component in the rule-based process of meaning construction which will augment the meaning representation with entailments that hold for complex phrasal units. ...|$|E
40|$|This paper {{presents}} {{an approach to}} the integration of paraphrases in an LFG-based, semantic process-ing system and outlines advantages of deep linguistic processing for dealing with paraphrases in large-scale semantics. We describe an XLE-based <b>conversion</b> <b>routine</b> that integrates the paraphrases acquired in a parallel corpus into the semantic system, providing a simple and effective representation for the meaning of phrasal expressions. Our paraphrase representation extends the seman-tic transfer system outlined in (Crouch and King, 2006) which makes use of XLE’s term rewrite en-gine to derive semantic representations from LFG F-structures. The system already integrates various lex-ical semantic resources like WordNet such that, e. g. ...|$|E
5000|$|Subroutine library {{containing}} input/output, mathematical, arithmetic and <b>conversion</b> <b>routines</b> ...|$|R
40|$|Syntax Notation 1) [ISO 88], XDR (eXternal Data Representation) [Sun 86], the {{interface}} description language defined by OSF-DCE (Open Systems Foundation-Distributed Computing 3 Environment) [Shir 92] and {{the interface}} description language {{defined by the}} Object Management Group [OMG 93]. Today, stub generators alternatively produce {{two different types of}} conversion routines: tabledriven <b>conversion</b> <b>routines</b> and procedure-driven <b>conversion</b> <b>routines.</b> These two alternatives differ in the way the control structure to convert composite types is represented. A stub generator that produces procedure-driven <b>conversion</b> <b>routines</b> generates code that consists of a sequence of procedure calls, one for each component of the composite type. In contrast, a stub generator that produces table-driven <b>conversion</b> <b>routines</b> generates a table that contains one entry per component of the composite type. At run-time, this table is interpreted by a generic routine. Well-known stub generators that produce procedure-d [...] ...|$|R
5000|$|... libswscale : A library {{containing}} {{video image}} scaling and colorspace/pixelformat <b>conversion</b> <b>routines.</b>|$|R
40|$|This manual {{outlines}} the procedures to collect data, poll the datalogger, and process the data. The processed data is then {{used with a}} calibrated DOE- 2 input file using statistical graphing routines for the U. S. D. O. E. Forrestal Child Development Center. Appendix A contains the data processing routines. Appendix B contains the final calibrated input file. Appendix C contains processing and column merging routines. Appendix D contains the SAS graphical routines. Appendix E contains a solar <b>conversion</b> <b>routine</b> that converts data collected at an 18 degree south facing tilt to global horizontal solar data. Appendix F contains information on the channel tables for the loggers located at the Forrestal building and the Forrestal Child Development Center...|$|E
40|$|This {{revision}} of NREC Technical Manual No. 144 (PARM 10 - Factor File Conversion) supersedes the manual dated June 1963, {{due to the}} fact that several modifications have been made to the Factor File <b>Conversion</b> <b>Routine.</b> The areas of the program which have been modified deal with the table parameters, the activity parameters, and the ancillary vector factors. The purpose of the Factor File Conversion computer program is to convert factor records from their maintenance format into the format required for computation. The inputs to the routine are a file of factor records in maintenance format (PARMMF tape), and either one or two j*j concordance files (PARMJJ tapes). The output from the routine is a file of factor records in computational format (PARMCF) for use by the PARM central routine. Section V-A 5 of the PARM Formulation Manual provides guidance in the use of the program. (Author). Supersedes report dated June 63. This {{revision of}} NREC Technical Manual No. 144 (PARM 10 - Factor File Conversion) supersedes the manual dated June 1963, {{due to the fact}} that several modifications have been made to the Factor File <b>Conversion</b> <b>Routine.</b> The areas of the program which have been modified deal with the table parameters, the activity parameters, and the ancillary vector factors. The purpose of the Factor File Conversion computer program is to convert factor records from their maintenance format into the format required for computation. The inputs to the routine are a file of factor records in maintenance format (PARMMF tape), and either one or two j*j concordance files (PARMJJ tapes). The output from the routine is a file of factor records in computational format (PARMCF) for use by the PARM central routine. Section V-A 5 of the PARM Formulation Manual provides guidance in the use of the program. (Author). Mode of access: Internet...|$|E
40|$|Abstract — This paper {{reports about}} the {{development}} of a Named Entity Recognition (NER) system in two leading Indian languages, namely Bengali and Hindi using the Maximum Entropy (ME) framework. We have used the annotated corpora, obtained from the IJCNLP- 08 NER Shared Task on South and South East Asian Languages 1 (NERSSEAL) and tagged with a fine-grained Named Entity (NE) tagset 2 of twelve tags. An appropriate tag <b>conversion</b> <b>routine</b> has been developed in order to convert these corpora to the forms, tagged with the four NE tags, namely Person name, Location name, Organization name and Miscellaneous name. The system makes use of the different contextual information of the words along with the variety of orthographic word-level features that are helpful in predicting the four NE classes. In this work, we have considered language independent features that are applicable to both the languages as well as the language specific features of Bengali and Hindi. Evaluation results show that the use of linguistic features can improve the performance of the system. Evaluation results of the 10 -fold cross validation tests yield the overall average recall, precision, and f-score values of 88. 01 %, 82. 63 %, an...|$|E
40|$|Using a Receiver Makes it Right (RMR) data {{conversion}} technique in PVM significantly improves the message-passing performance in heterogeneous environments. The improvements {{are due to}} 3 factors: 1). RMR reduces the need for conversions in a heterogeneous environment; 2). At most each message is converted only once compared to twice for XDR used in current public version of PVM; 3). Our <b>conversion</b> <b>routines</b> are streamlined 1 and are several {{times faster than the}} XDR routines. The drawback to RMR is the potential need for a large number of <b>conversion</b> <b>routines.</b> We demostrate that {{only a small number of}} routines are required because many vendors use the IEEE standard for data representation. Given this fact, RMR may emerge as a promising technique in distributed computing. 1 Introduction PVM (Parallel Virtual Machine) is a software system that enables a heterogeneous collection of parallel and serial computers to be programmed as a single machine and utilized as a unified general and fl [...] ...|$|R
40|$|This note {{discusses}} the main issues in performing correctly rounded decimal-to-binary and binary-to-decimal conversions. It reviews recent work by Clinger and by Steele and White on these conversions and describes some efficiency enhancements. Computational experience with {{several kinds of}} arithmetic suggests that the average computational cost for correct rounding can be small for typical conversions. Source for <b>conversion</b> <b>routines</b> that support this claim is available from netlib...|$|R
40|$|A {{package of}} {{computer}} programs for handling taxonomic databases Ft. J. Pankhurst A package of programs is described which processes taxonomic data, suitable {{to use when}} preparing monographs, handbooks, Floras or Faunas, in which species or other taxa are described and identified. There is also an interactive program for specimen identification, and <b>conversion</b> <b>routines</b> which prepare data for numerical taxonomy (clustering and cladistics). The programs are equally suitable for botany or for zoology, or even for non-biological data...|$|R
40|$|International audienceFor {{the purpose}} of reuse, {{terminology}} is exchangedbetween language applications, mostly translationorientated, and TBX. Data exchange with description formalisms of the Semantic Web like RDF, RDFS, OWL or SKOS constitute an increasingly important scenario for TBX. However, when migrating terminology between TBX and the serializations of the mentioned formalisms, the question arises as to which kind of information is relevant to data exchange. It would surely be presumptuous to try to determine an approximate number of exchange constellations, since data exchange between terminological databases and knowledge databases can range from relatively flat-structured andsimpleto very comprehensive and complexdata models. The present article describes a nearly automated, well-defined <b>conversion</b> <b>routine</b> based on a maximal TBX data model. Smaller data models may be derived from the model depicted here. Potential use cases for the maximal data model are knowledge databases targeted also for terminologists, translators or content authorswho wish to enrich their knowledge database with meaningful terminological information. In the reverse case, such terminology-enriched knowledge databases provide an added value for translation-orientated or monolingual language production environments so that data exchange from RDF, RDFS, OWL or SKOS serializations to TBX is also highly desirable...|$|E
40|$|The rapid {{development}} of language tools using machine learning techniques for less computerized languages requires appropriately tagged corpus. A Bengali news corpus {{has been developed}} from the web archive of a widely read Bengali newspaper. A web crawler retrieves the web pages in Hyper Text Markup Language (HTML) format from the news archive. At present, the corpus contains approximately 34 million wordforms. The date, location, reporter and agency tags present in the web pages have been automatically named entity (NE) tagged. A portion of this partially NE tagged corpus has been manually annotated with the sixteen NE tags {{with the help of}} Sanchay Editor 1, a text editor for Indian languages. This NE tagged corpus contains 150 K wordforms. Additionally, 30 K wordforms have been manually annotated with the twelve NE tags as part of the IJCNLP- 08 NER Shared Task for South and South East Asian Languages 2. A table driven semi-automatic NE tag <b>conversion</b> <b>routine</b> has been developed in order to convert the sixteen-NE tagged corpus to the twelve-NE tagged corpus. The 150 K NE tagged corpus has been used to develop Named Entity Recognition (NER) system in Bengali using pattern directed shallow parsing approach...|$|E
40|$|The hidden {{qualities of}} a product are often {{revealed}} in the process. Subsurface material damage, surface cracks, and unusual burr formation can occur during a poorly controlled machining process. Standard post process inspection is costly and may not reveal these conditions. However, by monitoring the proper process parameters, these conditions are readily detectable without incurring the cost of post process inspection. In addition, many unforeseen process anomalies may be detected using an advanced process monitoring system. This work created a process monitoring system for milling machines which mapped the forces, power, vibration, and acoustic emissions generated during a cutting cycle onto a 3 D model of the part being machined. The hyperpoint overlay can be analyzed and visualized with VRML (Virtual Reality Modeling Language). Once the Process Monitoring System is deployed, detailed inspection may be significantly reduced or eliminated. The project deployed a Pro-Engineer to VRML model <b>conversion</b> <b>routine,</b> advanced visualization interface, tool path transformation with mesh generation routine, hyperpoint overlay routine, stable sensor array, sensor calibration routine, and machine calibration methodology. The technology created in this project can help validate production of WR (War Reserve) components by generating process signatures for products, processes, and lot runs. The signatures of each product can be compared across all products made within and across lot runs {{to determine if the}} processes that produced the product are consistently providing superior quality. Furthermore, the qualities of the processes are visibly apparent, since the part model is overlaid with process data. The system was evaluated on three different part productions...|$|E
40|$|This paper {{describes}} {{the issues involved}} in sharing data among processes executing co-operatively in a heterogeneous computing environment. In a heterogeneous environment, the physical representation of a data object {{may need to be}} transformed when the object is moved from one type of processor to another. As a part of a larger project to build a heterogeneous distributed shared memory system we developed an automated tool for generating the <b>conversion</b> <b>routines</b> that are used to implement representation conversion for data objects. We developed a novel method for processing source programs that allowed us to extract detailed information about the physical representation of data objects without access to the source code of the compilers that were generating those representations. A performance comparison with the more general XDR heterogeneous data conversion package shows that the customized <b>conversion</b> <b>routines</b> that we generate are 4 to 8 times faster. key words: Heterogeneity External data representation Automatic data conversion XDR Mermaid HETEROGENEOUS DISTRIBUTED SHARED MEMORY Distributed shared memory (DSM) was developed by Li 1 as a mechanism for providing a logically shared and consistent virtual memory on processors connecte...|$|R
50|$|A Lilian date is {{the number}} of days since the {{beginning}} of the Gregorian Calendar on October 15, 1582, regarded as Lilian date 1. It was invented by Bruce G. Ohms of IBM in 1986 and is named for Aloysius Lilius, who devised the Gregorian Calendar. Lilian dates can be used to calculate the number of days between any two dates occurring {{since the beginning of the}} Gregorian calendar. It is currently used by date <b>conversion</b> <b>routines</b> that are part of IBM Language Environment (LE) software.|$|R
40|$|We examine several matrix layouts {{based on}} space-filling curves {{that allow for}} a cache-oblivious {{adaptation}} of parallel TU decomposition for rectangular matrices over finite fields. The TU algorithm of Dumas requires index <b>conversion</b> <b>routines</b> for which the cost to encode and decode the chosen curve is significant. Using {{a detailed analysis of}} the number of bit operations required for the encoding and decoding procedures, and filtering the cost of lookup tables that represent the recursive decomposition of the Hilbert curve, we show that the Morton-hybrid order incurs the least cost for index <b>conversion</b> <b>routines</b> that are required throughout the matrix decomposition as compared to the Hilbert, Peano, or Morton orders. The motivation lies in that cache efficient parallel adaptations for which the natural sequential evaluation order demonstrates lower cache miss rate result in overall faster performance on parallel machines with private or shared caches, on GPU's, or even cloud computing platforms. We report on preliminary experiments that demonstrate how the TURBO algorithm in Morton-hybrid layout attains orders of magnitude improvement in performance as the input matrices increase in size. For example, when N = 2 ^ 13, the row major TURBO algorithm concludes within about 38. 6 hours, whilst the Morton-hybrid algorithm with truncation size equal to 64 concludes within 10. 6 hours...|$|R
40|$|Fossils {{are the key}} to dating the past, but geoscience {{students}} are rarely exposed to their use in constraining geologic time and deciphering regional sediment history. A software package and an associated student exercise that uses real data sets of fossils and sediment trends from drill sites in the Gulf of Mexico is developed to illustrate the application of biostratigraphy to understand regional depositional history and sea-level variations. ^ TimeScaleCreator Crossplot, a visualization system to understand the depth-to-age conversion methodology for converting well datasets varying in depth to geologic timescale was developed and tested. A transect consisting of four offshore wells and two onshore wells spanning Miocene and Oligocene epochs along an approximate linear trend was chosen to illustrate the concepts of sea-level variations. Depth-to-age <b>conversion</b> <b>routine</b> enables the students to convert the accompanying sediment facies to geologic timescale by comparing the fossil assemblages observed in the well to the biostratigraphy of Gulf of Mexico. After creating a transect from depth-to-age converted wells, students can identify depositional episodes of sand-dominated influxes separated by shale-deposition. The interplay between various sand and shale sequences allows the students to interpret sea-level variations in the northern Gulf of Mexico basin. ^ The student exercise developed consists of an introduction to biostratigraphy, depositional history in the Gulf of Mexico basin and a manual on how to use the visualization tool, which serves as background material to convert the wells from depth-to-age and interpret for sea-level variations. The student exercise is tested with undergraduate students and evidence of student learning is discussed. The student exercise also contains a teacher 2 ̆ 7 s manual that provides our suggested solutions. ...|$|E
40|$|Much {{attention}} has focussed on {{the conversion of}} human pluripotent stem cells (PSCs) to a more naïve developmental status. Here we provide a method for resetting via transient histone deacetylase inhibition. The protocol is effective across multiple PSC lines and can proceed without karyotype change. Reset cells can be expanded without feeders with a doubling time of around 24 h. WNT inhibition stabilises the resetting process. The transcriptome of reset cells diverges markedly from that of primed PSCs and shares features with human inner cell mass (ICM). Reset cells activate expression of primate-specific transposable elements. DNA methylation is globally reduced to a level equivalent {{to that in the}} ICM and is non-random, with gain of methylation at specific loci. Methylation imprints are mostly lost, however. Reset cells can be re-primed to undergo tri-lineage differentiation and germline specification. In female reset cells, appearance of biallelic X-linked gene transcription indicates reactivation of the silenced X chromosome. On reconversion to primed status, $XIST$-induced silencing restores monoallelic gene expression. The facile and robust <b>conversion</b> <b>routine</b> with accompanying data resources will enable widespread utilisation, interrogation, and refinement of candidate naïve cells. This research is funded by the Medical Research Council of the United Kingdom (MR/P 00072 X/ 1) and European Commission Framework 7 (HEALTH-F 4 - 2013 - 602423, PluriMes), and in part by the UK Regenerative Medicine Platform (MR/L 012537 / 1). WR is supported by the BBSRC (BB/K 010867 / 1), Wellcome Trust (095645 /Z/ 11 /Z), EU BLUEPRINT, and EpiGeneSys. The Cambridge Stem Cell Institute receives core funding from the Wellcome Trust and the Medical Research Council. FvM was funded by a Postdoctoral Fellowship from the Swiss National Science Foundation (SNF) /Novartis. SM is funded by a Wellcome Trust PhD Studentship. AS is a Medical Research Council Professor...|$|E
40|$|The Weather Research and Forecasting (WRF) {{model is}} the next {{generation}} community mesoscale model designed to enhance collaboration between the research and operational sectors. The NM'S {{as a whole has}} begun a transition toward WRF as the mesoscale model of choice to use as a tool in making local forecasts. Currently, both the National Weather Service in Melbourne, FL (NWS MLB) and the Spaceflight Meteorology Group (SMG) are running the Advanced Regional Prediction System (AIRPS) Data Analysis System (ADAS) every 15 minutes over the Florida peninsula to produce high-resolution diagnostics supporting their daily operations. In addition, the NWS MLB and SMG have used ADAS to provide initial conditions for short-range forecasts from the ARPS numerical weather prediction (NWP) model. Both NM'S MLB and SMG have derived great benefit from the maturity of ADAS, and would like to use ADAS for providing initial conditions to WRF. In order to assist in this WRF transition effort, the Applied Meteorology Unit (AMU) was tasked to configure and implement an operational version of WRF that uses output from ADAS for the model initial conditions. Both agencies asked the AMU to develop a framework that allows the ADAS initial conditions to be incorporated into the WRF Environmental Modeling System (EMS) software. Developed by the NM'S Science Operations Officer (S 00) Science and Training Resource Center (STRC), the EMS is a complete, full physics, NWP package that incorporates dynamical cores from both the National Center for Atmospheric Research's Advanced Research WRF (ARW) and the National Centers for Environmental Prediction's Non-Hydrostatic Mesoscale Model (NMM) into a single end-to-end forecasting system. The EMS performs nearly all pre- and postprocessing and can be run automatically to obtain external grid data for WRF boundary conditions, run the model, and convert the data into a format that can be readily viewed within the Advanced Weather Interactive Processing System. The EMS has also incorporated the WRF Standard Initialization (SI) graphical user interface (GUT), which allows the user to set up the domain, dynamical core, resolution, etc., with ease. In addition to the SI GUT, the EMS contains a number of configuration files with extensive documentation to help the user select the appropriate input parameters for model physics schemes, integration timesteps, etc. Therefore, because of its streamlined capability, it is quite advantageous to configure ADAS to provide initial condition data to the EMS software. One of the biggest potential benefits of configuring ADAS for ingest into the EMS is that the analyses could be used to initialize either the ARW or NMM. Currently, the ARPS/ADAS software has a <b>conversion</b> <b>routine</b> only for the ARW dynamical core. However, since the NIvIM runs about 2. 5 times faster than the ARW, it is quite advantageous to be able to run an ADAS/NMM configuration operationally due to the increased efficiency...|$|E
50|$|In most languages, {{the word}} {{coercion}} {{is used to}} denote an implicit conversion, either during compilation or during run time. For example, in an expression mixing integer and floating point numbers (like 5 + 0.1), the compiler will automatically convert integer representation into floating point representation so fractions are not lost. Explicit type conversions are either indicated by writing additional code (e.g. adding type identifiers or calling built-in routines) or by coding <b>conversion</b> <b>routines</b> for the compiler to use when it otherwise would halt with a type mismatch.|$|R
5000|$|Because {{regions are}} bound to a {{specific}} orientation, a ninety degree rotation of a region would require both detailed reverse engineering of the structure and extensive coding. A general rotation is impractical when compared to rotating the original source boundary description and simply creating a new region. However, the API includes <b>conversion</b> <b>routines</b> to and from BitMaps. (Bitmaps may also be rotated using well known methods, but with various degrees of image degradation depending upon angle chosen, the storage and processor cycles available to the operation, and {{the complexity of the}} algorithm.) ...|$|R
40|$|The Limburg Soil Erosion Model (LISEM) is a {{physically}} based model incorporated in a raster geographical information system. This incorporation facilitates easy application in larger catchments, improves the user-friendliness by avoiding <b>conversion</b> <b>routines</b> {{and allows the}} use of remotely sensed data. Processes incorporated in this model are rainfall, interception, surface storage in microdepressions, infiltration and vertical movement {{of water in the}} soil, overland flow, channel flow, detachment by rainfall and through-fall, detachment by overland flow and transport capacity of the flow. Special {{attention has been paid to}} the influence of tractor wheeling, small roads and surface sealing...|$|R
50|$|Later, {{computer}} manufacturers {{began to}} make use of the spare bit to extend the ASCII character set beyond its limited set of English alphabet characters. 8-bit extensions such as IBM code page 37, PETSCII and ISO 8859 became commonplace, offering terminal support for Greek, Cyrillic, and many others. However, such extensions were still limited in that they were region specific and often could not be used in tandem. Special <b>conversion</b> <b>routines</b> had to be used to convert from one character set to another, often resulting in destructive translation when no equivalent character existed in the target set.|$|R
40|$|OGSA-DAI {{provides}} an extensible Web service-based framework that allows data resources to {{be incorporated into}} Grid fabrics. The current OGSA-DAI release (OGSA-DAI WSI/WSRF v 2. 2) has implemented a set of optimizations identified through the examination of common OGSA-DAI use patterns. In this paper we describe these patterns and detail the optimizations {{that have been made}} for the current release based on the profiles obtained. These optimizations include improvements to the performance of various data format <b>conversion</b> <b>routines,</b> the introduction of more compact data delivery formats, and the adoption of SOAP with attachments for data delivery. We quantify the performance improvements in comparison to the previous OGSA-DAI release...|$|R
40|$|Description This package {{provides}} an interface to Unidata's NetCDF library functions (version 3) and furthermore access to Unidata's UDUNITS calendar <b>conversions.</b> The <b>routines</b> and the documentation follow the NetCDF and UDUNITS C interface, so the corresponding manuals can be consulted for more detailed information...|$|R
40|$|Abstract. The {{shift of}} {{interest}} to web tables in HTML and PDF files, coupled with the incorporation of table analysis and <b>conversion</b> <b>routines</b> in commercial desktop document processing software, are likely to turn table recognition into more of a systems than an algorithmic issue. We illustrate the transition by some actual examples of web table conversion. We then suggest that the appropriate target format for table analysis, whether performed by conventional customized programs or by off-theshelf software, is a representation based on the abstract table introduced by X. Wang in 1996. We show that the Wang model is adequate for some useful tasks that prove elusive for less explicit representations, and outline our plans to develop a semi-automated table processing system to demonstrate this approach. Screen-snaphots of a prototype tool to allow table mark-up {{in the style of}} Wang are also presented. ...|$|R
40|$|Accurate {{geological}} modelling {{of features}} such as faults, fractures or erosion requires grids that are flexible with respect to geometry. Such grids generally contain polyhedral cells and complex grid cell connectivities. The grid representation for polyhedral grids in turn affects the efficient implementation of numerical methods for subsurface flow simulations. We give an overview of an open-source matlab toolbox that has been developed to support our research on new, consistent and convergent, computational methodologies. The toolbox offers flexibility and efficiency with respect to different grid formats, and in particular hierarchical grids used in multiscale methods. The MATLAB Reservoir Simulation Toolbox (MRST) The toolbox has the following functionality for rapid prototyping of solvers for flow and transport: I grid structure, grid factory routines, input/processing of industry-standard formats, real-life and synthetic example grids I petrophysical parameters and incompressible fluid models, <b>conversion</b> <b>routines</b> to/from SI and common field units, very simplified geostatistical routine...|$|R
40|$|Presentation {{conversion}} is a {{key operation}} in any development environment for distributed applications, such as Corba, Java-RMI, DCE or ASN. 1 -based environments. It is also well-known performance bottleneck in high-speed network communication. Presentation conversion code is usually generated by an automatic code generation tool referred to as stub compiler. The quality of the code generated by a stub compiler is often very low. The code is either very slow, or has a large code size, or both. This paper describes the design and experimental evaluation of an optimization stage for a stub compiler. The optimization stage automates the trade-off between code size and execution speed of the code generated by the compiler. This is achieved by using a hybrid of two implementation alternatives for presentation <b>conversion</b> <b>routines</b> (interpreted and procedure-driven code). The optimization problem is modeled as a Knapsack problem. A Markov model in combination with a heuristic branch predictor is used for [...] ...|$|R
40|$|Many {{advantages}} {{have been}} given for using formal specifications {{in the design and}} implementation of communication systems. Performance is usually not among them. It is commonly believed that code generated by an automatic tool from a formal specification is inherently slower than code implemented manually. This paper gives experimental evidence that this contention might be false. The key idea is to integrate heuristics used by a human programmer when optimizing code into the code generation tool. This way, the tool can generate code that is competitive with code written by a human programmer, and even better for specications of sufficient complexity. Experiments were conducted using the presentation <b>conversion</b> <b>routines</b> generated by an ASN. 1 compiler. The paper describes the design and implementation of an optimisation stage that automates the trade-off between code size and execution speed in these routines. For this purpose, a heuristic method to predict the frequency of type usage is dev [...] ...|$|R
40|$|Many cartographic systems {{currently}} rely on raster-scan digitizing {{to convert}} analog source material to digital form. Raster technology is very effective at rapid and accurate digitization of {{large volumes of}} cartographic data. At the same time, existing raster-to-vector (R-V) conversion processes rely on an inordinately large amount of human post-scan editing to coherently sort and combine the short, unattributed lineal segments into single cartographic spatial entities. The Automatic Feature Tracking (AFT) system addresses {{one of the major}} causes of this bottleneck in the digitization process—skeletonization. This is accomplished by directly converting symbolized linear features on a raster map image into sets of x,y coordinates. The system relies on Template Matching and Feature Tracking techniques to locate feature centerlines. This paper briefly reviews the history of map digitization techniques, illustrates inadequacies of those past approaches, and presents the AFT system {{as an alternative to the}} R-V <b>conversion</b> <b>routines</b> in existing raster digitization systems...|$|R
40|$|Datasets {{of images}} {{annotated}} with eye tracking data constitute important ground truth {{for the development}} of saliency models, which have applications in many areas of electronic imaging. While comparisons and reviews of saliency models abound, similar comparisons among the eye tracking databases themselves are rare. In an earlier paper, 1 we reviewed the content and purpose of over two dozen databases available in the public domain and discussed their commonalities and differences. A major issue is that the formats of the various datasets vary a lot owing to the nature of tools used for eye movement recordings, and often specialized code is required to use the data for further analysis. In this paper, we therefore propose a common reference format for eye tracking data, together with <b>conversion</b> <b>routines</b> for 16 existing image eye tracking databases to that format. Furthermore, we conduct a few analyses on these datasets as examples of what X-Eye facilitates...|$|R
40|$|In {{materials}} science {{the orientation of}} a crystal lattice is described {{by means of a}} rotation relative to an external reference frame. A number of rotation representations are in use, including Euler angles, rotation matrices, unit quaternions, Rodrigues–Frank vectors and homochoric vectors. Each representation has distinct advantages and disadvantages with respect to the ease of use for calculations and data visualization. It is therefore convenient to be able to easily convert from one representation to another. However, historically, each representation has been implemented using a set of often tacit conventions; separate research groups would implement different sets of conventions, thereby making the comparison of methods and results difficult and confusing. This tutorial article aims to resolve these ambiguities and provide a consistent set of conventions and conversions between common rotational representations, complete with worked examples and a discussion of the trade-offs necessary to resolve all ambiguities. Additionally, an open source Fortran- 90 library of <b>conversion</b> <b>routines</b> for the different representations is made available to the community...|$|R
40|$|In this study, {{we have a}} {{research}} of the noninvasive temperature measurement based on microwave temperature sensor. Moreover, in order to solve the surface temperature measurement for designing microwave temperature sensor, the microwave was issued by the transmitting antenna. Microwave encountered by the measured object to return back to the measured object and then convert it into electrical signals, {{the use of the}} quantitative relationship between this signal and input noise temperature to real-time calibration. In order to calculate the antenna brightness temperature and then after signal conditioning circuit, which can show the temperature value, in order to achieve the detection of microwave temperature. Microwave-temperature measurement system hardware based on 89 C 51 microcontroller consists of the microwave temperature sensor, signal conditioning circuitry and chip control circuit, AD converter circuit and display circuit. The system software is by the main program, the AD <b>conversion</b> <b>routines,</b> subroutines and delay subprogram. The microwave temperature measurement characterize has: without gain fluctuations, without the impact of changes in the noise of the machine, to provide continuous calibration, wide dynamic range...|$|R
40|$|This {{guide is}} {{targeted}} towards individuals that use {{geographic information system}} (GIS) technology. It describes several techniques for translating data from Geographic Resources Analysis Support System (GRASS) format data files to the ARC/INFO data format used with Environmental Systems Research Institute, INC (ESRI) products. The primary <b>conversion</b> <b>routines</b> discussed here {{are a result of}} a collaborative effort between the U. S. Army Construction Engineering Research Laboratories (USACERL) and ESRI. GRASS is a public domain geographic information system originally developed by USACERL. This report is intended {{to be used as a}} reference during the data conversion process; it describes the two data formats and contains tips that may facilitate the conversion process. Approved for public release; distribution is unlimited. The contents of this report are not to be used for advertising, publication, or promotional purposes. Citation of trade names does not constitute an official endorsement or approval of the use of such commercial products. The findings of this report are not to be construed as an official Department of the Army position, unless so designated by other authorized documents...|$|R
