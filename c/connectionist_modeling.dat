72|1306|Public
50|$|Joordens, S., & Besner, D. (1994). When {{banking on}} meaning is not (yet) {{money in the}} bank: Explorations in <b>connectionist</b> <b>modeling.</b> Journal of Experimental Psychology: Learning, Memory, and Cognition, 20, 1 - 12.|$|E
5000|$|For his PhD {{and later}} {{research}} he collected a corpus {{of several thousand}} naturally occurring speech errors, and focused on one word substitutes for another (e.g. saying [...] "pass the pepper" [...] instead of [...] "pass the salt"). He concluded that speech production is an interactive, parallel process, leading him to an interest in <b>connectionist</b> <b>modeling,</b> and research on computational modeling, ageing, and metacognition.|$|E
5000|$|Morten H. Christiansen is a Danish {{cognitive}} scientist {{known for}} his work on the evolution of language, and <b>connectionist</b> <b>modeling</b> of human language acquisition. He is Professor in the Department of Psychology and Co-Director of the Cognitive Science Program at Cornell University as well Senior Scientist at the Haskins Labs and Professor in the School of Communication and Culture at Aarhus University. His research has produced evidence for considering language to be a cultural system that is shaped by general-purpose cognitive and learning mechanisms, rather than from innate language-specific mental structures.|$|E
40|$|We {{investigate}} {{the performance of}} the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are <b>modeled</b> by <b>connectionist</b> <b>models.</b> The <b>connectionist</b> <b>models</b> use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, {{not only because of the}} inherent capability of the <b>connectionist</b> <b>model</b> in fighting the data sparseness problem, but also because of the sublinear growth in the model size when the context length is increased. The <b>connectionist</b> <b>models</b> can be further trained by an EM procedure, similar to the previously used procedure for training the SLM. Our experiments show that the <b>connectionist</b> <b>models</b> can significantly improve the PPL over the interpolated and back-off models on the UPENN Treebank corpora, after interpolating with a baseline trigram language model. The EM training procedure can improve the <b>connectionist</b> <b>models</b> further, by using hidden events obtained by the SLM parser...|$|R
40|$|In {{this paper}} we study the parallelization of the {{inference}} process for <b>connectionist</b> <b>models.</b> We use a symbolic formalism for {{the representation of}} the <b>connectionist</b> <b>models.</b> With this translation, the training mechanism is local in the elements of the network, the computing power is improved in the network nodes and a local hybridization with symbolic parts is achieved. The inference in the final knowledge network can be parallelized, whether the knowledge corresponds to a symbolic module, a <b>connectionist</b> <b>model</b> or a hybrid connectionist-symbolic module. Besides, the concurrency for knowledge networks corresponding to <b>connectionist</b> <b>models</b> is presented for the phases of processing and training. The parallelization is studied for a multiprocessor architecture with shared memory. 1 Introduction The solution usually applied for the implementation of <b>connectionist</b> <b>models</b> is the simulation by software. The training and processing mechanisms of the network are implemented on a hosts, that d [...] ...|$|R
40|$|There are {{now many}} {{different}} approaches to the computational modelling of cognition, e. g. symbolic models (Kieras & Meyer, 1997; Newell, 1990), cognitive <b>connectionist</b> <b>models</b> (McLeod, Plunkett, & Rolls, 1998) and neurophysiologically prescribed <b>connectionist</b> <b>models</b> (O'Reilly & Munakata, 2000). The relative value o...|$|R
5000|$|Ping Li (...) is a Professor of Psychology, Linguistics, and Information Sciences and Technology at Pennsylvania State University. He {{specializes in}} {{language}} acquisition, focusing on bilingual language processing in East Asian languages and <b>connectionist</b> <b>modeling.</b> [...] Li received a B.A. in Chinese linguistics from Peking University in 1983, an M.A. in theoretical linguistics from Peking University, a Ph.D. in psycholinguistics from Leiden University and the Max Planck Institute for Psycholinguistics in 1990, and completed post-doctoral fellowships at the Center for Research in Language at the University of California, San Diego and the McDonnell-Pew Center for Research in Cognitive Neuroscience in 1992. Li has been employed at the Chinese University of Hong Kong (1992-1996), the University of Richmond (1996-2006), and Pennsylvania State University (2008-present), {{and he has}} {{also served as a}} Visiting Associate Professor at Hong Kong University (2002-2003), an Adjunct Professor at the State Key Laboratory for Cognitive Neuroscience and Learning at Beijing Normal University (2000-present), as well as Program Director for the Perception, Action, and Cognition Program and the Cognitive Neuroscience Program at the National Science Foundation (2007-2009).|$|E
5000|$|In 1959, Bower {{was hired}} at the Stanford Psychology Department. Until the late 1960s, he {{continued}} the animal research {{he had begun}} as a graduate student, but when Bill Estes and Dick Atkinson joined the faculty, his focus shifted to mathematical models of memory. One model they produced explained [...] "hypothesis testing behavior of subjects learning very simple classifications (concepts) in the standard trial-by-trial procedures that overtaxed memory."After wearying of studying models of memory, Bower shifted his focus to study short-term memory. He worked {{on a team that}} created both the time-decay queuing model and the fixed-space displacement model to describe how items in short term memory might be lost before they could be encoded in long-term memory. This spawned into research into how organizational devices could expand the capacity of short term memory past the traditional 7 items. A particular mnemonic device that Bower researched that is still popular today is chunking, in which a person groups objects together to improve memory.His works during this time also included the huge benefits of mnemonic aids and how these aids are often converted into visual images, human associative memory and propositional learning, state dependent memory, <b>connectionist</b> <b>modeling</b> for categorical learning, and how we remember narratives. In 1979 he was honored with the Award for Distinguished Scientific Contribution by the American Psychological Association.|$|E
40|$|<b>Connectionist</b> <b>modeling</b> (AKA {{neural network}} modeling, connectionism) is rapidly {{becoming}} a dominant descriptive and theoretical {{tool for the}} psycholinguist. Below is a brief introduction {{to some of the}} terms and concepts used in <b>connectionist</b> <b>modeling.</b> Connectionist models are no different than any other sorts of theories in cognitive science, they merely offer a new computational toolbox, or set of algorithmic constraints on models and theories of cognitive phenomena. In this paper I review many of the important components of connectionist models and introduce some of strengths, pitfalls and caveats that casual readers and serious modelers must be aware of...|$|E
40|$|Abstract–A <b>connectionist</b> <b>model</b> is {{proposed}} to predict milk yield in dairy cattle for an organized herd. Various network architectural parameters including several heuristic rules for model selection are experimentally investigated {{to determine the}} best model. The performance of proposed <b>connectionist</b> <b>model</b> is assessed in terms of 2 R. As a result, it is found that the <b>connectionist</b> <b>model</b> based on the heuristic rule that the number of hidden neurons being equivalent to the number of input neurons can potentially be used as an alternative technique to predict milk yield in dairy cattle. I...|$|R
40|$|Abstract: Success {{of market}} {{segmentation}} {{depends on the}} use of appropriate data analysis techniques. <b>Connectionist</b> <b>models</b> (artificial neural networks) constitute an alternative to well-known methods such as linear discriminant analysis or logistic regression. So-called backpropagation networks attain higher classification rates than logistic regression models in a pilot-study of a-priori segmentation. This may be caused by the capability of <b>connectionist</b> <b>models</b> for discovering nonlinear relationships between descriptors and segment memberships as well as interaction effects between descriptors. Special attention is drawn to specification of <b>connectionist</b> <b>models</b> and interpretation of results obtained from estimating their parameters. ...|$|R
40|$|In {{the first}} section of the article, we examine some recent criticisms of the connectionist enterprise: first, that <b>connectionist</b> <b>models</b> are {{fundamentally}} behaviorist in nature (and, therefore, non-cognitive), and second that <b>connectionist</b> <b>models</b> are fundamentally associationist in nature (and, therefore, cognitively weak). We argue that, for a limited class of <b>connectionist</b> <b>models</b> (feed-forward, pattern-associator models), the first criticism is unavoidable. With respect to the second criticism, we propose that <b>connectionist</b> <b>models</b> are fundamentally associationist but that this is appropriate for building models of human cognition. However, we do accept the point that there are cognitive capacities for which any purely associative model cannot provide a satisfactory account. The implication that we draw from is this is not that associationist models and mechanisms should be scrapped, but rather that they should be enhanced...|$|R
40|$|Representations of {{linguistic}} {{information and the}} neural substrates that underlie them are incredibly complex. This chapter illustrates how <b>connectionist</b> <b>modeling</b> has furthered our understanding of normal and impaired processing in three related domains – semantic memory, knowledge of grammatical class, and word reading – and how th...|$|E
40|$|Cognitive {{developmental}} disorders cannot he properly understood without due {{attention to}} the developmental process, and we commend the authors' simulations in this regard. We note the contribution of these simulations to the nascent field of <b>connectionist</b> <b>modeling</b> of developmental disorders and outline a Set of criteria for assessing individual models {{in the hope of}} furthering future modeling efforts...|$|E
40|$|In {{this paper}} {{the idea of}} using Lindenmayer systems (L-systems) to create connectionist models is introduced. L-systems with some {{extensions}} provide us with a method for creating connectionist models with a close analogy to the growing process of nature. The main advantages to use L-systems in <b>connectionist</b> <b>modeling</b> is its capability for controling growth of connectionist model. Starting from a small initial state, the connectionist model is grown using a simple set of production rules. Learning is part of growth, where it can be implemented as a modification of connection parameter values or as a creation or deletion of connections themselves. The learning is implemented {{in the same way as}} growth was using internal production rules. A small scale example to use L-systems for the XOR problem is given. 1 Introduction In connectionist models knowledge is in the connections [20, page 132]. This implies that the <b>connectionist</b> <b>modeling</b> is knowledge representation modeling. In this paper we [...] ...|$|E
2500|$|Proceedings of the 1993 <b>Connectionist</b> <b>Models</b> Summer School (co-author) ...|$|R
5000|$|... #Subtitle level 3: The <b>Connectionist</b> <b>Model</b> of Reading Development ...|$|R
40|$|This paper {{highlights}} {{the theory of}} common-sense knowledge in terms of representation and reasoning. A <b>connectionist</b> <b>model</b> is proposed for common-sense knowledge representation and reasoning. A generic fuzzy neuron is employed as a basic element for the <b>connectionist</b> <b>model.</b> The representation and reasoning ability of the model is described through examples...|$|R
40|$|We {{argue that}} recent work in <b>connectionist</b> <b>modeling,</b> in {{particular}} the parallel constraint satisfaction processes that are central to many of these models, has great importancefor understanding issues ofboth historical and current concernfor social psychologists. We first provide {{a brief description of}} <b>connectionist</b> <b>modeling,</b> with particular emphasis on parallel constraint satisfaction processes. Second, we examine the tremendous similarities between parallel constraint satisfaction processes and the Gestalt principles that were thefoundationfor much ofmodern social psychology. We propose that parallel constraint satisfaction processes provide a computational im-plementation of the principles of Gestalt psychology that were central to the work of such seminal social psychologists as Asch, Festinger, Heider, and Lewin. Third, we then describe how parallel constraint satisfaction processes have been applied to three areas that were key to the beginnings ofmodern social psychology and remain central today: impression formation and causal reasoning, cognitive consistency (balance and cognitive dissonance), and goal-directed behavior. We conclude by discussing implications of parallel constraint satisfaction principles for a number of broade...|$|E
40|$|The {{application}} of a <b>connectionist</b> <b>modeling</b> method known as competition-based spreading activation to a camera tracking task is described. The potential is explored for automation of control and planning applications using connectionist technology. The emphasis is on applications suitable {{for use in the}} NASA Space Station and in related space activities. The results are quite general and could be applicable to control systems in general...|$|E
40|$|Abstract. <b>Connectionist</b> <b>modeling</b> and {{neuroscience}} {{have little}} common ground or mutual influence. Despite impressive algorithms and analysis within connectionism and neural networks, {{there has been}} little influence on neuroscience, which remains primarily an empirical science. This chapter advocates two strategies to increase the interaction between neuroscience and neural networks: (1) focus on emergent properties in neural networks that are apparently “cognitive”, (2) take neuroimaging data seriously and develop neural models of dynamics in the both spatial and temporal dimensions...|$|E
40|$|The neural {{basis of}} {{structure}} in language Bridging {{the gap between}} symbolic and <b>connectionist</b> <b>models</b> of language processing Gideon BorensztajnThe neural basis of structure in language Bridging the gap between symbolic and <b>connectionist</b> <b>models</b> of language processingILLC Dissertation Series DS- 2011 - 11 For further information about ILLC-publications, please contac...|$|R
40|$|How have <b>connectionist</b> <b>models</b> {{informed}} {{the study of}} development? This paper considers three contributions from specific <b>models.</b> First, <b>connectionist</b> <b>models</b> have proven useful for exploring nonlinear dynamics and emergent properties, and their role in nonlinear developmental trajectories, critical periods and developmental disorders. Second, <b>connectionist</b> <b>models</b> have {{informed the}} study of the representations that lead to behavioral dissociations. Third, <b>connectionist</b> <b>models</b> have provided insight into neural mechanisms, and why different brain regions are specialized for different functions. Connectionist and dynamic systems approaches to development have differed, with connectionist approaches focused on learning processes and representations in cognitive tasks, and dynamic systems approaches focused on mathematical characterizations of physical elements of the system and their interactions with the environment. The two approaches also share much in common, such as their emphasis on continuous, nonlinear processes and their broad application to a range of behaviors...|$|R
50|$|In most <b>connectionist</b> <b>models,</b> {{networks}} {{change over}} time. A closely related and very common aspect of <b>connectionist</b> <b>models</b> is activation. At any time, a {{unit in the}} network has an activation, which is a numerical value intended to represent {{some aspect of the}} unit. For example, if the units in the model are neurons, the activation could represent the probability that the neuron would generate an action potential spike. Activation typically spreads to all the other units connected to it. Spreading activation is always a feature of neural network models, and it is very common in <b>connectionist</b> <b>models</b> used by cognitive psychologists.|$|R
40|$|Book synopsis: This book {{introduces}} {{a host of}} connectionist models of cognition and behavior. The major areas covered are high-level cognition, language, categorization and visual perception, and sensory and attentional processing. All of the articles cover unpublished research work. The key contribution of this book is that it focuses exclusively on the advances in <b>connectionist</b> <b>modeling</b> in psychology. The papers are relatively short, and were explicitly written to be accessible to both connectionist modelers and experimental psychologists...|$|E
40|$|Copyright © 1997 Cambridge University PressPlease {{see page}} 40 of PDF for this article. Glenberg's account falls short in several respects. Besides {{requiring}} clearer explication of basic concepts, his account fails {{to recognize the}} autonomous nature of perception. His account of what is remembered, and its description, is too static. His strictures against <b>connectionist</b> <b>modeling</b> might be overcome by combining the notions of psychological space and principled learning in an embodied and situated network. Douglas Vickers and Michael D. Le...|$|E
40|$|Abstract. This {{paper is}} a brief summary {{of some of our}} recent {{research}} on the processing of nominal classifiers in two Sinitic languages spoken in Taiwan, Mandarin and Taiwanese (Southern Min Chinese). Our research has used data from classifier systems to address two interface issues, namely the relation between grammar and extra-grammatical cognition and between grammar and the lexicon. We have been researching these questions with several different theoretical and methodological approaches: descriptive linguistics, experiments with adults, longitudinal records of child language acquisition, and <b>connectionist</b> <b>modeling...</b>|$|E
40|$|Computational {{models of}} reading have {{typically}} focused on monosyllabic words. However extending those models to polysyllabic word reading can uncover critical points of distinction between competing models. We present a <b>connectionist</b> <b>model</b> of stress assignment that learned to map orthography onto stress position for English disyllabic words. We compared {{the performance of}} the <b>connectionist</b> <b>model</b> to Rastle and Coltheart's [(2000). ] rule-based model of stress assignment for words and nonwords. The <b>connectionist</b> <b>model</b> performed well on predicting human performance in reading nonwords that both contained and did not contain affixes, whereas the Rastle and Coltheart model performed well only oil nonwords with affixes. The <b>connectionist</b> <b>model</b> provides an important first step to simulating all aspects of polysyllabic word reading, and indicates that a probabilistic approach to stress assignment can reflect human performance on stress assignment for both words and nonwords. (C) 2008 Elsevier Ltd. All rights reserved...|$|R
40|$|About the book: <b>Connectionist</b> <b>Models</b> of Learning, Development and Evolution {{comprises}} {{a selection}} of papers presented at the Sixth Neural Computation and Psychology Workshop - the only international workshop devoted to <b>connectionist</b> <b>models</b> of psychological phenomena. With a main theme of neural network modelling {{in the areas of}} evolution, learning, and development, the papers are organized into six sections: The neural basis of cognition Development and category learning. Implicit learning Social cognition Evolution Semantics Covering artificial intelligence, mathematics, psychology, neurobiology, and philosophy, it will be an invaluable reference work for researchers and students working on <b>connectionist</b> <b>modelling</b> in computer science and psychology, or in any area related to cognitive science...|$|R
40|$|This paper {{explores the}} {{question}} of whether <b>connectionist</b> <b>models</b> of cognition should be considered to be scientific theories of the cognitive domain. It is argued that in traditional scientific theories, there is a fairly close connection between the theoretical (unobservable) entities postulated and the empirical observations accounted for. In <b>connectionist</b> <b>models,</b> however, hundreds of theoretical terms are postulated [...] viz., nodes and connections [...] that are far removed from the observable phenomena. As a result, many of the features of any given <b>connectionist</b> <b>model</b> are relatively optional. This leads to {{the question of}} what, exactly, is learned about a cognitive domain <b>modelled</b> by a <b>connectionist</b> network...|$|R
40|$|Abstract: 2 ̆ 2 Connectionism is {{a method}} of {{modeling}} cognition as the interaction of neuron-like units. Connectionism has received a grea[t] deal of interest and may represent a paradigm shift for psychology. The nature of a paradigm shift (Kuhn, 1970) is reviewed with respect to connectionism. The reader is provided an overview on connectionism including: an introduction to <b>connectionist</b> <b>modeling,</b> new issues it emphasizes, a brief history, its developing sociopolitical impact, theoretical impact, and empirical impact. Cautions, concerns, and enthusiasm for connectionism are expressed. 2 ̆...|$|E
40|$|<b>Connectionist</b> <b>modeling</b> {{offers a}} useful {{computational}} framework {{for exploring the}} nature of normal and impaired cognitive processes. The current work extends the relevance of <b>connectionist</b> <b>modeling</b> in neuropsychology to address issues in cognitive rehabilitation: the degree and speed of recovery through retraining, {{the extent to which}} improvement on treated items generalizes to untreated items, and how treated items are selected to maximize this generalization. A network previously used to model impairments in mapping orthography to semantics is retrained after damage. The degree of relearning and generalization varies considerably for different lesion locations, and has interesting implications for understanding the nature and variability of recovery in patients. In a second simulation, retraining on words whose semantics are atypical of their category yields more generalization than retraining on more typical words, suggesting a counterintuitive strategy for selecting items in patient therapy to maximize recovery. In a final simulation, changes in the pattern of errors produced by the network over the course of recovery is used to constrain explanations of the nature of recovery of analogous braindamaged patients. Taken together, the findings demonstrate that the nature of relearning in damaged connectionist networks can make important contributions to a theory of rehabilitation in patients. © 1996 Academic Press, Inc...|$|E
40|$|This article {{considers}} how <b>connectionist</b> <b>modeling</b> can contrib-ute {{to understanding}} ofhuman cognition. I argue that connec-tionist networks {{should not be}} thought of as theories or simu-lations of theories, but may nevertheless cOlllribute to the developmelll of theories. It is no exaggeration to say that connectionism has exploded into prominence within cognitive science over the last several years. The enormous upsurge of interest is attested by conferences and symposia too numerous to list, new journals (e. g., Neural Networks, Neural Com-putation), special issues of existing journals (e. g., Cog...|$|E
40|$|The {{challenge}} for supervised neural net models of morpho-syntax {{has been to}} demonstrate that language learning that appears to entail a data base of rules and exceptions can be simulated {{without the need for}} these structures to be present. This article reviews <b>connectionist</b> <b>models</b> of morpho-syntax which have attempted to meet this challenge. The article begins with a background description of how <b>connectionist</b> <b>models</b> work and then proceeds to explain the way in which both static and sequential models of the acquisition of morpho-syntax have been developed. That static <b>connectionist</b> <b>models</b> have been able to simulate the development of both verbal and nominal morphology is discussed in the context of how these models question the dual mechanism model of morphological processing (Pinker, 1999). The role of sequential <b>connectionist</b> <b>models</b> in understanding of the treatment of plural nouns in compound words is considered in detail. Finally, how morpho-syntax might be learnt in considered in an overview of developmental models...|$|R
50|$|Marcus, G. F. (1997). Extracting higher-level {{relationships}} in <b>connectionist</b> <b>models.</b> Behavioral and Brain Sciences, 20, 77.|$|R
40|$|Local <b>connectionist</b> <b>models</b> {{are built}} up from {{networks}} of connected nodes. In these models, each node represents exactly one item to be represented. In language production models, these items are words, morphemes, phonemes, and so on. <b>Connectionist</b> <b>models</b> incorporate parallel processing. Thus, they are ideal models {{for both the}} production of slips of the tongue and aphasic speech. With respect to aphasia, the model incorporates specific impairments. However, <b>connectionist</b> <b>models</b> are not ideal {{for the production of}} sequences of linguistic units. In this article we first explain why sequentialisation is that problematic at all. Then, we present a solution for our model, which is based on control chains {{on the one hand and}} lateral inhibition on the other hand...|$|R
