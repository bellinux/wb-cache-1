5|90|Public
30|$|The {{algorithm}} {{works with}} a Boolean vector containing the PVG pick power correction coefficient (k 1) and battery nominal <b>capacity</b> <b>correction</b> coefficient (k 2). The algorithm uses N pop vectors (k 1, k 2).|$|E
40|$|International audienceThis letter {{presents}} a "before convergence" early stopping criterion for the LDPC decoder {{defined in the}} second generation of DVB standards. The idea is to stop the decoding process once the estimated number of remaining errors is below the maximum <b>capacity</b> <b>correction</b> of the outer BCH decoder used in the DVB-S 2, T 2 and C 2 standards. Simulations show that {{the average number of}} iterations is reduced by up to 26 % compared with classical early stopping criterion up to a frame error rate of 10 ^- 6...|$|E
40|$|Combined {{heat and}} power (CHP) plants enable {{simultaneous}} {{production of electricity}} and useful heat allowing for high total fuel efficiency. 70 % of all electricity produced in Denmark in 2012 was produced from plants classified as CHP plants. Because of the close power market connection between Norway and Denmark, a sufficient modelling of the Danish production portfolio is important to Statnett, the Norwegian TSO. CHP plants are very complex to model at a system level as they participate in both power and heat markets and exist with such technological diversity. The objective of this thesis was to uncover potential for improvements and to implement new modelling elements to the modelling of Danish CHP plants in the SINTEF developed EMPS power market model. The EMPS model does not explicitly model heat markets. Three areas {{were found to have}} potential for improvements: 1. 	The average annual production profiles: the existing production profiles were too volatile, seemingly random and lacked documentation 2. 	The aggregation of small CHP plants: The existing aggregation of small CHP not sufficiently diversified to account for technological diversity at a system level 3. 	Temperature dependent capacity: the CHP production was not temperature dependent apart from a general seasonal variability. It was assumed that CHP units operation can be modelled by a linear feasible operating region describing the relation between instant heat and power production. CHP utilities must meet the heat load at all times. Based on assumptions about the heat load as a function of outdoor temperature and historical temperature data, new annual production profiles relating to average temperature were created. A new method of aggregating small CHP plants was developed based on decentral DH utility statistics and a new way of determining their marginal cost. In addition, a function developed by SINTEF that corrects the CHP production capacity according to the actual temperature was implemented. The new modelling elements were largely based on a CHP operation strategy developed for this thesis. The new elements were implemented in steps to see the effect of each step. The implementations formed three new EMPS model datasets, in addition to the one for the pre-existing modelling. Each element was shown to have been implemented correctly and addressed the issues as intended. When comparing thermal production per week [GWh] from observed data with modelling results for the period 2001 - 2008 it was shown to be a trend that modelled thermal production follows the observed thermal production in general for all datasets. This is largely due to general, seasonal variations in available back pressure capacity at a low MC. However, {{the degree to which the}} data fitted with this trend varied amongst the model datasets. The new modelling elements proved to be incremental improvements with regards to following the observed thermal production from week to week. The pre-existing modelling performed worst and the new dataset with all three new modelling elements, NewModTC, performed best, with regards to matching observed thermal electricity generation. Introduction of the forced aggregated small CHP production was the most effective new modelling element to increase the R 2 to indicate a better fit with the overall trend that modelled thermal production followed the observed. A comparison between observed data and results from the existing modelling showed that thermal electricity production in general was much more temperature dependent and less price dependent in reality compared to the model. The new modelling elements showed incremental improvements to the overall modelling, as thermal production became more temperature dependent and less price dependent, i. e. approaching the trends of the observed data. However, the comparisons also showed that there remains some work to increase temperature dependency and decrease price dependency for the modelled thermal production further. The new model datasets resulted in more volatile prices in Denmark on average across all scenarios compared to the existing modelling. The increased temperature dependency was the main reason for this. Implementing new production profiles changed the available, low cost back pressure capacity, so that less was produced, compared to the existing modelling, during high load hours, increasing prices, and more was produced during low load hours, contributing to decreased prices. It is likely, but not shown here, that this was due to a new, flat distribution of CHP production capacity over the week s 168 hours. Overall, the new small CHP aggregation resulted in a moderate price reduction, as production was forced at zero MC, increasing production especially during the winter. The function for temperature dependent <b>capacity</b> <b>correction</b> showed to change the prices for certain hours significantly, but no overall increase or decrease for neither initially low nor high price hours. The prices changed mainly due to the function regulating available back pressure capacity down, increasing prices, or up, lowering prices for individual hours...|$|E
5000|$|With {{the advent}} of high {{throughput}} sequencing, the volume of sequence to be corrected exceeded human <b>capacity</b> for sequence <b>correction.</b>|$|R
50|$|It is also {{possible}} to design artistic QR codes without reducing the error <b>correction</b> <b>capacity</b> by manipulating the underlying mathematical constructs.|$|R
50|$|In his <b>capacity</b> as <b>corrections</b> secretary, Lynn was sued by {{an inmate}} who claimed {{invasion}} of privacy during a body search carried out on all convicts in the Louisiana State Penitentiary in Angola. Under orders from Lynn, David Keith Elliott submitted to a visual body cavity search. The searches were conducted in the general presence of other inmates, guards, and three bystanders. The U.S. District Court for the Middle District of Louisiana granted Lynn's motion for summary judgment and dismissed Elliott's suit.|$|R
40|$|The {{continuing}} {{growth in}} the importance of oil and gas production and processing overall the globe increase the need for accurate prediction of various parameters and their impact on unit operations, process simulation and design. Because of the particular nature of various parameters, sometimes existing methods encounter difficulties. Currently several models are available to predict various design parameters in the oil and gas processing industries. However, their calculations may require rigorous computer solutions. Therefore, developing the new predictive tools to which are easier than the existing methods, less complicated with fewer computations to minimize the complex and time-consuming calculation steps is an essential need. It is apparent that mathematically compact, simple, and reasonably accurate predictive tools, as proposed in this thesis, would be preferable for computationally intensive simulations. In fact, the development of engineering correlations by a modification to the well-known Vogel-Tammann-Fulcher (VTF) [1921 - 1926] equation and Arrhenius equation (1889) was the primary motivation of the present thesis, which, nevertheless, yielded predictive tools with accuracy comparable to that of the existing rigorous simulations. Hence, some existing approaches lead to complicated equations for the purposes of engineering importance. This problem has been circumvented conveniently by resorting to simpler approaches, as described in this thesis. The purpose of the proposed Dissertation work is to develop and formulate accurate and reliable predictive tools to serve two purposes. First, being conversion of a set of highly correlated variables to a set of independent variables by using linear transformations. Second one is for variable reductions. When a dependent variable is specified, the method is very efficient for dimensional reduction due to the supervised nature of its methodology. The developed tools in this study can be immense engineering value to predict different process design parameters, including the prediction of hydrate forming conditions of natural gases, hydrate forming pressure of pure alkanes in the presence of inhibitors, water-hydrocarbon systems mutual solubilities, water content of natural gas, density, thermal conductivity and viscosity of aqueous glycol solutions, optimum size of inlet scrubber and contactor in natural gas dehydration systems, estimation of water-adsorption isotherms, estimation of equilibrium water dew point of natural gas in triethylene glycol dehydration systems, true vapour pressure (TVP) of LPG and natural gasoline, hydrocarbon components solubilities in hydrate inhibitors, methanol vaporization loss and solubility in hydrocarbon liquid phase for gas hydrate inhibition, storage pressure of gasoline in uninsulated tanks, emissivity of combustion gases, filling losses from storage containers,bulk modulus and volumetric expansion coefficient of water for leak tightness test of pipelines, silica solubility and carry-over in steam, carbon dioxide equilibrium adsorption isotherms, estimation of packed column size, estimation of thermal insulation thickness, transport properties of carbon dioxide, aqueous solubility of light hydrocarbons, estimation of economic thermal insulation thickness, water content of air at elevated pressures, surface tension of paraffin hydrocarbons, aqueous solubility and density of carbon dioxide, aqueous solubility of light hydrocarbons, thermal conductivity of hydrocarbons, downcomer design velocity and vapour <b>capacity</b> <b>correction</b> factor in fractionators, estimation of convection heat transfer coefficients and efficiencies for finned tubular sections, estimation of heat losses from process piping and equipment surfaces, prediction of absorption/stripping factors, correlating theoretical stages and operating reflux in fractionators, design of radiant and convective sections of direct fired heaters and many other engineering parameters. Following the development of predictive tools, experimental work was undertaken to measure the density and viscosity, of ethylene glycol + water, diethylene glycol + water, and triethylene glycol + water mixtures at temperatures ranging from 290 K to 440 K and concentrations ranging from 20 mol % glycol to 100 mol % glycol. Our data were correlated using a novel Arrhenius-type equation based predictive tool and a thermodynamical method (the generalized corresponding states principle (GCSP)). Both novel Arrhenius-type equation based predictive tool and GCSP method, with two adjustable parameters for each property, offer the potential for judicious extrapolation of density and viscosity data for all glycol + water mixtures. In addition, in this thesis, the PreTOG software package has been developed, which covers a wide range of parameters in oil, gas and chemical processing industries and is using PC-based Windows and Matlab graphical user interfaces and tool boxes. The PreTOG software is also available on an stand-alone CD. Finally the following typical case studies for potential benefits to various processing plants industries will be presented and the results of new proposed model are compared with partial least squares (PLS) and principal component analysis (PCA) : •Methanol vaporization loss during gas hydrate inhibition •Methanol loss in condensate liquid phase during gas hydrate inhibition •Estimation of potential savings from reducing unburned combustible losses in coal-fired systems •Recoverable heat from blowdown systems during steam generation •Energy conservation benefits in excess air controlled gas-fired systems •Prediction of salinity of salty crude oi...|$|E
40|$|Combined {{heat and}} power (&# 147;CHP&# 148;) plants enable {{simultaneous}} {{production of electricity}} and useful heat allowing for high total fuel efficiency. 70 % of all electricity produced in Denmark in 2012 was produced from plants classified as CHP plants. Because of the close power market connection between Norway and Denmark, a sufficient modelling of the Danish production portfolio is important to Statnett, the Norwegian TSO. CHP plants are very complex to model at a system level as they participate in both power and heat markets and exist with such technological diversity. The objective of this thesis was to uncover potential for improvements and to implement new modelling elements to the modelling of Danish CHP plants in the SINTEF developed EMPS power market model. The EMPS model does not explicitly model heat markets. Three areas {{were found to have}} potential for improvements: 1. 	The average annual production profiles: the existing production profiles were too volatile, seemingly random and lacked documentation 2. 	The aggregation of small CHP plants: The existing aggregation of small CHP not sufficiently diversified to account for technological diversity at a system level 3. 	Temperature dependent capacity: the CHP production was not temperature dependent apart from a general seasonal variability. It was assumed that CHP units&# 146; operation can be modelled by a linear feasible operating region describing the relation between instant heat and power production. CHP utilities must meet the heat load at all times. Based on assumptions about the heat load as a function of outdoor temperature and historical temperature data, new annual production profiles relating to average temperature were created. A new method of aggregating small CHP plants was developed based on decentral DH utility statistics and a new way of determining their marginal cost. In addition, a function developed by SINTEF that corrects the CHP production capacity according to the actual temperature was implemented. The new modelling elements were largely based on a CHP operation strategy developed for this thesis. The new elements were implemented in steps to see the effect of each step. The implementations formed three new EMPS model datasets, in addition to the one for the pre-existing modelling. Each element was shown to have been implemented correctly and addressed the issues as intended. When comparing thermal production per week [GWh] from observed data with modelling results for the period 2001 - 2008 it was shown to be a trend that modelled thermal production follows the observed thermal production in general for all datasets. This is largely due to general, seasonal variations in available back pressure capacity at a low MC. However, {{the degree to which the}} data fitted with this trend varied amongst the model datasets. The new modelling elements proved to be incremental improvements with regards to following the observed thermal production from week to week. The pre-existing modelling performed worst and the new dataset with all three new modelling elements, NewModTC, performed best, with regards to matching observed thermal electricity generation. Introduction of the forced aggregated small CHP production was the most effective new modelling element to increase the R 2 to indicate a better fit with the overall trend that modelled thermal production followed the observed. A comparison between observed data and results from the existing modelling showed that thermal electricity production in general was much more temperature dependent and less price dependent in reality compared to the model. The new modelling elements showed incremental improvements to the overall modelling, as thermal production became more temperature dependent and less price dependent, i. e. approaching the trends of the observed data. However, the comparisons also showed that there remains some work to increase temperature dependency and decrease price dependency for the modelled thermal production further. The new model datasets resulted in more volatile prices in Denmark on average across all scenarios compared to the existing modelling. The increased temperature dependency was the main reason for this. Implementing new production profiles changed the available, low cost back pressure capacity, so that less was produced, compared to the existing modelling, during high load hours, increasing prices, and more was produced during low load hours, contributing to decreased prices. It is likely, but not shown here, that this was due to a new, flat distribution of CHP production capacity over the week&# 146;s 168 hours. Overall, the new small CHP aggregation resulted in a moderate price reduction, as production was forced at zero MC, increasing production especially during the winter. The function for temperature dependent <b>capacity</b> <b>correction</b> showed to change the prices for certain hours significantly, but no overall increase or decrease for neither initially low nor high price hours. The prices changed mainly due to the function regulating available back pressure capacity down, increasing prices, or up, lowering prices for individual hours. </p...|$|E
50|$|Because of {{the high}} error <b>correction</b> <b>capacity</b> {{compared}} to code rate and form of parity-check matrix (which is usually hardly distinguishable from a random binary matrix of full rank), the binary Goppa codes are used in several post-quantum cryptosystems, notably McEliece cryptosystem and Niederreiter cryptosystem.|$|R
3000|$|... is {{the minimal}} Hamming {{distance}} between two distinct codewords and, since we restrict ourself to linear codes, {{it is the}} minimum weight of a nonzero codeword. The minimum distance {{is closely related to}} the error <b>correction</b> <b>capacity</b> of the code; a code of minimal distance [...]...|$|R
40|$|Unary coding {{has found}} {{applications}} in data compression, neural network training, and {{in explaining the}} production mechanism of birdsong. Unary coding is redundant; therefore it should have inherent error <b>correction</b> <b>capacity.</b> An expression for the error correction capability of unary coding for the correction of single errors has been derived in this paper. Comment: 7 pages, 5 figure...|$|R
40|$|This paper {{examines}} QR Codes and {{how they}} can be composed and scan and decode by a camera. QR code is 2 -dimensional barcode used for quick response in promotional and marketing purpose. The paper describes about QR code, how QR code is different from barcode, It’s formation, <b>Capacity</b> and Error <b>correction</b> code. It’s application in India and worldwide...|$|R
40|$|The {{problem of}} {{increasing}} {{the speed of the}} compensator reactive power and proposed technical solutions for improving compensation in the transition process on the basis of high-frequency control quantity <b>correction</b> <b>capacity.</b> Developed and theoretically justified method of reactive power compensation with variable in time loads. Established Matlabmodel of the compensator and considered its main modes of operation...|$|R
40|$|Abstract- This paper {{examines}} QR Codes and {{how they}} can be composed and scan and decode by a camera. QR code is 2 -dimensional barcode used for quick response in promotional and marketing purpose. The paper describes about QR code, how QR code is different from barcode, It’s formation, <b>Capacity</b> and Error <b>correction</b> code. It’s application in India and worldwide...|$|R
40|$|Most {{geotechnical}} {{design codes}} and books use the equations of Meyerhof or Terzaghi to calculate shallow foundations. These equations {{are based on}} the failure mechanism published by Prandtl for shallow strip foundations. The common idea is that failure of a footing occurs in all cases according to a Prandtl-wedge failure mechanism. To check the failure mechanism and the equations of the currently used bearing <b>capacity</b> factors and <b>correction</b> factors, a large number of ﬁnite-element calculations of strip and circular footings have been made. The ﬁnite-element calculations show that in cases of soils with high friction angles, soils without cohesion or a surcharge, footings with inclined loading or circular footings, not the Prandtl-wedge failure mechanism, but other failure mechanisms occur. In addition, the currently used equations for the bearing <b>capacity</b> factors and <b>correction</b> factors are too high. Therefore, new equations have been presented in this article. For some correction factors, for example, the inclination factors and the cohesion slope factor, an analytical solution is found...|$|R
30|$|On {{the other}} hand, Table 6 shows that for QPSK modulation, the {{minimum number of}} {{required}} TBICM-ID-SSD iterations yLimfor all code rates and for identical required arithmetic operations as 6 IDec is yLim= 4.2 with z= 0. So using y= 3 < 4.2 iterations will lead to less arithmetic complexity, meanwhile {{it has the same}} error <b>correction</b> <b>capacity</b> as illustrated in Figure 2 for (1).|$|R
50|$|During the DVB-T testing period, Indonesian {{government}} (via its Ministry of Information & Communication Technology ICT) {{wanted to}} switch to DVB-T2 technology which provides better signal efficiency, <b>capacity</b> and <b>corrections</b> compared to DVB-T. The TV broadcasters still testing their DVB-T broadcasts agreed to join the DVB-T2 conversion program offered by the government since they saw the significant benefits by switching to DVB-T2 (such as higher data rate for HD content and better carrier-to-noise ratio management), even though it would introduce additional cost {{for those who have}} bought DVB-T equipment. The official switch to DVB-T2 from DVB-T was started February 2012, based on ICT Minister decree (about 5 years from DVB-T introduction and adopting/nurturing period in Indonesia).|$|R
2500|$|Capacity of block {{interleaver}}: For an [...] block interleaver {{and burst}} of length [...] {{the upper limit}} on number of errors is [...] This is obvious {{from the fact that}} we are reading the output column wise and the number of rows is [...] By the theorem above for error <b>correction</b> <b>capacity</b> up to [...] the maximum burst length allowed is [...] For burst length of , the decoder may fail.|$|R
40|$|Reversible adduct {{formation}} in the reaction of Cl((sup 2) P(sub J)) with CS 2 has been observed over the temperature range 193 - 258 K by use of time-resolved resonance fluorescence spectroscopy to follow the decay of pulsed-laser-generated Cl((sup 2) P(sub J)) into equilbrium with CS 2 Cl. Rate coefficients for CS 2 Cl formation and decomposition have been determined {{as a function of}} temperature and pressure; hence, the equilbrium constant has been determined as a function of temperature. A second-law analysis of the temperature dependence of Kp and heat <b>capacity</b> <b>corrections</b> calculated with use of an assumed CS 2 Cl structure yields the following thermodynamic parameters for the association reaction: Delta-H(sub 298) = - 10. 5 +/- 0. 5 kcal/mol, Delta-H(sub 0) = - 9. 5 +/- 0. 7 kcal/mol, Delta-S(sub 298) = - 26. 8 +/- 2. 4 cal/mol. deg., and Delta-H(sub f, 298) (CS 2 Cl) = 46. 4 +/- 0. 6 kcal/mol. The resonance fluorescence detection scheme has been adapted to allow detection of Cl((sup 2) P(sub J)) in the presence of large concentrations of O 2, thus allowing the CS 2 Cl + Cl + O 2 reaction to be investigated. We find that the rate coefficient for CS 2 Cl + O 2 reaction via all channels that do not generate Cl((sup 2) P(sub J)) is less than 2. 5 x 10 (exp- 16) cu cm/(molecule. s) at 293 K and 300 -Torr total pressure and that the total rate coefficient is less than 2 x 10 (exp - 15) cu cm/(molecule. s) at 230 K and 30 -Torr total pressure. Evidence for reversible adduct {{formation in}} the reaction of Cl((sup 2) P(sub J)) with COS was sought but not observed, even at temperatures as low as 194 K...|$|R
40|$|Arce, GonzaloThis work {{introduces}} {{a method to}} improve the aesthetic appearance of QR codes subject to bounded probability of detection error. The resultant QR code image embedding is compatible with standard decoding applications for mobile devices available in the market. This method {{takes advantage of the}} immunity of QR readers against local luminance disturbances and intends to render the colors of a logo {{in such a way that}} the luminance in the dark regions of the codes is detected as dark and the light regions as light. This approach exploits the rich theory and practice of digital halftoning for pixel selection to optimally fuse images into QR codes. The technique minimizes the dual criteria of visibility and decodability. A tractable model for the probability of error is developed. The robustness of the method is adjustable by changing a set of parameters in order to handle variations in the tone given by poor illumination or other factors. The error <b>correction</b> <b>capacity</b> is given by the <b>correction</b> <b>capacity</b> of the QR code. Experimental results are presented evidencing considerable visual quality improvement. University of Delaware, Department of Electrical and Computer EngineeringM. S...|$|R
40|$|For {{communication}} systems with heavy burst noise, an optimal Forward Error Correction (FEC) scheme {{is expected to}} have a large burst error <b>correction</b> <b>capacity</b> while simultaneously owning moderate random error correction capability. This letter presents a new FEC scheme based on multiple-symbol interleaved Reed-Solomon codes and an associated two-pass decoding algorithm. It is shown that the proposed multi-symbol interleaved coding scheme can achieve nearly twice as much as the burst error correction capability of conventional symbol-interleaved Reed-Solomon codes with the same code length and code rate...|$|R
40|$|This paper {{presents}} {{an analysis of}} a two-level decoupled Hamming network, which is a high performance discrete-time/discrete-state associative memory model. The two-level Hamming memory generalizes the Hamming memory by providing for local Hamming distance computations in the first level and a voting mechanism in the second level. In this paper, we study the effect of system dimension, window size, and noise on the <b>capacity</b> and error <b>correction</b> capability of the two-level Hamming memory. Simulation results are given for both random images and human face images. 1...|$|R
40|$|For {{the problem}} of low code gain of the {{concatenation}} of Reed Solomon (RS) codes and convolutional codes (CC), the concatenated scheme of RS+CC and Luby Transform (LT) codes with an improved decoding algorithm is proposed. The improved decoding algorithm for LT codes decreases the receiving buffer consumption and improves the successful decoding probability and the real-time property of LT codes at some low computational cost. Simulations and analyses are made to evaluate {{the performance of the}} concatenated scheme in terms of the error <b>correction</b> <b>capacity</b> and cost...|$|R
30|$|The {{proposed}} scheme {{can easily be}} extended to general multi-source multi-relay networks. In this case, the selected relays might forward a number of symbols which {{is less than half}} of the codeword length. The major challenge in this case is how to select best (multiple) relays for network coding and partial relaying. A promising application of PARC is to design a cooperation scheme to support multiple sources with different error <b>correction</b> <b>capacities</b> to achieve a given target BER. This problem can be solved by carefully designing how many symbols of each source should be relayed depending on the corresponding channel code’s strength.|$|R
40|$|This {{contribution}} reports standard gas-phase enthalpies of formation (ΔfH° 298), entropies (S° 298), {{and heat}} capacities (Cp°(T)) for all plausible 64 bromochlorophenols (BCPs) at the M 062 X meta hybrid level using a polarized basis set of 6 - 311 +G(d,p). Isodesmic work reactions served {{to calculate the}} standard enthalpies of formation for all bromochlorophenol molecules and several bromochlorophenoxy radicals. Standard entropies and heat <b>capacities</b> comprise <b>correction</b> terms due {{to the treatment of}} O-H bonds as hindered rotors. Values of the bond dissociation enthalpies (BDHs) of O-H bonds, calculated for a selected series of bromochlorophenols, vary slightly with the change in the pattern and degree of halogenation of the phenyl ring. A thermodynamic cycle facilitated the estimation of pKa values, based on the calculated solvation and gas-phase deprotonation energies. We estimated the solvation energies of 19 out of 64 BCPs and their respective anions based on the integral equation formalism polarizable continuum model using optimized structures in the aqueous phase. Values of pKa decrease significantly from around 9 for monohalogenated to around 3 for pentahalogenated phenols...|$|R
40|$|Hybrid memories, {{structured}} from CMOS and non-CMOS devices, {{are potential}} candidates to replace existing memories {{due to their}} ultrascale integration. However, they are prone to high degree of nonpermanent faults. In this paper a fault tolerance architecture for hybrid memories with higher reliability requirements is proposed. Non-permanent faults that can occur both in the data CMOS-encoder/decoder {{as well as in}} non-CMOS memory cell array are detected and corrected; hence significantly improving the reliability. The architecture is mainly based on a combination of two different fault tolerance schemes: (a) Redundant Residue Number System (RRNS) error correcting codes to allow multiple-bit error correction, and (b) scrubbing to enhance the error <b>correction</b> <b>capacity...</b>|$|R
40|$|Synchronous code-division multiple-access (CDMA) {{communication}} with randomly chosen spherical spreading sequences and <b>capacity</b> achieving error <b>correction</b> coding is analyzed. Emphasis {{is put on}} the penalties to be paid by applying single user coding in conjuction with decorrelation as pre-equalization. In contrast to previous work on this topic, the analysis also extends to re-encoded decision feedback structures where an equal rate and and an equal power case are distinguished. The results are nonasymptotic {{in the number of}} users and show that fluctuations of the signal-to-noise ratio due to the random nature of the sequences has no deleterious effect onto capacity for the spherical random sequence model...|$|R
5000|$|Locally decodable {{codes are}} {{especially}} useful for data transmission over noisy channels. The Hadamard code (a special case of Reed Muller codes) {{was used in}} 1971 by Mariner 9 to transmit pictures of Mars back to Earth. It was chosen over a 5-repeat code (where each bit is repeated 5 times) because, for roughly {{the same number of}} bits transmitted per pixel, it had a higher <b>capacity</b> for error <b>correction.</b> (The Hadamard code falls under the general umbrella of forward error correction, and just happens to be locally decodable; the actual algorithm used to decode the transmission from Mars was a generic error-correction scheme.) ...|$|R
40|$|Objective: {{to examine}} the <b>capacities</b> of {{pharmacological}} <b>correction</b> of impairments in oxygen-transporting systems and metabolic processes with perfluorane and cytoflavin in critically ill patients with acute intoxication with neurotropic poisons. Subjects and methods. Metabolic sequels of severe hypoxia, free radical processes, and endogenous intoxications were studied in 62 patients with the severest acute intoxication with neurotropic poisons. Results. The studies have established that hypoxia and metabolic changes {{lead to the development}} of endotoxicosis. Intensifying endotoxicosis in turn enhances hypoxic lesion. Thus, the major task of intensive care is to restore oxygen delivery and to diminish metabolic disturbances and endotoxicosis. Ways of correcting hypoxia and metabolic disturbances are considered in the severe forms of acute poisoning.  </p...|$|R
40|$|We give {{analytic}} upper bounds to {{the channel}} capacity C for transmission of classical information in electromagnetic channels (bosonic channels with thermal noise). In the practically relevant regimes of high noise and low transmissivity, {{by comparison with}} know lower bounds on C, our inequalities determine {{the value of the}} <b>capacity</b> up to <b>corrections</b> which are irrelevant for all practical purposes. Examples of such channels are radio communication, infrared or visible-wavelength free space channels. We also provide bounds to active channels that include amplification. Comment: 6 pages, 3 figures. NB: the capacity bounds are constructed by generalizing to the multi-mode case the minimum-output entropy bounds of arXiv:quant-ph/ 0404005 [Phys. Rev. A 70, 032315 (2004) ...|$|R
40|$|OBJECTIVE: To {{examine the}} {{sagittal}} curves of patients treated with CD instrumentation using exclusively pedicle screws. METHODS: Image analysis of medical records of 27 patients (26 M and 1 F) {{with a minimum}} follow-up of 6 months, who underwent surgical treatment in our service between January 2005 and December 2010. The curves were evaluated on coronal and sagittal planes, {{taking into account the}} potential correction of the technique. RESULTS: In the coronal plan the following curves were evaluated: proximal thoracic (TPx), main thoracic (TPp), and thoracolumbar; lumbar (TL, L), and the average flexibility was 52 %, 52 %, and 92 % and the <b>capacity</b> of <b>correction</b> was 51 %, 72 %, and 64 %, respectively. In the sagittal plane there was a mean increase in thoracic kyphosis (CT) of 41 % and an average reduction of lumbar lordosis (LL) of 17 %. Correlation analysis between variables showed Pearson coefficient of correlation of 0. 053 and analysis of dispersion of R 2 = < 0. 001. CONCLUSION: The method has shown satisfactory results with maintenance of kyphosis correction in patients with normal and hyper kyphotic deformities...|$|R
40|$|Accurate {{measurement}} of the free metal ion is difficult, especially for trace metals present in very small concentrations (less than micromolar) in natural systems. The recently developed Donnan membrane technique can measure the concentrations in solution {{in the presence of}} inorganic and organic complexing agents. We have developed this method further to make it applicable for analysing soil samples. The major development is the linkage of a soil column with the Donnan cell. The operational aspects of the method, including equilibrium time, buffering <b>capacity</b> and <b>correction</b> for differences in ionic strength, were investigated and optimized. The method was applied to determine concentrations of free Cu 2, Cd 2, Pb 2 and Zn 2 in the soil solution of 15 soil samples (pH 2. 97. 1, organic C < 2. 9137. 4 g kg 1, clay 0. 251. 6 Ž Compared with other speciation methods, the Donnan membrane technique has the advantage of allowing the {{measurement of}} several elements simultaneously and it minimizes the disturbance of substrate. The detection limit of the technique is about 109 m. This method can be applied to study the biogeochemical behaviour of metals in soils, sediments and other solid materials...|$|R
40|$|Abstract — In this paper, {{we present}} new results on network error {{correction}} with unequal link capacities. We consider network error correction codes that can correct arbitrary errors occurring {{on up to}} z links. We find the capacity of a two-node network with multiple feedback links and show how feedback links {{can be used to}} increase the error <b>correction</b> <b>capacity.</b> We propose a new cut-set upper bound for general acyclic networks, and show its tightness for a family of four-node acyclic networks when each backward link has enough capacity. For a more general family of zig-zag networks, we present conditions under which our upper bound is tight. Finally, we propose an approach for highprobability network error correction with a causal adversary. I...|$|R
40|$|We {{show that}} solving a multiple-unicast network coding {{problem can be}} reduced to solving a single-unicast network error {{correction}} problem, where an adversary may jam at most a single edge in the network. Specifically, we present an efficient reduction that maps a multiple-unicast network coding instance to a network error correction instance while preserving feasibility. The reduction holds for both the zero probability of error model and the vanishing probability of error model. Previous reductions are restricted to the zero-error case. As an application of the reduction, we present a constructive example showing that the single-unicast network error <b>correction</b> <b>capacity</b> may not be achievable, a result of separate interest. Comment: ISIT 2015. arXiv admin note: text overlap with arXiv: 1410. 190...|$|R
40|$|We {{investigate}} {{the effects of}} a modified dispersion relation proposed by Majhi and Vagenas on the Reissner-Nordstrom black hole thermodynamics in a universe with large extra dimensions. It is shown that entropy, temperature and heat <b>capacity</b> receive new <b>corrections</b> and charged black holes in this framework have less degrees of freedom and decay faster compared to black holes in the Hawking picture. We also study the emission rate of black hole and compare our results with other quantum gravity approaches. In this regard, the existence of the logarithmic prefactor and the relation between dimensions and charge are discussed. This procedure is not only valid for a single horizon spacetime but it is also valid for the spacetimes with inner and outer horizons. Comment: 16 pages, 10 figure...|$|R
40|$|Abstract—This paper {{investigates the}} {{behavior}} of the noisy Min-Sum decoder over binary symmetric channels. A noisy decoder is a decoder running on a noisy device, which may introduce errors during the decoding process. We show that in some particular cases, the noise introduce by the device can help the Min-Sum decoder to escape from fixed points attractors, and may actually result in an increased <b>correction</b> <b>capacity</b> with respect to the noiseless decoder. We also reveal the existence of a specific threshold phenomenon, referred to as functional threshold. The behavior of the noisy decoder is demonstrated in the asymptotic limit of the code-length, by using “noisy ” density evolution equations, and it is also verified in the finite-length case by Monte-Carlo simulation. I...|$|R
