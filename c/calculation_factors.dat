10|995|Public
50|$|This {{method is}} used solely for binary systems. The {{mass of the}} binary system {{is assumed to be}} twice that of the Sun. Kepler's Laws are then applied and the {{separation}} between the stars is determined. Once this distance is found, the distance away can be found via the arc subtended in the sky, providing a temporary distance measurement. From this measurement and the apparent magnitudes of both stars, the luminosities can be found, and by using the mass-luminosity relationship, the masses of each star. These masses are used to re-calculate the separation distance, and the process is repeated a number of times, with accuracies as high as 5% being achieved. A more sophisticated <b>calculation</b> <b>factors</b> in a star's loss of mass over time.|$|E
5000|$|The mass/luminosity {{relation}} {{is important}} because {{it can be used to}} find the distance to binary systems which are too far for normal parallax measurements, using a technique called [...] "dynamical parallax". In this technique, the masses of the two stars in a binary system are estimated, usually as being the mass of the Sun. Then, using Kepler's laws of celestial mechanics, the distance between the stars is calculated. Once this distance is found, the distance away can be found via the arc subtended in the sky, giving a preliminary distance measurement. From this measurement and the apparent magnitudes of both stars, the luminosities can be found, and by using the mass-luminosity relationship, the masses of each star. These masses are used to re-calculate the separation distance, and the process is repeated. The process is iterated many times, and accuracies as high as 5% can be achieved. The mass/luminosity relationship can also be used to determine the lifetime of stars by noting that lifetime is approximately proportional to M/L. One finds that more massive stars live shorter. A more sophisticated <b>calculation</b> <b>factors</b> in a star's loss of mass over time.|$|E
40|$|The thesis {{deals with}} {{sources of the}} head loss in pipe and options of its <b>calculation.</b> <b>Factors</b> {{influencing}} the head loss and possibilities of decreasing pressure drop in pipe are also discussed in the thesis. The result of thesis contains comparison of calculation methods of head loss with calculation using Darcy-Weisbach equation...|$|E
30|$|SE {{represents}} {{the power of}} sensor nodes to transmit data. This is the opportunistic <b>calculation</b> <b>factor</b> for cloud platform.|$|R
30|$|There {{are roughly}} four steps in DDBSE, i.e., global smooth, {{threshold}} process, entropy <b>calculation,</b> and weighting <b>factor</b> <b>calculation.</b>|$|R
40|$|A causal {{relationship}} among key critical success factors of successful entrepreneurs in Thailand is proposed. A new business innovation management model is established using data sampling of 250 successful entrepreneurs from various industry sectors in Thailand. The {{data were analyzed}} by performing mean, standard deviation <b>calculations,</b> <b>factor</b> analysis and causal factors affected by LISREL program. Quantitative {{results showed that the}} model is consistent with empirical data...|$|R
40|$|This study {{aimed to}} {{identify}} a set of stable radiomic parameters in CT perfusion (CTP) maps with respect to CTP <b>calculation</b> <b>factors</b> and image discretization, as an input for future prognostic models for local tumor response to chemo-radiotherapy. Pre-treatment CTP images of eleven patients with oropharyngeal carcinoma and eleven patients with {{non-small cell lung cancer}} (NSCLC) were analyzed. 315 radiomic parameters were studied per perfusion map (blood volume, blood flow and mean transit time). Radiomics robustness was investigated regarding the potentially standardizable (image discretization method, Hounsfield unit (HU) threshold, voxel size and temporal resolution) and non-standardizable (artery contouring and noise threshold) perfusion <b>calculation</b> <b>factors</b> using the intraclass correlation (ICC). To gain added value for our model radiomic parameters correlated with tumor volume, a well-known predictive factor for local tumor response to chemo-radiotherapy, were excluded from the analysis. The remaining stable radiomic parameters were grouped according to inter-parameter Spearman correlations and for each group the parameter with the highest ICC was included in the final set. The acceptance level was 0. 9 and 0. 7 for the ICC and correlation, respectively. The image discretization method using fixed number of bins or fixed intervals gave a similar number of stable radiomic parameters (around 40 %). The potentially standardizable factors introduced more variability into radiomic parameters than the non-standardizable ones with 56 - 98 % and 43 - 58 % instability rates, respectively. The highest variability was observed for voxel size (instability rate[*][*]> 97 % for both patient cohorts). Without standardization of CTP <b>calculation</b> <b>factors</b> none of the studied radiomic parameters were stable. After standardization with respect to non-standardizable factors ten radiomic parameters were stable for both patient cohorts after correction for inter-parameter correlations. Voxel size, image discretization, HU threshold and temporal resolution have to be standardized to build a reliable predictive model based on CTP radiomics analysis...|$|E
40|$|This article {{presents}} {{the method of}} IT risk assessment from human behaviour perspective, developed by the author. It is an alternative for the commonly used approaches to risk assessment, based on vulnerability and threat identification and the probability estimation of their occurrence. The authors method applies to risk <b>calculation</b> <b>factors</b> such as administrators or users skills, attackers knowledge and determination, or attack method used. The key element of the proposed risk analysis competitive method is a mathematical formula which allows for risk level quantification...|$|E
40|$|Fatigue is {{the common}} failure mode for nuclear {{mechanical}} components during operation which could induce crack initiation and growth. In order to predict the behaviour of fatigue crack propagating, fatigue calculation is introduced. A new method of crack fatigue growth calculation has been {{presented in this paper}} according to French RSE-M Rules. Regarding this method, detail analysis on key formulas selecting, essential <b>calculation</b> <b>factors</b> of occurrences combination, K-value calculation and mechanic modelling, characterization interval option has been performed. One fatigue crack growth calculation example of dissimilar weld of Reactor Pressure Vessel outlet nozzle to the safe end has been taken and the result demonstrates a good expectation...|$|E
2500|$|When {{the errors}} are uncorrelated, it is {{convenient}} {{to simplify the}} <b>calculations</b> to <b>factor</b> the weight matrix as [...]|$|R
40|$|The {{recently}} proposed Nonrandom Two-Liquid Segment Activity Coefficient model (NRTL-SAC) of Chen and Song (2004) {{provides a}} simple and thermodynamically consistent framework to correlate and predict drug solubility in pure solvents and mixed solvents, based on a small initial set of measured solubility data. Used within a process simulator, or through an Excel spreadsheet, the model forms the scientific foundation of an effective solubility modeling tool in support of early stage crystallization process development. The methodology is also applicable to other unit operations where phase equilibrium <b>calculations</b> <b>factor</b> prominently in process design and development...|$|R
5000|$|The LEED 2009 {{documentation}} {{is based}} upon the daylight <b>factor</b> <b>calculation.</b> The daylight <b>factor</b> <b>calculation</b> is based on uniform overcast skies. It is most applicable in Northern Europe and parts of North America. Daylight factor is “the ratio of the illuminance at a point on a plane, generally the horizontal work plane, produced by the luminous flux received directly or indirectly at that point from a sky whose luminance distribution is known, to the illuminance on a horizontal plane produced by an unobstructed hemisphere of this same sky." ...|$|R
40|$|Technological {{advances}} in medical imaging have prompted accelerator manufactures {{to produce more}} and more advanced treatment delivery systems capable to precise shape the dole distribution. For several years radiation beams have been modulated by mechanical and dynamic wedges, compensators, individual shields and unequal beam weights. Nowadays three dimensional conformal treatment and intensity modulated radiation therapy techniques (IMRT) provide very precise conformation of the dose to the target volume while sparing adjacent healthy tissues. On the other hand conformal radiotherapy requires very precise definition of anatomical structures, improved patient repositioning systems. New challenge represents quality control program and treatment verification. IMRT was introduced in clinical practice in Center of Oncology, Glivvice in year 2000. Such treatment is delivered by Clinac 2300 with dynamic MLC option, {{on the base of}} dose distributions calculated by CadPlan-Helios treatment planning system, and sent via Varis to accelerator. The aim {{of this paper is to}} present our experience with IMRT technique with particular regard to IMRT treatment plan (definition of PTV and <b>calculation</b> <b>factors</b> like Termination Tolerance, Priority Factor, Scatter Factor) ...|$|E
40|$|Ever {{since the}} {{publication}} of the Fourth Assessment Report by the Intergovernmental Panel on Climate Change, the economy raised its awareness of the alarming climate change and is now faced with the question which contribution it can make to climate protection. For logistics companies, this paper provides an answer by explaining climate footprints as the essential basis for emission reduction and describing an approach for drawing them up. Corporate climate footprints comprise a statement of the shares that single business units take in the whole greenhouse gas (GHG) emission of a company. Thus they allow to identify strong emission drivers and to serve {{as a starting point for}} setting mitigation aims. Drawing up the climate footprint regularly, it will show if the taken mitigation measures have the desired impact. Drawing up a corporate climate footprint for all modes of transport, TU Dortmund University and the Öko-Institut - Institute for Applied Ecology are calculating the complete GHG emissions according to a distance based approach, which is provided by the GHG Protocol. However, the approach does not provide a strict guideline for the calculation. For this reason the distance based approach is improved by constituting adequate emission factors and the transport performance of road, rail, sea and air transports as <b>calculation</b> <b>factors.</b> To accurately determine the latter factor, the approach prescribes the usage of track and trace data and of transport-mode-dependent rules. For road freight transport the approach regards the traffic mode of every shipment, e. g. direct or break-bulk traffic. This enables to figure out the real-driven pickup and delivery routing of break bulk. For sea and air freight transports, the approach considers for instance that loading of ships does not correspond to their full capacity use and that detours are made due to round-trips of ships or base airports of airlines. By the use of this improved approach, a more precise and reasonable calculation of GHG emissions caused by transports can be achieved. Being applicable independent of transport and traffic mode, the improved approach further veers toward standardization...|$|E
40|$|The {{manner in}} which {{resources}} are allocated, the generation of cash through present resources and the allocation of new liquidities derive from a company's cost-benefit analysis, {{which is part of}} management control. The modern financial theory changes the company management objective of maximizing profit with the objective of maximizing its value. The traditional return measures are considered to be insufficient to express the economic reality. Traditional cost-benefit indicators exclude opportunity costs, effects of inflation and risks. The financial experts claim firm value maximization as the main objective of a company’s management. The emergence of modern return measures derived from firm value maximization reflects the changes in the economic environment, their emergence creating a dispute over the most appropriate approach regarding value creation. The fact that the data required for their calculation is taken directly from accounts makes them sensitive to accounting distortions. The emergence of modern cost-benefit indicators derived from value creation provides new perspectives on the return. Firm value can grow by generating a higher level of cash flow, by reducing financing costs and by extending the growth period. The value created can be measured by using both modern indicators derived from the theory of value creation and discounted cash flow methods. The value created can be calculated by using discounted cash flow models, which, moreover, are very complicated and take into account a lot of variables. The alternative to these methods is represented by modern cost-benefit indicators that have a more simple calculation methodology, and the forecast of <b>calculation</b> <b>factors</b> is narrower and easier to accomplish. In this article, we will present the connection between discounted cash flow methods and the indicators derived from value creation, based on the business finance theory, which says that firm value will increase if projects with positive net present value are accepted, while it will be destroyed if projects with negative net present value are accepted...|$|E
5000|$|When {{the errors}} are uncorrelated, it is {{convenient}} {{to simplify the}} <b>calculations</b> to <b>factor</b> the weight matrix as [...]The normal equations can then be writtenin the same form as ordinary least squares: ...|$|R
25|$|In his approach, {{the power}} output by muscles during {{exercise}} is continuously adjusted {{in regard to}} calculations made by the brain in regard to a safe level of exertion. These neural <b>calculations</b> <b>factor</b> in earlier experience with strenuous exercise, the planning duration of the exercise, and the present metabolic state of the body. These brain models ensure that body homeostasis is protected, and an emergency reserve margin is maintained. This neural control adjusts the number of activated skeletal muscle motor units, a control which is subjectively experienced as fatigue. This process, though occurring in the brain, is outside conscious control.|$|R
40|$|This report {{describes}} {{the use of}} adaptive integration for the <b>calculation</b> of view <b>factors</b> between simple convex polygons with obstructions. The accuracy of the view <b>factor</b> <b>calculation</b> is controlled by a convergence factor. The adaptive integration method is compared with two other common methods implemented in a modern computer program and found to have significant advantages i...|$|R
40|$|Extended {{abstract}} 1 - Introduction Issue {{of sustainable}} development and especially sustainable urban development {{has been one of}} the most important concerns of policy makers and planners since the Rio Conference. This point of view was given form environmental, economic, managerial and social comments that have collected from all fields of university studies about cities. With assuming “compact city form” as the stable urban form and also regarding to prevent suburbs spread of Tehran metropolis subject entitled as “organizing and protecting the city’s boundary and preventing any spread of span area”, the various aspects of Tehran sustainability, according to the characteristics of compact city, is necessary to study. Issue of social stability in the compact city model has become a challenging subject among researchers and planners. Some of them believe that if cities become more dense then social stability decreases. But others argue that denser cities have more social stable positions. To sale building density as a one of the most important sources of income for Tehran Municipality became rampant during the decade of nineties. This change has had different effects to urban region. This has various impacts on increase of pollution, low standard of urban services, traffic problems and disturb of social coherence. So research and explore with literature review attentions in Tehran city with real data will be the best way for understanding of density's decrement affects. Clearly, compact city and sustainability are from western theoretical achievements and not related to Iranian cities. But using of these theories is the way to find Iranian theories about affect of densities to social sustainability. And it will show that how much different is between western and Iranian cities from point of views and theoretical achievements. 2 - MethodologyDue to the necessity and importance of social stability in Tehran metropolitan and also according to its planned vision, this study discussed about the social stability in various area of Tehran. In addition, social stability level in some regions with different population density are investigated and analyzed. This study is not pursuing to establish relationship between density and social stability. Also, factor analysis was used for defining of primary factors of the research and expert's comments and factors' load were used for naming these new factors. Library studies and statistics collects, building an analytic model and indexes, using of experts for finalizing analytic model, statistics analysis and mathematical <b>calculation,</b> <b>factors</b> naming by attention to factors' load and experts Opinions, Conclusion and analysis of results by attention to experts Opinions, is the steps of doing this research in order. 3 – Discussion This research showed that social sustainability contains, Development and access to facilities, Medical and health's share, Cultural development and Voluntary participation, in order. The Correlation shows that there is negative and significant relationship between "Development and access to facilities" and "Cultural development". Result of that correlation was about - 0. 881 which is intensive relationship. Clearly, it means that we will lose social sustainability. However, this result is not reliable reason to accept or reject of positive or negative correlation between density and social sustainability because analysis have done in one city where it has not had clear organized plans for compressed constructions. Perhaps if social, economic, technical and environmental prerequisites of Compression are provided in Iranian's cities, different results will be achievable. In Comparison between pictures of Tehran density and social stability, it is clear that central regions of Tehran with low densities rather have higher social stabilities. Also, Southern regions of Tehran which have higher densities have had low social sustainability. Therefore, kind of densities that created in Tehran regions of municipality not related to factors' increment of welfare and social. It means this Situation does not create sustainable areas. 4 – ConclusionHowever, results of this study indicated that increasing density at 20 districts of Tehran caused to decrease the social sustainability. Therefore, more attention must be paid to social aspects of development because with reduction of the social sustainability in societies, economic and environmental sustainability and finally in total city sustainability will be at risk. In addition, correlation between density and rate of University education is about - 0. 834 and correlation between density and ratio of uneducated women to uneducated men is about + 0. 439. These indices had the most important effects in decrement of social sustainability. Finally, in this situation, more compressed districts of Tehran have had less welfare and cultural sustainability. Although, results does not mean that increment of density always causes decrement of sustainability. But it is acceptable that in this position, Tehran statistics shows that model of development of Tehran in density not along with social sustainability. If we want to discus about factors {{of sustainable development}}, we have to prepare Prerequisite of this development. From other vision we can understand that history of urbanization and cultural attitudes is the most important reason for this result. If we want to find affect of something in their social position we have to know their feelings and needs about that change. Key words: Sustainable development, social sustainability, compact city, Tehran regions, Tehran metropolisReferences Azizi, Mohamadmehdi, (2002), Density in urban planning,University Of Tehran Press,TehranBurton, E. (2000). the potential of the compact city for promoting social equity Achieving sustainable urban form: Spon press. Burton, E., Williams, K., & Jenks, M. (2000). The Compact City and Urban Sustainability: Conflicts and Complexities Tha compact city: a sustainable urban form? (pp. 198 - 212) : Oxford Brookes University. Frey, Hildebrand‬, (1999), Designing the city: towards a more sustainable urban form,Hossein Bahreyni,sherkat pardazesh va barnamerizi sahri. Gharakhloo, Mehdi, (2006), indicators of urban sustainable development, Journal Of Geography and Regional Development, 8, 157 - 177. Goodchild, Barry, (1999), Housing Design, Urban Form and Sustainable Development, journal of urban management, 4, 50 - 60. Guy, S., & Marvin, S. (2000). Models and pathways: the diversity of sustainable urban futures Achieving Sustainable Urban Form (pp. 9 - 18). Hall, Peter Geoffrey, (2008), Urban future 21 : a global agenda for twenty-first century cities, safaee, nahid, Society of Iranian Consulting Engineers. hosseinzade dalir, karim, sasanpour, farzaneh, 2005,ecological footprint aproach in metropole sustainability, geoghraphy research journal, 21, 83 - 101. incorporate report of comprehensive plan of tehran. 2002,Tehran. Jafari,Ali, 20007,The introduction of appropriate indicators for evaluating sustainable urban development and its measurement,Environment and Development journal, 3, 49 - 55. Kalantari, Mohsen, (2000), Geographical evaluation of crime in areas of Tehran, Department of urban planning, university of Tehran. kooshyar, golrokh, sustainable development indicators, journal of management, 79, 32 - 37. Maclaren, V. (2004). Urban Sustainability Reporting The sustainable urban development reader (pp. 203 - 210) : Routledge publication. Masnavi, Mohamadreza, (2002), sustainable development and new paradigm in urban development, hournal of environmental science, 29, 89 - 104. Mohamadzade, hamdie, (1997), Introduction of sustainable development concept and role of urban planning, journal of Fine Arts, 1, 32 - 43. Navabakhsh, Mehrdad, (2008), Urban sustainable development, jameshanasan, Tehran. NSI, t. N. (2001). ENVIRONMENTAL SUSTAINABILITY INDICATORS IN URBAN AREAS: AN ITALIAN EXPERIENCE. Retrieved from www. unece. org/stats/documents/ 2001 / 10 /env/wp. 16. e. pdf. Oxford, I. f. (2007). Measuring Social Sustainability: Best Practice from Urban Renewal in the EU. Retrieved from www. brookes. ac. uk/schools/be/oisd/sustainable_communities. Population and Housing Census, (2005), Statistical Center of Iran, Tehran. pourmohammadi, MohamadReza, Ghorbani, Rasool, (2002), Dimensions and strategies of compression paradigm in urban areas, Human Sciences MODARES, 29, 85 - 107. Sarrafi mozaffar, (1999), what is sustainable city?,urbanmanagement journal, 4, 6 - 13. Seattle, S. (2004). indicator of sustainable community. www. sustainableseattle. org,Statistical Center of Iran, (2005),Statistical Yearbook of Tehran, 2005, IT organization of tehran Municipality, Tehran. Tabibian, Mohamad, (1998), Determining sustainability indicators and its effect on Environment, Journal of Environmental Studies, 24, 1 - 12. UNDSD UK, U. N. (2005). INDICATORS OF SUSTAINABLE DEVELOPMENT IN THE UK. Retrieved from [URL] U. N. (2005). Indicators of Sustainable Development - Review and Assessment. Retrieved from [URL] keramat, 2000, sustainable development and urban pllaners responsibility in 21 th century,Journal of human Science and literature department of university of tehran, 108, 160 - 365...|$|E
40|$|Moving spent {{nuclear fuel}} between {{facilities}} often {{requires the use of}} lead-shielded casks. Criticality safety that is based upon calculations requires experimental validation of the fuel matrix and lead cross section libraries. A series of critical experiments using a high-enriched uranium-aluminum fuel element with a variety of reflectors, including lead, has been identified. Twenty-one configurations were evaluated in this study. The fuel element was modelled for KENO V. a and MCNP 4 a using various cross section sets. The experiments addressed in this report can be used to validate lead-reflected <b>calculations.</b> <b>Factors</b> influencing calculated k{sub eff} which require further study include diameters of styrofoam inserts and homogenization...|$|R
5000|$|The {{approximate}} rate {{of these}} interactions is {{set by the}} number density of electrons and positrons, the averaged product of the cross section for interaction and the velocity of the particles. The number density [...] of the relativistic electrons and positrons depends on the cube of the temperature , so that [...] The product of the cross section and velocity for weak interactions for temperatures (energies) below W/Z boson masses (~100 GeV) is given approximately by , where [...] is Fermi's constant (as is standard in particle physics <b>calculations,</b> <b>factors</b> {{of the speed of}} light [...] are set equal to 1). Putting it all together, the rate of weak interactions [...] is ...|$|R
40|$|This project {{deals with}} simulation´s {{possibilities}} of air flow in electric machines. A {{finite element method}} is the final element technique. Aim of this project is conversion universal formula for <b>calculation</b> friction dissipation <b>factor.</b> For its location is necessary to create 27 simulations according to the 2. plan order. Usage programme MATLAB for processing results obtain requisite algorithm for <b>calculation</b> friction dissipation <b>factor...</b>|$|R
30|$|As {{discussed}} in Section  1, if a power system is sophisticated and has many buses exceeding fault current limits, the enumeration method is very time-consuming and inefficient. As a result, a sensitivity <b>factor</b> <b>calculation</b> {{is used to}} screen the better candidate locations of FCLs [18], and this paper presents a new sensitivity <b>factor</b> <b>calculation</b> method based on {{the rate of the}} fault current mitigation.|$|R
25|$|This {{requires}} 2k precomputation {{and storage}} {{to speed up}} the resulting <b>calculation</b> by a <b>factor</b> of k, a space-time tradeoff.|$|R
5000|$|This {{equation}} is calibrated for an altimeter, up to 36,090 feet (11,000 m). Outside that range, an error {{will be introduced}} which can be calculated differently for each different pressure sensor. These error <b>calculations</b> will <b>factor</b> in the error introduced by the change in temperature as we go up.|$|R
50|$|An order-of-magnitude {{estimate}} is prepared when {{little or no}} design information is available for the project. It is called order of magnitude because that may be all that can be determined at an early stage. In other words, perhaps we can only determine {{that it is of}} a 10,000,000 magnitude as opposed to a 1,000,000 magnitude. Various techniques are employed for these estimates, including experience and judgment, historical values and charts, rules of thumb, and simple mathematical <b>calculations.</b> <b>Factor</b> estimating {{is one of the more}} popular methods. This involves taking the known cost of a similar facility and factoring the cost for size, place, and time. Cost modeling is another common technique. In cost modeling the estimator models the various parameters of the facility and applies costs to the derived scope.|$|R
50|$|As {{provided}} by a more accurate optical <b>calculations,</b> the conversion <b>factor</b> is 278 rather than 289 as demonstrated by simplified considerations above.|$|R
50|$|The {{following}} {{shows the}} times, times after <b>factoring,</b> <b>factor</b> <b>calculation</b> and overall {{place of the}} Australian team for each of their events.|$|R
5000|$|... is {{the outcome}} of <b>calculations</b> {{involving}} other <b>factors</b> mentioned above. Entropy ranges from 1.04 to 1.2 {{depending on the type}} of software being developed.|$|R
40|$|Thesis(MBA) [...] Stellenbosch University, 2001. The {{objectives}} {{of this research}} were as follows: • To develop a knowledge management diagnostic tool {{that can be used}} to evaluate the status of knowledge management initiatives from both a technology and a people perspective, • To conduct a pilot study using a convenience sample of 35 students from the Afrikaans Modular Masters of Business Administration course at the University of Stellenbosch in order to test this diagnostic tool for measurement reliability and validity, and • To present and discuss the results of the pilot study. A knowledge management diagnostic tool based on Bukowitz and Williams (2000) Knowledge Management Diagnostic was developed. The diagnostic tool consisted of seven section scales, each evaluating a step in Bukowitz and Williams (2000) knowledge management process. Each section scale contained ten items and responses to items were scored using s seven-point Likert scale. Correlation <b>calculations,</b> <b>factor</b> analyses and item analyses were conducted for each scale in order to test for measurement reliability and validity. A comparison of means between respondent group variables was also conducted using the Kruskal-Wallis and Mann-Whitney tests. The Kruskal-Wallis and Mann-Whitney comparison of means revealed no significant differences in total scores on the knowledge management diagnostic tool between groups. However, the fact that a convenience sample rather than a random sample was used, and the sample size was limited (n= 35) indicates that these results may not be conclusive. Notwithstanding the small, non-random sample used in this pilot study, the results of the correlation <b>calculations,</b> <b>factor</b> analyses and item analyses indicated that the scales had satisfactory internal consistency reliability and an examination of the items that loaded against each factor indicated that the scales displayed face validity. In the light of these results, recommendations for further research using a larger, random sample were therefore presented. In addition, items were recommended for deletion in order to improve internal consistency reliabilit...|$|R
40|$|Carbon {{emission}} that emited by {{electric plant}} in Jawa timur could be estimated. Estimation {{was done by}} emission <b>factor</b> <b>calculation</b> first. Emission <b>factor</b> was multipled with electric consumption value to got emmison carbon value. The method used ACM 0002 that issued by CDM-PDD Version 02 IPCC. Data supporting are NVC, SFC and CEF. The result could be found that coal and natural gas have emmision there are 1, 00688434 Ton CO 2 /Mwh and 0, 392860328 Ton CO 2 /Mwh. Keywords: carbon emission, emission factor, and electric plan...|$|R
40|$|Abstract. This article aims {{to improve}} the {{accuracy}} of gas flow measurement of cone flowmeter and get a high-precision expansibility <b>factor</b> <b>calculation</b> model with wide applicable pressure range and different upstream side pressure. The experimental research {{was based on the}} DN 50 and DN 100 two sets of V-Cone flowmeter designed by Tianjin University. As the standard gas flow facility by means of critical flow venturi nozzles was insufficient supply capacity and poor pressure stability in positive pressure method, the experimental data obtained from standard gas flow facility by using negative pressure method was used to fit out the V-Cone flowmeter expansibility <b>factor</b> <b>calculation</b> model [...] TJU- 2012 model. The model was tested with the experimental data obtained from the standard gas flow facility and compared with other four cone flowmeter expansibility <b>factor</b> <b>calculation</b> models, its maximum relative error was better than 0. 9 % and square averaged root error was better than 0. 6 %, both of them are the least...|$|R
40|$|Ratio {{factors for}} {{extending}} part lactation milk yield to a mature equivalent (ME), {{twice a day}} (2 X) milking and 305 day basis were constructed for Red Chittagong Cattle (RCC) reared at Bangladesh Agricultural University, Mymensingh, Bangladesh during 2005 to 2009. A total of 732 monthly test-day (TD) data of 89 different lactations from 40 selected cows were considered for <b>calculation.</b> The <b>factors</b> for standardizing incomplete lactations to 305 -da...|$|R
5000|$|Cristian's {{algorithm}} {{relies on}} the existence of a time server. [...] The time server maintains its clock by using a radio clock or other accurate time source, then all other computers in the system stay synchronized with it. A time client will maintain its clock by making a procedure call to the time server. Variations of this algorithm make more precise time <b>calculations</b> by <b>factoring</b> in network radio propagation time.|$|R
25|$|Tim Noakes, {{based on}} an earlier idea by the 1922 Nobel Prize in Physiology or Medicine winner Archibald Hill has {{proposed}} {{the existence of a}} central governor. In this, the brain continuously adjusts the power output by muscles during exercise in regard to a safe level of exertion. These neural <b>calculations</b> <b>factor</b> in prior length of strenuous exercise, the planned duration of further exertion, and the present metabolic state of the body. This adjusts the number of activated skeletal muscle motor units, and is subjectively experienced as fatigue and exhaustion. The idea of a central governor rejects the earlier idea that fatigue is only caused by mechanical failure of the exercising muscles ("peripheral fatigue"). Instead, the brain models the metabolic limits of the body to ensure that whole body homeostasis is protected, in particular that the heart is guarded from hypoxia, and an emergency reserve is always maintained. The idea of the central governor has been questioned since ‘physiological catastrophes’ can and do occur suggesting athletes (such as Dorando Pietri, Jim Peters and Gabriela Andersen-Schiess) can over-ride the ‘‘central governor’.|$|R
40|$|In {{the present}} study, {{the effect of}} gantry {{orientation}} on the photoneutron and capture gamma dose calculations for maze entrance door was evaluated. A typical radiation therapy room made of ordinary concrete was simulated using MCNPX Monte Carlo code. Gantry rotation was simulated at eight different angles around the isocenter. Both neutron and capture gamma dose vary considerably with gantry angle. The ratios of the maximum to the minimum values for neutron and capture gamma dose equivalents were 1. 9 and 1. 4, respectively. On the other hand, comparison of the Monte Carlo calculated mean value over all orientations with Monte Carlo calculated neutron and gamma dose showed that the Wu-McGinley method differed by 5 % and 2 %, respectively. However, for more conservative shielding <b>calculations,</b> <b>factors</b> of 1. 6 and 1. 3 should {{be applied to the}} calculated neutron and capture gamma doses at downward irradiation. Finally, it can be concluded that the gantry angle influences neutron and capture gamma dose at the maze entrance door and it should be taken into account in shielding considerations...|$|R
