19|411|Public
50|$|System/38 {{was one of}} the few {{commercial}} {{computers with}} capability-based addressing. (The earlier Plessey 250 {{was one of the}} few other computers with <b>capability</b> <b>architecture</b> ever sold commercially). Capability-based addressing was removed in the follow-on AS/400 and iSeries models.|$|E
50|$|The Flex Computer System was {{developed}} by Michael Foster and Ian Currie of Royal Signals and Radar Establishment (RSRE) in Malvern, England, during the 1980s. It used a tagged storage scheme to implement a <b>capability</b> <b>architecture,</b> and was designed for the safe and efficient implementation of strongly typed procedures.|$|E
5000|$|The {{application}} of CDF as an assessment methodology {{to measure the}} [...] "size of person" [...] {{in terms of their}} work capability and capacity provides a way forward for talent management systems to match the [...] "size of person" [...] to the [...] "size of role". Progressively more complex roles require progressively higher levels of social-emotional development and cognitive development in the role-holder. In this way requisite organizations can align their human <b>capability</b> <b>architecture</b> with their managerial accountability architecture and design [...] "growth assignments" [...] that facilitate the development of capability for more complex roles.|$|E
5000|$|Have Robust Network Management <b>capabilities</b> (Microsoft <b>architecture)</b> ...|$|R
40|$|This Article is {{a direct}} {{response}} to one written by Mike Rosen titled, “Are <b>Capabilities</b> <b>Architecture?</b> ” [1] published in February 2013 by BPTrends It offers a different and contrasting {{point of view on}} accepting capability modeling and mapping as “architecture. ” Business Architecture (BA) approaches and methods, while still evolving, have at least reached...|$|R
40|$|Capability-Based Computer Systems {{focuses on}} {{computer}} programs and their capabilities. The text first elaborates capability- and object-based system concepts, including capability-based systems, object-based approach, and summary. The book then describes early descriptor architectures and explains the Burroughs B 5000, Rice University Computer, and Basic Language Machine. The text also focuses on early <b>capability</b> <b>architectures.</b> Dennis and Van Horn's Supervisor; CAL-TSS System; MIT PDP- 1 Timesharing System; and Chicago Magic Number Machine are discussed. The book then describes Plessey System 2...|$|R
50|$|Using the {{semiconductor}} technology of its day, Intel's engineers weren't able {{to translate the}} design into a very efficient first implementation. Along {{with the lack of}} optimization in a premature Ada compiler, this contributed to rather slow but expensive computer systems, performing typical benchmarks at roughly 1/4 the speed of the new 80286 chip at the same clock frequency (in early 1982). This initial performance gap to the rather low-profile and low-priced 8086 line was probably the main reason why Intel's plan to replace the latter (later known as x86) with the iAPX 432 failed. Although engineers saw ways to improve a next generation design, the iAPX 432 <b>Capability</b> <b>architecture</b> had now started to be regarded more as an implementation overhead rather than as the simplifying support it was intended to be.|$|E
5000|$|The BiiN effort {{eventually}} failed, due {{to market}} forces, and the 960MX was {{left without a}} use. Myers attempted to save the design by outlining several subsets of the full <b>capability</b> <b>architecture</b> created for the BiiN system. He tried to convince Intel management to market the i960 (then still known as the [...] "P7") as a general-purpose processor, both {{in place of the}} Intel 80286 and i386 (which taped-out the same month as the first i960), as well as the emerging RISC market for Unix systems, including a pitch to Steve Jobs for use in the NeXT system. Competition within and outside of Intel came not only from the i386 camp, but also from the i860 processor, yet another RISC processor design emerging within Intel at the time. Myers was unsuccessful at convincing Intel management to support the i960 as a general-purpose or Unix processor, but the chip found a ready market in early high-performance 32-bit embedded systems.|$|E
50|$|Some of the {{innovative}} {{features of the}} iAPX 432 were detrimental to good performance. In many cases, the iAPX 432 had a significantly slower instruction throughput than conventional microprocessors of the era, such as the National Semiconductor 32016, Motorola 68010 and Intel 80286. One {{problem was that the}} two-chip implementation of the GDP limited it to the speed of the motherboard's electrical wiring. A larger issue was the <b>capability</b> <b>architecture</b> needed large associative caches to run efficiently, but the chips had no room left for that. The instruction set also used bit-aligned variable-length instructions instead of the usual semi-fixed byte or word-aligned formats used in the majority of computer designs. Instruction decoding was therefore more complex than in other designs. Although this did not hamper performance in itself, it used additional transistors (mainly for a large barrel shifter) in a design that was already lacking space and transistors for caches, wider buses and other performance oriented features. In addition, the BIU was designed to support fault-tolerant systems, and in doing so up to 40% of the bus time was held up in wait states.|$|E
50|$|The commonalities between {{messaging}} systems (in {{terms of}} <b>capabilities</b> and <b>architecture)</b> have been captuted in a platform-independent fashion as enterprise integration patterns (a.k.a. messaging patterns).|$|R
50|$|The CINO is {{responsible}} for managing the innovation process inside the organization that identifies strategies, business opportunities and new technologies and then develops new <b>capabilities</b> and <b>architectures</b> with partners, new business models and new industry structures to serve those opportunities.|$|R
40|$|Forwarding {{pointers}} allow {{safe and}} efficient data migration. However, they also {{introduce a new}} source of aliasing and as a result can have a serious impact on program performance. In this paper we introduce short quasi-unique ID's (squids), a simple hardware mechanism for <b>capability</b> <b>architectures</b> that mitigates the problems associated with aliasing. In the common case, squids can be used to prove that there is no aliasing and therefore avoid any overhead. The probability of having to perform expensive dereferencing operations to check for aliasing when comparing pointers to different objects is exponentially small in the number of bits used to implement squids. Benchmark programs on a simulated architecture show that squids can, in extreme cases, reduce execution time by more than a factor of four...|$|R
40|$|Contents {{include the}} following: Identify {{the need for}} a robust {{communications}} and navigation architecture for the success of exploration and science missions. Describe an approach for specifying architecture alternatives and analyzing them. Establish a top level architecture based on a network of networks. Identify key enabling technologies. Synthesize <b>capability,</b> <b>architecture</b> and technology into an initial capability roadmap...|$|E
40|$|A recent {{class of}} DDoS {{defenses}} based on network capabilities advocate fundamental {{changes to the}} Internet. However, despite the many point solutions, {{there has not been}} a rigorous study of the entire solution space for capability architectures. We believe these changes will inevitably introduce engineering tradeoffs in effectiveness and deployability. To understand the tradeoffs, and identify challenges, we build a taxonomy to categorize possible options and map the potential solution space. Our taxonomy identifies key components of a <b>capability</b> <b>architecture,</b> separates implementation from fundamentals, and opens up directions to explore for building a capabilities enabled Internet...|$|E
40|$|The {{contributors}} to this volume offer an original approach to debates about indigenous knowledge. Concentrating {{on the political}} economy of knowledge construction and dissemination, they look at the variety of ways in which development policies are received and constructed to explain how local knowledges are appropriated and recast, either by local elites or by development agencies. Until now, debates about indigenous knowledge have largely been conducted in terms of agricultural and environmental issues such as bio-piracy and gene patenting. The book opens up the theoretical debate to include areas such as post-war traumatic stress counselling, representations of nuclear <b>capability,</b> <b>architecture,</b> mining, and the politics of eco-touris...|$|E
50|$|DRAM: {{supported}} up to 6GB of GDDR5 DRAM memory {{thanks to}} the 64-bit addressing <b>capability</b> (see Memory <b>Architecture</b> section).|$|R
50|$|Marlin {{was created}} with {{specific}} design goals. First, Marlin allows consumer devices to import content from multiple independent services and accommodate peer-to-peer interactions. Second, Marlin {{is based on}} a general-purpose rights management architecture. Marlin specifications define the <b>capabilities</b> and <b>architecture</b> so that devices and services can interoperate.|$|R
40|$|International {{audience}} Personal {{cloud storage}} services are data-intensive applications already producing a significant share of Internet traffic. Several solutions offered by different companies attract {{more and more}} people. However, {{little is known about}} each service <b>capabilities,</b> <b>architecture</b> and  most of all  performance implications of design choices. This paper presents a methodology to study cloud storage services. We apply our methodology to compare 5 popular offers, revealing different system <b>architectures</b> and <b>capabilities.</b> The implications on performance of different designs are assessed executing a series of benchmarks. Our results show no clear winner, with all services suffering from some limitations or having potential for improvement. In some scenarios, the upload of the same file set can take seven times more, wasting twice as much capacity. Our methodology and results are useful thus as both benchmark and guideline for system design. </p...|$|R
40|$|Abstract. Face {{detection}} is {{an essential}} first step towards many advanced computer vision, biometrics recognition and multimedia applications, such as face tracking, face recognition, and video surveillance. In this paper, we proposed an FPGA hardware design with NoC (Network-on-Chip) architecture based on an AdaBoost face detection algorithm. The AdaBoost-based method is the state-of-the-art face detection algorithm in terms of speed and detection rates and the NoC provides high communication <b>capability</b> <b>architecture.</b> This design is verified on a Xilinx Virtex-II Pro FPGA platform. Simulation results show the improvement in speed 40 frames per second compared to software implementation. The NoC architecture provides scalability so that our proposed face detection method can be sped up by adding multiple classifier modules. Keywords: Face detection, Hardware Architecture, Network-on-Chip. ...|$|E
40|$|Capabilities {{define a}} uniform {{semantics}} for system service invocation, enforce separation of concerns and encapsulation, and allow each {{program to be}} restricted to exactly that set of authority it requires (the principle of least privilege). Capability systems therefore readily contain and reduce errors at the application level and improve component testability. If carefully architected, a capability system should be both faster and simpler than a comparable access-control-based system. In practice, implementations have failed to demonstrate such performance. This paper provides an architectural overview of EROS, the Extremely Reliable Operating System. EROS is a persistent capability system which provides complete accountability for persistent, consumable and multiplexed resources. By choosing abstractions to leverage conventional hardware protecgion, and exploiting hardware support in the implementation, a fast pure <b>capability</b> <b>architecture</b> can be demonstrated. This paper de [...] ...|$|E
40|$|EROS, the Extremely Reliable Operating System, {{addresses}} {{the issues of}} reliability and security by combining three ideas from earlier systems: capabilities and a persistent single-level store. Capabilities unify object naming with access control. Persistence extends this naming and access control uniformly across the memory hierarchy; main memory is viewed simply as a cache of the single-level store. The combination simplifies application design, allows programs to observe the "principle of least privilege," and enables active objects to be constructed securely. EROS is built on a simple object model that is easy to implement, understand, and reason about. These objects define the state of processes, memory segments, and user data {{in terms of the}} <b>capability</b> <b>architecture</b> and the store. The kernel implements structures that cache this state in machine-specific form. This design model reduces kernel size and complexity, limits the scope of software errors, and facilitates their detection [...] ...|$|E
40|$|Capability {{systems can}} be used to {{implement}} higher-level security policies including the *-property if a mechanism exists to ensure confinement. The implementation can be efficient if the "weak" access restriction described in this paper is introduced. In the course of developing EROS, a pure capability system, it became clear that verifying the correctness of the confinement mechanism was necessary in establishing the security of the operating system. This paper presents a verification of the EROS con nement mechanism with respect to a broad class of <b>capability</b> <b>architectures</b> (including EROS). We give a formal statement of the requirements, construct a model of the architecture's security policy and operational semantics, and show that architectures covered by this model enforce the confinement requirements if a small number of initial static checks on the con ned subsystem are satisfied. The method used generalizes to any capability system...|$|R
40|$|An {{increasing}} number of manufacturing businesses are moving towards global operations, with a manufacturing presence {{in most of the}} world's major regions. The use of information technology and business systems as a strategic weapon {{can be seen as a}} basis for achieving this end. The capabilities which organisations need to acquire in this situation include flexibility and innovation. Flexibility implies the ability to change swiftly and effectively, while innovation means the ability to renew and update products quickly. Global computer-integrated manufacture (CIM) can provide some of the above capabilities. Rather than relying upon theoretically-oriented, discipline-based and hierarchically-controlled staff, increasing emphasis is being placed on the organisation and integration of multi-skilled groups centred around particular projects. New information and co-ordination technologies, the globalisation of markets, post-Keynesian economic policies and decentralising forms of organisation are amongst the factors, it is suggested, are associated with this development. This paper describes the combination of business strategy and information technology within a manufacturing company required to support the goal of globalisation. The company described in this paper is involved in the manufacture of large engineering structures which are exported to many locations around the world. For many years, most of the company’s facilities and management had been based at one location in the United Kingdom. Following a take-over by a large American Corporation, the need to relocate heavy manufacturing operations at will to offset rising costs was recognised. The company has subsequently under¬taken a widespread business analysis exercise and is currently in the process of defining a range of global strategies, associated implemen¬tation methodologies and supporting systems. This paper identifies the issues that challenge organisations as they strive to achieve competitive advantage in today’s uncertain global markets. Conceptual solutions are offered in the forms of a strategic framework and strategic <b>capabilities</b> <b>architecture.</b> The strategic framework details all the organisation’s global business strategies and methodologies, and the strategic <b>capabilities</b> <b>architecture</b> acts as an implementation tool to enable these strategies...|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimitedDepartment of the Navy system acquisition {{begins with a}} statement of user need. Delivery of required capability depends heavily on the effective translation of user need to system requirements. Failure typically results in program cost overruns, schedule slippage, and sometimes partial or complete failure to deliver needed <b>capability.</b> <b>Architectures</b> as part of systems engineering were created {{to cope with the}} growing complexity of modern systems. The Navy develops and operates some of the most complex systems in the world. Yet, architecture development, while mandated, remains largely ancillary to the systems engineering process. As a result, much of the engineering advantage of architectures remains untapped. This study examined U. S. Navy policy, process, and current engineering and architectures standards and identified recommendations to improve the process of translating user needs to system requirements while facilitating the use of architectures. Naval Sea Systems Command author (civilian) ...|$|R
40|$|In a {{very short}} time, the {{interest}} about Wireless Sensors Networks(WSN) and their applications has grown both within the academic community and in the industry. At the same time, {{the complexity of the}} envisaged WSN-based systems has grown from a handful of homogeneous sensors to hundreds or thousands of devices, possibly differing in terms of <b>capability,</b> <b>architecture,</b> and operating system. Recently, some deployments of WSNs have been suggested in the literature both addressing the energy-aware and the adaptability to environmental changes issues; in both cases limitations arise which either prevent a long lifetime or/and the Quality of Service(QoS) of the envisaged applications. Energy is one of the scarcest resources in WSNs; energy harvesting technologies are thus required to design credible autonomous sensor networks. In addition to energy harvesting technologies, energy saving mechanisms play an important role to reduce energy consumption in sensor nodes. Moreover, in wireless sensor networks, the network topology may change over time due to permanent or transient node and communication faults, energy availability for the nodes (despite thepossible presence of energy harvesting mechanisms) and environmental changes (e. g., a landslide phenomenon, solar powerdensity made available to the nodes, presence of vegetation subject to seasonal dynamics). Development of smart routing algorithms is hence a must for granting effective communications in large scale wireless networks with adaptation ability combined with energy-aware aspects...|$|E
40|$|It is {{observed}} that most initiatives {{in the field}} of spatial (geospatial) information, applications and technologies are tackled without a holistic view of the enterprise and its organizational framework within which these initiatives are to be implemented, resulting in many difficulties. The solution to this problem is to utilize the routines that are common in enterprise architecture management in order to bring in the necessary level of transparency, manageability, and synergy across the complex environment of a corporation because architecting an enterprise is about understanding how things work together and how to make changes of architecture feasible and this is a prerequisite for every successful business change. For better understanding of the spatial capability one needs to realize what the architecture of the capability is and how it can be documented in order to communicate the architecture description clearly and allow one to establish a baseline for further work. The first step towards clarifying this is to describe the fundamental structure of how the capability is delivered and executed in the organization and thus the objective {{of this paper is to}} define a meta-model for the spatial <b>capability</b> <b>architecture.</b> Resources and method of meta-model development are documented as well as notation used to express the meta-model. Further work should address area of spatial capability development and definition of common spatial capabilities for reference purpose. The defined meta-model is a part of the framework which is intended to support spatial capability development...|$|E
40|$|Vulnerabilities in {{computer}} systems arise {{in part due}} to programmer's logical errors, and in part also due to programmer's false (i. e., over-optimistic) expectations about the guarantees that are given by the abstractions of a programming language. For the latter kind of vulnerabilities, architectures with hardware or instructionlevel support for protection mechanisms can be useful. One trend {{in computer}} systems protection is hardware-supported enforcement of security guarantees/policies. Capability-based machines are one instance of hardware-based protection mechanisms. CHERI is a recent implementation of a 64 -bit MIPS-based <b>capability</b> <b>architecture</b> with byte-granularity memory protection. The goal of this thesis is to provide a paper formal model of the CHERI architecture with the aim of formal reasoning about the security guarantees that can be offered by the features of CHERI. We first give simplified instruction operational semantics, then we prove that capabilities are unforgeable in our model. Second, we show that existing techniques for enforcing control-flow integrity can be adapted to the CHERI ISA. Third, we show that one notion of memory compartmentalization can be achieved with the help of CHERI's memory protection. We conclude by suggesting other security building blocks that would be helpful to reason about, and laying down a plan for potentially using this work for building a secure compiler, i. e., a compiler that preserves security properties. The outlook and motivation for this work is to highlight the potential of using CHERI as a target architecture for secure compilation...|$|E
40|$|Since {{the late}} 90 's IP telephony, {{commonly}} referred to as Voice over IP (VoIP), has been presented as a revolution on communications enabling the possibility to converge historically separated voice and data networks, reducing costs, and integrating voice, data and video on applications. This paper presents a study over the standard VoIP protocols H. 323, Session Initiation Protocol (SIP), Media Gateway Control Protocol (MGCP), and H. 248 /Megaco. Given the fact that H. 323 and SIP are more widespread than the others, we focus our study on them. For each of these protocols we describe and discuss its main <b>capabilities,</b> <b>architecture,</b> stack protocol, and characteristics. We also briefly point their technical limitations. Furthermore, we present the Advanced Multimedia System (AMS) project, a new system that aims to operate on Next Generation Networks (NGN) taking the advantage of its features, and it is viewed a...|$|R
50|$|The Security and Information Systems {{division}} covered {{network infrastructure}} and systems <b>architecture</b> <b>capabilities</b> for homeland protection and the complex urban environment {{as well as}} air and vessel traffic management.|$|R
40|$|This paper {{presents}} a system architecture for synthetic /natural hybrid coding toward future visual services. Scenedescription <b>capability,</b> terminal <b>architecture,</b> and network architecture are discussed by taking account of recent standardization activities: MPEG, VRML, ITU-T, and IETF. A consistent strategy to integrate scene-description capability and streaming technologies is demonstrated. Experimental results are also shown, in which synthetic/natural integration is successfully carried out...|$|R
40|$|In {{the current}} Internet, senders are not {{accountable}} for the packets they send. As a result, malicious users send unwanted traffic that wastes shared resources and degrades network performance. Stopping such attacks requires identifying the responsible principal and filtering any unwanted traffic it sends. However, senders can obscure their identity: a packet identifies its sender only by the source address, but the Internet Protocol does not enforce that this address be correct. Additionally, affected destinations {{have no way to}} prevent the sender from continuing to cause harm. An accountable network binds sender identities to packets they send for the purpose of holding senders responsible for their traffic. In this dissertation, I present an accountable network-level architecture that strongly binds senders to packets and gives receivers control over who can send traffic to them. Holding senders {{accountable for the}}ir actions would prevent many of the attacks that disrupt the Internet today. Previous work in attack prevention proposes methods of binding packets to senders, giving receivers control over who sends what to them, or both. However, they all require trusted elements on the forwarding path, to either assist in identifying the sender or to filter unwanted packets. These elements are often not {{under the control of the}} receiver and may become corrupt. This dissertation shows that the Internet architecture can be extended to allow receivers to block traffic from unwanted senders, even in the presence of malicious devices in the forwarding path. This dissertation validates this thesis with three contributions. The first contribution is DNA, a network architecture that strongly binds packets to their sender, allowing routers to reject unaccountable traffic and recipients to block traffic from unwanted senders. Unlike prior work, which trusts on-path devices to behave correctly, the only trusted component in DNA is an identity certification authority. All other entities may misbehave and are either blocked or evicted from the network. The second contribution is NeighborhoodWatch, a secure, distributed, scalable object store that is capable of withstanding misbehavior by its constituent nodes. DNA uses NeighborhoodWatch to store receiver-specific requests block individual senders. The third contribution is VanGuard, an accountable <b>capability</b> <b>architecture.</b> Capabilities are small, receiver-generated tokens that grant the sender permission to send traffic to receiver. Existing capability architectures are not accountable, assume a protected channel for obtaining capabilities, and allow on-path devices to steal capabilities. VanGuard builds a <b>capability</b> <b>architecture</b> on top of DNA, preventing capability theft and protecting the capability request channel by allowing receivers to block senders that flood the channel. Once a sender obtains capabilities, it no longer needs to sign traffic, thus allowing greater efficiency than DNA alone. The DNA architecture demonstrates that it is possible to create an accountable network architecture in which none of the devices on the forwarding path must be trusted. DNA holds senders responsible for their traffic by allowing receivers to block senders; to store this blocking state, DNA relies on the NeighborhoodWatch DHT. VanGuard extends DNA and reduces its overhead by incorporating capabilities, which gives destinations further control over the traffic that sources send to them...|$|E
40|$|On January 14, 2004, the President of the United States {{unveiled}} {{a new vision}} for robotic and human exploration of space entitled, "A Renewed Spirit of Discovery". As stated by the President in the Vision for Space Exploration (VSE), NASA must " [...] . implement a sustained and affordable human and robotic program to explore {{the solar system and}} beyond " and " [...] . develop new technologies and harness the moon's abundant resources to allow manned exploration of more challenging environments. " A key to fulfilling the goal of sustained and affordable human and robotic exploration will be the ability to use resources that are available at the site of exploration to "live off the land" instead of bringing everything from Earth, known as In-Situ Resource Utilization (ISRU). ISRU can significantly reduce the mass, cost, and risk of exploration through capabilities such as: mission consumable production (propellants, fuel cell reagents, life support consumables, and feedstock for manufacturing & construction); surface construction (radiation shields, landing pads, walls, habitats, etc.); manufacturing and repair with in-situ resources (spare parts, wires, trusses, integrated systems etc.); and space utilities and power from space resources. On January 27 th, 2004 the President's Commission on Implementation of U. S. Space Exploration Policy (Aldridge Committee) was created and its final report was released in June 2004. One of the report's recommendations was to establish special project teams to evaluate enabling technologies, of which "Planetary in situ resource utilization" was one of them. Based on the VSE and the commission's final report, NASA established fifteen Capability Roadmap teams, of which ISRU was one of the teams established. From Oct. 2004 to May 2005 the ISRU Capability Roadmap team examined the capabilities, benefits, architecture and mission implementation strategy, critical decisions, current state-of-the-art (SOA), challenges, technology gaps, and risks of ISRU for future human Moon and Mars exploration. This presentation will provide an overview of the ISRU <b>capability,</b> <b>architecture,</b> and implementation strategy examined by the ISRU Capability Roadmap team, along with a top-level review of ISRU benefits, resources and products of interest, and the current SOA in ISRU processes and systems. The presentation will also highlight the challenges of incorporating ISRU into future missions and the gaps in technologies and capabilities that need to be filled to enable ISRU...|$|E
40|$|Purpose This paper {{investigates the}} interrelationships between {{knowledge}} integration (KI), product innovation and capability development to enhance {{our understanding of}} how firms can develop capability at the firm level, which in turn enhances their performance. One of the critical underlying mechanisms for capability building identified in the literature is the role of knowledge integration, which operates within product innovation projects and contributes to dynamic capability development. Therefore, the main research question is “how does the integration of knowledge across product innovation projects lead to the development of capability?” Design/methodology/approach We adopted a case-based approach and investigated the case of a successful firm that was able to sustain its performance through a series of product innovation projects. In particular this research focused on the role of KI and firm-level capability development over the course of four projects, during which the firm successfully managed the transformation of its product base and renewal of its competitive advantage. For this purpose an in-depth case study of capability development was undertaken at the Iran Khodro Company (IKCO), the key player in the Iranian auto industry transformation. Originality/value This research revealed that along with changes at each level of product architecture “design knowledge” and “design capability” have been developed at the same level of product architecture, leading to capability development at that level. It can be argued that along the step by step maturation of radical innovation across the four case projects, architectural knowledge and capability have been developed at the case company, resulting in the gradual emergence of a modular product and <b>capability</b> <b>architecture</b> across different levels of product architecture. Such findings basically add to extensive emphasis in the literature on the interrelationship of the concept of modularity with knowledge management and capability development. Practical implications Findings {{of this study indicate that}} firms manage their knowledge in accordance with the level of specialization in knowledge and capability. Furthermore, firms design appropriate knowledge integration mechanisms within and among functions in order dynamically align knowledge processes at different levels of the product architecture. Accordingly, the outcomes of this study may guide practitioners in managing their knowledge processes, through dynamically employing knowledge integration modes step-by-step and from the part level to the architectural level of product architecture across a sequence of product innovation projects to encourage learning and radical innovation...|$|E
40|$|Grid {{computing}} {{is a kind}} of {{distributed computing}} that involve the integrated and collaborative use of distributed resources. It involves huge amounts of computational task which require reliable resource sharing across computing domains. Load balancing in grid is a technique which distributes the workloads across multiple computing nodes to get optimal resource utilization, minimum time delay, maximize throughput and avoid overload. It is a challenging problem that has been studied extensively is the past several years. This paper attempts to provide a comprehensive overview of load balancing in grid computing environment and also analyses the job distribution and system behavior. Furthermore, this survey various load balancing algorithms for the grid computing environment, identify several comparison metrics for the load balancing algorithms and carry out the comparison based on these identified metrics between them. it also reviews the latest research activities in the area of grid computing, including characteristics, <b>capabilities,</b> <b>architecture,</b> applications, design constraints, scheduling and load balancing and presents a set of challenges and problems...|$|R
40|$|In this paper, {{we present}} new Boolean {{matching}} methods for lookup table (LUT) -based programmable logic blocks (PLBs) and their applications to PLB architecture evaluations and field {{programmable gate array}} (FPGA) technology mapping. Our Boolean matching methods, which are based on functional decomposition operations, can characterize functions for complex PLBs consisting of multiple LUTs (possibly of different sizes) such as Xilinx XC 4 K CLBs. With these techniques, we conducted quantitative evaluation of four PLB architectures on their functional <b>capabilities.</b> <b>Architecture</b> evaluation results show that the XC 4 K CLB can implement 98 % of six-input and 88 % of seven-input functions extracted from MCNC benchmarks, while a simplified PLB architecture is more cost effective in terms of function implementation per LUT bit. Finally, we proposed new technology mapping algorithms that integrate Boolean matching and functional decomposition operations for depth minimization. Technology mapping results show that our PLB mapping approach achieves 12 % smaller depth or 15 % smaller area in XC 5200 FPGAs and 18 % smaller depth in XC 4 K FPGAs, compared to conventional LUT mapping approaches...|$|R
40|$|A {{commercial}} cation {{ion exchange}} resin, cross-linked polystyrene, {{has been successfully}} used as a template to fabricate 20 to 50 micron porous ceramic spheres. Ion exchange resins have dual template <b>capabilities.</b> Pore <b>architecture</b> of the ceramic spheres can be altered by changing the template pattern. Templating {{can be achieved by}} utilizing the internal porous structure or the external surface of the resin beads. Synthesis methods and chemical/physical characteristics of the ceramic spheres will be reported...|$|R
