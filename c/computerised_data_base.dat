3|10000|Public
40|$|ABSTRACT Much of our {{knowledge}} of drugs originates from clinical trials of drug efficacy performed on stringently selected patient groups, often without multiple concurrent diseases. However, the effectiveness of treatment under conditions of use in ordinary clinical practice may be very different to conditions in the randomised clinical trial. Use of large computerised data bases and record linkage has thus become increasingly common in pharmacoepidemiologic research. The greatest advantages of using routinely collected data are the minimisation of study costs and time required to complete a study, considerations that are particularly relevant for longitudinal studies. The advantages of using data bases also include the possibility of obtaining large sample sizes and to retrospectively study long-term outcomes. The risk for recall bias, a significant problem in interviews and questionnaires, is also reduced. However, computerised data bases also have some potentially serious disadvantages, primarily {{in the areas of}} data validity and data availability. The Tierp study, including individually based data bases of prescription drug use, will be used here as an example of research. In this paper an example of a comprehensive data base study concerning health care and drug utilisation in depressed patients is presented. Methodological considerations in data base research are discussed in relation to experiences from the antidepressant study. A well planned and research oriented <b>computerised</b> <b>data</b> <b>base</b> on prescription drugs represents an important tool {{in the study of the}} outcome of drug treatment in real world clinical practice. </span...|$|E
40|$|For decades, {{personality}} {{tests have}} been commonly used {{as one of}} the tools for personnel selection. However, through the use of various validity generalization techniques, researchers have claimed that they have very poor validity in predicting job performance. These claims were made by Guion and Gottier (1965), Ghiselli (1973), and Schmitt, Gooding, Noe and Kirsch (1984). Each of these researchers used different statistical techniques and methodologies to reach their conclusions. The latest study by Schmitt et al. (1984), used a statistical validity generalization technique called meta-analysis. Based on data collected from only two journal publications they claimed that personality tests had a validity of. 15. The present study tested the conclusions of the Schmitt et al. (1984) study, by re-analysing the same data using a more accurate meta-analysis technique and by incorporating a larger data base. In addition to this, any new data from 1952 up to 1990 was included in an overall analysis to find out the current validity of personality tests. A flexible coding technique which interacted with a <b>computerised</b> <b>data</b> <b>base</b> allowed any combination of data to be separately analysed. This made it possible to discover which types of personality tests worked best in differing situations such as different sample types and criterion measures. Results of the Schmitt et al. (1984) re-analysis showed that by correcting coefficients for unreliability, the overall validity was significantly higher than the Schmitt et al. (1984) result. A separate analysis revealed that vocational tests had the highest validity of the six personality test types. The sample-types with the highest validities were Supervisory and Skilled workers. The best criterion-types were in the "Other" category whereby measures were developed specifically for the type of job. The overall analysis incorporating 38 years of research showed that personality tests had a validity of. 22. This was significantly higher than the figure quoted by Schmitt et al. (1984). Results showed that personality tests in their present state are generally poor predictors of job performance, however when they are modified to become more job specific, their validity improves. It is suggested that in the future, personality tests should be specifically designed for the purpose of personnel selection and for specific jobs...|$|E
40|$|The Micoteca da Universidade do Minho culture {{collection}} (MUM) {{was established in}} 1996, at the Biological Engineering Department of Minho University, in Portugal, aiming to maintain and supply fungal strains for research in biotechnology and teaching, and to become a centre of knowledge, information and training in mycology. The original collection, containing 38 strains with specific degradatory capabilities, mainly cellulolytic, was typically a private research collection, and accompanied the researchers throughout various laboratories. Pressing solicitations from the local agro-food industry, from which several cooperative research projects had been developed, {{and the fact that}} University of Minho was developing an institutional policy for the establishment of horizontal services to support research, made possible, among other factors, the creation of a {{culture collection}} capable of addressing the issues of availability of strains and related information, of research and selection of appropriate technologies, and of training and building of expertise. MUM now harbours 138 species and 59 genera, totalling 396 strains. Among its holdings the collection includes fungi producing extracellular enzymes, mycotoxins, fungi from tap and bottled water, and airborne fungi, isolated during research activities on biodegradation and biodeterioration, mycotoxins in food products and studies of air quality in industrial environments. Penicillium and Aspergillus are the main genera represented in the culture collection in termos of species diversity and number of preserved strains. Standard procedures have been set up for strain reception within established admission criteria and for strain preservation, storage and supply. A <b>computerised</b> <b>data</b> <b>base</b> was built in-house, having in mind the storage and retrieval of strain data and also the stock management of preserved strains. External access to information on available strains is possible through the electronic catalogue at [URL] MUM was built on well established criteria taking into account relevant international quality standards. Membership in international organizations such as ECCO and WFCC was accomplished and MUM is also registered in the WDCM, BioCise, and BioCase information services. In the last three years MUM has been involved in the OECD initiatives related to guidance for the operation of Biological Resource Centres (BRC’s). It is foreseen that in the near future new facilities will be available at the Health of Sciences research building with better working and storage areas, in the vicinity of PCL 3 laboratories. MUM is therefore in position to plan ahead in relation to the widening of the scope of its holdings, and to propose the strengthening of ties with other specialised sectors in life sciences within the University in order to achieve a certified BRC for the University of Minho (BRC-UMinho) ...|$|E
40|$|Machinabihty {{assessment}} of a high strength steel was carried out using uncoated and coated inserts. The material investigated was steel EN 24 T/ 817 M 40 T 277 (BHN). The object of this assessment is to generate reliable machining data in terms of tool life, cutting force and surface finish in relation to cutting speed, feed rate and depth of cut. During this research cutting tests were carried out by using both one variable-at-a-time and design of experiment approaches. For one-variable-at-a-time experiments, cutting forces, tool life and surface finish were measured. In these tests, the cutting speed, feed rate and depth of cut were varied to study their effects on the tool life, cutting forces and surface finish. With the design of experiment approach, the combined effects of the cutting variables on the machining responses were investigated. Using the mathematical models for different responses, a <b>computerised</b> machinabihty <b>data</b> <b>base</b> system was developed to facilitate the optimum selection of cutting parameters. The selection of cutting parameters is applicable for EN 24 T/ 817 M 40 T steel 817...|$|R
40|$|During {{the last}} decades, the {{incidence}} of disability pension in Sweden has increased continuously and it peaked in 1993 at approximately 62, 000 new pensions. Early retirement due to occupational disability often constitutes {{a problem for the}} society {{as well as for the}} individual. The aim of this thesis was to analyse aspects on the risk of disability pension, to describe changes in health after the decision on disability pension and to estimate the financial expenditure. A total of 7, 697 men, comprising five complete birth-year cohorts (1926 - 1930) of male residents in Malmö, Sweden, were invited to a screening programme in the mid- 1970 ’s. Decisions on disability pension and mortality data were identified from national <b>computerised</b> <b>data</b> <b>bases.</b> At the end of follow-up (the calendar year when the men turned 58), 18 % had been granted disability pension, the incidence higher (31 %) among those who refrained to participate in the screening program. Frequent causes for disability pension, altogether accounting for 74 % of the cases, were musculoskeletal diseases, mental disorders (including alcohol dependence) and diseases of the circulatory system. Alcohol dependence was more common among non-participants. Mental disorders predominated in younger age groups and musculoskeletal diseases in older ones. Compared with normal body mass index, obesity in particular, but also under- and overweight, were associated with higher risk of disability pension. For those who were not teetotallers, alcohol consumption was estimated from the scores obtained at an alcohol screening test. Low alcohol consumption was related to low risk of disability pension while high alcohol consumption and high risk of disability pension showed a positive relation. Teetotallers were at higher risk of disability pension than those with low alcohol consumption, although conclusions regarding this association must be drawn with great prudence. Socioeconomic status, as defined by occupation, was associated with risk of disability pension. Compared with higher level white collar workers, blue collar workers in particular but also lower level white collar workers were at higher risk. During the period when the disability pensions were granted, an increase in circulatory disease was observed while the prevalence of psychiatric disease and use of hypnotics remained practically unchanged. The proportion of men with complaints of back problems was considerably higher after pension than before. A registration of the decisions on all new disability pensions in Malmöhus county was made during a period of three months. During the first two and a half years after the decision, a small number of the pensions were terminated mainly due to deaths but also because of pensions that were discontinued. The rankings of the diagnosis categories on the one hand and their respective financial costs on the other, were equal. However, the proportions of the costs were lower for musculoskeletal diseases and higher for mental disorders compared with the respective proportions of the diagnoses. For unemployed subjects, mental disorders made the largest contribution to the financial costs...|$|R
5000|$|Datafication is {{a modern}} {{technological}} trend turning many aspects of our life into <b>computerised</b> <b>data</b> [...] and transforming this information into new forms of value.|$|R
40|$|And {{goodbye to}} 2009 – {{but what a}} {{wonderful}} year it has been. We have had record numbers of doctors attending our courses, we premiered the new 2 day Short Course – it was a successful launch, so it will feature 3 times in the 2010 Calendar (see insert), Open Day was great and we’ve designed a new format for our next one ‘Pushing the Boundaries’, 14 th August 2010, we moved to new, beautiful premises in Dubbo, we have just finished implementing Medical Director (a specially customized interface was designed to seamlessly allow our old <b>computerised</b> patient <b>data</b> <b>base</b> {{to interact with the}} MD software) in all our sites and trained up 70 doctors and nurses in its use, we have run 4 cervical screening upskilling projects, trained up 118 doctors in Implanon insertion and have just put to bed a new handbook about vulval conditions – phew!! I was lucky enough to go to the FIGO (International Federation of O&G) Conference in Cape Town in October. I learned {{a great deal about the}} issues of developing countries in dealing with issues such as consent and young people, maternal mortality and medical abortion. There were interesting sessions on managing menorrhagia (now called heavy menstrual bleeding) and the new terminology being promoted to define abnormal uterine bleeding (instead of DUB) as well as new contraceptive methods available elsewhere and some that is proposed and not yet available anywhere. I hope you all have a wonderful New Year and relaxing January to build your mind and body strength for another block buster year with us...|$|R
40|$|OBJECTIVE [...] To {{examine the}} use of {{routinely}} collected <b>computerised</b> <b>data</b> in clinical audit. DESIGN [...] Retrospective review of all analyses of obstetric practice <b>based</b> on a <b>computerised</b> <b>data</b> system from January 1983 to June 1988. SETTING [...] Maternity department of the regional referral hospital in Oxford. MAIN OUTCOME MEASURES [...] Congruence {{with the principles of}} clinical audit; that is, comparing clinical practice with previously agreed standards and changing practice to meet these standards if necessary. RESULTS [...] Over the {{five and a half years}} of the study the data formed the basis of 130 special inquiries into different aspects of obstetric practice. Most inquiries seemed to be aimed only at describing current activities and identifying trends. Genuine clinical audit was rare. Simple audits [...] for example, concerning induction for pregnancy after term [...] could be supported by the <b>computerised</b> <b>data,</b> but for detailed and wide ranging audits [...] for example, reducing antenatal clinic visits for low risk multiparas [...] the data had to be supplemented from other sources. CONCLUSIONS [...] Routinely collected <b>computerised</b> <b>data</b> enable ongoing clinical audit, but it becomes a reality only when clinicians agree on standards of practice and have a flexible attitude towards change. Even then, genuine clinical audits of obstetric practice demand more detailed and comprehensive data than are generally available on such systems...|$|R
40|$|OBJECTIVE: To {{assess the}} {{potential}} for using routine <b>computerised</b> general practice <b>data</b> for surveillance of illness. DESIGN: Comparison of the incidence of influenza during the 1989 epidemic derived from a computerised database with that derived from the Royal College of General Practitioners's weekly returns service [...] a well established predominantly manual surveillance system. SETTING: 433 general practices throughout the United Kingdom that used a commercial computer system linked to a central databank. MAIN OUTCOME MEASURE: Incidence of influenza. RESULTS: The slope of the influenza epidemic curve was essentially the same whether derived from the routine <b>computerised</b> <b>data</b> or royal college's weekly returns service <b>data,</b> and the <b>computerised</b> <b>data</b> were geographically consistent. Throughout the study period, however, the computer derived incidence was between one third and one quarter of that derived from the royal college's system (which is served by practitioners trained in surveillance methods). The peak weekly rates were 164 cases per 100, 000 for the computerised system and 583 cases per 100, 000 for the royal college's surveillance system. CONCLUSIONS: The apparent underreporting in the routine <b>computerised</b> <b>data</b> probably reflects lack of motivation and experience in disease surveillance and haphazard computer entry (particularly of consultations that took place outside of the surgery and consultations that {{did not result in}} a prescription), along with overestimation of the population under surveillance. Nevertheless, routine computerised surveillance allows rapid data collection from a large number of practices over a wide geographical area and would greatly augment existing methods...|$|R
40|$|OBJECTIVE: Risk {{stratification}} {{systems are}} used in cardiac surgery to estimate mortality risk for individual patients and to compare surgical performance between institutions or surgeons. This study investigates the suitability of six existing risk stratification systems for these purposes. METHODS: Data on 5471 patients who underwent isolated coronary artery bypass grafting at two UK cardiac centres between 1993 and 1999 were extracted from a prospective <b>computerised</b> clinical <b>data</b> <b>base.</b> Of these patients, 184 (3. 3 %) died in hospital. In-hospital mortality risk scores were calculated for each patient using the Parsonnet score, the EuroSCORE, the ACC/AHA score and three UK Bayes models (old, new complex and new simple). The accuracy for predicting mortality at an institutional level was assessed by comparing total observed and predicted mortality. The accuracy of the risk scores for predicting mortality for a patient was assessed by the Hosmer-Lemeshow test. The receiver operating characteristic (ROC) curve {{was used to evaluate}} how well a system ranks the patient with respect to their risk of mortality and can be useful for patient management. RESULTS: Both EuroSCORE and the simple Bayes model were reasonably accurate at predicting overall mortality. However predictive accuracy at the patient level was poor for all systems, although EuroSCORE was accurate for low to medium risk patients. Discrimination was fair with the following ROC areas: Parsonnet 0. 73, EuroSCORE 0. 76, ACC/AHA system 0. 76, old Bayes 0. 77, complex Bayes 0. 76, simple Bayes 0. 76. CONCLUSIONS: This study suggests that two of the scores may be useful in comparing institutions. None of the risk scores provide accurate risk estimates for individual patients in the two hospitals studied although EuroSCORE may have some utility for certain patients. All six systems perform moderately at ranking the patients and so may be useful for patient management. More results are needed from other institutions to confirm that the EuroSCORE and the simple Bayes model are suitable for institutional risk-adjusted comparisons...|$|R
50|$|The {{laboratory}} includes {{machinery of}} singeing, desizing, mercerising, dyeing, printing and finishing. The laboratory has the facility to dye material as fibre, yarn and fabric form and styles of printing. The laboratory has a <b>computerised</b> <b>data</b> colour system, gas chromatography and mass spectrophotometer, light fastness and other quality control equipment.|$|R
40|$|This paper {{considers}} {{the design of}} a Scheduling Sub-System which could be attached to an existing <b>computerised</b> <b>data</b> processing system in order to enhance its overall operation. It draws heavily on the experience of the authors in designing a system to assist in the scheduling of video tape machines within BBC Television. ...|$|R
40|$|OBJECTIVE [...] To {{assess the}} {{potential}} for using routine <b>computerised</b> general practice <b>data</b> for surveillance of illness. DESIGN [...] Comparison of the incidence of influenza during the 1989 epidemic derived from a computerised database with that derived from the Royal College of General Practitioners's weekly returns service [...] a well established predominantly manual surveillance system. SETTING [...] 433 general practices throughout the United Kingdom that used a commercial computer system linked to a central databank. MAIN OUTCOME MEASURE [...] Incidence of influenza. RESULTS [...] The slope of the influenza epidemic curve was essentially the same whether derived from the routine <b>computerised</b> <b>data</b> or royal college's weekly returns service <b>data,</b> and the <b>computerised</b> <b>data</b> were geographically consistent. Throughout the study period, however, the computer derived incidence was between one third and one quarter of that derived from the royal college's system (which is served by practitioners trained in surveillance methods). The peak weekly rates were 164 cases per 100, 000 for the computerised system and 583 cases per 100, 000 for the royal college's surveillance system. CONCLUSIONS [...] The apparent underreporting in the routine <b>computerised</b> <b>data</b> probably reflects lack of motivation and experience in disease surveillance and haphazard computer entry (particularly of consultations that took place outside of the surgery and consultations that {{did not result in}} a prescription), along with overestimation of the population under surveillance. Nevertheless, routine computerised surveillance allows rapid data collection from a large number of practices over a wide geographical area and would greatly augment existing methods...|$|R
40|$|Objective: Reporting bias due {{to social}} {{desirability}} {{is an important}} consideration in carrying out surveys on sensitive issues. The study compared the frequency of self reported sensitive behaviours and response rates between the conventional "telephone interviewer method" (TIM) and a combined interviewer and <b>computerised</b> <b>data</b> capturing method (telephone interviewer and computerised questionnaire method, or TICQM) ...|$|R
40|$|The final {{instalment}} of {{the work}} derived from my PhD project compares a set of open ended questions used at a site which use a <b>computerised</b> <b>data</b> management system and compares it with two which use conventional charting. This produces theoretical constructs to explain how nurses use these systems and suggestions for their developmen...|$|R
50|$|The Yellow Cab Company has {{now become}} the largest cab fleet in Brisbane and {{introduced}} the first <b>computerised</b> <b>data</b> dispatch from the control room to taxis. The system was designed to increase efficiency and provide a better and safer service for the public and increase drivers' security. The computers have been installed into the fleet of over 580 taxis.|$|R
5000|$|The Wet Process Laboratory {{includes}} machinery for singeing, desizing, mercerising, dyeing, {{printing and}} finishing. The laboratory can dye material as fibre, yarn and fabric form {{and style of}} printing. The laboratory has a <b>computerised</b> <b>data</b> colour system, gas chromatography and mass spectrophotometer, light fastness and other quality control equipment required to ensure dyeing and finishing qualities.|$|R
50|$|This {{laboratory}} includes machinery for singeing, desizing, mercerising, dyeing, {{printing and}} finishing {{and students are}} able to gain knowledge of wet processing technology. It has the facility for dyeing material as fibre, yarn and fabric and various styles of printing. It has <b>computerised</b> <b>data</b> colour system, gas chromatography and mass spectrometer, light fastness and other quality control equipment required to ensure quality dyeing and finishing.|$|R
40|$|Sociological Data Archive (SDA) of the Institute of Sociology {{collects}} <b>computerised</b> <b>data</b> {{files from}} sociological surveys. Czech survey research {{has a long}} tradition, but it was deeply affected by communist regime. A debate concerning {{the formation of a}} social science data archive was opened in the late 60 s. The main access to the SDA'data library is provided on the Internet. The Archive pays a great attention to promoting secondary analysis and co-operates in large research projects...|$|R
50|$|Administrative support: Registration; {{personal}} identity documents for every patient; a record system; patient-owned health booklets; <b>computerised</b> patient <b>data</b> and data analysis; an economy department.|$|R
40|$|Previous {{studies of}} the {{mechanisms}} that precipitate acid reflux episodes and acid clearance have used unphysiological, short term hospital <b>based</b> <b>data.</b> A new 24 hour pH and motility recording system combined with <b>computerised</b> <b>data</b> analysis {{have been used to}} study naturally occurring acid reflux episodes in healthy ambulant volunteers. A variety of events that produced recognisable transdiaphragmatic pressure patterns were associated with acid reflux episodes (particularly belching). Peristaltic waves were the predominant contractions leading to oesophageal acid clearance and were the commonest contraction type during reflux episodes. Peristaltic wave parameters (amplitude, velocity, frequency, and percentage proportion) varied during different periods of the 24 hour recording. This study has produced new information about 24 hour oesophageal function and naturally occurring acid reflux which will provide a basis for comparison with patients with gastro-oesophageal reflux disease...|$|R
40|$|MIQUEST (Morbidity Information QUery and Export SynTax) is an {{approach}} to the collection of <b>computerised</b> <b>data</b> from general practice databases. It is being used increasingly in local and national data collection schemes because it offers the opportunity to access anonymised data in a common format from different general practice computer systems. For individual research projects, however, MIQUEST has yet to find widespread use. This paper describes the experiences from a MIQUEST-based study in five general practices that form part of the Trent Focus Collaborative Research Network...|$|R
40|$|In view {{of knowing}} {{the essence of}} {{phenomena}} {{it is necessary to}} perform statistical data processing operations. This allows for shifting from individual data to derived, synthetic indicators that highlight the essence of various phenomena. The high volume and diversity of processing operations presuppose developing plans of <b>computerised</b> <b>data</b> processing. To identify distinct and homogenous groups and classes it is necessary to realise well-pondered groupings and classifications that presuppose to comply with the requirements presented in the article. computerized data processing, statistics, synthetic indicators...|$|R
5000|$|Vinge {{published}} his first short story, [...] "Bookworm, Run!", in the March 1966 issue of Analog Science Fiction, then edited by John W. Campbell. The story explores {{the theme of}} artificially augmented intelligence by connecting the brain directly to <b>computerised</b> <b>data</b> sources. He became a moderately prolific contributor to SF magazines in the 1960s and early 1970s. In 1969, he expanded the story [...] "Grimm's Story" [...] (Orbit 4, 1968) into his first novel, Grimm's World. His second novel, The Witling, was published in 1975.|$|R
40|$|The {{immediate}} {{impact of the}} availability of general practitioner services on emergency department presentations EDITOR,—Emergency departments in NSW have been collecting <b>computerised</b> <b>data</b> about the patients they see using the Emergency Department Information System since 1994. 1 It is possible to use these data to investigate {{the extent to which the}} availability of general practitioner services varies with the rate of primary care presentations to NSW emer-gency departments. It might be expected that emergency departments would see more primary care patients on weekdays that were public holidays than weekdays that were not...|$|R
40|$|A genetic {{family study}} was {{undertaken}} by photofluorography of the first, second, and third degree relatives of 116 index patients with adolescent idiopathic scoliosis (AIS). The index patients were ascertained {{in the course}} of an epidemiological screening. The pattern of familial clusters and the recurrence risk related to the number of affected relatives and to the severity of the disorder in the index patients support the theory of polygenic inheritance, a multifactorial-threshold aetiological model. The recurrence risk table for first degree relative, prepared by <b>computerised</b> <b>data</b> processing and analysis, may contribute to the early diagnosis and prevention of the disorder...|$|R
40|$|Rapid {{technological}} {{developments in the}} field of information, computers and communications are leading to significant structural changes in the economies of Member countries. Flows of <b>computerised</b> <b>data</b> and information are an important consequence of technological advances and are playing an increasing role in national economies. With the growing economic interdependence of Member countries, these flows acquire an international dimension, known as Transborder Data Flows. It is therefore appropriate for the OECD to pay attention to policy issues connected with these transborder data flows. This Declaration was adopted by the Governments of OECD Member countries on 11 th April 1985. ...|$|R
40|$|Blog {{post for}} the Software Sustainability Institute. Excerpt: "For Open Science, it is {{important}} to cite the software you use in your research, as has been mentioned in previous articles on this blog. Particularly, you should cite any software that made a significant or unique impact on your work. Modern research relies heavily on <b>computerised</b> <b>data</b> analysis, and we should elevate its standing to a core research activity with data and software as prime research artefacts. Steps must be taken to preserve and cite software in a sustainable, identifiable and simple way. This is how digital repositories like Zenodo can help. ...|$|R
40|$|The {{handling}} of nuclear material is highly automated m the Melox fabrication plant. Consequently, the material follow-up system {{is based on}} computer technology. Euratom uses cornputerised plant operating data, along with independently generated <b>computerised</b> <b>data</b> in the safeguards scheme in place in Melox. The {{purpose of this paper}} is to explain in detail the data treatment performed by Euratom for safeguards purposes and to show how this treatment contributes to the attainment of the safeguards objectives. The paper concludes with a proposals for a methodology aimed at evaluating the impact of data treatment on the attainment of safeguards goals. 1...|$|R
40|$|Original article can {{be found}} at : [URL] Copyright ElsevierObjective: to develop an intrapartum {{intervention}} scoring tool which could be used to define maternity units as either ???lower intrapartum intervention??? or ???higher intrapartum intervention??? units. This scoring tool was designed to form the basis of a comparison of the perception of risk by midwives working in either ???lower intrapartum intervention??? or ???higher intrapartum intervention??? units. Design: three aspects were included: (1) the systematic data reduction of the St. Mary's Maternity Information System database used by 11 maternity units to include Caucasian nulliparous women suitable for midwifery-led care; (2) the calculation and the ranking of frequency distributions for the following interventions/management: (a) the management of breech presentation and of one previous caesarean section, the choice of home birth; and (b) augmentation of labour, use of electronic fetal monitoring, use of epidural, method of delivery; (3) the sum of the individual intrapartum ranking marks made up the final intrapartum score for each unit. Results: intrapartum interventions varied considerably between units. The scoring system enabled units to be described as either ???Lower intrapartum intervention??? or ???Higher intrapartum intervention??? units. Conclusions: routinely collected <b>computerised</b> <b>data</b> can be used to identify the outcomes of intrapartum care. This study suggests that the analysis of <b>computerised</b> <b>data</b> could provide a suitable basis for the audit and the comparison of intrapartum interventions for the care of women suitable for midwifery-led care...|$|R
50|$|In {{the post-war}} years, the DR in East Germany {{continued}} to develop {{independently of the}} DB, but very much in parallel. The locomotive classification scheme, based on that of the DRG, was extended. The production, conversion and development of steam locomotives initially continued in earnest; older, especially ex-Länderbahn classes being rationalised and withdrawn from service. A major conversion (Rekonstruction) programme to update steam locomotives and rectify flawed, mainly wartime austerity, classes {{was carried out in}} the 1950s. Gradually, however, they were replaced by the more economical and easier-to-maintain diesel and electric classes. In general this happened rather later than in the West. In 1970, the DR renumbered its locomotives in order to conform to new <b>computerised</b> <b>data</b> standards.|$|R
30|$|The {{measured}} response {{parameters are}} displacement and {{acceleration of the}} structure along the direction of force. The displacement response is measured by attaching Brüel & Kjær Deltatron 4507 – 001 accelerometers (Brüel & Kjær Sound & Vibration Measurement A/S, Nærum, Denmark) at the base and top of the structure. PULSE 3560 B <b>computerised</b> <b>data</b> acquisition and multi-analyser system is used to acquire and analyse the experimental data. In each set of experiments, the TLD-structure system is subjected to harmonic sinusoidal base motions with different excitation frequencies. The external sinusoidal {{force is applied to}} the structure by means of an induction motor mounted on the shaking table. The external excitation amplitude is accordingly maintained constant by keeping the displacement amplitude of the shaking table constant.|$|R
50|$|Jennett {{set up a}} {{prospective}} <b>computerised</b> <b>data</b> bank to collect the features and outcome of head injuries. Data was compiled from Glasgow, the United States, and the Netherlands {{over a long period}} and led to a series of papers in the 1970s, the introduction of the near universally adopted Glasgow Coma Scale (GCS) with Graham Teasdale, and the Glasgow Outcome Scale with Bond. In 1972 working with Dr Plum of America, Jennett published The Persistent Vegetative State - defining a condition and coining a phrase which remains in widespread use today. His work with the Glasgow-based Neuropathologists Adams and Graham significantly reduced mortality and disability. Many international collaborative studies followed, comparing outcomes after different severity of injury and with alternative therapeutic regimes.|$|R
50|$|The act {{required}} a prior permit from the DIB for each <b>computerised</b> personal <b>data</b> register. When a permit was given, the Board issued tailor-made conditions for that register. It {{did not contain}} many provisions on when and how the data should be processed, or general data protection principles.|$|R
40|$|The {{risks in}} an IT project {{are very high}} both because of its {{complexity}} and also because the context of rapidly-developing technology leads to {{a high degree of}} uncertainty. IT projects should have comprehensive formal quality management fully integrated within all aspects of project management. A review of the quality management in IT project literature suggests, customer-focused TQM is now synonymous with good management. TQM combines the use of <b>computerised</b> <b>data</b> collection and statistical experimentation with a focus on teamwork, group participation and a culture of continuous improvement in operating systems (Robert. 1993). Using the survey methodology and through two case studies, qualitative data was gathered to develop a model of quality management implementation process in New Zealand. Key words: Quality. Total Quality Management (TQM), Quality Control (QC), Quality Assurance (QA), Quality Model...|$|R
40|$|In Catalonia, from {{a general}} {{point of view}} and {{concerning}} Geolinguistics, three assessments can be done: a) no new initiatives for creating a general linguistic atlas are expected; on the contrary, the tendency would be to create regional or local atlases or, disregarding cartography, to develop of monographs concerning several linguistic aspects of a certain dialectal area; b) there is no perceived need for an electronic publication of the atlas or the release of an internet version (the general format used is paper); and c) there is a possibility of <b>computerising</b> the <b>data</b> contained in old atlases. The main aim {{of this paper is to}} describe the processes of systematisation and mapping of dialectal <b>data</b> <b>based</b> on "La flexió verbal en els dialectes catalans". The paper is structured in five parts: a) The corpus of morphological and phonetic data; b) Mapping the data; c) Using the program; d) Sound maps; e) Conclusions...|$|R
