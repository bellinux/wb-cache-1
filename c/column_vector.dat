666|708|Public
25|$|One {{extremely}} helpful {{view is that}} each unknown is a weight for a <b>column</b> <b>vector</b> in a linear combination.|$|E
25|$|Consider {{independent}} identically distributed (IID) {{random variables}} with a given probability distribution: standard statistical inference and estimation theory defines {{a random sample}} as the random vector given by the <b>column</b> <b>vector</b> of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.|$|E
25|$|Otherwise, {{suppose we}} have m vectors of n coordinates, with m<n. Then A is an n×m matrix and Λ is a <b>column</b> <b>vector</b> with m entries, {{and we are}} again {{interested}} in AΛ= 0. As we saw previously, this is equivalent {{to a list of}} n equations. Consider the first m rows of A, the first m equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈i1,...,i'm〉 is any list of m rows, then the equation must be true for those rows.|$|E
5000|$|... #Caption: The <b>column</b> <b>vectors</b> of a matrix. The column {{space of}} this matrix is the vector space {{generated}} by linear combinations of the <b>column</b> <b>vectors.</b>|$|R
5000|$|... where , [...] and [...] are [...] {{dimensional}} <b>column</b> <b>vectors,</b> [...] is [...] by [...] dimensional matrix and , [...] and [...] are [...] dimensional <b>column</b> <b>vectors.</b>|$|R
5000|$|Let [...] be {{the first}} m <b>column</b> <b>vectors</b> of , [...] the <b>column</b> <b>vectors</b> of V, and [...] the {{diagonal}} elements of Σ. The previous expression is then ...|$|R
500|$|... where [...] is a <b>column</b> <b>vector</b> of the outputs, [...] is {{a matrix}} of the {{transfer}} functions, and [...] is a <b>column</b> <b>vector</b> of the inputs.|$|E
500|$|Matrices {{can be used}} to compactly {{write and}} work with {{multiple}} linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a <b>column</b> <b>vector</b> (that is, n×1-matrix) of n variables x1, x2, ..., x'n, and b is an m×1-column vector, then the matrix equation ...|$|E
500|$|The {{problem of}} {{representation}} {{theory of the}} Lorentz group is, in the finite-dimensional case, to find new sets of matrices, not necessarily [...] in size that satisfies the same multiplication table as the matrices in the original Lorentz group. Returning to {{the example of the}} electromagnetic field, [...] matrices are needed that can be applied to a six-dimensional <b>column</b> <b>vector</b> containing the all together six components of the electromagnetic field. Thus -matrices are sought for, such that ...|$|E
5000|$|... where x, a are p-dimensional <b>column</b> <b>vectors,</b> y, b are q-dimensional <b>column</b> <b>vectors,</b> and A, B, C, D are as above. Multiplying {{the bottom}} {{equation}} by [...] and then subtracting {{from the top}} equation one obtains ...|$|R
5000|$|Matrix {{multiplication}} can {{be implemented}} as computing the <b>column</b> <b>vectors</b> of [...] as linear combinations of the <b>column</b> <b>vectors</b> in [...] using coefficients supplied by columns of [...] That is, each column of [...] can be computed as follows: ...|$|R
50|$|Let A {{denote the}} algebra {{generated}} by these matrices. By counting dimensions, A {{is a complete}} 2k&times;2k matrix algebra over the complex numbers. As a matrix algebra, therefore, it acts on 2k-dimensional <b>column</b> <b>vectors</b> (with complex entries). These <b>column</b> <b>vectors</b> are the spinors.|$|R
500|$|The {{individual}} items in an m × n matrix A, often denoted by a'i,j, where max i = m and max j = n, are called its elements or entries. Provided {{that they have}} the same size (each matrix has the same number of rows and the same number of columns as the other), two matrices can be added or subtracted element by element (see Conformable matrix). The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second (i.e., the inner dimensions are the same, n for A'm,n × B'n,p). Any matrix can be multiplied element-wise by a scalar from its associated field. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as [...] For example, the rotation of vectors in three-dimensional space is a linear transformation, which can be represented by a rotation matrix R: if v is a <b>column</b> <b>vector</b> (a matrix with only one column) describing the position of a point in space, the product Rv is a <b>column</b> <b>vector</b> describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable (along with other information) from the matrix's eigenvalues and eigenvectors.|$|E
2500|$|... where A is an m×n matrix, x is a <b>column</b> <b>vector</b> with n entries, and b is a <b>column</b> <b>vector</b> with m entries.|$|E
2500|$|The outer product [...] is {{equivalent}} to a matrix multiplication uvT, provided that u is represented as a [...] <b>column</b> <b>vector</b> and v as a [...] <b>column</b> <b>vector</b> (which makes vT a row vector). For instance, if [...] and , then ...|$|E
40|$|This paper {{addresses}} the second stage. A non-parametric maximum-likelihood approach based on Parzen windowing is presented. It is {{shown that the}} peak points in the probability distribution of measurements directions correspond to the directions of the <b>column</b> <b>vectors</b> of the mixing matrix. An algorithm to estimate the <b>column</b> <b>vectors</b> in the static case, and to track the <b>column</b> <b>vectors</b> in the dynamic case is presented. The tracking capability of the algorithm is determined and, using a simple wave propagation model, corresponding limitations on the speeds of mobile sources are derive...|$|R
50|$|There {{are also}} two {{conventions}} to define these matrices, {{depending on whether}} you want to work with <b>column</b> <b>vectors</b> or row vectors. Different graphics libraries have different preferences here. OpenGL prefers <b>column</b> <b>vectors,</b> DirectX row vectors. The decision determines from which side the point vectors are to be multiplied by the transformation matrices. For <b>column</b> <b>vectors,</b> the multiplication is performed from the right, ie , where vout and vin are 4x1 colum vectors. The concatenation of the matrices also is done from the right to left, ie, for example , when first rotating and then shifting.|$|R
30|$|The matrix W_ 0 ∈R^d× n {{including}} <b>column</b> <b>vectors</b> w_i.|$|R
2500|$|If A {{is a real}} [...] matrix, then A {{defines a}} linear map from [...] Rn to Rm by sending the <b>column</b> <b>vector</b> [...] to the <b>column</b> <b>vector</b> [...] Conversely, any linear map between finite-dimensional vector spaces can be {{represented}} in this manner; see the following section.|$|E
2500|$|Define [...] as a <b>column</b> <b>vector</b> of [...] random {{variables}} , and [...] as a <b>column</b> <b>vector</b> of [...] scalars [...] Therefore, [...] is a linear {{combination of these}} {{random variables}}, where [...] denotes the transpose of [...] Also let [...] be the covariance matrix of [...] The variance of [...] is then given by: ...|$|E
2500|$|Extending the Cramér–Rao {{bound to}} {{multiple}} parameters, define a parameter <b>column</b> <b>vector</b> ...|$|E
25|$|Only the n <b>column</b> <b>vectors</b> of U {{corresponding}} to the row vectors of V* are calculated. The remaining <b>column</b> <b>vectors</b> of U are not calculated. This is significantly quicker and more economical than the full SVD if n≪m. The matrix Un is thus m×n, Σn is n×n diagonal, and V is n×n.|$|R
50|$|The {{columns of}} A span the column space, {{but they may}} not form a basis if the <b>column</b> <b>vectors</b> are not linearly independent. Fortunately, {{elementary}} row operations do not affect the dependence relations between the <b>column</b> <b>vectors.</b> This makes it possible to use row reduction to find a basis for the column space.|$|R
40|$|We {{consider}} {{a system of}} m linearly independent equality constraints in n nonnegative variables: Ax = b, x ≧ 0. The fundamental problem that we discuss is the following: suppose we are given a set of r linearly independent <b>column</b> <b>vectors</b> of A, known as the special <b>column</b> <b>vectors.</b> The problem is to develop an efficient algorithm {{to determine whether there}} exists a feasible basis which contains all the special <b>column</b> <b>vectors</b> as basic <b>column</b> <b>vectors</b> and to find such a basis if one exists. Such an algorithm has several applications in the area of mathematical programming. As an illustration, we show that the famous travelling salesman problem can be solved efficiently using this algorithm. Recent published work indicates that this algorithm has applications in integer linear programming. An algorithm for this problem using a set covering approach is described...|$|R
2500|$|If {{the vector}} space [...] is finite-dimensional, then the linear {{transformation}} [...] {{can be represented}} as a square matrix A, and the vector [...] by a <b>column</b> <b>vector,</b> rendering the above mapping as a matrix multiplication on the left hand side and a scaling of the <b>column</b> <b>vector</b> on the right hand side in the equation ...|$|E
2500|$|Consider a <b>column</b> <b>vector</b> [...] of scalar fields [...] Let the Lagrangian density be ...|$|E
2500|$|Had a <b>column</b> <b>vector</b> {{representation}} {{been used}} instead, the transformation law {{would be the}} transpose ...|$|E
2500|$|This rotates <b>column</b> <b>vectors</b> {{by means}} of the {{following}} matrix multiplication, ...|$|R
2500|$|Using <b>column</b> <b>vectors,</b> we can {{represent}} {{the same result}} as follows: ...|$|R
5000|$|This rotates <b>column</b> <b>vectors</b> {{by means}} of the {{following}} matrix multiplication, ...|$|R
2500|$|... is an {{explicit}} system of ordinary differential equations of order n and dimension m. In <b>column</b> <b>vector</b> form: ...|$|E
2500|$|The outer {{product of}} the <b>column</b> <b>vector</b> [...] by the row vector [...] yields an [...] matrix : ...|$|E
2500|$|When D is a 1×1 matrix, B is a <b>column</b> <b>vector,</b> and C is a {{row vector}} then ...|$|E
5000|$|Using <b>column</b> <b>vectors,</b> we can {{represent}} {{the same result}} as follows: ...|$|R
50|$|Only the n <b>column</b> <b>vectors</b> of U {{corresponding}} to the row vectors of V* are calculated. The remaining <b>column</b> <b>vectors</b> of U are not calculated. This is significantly quicker and more economical than the full SVD if n ≪ m. The matrix Un is thus m×n, Σn is n×n diagonal, and V is n×n.|$|R
30|$|W {{correspond}} to pairs of <b>column</b> <b>vectors</b> in W that are non-orthogonal.|$|R
