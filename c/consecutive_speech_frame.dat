0|456|Public
30|$|A {{packet loss}} process that {{periodically}} drops a static number of <b>consecutive</b> <b>speech</b> <b>frames</b> {{preceded by a}} given inter-loss gap size.|$|R
50|$|Whenever a good <b>speech</b> <b>frame</b> is {{detected}} the RX DTX handler shall pass directly to speech decoder.Whenever a lost speech or lost SID frames are detected the substitution or mutation shall be applied.Whenever a valid SID frame result in comfort noise generation.In case of invalid SID <b>frame</b> after <b>consecutive</b> <b>Speech</b> <b>frames</b> the last valid SID frame will be applicable.|$|R
30|$|The audio-signal {{from each}} {{participant}} was processed by a voice activity detector (VAD). The VAD reports a change to the SPEECH state each time it detected {{a certain number of}} <b>consecutive</b> <b>speech</b> <b>frames</b> whilst in the SILENCE state, and vice-versa. Based on these state transitions, gestures were triggered in the respective SynFace avatar.|$|R
40|$|In {{this paper}} a mixed-split scheme is {{proposed}} {{in the context of}} 2 -D DPCM based LSF quantization scheme employing split vector product VQ mechanism. Experimental evaluation shows that the new scheme is successfully being able to show better distortion performance than existing safety-net scheme for noisy channel even at considerably lower search complexity, by efficiently exploiting LSF trajectory behavior across the <b>consecutive</b> <b>speech</b> <b>frames...</b>|$|R
40|$|International audienceThis paper {{presents}} a new countermeasure {{for the protection}} of automatic speaker verification systems from spoofed, converted voice signals. The new countermeasure exploits the common shift applied to the spectral slope of <b>consecutive</b> <b>speech</b> <b>frames</b> involved in the mapping of a spoofer's voice signal towards a statistical model of a given target. While the countermeasure exploits prior knowledge of the attack in an admittedly unrealistic sense, it is shown to detect almost all spoofed signals which otherwise provoke significant increases in false acceptance. The work also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge...|$|R
40|$|Abstract-To achieve good {{reconstruction}} {{speech quality}} {{in a very}} low bit rate speech codecs, an efficient dimension reduction quantization scheme for the linear spectrum pair (LSP) parameters is proposed based on compressed sensing (CS). In the encoder, the LSP parameters extracted from <b>consecutive</b> <b>speech</b> <b>frames</b> are shaped into a high dimensional vector, and then the dimension of the vector is reduced by CS to produce a low dimensional measurement vector, the measurements are quantized using the split vector quantizer. In the decoder, according to the quantized measurements, the original LSP vector is reconstructed by the orthogonal matching pursuit method. Experimental {{results show that the}} scheme is more efficient than that of conventional matrix quantization scheme, the average spectral distortion reduction of up to 0. 23 dB is achieved in the DFT transform domain. Moreover, in the approximate KLT transform domain, this scheme can obtain transparent quality at 5 bits/frame with drastic bits reduction compared to other methods...|$|R
3000|$|... where NCS, NTS, NFS, and NTN {{denote the}} number of {{correctly}} detected <b>speech</b> <b>frames,</b> total <b>speech</b> <b>frames,</b> falsely detected <b>speech</b> <b>frames</b> in silence regions, and total silence frames, respectively. In these experiments, we set N [...]...|$|R
3000|$|... (as {{described}} after (12)) strongly {{depends on}} the voicing characteristics of <b>speech</b> <b>frames</b> and the input noise. Because of inherent periodicity of the voiced <b>speech</b> <b>frame,</b> the degree of cross-correlation between two voiced <b>speech</b> <b>frames</b> of a person becomes higher in comparison to that between two unvoiced <b>speech</b> <b>frames</b> which are random in nature. Regarding signal power, the ratio of power of a voiced <b>speech</b> <b>frame</b> and an unvoiced <b>speech</b> <b>frame</b> {{is found to be}} higher in comparison to that of the two voiced <b>speech</b> <b>frames.</b> As white Gaussian noise is considered, the degree of cross-correlation between the speech and noise is found to be negligible and the noise powers in two different frames may not differ significantly. As a result, the effect of input noise is found to be negligible on the power ratio.|$|R
30|$|From Table  3, we {{can observe}} that the {{estimated}} embedding rate {{is similar to}} the real value, and the variance is small. The AZCR-OED of the blended speech increases as the embedding rate decreases, which increases the threshold that distinguishes the stego and pure <b>speech</b> <b>frames.</b> Therefore, many pure <b>speech</b> <b>frames</b> were misjudged to be stego <b>speech</b> <b>frames,</b> which caused the embedding rate to be overestimated.|$|R
40|$|A {{method and}} device for extrapolating past signal-history data for {{insertion}} into missing data segments {{in order to}} conceal digital <b>speech</b> <b>frame</b> errors. The extrapolation method uses past-signal history that is stored in a buffer. The method is implemented with a device that utilizes a finite-impulse response (FIR) multi-layer feed-forward artificial neural network that is trained by back-propagation for one-step extrapolation of speech compression algorithm (SCA) parameters. Once a speech connection has been established, the speech compression algorithm device begins sending encoded <b>speech</b> <b>frames.</b> As the <b>speech</b> <b>frames</b> are received, they are decoded and converted back into speech signal voltages. During the normal decoding process, pre-processing of the required SCA parameters will occur and the results stored in the past-history buffer. If a <b>speech</b> <b>frame</b> is detected to be lost or in error, then extrapolation modules are executed and replacement SCA parameters are generated and sent as the parameters required by the SCA. In this way, the information transfer to the SCA is transparent, and the SCA processing continues as usual. The listener will not normally notice that a <b>speech</b> <b>frame</b> has been lost because of the smooth transition between the last-received, lost, and next-received <b>speech</b> <b>frames...</b>|$|R
5000|$|Zero insertion: {{the lost}} <b>speech</b> <b>frames</b> are {{replaced}} with zero ...|$|R
5000|$|... iLBC handles lost <b>frames</b> through graceful <b>speech</b> quality degradation. Lost frames {{often occur}} in {{connection}} with lost or delayed IP packets. Ordinary low-bitrate codecs exploit dependencies between <b>speech</b> <b>frames,</b> which cause errors to propagate when packets are lost or delayed. In contrast, iLBC-encoded <b>speech</b> <b>frames</b> are independent and so this problem will not occur.|$|R
30|$|The {{logarithmic}} {{energy is}} less distorted in a <b>speech</b> <b>frame</b> {{than in a}} non-speech frame.|$|R
3000|$|... {{represents}} the enhanced <b>speech</b> <b>frame.</b> The final enhanced speech signal is reconstructed {{by using the}} standard overlap-and-add method.|$|R
3000|$|Through {{extensive}} experimentation {{on different}} <b>speech</b> <b>frames,</b> {{it is found}} that the negligibility of the cross-correlation terms r [...]...|$|R
40|$|This paper {{presents}} a speech enhancement method {{based on the}} tracking and denoising of the formants of a linear prediction (LP) model of the spectral envelope of speech and the parameters of a harmonic noise model (HNM) of its excitation. The main advantages of tracking and denoising the prominent energy contours of speech are the efficient use of the spectral and temporal structures of successive <b>speech</b> <b>frames</b> and a mitigation of processing artefact known as the ‘musical noise’ or ‘musical tones’. The formant-tracking linear prediction (FTLP) model estimation consists of three stages: (a) speech pre-cleaning based on a spectral amplitude estimation, (b) formant-tracking across successive <b>speech</b> <b>frames</b> using the Viterbi method, and (c) Kalman filtering of the formant trajectories across successive <b>speech</b> <b>frames.</b> The HNM parameters for the excitation signal comprise; voiced/unvoiced decision, the fundamental frequency, the harmonics’ amplitudes and the variance of the noise component of excitation. A frequency-domain pitch extraction method is proposed that searches for the peak signal to noise ratios (SNRs) at the harmonics. For each <b>speech</b> <b>frame</b> several pitch candidates are calculated. An estimate of the pitch trajectory across successive frames is obtained using a Viterbi decoder. The trajectories of the noisy excitation harmonics across successive <b>speech</b> <b>frames</b> are modeled and denoised using Kalman filters. The proposed method is used to deconstruct noisy speech, de-noise its model parameters and then reconstitute speech from its cleaned parts. Experimental evaluations show the performance gains of the formant tracking, pitch extraction and noise reduction stages...|$|R
40|$|Line {{spectrum}} frequencies (LSF's) uniquely {{represent the}} {{linear predictive coding}} (LPC) filter of a <b>speech</b> <b>frame.</b> In many vocoders LSF's are used to encode the LPC parameters. In this paper, an interframe differential coding scheme is presented for the LSF's. The LSF's of the current <b>speech</b> <b>frame</b> are predicted by using both the LSF's of the previous frame {{and some of the}} LSF's of the current frame. Then, the difference resulting from prediction is quantized. © 1994 IEE...|$|R
30|$|For voiced <b>speech</b> <b>frames,</b> LPW will be {{designed}} to retain only the local peaks of the harmonic structure {{as shown in the}} bottom-right graph in Figure 6 (see also Figure 3 (d)) For unvoiced <b>speech</b> <b>frames,</b> the result will be almost flat {{due to the lack of}} local peaks with the target harmonic structure. Unlike the comb weights, the LPW is not uniform over the target frequencies and is more focused on the frequencies where harmonic structures are observed in the input spectrum.|$|R
40|$|Cataloged from PDF {{version of}} article. Line {{spectrum}} frequencies (LSF's) uniquely represent the {{linear predictive coding}} (LPC) filter of a <b>speech</b> <b>frame.</b> In many vocoders LSF's are used to encode the LPC parameters. In this paper, an inter-frame differential coding scheme is presented for the LSF's. The LSF's of the current <b>speech</b> <b>frame</b> are predicted by using both the LSF's of the previous frame {{and some of the}} LSF's of the current frame. Then, the difference resulting from prediction is quantized...|$|R
30|$|From Fig.  5 a, we {{can observe}} that the AZCR-OED values {{of most of}} the stego <b>speech</b> <b>frames</b> are less than the value for the entire blended speech signal, and the AZCR-OED values {{of most of the}} pure <b>speech</b> <b>frames</b> are greater than the value for the entire blended speech signal. Comparing panels (a) and (b) of Fig.  5, we see that the {{estimated}} hidden location of the secret speech is similar to its actual hidden location. These experimental results demonstrate the effectiveness of the algorithm.|$|R
40|$|Abstract — In {{this paper}} a new {{efficient}} feature extraction methods for speech recognition have been proposed. The features are obtained from Cepstral Mean Normalized reduced order Linear Predictive Coding (LPC) coefficients {{derived from the}} <b>speech</b> <b>frames</b> decomposed using Discrete Wavelet Transform (DWT). In the literature {{it is assumed that}} the <b>speech</b> <b>frame</b> of size 10 msec to 30 msec is stationary, however, in practice different parts of the speech signal may convey different amount of information (hence may not be perfectly stationary). LPC coefficients derived from wavelet-decomposed subbands of <b>speech</b> <b>frame</b> provide better representation than modeling the frame directly. Experimentally it has been shown that, the proposed approach provides effective (better recognition rate), efficient (reduced feature vector dimension) features. The speech recognition system using the Continuous Density Hidden Markov Model (CDHMM) has been implemented. The proposed algorithms were evaluated using isolated Marathi digits database in presence of white Gaussian noise...|$|R
40|$|Abstract: The {{process of}} speech {{production}} in the human system is very complex, possesses nonlinearities, and can only be precisely modeled in terms of nonlinear dynamics. A non-linear speech classification approach is proposed, which classifies speech based on features extracted from Takens’ Method of Delays, a technique used to reconstruct signals into a trajectory in multi-dimensional state space. In this research, two types of speech detection are presented, namely, voiced and usable speech (for speaker identification purposes). The proposed approach {{has been able to}} yield a probability of error of 12 % in noisy environments for voiced speech detection, and 78 % correct usable speech detection by comparing the structures of embedded voiced <b>speech</b> <b>frames</b> with embedded unvoiced <b>speech</b> <b>frames,</b> and embedded usable <b>speech</b> <b>frames</b> with embedded unusable speech. Some applications of this speech detection technique include the enhancement of speaker identification and speech recognition systems. 1...|$|R
40|$|In this paper, a {{new feature}} {{selection}} method for speaker recognition is proposed {{to keep the}} high quality <b>speech</b> <b>frames</b> for speaker modelling and to remove noisy and corrupted <b>speech</b> <b>frames.</b> In order to obtain robust voice activity detection in variety of acoustic conditions, the spectral subtraction algorithm is adopted to estimate the frame power. An energy based frame selection algorithm is then applied to indicate the speech activity at the frame ame level. The eigenchannel based GMM-UBM speaker peaker recognition system is used to evaluate this proposed method. The experiments are conducted on the 2006 NIST ST Speaker ker Recognition Evaluation core test condition on (telephone hone channel) as well as microphone channel test condition. dition. It demonstrates that this approach can provide vide an efficient ficient way to select high quality <b>speech</b> <b>frames</b> in the noisy environment for speaker recognition. on. n. Index term- speaker recognition, voice activity detection, feature selection, on, n, spectral subtraction, noise reductio...|$|R
30|$|Step 2 : {{threshold}} process. In DDBSE, the entropy {{of signal}} y(t,k) {{will be used to}} detect the <b>speech</b> <b>frame.</b> The entropy is calculated by considering the amplitudes of the R continuous frames in each frequency point. Just as Fig.  1 shows, although the energy of clean speech signal (denoted by a red line) is much greater than that in the frames nearby, there are still lots of interference caused by noise. In order to detect the <b>speech</b> <b>frame</b> more clearly, a local threshold processing-based approach is adopted, which processes the amplitude of the R continuous frames, tmp(t, k), as follows.|$|R
50|$|The modem timings {{are also}} relevant, in that each <b>speech</b> vocoder <b>frame</b> outputs 28-bits every 40 ms. Since the modem has an 80 ms modem frame, it can {{transport}} two <b>speech</b> vocoder <b>frames.</b>|$|R
50|$|Background noise spikes {{can often}} be {{confused}} with the <b>speech</b> <b>frame</b> and hence, in order to nullify this issue, a check list for SID computation is Nelapsed >23, old SID is utilized with VAD=0.|$|R
30|$|Based on the {{evaluation}} results, {{in the sense}} of the energy test, the copula-based distributions using IFM method were mostly overcome by conventional distributions in the second experimental setup. As only one of parameter estimation methods of copula-based distribution, IFM method, was taken into account in the experimental evaluation, and the IFM method ends up a sub-optimal solution for parameter estimation, it is difficult to have a generic conclusion on copula-based distribution’s benefit in statistical modeling of <b>speech</b> <b>frame.</b> One of future work perspective might therefore be to study the power of statistical modeling of copula-based distribution of <b>speech</b> <b>frame</b> using optimal parameter estimation methods.|$|R
40|$|A {{sometimes}} annoying {{problem in}} the most internationally widespread cellular telephone system, the GSM system, is an interfering signal generated by the switching nature of TDMA cellular telephone system. A humming noise originating from the <b>speech</b> <b>frames,</b> equivalent to 160 samples of data corresponding to 20 ms at 8 kHz sampling rate is sometimes clearly audible. This paper describes a study of two different software solutions designed to suppress such interference internally in the mobile handset. The methods are Notch Filtering, which is performed on a sample-per-sample basis, and <b>Speech</b> <b>Frame</b> Noise Cancellation, which is an alternative method employing correlators and subtraction, similar to Active Noise Control [2, 3]. 1...|$|R
40|$|In this paper, a text {{dependent}} speaker recognition algorithm {{based on}} spectrogram is proposed. The spectrograms have been generated using Discrete Fourier Transform for varying frame sizes with 25 % and 50 % overlap between <b>speech</b> <b>frames.</b> Feature vector extraction {{has been done}} by using the row mean vector of the spectrograms. For feature matching, two distance measures, namely Euclidean distance and Manhattan distance have been used. The results have been computed using two databases: a locally created database and CSLU speaker recognition database. The maximum accuracy is 92. 52 % for an overlap of 50 % between <b>speech</b> <b>frames</b> with Manhattan distance as similarity measure...|$|R
3000|$|... [k]| increases. Therefore, for a noise-corrupted utterance, the {{logarithmic}} magnitude {{spectrum of}} the <b>speech</b> <b>frame</b> is often less vulnerable to noise {{than that of the}} non-speech (noise-only) frame. However, this condition does not hold for the (linear) magnitude spectrum.|$|R
30|$|The {{recognition}} accuracy improves as {{the value}} α is increased from 0 to 0.6, and the additional {{improvement in accuracy}} is 4.80 % (from 72.26 % to 77.06 %). Therefore, amplifying the magnitude spectrum of the <b>speech</b> <b>frames</b> correctly is helpful.|$|R
40|$|A {{nonlinear}} Hammerstein {{model is}} proposed for coding speech signals. Using Tsay’s nonlinearity test, we first {{show that the}} great majority of <b>speech</b> <b>frames</b> contain nonlinearities (over 80 % in our test data) when using 20 -millisecond <b>speech</b> <b>frames.</b> Frame length correlates with the level of nonlinearity: the longer the frames the higher the percentage of nonlinear frames. Motivated by this result, we present a nonlinear structure using a frame-by-frame adaptive identification of the Hammerstein model parameters for speech coding. Finally, the proposed structure is compared with the LPC coding scheme for three phonemes /a/, /s/, and /k/ by calculating the Akaike information criterion of the corresponding residual signals. The tests show clearly that the residual of the nonlinear model presented in this paper contains significantly less information compared to that of the LPC scheme. The presented method is a potential tool to shape the residual signal in an encode-efficient form in speech coding...|$|R
3000|$|... [...]). In {{practical}} situations, the <b>speech</b> <b>frames</b> is {{of length}} n= 180 – 220 {{and if we}} choose the compression rate as 5 %, the length of compressed signal is m= 9 – 11. Therefore, the order of magnitude of the keyspace is about 102000.|$|R
30|$|The ASR {{subsystem}} {{is based}} on the Kaldi open-source toolkit [81] and employs the DNN-based acoustic models. Specifically, a DNN-based context-dependent speech recognizer is trained following the DNN training approach presented in [95]. Forty-dimensional MFCCs, which are augmented with three pitch- and voicing-related features [96] and appended with their delta and double delta coefficients, are firstly extracted for each <b>speech</b> <b>frame.</b> The DNN has 6 hidden layers with 2048 neurons each. Each <b>speech</b> <b>frame</b> is spliced across ±[*] 5 frames to produce 1419 -dimensional vectors that are the input into the first layer. The output layer is a soft-max layer representing the log-posteriors of the context-dependent HMM states. The Kaldi LVCSR decoder generates word lattices [97] using these DNN-based acoustic models.|$|R
40|$|Building on {{algorithms}} {{developed in}} earlier work (Hawkins et al., 1994 a, 1994 b; Hawkins, 1997; Hawkins et al., 2002), this study develops {{a new technique}} for improving the accuracy of formant estimates produced by an analysis-by-synthesis formant tracker (DPTRAK, Clermont, 1992). DPTRAK is evaluated by comparing its formant estimates against those obtained manually by the first author when he inspected the spectrogram of each vowel produced by each speaker. Applied to 13 male speakers uttering the 11 monophthongs and eight diphthongs of Australian English, DPTRAK produced results that varied in accuracy across speakers. The percentage of <b>speech</b> <b>frames</b> tracked accurately varied from 99 % for the best speaker through to 58 % for the worst speaker. We develop the SpeechSifter algorithm to sift through the <b>speech</b> <b>frames</b> tracked by the DPTRAK formant tracker (or any other formant tracker) and select only those frames {{that are likely to}} be accurately tracked. This unsupervised algorithm first selects the ideal speaker on which to train a Replicator Neural Net (Hawkins et al., 2002). The trained Replicator Neural Net is then used to screen those <b>speech</b> <b>frames</b> on which the formant tracker is highly likely to have made accurate formant estimates and to discard the rest. We demonstrate the value of this approach. First, we demonstrate that we can accurately predict which speaker will provide the ideal training speaker for the RNN. Next, we apply the trained RNN to a speaker and show that {{that it is possible to}} achieve a 90 % accuracy rate whilst retaining 75 % of the speaker’s original <b>speech</b> <b>frames.</b> This is an improvement on the DPTRAK algorithm which achieves an accuracy rate of only 81 % for this speaker. 1...|$|R
30|$|Energy of each {{short-time}} <b>speech</b> <b>frame</b> in {{the recording}} is classified as either speech or silence using the likelihood ratio test (LRT). Because the test treats each frame independently, a second processing step is used where silence and speech segments that were shorter than four frames are removed.|$|R
