130|211|Public
500|$|Valve still recognizes it has {{a problem}} with what it calls [...] "fake games", those that are built around reused assets and little other innovation, {{designed}} only to generate profit from unsuspecting users. To help assist finding and removing these games from the service, the company plans to add Steam Explorers atop its existing Steam Curator program, according to various YouTube personalities that have spoken out about such games in the past and with Valve directly, including Jim Sterling and TotalBiscuit. Any Steam user is able to sign up to be an Explorer, and are asked to look at under-performing games on the service as to either vouch that the game is truly original and simply lost among other releases, or if it {{is an example of a}} [...] "fake game", at which point Valve can take action to remove the game. Valve also made changes to the trading card system in May 2017 to prevent abuse by [...] "fake games". Valve found that some of the [...] "bad actors" [...] that released these games with trading card support then distributed game codes to thousands of bot-operated accounts that would run the game to earn trading cards that they could then sell for profit; these games would also create false positives that make these titles appear more popular than they really were and would impact games suggested to legitimate players through their store algorithms. Subsequent to this patch, games must reach some type of <b>confidence</b> <b>factor</b> based on actual playtime before they can generate trading cards, with players credited for their time played towards receiving trading cards before this metric is met.|$|E
5000|$|... 2. The <b>confidence</b> <b>factor</b> δ is {{an element}} (a truth-value) of L; ...|$|E
50|$|Briles is {{an author}} of over thirty books {{including}} Show Me About Book Publishing with Rick Frishman and John Kremer, The <b>Confidence</b> <b>Factor</b> and Stabotage! She also travels internationally as a motivational speaker.|$|E
40|$|Stochastic multicriteria {{acceptability}} analysis (SMAA) is {{a family}} of methods for aiding multicriteria group decision making. These methods are based on exploring the weight space in order to describe the preferences that make each alternative the most preferred one, or that give a certain rank for a specific alternative. The main results of the analysis are rank acceptability indices, central weight vectors and <b>confidence</b> <b>factors</b> for different alternatives. The rank acceptability indices describe the variety of different preferences resulting in a certain rank for an alternative; the central weight vectors describe the typical preferences favoring each alternative; and the <b>confidence</b> <b>factors</b> measure whether the criteria data is sufficiently accurate for making an informed decision. In some cases, when the problem involves {{a large number of}} efficient alternatives, the analysis may fail to discriminate between them. This situation is revealed by low <b>confidence</b> <b>factors.</b> In this paper we develop socalled cross <b>confidence</b> <b>factors,</b> which are based on computing <b>confidence</b> <b>factors</b> for alternatives using each other’s central weight vectors. The cross <b>confidence</b> <b>factors</b> can be used for classifying efficient alternatives into sets of similar and competing alternatives. These sets are related to the concept of reference sets in Data Envelopment Analysis (DEA), but generalized for stochastic models. Forming these sets is useful when trying to identify one or more most preferred alternatives, or suitable compromise alternatives. The reference sets can also be used for evaluating whether criteria need to be measured more accurately, and at which alternatives the measurements should be focused. This may cause considerable savings in measurement costs. We demonstrate the use of the cross <b>confidence</b> <b>factors</b> and reference sets using a real-life example...|$|R
40|$|The recent European codes such as Euro Code 8 seem to {{synthesize}} {{the effect of}} structural modeling uncertainties in the so-called <b>confidence</b> <b>factors</b> (CF) that are applied to mean material property values. However, {{the effect of the}} application of the <b>confidence</b> <b>factors</b> on structural reliability is not explicitly stated. An alternative approach featured in the SAC-FEMA guidelines, considers the effect of both ground motion uncertainty and the structural modeling uncertainties on the global performance of the structure, in a closed-form analytical safety-checking format. This work strives to have a critical look at the <b>confidence</b> <b>factors</b> {{from the point of view}} of the characterization of uncertainties and structural reliability assessment. Moreover, an efficient Bayesian method is presented that can estimate both the robust structural reliability and also the joint probability distributions for structural fragility parameters, based on a small sample of structural model realizations and ground motion records. Based on findings featured in this work, a set of perspectives for the future European codes are outlined...|$|R
40|$|Stochastic multicriteria {{acceptability}} analysis (SMAA) is {{a family}} of methods for aiding multicriteria group decision making in problems with inaccurate, uncertain, or missing information. These methods are based on exploring the weight space in order to describe the preferences that make each alternative the most preferred one, or that would give a certain rank for a specific alternative. The main results of the analysis are rank acceptability indices, central weight vectors and <b>confidence</b> <b>factors</b> for different alternatives. The rank acceptability indices describe the variety of different preferences resulting in a certain rank for an alternative, the central weight vectors represent the typical preferences favouring each alternative, and the <b>confidence</b> <b>factors</b> measure whether the criteria measurements are sufficiently accurate for making an informed decision. [URL]...|$|R
5000|$|A {{definition}} match algorithm {{was created}} to automatically merge the correct meanings of ambiguous words between the two online resources, based on the words that the definitions of those meanings have in common in LDOCE and WordNet. Using a similarity matrix, the algorithm delivered matches between meanings including a <b>confidence</b> <b>factor.</b> This algorithm alone, however, did not match all meanings correctly on its own.|$|E
5000|$|In {{analytical}} chemistry, {{the detection}} limit, lower limit of detection, or LOD (limit of detection), {{is the lowest}} quantity of a substance that can be distinguished from the absence of that substance (a blank value) within a stated confidence limit (generally 1%). [...] The detection limit is estimated from {{the mean of the}} blank, the standard deviation of the blank and some <b>confidence</b> <b>factor.</b> Another consideration that affects the detection limit is the accuracy of the model used to predict concentration from the raw analytical signal.|$|E
5000|$|Zeus is a {{computer}} program developed by End Game Technologies that models and predicts the outcomes of coaching decisions in American football games. The {{program is designed to}} produce statistical outputs showing the expected odds of winning given choices of potential play calls and roster choices. It is capable of simulating 1,000,000 games in a few seconds, and was developed using extensive research on historical data from National Football League games. It expresses its output in terms of Game Winning Chance, which is the probability that the specified team will win the current game. It also evaluates its output using a range of possible [...] "worst case" [...] scenarios to determine a <b>Confidence</b> <b>Factor</b> of 1 to 10 for its results.|$|E
40|$|Field {{failure rates}} and <b>confidence</b> <b>factors</b> are {{presented}} for 88 identifiable {{components of the}} ground support equipment at the John F. Kennedy Space Center. For most of these, supplementary information regarding failure mode and cause is tabulated. Complete reliability assessments are included for three systems, eight subsystems, and nine generic piece-part classifications. Procedures for updating or augmenting the reliability results are also included...|$|R
40|$|Goal-based safety {{standards}} {{are now a}} reality. As well as confidently satisfying the mandatory requirements laid down in these standards, {{there are a number}} of other secondary factors that influence the confidence a regulator or assessor has in any safety case produced. These factors include novelty of argument approach, experience of stakeholders and scale of the system and safety case. Currently, the certainty with which requirements are satisfied and the consideration of the other <b>confidence</b> <b>factors</b> often remains implicit within the certification process. In a goal-based safety case regime, users and regulators require intelligent customers who are aware of these issues and can explicitly consider them during the production of their safety case to increase the confidence of all stakeholders involved. Standards, guidance and other publications have covered in detail the structure and content of safety cases and this paper does not intend to repeat this information. Instead, this paper brings together and discusses the other <b>confidence</b> <b>factors</b> and approaches to managing them within the safety case development process. ...|$|R
40|$|The {{issue of}} <b>confidence</b> <b>factors</b> in Knowledge Based Systems has become {{increasingly}} important and Dempster-Shafer (DS) theory {{has become increasingly}} popular {{as a basis for}} these factors. This paper discusses the need for an empirical lnterpretatlon of any theory of <b>confidence</b> <b>factors</b> applied to Knowledge Based Systems and describes an empirical lnterpretatlon of DS theory suggesting that the theory has been extensively misinterpreted. For the essentially syntactic DS theory, a model is developed based on sample spaces, the traditional semantic model of probability theory. This model is used to show that, if belief functions are based on reasonably accurate sampling or observation of a sample space, then the beliefs and upper probabilities as computed according to DS theory cannot be interpreted as frequency ratios. Since many proposed applications of DS theory use belief functions in situations with statistically derived evidence (Wesley [1]) and seem to appeal to statistical intuition to provide an lnterpretatlon of the results as has Garvey [2], it may be argued that DS theory has often been misapplied. Comment: Appears in Proceedings of the First Conference on Uncertainty in Artificial Intelligence (UAI 1985...|$|R
50|$|In 2009 a United States Special Operations Command {{market survey}} called for 1 MOA (0.3 mrad) extreme {{vertical}} spread for all shots in a 10-round group fired at targets at 300, 600, 900, 1,200 and 1,500 meters.The 2009 Precision Sniper Rifle requirements {{stated that the}} PSR when fired without suppressor shall provide a <b>confidence</b> <b>factor</b> of 80% that the weapon and ammunition combination is capable of holding 1 MOA extreme vertical spread. This shall be calculated from 150 ten (10) round groups that were fired unsuppressed. No individual group shall exceed 1.5 MOA (0.5 mrad) extreme vertical spread. All accuracy will be taken at the 1,500 meter point.Other requirements were that the rifle weigh less than 18 pounds loaded, have Picatinny rails, and have an easily changeable barrel.|$|E
5000|$|A 2008 United States {{military}} {{market survey}} for a Precision Sniper Rifle (PSR) calls for 1 MOA (0.3 mrad) extreme vertical spread for all shots in a 5-round group fired at targets at 300, 600, 900, 1,200 and 1,500 meters. In 2009 a United States Special Operations Command market survey calls for 1 MOA (0.28 mrad) extreme vertical spread for all shots in a 10-round group fired at targets at 300, 600, 900, 1,200 and 1,500 meters.The 2009 Precession Sniper Rifle requirements {{state that the}} PSR when fired without suppressor shall provide a <b>confidence</b> <b>factor</b> of 80% that the weapon and ammunition combination is capable of holding 1 MOA (0.28 mrad) extreme vertical spread. This shall be calculated from 150 ten (10) round groups that were fired unsuppressed. No individual group shall exceed 1.5 MOA (0.42 mrad) extreme vertical spread. All accuracy will be taken at the 1,500 meter point.In 2008 the US military adopted the M110 Semi-Automatic Sniper System which has corresponding maximum allowed extreme spread of 1.8 MOA (0.5 mrad) for a 5-shot group on 300 feet, using M118LR ammunition or equivalent. In 2010 the maximum bullet dispersion requirement for the M24 [...]300 Winchester Magnum corresponds to 1.4 MOA (0.39 mrad) extreme spread for 5 shot group on 100 meters.In 2011, the US military adapted the [...]300 Winchester Magnum M2010 Enhanced Sniper Rifle that has to meet an accuracy requirement to fire ≤ 1 MOA/0.28 mrad (less than a 2-inch shot group at 200 yards) before being released for fielding.|$|E
3000|$|... is the <b>confidence</b> <b>factor</b> {{described}} previously. It {{is obvious}} that {{as the number of}} interactions (and thus the <b>confidence</b> <b>factor,</b> C) increases, the direct trust value becomes more significant than the reputation information.|$|E
40|$|One of {{the most}} {{challenging}} aspects of the seismic assessment of existing buildings is the characterization of structural modeling uncertainties. Recent codes, such as Eurocode 8, seem to synthesize the effect of structural modeling uncertainties in the so-called <b>confidence</b> <b>factors</b> that are applied to mean material property estimates. The <b>confidence</b> <b>factors</b> are classified and tabulated as a function of discrete knowledge levels acquired {{based on the results of}} specific in-situ tests and inspections. In this approach, the effect of the application of the <b>confidence</b> <b>factors</b> on structural assessment is not explicitly stated. This work presents probabilistic performance-based proposals for seismic assessments of RC buildings based on the knowledge levels. These proposals take advantage of the Bayesian framework for updating the probability distributions for structural modeling parameters based on the results of tests and inspections. As structural modeling parameters, both the mechanical material properties and also the structural detailing parameters are considered. These proposals can be categorized based both on the amount of structural analysis effort required and on the type of structural analysis performed. An efficient Bayesian method is presented which relies on simplified assumptions and employs a small sample of structural model realizations and ground motion records in order to provide an estimate of structural reliability. As an alternative proposal suitable for code implementation, the simplified approach implemented in the SAC-FEMA guidelines is adapted to existing structures by employing the efficient Bayesian method. This method takes into account the effect of both ground motion uncertainty and the structural modeling uncertainties on the global performance of the structure, in a closed-form analytical safety-checking format. These alternative proposals are demonstrated for the case study structure which is an existing RC frame. In particular, it is shown how the parameters for the safety-checking format can be estimated and tabulated as a function of knowledge level, outcome of tests, and the type of structural analysis adopted...|$|R
40|$|The {{confidence}} that eyewit-nesses express in their lineup identifications of criminal sus-pects {{has a large}} impact on criminal proceedings. Many convictions of innocent people can be attributed {{in large part to}} confident but mistaken eye-witnesses. Although reason-able correlat ions between confidence and accuracy can be obtained under certain con-ditions, confidence is governed by some factors that are unre-lated to accuracy. An under-standing of these <b>confidence</b> <b>factors</b> helps establish the con-ditions under which confi-dence and accuracy are related and leads to important practi-cal recommendations for crimi-nal justice proceedings...|$|R
30|$|Moustapha and Selmic [324] detect faulty nodes in a WSN using RNN. The nodes in the RNN {{hidden layers}} model sensor nodes in WSN, while the weights {{on the edges}} are based on <b>confidence</b> <b>factors</b> of the {{received}} signal strength indicators (RSSI). Whereas, {{the output of the}} RNN is an approximation of the operation of the WSN. Fault detection is achieved by identifying discrepancies between approximated and real WSN values. The RNN successfully detect faults, without early false alarms, for a small scale WSN with 15 sensors and synthetically introduced faults.|$|R
40|$|A {{method for}} {{evaluating}} the energy flow confidence level in vibrating systems with randomly perturbed parameters is presented. The energy flow is predicted {{in terms of the}} mobilities of resonant subsystems or by the solution of the velocity wave field for non-resonant subsystems. The statistical moments of the energy flow are calculated by a perturbation technique and a <b>confidence</b> <b>factor</b> is defined as the ratio between mean and standard deviation. The properties of the <b>confidence</b> <b>factor</b> are investigated by a theoretical analysis as a function of frequency. Three cases are studied to compare the <b>confidence</b> <b>factor</b> obtained theoretically with a prediction provided by a Monte-Carlo simulation...|$|E
3000|$|... 10, and so H̃_ 1 is {{accepted}} against H̃_ 0 with <b>confidence</b> <b>factor</b> CF= 0.328 / 0.185 + 0.328 = 0.639. In other words, considering the <b>confidence</b> <b>factor</b> 0.639, one can {{assert that the}} mean absorption Cd in the lower radish parts does not coincides with the proposed amounts by Pais and Benton [12] {{and so it is}} not suitable.|$|E
3000|$|..., and γ> 0 is a <b>{{confidence}}</b> <b>factor.</b> The {{bigger the}} γ> 0 is, the higher our confidence {{is on the}} supervision sample.|$|E
40|$|The SPARTA Embedded Expert System (SEES) is an {{intelligent}} health monitoring system that directs analysis by placing <b>confidence</b> <b>factors</b> on possible engine status and then recommends {{a course of}} action to an engineer or engine controller. The technique can prevent catastropic failures or costly rocket engine down time because of false alarms. Further, the SEES has potential as an on-board flight monitor for reusable rocket engine systems. The SEES methodology synergistically integrates vibration analysis, pattern recognition and communications theory techniques with an artificial intelligence technique - the Embedded Expert System (EES) ...|$|R
50|$|The Scale of Protective Factors (SPF) is {{a measure}} of aspects of social relationships, {{planning}} behaviors and <b>confidence.</b> These <b>factors</b> contribute to psychological resilience in emerging adults and adults.|$|R
40|$|The {{adoption}} of microarray techniques in biological and medical research provides {{a new way}} for cancer diagnosis and treatment. In order to perform successful {{diagnosis and treatment of}} cancer, discovering and classifying cancer types correctly is essential. Class discovery {{is one of the most}} important tasks in cancer classification using biomolecular data. Most of the existing works adopt single clustering algorithms to perform class discovery from biomolecular data. However, single clustering algorithms have limitations, which include a lack of robustness, stability, and accuracy. In this paper, we propose a new cluster ensemble approach called knowledge based cluster ensemble (KCE) which incorporates the prior knowledge of the data sets into the cluster ensemble framework. Specifically, KCE represents the prior knowledge of a data set in the form of pairwise constraints. Then, the spectral clustering algorithm (SC) is adopted to generate a set of clustering solutions. Next, KCE transforms pairwise constraints into <b>confidence</b> <b>factors</b> for these clustering solutions. After that, a consensus matrix is constructed by considering all the clustering solutions and their corresponding <b>confidence</b> <b>factors.</b> The final clustering result is obtained by partitioning the consensus matrix. Comparison with single clustering algorithms and conventional cluster ensemble approaches, knowledge based cluster ensemble approaches are more robust, stable and accurate. The experiments on cancer data sets show that: 1) KCE works well on these data sets; 2) KCE not only outperforms most of the state-of-the-art single clustering algorithms, but also outperforms most of the state-of-the-art cluster ensemble approaches. Department of Computin...|$|R
30|$|The {{corresponding}} {{high frequency}} {{content of the}} predictor block {{is added to the}} block of the WZ frame, after scaling it by a <b>confidence</b> <b>factor.</b>|$|E
40|$|Wi-Fi {{fingerprinting}} {{has been}} a popular indoor positioning technique with the advantage that infrastructures are readily available in most urban areas. However wireless signals are prone to fluctuation and noise, introducing errors in the final positioning result. This paper proposes a new fingerprint training method where a number of users train collaboratively and a <b>confidence</b> <b>factor</b> is generated for each fingerprint. Fingerprinting is carried out where potential fingerprints are extracted based on the <b>confidence</b> <b>factor.</b> Positioning accuracy improves by 40 % when the new fingerprinting method is implemented and maximum error is reduced by 35 %...|$|E
40|$|The Fuzzy hyperline segment {{neural network}} (FHLSNN) is {{supervised}} classifier that forms n-dimensional hyperline segments (HLS) defined by two end points with a corresponding membership function {{for learning and}} testing. In this paper, the Pruned fuzzy hyperline segment neural network (PFHLSNN) and Pruned modified fuzzy hyperline segment neural network (PMFHLSNN) are proposed. The pruning method {{is based on a}} <b>confidence</b> <b>factor</b> calculated for each hyperline segment in the prediction phase after learning. The new definition of <b>confidence</b> <b>factor</b> is proposed. In PFHLSNN, the hyperline segments with low <b>confidence</b> <b>factor</b> are pruned using user defined threshold to reduce the network complexity. In order to improve the classification performance of PFHLSNN, the modification is proposed in its testing phase and the network is referred as PMFHLSNN. In this modification, the Euclidean distance is computed between the applied input pattern and the centroid of the patterns falling on the hyperline segment to decide the class of pattern. Finally, the HLS with smallest distance is selected as winner and the pattern is so classified that it belongs to the class associated with that HLS. The performance of PFHLSNN and PMFHLSNN is evaluated using benchmark problems and real world handwritten character recognition data set. The results are analyzed, discussed and compared with the FHLSNN. Thus, the proposed approach improved the classification accuracy without affecting the incremental learning of FHLSNN and reduces the network complexity by pruning the hyperline segments of low <b>confidence</b> <b>factor...</b>|$|E
40|$|AbstractBilattices, due to M. Ginsberg, are {{a family}} of truth-value spaces that allow elegantly for missing or {{conflicting}} information. The simplest example is Belnap's four-valued logic, based on classical two-valued logic. Among other examples are those based on finite many-valued logics and on probabilistic-valued logic. A fixed-point semantics is developed for logic programming, allowing any bilattice as the space of truth values. The mathematics is little more complex than in the classical two-valued setting, but the result provides a natural semantics for distributed logic programs, including those involving <b>confidence</b> <b>factors.</b> The classical two-valued and the Kripke-Kleene three-valued semantics become special cases, since the logics involved are natural sublogics of Belnap's logic, the logic given by the simplest bilattice...|$|R
40|$|We extract {{relevant}} and informative audio-visual features using multiple multi-class Support Vector Machines with probabilistic outputs, and demonstrate the approach in a noisy audio-visual speech reading scenario. We first extract visual spatio-temporal features and audio cepstral coefficients from pronounced digit se-quences. Two classifiers are then trained {{on a single}} modality to obtain <b>confidence</b> <b>factors</b> {{that are used to}} select the most appropri-ate fusion strategy. A final classifier is trained on the joint audio-visual feature space and used to recognize digits. We demonstrate the proposed approach on a standard database and compare it with alternative methods. The evaluation shows that the proposed ap-proach outperforms the alternatives both in terms of recognition accuracy and in terms of robustness. ...|$|R
40|$|While the {{advantages}} of flexible business processes have been highly recognized by the academia and organizations, the research focus has recently shifted to its trade-offs and how the negative consequences could be minimized. This research addressed the problem consisting of the difficulties encountered by process participants when interacting with flexible process aware information systems. In order to overcome these difficulties, several approaches for guiding or supporting the process participants during enactment based on process mining have been proposed. However, these solutions lacked the suitable semantics for human’s reasoning and decision making during enactment as they provided recommendations at a low granularity level. Consequently, {{the objective of this}} research was twofold. First, the implications of integrating flexible processes into process aware information systems for agents (process participant and process administrator) were analyzed through a systematic literature study. Secondly, in the settings of design science research, two artifacts were created to solve the problematic situation: an innovative process mining technique aimed at discovering the intentional model of the executable process in an unsupervised manner, and a recommendation tool formulating recommendations as intentions and <b>confidence</b> <b>factors,</b> based on partial traces and probabilistic calculus. The artifacts were evaluated in a case study with a Childcare application supporting flexible process enactment with a data-driven approach. The experiments revealed that the intention mining technique had a precision of 0. 7 in discovering the correct intentions. Regarding the recommendation tool, the majority of the participants agreed on the improved support for decision making, offered by the recommendations given as intentions in comparison to recommendations given as activities, while a majority disagreed on the utility of the <b>confidence</b> <b>factors</b> attached to each recommendation...|$|R
40|$|An {{approach}} {{to evaluate the}} <b>confidence</b> <b>factor</b> for structures {{at the end of}} an interval of time is proposed. The <b>confidence</b> <b>factor</b> indicates the adequacy of the performance level of a structure subjected to external loads. The factor considers the uncertainties implicit in the structural capacity and in the structural demand. The formulation is made in accordance with the Demand and Capacity Factor Design Format. Four scenarios are compared: a) structural capacity deteriorates over a time interval, while structural demand remains constant, b) only structural demand (for a given intensity) varies in time, c) both structural capacity and structural demand vary simultaneously in time, and d) the effect of structural deterioration is neglected. The approach is applied to an offshore jacket platform. Deterioration is taken into account by analyzing the growth of fatigue cracks in both ends of several critical structural elements. It is concluded that for the evaluation of the <b>confidence</b> <b>factor</b> over an interval of interest, for the case analyzed, it is more significant to consider the variation in time of the structural capacity rather than that of the structural demand; however, it is recommended to consider both (structural capacity and structural demand) in the analysis...|$|E
3000|$|... 10, and so {{we accept}} H̃_ 1 against H̃_ 0 with <b>confidence</b> <b>factor</b> CF= 0.572 / 0.156 + 0.572 = 0.786. Note that {{on the basis of}} the {{classical}} p-value method, one accepts H̃_ 0 against H̃_ 1 at any significance level α<p [...]...|$|E
30|$|Differently from {{previous}} works [16 – 18], we are adding high frequency content. We {{want to avoid}} adding noise in cases where a match is not very close. Hence, we use a <b>confidence</b> <b>factor</b> to scale the high-frequency contents before being added to the LR block.|$|E
40|$|This work {{introduces}} a rigorous uncertainty quantification framework that exploits concentration–of–measure inequalities to bound failure probabilities using a well-defined certification campaign regarding {{the performance of}} engineering systems. The framework is constructed {{to be used as}} a tool for deciding whether a system is likely to perform safely and reliably within design specifications. Concentration-of-measure inequalities rigorously bound probabilities-of-failure and thus supply conservative certification criteria, in addition to supplying unambiguous quantitative definitions of terms such as margins, epistemic and aleatoric uncertainties, verification and validation measures, and <b>confidence</b> <b>factors.</b> This methodology unveils clear procedures for computing the latter quantities by means of concerted simulation and experimental campaigns. Extensions to the theory include hierarchical uncertainty quantification, and validation with experimentally uncontrollable random variables. v Acknowledgment...|$|R
40|$|International audienceBesides the {{benefits}} of flexible processes, practical implementations of process aware information systems have also revealed difficulties encountered by process participants during enactment. Several support and guidance solutions based on process mining have been proposed, but they lack a suitable semantics for human reasoning and decisions making as they mainly rely on low level activities. Applying design science, we created FlexPAISSeer, an intention mining oriented approach, with its component artifacts: 1) IntentMiner which discovers the intentional model of the executable process in an unsupervised manner; 2) In-tentRecommender which generates recommendations as intentions and <b>confidence</b> <b>factors,</b> based on the mined intentional process model and probabilistic calculus. The artifacts were evaluated in a case study with a Netherlands software company, using a Childcare system that allows flexible data-driven process enactment...|$|R
40|$|This paper {{proposes a}} formal {{approach}} of constructing shared mental models between computational improvisational agents (improv agents) and human interactors {{based on our}} socio-cognitive studies of human improvisers. Creating shared mental models helps improv agents co-create stories {{with each other and}} interactors in real-time interactive narrative experiences. The approach described here allows flexible modeling of non-Boolean (i. e. fuzzy) knowledge about scene and background concepts through the use of fuzzy rules and <b>confidence</b> <b>factors</b> in order to allow reasoning under uncertainty. It also allows improv agents to infer new knowledge about a scene from existing knowledge, recognize when new knowledge may be divergent from the other actor’s mental model, and attempt to resolve this divergence to reach cognitive consensus despite the absence of explicit goals in the story environment...|$|R
