135|143|Public
500|$|Although it wagered only $947 on the clue, Watson was {{the only}} {{contestant}} to miss the Final Jeopardy! response in the category U.S. CITIES ("Its largest airport was named for a World War II hero; its second largest, for a World War II battle"). Rutter and Jennings gave the correct response of Chicago, but Watson's response was [...] "What is Toronto?????" [...] Ferrucci offered reasons why Watson would appear to have guessed a Canadian city: categories only weakly suggest the type of response desired, the phrase [...] "U.S. city" [...] did {{not appear in the}} question, there are cities named Toronto in the U.S., and Toronto in Ontario has an American League baseball team. Dr. Chris Welty, who also worked on Watson, suggested that {{it may not have been}} able to correctly parse the second part of the clue, [...] "its second largest, for a World War II battle" [...] (which was not a standalone clause despite it following a semicolon, and required context to understand that it was referring to a second-largest airport). Eric Nyberg, a professor at Carnegie Mellon University and a member of the development team, stated that the error occurred because Watson does not possess the comparative knowledge to discard that potential response as not viable. Although not displayed to the audience as with non-Final Jeopardy! questions, Watson's second choice was Chicago. Both Toronto and Chicago were well below Watson's <b>confidence</b> <b>threshold,</b> at 14% and 11% respectively. (This lack of confidence was the reason for the multiple question marks in Watson's response.) ...|$|E
50|$|MEME or Multiple EM for Motif Elicitation is a {{tool for}} {{discovering}} motifs {{in a group of}} related DNA or protein sequences.MEME takes as input a group of DNA or protein sequences and outputs as many motifs as requested up to a user-specified statistical <b>confidence</b> <b>threshold.</b> MEME uses statistical modeling techniques to automatically choose the best width, number of occurrences, and description for each motif.|$|E
50|$|A {{technique}} for speeding up processing of boosted classifiers, early termination refers to only testing each potential object {{with as many}} layers of the final classifier necessary to meet some <b>confidence</b> <b>threshold,</b> speeding up computation for cases where the class of the object can easily be determined. One such scheme is the object detection framework introduced by Viola and Jones: in an application with significantly more negative samples than positive, a cascade of separate boost classifiers is trained, the output of each stage biased such that some acceptably small fraction of positive samples is mislabeled as negative, and all samples marked as negative after each stage are discarded. If 50% of negative samples are filtered out by each stage, {{only a very small}} number of objects would pass through the entire classifier, reducing computation effort. This method has since been generalized, with a formula provided for choosing optimal thresholds at each stage to achieve some desired false positive and false negative rate.|$|E
30|$|In other simulations, we varied {{the size}} of {{confidence}} “bins,” simulating noisy individual <b>confidence</b> <b>thresholds.</b> This modification did not change {{the pattern of the}} data.|$|R
40|$|Making {{grounding}} decisions: Data-driven {{estimation of}} dialogue costs and <b>confidence</b> <b>thresholds</b> This paper presents a data-driven decisiontheoretic approach to making grounding decisions in spoken dialogue systems, i. e., {{to decide which}} recognition hypotheses to consider as correct and which grounding action to take. Based on task analysis of the dialogue domain, cost functions are derived, which take dialogue efficiency, consequence of task failure and information gain into account. Dialogue data is then used to estimate speech recognition <b>confidence</b> <b>thresholds</b> that are dependent on the dialogue context...|$|R
30|$|Sequential rule mining is to {{find out}} all {{significant}} rules that satisfy minSup and minConf from the given database. The support and <b>confidence</b> <b>thresholds</b> are usually predefined by users.|$|R
5000|$|Although it wagered only $947 on the clue, Watson was {{the only}} {{contestant}} to miss the Final Jeopardy! response in the category U.S. CITIES ("Its largest airport was named for a World War II hero; its second largest, for a World War II battle"). Rutter and Jennings gave the correct response of Chicago, but Watson's response was [...] "What is Toronto?????" [...] Ferrucci offered reasons why Watson would appear to have guessed a Canadian city: categories only weakly suggest the type of response desired, the phrase [...] "U.S. city" [...] did {{not appear in the}} question, there are cities named Toronto in the U.S., and Toronto in Ontario has an American League baseball team. Dr. Chris Welty, who also worked on Watson, suggested that {{it may not have been}} able to correctly parse the second part of the clue, [...] "its second largest, for a World War II battle" [...] (which was not a standalone clause despite it following a semicolon, and required context to understand that it was referring to a second-largest airport). Eric Nyberg, a professor at Carnegie Mellon University and a member of the development team, stated that the error occurred because Watson does not possess the comparative knowledge to discard that potential response as not viable. Although not displayed to the audience as with non-Final Jeopardy! questions, Watson's second choice was Chicago. Both Toronto and Chicago were well below Watson's <b>confidence</b> <b>threshold,</b> at 14% and 11% respectively. (This lack of confidence was the reason for the multiple question marks in Watson's response.) ...|$|E
30|$|Generating {{the desired}} rules from the {{frequent}} sequences if they also satisfy the minimum <b>confidence</b> <b>threshold</b> (minConf).|$|E
40|$|We propose an {{algorithm}} {{for monitoring}} timing constraints to satisfy <b>confidence</b> <b>threshold</b> requirements {{when there is}} uncertainty in the exact timing of event occurrences. In our model, a timed event trace is examined for possible satisfaction/violation {{with respect to a}} given set of timing constraints. Every event occurrence has a timestamp given by a time interval. Assuming that the time of occurrence is uniformly distributed over the time interval, our algorithm determines whether the probability that a timing constraint has been satisfied exceeds a specified threshold value. Timing constraints are composed of deadline and delay constraints for which satisfaction probabilities are defined. A <b>confidence</b> <b>threshold</b> is a minimum satisfaction probability of the timing constraint. A timing constraint is violated if the <b>confidence</b> <b>threshold</b> is not reached by the timed event trace. We present a PTime monitoring algorithm for detecting timing violation by finding the earliest expiration time (EET) of the deadline timer for each of the cases � � � � �, and � � �, where is the <b>confidence</b> <b>threshold</b> of the timing constraint. We give a derivation of the implicit constraints needed for computing the EET, and we show how to use an all-pairs shortest path algorithm to compute the implicit constraints. 1...|$|E
3000|$|We {{considered}} {{two types}} of “strong” indications as support, one type based on entropy and one based on lift. Most similar to IEM, we considered <b>confidence</b> <b>thresholds</b> based on entropy. Entropy {{can be defined as}} ∑ _i= 1 ^C -p_ilog_ 2 p_i where p [...]...|$|R
40|$|This paper {{presents}} {{a combination of}} out-of-vocabulary (OOV) word modeling and rejection techniques {{in an attempt to}} accept utterances embedding a keyword and reject utterances with nonkeywords. The goal of this research is to develop a robust, task-independent Spanish keyword spotter and to develop a method for optimizing <b>confidence</b> <b>thresholds</b> for a particular context. To model OOV words, we employed both word and sub-word units as fillers, combined with n-gram language models. We also introduce a methodology for optimizing <b>confidence</b> <b>thresholds</b> to control the tradeoffs between acceptance, confirmation, and rejection of utterances. Our experiments are based on a Mexican Spanish auto-attendant system using the SpeechWorks recognizer release 6. 5 Second Edition, in which we achieved a reduction in error of 8. 9...|$|R
40|$|FIGURE 10. Discriminant {{function}} {{scores for}} Polistes hirsuticornis sp. nov., P. parametricus sp. nov. and three related species. Analysis based on 11 morphometric parameters for females (top chart) and 12 for males (bottom chart). For Wilks' λ, F and p values see Table 5. Ellipses are for visualization purposes only {{and do not}} represent <b>confidence</b> <b>thresholds...</b>|$|R
40|$|The {{vectorial}} Deffuant {{model is}} a simple stochastic process for the dynamics of opinions that also includes a <b>confidence</b> <b>threshold.</b> To understand the role of space {{in this type of}} social interactions, we study the process on the one-dimensional lattice where individuals are characterized by their opinion - in favor or against - about $F$ different issues and where pairs of nearest neighbors potentially interact at rate one. Potential interactions indeed occur when the number of issues both neighbors disagree on does not exceed a certain <b>confidence</b> <b>threshold,</b> which results in one of the two neighbors updating her opinion on one of the issues both neighbors disagree on (if any). This paper gives sufficient conditions for clustering of the system and for coexistence due to fixation in a fragmented configuration, showing the existence of a phase transition between both regimes at a critical <b>confidence</b> <b>threshold.</b> Comment: 22 pages, 3 figure...|$|E
30|$|Instead {{of using}} the entire unlabeled data for semi-supervised {{learning}} adaptation, a <b>confidence</b> <b>threshold</b> {{can be used to}} select only a subset of the unlabeled data, as explained in Section 3.2.|$|E
30|$|In addition, two {{important}} thresholds are introduced in this model, namely, event reputation threshold (ERthld) and event <b>confidence</b> <b>threshold</b> (ECthld). The {{configuration of the}} event reputation threshold and event <b>confidence</b> <b>threshold</b> {{is based on the}} event type and sensor capability. When ERS detects that the event reputation value and the event confidence value of a traffic event are over the corresponding threshold, it means that the traffic event really exists and is still there. Therefore, the ERS will send this event information through the user interface to notify the driver {{and at the same time}} broadcast a traffic warning message with the current event reputation value and the corresponding confidence list to nearby vehicles.|$|E
30|$|Among {{two types}} of the {{condensed}} representation above, the second one is probably better and has been proven to be efficient in our previous works. Therefore, in this paper, we propose a new structure and an efficient representation of constrained association rule set based on closed itemsets and their generators. A new corresponding algorithm, named MAR_MinSC, is also developed for mining association rules satisfying the minimum single constraint and the maximum support and <b>confidence</b> <b>thresholds.</b>|$|R
40|$|In {{this paper}} {{we examine the}} effect that the choice of support and <b>confidence</b> <b>thresholds</b> has on the {{accuracy}} of classifiers obtained by Classification Association Rule Mining. We show that accuracy can almost always be improved by a suitable choice of threshold values, and we describe a method for finding the best values. We present results that demonstrate this approach can obtain higher accuracy without the need for coverage analysis of the training data...|$|R
40|$|Associative {{classification}} (AC) is a {{data mining}} approach that uses association rule discovery methods to build classification systems (classifiers). Several research studies reveal that AC normally generates higher accurate classifiers than classic classification data mining approaches such as rule induction, probabilistic and decision trees. This paper proposes a new multiclass AC algorithm called MAC. The proposed algorithm employs a novel method for building the classifier that normally reduces the resulting classifier size {{in order to}} enable end-user to more understand and maintain it. Experimentations against 19 different data sets from the UCI data repository and using different common AC and traditional learning approaches have been conducted with reference to classification accuracy {{and the number of}} rules derived. The results show that the proposed algorithm is able to derive higher predictive classifiers than rule induction (RIPPER) and decision tree (C 4. 5) algorithms and very competitive to a known AC algorithm named MCAR. Furthermore, MAC is also able to produce less number of rules than MCAR in normal circumstances (standard support and <b>confidence</b> <b>thresholds)</b> and in sever circumstances (low support and <b>confidence</b> <b>thresholds)</b> and for most of the data sets considered in the experiments...|$|R
30|$|The four {{functions}} {{supported in}} the ERS are event management, reputation value adaptation module, event reputation value collection, and event confidence list collection. We will introduce {{the first two}} functions in the next subsection. For the two collection functions, we have briefly illustrated how these functions work as previously stated in this subsection. Here we want to introduce two important thresholds used in ERS, that is, event reputation threshold and event <b>confidence</b> <b>threshold.</b> Event reputation threshold is used {{to set up the}} barrier for event intensity. If the event reputation value of a traffic event is higher than the predefined event reputation threshold, then the intensity of this event is sufficiently strong enough to indicate the continuous existence of this event. Otherwise, the event might not still exist anymore, even though it did occur sometime before. Event <b>confidence</b> <b>threshold</b> is used to set up the bottom line for event reliability. If the event confidence value of a traffic event is higher than the predefined event <b>confidence</b> <b>threshold,</b> then it indicates that there were sufficient amounts of vehicles that encountered the same traffic event and the occurrence plausibility of this event is much more reliable. By properly setting these thresholds and other configurable system parameters, the ERS can provide accurate and reliable traffic information to vehicle drivers. If a given ERS detects the event reputation value and the event confidence value of a traffic event is over the corresponding event reputation threshold and event <b>confidence</b> <b>threshold,</b> which indicate that the traffic event really exists and is still there, the ERS will send this event information through the user interface to notify the driver {{and at the same time}} broadcast a traffic warning message with current event reputation value and the corresponding confidence list to nearby vehicles.|$|E
30|$|We adopt {{a simple}} example to {{illustrate}} the operation flow of the ERS in this subsection. Assume that all vehicles have ERS installed and configured with the event reputation threshold, the event <b>confidence</b> <b>threshold,</b> and the message transmission range (in hop count) been set as 8, 2, and 3, respectively.|$|E
30|$|In {{the early}} stage, the CBA {{approach}} applies {{the concept of}} association rule classification. In CBA, the system initially executes the Apriori algorithm to progressively generate association rules that are satisfied with a user-defined minimum support and <b>confidence</b> <b>threshold.</b> One subset of the generated classification rules becomes the final classifier.|$|E
3000|$|... (Illustrating some {{disadvantages}} of PP-MAR-MinSC- 2). The {{rest of this}} paper considers database T shown in Fig. 1 a. For the minimum support threshold s_ 0 = 0.28, Charm-L [37] and MinimalGenerators [36] are used to mine a lattice of all closed frequent itemsets and their generators. The result is shown in Fig. 1 b. Let us choose the maximum support threshold s_ 1 = 0.5 and the minimum and maximum <b>confidence</b> <b>thresholds</b> c_ 0 = 0.4 and c_ 1 = 0.9, respectively.|$|R
30|$|In some methods, {{traditional}} Apriori {{methods are}} used in Map-Reduce. Transactions are allocated to Mappers and frequent k-itemsets are extracted from each Mapper before the results are shuffled through combiners and the final k-itemsets are extracted according to support and <b>confidence</b> <b>thresholds.</b> In Oruganti et al. [4], Kovacs et al. [5], Li et al. [6], Mappers and Reducers are used, but in [7, 8], in addition to Mappers and Reducers, Combiners are used for better shuffling and to address performance issues.|$|R
40|$|This paper {{looks into}} the {{feasibility}} of using neural networks to classify characters on electrical specification plates (ESP). This thesis is given by Verico AS (Verico). Verico performs large scale Asset Documentation, where photo documentation {{is one of their}} main tools. As such, they have large amounts of ESP imagery. Collecting data from the images is done manually, which is both time consuming and tedious work. This thesis seeks to further develop and utilize previous work done for Verico, such as background segmentation and vertical histogram analysis. The scope of the thesis is looking at the feasibility of using neural networks as a classifier for digits. MATLAB’s Neural Network Toolbox is used to train and classify data. The neural network is trained on 240318 images from the Street View House Numbers (SVHN) Dataset, and then tested on two different datasets. The first is on 26032 images from the SVHN test dataset, where the neural net achieved an overall accuracy of 84. 1 %. Through <b>confidence</b> <b>thresholding</b> 98 % accuracy is reached at 52. 8 % coverage. The other dataset consists of 600 images gathered from several classes of ESP. The neural net achieves 94. 1 % overall accuracy, and with <b>confidence</b> <b>thresholding</b> 98 % accuracy is reached at 85. 3 % coverage...|$|R
40|$|Abstract. The {{vectorial}} Deffuant {{model is}} a simple stochastic process for the dynamics of opinions that also includes a <b>confidence</b> <b>threshold.</b> To understand the role of space {{in this type of}} social interactions, we study the process on the one-dimensional lattice where individuals are characterized by their opinion – in favor or against – about F different issues and where pairs of nearest neighbors potentially interact at rate one. Potential interactions indeed occur when the number of issues both neighbors disagree on does not exceed a certain <b>confidence</b> <b>threshold,</b> which results in one of the two neighbors updating her opinion on one of the issues both neighbors disagree on (if any). This paper gives sufficient conditions for clustering of the system and for coexistence due to fixation in a fragmented configuration, showing the existence of a phase transition between both regimes. 1...|$|E
40|$|The voter {{model and}} the Axelrod model {{are two of the}} main {{stochastic}} processes that describe the spread of opinions on networks. The former includes social influence, the tendency of individuals to become more similar when they interact, while the latter also accounts for homophily, the tendency to interact more frequently with individuals which are more similar. The Axelrod model has been extensively studied during the past ten years based on numerical simulations. In contrast, we give rigorous analytical results for a generalization of the voter model that is closely related to the Axelrod model as it combines social influence and <b>confidence</b> <b>threshold,</b> which is modeled somewhat similarly to homophily. Each vertex of the network, represented by a finite connected graph, is characterized by an opinion and may interact with its adjacent vertices. Like the voter model, an interaction results in an agreement between both interacting vertices [...] social influence [...] but unlike the voter model, an interaction takes place if and only if the vertices' opinions are within a certain distance [...] <b>confidence</b> <b>threshold.</b> In a deterministic static approach, we first give lower and upper bounds for the maximum number of opinions that can be supported by the network {{as a function of the}} <b>confidence</b> <b>threshold</b> and various characteristics of the graph. The number of opinions coexisting at equilibrium is then investigated in a probabilistic dynamic approach for the stochastic process starting from a random configuration [...] . Comment: 18 pages, 4 figure...|$|E
30|$|Configuration {{of event}} {{reputation}} threshold and event <b>confidence</b> <b>threshold</b> in an ERS {{are dependent on}} the sensor capability of a vehicle and the type characteristics of a traffic event. In general, there are some design criteria and guidelines to help vehicle manufacturers or drivers determine these two thresholds. For example, when instant notification of event occurrence {{is more important than}} event reliability and event continuity in situations such as emergency braking event and speed decrease event, both thresholds should be set to a lower value. On the contrary, if event reliability and event continuity are more important than instant notification of event occurrence in situations such as vehicle accident event and traffic jam event, both thresholds should be set to a higher value. Therefore, we suggest that different pairs of event reputation threshold and event <b>confidence</b> <b>threshold</b> should be preconfigured in an ERS based on various event types and sensor capability of vehicle.|$|E
40|$|Data {{processing}} workflows evolve over time. For example, {{operators of}} web information extraction workflows change them continually by retraining classifiers, adjusting <b>confidence</b> <b>thresholds,</b> extracting additional structured data fields, and incorporating new information sources. In most workflow systems, even minor workflow logic changes trigger recomputation of derived data products from scratch. When data volumes are large {{this approach is}} grossly wasteful and generates huge latency hiccups. This paper develops a model and implementation of a workflow system that processes data incrementally {{in the face of}} workflow logic evolution, in addition to data evolution. Experiments on real data demonstrate large performance gains from this kind of incremental processing. 1...|$|R
40|$|This paper {{extends the}} theory of false {{discovery}} rates (FDR) pioneered by Benjamini and Hochberg (1995). We develop a framework in which the False Discovery Proportion (FDP) – the number of false rejections divided {{by the number of}} rejections – is treated as a stochastic process. After obtaining the limiting distribution of the process, we demonstrate the validitiy of a class of procedures for controlling the False Discovery Rate (the expected FDP). We construct a confidence envelope for the whole FDP process. From these envelopes we derive <b>confidence</b> <b>thresholds,</b> for controlling the quantiles of the distribution of the FDP as well as controlling the number of false discoveries. We als...|$|R
40|$|Advances in software-driven {{glycopeptide}} identification have facilitated N-glycoproteomics studies reporting {{thousands of}} intact N-glycopeptides, i. e., N-glycan-conjugated peptides, but the automated identification process {{remains to be}} scrutinized. Herein, we compare the site-specific glycoprofiling efficiency of the PTM-centric search engine Byonic relative to manual expert annotation utilizing typical glycoproteomics acquisition and data analysis strategies but with a single glycoprotein, the uncharacterized multiple N-glycosylated human basigin. Detailed site-specific reference glycoprofiles of purified basigin were manually established using ion-trap CID–MS/MS and high-resolution Q-Exactive Orbitrap HCD–MS/MS of tryptic N-glycopeptides and released N-glycans. The micro- and macroheterogeneous basigin N-glycosylation was site-specifically glycoprofiled using Byonic {{with or without a}} background of complex peptides using Q-Exactive Orbitrap HCD–MS/MS. The automated glycoprofiling efficiencies were assessed against the site-specific reference glycoprofiles and target/decoy proteome databases. Within the limits of this single glycoprotein analysis, the search criteria and <b>confidence</b> <b>thresholds</b> (Byonic scores) recommended by the vendor provided high glycoprofiling accuracy and coverage (both > 80 %) and low peptide FDRs (< 1 %). The data complexity, search parameters including search space (proteome/glycome size), mass tolerance and peptide modifications, and <b>confidence</b> <b>thresholds</b> affected the automated glycoprofiling efficiency and analysis time. Correct identification of ambiguous peptide modifications (methionine oxidation/carbamidomethylation) whose mass differences coincide with several monosaccharide mass differences (Fuc/Hex/HexNAc) and of ambiguous isobaric (Hex 1 NeuAc 1 -R/Fuc 1 NeuGc 1 -R) or near-isobaric (NeuAc 1 -R/Fuc 2 -R) monosaccharide subcompositions remains challenging in automated glycoprofiling, arguing particular attention paid to N-glycopeptides displaying such “difficult-to-identify” features. This study provides valuable insights into the automated glycopeptide identification process, stimulating further developments in FDR-based glycoproteomics. No Full Tex...|$|R
40|$|An {{ensemble}} is a classifier {{created by}} combining the predictions of multiple component classifiers. We present a new method for combining classifiers into an ensemble based on a simple estimation of each classifier's competence. The classifiers are grouped into an ordered list where each classifier has a corresponding threshold. To classify an example, the first classifier {{on the list is}} consulted and if that classifier's confidence for predicting the example is above the classifier's threshold, then that classifier's prediction is used. Otherwise, the next classifier and its threshold is consulted and so on. If none of the classifiers predicts the example above its <b>confidence</b> <b>threshold</b> then the class of the example is predicted by averaging all of the component classifier predictions. The key to this method is the selection of the <b>confidence</b> <b>threshold</b> for each classifier. We have implemented this method in a system called Sequel which has been applied to the task o [...] ...|$|E
40|$|Each {{document}} in a multi-label classification {{is connected to}} a subset of labels. These documents usually include a big number of features, which can hamper the performance of learning algorithms. Therefore, feature selection is helpful in isolating the redundant and irrelevant elements that can hold the performance back. The current study proposes a Naive Bayesian (NB) multi-label classification algorithm by incorporating a wrapper approach for the strategy of feature selection aiming at determining the best minimum <b>confidence</b> <b>threshold.</b> This paper also suggests transforming the multi-label documents prior to utilizing the standard algorithm of feature selection. In such a process, the document was copied into labels that belonged to by adopting all the assigned characteristics for each label. Then, this study conducted an evaluation of seven minimum confidence thresholds. Additionally, Class Association Rules (CARs) represents the wrapper approach for this evaluation. The experiments carried out with benchmark datasets revealed that the Naïve Bayes Multi-label (NBML) classifier with business dataset scored an average precision of 87. 9 % upon using a 0. 1 % of minimum <b>confidence</b> <b>threshold...</b>|$|E
40|$|The {{constrained}} voter model {{describes the}} dynamics of opinions in a population of individuals located on a connected graph. Each agent is characterized by her opinion, where the set of opinions is represented by a finite sequence of consecutive integers, and each pair of neighbors, {{as defined by the}} edge set of the graph, interact at a constant rate. The dynamics depends on two parameters: the number of opinions denoted by F and a so-called <b>confidence</b> <b>threshold</b> denoted by θ. If the opinion distance between two interacting agents exceeds the <b>confidence</b> <b>threshold</b> then nothing happens, otherwise one of the two agents mimics the other one just as in the classical voter model. Our main result shows that the one-dimensional system starting from any product measures with a positive density of each opinion fluctuates and clusters if and only if F ≤ 2 θ + 1. Sufficient conditions for fixation in one dimension when the initial distribution is uniform and lower bounds for the probability of consensus for the process on finite connected graphs are also proved. Comment: 27 pages, 5 figures, 1 tabl...|$|E
40|$|Association rule mining {{is a task}} in {{data mining}} for {{discovering}} the hidden, interesting associations between items in the database. To find the relevant associations, the user has to specify support and <b>confidence</b> <b>thresholds.</b> These thresholds {{play an important role}} in deciding the number of appropriate rules found. User has many problems in specifying the appropriate thresholds, without the knowledge of itemsets and their frequency in the database. A high support threshold keeps away from generating more number of rules, but at the cost of losing interesting rules of low support. This paper proposes an approach to set suitable support thresholds for frequent itemset generation. Experimental results show that this approach produces the interesting rules without specifying the user specified support threshold...|$|R
40|$|Association rule mining {{which is}} of great {{importance}} and use {{is one of a}} vital technique for data mining. Main among the association rule mining techniques have been Apriori and many more approaches have been introduced with minute changes to Apriori but their basic concept remains the same i. e use of support and <b>confidence</b> <b>threshold(s).</b> According to best of our knowledge we came to know that no work has been done in the field of improving the pruning step of Apriori. This paper introduces a new algorithm IP-APRIORI i. e. ‘Improved Pruning in Apriori’. This algorithm improves the pruning procedure of Apriori algorithm by using average support (supavg) instead of minimum support (supmin), to generate probabilistic item-set instead of large item-set...|$|R
3000|$|We {{fixed the}} maximum support and <b>confidence</b> <b>thresholds</b> at 1 (as per tradition). For each {{database}} and given minimum support, {{we chose the}} set [...] A^F of all frequent items. Ten pairs of maximum constraints (L_ 1, R_ 1 [...]), were randomly retrieved from [...] A^F of sizes | L_ 1 | =p_ 1 % *| [...] A^F| [...] and | R_ 1 | =p_ 1 % *| [...] A^F| [...]. We set p_ 1 = 30 % and p_ 2 = 70 % for Connect, Pumsb and Chess, and p_ 1 = 8 %, p_ 2 = 58 % for Mushroom (we achieved similar results for different values of p_ 1,p_ 2 [...]). We executed the three methods on each database (DB) with two given minimum supports MS ([...] [...]...|$|R
