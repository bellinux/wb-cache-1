5|101|Public
25|$|In {{those two}} weeks, the Brotherhood {{has been using}} the now-reactivated Liberty Prime to root out the {{remaining}} Enclave presence in the Capital Wasteland. The player joins them, only to watch Liberty Prime be destroyed by a devastating orbital strike. Taking out this new threat becomes the top priority. A short side-mission is arranged to equip the player with the powerful Tesla Cannon, after which they move on the Enclave's massive Mobile <b>Crawler</b> <b>base,</b> located outside of the Wasteland at Adams Air Force Base. After fighting through the base personnel, a control station at the top {{can be used to}} call an orbital strike on the base itself, destroying it. Alternatively, the Citadel can be destroyed, marking the player a traitor to the Brotherhood of Steel.|$|E
50|$|The Speedcrane in 1925 was a steam-driven, 15-ton {{capacity}} crane {{that sat}} on four wheels. Ten models were built by Manitowoc from this basic model. Eventually, after listening to customer feedback, Moore redesigned the crane and installed a gasoline engine. Another major change was {{the replacement of the}} wheels with a <b>crawler</b> <b>base</b> that allowed for better traction. The first model from the redesigned Speedcrane was a Model 100.|$|E
50|$|In {{those two}} weeks, the Brotherhood {{has been using}} the now-reactivated Liberty Prime to root out the {{remaining}} Enclave presence in the Capital Wasteland. The player joins them, only to watch Liberty Prime be destroyed by a devastating orbital strike. Taking out this new threat becomes the top priority. A short side-mission is arranged to equip the player with the powerful Tesla Cannon, after which they move on the Enclave's massive Mobile <b>Crawler</b> <b>base,</b> located outside of the Wasteland at Adams Air Force Base. After fighting through the base personnel, a control station at the top {{can be used to}} call an orbital strike on the base itself, destroying it. Alternatively, the Citadel can be destroyed, marking the player a traitor to the Brotherhood of Steel.|$|E
2500|$|... tkWWW Robot, a <b>crawler</b> <b>based</b> on the tkWWW {{web browser}} (licensed under GPL).|$|R
50|$|Demon Stalkers is a {{top-down}} action, role-playing video game. It {{was released}} in 1987 on the Commodore 64 and DOS. It's a dungeon <b>crawler</b> <b>based</b> on killing monsters during the descent. It had a sequel named Fire King released on the same platforms that played in a similar style.|$|R
40|$|In this paper, {{the effect}} of {{heuristic}} graph search algorithms like best first and A* best first search on the offline browsing efficiency is studied. A web <b>crawler</b> <b>based</b> multithreaded web archiving system is designed using these heuristic graph search algorithms and the offline browsing efficiency of the web archiving system is estimated...|$|R
30|$|The {{authors are}} {{grateful}} to Dr. H. Adachi, Prof. N. Koyachi and Dr. S. Sarata for their contribution in designing the <b>crawler</b> <b>base</b> frame.|$|E
40|$|Includes abstract. Includes bibliographical references. This {{document}} {{reports on}} the design, construction and testing of the manipulator arm {{that is to be}} fitted to UCT's Urban Search and Rescue Robot (USRR), named the Ratel. The 6 degree-of-freedom manipulator arm is mounted on the <b>crawler</b> <b>base.</b> The USRR is designed to traverse difficult terrain in search of survivors. The base is therefore equipped with variable geometry tracks to enable it to traverse stairs and other tricky terrain. The sensor payload is equipped with life detection equipment and the manipulator arm enables the USRR to manipulate with its environment (opening doors etc.) and to interact with survivors, passing them water or food packs...|$|E
40|$|We {{describe}} a novel, "focusable", scalable, distributed web <b>crawler</b> <b>based</b> on GNU/Linux and PostgreSQL that we {{designed to be}} easily extendible and which we have released under a GNU public licence. We also report a first use case related {{to an analysis of}} Twitter's streams about the french 2012 presidential elections and the URL's it contains...|$|R
50|$|Legend of Grimrock is {{an action}} {{role-playing}} game video game developed {{and published by}} Almost Human. The title is a 3D grid-based, real-time dungeon <b>crawler</b> <b>based</b> on the 1987 game Dungeon Master. It was originally released for Microsoft Windows in April 2012, and later ported for OS X and Linux in December 2012 and iOS in May 2015.|$|R
50|$|Swiftype’s Site Search {{product is}} {{available}} as an API, web <b>crawler</b> <b>based</b> engine or as a plugin. It enables site owners to enrich their visitor’s search experiences with faceted search, full text search, real-time search and concept search. The company's plans offer on-demand and live recrawls and indexing of websites. Other features include {{drag and drop}} result customization, real-time analytics and adjustable weights.|$|R
5000|$|Monkey-Spider [...] is a low-interaction client {{honeypot}} initially {{developed at}} the University of Mannheim by Ali Ikinci. Monkey-Spider is a <b>crawler</b> <b>based</b> client honeypot initially utilizing anti-virus solutions to detect malware. It is claimed to be fast and expandable with other detection mechanisms. The work has started as a diploma thesis and is continued and released as Free Software under the GPL.|$|R
40|$|Abstract — In this paper, {{the effect}} of {{heuristic}} graph search algorithms like best first and A * best first search on the offline browsing efficiency is studied. A web <b>crawler</b> <b>based</b> multithreaded web archiving system is designed using these heuristic graph search algorithms and the offline browsing efficiency of the web archiving system is estimated. Index Terms — Offline browsing efficiency, heuristic graph search algorithms, multithreaded, web archiving system. I...|$|R
40|$|In {{this paper}} we compare the {{performance}} characteristics of our selection based learning algorithm for Web crawlers with {{the characteristics of}} the reinforcement learning algorithm. The task of the crawlers is to find new information on the Web. The selection algorithm, called weblog update, modifies the starting URL lists of our <b>crawlers</b> <b>based</b> on the found URLs containing new information. The reinforcement learning algorithm modifies the URL orderings of the <b>crawlers</b> <b>based</b> on the received reinforcements for submitted documents. We performed simulations based on data collected from the Web. The collected portion of the Web is typical and exhibits scale-free small world (SFSW) structure. We have found that on this SFSW, the weblog update algorithm performs better than the reinforcement learning algorithm. It finds the new information faster than the reinforcement learning algorithm and has better new information/all submitted documents ratio. We believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the Web. Comment: 24 pages, 3 figure...|$|R
40|$|If the <b>crawler</b> <b>based</b> {{retrieval}} system is selected, this {{project management plan}} identifies the path forward for acquiring a crawler/track pump waste {{retrieval system}}, and completing sufficient testing to support deploying the crawler for {{as part of a}} retrieval technology demonstration for Tank 241 -C- 104. In the balance of the document, these activities will be referred to as the Crawler Acquisition and Testing Demonstration. During recent Tri-Party Agreement negotiations, TPA milestones were proposed for a sludge/hard heel waste retrieval demonstration in tank C- 104. Specifically one of the proposed milestones requires completion of a cold demonstration of sufficient scale to support final design and testing of the equipment (M- 45 - 03 G) by 6 / 30 / 2004. A crawler-based retrieval system was one of the two options evaluated during the pre-conceptual engineering for C- 104 retrieval (RPP- 6843 Rev. 0). The alternative technology procurement initiated by the Hanford Tanks Initiative (HTI) project, combined with the pre-conceptual engineering for C- 104 retrieval provide an opportunity to achieve compliance with the proposed TPA milestone M- 45 - 03 H. This Crawler Acquisition and Testing Demonstration project management plan identifies the plans, organizational interfaces and responsibilities, management control systems, reporting systems, timeline and requirements for the acquisition and testing of the <b>crawler</b> <b>based</b> retrieval system. This project management plan is complimentary to and supportive of the Project Management Plan for Retrieval of C- 104 (RPP- 6557). This project management plan focuses on utilizing and completing the efforts initiated under the Hanford Tanks Initiative (HTI) to acquire and cold test a commercial <b>crawler</b> <b>based</b> retrieval system. The crawler-based retrieval system will be purchased on a schedule to support design of the waste retrieval from tank C- 104 (project W- 523) and to meet the requirement of proposed TPA milestone M- 45 - 03 H. This Crawler Acquisition and Testing Demonstration project management plan includes the following: (1) Identification of acquisition strategy and plan to obtain a <b>crawler</b> <b>based</b> retrieval system; (2) Plan for sufficient cold testing to make a decision for W- 523 and to comply with TPA Milestone M- 45 - 03 H; (3) Cost and schedule for path forward; (4) Responsibilities of the participants; and (5) The plan is supported by updated Level 1 logics, a Relative Order of Magnitude cost estimate and preliminary project schedule...|$|R
40|$|Abstract — In this paper, {{the effect}} of various {{parameters}} on the offline browsing efficiency of a web archiving system using breadth first search Algorithm is investigated. A web <b>crawler</b> <b>based</b> multithreaded web archiving system is designed using breadth first search algorithm and the offline browsing efficiency of the web archiving system is estimated {{in the presence of}} the parameters like the searching algorithm, browsing tool, speed of the Internet connectivity and the processing system configuration...|$|R
50|$|The {{parallel}} between {{the growth of the}} AI and that of a child kept building up and, just as children learn how to speak and act by observing their parents and the people around them, Angel_F used its spyware and AI components to learn, to navigate websites and web portals using web <b>crawler</b> <b>based</b> techniques, and to interact with other people by using the contents hosted and generated in its database to create surreal dialogues in blogs and websites.|$|R
40|$|The World Wide Web (Www) {{offers a}} huge number of {{documents}} which deal with information concerning nearly any topic. Thus, search engines and meta search engines currently are the key to finding information. Search engines with <b>crawler</b> <b>based</b> indexes vary in recall and offer a very bad precision. Meta search engines try to overcome these lacks by simple methods for information extraction, information filtering and integration of heterogenous information resources. Only few search engines employ intelligent techniques in order to increase precision...|$|R
5|$|A dungeon <b>crawler</b> game <b>based</b> on {{the story}} of the company's five founders was made. The game was housed in an arcade cabinet inside Obsidian.|$|R
5000|$|Omgili (acronym for [...] "Oh My God I Love It") is a {{vertical}} search engine {{that focuses on}} [...] "many to many" [...] user generated content platforms, such as, forums, discussion groups, answer boards and others. This <b>crawler</b> <b>based,</b> vertical search engine, scans millions of online discussions worldwide in over 100,000 boards and forums, {{and is able to}} differentiate between discussion entities, such as, title, topic, answer and post date. Users can use Omgili to find consumer opinions, debates, discussions, personal experiences, answers and solutions.|$|R
40|$|Traditional {{search engines}} use a thin client, {{distributed}} model for crawling. This <b>crawler</b> <b>based</b> approach has certain drawbacks {{which could be}} removed with a proposed rich client based model. The rich client based search engine offers faster crawling and better updation time using lesser resources than thin client model, and it covers more of the World Wide Web than normal <b>crawler</b> <b>based</b> search engines. Although modern day search engine giants have improvised on various features such as ergonomics and utilities, along with several added goodies, little work is done to improve energy efficiency of such Large Scale Search Engines. As the Internet is increasing exponentially the search engines will involve more and more servers thus costing more and more energy. This ever increasing demand of search engines needs to be curbed down. Rather than multiplying server resources {{it is better to}} use existing servers which work in a congenial environment, using communication methods to reduce redundant downloading of data from different servers by the crawlers. This paper proposes a rich client based architecture for search engines along with analysis and comparison with present search engines. This could help into reducing the challenges of global warming, keeping up the speed and efficiency requirements...|$|R
40|$|We present GoGetIt!, a {{tool for}} {{generating}} structure-driven crawlers that requires a minimum effort from the users. The tool takes as input a sample page and an entry point to a Web site and generates a structure-driven <b>crawler</b> <b>based</b> on navigation patterns, sequences of patterns for the links a crawler has to follow to reach the pages structurally similar to the sample page. In the experiments we have performed, structure-driven crawlers generated by GoGetIt! were able to collect all pages that match the samples given, including those pages added after their generation...|$|R
50|$|At {{the opening}} Preston Hire {{displayed}} their Loading Platforms, Storage containers, a large display of Maeda mini crawler cranes, {{one of their}} Kato MR130 city class cranes, various <b>crawler</b> <b>based</b> Elevating Work Platforms and the Preston Hire Racing V8 Super Car team complete with the transporter and V8 supercars.The 673 {{has been added to}} Preston's growing fleet of Sennebogen telescopic crawler cranes which range from 15 tonne to 70 tonne capacities.The 673 has a transport width of 3m and is fitted with a 36m main boom and a 15m fly jib.|$|R
40|$|With the {{tremendous}} {{growth of the}} Internet, World Wide Web has become a huge source of hyperlinked information contained in hypertext documents. Search engines use web crawlers to collect these documents from web {{for the purpose of}} storage and indexing. An incremental crawler visits the web for updating its collection. There is a need to regulate the frequency of the crawler to visit web sites and provide latest information to the user. In this paper a novel approach to manage the revisiting frequency of an incremental <b>crawler</b> <b>based</b> on the users search history is being proposed...|$|R
40|$|We {{consider}} a collaborative application scenario in Open Hypermedia Systems. We describe a semantic search algorithm to discover semantically equivalent or related resources across distributed link databases, {{otherwise known as}} linkbases. Our approach di#ers from traditional <b>crawler</b> <b>based</b> search mechanisms because it relies on clustering of semantically related entities. It creates clusters of related semantic entities to expedite the search for resources in a random network. It uses a distance-vector based heuristic to guide the search. Our results confirm that the algorithm yields high search e#ciency in collaborative environments where the change in content published by each participant is rapid and random...|$|R
40|$|International audienceThe use of multi-agent topical Web <b>crawlers</b> <b>based</b> on the {{endogenous}} fitness model {{raises the}} problem of controling the population of agents. We tackle this question through an energy based model to balance the reproduction/life expectency of agents. Our goal is to simplify the tuning of parameters and to optimize the use of ressources available for the crawling. We introduce an energy based model designed to control the number of agents according to the precision of the crawling. We present some experiments that show {{that the size of}} the population remains under control during the crawling...|$|R
40|$|Conference Name: 2013 International Forum on Materials Analysis and Testing Technology, IFMATT 2013. Conference Address: Qingdao, China. Time:December 9, 2013 - December 10, 2013. E-commerce {{websites}} has abundant commercial data. Some very beneficial {{information to}} the analysis and prediction of the market can be discovered from these data by applying data mining techniques. The topic-focused web crawler can crawl and gather the subject-related web pages as soon as possible. This thesis has designed and realized the topic-focused <b>crawler</b> <b>based</b> on Scrapy. It firstly introduces the design idea of the crawler and highlights the functions of Scrapy's every part. Then, it uses this topic-focused crawler to realize the capture of information from the C 2 C e-commerce platform, for example TaoBao. At last, it obtains the running result and comparisons of crawling performance between Scrapy <b>based</b> <b>crawler</b> and general crawler. ? (2014) Trans Tech Publications, Switzerland...|$|R
40|$|Within {{recent years}} the World Wide Web (WWW) has grown {{enormously}} {{to a large extent}} where generic web crawlers have become unable to keep up with. As a result, focused web crawlers have gained its popularity which is focused only on a particular domain. But these <b>crawlers</b> are <b>based</b> on lexical terms where they ignore the information contained within named entities; named entities can be a very good source of information when crawling on narrow domains. In this paper we discuss a new approach to focus crawling based on named entities for narrow domains. We have conducted experiments in focused web crawling in three narrow domains: baseball, football and American politics. A classifier based on the centroid algorithm is used to guide the crawler which is trained on web pages collected manually from online news articles for each domain. Our results showed that during anytime of the crawl, the collection built with our crawler is better than the traditional focused <b>crawler</b> <b>based</b> on lexical terms, in terms of the harvest ratio. And this was true for all the three domains considered...|$|R
40|$|Abstract — The World Wide Web (WWW) {{contains}} {{large amount}} of information due to the increase of web pages. Extracting relevant information from web requires suitable search strategies. Conventional search engine are not suitable anymore. Web documents that are suitable to predefined user interest topics are extracted from web by using focused <b>crawler</b> <b>based</b> on seed URLs selection. Ontology is a formal specification of knowledge {{by a set of}} concepts with in a domain and their semantic relationship. Different Ontologies are developed by developer for a same domain differently. In this paper we survey and review issues related to focused crawling strategies in semantic web...|$|R
40|$|Precise, {{reliable}} and real-time financial information {{is critical for}} added-value financial services after the economic turmoil from which markets are still struggling to recover. Since the Web {{has become the most}} significant data source, intelligent <b>crawlers</b> <b>based</b> on Semantic Technologies have become trailblazers in the search of knowledge combining natural language processing and ontology engineering techniques. In this paper, we present the SONAR extension approach, which will leverage the potential of knowledge representation by extracting, managing, and turning scarce and disperse financial information into well-classified, structured, and widely used XBRL format-oriented knowledge, strongly supported by a proof-of-concept implementation and a thorough evaluation of the benefits of the approach...|$|R
40|$|Abstract: {{we propose}} an {{effective}} approach to obtain software security vulnerabilities in web with vertical search technique in this paper. We use a keyword-trainer to get domain keywords in software security domain. Then the web page filter is designed after analyzing the obtained domain keywords {{and the structure}} of the URL topology. Finally, we design a vertical search <b>crawler</b> <b>based</b> on the webpage filter to search for the information of software security vulnerabilities in Web. This approach effectively discovers and digs out the information of software security vulnerabilities presenting in Web. It helps provides the primary knowledge and information for constructing security-knowledge database and analyze the vulnerabilities of software...|$|R
40|$|Abstract- Focused Crawler aims {{to select}} {{relevant}} web pages from internet. These pages {{are relevant to}} some predefined topics. Previous focused crawlers have a problem of not keeping track of user interest and goals. The topic weight table is calculated only once statically and that is less sensitive to potential changes in environment. To address this problem we design a focused <b>crawler</b> <b>based</b> on dynamic computation of topic keywords and their weights. This weight table is constructed according to user query. To check the similarity of web page {{with respect to the}} topic keywords, a cosine similarity function is used and priority of extracted links is calculated...|$|R
25|$|PHP-Crawler is {{a simple}} PHP and MySQL <b>based</b> <b>crawler</b> {{released}} under the BSD License.|$|R
30|$|The {{data set}} {{for this study}} was {{collected}} from Yelp.com using a web <b>crawler</b> <b>based</b> on Jsoup [13]. The crawler starts with the Yelp homepage and it follows all the links present on that page, determining whether it is a user page or a business page and parsing it further along the lines to store the data in MySQL database. For the purpose of our experiments we took a subset of this dataset into consideration which consists of 135, 413 reviews, out of which 103, 020 were recommended reviews and 32, 393 not-recommended reviews. These reviews were written by 66, 936 unique users, consisting of 61, 083 users are genuine users and 5853 are fake users.|$|R
40|$|A Web crawler is a {{computer}} program that browses the World Wide Web in a methodical, automated manner or in an orderly fashion. Web crawling is an important method for collecting data on, and keeping up with, the rapidly expanding Internet. A vast number of web pages are continually being added every day, and information is constantly changing. This Paper is an overview of various types of Web Crawlers and the policies like selection, re-visit, politeness, parallelization involved in it. The behavioral pattern of the Web <b>crawler</b> <b>based</b> on these policies is also taken for the study. The evolution of these web crawler from Basic general purpose web crawler to the latest Adaptive web crawler is studied...|$|R
40|$|Abstract. A crawler is {{a program}} that {{downloads}} and stores Web pages. A crawler must revisit pages because they are frequently updated. In this paper we describe the implementation of CrawlWave, a distributed <b>crawler</b> <b>based</b> on Web Services. CrawlWave is written entirely in the. Net platform; it uses XML/SOAP and is therefore extensible, scalable and easily maintained. CrawlWave can use many client and server processors for the collection of data and therefore operates with minimum system requirements. It is robust, has good performance (download rate) and uses small bandwidth. Data updating {{was one of the}} main design issues of CrawlWave. We discuss our updating method, some bottleneck issues and present first experimental results. ...|$|R
