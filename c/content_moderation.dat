31|9|Public
5000|$|<b>Content</b> <b>moderation</b> {{and video}} {{publishing}} workflow (configurable) ...|$|E
50|$|Foiwe Info Global Solutions Llp is a {{management}} consulting, technology services, Photo and Video Moderation, <b>Content</b> <b>Moderation</b> and outsourcing company headquartered in Bangalore, India.|$|E
50|$|Quora {{supports}} {{various features}} to moderate content posted by users. The majority of <b>content</b> <b>moderation</b> {{is done by}} the users, though staff can intervene as well.|$|E
5000|$|As Moderator, Biira {{has hosted}} events across Africa and beyond. She hosted, {{facilitated}} and {{spoke at the}} African Development Infrastructure and Convention #ADIC2016 organized by AFIDEP https://www.afidep.com in Zurich Switzerland, the Renewable Energy Solutions for Africa event organized by [...] in Nairobi, Kenya and the Uganda Social Media Conferences among others. Some of the <b>content</b> from her <b>moderation</b> events has been aired on Television shows in the host countries and on shows she hosts.|$|R
40|$|Abstract—For current Web 2. 0 services, manual {{examination}} of user uploaded content is normally required {{to ensure its}} legitimacy and appropriateness, which is a substantial burden to service providers. To reduce labor costs and the delays caused by <b>content</b> censoring, social <b>moderation</b> has been proposed as a front-line mechanism, whereby user moderators are encouraged to examine <b>content</b> before system <b>moderation</b> is required. Given the immerse amount of new content added to the Web each day, {{there is a need}} for automation schemes to facilitate rear system moderation. This kind of mechanism is expected to automatically summarize reports from user moderators and ban misbehaving users or remove inappropriate content whenever possible. However, the accuracy of such schemes may be reduced by collusion attacks, where some work together to mislead the automatic summarization in order to obtain shared benefits. In this paper, we propose a collusion-resistant automation scheme for social moderation systems. Because some user moder-ators may collude and dishonestly claim that a user misbehaves, our scheme detects whether an accusation from a user moderator is fair or malicious based on the structure of mutual accusations of all users in the system. Through simulations we show that collusion attacks are likely to succeed if an intuitive count-based automation scheme is used. The proposed scheme, which is based on the community structure of the user accusation graph, achieves a decent performance in most scenarios...|$|R
5000|$|In less strict cases, posts may be [...] "held for moderation". Upon {{clicking}} “publish,” {{the user}} is {{presented with a}} message indicating that the content is being held for approval, apparently {{the result of an}} automated process triggered by the use of keywords. This often happened on the same services that have also prevented publication of other posts, indicating that some services categorize different types of content at different sensitivity levels, to be handled differently. In some cases the <b>content</b> held for <b>moderation</b> was eventually published, indicating that a human being reviewed it and determined that the content was acceptable. In other cases the content was “held for moderation” indefinitely.|$|R
50|$|In late August 2011, GoAnimate for Schools was {{publicly}} launched. GoAnimate for Schools is a school-safe {{version of}} GoAnimate featuring dedicated privacy, security, <b>content</b> <b>moderation</b> and group management features.|$|E
50|$|Samasource is a {{non-profit}} organization that allows people living in poverty the opportunity to complete microwork for a living wage. The service specializes in online <b>content</b> <b>moderation,</b> digital transcription, and data gathering and promotion.|$|E
50|$|Samasource {{currently}} offers {{five categories}} of digital services to its customers, including <b>content</b> <b>moderation,</b> digital transcription, and machine learning. Its current clients have include Google, Ebay and Walmart and past clients Intuit, LinkedIn, and Microsoft.|$|E
5000|$|Since 2003, MusicBrainz's {{core data}} (artists, recordings, releases, and so on) {{are in the}} public domain, and {{additional}} <b>content,</b> including <b>moderation</b> data (essentially every original content contributed by users and its elaborations), is placed under the Creative Commons CC-BY-NC-SA-2.0 license. [...] The relational database management system is PostgreSQL. The server software is covered by the GNU General Public License.The MusicBrainz client software library, libmusicbrainz, is licensed under the GNU Lesser General Public License, which allows use of the code by proprietary software products. In December 2004, the MusicBrainz project {{was turned over to}} the MetaBrainz Foundation, a non-profit group, by its creator Robert Kaye. On 20 January 2006, the first commercial venture to use MusicBrainz data was the Barcelona, Spain-based Linkara in their Linkara Música service. On 28 June 2007, BBC announced that it has licensed MusicBrainz's live data feed to augment their music Web pages. The BBC online music editors will also join the MusicBrainz community to contribute their knowledge to the database. On 28 July 2008, the beta of the new BBC Music site was launched, which publishes a page for each MusicBrainz artist.|$|R
40|$|Each {{state is}} {{required}} by the No Child Left Behind Act to report the percents of its students who have reached a score level called - proficient- or above for certain grades in the content areas of reading (or a similar construct) and math. Using 2005 data from public web sites of states and the National Assessment of Educational Progress (NAEP), state-to-state differences in percents were analyzed, both unconditionally and conditionally on NAEP, for (1) trend across <b>content</b> areas (horizontal <b>moderation),</b> (2) trend across grade levels (vertical moderation), and (3) consistency with NAEP. While there was considerable variation from state to state, especially on an idealistic-realistic dimension, the results generally show that states are relatively consistent in trends across grades and contents...|$|R
40|$|In the 1970 s at the Battelle Pacific Northwest Laboratory (PNL), {{a series}} of {{critical}} experiments using a remotely operated Split-Table Machine was performed with homogeneous mixtures of (Pu-U) O{sub 2 }-polystyrene fuels {{in the form of}} square compacts having different heights. The experiments determined the critical geometric configurations of MOX fuel assemblies with and without neutron poison plates. With respect to PuO{sub 2 } <b>content</b> and <b>moderation</b> [H/(Pu+U) atomic] ratio (MR), two different homogeneous (Pu-U) O{sub 2 }-polystyrene mixtures were considered: Mixture (1) 14. 62 wt% PuO{sub 2 } with 30. 6 MR, and Mixture (2) 30. 3 wt% PuO{sub 2 } with 2. 8 MR. In all mixtures, the uranium was depleted to about O. 151 wt% U{sup 235 }. Assemblies contained copper, copper-cadmium or aluminum neutron poison plates having thicknesses up to {approximately} 2. 5 cm. This evaluation contains 22 experiments for Mixture 1, and 10 for Mixture 2 compacts. For Mixture 1, there are 10 configurations with copper plates, 6 with aluminum, and 5 with copper-cadmium. One experiment contained no poison plate. For Mixture 2 compacts, there are 3 configurations with copper, 3 with aluminum, and 3 with copper-cadmium poison plates. One experiment contained no poison plate...|$|R
5000|$|In 2012, CoreMedia {{introduced}} Elastic Social, {{an enhanced}} {{version of its}} social toolset with new social interaction capabilities, <b>content</b> <b>moderation</b> tools, user management, and a scalable NoSQL data store. Two years later, CoreMedia LiveContext for IBM WebSphere Commerce was launched.|$|E
50|$|ModSquad is {{a global}} digital {{engagement}} services company based in the United States. The company currently has over 10,000 moderators in its network. ModSquad provides managed, on-demand customer service, <b>content</b> <b>moderation,</b> social media, and community management services and teams across online, e-commerce, in-game, in-app, and social media channels.|$|E
5000|$|Whisper {{allegedly}} {{stores or}} processes user information outside the United States despite having told its users that [...] "we process and store all {{information in the}} United States". Whisper has said that while it does use an outsourcing firm for <b>content</b> <b>moderation</b> based in the Philippines, no data is stored outside the US.|$|E
40|$|This study {{analyses}} {{arguments for}} and against the notion of healthy lifestyles, {{and the construction of}} responsibility for health, in group discussions in Finland. With data from four focus groups, we identified five interpretative repertoires: a strong activity repertoire reflects the dominant cultural value of health and emphasizes self-control. Three other repertoires [...] illness, external barriers, and weak character [...] share the underlying values of the activity repertoire, but exemplify situations where the individual lacks control, seeking to justify deviations from the norm of activity. One counter-repertoire, the pleasure repertoire, questions the hegemonic value of health, and discusses other competing values. The discussion of health is an ongoing dialectical process drawing from the different repertoires. In order to avoid stigmatization and to save face in the social situation of a focus group, the subjects strive to balance their accounts of behaviours considered unhealthy by also claiming healthy behaviours. They also strike a balance between extreme rigidity and carelessness, emphasizing the ideal of moderation and harmony. The findings point to a need to consider variations in and underpinnings of a "good life" at the individual level. Encouraging people to specify the meaning and <b>content</b> of <b>moderation</b> in their personal lives could provide a new perspective for health education and health promotion. Healthy lifestyle Health repertoires Health responsibility Finland...|$|R
40|$|Interactive Voice Response (IVR) systems play an {{important}} role in collecting and disseminating information in developing regions. Recently, researchers have used IVR technology to build voice forums, in which callers leave messages that can be heard over the Internet and over the phone. However, despite their appeal, voice forums remain difficult to set up, and difficult to scale due to the overhead of moderating content and the cost of phone calls. This paper discusses the challenges and opportunities in creating scalable voice forums. We also present a new open-source system, IVR Junction, that leverages existing free services and commercial tools to simplify the process of creating a voice forum. IVR Junction utilizes familiar cloud-based services to provide free <b>content</b> hosting and <b>moderation,</b> as well as a novel mechanism for automatically synchronizing content across geographically-dispersed offices, thereby enabling local access points with decreased calling costs. ...|$|R
40|$|AIMS AND OBJECTIVES: To {{describe}} how persons with psychosis perceive {{participation in a}} lifestyle intervention, and use these perceptions to present factors to for consideration in future interventions. BACKGROUND: Metabolic syndrome is common in persons with psychosis. A healthy lifestyle is the primary option for preventing and treating metabolic syndrome, {{which is why the}} importance of lifestyle interventions has come into focus among health care professionals. Identifying perceptions of participation in a lifestyle intervention can increase the understanding of how to design future interventions. DESIGN: A qualitative, phenomenographic approach was selected, using semi-structured interviews. METHODS: The sample consisted of 40 participants with a psychotic disorder, who had undergone a lifestyle intervention focusing on theoretical education in healthy eating and physical activities. The interviews were conducted in 2011 and 2012, six to seven months after the intervention had been completed. RESULTS: The findings comprise three categories that emphasise the need for a moderate intervention level that facilitates participation and thereby social interactions among group members. The experience of success in the intervention supported the perception of oneself as a capable individual. However, it could also be the opposite, another experience of failure. CONCLUSION: <b>Content</b> in <b>moderation</b> can facilitate participation, and participants can thereby achieve health benefits and find social contacts. In addition to physical activity and lifestyle habits, interventions should have a social focus and be continuous. Professional support is a prerequisite and should facilitate the participants' ability to mirror themselves against healthy people in society by introducing activities that ordinary people do. RELEVANCE FOR CLINICAL PRACTICE: Identifying perceptions of participation in a lifestyle intervention can increase the understanding of how to design and manage future interventions. This is also an aspect that is important to consider in everyday clinical practice...|$|R
50|$|SchoolTube's <b>content</b> <b>moderation</b> {{process is}} what makes it a safe {{platform}} for the classroom. When a student decides to upload a video, a teacher or administrator from that student's school must approve the video before it will be made live on the site. This allows schools to create strong video-sharing communities. Schools also have the option to limit the video viewing to its own school. This moderation process has led students to create higher-quality work.|$|E
50|$|In 2016, Webhelp has {{continued}} to grow through acquisition. In June 2016, Webhelp acquired GoExcellent, a leading Scandinavian customer experience firm. GoExcellent has nine centres across Sweden, Finland, Norway and Denmark, and employs 1,700 people. This acquisition allowed for the launch of Webhelp Nordic, and increased Webhelp's projected 2016 turnover to an estimated $1 billion (725 million Euros). In July, Webhelp Xtrasource in the Netherlands announced plans to acquire top Dutch customer experience firm Contact2Value, a specialist operator in inbound retention and outbound services employing 50 people. This allowed Webhelp to considerably strengthen {{its position in the}} Dutch marketplace. In October 2016, Webhelp announced it was acquiring Netino, a social <b>content</b> <b>moderation</b> company headquartered in Paris that extends across ten countries and has an international team of 400 people. The acquisition is set to be complete by the end of 2016.|$|E
5000|$|In an {{anonymous}} emailed {{statement to the}} Daily Pilot on Thursday, the [...] "Ogle team" [...] said: [...] "We {{are aware of the}} concern, and cyberbullying is absolutely NOT our intention for the app. Our goal for this app is to create a free and safe community space for students, for a better communication. We are currently working around the clock to improve the app. As a matter of fact, we are also in contact with local police departments, anti-bullying organizations and local high schools to try to help the students."In response to these incidents, Ogle expressed that they takes the safety of its users seriously and does not condone any type of behavior that is illegal or in violation of its content policies. The company also said it has instituted a <b>content</b> <b>moderation</b> team to increase review and identify and remove inappropriate content, and take action against “those who violate our community guidelines.” ...|$|E
5000|$|In March 2016, Fast Company {{reported}} that Whisper had {{recently begun to}} use deep learning to build a <b>content</b> <b>moderation</b> tool, internally dubbed the Arbiter. The article stated that the Arbiter runs on a PC with 128GB of RAM and four Nvidia GeForce Titan X graphics cards, {{each of which has}} a graphics processing unit with 3072 computing cores and 12GB of RAM. Those GPUs enable it to perform artificial intelligence and use neural network technology to assess incoming whispers. The article noted that because Whisper's Arbiter has knowledge to [...] "reflect both the real things that millions of Whisper users have said and how moderators handled them, its understanding of language can be remarkably subtle." [...] According to the Los Angeles Times, Whisper is also using AI technology to create content. That system, the Times reports, [...] "allows Whisper to gather related posts about issues at work, school or in relationships. Then, it organizes them, adds context and even writes a headline... With the AI’s help, Whisper hopes to produce more content than humans alone could — which in turn could mean more readership and ad revenue." ...|$|E
40|$|Social media {{platforms}} act as networked gatekeepers—by ranking, channeling, promoting, censoring, and deleting content {{they hold}} power to facilitate or hinder information flows. One of the mechanisms they use is <b>content</b> <b>moderation,</b> or {{the enforcement of}} which content is allowed or disallowed on the platform. Though <b>content</b> <b>moderation</b> relies on users’ labor to identify content to delete, users have little capacity to influence content policies or enforcement. Despite this, some social media users are turning to collective action campaigns, redirecting information flows by subverting the activities of moderators, raising the visibility of otherwise hidden moderation practices, and organizing constituencies in opposition to content policies. Drawing on {{the example of the}} campaign to change Facebook’s nudity policy, this paper examines the strategies and tactics of users turning to collective action, considering which factors are most influential in determining {{the success or failure of}} a campaign. It finds that network gatekeeping salience is a good model for assessing which collective action efforts are most likely to be effective in achieving individual user goals. This indicates that the users who are already most able to harness the attention economy of social media platforms are more likely to successfully navigate the <b>content</b> <b>moderation</b> process. The analysis concludes by attending to what users might learn from the dynamics of network gatekeeping as they seek to resist the asymmetrical power relations of platforms...|$|E
40|$|Most {{people on}} Facebook have {{probably}} seen something they wish they hadn’t, {{whether it be}} violent pictures or racist comments. How the social media giant decides what is and isn’t acceptable is often a mystery. Internal content guidelines, recently published in The Guardian, offer new insight into the mechanics of Facebook <b>content</b> <b>moderation</b> [...] ...|$|E
40|$|Recent {{years have}} shown that mobile devices and Twitter can play a {{significant}} role in providing real -time data from disaster-affected areas to disaster managers. Against this background we present a workflow for Twitter integration into a disaster management information system, and a concept for <b>content</b> <b>moderation</b> that can increase the quality of disseminated information. JRC. G. 2 -Global security and crisis managemen...|$|E
40|$|The {{internet}} {{has become a}} central medium through which `networked publics' express their opinions and engage in debate. Offensive comments and personal attacks can inhibit participation in these spaces. Automated <b>content</b> <b>moderation</b> aims to overcome this problem using machine learning classifiers trained on large corpora of texts manually annotated for offence. While such systems could help encourage more civil debate, they must navigate inherently normatively contestable boundaries, and {{are subject to the}} idiosyncratic norms of the human raters who provide the training data. An important objective for platforms implementing such measures might be to ensure that they are not unduly biased towards or against particular norms of offence. This paper provides some exploratory methods by which the normative biases of algorithmic <b>content</b> <b>moderation</b> systems can be measured, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various test sets. We conclude by discussing some of the ethical choices facing the implementers of algorithmic moderation systems, given various desired levels of diversity of viewpoints amongst discussion participants. Comment: 12 pages, 3 figures, 9 th International Conference on Social Informatics (SocInfo 2017), Oxford, UK, 13 [...] 15 September 2017 (forthcoming in Springer Lecture Notes in Computer Science...|$|E
40|$|Crowdsourcing is an {{effective}} tool to solve hard tasks. By bringing 100, 000 s of people to work on simple tasks that only humans can do, we can go far beyond traditional models of data analysis and machine learning. As technologies and processes mature, crowdsourcing is becoming mainstream. It powers many leading Internet companies and {{a wide variety of}} novel projects: from <b>content</b> <b>moderation</b> and business listing verification to real-time SMS translation for disaster response. However, quality assurance can be a major challenge. In this paper CrowdFlower presents various crowdsourcing applications, from business to ethics, to money and survival, all of which showcase the power of labor-on-demand, otherwise known as the human cloud...|$|E
40|$|In online {{life there}} is a {{normative}} supposition that the information- and image-rich environment of the web and other platforms should provide unfettered access to the circulation {{of all types of}} content. Less attention is paid to what is not seen, to the invisible—be it actual content that is rescinded, altered or removed, or the opaque decision-making processes that maintain its flow. In/visibility online is central to the intertwined functions/mechanisms of user experience and platform control, further operationalized under globalized, technologically driven capitalism. A digital labour phenomenon that is both responsible for it and relies upon it: is commercial <b>content</b> <b>moderation,</b> or CCM, taken up in this paper within the context of in/visibility...|$|E
30|$|As a response, SM {{companies}} have updated their users agreements, “Terms of Service” (ToS) and “Community Standards” (CS) {{and put into}} place systems of automated and human <b>content</b> <b>moderation.</b> From December 2015, Facebook and Twitter have updated their internal policies several times and expanded the content moderator teams that review reports on the networks, {{in an attempt to}} stop extremist, abusive and violent posts. In February 2016, Twitter shut down 125, 000 accounts for threatening or promoting terrorist acts. Similarly, all big SM {{companies have}} adopted new technological tools (e.g. artificial intelligence) and human moderators to regulate content on their platforms (Sophos 2017). In June 2017, the major Silicon Valley companies, i.e. Google, Facebook, Twitter and Microsoft announced the formation of the Global Internet Forum to Counter Terrorism (GIFCT).|$|E
40|$|In {{this chapter}} from the {{forthcoming}} Intersectional Internet: Race, Sex, Class and Culture Online (Noble and Tynes, Eds., 2016), I introduce both {{the concept of}} commercial <b>content</b> <b>moderation</b> (CCM) work and workers, {{as well as the}} ways in which this unseen work affects how users experience the Internet of social media and user-generated content (UGC). I tie it to issues of race and gender by describing specific cases of viral videos that transgressed norms and by providing examples from my interviews with CCM workers. The interventions of CCM workers on behalf of the platforms for which they labor directly contradict myths of the Internet as a site for free, unmediated expression, and highlight the complexities of how and why racist, homophobic, violent, and sexist content exists, and persists, in a social media landscape that often purports to disallow it...|$|E
40|$|This article {{examines}} how {{various aspects of}} US politics and culture may define the boundaries of transnational online discourse. The argument presented focuses on two general categories by which these dynamics may emerge, namely systemic and agential factors. Systemic limitations include language, codes and protocols, algorithms, and parameters set by media specific terms of services. Agential factors are tied to specific sets {{of political and economic}} interests, legal frameworks or cultural norms, as well as individual forms of human agency involved in <b>content</b> <b>moderation.</b> As these examples show, the speech environments we encounter in our daily online interactions do not take place in an ephemeral (cyber-) space devoid of power relations. In order to understand their boundaries, we must come to understand the forces that shape them. Doing so may illuminate how online content comes to reflect deep-seeded power dynamics within US culture, Western societies, as well as transnational politics...|$|E
40|$|Public {{displays}} {{have advanced}} from isolated and non interactive ”ad ” displays which show images and videos to displays that are networked, interactive, {{and open to}} a wide variety of content and applications. Prior work has shown large potential of user-generated content on public displays. However, one of the problems with user-generated content on public displays is moderation as content may be explicit or troublesome for a particular location. In this work we explore the expectations of users with regard to <b>content</b> <b>moderation</b> on public displays. An online survey revealed that people not only think that display content should be moderated but also that a delay of up to 10 minutes is acceptable if display content is moderated. In a subsequent in the wild deployment we compared different moderation delays. We found that a moderation delay significantly decreases the number of usergenerated posts while at the same time there is no significant effect on users ’ decision to repeatedly post on the display...|$|E
40|$|China's {{control of}} {{information}} online is often imposed by social media platforms {{in the name}} of “rumor management. ” This article examines the <b>content</b> <b>moderation</b> strategies of Sina Weibo, China's largest microblogging platform, in regulating discussion of rumors following the 2015 Tianjin blasts. More than 100, 000 Weibo posts were collected and categorized into three data sets: rumor discussion posts from the public, rumor-debunking posts from Weibo's official rumor rebuttal accounts, and posts removed from the system. Two content-moderation rumor strategies, namely rumor rebuttal and content removal, were identified. Clustering analysis and time series analysis was applied to test how these two strategies were used to filter posts of different topics and how they were associated with public discussion of rumor-related topics. Our findings suggest that the platform's response to rumor varied depending on the political sensitivity of the topic. Time-series analysis indicated that the implementation of both strategies was usually associated with a subsequent increase in general discussion about the rumor, suggesting that these strategies do not create a consistent chilling effect on public speech...|$|E
40|$|The {{story of}} a rogue Canadian garbage barge {{attempting}} to offload illegal garbage in the Philippines opens this article on techno-trash, in order to underline both the relationships between countries of the Global North with countries of the Global South in matters of waste, {{as well as to}} reframe discussions of techno-trash as one fundamentally tied to material things. The definition of techno-trash is then expanded, to cover digital detritus created through an entirely digital set of practices I term “Commercial <b>Content</b> <b>Moderation.</b> ” The attempt to offload mounds of e-waste and the similar ways in which a great deal of physical trash circulates around the globe are then directly connected to the kind of disposal that CCM workers do, increasingly undertaken in sites like the Philippines, the Business Process Outsourcing (or BPO) capital of world. Such e-waste arrives in the archipelago for dismantling, repurpose and storage alongside outsourced CCM work, with many of the objects now deemed “waste” once crucial to the production of the very material for which CCM workers now screen and remove...|$|E
40|$|Existing black-box {{attacks on}} deep neural {{networks}} (DNNs) {{so far have}} largely focused on transferability, where an adversarial instance generated for a locally trained model can "transfer" to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100 % adversarial success rates for both targeted and untargeted attacks on DNNs. We carry out extensive experiments for a thorough comparative evaluation of black-box attacks and show that the proposed Gradient Estimation attacks outperform all transferability based black-box attacks we tested on both MNIST and CIFAR- 10 datasets, achieving adversarial success rates similar to well known, state-of-the-art white-box attacks. We also apply the Gradient Estimation attacks successfully against a real-world <b>Content</b> <b>Moderation</b> classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks against state-of-the-art defenses. We show that the Gradient Estimation attacks are very effective even against these defenses. Comment: 25 pages, 7 figures, 10 table...|$|E
40|$|The {{amount of}} video has {{increased}} dramatically over recent years and continues to grow. Striking examples of this development include 4331 years of video uploaded to YouTube in 20111 and millions of CCTV cameras installed in UK. National TV archives also own large amounts of video, such as British BBC with 600 K hours (68 years) of video 2 and French INA 3 with 900 K hours (102 years) of video and audio content. While the growth of video continues 4, its access becomes critically dependent on efficient search engines. Current systems partly solve video search with text-based indexing using, e. g., tags for YouTube videos or program schedules for TV broadcasts. Most of the information in video, however, remains “hidden ” in its content and requires new automatic methods for video content analysis. People {{are by far the}} most frequent and important objects in the video. Automatic analysis of people and human actions has many potential applications in security, education and entertainment including, automatic video surveillance, video search, gaming, sports analysis, automatic <b>content</b> <b>moderation</b> and many others. Most applications require action recognition to operate reliably in diverse and realistic video settings. While significant progress towards this goal has been achieved during the last decade, the problem remains highly challenging...|$|E
