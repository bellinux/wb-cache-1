156|228|Public
25|$|<b>Composite</b> <b>{{hypothesis}}</b> : Any hypothesis {{which does}} not specify the population distribution completely.|$|E
5000|$|<b>Composite</b> <b>{{hypothesis}}</b> : Any hypothesis {{which does}} not specify the population distribution completely. Example: A hypothesis specifying a normal distribution with a specified mean and an unspecified variance.|$|E
50|$|Many {{different}} hypotheses {{have been}} advanced {{to address the}} evolutionary mechanisms to produce Haldane's rule. Currently, the most popular explanation for Haldane's rule is the <b>composite</b> <b>hypothesis,</b> which divides Haldane's rule into multiple subdivisions, including sterility, inviability, male heterogamety, and female heterogamety. The <b>composite</b> <b>hypothesis</b> states that Haldane's rule in different subdivisions has different causes. Individual genetic mechanisms may not be mutually exclusive, and these mechanisms may act together to cause Haldane's rule in any given subdivision. In contrast to these views that emphasise genetic mechanisms, another view hypothesizes that population dynamics during population divergence may cause Haldane's rule.|$|E
40|$|Ordering <b>composite</b> <b>hypotheses</b> in a Bayesian {{network based}} on its {{associated}} a posteriori probabilities can be ex ponentially hard. This paper discusses a qualitative rea soning approach which reduces the computational complex ity of deriving a partial ordering of <b>composite</b> <b>hypotheses.</b> Such a reasoning makes use of the meta-knowledge about the causal relationships among individual hypotheses to deduce qualitative conclusions about the ordering of local <b>composite</b> <b>hypotheses.</b> By doing so, we can employ &quot;divide and conquer &quot; strategy to derive the global ordering of the <b>composite</b> <b>hypotheses</b> from a set of local ordering in which consistencies are guaranteed. We view the contribution {{of this research is}} on the integration of qualitative reasoning with the use of local computations to find not only the most likely <b>composite</b> <b>hypotheses,</b> but also the partial ordering of the <b>composite</b> <b>hypotheses.</b> I...|$|R
40|$|A {{generalization}} of the Wald statistic for testing <b>composite</b> <b>hypotheses</b> is suggested for dependent data from exponential models which include Levy processes and diffusion fields. The generalized statistic is {{proved to be}} asymptotically chi-squared distributed under regular <b>composite</b> <b>hypotheses.</b> It is simpler and more easily available than the generalized likelihood ratio statistic. Simulations in an example where the latter statistic is available show that the generalized Wald test achieves higher average power than the generalized likelihood ratio test. ...|$|R
40|$|A Bayesian {{network is}} a {{knowledge}} representation technique {{for use in}} expert system development. The probabilistic knowledge encoded in a Bayesian network {{is a set of}} <b>composite</b> <b>hypotheses</b> expressed over the permutation of a set of variables (propositions). Ordering these <b>composite</b> <b>hypotheses</b> according to their posteriori probabilities can be exponentially hard. This paper presents a qualitative reasoning approach which takes advantage of certain types of topological structures and probability distributions of a Bayesian network to derive the partial ordering of <b>composite</b> <b>hypotheses.</b> Such an approach offers an attractive alternative to reduce the computational complexity of deriving a partial ordering in which consistency is guaranteed. Keywords: Qualitative reasoning, Bayesian networks, probabilistic inference, partial ordering. This work is supported in part by a grant to Queens College from the General Research Branch, National Institute of Health under grant No. RR- 07064. 1 I. I [...] ...|$|R
50|$|For example, a new {{termination}} criterion {{and scoring}} algorithm must be applied that classifies the examinee into a category rather than providing a point estimate of ability. There are two primary methodologies available for this. The more prominent {{of the two}} is the sequential probability ratio test (SPRT). This formulates the examinee classification problem as a hypothesis test that the examinee's ability is equal to either some specified point above the cutscore or another specified point below the cutscore. Note {{that this is a}} point hypothesis formulation rather than a <b>composite</b> <b>hypothesis</b> formulation that is more conceptually appropriate. A <b>composite</b> <b>hypothesis</b> formulation would be that the examinee's ability is in the region above the cutscore or the region below the cutscore.|$|E
5000|$|A <b>composite</b> <b>hypothesis</b> {{holds that}} early {{language}} {{took the form}} of part gestural and part vocal mimesis (imitative 'song-and-dance'), combining modalities because all signals (like those of nonhuman apes and monkeys) still needed to be costly in order to be intrinsically convincing. In that event, each multi-media display would have needed not just to disambiguate an intended meaning but also to inspire confidence in the signal's reliability. The suggestion is that only once community-wide contractual understandings had come into force could trust in communicative intentions be automatically assumed, at last allowing Homo sapiens to shift to a more efficient default format. Since vocal distinctive features (sound contrasts) are ideal for this purpose, it was only at this point—when intrinsically persuasive body-language was no longer required to convey each message—that the decisive shift from manual gesture to our current primary reliance on spoken language occurred.|$|E
40|$|The task of {{diagnosis}} {{is to find}} a <b>composite</b> <b>hypothesis</b> that best explains a set of data. Generally, the number of the potential hypotheses is exponentially large so that finding a <b>composite</b> <b>hypothesis</b> is computationally expensive. Recently, there have been made efforts to find parallel processing methods to solve the above difficulty. In this paper, we propose a neural network model for diagnostic problem solving where a diagnostic problem is treated as a combinatorial optimization problem. 1 Introduction For a set of manifestations (observations), diagnostic inference is necessary to find the most plausible faults or disorders which can explain why the manifestations are present. In general, an individual fault or disorder can explain only a portion of the manifestations. Therefore, a <b>composite</b> <b>hypothesis</b> which consists of several individual disorders has to be found. Such a <b>composite</b> <b>hypothesis</b> is able to explain all the manifestations. However, it is very expensive computationall [...] ...|$|E
40|$|This paper proposes sign-based {{tests for}} simple and <b>composite</b> <b>hypotheses</b> on the long-memory {{parameter}} {{of a time}} series process. The tests allow for nonstationary hypothesis, such as unit root, {{as well as for}} stationary hypotheses, such as weak dependence or no integration. The proposed generalized Lagrange multiplier sign tests for simple hypotheses on the long-memory parameter are exact and locally optimal among those in their class. We also propose tests for <b>composite</b> <b>hypotheses</b> on the parameters of ARFIMA processes. The resulting tests statistics have a standard normal limiting distribution under the null hypothesis. Publicad...|$|R
3000|$|A famous, {{classical}} {{result of}} Huber and Strassen [44] can be {{stated in the}} language of infinite cooperative games. Suppose one wishes to test between the <b>composite</b> <b>hypotheses</b> [...]...|$|R
40|$|The law of {{likelihood}} underlies {{a general}} framework, {{known as the}} likelihood paradigm, for representing and interpreting statistical evidence. As stated, the law applies only to simple hypotheses, {{and there have been}} reservations about extending the law to <b>composite</b> <b>hypotheses,</b> despite their tremendous relevance in statistical applications. This paper proposes a generalization of the law of likelihood for <b>composite</b> <b>hypotheses.</b> The generalized law is developed in an axiomatic fashion, illustrated with real examples, and examined in an asymptotic analysis. Previous concerns about including <b>composite</b> <b>hypotheses</b> in the likelihood paradigm are discussed in light of the new developments. The generalized law of likelihood is compared with other likelihood-based methods and its practical implications are noted. Lastly, a discussion is given on how to use the generalized law to interpret published results of hypothesis tests as reduced data when the full data are not available. Comment: Submitted to the Electronic Journal of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|We {{consider}} {{the problem of}} detecting hiding in the least significant bit (LSB) of images. Since the hiding rate is not known, this is a <b>composite</b> <b>hypothesis</b> testing problem. We show that under a mild condition on the host probability mass function (PMF), the optimal <b>composite</b> <b>hypothesis</b> testing problem is solved by a related optimal simple hypothesis testing problem. We then develop practical tests based on the optimal test and exhibit their superiority over Stegdetect, a popular steganalysis method used in practice...|$|E
3000|$|..., i ∈ { 0, 1 }, are known. When {{there are}} unknown {{parameters}} in the probability density functions (PDFs), {{the test is}} called <b>composite</b> <b>hypothesis</b> testing. Generalized likelihood ratio test (GLRT) is one kind of the <b>composite</b> <b>hypothesis</b> test. In the GLRT, the unknown parameters {{are determined by the}} maximum likelihood estimates (MLE) [19 – 21]. GLRT detectors have been proposed for multi-antenna systems in [19] and for sensing OFDM signals in [20, 21] by taking some of the system parameters, such as channel gains, noise variance, and PU signal variance as the unknown parameters.|$|E
40|$|This paper {{discusses}} {{issues related}} to the inherent ambiguity of the <b>composite</b> <b>hypothesis</b> testing problem, a problem that is central to the detection of target signals in cluttered backgrounds. In particular, the paper examines the recently proposed method of continuum fusion (which, because it combines an ensemble of clair-voyant detectors, might also be called clairvoyant fusion), and its relationship to other strategies for <b>composite</b> <b>hypothesis</b> testing. A specific example involving the affine subspace model adds to the confusion by illustrating irreconcilable differences between Bayesian and non-Bayesian approaches to target detection...|$|E
40|$|Finding the l Most Probable Explanations (MPE) {{of a given}} evidence, S e, in a Bayesian belief network can be {{formulated}} as {{identifying and}} ordering a set of <b>composite</b> <b>hypotheses,</b> H i s, of which the posterior probabilities are the l largest; i. e., P r(H 1 jS e) ::: P r(H l jS e). When an order includes all the <b>composite</b> <b>hypotheses</b> in the network {{in order to find}} all the probable explanations, it becomes a total order and the derivation of such an order has an exponential complexity. The focus of this paper is on the derivation of a partial order, with length l, for finding the l most probable composite hypotheses; where l typically is much smaller than the total number of <b>composite</b> <b>hypotheses</b> in a network. Previously, only the partial order of length two (i. e., l = 2) in a singly connected Bayesian network could be efficiently derived without further restriction on network topologies and the increase in spatial complexity. This paper discusses an efficient algorithm for the deri [...] ...|$|R
40|$|A {{hypothesis}} is nested within {{a more general}} hypothesis when it is a special case of the more general <b>hypothesis.</b> <b>Composite</b> <b>hypotheses</b> consist {{of more than one}} component, and in many cases different <b>composite</b> <b>hypotheses</b> can share some but not all of these components and hence are overlapping. In statistics, coherent measures of fit of nested and overlapping <b>composite</b> <b>hypotheses</b> are technically those measures that are consistent with the constraints of formal logic. For example, the probability of the nested special case must be {{less than or equal to}} the probability of the general model within which the special case is nested. Any statistic that assigns greater probability to the special case is said to be incoherent. An example of incoherence is shown in human evolution, for which the approximate Bayesian computation (ABC) method assigned a probability to a model of human evolution that was a thousand-fold larger than a more general model within which the first model was fully nested. Possible causes of this incoherence are identified, and corrections and restrictions are suggested to make ABC and similar methods coherent. Another coalescent-based method, nested clade phylogeographic analysis, is coherent and also allows the testing of individual components of <b>composite</b> <b>hypotheses,</b> another attribute lacking in ABC and other coalescent-simulation approaches. Incoherence is a highly undesirable property because it means that the inference is mathematically incorrect and formally illogical, and the published incoherent inferences on human evolution that favor the out-of-Africa replacement hypothesis have no statistical or logical validity...|$|R
40|$|Conventional {{tests for}} <b>composite</b> <b>hypotheses</b> in minimum {{distance}} models can be unreliable when {{the relationship between}} the structural and reduced‐form parameters is highly nonlinear. Such nonlinearity may arise for a variety of reasons, including weak identification. In this note, we begin by studying the problem of testing a “curved null” in a finite‐sample Gaussian model. Using the curvature of the model, we develop new finite‐sample bounds on the distribution of minimum‐distance statistics. These bounds allow us to construct tests for <b>composite</b> <b>hypotheses</b> which are uniformly asymptotically valid over a large class of data generating processes and structural models. Massachusetts Institute of Technology (Castle-Krob Career Development Chair) Alfred P. Sloan Foundation (Sloan Research Fellowship...|$|R
3000|$|Interestingly, similar notions {{also come}} up in the study of <b>composite</b> <b>hypothesis</b> testing but in the setting of a {{cooperative}} resource allocation game for infinitely many users. Let [...]...|$|E
40|$|We {{consider}} {{statistical inference}} {{on a single}} component of a parameter vector that satisfies {{a finite number of}} moment inequalities. The null hypothesis for this single component is given a dual characterization as a <b>composite</b> <b>hypothesis</b> regarding point identified parameters. We also are careful in the specification of the alternative hypothesis that also has a dual characterization as a <b>composite</b> <b>hypothesis</b> regarding point identified parameters. This setup substantially simplifies the conceptual basis of the inference problem. For an interval identified parameter we obtain closed form expressions for the confidence interval obtained by inverting the test statistic of the composite null against the composite alternative...|$|E
40|$|The {{sequential}} <b>composite</b> <b>hypothesis</b> contrast multiple-comparison {{procedure is}} introduced for comparing two treatment conditions {{with one or}} two control conditions on one or two outcome measures. The procedure deserves consideration insofar as its power advantage over other commonly applied multiple-comparison methods can be sizable...|$|E
40|$|AbstractFinding the l Most Probable Explanations (MPE) {{of a given}} evidence, Se, in a Bayesian belief network can be {{formulated}} as {{identifying and}} ordering a set of <b>composite</b> <b>hypotheses,</b> His, of which the posterior probabilities are the l largest; ie, Pr(H 1 ¦Se) ≥ … ≥ Pr(H 1 ¦Se). When an order includes all the <b>composite</b> <b>hypotheses</b> in the network {{in order to find}} all the probable explanations, it becomes a total order and the derivation of such an order has an exponential complexity. The focus of this paper is on the derivation of a partial order, with length l, for finding the l most probable composite hypotheses; where l typically is much smaller than the total number of <b>composite</b> <b>hypotheses</b> in a network. Previously, only the partial order of length two (ie, l = 2) in a singly connected Bayesian network could be efficiently derived without further restriction on network topologies and the increase in spatial complexity. This paper discusses an efficient algorithm for the derivation of the partial ordering of the <b>composite</b> <b>hypotheses</b> in a singly connected network with arbitrary order length. This algorithm is based on the propagation of quantitative vector streams in a feed-forward manner to a designated “root” node in a network. The time complexity of the algorithm is in the order of O(lkn); where l is the length of a partial order, k the length of the longest path in a network, and n the maximum number of node states—defined as the product {{of the size of the}} conditional probability table of a node and the number of incoming messages towards the node...|$|R
40|$|The {{estimated}} weighted empirical quantile {{process is}} introduced, and under mild regularity conditions {{is shown to}} converge weakly in L 2 (0, 1) to a Gaussian process. This leads to an elementary approach to the derivation of the asymptotic null distribution of Cramér-von Mises type statistics for testing a <b>composite</b> null <b>hypothesis</b> based on the sample quantile function and estimated parameters. Special emphasis {{is given to the}} location/scale <b>composite</b> null <b>hypothesis.</b> weak convergence Cramer-von Mises statistics sample quantile function <b>composite</b> null <b>hypothesis...</b>|$|R
40|$|In {{recent years}} several authors have {{recommended}} smooth tests for testing goodness of fit. However, {{the number of}} components in the smooth test statistic should be chosen well; otherwise, considerable loss of power may occur. Schwarz's selection rule provides one such good choice. Earlier results on simple null hypotheses are extended here to <b>composite</b> <b>hypotheses,</b> {{which tend to be}} of more practical interest. For general <b>composite</b> <b>hypotheses,</b> consistency of the data-driven smooth tests holds at essentially any alternative. Monte Carlo experiments on testing exponentiality and normality show that the data-driven version of Neyman's test compares well to other, even specialized, tests over a wide range of alternatives. KEY WORDS: Goodness of fit; Monte Car 10 study; Neyman's test; Schwarz's BIC criterion...|$|R
3000|$|In the Neyman-Pearson sense, {{the optimum}} {{solution}} for the <b>composite</b> <b>hypothesis</b> testing problem (1) is the likelihood ratio test (LRT). But for the case at hand, it cannot be implemented due to total ignorance of the signal parameter α, the multichannel AR parameters Q and A [...]...|$|E
30|$|Since the {{distribution}} of the measured level-crossing times in the noise-only case (FPNR= 0) is different from that in case a pulse is present (FPNR≠ 0), as shown in Fig.  4, this characteristic can be exploited in detection of the pulse from the noise floor using <b>composite</b> <b>hypothesis</b> testing.|$|E
40|$|In {{this paper}} the track {{initiation}} problem is formulated as multiple <b>composite</b> <b>hypothesis</b> testing using maximum likelihood estimation with probabilistic data association (ML-PDA). The hypothesis selection {{is based on}} the minimum description length (MDL) criterion. We first review some well-known approaches for statistical model selection and the advantage of the MDL criterion. Then we present one-dimensional examples to illustrate the MDL criterion used in multiple <b>composite</b> <b>hypothesis</b> testing and the performance limit of the ML-PDA for track initiation is interpreted in terms of the sharpness of the hypothesis testing. Finally, we apply the MDL approach for the detection and initiation of tracks of incoming tactical ballistic missiles in the exo-atmospheric phase using a surface-based electronically scanned array (ESA) radar. The targets are characterized by low signal-to-noise ratio (SNR), which leads to low detectio...|$|E
40|$|The {{problem of}} {{sequential}} testing of <b>composite</b> <b>hypotheses</b> under simultaneous distortions of observations and parameters is considered. Asymptotic expansions for the error type I and II probabilities, {{as well as}} for the conditional and unconditional expected sample sizes are constructed to analyse robustness of the considered test...|$|R
40|$|Abstract. The Posterior {{distribution}} of the Likelihood Ratio (PLR) is proposed by Dempster in 1974 for significance testing in the simple vs <b>composite</b> <b>hypotheses</b> case. In this hypotheses test case, classical frequentist and Bayesian hypotheses tests are irreconcilable, as emphasized by Lindley’s paradox, Berger & Selke in 1987 and many others. However, Dempster shows that the PLR (with inner threshold 1) {{is equal to the}} frequentist p-value in the simple Gaussian case. In 1997, Aitkin extends this result by adding a nuisance parameter and showing its asymptotic validity under more general distributions. Here we extend the reconciliation between the PLR and a frequentist p-value for a finite sample, through a framework analogous to the Stein’s theorem frame in which a credible (Bayesian) domain is equal to a confidence (frequentist) domain. This general reconciliation result only concerns simple vs <b>composite</b> <b>hypotheses</b> testing. The measures proposed by Aitkin in 2010 and Evans in 1997 have interesting properties and extend Dempster’s PLR but only by adding a nuisance parameter. Here we propose two extensions of the PLR concept to the general <b>composite</b> vs <b>composite</b> <b>hypotheses</b> test. The first extension can be defined for improper priors as soon as the posterior is proper. The second extension appears from a new Bayesian-type Neyman-Pearson lemma and emphasizes, from a Bayesian perspective, the role of the LR as a discrepancy variable for hypothesis testing. 1...|$|R
40|$|Abstract: In {{this paper}} are {{presented}} more precise results (tables of percentage points and statistic distribution models) for the nonparametric goodness-of-fit tests in testing <b>composite</b> <b>hypotheses</b> using the maximum likelihood estimate (MLE) for {{double exponential distribution}} law. Statistic distributions of the nonparametric goodness-of-fit tests are investigated by the methods of statistical simulation...|$|R
40|$|AbstractThis paper investigates a {{new family}} of {{statistics}} based on Burbea–Rao divergence for testing goodness-of-fit. Under the simple and composite null hypotheses the asymptotic distribution of these tests is shown to be chi-squared. For <b>composite</b> <b>hypothesis,</b> the unspecified parameters are estimated by maximum likelihood as well as minimum Burbea–Rao divergenc...|$|E
40|$|The easily computable {{asymptotic}} {{power of}} the locally asymptotically optimal test of a <b>composite</b> <b>hypothesis,</b> known as the optimal C(α) test, is obtained through a “double” passage to the limit: the number n of observations is indefinitely increased while the conventional measure ξ of the error in the hypothesis tested tends to zero so that ξnn½ → τ ≠ 0. Contrary to this, practical problems require information on power, say β(ξ,n), for a fixed ξ and for a fixed n. The present paper gives the upper and the lower bounds for β(ξ,n). These bounds {{can be used to}} estimate the rate of convergence of β(ξ,n) to unity as n → ∞. The results obtained can be extended to test criteria other than those labeled C(α). The study revealed a difference between situations in which the C(α) test criterion is used to test a simple or a <b>composite</b> <b>hypothesis.</b> This difference affects the rate of convergence of the actual probability of type I error to the preassigned level α. In the case of a simple hypothesis, the rate is of the order of n-½. In the case of a <b>composite</b> <b>hypothesis,</b> the best {{that it was possible to}} show is that the rate of convergence cannot be slower than that of the order of n-½ ln n...|$|E
40|$|A novel {{approach}} is presented for the long-standing problem of <b>composite</b> <b>hypothesis</b> testing. In <b>composite</b> <b>hypothesis</b> testing, unlike in simple hypothesis testing, the probability {{function of the}} observed data given the hypothesis, is uncertain as {{it depends on the}} unknown value of some parameter. The proposed {{approach is}} to minimize the worst-case ratio between the probability of error of a decision rule that is independent of the unknown parameters and the minimum probability of error attainable given the parameters. The principal solution to this minimax problem is presented and the resulting decision rule is discussed. Since the exact solution is, in general, hard to find, and a-fortiori hard to implement, an approximation method that yields an asymptotically minimax decision rule is proposed. Finally, a variety of potential application areas are provided in signal processing and communications with special emphasis on universal decoding...|$|E
40|$|The {{problems}} of robustness of parametric hypotheses testing are {{considered for the}} cases of simple and <b>composite</b> <b>hypotheses.</b> Conditional error probabilities and expected sample sizes of sequential tests are evaluated. Asymptotic expansions for these characteristics are obtained under the distortions of Tukey–Huber type. Robust sequential tests are constructed with the minimax criterion...|$|R
40|$|The {{inference}} {{problem we}} consider {{is that of}} model choice from a sequence of nested linear logistic models. The approach we take is to test successively, from most general to most specific, the corresponding sequence of <b>composite</b> <b>hypotheses.</b> This approach {{is based on the}} very general class of divergence measures, the phi-divergence. ...|$|R
40|$|While in principle, score {{tests and}} C[alpha]-tests provide {{asymptotically}} optimal tests for <b>composite</b> <b>hypotheses,</b> in practice, these techniques {{often lead to}} intractable calculations. The tests proposed here are asymptotically equivalent to C[alpha]-tests, and require only routinely available software, thus greatly enlarging the practical reach of asymptotic theory. C[alpha]-tests score statistic least squares regression...|$|R
