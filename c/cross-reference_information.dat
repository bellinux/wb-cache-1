17|64|Public
50|$|Despite {{the name}} HLASM {{on its own}} does not have many of the {{features}} normally associated with a high-level assembler, but does offer a number of improvements over Assembler H and Assembler(XF), such as labeled and dependent USINGs, more complete <b>cross-reference</b> <b>information,</b> and additional macro language capabilities {{such as the ability}} to write user-defined functions.|$|E
5000|$|The sources used by Collins were British Library Harley MS. 1560 and Stowe MS.555. The manuscripts {{represent}} the [...] "Quenis Majesties juelles plate and other stuff" [...] in 1574 and additions by gift or purchase {{over the next}} 20 years which were kept in the Jewel House at the Tower of London. Collins also collated information from other books and manuscripts to <b>cross-reference</b> <b>information</b> about the objects listed. Gifts of plate to the queen passed from the Privy Chamber to the Jewel House. Some pieces were melted down and others were given as diplomatic gifts. When the queen travelled, the towns she visited often gave her gifts of silver-gilt cups.|$|E
40|$|A set of {{programs}} designed to help R&D programmers document their FORTRAN programs more effectively were written. The central program reads FORTRAN source code and asks the programmer questions about things it has not heard of before. It inserts {{the answers to these}} questions as comments into the FORTRAN code. The comments, as well as extensive <b>cross-reference</b> <b>information,</b> are also written to an unformatted file. Other programs read this file to produce printed information or to act as an interactive document...|$|E
5000|$|Connect <b>cross-references</b> <b>information</b> {{from many}} other UK {{government}} databases, including: ...|$|R
50|$|Below {{is a basic}} {{overview}} list of Mego Micronaut toys with size/scale, release {{dates and}} Mego series numbers as well as <b>cross-referenced</b> <b>information</b> connected to the Takara equivalent toys they were based on; Microman and otherwise. This {{is not meant to}} be a comprehensive list of all Mego releases/variants but rather a high-level overview of their Micronaut line offerings.|$|R
5000|$|GARANT is a comprehensive, fully <b>cross-referenced</b> legal <b>information</b> resource, {{with the}} {{following}} elements: ...|$|R
40|$|During {{the last}} few years, a clear trend to-wards {{standardized}} names and exchange for-mats could be observed {{in the world of}} IT security. For example: • Vulnerability Information: CVE, a list of standard names for Common Vul-nerabilities and Exposures [8] allows the IT-security community to <b>cross-reference</b> <b>information</b> about vulnerabilities. The EISPP/DAF format [3, 4] – in produc-tive use by several CERTs within Europe – allows exchange of security-advisory in-formation. • Incident Information: The IODEF format [6] is used for exchanging incident information between CERTs...|$|E
40|$|In {{cooperation}} with several {{federal and state}} agencies, the Information Center for the Environment (ICE) at the University of California Davis is analyzing the ability to <b>cross-reference</b> <b>information</b> linked to California's hydrography dataset (a modified form of the U. S. Environmental Protection Agency's River Reach File) to the new National Hydrography Dataset. California's Reach File contains a critical but non-standard foreign key that is essential to many research applications. Additionally, ICE plans to test, verify, and recommend enhancements to cross-referencing methods to improve the transfer of reach-code information from earlier hydrography datasets in California to the NHD...|$|E
40|$|Interested in {{when your}} house was built? Curious about who owned your home before you, {{or what was}} on the land a cen-tury ago? The City of Toronto Archives {{contains}} many sources that can help you answer these questions. This guide explains the best sources for such research, how you use them, and what kind of information you can expect to fi nd. Researching a property can be like assembling a jigsaw puzzle—one with missing pieces, as well as pieces that belong to another puzzle! Be patient. Assemble information before you come to any conclusions. <b>Cross-reference</b> <b>information</b> from a variety of sources. Be aware that not all source...|$|E
50|$|Lemonade uses {{artificial}} {{intelligence in the}} form of chatbots and machine learning to provide insurance policies and handle claims, replacing brokers and paperwork traditionally associated with the process. When a customer applies for insurance, the company’s software pulls data and <b>cross-references</b> <b>information</b> about a particular home or neighborhood from a variety of sources. Policyholders file claims through Lemonade’s app with the company’s chatbot ‘A.I. Jim’ who reviews the claim, cross-checks it with the policy, runs 18-fraud algorithms, and determines whether or not to approve the claim.|$|R
30|$|Exploitation phase: In the {{exploitation}} phase, all the addresses in the cache template matrix are continuously monitored and cache hits are recorded. Events are detected by <b>cross-referencing</b> the <b>information</b> gathered with {{data in the}} matrix.|$|R
30|$|The {{ability to}} {{understand}} user’s interests and predict their behavior based on collected data is desirable in several commercial models and consequently a hot topic in the scientific literature [13 – 15]. However, the importance of privacy-preserving practices is still underestimated, a challenge to overcome. For instance, despite the anonymization efforts of Netflix, Narayanan and Shmatikov brilliantly demonstrated how to break anonymity of the Netflix’s dataset by <b>cross-referencing</b> <b>information</b> with public knowledge bases, as those provided by the Internet Movie Database (IMDB) [16]. Using a similar approach, New York Times’ reporters were capable of relating a subset of queries to a particular person by joining apparently innocent queries to non-anonymous real state public databases [17].|$|R
40|$|Abstract: This paper {{follows from}} {{previous}} research on the relatedness or un-relatedness, in Roget's International Thesaurus (RIT), of entries which have identical spellings. A comparison is made between the output from research on a VAX 11 / 780 computer-accessible version of RIT, the RIT text, and a micro-computer database of the hierarchical and <b>cross-reference</b> <b>information</b> of RIT. The author believes that all meaning is defined by associations and that {{the context of a}} word (the words associated with it) define its meaning in that context. Roget's Thesaurus is a culturally validated instantiation of an abstract thesaurus in which words are organized according to their relatedness...|$|E
40|$|Scribble is {{a system}} for writing library documentation, user guides, and tutorials. It builds on PLT Scheme’s {{technology}} for language extension, and at its heart is {{a new approach to}} connecting prose references with library bindings. Besides the base system, we have built Scribble libraries for JavaDoc-style API documentation, literate programming, and conference papers. We have used Scribble to produce thousands of pages of documentation for PLT Scheme; the new documentation is more complete, more accessible, and better organized, {{thanks in large part to}} Scribble’s flexibility and the ease with which we <b>cross-reference</b> <b>information</b> across levels. This paper reports on the use of Scribble and on its design as both a...|$|E
40|$|Abstract. We {{develop a}} method that can detect humans in a single image based on a new {{cascaded}} structure. In our approach, both the rectangle features and 1 -D edge-orientation features are employed in the feature pool for weak-learner selection, which can be computed via the integral-image and the integral-histogram techniques, respectively. To make the weak learner more discriminative, Real AdaBoost is used for feature selection and learning the stage classifiers from the training images. Instead of the standard boosted cascade, a novel cascaded structure that exploits both the stage-wise classification information and the interstage <b>cross-reference</b> <b>information</b> is proposed. Experimental results show that our approach can detect people with both efficiency and accuracy. ...|$|E
50|$|NOTAMs {{issued by}} a flight service station to {{highlight}} or point out another NOTAM, {{such as an}} FDC or NOTAM (D) NOTAM. This type of NOTAM will assist users in <b>cross-referencing</b> important <b>information</b> {{that may not be}} found under an airport or NAVAID identifier.|$|R
30|$|Relating image {{cases to}} {{external}} data (data, specimen): This involves: (a) receiving notifications from external parties, (b) <b>cross-referencing</b> identifiable <b>information</b> {{to connect the}} two databases and determine subjects for which this is feasible and reliable enough, and (c) updating the database with the references of external data.|$|R
40|$|The {{following}} research {{will take a}} closer look at the cross-Strait relations of mainland China and Taiwan starting from the 2008 (presidential and legislative) elections in Taiwan, when the Nationalist Party Kuomintang and their presidential candidate Ma Ying-jeou came to power. During the course of the work the political actions and statements of the leaders of the People’s Republic of China (PRC), the Republic of China (ROC) and the representatives of the relevant institutions such as the Taipei-based Straits Exchange Foundation (SEF) and the Beijing-based Association for Relations Across the Taiwan Straits (ARATS) will be reviewed while <b>cross-referencing</b> <b>information</b> about the concurrent public opinion surveys on Taiwan. This will help understand the public’s response to the KMT policies and therefore to deduct if the policies can be called legitimate...|$|R
40|$|There {{are several}} tools for {{generating}} PDF output from a TEX document. By choosing the appropriate tools and configuring them properly, {{it is possible}} to reduce the PDF output size by a factor of 3 or even more, thus reducing document download times, hosting and archiving costs. We enumerate the most common tools, and show how to configure them {{to reduce the size of}} text, fonts, images and <b>cross-reference</b> <b>information</b> embedded into the final PDF. We also analyze image compression in detail. We present a new tool called pdfsizeopt. py which optimizes the size of embedded images and Type 1 fonts, and removes object duplicates. We also propose a workflow for PDF size optimization, which involves configuration of TEX tools, running pdfsizeopt. py and the Multivalent PDF compressor as well. ...|$|E
40|$|Abstract—We {{propose a}} method that can detect humans in a single image based on a novel {{cascaded}} structure. In our approach, both intensity-based rectangle features and gradient-based 1 -D features are employed in the feature pool for weak-learner selection. The Real AdaBoost algorithm is used to select critical features from a combined feature set and learn the classifiers from the training images for each stage of the cascaded structure. Instead of using the standard boosted cascade, the proposed method employs a novel cascaded structure that exploits both the stage-wise classification information and the interstage <b>cross-reference</b> <b>information.</b> We introduce meta-stages to enhance the detection performance of a boosted cascade. Experiment {{results show that the}} proposed approach achieves high detection accuracy and efficiency. Index Terms—AdaBoost, cascaded feed-forward classifiers, edge-density features, edge orientation histograms, human detection, meta-stage, pedestrian detection, real AdaBoost, rectangle features. I...|$|E
40|$|Background: Tagging gene/protein {{names in}} text and mapping them to {{database}} entries are critical tasks in biological literature mining. Most {{of the existing}} tagging and normalization approaches, however, have not been evaluated for practical use in article retrieval towards efficient biocuration. Results: By utilizing literature <b>cross-reference</b> <b>information</b> provided by NCBI Entrez Gene database, {{we found that the}} coverage of gene/protein databases with respect to gene/protein names found in text is around 94 %. The upper bound of the recall in retrieving MEDLINE citations by gene/protein names is around 70 - 80 % when citations crossreferred by many genes are overlooked and flexible matching of names are used. Of genes/proteins failed to be retrieved by names, over 30 % are caused by citations not discussing cross-referred genes/proteins in the abstracts and around 60 % are caused by the gene/protein name tagging system trained on the BioCreAtIvE II gene mention corpus. Conclusions: The study demonstrates that existing gene/protein databases have a decent coverage of gene/protein names used in MEDLINE abstracts. Approaches and data resources for gene/protein tagging and mapping need to be selected appropriately for individual practical tasks. ...|$|E
40|$|One of the {{greatest}} technological improvements {{in recent years is}} the rapid progress using machine learning for processing visual data. Among all factors that contribute to this development, datasets with labels play crucial roles. Several datasets are widely reused for investigating and analyzing different solutions in machine learning. Many systems, such as autonomous vehicles, rely on components using machine learning for recognizing objects. This paper compares different visual datasets and frameworks for machine learning. The comparison is both qualitative and quantitative and investigates object detection labels with respect to size, location, and contextual information. This paper also presents a new approach creating datasets using real-time, geo-tagged visual data, greatly improving the contextual information of the data. The data could be automatically labeled by <b>cross-referencing</b> <b>information</b> from other sources (such as weather) ...|$|R
50|$|Another {{influence}} on PKBs are systems whose primary {{purpose is to}} help users organize documents, rather than personal knowledge derived from those documents. Such systems do not encode subjective knowledge per se, but they do create a personal knowledge base of sorts by allowing users to organize and <b>cross-reference</b> their <b>information</b> artifacts.|$|R
40|$|Electronic book is an {{application}} with a multimedia database of instructional resources, which include hyperlinked text, instructor's audio/video clips, slides, animation, still images, etc. {{as well as}} contentbased information about these data, and metadata such as annotations, tags, and <b>cross-referencing</b> <b>information.</b> Electronic books in the Internet or on CDs today {{are not easy to}} learn from. We propose the use of a multimedia database of instructional resources in constructing and delivering multimedia lessons about topics in an electronic book. We introduce an electronic book data model containing (a) topic objects and (b) instructional resources, called instruction module objects, which are multimedia presentations possibly capturing real-life lectures of instructors. We use the notion of topic prerequisites for topics at different detail levels, to allow electronic book users to request/compose multimedia lessons about topics in the electronic book. We present automated construc [...] ...|$|R
40|$|This {{contribution}} outlines {{a conceptual}} {{analysis of the}} dictionary-internal cross-reference structure in electronic dictionaries {{along the lines of}} Wiegand’s actional-theoretical text theory of print dictionaries. The discussion focuses on issues of XML-based data modeling, using the monolingual German online dictionary elexiko as a running example. The first part of the article demonstrates how Wiegand’s formal theory of mediostructure and its intricate nomenclature can be extended in a systematic and lexicographically justified way to cover the structure of the underlying lexicographical database of online dictionaries. The second part of the article applies the concepts developed to a more technical question, examining the extent to which <b>cross-reference</b> <b>information</b> can be stored and processed separately from the dictionary entry documents, e. g., in a relational database. The results are largely negative; in most real world cases, this leads to an unwanted duplication of XML-related structural information. The concluding third part briefly describes the strategy chosen for elexiko: mediostructural information is not externalized at all; cross-reference consistency checks are performed by a dictionary editing tool that takes advantage of a specialized XML database index and can easily be made more efficient and scalable by using a simple caching technique...|$|E
40|$|The Macromolecular Structure Database (MSD) group ([URL] {{continues}} {{to enhance the}} quality and consistency of macromolecular structure data in the worldwide Protein Data Bank (wwPDB) and to work towards the integration of various bioinformatics data resources. One of the major obstacles to the improved integration of structural databases such as MSD and sequence databases like UniProt {{is the absence of}} up to date and well-maintained mapping between corresponding entries. We have worked closely with the UniProt group at the EBI to clean up the taxonomy and sequence <b>cross-reference</b> <b>information</b> in the MSD and UniProt databases. This information is vital for the reliable integration of the sequence family databases such as Pfam and Interpro with the structure-oriented databases of SCOP and CATH. This information has been made available to the eFamily group ([URL] and now forms the basis of the regular interchange of information between the member databases (MSD, UniProt, Pfam, Interpro, SCOP and CATH). This exchange of annotation information has enriched the structural information in the MSD database with annotation from wider sequence-oriented resources. This work was carried out under the ‘Structure Integration with Function, Taxonomy and Sequences (SIFTS) ’ initiative ([URL] in the MSD group...|$|E
40|$|RECODER is a Java {{framework}} {{aimed at}} source code analysis and metaprogramming. It works on several layers {{to offer a}} set of semi-automatic transformations and tools, ranging from a source code parser and unparser, offering a highly detailed syntactical model, analysis tools which are able to infer types of expressions, evaluate compile-time constants and keep <b>cross-reference</b> <b>information,</b> to transformations of the very Java sources, containing a library of common transformations and incremental analysis capabilities. These make up an useful set of tools which can be extended to {{provide the basis for}} more advanced refactoring and metacompiler applications, in very different fields, from code beautification and simple preprocessors, stepping to software visualization and design problem detection tools to adaptive programming environments and invasive software composition. The core system development of RECODER started in the academic field and as such, it was confined into a small platform of users. Although a powerful tool, RECODER framework lacks usability and requires extensive and careful configuration to work properly. In order to overcome such limitations, we have taken advantage of the Eclipse Integrated Development Environment (Eclipse IDE) developed by IBM, specifically its Plugin Framework Architecture to build a tool and a vehicle where to integrate RECODER functionalities into a wide-used, well-known platform to provide a semi-automated and user-friendly interface. In this bachelor thesis we will document how we have integrated RECODER into an Eclipse plug-in to perform a “proof of concept” transformation of an Eclipse Java Project, directly mapping the Eclipse-generated configuration of the project into RECODER's to automate the tedious task of manually configuring RECODER. ...|$|E
5000|$|Six years: the {{dematerialization}} {{of the art}} object from 1966 to 1972; a <b>cross-reference</b> book of <b>information</b> on some esthetic boundaries. New York: Praeger. 1973.|$|R
40|$|Data quality {{limits the}} {{accuracy}} of biometrics. Poor data quality is responsible for many or even most matching errors in biometric systems and may be the greatest weakness of some implementations. This paper uses an expansive view of data quality focused on its implications for operational databases and systems. Data quality is defined here not just in regard to samples (images), but also in regard to metadata, which is the information associated with the samples, such as biographic or <b>cross-referencing</b> <b>information.</b> The impact of poor data quality can be reduced in various ways, many of which depend on effective methods of automated data quality measurement. This paper analyzes the causes and implications of poor-quality biometric data, methods of measurement, prevention, and potential remedies. The paper draws {{on the experience of}} the authors in engineering and analysis work on a variety of large-scale operational biometric systems and biometric evaluations...|$|R
25|$|In August, the synagogues {{were also}} ordered to produce lists of Jewish {{individuals}} who were not members. The resulting lists were <b>cross-referenced</b> with <b>information</b> Nasjonal Samling had compiled previously and information from the Norwegian Central Bureau of statistics. In the end, occupying authorities in Norway had a more complete list of Jewish residents in Norway than most other countries under Nazi rule.|$|R
40|$|The aim of {{this study}} is to have a good {{understanding}} of the environmental impact of glucose production. Glucose is generally produced from corn or wheat. Since agricultural processes are known to be difficult to evaluate by LCA, the results obtained with two different LCA databases, Gabi and EcoInvent, are compared in this work. The production of glucose from raw materials can be divided in two steps: the agricultural step allowing the plant production, and the conversion step including the extraction of the starch from the plant and its hydrolysis into glucose. Preliminary results underline the high impact of the agricultural step, so a special attention has been paid to these data. Specific Belgian data collected by the Walloon Agricultural Research Centre (CRA-W) (2014) [1] have been used as primary data (yield, amount of fertilizers, etc.), either using EcoInvent or Gabi datasets background data to model fertilizers, diesel consumption, etc. A third model was built using only data available in Ecoinvent for corn and wheat cultures. For the conversion step, literature data have been used along with some industrial data. As few studies are available in the literature concerning starch hydrolysis, the focus has been placed on data validation (mass balance checks, <b>cross-reference</b> <b>information,</b> etc.). Based on these multiple sources, it is possible to compare the LCA results for the production of 1 kg of glucose for three different cases, summarized in the following table. Table 1 : Summary of modelled cases 	Agricultural step	Conversion steps 	Primary data	Dataset	Primary data	Dataset Case 1 	Belgian	GaBi	Literature + Industry	GaBi Case 2 	Belgian	Ecoinvent	Literature + Industry	Ecoinvent Case 2 	Ecoinvent	Literature + Industry 	Ecoinvent The results obtained using these three models will be presented, at both the inventory and impact assessment steps. They show significant differences and highlight the need to understand in depth the involved assumptions when developing the datasets, in addition to the ones adopted for the inventory. Peer reviewe...|$|E
40|$|The aim of {{this study}} is to have a good {{understanding}} of the environmental impact of glucose production to be able to study material produce from glucose. Glucose is general-ly produced from corn or wheat. Since agricultural processes are known to be difficult to evaluate by LCA, the results obtained with two different LCA databases, Gabi and EcoIn-vent, are compared in this work. The production of glucose from raw materials can be divided in two steps: the agricul-tural step allowing the plant production, and the conversion step including the extraction of the starch from the plant and its hydrolysis into glucose. Preliminary results underline the high impact of the agricultural step, so a special attention has been paid to these data. Specific Belgian data collected by the Walloon Agricultural Research Centre (CRA-W) (2014) [1] have been used as primary data (yield, amount of fertilizers, etc.), either using EcoInvent or Gabi datasets background data to model fertilizers, diesel consumption, etc. A third model was built using only data available in Ecoinvent for corn and wheat cultures. For the conversion step, literature data have been used along with some industrial data. As few studies are available in the literature concerning starch hydrolysis, the focus has been placed on data validation (mass balance checks, <b>cross-reference</b> <b>information,</b> etc.). Based on these multiple sources, it is possible to compare the LCA results for the pro-duction of 1 kg of glucose for three different cases, summarized in the following table. Table 1 : Summary of modelled cases 	Agricultural step	Conversion steps 	Primary data	Dataset	Primary data	Dataset Case 1 	Belgian	GaBi	Literature + Industry	GaBi Case 2 	Belgian	Ecoinvent	Literature + Industry	Ecoinvent Case 2 	Ecoinvent	Literature + Industry 	Ecoinvent The results obtained using these three models will be presented, at both the inventory and impact assessment steps. They show significant differences and highlight the need to understand in depth the involved assumptions when developing the datasets, in addition to the ones adopted for the inventory. These differences in results lead to higher uncer-tainties, allowing only to have a range of possible values as a result. We make the choice to only communicate this range of value and not an absolute value when communicating the results of this study but this leads to some disadvantages such as difficulties in com-parison, etc. Peer reviewe...|$|E
40|$|Ideally, project {{documentation}} {{should be}} complete {{and there would}} be no need for subcontractors to seek further information from that which has already been provided. In practice, this is rarely the case. The use of 'Request For Information' (RFI) as a formalised process, by which information is gathered or clarified is very common throughout the Australian construction industry. This paper focuses on the use of simulation-based modelling to quantify the time and cost associated with this process as currently communicated between construction organisations. Information gathered from construction projects plus expert advice sought from industry professionals is incorporated as model input. The model shows that the mean cycle time for a typical RFI can be as high as 17 person-hours with most of that time being spent on gathering and <b>cross-referencing</b> <b>information.</b> The simulation model was then modified to explore the potential of implementing Electronic Data Management Technologies as a tool to significantly reduce the time and cost associated with the traditional paper-based RFI process. ...|$|R
5000|$|Even when no {{specific}} information is returned, {{public and private}} databases exist that <b>cross-reference</b> skiptracing <b>information</b> with others the [...] "skip" [...] may have lived with in the recent past. For instance, if previous records show a [...] "skip" [...] {{lived in the same}} house as a third party, the third party may also be [...] "skiptraced" [...] in an effort to locate the [...] "skip".|$|R
30|$|Semi-structured {{interviews}} were also given {{after completion of}} the first survey to qualitatively look at the issues these students were having within the post-graduate institution/Japan and also to gain insight into the preconceived notions of what participants expected of Japanese life. By <b>cross-referencing</b> the <b>information</b> gathered from the individuals during the interview process with the survey data, the interviewees are also looked at in relation to constancy of answers.|$|R
40|$|Abstract — While {{regulations}} provide {{many social}} benefits, such as protecting our environment and improving public safety, {{the complexity and}} volume of government regulations and related information are detrimental to business and hinder public understanding. The burden of complying with regulations can fall disproportionately on small businesses. Governmental portals have emerged to facilitate public access to government information and services {{but most of them}} are designed primarily for displaying the regulatory information and often usable only by experienced users, who are familiar with the subject and the portal. It remains difficult to locate <b>cross-referenced</b> <b>information</b> and to link regulatory information with useful applications. The REGNET project has dealt with four aspects in the utilization of IT related to regulatory information: (1) development of a regulationcentric compliance assistance framework [1]; (2) development of a relatedness analysis framework for comparing regulatory documents and supporting e-rulemaking analysis [2]; (3) utilizing text-mining techniques to gauge involvement of public agencies in environmental management [3]; and (4) developing an ontology-based framework for retrieval of patents and related legal and regulatory information [4]...|$|R
