0|10000|Public
40|$|Abstract. Is {{trust to}} web pages related to nation-level factors? Do trust levels {{change in time}} and how? What <b>categories</b> (topics) <b>of</b> <b>pages</b> tend to be {{evaluated}} as not trustworthy, and what <b>categories</b> <b>of</b> <b>pages</b> tend to be trustworthy? What could be the reasons of such evaluations? The goal {{of this paper is}} to answer these questions using large scale data of trustworthiness <b>of</b> web <b>pages,</b> two sets <b>of</b> websites, Wikipedia and an international survey...|$|R
40|$|Abstract — Optical {{character}} recognition {{is an active}} field for recognition pattern. In this paper we tried to present how processes work in OCR system, pre-processing in OCR and document analysis. To review the process for analysis pattern from document proper page segmentation should be done. So various <b>categories</b> <b>of</b> <b>page</b> segmentation algorithms ar...|$|R
40|$|Several {{studies show}} that the {{distribution}} of the number of links per web page follows a power law in the limit for large numbers of links. The same power law scaling appears in the connectivity distributions of a variety of other naturally occurring networks. For most of these networks, connectivity distributions drop o signicantly from power law scaling at small to moderate numbers of edges. We nd that, within particular <b>categories</b> <b>of</b> web <b>pages,</b> the deviation from a power law is especially prominent. The category-specic distributions actually exhibit a unimodal body and a power law tail, with modes varying across dierent <b>categories</b> <b>of</b> <b>pages.</b> We develop a generative model that accounts for the full shape of distributions|both body and tail|for subsets <b>of</b> web <b>pages</b> within the same category, for the web as a whole, and for other sociological networks. The model naturally predicts both the power law exponent at large connectivities, and the magnitude of deviation from power law at smaller connectivities, matching well with empirically observed dierences among real networks...|$|R
50|$|The Prague Post has won {{a number}} of European awards for its {{innovative}} newspaper design. In November 2007, the paper saw a fifth consecutive win in the <b>category</b> <b>of</b> “Front <b>Page</b> Weekly Newspaper” {{as well as a}} Portfolio award for Design Editor Caroline Wren at the European Newspaper Awards.|$|R
40|$|Abstract. The {{accurate}} {{prediction of}} user {{behavior on the}} Web has immense commercial value as the Web evolves into a primary medium for marketing and sales for many businesses. This broad and complex problem {{can be broken down}} into three more understandable problems: predicting (1) short and long visit sessions, (2) first three most probable <b>categories</b> <b>of</b> <b>pages</b> visited in a session, and (3) number <b>of</b> <b>page</b> views per <b>category</b> in a visit session. We present Bayesian solutions to these problems. The focus in our solutions is accuracy and computational efficiency rather than modeling the complex Web surfer behavior. We evaluate our solutions on four weeks of surfer data made available by the ECML/PKDD Discovery Challenge. Probabilities are estimated from the first three weeks of data and the resulting Bayesian models tested on last week’s data. The results confirm the high accuracy and good efficiency of our solutions. ...|$|R
30|$|We perform several {{experiments}} to investigate {{what can be}} seen in a single fixation, 120  ms, on a web <b>page.</b> Display times <b>of</b> this magnitude are typical for similar studies with natural scenes (Fei-Fei et al., 2007). In Experiment 1, we ask whether observers can rapidly ascertain the <b>category</b> <b>of</b> a web <b>page.</b> This is a new question in the human vision literature. Common wisdom in HCI suggests that a user cannot acquire much semantic information, such as the <b>category</b> <b>of</b> a web <b>page</b> or meaning <b>of</b> any text, in a presentation time of less than 500  ms (e.g., Lindgaard et al., 2006). However, researchers have not actually tested this hypothesis.|$|R
40|$|Wikipedia {{serves as}} the Internet 2 ̆ 7 s most widely viewed reference. In order to ensure its success, editors who create and {{maintain}} articles must resolve conflicts over appropriate article content. Previous research has measured Wikipedia conflict at two levels: single articles and <b>categories</b> <b>of</b> <b>pages.</b> I observe conflicts within small groups of articles, identifying their frequency, size, and intensity. Additionally, I identify individual conflicts spanning multiple articles and effects of conflict upon users 2 ̆ 7 editing habits. I analyze cross-article conflict in three stages. First, I cluster a group of 1. 4 million Wikipedia articles. Next, I find individual user conflicts within each article cluster using a list of reverts. Finally, I characterize the individual conflicts and analyze population statistics. While most conflicts are low-intensity and {{take place in a}} single article, high-intensity conflicts frequently span multiple articles...|$|R
30|$|In {{addition}} to the researchers’ specific educational interests, BROAD-RSI was able to identify several other interests, filtered by pre-selected categories. Each page on Facebook has a <b>category</b> or set <b>of</b> <b>categories.</b> The selection <b>of</b> <b>pages</b> considered in the extraction of interests is based on their associated categories, in this case disregarding those not related to educational issues.|$|R
40|$|The {{vast amount}} of user-generated content on the Web has {{increased}} the need for handling the problem of automatically processing content in web <b>pages.</b> The segmentation <b>of</b> web <b>pages</b> and noise (non-informative segment) removal are important pre-processing steps {{in a variety of}} applications such as sentiment analysis, text summarization and information retrieval. Currently, these two tasks tend to be handled separately or are handled together without emphasizing the diversity of the web corpora and the web page type detection. We present a unified approach that is able to provide robust identification of informative textual parts in web pages along with accurate type detection. The proposed algorithm takes into account visual and non-visual characteristics <b>of</b> a web <b>page</b> and is able to remove noisy parts from three major <b>categories</b> <b>of</b> <b>pages</b> which contain user-generated content (News, Blogs, Discussions). Based on a human annotated corpus consisting of diverse topics, domains and templates, we demonstrate the learning abilities of our algorithm, we examine its e↵ectiveness in extracting the informative textual parts and its usage as a rule-based classifier for web page type detection in a realistic web setting...|$|R
30|$|Firstly, we show users {{attention}} {{patterns with}} respect to different kind of contents, and then we look at who are the users more prone to interact with intentional false information according to the content they are usually exposed to. To do this, {{we focus on the}} Italian Facebook ecosystem and we define three <b>categories</b> <b>of</b> <b>pages</b> according to the kind of information they promote. The categorization <b>of</b> the <b>pages</b> is based on their different social functions together with the type of information they disseminate. The first <b>category</b> includes all <b>pages</b> <b>of</b> main stream newspapers; the second <b>category</b> consists <b>of</b> alternative information sources – pages which disseminate controversial information, often lacking supporting evidence and sometimes contradictory of the official news (e.g. conspiracy theories, link between vaccines and autism, etc); the third <b>category</b> is that <b>of</b> self-organized online political movements – with the role of gathering users to publicly convey discontent against the current political and socio-economic situation (e.g. one major political party in Italy has most of its activity online). The criteria to define pages categories are based on pages’ self-description and the kind of content they disseminate. All national newspapers active of Facebook belongs to mainstream news. <b>Pages</b> <b>of</b> political activism and alternative news have been identified with the help of Facebook groups very active in the debunking unsubstantiated rumors. However, all <b>pages</b> <b>of</b> alternative news in their mission declare to have the role to disseminate information neglected by “manipulated main stream media”.|$|R
40|$|This paper proposes an {{information}} system that classifies web pages according a taxonomy, which is mainly used from seven search engines/directories. The proposed classifier is a fourlayer Generalised Regression Neural Network (GRNN) {{that aims to}} perform the information segmentation according to information filtering techniques using content descriptor vectors. Eight <b>categories</b> <b>of</b> web <b>pages</b> were used in order to evaluate the robustness of the method, while no restrictions were imposed except for {{the language of the}} content, which is English. The system can be used as an assistant and consultative tool for classification purposes as well as for estimating the population <b>of</b> web <b>pages</b> at any given point in time...|$|R
40|$|Francisco Marmolejo {{pointed out}} a mistake in the {{statement}} of Proposition 4. 4 in our TAC paper (Vol. 16, No. 28). The mistaken version is used later in that paper. Our purpose here is to correct the error by providing an explicit description of the finite coproduct completion of the dual <b>of</b> the <b>category</b> <b>of</b> connected G-sets. The description uses the distinguished morphisms of a factorization system on the <b>category</b> <b>of</b> G-sets. 6 <b>page(s...</b>|$|R
40|$|International audienceIn this paper, {{we present}} a {{framework}} for evaluating segmentation algorithms for Web pages. Web page segmentation consists in dividing a Web page into coherent fragments, called blocks. Each block represents one distinct information element in the page. We define an evaluation model that includes different metrics to evaluate {{the quality of a}} segmentation obtained with a given algorithm. Those metrics compute the distance between the obtained segmentation and a manually built segmentation that serves as a ground truth. We apply our framework to four state-of-the-art segmentation algorithms (BOM, Block Fusion, VIPS and JVIPS) on several <b>categories</b> (types) <b>of</b> Web <b>pages.</b> Results show that the tested algorithms usually perform rather well for text extraction, but may have serious problems for the extraction of geometry. They also show that the relative quality of a segmentation algorithm depends on the <b>category</b> <b>of</b> the segmented <b>page...</b>|$|R
5000|$|In early 2012, Apartment Therapy {{incorporated}} {{three of}} its companion blogs into the main site. Ohdeedoh moved to the [...] "Family channel" [...] on Apartment Therapy.com, Unplggd to the [...] "tech channel," [...] and Re-Nest relocated to the [...] "Green Living" [...] <b>category</b> <b>of</b> the main <b>page.</b> The Kitchn kept its separate URL but was linked from Apartment Therapy's site.|$|R
40|$|Abstract. Maintaining {{contents}} of Web sites {{is an open}} and urgent problem on the current World Wide Web {{as well as on}} company intra-nets. Although many current tools deal with problems such as broken links and missing images, very few solutions exist for maintaining the {{contents of}} Web sites and intra-nets. We present a knowledge-based approach to the verification of Web-page contents. The user exploits semantic markup in Webpages to formulate rules and constraints that must hold on the information in a site. An inference engine subsequently uses these rules to categorise Web-pages in an ontology <b>of</b> <b>pages,</b> while the constraints are used to define <b>categories</b> <b>of</b> <b>pages</b> which contain errors. We have constructed WebMaster, a software tool for knowledge-based verification of Web-pages. WebMaster allows the user to define rules and constraints in a graphical format, and is then able to use these rules to detect outdated, inconsistent and incomplete information in Web-pages. In this paper, we describe the various options for semantic markup on the Web, we define a precise logical and graphical format for rules and constraints, and we report on our practical experiences with WebMaster. Acknowledgements The work reported in this paper has only been possible with the contributions from all current and past members of the WebMaster team at AIdministrator: Jan Bakker, Chris Fluit...|$|R
40|$|In {{this paper}} we {{describe}} a practical approach for modeling navigation patterns of visitors of unstructured websites. These patterns {{are derived from}} web logs that are enriched with 3 sorts of information: (1) content type <b>of</b> visited <b>pages,</b> (2) visitor type, and (3) location of the visitor. We developed an intelligent Text Mining system, iTM, which supports the process <b>of</b> classifying web <b>pages</b> into a number <b>of</b> pre-defined <b>categories.</b> With help <b>of</b> this system {{we were able to}} reduce the labeling effort by a factor 10 - 20 without affecting the accuracy of the final result too much. Another feature of our approach is the use of a new technique for modeling navigation patterns: navigation trees. They provide a very informative graphical representation of most frequent sequences <b>of</b> <b>categories</b> <b>of</b> visited <b>pages.</b> 1...|$|R
40|$|Accepted at The Workshop on the Economics of Information Security (WEIS), 2016 Free {{content and}} {{services}} on the Web are often supported by ads. However, with {{the proliferation of}} intrusive and privacy-invasive ads, a significant proportion of users have started to use ad blockers. As existing ad blockers are radical (they block all ads) and are not designed taking into account their economic impact, ad-based economic model of the Web is in danger today. In this paper, we target privacy-sensitive users and provide them with fine-grained control over tracking. Our working assumption is that some <b>categories</b> <b>of</b> web <b>pages</b> (for example, related to health, religion, etc.) are more privacy-sensitive to users than others (education, science, etc.). Therefore, our proposed approach consists in providing users with an option to specify the <b>categories</b> <b>of</b> web <b>pages</b> that are privacy-sensitive to them and block trackers present on such web pages only. As tracking is prevented by blocking network connections of third-party domains, we avoid not only tracking but also third-party ads. Since users will continue receiving ads on web pages belonging to non-sensitive categories, our approach essentially provides a trade-off between privacy and economy. To test the viability of our solution, we implemented it as a Google Chrome extension, named MyTrackingChoices (available on Chrome Web Store). Our real-world experiments with MyTrackingChoices show that the economic impact of ad blocking exerted by privacy-sensitive users can be significantly reduced...|$|R
40|$|One {{important}} {{challenge in}} the field of recommender systems is the sparsity of available data. This problem limits the ability of recommender systems to provide accurate predictions of user ratings. We overcome this problem by using the publicly available user generated information contained in Wikipedia. We identify similarities between items by mapping them to Wikipedia pages and finding similarities in the text and commonalities in the links and <b>categories</b> <b>of</b> each <b>page.</b> These similarities can be used in the recommendation process and improve ranking predictions. We find that this method is most effective in cases where ratings are extremely sparse or nonexistent. Preliminary experimental results on the Movielens dataset are encouraging...|$|R
5000|$|Approximately 200 {{parents and}} {{teachers}} consult with the company to prescreen sites to permit children to access the Internet a safe, controlled manner. The browser provides a portal to 1.5 million pre-screened websites in 8,600 <b>categories</b> and millions <b>of</b> <b>pages</b> with age-appropriate content. [...] The downloadable browser and search engine further divides webpages, videos and pictures by age category. KidZui launched the patented browser in spring 2008.|$|R
40|$|Cataloged from PDF {{version of}} article. Thesis (M. S.) : Bilkent University, Department of Computer Engineering, İhsan Doğramacı Bilkent University, 2017. Includes bibliographical {{references}} (leaves 53 - 57). A cluster {{is a set}} of related documents. Cluster labeling is the process of assigning descriptive labels to clusters. This study investigates several cluster labeling approaches and presents novel methods. The rst uses clusters themselves and extracts important terms, which distinguish clusters from each other, with different statistical feature selection methods. Then it applies di erent data fusion methods for combining their outcomes. Our results show that although it provides statistically signi cantly better results for some cases, it is not a stable and reliable labeling method. This {{can be explained by the}} fact that a good label may not occur in the cluster at all. The second exploits Wikipedia as an external resource and uses its anchor texts and categories to enrich the label pool. Labeling with Wikipedia anchor text fails because the suggested labels tend to focus on minor topics. Although the minor topics are related to the main topic, they do not exactly describe it. After this observation, we use <b>categories</b> <b>of</b> Wikipedia <b>pages</b> to improve our label pool in two ways. The rst fuses important terms and Wikipedia categories with rank based fusion methods. The second looks relatedness <b>of</b> Wikipedia <b>pages</b> to the clusters and use only <b>categories</b> <b>of</b> related <b>pages.</b> The experimental results show that both methods provide statistically signi - cantly better results than the other cluster labeling approaches that we examine in this study. by Gökçe Ayduğan. M. S...|$|R
40|$|In {{this paper}} {{we present a}} new {{approach}} for obtaining the terminology of a given domain using the <b>category</b> and <b>page</b> structures <b>of</b> the Wikipedia in a language independent way. The idea is to take profit <b>of</b> <b>category</b> graph <b>of</b> Wikipedia starting with a top category that we identify {{with the name of}} the domain. After obtaining the full set <b>of</b> <b>categories</b> belonging to the selected domain, the collection <b>of</b> corresponding <b>pages</b> is extracted, using some constraints. For reducing noise a bootstrapping approach implying several iterations is used. At each iteration less reliable pages, according to the balance between on-domain and off-domain <b>categories</b> <b>of</b> the <b>page,</b> are removed as well as less reliable <b>categories.</b> The set <b>of</b> recovered <b>pages</b> and <b>categories</b> is selected as initial domain term vocabulary. This approach has been applied to three broad coverage domains: astronomy, chemistry and medicine, and two languages: English and Spanish, showing a promising performance. The resulting set of terms has been evaluated using as reference those terms occurring in WordNet (using Magnini's domain codes) and those appearing in SNOMED-CT (a reference resource for the Medical domain available for Spanish). 1. Introduction an...|$|R
40|$|This {{presentation}} {{has been}} a research and an outline <b>of</b> www <b>pages</b> analyze from amber as a factor of developing touristic movement point o view. At the outline of article it has been discussed same chosen <b>category</b> <b>of</b> www <b>pages</b> according the amber contents, like history, geological aspects and geography of amber. The next group <b>of</b> <b>pages</b> been www information about touristic infrastructure like hotels, restaurants, shops and galleries, museums and exhibitions. The analyze focus on content pages from touristic point of view, including information important for organization and developing tourism movement. The article shows same example <b>of</b> www <b>pages</b> according maps, pictures and data connected with amber, as well as history of it and Amber route" in Poland and Europe. As a {{main part of the}} conclusion it has been found that even there has been more than 62 mln pages about amber and tourism it is extremely difficult to find this one, which present amber aspects of tourism. It is not only result of a great number <b>of</b> <b>pages</b> but as well a result of division touristic information connected with amber into a lot of portals and www pages. Marcin Polo...|$|R
40|$|In {{the present}} work, {{we have used}} Tesseract 2. 01 open source Optical Character Recognition (OCR) Engine under Apache License 2. 0 for {{recognition}} of handwriting samples of lower case Roman script. Handwritten isolated and free-flow text samples were collected from multiple users. Tesseract is trained to recognize user-specific handwriting samples <b>of</b> both the <b>categories</b> <b>of</b> document <b>pages.</b> On a single user model, the system is trained with 1844 isolated handwritten characters and the performance is tested on 1133 characters, taken form the test set. The overall character-level accuracy {{of the system is}} observed as 83. 5 %. The system fails to segment 5. 56 % characters and erroneously classifies 10. 94 % characters. Comment: Proc. National Conference on NAQC (2008) 141 - 14...|$|R
40|$|In {{this paper}} we present NEWER, a neuro-fuzzy Web {{recommendation}} system that dynamically suggests interesting pages {{to the current}} user. NEWER employs a neuro-fuzzy approach {{in order to determine}} categories ofusers sharing similar interests and to extract a recommendation model in the form of fuzzy rules expressing associations between user <b>categories</b> and relevances <b>of</b> <b>pages.</b> The derived model is used by an online recommendation module to dynamically suggest interesting links. Comparative accuracy resultsshow the effectiveness of NEWER...|$|R
40|$|We {{study the}} {{accessibility}} properties of trivial cofibrations and weak equivalences in a combinatorial model category and prove an estimate for the accessibility rank of weak equivalences. In particular, {{we show that}} the class of weak equivalences between simplicial sets is finitely accessible. Comment: The main result is improved and its proof is simplified. To appear in Theory and Applications <b>of</b> <b>Categories.</b> 15 <b>page...</b>|$|R
40|$|This is the {{translation}} of paper "Arborification de Wikipédia et analyse sémantique explicite stratifiée" submitted to TALN 2012. ] We present {{an extension of the}} Explicit Semantic Analysis method by Gabrilovich and Markovitch. Using their semantic relatedness measure, we weight the Wikipedia categories graph. Then, we extract a minimal spanning tree, using Chu-Liu & Edmonds' algorithm. We define a notion of stratified tfidf where the stratas, for a given Wikipedia page and a given term, are the classical tfidf and categorical tfidfs of the term in the ancestor <b>categories</b> <b>of</b> the <b>page</b> (ancestors {{in the sense of the}} minimal spanning tree). Our method is based on this stratified tfidf, which adds extra weight to terms that "survive" when climbing up the category tree. We evaluate our method by a text classification on the WikiNews corpus: it increases precision by 18...|$|R
40|$|Web page {{categorization}} {{is one of}} {{the challenging}} tasks in the world of ever increasing web technologies. There are many ways of categorization <b>of</b> web <b>pages</b> based on different approach and features. This paper proposes a new dimension in the way of categorization <b>of</b> web <b>pages</b> using artificial neural network (ANN) through extracting the features automatically. Here eight major <b>categories</b> <b>of</b> web <b>pages</b> have been selected for categorization; these are business & economy, education, government, entertainment, sports, news & media, job search, and science. The whole process of the proposed system is done in three successive stages. In the first stage, the features are automatically extracted through analyzing the source <b>of</b> the web <b>pages.</b> The second stage includes fixing the input values of the neural network; all the values remain between 0 and 1. The variations in those values affect the output. Finally the third stage determines the class of a certain web <b>page</b> out <b>of</b> eight predefined classes. This stage is done using back propagation algorithm of artificial neural network. The proposed concept will facilitate web mining, retrievals of information from the web and also the search engines. Comment: 4 Pages, International Conferenc...|$|R
40|$|Abstract—This paper proposes an auto-classification {{algorithm}} <b>of</b> Web <b>pages</b> using Data mining techniques. We {{consider the}} problem of discovering association rules between terms in a set <b>of</b> Web <b>pages</b> belonging to a category in a search engine database, and present an auto-classification algorithm for solving this problem that are fundamentally based on Apriori algorithm. The proposed technique has two phases. The first phase is a training phase where human experts determines the <b>categories</b> <b>of</b> different Web <b>pages,</b> and the supervised Data mining algorithm will combine these categories with appropriate weighted index terms according to the highest supported rules among the most frequent words. The second phase is the categorization phase where a web crawler will crawl through the World Wide Web to build a database categorized according to {{the result of the}} data mining approach. This database contains URLs and their categories. Keywords—Information Processing on the Web, Data Mining, Document Classification...|$|R
30|$|We first {{obtain a}} {{classifier}} to decide page similarity from layout features. This phase {{consists of the}} pre-processing stage and the training stage. The pre-processing stage takes as inputs two <b>categories</b> <b>of</b> pre-prepared web <b>page</b> pairs, visually similar web page pairs and visually different web page pairs. Our approach obtains features from web page layouts and creates the comparison vectors, which summarize the key similarity features, <b>of</b> every web <b>page</b> pairs accordingly. We label the comparison vector as “ c 1 ” to represent a pair <b>of</b> similar web <b>pages.</b> Correspondingly, the visually different web page pair is labelled as “ c 0.” The classifier training stage takes as inputs the labelled comparison vectors from the training set. The similar page classifier obtained in this stage {{can be used to}} determine whether two web pages are similar according to their comparison vectors.|$|R
40|$|In Melbourne, {{poems are}} everywhere. They are in lounge rooms, hotels, festivals, toilets, wedding ceremonies and funerals. Poems are pasted in alleyways, spray-painted on walls, emailed and facebooked and blogged. In this {{research}} project, by applying Bourdieu’s {{concept of a}} symbolic good, I track the exchange of symbolic and commodity values in the cultural field of poetry participation in Melbourne. I focus on two public domains of reception. The first domain is where poems are presented in hard-copy journals and books, {{and the second is}} where poems are seen and heard in public performance. I do not assume that a poet who presents their poems in performance made a decision during the composition process to present their poem in performance only. I define the dominant <b>categories</b> <b>of</b> <b>page</b> poetry and performance poetry as presentations of poems and acts of participation in poetry as a literary genre. I argue that the nomenclature <b>of</b> literary <b>categories,</b> which are given orthodoxy in arts funding, journal submission, prize entry and book solicitation guidelines in the cultural field of Australian poetry, constitutes symbolic violence. In current means of evaluation, poems presented in performance are often excluded from the literary genre of poetry. The conditions of access to the evaluation of a poem perpetuate this exclusion, as I find in the research setting of Melbourne, 2013 to 2015. The thesis has two parts, a written component and a performance component. The written component outlines the investigation and findings of the research project. I locate the means, terms and <b>categories</b> <b>of</b> poetry evaluation within international, national and local public settings and present the analysis as a series of contextual landscapes. The exegesis explores the practice-led investigation which is the last and overarching landscape. The production and presentation of poems is another approach to the research questions, and another way to further investigate previous findings. Yet even here, the poem drives the inquiry. I suggest that, in order to more closely evaluate poetry, the poem remain the object of evaluation and, in this case, the research subject. Additional material(s) submitted with thesis...|$|R
40|$|Web {{recommendation}} is a promising technology aimed {{to predict the}} needs of users by suggesting them information or services retained interesting according to their preferences. Web recommendation finds in Soft Computing techniques a valid tool to handle with the uncertainty and the ambiguity characterizing the Web and all phases of user interactions with Web sites. The main rationale behind this success {{seems to be the}} complementary nature of Soft Computing paradigms that properly combined enable the development of hybrid schemes exploiting the potential of each single paradigm. In this paper, we present NEWER, a neuro-fuzzy Web recommendation system that dynamically suggests interesting pages to the current user. NEWER employs a neuro-fuzzy approach in order to determine <b>categories</b> <b>of</b> users sharing similar interests and to extract a recommendation model in the form of fuzzy rules expressing associations between user <b>categories</b> and relevances <b>of</b> <b>pages.</b> The derived model is used by an online recommendation module to dynamically suggest interesting links. Comparative accuracy results show the effectiveness of NEWER...|$|R
40|$|The web {{is a large}} {{repository}} of information and to facilitate the search and retrieval <b>of</b> <b>pages</b> from it, categorization of web documents is essential. An effective means to handle the complexity of information retrieval from the internet is through automatic classification <b>of</b> web <b>pages.</b> Although lots <b>of</b> automatic classification algorithms and systems have been presented, most of the existing approaches are computationally challenging. In order to overcome this challenge, we have proposed a parallel algorithm, known as MapReduce programming model to automatically categorize the web pages. This approach incorporates three concepts. They are web crawler, MapReduce programming model and the proposed web page categorization approach. Initially, we have utilized web crawler to mine the World Wide Web and the crawled web pages are then directly given as input to the MapReduce programming model. Here the MapReduce programming model adapted to our proposed web page categorization approach finds the appropriate <b>category</b> <b>of</b> the web <b>page</b> according to its content. The experimental results show that our proposed parallel web page categorization approach achieve...|$|R
5000|$|One-way modems: One-way modems are {{controllers}} with integrated <b>paging</b> <b>receivers,</b> {{which are}} capable of taking local action based on messages and data they receive.|$|R
30|$|Jin et al. (2009) {{presents}} an adaptive memory compression method for migration. It analyses memory data to find regularities within it and divides memory pages in three categories: memory with many zero-bytes, memory with high similarity and memory with low similarity. Based on the <b>category</b> <b>of</b> memory <b>pages,</b> compression algorithm {{is applied to}} balance the overhead of compression. This approach tries to improve the performance of migration method while balancing the overhead due to compression. Compression based methods {{are affected by the}} overhead occurred due to compression/decompression process and to overcome from this, Jin et al. (2011) {{presents an}}other method using CPU scheduling. In this method, it controls the memory dirty rate by using CPU scheduling {{in such a way that}} dirty rate reaches to an acceptable desired small amount. The idea behind this is, to control the dirty rate as performance of migration method highly depends upon the dirty rate. This improves the performance of migration method specially downtime, which is one of the important performance parameters. Overhead occurred due to CPU-Scheduling affects the application performance of all the guest virtual machines.|$|R
40|$|In {{the era of}} the Web, {{there is}} urgent need for {{developing}} systems able to personalize the online experience of Web users {{on the basis of their}} needs. Web recommendation is a promising technology that attempts to predict the interests of Web users, by providing them with information and/or services that they need without explicitly asking for them. In this paper we propose NEWER, a usage-based Web recommendation system that exploits the potential of Computational Intelligence techniques to dynamically suggest interesting pages to users according to their preferences. NEWER employs a neuro-fuzzy approach in order to determine <b>categories</b> <b>of</b> users sharing similar interests and to discover a recommendation model as a set of fuzzy rules expressing the associations between user <b>categories</b> and relevances <b>of</b> <b>pages.</b> The discovered model is used by a online recommendation module to determine the list of links judged relevant for users. The results obtained on both synthetic and real-world data show that NEWER is effective for recommendation, leading to a quality of the generated recommendations comparable and often significantly better than those of other approaches employed for the comparison...|$|R
40|$|The {{objective}} of the paper is to recognize handwritten samples of lower case Roman script using Tesseract open source Optical Character Recognition (OCR) engine under Apache License 2. 0. Handwritten data samples containing isolated and free-flow text were collected from different users. Tesseract is trained with user-specific data samples <b>of</b> both the <b>categories</b> <b>of</b> document <b>pages</b> to generate separate user-models representing a unique language-set. Each such language-set recognizes isolated and free-flow handwritten test samples collected from the designated user. On a three user model, the system is trained with 1844, 1535 and 1113 isolated handwritten character samples collected from three different users and the performance is tested on 1133, 1186 and 1204 character samples, collected form the test sets of the three users respectively. The user specific character level accuracies were obtained as 87. 92 %, 81. 53 % and 65. 71 % respectively. The overall character-level accuracy {{of the system is}} observed as 78. 39 %. The system fails to segment 10. 96 % characters and erroneously classifies 10. 65 % characters on the overall dataset. Comment: Proc. International Conference on C 3 IT (2009) 240 - 24...|$|R
