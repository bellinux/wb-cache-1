58|10000|Public
5000|$|High speed Internet {{and wider}} <b>coverage</b> <b>of</b> <b>data</b> connections, at lower prices, {{allowing}} larger traffic rates, more reliable simpler traffic, and traffic from more locations, ...|$|E
40|$|Normal 0 false false false EN-US X-NONE X-NONE Data {{warehouse}} {{is widely}} {{recognized in the}} industry as the principal decision support system architecture and {{an integral part of}} the corporate information system.   However, the majority of academic institutions in the US and world-wide have been slow in developing curriculums that reflect this reality.   This paper examines the issues that have contributed to the lag in the <b>coverage</b> <b>of</b> <b>data</b> warehousing topics at universities. </p...|$|E
40|$|This paper {{deals with}} {{overlapping}} clustering, a trade off between crisp and fuzzy clustering. It has been motivated by recent applications in various domains such as information retrieval or biology. We {{show that the}} problem of finding a suitable <b>coverage</b> <b>of</b> <b>data</b> by overlapping clusters is not a trivial task. We propose a new objective criterion and the associated algorithm OKM that generalizes the k-means algorithm. Experiments show that overlapping clustering is a good alternative and indicate that OKM outperforms other existing methods. 1...|$|E
50|$|There is data {{available}} on UK unemployment from 1881 {{but it is}} not consistent with the current international definition. The data is more closely related to the “Claimant Count” but the <b>coverage</b> <b>of</b> the <b>data</b> between 1881 and 1948 is limited.|$|R
50|$|The {{details of}} the ESAW {{methodology}} are described in European statistics on accidents at work (ESAW) - Methodology - 2001 edition. A resume of the concepts and <b>coverage</b> <b>of</b> the <b>data</b> may {{also be found in}} Work and health in the EU: A statistical portrait 1994-2002.|$|R
40|$|TES (thermal {{emission}} spectrometry) {{has obtained}} high spatial resolution surface temperature observations from which thermal inertia has been derived. Seasonal <b>coverage</b> <b>of</b> these <b>data</b> now provides a nearly global view of Mars, including the polar regions, at high resolution. Additional information {{is contained in}} the original extended abstract...|$|R
40|$|Predicting plant {{community}} compositional responses to changing environmental conditions and disturbances {{is a key}} element of forecasting and managing for the effects of global climate change. With advances in ecological modeling, many forms of succession models are available. Empirical-based succession models have been criticized as inflexible and limited by the quality and <b>coverage</b> <b>of</b> <b>data</b> for formulation; however, mechanistic models are tied to the underlying theory (quality and comprehensiveness) from which they are developed and make key limiting assumptions that the modeled processes they represent are adequately understood, thus underscoring the continual necessity for empirical testing of successional processes...|$|E
40|$|In {{the absence}} of {{consolidated}} pipelines to archive biological data electronically, information dispersed in the literature must be captured by manual annotation. Unfortunately, manual annotation is time consuming and the coverage of published interaction data is therefore far from complete. The use of text-mining tools to identify relevant publications and {{to assist in the}} initial information extraction could help to improve the efficiency of the curation process and, as a consequence, the database <b>coverage</b> <b>of</b> <b>data</b> available in the literature. The 2006 BioCreative competition was aimed at evaluating text-mining procedures in comparison with manual annotation of protein-protein interactions...|$|E
40|$|This paper {{deals with}} {{overlapping}} clustering, a trade off between crisp and fuzzy clustering. It has been motivated by recent applications in various domains such as Information Retrieval or biology. We {{show that the}} problem of finding a suitable <b>coverage</b> <b>of</b> <b>data</b> by overlapping clusters is not a trivial task and we propose the algorithm OKM that generalizes the k-means algorithm combining a new objective criterion coupled with an optimization heuristic. Experimental results in the context of document clustering show that OKM first generates suitables overlaps between classes and then outperforms the overlapping clusters derived from fuzzy approaches (e. g. fuzzy-k-means). ...|$|E
30|$|In addition, {{signaling}} controller {{may also}} support {{part of the}} mobility management functions: when mobile users cross the <b>coverage</b> boundaries <b>of</b> <b>data</b> BSs or signaling controllers, handovers can be realized through the cooperation between signaling controllers and MME (linked via interface S 1).|$|R
50|$|JREAP {{provides}} {{a foundation for}} Joint Range Extension (JRE) of Link 16 and other tactical data links to overcome the line-of-sight limitations of radio terminals such as the Joint Tactical Information Distribution System (JTIDS) and Multifunctional Information Distribution System (MIDS), and extends <b>coverage</b> <b>of</b> these <b>data</b> links {{through the use of}} long-haul media.|$|R
5000|$|Level 0 {{provides}} worldwide <b>coverage</b> <b>of</b> geo-spatial <b>data</b> and {{is equivalent}} to a small scale (1:1,000,000). The data are offered either on CD-ROM or as direct download, {{as they have been}} moved to the public domain. Data are structured following the Vector Product Format (VPF), compliant with standards MIL-V-89039 and MIL-STD 2407 ...|$|R
40|$|Accurate {{descriptions}} of the solar cycle variations of ionospheric parameters are an important goal of ionospheric modeling. Reliable predictions of these variations are of essential importance for almost all applications of ionospheric models. Unfortunately {{there are very few}} global data sources that cover a solar cycle or more. In an effort to expand the solar cycle <b>coverage</b> <b>of</b> <b>data</b> readily available for ionospheric modeling, we have processed a large number of satellite data sets from the sixties, seventies, and early eighties and have made them online accessible as part of NSSDC's ftp archive (ftv://nssdcftp. gsfc. nasa. aov/spacecraft data/) and it's ATMOWeb retrieval and plotting syste...|$|E
40|$|This study {{provides}} quarterly time-series {{estimates of the}} misalignment in the REER of the Renminbi (RMB). The estimation {{is based on a}} commonly used economic approach, but with a wider and more up-to-date <b>coverage</b> <b>of</b> <b>data</b> and a more extensive use of econometric modelling techniques. Our estimates corroborate and explain most of the previous estimates. More importantly, our estimates demonstrate that there is no significant undervaluation in the REER of the RMB though downward misalignment exists in the trilateral rates between the RMB, US$ and euro. The finding refutes the claim that RMB appreciation is the primary and necessary solution to the current global trade imbalance. [...] Real exchange rate misalignment...|$|E
40|$|A {{total of}} 22 UNICEF country offices {{provided}} information {{for this report}} through their country child protection and education teams. Data informing were obtained from two secondary sources of information (37 generic reports identified during a purposive trawl of the internet, coupled with a scoping of recent academic literature on school violence) and one primary source (102 documents from the 22 countries). Three principal issues were identified as challenges in the analysis: differences in terminology, cultural variation/interpretation and the validity, representativeness and <b>coverage</b> <b>of</b> <b>data,</b> including baseline data. The resulting data provided insights to the common factors which appear to underpin violence in schools and to those practices {{that seem to be}} effective in addressing it...|$|E
50|$|Depending on {{the size}} of the genome FAIRE-seq is {{performed}} on, a minimum of reads is required to create an appropriate <b>coverage</b> <b>of</b> the <b>data,</b> ensuring a proper signal can be determined. In addition, a reference or input genome, which has not been cross-linked, is often sequenced alongside to determine the level of background noise.|$|R
40|$|Phonological {{rules have}} {{formed the basis}} of phonological theory for decades, {{although}} their form and their <b>coverage</b> <b>of</b> the <b>data</b> has changed over the years. Until recently, however, it was difficult to determine the relationship between hand-written phonological rules and actual speech data. The current availability of large speech corpora and pronunciation dictionaries has allowed us to connect rules and speech in muc...|$|R
40|$|This paper {{explores the}} {{application}} of text mining {{to the problem of}} detecting protein functional sites in the biomedical literature, and specifically considers the task of identifying catalytic sites in that literature. We provide strong evidence for the need for text mining techniques that address residue-level protein function annotation through an analysis of two corpora in terms <b>of</b> their <b>coverage</b> <b>of</b> curated <b>data</b> sources. We also explore the viability of building a text-based classifier for identifying protein functional sites, identifying the low <b>coverage</b> <b>of</b> curated <b>data</b> sources and the potential ambiguity of information about protein functional sites as challenges that must be addressed. Nevertheless we produce a simple classifier that achieves a reasonable ∼ 69 % F-score on our full text silver corpus on the first attempt to address this classification task. The work has application in computational prediction of the functional significance of protein sites as well as in curation workflows for databases that capture this information...|$|R
40|$|This paper {{presents}} a user oriented evaluation methodology for comparing person search services on the Web. Many established system oriented methods from information retrieval cannot {{be applied to}} this domain. Our user oriented methodology is applied to a test comparing the person search engines yasni, pipl. com and 123 people. The user study with over 30 participants led to relevant results. The <b>coverage</b> <b>of</b> <b>data</b> object types within the person search engine results is quite different. Especially the amount of pictures and social media network entries which are presented by the systems and which are perceived by the test users differ greatly. The results also revealed a tendency to judge people more positively when more information was found. 1...|$|E
30|$|The {{numerical}} {{studies show}} {{the capability of}} the DOVCs to improve the observation weights. Here, we assumed that the observations are contaminated with white noise {{and that they are}} independent. The white noise can be an approximation for the coloured noise if the observations are not highly correlated; otherwise, the presented method is not suited for such applications. Here, we have considered GBVPs, but if we want to combine solutions of different geodetic boundary value problems, the presented method will be relevant. The other assumption is to have a global <b>coverage</b> <b>of</b> <b>data,</b> which is a necessary condition for solving the GBVP and determining the gravity field using integral formulas; this is not a restriction for the combination method using DOVC presented here.|$|E
40|$|Abstract. Aerosol events (their {{frequency}} and intensity) {{in the broader}} Mediterranean basin were studied using 7 -year (2000 – 2007) aerosol data of optical depth (AOD at 550 nm) from the MODerate Resolution Imaging Spectroradiometer (MODIS) Terra. The complete spatial <b>coverage</b> <b>of</b> <b>data</b> revealed a signif-icant spatial variability of aerosol events which is also de-pendent on their intensity. Strong events occur more often in the western and central Mediterranean basin (up to 14 events/year) whereas extreme events (AOD up to 5. 0) are systematically observed in the eastern Mediterranean basin throughout the year. There is also a significant seasonal vari-ability with strong aerosol events occurring most frequently {{in the western part}} of the basin in summer and extreme episodes in the eastern part during spring. The events wer...|$|E
40|$|Availability, comparability, {{and quality}} of cross-national {{hospital}} discharge <b>data.</b> Descriptions <b>of</b> discharge reporting systems in six developed countries, with emphasis on <b>coverage,</b> types <b>of</b> <b>data</b> collected, procedures and definitions used in {{data collection and analysis}} and statistics routinely available. Discussion of health services system characteristics possibly affecting rates of hospital utilization. DHEW Publication No, (PHS) 80 - 135...|$|R
40|$|Study of {{comparability}} of cross-national hospital discharge <b>data.</b> Descriptions <b>of</b> discharge {{reporting systems}} {{with emphasis on}} <b>coverage,</b> types <b>of</b> <b>data</b> collected, procedures and definitions used in data collection and analysis, and statistics routinely available. Discussion of health services system characteristics likely to affect rates of hospital use. [Lola Jean Kozak, Ronald Andersen, and Odin W. Anderson]. Includes bibliographical references. 1580117...|$|R
40|$|Plate {{tectonics}} and {{its contribution}} to progress in studies of the Earth's gravitational field is discussed. In acquisition, the development of forced feedback accelerometers, satellite navigation, and satellite radar altimetry significantly improved the accuracy and <b>coverage</b> <b>of</b> gravity <b>data</b> over the oceans. In interpretation, gravity and geoid anomalies are used to determine information on the thermal and mechanical properties of the oceanic lithosphere and the forces that drive plate motions...|$|R
40|$|Geomagnetic {{data are}} held {{by a number of}} World Data Centres (WDC) with each Centre having {{different}} holdings and different methods of distribution. These Centres are run by institutes in Boulder, Edinburgh, Kyoto, Moscow and Mumbai. Using data from two of these WDCs we describe methods to compare temporal <b>coverage</b> <b>of</b> <b>data,</b> and, importantly, the data themselves. This study examines the hourly data common to Edinburgh and Kyoto and gives details of the number of observatory-years of data where disagreements are found, and quantifies the level of disagreement. We show several examples of datasets that differ between the two WDCs, and report {{on the nature of the}} differences in detail. Finally, we explore possible reasons for the differences found between the data holdings and suggest next steps towards the unification of the WDC data holdings. ...|$|E
40|$|ERS 1 Exact Repeat Mission (ERM) and TOPEX/POSEIDON {{data have}} been {{analyzed}} trackwise to separate via collocation filtering the stationary SST {{from the time}} dependent SST in residuals between altimetric data and geoid model (GEOMED). The stacked tracks have been merged via a cross over adjustment assuming TOPEX/POSEIDON as reference frame. Using a 2 -dimensional covariance function, estimated from stacked ERS 1 /ERM data, collocation filtering has been applied also to the Geodetic Mission data to separate the stationary SST from the time dependent part. The filtered geodetic tracks have been then trackwise connected to the reference frame defined by TOPEX/POSELDON and ERS 1 /ERM. Thus an improved estimate of the stationary SST has been obtained due to the dense <b>coverage</b> <b>of</b> <b>data</b> mainly related to the geodetic ERS 1 mission...|$|E
40|$|Patent {{information}} plays a {{key role}} for corporations and all the institutions involved in technological activities. But patent information is a very "labile" resource and requires a very professional approach based on sound management for being handled properly and used efficiently. This article focuses initially on the characteristics of patent information, bringing together some key statistics over the last 20 Â years. Then it deals with the management of patent information, the basic content of a policy to be defined and implemented, and finally with the key elements of best practice. The best practice analysis, applied to novelty searches performed in a patent office, covers the nature of data, the <b>coverage</b> <b>of</b> <b>data</b> and databases (e. g. geography and languages), and the functionalities of the tools (e. g. search engines, navigation tools). Finally, a model methodology of search strategy is presented. Patent information management Databases Search strategies Best practice...|$|E
40|$|We thank Boys, Farrow, and Germain (hereafter BFG) {{for raising}} various inter-esting points for {{discussion}} in their review of our paper. Our rejoinder {{will focus on}} their comments concerning temporal and spatial <b>coverage</b> <b>of</b> the <b>data,</b> model choice and specification of priors, accounting for excess zeros, and inclusion of covariates in the covariance structure. <b>COVERAGE</b> <b>OF</b> THE <b>DATA</b> BFG {{point out that the}} data were collected over a period that is quite short com-pared to both the life cycle and breeding cycle of one of the fish species, and suggest that to get {{a better understanding of the}} impact of environmental covariates it might be useful to collect more data over (at least) a complete year. Fish population dynamics are the outcome of biological processes (birth, death, immigration, and emigration) that are strongly context-dependent. For example, in Lake St. Pierre yellow perch rarely live beyond five years, and life stages such as larval fish and small young-of-the year are not amenable to capture by gear used to sample older fish. The littoral zone of Lake St. Pierre is ice-covered for approxi...|$|R
40|$|Code {{coverage}} {{is a common}} measure for quantitatively assessing the quality of software testing. Code coverage indicates the fraction of code that is actually executed by tests in a test suite. While code coverage {{has been around since}} the 60 's there has been little work on how to effectively analyze code coverage data measured in system tests. Raw <b>data</b> <b>of</b> this magnitude, containing millions <b>of</b> <b>data</b> records, is often impossible for a human user to comprehend and analyze. Even drill-down capabilities that enable looking at different granularities starting with directories and going through files to lines of source code are not enough. Substring hole analysis is a novel method for viewing the <b>coverage</b> <b>of</b> huge <b>data</b> sets. We have implemented a tool that enables automatic substring hole analysis. We used this tool to analyze <b>coverage</b> <b>data</b> <b>of</b> several large and complex IBM software systems. The tool identified coverage holes that suggested interesting scenarios that were untested. 1...|$|R
40|$|Data on monthly {{latitudinal}} {{variations in}} middle-atmosphere vertical ozone profiles are presented, based on extensive Nimbus- 7, AE- 2, and SME satellite measurements {{from the period}} 1978 - 1982. The <b>coverage</b> <b>of</b> the <b>data</b> sets, {{the characteristics of the}} sensors, and the analysis techniques applied are described, and the results are compiled in tables and graphs. These ozone data are intended to supplement the models of the 1986 COSPAR International Reference Atmosphere...|$|R
40|$|Abstract. Online social {{networks}} provide {{a unique opportunity}} to access and analyze the reactions of people as real-world events unfold. The quality of any analysis task, however, depends on the appropriateness and quality of the collected data. Hence, given the spontaneous nature of user-generated content, as well as the high speed and large volume of data, it is important to carefully define a data-collection campaign about a topic or an event, in order to maximize its coverage (recall). Motivated by the development of a social-network data management platform, in this work we evaluate the <b>coverage</b> <b>of</b> <b>data</b> collection campaigns on Twitter. Using an adaptive language model, we estimate the coverage of a campaign with respect to the total number of relevant tweets. Our findings support the development of adaptive methods to account for unexpected real-world developments, and hence, to increase the recall of the data collection processes...|$|E
40|$|Developing Web Information Systems brings {{together}} traditional system development methods {{that have been}} taught for many years on information systems and computer science courses with web/e-commerce development. It is the first book to bring together IS development and the web applications in a thorough and systematic way. There is a running case study that illustrates web IS development from start to finish. The case {{is easy to understand}} (a theatre) and results in a working web application. Most, if not all, analysis and design texts fall short of making that step into software. The book draws heavily on practical experiences of web-based IS development resulting from commercial system development, so as well as appealing to students and academics, it will also interest practitioners. The <b>coverage</b> <b>of</b> <b>data</b> management and e-business strategy gives the book the broader scope essential for understanding IS development properly in an Internet context...|$|E
40|$|The Scholarly Database aims {{to serve}} {{researchers}} and practitioners {{interested in the}} analysis, modelling, and visualization of large-scale data sets. A specific focus of this database is to support macro-evolutionary studies of science and to communicate findings via knowledge-domain visualizations. Currently, the database provides access to about 18 million publications, patents, and grants. About 90 % of the publications are available in full text. Except for some datasets with restricted access conditions, the data can be retrieved in raw or pre-processed formats using either a web-based or a relational database client. This paper motivates {{the need for the}} database from the perspective of bibliometric/scientometric research. It explains the database design, setup, etc., and reports the temporal, geographical, and topic <b>coverage</b> <b>of</b> <b>data</b> sets currently served via the database. Planned work and the potential for this database to become a global testbed for information science research are discussed {{at the end of the}} paper...|$|E
40|$|The rise {{of social}} media, {{and the data}} {{gathering}} potential that it holds, have made the issue of online privacy increasingly prevalent, with citizens often unaware {{of what is being}} done with information they had previously assumed would be used responsibly. Commenting on a recent Court of Justice of the European Union case, Claire Overman argues that the need for legislative <b>coverage</b> <b>of</b> internet <b>data</b> protection has never been more sorely needed...|$|R
40|$|The {{second edition}} of a bestseller, Statistical and Machine-Learning Data Mining: Techniques for Better Predictive Modeling and Analysis <b>of</b> Big <b>Data</b> {{is still the}} only book, to date, to {{distinguish}} between statistical data mining and machine-learning data mining. The first edition, titled Statistical Modeling and Analysis for Database Marketing: Effective Techniques for Mining Big Data, contained 17 chapters of innovative and practical statistical data mining techniques. In this second edition, renamed to reflect the increased <b>coverage</b> <b>of</b> machine-learning <b>data</b> mining techniques, the author ha...|$|R
40|$|The high {{precision}} and large kinematic <b>coverage</b> <b>of</b> the <b>data</b> from the HERA-I running period (1994 - 2000) have already allowed precise extractions of proton parton distribution functions. The HERA-II program is now underway {{and is expected}} to provide a substantial increase in the luminosity collected at HERA. In this paper, the first estimate of the impact <b>of</b> future <b>data</b> from HERA, on the proton PDF uncertainties, is presented. Next-to-leading order QCD predictions for inclusive jet cross sections at the LHC are presented using the projected PDFs...|$|R
