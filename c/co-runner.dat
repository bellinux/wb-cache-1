16|7|Public
50|$|In August 2016, Deathstroke was teased via Twitter by Batman actor Ben Affleck through test footage. DCEU <b>co-runner</b> and {{producer}} Geoff Johns has since {{confirmed that the}} character will be played by Joe Manganiello in the upcoming Batman reboot, which will be co-written by Johns.|$|E
50|$|He {{is also a}} {{comic book}} {{retailer}} who co-owns Earth-2 Comics in Northridge, California with Carr D'Angelo and Jud Meyers. In 2016, Johns was made the <b>co-runner</b> of the DC Extended Universe, and co-chairman of DC Films {{in order to create}} a unified vision and overarching story for the franchise.|$|E
50|$|Terrio re-wrote David S. Goyer's {{script for}} Warner Bros.' Batman v Superman: Dawn of Justice (2016). On July 25, 2014, Variety {{reported}} that Terrio was also eyed by Warner Bros. {{to write the}} screenplay for Justice League, and on January 30 2017, {{it was announced that}} Terrio had done a re-write Ben Affleck's script for an untitled Batman movie, which Affleck co-wrote with DCEU <b>co-runner</b> and producer Geoff Johns.|$|E
50|$|She won {{a bronze}} medal in the 4×100 metres relay, the first medal ever for Italian women in a relay race, at the 1938 European Athletics Championships in Vienna, with <b>co-runners</b> Maria Apollonio, Maria Alfero and Italia Lucchini She has 8 caps in {{national}} team from 1937 to 1942.|$|R
40|$|The inter-core {{interference}} {{that affects}} multicore processors highly complicates the timing analysis of embedded applications. The {{contribution of the}} execution-time penalty that software programs incur on access to hardware shared resources is hard to estimate, as it depends on both the resource arbitration policy and the quantity and activity of <b>co-runners.</b> An interesting vicious circle arises: the execution-time behavior of the application of interest must be known to determine its best allocation to a processor core; this decision however determines the actual set of <b>co-runners,</b> which in turns effects the inter-core interference suffered by the application of interest and consequently its execution-time behavior. This work presents a framework that aids the timing analysis of applications in presence of inter-core interference and addresses the cited circular dependency by providing a suite of synthetic <b>co-runners</b> designed to access hardware shared resources to produce fine-grained controlled interference. The framework specifically targets the Aurix Tricore family of processors designed by Infineon for the automotive domain, and includes a highly integrated toolchain to build, execute and trace applications. The toolchain includes a tailored version of Erika Enterprise, modified to exhibit time-composable behavior at run time, and the RT-Druid build environment. The paper includes {{an evaluation of the}} timing behavior of a real-world automotive application, adapted to fit the run-time target of choice and trialed under different levels of inter-core interference...|$|R
5000|$|As {{part of the}} reward, Nagar {{abducted}} Ervic {{to become}} their tribemate. Vote count does not include three votes against Aira and one against Moi when they played their Immunity Bracelets during {{the second round of}} voting on Day 19. Vote count does not include four votes against Michelle when she played Jons Immunity Bracelet on Day 22. Vote count does not include five votes against Aubrey and three votes against Akihiro during a tiebreaker vote to determine who would be the last jury member. Ervic and Solenn were considered <b>co-runners</b> up due to being tied at two votes each in the final Tribal Council vote.|$|R
50|$|After seeing RuPaul's Drag Race {{contestant}} Raven {{perform at}} the nightclub Micky's in West Hollywood, Noriega was inspired to enter a drag competition at the club, later winning it. Following the win, Noriega started performing in Southern California as Adore Delano. Along with other RuPaul's Drag Race contestants, Adore walked the Marco Marco runway for Los Angeles Fashion Week in 2013. In December 2013, Logo announced that Adore Delano was among 14 drag queens who would be competing on the sixth season of RuPaul's Drag Race. Adore had previously competed for the season 5 fan-vote cok, ultimately losing to Penny Tration. Despite a slow start to the competition, Delano eventually {{went on to win}} three challenges, the joint most of the season. She {{became a member of the}} final three and in the season finale, was ultimately named <b>co-runner</b> up along with Courtney Act, losing out to Bianca Del Rio.|$|E
40|$|First, we {{measured}} {{the performance of}} five HTC applications on three ypes of computing platforms. Then we analyzed the performance degradation factors due to resource sharing among the concurrently executed HTC applications on the platforms. Second, we analyzed the platform affinity and <b>co-runner</b> affinity of HTC applications, considering the unique characteristics of platforms and the resource sharing effects. Third, we defined the platform affinity metric and <b>co-runner</b> affinity metric. Then we proposed a scheduling algorithm to minimize the total system makespan, which distributes the resource of different platform to each of applications based on platform affinity, and for each platforms, maps tasks of the application to computing nodes based on <b>co-runner</b> affinity. We also discuss the Fairness-Allcore scheduling algorithm to distribute resources fairly to applications, and the Worst algorithm to simulate the worst case scheduling for comparing {{the performance of the}} proposed algorithm with these algorithms. Finally, we developed a trace-based simulator to simulate a proposed algorithm. We conducted simulations over the various scenarios using the simulator and analyzed the performance of the proposed scheduling algorithm. ○ 대규모 과학 응용에 대해 자원공유에 의한 성능 저하 요소 파악 및 분석방법 연구 ● 이종 컴퓨팅 플랫폼에서 HTC/MTC 응용의 성능 및 자원 사용 패턴 분석 ● 대규모 과학 응용간의 자원 공유에 의한 성능 저하 요소 파악 ○ 자원 특성 및 자원 공유를 고려한 성능 저하 개선 방법 연구 ● 응용간의 자원 공유에 의한 성능 영향을 고려한 <b>Co-runner</b> Affinity 분석 ● 응용의 자원 선호도를 고려한 Platform Affinity 분석 ○ 자원 특성 및 자원 공유를 고려한 최적 스케줄링 방법 연구 ● 응용 간 자원 공유에 의한 영향을 고려하는 <b>Co-runner</b> Affinity Metric 및 Platform의 특성에 의한 영향을 고려하는 Platform Affinity Metric 정의 ● 응용의 자원 선호도와 응용 간 성능 영향을 고려한 Affinity-aware 스케줄링 알고리즘 연구 개발 ○ 다수의 사용자 및 응용 지원하는 대규모 과학 컴퓨팅 시스템을 위한 최적 스케줄링 방법 연구 ● 스케줄링 알고리즘의 성능 평가 분석을 위한 Trace 기반 시뮬레이터 개발 ● 다양한 시나리오에 대해 제안된 스케줄링 알고리즘의 성능 평가 및 분석clos...|$|E
40|$|Computer {{resource}} allocation represents a significant challenge particularly for multiprocessor systems, which consist of shared computing resources to be allocated among <b>co-runner</b> processes and threads. While an efficient {{resource allocation}} {{would result in}} a highly efficient and stable overall multiprocessor system and individual thread performance, ineffective poor resource allocation causes significant performance bottlenecks even for the system with high computing resources. This thesis proposes a cache aware adaptive closed loop scheduling framework as an efficient resource allocation strategy for the highly dynamic resource management problem, which requires instant estimation of highly uncertain and unpredictable resource patterns. Many different approaches to this highly dynamic resource allocation problem have been developed but neither the dynamic nature nor the time-varying and uncertain characteristics of the resource allocation problem is well considered. These approaches facilitate either static and dynamic optimization methods or advanced scheduling algorithms such as the Proportional Fair (PFair) scheduling algorithm. Some of these approaches, which consider the dynamic nature of multiprocessor systems, apply only a basic closed loop system; hence, they fail to take the time-varying and uncertainty of the system into account. Therefore, further research into the multiprocessor resource allocation is required. Our closed loop cache aware adaptive scheduling framework takes the resource availability and the resource usage patterns into account by measuring time-varying factors such as cache miss counts, stalls and instruction counts. More specifically, the cache usage pattern of the thread is identified using QR recursive least square algorithm (RLS) and cache miss count time series statistics. For the identified cache resource dynamics, our closed loop cache aware adaptive scheduling framework enforces instruction fairness for the threads. Fairness in the context of our research project is defined as a resource allocation equity, which reduces corunner thread dependence in a shared resource environment. In this way, instruction count degradation due to shared cache resource conflicts is overcome. In this respect, our closed loop cache aware adaptive scheduling framework contributes to the research field in two major and three minor aspects. The two major contributions lead to the cache aware scheduling system. The first major contribution is the development of the execution fairness algorithm, which degrades the <b>co-runner</b> cache impact on the thread performance. The second contribution is the development of relevant mathematical models, such as thread execution pattern and cache access pattern models, which in fact formulate the execution fairness algorithm in terms of mathematical quantities. Following the development of the cache aware scheduling system, our adaptive self-tuning control framework is constructed to add an adaptive closed loop aspect to the cache aware scheduling system. This control framework in fact consists of two main components: the parameter estimator, and the controller design module. The first minor contribution is the development of the parameter estimators; the QR Recursive Least Square(RLS) algorithm is applied into our closed loop cache aware adaptive scheduling framework to estimate highly uncertain and time-varying cache resource patterns of threads. The second minor contribution is the designing of a controller design module; the algebraic controller design algorithm, Pole Placement, is utilized to design the relevant controller, which is able to provide desired timevarying control action. The adaptive self-tuning control framework and cache aware scheduling system in fact constitute our final framework, closed loop cache aware adaptive scheduling framework. The third minor contribution is to validate this cache aware adaptive closed loop scheduling framework efficiency in overwhelming the <b>co-runner</b> cache dependency. The timeseries statistical counters are developed for M-Sim Multi-Core Simulator; and the theoretical findings and mathematical formulations are applied as MATLAB m-file software codes. In this way, the overall framework is tested and experiment outcomes are analyzed. According to our experiment outcomes, it is concluded that our closed loop cache aware adaptive scheduling framework successfully drives <b>co-runner</b> cache dependent thread instruction count to <b>co-runner</b> independent instruction count with an error margin up to 25 % in case cache is highly utilized. In addition, thread cache access pattern is also estimated with 75 % accuracy...|$|E
40|$|Co-location, where {{multiple}} jobs share compute nodes in large-scale HPC systems, {{has been}} shown to increase aggregate throughput and energy efficiency by 10 – 20 %. However, system operators disallow co-location due to fair-pricing concerns, i. e., a pricing mechanism that considers performance interference from co-running jobs. In the current pricing model, application execution time determines the price, which results in unfair prices paid by the minority of users whose jobs suffer from co-location. This paper presents POPPA, a runtime system that enables fair pricing by delivering precise online interference detection and facilitates the adoption of supercomputers with co-locations. POPPA leverages a novel shutter mechanism – a cyclic, fine-grained interference sampling mechanism to accurately deduce the interference between <b>co-runners</b> – to provide unbiased pricing of jobs that share nodes. POPPA is able to quantify inter-application interference within 4 % mean absolute error on a variety of co-located benchmark and real scientific workloads...|$|R
40|$|International audienceIn {{this paper}} we {{introduce}} a response time analysis technique for Synchronous Data Flow programs mapped to multipleparallel dependent tasks {{running on a}} compute cluster of the Kalray MPPA- 256 many-core processor. The analysis we derivecomputes a set of response times and release dates that respect the constraints in the task dependency graph. We extend the Multicore Response Time Analysis (MRTA) framework by deriving a mathematicalmodel of the multi-level bus arbitration policy used by the MPPA. Further, we refine the analysis to account for therelease dates and response times of <b>co-runners,</b> {{and the use of}} memory banks. Further improvements to the precision ofthe analysis were achieved by splitting each task into two sequential phases, with the majority of the memory accessesin the first phase, and a small number of writes in the second phase. Our experimental evaluation focused on anavionics case study. Using measurements from the Kalray MPPA- 256 as a basis, we show that the new analysis leads toresponse times that are a factor of 4. 15 smaller for this application, than the default approach of assumingworst-case interference on each memory access...|$|R
40|$|We {{describe}} a new operating system scheduling algorithm that improves performance isolation on chip multiprocessors (CMP). Poor performance isolation is the phenomenon where an application’s performance {{is determined by}} the behavior of other simultaneously running, or co-running, applications. This phenomenon is caused by unfair, co-runner-dependent allocation of shared cache on CMPs, which causes co-runner-dependent performance. Poor performance isolation, interferes with the operating system’s control over priority enforcement, complicates per-CPU-hour billing and hinders QoS provisioning. Previous solutions required modifications to the hardware, and thus had limited flexibility and long time-to-market. We present a new software solution. The cache-fair scheduling algorithm reduces the dependency of an application’s performance on the <b>co-runners</b> by ensuring that the application always runs as quickly as it would under fair cache allocation, regardless of how the cache is actually allocated. If the application executes fewer instructions per cycle (IPC) than it would under fair cache allocation, the scheduler increases that thread’s CPU timeslice. This way, even if the thread’s IPC is lower that it would be under fair cache allocation, the overall performance does not suffer because the application is allowed to use the CPU longer. We demonstrate, using our implementation of the algorithm in Solaris 10, that this algorithm significantly improves performance isolation for workloads such as SPEC CPU, SPEC JBB and TPC-C. 1...|$|R
40|$|International audienceThe {{increasing}} {{performance requirements}} of safety-critical real-time embedded systems made traditional single-corearchitectures obsolete. Moving to more complex many-core systems requires new techniques and tools for the certificationof the embedded software. Timing and functional behaviours {{are subject to}} specific requirements of certification guidelinessuch as D- 178 B/C for avionics and ISO 26262 for automotive systems. Determinism and predictability of such systems are amajor challenge. Running tasks should be known in advance which makes the static and non-preemptive scheduling a suitableapproach to reach an optimal execution with a guarantee of {{a certain degree of}} determinism. Recent work on mapping andscheduling problems [1], [2], [3] consider a known value (or a set of possible values) of the Worst-Case Response Time (WCRT) and computes a mapping that optimizes a predefined cost function. When the response time analysis is too pessimistic, thestatic scheduling may introduce an idle time which reduces the core utilization. Scheduling techniques must rely on tight estimations of the WCRT which in turn depends on <b>co-runner</b> tasks. However, inorder to obtain a tight upper-bound on the response time, a mapping and scheduling should be known in advance. Indeed, theresponse time is highly influenced by the <b>co-runner</b> tasks. Concurrent accesses to the same shared resource may introduceinterferences that should be accounted for in the response time analysis. The search for an optimal scheduling with a tightWCRT analysis that includes the shared resource interferences is a challenging open problem...|$|E
40|$|Upper {{bounding}} {{the execution}} time of tasks running on multicore processors {{is a hard}} challenge. This is especially so with commercial-off-the-shelf (COTS) hardware that conceals its internal operation. The main difficulty stems from the contention effects on access to hardware shared resources (e. g., buses) which cause task's timing behavior {{to depend on the}} load that <b>co-runner</b> tasks place on them. This dependence reduces time composability and constrains incremental verification. In this paper we introduce the concepts of resource-usage signatures and templates, to abstract the potential contention caused and incurred by tasks running on a multicore. We propose an approach that employs resource-usage signatures and templates to enable the analysis of individual tasks largely in isolation, with low integration costs, producing execution time estimates per task that are easily composable throughout the whole system integration process. We evaluate the proposal on a 4 -core NGMP-like multicore architecture...|$|E
40|$|This {{exhibition}} {{is based upon}} the initial phase of a doctoral research project, which explores in depth the interactions between physical activity and artistic creativity through the concept of simultaneous recreational running and creative drawing. ‘Run-to-draw’ is a collaborative endeavour in which the researcher creates drawings on Phidippeddes, an original engineered product designed by, and worn on the back of, a <b>co-runner.</b> It contains an automatic scroll for drawing and mounted video camera to capture the run-to-draw experience in situ. Here we draw attention to the visual (Phoenix, 2010), displaying some initial artistic drawings and film footage produced over a variety of running contexts and environments. Combining physical recreation and art, these drawings offer a unique form of data where the artistic is a different way of ‘knowing’ the physical, and the physical invigorates artist creativity. We hope that these drawings amass multilayered meanings and evocative interpretations in those observing them, and offer some insight into the nuanced, sensory dimensions of the lived experience of run-to-draw engagement...|$|E
40|$|Ensuring {{the quality}} of service (QoS) for latency-sensitive {{applications}} while allowing co-locations of multiple applications on servers is critical for improving server utilizationandreducingcostinmodernwarehouse-scalecomputers (WSCs). Recent work relies on static profiling to precisely predict the QoS degradation that results from performance interference among co-running applications {{to increase the number}} of “safe ” co-locations. However, these static profiling techniques have several critical limitations: 1) a priori knowledge of all workloads is required for profiling, 2) it is difficult for the prediction to capture or adapt to phase or load changes of applications, and 3) the prediction technique is limited to only two co-running applications. To address all of these limitations, we present Bubble-Flux, an integrated dynamic interference measurement and onlineQoSmanagementmechanismtoprovideaccurateQoS control and maximize server utilization. Bubble-Flux uses a Dynamic Bubble to probe servers in real time to measure the instantaneous pressure on the shared hardware resources and precisely predict how the QoS of a latency-sensitive job will be affected by potential <b>co-runners.</b> Once “safe ” batch jobs are selected and mapped to a server, Bubble-Flux uses an Online Flux Engine to continuously monitor the QoS of the latency-sensitive application and control the execution of batch jobs to adapt to dynamic input, phase, and load changes to deliver satisfactory QoS. Batch applications remain in a state of flux throughout execution. Our results show that the utilization improvement achieved by Bubble-Flux is up to 2. 2 x better than the prior static approach. 1...|$|R
40|$|Abstract—In {{this paper}} {{we examine the}} use of base vector {{applications}} {{as a tool for}} classifying an application’s usage of a processor’s resources. We define a series of base vector applications, simple applications designed to place directed stress on a single processor resource. By co-scheduling base vector applications with a target application on a CMT multi-threaded processor, we can determine an application’s sensitivity to a particular processor resource, and an application’s intensity with respect to a particular processor resource. An application’s sensitivities and intensities for a set of processor resources comprise that applications sensitivity and intensity vectors. We envision that sensitivity and intensity vectors could be used for (a) understanding micro-architectural properties of the application; (b) forecasting an optimal co-schedule for an application on a multi-threaded processor; (c) evaluating the suitability of a particular architecture or micro-architecture for the workload without executing the workload on that architecture. We describe the methods for constructing base vector applications and for obtaining an application’s sensitivity and intensity vectors. Using UltraSPARC T 1 (Niagara) as the experimental platform, we validate base vectors as a method for classifying applications by showing that sensitivity and intensity vectors can be used to successfully predict the optimal static <b>co-runner</b> for an application. I...|$|E
40|$|It {{has been}} {{suggested}} that exercise can improve our creative ability (Blanchette, Ramocki, O'del and Casey, 2005) but to date there is scant research upon the lived experiential elements of this relationship. This presentation is based upon the initial phase of a doctoral research project, which explores in depth the interactions between physical activity and artistic creativity through the concept of simultaneous recreational running and creative drawing. ‘Run-to-draw’ is a collaborative endeavour in which the researcher creates drawings on Phidippeddes, an original engineered product designed by, and worn on the back of, a <b>co-runner.</b> In this paper, we focus analytic attention on the richly textured lived experience and physically-active embodiment of the running-drawing-researching self, drawing on an innovative autophenomenographic research approach (Allen-Collinson, 2011). We examine, amongst other aspects, the nuanced, sensory dimensions of the lived experience of run-to-draw engagement. Our ‘data’ are explored and displayed through initial drawings and reflective texts used to interpret and relive what is experienced, imagined, drawn, remembered and felt. In addition, Phidippedes will be present to offer further insight into the research process, and to stimulate dialogue with the researcher and audience pertaining to its methodological and pedagogical possibilities, including for use with children in an educational context...|$|E
40|$|In {{this paper}} we study {{the impact of}} sharing memory {{resources}} on five Google datacenter applications: a web search engine, bigtable, content analyzer, image stitching, and protocol buffer. While prior work has found neither positive nor negative effects from cache sharing across the PARSEC benchmark suite, we find that across these datacenter applications, there is both a sizable benefit and a potential degradation from improperly sharing resources. In this paper, we first present {{a study of the}} importance of thread-tocore mappings for applications in the datacenter as threads can be mapped to share or to not share caches and bus bandwidth. Second, we investigate the impact of co-locating threads from multiple applications with diverse memory behavior and discover that the best mapping for a given application changes depending on its <b>co-runner.</b> Third, we investigate the application characteristics that impact performance in the various thread-to-core mapping scenarios. Finally, we present both a heuristics-based and an adaptive approach to arrive at good thread-to-core decisions in the datacenter. We observe performance swings of up to 25 % for web search and 40 % for other key applications, simply based on how application threads are mapped to cores. By employing our adaptive thread-to-core mapper, the performance of the datacenter applications presented in this work improved by up to 22 % over status quo thread-to-core mapping and performs within 3 % of optimal...|$|E
40|$|Abstract—For higher {{processing}} and computing power, chip multiprocessors (CMPs) {{have become the}} new mainstream architecture. This shift to CMPs has created many challenges for fully utilizing the power of multiple execution cores. One of these challenges is managing contention for shared resources. Most of the recent research address contention for shared resources by single-threaded applications. However, as CMPs scale up to many cores, the trend of application design has shifted towards multi-threaded programming and new parallel models to fully utilize the underlying hardware. There are differences between how single- and multi-threaded applications contend for shared resources. Therefore, to develop approaches to reduce shared resource contention for emerging multi-threaded applications, {{it is crucial to}} understand how their performances are affected by contention for a particular shared resource. In this research, we propose and evaluate a general methodology for characterizing multi-threaded applications by determining the effect of shared-resource contention on performance. To demonstrate the methodology, we characterize the applications in the widely used PARSEC benchmark suite for shared-memory resource contention. The characterization reveals several interesting aspects of the benchmark suite. Three of twelve PARSEC benchmarks exhibit no contention for cache resources. Nine of the benchmarks exhibit contention for the L 2 -cache. Of these nine, only three exhibit contention between their own threads–most contention is because of competition with a <b>co-runner.</b> Interestingly, contention for the Front Side Bus is a major factor with all but two of the benchmarks and degrades performance by more than 11 %. I...|$|E
40|$|Abstract—Rampant {{dynamism}} due to load fluctuations, <b>co-runner</b> changes, and {{varying levels}} of interference {{poses a threat to}} application quality of service (QoS) and has limited our ability to allow co-locations in modern warehouse scale computers (WSCs). Instruction set features such as the non-temporal memory access hints found in modern ISAs (both ARM and x 86) may be useful in mitigating these effects. However, despite the challenge of this dynamism and the availability of an instruction set mechanism that might help address the problem, a key capability missing in the system software stack in modern WSCs is the ability to dynamically transform (and re-transform) the executing application code to apply these instruction set features when necessary. In this work we introduce protean code, a novel approach for enacting arbitrary compiler transformations at runtime for native programs running on commodity hardware with negligible (< 1 %) overhead. The fundamental insight behind the underlying mechanism of protean code is that, instead of maintaining full control throughout the program’s execution as with traditional dynamic optimizers, protean code allows the original binary to execute continuously and diverts control flow only at a set of virtualized points, allowing rapid and seamless rerouting to the new code variants. In addition, the protean code compiler embeds IR with high-level semantic information into the program, empowering the dynamic compiler to perform rich analysis and transformations online with little overhead. Using a fully functional protean code compiler and runtime built on LLVM, we design PC 3 D, Protean Code for Cache Contention in Datacenters. PC 3 D dynamically employs non-temporal access hints to achieve utilization improvements of up to 2. 8 x (1. 5 x on average) higher than state-of-the-art contention mitigation runtime techniques at a QoS target of 98 %. Keywords-compiler; dynamic compiler; optimization; cache; resource sharing; datacenter; warehouse scale computer I...|$|E
40|$|Iterative {{optimization}} is {{a simple}} but powerful approach that searches the best possible combination of compiler optimizations for a given workload. However, iterative optimization is plagued by several practical issues that prevent it from being widely used in practice: {{a large number of}} runs are required to find the best combination, the optimum combination is dataset dependent, and the exploration process incurs significant overhead that needs to be compensated for by performance benefits. Therefore, although iterative optimization has been shown to have a significant performance potential, it seldom is used in production compilers. In this article, we propose iterative optimization for the data center (IODC) : we show that the data center offers a context in which all of the preceding hurdles can be overcome. The basic idea is to spawn different combinations across workers and recollect performance statistics at the master, which then evolves to the optimum combination of compiler optimizations. IODC carefully manages costs and benefits, and it is transparent to the end user. To bring IODC to practice, we evaluate it in the presence of co-runners to better reflect real-life data center operation with multiple applications co-running per server. We enhance IODC with the capability to find compatible co-runners along with a mechanism to dynamically adjust the level of aggressiveness to improve its robustness in the presence of co-running applications. We evaluate IODC using both Map Reduce and compute-intensive throughput server applications. To reflect the large number of users interacting with the system, we gather a very large collection of datasets (up to hundreds of millions of unique datasets per program), for a total storage of 16. 4 TB and 850 days of CPU time. We report an average performance improvement of 1. 48 x and up to 2. 08 x for five MapReduce applications, and 1. 12 x and up to 1. 39 x for nine server applications. Furthermore, our experiments demonstrate that IODC is effective in the presence of co-runners, improving performance by greater than 13 % compared to the worst possible <b>co-runner</b> schedule...|$|E
40|$|Emerging {{architecture}} designs include tens {{of processing}} cores {{on a single}} chip die; {{it is believed that}} the number of cores will reach the hundreds in not so many years from now. However, most common parallel workloads cannot fully utilize such systems. They expose fluctuating parallelism, and do not scale up indefinitely as there is usually a point after which synchronization costs outweigh the gains of parallelism. The combination of these issues suggests that large-scale systems will be either multiprogrammed or have their unneeded resources powered off. Multiprogramming leads to hardware resource contention and as a result application performance degradation, even when there are enough resources, due to negative share effects and increased bus traffic. Most often this degradation is quite unbalanced between co-runners, as some applications dominate the hardware over others. Current Operating Systems blindly provide applications with access to as many resources they ask for. This leads to over-committing the system with too many threads, memory contention and increased bus traffic. Due to the inability of the application to have any insight on system-wide resource demands, most parallel workloads will create as many threads as there are available cores. If every co-running application does the same, the system ends up with threads $N$ times the amount of cores. Threads then need to time-share cores, so the continuous context-switching and cache line evictions generate considerable overhead. This thesis proposes a novel solution across all software layers that achieves throughput optimization and uniform performance degradation of co-running applications. Through a novel fully automated approach (DVS and Palirria), task-parallel applications can accurately quantify their available parallelism online, generating a meaningful metric as parallelism feedback to the Operating System. A second component in the Operating System scheduler (Pond) uses such feedback from all co-runners to effectively partition available resources. The proposed two-level scheduling scheme ultimately achieves having each <b>co-runner</b> degrade its performance by the same factor, relative to how it would execute with unrestricted isolated access to the same hardware. We call this fair scheduling, departing from the traditional notion of equal opportunity which causes uneven degradation, with some experiments showing at least one application degrading its performance 10 times less than its co-runners. QC 20151016 </p...|$|E

