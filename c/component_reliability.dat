348|631|Public
50|$|A {{reliability}} {{block diagram}} (RBD) is a diagrammatic method for showing how <b>component</b> <b>reliability</b> {{contributes to the}} success or failure of a complex system. RBD is also known as a dependence diagram (DD).|$|E
50|$|The {{intention}} is to detect those particular components that would fail {{as a result of}} the initial, high-failure rate portion of the bathtub curve of <b>component</b> <b>reliability.</b> If the burn-in period is made sufficiently long (and, perhaps, artificially stressful), the system can then be trusted to be mostly free of further early failures once the burn-in process is complete.|$|E
50|$|One way to {{increase}} the mass ratio {{is to reduce the}} mass of the empty vehicle by using very lightweight structures and high-efficiency engines. This tends to push up maintenance costs as <b>component</b> <b>reliability</b> can be impaired, and makes reuse more expensive to achieve. The margins are so small with this approach that there is uncertainty whether such a vehicle would be able to carry any payload into orbit.|$|E
40|$|System {{reliability}} equation, {{an exact}} function of <b>component</b> <b>reliabilities,</b> {{for a system}} with {{a finite number of}} points is derived from the minimal states which are found by logical analysis of the configuration. The numerical value is obtained by substituting the <b>component</b> <b>reliabilities</b> or unreliabilities...|$|R
40|$|This paper {{introduces}} estimations {{of reliability}} {{values for the}} individual components in a series system using masked system life data. In particular, we compute the maximum likelihood and Bayes estiinates of <b>component</b> <b>reliabilities</b> when the system components have constant failure rates. In obtaining Bayes estimates, {{it is assumed that}} the <b>component</b> <b>reliabilities</b> are independent random variables having piecewise linear priordistributions. The model is illustrated fora two-component series. A numerical simulation study is presented to show how one can utilize the present approach to compute estimations of <b>component</b> <b>reliabilities</b> fora practical problem. Further, we investigate the comparison between the maximum likelihood and Bayes estimates, based on the respective percentage errors...|$|R
40|$|We {{consider}} a monotone multistate system with conditionally independent components given the <b>component</b> <b>reliabilities,</b> and random <b>component</b> <b>reliabilities.</b> Upper and lower bounds are derived for {{the moments of}} the random reliability function, extending results for binary systems. The second moment of the reliability function is given special attention, as this quantity {{is used to calculate}} the standard deviation of the system availability estimate. key words: monotone multistate syste...|$|R
50|$|Safety {{generally}} {{cannot be}} achieved through <b>component</b> <b>reliability</b> alone. Catastrophic failure probabilities of 10−9 per hour correspond to the failure rates of very simple components such as resistors or capacitors. A complex system containing {{hundreds or thousands of}} components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10−4 or 10−5 per hour. If a system failure is catastrophic, usually the only practical way to achieve 10−9 per hour failure rate is through redundancy.|$|E
50|$|Probabilistic risk {{assessment}} {{has created a}} close relationship between safety and reliability. <b>Component</b> <b>reliability,</b> generally {{defined in terms of}} component failure rate, and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system Mean Time Between Failure (MTBF), system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.|$|E
50|$|Despite {{its high}} initial strength-to-weight ratio, a design {{limitation}} of CFRP is {{its lack of}} a definable fatigue endurance limit. This means, theoretically, that stress cycle failure cannot be ruled out. While steel and many other structural metals and alloys do have estimable fatigue endurance limits, the complex failure modes of composites mean that the fatigue failure properties of CFRP are difficult to predict and design for. As a result, when using CFRP for critical cyclic-loading applications, engineers may need to design in considerable strength safety margins to provide suitable <b>component</b> <b>reliability</b> over its service life.|$|E
40|$|Many papers propose {{different}} approaches to evaluate the <b>reliability</b> of black-box <b>components</b> based on component's success. This paper determines or evaluates <b>component's</b> <b>reliability</b> based on its failure. Since even reliable components might fail, we argue that component's acquirer should evaluate the <b>component's</b> <b>reliability</b> based on its failure and not on its success- In this {{paper we propose a}} new approach towards determining the component's failure. Both the operational profile and the appropriate test cases are needed to support our approach. ...|$|R
30|$|Thus, in {{case the}} order of <b>component</b> <b>reliabilities</b> is known, the problem can be solved {{in a similar manner}} as will be {{discussed}} in this paper.|$|R
40|$|The (n,f,k) system {{consists}} of n components ordered {{in a line}} or a cycle, while the system fails if, and only if, there exist at least f failed components or at least k consecutive failed components. For the linear (n,f,k) system with equal <b>component</b> <b>reliabilities,</b> the system reliability formula was given by Sun and Liao (1990). In this paper, we obtain the system reliability formulas for the linear and the circular systems with different <b>component</b> <b>reliabilities</b> {{by means of a}} Markov chain method. Consecutive system Markov chain (n,f,k) system Operations Reliability...|$|R
50|$|Leverage-point {{modeling}} (LPM) is a demonstrated {{approach for}} improved planning and spending for operations and support (O&S) activities. LPM is a continuous-event simulation technique {{that uses the}} system dynamics approach of model building. Dr. Nathaniel Mass championed the potential of LPM, and adapted it for the Department of Defense (DoD) {{as a tool for}} jumping to a higher performance curve as a means of offsetting higher costs and declining budgets. The purpose of LPM is to test policies and investments that improve mission capability for a given level of investment or funding. It is particularly used to evaluate investments in <b>component</b> <b>reliability</b> and parts availability.|$|E
50|$|One of {{the most}} visible areas of Billinton’s {{research}} is that of reliability cost/worth evaluation involving customer power interruption costs. This research extends the calculation of conventional reliability indices to include customer damage {{in the form of}} increased monetary costs due to power supply failures. This is now known as value-based reliability assessment (VBRA), in which the unreliability costs are added to the capital and operating costs to produce the total cost value used in project decision making. The VBRA process involves having <b>component</b> <b>reliability</b> data, the ability to calculate suitable load point reliability indices and applicable customer damage costs. These three requirements are highly visible contributions in Billinton’s list of journal and conference publications.|$|E
5000|$|Building a {{computer}} with the required performance, {{size and weight}} demanded the use of transistors, which were at that time very expensive and not very reliable. Earlier efforts to use transistorized computers for guidance, BINAC and the system on the SM-64 Navaho, had failed and were abandoned. The Air Force and Autonetics spent millions on a program to improve transistor and <b>component</b> <b>reliability</b> 100 times, leading to the [...] "Minuteman high-rel parts" [...] specifications. The techniques developed during this program were equally useful for improving all transistor construction, and greatly reduced the failure rate of transistor production lines in general. This improved yield, which {{had the effect of}} greatly lowering production costs, and had enormous spin-off effects in the electronics industry.|$|E
40|$|In {{the present}} {{scenario}} {{of globalization and}} liberalization, {{it is imperative that}} Indian industries become fully conscious of the needs to produce reliable products meeting International standards. Reliability of a system can be maximized by attaching parallel components to each of the components in the system such that if one component fails, one of its parallel components comes into operation and the system does not fail until all parallel units fail. Integrated Reliability Model refers to the determination of the number of <b>components,</b> <b>component</b> <b>Reliabilities,</b> stage Reliabilities and the system Reliability wherein the problem considers both the unknowns that is the <b>component</b> <b>Reliabilities</b> and the number of components in each stage for the given cost constraint to maximize the System Reliability. In this work an attempt is made to develop an integrated reliability model for a series-parallel configuration subject to the cost, weight and volume constraints. The Lagrangian Multiplier method is used to determine the <b>component</b> <b>reliabilities,</b> stage <b>reliabilities,</b> number of <b>components</b> in each stage and the system reliability for a series-parallel configuration. Using this method, rounded values cannot be obtained, but number of components should be an integer. To get the integer value, Integer programming has been successfully used. NOMENCLATURE: Rs = System Reliabilit...|$|R
40|$|We {{consider}} a binary system with n binary components and random <b>component</b> <b>reliabilities.</b> An upper {{bound for the}} mth moment (m [greater-or-equal, slanted] 2) of the system reliability function is derived, using a stochastic domination argument. Coherent system uncertainty moment inequality stochastic order...|$|R
50|$|It is one <b>component</b> of <b>reliability</b> engineering.|$|R
50|$|System {{reliability}} is {{an important}} factor in organizations. Reliability is the consistency with which operations are maintained, and may vary from zero output (a complete breakdown or work stoppage) to a constant or predictable output. The typical system operates somewhere between these two extremes. The characteristics of reliability can be designed into the system by carefully selecting and arranging the operating components; the system is no more reliable than its weakest segment. When the requirements for a particular component — such as an operator having unique skills — are critical, it may be worthwhile to maintain a standby operator. In all situations, provisions should be made for quick repair or replacement when failure occurs. One valid approach to the reliability-maintenance relationship is to use a form of construction that permits repair by replacing a complete unit. In some television sets, for example, it is common practice to replace an entire section of the network rather than try to find the faulty <b>component.</b> <b>Reliability</b> is not as critical an issue when prompt repair and recovery can be instituted.|$|E
50|$|Such monolithic, all-fiber {{devices are}} {{produced}} by many companies worldwide and at power levels exceeding 1 kW. The major advantage of these all fiber systems, where the free space mirrors are replaced {{with a pair of}} fiber Bragg gratings (FBG’s), is the elimination of realignment during the life of the system, since the FBG is spliced directly to the doped fiber and never needs adjusting. The challenge is to operate these monolithic cavities at the kW CW power level in large mode area (LMA) fibers such as 20/400 (20 um diameter core and 400 um diameter inner cladding) without premature failures at the intra-cavity splice points and the gratings. Once optimized, these monolithic cavities do not need realignment during the life of the device, removing any cleaning and degradation of fiber surface from the maintenance schedule of the laser. However, the packaging and optimization of the splices and FBGs themselves are non-trivial at these power levels as are the matching of the various fibers, since the composition of the Yb-doped fiber and various passive and photosensitive fibers needs to be carefully matched across the entire fiber laser chain. Although the power handling capability of the fiber itself far exceeds this level, and is possibly as high as >30 kW CW, the practical limit is much lower due to <b>component</b> <b>reliability</b> and splice losses.|$|E
50|$|In the 1960s more {{emphasis}} {{was given to}} reliability testing on component and system level. The famous military standard 781 was created at that time. Around this period also the much-used (and also much-debated) military handbook 217 was published by RCA (Radio Corporation of America) and {{was used for the}} prediction of failure rates of components. The emphasis on <b>component</b> <b>reliability</b> and empirical research (e.g. Mil Std 217) alone slowly decreases. More pragmatic approaches, as used in the consumer industries, are being used. In the 1980s, televisions were increasingly made up of solid-state semiconductors. Automobiles rapidly increased their use of semiconductors with a variety of microcomputers under the hood and in the dash. Large air conditioning systems developed electronic controllers, as had microwave ovens {{and a variety of other}} appliances. Communications systems began to adoptelectronics to replace older mechanical switching systems. Bellcore issued the first consumer prediction methodology for telecommunications, and SAE developed a similar document SAE870050 for automotive applications. The nature of predictions evolved during the decade, and it became apparent that die complexity wasn't the only factor that determined failure rates for Integrated Circuits (ICs).Kam Wong published a paper questioning the bathtub curve—see also reliability-centered maintenance. During this decade, the failure rate of many components dropped by a factor of 10. Software became important to the reliability of systems. By the 1990s, the pace of IC development was picking up. Wider use of stand-alone microcomputers was common, and the PC market helped keep IC densities following Moore's law and doubling about every 18 months. Reliability engineering now was more changing towards understanding the physics of failure. Failure rates for components kept on dropping, but system-level issues became more prominent. Systems thinking became more and more important. For software, the CCM model (Capability Maturity Model) was developed, which gave a more qualitative approach to reliability. ISO 9000 added reliability measures as part of the design and development portion of Certification. The expansion of the World-Wide Web created new challenges of security and trust. The older problem of too little reliability information available had now been replaced by too much information of questionable value. Consumer reliability problems could now have data and be discussed online in real time. New technologies such as micro-electromechanical systems (MEMS), handheld GPS, and hand-held devices that combined cell phones and computers all represent challenges to maintain reliability. Product development time continued to shorten through thisdecade and what had been done in three years was being done in 18 months. This meant that reliability tools and tasks must be more closely tied to the development process itself. In many ways, reliability became part of everyday life and consumer expectations.|$|E
40|$|International audienceThis work {{presents}} a general framework {{taking into account}} system and <b>components</b> <b>reliability</b> in a Model Predictive Control (MPC) algorithm. The objective is to deal from an availability point of view with a closed-loop system combining a deterministic part related to the system dynamics and a stochastic part related to the system reliability. The main contribution of this work consists in integrating the reliability assessment computed on-line using a Dynamic Bayesian Network (DBN) through the weights of the multiobjective cost function of the MPC algorithm. A comparison between a method based on the <b>components</b> <b>reliability</b> (local approach) and a method focused on the system reliability sensitivity analysis (global approach) is considered. The effectiveness and benefits of the proposed control framework are presented through a Drinking Water Network (DWN) simulation...|$|R
50|$|It {{has three}} major <b>components,</b> multicast, <b>reliability</b> and scalability.|$|R
40|$|Published in Advanced and Intelligent Computations in Diagnosis and Control, Advances in Intelligent Systems and Computing Series, vol. 386, pp. 161 - 177, Springer International Publishing, 2016 International audienceThis paper {{presents}} a general framework {{that takes into}} account system and <b>components</b> <b>reliability</b> in a Model Predictive Control (MPC) algorithm. The objective is to deal from an availability point of view with a closed-loop system combining a deterministic part related to the system dynamics and a stochastic part related to the system reliability. The main contribution of this work consists in integrating the reliability assessment computed on-line using a Dynamic Bayesian Network (DBN) to compute the weights of the multiobjective cost function into the MPC algorithm. Also, a comparison between a method based on the <b>components</b> <b>reliability</b> (local approach) and a method focused on the system reliability sensitivity analysis (global approach) is considered. The effectiveness and benefits of the proposed control framework are shown by its application on a Drinking Water Network (DWN) ...|$|R
40|$|International audienceWe {{extend a}} multi-state physics model (MSPM) {{framework}} for <b>component</b> <b>reliability</b> assessment by including semi-Markov and random shock processes. Dependences {{between the two}} processes are considered. A Monte Carlo simulation algorithm is developed to compute <b>component</b> <b>reliability.</b> An example is illustrated {{with respect to a}} literature case study...|$|E
40|$|ABSTRACT: We {{extend a}} multi-state physics model (MSPM) {{framework}} for <b>component</b> <b>reliability</b> assessment by including semi-Markov and random shock processes. Dependences {{between the two}} processes are considered. A Monte Carlo simulation algorithm is developed to compute <b>component</b> <b>reliability.</b> An example is illustrated {{with respect to a}} literature case study. hal- 00838745, version 1 - 26 Jun 2013...|$|E
40|$|The {{analytical}} {{expressions of}} reliability indices such as LOLP, LOLF and EDNS, {{with respect to}} <b>component</b> <b>reliability</b> parameters, are firstly presented, and then the actual physical meanings of the coefficients K 1 -K 4 and D 1, D 2 in these analytical formulas are detailed. Based on these analytical formulas, {{a full set of}} sensitivity formulas of reliability indices with respect to <b>component</b> <b>reliability</b> parameters are deduced. Utilizing these analytical formulas, we can not only easily calculate the system reliability indices accurately in an analytical mode when <b>component</b> <b>reliability</b> parameters vary, but also obtain the function curves of the system reliability indices versus the <b>component</b> <b>reliability</b> parameters. The proposed method can effectively alleviate calculation catastrophe and provide an advanced technique to promote the engineering application of bulk power systems reliability assessment. Furthermore, using the sensitivity formulas we can effectually identify system 'bottlenecks' and get important guide information for power system planning and operation...|$|E
40|$|Present {{electronics}} industry component {{sizes are}} efficiently reducing {{to meet the}} product requirement with compact size with greater performance in compact size products resulting in different problems from thermal prospective to bring product better performance electrically and mechanically. In this paper we will study how to overcome the thermal problem for a product which includes <b>components</b> <b>reliability</b> and PCB performance by using CFD thermal simulation tool (Ansys Icepak) ...|$|R
40|$|We give {{a hybrid}} two stage design {{which can be}} useful to {{estimate}} the reliability of a parallel-series and/or by duality a series-parallel system, when the <b>component</b> <b>reliabilities</b> are unknown {{as well as the}} total numbers of units allowed to be tested in each subsystem. When a total sample size is fixed large, asymptotic optimality is proved systematically and validated via Monte Carlo simulation. Comment: 16 pages, 4 figure...|$|R
40|$|This work {{deals with}} setting of {{dependability}} of static electricity meters. The first two chapters deal about electricity meters and dependability in general {{than there are}} introduced three possible ways for getting dependability parameters. The first methode is data collection from the field. The second methode is <b>reliability</b> prediction from <b>component's</b> <b>reliability.</b> The third methode is aging life tests. Conclusion contains evaluation of results and highligting of importance for practical usage...|$|R
40|$|This {{paper is}} to address the basics, the {{limitations}} {{and the relationship between}} <b>component</b> <b>reliability</b> and system reliability through a study of flight computing architectures and related avionics components for NASA future missions. <b>Component</b> <b>reliability</b> analysis and system reliability analysis need to be evaluated at the same time, and the limitations of each analysis and {{the relationship between the two}} analyses need to be understood...|$|E
40|$|Although many {{approaches}} for architecture- based reliability estimation exist, these approaches are typically limited to certain classes or exclusively concentrate on software reliability, neglecting {{the influence of}} hardware resources, <b>component</b> <b>reliability,</b> component replica and software deployment. In this paper, a reliability estimation model based on software architecture (SA) is presented. This model incorporates the influence of software deployment, <b>component</b> <b>reliability</b> and component replica. Component lifetimes can be modeled by exponential distribution. The approach of calculating system reliability considering component replica and <b>component</b> <b>reliability</b> is proposed. The influences of different deployment architectures on component reliabilities and system reliability are investigated. The improvement of system reliability by redeployment or component replica is discussed. </p...|$|E
40|$|Availability is an {{important}} factor to be considered when designing wind turbine generator systems. The quest for increasing availability is based on the following five design approaches - design for <b>component</b> <b>reliability,</b> active control for reliability, design for fault tolerance, prognostics, and design for maintainability. This paper reviews methods focussing on the first three, i. e. <b>component</b> <b>reliability,</b> active control, and fault tolerance. The paper further identifies some promising directions for further research. DC systems, Energy conversion & StorageElectrical Power Processin...|$|E
40|$|Abstract. According to <b>components</b> <b>reliability</b> {{sensitivity}} theory methods, looking {{upon the}} dynamic characteristics of hydrodynamic lubrication sliding bearing as the target for study, doing some reliability sensitivity {{analysis for the}} bearing-rotor system instability issues, numerical simulation results are given though analyzing. The outcome of the dynamic performance index is proposed based on the sliding bearing selection method for engineering practice to {{the selection of the}} radial journal bearing and provides a theoretical basis for overall design of shafting...|$|R
40|$|Traditionally, {{reliability}} {{models of}} component-based software systems compute the point estimate of system reli-ability by plugging point estimates of unknown parameters into the model. These models discard {{the uncertainty of}} the parameters, that is, do not attempt to answer the question how parameters uncertainties affect the estimates of system reliability. In this paper we focus on uncertainty analysis in software reliability based on method of moments. In par-ticular, we present a generalization of our earlier work that allows us to consider the uncertainty in the operational pro-file (i. e., the way software is used) in addition to the uncer-tainty in components failure behavior (i. e., component re-liabilities) considered earlier. The method of moments is an approximate analytical method that allows us to gener-ate system reliability moments based on (1) the knowledge of software architecture reflected in the expression of sys-tem reliability as a function of <b>component</b> <b>reliabilities</b> and frequencies of control transfer between components, (2) es-timates of the moments of <b>components</b> <b>reliabilities,</b> and (3) estimates of the moments of probabilities of control transfer between components. Further, we apply the method of mo-ments on two case studies and discuss its advantages and disadvantages. ...|$|R
30|$|We now {{present the}} {{prototype}} currently running at the Conrad Observatory using a data set covering the past 2  h of near real-time ACE solar wind data and geomagnetic data measured (with a few ms delay) at the observatory. The software {{was put together}} after analyzing the various methods {{in this study and}} considering how best to implement these in a continuous detection software. The method determined to bring the best of all <b>components</b> (<b>reliability,</b> speed, and accuracy) was the MODWT method.|$|R
