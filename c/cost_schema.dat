1|11|Public
40|$|Traditional project {{management}} principles strongly emphasize the triple constraints: time, cost and quality. The new paradigm, however, includes people, who remain {{the focus of}} all activities. The aim {{of this paper is}} to examine the result of a current research in Indonesia where a standard International Construction Contract is applied in the infrastructure sector because it was financed by multilateral agencies. Project people from those stakeholders groups were interviewed to collect their perceptions about the value of time, quality and cost aside from the issues of International contruction contract and project leadership. Initially, Owners, Designers, and Contractors were targeted. Use of the theoretical sampling process determined the need to target a second set of respondents. A construtivist ontology, interpretive epistemology and qualitative methodology were adopted. In-depth interviews were held with twenty-seven respondents from the stakeholder groups. Initial findings emerged concept that were then presented to expert from the Owner, Industry and University. Using constructivist paradigm, two major emerging constructs appeared from the respondents. The findings lead to the suggestion that modifications are needed to enhance the existing triple constraints in {{project management}}: time, quality and cost. The model of P & LESS was derived from the respondents. This embeds the elements of People, namely Leadership, Ethic, and Social Status with the time, quality and <b>cost</b> <b>schema</b> of project management. With this new concept, people and their elements would be more taken into consideration by the project stakeholders in securing the project success. Preliminary findings have shown that there are significant in the literature as opposed to practices in the real world of project implementation. This paper challenges the quantitative, technical, outcome based approach as unitary in nature. It presents a different view or paradigm that adds, as a central focus, the ‘people 2 ̆ 7 connection to time, quality and cost. It suggests that the importance of various aspects of people should also be taken into serious consideration in the evolution of project management, not only as a technical tool but more importantly as a field of knowledge required to solve organizational problems. When operating in another culture, relational aspects become a central focus...|$|E
40|$|International audienceThe global {{deployment}} of cloud datacenters is enabling large web services to deliver fast response to users worldwide. This unprecedented geographical {{distribution of the}} computation also brings new challenges related to the efficient data management across sites. High throughput, low latencies, cost- or energy-related trade-offs {{are just a few}} concerns for both cloud providers and users when it comes to handling data across datacenters. Existing cloud data management solutions are limited to cloud-provided storage, which offers low performance based on rigid <b>cost</b> <b>schemas.</b> Users are therefore forced to design and deploy custom solutions, achieving performance at the cost of complex system configurations, maintenance overheads, reduced reliability and reusability. In this paper, we are proposing a dedicated cloud data transfer service that supports largescale data dissemination across geographically distributed sites, advocating for a Transfer as a Service (TaaS) paradigm. The system aggregates the available bandwidth by enabling multiroute transfers across cloud sites. We argue that the adoption of such a TaaS approach brings several benefits for both users and the cloud providers who propose it. For users of multi-site or federated clouds, our proposal is able to decrease the variability of transfers and increase the throughput up to three times compared to baseline user options, while benefiting from the well-known high availability of cloud-provided services. For cloud providers, such a service can decrease the energy consumption within a datacenter down to half compared to user-based transfers. Finally, we propose a dynamic <b>cost</b> model <b>schema</b> for the service usage, which enables the cloud providers to regulate and encourage data exchanges via a data transfer market...|$|R
40|$|Purpose: To compare from a {{societal}} perspective the cost-effectiveness and cost-utility of schema therapy, clarification-oriented psychotherapy, and treatment as usual {{for patients with}} avoidant, dependent, obsessive-compulsive, paranoid, histrionic, and/or narcissistic personality disorder. Method: A multicenter, randomized controlled trial, single-blind parallel design, was conducted between May 2006 and December 2011 in 12 Dutch mental health institutes. Data from 320 patients (diagnosed according to DSM-IV criteria) randomly assigned to schema therapy (n = 145), treatment as usual (n = 134), or clarification-oriented psychotherapy (n = 41) were analyzed. Costs were repeatedly measured during 36 months by interview and patient registries. Primary outcome measures were proportion of recovered patients as measured with the Structured Clinical Interview for DSM-IV Axis II Personality Disorders for the cost-effectiveness analysis, and quality-adjusted life-years (QALYs) for the cost-utility analysis. Bootstrap replications in the cost-effectiveness and the cost-utility planes were used to estimate the probability that one treatment was more cost-effective than the other. Mixed gamma regression on net monetary benefit for different levels of willingness to pay for extra effects was used as sensitivity analysis. Additional sensitivity analyses were done to assess robustness of the results. Results: Due to higher clinical effects and lower <b>costs,</b> <b>schema</b> therapy was dominant over the other treatments in the cost-effectiveness analyses. Schema therapy has the probability of being the most cost-effective treatment (78 % at € 0 to 96 % at € 37, 500 [$ 27, 375] willingness to pay per extra recovery). Treatment as usual was more cost-effective than clarification-oriented psychotherapy due to lower costs. In the cost-utility analysis, schema therapy had a stable 75 % probability of being cost-effective. Sensitivity analyses confirmed these findings. Conclusions: The results support the cost-effectiveness of schema therapy but not of clarification-oriented psychotherapy...|$|R
40|$|International audienceOver the years, many schema {{matching}} {{approaches have}} been developed to discover correspondences between schemas. Although this task is crucial in data integration, its evaluation, both in terms of matching quality and time performance, is still manually performed. Indeed, there is no common platform which gathers a collection of schema matching datasets to fulfil this goal. Another problem deals with the measuring of the post-match effort, a human <b>cost</b> that <b>schema</b> matching approaches aim at reducing. Consequently, we propose XBenchMatch, a schema matching benchmark with available datasets and new measures to evaluate this manual post-match effort and the quality of integrated schemas. We finally report the results obtained by different approaches, namely COMA++, Similarity Flooding and YAM. We show that such a benchmark is required to understand the advantages and failures of schema matching approaches. Therefore, it could help an end-user to select a schema matching tool which covers his/her needs...|$|R
40|$|Over the years, many schema {{matching}} {{approaches have}} been developed to discover correspondences between schemas. Although this task is crucial in data integration, its evaluation, both in terms of matching quality and time performance, is still manually performed. Indeed, there is no common platform which gathers a collection of schema matching datasets to fulfil this goal. Another problem deals with the measuring of the post-match effort, a human <b>cost</b> that <b>schema</b> matching approaches aim at reducing. Consequently, we propose XBenchMatch, a schema matching benchmark with available datasets and new measures to evaluate this manual post-match effort and the quality of integrated schemas. We finally report the results obtained by different approaches, namely COMA++, Similarity Flooding and YAM. We show that such a benchmark is required to understand the advantages and failures of schema matching approaches. Therefore, it could help an end-user to select a schema matching tool which covers his/her needs. TYPE OF PAPER AND KEYWORDS Regular research paper: schema matching, benchmark, data integration, heterogeneous databases, evaluation, inte...|$|R
40|$|This paper	explores	the	societal	underpinnings	of	child	abuse	and	neglect. It looks	at	child	abuse	and	neglect	as	systemic	violence	against	children, and argues	the	importance	of	recognising	its	occurrence	as	the	collateral damage of	social	strategy	and	not	just	individual	happenstance. Thus,	 {{in order}} to	address	 the	problem	of	 child	abuse	and	neglect	 effectively,	 we need to	understand	the	normative	biases	in	its	favour,	 its	structural	causes, dynamic <b>schema,</b>	 <b>costs,</b>	 fallout	 and	 payoff. In	 other	words,	 we	 need	 to know why,	 how,	 in	which	ways	and	for	whose	benefit	societies	operate	as though (and	people	seem	to	think) 	 violence	against	children	is	ok	 –	 even {{necessary}} –	 and,	 so,	 perpetuated. The	paper	particularly	 focuses	on	 the evidence of	indigenous	children...|$|R
40|$|Person re-identification {{concerns}} the matching of pedestrians across disjoint camera views. Due {{to the changes}} of viewpoints, lighting condition-s and camera features, images of the same person from different views always appear differently, and thus feature representations across disjoint camera views of the same person follow different distribu-tions. In this work, we propose an effective, low <b>cost</b> and easy-to-apply <b>schema</b> called the Mirror Representation, which embeds the view-specific feature transformation and enables alignment of the feature distributions across disjoint views for the same person. The proposed Mirror Representation is also designed to explicitly model the relation be-tween different view-specific transformations and meanwhile control their discrepancy. With our Mir-ror Representation, we can enhance existing sub-space/metric learning models significantly, and we particularly show that kernel marginal fisher anal-ysis significantly outperforms the current state-of-the-art methods through extensive experiments on VIPeR, PRID 450 S and CUHK 01. ...|$|R
40|$|Consistency of a {{database}} is {{as an important}} property that must be preserved at all times. In most OODB systems today, application code can directly access and alter both the data {{as well as the}} structure of the database. As a consequence application code can potentially violate the integrity of the database, in terms of the invariants of the data model, the user-specified application constraints, and even the referential integrity of the objects themselves. A common form of consistency management in most databases today is to encode constraints at the system level (e. g., foreign keys), or at the trigger based level (e. g., user constraints) and to perform transaction rollback on discovery of any violation of these constraints. However, for programs that alter the structure as well as the objects in {{a database}}, such as an extensible schema evolution program, roll-backs are expensive and add to the already astronomical <b>cost</b> of doing <b>schema</b> evolution. In this paper, pre-execution [...] ...|$|R
40|$|Design of a multicentered {{randomized}} controlled trial on the clinical and <b>cost</b> effectiveness of <b>schema</b> therapy for personality disorders Lotte LM Bamelis 1 *, Silvia MAA Evers 2 and Arnoud Arntz 1, 3 Background: Despite international guidelines describing psychotherapy as first choice {{for people with}} personality disorders (PDs), well-designed research on the effectiveness and cost-effectiveness of psychotherapy for PD is scarce. Schema therapy (ST) is a specific form of psychological treatment {{that proved to be}} effective for borderline PD. Randomized controlled studies on the effectiveness of ST for other PDs are lacking. Another not yet tested new specialized treatment is Clarification Oriented Psychotherapy (COP). The aim of this project is to perform an effectiveness study as well as an economic evaluation study (cost effectiveness as well as cost-utility) comparing ST versus COP versus treatment as usual (TAU). In this study, we focus on avoidant, dependent, obsessive-compulsive, paranoid, histrionic and narcissistic PD. Methods/Design: In a multicentered {{randomized controlled}} trial, ST, and COP as an extra experimental condition, are compared to TAU. Minimal 300 patients are recruited in 12 mental health institutes throughout th...|$|R
40|$|BACKGROUND: Several {{studies have}} {{evaluated}} the (<b>cost)</b> effectiveness of <b>schema</b> therapy for personality disorders, but {{little research has}} been done on the perspectives of patients and therapists. AIM: The present study aims to explore patients' and therapists' perspectives on schema therapy. METHOD: Qualitative data were collected through in-depth semi-structured interviews with 15 patients and a focus group of 8 therapists. A thematic analysis was performed. RESULTS: Most patients and therapists agreed that helpful aspects in schema therapy were the highly committed therapeutic relationship, the transparent and clear theoretical model, and the specific schema therapy techniques. About unhelpful aspects, several patients and some therapists shared the opinion that 50 sessions was not enough. Furthermore, patients lacked clear advance information about the possibility that they might temporarily experience stronger emotions during therapy and the possibility of having telephone contact outside session hours. They missed practical goals in the later stage of therapy. With regard to imagery, patients experienced time pressure and they missed a proper link between the past and the present. For therapists, it was hard to manage the therapeutic relation, to get used to a new kind of therapy and to keep the treatment focused on personality problems. CONCLUSIONS: Patients and therapists found some aspects of the schema therapy protocol helpful. Their views about which aspects are unhelpful and their recommendations need to be taken into consideration when adjusting the protocol and implementing schema therapy...|$|R
40|$|Consistency of a {{database}} {{is an important}} property that must be preserved at all times. In most OODB systems today, application code can directly access and alter both the data {{as well as the}} structure of the database. As a consequence application code can potentially violate the integrity of the database, in terms of the invariants of the data model, the user-specified application constraints, and even the referential integrity of the objects themselves. A common form of consistency management in most databases today is to encode constraints at the system level (e. g., foreign keys), or at the trigger based level (e. g., user constraints) and to perform transaction rollback on discovery of any violation of these constraints. However, for programs that alter the structure as well as the objects in {{a database}}, such as an extensible schema evolution program, roll-backs are expensive and add to the already astronomical <b>cost</b> of doing <b>schema</b> evolution. In this paper, pre-execution formal verification of schema evolution programs is proposed as an alternative solution to the traditional rollback solution for consistency management. As part of this work we introduce the notion of contracts, i. e., pre- and post-conditions for an extensible schema evolution program, and demonstrate that they can be specified using a familiar language, OQL. We also demonstrate the ease and practicality of using a theorem prover for the formal verification of schema evolution programs. The theorem prover tool can be set up initially with all the information about the environment, i. e., the axioms of the database, the invariants and the basic schema evolution primitives. A writer then of an extensible schema evolution program need only supply the contracts and the program written in OQL to guarantee the correctness of their program. We highlight the main features of the verification process using a complete walk-through example. The end result of our approach is a more efficient consistency management framework that has limited overhead to the users and yet provides flexibility to safely add new schema evolution transformations to the system while assuming complete correctness...|$|R
40|$|In dieser Arbeit wurde eine Integrationsarchitektur speziell für die Bedürfnisse kleiner und mittlerer Unternehmen entworfen. Hierfür wurden die Gründe, welche der Integration in KMU-Wertschöpfungsnetzwerken entgegenstehen, untersucht. Als zentrale Barrieren wurden die hohen Kosten für Integrationsprojekte, die mangelnde Autonomieerhaltung der Wertschöpfungspartner, komplexe Standards, nicht für die Integration vorbereitete Legacy-Anwendungen sowie ein fehlendes Dienstleistungsangebot für diese Anwendergruppe identifiziert. Das Konzept der Integrationsarchitektur VIANA sieht daher eine dezentrale Integration für Wertschöpfungsnetzwerke vor. Standardisiert wurde die Architektur mit dem Fokus auf eine hohe Wiederverwendung von softwaretechnischen Komponenten und der Autonomie-erhaltung der beteiligten Unternehmen. Die zentrale Innovation der Integrationsarchitektur VIANA besteht darin, dass sie nicht von einer zentralen Integrationskomponente abhängt, gleichzeitig jedoch die Abstraktion bereitstellt, die ein zentraler Dienst bietet. Durch die Verwendung von teilweise standardisierten Konnektoren, zur Realisierung der Integration in einem dezentralen P 2 P-Netzwerk, bleiben die integrierten Informationssysteme zur Laufzeit autonom. Weiterhin hat eine Änderung eines integrierten Informationssystems nur {{minimale}} Auswirkungen auf das gesamte Integrationsnetzwerk. Erreicht wird dies durch optimistische Integration. Hierbei werden Objekte zwischen heterogenen Informationssystemen materialisierend integriert. Der Autonomieerhaltung wird durch mehrere Aspekte Rechnung getragen. Durch optimistische Integration werden Objekte sperrenfrei zwischen Informationssystemen ausgetauscht. Die sperrenfreie materialisierende Integration erhält die Autonomie der Datenzugriffsschicht der integrierten Systeme. Ferner wurde Flexibilität in der Wahl von Datenaustauschstandards ermöglicht. Konfliktsituationen wurden analysiert und Maßnahmen zu ihrer Verhinderung oder Behebung wurden ergriffen. Insbesondere wurde dies durch die Verwendung von globalen IDs und Operations-IDs erreicht. Zur manuellen Konfliktbehebung wurde eine Benutzungs-schnittstelle realisiert. Die Integrationsarchitektur wurde implementiert und in einem realen Szenario für Handelsvertreter angewendet. Die Implementierung wurde weiter in ihrem Laufzeitverhalten analysiert. So konnte die Realisierbarkeit gezeigt und das Konzept evaluiert werden. Durch die Evaluation konnte gezeigt werden, dass das Konzept die geforderten Anforde-rungen erfüllt und daher einen Ansatz darstellt, die beschriebenen Barrieren zu adressieren. Wie für Softwaresysteme allgemein und spezieller Integrationsarchitekturen hängt ihre Akzeptanz jedoch von weiteren Faktoren ab, etwa der Beziehung zum Dienstleister und Softwarehersteller, sowie deren Geschäftsmodellen. Durch die Erfüllung der Anforderungen von KMU-Wertschöpfungsnetzwerken kann jedoch davon ausgegangen werden, dass VIANA für diesen Anwendungsfall besser geeignet ist als bestehende Systeme. There {{are very}} few IT-integrations implemented by small and medium sized enterprises (SME). Significant efforts have been undertaken to identify the relevant barriers. A review of the relevant literature revealed inhibitors to SME IS integration adoption. The four most often mentioned barriers are: Even though costs need to be evaluated {{in light of their}} expected benefits, high cost expectations is the most often mentioned barrier. Especially for SME with relatively small liquidity this requires special consideration. Low in-house expertise, both at the technical level as well as the impact of IS integration on the business model requires SME specific consultancy modes. An integration architecture needs to be deployable and administrable by SME specific service offerings, as offers to larger firms are not applicable to SME. This implies that the service needs to be scalable such that a low cost offer can be made. The third important inhibitor is the inadequate fit of IS integration structure with the organizational structure. IS must be designed and adjusted to fit the organizational culture and not vice versa. Following Conway (1968) the structure of an IS has to resemble the structure of the user’s organization. The missing adequate integration technology for SME is a barrier to its adoption. The structure of SME value networks is characterized by independent peers collaborating mutually without central coordination. The homomorphism to an IS integration architecture is therefore best given by a P 2 P network topology. For the integration unprepared legacy systems impose substantial efforts in the realization of an integration project, as no existing components may be reused. At times it is even necessary to replace legacy with new application to achieve integration readiness. Optimistic replication refers to a concept of materializing integration where conflicts are handled optimistically. If conflicts are expected to happen rather infrequently, it is better to wait for them and react then to prevent conflicts. This is called optimistic. Preventing conflicts as in pessimistic replication relies on locking mechanisms which reduce the scalability of the integration architecture. Furthermore, locking not only locks the integration but needs to (partially) lock the integrated EIS. It influences therefore its autonomy and availability. As replication refers to the concept of copying data between identical schemas, the author proposes the term optimistic integration as a technique that integrates data from heterogeneous sources optimistically. Views are used to achieve high adaptability to legacy application and to control the complexity of the integration with network partners. The adaptability is achieved by the resemblance of the integrated system’s exported schema by the adaptors native schema. It is the semantic interface to the integrated information system. A second system dependent schema is necessary for object identification. It is realized in the adaptor as the local <b>schema.</b> <b>Cost</b> reduction can be targeted by reuse of functionalities, which therefore need a system independent view on the data. This is realized as a generic XML schema, the conceptual schema. The complexity reduction of the integration with network partners requires standardized interfaces. Yet the organizational structure forbids a dictated single schema as interface. The fourth schematic level is therefore designed as the logical interfaces to connected peers. It consists of exported schemas. The proposed artifact is based on a multi-level research approach using design science as its core methodology. The multiple levels are (i) acceptance by SME value network partners at the individual level, (ii) integration of heterogeneous information systems at the business case level, and (iii) optimistic integration at the internal technological level. The feasibility of the overall approach can only be positively evaluated, if it is positively evaluated on each level separately. To evaluate the dynamic behavior of optimistic integration as a complex system it was stimulated and the decay time to zero activity measured. The results show, that conflicts are handled properly. Although a multitude of integration approaches exists, the adoption of B 2 B integration by SME value networks is still low. The contribution of this dissertation is twofold: Following the two principals of Hanseth and Lyytinen (2010) it (i) analyzed the requirements of SME value networks from a socio-technical perspective on a B 2 B IT-integration architecture in order to meet early users’ needs directly. As existent integration approaches cannot find acceptance in the analyzed domain it then (ii) derived an architecture based on the local design needs and the distributed topology of SME value networks. Through the evaluation it was shown that the concept meets the requirements and hence is an approach to address the described barriers...|$|R

