38|10|Public
40|$|Recent {{progress}} in high-throughput instrumentations {{has led to}} an astonishing growth in both volume and complexity of biomedical data collected from various sources. The planet-size data brings serious challenges to the storage and computing technologies. Cloud computing is an alternative to crack the nut because it gives <b>concurrent</b> <b>consideration</b> to enable storage and high-performance computing on large-scale data. This work briefly introduces the data intensive computing system and summarizes existing cloud-based resources in bioinformatics. These developments and applications would facilitate biomedical research to make the vast amount of diversification data meaningful and usable...|$|E
40|$|Graduation date: 2012 Interest in {{accounting}} for {{environmental impacts of}} products, processes, and systems during the design phase is increasing. Numerous studies have undertaken investigations for reducing environmental impacts across the product life cycle. Efforts have also been launched to quantify such impacts more accurately. Life cycle energy consumption and carbon footprint {{are among the most}} frequently adopted and investigated environmental performance metrics. As efforts continue to incorporate environmental sustainability into product design, struggles persist in <b>concurrent</b> <b>consideration</b> of environmental impacts resulting from the manufacturing processes and supply chain network design. Thus, the objective of this research is to present a framework for reducing product cradle-to-gate energy consumption and carbon footprint through simultaneous consideration of manufacturing processes and supply chain activities. The framework developed in this thesis relies on unit process modeling, and is demonstrated for production of a bicycle pedal. It is shown that simultaneous consideration of manufacturing and supply chain processes can impact decision-making and improve product environmental sustainability at the design stage. The work presented contributes to the state of the science in sustainable design and manufacturing research. In addition, a point of departure is established for the research community to move current efforts forward for <b>concurrent</b> <b>consideration</b> of multiple stages of the product life cycle in pursuit of environmental, economic, and social sustainability...|$|E
40|$|In this report, {{the authors}} {{describe}} a detailed research {{model for the}} railway component of intermodal trucking operations, using discrete event system simulation. Emphasis is placed on strategic issues including railhead location analysis in multi-facility settings and product mix analysis (container versus trailer) by railhead within rail networks. The research models developed herein focus {{on the effects of}} railhead location and mix on drayage efficiency relative to shipment density profiles provided by BNSF Railway in the Chicago, IL area. The research advances {{the state of the art}} in intermodal simulation modeling through <b>concurrent</b> <b>consideration</b> of multiple-terminal network design and termina...|$|E
40|$|Submitted manuscripts {{should be}} {{original}} piece of work. And {{it should not}} be under <b>concurrent</b> <b>considerations</b> or published by any other publishers. All new submissions should be submitted only electrically and all manuscripts should adhere to the style of TIARJ. And accepted papers must follow the stipulations of TIARJ. Authors may submit the articles only to using option Submit Paper or by direct mail on editortiarj@gmail. com. Submitted manuscripts should be typed in single space (Font Size: 10 Times News Romans). Divide your article into clearly defined and numbered sections. Sub sections should be numbered 1. 1 (then 1. 1. 1, 1. 1. 2, …), 1. 2, etc. (the abstract is not included in section numbering). Use this numbering also for internal cross-referencing. Do not just refer to “the text”. Any sub section may be given a brief heading. Each heading should appear on its own separate line. Provide Address, Phone No., E-mail address and Fax No. All the correspondence is addressed to Editor-in-Chief. A concise and factual abstract is required. The abstract should state briefly the purpose of the research, the foremost results and major conclusions. Each article should include an abstract of maximum 175 words. References should be avoided, but if essential, then cite the author(s) an...|$|R
40|$|The {{product of}} ratios that equals 1 in Ceva's Theorem is {{analyzed}} {{in the case}} of non-concurrent Cevians, for triangles as well as arbitrary convex polygons. A general lemma on complementary systems of inequalities is proved, and used to classify the possible cases of non-concurrent Cevians. In the <b>concurrent</b> case, particular <b>consideration</b> is given to the Brocard configuration defined by equal angles between Cevians and polygon sides. 1...|$|R
40|$|We {{believe that}} the {{efficiency}} and effectiveness of human designers can be improved by making available tools {{that can be used}} to help negotiate solutions to open or unstructured parts of the process of designing. We assert that the efficiency and effectiveness of a designer can be increased by increasing the speed with which the design iteration is accomplished and reducing of the number of iterations. An increment in the iteration speed can be achieved if at least some parts of a design process are known and can be modelled on a computer. One way of reducing the number of iterations in design is by avoiding this corrective redesign. This provides the stimulus for developing approaches to design that include <b>Concurrent</b> Engineering <b>considerations.</b> Thus, in our opinion, a necessary ingredient in increasing efficiency and effectiveness of human designers is the modeling of design processes in a manner that they can be analyzed, manipulated and implemented. This is the central theme of our paper...|$|R
40|$|Previous {{research}} on port efficiency focuses {{primarily on the}} provider’s perspective and assumes that maximizing the output is always desirable. This paper recognizes that maximizing the final output does not necessarily guarantee an efficient system {{and the notion of}} port efficiency and service effectiveness needs to be considered from the perspectives of both the provider and the consumer of the port service. The paper proposes a network-DEA model to evaluate the performances of 30 seaports worldwide. The <b>concurrent</b> <b>consideration</b> of efficiency scores from the network-DEA model and the traditional DEA-CCR model will offer valuable insights to port operators on how to improve port performances as part of a seaborne cargo supply chain...|$|E
40|$|Optimization of a microelectronic {{system is}} a {{difficult}} task involving {{a number of different}} disciplines. Often, an optimization in one discipline will result in a sub-optimal solution in other areas and the overall system. This paper looks into the optimization of a microelectronics system by <b>concurrent</b> <b>consideration</b> of the micro-architecture, package, and logic partitioning. This approach will attempt to identify an optimized design by helping the designer to explore the multi-dimensional solution space and evaluate the design candidates based on their system-level cost/performance. As a demonstration vehicle, we have evaluated the SUN MicroSparc CPU for possible MCM packaging based on sets of smaller dies using this approach. Cost/performance figure-of-merits are presented for various cache sizes using cost-optimized partitioning for flip-chip MCM-D packaging. ...|$|E
40|$|Copyright © 2013 Ying-Chih Lin et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Recent progress in high-throughput instrumentations {{has led to an}} astonishing growth in both volume and complexity of biomedical data collected from various sources. The planet-size data brings serious challenges to the storage and computing technologies. Cloud computing is an alternative to crack the nut because it gives <b>concurrent</b> <b>consideration</b> to enable storage and high-performance computing on large-scale data. Thiswork briefly introduces the data intensive computing systemand summarizes existing cloud-based resources in bioinformatics. These developments and applicationswould facilitate biomedical research tomake the vast amount of diversification data meaningful and usable. 1...|$|E
40|$|This study {{traces the}} {{sequence}} of occupation {{of the land of}} the Upper San Pedro River Valley from its beginnings with the discovery and exploitation of its mineral resources through the exploitation of the free open range to eventual necessity to own or lease all grazing lands used. It concludes with a description of the development of small tract suburban residential occupation of significant magnitude since establishment of a payroll source at Fort Huachuca in 1954 and later the growth of popularity for suburban retirement living in Arizona. <b>Concurrent</b> with <b>consideration</b> of {{the sequence of}} occupation in the area, the consequences of denudation of the mountain woodlands for fuel prior to 1900 and the disturbance of vegetation and soil of the grasslands due to continuous uncontrolled overgrazing until 1930 is described and discussed. The study concludes that the changes to the landscape of the area since 1870 were directly due to the occupation and use of the land. Denudation of the mountain woodlands and severe disturbance of the grasslands caused significant change to the drainage and moisture content of the soil leading to an invasion of desert shrub into what was once a grass dominated landscape...|$|R
40|$|Image {{transcoding}} proxies {{are used}} to improve Web browsing over low bandwidth networks by adapting content-rich web images to bandwidth-constrained clients. Such transcoding proxies dynamically analyze, manipulate and transcode images (e. g. quality reduction, down sampling) on the fly enabling significant reductions in download times over low bandwidth links. However, transcoding proxies have scalability problems if the objective policy that decides whether to transcode an image does not take client load (e. g. number of <b>concurrent</b> clients) into <b>consideration.</b> We show that seemingly intuitive policies that decide solely based on whether transcoding yields savings in transmission time fail to scale. Transcoding performance can be improved {{by a factor of}} two in terms of average Web page latency as perceived by a client, by taking overall client load into consideration and by properly scheduling transcoding operations on a single CPU. We also show that an Earliest Deadline First Based (EDF) scheduling policy further improves transcoding performance...|$|R
40|$|We {{consider}} {{the use of}} search trees to represent the dictionary aspects of a main-memory database in a <b>concurrent</b> environment. Efficiency <b>considerations</b> require that the trees be balanced and that operations on a search tree should not block too large {{a part of the}} tree for too long a time. These two requirements conflict, since rebalancing a tree after an update can, in a straightforward implementation, block too large a part of the tree. We would prefer that au search operations reserve only a small, fixed-size part of a tree at all times. We propose a new, elegant solution for this problem based on the notion of relaxed AVL trees and the decoupling of updates and rebalancing. The main advantage of our solution is that the implementation of the dictionary operations in a concurrent environment is as simple as their implementation in a sequential environment, whereas previous concurrent solutions are more descriptively complex...|$|R
40|$|This paper {{presents}} a design methodology for a high-performance, programmable video signal processor (VSP). The proposed design methodology explores both technology-driven hardware tradeoffs and application-driven architectural tradeoffs for optimizing cost and performance within {{a class of}} processor architectures. In particular, this methodology allows <b>concurrent</b> <b>consideration</b> of these competing factors {{at different levels of}} design sophistication, ranging from early design exploration towards full processor simulation. We present the results of this methodology for an aggressive VeryLong -Instruction-Word (VLIW) video signal processor design and discuss its utility for other programmable signal processor designs. Keywords: processor design, video signal processing, multimedia architecture, VLIW, video compression 1. INTRODUCTION The extraordinary growth of the multimedia industry has generated considerable demand for digital video in today's applications. General-purpose processor [...] ...|$|E
40|$|This paper {{utilizes}} the Pacific Northwest National Laboratory?s Integrated Assessment modeling {{tools to}} draw out concepts {{that should be considered}} when examining purpose-grown biomass as a low-emissions energy source and/or as a key technology for addressing climate change. The paper concludes that using biomass as a significant element of our future energy system will be an enormous undertaking that will transform the global energy and agricultural system. Further, large-scale biomass energy requires substantial advances in the basic science of plant design, an integrated approach to basic and applied research, <b>concurrent</b> <b>consideration</b> of ethical and economic issues, effective planning for market transition, and reliable monitoring systems. Biomass energy is a straightforward concept but a complex endeavor necessitating a coordinated, programmatic effort...|$|E
40|$|The correct {{performance}} assessment and characterization of Multiple Input Multiple Output (MIMO) systems requires the <b>concurrent</b> <b>consideration</b> of antennas and the propagation environment {{to a higher}} degree than ever before. In this paper we describe a measurement set-up and a measurement campaign aiming at the derivation of {{certain aspects of the}} propagation environment. In contrast to the majority of published methods, the described set-up is based on broadband sweeps using a Vector Network Analyzer (VNA) and long, low loss cables to enable magnitude and phase measurements. By construction, the in-home and outdoor–to-indoor environments are of primary focus for this paper. Preliminary measurements in an industrial office building confirm previously published cluster models and extend those to include non-horizontal plane directions of arrival and departure...|$|E
40|$|We {{consider}} {{the use of}} search trees to represent the dictionary aspects of a main-memory database in a <b>concurrent</b> environment. Efficiency <b>considerations</b> require that the trees be balanced and that operations on a search tree should not block too large {{a part of the}} tree for too long a time. We propose a new, elegant solution for this problem based on the notion of relaxed AVL trees and the decoupling of updates and rebalancing. The main advantage of our solution is that the implementation of the dictionary operations in a concurrent environment is as simple as their implementation in a sequential environment, whereas previous concurrent solutions are more descriptively complex. x This research was supported by grants from the Natural Sciences and Engineering Research Council of Canada, from the Information Technology Research Centre of Ontario, and from the Research Grants Committee of Hong Kong, and partially by the Academy of Finland. The Hong Kong University of Science [...] ...|$|R
40|$|Linearizability {{has become}} the key {{correctness}} criterion for concurrent data structures, ensuring that histories of the <b>concurrent</b> object under <b>consideration</b> are consistent, where con-sistency is judged {{with respect to a}} sequential history of a corresponding abstract data struc-ture. Linearizability allows any order of concurrent (i. e., overlapping) calls to operations to be picked, but requires the real-time order of non-overlapping to be preserved. A history of overlapping operation calls is linearizable if {{at least one of the}} possible order of operations forms a valid sequential history (i. e., corresponds to a valid sequential execution of the data structure), and a concurrent data structure is linearizable iff every history of the data structure is linearizable. Over the years numerous techniques for verifying linearizability have been de-veloped, using a variety of formal foundations such as refinement, shape analysis, reduction, etc. However, as the underlying framework, nomenclature and terminology for each method differs, it has become difficult for practitioners to judge the differences between each approach, and hence, judge the methodology most appropriate for the data structure at hand. We com-pare the major of methods used to verify linearizability, describe the main contribution of each method, and compare their advantages and limitations. ...|$|R
40|$|State space {{analysis}} {{is one of}} the main approaches to model-based verification of concurrent systems and {{is one of the}} most successfully applied analysis methods for Coloured Petri Nets (CP-nets or CPNs) [13, 16, 17]. The basic idea of state space exploration and {{analysis is}} to compute all reachable states and state changes of the <b>concurrent</b> system under <b>consideration</b> and represent these as a directed graph. From a constructed state space it is possible to verify and analyse a large class of behavioural properties by considering, e. g., the standard behavioural properties of CP-nets, traverse a constructed state space by means of user-defined queries, or conduct LTL and CTL model checking [4, 10]. Another main advantage of state space analysis is that it can be supported by computer tools in a highly automatic way, and it can provide counter examples demonstrating why the system does not have a certain property. The main limitation of using state spaces to verify behavioural properties of systems is the state explosion problem [24], i. e., that state spaces of systems may have an astronomical number of reachable states which means that they are too large to be handled with the available computing power (CPU speed and memory). Methods for alleviating this inheren...|$|R
40|$|This paper {{emphasizes}} <b>concurrent</b> <b>consideration</b> of the partitioning of a microelectronic {{circuit design}} into multiple dies and {{the selection of}} the appropriate packaging technology for implementation of the entire system. Partitioning a large design into a multichip package is a non-trivial task. Similarly, selection of the MCM packaging technology to accommodate a multichip solution can also be puzzling. The interdependencies of these two problems a ord the opportunity to achieve a global optimum when considered concurrently. In this paper we address the partitioning/MCM technology tradeo, their interdependency, and previous work in this area. The SUN MicroSparc CPU is used as a demonstration vehicle and is partitioned for di erent MCM technologies. The preliminary results show that the optimum number of partitions and contents of each partition depend heavily on the choice of MCM technologies for a given application. ...|$|E
40|$|The {{design process}} can be {{considered}} as a series of design phases. Each design phase can be further divided into number of design activities. In the network of design activities, the design activities generate information which is used by the succeeding activities. Since importance of the input information available from different activities changes with respect to the requirement of the design activities, a pull system approach for the management of design activities is proposed. Due to the <b>concurrent</b> <b>consideration</b> of many factors in the design process, it is required that the design activities are performed by teams of experts. In addition to the multiple resource requirement, the uncertainty in the weights and duration of design activities makes the problem of scheduling design activities complex. A heuristic algorithm is proposed to solve the problem considered in this paper...|$|E
40|$|Successful {{modelling}} {{and design}} of industrial products require excellent communication within a multifunctional team, working in a concurrent engineering environment. Lack of a common vocabulary expressing design decisions leads to bad comprehension and insight. Through the use of patterns {{we are able to}} describe design decisions consisting of composition of objects and provide an extension of the existing design vocabulary supporting better human communication with leading to a base for design automatisation. I. INTRODUCTION Concurrent Engineering (CE) has been recognised as a viable approach which consists of the simultaneous design of a product and the consideration of all its related processes and requirements [1]. Simultaneous design leads to parallel activities of the systems life-cycle which is performed sequently in the classic systems engineering approach [2]. Parallel design activities contribute to launch a product in less time. <b>Concurrent</b> <b>consideration</b> of all constraints of [...] ...|$|E
40|$|The {{intention}} of this chapter is to align {{the perspectives of}} planning and assessment. It draws on the current debate about what drives planning and whether assessment informs, or is the endpoint, of planning. The chapter acknowledges that planning and assessment occur together and require <b>concurrent</b> attention and <b>consideration.</b> Learning how to plan has long been recognised as a foundation skill in teaching. Recent research indicates that teachers learn about teaching through planning, and in tum learn about planning through their teaching (Mutton et al., 2011). For you, as a pre-service teacher, lesson planning will involve making decisions about strategies for teaching and learning, questioning and assessment of, as, and for, learning. While {{much has been written}} about different approaches to teaching, practitioners in the classroom have some common intentions. The first is an acknowledged focus on learningwhere students are active participants in the learning journey. The second is the need to encourage students to think- learning is not meant to occur within a two-dimensional 2 ̆ 7 serve it on a platter 2 ̆ 7 mode, but requires active cognitive involvement or engagement. The third intention is to engage students in activities. The underpinning of action and a focus on doing can be aligned with experiential learning- it is where students are encouraged to link their learning; to give it meaning and relevance. Context is also important, and creating a positive learning environment is not only conducive to effective learning, but also vital in developing self-efficacy and self-esteem. Ultimately, teachers want to help their students learn how to learn. This is integral to their role, as is their function as facilitator of learning...|$|R
40|$|Occupational {{hazards and}} obesity {{can lead to}} {{extensive}} morbidity and mortality and put great financial burden on society. Historically, occupational hazards and obesity have been addressed as separate unrelated issues, but both are public health problems {{and there may be}} public health benefits from considering them together. This paper provides a framework for the <b>concurrent</b> <b>consideration</b> of occupational hazards and obesity. The framework consists of the following elements: (i) investigate the relationship between occupational hazards and obesity, (ii) explore the impact of occupational morbidity and mortality and obesity on workplace absence, disability, productivity and healthcare costs, (iii) assess the utility of the workplace as a venue for obesity prevention programs, (iv) promote a comprehensive approach to worker health and (v) identify and address the ethical, legal and social issues. Utilizing this framework may advance the efforts to address the major societal health problems of occupational hazards and obesity...|$|E
40|$|Enhanced {{understanding}} of the role of flood plains in dictating open-channel flow is essential for environmental and ecological management of fluvial systems. This paper presents the field measurements and primary analysis of flood flow across a compound channel in the River Severn, England. A three-dimensional acoustic Doppler velocimeter in combination with a directional current meter is deployed to measure the velocity. The statistical flow structure is compared with existing analytical formulations derived for single channel flows. The existence of a vertically double-layer structure around the interface between the main channel and the flood plain is demonstrated, indicating (a) a vertical shear-dominated flow zone near the bed; and (b) away from the bed a transverse shear-dominated flow zone with enhanced turbulent mixing. This feature necessitates <b>concurrent</b> <b>consideration</b> of both the transverse and the vertical resolution in mathematical modelling. The measured data can be utilized to assess the performance of mathematical river models...|$|E
40|$|In {{order to}} teach science well, science {{teachers}} {{need to know what}} to focus on in order to ensure their assessment of student learning is meaningful and useful for the students’ on going learning and development. The diversity and range of content and skills within the subject of science mean that the assessment capabilities required by science teachers are wide ranging and complex, requiring specialist knowledge and skills in the assessment of science learning as part of the teachers’ pedagogical content knowledge (PCK). Based on {{a review of the literature}} this paper proposes a framework for quality assessment in science which focuses on five areas: teaching, students, evidence of learning, future decision-making and impact. This paper advocates a <b>concurrent</b> <b>consideration</b> of all five areas of the framework to provide a substantial, rich, broad, rigorous quality assessment approach on which teachers and students can base teaching and learning...|$|E
30|$|In recent years, {{active and}} {{semi-active}} mounts {{have become a}} hot topic in the research on powertrain mounting systems. Fan et al. [18] analyzed the configuration of a new semi-active hydraulic mount with a variable-stiffness decoupling membrane and tested its dynamic characteristics. Chen et al. [19] researched the dynamic model and experimental testing of magnetorheological fluid mounts, especially at a wide frequency. Ladipo et al. [20] presented the simulation of magnetorheological elastomers (MREs) as engine mounts. A four-parameter model was used to model the MRE mounts and the performance was {{compared with those of}} passive or rubber mounts. Zheng et al. [21, 22] used the aforementioned model to control the engine mount system with <b>concurrent</b> <b>consideration</b> of random road input and engine excitation. Pan et al. [23] designed the fuzzy-proportional-integral- derivative switching control strategy for the magnetorheological semi-active mounting system. Farjoud et al. [24] developed a detailed mathematical model of semi-active magnetorheological engine and transmission mounts using multi-physics modeling techniques for physical systems with various energy domains.|$|E
40|$|Interest in {{accounting}} for {{environmental impacts of}} products, processes, and systems during the design phase is increasing. Numerous studies have undertaken investigations for reducing environmental impacts across the product life cycle. Efforts have also been launched to quantify such impacts more accurately. Life cycle energy consumption and carbon footprint {{are among the most}} frequently adopted and investigated environmental performance metrics. As efforts continue to incorporate environmental sustainability into product design, struggles persist in <b>concurrent</b> <b>consideration</b> of environmental impacts resulting from the manufacturing processes and supply chain network design. Thus, the objective of this research is to present a framework for reducing product cradle-to-gate energy consumption and carbon footprint through simultaneous consideration of manufacturing processes and supply chain activities. The framework developed in this thesis relies on unit process modeling, and is demonstrated for production of a bicycle pedal. It is shown that simultaneous consideration of manufacturing and supply chain processes can impact decision-making and improve product environmental sustainability at the design stage...|$|E
40|$|The present paper {{investigated}} {{the implementation of}} environmentally responsible IT measures in ten American and nine European organizations. The environmentally responsible IT measures include the implementation of existing hardware and software technologies as well as organizational and managerial actions that aim to reduce environmental impacts of IT use. As a result, adoption of Green IT measures requires adjustments that may modify the physical IT infrastructure and various organizational processes. These adjustments to organizational processes {{in addition to the}} infrastructure are ‘techno-economic’ in nature. The term techno-economic denotes <b>concurrent</b> <b>consideration</b> of technological, social, and economic issues surrounding an innovation. We interviewed at least two executives in each of the organizations in our study to assess the extent of Green IT implementation. Based on the analysis of these interviews, we conclude that the techno-economic adjustments were necessary for widespread implementation of environmentally responsible IT measuresGreen IT, Data Center Management, Energy Efficiency, Environmental Responsibility, Organizational Development, Corporate Strategy. ...|$|E
40|$|A {{fundamental}} {{step towards}} broadening {{the use of}} real world image-based visual servoing is {{to deal with the}} important issues of reliability and robustness. In order to address this issue, a closed loop control law is proposed that simultaneously accomplishes a visual servoing task and is robust to a general class of external errors. This generality allows <b>concurrent</b> <b>consideration</b> {{of a wide range of}} errors including: noise from image feature extraction, small scale errors in the tracking and even large scale errors in the matching between current and desired features. This is achieved with the application of widely accepted statistical techniques of robust M-estimation. The M-estimator is integrated by an iteratively re-weighted method. The Median Absolute Deviation is used as an estimate of the standard deviation of the inlier data and is compared with other methods. This combination is advantageous because of its high efficiency, high breakdown point and desirable influence functions. The robustness and stability of the control law is shown to be dependent on a subsequent measure of position uncertainty. Furthermore the convergence criteria of the control law are investigated. Experimental results are presented which demonstrate visual servoing tasks which resist severe outlier contamination...|$|E
40|$|Computational {{protein design}} (CPD) is a {{powerful}} technique to engineer existing proteins or to design novel ones that display desired properties. Rosetta is a software suite including algorithms for computational modeling and analysis of protein structures and offers many elaborate protocols created to solve highly specific tasks of protein engineering. Most of Rosetta's protocols optimize sequences based on a single conformation (i. e. design state). However, challenging CPD objectives like multi-specificity design or the <b>concurrent</b> <b>consideration</b> {{of positive and negative}} design goals demand the simultaneous assessment of multiple states. This is why we have developed the multi-state framework MSF that facilitates the implementation of Rosetta's single-state protocols in a multi-state environment and made available two frequently used protocols. Utilizing MSF, we demonstrated for one of these protocols that multi-state design yields a 15 % higher performance than single-state design on a ligand-binding benchmark consisting of structural conformations. With this protocol, we designed de novo nine retro-aldolases on a conformational ensemble deduced from a (βα) 8 -barrel protein. All variants displayed measurable catalytic activity, testifying to a high success rate for this concept of multi-state enzyme design...|$|E
40|$|This {{manuscript}} is original, has {{not been}} previously published and is not under <b>concurrent</b> <b>consideration</b> elsewhere. Objective: A wide range of behavioural and psychological symptoms (BPSD) are common in dementia, {{and it has been}} suggested that groups of correlated symptoms should be studied together. Here, we describe the groups of BPSD that have been identified in the literature and how they have been used to study associations, burden, treatment and underlying biology. Methods: The literature database PubMed was searched for articles that identified clusters or factors of BPSD or used previously defined symptom groups. Results: Sixty-two studies were included. Generally, the following symptom groups were suggested: affective symptoms, including depression and anxiety; psychosis, including delusions and hallucinations; hyperactivity, including irritability and aggression; and euphoria. Symptoms that did not show consistent results include apathy, eating disturbances, night-time behaviour disturbances, disinhibition and aberrant motor behaviour. Symptom groups differed in their associations, treatment and biology. Conclusions: Studies investigating symptomgroups show relatively consistent results. Studying symptomgroups allows similar symptoms to be studied together, which might strengthen results and may point to differences in their aetiology and treatment. However, a large amount of the individual variability of the symptoms could no...|$|E
40|$|Abstract: 2 ̆ 2 By {{concurrent}} design we mean, in part, <b>concurrent</b> <b>consideration</b> {{of a broad}} range of life-cycle constraints concerning, for example manufacturing and maintenance. The multitude of constraints arising from these considerations make it difficult to identify satisfactory designs. An alternative to explicitly considering all constraints is to determine which of the constraints are relevant, redundant or inconsistent and to consider only those which impact design decisions. The proposed approach is based on two simple ideas: (1) Constraints provide a uniform representation for a variety of life-cycle concerns, and (2) Interval methods applied to constraints can be used to identify critical constraints, eliminate redundant constraints and to narrow the space of design alternatives. The application of the necessary and sufficient intervals of contraints and constraint propagation techniques are used to classify constraints in this way and to focus design activity. Regional monotinicity properties are used to identify critical constraints. A related aspect of {{concurrent design}} problems is the large number of complex constraints which have to be satisfied to complete a design task. As it is impossible to guarantee the simultaneous solution of a large set of design constraints, we have investigated algorithms for planning and simplifying such constraint problems. 2 ̆...|$|E
40|$|We {{describe}} the integrated development of PowerDOE, {{a new version}} of the DOE- 2 building energy analysis program, and the Building Design Advisor (BDA), a multimedia-based design tool that assists building designers with the <b>concurrent</b> <b>consideration</b> of multiple design solutions with respect to multiple design criteria. PowerDOE has a windows-based Graphical User Interface (GUI) that makes it easier to use than DOE- 2, while retaining DOE- 2 `s calculation power and accuracy. BDA, with a similar GUI, is designed to link to multiple analytical models and databases. In its first release it is linked to PowerDOE and a Daylighting Analysis Module, as well as to a Case Studies Database and a Schematic Graphic Editor. These allow building designers to set performance goals and address key building envelope parameters from the initial, schematic phases of building design to the detailed specification of building components and systems required by PowerDOE. The consideration of the thermal performance of building envelopes through PowerDOE and BDA is integrated with non-thermal envelope performance aspects, such as daylighting, as well as with the performance of non-envelope building components and systems, such as electric lighting and HVAC. Future versions of BDA will support links to CAD and electronic product catalogs, as well as provide context-dependent design advice to improve performance...|$|E
40|$|This thesis {{examined}} the developmental aspects of auditory stimulus processing {{in children and}} adolescents with Attention-deficit/Hyperactivity Disorder (AD/HD) and age-matched control children via event-related potentials (ERPS). A simple auditory oddball task was used for each study. In addition to four studies addressing ERP data in the time-domain, which extended previous research via consideration of (i) topographic analyses, (ii) components to standard stimuli, and (iii) two subtypes of AD/HD as defined by DSM- 1 Y, an additional two studies examined similar considerations in the time-frequency domain. Main results indicated that (a) in healthy children and adolescents the activation and timing of some cognitive processes related to auditory stimulus processing were age-dependent, while others showed age-independence, (b) some topographic differences from controls were common to both AD/HD subtypes, while others were unique to a particular subtype, and these group differences showed different age effects, (c) event-related low-frequency activity contributes importantly the differences in component amplitude and morphology between sites observed in raw ERPs, and (d) the event-related low-frequency activity of the two AD/HD subtypes differed from controls in entirely unique ways, while the majority of the clinical versus control group-differences evident in components from a residual ERP (i. e. the raw ERP minus the low-frequency activity) were similar between the subtype groups. Overall, the results suggest that increased diagnostic specificity would be achieved with <b>concurrent</b> <b>consideration</b> of the low and high frequency components of the ERP...|$|E
40|$|Mitigation of {{displacement}} and acceleration responses of a three story benchmark structure excited by seismic motions is pursued in this study. Multiple 20 -kN magnetorheological (MR) dampers are {{installed in the}} three-story benchmark structure and managed by a global fuzzy logic controller to provide smart damping forces to the benchmark structure. Two configurations of MR damper locations are considered to display multiple-input, single-output and multiple-input, multiple-output control capabilities. Characterization tests of each MR damper are performed in a laboratory to enable the formulation of fuzzy inference models. Prediction of MR damper forces by the fuzzy models shows sufficient agreement with experimental results. A controlled-elitist multi-objective genetic algorithm is utilized to optimize a set of fuzzy logic controllers with <b>concurrent</b> <b>consideration</b> to four structural response metrics. The genetic algorithm is able to identify optimal passive cases for MR damper operation, and then further improve their performance by intelligently modulating the command voltage for concurrent reductions {{of displacement}} and acceleration responses. An optimal controller is identified and validated through numerical simulation and fullscale experimentation. Numerical and experimental results show that performance of the controller algorithm is superior to optimal passive cases in 43 % of investigated studies. Furthermore, the state-space model of the benchmark structure that is used in numerical simulations has been improved by {{a modified version of}} the same genetic algorithm used in development of fuzzy logic controllers. Experimental validation shows that the state-space model optimized by the genetic algorithm provides accurate prediction of response of the benchmark structure to base excitation...|$|E
40|$|Genetic polymorphisms of apolipoprotein E (APOE) and brain-derived {{neurotrophic}} factor (BDNF) have shown inconsistent associations with healthy adult cognitive functions. Recent investigations have suggested that APOE polymorphisms do not contribute to non-pathological cognitive function and that any effect is likely due to prodromal Alzheimer's disease (AD). Similarly, although BDNF Val 66 Met polymorphisms affect hippocampal morphology and function, associations with learning and/or memory {{have not always been}} found. This study sought to determine whether APOE and BDNF polymorphisms were associated, either independently or in combination, with adult cognition. Comprehensive neuropsychological assessments were conducted on 433 older adults, aged 50 – 79 years (M = 62. 16, SD = 6. 81), which yielded measures of episodic memory, working memory, executive function, and language processing. Participants underwent comprehensive neuropsychological assessment to ensure that only cognitively intact individuals comprised the sample. APOE and BDNF polymorphic data were used as predictors in general linear models that assessed composite cognitive domain variables, while covarying for education and age. Although no main effects for APOE or BDNF were found, the analysis identified a significant APOE × BDNF interaction that predicted episodic memory performance (p =. 02, η 2 =. 02). Post-hoc analyses demonstrated that in BDNF Val homozygotes, the cognitive consequences of APOE polymorphisms were minimal. However, in BDNF Met carriers, the hypothesized beneficial/detrimental effects of APOE polymorphisms were found. Our data show that <b>concurrent</b> <b>consideration</b> of both APOE and BDNF polymorphisms are required in order to witness a cognitive effect in healthy older adults...|$|E
