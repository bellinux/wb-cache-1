10|10000|Public
5000|$|When {{the auditor}} {{is unable to}} obtain audit {{evidence}} regarding particular account balance, <b>class</b> <b>of</b> <b>transaction</b> or disclosure {{that does not have}} pervasive effect on the financial statements.|$|E
5000|$|When the {{financial}} statements are materially misstated due to misstatement in one particular account balance, <b>class</b> <b>of</b> <b>transaction</b> or disclosure {{that does not have}} pervasive effect on {{the financial}} statements.|$|E
50|$|For example, an auditor may: {{physically}} examine inventory {{as evidence}} that inventory shown in the accounting records actually exists (existence assertion);inspect supporting documents like invoices to confirm that sales did occur (occurrence); arrange for suppliers to confirm in writing {{the details of the}} amount owing at balance date {{as evidence that}} accounts payable is a liability (rights and obligation assertion); and make inquires of management about the collectibility of customers' accounts as evidence that trade debtors are accurate as to its valuation. Evidence that an account balance or <b>class</b> <b>of</b> <b>transaction</b> is not complete, valid or accurate is evidence of a substantive misstatement but only becomes a material misstatement when it is large enough that it can be expected to influence the decisions of the users of the financial statement.|$|E
5000|$|Substantive {{analytical}} procedures {{are used to}} obtain evidential matter about particular assertions related to account balances or <b>classes</b> <b>of</b> <b>transactions.</b>|$|R
5000|$|Assertions used by {{the auditor}} fall into the {{following}} categories:(a) Assertions about <b>classes</b> <b>of</b> <b>transactions</b> and events for the period ended: ...|$|R
50|$|Financial Reinsurance (or fin re), {{is a form}} of {{reinsurance}} {{which is}} focused more on capital management than on risk transfer. In the non-life segment of the insurance industry this <b>class</b> <b>of</b> <b>transactions</b> {{is often referred to as}} finite reinsurance.|$|R
30|$|The second <b>class</b> <b>of</b> <b>transaction</b> risk is the ‘opportunism risks’, i.e. {{the risk}} which arises when another {{contracting}} party, with monopsonistic or monopolistic {{control over a}} complementary investment or service, removes, or threatens to remove, it from the supply chain after a player has made an investment that depends upon it. The {{findings of this study}} have shown that the bone and tallow value chains failed to grow and modernise due to limited options for business financing, and on several occasions, production had to be suspended until payments were received from distributors.|$|E
40|$|AbstractThe {{frequent}} connected subgraph mining problem, i. e., {{the problem}} of listing all connected graphs that are subgraph isomorphic to at least {{a certain number of}} transaction graphs of a database, cannot be solved in output polynomial time in the general case. If, however, the transaction graphs are restricted to forests then the problem becomes tractable. In this paper we generalize the positive result on forests to graphs of bounded tree-width. In particular, we show that for this <b>class</b> <b>of</b> <b>transaction</b> graphs, frequent connected subgraphs can be listed in incremental polynomial time. Since subgraph isomorphism remains NP-complete for bounded tree-width graphs, the positive complexity result of this paper shows that efficient frequent pattern mining is possible even for computationally hard pattern matching operators...|$|E
40|$|This article {{offers a}} {{critique}} and extension of recent work by Bruce Kogut and Udo Zander. Kogut and Zander are correct {{to argue that}} opportunism is unnecessary to explain {{the existence of the}} multinational, but wrong to infer that this indicates an absence of market failure. They have identified an important <b>class</b> <b>of</b> <b>transaction</b> costs and market failure which arises in the absence of opportunism, but do not cat doubt on the market failure approach to the theory of the multinational. Their argument that they have identified an important source of ‘ownership’ advantage is unconvincing and is not supported by the empirical evidence. However, Kogut and Zander's work is of considerable significance, but in areas different from those that they highlight. © 1995 JIBS. Journal of International Business Studies (1995) 26, 399 – 40...|$|E
50|$|Management implicitly {{assert that}} account {{balances}} and underlying <b>classes</b> <b>of</b> <b>transaction</b> {{do not contain}} any material misstatements: in other words, that they are materially complete, valid and accurate. Auditors gather evidence about these assertions by undertaking activities referred to as substantive procedures.|$|R
40|$|Evidence from {{research}} in psychology and auditor judgment {{has shown that}} perceptions that form early in a sequential judgment process can influence subsequent judgments. Auditing Standard 12 requires auditors to identify fraud risk factors and assess the risk of fraud {{as part of the}} process of assessing overall misstatement risk. While it is expected that fraud risk assessments should have a bearing on overall risk assessments, it is possible that perceptions formed from assessments of fraud risk can negatively affect the evaluation of any evidence reviewed thereafter. Because different <b>classes</b> <b>of</b> <b>transactions</b> may be affected by fraud risk factors in different ways, fraud risk assessments may differ across <b>classes</b> <b>of</b> <b>transactions.</b> These differences may make subsequent auditor judgments susceptible to the contrast effects bias, where subjects overreact to the differences such that the fraud risk assessments influence auditor judgment more than they should. This study examines whether auditors who learn that fraud risk is low for one <b>class</b> <b>of</b> <b>transactions</b> immediately after examining a <b>class</b> <b>of</b> <b>transactions</b> that has high fraud risk, can overreact to the contrast such that they reduce their sensitivity to evidence that suggests increased misstatement risk. The study also examines whether these contrast effects can be mitigated by acquiring information about fraud risk assessments later in the sequence of evidence, after auditors have reviewed and assimilated evidence related to other risks. The study finds that, as predicted, auditor judgments are influenced by contrast effects. Auditors who examined <b>classes</b> <b>of</b> accounts for which fraud risk assessments were different were less sensitive to evidence suggesting increased risk in accounts that had been identified as having low fraud risk. However, contrary to predictions, these contrast effects were not mitigated by evidence order...|$|R
50|$|It is {{stated in}} ISA 315 (paragraph A.124) that the auditor should use assertions for <b>classes</b> <b>of</b> <b>transactions,</b> account balances, and {{presentation}} and disclosures in sufficient detail {{to form a}} basis for the assessment ofrisks of material misstatement and the design and performance of further audit procedures.|$|R
40|$|Acceptance rate = 104 / 521 = 20 %The {{frequent}} connected subgraph mining problem, i. e., {{the problem}} of listing all connected graphs that are subgraph isomorphic to at least a certain specified number of elements of a database of transaction graphs, cannot be solved in output polynomial time in the general case. If, however, the transaction graphs are restricted to trees then the problem becomes tractable. In this paper, we generalize the positive complexity result on trees to graphs of bounded treewidth. In particular, we show that for this <b>class</b> <b>of</b> <b>transaction</b> graphs, frequent connected subgraphs can be listed with incremental polynomial delay. Since subgraph isomorphism remains NP-complete for bounded treewidth graphs, the positive complexity result of this paper implies that efficient frequent pattern mining is possible even for computationally hard pattern matching operators. status: publishe...|$|E
40|$|Transaction {{processing}} {{applications such}} as those exemplified by the TPC-C benchmark {{are among the most}} demanding I/O applications for conventional storage systems. Two complementary techniques exist to improve the performance of these systems. Eager-writing allows the free block that is closest to a disk head to be selected for servicing a write request, and mirroring allows the closest replica to be selected for servicing a read request. Applied individually, the e#ectiveness of each of these techniques is limited. An eager-writing disk array (EW-Array) combines these two complementary techniques. In such a system, eager-writing enables low-cost replica propagation so that the system can provide excellent performance for both read and write operations while maintaining a high degree of reliability. To fully realize the potential of an EW-Array, we must answer at least two key questions. First, since both eager-writing and mirroring rely on extra capacity to deliver performance improvements, how do we satisfy competing resource demands given a fixed amount of total disk space? Second, since eagerwriting allows data to be dynamically located, how do we exploit this high degree of location independence in an intelligent disk scheduler? In this paper, we address these two key questions and compare the resulting EW-Array prototype performance against that of conventional approaches. The experimental results demonstrate that the eager-writing disk array is an e#ective approach to providing scalable performance for an important <b>class</b> <b>of</b> <b>transaction</b> processing applications...|$|E
40|$|One of {{the recent}} trends in {{computer}} input is to utilize users' natural bimanual motor skills. This paper further explores {{the potential benefits of}} such two-handed input. We have observed that bimanual manipulation may bring two types of advantages to human-computer interaction: manual and cognitive. Manual benefits come from increased time-motion efficiency, due to the twice as many degrees of freedom simultaneously available to the user. Cognitive benefits arise as a result of reducing the load of mentally composing and visualizing the task at an unnaturally low level imposed by traditional unimanual techniques. Area sweeping was selected as our experimental task. It is representative of what one encounters, for example, when sweeping out the bounding box surrounding a set of objects in a graphics program. Such tasks can not be modelled by Fitts' Law alone (Fitts, 1954) and have not been previously studied in the literature. In our experiments, two bimanual techniques were compared with the conventional one-handed GUI approach. Both bimanual techniques employed the two-handed "stretchy" technique first demonstrated by Krueger (1983). We also incorporated the "Toolglass" technique introduced by Bier, Stone, Pier, Buxton and DeRose (1993). Overall, the bimanual techniques resulted in significantly faster performance than the status quo one-handed technique, and these benefits increased with the difficulty of mentally visualizing the task, supporting our bimanual cognitive advantage hypothesis. There {{was no significant difference between}} the two bimanual techniques. This study makes two types of contributions to the literature. First, practically we studied yet another <b>class</b> <b>of</b> <b>transaction</b> where significant benefits can be realized by applying bimanual techniques. Furth [...] ...|$|E
50|$|Certain <b>classes</b> <b>of</b> <b>transactions</b> are exempt, {{or may be}} {{exempted}} on application. For example, established customers transacting amounts {{typical of}} their lawful business, such as for payroll, or retail or vending machine takings, etc. Motor vehicle traders are specifically not eligible for exemption, as are boats, farm machinery and aircraft traders.|$|R
40|$|AbstractIt {{is often}} {{necessary}} to ensure that database transactions preserve integrity constraints that specify valid database states. While {{it is possible to}} monitor for violations of constraints at run-time, rolling back transactions when violations are detected, it is preferable to verify correctness statically,beforetransactions are executed. This can be accomplished if we can verify transaction safety with respect to a set of constraints by means of calculatingweakest preconditions. We study properties of weakest preconditions for a number <b>of</b> <b>transaction</b> and specification languages. We show that some simple transactions do not admit weakest preconditions over first-order logic and some of its extensions such as first-order logic with counting and monadicΣ 11. We also show that the <b>class</b> <b>of</b> <b>transactions</b> that admit weakest preconditions over first-order logic cannot be captured by any transaction language. We consider a strong local form of verifiability, and show that it is different from the general form. We define robustly verifiable transactions as those that can be statically analyzed regardless of extensions to the signature of the specification language, and we show that the <b>class</b> <b>of</b> robustly verifiable <b>transactions</b> over first-order logic is exactly the <b>class</b> <b>of</b> <b>transactions</b> that admit the local form of verifiability. We discuss the implications of these results for the design <b>of</b> verifiable <b>transaction</b> languages...|$|R
40|$|Relational update <b>transactions</b> {{consisting}} <b>of</b> line {{programs of}} inserts, deletes, and modifications are studied {{with respect to}} equivalence and simplification. A sound and complete set of axioms for proving transaction equivalence is exhibited. The axioms yield a set of simplification rules {{that can be used}} to optimize efficiently a large <b>class</b> <b>of</b> <b>transactions</b> <b>of</b> practical interest. The simplification rules are particularly well suited to a dynamic environment where transactions are presented in an on-line fashion, and where the time available for optimization may consist of arbitlcarily short and sparse intervals...|$|R
40|$|Transactional graph {{transformation}} systems (t-gtss) {{have been}} recently proposed as a mild {{extension of the}} standard dpo approach to graph transformation, equipping it with a suitable notion of atomic execution for computations. A typing mechanism induces a distinction between stable and unstable items, and a transaction {{is defined as a}} shift-equivalence <b>class</b> <b>of</b> computations such that the starting and ending states are stable and all the intermediate states are unstable. The paper introduces an equivalent, yet more manageable definition <b>of</b> <b>transaction</b> based on graph processes. This presentation is used to provide a universal characterisation for the <b>class</b> <b>of</b> <b>transactions</b> <b>of</b> a given t-gts. More specifically, we show that the functor mapping a t-gts to a graph transformation system having as productions exactly the <b>transactions</b> <b>of</b> the original t-gts is the right adjoint to an inclusion functor...|$|R
5000|$|Discrete control procedures, or {{controls}} {{are defined by}} the SEC as: [...] "...a specific set of policies, procedures, and activities designed to meet an objective. A control may exist within a designated function or activity in a process. A control’s impact...may be entity-wide or specific to an account balance, <b>class</b> <b>of</b> <b>transactions</b> or application. Controls have unique characteristics - for example, they can be: automated or manual; reconciliations; segregation of duties; review and approval authorizations; safeguarding and accountability of assets; preventing or detecting error or fraud. Controls within a process may consist of financial reporting controls and operational controls (that is, those designed to achieve operational objectives)." ...|$|R
25|$|Governments use {{different}} kinds of taxes and vary the tax rates. They do this in order to distribute the tax burden among individuals or <b>classes</b> <b>of</b> the population involved in taxable activities, such as the business sector, or to redistribute resources between individuals or classes in the population. Historically, taxes on the poor supported the nobility; modern social-security systems aim to support the poor, the disabled, or the retired by taxes on those who are still working. In addition, taxes are applied to fund foreign aid and military ventures, to influence the macroeconomic performance of the economy (a government's strategy for doing this is called its fiscal policy; see also tax exemption), or to modify patterns of consumption or employment within an economy, by making some <b>classes</b> <b>of</b> <b>transaction</b> more or less attractive.|$|R
40|$|In {{this paper}} {{we present a}} {{framework}} for consistency checking of a database with respect to its integrity constraints, overcoming the restriction of considering stratified databases only. To reach this goal we have considered the well-founded and stable models semantics and then {{decided to use the}} well-founded one. The basic idea is that of finding the widest <b>class</b> <b>of</b> <b>transactions</b> for which it can be stated that the updated database D 2 ̆ 7 satisfies the integrity constraints IC, without having to compute its entire well-founded model, WF(D 2 ̆ 7). To this purpose the concept <b>of</b> conservative <b>transactions</b> has been introduced and furthermore a method to compute a (suitable approximation of the) minimal subset of WF(D 2 ̆ 7) that permits to decide the satisfiability of contraints is presented...|$|R
40|$|To be {{necessary}} {{means to be}} useful, indispensable for a particular purpose. To what extent is the samplingmethod necessary in the audit of financial statements? This paper aims to answer the question, why is samplingimportant, following {{the advantages and disadvantages}} of the two known sampling methods: statistical sampling andnon-statistical sampling. Using sampling saves time and effort for the team of auditors. Reduced costs for the auditorsand for the audited entity, resulting from the use of procedures only for the selected units, is another advantage ofusing this technique. The auditors should use their professional judgment to assess audit risk and to establishappropriate procedures for accountsbalances and <b>class</b> <b>of</b> <b>transactions</b> tested. Sampling should be used in order togive a correct opinion of the financial statements...|$|R
40|$|The rapid {{increase}} {{in the size of}} U. S. companies from the early twentieth century created the need for audit procedures based on the selection of a part of the total population audited to obtain reliable audit evidence, to characterize the entire population consists of account balances or <b>classes</b> <b>of</b> <b>transactions.</b> Sampling is not used only in audit – is used in sampling surveys, market analysis and medical research in which someone wants to reach a conclusion about a large number of data by examining only a part of these data. The difference is the “population” from which the sample is selected, ie that set of data which is intended to draw a conclusion. Audit sampling applies only to certain types of audit procedures...|$|R
40|$|This paper {{deals with}} the <b>transaction</b> {{management}} aspects <b>of</b> the R * distributed database system. It concentrates primarily on {{the description of the}} R * commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2 P) commit protocol. PA is optimized for read-only <b>transactions</b> and a <b>class</b> <b>of</b> multisite update <b>transactions,</b> and PC is optimized for other <b>classes</b> <b>of</b> multisite update <b>transactions.</b> The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R*‘s approach toward distributed deadlock detection and resolution...|$|R
40|$|We {{consider}} {{the use of}} a cluster system for Application Service Providers. To obtain high-performance and high-availability, we replicate databases (and DBMS) at several nodes, so they can be accessed in parallel through applications. Then the main problem is to assure the consistency of autonomous replicated databases. Preventive replication [8] provides a good solution that exploits the cluster's high speed network, without the constraints of synchronous replication. However, the solution in [8] assumes full replication and a restricted <b>class</b> <b>of</b> <b>transactions.</b> In this paper, we address these two limitations in order to scale up to large cluster configurations. Thus, the main contribution is a refreshment algorithm that prevents conflicts for partially replicated databases. We describe the implementation of our algorithm over a cluster of 32 nodes running PostGRESQL. Our experimental results show that our algorithm has excellent scale up and speed up...|$|R
40|$|International audienceHybrid Transactional Memory (TM) uses {{available}} hardware TM {{resources to}} execute language-level transactions, and falls {{back to a}} software TM implementation for those transactions that cannot complete in hardware. Ideally, a hybrid TM would allow hardware and software transactions to run concurrently, but would not waste hardware TM resources on coordination between the two <b>classes</b> <b>of</b> <b>transactions.</b> In addition, it should scale well, incur little latency, offer strong safety guarantees, and provide some degree of fairness. We introduce a new hybrid TM algorithm, “Hybrid Cohorts”, in which hardware transactions do not modify global metadata, and software transactions have ex- tremely low per-access overhead. The tradeoff is that hardware transactions cannot commit while software transactions are in flight. Evaluation on an 8 -thread Intel Haswell CPU shows competitive performance with the current state-of-the-art. Furthermore, it does so while providing acceptable levels of fairness and safety, and offering opportunities for hardware acceleration...|$|R
40|$|The {{most recent}} advancements in {{computer}} hardware and communications make the mobile computing paradigm tangible and feasible. One {{of the major}} factors affecting mobile computing is communication protocols efficiency. This paper proposes and discusses an Adaptive Queuing Protocol which targets advanced mobile database applications. The protocol has two main objectives. Firstly, {{to compensate for the}} relatively slow speed of some existing mobile communication links. Secondly, to reduce the cost of communications by reducing link usage. In achieving these aims, our goal has been to reduce the total data volume that the link must carry, {{and at the same time}} ensure adequate response time for all <b>classes</b> <b>of</b> <b>transactions.</b> Results <b>of</b> computer simulation are presented and discussed. 1. Introduction Recent advances in miniaturisation and cellular technology make the computing paradigm ubiquitous and are extending the scope of database applications. While distributed databases have been studied [...] ...|$|R
40|$|AbstractAudit {{sampling}} means, {{according to}} International Standard on Auditing 530 “Audit Sample”, applying audit procedures {{to less than}} 100 % of the items or <b>class</b> <b>of</b> <b>transactions</b> within an account balance. Sampling is used not only in auditing financial statements, but widely in market research, scientific analysis, market analysis, surveys. An ideal situation would involve studying the entire population under investigation. This is impossible in situations where we find large populations of data. Using sampling saves time and effort for the team of auditors. Lower costs for auditors and the audited entity resulting from the application procedures for selected units only, is another advantage of using this technique. The auditor must use professional judgment to assess audit risk and establish appropriate procedures for the transactions and accounts tested. Sampling must ensure a correct opinion of the financial statements. This article studies the non-statistical sampling technique used in testing area suppliers, presenting a selection of sample application based on this method...|$|R
40|$|According to the Statement on Auditing Standards (SAS) No. 39 (AU 350. 01), audit {{sampling}} {{is defined}} as “the application of an audit procedure to less than 100 % of the items within an account balance or <b>class</b> <b>of</b> <b>transactions</b> {{for the purpose of}} evaluating some characteristic of the balance or class”. The audit system develops in different steps: some are not susceptible to sampling procedures, while others may be held using sampling techniques. The auditor may also be interested in two types of accounting error: the number of incorrect records in the sample that overcome a given threshold (natural error rate), which may be indicative of possible fraud, and the mean amount of monetary errors found in incorrect records. The aim {{of this study is to}} monitor jointly both types of errors through an appropriate system of hypotheses, with particular attention to the second type error that indicates the risk of non-reporting errors overcoming the upper precision limits...|$|R
40|$|Instructional {{transactions}} are instructional algorithms, patterns of learner interactions, usually {{far more complex}} than a single display and a single response, which {{have been designed to}} enable the learner to acquire a certain kind of knowledge or skill. Different kinds of knowledge and skill would require different kinds <b>of</b> <b>transactions.</b> The necessary set <b>of</b> these instructional <b>transactions</b> are designed and programmed once, like other computer applications such as spread sheets. They can then be used with different content topics as long as these topics are of a similar kind of knowledge or skill. A transaction shell is the structure <b>of</b> a <b>transaction</b> identifying the interactions, parameters, and knowledge representation needed for a given <b>class</b> <b>of</b> <b>transactions.</b> A transaction shell is a piece of computer code that, when delivered to a student via an appropriate delivery system, causes a transaction to occur. A <b>transaction</b> shell consists <b>of</b> two subsystems: an authoring environment and a delivery environment. The users of the authoring environment are subject matter experts and instructors; the users of the delivery environment are students. The authoring environment has a knowledge acquisition system and a transaction configuration system. The knowledge acquisition system queries a subject matter expert concerning the knowledge and skill required by the enterprise. This information is organized and stored in a knowledge base. The transaction configuration system enables the instructor, or designer, to provide values {{for a wide range of}} instructional parameters. These parameters control the nature of the interactions with the learner. Instructional parameters enable a given transaction shell to be customized for a particular student population, learning environment, [...] ...|$|R
40|$|AbstractAudit {{sampling}} means, {{according to}} International Standard on Auditing 530 “Audit Sample”, applying audit procedures {{to less than}} 100 % of the items or <b>class</b> <b>of</b> <b>transactions</b> within an account balance. Sampling is used not only in auditing financial statements, but widely in market research, scientific analysis, market analysis, surveys. An ideal situation would involve studying the entire population under investigation. This is impossible in situations where we find large populations of data. The auditor must use professional judgment to assess audit risk and establish appropriate procedures for the transactions and accounts tested. When the auditor uses sampling, {{his goal is to}} ensure that the sample provides a reasonable basis to draw conclusions about the population from which the sample is selected. Using statistical sampling assumes a computer program, more expensive, it requires statistical knowledge and assumes in a lesser extent the use of professional judgment. Non-statistical sampling does not allow quantification of risk, leaving large margin of interpretation, exposing the auditor to a high risk of malpractice...|$|R
40|$|Approved {{for public}} release, {{distribution}} is unlimitedThe analysis of LAN performance {{is the main}} objective of this research. LANs can be configured in various ways combining different medium access control mechanisms and different physical layer specifications. Details on these alternatives are specified in IEEE 802. 3 through IEEE 802. 5. We study th performance {{of different types of}} LANs under various configurations of servers and stations. The queueing network model is one of the analytical tools to help investigate the performance characteristics of various LAN configurations. Since the analytical approach based on queueing network models is often too complicated to be practically used, we rely on simulations. Thus our analysis will be based on simulations, and SIMLAN II will be the simulation tool for our work. Our specification of simulation models involves three <b>classes</b> <b>of</b> <b>transactions,</b> and one or two servers. There are 24 simulation results in this thesis. These results, which are arranged in tables and figures, help compare th performance characteristics of various LAN configurations. Lieutenant Commander, Republic of China (Taiwan) Nav...|$|R
40|$|The {{problem of}} finding itemsets that are {{statistically}} significantly enriched in a <b>class</b> <b>of</b> <b>transactions</b> {{is complicated by}} the need to correct for multiple hypothesis testing. Pruning untestable hypotheses was recently proposed as a strategy for this task of significant itemset mining. It was shown to lead to greater statistical power, the discovery of more truly significant itemsets, than the standard Bonferroni correction on real-world datasets. An open question, however, is whether this strategy of excluding untestable hypotheses also leads to greater statistical power in subgraph mining, in which the number of hypotheses is much larger than in itemset mining. Here we answer this question by an empirical investigation on eight popular graph benchmark datasets. We propose a new efficient search strategy, which always returns the same solution as the state-of-the-art approach and is approximately two orders of magnitude faster. Moreover, we exploit the dependence between subgraphs by considering the effective number of tests and thereby further increase the statistical power. Comment: 18 pages, 5 figure, accepted to the 2015 SIAM International Conference on Data Mining (SDM 15...|$|R
5000|$|For audits {{performed}} by an outside audit firm, risk assessment {{is a crucial}} stage before accepting an audit engagement. According to ISA315 Understanding the Entity and its Environment and Assessing the Risks of Material Misstatement, [...] "the auditor should perform risk assessment procedures to obtain {{an understanding of the}} entity and its environment, including its internal control". Evidence relating to the auditor’s risk assessment of a material misstatement in the client’s financial statements. Then, the auditor obtains initial evidence regarding the <b>classes</b> <b>of</b> <b>transactions</b> at the client and the operating effectiveness of the client’s internal controls. Audit risk is defined as the risk that the auditor will issue a clean unmodified opinion regarding the financial statements, when in fact the financial statements are materially misstated, and therefore do not qualify for a clean unmodified opinion. As a formula, audit risk is the product of two other risks: Risk of Material Misstatement and Detection risk. This formula can be further broken down as follows: inherent risk × control risk × detection risk.|$|R
40|$|In this decade, the United States Supreme Court decided {{two cases}} which {{revolved}} around the enforceability of choice of forum clauses contained in transnational commercial contracts. The decisions which the Court rendered reshaped significantly the legal contours of the enforceability of such clauses. In the two cases, the Court signaled that it was prepared to recognize the distinction between what may be termed 2 ̆ 2 internal 2 ̆ 2 public policy and what may be termed 2 ̆ 2 international 2 ̆ 2 public policy. The recognition of this distinction {{is likely to have}} a vital bearing on the right of persons to provide for a specific foreign judicial or arbitral forum as part of the transnational agreement into which they have entered. Before discussing the impact of the two cases on the effectiveness of choice of forum clauses, it is appropriate to define the two varieties of public policy, particularly in terms of how they pertain to issues raised by choice of forum clauses. The relationship between 2 ̆ 2 internal 2 ̆ 2 public policy and 2 ̆ 2 international 2 ̆ 2 public policy was an approximate analogue in the distinction made in French law between ordre public interne and ordre public international. The distinction is that one set of standards is applied to transactions which are performed in a local context, while another set of standards is applied to transactions which have international elements. An example <b>of</b> the former <b>class</b> <b>of</b> <b>transactions</b> is a contract which is entered into by two nationals and which is to be performed within their country; an example <b>of</b> the latter <b>class</b> <b>of</b> <b>transactions</b> is a contract which is entered into by persons of different nationalities and which is to be performed in a country other than the place where the contract is entered into. The standards required by ordre public interne are parts of the total law of the forum from which there may be no derogation. The standards required by ordre public international can differ from standards of the local law of the forum, provided that the transaction contains sufficient transnational elements. Nevertheless, there remain certain forum law standards that even transnational transactions must meet. That is, the ordre public international of the forum is deemed to include certain of the standards required by the ordre public interne of the forum...|$|R
