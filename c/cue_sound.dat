2|398|Public
40|$|In {{a visual}} {{occlusion}} task, 4 -month-olds {{were given a}} dynamic sound cue (following the trajectory of an object), or a static <b>cue</b> (<b>sound</b> remained stationary). Infants’ oculomotor anticipations were greater in the Dynamic condition, suggesting that representations of visual occlusion were supported by auditory information...|$|E
40|$|This study explores {{auditory}} processing mechanisms {{underlying the}} computation of sound source location. A previous study examined whether a <b>cue</b> <b>sound</b> that indicated which left-right spatial hemifield to attend could improve localization accuracy (Kopčo, Ler and Shinn-Cunningham, 2001) {{in the same}} way that it decreases response latency (Spence and Driver, 1994). Results show that, in an ordinary room, an informative preceding auditory cue does not improve localization accuracy; however, the presence of a preceding cue causes consistent localization bias of the target stimulus for cue-target delays as long as 300 ms. The current paper presents acoustic analysis that examines the extent to which localization bias can be explained by purely acoustic effects (including room reverberation) as opposed to neural processing effects (e. g., see Carlile, Hyams and Delaney, 2001) ...|$|E
50|$|Localization cues are {{features}} that help localize <b>sound.</b> <b>Cues</b> for <b>sound</b> localization include binaural and monoaural cues.|$|R
40|$|ME 450 Capstone Design and Manufacturing Experience: Winter 2007 The goal of {{this project}} is to {{eliminate}} undesired <b>sound</b> <b>cues</b> during the delivery of food pellets to research rats. Unnecessary mechanical noise currently accompanies food delivery, cuing the rats to the incoming pellet before it arrives. Evidence {{has shown that the}} <b>sound</b> <b>cues</b> influence neural response of the rats, thus, affecting the research of our sponsor. Our objective is to redesign the dispenser setup to eliminate any unwanted <b>cues</b> (<b>sound,</b> smell, vibration, etc.), while maintaining current functionality. In so doing, the neural response of rats to receiving reward can be isolated and studied more accurately...|$|R
50|$|The {{channel catfish}} {{is adapted to}} limited light conditions. Members of the genus Ictalurus, which inhabit muddy waters, do not depend solely on visual cues. Instead, they are known to rely heavily on chemotaxic <b>cues.</b> <b>Sound</b> {{production}} may be another important means of communication among channel catfish and other species living in turbid habitats.|$|R
40|$|The {{interaural}} time difference (ITD) {{is a major}} <b>cue</b> to <b>sound</b> localization along the horizontal plane. The A primary <b>cue</b> to <b>sound</b> localization is the interaural maximum natural ITD occurs when a sound source is time difference (ITD). Sounds arriving at the left ear positioned opposite to one ear. We examined the abil- before the right, for example, indicate a source posi...|$|R
5000|$|Acoustic wayfinding, the {{practice}} of using auditory <b>cues</b> and <b>sound</b> markers to navigate indoor and outdoor spaces ...|$|R
5000|$|Contestants had {{to remove}} items {{from a large}} fake nose which when {{gathered}} together would make {{the name of a}} celebrity. Using charades like <b>cues</b> with <b>sounds</b> like being an ear attached to the object.|$|R
30|$|We {{instructed}} {{participants to}} twist their trunks {{and to start}} their motion from the waist {{and move to the}} elbow in turn. After this instruction, participants listened for auditory <b>cue</b> <b>sounds</b> while swinging the bat. If participants could not realize the ideal cue, they revised their own motion. Participants repeated these practices until they acquired ideal motion. The swing motion from the elbow to the wrist is almost one flow. For upper limb practice, we instructed beginners to flex their elbows. After this first stage, the beginners repeatedly practiced their swing and listened to the auditory BF system to develop more automatic body movements.|$|R
40|$|Objectives: In a group home, {{caregivers}} {{should be}} aware of the inhabitant's real-time situation. The aim of our study is to facilitate the awareness of an inhabitant's situation by means of enhanced <b>sound</b> <b>cues.</b> Methods: We propose an audio notification system that indicates the real-time situation of persons in a group home environment using <b>sound</b> <b>cues</b> instead of visual surveillance. The notification system comprises a prediction and a notification function. The prediction function estimates a person's real-time situation using a Bayesian network and sensed information; the notification function informs recipients of the predicted situation and the confidence level of the prediction by means of <b>sound</b> <b>cues.</b> We use natural <b>sounds</b> as <b>sound</b> <b>cues.</b> Results: As a first step to examine our system in a group home, we conducted operation and performance tests of each unit under a simple test environment. The correct prediction of the subject's situation is approximately 90 %; further, it is shown that the <b>sound</b> <b>cues</b> should be selected according to their environmental dependence. Conclusions: The results show that the method is useful for monitoring persons. As future study, we will conduct a field test on an implemented system and improve it for practical use in a group home...|$|R
50|$|The sound {{localization}} {{mechanisms of}} the mammalian auditory system have been extensively studied. The auditory system uses several <b>cues</b> for <b>sound</b> source localization, including time- and level-differences (or intensity-difference) between both ears, spectral information, timing analysis, correlation analysis, and pattern matching.|$|R
40|$|Non-individualized head-related {{transfer}} function (HRTF) devices cannot generate accurate directional <b>sound</b> <b>cues.</b> In particular, listeners often incorrectly perceive a <b>sound</b> <b>cue</b> {{from the front}} as coming from the back (referred to as front/back confusion). This study presents a methodology to reduce the occurrence of front/back confusion when listening to <b>sound</b> <b>cues</b> generated using non-individualized HRTF devices. The study investigates and identifies that front/back confusion occurs because of mis-matches between non-individualized HRTFs and the acoustics characteristics of the ears of individual listeners. Using literature concerning the acoustics characteristics of human ears, spectrum of 196 open-copyrighted non-individualized HRTFs have been analyzed and clustered according to their abilities to generated directional <b>sound</b> <b>cues</b> that are coming from the front and from the back. These clusters are further processed to determine six standard directional <b>sound</b> <b>cues</b> for the center-front and center-back directions, respectively. These twelve standard <b>sound</b> <b>cues</b> enable listeners to choose and minimize the possible mis-match that leads to front/back confusion. Three experiments have been conducted and {{the results indicate that}} providing the choice of these six standard frontal cues and the six standard backward cues can significantly reduce the sound localization errors and the rates of front/back and back/front confusion. In addition, providing the choice of these twelve cues have been shown to achieve better sound localization performance than using the <b>sound</b> <b>cues</b> generated with the MIT KEMAR non-individualized HRTF set. This finding is important because the MIT KEMAR HRTF set is the most widely-used open-copyrighted HRTF data set. Results reported in this thesis enable engineers to design and develop customizable non-individualized HRTF devices in a cost-effective way. Currently, most digital audio products are manufactured in China but the profit margins for such products are decreasing annually. If industrialists in Hong Kong can acquire the ability to produce customizable virtual surround sound systems using non-individualized HRTF devices at low cost, new innovative products with much greater profit margins can be designed and produced...|$|R
50|$|If a girl doesn’t {{like the}} man, she will turn her light off (followed by a <b>sound</b> <b>cue).</b>|$|R
40|$|International audienceBottlenose {{dolphins}} (Tursiops truncatus) spontaneously emit individual acoustic {{signals that}} identify them to group members. We tested whether these cetaceans could learn artificial individual <b>sound</b> <b>cues</b> played underwater {{and whether they}} would generalize this learning to airborne sounds. Dolphins are thought to perceive only underwater sounds and their training depends largely on visual signals. We investigated the behavioral responses of seven dolphins in a group to learned human-made individual <b>sound</b> <b>cues,</b> played underwater and in the air. Dolphins recognised their own <b>sound</b> <b>cue</b> after hearing it underwater as they immediately moved towards the source, whereas when it was airborne they gazed more at {{the source of their}} own <b>sound</b> <b>cue</b> but did not approach it. We hypothesize that they perhaps detected modifications of the sound induced by air or were confused by the novelty of the situation, but nevertheless recognized they were being “targeted”. They did not respond when hearing another group member’s cue in either situation. This study provides further evidence that dolphins respond to individual-specific sounds and that these marine mammals possess some capacity for processing airborne acoustic signals...|$|R
50|$|Pat Southard was the Workshop's {{indispensable}} Keyboard {{player and}} offstage collaborator, contributing musical <b>cues,</b> live <b>sound</b> effects (including billiard balls in a one foot square pool table) and occasional voices. Pat, along with Rich Mills {{moved on to}} the Radio Music Theater with the Farrells.|$|R
40|$|In many {{computer}} games, {{players are}} posed {{with the task}} of navigating from one area to another, or finding a specific item or character within an area. Such navigation tasks can be frustrating if the game play is not in some way enhanced. This essay investigates whether or not game designs with diegetic navigation <b>sound</b> <b>cues</b> correspond consistently to a sense of flow in games. A number of flow critera are compared in game designs with and without non-diegetic <b>sound</b> <b>cues.</b> A test game was created where subjects navigate through two different 3 D-mazes, where the different navigation sounds were tested. Findings show that diegetic navigation <b>sound</b> <b>cues</b> were most often associated with the attributes of a flow experience. Validerat; 20130624 (global_studentproject_submitter...|$|R
50|$|Lastly, for stage shows, the fly rigs or battens {{are tested}} for weight and {{accuracy}} of <b>cueing</b> with <b>sound</b> and lights. If there are moving set pieces, the crew will test their operation and mechanics (if they are automated) and practice their movement, flow, and position on and offstage.|$|R
40|$|<b>Sound</b> <b>cues</b> {{filtered}} from individualized head-related transfer functions (HRTFs) {{can provide}} accurate up / down directional cues {{to that particular}} listener. When the same cues are presented to other listeners, errors occur in the perceived up / down directions and these errors vary greatly among listeners. This thesis presents a study to relate individual’s localization errors of non-individualized HRTF-filtered up / down <b>sound</b> <b>cues</b> with a proposed index (referred to as the ‘matching score’). These scores are calculated from individual’s ear dimensions {{as well as the}} spectra of the HRTFs used in filtering the <b>sound</b> <b>cues.</b> It is hypothesized that for a particular listener and a particular up / down HRTF-filtered <b>sound</b> <b>cue,</b> the higher the matching score, the lower the localization errors (H 1). The matching score, based upon the ‘delay-and-add’ theory proposed by Hebrank and Wright (1974), is new and original and forms part of the academic contribution of the thesis. If H 1 is proven, this thesis will be the first study to provide empirical evidence to support the proposed ‘delay-and-add’ theory. Three dimensional moulds of outer ears from thirty-three participants have been collected. Using the ear dimensions, matching scores have been calculated between each of the 33 participants and 192 open-copyrighted non-individualized HRTFs (from the LISTEN database: IRCAM and AKG Acoustics, 2004 and CIPIC database: Algazi et al., 2001). These calculations have been repeated {{for each of the four}} selected elevation angles (30 and 15 degrees below ear level and 30 and 60 degrees above ear level) to give 25344 matching scores. Using these matching scores, five non-individualized HRTFs having the 0 th, 25 th, 50 th, 75 th, and 100 th percentile average matching scores have been selected for each of the four elevation angles. These twenty HRTFs are then used to produce a total of twenty <b>sound</b> <b>cues</b> (4 angles x 5 HRTFs). Seventeen participants (randomly selected from the 33 participants of the survey) were invited back to take part in a within-subject design experiment in which each listener needed to localize 120 <b>sound</b> <b>cues</b> presented in random order. These 120 <b>sound</b> <b>cues</b> represent the combination of 20 <b>sound</b> <b>cues</b> and 6 repetitions. The objective of the experiment is to evaluate the relationship between the matching score and the localization errors. In this experiment, listeners obtained an average localization error of about 36 degrees to an elevation cue, which is comparable to the results from other studies (Zotkin et al., 2003, Seeber and Fastl, 2003). Our proposed matching scores have been found to significantly correlate with the localization errors and regression analyses confirm that as the scores increase, the errors reduce significantly. In other words, H 1 has been supported. Further analyses using the ‘school-effect’ model indicates that the matching score alone can explain 27 % of between listener variations in the localization errors. Discussions on inter- and intra-listener variability in perceiving up/down direction of a <b>sound</b> <b>cue</b> are included. The study is the first to provide empirical support to the ‘delay-and-add’ theory to explain human perception of up / down direction of a <b>sound</b> <b>cue.</b> Future work to refine the ‘matching score’ calculation is desirable...|$|R
5000|$|Cueing {{consists}} of the caregiver making a particular <b>sound</b> or other <b>cue</b> when they provide the baby {{with an opportunity to}} eliminate. At first, the caregiver can make the <b>cueing</b> <b>sound</b> when the baby is eliminating to develop an association between the sound and the action. Once the association is established, the cue can be used to indicate to the baby {{that he or she is}} in an appropriate potty place. This is especially useful for infants who may not recognize public toilets or unfamiliar receptacles as a [...] "potty." [...] Common <b>sound</b> <b>cues</b> include [...] "psss psss" [...] for urination and [...] "hmm hmm" [...] (grunting) for defecation. Older babies (late starters) may respond better to more word-like cues. Cues do not have to be auditory; the act of sitting on the potty itself or being held in position can serve as a cue, or the sign language sign for [...] "toilet" [...] can be a cue. The American Sign Language sign for [...] "toilet" [...] involves forming a hand into the letter [...] "T" [...] (a fist with the thumb inserted between the first and middle fingers) and shaking the hand side to side from the wrist.|$|R
40|$|Indoors and {{in nature}} alike, the {{auditory}} scenes that we perceive unfold in reverberant environments. In a reverberant sound field, reflected acoustic waves reach the listener from all directions, {{interfering with the}} direct sound and distorting the binaural <b>cues</b> for <b>sound</b> localization such as interaural time and level differences (ITD and ILD). I...|$|R
40|$|This paper {{discusses}} how {{the barn}} owl’s brain stem auditory pathway {{is divided into}} two physiologically and anatomically segregated channels for separate processing of interaural phase and intensity <b>cues</b> for <b>sound</b> localization. The paper also points out the power of the ‘‘downstream’’ approach by which the emergence of a higher‐order neuron’s stimulus selectivity can be traced through lower‐order stations...|$|R
40|$|Cochlear implant (CI) users’ limited {{ability to}} use {{acoustical}} <b>cues</b> for <b>sound</b> localization causes left/right confusions and front/back reversals. Head movement is beneficial in reducing these errors in acoustically hearing listeners. This study investigated the effect of head movement on localization throughout 360 o of azimuth for both real and simulated CI users. Listeners in a bilateral electro-acoustic (CI with ipsilateral hearing aid) simulation derived the greatest head movement benefit in reducing front/back reversals. Left/right confusions were reduced in simulations with matched bilateral stimulation. Sensitivity to both timing and level <b>cues</b> for <b>sound</b> localization was correlated with sound localization performance without head movement for simulated device users. Sensitivity to timing cues was correlated with sound localization performance with head movement cues for simulated device users. Simulations of bilateral CI and bimodal users (CI with contralateral hearing aid) listening predicted real users’ sound localization performance, binaural sensitivity and head movement patterns...|$|R
2500|$|The {{speech sound}} signal {{contains}} {{a number of}} acoustic cues {{that are used in}} speech perception. The <b>cues</b> differentiate speech <b>sounds</b> belonging to different phonetic categories. For example, one of the most studied cues in speech is voice onset time or VOT. VOT is a primary cue signaling the difference between voiced and voiceless plosives, such as [...] "b" [...] and [...] "p". Other <b>cues</b> differentiate <b>sounds</b> that are produced at different places of articulation or manners of articulation. The speech system must also combine these cues to determine the category of a specific speech sound. This is often thought of in terms of abstract representations of phonemes. These representations can then be combined for use in word recognition and other language processes.|$|R
5000|$|Although {{the term}} is most {{commonly}} applied to <b>sound</b> <b>cues</b> in a computer interface, examples of the concept {{can be seen in}} broadcast media such as radio and television: ...|$|R
50|$|The term “soundscape” {{refers to}} the sonic {{environment}} of a specific locale. It may also refer to actual environments, or to abstract constructions such as musical compositions and tape montages, particularly when considered as an artificial environment. The objective of sound maps is to represent a specific environment using its soundscape as primary references as opposed to visual <b>cues.</b> <b>Sound</b> maps are {{in many ways the}} most effective auditory archive of an environment. Sound maps are similar to sound walks which are a form of active participation in the soundscape. Soundwalks and indeed, sound maps encourage the participants to listen discriminatively, and moreover, to make critical judgments about the sounds heard and their contribution to the balance or imbalance of the sonic environment. However, soundwalks will plot out a route for the user to follow and give guidance as to what the user may be hearing at each checkpoint. Sound maps, on the other hand, have specific soundscapes recorded that users can listen to at each checkpoint.|$|R
40|$|An audio {{platform}} {{game was}} created and evaluated {{in order to}} answer {{the question of whether or}} not an audio game could be designed that effectively conveys the spatial information necessary for persons with visual impairments to successfully navigate the game levels and respond to audio cues in time to avoid obstacles. The game used several types of audio <b>cues</b> (<b>sounds</b> and speech) to convey the spatial setup (map) of the game world. Most audio-only players seemed to be able to create a workable mental map from the game 2 ̆ 7 s <b>sound</b> <b>cues</b> alone, pointing to potential for the further development of similar audio games for persons with visual impairments. The research also investigated the navigational strategies used by persons with visual impairments and the accuracy of the participants 2 ̆ 7 mental maps as a consequence of their navigational strategy. A comparisons of the maps created by visually impaired participants with those created by sighted participants playing the game with and without graphics, showed no statistically significant difference in map accuracy between groups. However, there was a marked difference between the number of 2 ̆ 2 invented 2 ̆ 2 objects when we compared this value between the sighted audio-only group and the other groups, which could serve as an area for future research...|$|R
40|$|In the {{ascending}} auditory pathway, {{the context}} in which a particular stimulus occurs can influence the character of the responses that encode it. Here we demonstrate that the cortical representation of a binaural <b>cue</b> to <b>sound</b> source location is profoundly context-dependent: spike rates elicited by a 0 ° interaural phase disparity (IPD) were very different when preceded by 90 ° versus � 90 ° IPD. The changes in firing rate associated with equivalent stimuli occurring in different contexts are comparable to changes in discharge rate that establish cortical tuning to the cue itself. Single-unit responses to trapezoidally modulated IPD stimuli were recorded in the auditory cortices of awake rhesus monkeys. Each trapezoidal stimulus consisted of linear modulations of IPD between two steady-state IPDs differing by 90 °. The stimulus set was constructed so that identical IPDs and sweeps through identical Sounds originating from locations to the left or right of the head reach the ears at slightly different times, resulting in interaural phase disparity (IPD) cues to their localization in the horizontal plane. Motion of sound sources relative to the head, produced by motion of either the listener or the source, results in dynamic variations in IPD. Although relative head motion also impacts other <b>cues</b> for <b>sound</b> localization, such as interaural intensity differences and monaural spectral cues, human psychophysical studies suggest that IPD <b>cues</b> dominate <b>sound</b> localization judgments for broadband sounds when low frequencies are present (Wightman and Kistler, 1992). Numerous studies suggest that cortical neurons in a range of mammalian species, including monkeys, are sensitive to auditory motion (Sovijärvi and Hyvärinen...|$|R
50|$|Parents {{anticipate}} {{when the}} child needs to go, then at that moment, give a <b>cue</b> signal (a <b>sound,</b> hand signal, word or phrase). The child will associate these with potty time.|$|R
40|$|In this study, we have {{proposed}} an audio notification system that indicates the real-time situation of persons {{in a group}} home environment without visual surveillance. In this paper, we present an approach to predict the events in a real living environment and notify them using <b>sound</b> <b>cues.</b> The notification system comprises a predicting and a notification function. The predicting function estimates a personpsilas real-time situation using a Bayesian network and sensed information; the notification function informs recipients of the predicted situation and the confidence level of the prediction by means of <b>sound</b> <b>cues.</b> We use natural <b>sounds</b> as <b>sound</b> <b>cues,</b> considering the use of our system at a group home of elderly people. We estimate the usefulness of our method under a simple test environment. The {{results show that the}} method is useful for monitoring persons by employing sound effects that are appropriate to usage environments. As a future study, we will conduct a field test on an implemented system and improve it for practical use in a group home...|$|R
40|$|International audienceThis paper {{presents}} a study {{about the effect}} of using additional audio cueing and Head-Related Transfer Function (HRTF) on human performance in sound source localization task without using head movement. The existing techniques of sound spatialization generate reversal errors. We intend to reduce these errors by introducing sensory <b>cues</b> based on <b>sound</b> effects. We conducted and experimental study to evaluate the impact of additional <b>cues</b> in <b>sound</b> source localization task. The results showed the benefit of combining the additional cues and HRTF in terms of the localization accuracy and the reduction of reversal errors. This technique allows significant reduction of reversal errors compared {{to the use of the}} HRTF separately. For instance, this technique could be used to improve audio spatial alerting, spatial tracking and target detection in simulation applications when head movement is not included...|$|R
40|$|We {{present an}} {{interaction}} tool based on rendering distance <b>cues</b> for ordering <b>sound</b> sources in depth. The user interface {{consists of a}} linear position tactile sensor made by conductive material. The touch position is mapped onto the listening position on a rectangular virtual membrane, modeled by a bidimensional Digital Waveguide Mesh and providing distance <b>cues.</b> Spatialization of <b>sound</b> sources in depth allows a hierarchical display of multiple audio streams, as in auditory menus. Besides, the similar geometries of the haptic interface and the virtual auditory environment allow a direct mapping between the touch position and the listening position, providing an intuitive and continuous interaction tool for auditory navigation...|$|R
50|$|Around {{the year}} 1900 Lord Rayleigh {{developed}} the duplex (combination of two) theory of human sound localisation using two binaural cues, interaural phase difference (IPD) and interaural level difference (ILD) (based on {{analysis of a}} spherical head with no external pinnae). The theory posits that we use two primary <b>cues</b> for <b>sound</b> lateralisation, using {{the difference in the}} phases of sinusoidal components of the sound and the difference in amplitude (level) between the two ears.|$|R
50|$|If {{a girl in}} the heartthrob group doesn’t {{like the}} man, she will turn her light off (followed by a <b>sound</b> <b>cue).</b> The girls in the {{observation}} group may use their thermometers to measure their interest in the single man.|$|R
40|$|By {{the year}} 2029, America’s entire baby boomer {{generation}} will be 65 {{years of age}} or older. An estimated 7. 7 million people in the United States will be living with Alzheimer’s disease, compared to the 5 million individuals afflicted today. The illness, which often begins with a failure to remember new information, can eventually result in a complete loss of the ability to communicate. Common symptoms of Alzheimer’s include disorientation in relation to both space and time and disorganized thinking. Victims of Alzheimer’s often require care beyond what some individuals and families can provide, and so many elderly people are relocated from their homes to long-term care facilities. Can these spaces be designed to respond to the cognitive challenges of Alzheimer’s patients in a progressive way, providing not just a place of shelter, but a home that is sensitive to the processes of human memory? Advances in the field of neuroscience provide new insight into the workings of the human mind. Scientists investigate how the brain reacts to sensations of light, visual <b>cues,</b> <b>sounds,</b> smells, and the varying scales of space. Theories abou...|$|R
50|$|Often followspot {{operators}} do {{not take}} their cues from stage managers. This is generally because the timing of actors' entrances and exits and other movements may vary from night to night, and because calling every followspot cue could become too complicated and interfere with the calling and execution of other cues. However, {{if there is a}} problem with the actor appearing on stage, the stage manager will notify the followspots of this over headset. More commonly, a stage manager may only call very specific followspot cues, like a blackout—frequently on a blackout cue there is a light <b>cue,</b> a <b>sound</b> <b>cue,</b> a followspot cue and sometimes even a set cue, so it is very important that everything happens all at the same time. Aside from this, followspot operators take their own cues and follow their own cue sheet, or take direction from the lighting board operator on a communications subsystem dedicated to lighting cues.|$|R
40|$|The vocalizations of nonhuman {{animals are}} {{considered}} potential indicators of motivational or internal state. In many species, different call types, and structural variation within call types, encode information about physical {{characteristics such as}} age or sex, or about variable traits such as motivation. Domestic chickens, Gallus gallus, have an elaborate vocal repertoire, enabling investigation into whether reward-related arousal is encoded within their call type and structure. Twelve hens were given a Pavlovian conditioning paradigm using <b>sound</b> <b>cues</b> to signal the availability of two food rewards (mealworms, normal food), one nonfood reward (a container of substrate suitable for dustbathing), and a sound-neutral event (<b>sound</b> <b>cue,</b> no reward). A muted-neutral treatment (no <b>sound</b> <b>cue,</b> no reward) provided a baseline for vocal behaviour. <b>Sound</b> <b>cues</b> preceded a 15  s anticipation period during which vocalizations were recorded. Hens produced a ‘Food call’ (previously defined in other studies) in anticipation of all rewards, including the nonfood reward. ‘Food calls’ and ‘Fast clucks’ were more prevalent in anticipation of rewards, and most prevalent following the cue signalling the dustbathing substrate, suggesting that this reward induced the most arousal in hens. The peak frequency of ‘Food calls’ made {{in anticipation of the}} dustbathing substrate was significantly lower than those made in anticipation of food rewards, potentially reflecting differences in arousal. Vocalizations that reliably indicate hens 2 ̆ 7 motivational state could be used as measures of welfare in on-farm assessment situations. Our study is the first to reveal variation in the frequency-related parameters of the ‘Food call’ in different contexts, and to show the prevalence of different call types in reward and nonreward contexts, which may have implications for welfare assessments...|$|R
