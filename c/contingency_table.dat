1256|1613|Public
25|$|The trial can be {{summarized}} and analyzed {{in terms of the}} following <b>contingency</b> <b>table.</b>|$|E
25|$|Plots of {{the four}} results above in the ROC space are given in the figure. The result of method A clearly shows the best {{predictive}} power among A, B, and C. The result of B lies on the random guess line (the diagonal line), {{and it can be}} seen in the table that the accuracy of B is 50%. However, when C is mirrored across the center point (0.5,0.5), the resulting method C′ is even better than A. This mirrored method simply reverses the predictions of whatever method or test produced the C <b>contingency</b> <b>table.</b> Although the original C method has negative predictive power, simply reversing its decisions leads to a new predictive method C′ which has positive predictive power. When the C method predicts p or n, the C′ method would predict n or p, respectively. In this manner, the C′ test would perform the best. The closer a result from a <b>contingency</b> <b>table</b> is to the upper left corner, the better it predicts, but the distance from the random guess line in either direction is the best indicator of how much predictive power a method has. If the result is below the line (i.e. the method is worse than a random guess), all of the method's predictions must be reversed in order to utilize its power, thereby moving the result above the random guess line.|$|E
2500|$|Let us define an {{experiment}} from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 <b>contingency</b> <b>table</b> or confusion matrix, as follows: ...|$|E
40|$|First we derive the maximal {{breakdown}} {{value of}} regression equivariant estimators in two-way <b>contingency</b> <b>tables</b> under the loglinear independence model. We then {{prove that the}} L 1 estimator achieves this maximal breakdown value. Finally, we illustrate how these results can be generalized towards the uniform association model for <b>contingency</b> <b>tables</b> with ordered categories. L 1 regression Breakdown value <b>Contingency</b> <b>tables</b> Uniform association model...|$|R
5000|$|Cochran-Mantel-Haenszel {{test for}} {{stratified}} <b>contingency</b> <b>tables</b> ...|$|R
40|$|A {{comprehensive}} study of graphical log-linear models for <b>contingency</b> <b>tables</b> is presented. High-dimensional <b>contingency</b> <b>tables</b> arise in many areas. Analysis of <b>contingency</b> <b>tables</b> involving several factors or categorical variables is very hard. To determine interactions among various factors, graphical and decomposable log-linear models are preferred. Connections between the conditional independence in probability and graphs are explored, followed with illustrations to describe how graphical log-linear model are useful to interpret the conditional independences between factors. The problem of estimation and model selection in decomposable models is discussed...|$|R
2500|$|Given a set [...] of [...] elements, and two groupings or {{partitions}} (e.g. clusterings) {{of these}} points, namely [...] and , the overlap between [...] and [...] {{can be summarized}} in a <b>contingency</b> <b>table</b> [...] where each entry [...] denotes the number of objects in common between [...] and [...] : [...]|$|E
2500|$|The <b>contingency</b> <b>table</b> can derive several {{evaluation}} [...] "metrics" [...] (see infobox). To draw a ROC curve, {{only the}} true positive rate (TPR) and false positive rate (FPR) are needed (as functions of some classifier parameter). The TPR defines how many correct positive results occur among all positive samples available during the test. FPR, {{on the other}} hand, defines how many incorrect positive results occur among all negative samples available during the test.|$|E
2500|$|The {{classical}} {{application of}} the hypergeometric distribution is sampling without replacement. Think of an urn with two types of marbles, red ones and green ones. Define drawing a green marble as a success and drawing a red marble as a failure (analogous to the binomial distribution). If the variable N describes the number of all marbles in the urn (see <b>contingency</b> <b>table</b> below) and K describes the number of green marbles, then N−K corresponds {{to the number of}} red marbles. In this example, X is the random variable whose outcome is k, the number of green marbles actually drawn in the experiment. This situation is illustrated by the following contingency table: ...|$|E
40|$|This paper {{presents}} the computation of Markov bases for <b>contingency</b> <b>tables</b> when the cell entries are bounded. Markov bases allow a connected random {{walk on the}} reference set of the table. Using some theory in Commutative Algebra, we show that a Markov basis for bounded <b>contingency</b> <b>tables</b> is in general different from the Markov basis for unbounded <b>contingency</b> <b>tables.</b> In addition, we give a simple method to compute Markov bases {{in this case and}} we provide some practical examples both in one-dimensional and in multi-dimensional settings...|$|R
40|$|The aim of {{this paper}} is to {{demonstrate}} the R package conting for the Bayesian analysis of complete and incomplete <b>contingency</b> <b>tables</b> using hierarchical log-linear models. This package allows a user to identify interactions between categorical factors (via complete <b>contingency</b> <b>tables)</b> and to estimate closed population sizes using capture-recapture studies (via incomplete <b>contingency</b> <b>tables).</b> The models are fitted using Markov chain Monte Carlo methods. In particular, implementations of the Metropolis-Hastings and reversible jump algorithms appropriate for log-linear models are employed. The conting package is demonstrated on four real examples. Publisher PDFPeer reviewe...|$|R
40|$|Funo (2008) {{considered}} the maximum likelihood estimator (MLE) for three-dimensional cubic <b>contingency</b> <b>tables</b> {{under the assumption}} of quasi-symmetry. In this paper, we consider the quasi-symmetric model and some modified quasi-symmetric models for four-dimensional cubic <b>contingency</b> <b>tables</b> and obtain the maximum likelihood equations. Iterative scaling procedure (ISP) for obtaining the MLE is also discussed...|$|R
5000|$|It is {{a special}} kind of <b>contingency</b> <b>table,</b> with two {{dimensions}} ("actual" [...] and [...] "predicted"), and identical sets of [...] "classes" [...] in both dimensions (each combination of dimension and class is a variable in the <b>contingency</b> <b>table).</b>|$|E
5000|$|For {{the general}} <b>contingency</b> <b>table,</b> we can write the log-likelihood ratio {{statistic}} as ...|$|E
50|$|In {{the case}} of a 2×2 <b>contingency</b> <b>table</b> Cramér's V is equal to the Phi coefficient.|$|E
40|$|This paper {{describes}} {{the historical development of}} analytic techniques for frequency data presented as 2 x 1 or 2 x 2 <b>contingency</b> <b>tables.</b> The issues raised include: {{the need for}} a continuity correction when estimating the exceedance probability of a discrete statistic; the vulnerability of such statistics to a conservative bias when used in hypothesis testing; and the different statistical models that may underlie <b>contingency</b> <b>tables.</b> In the case of 2 x 1 <b>contingency</b> <b>tables</b> the recommended technique is the binomial test with a modified decision procedure for hypothesis testing. In the case of 2 x 2 <b>contingency</b> <b>tables</b> there is a single statistic approximating the chi-square distribution which can be used to test the hypothesis of an association between the two relevant variables and which in most practical situations is robust with respect to the occurrence of small expected cell frequencies...|$|R
40|$|Currently used {{methods for}} the {{analysis}} of <b>contingency</b> <b>tables</b> with log-linear models and their extensions are essentially a way to subject the parameters of the saturated model to a set of linear constraints. However, in many contexts, expecially when a subset of the variables are ordinal, it would be very useful to be able to consider also linear inequality constraints. In this paper we show how to extend previous models for <b>contingency</b> <b>tables</b> by considering simultaneously both equality constraints. We also give a self contained discussion of the problem of testing equality and inequality constraints in the context of <b>contingency</b> <b>tables</b> analysis...|$|R
50|$|Poisson {{regression}} for <b>contingency</b> <b>tables,</b> {{a type of}} generalized linear model.|$|R
50|$|The <b>contingency</b> <b>table</b> and {{the most}} common derived ratios are {{summarized}} below; see sequel for details.|$|E
50|$|The pivot {{operation}} in spreadsheet software {{can be used}} to generate a <b>contingency</b> <b>table</b> from sampling data.|$|E
50|$|A common {{procedure}} for detecting DIF is the Mantel-Haenszel (MH) approach. The MH procedure is a chi-squared <b>contingency</b> <b>table</b> based approach which examines {{differences between the}} reference and focal groups on all items of the test, one by one. The ability continuum, defined by total test scores, is divided into k intervals which then serves {{as the basis for}} matching members of both groups. A 2 x 2 <b>contingency</b> <b>table</b> is used at each interval of k comparing both groups on an individual item. The rows of the <b>contingency</b> <b>table</b> correspond to group membership (reference or focal) while the columns correspond to correct or incorrect responses. The following table presents the general form for a single item at the kth ability interval.|$|E
30|$|Categorical data, {{analysed}} using Fisher's, Chi-Square and 2 × 2 <b>contingency</b> <b>tables.</b>|$|R
40|$|In this paper, {{we propose}} a new {{counting}} m x n <b>contingency</b> <b>tables.</b> Our scheme is a modificationof Dyer and Greenhill's schemefm two rowed <b>contingency</b> <b>tables</b> [5]. We can estimate {{not only the}} sizesof error, but also the sizesof the biasof the numberof tables obtained by our scheme, {{on the assumption that}} we have an approximate sampler...|$|R
40|$|The {{analysis}} of R×C <b>contingency</b> <b>tables</b> usually features {{a test for}} independence between row and column counts. Throughout the social sciences, {{the adequacy of the}} independence hypothesis is generally evaluated by the outcome of a classical p-value null-hypothesis significance test. Unfortunately, however, the classical p-value comes with a number of well-documented drawbacks. Here we outline an alternative, Bayes factor method to quantify the evidence for and against the hypothesis of independence in R×C <b>contingency</b> <b>tables.</b> First we describe different sampling models for <b>contingency</b> <b>tables</b> and provide the corresponding default Bayes factors as originally developed by Gunel and Dickey (Biometrika, 61 (3) : 545 – 557 (1974)). We then illustrate the properties and advantages of a Bayes factor {{analysis of}} <b>contingency</b> <b>tables</b> through simulations and practical examples. Computer code is available online and has been incorporated in the “BayesFactor” R package and the JASP program (jasp-stats. org) ...|$|R
5000|$|... #Caption: A <b>contingency</b> <b>table,</b> {{also called}} a crosstabulation, of {{possible}} test outcomes, {{and the associated}} equations for evaluating test accuracy.|$|E
5000|$|From these data, we {{can build}} a <b>contingency</b> <b>table,</b> where [...] and , and {{the total number of}} {{replicates}} is equal to [...]|$|E
50|$|Suppose that we {{have two}} variables, sex (male or female) and {{handedness}} (right or left handed). Further suppose that 100 individuals are randomly sampled from a very large population {{as part of a}} study of sex differences in handedness. A <b>contingency</b> <b>table</b> can be created to display the numbers of individuals who are male and right handed, male and left handed, female and right handed, and female and left handed. Such a <b>contingency</b> <b>table</b> is shown below.|$|E
40|$|We {{consider}} connected Markov chain for sampling 3 × 3 × K <b>contingency</b> <b>tables</b> having fixed two-dimensional marginal totals. Such sampling {{arises in}} performing various {{tests of the}} hypothesis of no three-factor interactions. Markov chain algorithm is a valuable tool for evaluating p values, especially for sparse data sets where large-sample theory does not work well. For constructing a connected Markov chain over high dimensional <b>contingency</b> <b>tables</b> with fixed marginals, algebraic algorithms were proposed by Diaconis and Sturmfels (1998). Their algorithms involve computations in polynomial rings using Gröbner bases. However, algorithms based on Gröbner bases do not incorporate symmetry among variables and are very time consuming when the size of <b>contingency</b> <b>tables</b> is large. We construct a minimal basis for connected Markov chain over 3 × 3 × K <b>contingency</b> <b>tables.</b> Some numerical examples are also given to illustrate the practicality of our algorithms...|$|R
40|$|We {{present a}} {{comprehensive}} study of graphical log-linear models for <b>contingency</b> <b>tables.</b> High dimensional <b>contingency</b> <b>tables</b> arise in many areas such as computational biology, collection of survey and census data and others. Analysis of <b>contingency</b> <b>tables</b> involving several factors or categorical variables is very hard. To determine interactions among various factors, graphical and decomposable log-linear models are preferred. First, we explore connections between the conditional independence in probability and graphs; thereafter we provide a few illustrations to describe how graphical log-linear model are useful to interpret the conditional independences between factors. We also discuss the problem of estimation and model selection in decomposable models...|$|R
40|$|Abstract. We {{study the}} {{geometric}} {{structure of the}} statistical mod-els for two-by-two <b>contingency</b> <b>tables.</b> One or two odds ratios are ¯xed and the corresponding models are {{shown to be a}} portion of a ruled quadratic surface or a segment. Some pointers to the general case of two-way <b>contingency</b> <b>tables</b> are also given and an application to case-control studies is presented. 1...|$|R
50|$|When {{the test}} is applied to a <b>contingency</b> <b>table</b> {{containing}} two rows and two columns, {{the test is}} equivalent to a Z-test of proportions.|$|E
5000|$|... where ti is a row {{marginal}} {{total and}} ui a column marginal total in the <b>contingency</b> <b>table.</b> The z-score equivalent is then given by ...|$|E
50|$|The example {{above is}} the {{simplest}} kind of <b>contingency</b> <b>table,</b> {{a table in}} which each variable has only two levels; {{this is called a}} 2 × 2 <b>contingency</b> <b>table.</b> In principle, any number of rows and columns may be used. There may also be more than two variables, but higher order contingency tables are difficult to represent visually. The relation between ordinal variables, or between ordinal and categorical variables, may also be represented in contingency tables, although such a practice is rare.|$|E
30|$|After having amalgamated a {{group of}} <b>contingency</b> <b>tables,</b> we can define the Measure of Amalgamation.|$|R
40|$|Markov {{basis for}} {{statistical}} model of <b>contingency</b> <b>tables</b> gives {{a useful tool}} for performing the conditional test of the model via Markov chain Monte Carlo method. In this paper we derive explicit forms of Markov bases for change point models and block diagonal effect models, which are typical block-wise effect models of two-way <b>contingency</b> <b>tables,</b> and perform conditional tests with some real data sets. Comment: 16 page...|$|R
50|$|The {{iterative}} proportional fitting procedure essentially manipulates <b>contingency</b> <b>tables</b> {{to match}} altered joint distributions or marginal sums.|$|R
