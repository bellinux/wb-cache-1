31|185|Public
30|$|Therefore, the {{different}} changes in DC link voltages are selected as a <b>clustering</b> <b>principle,</b> e.g., PMSGs {{are divided into}} three groups at most: Group 1) chopper circuit conduction within the fault duration; Group 2) the DC link voltage continuously rising without reaching the chopper circuit threshold within the fault duration; Group 3) the DC link voltage fluctuating {{in the neighborhood of}} the normal value within the fault duration.|$|E
40|$|We first extend Hopfield {{networks}} to clustering bipartite graphs (words-to-document association) {{and show that}} the solution is the principal component analysis. We then generalize this via the min-max <b>clustering</b> <b>principle</b> into a self-aggregation networks which are composed of scaled PCA components via Hebb rule. Clustering amounts to an updating process where connections between different clusters are automatically suppressed while connections within same clusters are enhanced. This framework combines dimension reduction with clustering via neural networks and PCA. Self-aggregation networks can also improve information retrieval performance. Applications are presented...|$|E
40|$|Abstract. This paper {{proposes a}} two-step graph {{partitioning}} method to discover constrained clusters with an objective function {{that follows the}} well-known minmax <b>clustering</b> <b>principle.</b> Compared with traditional approaches, the proposed method has several advantages. Firstly, the objective function not only follows the theoretical min-max principle but also reflects certain practical requirements. Secondly, a new constraint is introduced and solved to suit more application needs while unconstrained methods can only control the number of produced clusters. Thirdly, the proposed method is general {{and can be used}} to solve other practical constraints. The experimental studies on word grouping and result visualization show very encouraging results. ...|$|E
40|$|The <b>cluster</b> {{decomposition}} <b>principle</b> is used {{to derive}} Ward-Takahashi-like identities in QCD. Comparison with published calculations of QCD vertices finds that these new identities are overconstrained Slavnov-Taylor identities, thus demonstrating by contradiction {{the failure of the}} <b>cluster</b> decomposition <b>principle</b> in the perturbative and far-infrared regimes...|$|R
40|$|International audienceIn this paper, {{we propose}} a {{collaborative}} framework that {{is able to}} solve multi-view and alternative clustering problems using some clustering ensemble and semi-supervised <b>clustering</b> <b>principles.</b> We provide a mechanism to control, via a information sharing model, different clustering algorithms to obtain consensus or alternative clustering solutions. The strong point is that our approach {{does not need to}} know which clustering algorithms to use nor their parameter...|$|R
40|$|The {{clustering}} {{of electricity}} customers {{might have an}} effective meaning if, and only if, it is verified by domain experts. Most of the previous studies on customer clustering, however, do not consider real applications, but only the structure of clusters. Therefore, {{there is no guarantee}} that the clustering results are applicable to real domains. In other words, the results might not coincide with those of domain experts. In this paper, we focus on formulating clusters that are applicable to real applications based on domain expert knowledge. More specifically, we try to define a distance between customers that generates clusters that are applicable to demand response applications. First, the k-sliding distance, which is a new distance between two electricity customers, is proposed for customer clustering. The effect of k-sliding distance is verified by expert knowledge. Second, a genetic programming framework is proposed to automatically determine a more improved distance measure. The distance measure generated by our framework can be considered as a reflection of the <b>clustering</b> <b>principles</b> of domain experts. The results of the genetic programming demonstrate the possibility of deriving <b>clustering</b> <b>principles...</b>|$|R
40|$|Abstract. In this paper, {{starting}} from {{a collection of}} training examples, we show how to produce a very compact set of classification rules. The induction idea is a <b>clustering</b> <b>principle</b> based on Kohonen’s self-organizing algorithms. The function to optimize in the aggregation of examples to become rules is a classificatory quality measure called impurity level, which was previously employed in our system called FAN. The rule conditions obtained in this way are densely populated areas in the attribute space. The main goal of our system, {{in addition to its}} accuracy, is the high quality of explanations that it can provide attached to the classification decisions. ...|$|E
40|$|Magnetic {{resonance}} imaging {{studies of the}} resting brain have recently revealed the existence of low-frequency fluctuations of the cerebral hemodynamics. It {{has been suggested that}} these fluctuations are linked to baseline neural activity, organized in functional networks. This paper presents a novel method for the detection of these resting-state networks from blood-oxygen level dependent signals, through their wavelet representation in the appropriate frequency range. A valley-seeking <b>clustering</b> <b>principle</b> is employed, requiring no a priori knowledge of the number of functional networks. The technique is applied to a data set acquired at rest and is shown to retrieve a number of identifiable functional networks. The method is proposed as an alternative to e. g. independent component analysis and exhibits an enhanced network separation capability and stability against noise...|$|E
40|$|In this paper, {{we take a}} {{new look}} at the problem of {{texturing}} surfaces {{so that they can be}} displayed layered over each other but remain clearly visible. Finding optimal textures that solve this problem is complex because of the perceptual interactions between the visual effects of parameters controlling texture generation. Instead of using controlled experiments to investigate this problem, we use a genetic algorithm based human-in-the-loop parameter space search to build a large database of human-rated textures. This database is then analyzed with a varity of datamining techniques, including <b>clustering,</b> <b>principle</b> component analysis, neural networks, and histogram analysis. We detail this analysis, concluding with a set of guidelines for building strong layered surface textures, and a display of a number of example textures...|$|E
50|$|Little Joe was an {{unmanned}} United States solid-fueled {{booster rocket}} used for eight launches from 1959 - 1960 from Wallops Island, Virginia {{to test the}} launch escape system and heat shield for Project Mercury capsules, {{as well as the}} name given to the test program using the booster. The first rocket designed solely for manned spacecraft qualifications, Little Joe {{was also one of the}} pioneer operational launch vehicles using the rocket <b>cluster</b> <b>principle.</b>|$|R
40|$|The {{article focuses}} on {{comparative}} analyses of the investment policy of the UAE and the IRI in African countries {{in a context of}} foreign policy activities. The author examines the main approaches and methods practiced by the OAE and the IRI investment bodies for achieving success in Africa in various spheres of economy. The special attention is given to <b>cluster</b> <b>principle</b> of diversification strategy, which ensured the transformation of the OAE from a weakly developed country fully dependent on the export of oil into a state with a diversified economy...|$|R
40|$|Outlying {{data can}} heavily {{influence}} standard clustering methods. At the same time, <b>clustering</b> <b>principles</b> {{can be useful}} when robustifying statistical procedures. These two reasons motivate the development of feasible robust model-based clustering approaches. With this in mind, an R package for performing non-hierarchical robust clustering, called tclust, is presented here. Instead of trying to “fit” noisy data, a proportion α of the most outlying observations is trimmed. The tclust package efficiently handles different cluster scatter constraints. Graphical exploratory tools are also provided to help the user make sensible choices for the trimming proportion {{as well as the}} number of clusters to search for...|$|R
40|$|A {{public key}} {{cryptosystem}} is an asymmetric cryptosystem where the key {{consist of a}} public key and a private key. As the public key is public (known to all), {{can be used to}} encrypt the messages. The private key, kept as secret, can be used to decrypt the messages by the owner. We describe a new public key cryptosystem based on logarithmic approach. PCLA with its simplicity, it’s very easier to create keys and can be use securely with low memory requirements. PCLA encryption and decryption use a mixing system suggested by logarithmic approach combined with a <b>clustering</b> <b>principle</b> based on elementary mathematical theory. The security of the PCLA cryptosystem comes from the interaction of the logarithmic mixing system with the independence of relatively prime integer’s p and q...|$|E
40|$|Abstract—Model {{selection}} in clustering requires (i) to specify a suitable <b>clustering</b> <b>principle</b> and (ii) {{to control the}} model order complexity by choosing an appropriate number of clus-ters depending on the noise level in the data. We advocate an information theoretic perspective where the uncertainty in the measurements quantizes the set of data partitionings and, thereby, induces uncertainty in the solution space of clusterings. A clustering model, which can tolerate {{a higher level of}} fluctuations in the measurements than alternative models, is considered to be superior provided that the clustering solution is equally informative. This tradeoff between informativeness and robustness is used as a model selection criterion. The requirement that data partitionings should generalize from one data set to an equally probable second data set gives rise to a new notion of structure induced information. I...|$|E
40|$|Figure 1 : Experimentally {{determined}} highly-rated layered surface texturing solutions. In this paper, {{we take a}} {{new look}} at the problem of texturing surfaces {{so that they can be}} displayed layered over each other but remain clearly visible. Finding optimal textures that solve this problem is complex because of the perceptual interactions between the visual effects of parameters controlling texture generation. Instead of using controlled experiments to investigate this problem, we use a genetic algorithm based human-in-the-loop parameter space search to build a large database of human-rated textures. This database is then analyzed with a variety of data-mining techniques, including <b>clustering,</b> <b>principle</b> component analysis, neural networks, and histogram analysis. We detail this analysis, concluding with a set of guidelines for building strong layered surface textures, and a display of a number of example textures...|$|E
40|$|In this paper, an {{efficient}} hierarchical clustering algorithm, suitable for large data sets is proposed for effective clustering and prototype selection for pattern classification. It is another simple and efficient technique which uses incremental <b>clustering</b> <b>principles</b> {{to generate a}} hierarchical structure for finding the subgroups/subclusters within each cluster. As an example, a two level clustering algorithm-`Leaders-Subleaders', {{an extension of the}} leader algorithm is presented. Classification accuracy (CA) obtained using the representatives generated by the Leaders-Subleaders method is found to be better than that of using leaders as representatives. Even if more number of prototypes are generated, classification time is less as only a part of the hierarchical structure is searche...|$|R
40|$|The article {{presents}} {{a tool to}} analyze the application of efficient algorithms of data mining, namely hierarchical clustering algorithms {{to be used in}} the analysis of geological data. It introduces a description of hierarchical <b>clustering</b> <b>principles</b> and methods for learning dependencies from geological data. The authors are using statistical formulation of algorithms to represent the most natural framework for learning from data. The geological data come from mining holes, and describe the structure of sedimental layers of vertical section of geological body. The analysis of such data is intended to give a basis for uniform description of lithological characteristics, and for the identification of them via formal methods...|$|R
30|$|Step 2 : Assign each sample {{into the}} {{neighboring}} <b>cluster</b> using the <b>principle</b> of minimum-in-cluster-distance.|$|R
40|$|The Chiu’s method which {{generates a}} Takagi-Sugeno Fuzzy Inference System (FIS) {{is a method}} of fuzzy rules extraction. The rules output is a linear {{function}} of inputs. In addition, these rules are not explicit for the expert. In this paper, we develop a method which generates Mamdani FIS, where the rules output is fuzzy. The method proceeds in two steps: first, it uses the subtractive <b>clustering</b> <b>principle</b> to estimate both the number of clusters and the initial locations of a cluster centers. Each obtained cluster corresponds to a Mamdani fuzzy rule. Then, it optimizes the fuzzy model parameters by applying a genetic algorithm. This method is illustrated on a traffic network management application. We suggest also a Mamdani fuzzy rules generation method, where the expert wants to classify the output variables in some fuzzy predefined classes...|$|E
40|$|The Chiu’s method which {{generates a}} Takagi-Sugeno Fuzzy Inference System (FIS) {{is a method}} of fuzzy rules extraction. The rules output is a linear {{function}} of inputs. Those rules are not explicit for the expert. This paper proposes a new method to generate Mamdani FIS, where the rules output is fuzzy. The method proceeds in two steps. The first step consists in using the subtractive <b>clustering</b> <b>principle</b> to estimate the number of clusters and the initial locations of cluster centers, each obtained cluster corresponds to a Mamdani fuzzy rule. The second step optimizes the fuzzy model parameters by using a genetic algorithm. This method has been implemented and tested {{in the framework of}} a traffic network management application. This method has been extended to generation of Mamdani fuzzy rules when fuzzy classes can be predefined by the expert...|$|E
40|$|Abstract—In this paper, image {{segmentation}} of brain magnetic resonance (MR) image is addressed in an unsupervised framework. We propose a novel method considering the hidden Markov random field model (HMRF) {{to model the}} image class labels, which {{takes into account the}} mutual influences of neighbouring sites formulated on the basis of fuzzy <b>clustering</b> <b>principle.</b> By introducing the effective means to incorporate the explicit assumptions of the HMRF model into fuzzy clustering procedure, an efficient fuzzy clustering- type treatment is yielded. This combines the benefits from the spatial coherency modelling capabilities of the HMRF model, and the enhanced flexibility obtained by the fuzzy clustering algorithm, i. e. fuzzy c-means algorithm (FCM). The proposed HMRF-FCM segmentation framework is validated with noisy synthesis as well as brain MR images. We experimentally demonstrate the superiority of the proposed approach over the existing HMRF-EM framework applied to brain MR {{image segmentation}}...|$|E
40|$|Abstract — This paper {{addresses}} {{the problem of}} evaluating and estimating the mechanical robustness of footholds for legged robots in unstructured terrain. In contrast to approaches that rely on human expert knowledge or human defined criteria to identify appropriate footholds, our method uses the robot itself to assess whether a certain foothold is adequate or not. To this end, one of the robot’s legs is employed to haptically explore an unknown foothold. The robustness of the foothold is defined by a simple metric {{as a function of}} the achievable ground reaction forces. This haptic feedback is associated with the foothold shape to estimate the robustness of untouched footholds. The underlying shape <b>clustering</b> <b>principles</b> are tested on synthetic data and in hardware experiments using a single-leg testbed. I...|$|R
40|$|Sixty-nine {{different}} stands within 15 different previously managed {{communities were}} sampled {{in what is}} now the Turkey Hill Wilderness of the Angelina National Forest. Within each stand, 0. 04 ha plots were randomly located, and the height, diameter, crown class and species of each tree recorded. Soil samples were collected in two locations within each plot, and the soil type confirmed at plot center. Soil samples were analyzed for selected soil chemical and physical properties. Due to the low occurrence (less than 4 times) of some communities, only six community types were analyzed Analysis of the vegetation and soil data using <b>Cluster,</b> <b>Principle</b> Component and Pearson’s Correlation Analyses showed some relationships between vegetation communities, species and the soil properties. This information may be useful toward development of better understanding about plant community relationships...|$|R
40|$|This {{introduction}} to the R package tclust is a (slightly) modified version of Fritz et al. (2012), published in the Journal of Statistical Software. Outlying data can heavily influence standard clustering methods. At the same time, <b>clustering</b> <b>principles</b> can be useful when robustifying statistical procedures. These two reasons motivate the development of feasible robust model-based clustering approaches. With this in mind, an R package for performing non-hierarchical robust clustering, called tclust, is presented here. Instead of trying to “fit ” noisy data, a proportion α of the most outlying observations is trimmed. The tclust package efficiently handles different cluster scatter constraints. Graphical exploratory tools are also provided to help the user make sensible choices for the trimming proportion {{as well as the}} number of clusters to search for...|$|R
40|$|Abstract- In {{this paper}} {{we present a}} method for {{unsupervised}} image clustering and B-spline curve interpolation for remove the noise. Images are clustered such that the mutual information between the clusters and the image content is maximally preserved. The <b>clustering</b> <b>principle</b> is applied to both discrete and continuous image representations, using Gaussian densities. We find that whole-brain analysis in this manner allows automatic classification of images based on Disease if the whole brain is included, If a sequence of planar points and vectors are given then a free-form curve can calculated which interpolates the points and has the given tangent vectors in these points. Our method gives a fast interpolation of these data using extra control points. Then we provide a method which allows to interpolate {{the same set of}} data without any predefined order of the points, i. e. a set of scattered points with the vectors...|$|E
40|$|An {{important}} {{application of}} graph partitioning is data clustering using a graph model [...] the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering. Here we propose a new algorithm for graph partition with an objective function {{that follows the}} min-max <b>clustering</b> <b>principle.</b> The relaxed version of the optimization of the min-max cut objective function leads to the Fiedler vector in spectral graph partition. Theoretical analyses of min-max cut indicate that it leads to balanced partitions, and lower bonds are derived. The min-max cut algorithm is tested on newsgroup datasets and is found to outperform other current popular partitioning/clustering methods. The linkage-based re nements in the algorithm further {{improve the quality of}} clustering substantially. We also demonstrate that the linearized search order based on linkage differential is better than that based on the Fiedler vector, providing another effective partition method...|$|E
40|$|How to {{discover}} hotspot in the Internet public opinions effectively {{is a hot}} research field for the researchers related which {{plays a key role}} for governments and corporations to find useful information from mass data in the Internet. An improved -means algorithm for hotspot discovery in internet public opinions is presented based on the analysis of existing defects and calculation principle of original -means algorithm. First, some new methods are designed to preprocess website texts, select and express the characteristics of website texts, and define the similarity between two website texts, respectively. Second, <b>clustering</b> <b>principle</b> and the method of initial classification centers selection are analyzed and improved in order to overcome the limitations of original -means algorithm. Finally, the experimental results verify that the improved algorithm can improve the clustering stability and classification accuracy of hotspot discovery in internet public opinions when used in practice...|$|E
40|$|The {{family and}} motif databases, PROSITE, PRINTS, Pfam and ProDom, have been {{integrated}} into a powerful resource for protein secondary annotation. As of June 2000, InterPro had processed 384 572 proteins in SWISS-PROT and TrEMBL. Because the contributing databases have different <b>clustering</b> <b>principles</b> and scoring sensitivities, the combined assignments compliment each other for grouping protein families and delineating domains. The graphic displays of all matches above the scoring thresholds enables judgements {{to be made on}} the concordances or differences between the assignments. The website links can be used to analyse novel sequences and for queries across the proteomes of 32 organisms, including the partial human set, by domain and/or protein family. An analysis of selected HtrA/DegQ proteases demonstrates the utility of this website for detailed comparative genomics. Further information on the project can be found at th...|$|R
40|$|We {{begin by}} explicating a recent {{proof of the}} <b>cluster</b> {{decomposition}} <b>principle</b> in AdS ≥ 4 from the CFT ≥ 3 bootstrap. The CFT argument also computes the leading interactions between distant objects in AdS ≥ 4, and we confirm the universal agreement between the CFT bootstrap and AdS gravity in the semi-classical limit...|$|R
40|$|This {{bachelor}} thesis {{deals with}} clustering objects with the MCluster-Miner procedure of the LISp-Miner system. The first {{aim of this}} bachelor thesis is clustering objects with the mentioned pro-cedure and analyzing its possible usage on different datasets. To achieve this goal, the procedure was applied on six different datasets. The secong aim of this thesis is to analyze and compare implemented algorithms, similarity measures and to propose recommendations for clustering parameters. To achieve this goal, the available algorithms and similarity measures are compared based on achieved results (the quality of distribution objects into clusters, the time of clustering task, the number of attributes used for clustering). Based on these comparisons, the recommen-dations for clustering parameters are proposed. The benefits of this thesis are these recommenda-tions, comparisons of available algorithms and similarity measures, summary of actual state (da-ted to May 2017) of the MCluster-Miner module and showing the possibility of displaying results of clustering task at the interactive analysis of geodata. The theoretical part comprises {{the description of the}} LISp-Miner system, basic <b>clustering</b> <b>principles,</b> <b>clustering</b> methods and similari-ty measures used by the GUHA-procedure MCluster-Miner, and the MCluster-Miner module. In the practical part the MCluster-Miner procedure is being applied on six different datasets and the achieved results are summarized there...|$|R
40|$|In this paper, {{a method}} for extracting {{fundamental}} frequencies (F 0 s) from single channel input signal of concurrent sounds is described. By considering that an observed spectral density distribution is a statistical distribution of (imaginary) micro-energies, we attempt to classify them into each sound {{by the use of}} <b>clustering</b> <b>principle.</b> We call this approach a "Harmonic Clustering. " One of the formulation of this clustering can be expressed in same way as a maximum likelihood of Gaussian mixture model (GMM) using EM algorithm. Our algorithm enables to estimate not only F 0 s but also a number and each spectral envelope of underlying harmonic structure on the basis of an information criterion. It operates without restriction of a number of mixed sounds and a variety of sound sources, and extracts F 0 s as accurate values with spectral domain procedures. Experimental results showed high performance of our algorithm...|$|E
40|$|Model {{selection}} in clustering requires (i) to specify a suitable <b>clustering</b> <b>principle</b> and (ii) {{to control the}} model order complexity by choosing an appropriate number of clusters depending on the noise level in the data. We advocate an information theoretic perspective where the uncertainty in the measurements quantizes the set of data partitionings and, thereby, induces uncertainty in the solution space of clusterings. A clustering model, which can tolerate {{a higher level of}} fluctuations in the measurements than alternative models, is considered to be superior provided that the clustering solution is equally informative. This tradeoff between informativeness and robustness is used as a model selection criterion. The requirement that data partitionings should generalize from one data set to an equally probable second data set gives rise to a new notion of structure induced information. Comment: 9 pages, 2 figures, International Symposium on Information Theory 2010 (ISIT 10 E-Mo- 4. 2), June 13 - 18 in Austin, TX...|$|E
40|$|Hidden Markov random field (HMRF) {{models have}} been widely used for image segmentation, as they appear {{naturally}} in problems where a spatially constrained clustering scheme, taking into account the mutual influences of neighboring sites, is asked for. Fuzzy c-means (FCM) clustering has also been successfully applied in several image segmentation applications. In this paper, we combine the benefits of these two approaches, by proposing a novel treatment of HMRF models, formulated {{on the basis of a}} fuzzy <b>clustering</b> <b>principle.</b> We approach the HMRF model treatment problem as an FCM-type clustering problem, effected by introducing the explicit assumptions of the HMRF model into the fuzzy clustering procedure. Our approach utilizes a fuzzy objective function regularized by Kullback [...] Leibler divergence information, and is facilitated by application of a mean-field-like approximation of the MRF prior. We experimentally demonstrate the superiority of the proposed approach over competing methodologies, considering a series of synthetic and real-world image segmentation applications. © 2008 IEEE...|$|E
40|$|M. L. Walker Abstract: We utilise a non-local gauge {{transform}} which {{renders the}} entire action of SUSY QED invariant and respects the SUSY algebra modulo the gauge-fixing condition, to derive two- and three-point ghost-free SUSY Ward identities in SUSY QED. We use the <b>cluster</b> decomposition <b>principle</b> {{to find the}} Green’s function Ward identities and then takes linea...|$|R
40|$|Electrical {{load pattern}} {{clustering}} provides useful {{information on how}} to partition the customers {{on the basis of the}} shape of their actual consumption. The results of the clustering procedure are then used to group the customers into classes by identifying a suitable set of attributes describing each class in a non-overlapping way. Starting from an initial set of centroids, it is possible to construct a clustering algorithm to upgrade the clustering results by refining the initial centroids. The Electrical Pattern Ant Colony Clustering (EPACC) method recently introduced by the authors, based on ant colony <b>clustering</b> <b>principles,</b> has been shown to outperform the classical k-means for electrical load pattern clustering based on an initial centroid model. This paper illustrates the genesis of the EPACC algorithm by providing additional information on how the method parameters have been chosen and set up. Examples are taken from a case study application with actual load patterns...|$|R
40|$|This paper {{proposes a}} novel {{approach}} for processing digital mammograms to detect micro-calcifications. They may be so small that they are almost undetectable visually, {{but it could be}} indicators of a possible malignancy. An analysis algorithm based on optical holographic property of images and <b>clustering</b> <b>principles</b> are proposed to detect the micro-calcifications. This process consists of three stages. In the first stage the mammographic patterns are subjected to optical holographic analysis. The resulting images are passed to the second stage, in which morphological operations are performed. The third stage detects the malignant portions of the mammographic pattern using unsupervised texture classification by extracting laws features. Texture classification is an important image processing task with a broad application range. Many different techniques for texture classification have been explored. This paper explores the unsupervised classifications of digital mammograms using K-means and Fuzzy C-means approaches. Results show that the proposed techniques detect the malignant portions of the breast very well thus enabling earlier detection of tumor...|$|R
