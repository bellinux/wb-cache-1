2|8|Public
50|$|The aspen, or hardwood, line {{is similar}} to the {{softwood}} line except that logs are debarked in a Nicholson mechanical ring debarker. Wood is chipped separately and stored in a 400-cord <b>chip</b> <b>bin.</b>|$|E
40|$|Abstract This thesis {{describes}} efforts made on {{the development}} of an existing catalytic incinerator. The development work, called process characterization, consists of four general parts. These are the development of measurement methodology, the studying of construction materials, the selection of suitable catalysts and the testing of the effects of process operation conditions. The two application areas for catalytic incineration considered in this thesis are solvent emission abatement (VOC, volatile organic compounds) and <b>chip</b> <b>bin</b> emission abatement (SVOC, sulphur-containing volatile organic compounds). As a baseline, the process characterization is started with the development of measurement methodology. In general, the methodology will decrease costs and simplify the carrying out of the actual measurements and thereby make the measurement time more effective. In the methodology it is proposed that continuous total concentration measurement should be used in connection with qualitative sampling to obtain reliable measurement data. The selection of suitable construction materials for the application is very important. As shown in this thesis, the end conversions in solvent emission abatement may even be improved through the selection of the proper construction materials. In <b>chip</b> <b>bin</b> emission abatement, the problem arises from corrosive oxidation products that set limits on the construction materials used as well as on oxidation conditions. Catalyst selection is based on the following catalytic properties: activity, selectivity and durability. These catalytic properties are studied either at the laboratory or on an industrial scale. The catalytic materials tested are Pt, Pd, Pt-Pd, Cu-Mn oxides, MnO 2 -MgO, CuxMg(1 -x) Cr 2 O 4 and CuxCr 2 O 4. The most important selection criteria in solvent emission abatement are proposed to be activity and selectivity. In the case of chip bin-SVOC-abatement, these are selectivity and durability. Based on these criteria, catalysts containing Cu-Mn oxides and Pt were demonstrated to be the best catalysts in VOC oxidation, and catalyst containing MnO 2 -MgO was shown to be best catalyst in SVOC oxidation. A study on the effect of process operation parameters (temperature, concentration and gas hourly space velocity (GHSV)) and moisture was carried out with the aid of factorial design. In VOC (n-butyl acetate) oxidation, the most influential process parameter was GHSV, which decreased the end conversion when it was increased. In SVOC (DMDS) oxidation, the effect of temperature was most significant. The end conversions increased as the temperature increased. Moisture slightly decreased the formation of by-products in n-butyl acetate oxidation. In DMDS oxidation, moisture slightly increased the end conversions at a lower temperature level (300 °C). At the end of the thesis, these process parameters are also discussed {{from the standpoint of the}} catalysts' activity, selectivity and durability. Finally, proposals for process improvements are suggested...|$|E
50|$|The GeForce GTX 280 and GTX 260 {{are based}} on the same {{processor}} core. During the manufacturing process, GTX <b>chips</b> are <b>binned</b> and separated through defect testing of the core's logic functionality. Those that fail to meet the GTX 280 hardware specification are re-tested and binned as GTX 260 (which is specified with fewer stream processors, less ROPs and a narrower memory bus).|$|R
50|$|Fir is {{fed through}} the {{softwood}} line and debarked in a 12-foot by 68-foot debarking drum. Equipment {{has been added}} for log washing and deicing prior to the drum to facilitate debarking. After debarking, the wood flows through a log inspection and sorting system where individual sticks may be recycled to the drum. Properly debarked logs are directed to a 12-knife Carthage chipper. Chips are screened and conveyed to <b>chip</b> storage <b>bins.</b>|$|R
40|$|As {{transistor}} feature sizes {{continue to}} shrink into the sub- 90 nm range and beyond, {{the effects of}} process variations on critical path delay have amplified. A common concept to remedy the effects of variation is speed-binning, by which chips from a single batch are rated by a discrete range of frequencies. In this paper, we argue that under these conditions, architectural optimizations should consider their effect on the “batch ” of microprocessors rather than aiming at increasing {{the performance of a}} single processor. We first show that the critical paths are mostly determined by the level 1 data caches on a set of manufactured microprocessors. Then, we propose three new microarchitectural techniques aimed at masking the effects of process variations on level 1 caches. The first two techniques allow individual highlatency cache lines spanning single or multiple sets to be disabled at the post-manufacture testing stage. The third approach introduces a small substitute cache associated with each cache way to replicate the data elements stored in the high latency lines. Our new schemes can be effectively used to boost up the overall chip yield and also shift the <b>chip</b> <b>binning</b> distribution towards higher frequencies. To make a quantitative comparison between the different schemes, we first define a metric called batchperformance that takes into account the chip yield and frequency of <b>chips</b> in each <b>bin.</b> We then analyze our proposed schemes and show that the resizing schemes and the substitute cache can increase the batch-performance by as much as 5. 8 % and 11. 6 %, respectively...|$|R
40|$|Abstract—As {{transistor}} feature sizes {{continue to}} shrink into the sub- 90 nm range and beyond, {{the effects of}} process variations on critical path delay and chip yields have amplified. A common concept to remedy the effects of variation is speed-binning, by which chips from a single batch are rated by a discrete range of frequencies and sold at different prices. In this paper, we discuss strategies to modify the number of <b>chips</b> in different <b>bins</b> and hence enhance the profits obtained from them. Particularly, we propose a scheme that introduces a small Substitute Cache associated with each cache way to replicate the data elements that will be stored in the high latency lines. Assuming a fixed pricing model, this method increases the revenue {{by as much as}} 13. 8 % without any impact on the performance of the chips...|$|R
40|$|Abstract — Determination {{of maximum}} {{operating}} frequencies (Fmax) during manufacturing test at different operating voltages is required to: (a) to ensure that, for a Dynamic Voltage and Frequency Scaling (DVFS) system, the adaptation hardware actually applies the correct operating frequency corresponding to a scaled supply and (b) to sort chips in different voltage-frequency (V-Fmax) <b>bins,</b> so that <b>chips</b> at different <b>bins</b> {{can be used}} for different applications. Existing speed binning approach requires extensive delay testing at all operating points with all possible frequencies, which increases test cost and test time significantly. In this paper, we propose a low-overhead solution for characterizing Fmax of a circuit at different operating voltages that can eliminate the complex and expensive Fmax calibration at multiple voltage points. The basic idea is to choose a small set of representative paths in a circuit based on their voltage sensitivity and dynamically configuring them into ring oscillator to compute the Fmax. The proposed calibration mechanism is all-digital, robust to process variations, reasonably accurate (average 2. 8 % error) and incorporates minimal hardware overhead (average 1. 7 % delay, 3. 5 % area and 0. 28 % power overhead) ...|$|R
40|$|Abstract — Traditional {{methods for}} testing {{integrated}} circuits will reject circuits {{with a single}} point failure regardless of the functionality of the chip {{from the perspective of}} the ultimate application. As the size of integrated circuits increase, the probability of such a single point failure increases resulting in a trend of decreasing yield with increasing circuit size. In many applications, the circuits perform processing that is inherently robust to errors (e. g., signal processing, compression, error correction) and therefore many of the rejected chips function acceptably well. This motivates the area of error tolerant circuit design and test in which one seeks to characterize acceptable errors, design architectures most robust to errors, and develop automated tests to accept or <b>bin</b> <b>chips</b> according to functionality. In this paper we describe a hardware testbed developed to investigate the error tolerant properties of an iterative decoder. Specifically, an FPGA-based hardware simulator for a systematic repeat accumulate code was designed. This design includes the capability to insert logic errors at various locations in the data path and to vary the level of parallelism. The testbed provides rapid evaluation of the effects of many single point circuit errors. Analyzing the architecture, we characterize errors as either catastrophic or non-catastrophic and further predict the loss of non-catastrophic using modified SNR-threshold prediction methods (e. g., EXIT charts). These quasi-analytical predictions correlate well with the collected experimental data from the testbed. I...|$|R
40|$|As {{manufacturing}} technology scales down to 65 nm and below, fabricated chips {{are becoming increasingly}} vulnerable to timing-related defects and parametric failures. This leads to increased yield loss and escape and reduced reliability. Scan-based at-speed delay test is becoming an indispensable method for nanometer technology designs to target the timing-related failures. However, the delay test patterns that are generally generated cannot meet the specific requirements raised from manufacturing test experience. In this work, we develop techniques to target several practical test pattern issues. The first is Small-delay defect (SDD) pattern generation using timing-aware method or N-detect method is extremely time-consuming and requires very large memory to store the test data. This is not affordable for most IC companies thus {{both of them are}} rarely used in practice. The second issue is the widely used transition delay test patterns which are generated using 1 -detect method have very low SDD coverage which is not acceptable to guarantee low DPM level. A pattern set that has a compact size as well as satisfying SDD coverage is highly required. The third issue is comparing with functional test pattern, delay test pattern (structural test pattern) cannot provide a reliable result in the speed binning process because of inaccurate mimicking functional running status. It is highly possible that the incorrectly <b>binned</b> <b>chips</b> would fail in the field, thus increase customer return and reduced profitability. The performance mismatch between delay test and functional test needs to be reduced. Besides, In order to solve above problems, we propose novel testing techniques including: (1) TDF pattern selection and pattern evaluation using critical faults; (2) Compact TDF pattern generation to maximize SDD coverage; (3) Worst-case path-based delay test generation considering power supply noise (PSN). The developed procedures help improve yield, reliability and reduce the test cost. Based on the research investigation and development, test time reduction, speed binning delay test generation, and physical diagnosis are also implemented in the industrial work derived from the research topics within this program...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. Issued also on microfiche from Lange Micrographics. In a mixed-product semiconductor foundry, optimization of product binning, packaging, product quantity control and rapid problem diagnosis are all critical to economic success. Binning is the placement of chips into several different specification categories. Binning has traditionally been done on the basis of performance (e. g.,speed, operating range). For maximum profit, it must be accurately determined which specification <b>bin</b> a <b>chip</b> ties in. The integrated circuit pack-age makes up a large fraction of total product cost, and has a major influence on product performance and reliability. To minimize packaging costs, packaging has to be selected according to how a <b>chip</b> will be <b>binned.</b> Performance prediction is also important for product control and when the chip is to be packaged away from the foundry, such as when it will be used in Multi-Chip Module (MCM). The difficulty in achieving these goals is that there is limited data available at the time each decision must be made. For example, performance banning and package selection must be done after the water-level electrical test, but the water test is not a thorough performance test. The current practice is to accept some loss of profits due to imperfect predictions that result in under-specified and over-packaged products, additional failures in final test, a mismatch of product supply and demand, or additional yield loss. The IC performance prediction system (IPPS) proposed in this research will use primarily wafer-level functional and parametric electrical test data, supplemented with in-line and in-situ data to make performance predictions. Based on the waterlevel parametric test, we will predict chip performance in order to select the appropriate package. Predictions that fall outside acceptable limits will be used for process diagnosis purposes. We will use the predicted performance to determine additional wafer starts necessary to meet product demand. This can be coupled with equipment utilization to predict and minimize fabrication costs...|$|R

