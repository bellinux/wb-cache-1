781|1955|Public
2500|$|The {{integration}} of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At {{the same time}} as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving. [...] One of the most influential languages in this research was the KL-ONE language of the mid 80's. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation. KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide <b>consistency</b> <b>checking</b> on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).|$|E
5000|$|Semantic diagram editor, {{real-time}} model <b>consistency</b> <b>checking</b> ...|$|E
5000|$|Elicitation {{from various}} sources (users, {{interfaces}} to other systems), specification, and <b>consistency</b> <b>checking</b> ...|$|E
30|$|Step 4 : <b>Consistency</b> <b>check</b> (CR).|$|R
30|$|Step 5 Balancing the {{database}} and <b>consistency</b> <b>checks.</b>|$|R
50|$|When opening {{existing}} archives, {{a strict}} <b>consistency</b> <b>check</b> can be requested.|$|R
50|$|MPDS4 {{includes}} {{hard and}} soft clash detection, {{which can be}} applied to a whole project, to separate systems or between selected components. <b>Consistency</b> <b>checking</b> tools allow users to check work against specific design rules. Results can be passed to customisable reports, and components used in a design are automatically included in parts lists.|$|E
50|$|Writers of lint-like tools have {{continued}} to improve the range of suspicious constructs that they detect. Modern tools perform forms of analysis that many optimizing compilers typically don't do, such as cross-module <b>consistency</b> <b>checking,</b> checking that the code will be portable to other compilers, and supporting annotations that specify intended behavior or properties of code.|$|E
5000|$|UMBEL {{is written}} in the Semantic Web {{languages}} of SKOS and OWL 2. It is a class structure used in Linked Data, along with OpenCyc, YAGO, and the DBpedia ontology. Besides data integration, UMBEL {{has been used to}} aid concept search, concept definitions, query ranking, ontology integration, and ontology <b>consistency</b> <b>checking.</b> It has also been used to build large ontologies [...] and for online question answering systems.|$|E
40|$|The {{role of the}} {{viewpoint}} concept, known from software engineering, is of increasing importance for modeling in production automation. While most often only a syntactical <b>consistency</b> <b>check</b> is performed for viewpoint oriented modeling techniques, this work examines semantical consistency. Semantical modeling and model based <b>consistency</b> <b>checks</b> are presented along {{an example of a}} machining tool robot...|$|R
3000|$|... [35] {{integrates}} a third {{camera to}} conduct projection <b>consistency</b> <b>check</b> into the probabilistic approach.|$|R
40|$|We present view transactions, a {{model for}} relaxed <b>consistency</b> <b>checks</b> in {{software}} transactional memory (STM). View transactions always operate on a consistent snapshot of memory but may commit in a different snapshot. They are therefore simpler to reason about, provide opacity and maintain composability. In addition, view transactions avoid many of the overheads associated with previous approaches for relaxing <b>consistency</b> <b>checks.</b> ...|$|R
5000|$|There are {{numerous}} other standards and formats using [...] "zip" [...] {{as part of}} their name. For example, zip is distinct from gzip, and the latter is defined in an IETF RFC (RFC 1952). Both zip and gzip primarily use the DEFLATE algorithm for compression. Likewise, the ZLIB format (IETF RFC 1950) also uses the DEFLATE compression algorithm, but specifies different headers for error and <b>consistency</b> <b>checking.</b> Other common, similarly named formats and programs with different native formats include 7-Zip, bzip2, and rzip.|$|E
50|$|As part of ILM, SAP IQ {{allows users}} to create {{multiple}} user DBSpaces (logical units of storage/containers for database objects) for organizing data. This {{can be used to}} separate structured or unstructured data, group it together according to age and value, or to partition table data. DBSpaces can also be marked as read-only to enable one-time <b>consistency</b> <b>checking</b> and back-up. Another application of ILM is the ability to partition tables, and place moving portions along the storage fabric and backup capabilities; this enables a storage management process where data cycles through tiered storage, moving from faster more expensive storage to slower, cheaper storage as it ages, partitioning data according to value.|$|E
50|$|Peercoin is {{designed}} {{so that it will}} theoretically experience a steady 1% inflation per year, yielding an unlimited number of coins. This is a combined result of the proof-of-stake minting process, and scaling of mining difficulty with popularity. Although Peercoin technically has a cap of 2 billion coins, it is only for <b>consistency</b> <b>checking,</b> and the cap is unlikely to be reached for the foreseeable future. If the cap were to be reached, it could easily be raised, hence for all practical purposes Peercoin can be considered to have inflation of 1% per year, with a limitless money supply. This was partially designed to address the growing population.|$|E
40|$|Every Argo {{data file}} {{submitted}} by a DAC for distribution on the GDAC has its format and data <b>consistency</b> <b>checked</b> by the Argo FileChecker. Two {{types of checks}} are applied: 1. Format checks. Ensures the file formats match the Argo standards precisely. 2. Data <b>consistency</b> <b>checks.</b> Additional data <b>consistency</b> <b>checks</b> are performed on a file after it passes the format checks. These checks do not duplicate any of the quality control checks performed elsewhere. These checks {{can be thought of}} as “sanity checks” to ensure that the data are consistent with each other. The data <b>consistency</b> <b>checks</b> enforce data standards and ensure that certain data values are reasonable and/or consistent with other information in the files. Examples of the “data standard” checks are the “mandatory parameters” defined for meta-data files and the technical parameter names in technical data files. Files with format or consistency errors are rejected by the GDAC and are not distributed. Less serious problems will generate warnings and the file will still be distributed on the GDAC. Reference Tables and Data Standards: Many of the <b>consistency</b> <b>checks</b> involve comparing the data to the published reference tables and data standards. These tables are documented in the User’s Manual. (The FileChecker implements “text versions” of these tables. ...|$|R
5000|$|... it is a {{comprehensive}} <b>consistency</b> <b>check</b> {{as it should be}} able to reproduce its own object code.|$|R
50|$|Lustre 2.6, {{released}} in July 2014, {{was a more}} modest release feature wise, adding LFSCK functionality to do local <b>consistency</b> <b>checks</b> on the OST as well as <b>consistency</b> <b>checks</b> between MDT and OST objects. Single-client IO performance was improved over the previous releases. This release also added a preview of DNE striped directories, allowing single large directories to be stored on multiple MDTs to improve performance and scalability.|$|R
50|$|The {{integration}} of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At {{the same time}} as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving. One of the most influential languages in this research was the KL-ONE language of the mid 80's. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation. KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide <b>consistency</b> <b>checking</b> on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).|$|E
40|$|<b>Consistency</b> <b>checking</b> of {{cardinal}} directions {{is one of}} {{the important}} problems in qualitative spatial reasoning. This paper presents a graph model to visually represent direction specifications. In the model, nodes represent regions occupied by objects, and directed edges indicate direction relationships between objects. This graph model can be applied not only to <b>consistency</b> <b>checking,</b> but also to general spatial reasoning. Based on this model, we present an efficient algorithm that performs <b>consistency</b> <b>checking</b> on a set of definitive direction specifications by analyzing the connectivity of the participating nodes. The <b>consistency</b> <b>checking</b> algorithm is performed in O(n 4) time. 1...|$|E
30|$|Local-based <b>consistency</b> <b>checking.</b>|$|E
3000|$|... -locators. As the {{distance}} <b>consistency</b> <b>check</b> needs as least three locators, if the sensor identifies {{no less than}} two [...]...|$|R
40|$|Many {{programs}} need {{to access}} {{data in a}} relational database. This is usually done by means of queries written in SQL. Although the language SQL is declarative, certain runtime errors are possible. Since the occurrence of these errors depend on the data, they are not easily found during testing. The question whether a query is safe {{can be reduced to}} a <b>consistency</b> <b>check.</b> It is well known that consistency is in general undecidable, and that this applies also to SQL queries. However, in this paper, we propose a <b>consistency</b> <b>check</b> that can handle a surprisingly large subset of SQL (it uses Skolemization with sorted Skolem functions, and a few other tricks). This <b>consistency</b> <b>check</b> is also the basis for generating other semantic warnings. Furthermore, {{it can be used to}} generate test data for SQL queries...|$|R
50|$|The second {{modification}} introduces <b>consistency</b> <b>checks</b> in {{the control}} flow, so that consistency between the two copies of each variable is verified.|$|R
30|$|<b>Consistency</b> <b>checking</b> {{for each}} MIC set.|$|E
40|$|International audienceWe {{study the}} problem of <b>consistency</b> <b>checking</b> for {{constraint}} networks over combined qualitative formalisms. We propose a framework which encompasses loose integrations and a form of spatio-temporal reasoning. In particular, we identify sufficient conditions ensuring the polynomiality of <b>consistency</b> <b>checking,</b> and we use them to find tractable subclasses...|$|E
40|$|This paper {{describes}} {{a class of}} formal analysis called <b>consistency</b> <b>checking</b> that mechanically checks requirements speci cations, expressed in the SCR tabular notation, for application-independent properties. Properties include domain coverage, type correctness, and determinism. As background, the SCR notation for specifying requirements is reviewed. A formal requirements model describing {{the meaning of the}} SCR notation is summarized, and consistency checks derived from the formal model are described. The results of experiments to evaluate the utility of automated <b>consistency</b> <b>checking</b> are presented. Where <b>consistency</b> <b>checking</b> of requirements ts in the software development process is discussed. ...|$|E
40|$|Abstract. This paper {{studies the}} {{root-cause}} analysis {{based on the}} time delays among various signals, for reducing nuisance alarms in modern industrial systems including power grids. Time delays are estimated via the revised nearest neighbor imputation method, and are validated via the subsequent <b>consistency</b> <b>check.</b> Numerical examples including the IEEE 5 -node system as a prototype power grid are provided to demonstrate {{the effectiveness of the}} proposed time delay estimation method and the subsequent <b>consistency</b> <b>check...</b>|$|R
50|$|Receiver {{autonomous}} integrity monitoring (RAIM) provides integrity {{monitoring of}} GPS for aviation applications. In order for a GPS receiver to perform RAIM or fault detection (FD) function, a minimum of five visible satellites with satisfactory geometry must be visible to it. RAIM has various kind of implementations; one of them performs <b>consistency</b> <b>checks</b> between all position solutions obtained with various subsets of the visible satellites. The receiver provides an alert to the pilot if the <b>consistency</b> <b>checks</b> fail.|$|R
30|$|Once the {{robustness}} of our reference {{model has been}} verified through this <b>consistency</b> <b>check,</b> we proceed to {{the identification of the}} key parameters.|$|R
40|$|Multiperspectives {{naturally}} arise out of co-operative work {{in applying}} appropriate technologies to construct {{different parts of}} an application. The representation styles of various perspectives can be highly heterogeneous and open-ended since those perspectives should be presented in a form appropriate to each participant in the software development process. This {{makes it difficult to}} provide <b>consistency</b> <b>checking</b> and integration mechanisms for the perspectives. This paper presents an approach of dealing with automated <b>consistency</b> <b>checking</b> for multiperspective software specifications regardless of their heterogeneity of representation. We apply expressive graph notations called Conceptual Graphs (CGs) as visual <b>consistency</b> <b>checking</b> notations. The combination of underlying logical reasoning and graph-based reasoning of CGs provide a powerful <b>consistency</b> <b>checking</b> mechanism. Our framework is illustrated with excerpts of a case study using Unified Modeling Language (UML) ...|$|E
40|$|This paper {{describes}} a formal analysis technique, called <b>consistency</b> <b>checking,</b> for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique {{is designed to}} analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for <b>consistency</b> <b>checking,</b> a formal requirements model is introduced; the model represents a software system as a finite state automaton, which produces externally visible outputs in response to changes in monitored environmental quantities. Results are presented of two experiments which evaluated the utility and sealability of our technique for <b>consistency</b> <b>checking</b> in a real-world avionics application. The role of <b>consistency</b> <b>checking</b> during the requirements phase of software development is discussed...|$|E
40|$|This paper {{considers}} {{definitions of}} consistency {{arising from the}} RM-ODP and relates these in a mathematical framework for <b>consistency</b> <b>checking.</b> We place existing FDTs, in particular LOTOS, into this framework. Then we consider the prospects for viewpoint translation. Our conclusions centre {{on the relationship between}} the different definitions of consistency and on the requirements for realistic <b>consistency</b> <b>checking...</b>|$|E
50|$|Administrative {{functions}} that allow programmatic control over users, databases, and devices, {{as well as}} administrative procedures such as backup, defragmentation and <b>consistency</b> <b>checks.</b>|$|R
40|$|This paper {{presents}} {{a novel approach}} that performs post-processing for stereo correspondence. We improve the performance of stereo correspondence by performing <b>consistency</b> <b>check</b> and adaptive filtering in an iterative filtering scheme. The <b>consistency</b> <b>check</b> is done with asymmetric information only so that very few additional computational loads are necessary. The proposed post-filtering method {{can be used in}} various methods for stereo correspondence without any modification. We demonstrate the validity of the proposed method by applying it to hierarchical belief propagation and semiglobal matching. ...|$|R
5000|$|Auditing: GiveDirectly uses {{independent}} {{checks to}} verify that recipients are eligible and did not pay bribes, including physical back-checks, image verification, and data <b>consistency</b> <b>checks.</b>|$|R
