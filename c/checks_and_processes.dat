0|10000|Public
40|$|This {{poster paper}} {{highlights}} the various checks {{to ensure the}} quality of the delivery of newly designed BIT degree programme at AIS St. Helens. All these quality checks are verified at deferent stages of the delivery of the course. Entire <b>checking</b> <b>and</b> verification <b>process</b> involves institutional internal and external resources. These checks ensure that any course that is being delivered under newly designed BIT programme is approved for quality after undergoing a series of quality <b>checks</b> <b>and</b> verification <b>process...</b>|$|R
40|$|We {{discuss the}} {{collective}} {{behavior of a}} network of individuals that receive, <b>process</b> <b>and</b> forward to each other tasks. Given costs they store those tasks in buffers, choosing optimally the frequency at which to <b>check</b> <b>and</b> <b>process</b> the buffer. The individual optimizing strategy of each node determines the aggregate behavior of the network. We find that, under general assumptions, the whole system exhibits coexistence of equilibria and hysteresis. Comment: 18 pages, 3 figures, submitted to JSTA...|$|R
40|$|Different {{implementations}} of {{the same}} protocol specification usually contain deviations, i. e., differences in how they <b>check</b> <b>and</b> <b>process</b> some of their inputs. Deviations are commonly introduced as implementation errors or as different interpretations {{of the same}} specification. Automatic discovery of these deviations is important for several applications. In this paper, we focus on automatic discovery of deviations for two particular applications: error detection and fingerprint generation. We propose a novel approach for automatically detecting deviations in the way different implementations of the same specification <b>check</b> <b>and</b> <b>process</b> their input. Our approach has several advantages: (1) by automatically building symbolic formulas from the implementation, our approach is precisely faithful to the implementation; (2) by solving formulas created from two different implementations of the same specification, our approach significantly reduces the number of inputs needed to find deviations; (3) our approach works on binaries directly, without access to the source code. We have built a prototype implementation of our approach and have evaluated it using multiple implementations of two different protocols: HTTP and NTP. Our results show that our approach successfully finds deviations between different implementations, including errors in input <b>checking,</b> <b>and</b> {{differences in the interpretation}} of the specification, which can be used as fingerprints. ...|$|R
30|$|Finally, stops {{criteria}} are <b>checked</b> <b>and</b> the optimization <b>process</b> is continued until {{they will be}} fulfilled.|$|R
40|$|Abstract. Bio-jETI is a {{platform}} for the intuitive graphical design and execution of bioinformatics workflows composed from heterogeneous remote services. In this paper we use a simple phylogenetic analysis process to show how formal approaches like model <b>checking</b> <b>and</b> <b>process</b> synthesis {{can be applied to}} further support the workflow development in Bio-jETI. To unfold their full potential these methods need a comprehensive knowledge base about the domain, containing semantic information about the single services as well as ontological classifications of the used terms. We outline how to systematically integrate these semantic web concepts into our framework and discuss the implications on <b>checking</b> <b>and</b> synthesis...|$|R
50|$|The pattern {{typically}} {{does some}} kind of task that is not subject of real-time computing. An example would be the task of sending e-mails, where the end-user waiting several minutes is not a problem. The pattern design will typically implement worker thread that accesses a message queue that holds all unprocessed tasks <b>and</b> regularly <b>check</b> <b>and</b> <b>process</b> these tasks. If there are no tasks the worker thread will sleep for an increasingly long time until there are tasks in the queue again; when this occurs the sleep time is reset.|$|R
2500|$|<b>Process</b> <b>and</b> thread blocks: <b>check</b> whether <b>process</b> <b>and</b> thread blocks {{have been}} {{manipulated}} ...|$|R
40|$|We {{propose a}} loop-reduction LLL (LR-LLL) {{algorithm}} for lattice-reduction-aided (LRA) multi-input multioutput (MIMO) detection. The LLL algorithm is an iterative algorithm that contains many <b>check</b> <b>and</b> <b>process</b> operations; however, the traditional LLL algorithm itself possesses {{a lot of}} redundant check operations. To solve this problem, we propose a look-ahead check technique that not only reduces {{the complexity of the}} LLL algorithm but also produces the lattice-reduced matrix which obeys the original LLL criterion. Simulation results show that the proposed LR-LLL algorithm reduces the average number of loops or computation complexity. Besides, it also shortens the latency of clock cycles about 19. 4 %, 29. 1 %, and 46. 1 % for 4 × 4, 8 × 8, and 12 × 12 MIMO systems, respectively...|$|R
40|$|There is a {{wide variety}} of drivers for {{business}} process modelling initiatives, reaching from business evolution <b>and</b> <b>process</b> optimisation over compliance <b>checking</b> <b>and</b> <b>process</b> certification to process enactment. That, in turn, results in models that differ in content due to serving different purposes. In particular, processes are modelled on different abstraction levels and assume different perspectives. Vertical alignment of process models aims at handling these deviations. While the advantages of such an alignment for inter-model analysis and change propagation are out of question, a number of challenges has still to be addressed. In this paper, we discuss three main challenges for vertical alignment in detail. Against this background, the potential application of techniques from the field of process integration is critically assessed. Based thereon, we identify specific research questions that guide the design of a framework for model alignment...|$|R
50|$|The {{real value}} of an IBE is in the {{business}} rules <b>and</b> <b>processes</b> that package the content and provide the capability to shop and purchase. This includes packaging and pricing rules, customized displays for different customers and channels, business rules, <b>and</b> <b>check</b> out <b>and</b> payment <b>processes.</b>|$|R
50|$|While {{the check}} sheets {{discussed}} above are all for capturing and categorizing observations, the checklist is {{intended as a}} mistake-proofing aid when carrying out multi-step procedures, particularly during the <b>checking</b> <b>and</b> finishing of <b>process</b> outputs.|$|R
40|$|Abstract. There is a {{wide variety}} of drivers for {{business}} process modelling initiatives, reaching from business evolution <b>and</b> <b>process</b> optimisation over compliance <b>checking</b> <b>and</b> <b>process</b> certification to process enactment. That, in turn, results in models that differ in content due to serving different purposes. In particular, processes are modelled on different abstraction levels and assume different perspectives. Vertical alignment of process models aims at handling these deviations. While the advantages of such an alignment for inter-model analysis and change propagation are out of question, a number of challenges has still to be addressed. In this paper, we discuss three main challenges for vertical alignment in detail. Against this background, the potential application of techniques from the field of process integration is critically assessed. Based thereon, we identify specific research questions that guide the design of a framework for model alignment. Key words: process model alignment, business-IT gap, model consistency, model correspondence...|$|R
30|$|Currently, {{the study}} has not {{analysed}} {{the findings of}} the above ontological research. As a matter of facts,a further analysis task is planned, that will try to investigate on algorithms and functions for appropriate data processing, using MA approach. Finally, some suggestions coming from the concept of frames, used in a MA cognitive approach à la Minsky (1987), will be <b>checked</b> <b>and</b> further <b>processed.</b>|$|R
40|$|Summary: Systems Biology Markup Language (SBML) is {{the leading}} {{exchange}} format for mathematical models in Systems Biology. Semantic annotations link model elements with external knowledge via unique database identifiers and ontology terms, enabling software to <b>check</b> <b>and</b> <b>process</b> models by their biochemical meaning. Such information is essential for model merging, {{one of the key}} steps towards the construction of large kinetic models. SemanticSBML is a tool that helps users to <b>check</b> <b>and</b> edit MIRIAM annotations and SBO terms in SBML models. Using a large collection of biochemical names and database identifiers, it supports modellers in finding the right annotations and in merging existing models. Initially, an element matching is derived from the MIRIAM annotations and conflicting element attributes are categorized and highlighted. Conflicts can then be resolved automatically or manually, allowing the user to control the merging process in detail. Availability: SemanticSBML comes as a free software written in Python and released under the GPL 3. A Debian package, a source package for other Linux distributions, a Windows installer and an online version of semanticSBML with limited functionality are available a...|$|R
40|$|Fresh sausage {{products}} {{have been considered}} at low risk of causing listeriosis due to obstacles created in the manufacturing <b>process</b> <b>and</b> characteristics of pH, high salt concentration and presence of lactic acid bacteria. However, the survival of Listeria monocytogenes in these products is <b>checked</b> <b>and</b> <b>process</b> studies for the reduction of contamination by this pathogen, have demonstrated that characteristic as a variation of process parameters, strains of lactic acid bacteria and L. monocytogenes directly influence the results. In this study, two formulations were evaluated in vitro (a culture of Lactobacillus plantarum by adding 104 CFU mL- 1 culture of L. monocytogenes {{and the other with}} addition of L. monocytogenes and a starter culture of L. plantarum 0, 0025 %) using process parameters commonly used in Brazil. The sausages naturally contaminated had slight increase in population of L. monocytogenes {{at the beginning of the}} process, followed by reduction by the end of maturation. The sausages were spiked considerably reduced the count of L. monocytogenes...|$|R
40|$|The Recreational Sport Department at the University of Wisconsin La Crosse {{wanted a}} {{software}} tool to help manage membership services, track equipment, <b>check</b> in <b>and</b> <b>check</b> out equipment, monitor locker assignment, and finally analyze each equipment's usage. Currently, the Recreational Sport Department {{is using a}} manual <b>process</b> <b>and</b> paper documents {{to keep track of}} all the above mentioned activities. This manuscript addresses the development of a software tool to assist the Recreational Sports Department. The tool consists of four major components: (1) Membership registration, (2) Equipment inventory management, (3) Equipment <b>check</b> in <b>and</b> <b>check</b> out <b>processes,</b> <b>and</b> (4) Receipt and report generation. The Membership Registration component consists of addition, modification and deletion of membership; in addition, it also includes locker assignment <b>and</b> cost calculation <b>processes.</b> Equipment inventory management consists of addition, modification and deletion of individual equipment. An added side component of equipment inventory management allows the users to group a list of desired equipment and create a team equipment group to be checked out later. Equipment <b>check</b> out <b>and</b> <b>check</b> in <b>processes</b> shall have an intuitive GUI to make the activities easy to use. A user will be able to print the membership receipt at any time. In addition, users will also be able to generate any report on usage of equipment with a variety of information...|$|R
40|$|Semantic {{annotations}} in SBML (systems biology markup language) enable {{computer programs}} to <b>check</b> <b>and</b> <b>process</b> biochemical models {{based on their}} biochemical meaning. Annotations are an important prerequisite for model merging, {{which would be a}} major step towards the construction of large-scale cell models. The software tool semanticSBML allows users to <b>check</b> <b>and</b> edit MIRIAM annotations and SBO terms, the most common forms of annotation in SBML models. It uses a large collection of biochemical names and database identifiers to support modellers in finding the right annotations. Annotated SBML models can also be built from lists of chemical reactions. In model merging, semanticSBML suggests a preliminary merged model based on MIRIAM annotations in the original models. This model provides a starting point for manually aligning the elements of all input models. To resolve conflicting element properties, conflicts are highlighted and categorised. The user can navigate through the models, change the matching of model elements, check the conflicts between them and decide how they should be resolved. Alternatively, the software can resolve all conflicts automatically, selecting each time the attribute value from the input model with highest priority. |$|R
50|$|The Name <b>Check</b> <b>process</b> <b>and</b> the way {{in which}} it is used are blamed some by {{immigration}} advocates, political commentators, and government officials for creating substantial delays in the processing of green card and citizenship applications.|$|R
40|$|Abstract—In {{order to}} improve the overall safety of chemotherapy, safety-protecting net was {{established}} for the whole process from prescribing by physicians, transcribing by nurses, dispensing by pharmacists to administering by nurses. The information system was used to <b>check</b> <b>and</b> monitor whole <b>process</b> of administration <b>and</b> related sheets were computerized to simplify the paperwork...|$|R
40|$|Helping {{end users}} build <b>and</b> <b>check</b> <b>process</b> models is a {{challenge}} for many science and engineering fields. Many AI researchers have investigated useful ways of verifying and validating knowledge bases for ontologies and rules, {{but it is not}} easy to directly apply them to checking process models. Also the techniques developed for <b>checking</b> <b>and</b> refining planning knowledge tend to focus on automated plan generation rather than helping users author process information. In this paper, we propose a complementary approach which helps users author <b>and</b> <b>check</b> <b>process</b> models. Our system, called KANAL, relates pieces of information in process models among themselves and to the existing KB, analyzing how different pieces of input are put together to generate an effect. It builds interdependency models from the analysis and uses them to find errors and propose fixes. Our initial evaluation focused on how useful KANAL is in helping users detect and fix errors. The results show that KA [...] ...|$|R
40|$|Process discovery—discovering {{a process}} model from example {{behavior}} recorded in an event log—is {{one of the}} most challenging tasks in process mining. Discovery approaches need to deal with competing quality criteria such as ¿tness, simplicity, precision, and generalization. Moreover, event logs may contain low frequent behavior and tend to be far from complete (i. e., typically {{only a fraction of the}} possible behavior is recorded). At the same time, models need to have formal semantics in order to reason about their quality. These complications explain why dozens of process discovery approaches have been proposed in recent years. Most of these approaches are time-consuming and/or produce poor quality models. In fact, simply checking the quality of a model is already computationally challenging. This paper shows that process mining problems can be decomposed into a set of smaller problems after determining the so-called causal structure. Given a causal structure, we partition the activities over a collection of passages. Conformance <b>checking</b> <b>and</b> discovery can be done per passage. The decomposition of the process mining problems has two advantages. First of all, the problem can be distributed over a network of computers. Second, due to the exponential nature of most process mining algorithms, decomposition can signi¿cantly reduce computation time (even on a single computer). As a result, conformance <b>checking</b> <b>and</b> <b>process</b> discovery can be done much more ef¿ciently...|$|R
40|$|Aims. In an {{additive}} design, {{test the}} ef ® cacy of cue exposure treatment for smoking relapse prevention {{as an adjunct}} to current standard cognitive behavioral and pharmacological treatments. Design. Randomized, controlled clinical trial. Setting. Outpatient behavioral medicine clinic. Participants. One hundred and twenty-nine cigarette smokers recruited through newspaper advertisements. Intervention. After receiving an initial counseling session for cessation and setting a quit day, 129 smokers were {{randomly assigned to one of}} four relapse prevention treatment conditions: (1) brief cognitive behavioral; (2) cognitive behavioral and nicorette gum; (3) cognitive behavioral and cue exposure; and (4) cognitive behavioral and cue exposure with nicorette gum. All smokers met individually with their counselor for six RP sessions. Measures. Seven-day, point-prevalence abstinence rates (CO veri ® ed) taken at 1, 3, 6 and 12 -months post-treatment and time to ® rst slip. Findings. All manipulation <b>checks</b> <b>and</b> <b>process</b> measures suggested that the treatments were delivered as intended. There were no signi ® cant differences between conditions in point-prevalence abstinence rates or in time to ® rst slip. Conclusions. These results call into question the utility of cue exposure treatment for smoking relapse prevention...|$|R
40|$|International audienceSystems and Networks on Chips (NoCs) are a prime design {{focus of}} many {{hardware}} manufacturers. In addition to functional verification, {{which is a}} difficult necessity, the chip designers are facing extremely demanding performance prediction challenges, such as the need to estimate the latency of memory accesses over the NoC. This paper attacks this problem {{in the setting of}} designing globally asynchronous, locally synchronous systems (GALS). We describe foundations and applications of a combination of compositional modeling, model <b>checking,</b> <b>and</b> Markov <b>process</b> theory, to arrive at a viable approach to compute performance quantities directly on industrial, functionally verified GALS models...|$|R
5000|$|Calacanis was {{involved}} in a 2010 Internet hoax involving his Twitter postings regarding the introduction of the Apple iPad. In his tweets, he claimed to have a [...] "reviewer's copy" [...] of an iPad device describing in great detail the features of such device. The device in question was not in his possession nor did it exist. It was explained to have been an attempt by Calacanis to expose the hysteria regarding Apple product launches. The hoax also called into question the fact <b>checking</b> <b>and</b> verification <b>processes</b> of the mainstream media who published the hoax story as true.|$|R
50|$|Homejoy {{was run by}} a team of over 100 employees, {{and worked}} with {{thousands}} of independent professional cleaners in their cities of operation as of early 2014. They charged a uniform rate of $25 an hour for service. Cleanings were fully bonded, and cleaners contracting on the platform had to go through a screening process which involved third-party background <b>checks</b> <b>and</b> a certification <b>process.</b>|$|R
40|$|Now a days people facing many {{problems}} because of traffic. To avoid this problem VANET is used here. In this group signature {{is used in}} VANETs to realized unauthorized authentication. In the existing system, group signatures is used and its suffers from long delay in the certificate revocation list (CRL) <b>checking</b> <b>and</b> <b>process</b> of signature verification, high message loss. In VANET in which roadside units (RSUs) are responsible for distributing group private keys and to managing vehicles in a localized manner. Then to avoid time consuming, hash message authentication (HMAC) is used here. The proposed work of this project is trinary partitioned black-burst-based broadcast protocol(3 P 3 B) consists of two primary mechanisms. First, a mini distributed interframe space (DIFS) in a medium access control (MAC) sublayer is used to give higher access priority to the time critical emergency message as a higher priority and to communication channel compared with other messages. Second, a trinary is designed to iteratively partition the communication range into small sectors. In 3 P 3 B outperforms benchmarks of the existing broadcast protocols in VANETs {{in terms of the}} packet delivery ratio, average message speed, message progress and communication delay. Index terms—Batch group signature, cooperation, hash message authentication code (HMAC), 3 P 3 B, emergency message(EM...|$|R
40|$|This paper {{presents}} wireless sensing {{systems to}} increase safety and robustness in industrial process control, particularly in industrial machines for marble slab working. The process is performed by abrasive or cutting heads activated independently by the machine controller when the slab, transported on a conveyer belt, is under them. Current slab detection systems {{are based on}} electromechanical or optical devices at the machine entrance stage, suffering from deterioration and from the harsh environment. Slab displacement or break inside the machine due to the working stress may result in safety issues and damages to the conveyer belt due to incorrect driving of the working tools. The experimented contactless sensing techniques are based on four RFID and two capacitive sensing technologies and on customized hardware/software. The proposed solutions aim at overcoming some limitations of current state-of-the-art detection systems, allowing for reliable slab detection, outside and/or inside the machine, while maintaining low complexity {{and at the same}} time robustness to industrial harsh conditions. The proposed sensing devices may implement a wireless or wired sensor network feeding detection data to the machine controller. Data integrity <b>check</b> <b>and</b> <b>process</b> control algorithms have to be implemented for the safety and reliability of the overall industrial process...|$|R
40|$|New {{challenges}} concerning bias from {{measurement error}} have arisen {{due to the}} increasing use of paid participants: semi-plausible response patterns (SpRPs). SpRPs result when participants only superficially process the information of (online) experiments/questionnaires and attempt only to respond in a plausible way. This {{is due to the fact}} that participants who are paid are generally motivated by fast cash, and try to efficiently overcome objective plausibility <b>checks</b> <b>and</b> <b>process</b> other items only superficially, if at all. Thus, those participants produce not only useless but detrimental data, because they attempt to conceal their malpractice. The potential consequences are biased estimation and misleading statistical inference. The statistical nature of specific invalid response strategies and applications are discussed, effectually deriving a meta-theory of response strategy, <b>process,</b> <b>and</b> plausibility. A new test measure to detect SpRPs was developed to accommodate data of survey type, without the need of a priori implemented mechanisms. Under a latent class latent variable framework, the effectiveness of the test measure was empirically and theoretically evaluated. The empirical evaluation is based on an experimental and online questionnaire study. These studies operate under a very well established psychological framework on five stable personality traits. The measure was theoretically evaluated through simulations. It was concluded that the measure is successfully discriminating between valid responders and invalid responders under certain conditions. Indicators for optimal settings of high discriminatory power were identified and limitations discussed...|$|R
5000|$|Typically every {{website that}} allows you to make purchases, have an online {{shopping}} cart to place your desired items. Locate the [...] "Shopping Cart" [...] and click the [...] "Check Out" [...] (usually located at the top right of the webpage) to complete the buying <b>and</b> <b>check</b> out <b>process.</b>|$|R
5000|$|The monitor thread is {{a single}} {{independent}} process despite being named Monitor Thread. It {{is the first thing}} created after Listener when Tibero starts. It is the last process to finish when Tibero terminates. The monitor thread creates other processes when Tibero starts <b>and</b> <b>checks</b> each <b>process</b> status <b>and</b> deadlocks periodically.|$|R
40|$|Plasma polymer {{films of}} tetravinylsilane and mixture of tetravinylsilane and oxygen gas were {{deposited}} on silicon wafers. Oxygen gas was mixed in tetravinylsilane {{to improve the}} compatibility of thin films on glass substrates. Mass spectroscopy was employed during the cleaning of the deposition chamber to <b>check</b> residual gases <b>and</b> <b>process</b> gases, during plasma deposition to monitor neutral plasma species and to follow plasma stability...|$|R
40|$|This chapter {{describes}} the design {{aspects of a}} typical Mission Control Center (MCC). The German Space Operations Center (GSOC) is taken as an example. The Mission Control Center – {{as the name implies}} – is the central ground facility of a space mission. It is the central point where all data and management information concerning the spacecraft are consolidated. These data are received, <b>checked</b> <b>and</b> <b>processed,</b> decisions are made and – {{in case of an emergency}} – the respective procedures are performed in order to restore the nominal conditions of the mission. The way how the MCC operates is defined by its design which specifies its capabilities, flexibility and robustness. MCC operations are also defined by the people working within, primarily of course by the Flight Operations Team, but also by all personnel responsible for interfaces and infrastructure. Their work in the background is equally important. Finally, the design of the MCC needs to conform to the customer requirements and provide a safe and secure environment for spacecraft operations. This includes not only purely technical solutions, but also the respective environment for the people working there. Within this chapter we focus on several aspects of the design of a control center. At first the necessary infrastructure is analyzed. Then the design of the local control center network is examined followed by the software needed...|$|R
40|$|Two commercially {{available}} nanosized TiO 2 colloids and their suspensions in Triton X- 100 have been precipitated and dried carefully. The solids obtained {{have been studied}} up to 500 degrees C by simultaneous thermogravimetric and differential thermal analysis (TG/DTA) coupled online with quadrupole mass spectrometer for analysis of evolved gases (EGA-MS) in flowing air, nitrogen and helium atmosphere in order to <b>check</b> <b>and</b> model pyrolytic <b>processes</b> taking place at elevated temperature ranges of porous TiO 2 layer fabrication techniques. Pure TiO 2 (P 25, Degussa) has bounded ca. 12...|$|R
40|$|Computer Science An eficient timing {{synchronization}} {{technique for}} a low complexdp FFT based multi-carrier direct sequence spread spectrum IF/baseband transceiver is presented. The timing synchronization control loop {{consists of a}} ro-bust acquisition loop and a tracking loop that eliminates the timing phase ofiset caused by the transmission delay, sampiing error, and channel noise. The acquisition loop reduces the probability y of false lock by continuous thresh-old <b>checking</b> <b>and</b> adjusting <b>process.</b> Both the acquisition and the tracking loop incorporate a simple feedback mech-anism for a practical VLSI transceiver implementation. The tz~ect of the sampling oflset {{on the performance of}} the timing acquisition is discussed. The highest oper-ating speed of the receiver is the Nyquist sampling rate, but the entire synchronization is performed using the low speed baseband signals. The performance of the synchro-nization under the additive white Gaussian noise chan-nel is assumed. I...|$|R
40|$|Configuration of feature {{models in}} {{software}} product-lines typically involves manipulating {{a model to}} modify the feature selections and analyzing the model {{to ensure that no}} configuration constraints are violated. In order to capture and reuse configuration knowledge from different users, model transformation and constraint languages can be used to specify and automate the constraint <b>checking</b> <b>and</b> model manipulation <b>processes.</b> However, this approach presents challenges to general end-users (e. g., domain experts who may not be programmers) who do not have experience using these languages. This paper presents a demonstration-based technique to support the capture and reuse of feature model configurations...|$|R
30|$|This {{is where}} TPAs could assist in guaranteeing the CU’s data privacy. Through a fair <b>and</b> {{impartial}} auditing <b>process</b> {{as well as}} the preservation of the CU’s computational resources, a higher standard could be set for trust in cloud services. This auditing process could also help in improving the QoS provided by cloud-based platforms and resources. In the field of data security and privacy, the possibility of an insider threat can not be avoided. This raises the concern that a TPA could be malicious. The TPA will have free and open access to the CU’s data and the cloud services, which leaves the CU vulnerable to attacks. Therefore, to circumvent potential financial losses or insider threats, a strict <b>check</b> <b>and</b> balance <b>process</b> over the TPA performance should exist. To address all of these issues, {{there is a need for}} a privacy-preserving model (PPM) that can provide a mechanism to authenticate all cloud stakeholders (i.e., CSP, TPA and CU) in order to safeguard the cloud computing environment.|$|R
