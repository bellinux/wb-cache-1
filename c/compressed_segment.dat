3|56|Public
40|$|Intra-arterial {{pressure}} was recorded {{at various points}} in the <b>compressed</b> <b>segment</b> of brachial artery during deflation of a standard blood pressure cuff in human subjects with arms of normal girth. Cuff pressure and the Korotkoff sounds also were recorded simultaneously. The data were analyzed {{in terms of the}} various dynamic reactions produced by cuff deflation, which may influence the auscultatory indications of systolic and diastolic blood pressure. Cuff {{pressure was}} incompletely transmitted to the com-pressed arterial segment. As a result, muffling, the auscultatory indication of diastolic pressure, occurred at a cuff pressure higher than the directly recorded intra-arterial diastolic blood pressure. The first Korotkoff sound, on the other hand, provided a close approximation of intra-arterial systolic pressure. This may be due to a delay in pene-tration of the diminutive pulse waves into the distal part of the compressed arterial segment at systolic levels of cuff pressure. This effect appears to compensate for other influences tending to raise the indirect reading of systolic blood pressure. The extent of the delay in penetration should be dependent on the length of the collapsed segment, which in turn is a function of cuff width...|$|E
40|$|ObjectiveTo {{study the}} use of a nerve “bypass” graft as a {{possible}} alternative to neurolysis or segmental resection with interposition grafting in the treatment of neuroma-in-continuity. MethodsA sciatic nerve crush injury model was established in the Sprague-Dawley rat by compression with a straight hemostatic forceps. Epineurial windows were created proximal and distal to the injury site. An 8 -mm segment of radial nerve was harvested and coaptated to the sciatic nerve at the epineurial window sites proximal and distal to the <b>compressed</b> <b>segment</b> (bypass group). A sciatic nerve crush injury without bypass served as a control. Nerve conduction studies were performed over an 8 -week period. Sciatic nerves were then harvested and studied under transmission electron microscopy. Myelinated axon counts were obtained. ResultsNerve conduction velocity was significantly faster in the bypass group than in the control group at 8 weeks (63. 57 m/s± 5. 83 m/s vs. 54. 88 m/s± 4. 79 m/s, P< 0. 01). Myelinated axon counts in distal segments were found more in the experimental sciatic nerve than in the control sciatic nerve. Significant axonal growth was noted in the bypass nerve segment itself. ConclusionNerve bypass may serve to augment peripheral axonal growth while avoiding further loss of the native nerve...|$|E
40|$|Rabbit tibial nerves were {{subjected}} to direct, acute graded compression {{by means of an}} inflatable compression chamber. The acute and long term effects of 50, 200 and 400 mmHg applied for two hours on nerve function and nerve fibre structure were investigated. A pressure of 50 mmHg applied for two hours induced only minimal or no acute deterioration of maximal conduction velocity and nerve fibre structure. Conduction velocity was gradually reduced during compression at 200 - 400 mmHg pressure for two hours and in those cases the recovery of nerve conduction after pressure release was incomplete. Ultrastructural analysis revealed pronounced, early nerve fibre damage in these nerves. Three weeks after compression, nerves compressed at 50 mmHg for two hours had normal afferent and motor conduction velocity, although there were morphological signs of slight nerve fibre damage. Nerves compressed at 200 mmHg for two hours exhibited reduction of conduction velocity only at the level of compression, in contrast to the nerves compressed at 400 mmHg for two hours in which conduction velocity was reduced both at the level of compression and distal to the <b>compressed</b> <b>segment.</b> Morphologically, the nerves compressed at 200 - 400 mmHg for two hours showed varying degrees of demyelination and axonal degeneration three weeks after compression...|$|E
25|$|Some {{implementations}} do {{support such}} data compression within dynamic sparse tries and allow insertions and deletions in compressed tries. However, this usually {{has a significant}} cost when <b>compressed</b> <b>segments</b> need to be split or merged. Some tradeoff {{has to be made}} between data compression and update speed. A typical strategy is to limit the range of global lookups for comparing the common branches in the sparse trie.|$|R
50|$|A1) Right {{anterior}} {{oblique view}} taken at end systole. The <b>compressed</b> vessel <b>segment</b> {{is indicated by}} the two arrows.|$|R
30|$|In practice, {{accurate}} cepstral reconstruction is {{not sufficient}} for acoustically undistorted enhancement of MP 3 <b>compressed</b> <b>segments.</b> Especially {{in the case of}} a very low bit rate source (e.g., 32 [*]kbps MP 3), many audible artifacts are present in the source signals compared to the target signals. Instruments that are inaudible in the source signal will usually appear in the enhanced signal as distortions since the LPC coefficients alone fail to represent them. In such cases, the signal differences lie mainly in the residuals and therefore some residual processing is essential for better enhancement results.|$|R
40|$|Objectives: The aim of {{this study}} was to examine the localizing value of short segment {{conduction}} studies (SSCSs) by comparing pre-surgical SSCS findings with intraoperative findings in patients with ulnar neuropathy of the elbow. Patients and Methods: Pre-surgical SSCSs were performed in 20 patients and compared with the intraoperative findings. Functional and electrophysiological recoveries were assessed at postoperative month 3. Overall, <b>compressed</b> nerve <b>segments</b> identified by SSCSs were compatible with the intraoperative findings in 90 % of patients. Results: The success rate of surgery was higher in the patients with SSCS-determined single-level compression as compared to multi-level compression. SSCS abnormalities persistent in six patients with poor functional recovery. Conclusion: SSCSs are valuable for localizing <b>compressed</b> <b>segments,</b> thus they can be used as a guide for minimalist surgical techniques. These studies also appear valuable for predicting surgical outcome in patients with ulnar neuropathy of the elbow. [Hand Microsurg 2017; 6 (2. 000) : 68 - 74...|$|R
30|$|In this work, {{which is}} an {{extension}} of a previous work[10], we propose an approach exploiting the effect of double compression in the statistical properties of quantized MDCT coefficients in MP 3 audio files. The method, that will be presented in Section 2, relies on a single measure derived from the statistics of MDCT coefficients, allowing us to apply a simple threshold detector to decide whether a given MP 3 file is singly compressed or it has been doubly compressed, without resorting to SVM classifiers. Moreover, the proposed method is able to derive the bit-rate of the first compression by means of a nearest neighbour classifier. The ability of the algorithm to detect doubly MP 3 compressed files remains valid also on tracks of reduced length, allowing its application to the localization of singly/doubly <b>compressed</b> <b>segments</b> within a MP 3 audio file, as shown in Section 3.|$|R
50|$|DivX is a {{brand of}} video codec {{products}} developed by DivX, LLC. The DivX codec is notable {{for its ability to}} <b>compress</b> lengthy video <b>segments</b> into small sizes while maintaining relatively high visual quality.|$|R
50|$|External imagesFor terms, see: Morphology of Diptera. Front tibiae are not dilated. Tarsae 2 first 2 {{segments}} are yellow, {{the first}} strongly compressed, the second slightly <b>compressed,</b> last 3 <b>segments</b> are black.Head {{is large and}} broad.|$|R
3000|$|... {{are lower}} in {{comparison}} with the results of SMOSQRS as shown in Figure 14. This is an expected result since the proposed compression algorithm further <b>compresses</b> the ECG <b>segments</b> with low energy {{in comparison with}} the ECG segments with high energy.|$|R
40|$|Compressed bitmap indexes {{are used}} in {{databases}} and search engines. Many bitmap compression techniques have been proposed, almost all relying primarily on run-length encoding (RLE). However, on unsorted data, we can get superior performance with a hybrid compression technique that uses both uncompressed bitmaps and packed arrays inside a two-level tree. An instance of this technique, Roaring, has recently been proposed. Due to its good performance, it has been adopted by several production platforms (e. g., Apache Lucene, Apache Spark, Apache Kylin and Druid). Yet there are cases where run-length encoded bitmaps are smaller than the original Roaring bitmaps [...] -typically when the data is sorted so that the bitmaps contain long compressible runs. To better handle these cases, we build a new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE <b>compressed</b> <b>segments.</b> The result is a new Roaring format that compresses better. Overall, our new implementation of Roaring can be several times faster (up to two orders of magnitude) than the implementations of traditional RLE-based alternatives (WAH, Concise, EWAH) while compressing better. We review the design choices and optimizations that make these good results possible...|$|R
50|$|Uintatherium {{was a large}} {{browsing}} animal. With {{a length}} of about 4 m (13 ft), a height of 1.70 m (5.6 ft), and a weight up to 2 tonnes, it was similar to today's rhinoceros, both in size and in shape, although they are not closely related. Its legs were robust to sustain {{the weight of the}} animal and were equipped with claws. Moreover, a Uintathere's sternum was made up of horizontal segments, unlike today's rhinos, which have <b>compressed</b> vertical <b>segments.</b>|$|R
40|$|Indexing {{and editing}} digital video {{directly}} in the compressed domain offer many advantages in terms of storage efficiency and processing speed. We have designed automatic tools in the compressed domain for extracting key visual features such as scene cut, dissolve, camera operations (zoom, pan), and moving object detection and tracking. In addition, we have developed algorithms to solve the decoder buffer control problems and allow users to “cut, copy and paste ” arbitrary <b>compressed</b> video <b>segments</b> {{directly in the}} compressed domain. The compressed-domain approach does not require full decoding. Thus fast software implementations can be achieved. Our compressed video editing techniques will enhance the reusability of existing compressed videos...|$|R
40|$|FIGURE 2. Guaranidrilus marquesi sp. nov. A. Segments IV – VI, lateral view. B. Epidermal gland cells, dorsal view, {{as seen in}} {{a living}} and <b>compressed</b> specimen. C. <b>Segments</b> IV – VIII, dorsal view. D. Coelomocytes, schematic. E. Male efferent apparatus. F. Preclitellar nephridia. G. Postclitellar nephridium. A, E, F, G, from whole mounts, B, C, D from living specimens...|$|R
40|$|With {{the rapid}} {{increase}} in the volume of Big data of this digital era, fax documents, invoices, receipts, etc are traditionally subjected to compression for the efficiency of data storage and transfer. However, in order to process these documents, they need to undergo the stage of decompression which indents additional computing resources. This limitation induces the motivation to research {{on the possibility of}} directly processing of compressed images. In this research paper, we summarize the research work carried out to perform different operations straight from run-length compressed documents without going through the stage of decompression. The different operations demonstrated are feature extraction; text-line, word and character segmentation; document block segmentation; and font size detection, all carried out in the compressed version of the document. Feature extraction methods demonstrate how to extract the conventionally defined features such as projection profile, run-histogram and entropy, directly from the compressed document data. Document segmentation involves the extraction of <b>compressed</b> <b>segments</b> of text-lines, words and characters using the vertical and horizontal projection profile features. Further an attempt is made to segment randomly a block of interest from the compressed document and subsequently facilitate absolute and relative characterization of the segmented block which finds real time applications in automatic processing of Bank Cheques, Challans, etc, in compressed domain. Finally an application to detect font size at text line level is also investigated. All the proposed algorithms are validated experimentally with sufficient data set of compressed documents. Comment: 2014 Fourth IDRBT Doctoral Colloquium, December 11 - 12, 2014 Hyderabad, Indi...|$|R
40|$|This paper {{introduces}} a fast algorithm for microarray image segmentation and an object-based coding technique to <b>compress</b> the <b>segmented</b> DNA microarray images using modified embedded block coding with optimized truncation (EBCOT) [1]. Microarray images are segmented into foreground and background by a modified Mann-Whitney test-based algorithm, {{which is much}} faster than Chen et al. 's original one [2]. By extending EBCOT to arbitrarily shaped objects, our scheme can realize lossless coding of foreground and lossy-to-lossless coding of background separately, a feature EBCOT does not offer, and achieve better compression results than other popular coding schemes like LZW, JPEG 2000 and JPEG-LS. In addition, our scheme offers similar lossy coding performance as JPEG- 2000, in which EBCOT is used...|$|R
40|$|Effects of graded {{compression}} on nerve function {{were analyzed}} {{in order to}} evaluate {{the relative importance of}} pressure level and duration of compression for functional deterioration. The pressure was applied by means of a small inflatable cuff. The effects of two pressure levels, i. e., 80 mm Hg applied for 2 hr or 400 mm Hg applied for 15 min, were studied in rabbit tibial nerves. The lower pressure tested, which is known to induce ischemia of the <b>compressed</b> nerve <b>segment,</b> also causes some degree of mechanical deformation of the nerve trunk, which leads to incomplete recovery following pressure release. The duration of compression is of importance for the degree of nerve injury even at the higher pressure level tested...|$|R
40|$|Abstract — Wearable sensing systems (WSS’s) are {{emerging}} {{as an important}} class of distributed embedded systems in application domains ranging from medical to military. Such systems can be expensive and power hungry due to their multisensor implementations that require constant use, yet by nature they demand low-cost and low-power implementations. Semantic multimodal compression (SMC) mitigates these metrics in terms of data size by leveraging the natural tendency of signals in many types of embedded sensing systems to be composed of phases. In our driving example of a medical shoe with an insole lined with pressure sensors, {{we find that the}} natural airborne, landing, and take-off segments have sharply different and repetitive properties. SMC models and <b>compresses</b> each <b>segment</b> independently, selecting the best compression scheme for each segment and thus reducing total transmission energy. I...|$|R
40|$|We study {{shapes of}} planar arcs and closed {{contours}} modeled on elastic curves obtained by bending, stretching or <b>compressing</b> line <b>segments</b> non-uniformly along their extensions. Shapes are represented as {{elements of a}} quotient space of curves obtained by identifying those that differ by shape-preserving transformations. The elastic properties of the curves are encoded in Riemannian metrics on these spaces. Geodesics in shape spaces are used to quantify shape divergence and to develop morphing techniques. The shape spaces and metrics constructed are novel and offer an environment {{for the study of}} shape statistics. Elasticity leads to shape correspondences and deformations that are more natural and intuitive than those obtained in several existing models. Applications of shape geodesics to the definition and calculation of mean shapes and to the development of shape clustering techniques are also investigated...|$|R
40|$|This paper {{describes}} the tree-structured maximum mutual information (MMI) encoders used in SSrs Phonetic Engine ® to perform large-vocabulary, continuous speech recognition. The MMI encoders are arranged into a two-stage cascade. At each stage, the encoder is trained {{to maximize the}} mutual information between a set of phonetic targets and corresponding codes. After each stage, the codes are <b>compressed</b> into <b>segments.</b> This step expands acoustic-phonetic context and reduces subsequent computation. We evaluated these MMI encoders by comparing them against a standard minimum distortion (MD) vector quantizer (encoder). Both encoders produced code streams, which were used to train speaker-independent discrete hidden Markov models in a simplified version of the Sphinx system [3]. We used data from the DARPA Resource Management (RM) task. The two-stage cascade of MMI encoders significantly outperforms the standard MD encoder in both speed and accuracy...|$|R
40|$|Abstract. We study {{shapes of}} planar arcs and closed {{contours}} modeled on elastic curves obtained by bending, stretching or <b>compressing</b> line <b>segments</b> non-uniformly along their extensions. Shapes are represented as {{elements of a}} quotient space of curves obtained by identifying those that differ by shape-preserving transformations. The elastic properties of the curves are encoded in Riemannian metrics on these spaces. Geodesics in shape spaces are used to quantify shape divergence and to develop morphing techniques. The shape spaces and metrics constructed are novel and offer an environment {{for the study of}} shape statistics. Elasticity leads to shape correspondences and deformations that are more natural and intuitive than those obtained in several existing models. Applications of shape geodesics to the definition and calculation of mean shapes and to the development of shape clustering techniques are also investigated. Keywords: planar shapes, shape geodesics, mean shape, shape analysis, clustering shape...|$|R
40|$|Formation {{of model}} {{describing}} dynamic straining of reinforced concrete requires {{taking into account}} the basic aspects influencing the stress-strain state of structures. Strength of concrete segments in crack spacing is one of the crucial aspects that affect general strain behavior of reinforced concrete. Experimental results demonstrate significant change in strength of tensed and <b>compressed</b> concrete <b>segments</b> in crack spacing both under static and under dynamic loading. In this case, strength depends on tensile strain level and the slope angle of rebars towards the cracks direction. Existing theoretical and experimental studies estimate strength of concrete segments in crack spacing under static loading. The present work presents results of experimental and theoretical studies of dynamic strength of plates between cracks subjected to compression-tension. Experimental data was analyzed statistically; the dependences were suggested to describe dynamic strength of concrete segments depending on tensile strain level and slope angle of rebars to cracks direction...|$|R
40|$|The {{reliable}} {{optical character}} recognition is not available for scripts of Indian languages. Thus, {{the only way to}} make legacy documents in Indian languages available on the web is by scanning them. This work is an attempt to cater to the need for a better representation and efficient storage technique for Indian language documents and their near perfect regeneration at the browser. We work with the segments (corresponding to text, image or white spaces) extracted from the original document page. For <b>compressing</b> the <b>segments</b> separately, we use Shape-Adaptive Wavelet based coding scheme, Run Length encoding and Arithmetic Bit-plane coding. An XML representation scheme is being used to represent the document page and the data is stored at a server. A plug-in has been implemented that decodes the data encoded coming from the server and displays the document page on the web browser thereby making the document pages web accessible...|$|R
40|$|Nickel and {{compressed}} rhenium powder targets {{have been}} {{installed in the}} FNAL antiproton source target sta-tion. Ni was chosen for its high melting point energy and resistance to stress wave fractures. As well, <b>compressed</b> powdered rhenium <b>segments</b> were constrained by a thin-wall Ti jacket to insure resistance to stress fractures. The p yield of these new targets is {{compared with that of}} copper-the previous standard production target. The target de-pletion characteristics of nickel and rhenium for a beam intensity of 1. 6 x lOr 2 protons per pulse are also presented. I...|$|R
40|$|Sparse Matrix-Vector {{multiplication}} (SpMV) is {{an important}} computational kernel in scientific applications. Its performance highly depends on the nonzero distribution of sparse matrices. In this paper, we propose a new storage format for diagonal sparse matrices, defined as <b>Compressed</b> Row <b>Segment</b> with Diagonal-pattern (CRSD). We design diagonal patterns to represent the diagonal distribution. As the diagonal distributions are similar within matrices from one application, some diagonal patterns remain unchanged. First, we sample one matrix to obtain the unchanged diagonal patterns. Next, the optimal SpMV codelets are generated automatically for those diagonal patterns. Finally, we combine the generated codelets as the optimal SpMV implementation. In addition, the information collected during auto-tuning process is also utilized for parallel implementation to achieve load-balance. Experimental results demonstrate that the speedup reaches up to 2. 37 (1. 70 on average) in comparison with DIA and 4. 60 (2. 10 on average) in comparison with CSR under {{the same number of}} threads on two mainstream multi-core platforms. © 2011 Springer-Verlag...|$|R
40|$|AbstractThe role {{of network}} {{security}} {{systems such as}} network intrusion detection system (NIDS) has become more important. The regular expression (a. k. a. regex) matching algorithm used for inspecting the payloads of packets {{is one of the}} most intensive tasks in NIDS. When multiple regular expressions are processed together, the corresponding Deterministic Finite Automata (DFA) becomes so complicated and needs a large amount of memory. In this paper, we propose a memory efficient parallel compatible DFA algorithm that uses the techniques of compression and pattern segmentation to reduce the memory usage. Extended from PFAC (Parallel Failureless-AC) algorithm 4, the proposed <b>compressed</b> and <b>segmented</b> DFA (CSDFA) needs less numbers of states and transitions than δFA. Without considering the leading symbols “. *” in the regular expressions, the transition table can be compressed very efficiently by the run-length encoding. The number of transitions in CSDFA is about a half of the transitions needed in δFA, and uses only 74 % of the memory consumed by δFA. Based on our experiments, the throughput of the proposed CSDFA is also much better than δFA...|$|R
40|$|Compressed Electrocardiography (ECG) {{is being}} used in modern telecardiology {{applications}} for faster and efficient transmission. However, existing ECG diagnosis algorithms require the compressed ECG packets to be decompressed before diagnosis can be applied. This additional process of decompression before performing diagnosis for every ECG packet introduces undesirable delays, which can have severe impact on the longevity of the patient. In this paper, we first used an attribute selection method that selects only a few features from the compressed ECG. Then we used Expected Maximization (EM) clustering technique to create normal and abnormal ECG clusters. Twenty different segments (13 normal and 7 abnormal) of compressed ECG from a MIT-BIH subject were tested with 100 % success using our model. Apart from automatic clustering of normal and abnormal <b>compressed</b> ECG <b>segments,</b> this paper presents an algorithm to identify initiation of abnormality. Therefore, emergency personnel can be contacted for rescue mission, within the earliest possible time. This innovative technique based on data mining of compressed ECGs attributes, enables faster identification of cardiac abnormalities resulting in an efficient telecardiology diagnosis system...|$|R
30|$|The work {{presented}} in [11] {{can be considered}} as the most relevant to the proposed sub-sampling method since it concerns compressed data recovery in an OFDM environment. The authors describe a Parallel <b>Segmented</b> <b>Compressed</b> Sensing (PSCS or its extended version EPSCS) architecture where AIC instead of Nyquist sampling is applied. Their target is to use K subcarriers of the 112 offered by the ECMA 368 standard. The BER can fall below 10 − 4, regardless of the optimization algorithm used provided that: (a) the number of subcarriers K is less than 86, (b) the SNR is higher than 40 dB, and (c) channel estimation is applied.|$|R
40|$|In {{the last}} few years, there has been active {{research}} on aggregating advanced statistical measures in multidimensional data cubes from partitioned subsets of data. In this paper, we propose an online compression and aggregation scheme to support Bayesian estimations in data cubes based on the asymptotic properties of Bayesian statistics. In the proposed approach, we <b>compress</b> each data <b>segment</b> by retaining only the model parameters and {{a small amount of}} auxiliary measures. We then develop an aggregation formula that allows us to reconstruct the Bayesian estimation from partitioned segments with a small approximation error. We show that the Bayesian estimates and the aggregated Bayesian estimates are asymptotically equivalent...|$|R
40|$|In {{this paper}} an image {{compression}} approach is proposed for medical applications. The algorithm {{is based on}} the Vector Quantization and adopts the idea of Region-OfInterest. The image to be <b>compressed</b> is first <b>segmented</b> into regions and a separate codebook is used for compressing every region. The size and the number of codewords may be different in the codebooks according to the diagnostic importance of the corresponding image region. This permits to create appropriate codebooks with representative codewords, and to obtain good reconstruction quality in relevant zones, while reinforcing the compression in less important regions. The proposed approach is tested on ultrasound esophagus images and is shown to be very promising...|$|R
30|$|Let a DS be {{lowering}} {{in a plane}} circular bore hole. Then, it slides along the bore hole bottom line and its axis line is a circle of a radius ρ + a, where ρ is the radius of the bore hole axis T and a is the system clearance. In sliding, the DS is subjected to action of gravity (f gr), contact (f cont), and friction (f fr) forces. In consequence of these forces, the DS can be <b>compressed</b> in some <b>segments</b> of its length where it can begin to buckle without losing its contact with the well wall. It is necessary to predict the critical states of the DS and to construct the modes of its stability loss.|$|R
40|$|We {{present in}} this paper a new method to <b>segment</b> <b>compressed</b> or {{uncompressed}} image sequences in the temporal domain. The processed images are colour images and the sequence is dynamic (camera is moving). The shot change detection {{will be used as}} a pre-processing step for object tracking. So it has to be performed in real time. We use low resolution images to satisfy this constraint. In order to avoid illumination changes effects, the colour space is changed from RGB to HSV, where H and S are used to compare two successive frames. Detection of cuts and of other effects are done thanks to tracking of a inter-frame difference value and comparison with adaptative threshold...|$|R
40|$|The {{analytical}} modeling of physical processes {{is an integral}} part of scientific and technical research. Physical build-and-test procedures used for designing forging dies are prohibitively expensive and result in long lead times in obtaining satisfactory designs. In the present study the wedge test was advanced to the level of a standard laboratory test in order to verify the analytical results of a viscoplastic finite-element program, ALPID (Analysis of Large Plastic Incremental Deformation), which was developed to simulate the metal flow in deformation processes such as forging and extrusion. Wedge-shaped specimens were machined from plates of 1100 -F and 6061 -T 6 aluminum alloy and the grids engraved on the meridian plane by means of a CNC engraver. The specimens were <b>compressed</b> in <b>segmented</b> dies at room temperature. The undeformed and deformed grids were digitized, and the true effective strains were calculated using a computer program developed for that purpose. The effective strains were then displayed as contour plots for comparison with the ALPID-generated strain values. Comparison of the experimental and ALPID results indicates that the values predicted by the ALPID code are very near the experimental values. The minor differences in the results are attributed to unavoidable experimental errors...|$|R
40|$|Digital {{multimedia}} data (i. e audio, video, and text) {{are difficult to}} store and replay due to their real time nature and large size. The requirements of continuous retrieval, {{in the presence of}} multiple media streams whose display must proceed in a mutually synchronized manner are the distinguishing features that are unique to digital multimedia. In this study, the problem of integration of <b>compressed</b> media <b>segments</b> is addressed, which are sequences of continuously recorded video frames, audio samples, and text on disk, for storage and retrieval. Motion based JPEG has been used for the compression of video frames as it allows on-line processing and random access of video frames. Companding scheme has been used for audio compression, whereas uncompressed text is used. The schemes used for compression of audio and video data both yield a fixed length code to save on the buffering requirements. A mechanism for integration is proposed that can be applied to arrange video along with the associated audio and text, into fixed length media segments before storing them on disk. All the algorithms have been implemented in the C language...|$|R
40|$|In segmentation-based {{image coding}} {{techniques}} {{the image to}} be <b>compressed</b> is first <b>segmented.</b> Then, the information is coded describing the shape and {{the interior of the}} regions. A new method to encode the texture obtained in segmentation-based coding schemes is presented. The approach combines 2 -D linear prediction and stochastic vector quantization. To encode a texture, a linear predictor is computed first. Next, a codebook following the prediction error model is generated and the prediction error is encoded with VQ. In the decoder, the error image is decoded first and then filtered as a whole, using the prediction filter. Hence, correlation between pixels is not lost from one block to another and a good reproduction quality can be achieved. Peer ReviewedPostprint (published version...|$|R
