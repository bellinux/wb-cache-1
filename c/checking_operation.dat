11|303|Public
50|$|Each car {{has to be}} {{prepared}} according to some criteria. These criteria are defined so that each car {{will be able to}} cross the desert with minimum troubles. The main modification is the protection plate each team has to add to its car in order to avoid troubles linked to stones you can find in the desert zone. The organization will check each car before the departure. <b>Checking</b> <b>operation</b> focuses mostly on the structure of the car and the problem that it may cause. It may happen that a car is not accepted and does not meet the requirement. In this case, the team won’t get the right to take the departure.A technical help is available in the camp each day after joining the camp. The help is provided by volunteers who are mechanical professional. This service is included in the enrollment fee and is free of charge during the raid.|$|E
3000|$|... sceptic agents pay an epistemic {{cost for}} {{performing}} a <b>checking</b> <b>operation</b> before trusting the received information; [...]...|$|E
40|$|Abstract—A new {{algorithm}} called Character-Comparison to Character-Access (CCCA) {{is developed}} {{to test the}} effect of both: 1) converting character-comparison and number-comparison into character-access and 2) {{the starting point of}} checking on the performance of the <b>checking</b> <b>operation</b> in string searching. An experiment is performed using both English text and DNA text with different sizes. The results are compared with five algorithms, namely, Naive, BM, Inf_Suf_Pref, Raita, and Cycle. With the CCCA algorithm, the results suggest that the evaluation criteria of the average number of total comparisons are improved up to 35 %. Furthermore, the results suggest that the clock time required by the other algorithms is improved in range from 22. 13 % to 42. 33 % by the new CCCA algorithm. Keywords—Pattern matching, string searching, charactercomparison, character-access, text type, and checking I...|$|E
40|$|Abstract—Distributed shared-memory (DSM) multiprocessors {{provide a}} {{scalable}} hardware platform, but lack the necessary redundancy for mainframe-level reliability and availability. Chip-level redundancy in a DSM server faces a key challenge: the increased latency to check results among redundant components. To address performance overheads, we propose a checking filter {{that reduces the}} number of <b>checking</b> <b>operations</b> impeding the critical path of execution. Furthermore, we propose to decouple <b>checking</b> <b>operations</b> from the coherence protocol, which simplifies the implementation and permits reuse of existing coherence controller hardware. Our simulation results of commercial workloads indicate average performance overhead is within 4 % (9 % maximum) of tightly coupled DMR solutions. I...|$|R
40|$|Photograph {{used for}} a {{newspaper}} owned by the Oklahoma Publishing Company. Caption: "Hugh Hedger, former Oklahoma extension service worker, and an Ethiopian farm expert, Dagnyachew Yeargu, graduate of Purdue university, <b>check</b> <b>operation</b> of a new irrigation system at the Bishofu experiment station in Ethiopia. ...|$|R
50|$|The crew {{performed}} the standard one-hour leak <b>check</b> <b>operation</b> before getting started with hatch opening procedures. After leak checks and system checks were completed, the crew opened the hatches {{and entered the}} Space Station at around 15:15 GMT. They were greeted by Expedition 33 crewmembers Sunita Williams, Yuri Malenchenko and Akihiko Hoshide.|$|R
40|$|We {{argue that}} some word order {{phenomena}} in Romanian and Sardinian {{are the result}} of a <b>checking</b> <b>operation</b> in the left periphery involving verum focus (i. e. focus on the polarity component of the sentence). In particular, this operation accounts for some word order patterns found in polar questions. In Romanian, polarity fronting is realized as head-movement of (V+) T to a higher peripheral head which bears a Focus-probe. This licenses VS orders for predications in which VS is not allowed as a neutral order (i-level predicates, iteratives, generics). In Sardinian, an entire phrase headed by the lexical predicate (verbal non-finite form or non-verbal predicate) is fronted before the auxiliary. We argue that this order is obtained by two movement operations, head-raising of Aux to Foc and movement of the predicate phrase to SpecFoc. We also present the semantics of polarity focus, distinguishing several types of focus (informational, emphatic, contrastive) ...|$|E
40|$|Plaintext checkable {{encryption}} (PCE), {{proposed by}} Canard et. al., in CT-RSA 2012, is a public-key primitive with an added functionality that given a plaintext, a ciphertext {{and the public}} key under which the ciphertext is created, anyone can check whether the ciphertext encrypts the plaintext under the key. However, in many situations, users may not want everybody to have plaintext checking right on their ciphertexts. In this paper, we introduce a primitive called designated plaintext checkable encryption (DPCE), where only a designated checker can perform the plaintext <b>checking</b> <b>operation.</b> We note that, unlike PCE, there can be two types of DPCE (of Type I and II), depending upon whether the user delegates (at his will) or is bound to provide the plaintext checking right to a designated checker. We also provide various generic random-oracle and standard model constructions for DPCE of both the types based on any probabilistic or deterministic encryption scheme...|$|E
40|$|By the {{increasing}} needs of software industry, software systems became more complex constructions than ever before. As {{a result of}} increasing complexity in software systems, functional decomposition of these systems gains {{the status of the}} most important aspect in the software development process. Dividing problems to sub-problems and producing specific solutions for divided parts makes it easier to solve the main problem. Component Based Software Engineering is a way of developing software systems that consists of logically or functionally decomposed components which integrated to each other by the help of well-defined interfaces. CBSE relies on architectural design of a software system. Planning phase and implementation of a software project may differ time to time. Because of the complexity of software systems, solving specific problems may affect the architecture of the whole system. In spite of sophisticated software engineering processes and CASE tools there is still a large gap between the planned and implemented architecture of software systems. Finding deviations from architecture in source code is a non-trivial task requiring tool support. Since, matching operation of designed software architecture and implemented software architecture needs to check design documents against implementation code. This manual <b>checking</b> <b>operation</b> is nearly impossible for major software systems. Software Architecture Checker provides a great approach to check the architecture of any software system. This bachelor thesis examines the approach behind the Software Architecture Checker...|$|E
5000|$|... 1. Agents Chris Aller and Jason Chapman {{were accused}} of {{participating}} in a bar <b>check</b> <b>operation</b> with Fort Worth police officers on June 28, 2009, which targeted the Rainbow Lounge specifically because of the bar's gay and lesbian customer base. The allegation that the Rainbow Lounge was targeted for being a gay bar was unfounded.|$|R
50|$|A layered Bloom filter {{consists}} of multiple Bloom filter layers. Layered Bloom filters allow {{keeping track of}} how many times an item {{was added to the}} Bloom filter by checking how many layers contain the item. With a layered Bloom filter a <b>check</b> <b>operation</b> will normally return the deepest layer number the item was found in.|$|R
30|$|When {{we use our}} novel {{proposed}} algorithm, we {{can select}} a code rate of 3 / 5 when the SNR is 2.7, and the average iteration count will be 23.62. However, since the minimum iteration count will be 19 in this case, we may skip the parity check and tentative <b>checking</b> <b>operations</b> up to the 19 th iteration without affecting performance.|$|R
40|$|This {{abstract}} {{reports on}} the use of Algebraic and Logical methods for the computation of the worstcase execution time (WCET). The derivation of safe and precise bounds for the WCET of each task is a pre-requisite for schedulability analysis, and hence of paramount importance for real-time systems. We use abstract interpretation [1] as the underlying technique, where an adequate fixpoint calculation allow us to derive an accurate WCET. Moreover, {{in the context of a}} code producer/code consumer software distribution model, and in order to ensure the correctness of our calculation to a program consumer, we produce a certificate (or proof) whose validity entails compliance with the calculated WCET. Our methodology is based on the work introduced by Hermenegildo et al: Abstraction-Carrying Code [3]. The produced certificate allows a program consumer to locally check the calculated WCET, avoiding a blind confidence on the producer side. This <b>checking</b> <b>operation</b> is a much more efficient process than the certificate production itself. While on the producer side one has to compute a fixpoint in a complex and possibly long iterative process, on the consumer side only a one pass process is required to confirm that the certificate is indeed a fixpoint. Our approach has as starting point the extension of the C programming language with annotations that define the intended timing properties for each function. Moreover, by placing this annotations directly int...|$|E
40|$|This thesis {{describes}} {{the design and}} implementation of an interactive, dynamic and functional web-based Graphical User Interface for RDFGears. RDFGears is a data integration framework for the semantic web. It allows users to express and execute complex data algorithms without {{the need to be}} concerned with the implementation details. The framework has its own language namely RDF Gears Language (RGL) that is used to formally define the data integration process as a workflow and an engine to evaluate the workflow and perform the data integration. This thesis proposes an application that can be used as an interface between RDF Gears and users. We introduce a new design of the user interface and new features to improve the usability of RDF Gears. More support for editing nested workflows, query editor, interactive nested node copier and property editor are some of the features that are introduced to improve the usability and the interactivity. RDFGears Function Definition Language is introduced to improve the extensibility of RDF gears engine. Core functions or user defined functions of RDF Gears can be extended without worried of the UI support. We also introduce a type system that implemented in the user interface. It performs the type <b>checking</b> <b>operation</b> during the workflow construction and provides live feedback. An evaluation to evaluate the correctness of our implementation is presented. The evaluation shows the improvement that we have made and the usability of the new features. Web Information SystemSoftware TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|Zu, Xin. "August 2011. "Thesis (M. Phil.) [...] Chinese University of Hong Kong, 2011. Includes bibliographical {{references}} (leaves 221 - 233). Abstracts in English and Chinese. Abstract [...] - p. i論文提要 [...] - p. iiDedication [...] - p. iiiAcknowledgements [...] - p. ivTable of Contents [...] - p. viiList of Notations [...] - p. xList of Abbreviations [...] - p. xiChapter Chapter 1 [...] - Introduction [...] - p. 1 Chapter 1. 1 [...] - Background {{and major}} claims [...] - p. 1 Chapter 1. 2 [...] - Overview of the thesis [...] - p. 3 Chapter 1. 3 [...] - A note on data collection [...] - p. 7 Chapter Chapter 2 [...] - Theoretical Background [...] - p. 8 Chapter 2. 1 [...] - Introduction [...] - p. 8 Chapter 2. 2 [...] - Minimalism [...] - p. 8 Chapter 2. 2. 1 [...] - Levels of representation [...] - p. 10 Chapter 2. 2. 2 [...] - Distributed morphology [...] - p. 12 Chapter 2. 2. 3 [...] - Economy as the guiding principle [...] - p. 16 Chapter 2. 2. 4 [...] - The feature <b>checking</b> <b>operation</b> [...] - p. 19 Chapter 2. 3 [...] - Theoretical assumptions [...] - p. 21 Chapter 2. 3. 1 [...] - The syntax of extended projections [...] - p. 22 Chapter 2. 3. 1. 1 [...] - Projecting CP [...] - p. 23 Chapter 2. 3. 1. 2 [...] - Projecting DP [...] - p. 24 Chapter 2. 3. 2 [...] - The cartographic approach [...] - p. 26 Chapter 2. 3. 2. 1 [...] - Splitting CP [...] - p. 27 Chapter 2. 3. 2. 2 [...] - Splitting DP [...] - p. 36 Chapter 2. 4. [...] - Concluding remarks [...] - p. 50 Chapter Chapter 3 [...] - Language Background [...] - p. 52 Chapter 3. 1 [...] - Introduction [...] - p. 52 Chapter 3. 2 [...] - A {{sketch of the}} Jingpo language [...] - p. 53 Chapter 3. 2. 1 [...] - Word order [...] - p. 54 Chapter 3. 2. 2 [...] - Morphological typology [...] - p. 61 Chapter 3. 2. 3 [...] - The pro drop [...] - p. 63 Chapter 3. 3 [...] - Previous studies on Jingpo noun phrase structure [...] - p. 66 Chapter 3. 3. 1 [...] - Simplex noun phrases [...] - p. 67 Chapter 3. 3. 1. 1 [...] - Bare nouns and referentiality [...] - p. 67 Chapter 3. 3. 1. 2 [...] - The fixed order N-Cl-Num [...] - p. 69 Chapter 3. 3. 1. 3 [...] - The optionality of classifiers [...] - p. 71 Chapter 3. 3. 1. 4 [...] - The two ones - langai and mi [...] - p. 75 Chapter 3. 3. 1. 5 [...] - The two plural markers - -hte and ni [...] - p. 80 Chapter 3. 3. 2 [...] - Complex nominals [...] - p. 90 Chapter 3. 3. 2. 1 [...] - Prenominal and postnominal adjectives [...] - p. 91 Chapter 3. 3. 2. 2 [...] - Prenominal and postnominal demonstratives [...] - p. 97 Chapter 3. 4 [...] - Concluding remarks [...] - p. 98 Chapter Chapter 4 [...] - The Right Periphery of Jingpo Clauses [...] - p. 100 Chapter 4. 1 [...] - Introduction [...] - p. 100 Chapter 4. 2 [...] - The rightmost edge of Jingpo clauses [...] - p. 101 Chapter 4. 2. 1 [...] - Evidentiality [...] - p. 101 Chapter 4. 2. 2 [...] - Speech acts [...] - p. 106 Chapter 4. 3 [...] - Sentence final particles [...] - p. 109 Chapter 4. 3. 1 [...] - Clause typing [...] - p. 110 Chapter 4. 3. 2 [...] - Agreement [...] - p. 118 Chapter 4. 3. 2. 1 [...] - Subject agreement [...] - p. 119 Chapter 4. 3. 2. 2 [...] - Object agreement [...] - p. 120 Chapter 4. 3. 2. 3 [...] - Possessor agreement [...] - p. 122 Chapter 4. 3. 2. 4 [...] - The simplification of Jingpo agreement system [...] - p. 124 Chapter 4. 3. 3 [...] - Change of state [...] - p. 130 Chapter 4. 3. 4 [...] - Emphatic mood [...] - p. 134 Chapter 4. 3. 5 [...] - Spatial deixis [...] - p. 137 Chapter 4. 3. 6 [...] - Jingpo SFPs as portmanteau forms [...] - p. 137 Chapter 4. 4 [...] - The structure of the clause periphery in Jingpo [...] - p. 139 Chapter 4. 5 [...] - The asymmetry between Jingpo matrix and embedded clauses [...] - p. 142 Chapter 4. 6 [...] - Concluding remarks [...] - p. 149 Chapter Chapter 5 [...] - Feature Checking at the Right Periphery [...] - p. 151 Chapter 5. 1 [...] - Introduction [...] - p. 151 Chapter 5. 2 [...] - Syntactic analysis of Jingpo evidentiality [...] - p. 151 Chapter 5. 2. 1 [...] - The syntax of speech act and sentience [...] - p. 151 Chapter 5. 2. 2 [...] - The feature geometry of referring expressions [...] - p. 153 Chapter 5. 2. 3 [...] - The syntactic representations of {{the two types of}} evidentiality [...] - p. 154 Chapter 5. 2. 4 [...] - Accounting for the ordering constraints [...] - p. 157 Chapter 5. 3 [...] - The consequences of the feature checking analysis [...] - p. 161 Chapter 5. 3. 1 [...] - Agreement with pragmatic roles [...] - p. 161 Chapter 5. 3. 2 [...] - The shifting of agreement relations across clause types [...] - p. 167 Chapter 5. 3. 3 [...] - Person constraint on subjects [...] - p. 170 Chapter 5. 4 [...] - Concluding remarks [...] - p. 171 Chapter Chapter 6 [...] - Jingpo from the Cartographic Perspective [...] - p. 173 Chapter 6. 1 [...] - Introduction [...] - p. 173 Chapter 6. 2 [...] - Functional heads and their specifiers [...] - p. 175 Chapter 6. 2. 1 [...] - Jingpo auxiliaries and their relation to adverbs [...] - p. 176 Chapter 6. 2. 2 [...] - Prenominal and postnominal adjectives [...] - p. 180 Chapter 6. 3 [...] - Evidence for postulating an articulated DP structure [...] - p. 185 Chapter 6. 3. 1 [...] - Multiple occurrences of demonstratives [...] - p. 185 Chapter 6. 3. 2 [...] - The internal DP layer [...] - p. 192 Chapter 6. 3. 3 [...] - The differential object marker hpe [...] - p. 198 Chapter 6. 4 [...] - Concluding remarks [...] - p. 208 Chapter Chapter 7 [...] - Conclusion [...] - p. 210 Chapter 7. 1 [...] - Introduction [...] - p. 210 Chapter 7. 2 [...] - Recapitulation of major claims [...] - p. 211 Chapter 7. 3 [...] - Future directions of research [...] - p. 212 Chapter Appendix A [...] - Pear Story [...] - p. 215 Bibliography [...] - p. 22...|$|E
5000|$|... iChecks - A MICR laser check {{printing}} {{systems to}} enable secure in-house, laser <b>check</b> printing <b>operations.</b>|$|R
40|$|Economical tool <b>checks</b> <b>operation</b> of {{automatic}} circuit analyzer. Each loop is addressed directly from analyzer console by switching internal analyzer bridge to resistance equal that of connecting cable plus specified limiting test value. Procedure verifies whether detected faults in circuit under test are actually due to analyzer malfunction. Standard-length universal test cables {{make it possible}} to shift detector tool from cable to cable without resistance compensation...|$|R
40|$|This paper {{presents}} {{a new class}} of q-ary erasure-correcting codes based on Latin and Sudoku squares of order q, and an iterative decoding algorithm similar to the one used for the Low Density Parity Check code. The algorithm works by assigning binary variables to the q-ary values, and by generalizing the definition of parity <b>check</b> <b>operation</b> to represent the constraints that define the Latin and Sudoku squares. ...|$|R
40|$|The {{objective}} of this thesis {{is to provide a}} formalization of a minimalist description of a small fragment of Dutch. The fragment that is described is outlined in Section 3. 4. Although the Minimalist Program is still in development precise definitions of the theory in its current stage of the development will be of use for linguists working inside and outside the Minimalist Program. As a first attempt to formalization two small implementations in Prolog were made. These implementations are described in Chapter 2. The first implementation, which is outlined in Section 2. 1, gives a survey of the two structure-building operations Merge and Move. This implementation reveals that the two structure-building operations the Minimalist Program presupposes actually both consist of more disjunctively specifed sub-cases. The structure-building operation Merge has three sub-cases: tree insertion in the complement position, tree insertion in the specifier position, and lexical insertion in the head position. The structure-building operation Move has two sub-cases: head movement and to-specifier movement. Furthermore a new definition of the Move operation is given. This definition states that the moved element in a Move operation has to be contained in the complement domain of the head of the target tree, not in the target tree as the original denition says. From this new definition we can derive that movement to the complement position is impossible. An element (tree) cannot be moved to a position that contains the tree from which the moved element must originate. The second implementation, which is outlined in Section 2. 2, is a head-corner parser for a fragment of the Minimalist Program. I argue that {{because of the nature of}} the structure-building operations of the Minimalist Program head-corner parsing is a suitable parsing technique for the Minimalist Program. The idea of head-corners from head-corner parsing resembles closely the idea of target trees from the Minimalist Program, as we see in Subsection 2. 2. 2. In the Chapters 4 through 9 the actual formalization of the minimalist description of the fragment of Dutch is given. The formalization is written in the formal-specification language AFSL. Afterwards, the entire formalization was validated by implementing it in Prolog. The formalization is declaratively stated, not derivationally as the Minimalist Program itself: it describes which trees are correct according to the Minimalist Program. In Chapter 4 I give a description of trees. By considering minimalist trees as some kind of directed graphs, we can omit indices. The function ConnectionTarget represents connections between nodes. It is used instead of indices to indicate movements within trees. In Chapter 5 minimalist ideas on features are formalized. I argue that the notion ‘feature structure’ be introduced in order to be able to treat the features of a node as a unit, although feature structures are not applied in the Minimalist Program. Furthermore I introduce some new features, among which the features [object], [subject], [compcat] and [speccat]. The first two features are used to refer to the object and the subject features of the verb. Since a verb must agree with both its subject and its object (if it is a transitive verb), we need a way to separate the subject agreement features from the object agreement features of the verb. Furthermore, the verb assigns case to the object. This case feature is also, as the agreement feature, a part of the value of the [object] feature. The features [compcat] and [speccat] are introduced to be able to implement subcategorization. The feature [compcat] indicates the category of the complement of a given head, while the feature [speccat] indicates the category of the specifier of a given head. Subcategorization receives no explicit mention in the Minimalist Program, but it turns out to be vital for the formalization. It is important that there is a way to represent what kind of complement or specifier a given head may select, because otherwise the formalization would for instance allow transitive verbs to behave like intransitive verbs, by not forcing transitive verbs to select both a subject and an object. This is especially essential in Zwart’s version (and Chomsky’s 1995 version) since in this version the derivation is not guided by the features of the lexical heads in a sentence. In Chomsky’s 1993 version all lexical heads need to check all their formal features. Therefore the object features of the verb will require the verb to select an object to check against. Furthermore, subcategorization proved to be essential for the word order of sentences. In Chapter 5 I also give an exact definition of the notion ‘feature checking’. I argue that the features of a lexical constituent can only be checked against the features of a functional constituent if the feature structure belonging to the lexical constituent contains at least as many feature-value pairs as the feature structure belonging to the functional head. In Chapter 6 I describe the way the lexicon is treated in the Minimalist Program. In Zwart’s version of the Minimalist Program there are two lexicons. The first lexicon, which I call the prelexicon, is consulted when a lexical item enters the derivation. The second lexicon, which I call the postlexicon, is consulted after the derivation (at PF) to obtain the phonological features of a lexical item. I argue that the lexicon which is consulted at the beginning of the derivation may not contain underspecified feature structures because of the nature of the feature <b>checking</b> <b>operation.</b> Checking is only possible if a certain feature is present in a given functional head as well as in the lexical constituent that checks its features against it. Hence, we cannot indicate that any possible value for a given feature name can be chosen by not representing it at all (under-specification). X-Theory is described in Chapter 7. In the X-rules that are specified in the formalization only bar-levels play a role, while in the original X-rules of the Minimalist Program category features also play a role (for instance: X ! X, YP (where X and Y represent categories)). In the formalization, feature percolation, including the percolation of the category feature, is taken care of separately. Feature percolation is based on the X-rules, since features percolate up from the head with bar-level zero to higher bar-levels. The positions of specifiers, complements, heads and adjuncts are explicitly defined in the X-module. This module seems to be the right location to represent this type of knowledge since in the literature X-rules often implicitly define notions such as ‘specifier’ and ‘complement’. Furthermore I argue that the two-level X-Theory as applied in Zwart’s version and Chomsky’s 1995 version of the Minimalist Programs is problematic. X-Theory and the theory of movement (or chains in our case) are mutually dependent. In the new version of X-Theory heads do not always need to project, but without projection we cannot maintain the notions ‘complement domain’ and ‘checking domain’. In Chapter 8 I show that it is not problematic to treat head movement in a representational way by considering traces to be copies (contra Rizzi [Riz 90] and Brody [Bro 95]). Furthermore I argue that a [determiner] feature is needed to distinguish the treatment of pronouns and interrogative words on the one hand from nouns on the other hand with respect to the selection of determiners. The former group requires an ‘empty’ D (i. e. a D with no phonological content), while nouns, except for the plural forms, require a D with lexical content. In Chapter 9 the interfaces (LF and PF) of the Minimalist Program are described. The main result of the formalization of the interfaces is the discovery that an additional lexicon, which contains templates for all types of sentences covered by the formalization, is needed for Zwart’s version of the Minimalist Program. Since LF in Zwart’s framework is reached when all functional heads in a tree checked all their features, I needed something to make sure that an LF-tree always contains the required functional projections. Otherwise an LF-tree consisting of only a VP could be approved of by the formalization. In Chomsky’s 1993 framework, lexical heads needed to check all their features before LF, and to check their features they require functional heads. Suggestions for future research The formalization described in this work shows that a more formal approach to minimalist ideas leads to clearer definitions and sometimes to the discovery of inconsistencies and incompleteness. Therefore I think it can be interesting and useful to develop tools with which linguists, for instance, can compare several solutions to the same problem or can test the result of a change in a definition. The formalization described here could be used as a basis for a parser that could serve as such a tool. Furthermore, research on the comparison of aspects of the Minimalist Program with other linguistic theories might be of interest. The formalization presented here could help with such a comparison. For example, the formalization lead to the conclusion that under-specification in the lexicon is not allowed in the Minimalist Program because of the nature of feature checking, although under-specification is common in feature-based theories such as HPSG. ...|$|E
40|$|A {{number of}} {{commonly}} encountered simple neural network types are discussed, with particular attention {{being paid to}} their applicability in automation and control when applied to food processing. In the first instance n-tuple networks are considered, these being particularly useful for high speed production <b>checking</b> <b>operations.</b> Subsequently backpropagation networks are discussed, these being useful both in a more familiar feedback control arrangement and also {{for such things as}} recipe prediction...|$|R
30|$|Iris {{recognition}} technology nowadays {{is widely}} deployed in various large-scale {{applications such as}} the border crossing system in the United Arab Emirates, Mexico national ID program, and the Unique Identification Authority of India (UIDAI) project [5]. As a case in point, more than one billion residents have been enrolled in the UIDAI project where about 1015 all-to-all <b>check</b> <b>operations</b> are carried out daily for identity de-duplication using iris biometrics as the main modality [5, 6].|$|R
40|$|The {{check valve}} for the Hallam Power Reactor uses a {{knife-edge}} bearing for the flapper {{in place of}} the usual journal-type bearing. Mechanical cycling in sodium at 600 deg F was used to <b>check</b> <b>operation</b> of this bearing. A total of 309 mechanical cycles was completed with no apparent malfunctioning of the valve. Measured leskage rates were 0. 46 gpm at 0. 93 psig, 0. 73 gpm at 3. 4 psig. and 0. 32 gpm at 5. 9 psig. (M. C. G. ...|$|R
40|$|The Monetary Control Act of 1980 {{requires}} the Federal Reserve to charge customers for financial services, {{with the intent}} of improving the efficiency with which Fed offices deliver those services. Prior studies found little improvement in the efficiency of Fed <b>check</b> processing <b>operations</b> after pricing was implemented in 1982. This article examines the efficiency of Fed <b>check</b> <b>operations</b> using a longer sample period (1980 :Q 1 – 2003 :Q 3) than previous studies and new methods for estimating efficiency. The authors find that the median office became somewhat less efficient when pricing was introduced, but that efficiency improved through the 1990 s. Although they find that Fed offices became somewhat less efficient on average after 1999, this might reflect adjustments associated with declining check volumes and implementation of a common operating platform across System offices. Payment systems; Check collection systems...|$|R
5000|$|... "Mauritanian postal company" [...] {{carries out}} postal service between provincial capitals, carries out money transfers, money orders, {{processing}} of mail <b>checks,</b> savings <b>operations,</b> and investment activities.|$|R
5000|$|... {{complex of}} the {{technical}} solutions worked-out and <b>checked</b> through <b>operation</b> considering the efforts targeted at their upgrading, and elimination of the “weak links” revealed during operation; ...|$|R
40|$|This diploma theses is {{concerned}} with the meaning of sport sailing and sailing in general. This draft develops a small craft conception with regart to the characteristics of the machines in the future, where control systems will actively participate with <b>checking</b> <b>operations</b> and facilitate not only running complex unstable systems but above all, smooth processes operated by man. The designed sports craft is intended for three actively living passengers with positive relations to high adrenalin activities who long for a very near contact with the nature...|$|R
50|$|Soyuz TMA-05M {{completed}} a successful docking at 4:51 GMT while {{the space station}} was flying high above North-East Kazakhstan: one minute earlier than planned. Shortly after, the docking probe was retracted and hooks started closing to establish the hard mate. The hard mate {{was followed by the}} standard one-hour leak <b>check</b> <b>operations.</b> After leak <b>checks</b> were completed, the crew opened the hatches and floated into the ISS. The arrival of Malenchenko, Williams and Hoshide on board Soyuz TMA-05M restored the space station's crew to full strength at six.|$|R
50|$|Fitchen {{served for}} 23 {{years with the}} Federal Reserve Bank of New York in banking {{relations}} and as an examiner {{and then with the}} bank's cash and <b>check</b> handling <b>operations.</b>|$|R
40|$|The human {{resources}} department {{of a financial}} institution implemented a multi-component intervention to replace a paper-based hiring system. Organization-wide impacts included changes in the background <b>check</b> <b>operations.</b> To support changes, training was conducted and procedure manuals distributed. Turn time for background checks decreased, but {{a combination of factors}} may be responsible. Other metrics are either inconclusive or suggest a confounding variable, yet quality of work did not suffer was maintained. Desired system use was achieved, accompanied by improvements in time-to-fill, voluntary turnover, and quality of applicants. Considerations for analysis and challenges faced are discussed, along with suggestions for further clarification and improvements...|$|R
5000|$|X9B - <b>Checks</b> and Back-office <b>Operations</b> (all things {{related to}} checks); ...|$|R
50|$|CPCS (Check Processing Control System) is an IBM {{software}} product that supports high speed <b>check</b> sorting <b>operations</b> within financial institutions. The software works {{in conjunction with}} check sorting equipment, such as the IBM 3890.|$|R
30|$|A {{few minutes}} of {{practice}} time were given to <b>check</b> the <b>operation</b> of the PC and {{the motion of the}} robot in the operation area after the task had been explained to the participants.|$|R
40|$|We {{propose a}} new {{framework}} for turning quantum search algorithms that decide into quantum algorithms for finding a solution. Consider {{we are given}} an abstract quantum search algorithm A that can determine whether a target g exists or not. We give a general construction of another operator U that both determines and finds the target, whenever one exists. Our amplification method at most doubles the cost over using A, has little overhead, and works by controlling the evolution of A. This is the first known general framework to the open question of turning abstract quantum search algorithms into quantum algorithms for finding a solution. We next apply the framework to random walks. We develop a new classical algorithm and a new quantum algorithm for finding a unique marked element. Our new random walk finds a unique marked element using H update operations and 1 /eps <b>checking</b> <b>operations.</b> Here H is the hitting time, and eps is {{the probability that the}} stationary distribution of the walk is in the marked state. Our classical walk is derived via quantum arguments. Our new quantum algorithm finds a unique marked element using H^(1 / 2) update operations and 1 /eps^(1 / 2) <b>checking</b> <b>operations,</b> up to logarithmic factors. This is the first known quantum algorithm being simultaneously quadratically faster in both parameters. We also show that the framework can simulate Grover 2 ̆ 7 s quantum search algorithm, amplitude amplification, Szegedy 2 ̆ 7 s quantum walks, and quantum interpolated walks...|$|R
50|$|Eunomus (Εὔνομος) was an Athenian admiral {{during the}} Corinthian War. In 389 BC {{he was put}} in charge of a fleet of 13 triremes to <b>check</b> the <b>operation</b> of the Spartan {{commander}} Gorgopas operating out of Aegina.|$|R
50|$|FOP method {{may be used}} in {{bottom-up}} nanofabrication {{to implement}} high-precision movement of the nanolithograph/nanoassembler probe along the substrate surface. Moreover, once made along some route, FOP may be then exactly repeated the required number of times. After movement in the specified position, an influence on the surface or manipulation of a surface object (nanoparticle, molecule, atom) is performed. All the operations are carried out in automatic mode. With multiprobe instruments, FOP approach allows to apply any number of specialized technological and/or analytical probes successively to a surface feature/object or to a specified point of the feature/object neighborhood. That opens a prospect for building a complex nanofabrication consisting {{of a large number of}} technological, measuring, and <b>checking</b> <b>operations.</b>|$|R
40|$|We {{examine the}} problem of compactly expressing models of non-Markovian reward {{decision}} processes (NMRDP). In the field of decision-theoretic planning NMRDPs are used whenever the agent's reward {{is determined by the}} history of visited states. Two different propositional linear temporal logics can be used to describe execution histories that are rewarding. Called PLTL and $FLTL, they are backward and forward looking logics respectively. In this paper we find both to be expressively weak and propose a change to $FLTL resulting in a much more expressive logic that we have called $* FLTL. The time complexities of $* FLTL and $FLTL related model <b>checking</b> <b>operations</b> performed in planning are the same. Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|R
