1|36|Public
40|$|AbstractComputation of {{approximate}} polynomial greatest common divisors (GCDs) {{is important}} both theoretically {{and due to}} its applications to control linear systems, network theory, and computer-aided design. We study two approaches to the solution so far omitted by the researchers, despite intensive recent work in this area. Correlation to numerical Padé approximation enabled us to improve computations for both problems (GCDs and Padé). Reduction to the approximation of polynomial zeros enabled us to obtain a new insight into the GCD problem and to devise effective solution algorithms. In particular, unlike the known algorithms, we estimate the degree of approximate GCDs at a low computational cost, and this enables us to obtain <b>certified</b> <b>correct</b> solution for a large class of input polynomials. We also restate the problem {{in terms of the}} norm of the perturbation of the zeros (rather than the coefficients) of the input polynomials, which leads us to the fast certified solution for any pair of input polynomials via the computation of their roots and the maximum matchings or connected components in the associated bipartite graph...|$|E
40|$|We {{present a}} new {{elementary}} function library, called CR-LIBM. This library implements the various functions {{defined by the}} Ansi 99 C standard. It provides correctly rounded functions. When writing this library, our primarily goal was to <b>certify</b> <b>correct</b> rounding, and make it reasonably fast, and with a low utilisation of memory. Hence, our library can be used without any problem on real-scale problems. We are also giving the proof and the elements to understand {{the implementation of the}} exponential function of CR-LIBM...|$|R
60|$|I {{leave it}} to Professor Max Muller to <b>certify</b> or <b>correct</b> for you the details of Mr. Cockburn's research,[11]--this main head of it I can {{positively}} confirm, that in old Scotch,--that of Bishop Douglas,--the word 'Rois' stands alike for King, and Rose.|$|R
50|$|Railworthiness is the {{condition}} of the rail system and its suitability for rail operations in that it has been designed, constructed, maintained and operated to approved standards and limitations by competent and authorised individuals, who are acting as members of an approved organisation and whose work is both <b>certified</b> as <b>correct</b> and accepted on behalf of the rail system owner.|$|R
40|$|Abstract – In {{this paper}} {{the study of}} direct AC-AC {{converter}} topologies, which make use of switches in commercial settings, will be performed. Several topologies are presented, including some already known in the literature and new ones. For one of the presented topologies, {{the design of a}} 3 kVA line conditioner is developed and experimental results are shown, <b>certifying</b> the <b>correct</b> operation of the drive strategy used...|$|R
50|$|The {{application}} of airworthiness defines {{the condition of}} an aircraft and supplies the basis for judgment of the suitability for flight of that aircraft, in {{that it has been}} designed with engineering rigor, constructed, maintained and is expected to be operated to approved standards and limitations, by competent and approved individuals, who are acting as members of an approved organization and whose work is both <b>certified</b> as <b>correct</b> and accepted on behalf of the State.|$|R
25|$|The Prime Minister may {{from time}} to time, but not later than {{three years after the}} last general election, direct that the electoral {{registers}} be revised; and may, before a general election, require the registers to be brought up to date by reference to a particular year. After registers have been prepared or updated, they are made available for public inspection to enable people to submit claims to be included in registers or to raise objections concerning the inclusion of other people in the registers. After all claims and objections have been dealt with, the registers are <b>certified</b> as <b>correct.</b>|$|R
50|$|The {{second phase}} begins when the accused person {{is brought to}} court and the summary {{procedure}} is invoked. At this stage {{it is not necessary}} for the court to be satisfied afresh as to whether the two pre-conditions exist. Their existence will ordinarily appear from the record and therefore be prima facie established. The court is indeed required to record in full the proceedings at which the warning is given, and an extract of such proceedings, if <b>certified</b> as <b>correct,</b> is prima facie proof of the warning given. It is therefore imperative that the warning be recorded in full.|$|R
40|$|Precise {{specifications}} {{are needed}} for verifying and <b>certifying</b> the <b>correct</b> behavior of critical systems. However, traditional proofreading and test based verification techniques are usually not exhaustive and as systems become more complex, their coverage is less and less adequate. Use of models allows early verification, validation and automated building of "correct by construction" systems. Our work targets formal specification and verification of model transformations. In a previous paper we tackled the problem of writing formal speci- fications for model transformations independently to the implementation technique. In this paper we investigate the implementation phase of these specifications as model transforma- tions using traditional MDE techniques and the difficulties encountered while generating the verification materials...|$|R
40|$|The Morse-Smale {{complex is}} an {{important}} tool for global topological analysis in various problems of computational geometry and topology. Algorithms for Morse-Smale complexes have been presented in case of piecewise linear manifolds. However, previous {{research in this field}} is incomplete in the case of smooth functions. In the current paper we address the following question: Given an arbitrarily complex Morse-Smale system on a planar domain, is it possible to compute its <b>certified</b> (topologically <b>correct)</b> Morse-Smale complex? Towards this, we develop an algorithm using interval arithmetic to compute certified critical points and separatrices forming the Morse-Smale complexes of smooth functions on bounded planar domain. Our algorithm can also compute geometrically close Morse-Smale complexes. Comment: Under Review in Journa...|$|R
40|$|Abstmct-Synchronous and {{asynchronous}} operation of software cesses. Such a state {{may not have}} been anticipated by the syssystems are defined. It is argued that <b>certifying</b> the <b>correct</b> operation tem designers, nor can it be easily reproduced. of a system in the synchronous mode is significantly simpler than in the The first proposals to control the complexity of a system asynchronous mode. A series of compile-time and run-time restrictions for systems constructed in Concuirent Pascal are presented which were made by Dijkstra [6], [7]. He described a level struc-assure equivalent operation in the synchronous and asynchronous tured approach to system construction in which a system can modes. be implemented and debugged incrementally. This method Index Terms-Asynchronous processes, classes, Concurrent Pascal, concurrent processes, correctness, hierarchical operating systems...|$|R
40|$|Abstract. In {{this paper}} we present our {{formalization}} of two important termination techniques for term rewrite systems: the subterm criterion and the reduction pair processor {{in combination with}} usable rules. For both techniques we developed executable check functions using the theorem prover Isabelle/HOL. These functions are able to <b>certify</b> the <b>correct</b> application of the formalized techniques in a given termination proof. As there are several variants of usable rules, we designed our check function {{in such a way}} that it accepts all known variants, even those which are not explicitly spelled out in previous papers. We integrated our formalization in the publicly available IsaFoR-library. This led to a significant increase in the power of CeTA, a certified termination proof checker that is extracted from IsaFoR. 1...|$|R
40|$|In {{this paper}} we present our {{formalization}} of two important termination techniques for term rewrite systems: the subterm criterion and the reduction pair processor {{in combination with}} usable rules. For both techniques we developed executable check functions in the theorem prover Isabelle/HOL which can <b>certify</b> the <b>correct</b> application of these techniques in some given termination proof. As there are several variants of usable rules we designed our check function {{in such a way}} that it accepts all known variants, even those which are not explicitly spelled out in previous papers. We integrated our formalization in the publicly available IsaFoR-library. This led to a significant increase in the power of CeTA, the corresponding certified termination proof checker that is extracted from IsaFoR...|$|R
40|$|STATCHECK is an R {{algorithm}} {{designed to}} scan papers automatically for inconsistencies between test statistics and their associated p values (Nuijten et al., 2016). The {{goal of this}} comment is to point out an important and well-documented flaw in this busily applied algorithm: It cannot handle corrected p values. As a result, statistical tests applying appropriate corrections to the p value (e. g., for multiple tests, post-hoc tests, violations of assumptions, etc.) {{are likely to be}} flagged as reporting inconsistent statistics, whereas papers omitting necessary corrections are <b>certified</b> as <b>correct.</b> The STATCHECK algorithm is thus valid for only a subset of scientific papers, and conclusions about the quality or integrity of statistical reports should never be based solely on this program...|$|R
40|$|Logical interpolants {{have found}} {{a wide array of}} {{applications}} in automated verification, including symbolic model checking and predicate abstraction. It is often critical to these applications that reported interpolants exhibit desired properties, correctness being first and foremost. In this paper, we introduce a method in which interpolants are computed by type inference within the trusted core of a proof checker. Interpolants produced this way from a proof of the joint unsatisfiability of two formulas are <b>certified</b> as <b>correct</b> by construction. We focus our attention to the quantifier-free theory of equality and uninterpreted functions (EUF) and present an interpolant generating proof calculus that can be encoded in the LFSC proof checking framework with limited reliance upon computational side conditions. Our experimental results show that our method generates certified interpolants with small overhead with respect to solving. ...|$|R
40|$|Along the years, {{researchers}} have made {{efforts in the}} sense of quantifying structural performance and damage in function of vibration measurements, [1] and [2]. Similarly the present work investigates a vibration based criteria for bridge structures as a basis to evaluate damage in existing bridges. With this purpose, a bridge data base collected during field tests accomplished in Brazilian bridges was used. The results show a clear correlation between the damage index and the intensity of bridge vibration. As a final result, from the observed correlations, limits of acceleration are proposed to be considered in existing bridges to <b>certify</b> a <b>correct</b> long-term condition. A critical approach of current Brazilian Standard (NBR- 15307 2005) which suggests damage estimation in function of bridge vibration is also carried out. Postprint (published version...|$|R
40|$|We {{introduce}} {{an intermediate}} quantum computing model built from translation-invariant Ising-interacting spins. Despite being non-universal, the model cannot be classically efficiently simulated unless the polynomial hierarchy collapses. Equipped with the intrinsic single-instance-hardness property, a single fixed unitary evolution in our model {{is sufficient to}} produce classically intractable results, compared to several other models that rely on implementation of an ensemble of different unitaries (instances). We propose a feasible experimental scheme to implement our Hamiltonian model using cold atoms trapped in a square optical lattice. We formulate a procedure to <b>certify</b> the <b>correct</b> functioning of this quantum machine. The certification requires only a polynomial number of local measurements assuming measurement imperfections are sufficiently small. Comment: Phys. Rev. Lett. (2017, in press), "one-instance" is replaced by "single-instance-hardness", references added, "Simulation with variation Distance Errors" in Supplemental Material is rewritten in a clearer wa...|$|R
40|$|One of {{the main}} {{challenges}} {{in the field of}} quantum simulation and computation is to identify ways to <b>certify</b> the <b>correct</b> functioning of a device when a classical efficient simulation is not available. Important cases are situations in which one cannot classically calculate local expectation values of state preparations efficiently. In this work, we develop weak-membership formulations of the certification of ground state preparations. We provide a non-interactive protocol for certifying ground states of frustration-free Hamiltonians based on simple energy measurements of local Hamiltonian terms. This certification protocol can be applied to classically intractable analog quantum simulations: For example, using Feynman-Kitaev Hamiltonians, one can encode universal quantum computation in such ground states. Moreover, our certification protocol is applicable to ground states encodings of IQP circuits demonstration of quantum supremacy. These can be certified efficiently when the error is polynomially bounded. Comment: 10 pages, corrected a small error in Eqs. (2) and (5...|$|R
40|$|Contained in the {{following}} report are data for radioactivity in the environment collected and analyzed by Princeton Plasma Physics Laboratory’s Princeton Environmental, Analytical, and Radiological Laboratory (PEARL). The PEARL is located on‐site and is certified for analyzing radiological and non‐radiological parameters through the New Jersey Department of Environmental Protection’s Laboratory Certification Program, Certification Number 12471. Non‐radiological surface and ground water samples are analyzed by NJDEP certified subcontractor laboratories – QC, Inc. and Accutest Laboratory. To {{the best of our}} knowledge, these data, as contained in the “Annual Site Environmental Report for 2011,” are documented and <b>certified</b> to be <b>correct...</b>|$|R
40|$|Background: Invasive {{practical}} procedures require {{identification of}} surface anatomical landmarks to reduce {{risk of damage}} to other structures. Needle thoracocentesis has specific complications, which have been previously documented. An observational study was performed among emergency physicians to name the landmark for needle thoracocentesis and identify this point on a human volunteer as per Advanced Trauma and Life Support (ATLS) guidelines. Results: A cohort of 25 emergency physicians was studied, 21 (84 %) of which were ATLS <b>certified.</b> The <b>correct</b> landmark was named by 22 (88 %). Only 15 (60 %) correctly identified the second intercostal space on the human volunteer, all placing the needle medial to the midclavicular line, {{with a range of}} 3 cm. Two (8 %) named and identified the site of needle pericardiocentesis; one (4 %) named and identified the fifth intercostal space in the anterior axillary line. Discussion: These results demonstrate a low accuracy among emergency physicians in identifying correct landmarks for needle thoracocentesis under elective conditions. Should greater emphasis be placed on competency based training in ATLS...|$|R
40|$|Abstract. Oval {{bevel gear}} is {{introduced}} {{as a new}} gear transmission type of intersecting axes, in which high-order, denaturing and eccentric one are included. Based on the theory of spatial engagement, spatial spherical coordinate system of the noncircular bevel gearing was developed and a generalized tooth profile equation of noncircular bevel gears was also proposed. The gear profile can be enveloped for the generating movement between conical cutter and oval bevel gear according to the envelope principle of conjugate profiles. Then a new and general parametric design method for oval bevel gear was founded. The classification and gear variability condition of oval bevel gear were discussed. Meanwhile, its pitch curve, addendum and dedendum curve were studied, and the corresponding equations were deduced. A roll check experiment is performed on the prototype workpiece. Then the influences of motor speed and load on transmission ratio were analyzed. Finally, the design method, profile generation method and manufacture method of oval bevel gear were <b>certified</b> as <b>correct</b> by comparing experimental curve of transmission ratio with theoretical one...|$|R
50|$|The Preceptory gained {{somewhat}} of a reputation for fraud and abusing their privileges during the 13th and 14th centuries. The preceptory gained land in Compton {{on the outskirts of}} the town of Ashbourne, Derbyshire; but the Hospitallers were unpopular in the town (at the time a Royal Borough), and the complaints are recorded as early as 1276. Their privileges included <b>certifying</b> as <b>correct</b> the gallon and bushel measures: they abused this privilege by allowing their tenants to sell bread and bear in false measures. The Hospitallers were also able to extend their privileges, such as freedom from road and bridge tolls, to their tenants; this further aided their unpopularity as the royal borough of Ashbourne was seen to suffer as the Hospitallers increased their number of tenants and profiteered further. Similar dodgy dealings occurred in 1330 when a brother of the order, William Brix, slammed the door of the manor house at Barlow in the face of the Sheriff's Officer who had come to check the order's weights and measures.|$|R
40|$|Broadside view {{of steam}} frigate in foreground, {{broadside}} view of ship on fire in background. Signed lower right of image: G. Perkins. Title inscribed below image. Inscribed on mat : Ord. Div. No. 6035. A Lieutenant George Perkins {{served in the}} Union Navy. See: The Coastal War (in the Time-Life Civil War Series), pp. 73 - 74. According to the Harper's story, {{the captain of the}} Brilliante certified that this image is an accurate rendering of the Alabama. The certification is dated 18 October 1862, apparently after the Brilliante and its goods were seized. Signed G. Perkins, probably the artist Granville Perkins. Published in: Harper's Weekly, 1 November 1862, p. 689 (cover), as: The Pirate "Alabama," Alias " 290," <b>Certified</b> to be <b>Correct</b> y Captain Hagar of the "Brilliant. "Source unknown...|$|R
40|$|Along the years, {{there were}} efforts by {{researchers}} in quantifying structural performance and damage in function of vibration measurements. Curves proposed by several authors try to relate acceleration spectrum to damage level, which was determined based on experimental survey accomplished on buildings. The present work investigates a vibration based criteria for bridge structures {{as a basis}} to evaluate damage and performance in existing bridges and also {{to be used for}} long-term performance at the time of bridge design. With this purpose, it was analyzed a data base of Brazilian bridges which were structurally identified and had their dynamic effects measured during traffic flow. For the damage evaluation, a damage index based on the change of the dynamic properties and the general condition of these structures observed during a detailed inspection were taken into account. After that, damage indexes were related to measured vibration too. The results show a clear correlation between the damage index and the fatigue reliability index with the intensity of bridge vibration. As a final result, from the observed correlations, limits of acceleration are proposed to be considered in existing and newly designed bridges to <b>certify</b> a <b>correct</b> long-term condition and safety to fatigue effects. A critical approach of current Brazilian Standard (NBR- 15307 2005) which suggests damage estimation in function of bridge vibration index is also carried out. Postprint (published version...|$|R
40|$|Abstract — Existing {{electronic}} healthcare systems {{based on}} PACS and Hospital IS {{are designed for}} clinical practice. Yet, both for security, technical and legacy reasons, they are often weakly connected to computing infrastructures and data networks. In {{the context of the}} RAGTIME project, grid infrastructures are studied to propose a cheap and reliable infrastructure enabling computerized medical applications. This raises various concerns, in particular in terms of security and data privacy. This paper presents {{the results of this study}} and proposes a complete grid-based architecture able to process medical image for assisted diagnosis in a secured way. Using this infrastructure, care practitioner are able to execute the application from any machine connected to the Internet, therefore improving their mobility. Medical image analysis jobs are <b>certified</b> to be <b>correct</b> using the latest advances in result checking and fault-tolerant algorithms provided in [1], [2]. The architecture has been successfully de-ployed and validated on the Grid 5000 large scale infrastructure. I...|$|R
40|$|International audienceExisting {{electronic}} healthcare systems {{based on}} PACS and Hospital IS {{are designed for}} clinical practice. Yet, both for security, technical and legacy reasons, they are often weakly connected to computing infrastructures and data networks. In {{the context of the}} RAGTIME project, grid infrastructures are studied to propose a cheap and reliable infrastructure enabling computerized medical applications. This raises various concerns, in particular in terms of security and data privacy. This paper presents {{the results of this study}} and proposes a complete grid-based architecture able to process medical image for assisted diagnosis in a secured way. Using this infrastructure, care practitioner are able to execute the application from any machine connected to the Internet, therefore improving their mobility. Medical image analysis jobs are <b>certified</b> to be <b>correct</b> using the latest advances in result checking and fault-tolerant algorithms. The architecture has been successfully deployed and validated on the Grid 5000 large scale infrastructure...|$|R
40|$|This paper {{presents}} how {{to automatically}} prove that an "optimized " program is correct {{with respect to}} a set of given properties that is a specification. Proofs of specifications contain logical and computational parts. Programs can be seen as computational parts of proofs. They can thus be extracted from proofs and be <b>certified</b> to be <b>correct.</b> The inverse problem can be solved: it is possible to reconstruct proof obligations from a program and its specification [18, 19]. The framework is a type theory where a proof can be represented as a typed -term [2, 14] and, particularly, the Calculus of Inductive Constructions [7]. This paper shows how programs can be simplified in order to be written in a much closer way to the ML one's. Indeed, proofs structures are often much more heavy than programs structures. The problem is consequently to consider natural programs (in a ML sense) and see how to retrieve natural structures of proofs from them. This research was partly supported by ESPRIT [...] ...|$|R
40|$|We want {{to prove}} "automatically" that a program is correct {{with respect to a}} set of given {{properties}} that is a specification. Proofs of specifications contain logical parts and computational parts. Programs can be seen as computational parts of proofs. They can then be extracted from proofs and be <b>certified</b> to be <b>correct.</b> We focus on the inverse problem : is it possible to reconstruct proof obligations from a program and its specification ? The framework is the type theory where a proof can be represented as a typed -term [Con 86, NPS 90] and particularly the Calculus of Inductive Constructions [Coq 85]. A notion of coherence is introduced between a specification and a program containing annotations as in the Hoare sense. This notion is based on the definition of an extraction function called the weak extraction. Such an annotated program can give a method to reconstruct a set of proof obligations needed to have a proof of the initial specification. This can be seen either as a method o [...] ...|$|R
40|$|Agricultural {{production}} {{is an important}} source of income and employment for developing countries, yet it is the cause of serious environmental problems. Though ECO-labels appear as a promising alternative to control the negative effects of agriculture on the environment and to increase the income of rural poor, the proportion of agricultural land and exports certified as is quite small. We investigate the factors that affect the adoption of certified organic coffee in Colombia and in particular study the effect of economic incentives on adoption. We find that those who have lower cost of adoption {{are more likely to be}} <b>certified</b> as organic. <b>Correcting</b> for sample selection, we find that certified organic {{production is}} 40 % less productive and 31 % less costly than non-certified production. Given the price premium in 2007, certified organic production is 15 % less profitable than non-organic production. We find that in order to make organic production attractive, the price premium of certified organic coffee should be about 5 times higher than in 2007. [...] Job Creation,Job Destruction,Job Reallocation,Firm Dynamics,Africa,Ethiopia...|$|R
40|$|AbstractWe present here {{a general}} and {{efficient}} strategy for simulating a synchronous network by {{a network of}} limited asynchrony. Our proposed synchronizer is optimistic {{in the sense that}} it uses very efficient but tentative protocols to simulate a contiguous block of synchronous steps. However, since a tentative execution does not guarantee correct simulation, we audit the computation at selected points. The audits are used to check whether the computation of the block can be <b>certified</b> to be <b>correct.</b> We show that a wide class of networks of limited asynchrony admits practical tentative protocols which are highly likely to produce a correct simulation of one step with very small overhead. For those networks, the synchronizer exhibits a trade off between its communication and time complexities which is below the lower bounds for deterministic synchronizers. On one extreme the amortized complexity of our synchronizer is O(1) messages and O(log n) time (expected) per “step” of the simulated synchronous protocol. On other extreme the communication complexity is O(eΔ 2) and the time complexity is O(log Δ), for networks with e edges and maximum degree Δ...|$|R
40|$|This {{contribution}} {{is a response}} to the rebuttal of Agnew et al. (2012) to Froese and Proelss (2012) “Evaluation and legal assessment of <b>certified</b> seafood”. It <b>corrects</b> some factually wrong statements in the rebuttal, revisits the definitions of ‘depleted’ and ‘overfished’, and notes that the rebuttal agrees with the international definition of ‘overfishing’ (F>FMSY) that was used by Froese and Proelss (2012). The rebuttal presents an analysis of 45 MSC-certified stocks. Of these, 27 % are ‘depleted’ (according to the definition used by MSC) or ‘overfished’ (according to the definition used by Froese and Proelss 2012) and 16 % are subject to ‘overfishing’, basically confirming the critique of Froese and Proelss (2012). This response concludes that MSC has to change its rules for certification such that (1) overfishing is not allowed and (2) ‘depleted’ stocks are marked as such. âº This paper confirms previous definitions of ‘overfishing’ and ‘overfished’. âº It notes that MSC-certified fisheries may engage in overfishing. âº It notes that MSC acknowledges ongoing overfishing in 16 % of certified stocks. âº It notes that MSC acknowledges that 27 % of certified stocks are depleted...|$|R
40|$|Distributed, {{real-time}} embedded (DRE) systems, such as {{that being}} developed under the DARPA Adaptive Reflective Middleware System (ARMS) program, require predictable, controlled real-time behavior. It {{is a challenge to}} develop ways to measure and evaluate the quality, or utility, of DRE system performance {{in the context of the}} dynamic, unpredictable environments in which they operate. This difficulty runs hand-in-hand with the challenge of providing adequate control of the system to effect behavior in the system that will allow the controlled system to be <b>certified</b> as exhibiting <b>correct</b> behavior. As part of the ARMS program, we have been developing utility-based measures of system quality for assessing correct behavior of a system and for driving feedback control at runtime. This paper describes the real-time utility measures we have developed for ARMS and their use as feedback signals to a control system that manages the dynamic allocation of resources to applications in the system. We then explore the issues associated with using utility-based assessment as part of an evidence-based certification process for dynamic real-time systems. Categories and Subject Descriptors C. 3 [Computer Systems Organization] Special-Purpose and Application-Based- real-time and embedded systems C. 4 [Computer Systems Organization] Performance of Systems- measurement techniques, performance attributes, reliability, availability, and serviceabilit...|$|R
40|$|Management of data {{collected}} during projects that involve {{large numbers of}} scientists is an often overlooked aspect of the experimental plan. Ecosystem science projects like the Oregon Transect Ecosystem Research (OTTER) Project that involve many investigators from many institutions and that run for multiple years, collect and archive large amounts of data. These data {{range in size from}} a few kilobytes of information for such measurements as canopy chemistry and meteorological variables, to hundreds of megabytes of information for such items as views from multi-band spectrometers flown on aircraft and scenes from imaging radiometers aboard satellites. Organizing and storing data from the OTTER Project, <b>certifying</b> those data, <b>correcting</b> errors in data sets, validating the data, and distributing those data to other OTTER investigators is a major undertaking. Using the National Aeronautics and Space Administration's (NASA) Pilot Land Data System (PLDS), a Support mechanism was established for the OTTER Project which accomplished all of the above. At the onset of the interaction between PLDS and OTTER, it was not certain that PLDS could accomplish these tasks in a manner that would aid researchers in the OTTER Project. This paper documents the data types that were collected {{under the auspices of the}} OTTER Project and the procedures implemented to store, catalog, validate, and certify those data. The issues of the compliance of investigators with data-management requirements, data use and certification, and the ease of retrieving data are discussed. We advance the hypothesis that formal data management is necessary in ecological investigations involving multiple investigators using many data gathering instruments and experimental procedures. The issues and experience gained in this exercise give an indication of the needs for data management systems that must be addressed in the coming decades when other large data-gathering endeavors are undertaken by the ecological science community...|$|R
40|$|Estruturas cada vez mais complexas e de maiores dimensões vêm aumentando a aplicabilidade de aços de baixa liga e alta resistência, devido à redução de peso e custo dessas estruturas. Um dos requisitos para o uso desses materiais é a manutenção do desempenho após soldagem. Entretanto, as normas em que se baseiam as Especificações de Procedimentos de Soldagem (EPS) ainda não consideram aços mais modernos em termos de rota de fabricação, o que pode fazer com que custos desnecessários de soldagem minimizem os ganhos da aplicação desses aços. Este trabalho teve como objetivo o desenvolvimento e avaliação de uma metodologia para, experimentalmente, orientar a elaboração e o controle da aplicação de procedimentos de soldagem para aços estruturais, através de atlas microestrutural de regiões da zona afetada pelo calor (ZAC). Propõe-se que, através de um atlas microestrutural de um dado aço, seja possível determinar a faixa otimizada de energia de soldagem para um dado processo na elaboração e aplicação da EPS e, consequentemente, as velocidades de resfriamento que o aço possa sofrer durante a soldagem, sem perder as propriedades mecânicas e sem colocálo em risco quanto a trincas a frio. Tomou-se como estudo de caso o aço produzido por laminação controlada de classe de resistência de 65 ksi (ASTM A 572 Grau 65), utilizado em um projeto de um prédio {{industrial}} na empresa CBMM. Trata-se de um aço fabricado pelo processo TMCP com resfriamento acelerado. A elaboração do Atlas se deu através da construção de um diagrama CCT, por simulação física (dilatômetro e Gleeble), da região de grãos grosseiros da zona afetada pelo calor (ZAC GG). Foram feitas caracterizações metalográficas e mecânicas das regiões simuladas. Microestruturas de soldas realizadas com EPS qualificadas foram comparadas com as do Atlas para se certificar da adequabilidade dos parâmetros utilizados e validação da abordagem. Foram realizadas ainda a qualificação e quantificação de potenciais benefícios econômicos no citado projeto industrial, obtidos pelo uso desta metodologia. As microestruturas apresentadas no mapa variavam de martensíta, para altas taxas de resfriamento, até perlita/ferrita de tamanho de grão elevado, para baixas taxas de resfriamento. Observou-se notável predominância da microestrutura bainítica em uma larga faixa de taxas de resfriamento, compatível com as propriedades e composição do aço estudado (alta soldabilidade). As comparações com as microestruturas de soldas reais mostraram que o Atlas pode descrever de forma precisa o ciclo térmico efetivamente imposto ix na ZAC GG. Concluiu-se que a aplicação desta metodologia na elaboração de novas EPS permitiria uma maior flexibilidade nos procedimentos de soldagem, admitindo inclusive soldagem sem pré-aquecimento. Em relação a não necessidade de pré-aquecimento, podese prever uma economia significante de custos e redução de emissão de gases que provocam efeito estufa. More {{complex and}} bigger structures {{have increased the}} applicability of low alloy high strength steels due to weight and cost reductions in these projects. One of the requirements {{for the use of}} these materials is the preservation of performance after welding. Meanwhile, the norms on which the Welding Procedures Specifications (WPS) are based have not yet considered the development of modern steel and its new production process, resulting in unnecessary welding costs that diminish the profits of the application of this type of steel. This thesis aimed to develop and evaluate an experimental methodology to guide the creation and control of welding procedures for structural steel through a microstructural atlas of the heat affected zone (HAZ) in a thermomechanical control process (TMCP), 65 ksi steel (ASTM A 572 Grade 65). This steel was used in the project of an industrial building for CBMM in Araxá, Minas Gerais, Brazil. It is proposed that through a microstructural atlas of a given steel, it is possible to determine the range of cooling rates that the steel may suffer during welding without affecting mechanical properties and without risking cold cracks. When comparing the microstructure of steel welds performed in field conditions, it is possible to determine the heat input range for a given process in the preparation of a WPS. The selected case study is from a high strength low alloy class 65 ksi steel (ASTM A 572 Grade 65) that was used in the structure of an industrial building. The steel was produced using TMCP. The atlas was created via the construction of a continuous cooling transformation diagram using physical simulation (dilatometer and Gleeble) of the coarse grain HAZ (GCHAZ). The characterization of the simulated region was performed by metallography and mechanical tests. The microstructure of real welds made by a qualified WPS were compared to the atlas in order to <b>certify</b> the <b>correct</b> use of parameters and to validate the method. The methodology was also qualified and the potential economic benefits were quantified (based only on the reduction of consumables used and the increased availability of the welding process machine) for the selected industrial project. The mapped microstructures varied from martensite (at high cooling rates) to pearlite/ferrite with large grain size (at low cooling rates). There was remarkable prevalence of bainitic microstructure {{in a wide range of}} cooling rates, consistent with the chemical composition of the steel studied. Comparisons with real weld microstructures showed the atlas is compatible with them, and that it can more accurately describe the effective thermal cycle xi that occurs in the coarse grain region of the HAZ (other regions were not included). The application of this methodology in the development of new WPS would allow greater flexibility in the welding procedures, including welding without preheating. In this respect alone, it was possible to forecast savings of approximately R$ 200, 000. 00, 1, 000 hours of processing and 172 tonnes of carbon equivalent emissions...|$|R

