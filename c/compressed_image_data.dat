57|10000|Public
2500|$|Medical imaging {{techniques}} produce very {{large amounts of}} data, especially from CT, MRI and PET modalities. [...] As a result, storage and communications of electronic image data are prohibitive {{without the use of}} compression. [...] JPEG 2000 is the state-of-the-art image compression DICOM standard for storage and transmission of medical images. [...] The cost and feasibility of accessing large image data sets over low or various bandwidths are further addressed by use of another DICOM standard, called JPIP, to enable efficient streaming of the JPEG 2000 <b>compressed</b> <b>image</b> <b>data.</b>|$|E
40|$|Transmission of <b>compressed</b> <b>image</b> <b>data</b> over Gaussian {{wireless}} channel using OFDM {{scheme is}} addressed. A wavelet thresholding technique based on automatic image quality assessment is proposed. Structural similarity quality assessment of image, which was {{proved to be}} highly conformed with the human visual evaluation, is utilized to obtain an optimum tradeoff between the compression ratio and image quality...|$|E
40|$|<b>Compressed</b> <b>image</b> <b>data</b> using pyramid coding {{technique}} vary {{in their}} importance for {{the reconstruction of the}} original image. In this paper we propose a method for joint error protection and encryption of such data where the important data are given more protection than the less important ones. This will allow for both reliable and secure transmission and/or storage of such data. IEE...|$|E
5000|$|Hauskes {{research}} interests {{are in the}} field of modelling visual perception in relation its detection and localization, and the processing, storing and <b>compressing</b> <b>image</b> <b>data.</b>|$|R
40|$|An {{algorithm}} that effects fast {{lossless compression}} of multispectral-image data {{is based on}} low-complexity, proven adaptive-filtering algorithms. This algorithm is intended for use in compressing multispectral-image data aboard spacecraft for transmission to Earth stations. Variants of this algorithm could be useful for lossless compression of three-dimensional medical imagery and, perhaps, for <b>compressing</b> <b>image</b> <b>data</b> in general...|$|R
40|$|Digital images require {{large amounts}} of memory to store and, when {{retrieved}} from the internet, can take {{a considerable amount of}} time to download. The Haar wavelet transform provides a method of <b>compressing</b> <b>image</b> <b>data</b> so that it takes up less memory. This paper will discuss the implementation of the Haar wavelet transform and some of its applications. 1...|$|R
40|$|In this paper, {{iterative}} projection algorithms {{are presented}} to reconstruct visually pleasing images from Block Discrete Cosine Transform (BDCT) <b>compressed</b> <b>image</b> <b>data.</b> Two algorithms are proposed. The first {{is based on}} the theory of Projections Onto Convex Sets(P 0 CS). The second is motivated by the theory of POCS. Experimental results are presented which demonstrate that the proposed algorithms yield supe-rior images to those obtained by direct reconstruction from the compressed data only. 1...|$|E
40|$|In {{a variety}} of {{applications}} (including automatic target recognition) image classification algorithms operate on <b>compressed</b> <b>image</b> <b>data.</b> This paper explores the design of optimal transform coders and scalar quantizers using Chernoff bounds on probability of misclassification as the measure of classification accuracy. This design improves classification performance but the mean square error (as well as the visual quality) of the coded image degrades. However, by appropriately combining classification accuracy and mean square error in the cost function, one can achieve good classification with low (visual) distortion, which is desirable in classification systems requiring visual authentication...|$|E
40|$|Abstract—As {{the circuit}} {{complexity}} is increasing {{in demand for}} the more computations on a single VLSI chip, low power VLSI design has become important specially for portable devices powered by battery. Digital camera {{is one of them}} where real-time image capturing, compression and storage of <b>compressed</b> <b>image</b> <b>data</b> is done. Most of the digital camera implement JPEG baseline algorithm to store highly compressed image in camera memory. In this paper we report and present low cost, low power and computationally efficient circuit design of JPEG for digital camera to get highly compressed image by exploiting removal of subjective redundancy from the image...|$|E
40|$|<b>Compressed</b> <b>images</b> {{are used}} very {{frequently}} in interactive applications in digital video broadcasting. New methods increasing {{efficiency of the}} image transmission in digital video broadcasting networks are proposed. Adaptive spatial filtering methods have been proposed for enhancement of the visual perception of the <b>compressed</b> <b>images.</b> New optimalization method is based on application of the filtering algorithms on more <b>compressed</b> <b>images</b> (<b>data</b> size are reduced). Visual quality enhancement is processed in interactive application. Further, new compression methods JPEG 2000 and H. 264 for image compression have been analysed. Novel compound image compression method for standard and high spatial television resolution is proposed in the thesis...|$|R
40|$|Traditionally, lossy {{compression}} schemes {{have focused on}} compressing data at fixed bit rates to either communicate information over limited bandwidth communi- cation channels, or to store information in a fixed-size storage media. In this paper we describe a class of 1 ossy algorithms {{that is capable of}} <b>compressing</b> <b>image</b> <b>data</b> over a wide range of rates so that quick browsing of large amounts of information as well as detailed examination of high resolution areas can be achieved by the same compression system...|$|R
40|$|Now a days {{most of the}} {{researchers}} are doing lots {{of work in the}} area of image compression. Fractal image compression requires lots of mathematical computation to <b>compress</b> an <b>image.</b> Fractal image compression is a recent technique based on the representation of an image by a contractive transform, on the space of images, for which the fixed point is close approximation to the original image. Main aim of fractal image compression algorithm is to reduce computation time required to <b>compress</b> <b>image</b> <b>data.</b> Fractal <b>image</b> compression is a lossy compression method for digital images, based on fractals. It is based on affine contractive transforms and utilizes the existence of self-symmetry in the image. This paper presents method for generating fractal images using iterated function system, method to partition <b>image</b> for <b>compressing</b> <b>image</b> using fractal image compression technique and various quality measures in fractal image compression...|$|R
40|$|Abstract- The {{rapid growth}} of {{wireless}} communication {{has resulted in a}} demand for robust transmission of compressed images over wireless channels. The challenge of robust transmission is to protect the <b>compressed</b> <b>image</b> <b>data</b> against loss, {{in such a way as}} to maximize the received image quality. The paper addresses this problem; investigating unequal error protection of JPEG 2000 compressed imagery. More particularly, the results reported in this paper provide guidance concerning the selection of JPEG 2000 coding parameters and appropriate combinations of RS (Reed-Solomon) codes, for typical wireless bit error rates in the range 10 - 4 to 10 - 3. 1...|$|E
40|$|Conference PaperTransmission of <b>compressed</b> <b>image</b> <b>data</b> over noisy {{channels}} is {{an important}} problem and has been investigated {{in a variety of}} scenarios. In this paper, we propose a progressive time-varying source-channel coding system for transmitting images over wireless channels. The core result of this paper is a systematic method of instantaneous rate allocation between the progressive source coder and channel coder. We develop closed form expressions for end-to-end distortion, as well as rate allocation, in memoryless channels. We extend the memoryless results to an algorithm for fading channels. Experimental results demonstrate the performance of this method. NokiaNational Science Foundatio...|$|E
40|$|The {{rapid growth}} of {{wireless}} communication {{has resulted in a}} demand for robust transmission of compressed images over wireless channels. The challenge of robust transmission is to protect the <b>compressed</b> <b>image</b> <b>data</b> against loss, {{in such a way as}} to maximize the received image quality. The paper addresses this problem; investigating unequal error protection of JPEG 2000 compressed imagery. More particularly, the result reported in this paper provide gidance concerning the selection of JPEG 2000 coding parameters and appropriate combinations of RS (Reed-Solomon) codes, for typical wireless bit error rates in the range 104 to 10 ' 5...|$|E
25|$|JPEG (Joint Photographic Experts Group) format {{can produce}} a smaller file than PNG for {{photographic}} (and photo-like) images, since JPEG uses a lossy encoding method specifically designed for photographic <b>image</b> <b>data,</b> which is typically dominated by soft, low-contrast transitions, and an amount of noise or similar irregular structures. Using PNG instead of a high-quality JPEG for such images {{would result in a}} large increase in filesize with negligible gain in quality. In comparison, when storing images that contain text, line art, or graphics – images with sharp transitions and large areas of solid color – the PNG format can <b>compress</b> <b>image</b> <b>data</b> more than JPEG can. Additionally, PNG is lossless, while JPEG produces noticeable visual artifacts around high-contrast areas. Where an image contains both sharp transitions and photographic parts, a choice must be made between the two effects. JPEG does not support transparency.|$|R
40|$|Abstract: Wavelet Transform {{has been}} {{proved to be a}} very useful tool for image {{processing}} in recent years. Digital images require large amounts of memory to store and, when retrieved from the internet, can take a considerable amount of time to download. Compression makes it possible for creating file sizes of manageable, storable and transmittable dimensions The Haar wavelet transform provides a method of <b>compressing</b> <b>image</b> <b>data</b> so that it takes up less memory. The most distinctive feature of Haar Transform {{lies in the fact that}} it lends itself easily to simple manual calculations. Modified Fast Haar Wavelet Transform is one of the algorithms which can reduce the calculation work in Haar Transform. The present paper attempts to describe the algorithm for image compression using MFHWT...|$|R
40|$|This paper {{describes}} a new straight-forward technique for lossless image compression, entitled SPM (Simple Prediction Method), {{which results in}} compression ratios similar to those achieved by the most powerful techniques described in the literature. The predictive model used by the method {{is one in which}} the current point is predicted as a weighted average of the preceding neighbouring points. The weights for this mask are encoded within the <b>compressed</b> <b>image.</b> The difference between this prediction and the actual value is generally small with a symmetric exponential distribution, and this difference is represented using arithmetic encoding. 1 Introduction. The need for image compression is due mainly to the large storage space required by most images. In many situations it is acceptable to <b>compress</b> <b>image</b> <b>data</b> in a lossy fashion, degrading the accuracy of the representation somewhat in order to gain significantly in terms of compression. For example, the original aims of JPEG were to pr [...] ...|$|R
40|$|One of {{the most}} {{important}} goals of current and future sensor networks is energy-efficient communication of images. This work presents a quantitative comparison between the energy costs associated with 1) direct transmission of uncompressed images and 2) sensor platform-based JPEG compression followed by transmission of the <b>compressed</b> <b>image</b> <b>data.</b> JPEG compression computations are mapped onto various resource-constrained sensor platforms using a design environment that allows computation using the minimum integer and fractional bit-widths needed in view of other approximations inherent in the compression process and choice of image quality parameters. Advanced applications of JPEG such as region of interest coding and successive/progressive transmission are also examined...|$|E
40|$|An {{efficient}} {{scheme for}} JPEG 2000 SNR progressive decoding is proposed, which {{is capable of}} handling JPEG 2000 <b>compressed</b> <b>image</b> <b>data</b> with SNR progressiveness. In order to avoid entropy decoding of the same compressed data more than once when decoding SNR progressive images, two techniques are introduced in our decoding scheme; reuse of intermediate decoding result and differential inverse discrete wavelet transform (differential IDWT). Comprehensive evaluation of our scheme demonstrating that with 26. 6 % increase of required memory size, up to 50 % of computational cost of entropy decoding can be reduced in comparison with conventional non-progressive decoding scheme when 9 / 7 irreversible DWT filter is used. Key words: JPEG 2000, Progressive decoding, Discrete wavelet transfor...|$|E
40|$|Energy-efficient image {{communication}} {{is one of}} the most important goals for a large class of current and future sensor network applications. This paper presents a quantitative comparison between the energy costs associated with 1) direct transmission of uncompressed images and 2) sensor platform-based JPEG compression followed by transmission of the <b>compressed</b> <b>image</b> <b>data.</b> JPEG compression computations are mapped onto various resource-constrained sensor platforms using a design environment that allows computation using the minimum integer and fractional bit-widths needed in view of other approximations inherent in the compression process and choice of image quality parameters. Detailed experimental results examining the tradeoffs in processor resources, processing/transmission time, bandwidth utilization, image quality, and overall energy consumption are presented...|$|E
40|$|In this thesis, I {{designed}} and implemented a model-adaptive data compression {{system for the}} compression of <b>image</b> <b>data.</b> The system is a realization and extension of the Model-Quantizer-Code-Separation Architecture for universal data compression which uses Low-Density-Parity-Check Codes for encoding and probabilistic graphical models and message-passing algorithms for decoding. We implement a lossless bi-level <b>image</b> <b>data</b> compressor {{as well as a}} lossy greyscale image compressor and explain how these compressors can rapidly adapt to changes in source models. We then show using these implementations that Restricted Boltzmann Machines are an effective source model for <b>compressing</b> <b>image</b> <b>data</b> compared to other compression methods by comparing compression performance using these source models on various image datasets. by Joshua Ka-Wing Lee. Thesis: S. M., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2017. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (pages 59 - 61) ...|$|R
5000|$|In July 1994, Daher {{was awarded}} a patent: # 5,327,254 Method and Apparatus for <b>Compressing</b> and Decompressing <b>Image</b> <b>Data.</b> [...] This {{invention}} became the foundation of LEAD's product line and business ventures.|$|R
40|$|The rise {{in digital}} {{technology}} has also rose {{the use of}} digital images. The digital imagesrequire much storage space. The compression techniques are used to compress the dataso that it takes up less storage space. In this regard wavelets play important role. Inthis thesis, we studied the Haar wavelet system, which is a complete orthonormal systemin L 2 (R) : This system consists of the functions j the father wavelet, and y the motherwavelet. The Haar wavelet transformation {{is an example of}} multiresolution analysis. Ourpurpose is to use the Haar wavelet basis to <b>compress</b> an <b>image</b> <b>data.</b> The method ofaveraging and differencing is used to construct the Haar wavelet basis. We have shownthat averaging and differencing method is an application of Haar wavelet transform. Afterdiscussing the compression by using Haar wavelet transform we used another method tocompress that is based on singular value decomposition. We used mathematical softwareMATLAB to <b>compress</b> the <b>image</b> <b>data</b> by using Haar wavelet transformation, and singularvalue decomposition...|$|R
40|$|An {{important}} {{advantage of}} the fixed rate source coding scheme DCT and Product Pyramid VQ with (L-K) -thresholding is, besides its very good performance, the ability to calculate very precisely the expected mean square error caused by data compression mses. This together with the a-priori ability to calculate the bit-sensitivity of the <b>compressed</b> <b>image</b> <b>data</b> enables us to perform a highly efficientunequal error protection for image transmission over noisy channels. We show how to optimize the ratio of source to channel rate for transmission of compressed still images over a Gaussian channel (and a Rayleigh fading channel) using unequal error protection, and we present gains of up to 6 dB (7 dB) in PSNR compared to the image compression standard JPEG...|$|E
40|$|Abstract—Bit {{dependency}} {{of image}} compression algorithms introduce error extension effects in transmission of compressed images and considerably reduce the received image quality. Channel coding provides protection against these errors {{but at the}} cost of increased bandwidth. Asymmetric modulation methods like QAM and QPSK provide alternative means of equal error protection to all the coded bits without increasing the bandwidth. This paper examines the suitability of Hierarchical QAM (HQAM) where non-uniform signal constellation is used to provide different degrees of protection to the significant and nonsignificant bits in the <b>compressed</b> <b>image</b> <b>data</b> at lower channel Signal to Noise Ratio (SNR) and compares it with that of QAM. The performance of HQAM is evaluated using gray test image for different values of the modulation parameter...|$|E
40|$|In this paper, we {{proposed}} a progressive time varying source channel coding system for transmitting image over wireless channels. Transmission of <b>compressed</b> <b>image</b> <b>data</b> over noisy channel is an important problem and has been investigated {{in a variety of}} scenarios. the core results obtained by a systematic method of instantaneous rate allocation between the progressive source coder and progressive channel coder. It is developed by closed form of expression for end-to-end distortion, rate allocation respectively in static channels. It is extended the static result to an algorithm for fading channels. It is introduced set DCT (blocks) approach is adapted to perform sub band decomposition followed by SPIHT (Set partitioning in Hierarchical tree) Keywords: Progressive source –channel coding, HC-RIOT, SPIHT 1...|$|E
40|$|Wavelet-based spatial {{quantization}} is {{a technique}} to <b>compress</b> <b>image</b> <b>data</b> that adapts the compression to the data in each region of an image. This approach is motivated {{due to the fact}} that quantization with a single step-size does not result in a uniform visual effect across each spatial location; differ-ent types of image content mask quantization errors in dif-ferent ways. While many spatial quantization techniques de-termine step-sizes through local activity measures, the pro-posed method induces local quantization distortion based on experiments that quantify human detection of this distortion as function of both the contrast and the type of the <b>image</b> <b>data.</b> Three types in particular, textures, structures and edges, are considered. A classifier is utilized to detect to which of these 3 categories a region of <b>image</b> <b>data</b> belongs, and step-sizes are then derived based on the contrast and classification of local regions. The classification and constrast data are con-veyed to the coder with explicit side information. For <b>images</b> <b>compressed</b> at threshold, the proposed method requires 3 - 10 % less rate than a similar previous approach without classifi-cation, and on average are preferred by 2 / 3 of tested viewers. 1...|$|R
40|$|A nero image {{compression}} technique is presented using hybrid neural netvm. s that combine tmo different learning networks, the auto-associative multi-laver perceptran (AMP) and the sd]-orgenidng Jeature mop ($OFM). The neural networks simuituncoudV perJorm dimensionalitel reduction with the AMP and categerization with the to <b>compress</b> <b>image</b> <b>data.</b> To hgbrid neural networks $ormlag parolld and serial architectures am examined through theoretical analllsis and computer simdation. The parolld structure network reduces the dimensionalitp o] input pat. tern vectors bil mapping them to different hidden lavers o. f the AMP selected by winner-take-all units o] the The serial structure network cotegorises the input patgem ectors into several classes representing pratotyp ectors. Both the serial and parallel structures are combinations o. f the AMLP and $OFM netmorks. These hghrid neural net. works achieve clear peJormonce improvement {{with respect to}} decoded picture qualitl! and compression ratios, compared to ezisting {{image compression}} techniques...|$|R
40|$|Overhead of data {{transmission}} over internet is increasing exponentially every day. Optimization of natural bandwidth {{is the basic}} motive by <b>compressing</b> <b>image</b> <b>data</b> to the maximum extend. For the same objective, combination of lossy half tone and lossless Wavelet Transform techniques is proposed so as to obtain low-bit rate video {{data transmission}}. Decimal values of bitmapped image are to be converted into either 1 or 0 in half toning process that incur pictorial loss and gives 8 : 1 compression ratio (CR) irrespective of image. Wavelet Transform is applied on half tone image for higher compression for various levels. An experimental result shows the higher CR, minimum Mean Square Error (MSE). Ten sample images of different people captured by Nikon camera are used for experimentation. All images are bitmap (. BMP) 512 X 512 in size. The proposed technique {{can be used for}} video conferencing, storage of movies and CCTV footage etc...|$|R
40|$|This work {{conducted}} {{within the}} Army's Center for Imaging Science (CIS) focuses on quantifying performance loss when Automatic Target Recognition (ATR) systems operate on <b>compressed</b> <b>image</b> <b>data.</b> We consider {{the problem of}} target detection based on sensor data compressed using transform-based coders. Information-theoretic distances such as Kullback-Leibler and Chernoff distances are used to bound detection performance. Detection problems under known and unknown orientation of target are considered. Analytical expressions are derived for a simple sensor noise model. This study provides a systematic framework in which to study degradation in detection performance due to lossy compression. Monte-Carlo simulations for detection of a T 62 tank in additive white Gaussian noise are presented, and actual detection performance is compared with Chernoff bounds...|$|E
40|$|Includes bibliographical {{references}} (leaf [59]) This thesis {{presents the}} development process and implementation procedure for an assembly language code generator utility. This utility {{can be used to}} generate assembly language programs for implementing the Joint Photographic Expert Group’s (JPEG) proposed image compression standard on a digital signal processor (DSP) based system. The output is assembly language programs which can directly execute on a signal processor without additional coding or modification. The code generator supports the Baseline System of the JPEG image compression standard. Image data compression can be used to reduce channel bandwidth requirements when transmitting still or moving images over a band-limited channel. It also enables images to be stored in a relatively smaller memory space. JPEG is an international standard which specifies processes for converting source image data to <b>compressed</b> <b>image</b> <b>data</b> as well as converting <b>compressed</b> <b>image</b> <b>data</b> to reconstructed image data. Writing efficient hardware-dependent software is a time-consuming and tedious task. A 'C' compiler can be used to compile the C source code into assembly programs, but the output of the C compiler often contains redundant instructions. It is the intent of the code generator to produce efficient compact assembly level programs with minimum redundancy. Until now, image compression techniques have been available only through customized hardware or through software which directly implements the compression, primarily on personal computers. This code generator makes available to the user the assembly code for image compression on a DSP(TMS 320 C 30) based system and provides 'C' code for image compression implementation on any platform with C compilers. Providing this capability to the user at the computer interface will reduce the time required in rewriting code for JPEG image compression standard implementation for different images or parameters. M. S. (Master of Science...|$|E
40|$|Abstract—One of {{the most}} {{important}} goals of current and future sensor networks is energy-efficient communication of images. This paper presents a quantitative comparison between the energy costs associated with 1) direct transmission of uncompressed images and 2) sensor platform-based JPEG compression followed by transmission of the <b>compressed</b> <b>image</b> <b>data.</b> JPEG compression computations are mapped onto various resource-constrained platforms using a design environment that allows computation using the minimum integer and fractional bit-widths needed in view of other approximations inherent in the compression process and choice of image quality parameters. Advanced applications of JPEG, such as region of interest coding and successive/progressive transmission, are also examined. Detailed experimental results examining the tradeoffs in processor resources, processing/transmission time, bandwidth utilization, image quality, and overall energy consumption are presented. Index Terms—Fixed-point arithmetic, image coding, image communication. I...|$|E
40|$|Abstract—Overhead of data {{transmission}} over internet is increasing exponentially every day. Optimization of natural bandwidth {{is the basic}} motive by <b>compressing</b> <b>image</b> <b>data</b> to the maximum extend. For the same objective, combination of lossy half tone and lossless Wavelet Transform techniques is proposed so as to obtain low-bit rate video {{data transmission}}. Decimal values of bitmapped image are to be converted into either 1 or 0 in half toning process that incur pictorial loss and gives 8 : 1 compression ratio (CR) irrespective of image. Wavelet Transform is applied on half tone image for higher compression for various levels. An experimental result shows the higher CR, minimum Mean Square Error (MSE). Ten sample images of different people captured by Nikon camera are used for experimentation. All images are bitmap (. BMP) 512 X 512 in size. The proposed technique {{can be used for}} video conferencing, storage of movies and CCTV footage etc. Keywords-Half tone; Low-Bit rate; video data compression...|$|R
40|$|Wavelet {{decomposition}} is {{a well-known}} technique to <b>compress</b> <b>image</b> <b>data.</b> Here, we have used wavelet decomposition to compress digital elevation models, DEM. The objectives for compressing DEMs are obtaining manageable and small data sets, and reducing data access time. In the paper, different aspects of wavelets {{as a base for}} data compression are described. The (de-) compression scheme consists of three steps: wavelet decomposition, quantization, and de-/encoding. A few selected techniques to carry out these steps are presented here. Wavelet decomposition does not destruct or compress data. In the decomposition, data are re-organised in a way that facilitates compression. In the quantization step, data can be destructed. It is shown that the quantizer can be constructed to obtain high compression ratios and a low degree of destruction of data. An adaptive quantizer that considers the distribution of data gives the best result. Performance measures for the 50 x 50 m DEM of all of Sweden are presented...|$|R
40|$|We {{invented a}} {{progressive}} image transmission scheme {{that is used}} for transmitting Chinese calligraphy. The scheme employs the property of simple colors of calligraphy images to design a method of transmitting images phase by phase. Overall, our scheme can achieve the following two goals. One is <b>compressing</b> the <b>image</b> <b>data</b> to reduce the transmission time while the other is gaining less response time by using progressive image transmission. Furthermore, the recovered image still maintains {{the colors of the}} seals with high image quality...|$|R
