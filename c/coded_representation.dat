46|712|Public
5000|$|Associated control {{functions}} and their <b>coded</b> <b>representation</b> for information interchange ...|$|E
5000|$|... the <b>coded</b> <b>representation</b> of {{synthetic}} two-dimensional (2D) or three-dimensional (3D) objects {{that can be}} manifested audibly or visually; ...|$|E
5000|$|... the <b>coded</b> <b>representation</b> of the spatio-temporal {{positioning}} of audio-visual objects {{as well as}} their behaviour in response to interaction (scene description); ...|$|E
40|$|Over {{the past}} decade, since Java was first {{introduced}} and integrated into the Netscape web browser, several intermediate representations have been developed that might be potentially used for mobile code applications. This paper examines the requirements for a mobile <b>code</b> <b>representation,</b> presents several examples of stack-based, tree-oriented, and proof-annotating mobile <b>code</b> <b>representations,</b> and evaluates each of these representations according to the requirements. Povzetek: Članek podaja pregled mobilnih kod. ...|$|R
25|$|The ITU {{has never}} codified formal Morse <b>Code</b> <b>representations</b> for {{currencies}} as the ISO 4217 Currency Codes are preferred for transmission.|$|R
30|$|The {{rationale}} {{beyond the}} use of gray <b>code</b> <b>representation</b> is the following. In the gray code space, just a subset of all possible bit combinations {{is related to the}} eyes patches. We wish to select those bits that usually differ in terms of binary value between eye and non-eye patches. Moreover, by using gray <b>code</b> <b>representation</b> rather than classic bit planes decomposition, we reduce the impact of small changes in intensity of patches that could produce significant variations in the corresponding binary code [17].|$|R
5000|$|ISO/IEC JTC 1/SC 29 {{currently}} has 541 published standards {{within the field}} of <b>coded</b> <b>representation</b> of audio, picture, multimedia, and hypermedia information, including: ...|$|E
5000|$|The {{scope of}} ISO/IEC JTC 1/SC 29 is “Standardization of <b>coded</b> <b>representation</b> of audio, picture, {{multimedia}} and hypermedia information - and sets of compression and control functions {{for use with}} such information - such as:” ...|$|E
5000|$|The noise {{can be seen}} as all the {{differences}} between the surface form of a <b>coded</b> <b>representation</b> of the text and the intended, correct, or original text. It can be due to e.g. typographic errors or colloquialisms always present in natural language and usually lowers the data quality {{in a way that makes}} the text less accessible to automated processing by computers such as natural language processing. The noise can also get introduced through an extraction process (i.e. transcription, OCR) from media other than original electronic texts.|$|E
40|$|There is an {{increasing}} interest within the reverse engineering community in using the XML language and its related technologies {{as a way to}} foster the development, reuse and interoperability of code analysis tools. However, most of the work done in this direction has thus far concentrated on the interoperability issue, mainly with the proposal of new XML-based source <b>code</b> <b>representations,</b> and the development of new facilities for extracting those <b>representations</b> from source <b>code.</b> This paper attempts to fill this gap by showing how XML-based source <b>code</b> <b>representations</b> and query technologies can be used to facilitate the development and reuse of code analysis tools...|$|R
30|$|In[2], {{a mixture}} of model-driven and {{generative}} programming techniques are proposed to support variability configuration of video surveillance systems. In order to better manage features combination at runtime, it separates the variability in domain and <b>code</b> <b>representation</b> spaces.|$|R
50|$|The LLVM Compiler Infrastructure uses SSA {{form for}} all scalar {{register}} values (everything except memory) in its primary <b>code</b> <b>representation.</b> SSA form is only eliminated once register allocation occurs, {{late in the}} compile process (often at link time).|$|R
5000|$|ISO/IEC JTC 1/SC 2 was {{established}} in 1987, originally with the title “Character Sets and Information Coding,” with the area of work being, “the standardization of bit and byte <b>coded</b> <b>representation</b> of information for interchange including among others, sets of graphic characters, of control functions, of picture elements and audio information coding of text for processing and interchange; code extension techniques; implementation of these coded representations on interchange media and transmission systems." [...] The standardization activities of the subcommittee were originally {{under the jurisdiction of}} ISO/TC 97. However, when the Joint Technical Committee, ISO/IEC JTC 1, {{was established}} in 1987 as a merger between ISO/TC 97, Information Technology, and IEC/TC 83, the standardization activity was transferred to the new subcommittee, ISO/IEC JTC 1/SC 2. Certain standards that are now {{under the jurisdiction of the}} ISO/IEC JTC 1/SC 2, such as ISO/IEC 646, were published before the creation of ISO/IEC JTC 1, under ISO/TC 97. The original working groups of ISO/IEC JTC 1/SC 2 were: WG 1 “Code Extension Techniques,” WG 2 “Multiple-octet Coded character set,” WG 3 “7-bit and 8-bit Codes,” WG 6 “Control Functions,” and WG 8 “Coded Representation of Picture and Audio Information.” Since then, the subcommittee has established or disbanded working groups in response to changing standardization needs within the field of coded character sets.|$|E
40|$|<b>Coded</b> <b>representation</b> of {{time signals}} [...] day, hour, minute, second [...] is changed from binary-coded decimal (BCD) to IRIG {{standard}} time-code format B by circuit that uses nine integrated circuits. Input to code-converter circuit is parallel BCD pulses on bus output is serial pulses of IRIG-B on single line...|$|E
40|$|International Telemetering Conference Proceedings / October 09 - 11, 1973 / Sheraton Inn Northeast, Washington, D. C. Prefix {{insertion}} {{prior to}} the <b>coded</b> <b>representation</b> of every fixed length block of data provides a simple synchronization method for variable length coding. Unlike fixed length coding where the prefix appears with a set period, the appearance time of each prefix word in the variable length <b>coded</b> <b>representation</b> is a random variable. At the receiver a synchronization decision is made whenever a pattern within a threshold Hamming distance of the prefix is received. In this paper an expression is found for small synchronization error probabilities. This egression depends on the coded block length only through its average value L. The optimal value for the recognition threshold is found. The necessary and sufficient condition for an arbitrarily small synchronization error probability is shown {{to be that the}} prefix length grows as log L. The results are discussed from the viewpoint of data compression and source encoding...|$|E
50|$|Markdown {{also uses}} {{indentation}} {{as part of}} the formal syntax of the language. Indentation is required in Markdown when creating certain source <b>code</b> <b>representations</b> of block quotations as well as when creating sections of source code to be rendered as the code itself.|$|R
40|$|Nowadays, a {{large number}} of {{extraction}} tools are available. However, using them, {{it is often difficult to}} gather and incorporate new metrics. On the other hand, the metric specifications often lack precision and therefore lead to multiple implementation patterns. In this paper, we propose a new approach of metric gathering. This approach, which is at the same time generic and practical, is based on a metric description mechanism. It uses a language that makes it possible to manipulate data from the source <b>code</b> <b>representation</b> model. In a first phase, we have defined a generic model for object oriented <b>code</b> <b>representation.</b> A second phase defines a description language that offers the syntactic constructions necessary for data manipulation of the generic mode...|$|R
50|$|Bit arrays {{are also}} a useful {{abstraction}} for examining streams of compressed data, which often contain elements that occupy portions of bytes or are not byte-aligned. For example, the compressed Huffman <b>coding</b> <b>representation</b> of a single 8-bit character can be anywhere from 1 to 255 bits long.|$|R
40|$|A {{method of}} {{representing}} an object appearing in a still or video image, by processing signals {{corresponding to the}} image, comprises deriving a plurality of sets of co-ordinate values representing {{the shape of the}} object and quantising the co-ordinate values to derive a <b>coded</b> <b>representation</b> of the shape, and further comprises quantising a first co-ordinate value over a first quantisation range and quantising a smaller co-ordinate value over a smaller range...|$|E
40|$|The {{domain of}} {{biometrics}} lacks of systematical approach for classifying biometric signatures for biometric authentication, detection, and reaction systems. This paper presents a first approach {{to fill this}} gap. Outlining the general authentication process and analyzing {{the meaning of the}} term signature from selected sciences, a definition of the term biometric signature as (bin|n-) ary <b>coded</b> <b>representation</b> of biometric characteristics is derived. To show the suitability of the suggested definition, its role within the core processes of biometric authentication systems (enrollment, authentication, derollment) is described...|$|E
40|$|Wolpert and Macready’s No Free Lunch theorem {{proves that}} no search {{algorithm}} {{is better than}} any other over all possible discrete functions. The mean-ing of the No Free Lunch theorem has, however, been the subject of intense debate. We prove that for local neighborhood search on problems of bounded complexity, where complexity is measured in terms of number of basins of attraction in the search space a Gray <b>coded</b> <b>representation</b> is better than Binary in the sense that on average it induces fewer minima in a Hamming distance 1 search neighborhood...|$|E
5000|$|Strictly speaking, JIS X 0201 {{encoding}} as [...] "half-width katakana" [...] is incorrect, as {{the standard}} does not define character widths - it defines only the <b>code</b> <b>representation</b> of katakana characters. In the JIS X 0201 standard, katakana characters are printed in normal (full) width, not half-width.|$|R
40|$|Research title: feature {{extraction}} and selection algorithm for chain <b>code</b> <b>representation</b> of handwritten character Aug 2008 Universiti Teknologi Malaysia MSc in Computer Science CGPA 3. 47 Thesis title: conversion and optimization of SIMULINK model to C source code for robot simulation software (case study: robot mitsubishi RV- 2 AJ...|$|R
40|$|By {{allowing}} the programmer to write code that can generate code at run-time, meta-programming offers a powerful approach to program construction. For instance, meta-programming {{can often be}} employed to enhance program efficiency and facilitate the construction of generic programs. However, meta-programming, especially in an untyped setting, is notoriously error-prone. In this paper, we aim at making meta-programming less error-prone by providing a type system to facilitate the construction of correct meta-programs. We first introduce some code constructors for constructing typeful <b>code</b> <b>representation</b> in which program variables are replaced with deBruijn indices, and then formally demonstrate how such typeful <b>code</b> <b>representation</b> {{can be used to}} support meta-programming. The main contribution of the paper lies in recognition and then formalization of a novel approach to typed meta-programming that is practical, general and flexible...|$|R
40|$|This report {{describes}} a two layered pattern recognition program. The first layer scans an input for features and produces a <b>coded</b> <b>representation.</b> The second layer looks for combinations of code which signify relations between {{features in the}} input. It is argued {{that the nature of}} the representation produced by the first layer determined the amount and quality of subsequent processing. Both layers create and modify their operators. Good results are obtained using the Highleyman hand printed data. The program is discussed as a model of an aspect of human perception...|$|E
40|$|A {{proof is}} {{presented}} {{that shows how}} the neighborhood structure that is induced under a Gray Code representation repeats under shifting. Convergence proofs are also presented for steepest ascent using a local search bit climber and a Gray code representation: for unimodal 1 -D functions and multimodal functions that are separable and unimodal in each dimension, the worst case number of steps needed to reach the global optimum is O(L) with a constant 2. We also show how changes in precision impact the Gray neighborhood. Finally, we also show how both the Gray and Binary neighborhoods are easily reachable from the Gray <b>coded</b> <b>representation.</b> ...|$|E
40|$|Codification and {{transfer}} of knowledge is essential {{in the practice of}} knowledge management. Theoretical knowledge, like scientific theories and models, by nature comes in <b>coded</b> <b>representation</b> for the explicit purpose of transfer. Practical knowledge, as involved frequently in engineering or business operations, however, is a priori uncoded, making transfer for further use or the generation of new knowledge difficult. A great deal of systems engineering effort {{in recent years has been}} focused on resolving issues related to this sort of knowledge transfer. Semantic technologies play a major role in here, along with the development of ontologies. This paper presents a semiotic perspective on {{transfer of knowledge}} within collaborations...|$|E
40|$|A {{variety of}} {{representation}} learning approaches {{have been investigated}} for reinforcement learning; much less attention, however, {{has been given to}} investigating the utility of sparse coding. Outside of reinforcement learning, sparse <b>coding</b> <b>representations</b> have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse <b>coding</b> <b>representation</b> outperforms a wide variety of tilecoding representations. Comment: 6 (+ 1) pages, 2 figures, International Joint Conference on Artificial Intelligence 201...|$|R
5000|$|ISO 639-4:2010 <b>Codes</b> for the <b>representation</b> {{of names}} of languages—Part 4: General {{principles}} of <b>coding</b> of the <b>representation</b> of names of languages and related entities ...|$|R
30|$|Sparse coding adopts {{independent}} coding {{method for}} features, and the coding {{of the same}} image may change because {{of the influence of}} illumination changes, occlusion, and other noises. In addition, sparse coding also ignores the correlation between local descriptors and good <b>coding</b> <b>representation</b> should make similar image features have similar coding to minimize intra-class differences.|$|R
40|$|A {{class of}} hybrid niching {{evolutionary}} algorithms (HNE) using clustering crowding and parallel local searching is proposed. By analyzing topology of fitness landscape and extending {{the space for}} searching similar individual, HNE determines the locality of search space more accurately, and decreases the replacement errors of crowding and suppressing genetic drift of the population. The integration of deterministic and probabilistic crowding increases the capacity of both parallel local hill-climbing and maintaining multiple subpopulations. Parallel local search based on simplex method over disjoint subpopulations greatly speeds up the convergence of the population towards various optima simultaneously. Real <b>coded</b> <b>representation</b> and Gaussian mutation improve the precision of the solutions founded. The experimental results optimizing various multimodal function...|$|E
30|$|Genetic {{algorithm}} (GA) is {{the global}} numerical optimization methods based on genetic recombination and evaluation in nature [32, 33]. They use the iterative optimization procedures, which {{start with a}} randomly selected population of potential solutions and then gradually evolve toward a better solution {{through the application of}} the genetic operators. GA typically operates on a discretized and <b>coded</b> <b>representation</b> of the parameters rather than on the parameters themselves. These representations are often considered to be 'chromosomes’[*], while the individual element, which constitutes chromosomes, is the 'gene’[*]. Simple but often very effective chromosome representations for optimization problem involving several continuous parameters can be obtained through the juxtaposition of discretized binary representations of the individual parameter.|$|E
40|$|Automatic {{analysis}} of soil limitations studied by an automatic color TV density slicing system was accomplished. This system color codes the density {{range of the}} value component of color of an image. Maps of soil limitations or other similar soil groups are produced by photographing the color <b>coded</b> <b>representation</b> of an area. The planimeter feature of the density slicing system measures the area of each soil limitation providing information {{on the importance of}} a soil limitation in an area. The {{results of this study suggest}} that an automatic color TV density slicing system has great potential not only for identifying and mapping similar soil areas, but also for indicating the percentage composition of an area...|$|E
5000|$|Standards {{such as the}} UDEF {{depend on}} {{accurate}} <b>coding</b> of <b>Representation</b> Terms.|$|R
40|$|Abstract—A fast {{method of}} {{handwritten}} word recognition suitable for real time applications {{is presented in}} this paper. Preprocessing, segmentation and feature extraction are implemented using a chain <b>code</b> <b>representation</b> of the word contour. Dynamic matching between characters of a lexicon entry and segment(s) of the input word image is used to rank the lexicon entries in order of best match. Variable duration for each character is defined and used during the matching. Experimental results prove that our approach using the variable duration outperforms the method using fixed duration {{in terms of both}} accuracy and speed. Speed of the entire recognition process is about 200 msec on a single SPARC- 10 platform and the recognition accuracy is 96. 8 percent are achieved for lexicon size of 10, on a database of postal words captured at 212 dpi. Index Terms—Handwritten word recognition, segmentation algorithm, variable duration, chain <b>code</b> <b>representation,</b> dynami...|$|R
40|$|In this paper, {{adaptive}} coding schemes for contour line drawings based on chain <b>code</b> <b>representation</b> is presented. In this scheme, the chain code or the chain-difference code of a contour is modeled as an n-order Markov sequence and then coded with arithmetic coding scheme adaptively. Experimental result {{shows that the}} proposed approach is better than some other conventional approaches. Department of Electrical Engineerin...|$|R
