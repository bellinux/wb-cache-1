35|49|Public
2500|$|The {{negative}} {{binomial distribution}} also arises as a continuous mixture of Poisson distributions (i.e. a <b>compound</b> <b>probability</b> distribution) where the mixing distribution of the Poisson rate is a gamma distribution. That is, we can view the negative binomial as a [...] distribution, where λ is itself a random variable, distributed as a gamma distribution with shape = r and scale [...] θ = [...] or correspondingly rate β = [...]|$|E
50|$|This {{family of}} {{distributions}} {{is a special}} or limiting case of the normal-exponential-gamma distribution. The distribution is a <b>compound</b> <b>probability</b> distribution in which the mean of a normal distribution varies randomly as a shifted exponential distribution.|$|E
50|$|Where {{the set of}} {{component}} distributions is uncountable, {{the result}} is often called a <b>compound</b> <b>probability</b> distribution. The construction of such distributions has a formal similarity to that of mixture distributions, with either infinite summations or integrals replacing the finite summations used for finite mixtures.|$|E
40|$|The {{results of}} an {{experiment}} extending Ellsberg's setup demonstrate that attitudes towards ambiguity and compound uncertainty are closely related. However, this association is much stronger when the second layer of uncertainty is subjective than when it is objective. Provided that the <b>compound</b> <b>probabilities</b> are simple enough, we find that most subjects, consisting of both students and policy makers, (1) reduce <b>compound</b> objective <b>probabilities,</b> (2) do not reduce <b>compound</b> subjective <b>probabilities,</b> and (3) are ambiguity non-neutral. By decomposing ambiguity into risk and model uncertainty, and jointly eliciting the attitudes individuals manifest towards {{these two types of}} uncertainty, we characterize individuals' degree of ambiguity aversion. Our data provides evidence of decreasing absolute ambiguity aversion and constant relative ambiguity aversion...|$|R
40|$|We {{consider}} the extremal behaviour of Markov chains. Rootz'en [18] gives conditions for stationary, regenerative sequences {{so that the}} normalized process of level exceedances converges in distribution to a compound Poisson process. He also provides expressions for the extremal index and the <b>compounding</b> <b>probabilities,</b> though in general {{it is not easy}} to evaluate these. We show how in a number of instances Markov chains can be coupled with two random walks which, in terms of extremal behaviour, bound the chain from above and below. Using a limiting argument it is shown that the lower bound converges to the upper one, yielding the extremal index and the <b>compounding</b> <b>probabilities</b> of the Markov chain. Fluctuation properties of random walks can be characterised accurately using Grubel's FFT technique [9]. His algorithm for the stationary distribution of a G/G/ 1 queue is adapted for the extremal index; it yields approximate, but very accurate results. <b>Compounding</b> <b>probabilities</b> are calculated explicitly in a similar fashion. The technique is applied to: (i) the G/G/ 1 queue; (ii) G/M/c queues; (iii) autoregressive conditional heteroscedastic (ARCH) processes, whose extremal behaviour de Haan et al. [6] characterized using simulation...|$|R
40|$|AbstractWe {{consider}} extremal {{properties of}} Markov chains. Rootzén (1988) gives conditions for stationary, regenerative sequences {{so that the}} normalized process of level exceedances converges in distribution to a compound Poisson process. He also provides expressions for the extremal index and the compounding probabilities; in general {{it is not easy}} to evaluate these. We show how in a number of instances Markov chains can be coupled with two random walks which, in terms of extremal behaviour, bound the chain from above and below. Using a limiting argument it is shown that the lower bound converges to the upper one, yielding the extremal index and the <b>compounding</b> <b>probabilities</b> of the Markov chain. An FFT algorithm by Grübel (1991) for the stationary distribution of a G/G/ 1 queue is adapted for the extremal index; it yields approximate, but very accurate results. <b>Compounding</b> <b>probabilities</b> are calculated explicitly in a similar fashion. The technique is applied to the G/G/ 1 queue, G/M/c queues and ARCH processes, whose extremal behaviour de Haan et al. (1989) characterized using simulation...|$|R
50|$|In {{probability}} and statistics, a <b>compound</b> <b>probability</b> distribution (also {{known as}} a mixture distribution or contagious distribution) is the probability distribution that results from assuming that a random variable is distributed according to some parametrized distribution, with (some of) the parameters of that distribution themselves being random variables.|$|E
5000|$|The {{entire class}} of discrete-stable {{distributions}} can be formed as Poisson <b>compound</b> <b>probability</b> distributions where the mean, , of a Poisson distribution {{is defined as}} a random variable with a probability density function (PDF). When the PDF of the mean is a one-sided continuous-stable distribution with stability parameter [...] and scale parameter [...] the resultant distribution is discrete-stable with index [...] and scale parameter [...]|$|E
5000|$|... where [...] and [...] is a {{modified}} Bessel {{function of the}} second kind. In this derivation, the K-distribution is a <b>compound</b> <b>probability</b> distribution. It is also a product distribution: it is {{the distribution of the}} product of two independent random variables, one having a gamma distribution with mean 1 and shape parameter , the second having a gamma distribution with mean [...] and shape parameter [...]|$|E
40|$|We {{consider}} extremal {{properties of}} Markov chains. Rootzén (1988) gives conditions for stationary, regenerative sequences {{so that the}} normalized process of level exceedances converges in distribution to a compound Poisson process. He also provides expressions for the extremal index and the compounding probabilities; in general {{it is not easy}} to evaluate these. We show how in a number of instances Markov chains can be coupled with two random walks which, in terms of extremal behaviour, bound the chain from above and below. Using a limiting argument it is shown that the lower bound converges to the upper one, yielding the extremal index and the <b>compounding</b> <b>probabilities</b> of the Markov chain. An FFT algorithm by Grübel (1991) for the stationary distribution of a G/G/ 1 queue is adapted for the extremal index; it yields approximate, but very accurate results. <b>Compounding</b> <b>probabilities</b> are calculated explicitly in a similar fashion. The technique is applied to the G/G/ 1 queue, G/M/c queues and ARCH processes, whose extremal behaviour de Haan et al. (1989) characterized using simulation. Extremal index Clustering of extreme values Harris chains...|$|R
40|$|A {{carefully}} {{written paper}} by A. Caticha [Phys. Rev. A 57, 1572 (1998) ] applies consistency arguments to derive the quantum mechanical rules for <b>compounding</b> <b>probability</b> amplitudes {{in much the}} same way as earlier work by the present author [J. Math. Phys. 29, 398 (1988) and Int. J. Theor. Phys. 27, 543 (1998) ]. These works are examined together to find the minimal assumptions needed to obtain the most general results. PACS number(s) : 03. 65. Bz, 03. 65. Ca Typeset using REVTE...|$|R
40|$|Abstract—Outage {{probability}} {{and capacity}} of a class of blockfading MIMO channels are considered under partial channel distribution information. Specifically, the channel or its distribution is not known but the latter is known {{to belong to a}} class of distributions where each member is within a certain distance (uncertainty) from a nominal distribution. Relative entropy is used as a measure of distance between distributions. <b>Compound</b> outage <b>probability</b> defined as min (over the transmitted signal distribution) -max (over the channel distribution class) outage probability is introduced and investigated. This generalizes the standard outage probability to the case of partial channel distribution information. <b>Compound</b> outage <b>probability</b> characterization (via 1 -D convex optimization and in a closed form), its properties, and approximations are given. It is shown to have two-regime behavior: when the nominal outage probability decreases (e. g., by increasing the SNR), the compoun...|$|R
50|$|In {{probability}} theory, a beta negative {{binomial distribution}} is the probability distribution of a discrete random variable X equal {{to the number of}} failures needed to get r successes in a sequence of independent Bernoulli trials where the probability p of success on each trial is constant within any given experiment but is itself a random variable following a beta distribution, varying between different experiments. Thus the distribution is a <b>compound</b> <b>probability</b> distribution.|$|E
50|$|The {{negative}} {{binomial distribution}} also arises as a continuous mixture of Poisson distributions (i.e. a <b>compound</b> <b>probability</b> distribution) where the mixing distribution of the Poisson rate is a gamma distribution. That is, we can view the negative binomial as a Poisson(λ) distribution, where λ is itself a random variable, distributed as a gamma distribution with shape = r and scale θ = p/(1 − p) or correspondingly rate β = (1 − p)/p.|$|E
50|$|In general, {{distributions}} {{that result}} from a finite or infinite mixture of other distributions, e.g. mixture model densities and <b>compound</b> <b>probability</b> distributions, are not exponential families. Examples are typical Gaussian mixture models {{as well as many}} heavy-tailed distributions {{that result from}} compounding (i.e. infinitely mixing) a distribution with a prior distribution over one of its parameters, e.g. the Student's t-distribution (compounding a normal distribution over a gamma-distributed precision prior), and the beta-binomial and Dirichlet-multinomial distributions. Other examples of distributions that are not exponential families are the F-distribution, Cauchy distribution, hypergeometric distribution and logistic distribution.|$|E
40|$|This article {{explores the}} configural {{weighted}} average (CWA) hypothesis suggesting that extension biases, like conjunction and disjunction errors, occur because people estimate <b>compound</b> <b>probabilities</b> {{by taking a}} CWA of the constituent probabilities. The hypothesis suggests a process consistent with well-known cognitive constraints, which nonetheless achieves high robustness and bounded rationality in noisy real-life environments. Predictions by the CWA hypothesis are that in error-free data, conjunction and disjunction errors should be the rule {{rather than the exception}} when pairs of statements are randomly sampled from an environment, the rate of extension errors should increase when noise in data is decreased, and that adding a likely component should increase the probability of a conjunction. Four experiments generally verify the predictions by the hypothesis, demonstrating that extension errors are frequent also when tasks are selected according to representative design...|$|R
40|$|Outage {{probability}} {{and capacity}} of a class of block-fading MIMO channels are considered with partial channel distribution information. Specifically, the channel or its distribution are not known but the latter is known {{to belong to a}} class of distributions where each member is within a certain distance (uncertainty) from a nominal distribution. Relative entropy is used as a measure of distance between distributions. <b>Compound</b> outage <b>probability</b> defined as min (over the transmit signal distribution) -max (over the channel distribution class) outage probability is introduced and investigated. This generalizes the standard outage probability to the case of partial channel distribution information. <b>Compound</b> outage <b>probability</b> characterization (via one-dimensional convex optimization), its properties and approximations are given. It is shown to have two-regime behavior: when the nominal outage probability decreases (e. g. by increasing the SNR), the compound outage first decreases linearly down to a certain threshold (related to relative entropy distance) and then only logarithmically (i. e. very slowly), so that no significant further decrease is possible. The compound outage depends on the relative entropy distance and the nominal outage only, all other details (nominal fading and noise distributions) being irrelevant. The transmit signal distribution optimized for the nominal channel distribution is shown to be also optimal for the whole class of distributions. The effect of swapping the distributions in relative entropy is investigated and an error floor effect is established. The <b>compound</b> outage <b>probability</b> under Lp distance constraint is also investigated. The obtained results hold for a generic channel model (arbitrary nominal fading and noise distributions). Comment: submitted to IEEE IT Transactions, 201...|$|R
5000|$|... 4. Electricity {{supplies}} are unreliable at both sites; Hillside where K22HN will originate 8VSB signals for the region, and Eagle River where KYES-DT will translate those signals. Both sites {{are subject to}} winds in excess of 100 mph. Serial retransmission will <b>compound</b> the <b>probability</b> of off air time. The station needs back up generators, but has no money for one, no less two. Public safety will be compromised after the analog signal is switched off {{due to lack of}} back up power.|$|R
5000|$|A <b>compound</b> <b>probability</b> {{distribution}} is the probability distribution {{that results from}} assuming that a random variable [...] is distributed according to some parametrized distribution [...] with an unknown parameter [...] that is again distributed according to some other distribution [...] The resulting distribution [...] {{is said to be}} the distribution that results from compounding [...] with [...] The parameter's distribution [...] is also called the mixing distribution or latent distribution. Technically, the unconditional distribution [...] results from marginalizing over , i.e., from integrating out the unknown parameter(s) [...] Its probability density function is given by: ...|$|E
5000|$|If the {{distribution}} of X is either an exponential distribution or a gamma distribution, then the conditional distributions of Y | N are gamma distributions in which the shape parameters are proportional to N. This shows that the formulation of the [...] "compound Poisson distribution" [...] outlined above {{is essentially the same}} as the more general class of <b>compound</b> <b>probability</b> distributions. However, the properties outlined above do depend on its formulation as the sum of a Poisson-distributed number of random variables. The distribution of Y {{in the case of the}} compound Poisson distribution with exponentially-distributed summands can be written in an form.|$|E
50|$|Note {{that both}} types of {{predictive}} distributions have {{the form of a}} <b>compound</b> <b>probability</b> distribution (as does the marginal likelihood). In fact, if the prior distribution is a conjugate prior, and hence the prior and posterior distributions come from the same family, it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the conjugate prior article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.|$|E
40|$|Abstract—Outage {{probability}} of a class of block-fading (MIMO) channels is considered under channel distribution uncertainty, when the channel or its distribution are not known but the latter is known {{to belong to a}} class of distributions where each member is within a certain distance from a nominal distribution. Relative entropy is used as a measure of distance between distributions. <b>Compound</b> outage <b>probability</b> defined as min (over the input distribution) -max (over the channel distribution class) outage probability is introduced and investigated, which generalizes the standard outage probability to the case of partial channel distribution information. <b>Compound</b> outage <b>probability</b> characterization via one-dimensional convex optimization, its properties and approximations are given. It is shown to have a two-regime behavior: when the nominal outage <b>probability</b> decreases, the <b>compound</b> outage first decreases linearly down to a certain threshold and then only logarithmically (i. e. very slowly), so that no significant further decrease is possible. The input distribution optimized for the nominal channel distribution is shown to be also optimal for the whole class of distributions. The effect of swapping the distributions in relative entropy is investigated and an error floor effect is established. The obtained results hold for a generic channel model (arbitrary nominal fading and noise distributions). I...|$|R
40|$|When direct {{reactions}} populate highly excited, unbound configurations in {{the residual}} nucleus, the nucleus may further {{evolve into a}} compound nucleus. Alternatively, the residual system may decay by emitting particles into the continuum. Understanding the relative weights of these two processes {{as a function of}} the angular momentum and parity deposited in the nucleus is important for the surrogate-reaction technique. A particularly interesting case is compound-nucleus formation via the (d, p) reaction, which may be a useful tool for forming compound nuclei off the valley of stability in inverse-kinematics experiments. We present here a study of the <b>compound</b> formation <b>probability</b> for a closely-related direct reaction, direct-semidirect radiative neutron capture...|$|R
40|$|We {{show that}} the {{microscopic}} TDHF approach provides an important tool to {{shed some light on}} the nuclear dynamics leading to the formation of superheavy elements. In particular, we discuss studying quasifission dynamics and calculating ingredients for <b>compound</b> nucleus formation <b>probability</b> calculations. Comment: Submitted to the proceedings of the 6 th International Conference on Fission and Properties of Neutron-Rich Nuclei (ICFN 6...|$|R
50|$|In {{probability}} theory and statistics, the Dirichlet-multinomial distribution {{is a family}} of discrete multivariate probability distributions on a finite support of non-negative integers. It is also called the Dirichlet compound multinomial distribution (DCM) or multivariate Pólya distribution (after George Pólya). It is a <b>compound</b> <b>probability</b> distribution, where a probability vector p is drawn from a Dirichlet distribution with parameter vector , and an observation drawn from a multinomial distribution with probability vector p and number of trials n. The compounding corresponds to a Polya urn scheme. It is frequently encountered in Bayesian statistics, empirical Bayes methods and classical statistics as an overdispersed multinomial distribution. It reduces to the Categorical distribution as a special case when n = 1. It also approximates the multinomial distribution arbitrarily well for large α. The Dirichlet-multinomial is a multivariate extension of the Beta-binomial distribution, as the multinomial and Dirichlet distributions are multivariate versions of the binomial distribution and beta distributions, respectively.|$|E
3000|$|... of (X,Y), as is {{the case}} in Definition 3. See also Zolotarev (1997, Rachev (1991) for the similar notion of <b>compound</b> <b>probability</b> metric. For risk {{measures}} ρ(X) on [...] X, there is the analog notion of law-invariant risk measures which depend only on the law μ [...]...|$|E
40|$|The {{generalized}} <b>compound</b> <b>probability</b> {{density function}} (GC-pdf) is presented for modeling high resolution radar clutter. In particular, {{the model is}} used to describe deviation of the speckle component from the Rayleigh to Weibull or other pdfs with longer tails. The GC-pdf is formed using the generalized gamma (GΓ) pdf to describe both the speckle and the modulation component of th...|$|E
50|$|K-distribution arises as the {{consequence}} of a statistical or probabilistic model used in Synthetic Aperture Radar (SAR) imagery. The K-distribution is formed by <b>compounding</b> two separate <b>probability</b> distributions, one representing the radar cross-section, and the other representing speckle that is a characteristic of coherent imaging. It is also used in wireless communication to model composite fast fading and shadowing effects.|$|R
40|$|We use stated-preference {{methods to}} {{estimate}} the cancer Value per Statistical Life (VSL) and Value per Statistical Case (VSCC) from {{a representative sample of}} 45 - 60 -year olds in four countries in Europe. We ask respondents to report information about their willingness to pay for health risk reductions that are different from those used in earlier valuation work because they are comprised of two probabilities 2 ̆ 014 that of getting cancer, and that of dying from it (conditional on getting it in the first place). The product of these two probabilities is the unconditional cancer mortality risk. Our hypothetical risk reductions also include two qualitative attributes 2 ̆ 014 quality-of-life impacts and pain. The results show that respondents did appear to have an intuitive grasp of <b>compound</b> <b>probabilities,</b> and took into account each component of the unconditional cancer mortality risk when answering the valuation questions. We estimate the cancer VSL to be between 2 ̆ 0 ac 1. 9 and 5. 7 million, depending on whether the (unconditional) mortality risk was reduced by lowering the chance of getting cancer, increasing the chance of surviving cancer, or both. The VSCC is estimated to be up to 2 ̆ 0 ac 0. 550 million euro, and its magnitude depends on the initial (conditional) cancer mortality and on the improvement in survival. We interpret these as 2 ̆ 01 cpure 2 ̆ 01 d mortality and cancer risk values, stripped of morbidity, pain or quality-of-life effects. The survey responses show that impacts on daily activities and pain have little or no effect on the WTP to reduce the adverse health risks...|$|R
40|$|Statisticians {{are usually}} {{concerned}} with the proposition of new distributions. In this paper we point out that a unified and concise derivation procedure {{of the distribution of}} the minimum or maximum of a random number N of indepen-dent and identically distributed continuous random variables Yi,{i = 1, 2,…,N} is obtained if one <b>compounds</b> the <b>probability</b> generating function of N with the survival or the distribution func-tion of Yi. Expressions are then derived in closed form for the density, hazard and quantile func-tions of the minimum or maximum. The methodology is illustrated with examples of the distributions proposed by Adamidis and Loukas (1998), Kus (2007), Tahmasbi and Rezaei (2008), Barreto-Souza and Cribari-Neto (2009), Cancho, Louzada, and Barriga (2011) and Louzada, Roman and Cancho (2011) ...|$|R
40|$|Abstract. The aim of {{this paper}} is to give anotion of uniform {{tightness}} for transition probabilities on topological spaces, which assures the uniform tightness of <b>compound</b> <b>probability</b> measures. Then the upper semicontinuity of set-valued mappings are used in essence. As an important example, the uniform tightness for Gaussian transition probabilities on the strong dual of anuclear real Prechet space is studied. It is also shown that some of our results contain well-known results concerning the uniform tightness and the weak convergence of probability measures. 1...|$|E
40|$|Nonextensive quantum gas {{distributions}} are investigated on {{the basis}} of the factorization hypothesis of <b>compound</b> <b>probability</b> required by thermodynamic equilibrium. It is shown that the formalisms of Tsallis nonextensive statistical mechanics with normalized average give distribution functions for standard bosons and fermions obeying Pauli principle. The formalism with unnormalized average leads to a intermediate quantum distribution comparable to that of fractional exclusion statistics, with Fermi surface at T= 0 depending on the parameter $q$. Comment: 5 pages, no figure, RevTeX, to be published in Chaos, Solitons & Fractals (2002...|$|E
40|$|The {{probability}} distribution of a random variable created by summing a random {{number of the}} independent and identically distributed random variables is called a <b>compound</b> <b>probability</b> distribution. In this work is described a compound distribution {{as well as a}} calculation of its characteristics. Especially, the thesis is focused on studying a special case of compound distribution where each addend has the log-normal distribution and their number has the negative binomial distribution. Here are also described some approaches to estimate the parameters of LN and NB distribution. Further, the impact of these estimates on the final compound distribution is analyzed...|$|E
40|$|We {{demonstrate}} that the microscopic Time-dependent Hartree-Fock (TDHF) theory provides an important approach {{to shed light on}} the nuclear dynamics leading to the formation of superheavy elements. In particular, we discuss studying quasifission dynamics and calculating ingredients for <b>compound</b> nucleus formation <b>probability</b> calculations. We also discuss possible extensions to TDHF to address the distribution of observables. Comment: Proceedings of a talk given at FUSION 17, Hobart, Tasmania, AU (20 - 24 February, 2017...|$|R
40|$|AbstractThe data of Anderson and Burr [1985. Vision Research, 25, 1147 – 1154] on the temporal-frequency (TF) {{specificity}} of noise maskers {{indicate that the}} effect of TF masking is broad and varies across spatial frequency (SF) channels. One subtle but significant feature of the data is that the TF at which the effect of masking is maximal falls continuously as the test TF falls. This continuous shift is hard to reconcile with models of detection in the literature that relate detection to the most sensitive filter, without resorting to a large number of temporal filters. We developed a new model, which relies on only three temporal filters and posits that detection {{is the result of a}} threshold decision based on the <b>compound</b> Bayesian <b>probability</b> of all filter responses, not just the most sensitive filter...|$|R
40|$|We present large {{deviation}} {{results for}} estimators of unknown probabilities which satisfy a suitable exponential decay condition. These results provide some {{extensions of the}} large deviation estimates given in Macci and Petrella (2006). Furthermore we propose a classical approach which {{is different from the}} one presented in Ganesh et al. (1998) and we cannot say that the Bayesian approach is more conservative as in that paper. Large deviations Varadhan's Lemma Level crossing <b>probability</b> <b>Compound</b> Poisson process Brownian motion...|$|R
