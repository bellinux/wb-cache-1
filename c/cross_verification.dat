44|11|Public
50|$|Triangulation is a {{powerful}} technique that facilitates validation of data through <b>cross</b> <b>verification</b> from two or more sources. In particular, {{it refers to the}} application and combination of several research methods {{in the study of the}} same phenomenon.|$|E
50|$|Level 4: {{multinational}} {{collaboration of}} large and adaptive systems. A crowdsourcing website {{at this level}} may contain domain-oriented crowdsourcing with ontology, reasoning, and annotation; automated <b>cross</b> <b>verification</b> and test generation processes; automated configuration of crowdsourcing platform; and may restructure the platform as SaaS with tenant customization.|$|E
50|$|Level 3: {{teams of}} people (< 100 and > 10), {{well-defined}} system, large systems, long time span (< 2 years), automated <b>cross</b> <b>verification</b> and cross comparison among contributions. A crowdsourcing website {{at this level}} may contain automated matching of requirements to existing components including matching of specification, services, and tests; automated regression testing.|$|E
5000|$|Clock Domain <b>Crossing</b> <b>Verification</b> (CDC check): Similar to linting, {{but these}} checks/tools {{specialize}} in detecting and reporting potential issues like data loss, meta-stability due {{to use of}} multiple clock domains in the design.|$|R
40|$|Shallow Trench Isolation(STI) {{is widely}} used in {{advanced}} CMOS technologies. This paper describes a shallow trench isolation for 0. 13 μm CMOS technologies development which utilizes AMAT Ultima Plus High Density Plasma (HDP) CVD oxide process to fill 0. 18 μm wide and 0. 5 μm deep trenches with void free. Through optimizing source/bias RF power, process gas flow and <b>cross</b> section <b>verification,</b> as a result, we got a robust gap-fill recipe with void free...|$|R
40|$|Abstract — A Modern complex SOC has {{a number}} of {{different}} asynchronous clock domains and data is frequently transferred from one clock domain to another which needs to be synchronized. Designer uses many types of synchronizer, most commonly used synchronizer is 2 flop synchronizer which is mainly used for control signals, there are other types of synchronizer such as MUX synchronizer, FIFO synchronizer, these are used when a multi bit signal or a bus needs to be synchronized in destination domain. Due to large design size and presence of multiple asynchronous clocks in a SOC, synchronizer is needed at thousands of places and it becomes very important to ensure if proper synchronization is done at all required places. STA does not cover clock domain crossing paths while gate level simulation’s are very limited since it does not cover all the CDC paths, it flags violation on CDC path only if timing’s are not met. As a standard practice, CDC verification for a SOC is mostly done using formal tools such as ―lec –verify‖, Questa CDC solution etc. These tools perform structural CDC checks for a given design. This paper explains about clock domain crossing requirement, synchronizer structure and a methodology to find out various issues in structural CDC checks. It also explains about reset synchronizer requirement & its verification. Keywords [...] CDC, Clock domain <b>crossing</b> <b>verification,</b> Synchronizers,Gate level simulation. I...|$|R
30|$|The paper employs two-step research: content {{analysis}} of both coaching definitions extracted from the literature and experts’ interview with subsequent comparative analysis of the obtained results. Triangulation of research results was obtained through <b>cross</b> <b>verification</b> from two sources.|$|E
40|$|Key {{research}} activities:To {{address the}} research question on: was there gender roles and preferences in animal feeding a socio-cultural {{study was conducted}} in purposively selected two districts Bougouni and Koutiala in six village of Mali, using 23 participatory qualitative tools. Data was collected from July to January 2016. Triangulation method between various tools, secondary sources were used for <b>cross</b> <b>verification</b> of dat...|$|E
30|$|FVC. The {{images were}} {{specifically}} chosen to exhibit alike characteristics (no deliberate distortion) and were all captured with optical fingerprint scanners. Specifically, we selected all third prints of FVC 2000 DB 3 [21] and FVC 2004 DB 2 [22] and all sixth prints of FVC 2002 DB 1 [23]. This {{leads to a}} database consisting of 330 imposter prints. We verified via the <b>cross</b> <b>verification</b> scores that no duplicate of any imposter is included.|$|E
40|$|Shape sensing {{techniques}} utilizing Fiber Bragg grating (FBG) arrays can enable {{real-time tracking}} {{and control of}} dexterous continuum manipulators (DCM) used in minimally invasive surgeries. For many surgical applications, the DCM may need to operate with much larger curvatures than what current shape sensing methods can detect. This paper proposes a novel shape sensor, which can detect a radius of curvature of 15 mm for a 35 mm long DCM. For this purpose, we used FBG sensors along with nitinol wires as the supporting substrates to form a triangular <b>cross</b> section. For <b>verification,</b> we assembled the sensor inside {{the wall of the}} DCM. Experimental results indicate that the proposed sensor can detect the DCM&# 39;s curvature with an average error of 3. 14 %...|$|R
40|$|One {{measure of}} the quality of a product {{requirement}} is that it be verifiable. Verifiability assessment is one of the exit criteria for the Systems Requirements Review and is necessary for requirement validity. Nomination of one or more verification methods (inspection, analysis, modeling and simulation, demonstration or test) is often taken as the sole evidence of verifiability A completed <b>Verification</b> <b>Cross</b> Reference Matrix is frequently considered as the. final verifiability assessment and responsibility {{for the remainder of the}} verification effort is transferred to the test and evaluation and other implementing communities for completion. • Lessons learned from many Programs have shown that a more robust application of systems engineering should include the requirements engineers (with detailed knowledge of product requirement intent) working with the implementing organizations as the best combination to define the verification requirements. Such definition should include statement of the verification objectives, success criteria and environment. Including this information in the ”Quality Assurance ” section of the requirements document allows for buy-in by the customer well in advance of implementing the verification activities. This information is used b...|$|R
5000|$|In {{the late}} 1970s and {{throughout}} the 1980s a significant strand on AI and Law work involved the production of executable models of legislation. Originating in the LEGOL work of Ronald Stamper [...] {{the idea was to}} represent legislation using a formal language and to use this formalisation (typically with some sort of user interface to gather the facts of a particular case) as the basis for an expert system. This became popular, mainly using the Horn Clause subset of first order predicate calculus. In particular Sergot et al.'s representation of the British Nationality Act did much to popularise the approach. In fact, as later work showed, this was an untypically suitable piece of legislation on which to employ the approach: it was new, and so had not been amended, relatively simple and almost all of the concepts were non-technical. Later work, such as that on Supplementary Benefits, showed that larger, more complicated (containing many cross references, exceptions, counterfactuals, and deeming provisions), legislation which used many highly technical concepts (such as contribution conditions) and which had been the subject of many amendments produced a far less satisfactory final system. Some efforts were made to improve matters from a software engineering perspective, especially to handle problems such as <b>cross</b> reference, <b>verification</b> and frequent amendment. The use of hierarchical representations [...] was suggested to address the first problem, and so-called isomorphic representation was intended to address the other two. As the 1990s developed this strand of work became largely absorbed in the development of formalisations of domain conceptualisations, (so-called ontologies), which became popular in AI following the work of Gruber. Early examples in AI and Law include Valente's functional ontology and the frame based ontologies of Visser and van Kralingen. Legal ontologies have since become the subject of regular workshops at AI and Law conferences and there are many examples ranging from generic top-level and core ontologies to very specific models of particular pieces of legislation.|$|R
40|$|The {{presentation}} {{contains the}} following items to analyze aerodynamic flow properties around the NGT 2 train model in strong cross winds -	Reynolds number 250 k and 450 k were considered, wind impact angles 0, 10, 20, 30 degrees -	A series of RANS calculations performed with OpenFOAM code -	Comparison of wind tunnel data with numerical results {{in terms of}} aerodynamic coefficients, PIV slices and wall shear stress profiles -	<b>Cross</b> <b>verification</b> simulation using the DLR TAU code -	URANS and DDES calculations performed with OpenFOA...|$|E
40|$|This paper {{attempts}} to verify whether liberal economic measures introduced in India since 1991 / 92 has brought any statistically significant growth {{difference in the}} growth performance of the manufacturing sector in India. It used the time series data from 1973 / 74 to 2007 / 08. Periodised it based on both the exogenously and endogenously determined breaks for <b>cross</b> <b>verification</b> of the growth results and arrived at {{the conclusion that the}} liberal economic regime failed to contribute to the growth performance of the manufacturing sector in India...|$|E
30|$|This {{two-stage}} study {{seeks to}} answer the following research questions. How is coaching defined? What is the aim of coaching? Who are involved in coaching? What coaching outcomes are expected? During the first stage, the definitions of coaching are extracted from the literature and analyzed to identify the distinctive features of coaching. During the second stage, the experts are interviewed to explore the views of practitioners in coaching about a place of coaching in organizations. Literature review, content analysis and comparative analysis are used {{for the purposes of this}} study. Triangulation of research results is obtained through <b>cross</b> <b>verification</b> from two sources.|$|E
60|$|I {{think all}} {{verification}} is ultimately {{of the above}} sort. We verify a scientific hypothesis indirectly, by deducing consequences as to the future, which subsequent experience confirms. If somebody were to doubt whether Caesar had <b>crossed</b> the Rubicon, <b>verification</b> could only {{be obtained from the}} future. We could proceed to display manuscripts to our historical sceptic, in which it was said that Caesar had behaved in this way. We could advance arguments, verifiable by future experience, to prove the antiquity of the manuscript from its texture, colour, etc. We could find inscriptions agreeing with the historian on other points, and tending to show his general accuracy. The causal laws which our arguments would assume could be verified by the future occurrence of events inferred by means of them. The existence and persistence of causal laws, it is true, must be regarded as a fortunate accident, and how long it will continue we cannot tell. Meanwhile verification remains often practically possible. And since it is sometimes possible, we can gradually discover what kinds of beliefs tend to be verified by experience, and what kinds tend to be falsified; to the former kinds we give an increased degree of assent, to the latter kinds a diminished degree. The process is not absolute or infallible, but it has been found capable of sifting beliefs and building up science. It affords no theoretical refutation of the sceptic, whose position must remain logically unassailable; but if complete scepticism is rejected, it gives the practical method by which the system of our beliefs grows gradually towards the unattainable ideal of impeccable knowledge.|$|R
40|$|Recurrence rate {{is often}} used to {{describe}} volcanic activity. There are numerous documented ex- amples of non-constant recurrence rate (e. g. Dohrenwend et al., 1984; Condit and Connor, 1996; Cronin et al., 2001; Bebbington and Cronin, 2011; Bevilacqua, 2015), but current techniques for calculating recurrence rate are unable to fully account for temporal changes in recurrence rate. A local–window recurrence rate model, which allows for non-constant recurrence rate, is used to calculate recurrence rate from an age model consisting of estimated ages of volcanic eruption from a Monte Carlo simulation. The Monte Carlo age assignment algorithm utilizes paleomagnetic and stratigraphic information to mask invalid ages from the radiometric date, represented as a Gaussian probability density function. To verify the age assignment algorithm, data from Heizler et al. (1999) for Lathrop Wells is modeled and compared. Synthetic data were compared with expected results and published data were used for <b>cross</b> comparison and <b>verification</b> of recurrence rate and volume flux calculations. The latest recurrence rate fully constrained by the data is reported, based upon data provided in the referenced paper: Cima Volcanic Field, 33 + 55 /- 14 Events per Ma (Dohren- wend et al., 1984), Cerro Negro Volcano, 0. 29 Events per Year (Hill et al., 1998), Southern Nevada Volcanic Field, 4. 45 + 1. 84 /- 0. 87 (Connor and Hill, 1995) and Arsia Mons, Mars, 0. 09 + 0. 14 /- 0. 06 Events per Ma (Richardson et al., 2015). The local–window approach is useful for 1) identifying trends in recurrence rate and 2) providing the User the ability to choose the best median recurrence rate and 90 % confidence interval with respect to temporal clustering...|$|R
40|$|Prior to the {{development}} of a production standard control system for ML Aviation's plan-symmetric remotely piloted helicopter system, SPRITE, optimum solutions to technical requirements had yet to be found for some aspects of the work. This thesis describes an industrial project where solutions to real problems have been provided within strict timescale constraints. Use has been made of published material wherever appropriate, new solutions have been contributed where none existed previously. A lack of clearly defined user requirements from potential Remotely Piloted Air Vehicle (RPAV) system users is identified, A simulation package is defined to enable the RPAV designer to progress with air vehicle and control system design, development and evaluation studies and to assist the user to investigate his applications. The theoretical basis of this simulation package is developed including Co-axial Contra-rotating Twin Rotor (CCTR), six degrees of freedom motion, fuselage aerodynamics and sensor and control system models. A compatible system of equations is derived for modelling a miniature plan-symmetric helicopter. Rigorous searches revealed a lack of CCTR models, based on closed form expressions to obviate integration along the rotor blade, for stabilisation and navigation studies through simulation. An economic CCTR simulation model is developed and validated by comparison with published work and practical tests. Confusion in published work between attitude and Euler angles is clarified. The implementation of package is discussed. dynamic adjustment of assessment. the theory into a high integrity software Use is made of a novel technique basing the integration time step size on error Simulation output for control system stability <b>verification,</b> <b>cross</b> coupling of motion between control channels and air vehicle response to demands and horizontal wind gusts studies are presented. Contra-Rotating Twin Rotor Flight Control System Remotely Piloted Plan-Symmetric Helicopter Simulation Six Degrees of Freedom Motion (i i...|$|R
40|$|The article {{investigates the}} {{phenomenon}} of Creativity – the background of this term, its development and what we understand with creativity in business organizations nowadays. The concept of Creativity, Individual creativity and Organizational creativity are given, as well as provided differences between Individual and Organizational creativity. Specifically, the authors analyze the Organizational creativity, its features and influencing factors. This article provides two-step research: 1) content analysis of scientific literature, extracting factors of organizational creativity and 2) interview of business representatives with subsequent comparative analysis of the obtained results. Triangulation of research was obtained through <b>cross</b> <b>verification</b> from two sources...|$|E
40|$|Calibration is {{indispensable}} for automatic micromanipulation. In this paper, using the height difference between different focus planes detected from the microscope and the coordinates of the probe measured from encoders, systematic method to calibrate the relative orientation between the probe and the microscope is developed. By analyzing and overlapping the images under different focus planes, the tip position and tool orientation are also derived. Using the Least Square Error (LSE) method {{to process the}} data measured, the influence of various positioning errors can be reduced. Experiments are performed to calibrate a micromanipulation system using the proposed methods. The calibration results are validated by <b>cross</b> <b>verification</b> through further experiments. Link_to_subscribed_fulltex...|$|E
40|$|This paper {{describes}} {{the evolution of}} a design and verification methodology successfully used to develop advanced ASICs as components of multiple new commercial products. The ASICs are typically large, high speed, algorithmically complex and implement novel functionality. The ASIC development process is driven by the commercial pressures of low cost and short schedules of multiple projects. It is carried out using a team of designers of varying experience including new staff. The dual emphasis of our methodology is maintaining fine control over the design and verification process, together with full independent <b>cross</b> <b>verification</b> {{as an integral part of}} the entire ASIC and system development process. ...|$|E
40|$|The monthly global 28 3 28 Extended Reconstructed Sea Surface Temperature (ERSST) {{has been}} revised and updated from version 4 to version 5. This update {{incorporates}} a new release of ICOADS release 3. 0 (R 3. 0), {{a decade of}} near-surface data from Argo floats, and a new estimate of centennial sea ice from HadISST 2. A number of choices in aspects of quality control, bias adjustment, and interpolation have been substantively revised. The resulting ERSST estimates have more realistic spatiotemporal variations, better representation of high-latitude SSTs, and ship SST biases are now calculated relative to more accurate buoy measurements, while the global long-term trend remains about the same. Progressive experiments have been undertaken to highlight the effects of each change in data source and analysis technique upon the final product. The reconstructed SST is systematically decreased by 0. 0778 C, as the reference data source is switched from ship SST in ERSSTv 4 to modern buoy SST in ERSSTv 5. Furthermore, high-latitude SSTs are decreased by 0. 18 – 0. 28 C by using sea ice concentration from HadISST 2 over HadISST 1. Changes arising from remaining innovations are mostly important at small space and time scales, primarily having an impact where and when input observations are sparse. <b>Cross</b> validations and <b>verifications</b> with independent modern observations show that the updates incorporated in ERSSTv 5 have improved the representation of spatial variability over the global oceans, the magnitude of El Niño and La Niña events, and the decadal nature of SST changes over 1930 s– 40 s when observation instruments changed rapidly. Both long- (1900 – 2015) and short-term (2000 – 15) SST trends in ERSSTv 5 remain significant as in ERSSTv 4...|$|R
40|$|An {{integrated}} radiobiological {{model has}} been developed in this thesis using the Monte Carlo toolkit “Geant 4 ” to produce a radiobiological modelling software package. The result is a simulation capable of: (a) growing a simulated 3 D cell structure (i. e. tumour or mammalian tissue) composed of individual cells (with accurate chemical composition and geometry), (b) irradiating the cells and recording the microdosimetric track structure in each cell, (c) clustering spatially correlated ionisation events into DNA double strand breaks and then (d) predicting the likelihood that any given cell will survive. The novelty of this model {{is its ability to}} predict both the microscopic and macroscopic outcome of radiobiology experiments while varying input parameters such as cell line, radiation type, tumour geometry, dose etc. Previous research in this area has been limited to simple water volumes as representations of cells and none have been combined into an integrated radiobiological model. Model Development Cellular Growth Model The cellular growth model consists of two parts. The first part is a geometrical and chemical description of a single cell. The second is a cellular growth model describing the growth kinetics of a group of cells. When combined, they form a simulated macroscopic cell mass composed of individual microscopic cells. A template was first designed within Geant 4 for a single cell containing properties such as cell size, nucleus dimensions, and cytoplasm composition. Cellular dimensions and composition were obtained from previous publications. The cellular growth model is a mathematical model which attempts to replicate the growth characteristics of either regular or cancerous cells. The code populates a volume of specified dimension and shape with cells of random dimension, rotation and position governed by the characteristics of a given cell line. A general requirement for Geant 4 simulations is a non-overlapping geometry. A custom algorithm was developed in the current work to ensure that the cells in the tumour geometry did not overlap before importing into Geant 4. The time required to “grow” a tumour using this code is proportional to the number of cells in the volume. The time to produce a tumour containing approximately 10 ⁵ cells is typically less than 1 hour. However, the time required to complete the irradiation stage of the code is strongly dependant on the number of cells in the tumour. To produce results in a reasonable amount of time, we were limited to using tumours containing less than 10 ⁴ cells. The position, rotation and size of each tumour were then exported to a file in a format which could be imported by our custom Geant 4 simulation. Cellular Irradiation A method called parameterisation was used inside Geant 4 to combine the single cell structure and the spatial properties of the cells (generated by the cellular growth model) to produce a cellular mass which can be irradiated virtually. To our knowledge, Geant 4 has not been used to simulate particle interactions in such a large number of complex volumes. This achievement {{is the result of the}} efficiency of the unique parameterisation code developed in this thesis. The Geant 4 particle tracking tool kit enables the cells to be irradiated with different types of radiation (such as protons, electrons, photons) and records the positions of the ionisation damage in each cell. For high LET radiation (such as heavy ions), the primary cell damage mechanism is direct ionisation damage. By recording the position and energy deposited in each ionisation event, the probability of a cell surviving or dying can be calculated. The first two stages of the code were tested by predicting and quantifying the radiosensitisation effect of cells by gold nanoparticles. Gold nanoparticles were introduced into the cellular geometries and the frequency of ionisation effects within the cells was measured. It was determined that the radiosensitisation effect of gold nanoparticles is proportional to the concentration of gold within the cell, inversely proportional to the energy of the incident photon and strongly dependant on position within the cell. When the cells were irradiated with 80 kVp x-rays, the damage to the cells was determined to be approximately 10 times that of cells irradiated without gold nanoparticles. When a typical 6 MV linear accelerator x-ray beam was used to irradiate the cells, the damage to the cells was only 1. 2 times higher that measured without gold nanoparticles. These results suggest that the primary dose enhancement effect is the result of the increase in photoelectric cross section caused by the local increase in the effective atomic number within the cell. Ionisation Clustering In order to predict the biological damage to the cells, the ionisation damage calculated in the previous two stages of the code needed to be clustered into DNA strand breakages. To cluster the ionisation events into double strand breaks, a hierarchical clustering algorithm was developed. Ionisation events are clustered into a DSB if the Euclidean distance from the centre of the DSB centroid ("centre of mass" of the cluster) is less than 3. 4 nm (length of 10 base pairs in DNA). A DSB is defined to be “simple” if the cluster contains two ionisation events. A DSB is “complex” if it contains three or more ionisation events. In typical radiotherapy treatments, several grays of radiation dose are delivered. Microscopically, this corresponds to billions or trillions of individual ionisation events. When simulated computationally, this has obvious storage and processing issues. The effects of this problem were minimised by considering only a small volume of cells (< 10 ³ cells). Even with such a small number of cells, the total number of ionisation events to process was in excess of 10 ⁷. In terms of computational time, this section of the code is highly efficient but at the expense of system resources. The large system RAM requirements mean that the code can only be executed on a 64 bit processor in its current form. DNA Repair Model The basis for the DNA repair model in the current work is the two lesion kinetics (TLK) model. In the current work we have expanded on the TLK model and implemented it as a method of describing the repair of the ionisation damages produced in Geant 4. The most notable improvement lies in implementing the model on a cell by cell basis instead of modelling the repair kinetics as an average of all cells in the volume (e. g. a tumour). The DNA repair model was calibrated and tested using experimental data for V 79 Chinese hamster cells irradiated with 0. 76 and 1. 9 MeV proton radiation. Once calibrated, the experimental and calculated values for cell survival were in good agreement (< 7 % difference). <b>Cross</b> Section <b>Verification</b> The validity and accuracy of the Geant 4 cross sectional data was tested by comparing simulated and experimental data. Geant 4 has been shown to be able to simulate radiation interactions of very low energy particles (to approximately ~ 1 eV (debated)). However, our investigation (and previous publications) has shown differences of up to ± 30 % (between simulated and experimental data) in the differential cross section of electrons and protons at energies below 100 eV. Our investigation also revealed a similar discrepancy with other comparable Monte Carlo packages including PARTRAC and RITRACKS. However, within the energy range we have investigated throughout this research, there is good agreement between both experimental data and data predicted by other MC packages. Effect of Indirect Radiation Damages on Cellular Survival To investigate the contribution from indirect damages in our previous studies, a MC software package called RITRACKS was used which is capable of simulating the production yields of free radical species due to the physical interactions of ionising radiation in water. Utilising the clustering algorithm and DNA repair model from our previous Geant 4 investigation, we attempted to quantify the cellular lethality of direct and indirect radiation damages. Our study has shown that for particles with LET ~ 1000 keV/μm (50 MeV/amu ⁵⁶Fe ions, the contribution to DNA damage from indirect damages is still approximately 50 %. Conclusion A comprehensive radiobiological simulation has been developed capable of predicting the complex ionisation track structure of ionising radiation (e. g. photons, protons and carbon ions) through individual cells. Subsequently, predicting the biological outcome within individual cells by tracking the formation and repair of DNA strand breaks. The capabilities of the software have been demonstrated for use in novel radiotherapy treatment techniques. Thesis (Ph. D.) [...] University of Adelaide, School of Chemistry and Physics, 201...|$|R
40|$|The {{thermal load}} {{protection}} of hypersonic and space vehicle structures {{can be achieved}} by either passive or active methods, such as ablative materials or active cooling. For the latter, porous Ceramic Matrix Composite media offer a possibility to exploit thermal protection by means of transpiration cooling due to their higher permeability. Ceramic materials used for regenerative cooling have a much smaller permeability (several orders of magnitude of difference). The cooling techniques based on fluid transpiration are particularly interesting for reusable systems. However, one of the related key issues is the determination of permeability parameters such as the Darcy's and Forchheimer's terms which are highly dependent on the fabrication process, cracks, delamination and heterogeneities notably. After a review of available permeation laws, the present paper aims at proposing an analytical and applied comparison of two of them (one based on the international norm ISO 4022 and one derived for compressible flows). To apply these mathematically equivalent laws, a <b>cross</b> <b>verification</b> and validation has been realized on two different test rigs with different porous media (metallic and composite) with a range of Darcian permeability varying from 10 E- 17 m² to 10 E- 11 m². The PRISME test bench has a lower accuracy for thick samples (over 3 mm) due to lateral permeation while the DLR rig is free from such a phenomenon thanks to an innovative sealing (which is however not adapted to samples thinner than 3 mm). The two test rigs are found to be complementary since the PRISME one is more accurate for structures related to regenerative cooling (10 E- 13 m² and lower) while the DLR one is better for active cooling structures (10 E- 14 m² and higher). The range of permeability is very thin for <b>cross</b> <b>verification</b> but the results are nevertheless judged to be satisfactory (discrepancy around 14...|$|E
40|$|The {{thermal load}} {{protection}} of hypersonic and space vehicle structures {{can be achieved}} by either passive or active methods, such as ablative materials or active cooling. For the latter, porous Ceramic Matrix Composite media offer a possibility to exploit thermal protection by means of transpiration cooling due to their higher permeability. One of the related key issues is the determination of permeability parameters such as the Darcy's and Forchheimer's terms. The present paper aims at proposing an analytical and applied comparison of two of them (one based on the international norm ISO 4022 and one derived for compressible flows, the P method). To apply these mathematically equivalent laws, a <b>cross</b> <b>verification</b> and validation has been realized on two different test rigs with different porous media (metallic and 1 Associate Professor, corresponding author...|$|E
40|$|A {{method for}} human {{settlements}} extraction from high resolution remote sensing imagery using feature-level-based fusion of right-angle-corners and right-angle-sides is proposed in this paper. First, the corners and line segments are detected, the right-angle-corners and right-angle-sides {{are determined by}} <b>cross</b> <b>verification</b> of the detected corners and line segments, and {{these two types of}} features are rasterized. Second, a human settlement index image is built based on the density and distance of the right-angle-corners and right-angle-sides in a local region. Finally, the polygons of human settlements are generated through binary thresholding of the index image, conversion from raster format to vector format, and sieving. Three images are used for testing the proposed method. The experimental results show that our proposed method has higher accuracy than the existed method. Specifically, the correctrate, completeness, and quality of our method is higher 6. 76 %, 10. 12 %, 12. 14 % respectively than the existed method...|$|E
40|$|When {{multiple}} workspaces {{for resources}} are allocated within a narrow work area, the constructability of work may deteriorate {{based on the}} conflicts that occur due to the mutual adjacency levels between the workspaces. In this situation, {{the identification of the}} workspace overlapping status is essential for establishing the optimized layout planning of workspaces. However, current 3 D environments consider only physical collision and reaction by the <b>cross</b> <b>verification</b> of geometric properties to detect collisions between 3 D objects. Thus, the configuration of methodology to visually check the overlapping level of workspaces with conflicting status is insufficient for a successful interference management. This study develops a new grid cell-based analysis algorithm and its 4 D simulation system for visually verifying the workspace overlapping of dynamic resources to be allocated multiply within a confined area, such as a tunnel or underground facility. The proposed algorithm and system can assist in the intensive management of activities that have a high level of workspace overlap by visualizing their overlapping level...|$|E
40|$|Much legal {{evidence}} is being generated by {{and stored in}} in-formation systems. In this paper we look at evidence from an auditing point of view. Auditors rely on evidence of the party being audited, who may have a legitimate or illegit-imate interest to manipulate it. To assess the quality of audit evidence, we argue for an approach called model-based auditing. It {{is based on a}} mathematically precise model of the expected relationships between the flow of money and the flow of goods or services. Such equations are used for <b>cross</b> <b>verification.</b> If the equations do not hold, either some-thing is wrong (violation) or some underlying assumption is false (exception). To show the usefulness of the approach, we look in particular at a case study of a legal dispute about automated contract monitoring. A precise revenue model is instrumental in demonstrating that the data set does indeed constitute appropriate evidence to settle the case...|$|E
40|$|Abstract. In this study, the {{predictive}} model of friction coefficient using cylindrical compression was constructed through combining the {{finite element method}} and neutral networks. Namely, the related data of the materials characters, cylinder compression bulging, {{and how they were}} associated with friction coefficient was obtained by the finite element method. Based on those analysis data, the relationship model, reflecting the relationship among the materials characters such as strength coefficient and strain-hardening exponent, the compression bulging such as reduction height, expanding in upper ending, expanding in bottom ending, maximum expanding in outside diameter and the friction coefficient in workpiece/die interface, was constructed. Finally, the <b>cross</b> <b>verification</b> between finite element analysis, prediction by neutral network model and the experiments of cylindrical compression testing and ring compression testing are repeatedly checked to ensure the accuracy and reliability of the constructed model. Results of the current study indicate that their errors are extremely limited, and the developed predictive system is reliable and feasible...|$|E
40|$|For {{a novice}} programmer, coding is {{equivalent}} to a nightmare. A novice programmer tries to replicate steps provided by the faculty and on compilation gets a number of errors which the novice programmer {{is not able to}} resolve. This system provides support to the faculty about the coding ability of the students and their ability to solve those errors. Also, the faculty can provide a solution to the errors which are occurring to the students and the solution is displayed accordingly. The emphasis of this paper is on developing this system within JAVA and making use of Online Compilers. Moreover, we focus on a new system which is able to provide online code management and these codes get compiled using an online compiler and these programs can be viewed by the respective faculty for <b>cross</b> <b>verification.</b> This paper takes into account the syntactic errors, runtime and semantic errors. Comment: 4 pages, 1 figur...|$|E
40|$|Safety {{assessments}} of cross-wind influence on high-speed train operation requires a detailed {{investigation of the}} aerodynamic forces acting on a vehicle. European norm 14067 - 6 permits the derivation of required integral force and moment coefficients by experiments {{as well as by}} numerical simulation. Utilising the DLR’s Next Generation Train 2 model geometry, we have performed a case study using incompressible steady RANS simulations from the OpenFOAM fluid dynamics solver software. Validation data for the exact same model configuration and moderate Reynolds numbers 250, 000 and 450, 000 is provided by side wind tunnel experiments. Highly resolved <b>cross</b> <b>verification</b> computations with the compressible DLR TAU code confirm that yaw angles > 30 degrees create major vortex systems on the leeward side of the train leading to sizeable uncertainties in predicted integral coefficients. At low to intermediate wind angles the flow remains attached and absolute errors in integral quantities decline with decreasing yaw angles. A consistent relative difference to the experimental results greater than 10...|$|E
40|$|Purpose-Microfinance holds a big {{promise to}} {{generate}} income and employment and alleviate poverty in developing countries. NGO-MFIs have significant contribution in building capacity of SHGs and shaping {{them to grow}} for self sustainable. This paper attempts to assess the economic empowerment of SHGs, the social empowerment of SHGs and the overall socio-economic empowerment of SHGS due to microfinance intervention of NGO-MFIs in Manipur State {{which is in the}} North-Eastern corner of India. Design/methodology/approach-A sample size of 120 SHG members of 60 SHGs from 20 NGOs is considered for the study. Socio-economic empowerment index before and after microfinance intervention is used for assessing the socio-economic empowerment of SHG members. Recall method is used for assessing the impact of microfinance as it is cost effective and less time consuming. However, <b>cross</b> <b>verification</b> is also done with records of SHG members maintained by NGO-MFIs and thereafter data is normalized to give near accurate result. Findings – It is found that after joining the microfinance programme, there is a significant improvement in overall economic empowermen...|$|E
40|$|In this paper, {{we propose}} a <b>cross</b> <b>verification</b> {{mechanism}} for secure execution and dynamic component loading. Our mechanism {{is based on}} a combination of code signing and same-origin policy, and it blocks several types of attacks from drive-by download attacks to malicious component loadings such as DLL hijacking, DLL side-loading, binary hijacking, typical DLL injection and loading of newly installed malware components, even when malicious components have valid digital signatures. Considering modern malware often uses stolen private keys to sign its binaries and bypass code signing mechanism, we believe the proposed mechanism can significantly improve the security of modern computing platforms. In addition, the proposed mechanism protects proprietary software components so that unauthorised use of such components cannot occur. We have implemented a prototype for Microsoft Windows 7 and XP SP 3, and evaluated application execution and dynamic component loading behaviour under our security mechanism. The proposed mechanism is general, and can be applied to other major computing platforms including Android, Linux and Mac OS X. 12 page(s...|$|E
40|$|The {{robustness}} {{and security}} of the biometric watermarking approach can be improved by using a multiple watermarking. This multiple watermarking proposed for improving security of biometric features and data. When the imposter tries to create the spoofed biometric feature, the invisible biometric watermark features can provide appropriate protection to multimedia data. In this paper, a biometric watermarking technique with multiple biometric watermarks are proposed in which biometric features of fingerprint, face, iris and signature {{is embedded in the}} image. Before embedding, fingerprint, iris, face and signature features are extracted using Shen-Castan edge detection and Principal Component Analysis. These all biometric watermark features are embedded into various mid band frequency curvelet coefficients of host image. All four fingerprint features, iris features, facial features and signature features are the biometric characteristics of the individual and they are used for <b>cross</b> <b>verification</b> and copyright protection if any manipulation occurs. The proposed technique is fragile enough; features cannot be extracted from the watermarked image when an imposter tries to remove watermark features illegally. It can use for multiple copyright authentication and verification...|$|E
40|$|The {{diversity}} of medical factors makes {{the analysis and}} judgment of uncertainty {{one of the challenges}} of medical diagnosis. A well-designed classification and judgment system for medical uncertainty can increase the rate of correct medical diagnosis. In this paper, a new multidimensional classifier is proposed by using an intelligent algorithm, which is the general fuzzy cerebellar model neural network (GFCMNN). To obtain more information about uncertainty, an intuitionistic fuzzy linguistic term is employed to describe medical features. The solution of classification is obtained by a similarity measurement. The advantages of the novel classifier proposed here are drawn out by comparing the same medical example under the methods of intuitionistic fuzzy sets (IFSs) and intuitionistic fuzzy cross-entropy (IFCE) with different score functions. <b>Cross</b> <b>verification</b> experiments are also taken to further test the classification ability of the GFCMNN multidimensional classifier. All of these experimental results show the effectiveness of the proposed GFCMNN multidimensional classifier and point out that it can assist in supporting for correct medical diagnoses associated with multiple categories...|$|E
40|$|Networking testbeds {{are playing}} an {{increasingly}} {{important role in the}} development of new communication technologies. Testbeds are traditionally built for a particular project or to study a specific technology. An alternative approach is to federate existing testbeds to a) cater for experimenter needs which cannot be fulfilled by a single testbed, and b) provide a wider variety of environmental settings at different scales. These heterogenous settings allow the study of new approaches in environments similar to what one finds in the real world. This paper presents OMF, a control, measurement, and management framework for testbeds. It describes through some examples the versatility of OMF’s current architecture and gives directions for federation of testbeds through OMF. In addition, this paper introduces a comprehensive experiment description language that allows an experimenter to describe resource requirements and their configurations, as well as experiment orchestration. Researchers would thus be able to reproduce their experiment on the same testbed or in a different environment with little changes. Along with the efficient support for large scale experiments, the use of testbeds and support for repeatable experiments will allow the networking field to build a culture of <b>cross</b> <b>verification</b> and therefore strengthen its scientific approach. 1...|$|E
