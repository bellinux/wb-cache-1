3|6991|Public
40|$|Financial {{literacy}} is reviewed {{in the article}} as a factor influencing any individual’s well-being. Characteristics of a financially competent individual are defined. Behavioral mistakes impeding rational decision-making are outlined. Structures bearing the signs of financial pyramids are described {{as an example of}} their participants’ cognitive limitations. The importance of creating a <b>common</b> <b>information</b> <b>area</b> is stressed. This process is aimed at remedying negative consequences for all economic agents and preventing inefficient financial decisions when executing financial transactions. The major task of the process is to incorporate information about social and economic activity of institutions (state, business and non-governmental) and population in the <b>common</b> <b>information</b> <b>area.</b> Therefore, every economic agent will get prompt and trustworthy information. It will encourage an individual to make financially adequate decisions. The article also presents fundamental solutions for improving individuals’ well-being when raising their financial literac...|$|E
40|$|Part 2 : Short and Position PapersInternational audienceThe {{main goal}} {{of this paper is}} to {{describe}} the integrated approach to product lifecycle management in the context of enterprise information system landscape. Product lifecycle management is a part of the <b>common</b> <b>information</b> <b>area</b> in the enterprise. In this work the analysis of enterprise interoperability problems was presented. To classify such problems and solution approaches we use a framework for enterprise interoperability, described by D. Chen at [1]. The paper suggests reference architecture for product lifecycle management systems based on multi-agent concept. It promotes understanding of the interrelationships of different lifecycle stages for acquiring and manipulating concurrent engineering knowledge...|$|E
40|$|The EU has {{set out to}} {{introduce}} {{information and communication technologies}} into all areas of the socioeconomic structure. Initially, however, a <b>Common</b> <b>Information</b> <b>Area</b> (CIA) needs to be established to ensure that information and communication are freely mobile throughout the economic and social space. To this end the EU needs to develop a strategy to ensure that a series of high-powered telecommunication networks emerge in an integrated and coordinated fashion. While several initiatives to assist the market-based provision of the CIA have been established, {{there seems to be an}} absence of an overarching coordinated strategy to aid its development. A single body to oversee the EU telecommunication sector may need to be established. ...|$|E
50|$|The {{library is}} at the North Hall, and this was opened on 2012. There are three floors in this building, with the ground floor having 15 classrooms. The library uses {{the second and third}} floors of the building. On the second floor, {{computers}} are available in the <b>information</b> <b>common</b> <b>area.</b> There are 25 group study rooms. and one classroom for library instruction. Appropriate assistance can be found facilitated by library employees within three desk areas: Reference, Circulation and New Media. On the third floor, there is the law collection classroom and the main stacks of circulation books. The North Hall has a distinction of being a green building, with LEED Silver certification.|$|R
50|$|The Nathan W. Collier Library {{was named}} after Nathan White Collier, {{who served as the}} third {{president}} of Florida Baptist Academy for forty-five years. Dr. Collier was responsible for tireless fundraising and advocating; acquiring property and land; increasing enrollment and attracting nationally renowned faculty. Collier tried to replicate the educational aims and programs of Booker T. Washington. His greatest contribution was to the education and training of African American teachers throughout Florida. The Collier library houses 120,000 volumes, two <b>Information</b> <b>Commons</b> <b>areas</b> as well as separate Electronic, Teaching, Periodicals, Audiovisual and Group study rooms. The library subscribes to 30 databases, 519 periodicals and contains two special collections: The Rev. I. C. Mickins Theological and Sermonic Research as well as the Dr. Laban Connor Black Collection.|$|R
40|$|A {{distributed}} processing architecture can support existing hospital <b>information</b> handling <b>areas</b> and can expand to accommodate additional application areas. The {{elements of the}} proposed architecture consist of heterogeneous and autonomous minicomputer host processors that {{serve the needs of}} distinct application areas, intelligent terminals that access {{one or more of these}} processors, a bus communication system featuring a single <b>common</b> <b>information</b> flow channel, and microprocessor based interface units between each network subscriber (processor or terminal) and the communication bus...|$|R
40|$|Abstract — We {{consider}} jointly distributed random vari-ables X and Y. After {{describing the}} Gács-Körner <b>common</b> <b>information</b> between the random variables from the view-point of the capacity {{region of the}} Gray-Wyner system, we propose a new notion of <b>common</b> <b>information</b> between the random variables that is dual to the Gács-Körner <b>common</b> <b>information</b> from this viewpoint in a well-defined sense. We characterize this quantity explicitly in terms of two auxiliary quantities that are asymmetric in nature, and illustrate the operational significance of these new quantities by characterizing a corner point of the {{solution to a problem}} of source coding with side-information in terms of them. We also contrast this new concept of <b>common</b> <b>information</b> for a pair of random variables with the Wyner <b>common</b> <b>information</b> of the random variables, which is also a kind of dual to the Gács-Körner <b>common</b> <b>information.</b> I...|$|R
40|$|In literature, {{different}} <b>common</b> <b>informations</b> {{were defined}} by Gács and Körner, by Wyner, and by Kumar, Li, and Gamal, respectively. In this paper, we define two generalized versions of <b>common</b> <b>informations,</b> named approximate and exact information-correlation functions, by exploiting the conditional maximal correlation as a commonness or privacy measure. These two generalized <b>common</b> <b>informations</b> encompass {{the notions of}} Gács-Körner's, Wyner's, and Kumar-Li-Gamal's <b>common</b> <b>informations</b> as special cases. Furthermore, to give operational characterizations of these two generalized <b>common</b> <b>informations,</b> we also study the problems of private sources synthesis and <b>common</b> <b>information</b> extraction, and show that the information-correlation functions are equal to the minimum rates of commonness needed to ensure that some conditional maximal correlation constraints are satisfied for the centralized setting versions of these problems. As a byproduct, the conditional maximal correlation has been studied as well. Comment: Some typos are correcte...|$|R
40|$|ABSTRACT: This paper generalizes Wyner’s {{definition}} of <b>common</b> <b>information</b> {{of a pair}} or random variables to that of N random variables. We prove coding theorems that show the same operational meanings for the <b>common</b> <b>information</b> of two random variables generalize to that of N random variables. As a byproduct of our proof, we show that the Gray-Wyner source coding network can be generalized to N source sequences with N decoders. We also establish a monotone property of Wyner’s <b>common</b> <b>information</b> which {{is in contrast to}} other notions of the <b>common</b> <b>information,</b> specifically Shannon’s mutual information and Gács and Körner’s common randomness. Examples about the computation of Wyner’s <b>common</b> <b>information</b> of N random variables are also given...|$|R
40|$|Wyner’s <b>common</b> <b>information</b> was {{originally}} defined {{for a pair}} of dependent discrete random variables. This thesis generalizes its definition in two directions: the num-ber of dependent variables can be arbitrary, so are the alphabets of those random variables. New properties are determined for the generalized Wyner’s common in-formation of multiple dependent variables. More importantly, a lossy source coding interpretation of Wyner’s <b>common</b> <b>information</b> is developed using the Gray-Wyner network. It is established that the <b>common</b> <b>information</b> equals to the smallest com-mon message rate when the total rate is arbitrarily close to the rate distortion function with joint decoding if the distortions are within some distortion region. The application of Wyner’s <b>common</b> <b>information</b> to inference problems is also explored in the thesis. A central question is under what conditions does Wyner’s <b>common</b> <b>information</b> capture the entire information about the inference object. Under a simple Bayesian model, it is established that for infinitely exchangeable random variables that the <b>common</b> <b>information</b> is asymptotically equal to the information o...|$|R
40|$|Abstract—An {{important}} subclass of secure multiparty computation {{is secure}} sampling: two parties output sam-ples {{of a pair}} of jointly distributed random variables such that neither party learns more about the other party’s output than what its own output reveals. The parties make use of a setup — correlated random variables with a different distribution — as well as unlimited noiseless communication. An upperbound on the rate of producing samples of a desired distribution from a given setup is presented. The region of tension developed in this paper measures how well the dependence between a pair of random vari-ables can be resolved by a piece of <b>common</b> <b>information.</b> The bounds on rate are a consequence of a monotonicity property: a protocol between two parties can only lower the tension between their “views”. Connections are drawn between the region of tension and the notion of <b>common</b> <b>information.</b> A generalization of the Gács-Körner <b>common</b> <b>information,</b> called the Assisted <b>Common</b> <b>Information,</b> which takes into account “almost <b>common</b> ” <b>information</b> ignored by Gács-Körner <b>common</b> <b>information</b> is defined. The region of tension is shown {{to be related to the}} rate regions of both the Assisted <b>Common</b> <b>Information</b> and the Gray-Wyner systems (and, a fortiori, Wyner’s <b>common</b> <b>information).</b> I...|$|R
40|$|How do I print my transcripts? Where do I get a FAFSFA form? How do I drop a class? I {{need help}} with math, who can tutor me? Can I get a student ID? Librarians get all kinds of {{questions}} that don’t necessarily {{have anything to do}} with traditional “library services,” like database and collection inquiries. This summer, Gateway Community and Technical College is embarking on an <b>Information</b> <b>Commons</b> like none other. It’s tearing down barriers to ensure seamless transactions for students, more engagement with staff and faculty and encouraging teamwork to provide superior customer service. Individual departments including the library, student services, advising, and financial aid will join forces to create one giant, information-producing service model. Staff from these departments will work side-by-side to help meet the varying needs of our students to provide Gateway Personalized Service! [...] Think concierge of the <b>Information</b> <b>Commons.</b> Staff working in the <b>Information</b> <b>Commons</b> will be cross-trained to meet the most basic and common needs of students across departmental divides. Specialists will be on hand for those needing extra attention. A truly unique <b>Information</b> <b>Commons</b> such as this is complemented by design elements encouraging spontaneous, active learning opportunities. Thanks in large part to the U. S. Department of Education First in the World Grant and partnership with Steelcase, Inc., Gateway is literally tearing down walls and incorporating furniture and collaborative design principles into an <b>Information</b> <b>Commons</b> <b>area</b> at each of our three campuses. This will undoubtedly eliminate the feeling of getting “the run-around. ” <b>Information</b> <b>Commons</b> will become synonymous with customer service and a one-stop shop. No longer will we ask them to remember each department’s name and function. Convoluted college jargon will be eliminated from our vocabulary when dealing with students, helping make this a key place for student success, retention and collaboration. We will share our concept, plans, and progress of this one-of-a-kind service model. We see challenges as opportunity and look forward to seeing the results of what our exceptional staff and faculty can do once united to serve students...|$|R
40|$|Abstract—The {{two most}} {{prevalent}} notions of <b>common</b> <b>information</b> {{are due to}} Wyner and Gács-Körner and both the notions can be stated as two different characteristic points in the lossless Gray-Wyner region. Although these quantities can be easily evaluated for random variables with infinite entropy (eg. continuous random variables), the operational significance underlying their definition is applicable only to the lossless framework. The primary objective {{of this paper is}} to generalize these two notions of <b>common</b> <b>information</b> to the lossy Gray-Wyner network, which extends the theoretical intuition underlying their definitions for general sources and distortion measures. We begin with the lossy generalization of Wyner’s <b>common</b> <b>information,</b> defined as the minimum rate on the shared branch of the Gray-Wyner network at minimum sum rate when the two decoders reconstruct the sources subject to individual distortion constraints. We derive a complete single letter information theoretic characterization for this quantity and use it to compute the <b>common</b> <b>information</b> of symmetric bivariate Gaussian random variables. We then derive similar results to generalize Gács-Körner’s definition to the lossy framework. These two characterizations allow us to carry the practical insight underlying the two notions of <b>common</b> <b>information</b> to general sources and distortion measures. Index Terms—Wyner’s <b>common</b> <b>information,</b> Gács and Körner’s <b>common</b> <b>information,</b> Lossy Gray-Wyner networ...|$|R
40|$|This paper generalizes Wyner's {{definition}} of <b>common</b> <b>information</b> {{of a pair}} of random variables to that of N random variables. We prove coding theorems that show the same operational meanings for the <b>common</b> <b>information</b> of two random variables generalize to that of N random variables. As a byproduct of our proof, we show that the Gray-Wyner source coding network can be generalized to N source squences with N decoders. We also establish a monotone property of Wyner's <b>common</b> <b>information</b> which is in contrast to other notions of the <b>common</b> <b>information,</b> specifically Shannon's mutual information and Gács and Körner's common randomness. Examples about the computation of Wyner's <b>common</b> <b>information</b> of N random variables are also given. Comment: 8 pages, 4 figures,presented at the Forty-Eighth Annual Allerton Conference on Communication, Control, and Computing, September 29 - October 1, 2010, Monticello, IL, US...|$|R
40|$|This paper {{introduces}} {{the notion of}} exact <b>common</b> <b>information,</b> which is the minimum description length of the common randomness needed for the exact distributed generation of two correlated random variables (X,Y). We introduce the quantity G(X;Y) =_X→ W → Y H(W) as a natural bound on the exact <b>common</b> <b>information</b> and study its properties and computation. We then introduce the exact <b>common</b> <b>information</b> rate, which is the minimum description rate of the common randomness for the exact generation of a 2 -DMS (X,Y). We give a multiletter characterization for it as the limit G̅(X;Y) =_n→∞(1 /n) G(X^n;Y^n). While in general G̅(X;Y) is {{greater than or equal}} to the Wyner <b>common</b> <b>information,</b> we show that they are equal for the Symmetric Binary Erasure Source. We do not know, however, if the exact <b>common</b> <b>information</b> rate has a single letter characterization in general...|$|R
30|$|The key {{argument}} of Morck et al. (2000) {{rests on the}} intensity of risk arbitrage by informed investors in incorporating firm-specific information into stock price. 1 Acquiring firm-specific versus <b>common</b> <b>information</b> has different fixed costs and generates different arbitrage profits. Investors therefore have different incentives to acquire firm-specific versus <b>common</b> <b>information.</b> Veldkamp’s (2006) model shows that investors’ information choice depends {{on their ability to}} bear the fixed cost of acquiring firm-specific versus <b>common</b> <b>information.</b> To ensure a low cost, investors and analysts tend to rely on information useful for evaluating multiple assets. 2 Even though <b>common</b> <b>information</b> is less valuable than firm-specific information, investors still acquire it because its high demand reduces its acquisition cost. The clustered use of <b>common</b> <b>information</b> adds <b>common</b> shocks to related stocks and contributes to excess stock return comovement. Empirical evidence is consistent with the information choice argument as an explanation for stock return comovement (See Brockman et al. (2010) and Hameed et al. (2015) for recent examples).|$|R
40|$|Distributed Compressive Sensing (DCS) {{improves}} the signal recovery performance of multi signal ensembles by exploiting both intra- and inter-signal correlation and sparsity structure. However, the existing DCS was proposed {{for a very}} limited ensemble of signals that has single <b>common</b> <b>information</b> Baron: 2009 vd. In this paper, we propose a generalized DCS (GDCS) which can improve sparse signal detection performance given arbitrary types of <b>common</b> <b>information</b> which are classified into not just full <b>common</b> <b>information</b> but also a variety of partial <b>common</b> <b>information.</b> The theoretical bound on the required number of measurements using the GDCS is obtained. Unfortunately, the GDCS may require much a priori-knowledge on various inter <b>common</b> <b>information</b> of ensemble of signals to enhance the performance over the existing DCS. To deal with this problem, we propose a novel algorithm that can search for the correlation structure among the signals, with which the proposed GDCS improves detection performance even without a priori-knowledge on correlation structure for the case of arbitrarily correlated multi signal ensembles...|$|R
40|$|The {{main purpose}} of this paper is to show that the lack of misreaction to <b>common</b> <b>information</b> in {{previous}} research may be due to methodological weakness. As of now, there is no evidence which suggests that stocks under-react to <b>common</b> <b>information</b> at short horizons and over-react at longer horizons. Even if stocks under- and/or over-react to <b>common</b> <b>information</b> at the security level, the reaction pattern may not be evident at the market level if only some stocks have such a pattern and their capitalization is small. We show in this manuscript that the lack of misreaction to <b>common</b> <b>information</b> in previous research may be due to methodological weakness. By focusing on the stock level reaction, we find a statistically and economically significant reaction pattern to <b>common</b> <b>information</b> as the behavioral models suggest. This finding thus complements the findings of stock misreaction to firm-specific information, and may benefit researchers attempting to understand investor behavior. Market-wide information, intangible information, size factor, behavioral models of asset pricing...|$|R
30|$|Investors choose <b>common</b> <b>{{information}}</b> because complementarities in <b>information</b> demand make <b>common</b> <b>information</b> affordable. For example, they cluster {{their information}} production on bellwether stocks to gauge industry-wide information and {{use this information}} to evaluate other related stocks in the same industry (Veldkamp, 2006).|$|R
40|$|Abstract—We {{presented}} assisted <b>common</b> <b>information</b> as a {{generalization of}} Gács-Körner (GK) <b>common</b> <b>information</b> at ISIT 2010. The motivation for our formulation was to improve upperbounds on {{the efficiency of}} protocols for secure two-party sampling (which {{is a form of}} secure multi-party computation). Our upperbound was based on a monotonicity property of a rate-region (called the assisted residual information region) associated with the assisted <b>common</b> <b>information</b> formulation. In this note we present further results. We explore the connection of assisted <b>common</b> <b>information</b> with the Gray-Wyner system. We show that the assisted residual information region and the Gray-Wyner region are connected by a simple relationship: the assisted residual information region is the increasing hull of the Gray-Wyner region under an affine map. Several known relationships between GK <b>common</b> <b>information</b> and Gray-Wyner system fall out as consequences of this. Quantities which arise in other source coding contexts acquire new interpretations. In previous work we showed that assisted <b>common</b> <b>information</b> can be used to derive upperbounds on the rate at which a pair of parties can securely sample correlated random variables, given correlated random variables from another distribution. Here we present an example where the bound derived using assisted <b>common</b> <b>information</b> is much better than previously known bounds, and in fact is tight. This example considers correlated random variables defined in terms of standard variants of oblivious transfer, and is interesting on its own as it answers a natural question about these cryptographic primitives. I...|$|R
40|$|We study {{sequential}} {{decision making}} problems in cooperative systems where different agents with different information {{want to achieve}} a common objective. The sequential nature of the decision problem implies that all decisions can be arranged in a sequence such that the information available to make the t-th decision only depends on preceding decisions. Markov decision theory provides tools for addressing sequential decision making problems with classical information structures. In this thesis, we introduce a new approach for decision making problems with non-classical information structures. This approach relies {{on the idea of}} <b>common</b> <b>information</b> between decision-makers. Intuitively, <b>common</b> <b>information</b> consists of past observations and decisions that are commonly known to the current and future decision makers. We show that a <b>common</b> <b>information</b> based approach can allow us to discover new structural results of optimal decision strategies and provide a sequential decomposition of the decision-making problems. We first demonstrate this approach on two specific instances of sequential problems, namely, a real-time multi-terminal communication system and a decentralized control system with delayed sharing of information. We then show that the <b>common</b> <b>information</b> methodology applies more generally to any sequential decision making problem. Moreover, we show that our <b>common</b> <b>information</b> methodology unifies the separate sequential decomposition results available for classical and non-classical information structures. We also present sufficient conditions for simplifying <b>common</b> <b>information</b> based sequential decompositions. This simplification relies on the concept of state sufficient for the input output map of a coordinator that only knows the <b>common</b> <b>information...</b>|$|R
40|$|This paper gives a brief {{description}} of the background and emergence of the concept of a <b>common</b> <b>information</b> environment. It offers a set of characteristics that define implementations of the concept, and describes some of the associated issues. A discussion of scaling factors leads to the conclusion that smaller countries are at the best scale and scope to achieve good benefits from public investment in a <b>common</b> <b>information</b> environment. Recent developments associated with the implementation of a <b>common</b> <b>information</b> environment in Scotland are described...|$|R
5000|$|H.450.12, <b>Common</b> <b>Information</b> Additional Network Feature for H.323 ...|$|R
40|$|We {{presented}} assisted <b>common</b> <b>information</b> as a {{generalization of}} Gács-Körner (GK) <b>common</b> <b>information</b> at ISIT 2010. The motivation for our formulation was to improve upperbounds on {{the efficiency of}} protocols for secure two-party sampling (which {{is a form of}} secure multi-party computation). Our upperbound was based on a monotonicity property of a rate-region (called the assisted residual information region) associated with the assisted <b>common</b> <b>information</b> formulation. In this note we present further results. We explore the connection of assisted <b>common</b> <b>information</b> with the Gray-Wyner system. We show that the assisted residual information region and the Gray-Wyner region are connected by a simple relationship: the assisted residual information region is the increasing hull of the Gray-Wyner region under an affine map. Several known relationships between GK <b>common</b> <b>information</b> and Gray-Wyner system fall out as consequences of this. Quantities which arise in other source coding contexts acquire new interpretations. In previous work we showed that assisted <b>common</b> <b>information</b> can be used to derive upperbounds on the rate at which a pair of parties can securely sample correlated random variables, given correlated random variables from another distribution. Here we present an example where the bound derived using assisted <b>common</b> <b>information</b> is much better than previously known bounds, and in fact is tight. This example considers correlated random variables defined in terms of standard variants of oblivious transfer, and is interesting on its own as it answers a natural question about these cryptographic primitives. Comment: 8 pages, 3 figures, 1 appendix; to be presented at the IEEE International Symposium on Information Theory, 201...|$|R
5000|$|IEC 62325-301: <b>Common</b> <b>information</b> model (CIM) {{extensions}} for markets ...|$|R
50|$|The {{citations}} for {{the series}} of sites provide <b>common</b> <b>information.</b>|$|R
5000|$|ITU-T Recommendation H.450.12, <b>Common</b> <b>Information</b> Additional Network Feature for H.323.|$|R
5000|$|... #Subtitle level 3: <b>Common</b> <b>information</b> {{technology}} {{enterprise and}} desktop, 2012 ...|$|R
5000|$|IEC 61968-11 - <b>Common</b> <b>Information</b> Model (CIM) Extensions for Distribution Published ...|$|R
40|$|A {{couple of}} professors at Stanford were being {{quoted in the}} press saying that CDMA {{violated}} the laws of physics. ” El Gamal (Stanford University) Disclaimer Viterbi Lecture 2 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources El Gamal (Stanford University) Outline Viterbi Lecture 3 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources ∙ The lecture is tutorial in nature with many examples El Gamal (Stanford University) Outline Viterbi Lecture 3 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources ∙ The lecture is tutorial in nature with many examples ∙ No proofs, but a potentially interesting framework and new result El Gamal (Stanford University) Outline Viterbi Lecture 3 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources ∙ The lecture is tutorial in nature with many examples ∙ No proofs, but a potentially interesting framework and new result ∙ Outline: 㶳 Brief introduction to information theory 㶳 Information sources and measuring information El Gamal (Stanford University) Outline Viterbi Lecture 3 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources ∙ The lecture is tutorial in nature with many examples ∙ No proofs, but a potentially interesting framework and new result ∙ Outline: 㶳 Brief introduction to information theory 㶳 Information sources and measuring information 㶳 Brief introduction to network information theory 㶳 Correlated information sources and measuring <b>common</b> <b>information</b> El Gamal (Stanford University) Outline Viterbi Lecture 3 / 36 About this lecture ∙ <b>Common</b> <b>information</b> between correlated information sources ∙ The lecture is tutorial in nature with many examples ∙ No proofs, but a potentially interesting framework and new result ∙ Outline: 㶳 Brief introduction to information theory 㶳 Information sources and measuring information 㶳 Brief introduction to network information theory 㶳 Correlated information sources and measuring <b>common</b> <b>information</b> ∙ Some of the basic models, ideas, and results of information theor...|$|R
40|$|Wyner's <b>common</b> <b>information</b> was {{originally}} defined {{for a pair}} of dependent discrete random variables. Its significance is largely reflected in, hence also confined to, several existing interpretations in various source coding problems. This paper attempts to both generalize its definition and to expand its practical significance by providing a new operational interpretation. The generalization is two-folded: the number of dependent variables can be arbitrary, so are the alphabet of those random variables. New properties are determined for the generalized Wyner's <b>common</b> <b>information</b> of N dependent variables. More importantly, a lossy source coding interpretation of Wyner's <b>common</b> <b>information</b> is developed using the Gray-Wyner network. In particular, it is established that the <b>common</b> <b>information</b> equals to the smallest common message rate when the total rate is arbitrarily close to the rate distortion function with joint decoding. A surprising observation is that such equality holds independent of the values of distortion constraints as long as the distortions are within some distortion region. Examples about the computation of <b>common</b> <b>information</b> are given, including that of a pair of dependent Gaussian random variables. Comment: 31 pages, 5 figures. Submitted to IEEE Transactions on Information Theor...|$|R
5000|$|IEC 61968-12 - <b>Common</b> <b>Information</b> Model (CIM) Use Cases for 61968 Retired ...|$|R
5000|$|IEC 61968-13 - <b>Common</b> <b>Information</b> Model (CIM) RDF Model {{exchange}} {{format for}} distribution Published ...|$|R
40|$|This paper {{discusses}} the basic functions of a <b>common</b> <b>information</b> environment {{and how they}} are supported by metadata. Several distinct categories of information landscapes are described, characterised by the availability and quality of metadata at the item and collection level. The paper suggests elements of professional practice which can improve the functionality of landscapes, and presents an illustrative scenario of how a <b>common</b> <b>information</b> environment might be effective. Keywords <b>common</b> <b>information</b> environment; information landscapes; metadata aggregationFuture information environments: deserts, jungles or parks? This paper is a follow-up to the paper presented last year at AKM 9 { 1 }. That presentation was summed-up with "A <b>common</b> <b>information</b> environment offers easy, convenient access to the widest range of information resources catering for the widest range of needs of the widest range of users- It requires the collaboration of archives, libraries, museums and governments " { 2 }. This paper focuses on the general functionality of resource discovery and the metadat...|$|R
50|$|In 2015 the House of <b>Commons</b> <b>Information</b> Office {{became the}} House of Commons Enquiry Service.|$|R
50|$|The seven most <b>common</b> <b>information</b> types were concept, procedure, process, principle, fact, structure, and classification.|$|R
