1|58|Public
40|$|In data aggregation, sensor {{measurements}} {{from the}} whole sensory field or a sub-field are collected as a single report at an actor by using aggregate functions such as sum, average, maximum, minimum, <b>count,</b> <b>deviation,</b> and so on. We propose a localized delay-bounded and energy-efficient data aggregation (DEDA) protocol for request-driven wireless sensor networks with IEEE 802. 11 {{carrier sense multiple access}} with collision avoidance run at media access control layer. This protocol uses a novel two-stage delay model, which measures end-to-end delay by using either hop count or degree sum along a routing path depending on traffic intensity. It models the network as a unit disk graph (UDG) and constructs a localized minimal spanning tree (LMST) sub-graph. Using only edges from LMST, it builds a shortest-path (thus energyefficient) tree rooted at the actor for data aggregation. The tree is used without modification if it generates acceptable delay, compared with a given delay bound. Otherwise, it is adjusted by replacing LMST sub-paths with UDG edges. The adjustment is done locally on the fly, according to the desired progress value computed at each node. We further propose to integrate DEDA with a localized sensor activity scheduling algorithm and a localized connected dominating set algorithm, yielding two DEDA variants, to improve its energy efficiency and delay reliability. Through an extensive set of simulation, we evaluate the performance of DEDA with various network parameters. Our simulation results indicate that DEDA far outperforms the only existing competing protocol. Copyright © 2011 John Wiley & Sons, Ltd...|$|E
40|$|This study {{examines}} {{the effectiveness of}} physical exercise, during a prepathology state, on locomotor balance compensation after subsequent unilateral labyrinthectomy in squirrel monkeys. An experimental group underwent 3 hr. of daily running exercise on a treadmill for 3 mo. prior to the surgery, whereas a control group was not exercised. Postoperatively, the locomotor balance function of both groups was tested for 3 mo. There {{was no significant difference}} in gait <b>deviation</b> <b>counts</b> in the acute phase of compensation. However, in the chronic compensation maintenance phase, the number of gait <b>deviation</b> <b>counts</b> was fewer in the exercise group, which showed significantly better performance stability...|$|R
40|$|Dust-filled Tolman-Bondi (1934, 1947) spherically {{symmetric}} cosmological {{models are}} studied with application to superclustering of galaxies. Asymptotic expressions are {{given for the}} density and curvature contrast, valid near the singularity and at long times. The density profile of infalling material is {{expressed in terms of}} a single arbitrary function (the distribution of total energy per unit mass). Measurement of the distribution of excess galaxy <b>counts</b> and <b>deviations</b> from the Hubble flow in the vicinity of a supercluster could provide a determination of the cosmological deceleration parameter...|$|R
50|$|A {{pivot table}} usually {{consists}} of row, column and data (or fact) fields. In this case, the column is Ship Date, the row is Region and the datum {{we would like}} to see is (sum of) Units. These fields allow several kinds of aggregations, including: sum, average, standard <b>deviation,</b> <b>count,</b> etc. In this case, the total number of units shipped is displayed here using a sum aggregation.|$|R
40|$|A {{comparative}} analysis of TM and MSS data was completed {{and the results}} indicate that there are half as many separable spectral classes in the MSS data than in TM. In addition, the minimum separability between classes was also much less in MSS data. Radiometric data quality was also investigated for the TM by computing power spectrum estimates for dark-level data from Lake Michigan. Two significant coherent noise frequencies were observed, one with a wavelength of 3. 12 pixels and the other with a 17 pixel wavelength. The amplitude was small (nominally. 6 digital <b>count</b> standard <b>deviation)</b> and the noise appears primarily in Bands 3 and 4. No significant levels were observed in other bands. Scan angle dependent brightness effects were also evaluated...|$|R
40|$|In general, UiTM {{students}} are often {{said to be}} weak in the English Language. In this study, the students home and their former school experience, their motivations and attitudes towards their English Language proficiency were investigated Pre-diploma students answered the questionnaires and the data were analysed by looking into the frequency <b>count,</b> mean, standard <b>deviation</b> and correlations between the variables. The results ofthe analysis displayed that homes and schools ofthe respondents were the contributing factors to their English Language competency. The implications of these results are discusse...|$|R
30|$|For the analysis, {{circular}} {{regions of}} interest (ROIs) were {{drawn on the}} six spheres in the reconstructed images as well as 60 background regions {{of the same size}} as the spheres in different image slices. As described in the NEMA NU 2 - 2012 protocol, for each sized sphere, the contrast recovery coefficient (CRC) was computed as the ratio of the measured average sphere-to-background ROI count ratio and the actual sphere-to-background activity concentration ratio, and the background variability was calculated as the ratio of the ROI <b>count</b> standard <b>deviation</b> (SD) of the same sized background ROIs and the average background ROI counts. In addition, a ROI of 3  cm in diameter was drawn (in each slice of the phantom) in the central cylindrical insert. The residual error was calculated as the ratio of the average counts in the lung insert ROI to 60 background ROIs.|$|R
50|$|The ACD {{system is}} based on GPS based {{positioning}} and track detection. This has inherent problems as with GPS service and course acquisition, the best possible horizontal accuracy is 10 m. This is inadequate for detection of rail tracks separated by a distance of 10-15 feet. Precision positioning is only available in the US for military use. ACD does not even have DGPS, differential GPS that gives an accuracy close to 2.5 m, and hence had errors in track detection using their patented <b>Deviation</b> <b>Count</b> Theory that worked in block sections but failed in station sections. The result was erratic braking that disrupted train movements and proved to be ineffective.|$|R
40|$|Four field {{experiments}} were conducted at two locations in Nebraska in 1982 {{to investigate the}} genetic factors and seed quality traits involved in seedling cold tolerance of grain sorghum. The planting dates at Sidney and Mead were {{more than three weeks}} earlier than normal, and the material was subjected to cooler than normal conditions. Traits observed were stand counts, a visual rating for vigor, and average dry weight of plants harvested at approximately the ten leaf stage. ^ Twenty-eight parents and F(, 1) hybrids were used to quantify heterosis and compare sterility-inducing and fertility-inducing cytoplasms; the experimental design was a randomized complete block with subplots. Heterosis was strongly expressed for all traits. Cytoplasmic and nuclear interactions accounted for different cold responses in only a few genotypes, but in one inbred the sterility-inducing form consistently out-performed the fertility-inducing form. ^ The visual rating variable correlated well with both average dry weight and the plant <b>counts.</b> <b>Deviation</b> in results from the other variables indicated that stand counts made before 30 days after planting were not representative. Percent viable seedlings and average dry weight from an optimum temperature vigor test in a growth chamber were significantly correlated with the field results at Mead but not at Sidney. ^ In a second experiment, the material differed with respect to the location of production in the previous season and in whether or not the panicle had been covered with a pollination bag or left uncovered during the grain filling period of seed production. The hybrid Martin X DA 3494 was produced at Glenvil and Mead, and no differences existed due to location. The hybrid CK 60 X DA 3494 was produced at Glenvil, Mead, and North Platte, and seed from Glenvil was superior to the other two. For the eight hybrids tested, the comparison between panicles covered or uncovered was nonsignificant. There was little to suggest that {{the presence or absence of}} a pollination bag during the grain filling period contributed to cold tolerance of the seed in the following season. ^ In a third experiment, the treatments were the quadrants of the panicle during the seed filling period. Upon harvest the previous year, the seed was divided into four positions using the upper and lower half and the north and south aspect.... (Author 2 ̆ 7 s abstract exceeds stipulated maximum length. Discontinued here with permission of author.) UMI...|$|R
40|$|The {{concept of}} {{negative}} externalities is firmly entrenched in economic analysis {{even though it}} is almost impossible to apply with any rigor in many important real-world contexts. For instance, what is the baseline from which “pollution” is measured? How clean must the air and water surrounding the firm be? And whose costs must the firm take into account in order to internalize the externalities? Clearly, the firm’s next door neighbors harmed by the polluted air generated by the firm. But what about people who are more remotely affected? There is no neutral way to set the baseline below which <b>deviations</b> <b>count</b> as costs (and above which positive <b>deviations</b> <b>count</b> as benefits), nor is there a neutral way of determining whose costs count. Indeed, the baseline that separates negative and positive externalities and, more broadly, taking only one’s own versus others’ interests into account, is not only indeterminate, it is also dynamic, affected by actions and reactions. The rhetoric of negative externalities has the pernicious effect of reinforcing a view to the contrary: that a firm should, and should be required by law to, avoid imposing what can uncontroversially be characterized as negative externalities, but that it need not otherwise take others’ interests into account. An uncritical acceptance of the concept of negative externalities has led people on seemingly opposing sides of a hot debate in corporate law to miss large areas of existing convergence and fail to capitalize on possibilities for more convergence. The difference between pure profit-maximizing firms and socially responsible firms may be far more evident in theory than in practice...|$|R
40|$|In data {{aggregation}} problems, sensor measurements {{from the whole}} sensory field are collected at the data sink periodically or on-demand as a single report using functions such as average, maximum, minimum, <b>counts,</b> <b>deviation,</b> etc. This thesis is to design a {{data aggregation}} framework applicable for real-time sensor-actor networks. Our goal {{is to set up}} a reporting tree that will minimize power consumption at individual nodes while preserving delay requirements. Existing solutions to data aggregation problem usually use hop count as the energy cost metric and/or operate in a centralized fashion. The only known delay bounded power efficient localized algorithm [MGPA] sets up an initial power efficient tree (regardless of delay), and then dynamically changes the tree based on measured delay in ongoing traffic, with speed-ups and slow-downs achieved by using maximal/minimal transmission ranges at some nodes. We show here that the initial tree is closer to hop count than power optimal while the energy consumption per node is apparently unbalanced. We propose to construct a power optimal delay bounded data aggregation tree, assuming delay is proportional to hop count, which is a reasonable approximation in low traffic scenarios. Our desired hop progress (DHP) scheme constructs a data aggregation tree rooted at a sink/actor using only edges of localized minimal spanning tree (LMST) over all sensors, if delay along this tree is acceptable and power consumption is to be near optimal, like in [TKS]. Otherwise, hop selection (along LMST) made at each step is subject to the ratio of potential delay to message lifetime. The main idea is to reduce the network's overall energy consumption and balance energy consumptions at nodes by applying approximately equal hop lengths along the tree. There are two variants of DHP: DHP with Area coverage algorithm (DHPA), and DHP with Area coverage algorithm and CDS construction algorithm (DHPAC). In DHPA algorithm, area coverage algorithm is applied first to select a subset of active sensors that monitors the same area as the original set, thus allowing the rest of sensors to sleep. Then DHP is applied on the set of active sensors. In DHPAC protocol, a connected dominating set (CDS) is constructed over active sensors. Active sensors not in CDS report to their nearest CDS neighbors (related delay counted at the parent node), and DHP is then applied over LMST of CDS nodes and links in CDS. Experimental results indicate that DHP can totally save up to 75 % energy and extend up to 123 % network lifetime in comparison with [MPGA]. Meanwhile, DHPA and DHPAC also present significantly better energy efficiency than the variant of [MPGA], where area coverage algorithm is also implemented...|$|R
40|$|A radiochemical {{method has}} been {{developed}} for the estimation of atmospheric mercury. When air containing mercury is passed through a solution of 203 Hg-mercuric acetate and KCL, isotope exchange takes place so that the issuing air contains the same concentration of mercury, but labelled and with the same specific activity as the reagent solution. The 203 Hg is absorbed on hopcalite and estimated by gamma scintillation <b>counting.</b> The standard <b>deviation</b> of the method is 0 · 004 μg. Hg/litre in concentrations up to 0 · 2 μg. Hg/litre, and is 0 · 075 μg. Hg/litre in the range 0 · 2 - 1 · 2 μg. Hg/litre concentration. The method is simple {{and can be used}} for snap or long-run sampling, and with continuous recording...|$|R
40|$|A {{summary table}} with {{statistical}} comparisons {{is one of}} the most common tables included in clinical study reports. The table usually contains not only summary statistics such as, <b>count,</b> mean, standard <b>deviations,</b> medians, ranges for relevant continuous variables of interest, and number and percentage of patients falling within a particular category for categorical variables of interest, but also some statistical information such as p-values from Chi-square test, Fisher’s exact test, t-test, ANOVA test, etc. We have developed a user-friendly macro program to calculate summary statistics and test results for both continuous and categorical variables of interest, and output a highly customized, presentable table into Microsoft Word. The SAS Macro program is flexible, allowing you to choose the descriptive statistics and the statistical methods for comparisons among groups...|$|R
40|$|We use Hubble Space Telescope archival {{images to}} measure central surface {{brightness}} profiles of globular clusters around satellite galaxies of the Milky Way. We report results for 21 clusters around the LMC, 5 around the SMC, and 4 around the Fornax dwarf galaxy. The profiles are obtained using a recently developed technique based on measuring integrated light, which is tested on an extensive simulated dataset. Our {{results show that}} for 70 % of the sample, the central photometric points of our profiles are brighter than previous measurements using star <b>counts</b> with <b>deviations</b> as large as 2 mag/acrsec 2. About 40 % of the objects have central profiles deviating from a flat central core, with central logarithmic slopes continuously distributed between- 0. 2 and- 1. 2. These results are compared with those found for a sample of Galactic clusters using the same method. We confirm the known correlation in which younger clusters tend to have smaller core radii, and we find that they also have brighter central surface brightness values. This {{seems to indicate that}} globular clusters might be born relatively concentrated, and that a profile with extended flat cores might not be the ideal choice for initial profiles in theoretical models. Subject headings: globular clusters: general, stellar dynamics 1...|$|R
40|$|Minna and Enugu-Nigeria respectively. Abstract. Following {{constant}} complains by {{our schools}} on short {{falls in the}} supply of laboratory apparatus and even when available they are sub – standard, {{this study was conducted}} to determine the re-training needs of mechanical engineering technologists who are directly involved in the production of these laboratory apparatus for improved performance in Scientific Equipment Development Institutes (SEDI) in Nigeria. Two research questions and 2 hypotheses were formulated to guide the study. The study was conducted in the two Scientific Development Institutes located at Minna and Enugu. A survey research design approach was adopted. The entire population of 82 mechanical engineers and 140 mechanical engineering technologists served as the respondents. No sampling was done. A 50 item structured questionnaire was used to collect the relevant data for the study. Data collected were analyzed using frequency <b>counts,</b> standard <b>deviation,</b> mean and t – test statistics. Results from analysis of data showed that all the 50 proposed items were accepted as retraining needs of mechanical engineering technologists. Specifically, the study revealed that the technologists were most deficient in areas of the use of automatic, NC, and CNC machines. It was recommended that as a matter of urgency government should put i...|$|R
40|$|ABSTRACT DETERMINATION OF SODIUM BENZOIC IN BEVERAGES SAMPLE IN THE MARKET OF PADANG CITY WITH HIGH PERFORMANCE LIQUID CHROMATOGRAPHY (HPLC) By : Navraty Putri Handayani (0910413114) Yulizar Yusuf, MS* Bustanul Arifin, M. Si** Advised I* Advised ** The {{research}} about determination of sodium benzoic in beverages sample. The {{purpose was to}} know the concentration of sodium benzoic as preservative in beverages. The research use High Performance Liquid Chromatography (HPLC) reversed phased method. The optimum measure conditions by using C 18 (250 mm x 4, 6 mm) column, UV detector at 224 nm of wavelength, mobile phase methanol : water 70 : 30 and flow rate 0, 45 mL/min. The count resulted the determination coefisien (R 2) 0, 9998, relative standar <b>deviation</b> <b>counted</b> based on peak area and retention time 0, 18...|$|R
40|$|Abstract Inferred {{temperatures}} from chironomids {{preserved in}} the varved sediment of Lake Silvaplana in the Eastern Swiss Alps were compared with instru-mental data obtained from a meteorological station in Sils-Maria, {{on the shore of}} Lake Silvaplana, for the time interval 1850 – 2001. At near-annual resolution, the general patterns of chironomid-inferred tempera-ture changes followed the meteorological record over the last * 150 years (rPearson = 0. 65, P = 0. 01) and 87 % of the inferences had deviations from the instru-mental data below the root-mean-square error of prediction (RMSEP). When the inferences were compared with a 2 -year running mean in the meteo-rological data, 94 % of the inferences had differences with the instrumental data below the RMSEP, indicat-ing {{that more than half of}} the inaccurate inferences may have been due to errors in varve <b>counting.</b> Larger <b>deviations</b> from the instrumental data were also obtained from samples with low percentages of fossil taxa represented in the training set used for temperature reconstruction and/or assemblages with poor fit to temperature. Changes in total phosphorus (TP, as inferred by diatoms) and/or greater precipitation were possible factors affecting the accuracy of the temper-ature reconstruction. Although these factors might affect the quantitative estimates, obtaining [80 % accurate temperature inferences suggests that chiron-omid analysis is a reliable tool for reconstructing mean July air temperature quantitatively over the last * 150 years in Lake Silvaplana...|$|R
40|$|The analisys in this {{research}} was purposed for firstly, to determine the occurence or non-occurence of σ-convergence in Indonesia. Secondly, to determine the occurence or non-occurence of absolute and conditional convergence in Indonesia. Thirdly, to determine the magnitude of speed of convergence if there was the absolute or conditional convergence in Indonesia. Fourthly, to determine the half life of convergence or {{the time needed to}} cover the half of initial discrepancy. This research used the secondary data which is rate of economic growth year 2002 - 2012, income per capita year 2002 – 2012, gross fixed capital formation year 2002 – 2012, labor market year 2002 – 2012 and total factory productivity data year 2002 – 2012 of 33 provinces in Indonesia. The methode used in {{this research}} is panel data with fixed effect model and dummy region. This research had two different analisyses, first, σ-convergence analisys with <b>counting</b> the <b>deviation</b> standard of log of income per capita. And second, β-convergence analisys conducted through analisis absolute convergence and conditional convergence models. The result of this research showed that the σ-convergence have a declining scheme, this scheme illustrated the possibility of a decrease in inequality. The result of absolute convergence showed that there was no convergence in Indonesia. Meanwhile the result of conditional convergence showed that there was economy growth convergence in Indonesia with speed of convergence 5, 9 percent per year and the half life of convergence in the amount of 12 years...|$|R
40|$|We survey {{a new way}} to get quick {{estimates}} of the values of simple statistics (like <b>count,</b> mean, standard <b>deviation,</b> maximum, median, and mode frequency) on a large data set. This approach is a comprehensive attempt (apparently the first) to estimate statistics without any sampling, by reasoning about various sets containing a population interest. Our antisampling techniques have connections to those of sampling (and have duals in many cases), but they have different advantages and disadvantages, making antisampling sometimes preferable to sampling, sometimes not. In particular, they can only be efficient when data is in a computer, and they exploit computer science ideas such as production systems and database theory. Antisampling also requires the overhead of construction of an auxiliary structure, a database abstract. Tests on sample data show similar or better performance than simple random sampling. We also discuss more complex methods of sampling and their disadvantagesPrepared for: Chief of Naval Research[URL] RR 000 - 01 - 10 N 0001484 WR 41001 N...|$|R
40|$|We {{provide a}} {{consistent}} framework for estimating galaxy counts and variances in wide-field images {{for a range}} of photometric bands. We demonstrate that our statistical theory is consistent with the counts in the deepest multiband surveys available. The statistical estimates depend on several observational parameters (e. g. seeing, signal to noise ratio). The JAVA calculator is freely available and offers the user the option to adopt our consistent framework or a different scheme. We also provide a summary table of <b>counts</b> and standard <b>deviations</b> in the different bands {{for a range of}} different fields of view. Reliable estimation of the background counts has profound consequences in many areas of observational astronomy. We provide one such example from a recent study of the Sculptor galaxy NGC 300 where stellar photometry has been used to demonstrate that the outer disc extends to 10 effective radii, far beyond what was thought possible for a normal low-luminosity spiral. We confirm this finding by a reanalysis of the background counts...|$|R
40|$|Copyright © 2014 Omid Hamidi et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Microarray technology results in high-dimensional and low-sample size data sets. Therefore, fitting sparse models is substantial because {{only a small number}} of influential genes can reliably be identified. A number of variable selection approaches have been proposed for high-dimensional time-to-event data based on Cox proportional hazards where censoring is present. The present study applied three sparse variable selection techniques of Lasso, smoothly clipped absolute deviation and the smooth integration of <b>counting,</b> and absolute <b>deviation</b> for gene expression survival time data using the additive risk model which is adopted when the absolute effects ofmultiple predictors on the hazard function are of interest. The performances of used techniques were evaluated by time dependent ROC curve and bootstrap. 632 + prediction error curves. The selected genes by all methods were highly significant...|$|R
40|$|A {{statistical}} analysis of the carbohydrate–protein interaction in the PDB was carried out to investigate sequential aspects of the amino acids that are preferred for the selective binding of N-acetyl-Dneuraminic acid (D-Neup 5 Ac) chain. 2264 amino acids from 106 protein sequences were analysed {{in the vicinity of}} 277 Neup 5 Ac residues using GlyVicinity. Within a distance of 4 A ° in vicinity of D-Neup 5 Ac, polar amino acids are more frequently observed as compare to other amino acids. There is a high frequency of occurrence of tyrosine, serine, asparagine, arginine residues in the vicinity of D-Neup 5 Ac chain whereas cysteine is the rarest amino acid found in protein–carbohydrate interaction. The positional preference of certain potential amino acids in vicinity of Neup 5 Ac residues has been discussed in terms of absolute <b>counts,</b> percentage, and <b>deviations</b> from natural abundances. This analysis provides a basis for further studies on the molecular mechanism of carbohydrate–protein interaction...|$|R
40|$|The {{study was}} carried out to {{investigate}} the poverty level differential among rural and urban households in Ekiti and Ondo states of Nigeria. A total of 180 households, were randomly selected, from nine Local Government Areas of the two states. A structured interview schedule was used in eliciting information from them. The data collected were analysed using descriptive statistics such as frequency <b>counts</b> Mean, Standard <b>Deviation</b> and inferential statistics (Foster-Greer-Thorbeck, FGT). The Foster-Grear-Thorbecke (FGT) measure showed that 78 % and 57 % of the rural and urban farmers from the two states were poor respectively. Based on the poverty line of N 5668, the depth of poverty is 0. 3889 and 0. 1875 for the urban dwellers. For the rural dweller, the severity of poverty is 0. 2613 and 0. 0856, and this showed {{that there was a}} higher level of poverty among households in the rural areas than the urban areas of the study area...|$|R
40|$|Analyse doubled linear regresi {{represent}} {{technique used}} to develop;build connective equation changer {{is not free}} (Y) with a few free changer that is X 1, X 2 …, Xk. Model its equation : Yi = β 0 + β 1 X 1 i + β 2 X 2 i + … + βkXki + εi. Equation to be formed later have to fulfill assumption- assumption in analysis of regresi doubled linear, which one {{of them do not}} fufilled of multikolinearitas among free changer of examinee with test of Farrar-Glauber, so that will be obtained by best equation. Method used to determine the the equation among others with procedure of PRESS Target of in this writing there is two the following matter (1) To know procedure examination of data multikolinearitas with test of Farrar-Glauber (2) To know usage of procedure of PRESS in searching equation of best doubled linear regresi at data with problem of multikolinearitas. Method test Farrar-Glauber cover test of chi-kuadrat to check multikolinearitas, test-F to determine and multikolinearitas of test-t to know pattern of multikolinearitas. While for the procedure of used PRESS step that is anticipating model for all possibility of regresi, with a purpose to get deviation is forecast of (remains) by <b>counting</b> its <b>deviation</b> square. Smallest value from amount of deviation square and do not contain too much free changer that's representing equation of best regresi. Result of study at this writing show the existence of multikolinearitas which after analysed with procedure of PRESS got by the following equation Ŷ = - 0, 311 + 0, 436 X 1 + 0, 012 X 5...|$|R
40|$|Sedimentation and {{accumulation}} {{rates were}} derived for twelve metalliferous sediment cores from the Nazca Plate in the Southeast Pacific. Within the area studied, the highest sedimentation rates {{occur on the}} East Pacific Rise (EPR) at 200 South (0. 39 - 1. 91 cm/iD 3 yrs.). Sedimentation rates decrease both along the Rise to 300 South (0. 28 - 0. 39 cm/lU 3 yrs.) and within the deeper Bauer, Yupanqui, and Roggeveen Basins to the east (0. 13 - 0. 26 cm/ 103 yrs.). The deter-mination of sedimentation rate was greatly facilitated by a modification of the excess Th- 230 sedimentation rate technique involving the garsna-ray spectrometric measurement of Pb- 214 using a high-resolution, Ge(Li) gamma-radiation detector. One standard <b>deviation</b> <b>counting</b> statistics for the 352 KeV Pb- 214 gamma energy peak were accurate to within plus or minus 3 % or better. The Pb- 214 activity depth profiles actually define the depth distribution of radium in these metalliferous sediments, and the data support the conclusion that no processes, including Ra- 22...|$|R
40|$|INTRODUCTION: Cultural {{congruence}} is {{the idea}} that to the extent a belief or experience is culturally shared it is not to feature in a diagnostic judgement, irrespective of its resemblance to psychiatric pathology. This rests on the argument that since deviation from norms is central to diagnosis, and since what <b>counts</b> as <b>deviation</b> is relative to context, assessing the degree of fit between mental states and cultural norms is crucial. Various problems beset the cultural congruence construct including impoverished definitions of culture as religious, national or ethnic group and of congruence as validation by that group. This article attempts to address these shortcomings to arrive at a cogent construct. RESULTS: The article distinguishes symbolic from phenomenological conceptions of culture, the latter expanded upon through two sources: Husserl’s phenomenological analysis of background intentionality and neuropsychological literature on salience. It is argued that culture is not limited to symbolic presuppositions and shapes subjects’ experiential dispositions. This conception is deployed to re-examine the meaning of (in) congruence. The main argument is that a significant, since foundational, deviation from culture is not from a value or belief but from culturally-instilled experiential dispositions, in what is salient to an individual in a particular context. CONCLUSION: Applying the concept of cultural congruence must not be limited to assessing violations of the symbolic order and must consider alignment with or deviations from culturally-instilled experiential dispositions. By virtue of being foundational to a shared experience of the world, such dispositions are more accurate indicators of potential vulnerability. Notwithstanding problems of access and expertise, clinical practice should aim to accommodate this richer meaning of cultural congruence. This paper emerged from a workshop on cultural congruence presented with Professor Derek Bolton at the 15 th Conference of the International Network of Philosophy and Psychiatry, Dunedin, New Zealand (July 2012). [URL]...|$|R
40|$|Background. Inflammatory cytokines in {{bone marrow}} may impair hematolymphopoiesis in human {{immunodeficiency}} virus (HIV) –infected subjects who do not experience reconstitution of CD 4 + T cells despite suppression of virus replication while receiving {{highly active antiretroviral therapy}} (HAART) (immunological nonresponders). Methods. Bone marrow samples from 12 immunological nonresponders receiving HAART were studied and compared with samples from 11 immunological responders. The mean CD 4 + T cell <b>count</b> (standard <b>deviation)</b> was 17468 cells/mm 3 and plasma HIV RNA levels had been ! 50 copies/mL for at least 1 year for individuals enrolled in the study. The clonogenic capability of bone marrow samples was evaluated using the colony forming cell assay and the long-term culture-initiating cell assay. CD 34 + cells from the colony forming cell assay were pooled for real-time polymerase chain reaction analysis of Fas and Fas ligand. Bone marrow cytokine production (interleukin- 2 and tumor necrosis factor–a) and stromal interleukin- 7 levels were analyzed by enzyme-linked immunosorbent assay in both groups. Flow cytometric analysis of CD 4 + and CD 8 + T cell subsets was performed. Results. A reduced clonogenic capability and a decrease in the level of more primitive progenitor cells were observed in parallel with lower production of interleukin- 2 and increased tumor necrosis factor–a levels. A significant upregulation of Fas and Fas ligand on CD 34 + cells and a higher stromal interleukin- 7 production were observed. Impairment of the naive T cell compartment and persistent T cell activation were observed in peripheral blood. Conclusions. Samples from immunological nonresponders show reduced growth of in vitro colonies and an altered cytokine production in bone marrow. The cytokine pattern observed and the altered Fas and Fas ligand pathway may determine stem cell apoptosis and low CD 4 + cell recovery. These features, which are similar to those observed in HIV-infected subjects before starting therapy, persist despite treatment...|$|R
40|$|The {{purpose of}} the study was to {{determine}} the comparative variables of accounting learning outcomes taught by Syndicate Group learning methods higher than the results of learning taught by using conventional learning methods in students of class X AK in SMK BM Budi Agung Medan Year Learning 2016 / 2017. This research was conducted at SMK BM Budi Agung Medan. The population in the study were all students of class X AK in SMK BM Budi Agung Medan Learning Year 2016 / 2017 which amounted to 71 people. The sampling technique used was purposive sampling consisting of experiment class and control class, each of which amounted to 30 students. Data collection techniques conducted in the study is a test of learning outcomes. The test is a multiple choice of 20 questions. Before the test is given to the actual sample, the test is tested first to see the validity of the test, the reliability of the test, the distinguishing power of the test and the difficulty of the problem. Data analysis techniques used are determining the average <b>count,</b> standard <b>deviation,</b> normality test, homogeneity test, and hypothesis test using t-test. The results of study in the experimental class with the average value of pre-test and post-test are 34, 167 and 74, 167. While the learning outcomes in the control class the average value of pre-test and post-test that is 32. 833 and 70. 333. Hypothesis testing of post-test results obtained t count = 2. 018 and ttabel = 1. 671. By comparing the two values, we conclude thitung > ttable that is 2. 018 > 1. 671. Through the hypothesis testing criteria can be determined if thitung> ttable then the hypothesis accepted. The results of this study can be concluded that the results of accounting learning taught by using the method of learning Syndicate Group is significantly higher than the results of accounting learning taught by conventional learning methods in students of class X AK in SMK BM Budi Agung Medan Year Learning 2016 / 2017...|$|R
40|$|High-dimensional sparse {{modeling}} with censored {{survival data}} {{is of great}} practical importance, as exemplified by modern applications in high-throughput genomic data analysis and credit risk analysis. In this article, we propose a class of regularization methods for simultaneous variable selection and estimation in the additive hazards model, by combining the nonconcave penalized likelihood approach and the pseudoscore method. In a high-dimensional setting where the dimensionality can grow fast, polynomially or nonpolynomially, with the sample size, we establish the weak oracle property and oracle property under mild, interpretable conditions, thus providing strong performance guarantees for the proposed methodology. Moreover, we show that the regularity conditions required by the $L_ 1 $ method are substantially relaxed by a certain class of sparsity-inducing concave penalties. As a result, concave penalties such as the smoothly clipped absolute deviation (SCAD), minimax concave penalty (MCP), and smooth integration of <b>counting</b> and absolute <b>deviation</b> (SICA) can significantly improve on the $L_ 1 $ method and yield sparser models with better prediction performance. We present a coordinate descent algorithm for efficient implementation and rigorously investigate its convergence properties. The practical utility and effectiveness of the proposed methods are demonstrated by simulation studies and a real data example. Comment: 41 pages, 3 figures, to appear in Journal of the American Statistical Association ([URL]...|$|R
40|$|The {{views and}} {{conclusions}} contained {{in this document}} {{are those of the}} author and should not be interpreted as representative of the official policies of DARPA, the Navy, or the U. S. Government. IEEE Transactions on Software Engineering, SE- 11, no. 10 (October 1985), 1081 - 1091. The equations were redrawn in 2008. We survey a new way to get quick estimates of the values of simple statistics (like <b>count,</b> mean, standard <b>deviation,</b> maximum, median, and mode frequency) on a large data set. This approach is a comprehensive attempt (apparently the first) to estimate statistics without any sampling. Our "antisampling" techniques have analogies to those of sampling, and exhibit similar estimation accuracy, but can be done much faster than sampling with large computer databases. Antisampling exploits computer science ideas from database theory and expert systems, building an auxiliary structure called a "database abstract". We make detailed comparisions to several different kinds of sampling. Supported by the Foundation Research Program of the Naval Postgraduate School. Chief of Naval Research and the Knowledge Base Management Systems Project at Stanford University under contract #N 00039 - 82 -G- 0250 from the Defense Advanced Research Projects Agency of the United States Department of Defense...|$|R
40|$|This paper {{develops}} a multi-industry growth {{model in which}} firms require external funds to conduct productivity-enhancing R&D. The cost of research is industry-specific. The tightness of financing constraints depends {{on the level of}} financial development and on industry characteristics. Over time, a financially constrained economy may converge to the growth path of a frictionless economy, so long as an industry with the fastest expanding technological frontier does not permanently fall behind due to low R&D. The model’s industry dynamics map into a differences-in-differences regression, in which industry growth depends on the interaction between financial development and industry level R&D intensity. Economic growth;Economic models;External sector;Industrial sector;Production;Productivity;r & d, r & d intensity, equation, r & d intensive industries, standard errors, survey, r & d spending, correlations, correlation, statistical significance, research spending, research activity, statistics, r & d-intensive industries, r & d investment, prediction, predictions, r & d expenditures, empirical validity, r & d expenditure, outliers, amount of r & d, research expenditures, standard <b>deviations,</b> <b>counting,</b> faces of r & d, research labs, research and development, research lab, industry r & d, total r & d spending, functional form, empirical specification, r & d investments, number of researchers, cross-country variation, absorptive capacity, r & d share, financial statistics, industry research, r & d activity, standard deviation, verifiability...|$|R
40|$|Microarray {{technology}} {{results in}} high-dimensional and low-sample size data sets. Therefore, fitting sparse models is substantial because {{only a small}} number of influential genes can reliably be identified. A number of variable selection approaches have been proposed for high-dimensional time-to-event data based on Cox proportional hazards where censoring is present. The present study applied three sparse variable selection techniques of Lasso, smoothly clipped absolute deviation and the smooth integration of <b>counting,</b> and absolute <b>deviation</b> for gene expression survival time data using the additive risk model which is adopted when the absolute effects of multiple predictors on the hazard function are of interest. The performances of used techniques were evaluated by time dependent ROC curve and bootstrap. 632 + prediction error curves. The selected genes by all methods were highly significant (P< 0. 001). The Lasso showed maximum median of area under ROC curve over time (0. 95) and smoothly clipped absolute deviation showed the lowest prediction error (0. 105). It was observed that the selected genes by all methods improved the prediction of purely clinical model indicating the valuable information containing in the microarray features. So it was concluded that used approaches can satisfactorily predict survival based on selected gene expression measurements...|$|R
40|$|In certain {{exponential}} absorption experiments, notably measurements of cross sections by transmission, {{it is important}} to achieve minimum statistical error in a limited time, or to minimize the counting time required to measure the absorption coefficient with a preassigned accuracy. The conditions required to attain these ends, i. e., the geometry for optimum transmission, and the best apportionment of counting times among the incident and transmitted beams and background, have been investigated {{for a wide range of}} relative backgrounds (0. 001 to 0. 01), and for two geometries: (1) Beam area fixed, absorber thickness alone is varied, and (2) Beam area and absorber thickness are both disposable parameters, while the total amount of absorber intercepting the beam remains fixed. In both cases the incident flux density and the background rate are assumed constant. The optimum transmissions are shown to be, in general, considerably smaller than those commonly used in absorption experiments. Thus, in Case 1, a useful rule is to employ a transmission of about 0. 1 for low backgrounds, 0. 2 for moderate backgrounds, and 0. 3 for high backgrounds. The following have also been determined: (a) minimum statistical error for a given total counting time, (b) statistical error and the best distribution of counting times for nonoptimum geometry, and (c) sensitivity of the accuracy or total <b>counting</b> time to <b>deviations</b> from optimum transmission. Work performed at the Oak Ridge National Laboratory. "Date Declassified: July 13, 1948. "Includes bibliographical references. In certain {{exponential absorption}} experiments, notably measurements of cross sections by transmission, {{it is important to}} achieve minimum statistical error in a limited time, or to minimize the counting time required to measure the absorption coefficient with a preassigned accuracy. The conditions required to attain these ends, i. e., the geometry for optimum transmission, and the best apportionment of counting times among the incident and transmitted beams and background, have been investigated for a wide range of relative backgrounds (0. 001 to 0. 01), and for two geometries: (1) Beam area fixed, absorber thickness alone is varied, and (2) Beam area and absorber thickness are both disposable parameters, while the total amount of absorber intercepting the beam remains fixed. In both cases the incident flux density and the background rate are assumed constant. The optimum transmissions are shown to be, in general, considerably smaller than those commonly used in absorption experiments. Thus, in Case 1, a useful rule is to employ a transmission of about 0. 1 for low backgrounds, 0. 2 for moderate backgrounds, and 0. 3 for high backgrounds. The following have also been determined: (a) minimum statistical error for a given total counting time, (b) statistical error and the best distribution of counting times for nonoptimum geometry, and (c) sensitivity of the accuracy or total <b>counting</b> time to <b>deviations</b> from optimum transmission. Mode of access: Internet...|$|R
40|$|Open Access JournalIn {{an effort}} to {{determine}} factors influencing cocoa farmer’s participation in innovation platform (IP) activities of the Humidtropics programme, data was collected from purposively selected 177 farmers using multistage technique sampling technique and was gathered {{through the use of}} structured interview schedule. Data were collected and analyzed with percentage, frequency <b>counts,</b> mean, standard <b>deviation</b> and factor analysis. The study shows {{the mean age of the}} cocoa farmers in the IP to be 51. 16 ± 12. 64 with about 52 % aged above 50 years, female were only (23. 73 %), with more than 75 th percentile literacy level and only about 31 % of respondents generate annual income from farming above ₦ 50, 000 while about 70 % made below ₦ 40, 000 extra income from other occupation. The mean farm size was 16. 87 ± 16. 04 acre, farming experience 25. 42 ± 10. 48 years and household size was 9. 78 ± 5. 52. The six significant determinants of cocoa farmer’s participation in IP arranged in order of magnitude are psychological factor (λ = 3. 158), experience factor (λ = 2. 164), community related factor (λ = 1. 697) educational factor (λ = 1. 854), economic factor (λ = 1. 438) and internal factor (λ = 1. 113). The summative effect of the identified factors accounted for 76. 17 % variation observed in cocoa farmer’s participation in the IP. Peer Revie...|$|R
40|$|In recent years, {{scientific}} {{research has been}} conducted on the relaxation effects of Snoezelen on infants with severe motor and intellectual disabilities, etc. However, there have not been any reports on the effects of Snoezelen environments created by different combinations of Snoezelen equipment. In view of this, I prepared two Snoezelen environments that combined two distinct visual stimuli with audio stimuli and olfactory stimuli, and by investigating the case of an infant A with severe motor and intellectual disabilities, I measured and examined the infant’s heart rates in a Morning Meeting environment and the two Snoezelen environments. The results showed that the Snoezelen 1 environment with audio stimuli, olfactory stimuli and visual stimuli (mirror ball and solar projector) and the Snoezelen 2 environment with audio stimuli, olfactory stimuli and visual stimuli (fiber glow and bubble tube) have virtually the same relaxation effects. In addition, the mean of heart rates <b>counts</b> and standard <b>deviation</b> values of heart rates counts decreased together as I repeated session by Individual Infantcare after Snoezelen 2 and understood that infant A was relaxed. Based on these findings, it was thought that, in ongoing individual care and treatment using Snoezelen, adequate relaxation effects could be induced even in a small space or with small equipment. This suggested that, in the future, relaxation could be conducted by using a single room at hospital, a class room at school or a corner of a room at home...|$|R
40|$|This study aims to {{to observe}} the {{convergence}} and disparities occurring in 6 provinces in java within 2004 - 2014. In see disparities happened used Index Williamson and an Index Entropy Theil {{and the result of}} index value evidenced by hypothesis Kuznet (a u-shaped inverted) Within view of convergence of occurring, is by seeing sigmaconvergence, by <b>counting</b> standard value <b>deviation</b> of the Gross Domestic Product (GDP) per capita and beta-convergence with saw the value coefficient GDP per capita. Beta-convergence where divided into two, absolute convergence and conditional convergence. The data used in this research using data during the period 2004 - 2014, of them are. GDP growth rate, GDP riil per capita 2000 without oil and gas, foreign capital investment, investment domestic and labor force.. The method used in research this is data panel with the approach Fixed Effect Method (FEM) and dummy variable areas. Data processed using eviews 6. The result of the research indicated hypothesis Kuznet does not happen, which means economic growth in Java still is directly proportional to the gap. A pattern of the coefitiien variation sigma-convergence every year is increasing, this pattern shows that increase disparities in Java. Meanwhile a calculation beta-convergence shows that the absolute convergence and conditional convergence not happened in Java. Of the result of this research can be taken the conclusion that economic growth in Java are divergent, where economic growth still not equally and a ravine the gap is still high...|$|R
