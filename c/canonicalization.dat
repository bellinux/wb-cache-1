88|2|Public
2500|$|Crawlers usually perform {{some type}} of URL {{normalization}} {{in order to avoid}} crawling the same resource more than once. The term URL normalization, also called URL <b>canonicalization,</b> refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of [...] "." [...] and [...] ".." [...] segments, and adding trailing slashes to the non-empty path component.|$|E
50|$|A {{number of}} {{identifiers}} for chemical substances, such as SMILES and InChI use <b>canonicalization</b> steps in their computation, {{which is essentially}} the <b>canonicalization</b> of the graph which represents the molecule.|$|E
50|$|Another {{complication}} arises {{because of}} the way that the default <b>canonicalization</b> algorithm handles namespace declarations; frequently a signed XML document needs to be embedded in another document; in this case the original <b>canonicalization</b> algorithm will not yield the same result as if the document is treated alone. For this reason, the so-called Exclusive <b>Canonicalization,</b> which serializes XML namespace declarations independently of the surrounding XML, was created.|$|E
40|$|We {{propose a}} new {{efficient}} algorithm for detecting if a cycle in a timed automaton can be iterated infinitely often. Existing methods for this problem have a complexity which is exponential {{in the number}} of clocks. Our method is polynomial: it essentially does a logarithmic number of zone <b>canonicalizations.</b> This method can be incorporated in algorithms for verifying Büchi properties on timed automata. We report on some experiments that show a significant reduction in search space when our iteratability test is used...|$|R
50|$|<b>Canonicalization</b> problem.|$|E
50|$|To {{avoid these}} {{problems}} and guarantee that logically-identical XML documents give identical digital signatures, an XML <b>canonicalization</b> transform (frequently abbreviated C14n) is employed when signing XML documents (for signing the , a <b>canonicalization</b> is mandatory). These algorithms guarantee that semantically-identical documents produce exactly identical serialized representations.|$|E
5000|$|DNs {{are complex}} and little {{understood}} (lack of <b>canonicalization,</b> internationalization problems, [...].) ...|$|E
5000|$|CXTM [...] - Canonical XML Topic Maps format (<b>canonicalization</b> of topic maps) ...|$|E
50|$|There are criticisms {{directed}} at the architecture of XML security in general, and at the suitability of XML <b>canonicalization</b> in particular as a front end to signing and encrypting XML data due to its complexity, inherent processing requirement, and poor performance characteristics. The argument is that performing XML <b>canonicalization</b> causes excessive latency that is simply too much to overcome for transactional, performance sensitive SOA applications.|$|E
5000|$|Directory {{traversal}} is {{also known}} as the [...]./ (dot dot slash) attack, directory climbing, and backtracking. Some forms of this attack are also <b>canonicalization</b> attacks.|$|E
5000|$|Formally, a <b>canonicalization</b> {{with respect}} to an {{equivalence}} relation R on a set S is a mapping c:S→S such that for all s, s1, s2 ∈ S: ...|$|E
50|$|In {{computer}} science, {{data that}} {{has more than one}} possible representation can often be canonicalized into a completely unique representation called its canonical form. Putting something into canonical form is <b>canonicalization.</b>|$|E
5000|$|The {{merging of}} several XML schemata like SOAP, SAML, XML ENC, XML SIG might cause {{dependencies}} on {{different versions of}} library functions like <b>canonicalization</b> and parsing, which are difficult to manage in an application server.|$|E
5000|$|Signature Validation: The [...] {{element is}} {{serialized}} using the <b>canonicalization</b> method specified in , the key data is retrieved using [...] or by other means, and the signature is verified {{using the method}} specified in [...]|$|E
5000|$|Always {{look for}} the most compact and direct URL available, and ensure that it’s clean, with no {{unnecessary}} information after {{the core of the}} URL. This process {{is often referred to as}} URL normalization or URL <b>canonicalization.</b>|$|E
50|$|The InChI {{algorithm}} converts input structural {{information into}} a unique InChI identifier in a three-step process: normalization (to remove redundant information), <b>canonicalization</b> (to generate {{a unique number}} label for each atom), and serialization (to give a string of characters).|$|E
50|$|Variable-length {{encodings}} in the Unicode standard, {{in particular}} UTF-8, may cause an additional need for <b>canonicalization</b> in some situations. Namely, by the standard, in UTF-8 {{there is only}} one valid byte sequence for any Unicode character, but some byte sequences are invalid, i. e. cannot be obtained by encoding any string of Unicode characters into UTF-8. Some sloppy decoder implementations may accept invalid byte sequences as input and produce a valid Unicode character as output for such a sequence. If one uses such a decoder, some Unicode characters have effectively more than one corresponding byte sequence: the valid one and some invalid ones. This could lead to security issues similar to the one described in the previous section. Therefore, if one wants to apply some filter (e. g. a regular expression written in UTF-8) to UTF-8 strings that will later be passed to a decoder that allows invalid byte sequences, one should canonicalize the strings before passing them to the filter. In this context, <b>canonicalization</b> is the process of translating every string character to its single valid byte sequence. An alternative to <b>canonicalization</b> is to reject any strings containing invalid byte sequences.|$|E
50|$|The first example {{contains}} extra {{spaces in}} the closing tag of the first node. The second example, which has been canonicalized, has had these spaces removed. Note that only the spaces within the tags are removed under W3C <b>canonicalization,</b> not those between tags.|$|E
50|$|In web {{search and}} search engine {{optimization}} (SEO), URL <b>canonicalization</b> deals with web content {{that has more}} than one possible URL. Having multiple URLs for the same web content can cause problems for search engines - specifically in determining which URL should be shown in search results.|$|E
50|$|A Canonical XML {{document}} {{is by definition}} an XML document that is in XML Canonical form, defined by The Canonical XML specification. Briefly, <b>canonicalization</b> removes whitespace within tags, uses particular character encodings, sorts namespace references and eliminates redundant ones, removes XML and DOCTYPE declarations, and transforms relative URIs into absolute URIs.|$|E
5000|$|Malicious {{users are}} likely to invent new kinds of {{representations}} of incorrect data. For example, if a program checks if the requested file is not [...] "/etc/passwd", a cracker might pass another variant of this file name, like [...] "/etc/./passwd". <b>Canonicalization</b> libraries can be employed to avoid bugs due to non-canonical input.|$|E
50|$|However, the {{original}} UTF-8 was not canonical, and several strings were now string encodings translatable {{into the same}} string. Microsoft performed the anti-traversal checks without UTF-8 <b>canonicalization,</b> and therefore not noticing that (HEX) C0AF and (HEX) 2F were the same character when doing string comparisons. Malformed percent encodings, such as %c0%9v was also utilized.|$|E
50|$|This {{procedure}} establishes {{whether the}} resources were really {{signed by the}} alleged party. However, because of the extensibility of the <b>canonicalization</b> and transform methods, the verifying party must also make sure that what was actually signed or digested is really what was present in the original data, in other words, that the algorithms used there can be trusted not to change {{the meaning of the}} signed data.|$|E
5000|$|In {{computer}} science, <b>canonicalization</b> (sometimes standardization or normalization) is {{a process}} for converting data that {{has more than one}} possible representation into a [...] "standard", [...] "normal", or canonical form. This can be done to compare different representations for equivalence, to count the number of distinct data structures, to improve the efficiency of various algorithms by eliminating repeated calculations, or to make it possible to impose a meaningful sorting order.|$|E
5000|$|XML Signature is more {{flexible}} than {{other forms of}} digital signatures such as Pretty Good Privacy and Cryptographic Message Syntax, {{because it does not}} operate on binary data, but on the XML Infoset, allowing to work on subsets of the data, having various ways to bind the signature and signed information, and perform transformations. Another core concept is <b>canonicalization,</b> that is to sign only the [...] "essence", eliminating meaningless differences like whitespace and line endings.|$|E
5000|$|Crawlers usually perform {{some type}} of URL {{normalization}} {{in order to avoid}} crawling the same resource more than once. The term URL normalization, also called URL <b>canonicalization,</b> refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of [...] "." [...] and [...] ".." [...] segments, and adding trailing slashes to the non-empty path component.|$|E
5000|$|DKIM {{currently}} features two <b>canonicalization</b> algorithms, {{simple and}} relaxed, {{neither of which}} is MIME-aware. [...] Mail servers can legitimately convert to a different character set, and often document this with X-MIME-Autoconverted header fields. In addition, servers in certain circumstances have to rewrite the MIME structure, thereby altering the preamble, the epilogue, and entity boundaries, any of which breaks DKIM signatures. Only plain text messages written in us-ascii, provided that MIME header fields are not signed, enjoy the robustness that end-to-end integrity requires.|$|E
50|$|In Unicode, many {{accented letters}} can be {{represented}} {{in more than one}} way. For example, é {{can be represented}} in Unicode as the Unicode character U+0065 (LATIN SMALL LETTER E) followed by the character U+0301 (COMBINING ACUTE ACCENT), {{but it can also be}} represented as the precomposed character U+00E9 (LATIN SMALL LETTER E WITH ACUTE). This makes string comparison more complicated, since every possible representation of a string containing such glyphs must be considered. To deal with this, Unicode provides the mechanism of canonical equivalence. In this context, <b>canonicalization</b> is Unicode normalization.|$|E
50|$|Homographs of {{all kinds}} can be {{detected}} through a process called 'dual canonicalization'. The first step in this process is to identify homograph sets h, namely characters appearing the same to a given observer o. From here, a single token is specified to represent the homograph set. This token is called a canon c. The {{next step is to}} covert each character in the text to the corresponding canon in a process called <b>canonicalization.</b> If the canons of two runs of text are the same but the original text is different, then a homograph exists in the text.|$|E
50|$|The {{canonical}} {{form of a}} graph {{is an example of}} a complete graph invariant: every two isomorphic graphs have the same {{canonical form}}, and every two non-isomorphic graphs have different canonical forms. Conversely, every complete invariant of graphs may be used to construct a canonical form. The vertex set of an n-vertex graph may be identified with the integers from 1 to n, and using such an identification a canonical form of a graph may also be described as a permutation of its vertices. Canonical forms of a graph are also called canonical labelings, and graph canonization is also sometimes known as graph <b>canonicalization.</b>|$|E
5000|$|<b>Canonicalization</b> of filenames is {{important}} for computer security. For example, a web server may have a restriction that only files under the cgi directory [...] may be executed. This rule is enforced by checking that the path starts with [...] and only then executing it. While the file [...] initially {{appears to be in}} the cgi directory, it exploits the [...] path specifier to traverse back up the directory hierarchy in an attempt to execute a file outside of [...] Permitting [...] to execute would be an error caused by a failure to canonicalize the filename to the simplest representation, , and is called a directory traversal vulnerability. With the path canonicalized, it is clear the file should not be executed.|$|E
5000|$|Typically, {{a number}} of equally valid SMILES strings can be written for a molecule. For example, , [...] and [...] all specify the {{structure}} of ethanol. Algorithms {{have been developed to}} generate the same SMILES string for a given molecule; of the many possible strings, these algorithms choose only one of them. This SMILES is unique for each structure, although dependent on the <b>canonicalization</b> algorithm used to generate it, and is termed the canonical SMILES. These algorithms first convert the SMILES to an internal representation of the molecular structure; an algorithm then examines that structure and produces a unique SMILES string. Various algorithms for generating canonical SMILES have been developed and include those by Daylight Chemical Information Systems, OpenEye Scientific Software, MEDIT, Chemical Computing Group, MolSoft LLC, and the Chemistry Development Kit. A common application of canonical SMILES is indexing and ensuring uniqueness of molecules in a database.|$|E
40|$|Tensor {{expression}} simplification is an "ancient" {{topic in}} computer algebra, {{a representative of}} which is the <b>canonicalization</b> of Riemann tensor polynomials. Practically fast algorithms exist for monoterm <b>canonicalization,</b> but not for multiterm <b>canonicalization.</b> Targeting the multiterm difficulty, in this paper we establish the extension theory of graph algebra, and propose a <b>canonicalization</b> algorithm for Riemann tensor polynomials based on this theory...|$|E
40|$|XML <b>Canonicalization</b> enables {{reliable}} textual and binary {{comparison of}} XML documents through {{the removal of}} irrelevant differences in structure and content. Though XML <b>Canonicalization</b> is critical for XML Signatures, it also has value in other XML applications such as version control. Currently, the approach to XML <b>Canonicalization</b> is to write a single specification that details how all parts of XML instances are to be canonicalized. This position paper suggests an alternative approach in which <b>canonicalization</b> is specified separately for the different layers in XML data stack (core, schema, and namespace). In this way, an application can select different <b>canonicalization</b> methods for the different layers; this allows the customization of <b>canonicalization</b> to the application's needs while simplifying the task of writing <b>canonicalization</b> specifications because the specifications need not be all encompassing. As well, {{it may also be}} feasible to canonicalize data belonging to different namespaces differently; for example, numbers in an accounting spreadsheet could be canonicalized differently than numbers appearing in descriptive text. Description Three layers of <b>canonicalization</b> are proposed: core, schema-aware, and namespace-aware. They are described in the following table...|$|E
30|$|The <b>canonicalization</b> {{function}} {{allows us}} to set an upper bound {{on the number of}} elements that can simultaneously be in a set. For example in electronic voting, <b>canonicalization</b> would remove malformed ballots and combine multiple different (encrypted) ballots submitted by the same voter into a single “invalid” ballot for that voter.|$|E
40|$|Information {{extraction}} and crawling {{from the}} Web have been increasingly common, yet raw data are often noisy and redundant due to heterogeneous sources. Although much work {{has focused on}} duplicate records detection, there is little investigation in providing a uniform, standard result from the duplicates to users, which we refer to as a canonical result, and the process is referred to record <b>canonicalization.</b> In this paper, {{we focus on the}} situation of imperfect and duplicate documents on the Web, and propose a preprocessing method of graph <b>canonicalization.</b> We first formalize the problem of graph records <b>canonicalization,</b> and then we propose three possible solutions in order. Upon the framework, we implement graph selection <b>canonicalization,</b> which aims to construct a canonical graph by selecting the central graph among records. Experiment results demonstrate its performance in representing real world entities...|$|E
