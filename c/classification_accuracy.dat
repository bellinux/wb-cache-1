10000|1785|Public
25|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel {{support vector machine}} decision tree had 80% <b>classification</b> <b>accuracy,</b> with an average computation time of 0.022s for each image classification.|$|E
25|$|In 2010, Wang and Wu {{presented}} a forward neural network (FNN) based method to classify a given MR brain image as normal or abnormal. The parameters of FNN were optimized via adaptive chaotic {{particle swarm optimization}} (ACPSO). Results over 160 images showed that the <b>classification</b> <b>accuracy</b> was 98.75%.|$|E
25|$|As an example, in an {{extension}} of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment <b>classification</b> <b>accuracy</b> and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.|$|E
30|$|In {{the summary}} section, {{it shows the}} average <b>{{classification}}</b> <b>accuracies</b> of the LR,DT and SVM classification groups.|$|R
30|$|In {{addition}} to the ROC curve analysis, a one way ANOVA test is also utilised for the performance evaluation of the best classification groups. The one-way ANOVA test is used to compare means of <b>classification</b> <b>accuracies</b> obtained in three experimental setups. This test is used to ascertain whether the difference/improvement in <b>classification</b> <b>accuracies</b> within different <b>classification</b> groups and other classifiers (across different classification methods) is significant or they all are equal.|$|R
3000|$|... {{descriptors}} {{perform better}} than the other tested descriptors. They both show high <b>classification</b> <b>accuracies</b> and low standard deviations over different orientations.|$|R
25|$|In 2013, Saritha et al. {{were the}} first to apply wavelet entropy (WE) to detect {{pathological}} brains. Saritha also suggested to use spider-web plots. Later, Zhang et al. proved removing spider-web plots did not influence the performance. Genetic pattern search method was applied to identify abnormal brain from normal controls. Its <b>classification</b> <b>accuracy</b> was reported as 95.188%. Das et al. proposed to use Ripplet transform. Zhang et al. proposed to use particle swarm optimization (PSO). Kalbkhani et al. suggested to use GARCH model.|$|E
25|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example. However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning <b>classification</b> <b>accuracy</b> comparisons than resolution. Nonetheless, the coherence of AUC as a measure of aggregated classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|E
50|$|Several {{variations}} of the window function have been proposed {{to allow for a}} range of learning speeds and <b>classification</b> <b>accuracy.</b>|$|E
40|$|Using the gray-level {{difference}} vector approach, <b>classification</b> <b>accuracies</b> with 1 / 8 -km spatial-resolution {{data are}} similar to those obtained using the full spatial-resolution features. Hence no advantage is to be gained in cloud <b>classification</b> <b>accuracies</b> by using even higher spatial resolutions obtained from Landsat TM or SPOT imagery. The optimum spatial resolution is 1 / 4 km. However, significant improvement in cloud-classification accuracy compared to that available from the 1 -km resolution of AVHRR and GOES imagery is obtained using 1 / 2 -km-resolution data. Cirrus-classification accuracy is especially compromised as spatial resolution is degraded. However, texture measures defined at the combination of pixel separations d = 1, 4 improve <b>classification</b> <b>accuracies</b> by several percent, even for 1 -km spatial-resolution data. Cirrus-classification accuracy is significantly improved by the use of multiple distance features...|$|R
40|$|A {{case study}} for a {{supervised}} classification of multispectral Landsat ETM imagery {{from the urban}} area of Cuiabá / Várzea Grande is presented. Two classification techniques implemented in the SPRING software were compared: Maximum Likelihood and Bhattacharrya with previous segmentation by region growing. Overall <b>classification</b> <b>accuracies</b> of about 61 and 55 % indicate the limitations of mid resolution imagery for land use mapping in urban areas. Previous segmentation (Bhattacharrya) improve <b>classification</b> <b>accuracies</b> substantially. Pages: 6011 - 601...|$|R
30|$|First of all, we {{evaluate}} the effectiveness of VLAD-FV by comparing the <b>classification</b> <b>accuracies</b> of all datasets using Fisher Vectors, VLAD, and VLAD-FV.|$|R
5000|$|In {{order to}} define , we define an {{objective}} function describing <b>classification</b> <b>accuracy</b> in the transformed space {{and try to}} determine [...] such that this objective function is maximized.|$|E
50|$|Exponential linear units try to {{make the}} mean activations closer to zero which speeds up learning. It has been shown that ELUs can obtain higher <b>classification</b> <b>accuracy</b> than ReLUs.|$|E
5000|$|In {{the case}} of Gaussian-distributed data and {{unbiased}} class distributions, this statistic {{can be related to}} <b>classification</b> <b>accuracy</b> given an ideal linear discrimination, and a decision boundary can be derived.|$|E
30|$|In {{addition}} to the ROC curve analysis {{which is used to}} evaluate the performance of best classification setups. A one way ANOVA (analysis of variance) is also employed to compare means of <b>classification</b> <b>accuracies</b> obtained in three experimental setups to establish whether the difference in <b>classification</b> <b>accuracies</b> within groups and among other classifiers is significant or they are statistically equal. Table 7 shows detailed analysis of the one-way ANOVA test which is performed using LR, DT and SVM experimental setups.|$|R
30|$|The <b>classification</b> <b>accuracies</b> {{using the}} SVM are almost higher {{due to its}} {{supervised}} aspects, and more consistent which {{can be explained by}} the data nature.|$|R
40|$|This paper {{examines}} {{the effects on}} timber stand computer <b>classification</b> <b>accuracies</b> caused by changes in the resolution of remotely sensed multispectral data. This investigation is valuable, especially for determining optimal sensor and platform designs. Theoretical justification and experimental verification support the finding that <b>classification</b> <b>accuracies</b> for low resolution data could {{be better than the}} accuracies for data with higher resolution. The increase in accuracy is constructed as due to the reduction of scene inhomogeneity at lower resolution. The computer classification scheme was a maximum likelihood classifier...|$|R
5000|$|Classification accuracy: <b>Classification</b> <b>accuracy</b> was {{compared}} to the accuracy displayed by the classifier yielded by the pLSA methods discussed earlier. It was discovered that OPTIMOL achieved slightly higher accuracy, obtaining 74.8% accuracy on 7 object categories, as compared to 72.0%.|$|E
50|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel {{support vector machine}} decision tree had 80% <b>classification</b> <b>accuracy,</b> with an average computation time of 0.022s for each image classification.|$|E
50|$|In 2010, Wang and Wu {{presented}} a forward neural network (FNN) based method to classify a given MR brain image as normal or abnormal. The parameters of FNN were optimized via adaptive chaotic {{particle swarm optimization}} (ACPSO). Results over 160 images showed that the <b>classification</b> <b>accuracy</b> was 98.75%.|$|E
40|$|We {{characterized}} {{features of}} magnetoencephalographic (MEG) and electroencephalographic (EEG) signals {{generated in the}} sensorimotor cortex of three tetraplegics attempting index finger movements. Single MEG and EEG trials were classified offline into two classes using two different classifiers, a batch trained classifier and a dynamic classifier. <b>Classification</b> <b>accuracies</b> obtained with dynamic classifier were better, at 75 %, 89 %, and 91 % in different subjects, when features were in the 0. 5 - 3. 0 -Hz frequency band. <b>Classification</b> <b>accuracies</b> of EEG and MEG did not differ...|$|R
40|$|Sample {{segments}} of ground-verified land cover data collected {{in conjunction with}} the USDA/ESS June Enumerative Survey were merged with LANDSAT data and served as a focus for unsupervised spectral class development and accuracy assessment. Multitemporal data sets were created from single-date LANDSAT MSS acquisitions from a nominal scene covering an eleven-county area in north central Missouri. <b>Classification</b> <b>accuracies</b> for the four land cover types predominant in the test site showed significant improvement in going from unitemporal to multitemporal data sets. Transformed LANDSAT data sets did not significantly improve <b>classification</b> <b>accuracies.</b> Regression estimators yielded mixed results for different land covers. Misregistration of two LANDSAT data sets by as much and one half pixels did not significantly alter overall <b>classification</b> <b>accuracies.</b> Existing algorithms for scene-to scene overlay proved adequate for multitemporal data analysis as long as statistical class development and accuracy assessment were restricted to field interior pixels...|$|R
30|$|Table 14 {{provides}} {{detailed analysis}} of the one-way ANOVA. In the summary section, the average <b>classification</b> <b>accuracies</b> are calculated based on LR, DT and SVM classification setups.|$|R
50|$|Bootstrap aggregating, often {{abbreviated}} as bagging, involves having {{each model}} in the ensemble vote with equal weight. In order to promote model variance, bagging trains each {{model in the}} ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high <b>classification</b> <b>accuracy.</b>|$|E
50|$|Multimodal deep Boltzmann {{machines}} is {{successfully used}} in classification and missing data retrieval. The <b>classification</b> <b>accuracy</b> of multimodal deep Boltzmann machine outperforms support vector machines, latent Dirichlet allocation and deep belief network, when models are tested on data with both image-text modalities or with single modality. Multimodal deep Boltzmann machine is {{also able to}} predict the missing modality given the observed ones with reasonably good precision.|$|E
50|$|At {{its most}} simple, it can include the {{decision}} to remove outliers, after noticing this might help improve the analysis of an experiment. The effect can be more subtle. In {{functional magnetic resonance imaging}} (fMRI) data, for example, considerable amounts of pre-processing is often needed. These might be applied incrementally until the analysis 'works'. Similarly, the classifiers used in a multivoxel pattern analysis of fMRI data require parameters, which could be tuned to maximise the <b>classification</b> <b>accuracy.</b>|$|E
3000|$|Generally, {{trends of}} Figure 4 shows similar {{properties}} as was discussed for Figure 3. The major {{point is that}} <b>classification</b> <b>accuracies</b> of best models on mixed signals ξ [...]...|$|R
40|$|Several {{published}} reports show that instance-based learning algorithms yield high <b>classification</b> <b>accuracies</b> and have low storage requirements during supervised learning applications. However, these learning algorithms are highly sensitive to noisy training instances. This paper describes a simple extension of instance-based learning algorithms for detecting and removing noisy instances from concept descriptions. This extension requires evidence that saved instances be significantly good classifiers before it {{allows them to}} be used for subsequent classification tasks. We show that this extension's performance degrades more slowly in the presence of noise, improves <b>classification</b> <b>accuracies,</b> and further reduces storage requirements in several artificial and real-world database applications...|$|R
30|$|Six of the 12 {{subjects}} were {{qualified for the}} on-line experiments based on their high off-line <b>classification</b> <b>accuracies</b> (CAs > 75 %). The overall mean on-line accuracy {{was found to be}} 80 %.|$|R
50|$|Automatic project {{classification}} is used {{to identify}} projects related to aging research within the large data sets and to classify projects into relevant semantic groups. The system utilizes two classification algorithms with elements of machine learning: Support Vector Machine SVM and Recurrent-Neural-Network-Based Boolean Factor Analysis (BFA).Since 2014 the SVM algorithm was modified to facilitate for multilabel classification of incompletely-labelled data sets where few labels assigned by the IARP experts are present. This allowed for improved <b>classification</b> <b>accuracy.</b>|$|E
50|$|In 2013, Saritha et al. {{were the}} first to apply wavelet entropy (WE) to detect {{pathological}} brains. Saritha also suggested to use spider-web plots. Later, Zhang et al. proved removing spider-web plots did not influence the performance. Genetic pattern search method was applied to identify abnormal brain from normal controls. Its <b>classification</b> <b>accuracy</b> was reported as 95.188%. Das et al. proposed to use Ripplet transform. Zhang et al. proposed to use particle swarm optimization (PSO). Kalbkhani et al. suggested to use GARCH model.|$|E
50|$|This {{theory can}} also be {{extended}} for the speech recognition domain.As an example, in an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment <b>classification</b> <b>accuracy</b> and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.|$|E
40|$|The {{limit of}} active ice, {{accurate}} to within 49 m, is extracted from SAR {{imagery of the}} Barnes Ice Cap, using a sequence of image speckle filtering, image texture analysis, supervised image classification, image segmentation and edge detection. Overall <b>classification</b> <b>accuracies</b> of the ice marginal environment are between 42 and 53 %. Despite misclassification of some proglacial landforms {{it is possible to}} detect the limit of active ice as these surfaces are separated from the ice cap by supraglacial debris cover, an elevated ice cored debris ridge and perennial snowbeds. [...] Comparisons of opposing look angles reveal that 'downglacier' illumination produces the highest <b>classification</b> <b>accuracies</b> of ice marginal features and debris covered ice surfaces, whereas illumination from an off-ice perspective, looking 'upglacier', is found to produce higher <b>classification</b> <b>accuracies</b> for proglacial surfaces. Comparisons of standard and fine mode imagery conclude that 25 m spatial resolution yields higher <b>classification</b> <b>accuracies</b> than 8 m spatial resolution. Quantitative analysis of surface roughness demonstrates that the dominant grain size of surficial materials is the best method for relating surface cover to radar brightness. [...] Second order texture measures mean, homogeneity and correlation are found to be effective variables for maximum likelihood classification of ice marginal SAR imagery. Texture window size should be as large as the smallest feature to be identified, in this case approximately 85 x 85 m for the mean and homogeneity measures and 185 x 185 m for the correlation measure...|$|R
30|$|We {{observe that}} the {{features}} {{proposed in the}} present paper cause a reduction of the <b>classification</b> <b>accuracies</b> for text, mix, and photo documents. This {{is due to the}} fact that our features avoid vertical computations while the ones in [1] do not. However, thanks to our design of halftone noise triplets 3, our new features improve the correct classification rates of picture documents. Specifically, features from [1] have 2, 6, 9, 3, and 8 % higher <b>classification</b> <b>accuracies</b> for color-text, color-photo, mono-text, mono-mix, and mono-photo, respectively; while our proposed features have the correct classification gain of 1 and 19 % for color-picture and mono-picture, respectively.|$|R
40|$|The Urban Land Use Team {{conducted}} a year's investigation of ERTS- 1 MSS data {{to determine the}} number of Land Use categories in the Houston, Texas, area. They discovered unusually low <b>classification</b> <b>accuracies</b> occurred when a spectrally complex urban scene was classified with extensive rural areas containing spectrally homogeneous features. Separate computer processing of only data in the urbanized area increased <b>classification</b> <b>accuracies</b> of certain urban land use categories. Even so, accuracies of urban landscape were in the 40 - 70 percent range compared to 70 - 90 percent for the land use categories containing more homogeneous features (agriculture, forest, water, etc.) in the nonurban areas...|$|R
