4|10000|Public
40|$|Abstract: The Holocene estuarine silts of the Severn Estuary Levels (southwest Britain) are {{representative}} of their kind in northwest Europe. They contain two broad types of plant material: particles codeposited with mineral grains from the estuarine water body, and extraneous debris (stems of indigenous prior plants; post depositional root matter) which is difficult to remove completely by physical means. Treatment with hydrogen peroxide before laser granulometry removes all plant material regardless of kind, drastically reduces values for the mean grain size and median size relative to untreated samples, but has {{little effect on the}} mode, except for a restricted group of bimodal-platykurtic, medium-coarse silts. It is concluded that, in the case of sediments of the general kind examined, no advantages acrue from the treatment of samples with hydrogen peroxide prior to analysis. Although a discrete rather than <b>continuous</b> <b>variable,</b> <b>values</b> of the mode obtained from untreated sediments are suggested to be acceptable for most purposes where a measure of central tendency is required...|$|E
40|$|This {{paper is}} {{concerned}} with augmenting genetic algorithms (GAs) to include memory for continuous variables, and applying this to stacking sequence design of laminated sandwich composite panels that involves both discrete variables and a continuous design variable. The term “memory” implies preserving data from previously analyzed designs. A balanced binary tree with nodes corresponding to discrete designs renders efficient access to the memory. For those discrete designs that occur frequently, an evolving database of <b>continuous</b> <b>variable</b> <b>values</b> is used to construct a spline approximation to the fitness {{as a function of}} the single continuous variable. The approximation is then used to decide when to retrieve the fitness function value from the spline and when to do an exact analysis to add a new data point for the spline. With the spline approximation in place, it is also possible to use the best solution of the approximation as a local improvement during the optimization process. The demonstration problem chosen is the stacking sequence optimization of a sandwich plate with composite face sheets for weight minimization subject to strength and buckling constraints. Comparisons are made between the cases with and without the binary tree and spline interpolation added to a standard genetic algorithm. Reduced computational cost and increased performance index of a genetic algorithm with these changes are demonstrated...|$|E
40|$|This {{dissertation}} presents methodologies for {{operational planning}} in Combined Heat and Power (CHP) systems. The subject of experimentation is the University of Massachusetts CHP system, {{which is a}} 22 MWe/ 640 MBh system for a district energy application. Systems like this have complex energy flow networks due to multiple interconnected thermodynamic components like gas and steam turbines, boilers and {{heat recovery steam generators}} and also interconnection with centralized electric grids. In district energy applications, heat and power requirements vary over 24 hour periods (planning horizon) due to changing weather conditions, time-of-day factors and consumer requirements. System thermal performance is highly dependent on ambient temperature and operating load, because component performances are nonlinear functions of these parameters. Electric grid charges are much higher for on-peak than off-peak periods, on-site fuel choices vary in prices and cheaper fuels are available only in limited quantities. In order to operate such systems in energy efficient, cost effective and least polluting ways, optimal scheduling strategies need to be developed. For such problems, Mixed-Integer Nonlinear Programming (MINLP) formulations are proposed. Three problem formulations are of interest; energy optimization, cost optimization and emission optimization. Energy optimization reduces system fuel input based on component nonlinear efficiency characteristics. Cost optimization addresses price fluctuations between grid on-peak and off-peak periods and differences in on-site fuel prices. Emission optimization considers CO 2 emission levels caused by direct utilization of fossil fuels on-site and indirect utilization when importing electricity from the grid. Three solution techniques are employed; a deterministic algorithm, a stochastic search and a heuristic approach. The deterministic algorithm is the classical branch-and-bound method. Numerical experimentation shows that as planning horizon size increases linearly, computer processing time for branch-and-bound increases exponentially. Also in the problem formulation, fuel availability limitations lead to nonlinear constraints for which branch-and-bound in unable to find integer solutions. A genetic algorithm is proposed in which genetic search is applied only on integer variables and gradient search is applied on continuous variables. This hybrid genetic algorithm finds more optimal solutions than branch-and-bound within reasonable computer processing time. The heuristic approach fixes integer values over the planning horizon based on constraint satisfaction. It then uses gradient search to find optimum <b>continuous</b> <b>variable</b> <b>values.</b> The heuristic approach finds more optimal solutions than the proposed genetic algorithm and requires very little computer processing time. A numerical study using actual system operation data shows optimal scheduling can improve system efficiency by 6 %, reduce cost by 11 % and emission by 14 %...|$|E
2500|$|Suppose X is a <b>continuous</b> random <b>variable</b> whose <b>values</b> {{lie in the}} non-negative real numbers ...|$|R
40|$|Recently {{there has}} been {{increased}} research interest {{in the study of}} the hybrid dynamical systems (Sun & Ge, 2005) and (Li et al., 2005). These systems involve the interaction of discrete and <b>continuous</b> dynamics. <b>Continuous</b> <b>variables</b> take the <b>values</b> from the set of real numbers and the discrete <b>variables</b> take the <b>values</b> from finite set of symbols. The hybri...|$|R
30|$|The 75 th {{percentile}} of mean myoma diameters {{was used}} to define the categorical variable small/large myoma. A multivariate logistic analysis was performed to identify variables independently and significantly associated to surgical outcomes. Similarly, a multiple regression analysis was performed using myoma mean diameter and submucous myoma grading as <b>continuous</b> <b>variables.</b> Probability <b>values</b> less than 5 % were considered significant. The time to event (abnormal uterine bleeding) analysis for all the patients was performed by the Kaplan–Meyer curve.|$|R
50|$|In automata theory, {{a hybrid}} {{automaton}} (plural: hybrid automata or hybrid automatons) is {{a mathematical model}} for precisely describing systems in which digital computational processes interact with analog physical processes. A hybrid automaton is a finite state machine with a finite set of <b>continuous</b> <b>variables</b> whose <b>values</b> are described {{by a set of}} ordinary differential equations. This combined specification of discrete and continuous behaviors enables dynamic systems that comprise both digital and analog components to be modeled and analyzed.|$|R
40|$|Nowadays a lot {{of methods}} of {{intelligent}} software agents learning and adaptation exist. One of them represents the reinforcement learning. This method {{has proved to be a}} mechanism capable to be a success in coping with different tasks, types of environment (with or without Markov property), discrete and <b>continuous</b> <b>variables</b> <b>values.</b> Taking into account that in the basis of the algorithm there are mechanisms of random selection, the methods of reinforcement learning suffer from the problem of “curse of dimensionality”. This paper offers an approach considerably reducing the space of search without losing the quality of Q-table obtained. The most ordinary but popular method of learning - SARSA(λ) (temporal-difference with eligibility traces) – is an example where the developed algorithm was applied. As a task, not less popular example of agent management in the cellular world possessing Markov property is used. The essence of this method is that the agent, as in the case of the eligibility traces, uses additional labels (marks) operating as an award. The approach does not go outside the framework of the actions available for the agent...|$|R
30|$|Differences between patient {{characteristics}} {{in the open}} and laparoscopic groups were evaluated using the Chi square test for categorical variables and the Mann–Whitney U test for <b>continuous</b> <b>variables.</b> A p <b>value</b> of less than 0.05 was considered to denote significance. All {{statistical analyses were performed}} using the statistical software program JMP version 11.0. 2 (SAS Institute Inc., Cary, NC, USA).|$|R
30|$|Comparison of {{demographic}} and clinical variables {{between the two}} groups was performed using a Chi-square test (or Fisher’s exact test) for categorical data, and a t test for <b>continuous</b> <b>variables.</b> Probability (p) <b>values</b> less than 0.05 were considered statistically significant. The same comparisons between good vs. moderate and poor responders were also applied to the BD-I subgroup. All statistical analyses were done with IBM SPSS version 23.0.|$|R
40|$|Abstract This paper {{introduces}} {{tabu search}} for the solution of general linear integer prob-lems. Search is done on integer variables; if there are <b>continuous</b> <b>variables,</b> their corresponding <b>value</b> is determined through the solution of a linear program, which {{is also used to}} evaluate the integer solution. The complete tabu search pro-cedure includes an intensification and diversification procedure, whose effects are analysed on a set of benchmark problems...|$|R
30|$|<b>Continuous</b> <b>variables</b> are {{reported}} as {{means and standard}} deviation (SD) for normally distributed data and as medians and interquartile ranges (IQR) for non-normally distributed data. Categorical variables are presented as percentages. Between-group differences in <b>continuous</b> <b>variables</b> were compared using Mann–Whitney U test. Differences in ventilation or clinical parameters in the same patient between different periods were compared with Friedman’s two-way analysis of variance by ranks. Spearman’s rho {{was used to evaluate}} correlations between <b>continuous</b> <b>variables.</b> A p <b>value</b> of <[*] 0.05 was considered significant. We used IBM SPSS Statistics for Windows version 22 (Armonk, NY) for analysis.|$|R
30|$|Associations {{between a}} {{decrease}} in blood counts from baseline values and the mean absorbed dose to the spleen were investigated by analysis of variance (ANOVA) in linear regression analysis. Residuals of the linear regression analysis (Hb and PLT values) {{were found to be}} normally distributed according to the Anderson-Darling test. Means and standard deviation are used to report normally distributed <b>continuous</b> <b>variables,</b> otherwise median <b>values</b> and range are reported. p values < 0.05 were considered significant.|$|R
40|$|Abstract This paper {{introduces}} {{tabu search}} for the solution of general linear integer problems. Search is done on integer variables; if there are <b>continuous</b> <b>variables,</b> their corresponding <b>value</b> is determined through the solution of a linear program, which is also used toevaluate the integer solution. The complete tabu search procedure includes an intensification and diversificationprocedure, whose effects are analysed {{on a set of}} benchmark problems. Key-words: Tabu search, Linear Integer Programming, Mixed IntegerProgramming 1 Introduction In this work we focus on a tabu {{search for the}} problem of optimizing a linear functionsubject to a set of linear constraints, in the presence of integer and, possibly, <b>continuous</b> <b>variables.</b> If the subset of <b>continuous</b> <b>variables</b> is empty, the problem is called pure integer(IP). In the more general case, where there are also <b>continuous</b> <b>variables,</b> the problem is usually called mixed integer (MIP). The mathematical programming formulation of a mixed integer linear program i...|$|R
40|$|We {{present a}} {{methodology}} {{that enables the}} use of existent classification inductive learning systems on problems of regression. We achieve this goal by transforming regression problems into classification problems. This is done by transforming the range of <b>continuous</b> goal <b>variable</b> <b>values</b> into a set of intervals that will be used as discrete classes. We provide several methods for discretizing the goal <b>variable</b> <b>values.</b> These methods are based on the idea of performing an iterative search for the set of final discrete classes. The search algorithm is guided by a N-fold cross validation estimation of the prediction error resulting from using a set of discrete classes. We have done extensive empirical evaluation of our discretization methodologies using C 4. 5 and CN 2 on four real world domains. The results of these experiments show the quality of our discretization methods compared to other existing methods. Our method is independent of the used classification inductive system. The method is [...] ...|$|R
40|$|AbstractThe present {{paper is}} {{concerned}} with the grouping of book covers on offset plates in order to minimize the total production cost. The mathematical formulation of the problem involves both binary and <b>continuous</b> <b>variables.</b> As exact methods are unable to provide solutions in reasonable time, a heuristic algorithm of the simulated annealing type is proposed. At each iteration, the values of the current solution binary variables are altered in order to yield a neighboring solution. To compute the corresponding <b>values</b> of the <b>continuous</b> <b>variables</b> and the <b>value</b> of the objective function, a linear programming routine is called at each iteration. This constitutes the main originality of the present approach and is in principle applicable in mixed integer programming problems. The procedure is tested on several examples...|$|R
40|$|This paper {{proposes a}} novel {{framework}} for mining regional colocation patterns {{with respect to}} sets of <b>continuous</b> <b>variables</b> in spatial datasets. The goal is to identify regions in which multiple <b>continuous</b> <b>variables</b> with <b>values</b> from the wings of their statistical distribution are co-located. A co-location mining framework is introduced that operates in the continuous domain and which views regional co-location mining as a clustering problem in which an externally given fitness function has to be maximized. Interestingness of co-location patterns is assessed using products of z-scores of the relevant <b>continuous</b> <b>variables.</b> The proposed framework is evaluated by a domain expert in a case study that analyzes Arsenic contamination in Texas water wells centering on regional co-location patterns. Our approach is able to identify known and unknown regional co-location patterns, and different sets of algorithm parameters lead to the characterization of Arsenic distribution at different scales. Moreover, inconsistent colocation sets are found for regions in South Texas and West Texas that can be clearly attributed to geological differences in the two regions, emphasizing the need for regional co-location mining techniques. Moreover, a novel, prototype-based region discovery algorithm named CLEVER is introduced that uses randomized hill climbing, and searches a variable number of clusters and larger neighborhood sizes...|$|R
40|$|We {{present a}} {{generalization}} {{of the master}} solution to the quantum Yang-Baxter equation (obtained recently in arXiv: 1006. 0651) {{to the case of}} multi-component <b>continuous</b> spin <b>variables</b> taking <b>values</b> on a circle. The Boltzmann weights are expressed in terms of the elliptic gamma-function. The associated solvable lattice model admits various equivalent descriptions, including an interaction-round-a-face formulation with positive Boltzmann weights. In the quasi-classical limit the model leads to a new series of classical discrete integrable equations on planar graphs. Comment: 22 pages, 5 figure...|$|R
40|$|We {{consider}} {{the issue of}} performing accurate small sample inference in beta autoregressive moving average model, which is useful for modeling and forecasting <b>continuous</b> <b>variables</b> that assumes <b>values</b> in the interval $(0, 1) $. The inferences based on conditional maximum likelihood estimation have good asymptotic properties, but their performances in small samples may be poor. This way, we propose bootstrap bias corrections of the point estimators and different bootstrap strategies for confidence interval improvements. Our Monte Carlo simulations show that finite sample inference based on bootstrap corrections is much more reliable than the usual inferences. We also presented an empirical application. Comment: Accepted pape...|$|R
40|$|Abstract. Cost {{sensitive}} {{prediction is}} a key task in many real world applications. Most existing {{research in this area}} deals with classification problems. This paper addresses a related regression problem: the prediction of rare extreme <b>values</b> of a <b>continuous</b> <b>variable.</b> These <b>values</b> are often regarded as outliers and removed from posterior analysis. However, for many applications (e. g. in finance, meteorology, biology, etc.) these are the key values that we want to accurately predict. Any learning method obtains models by optimizing some preference criteria. In this paper we propose new evaluation criteria that are more adequate for these applications. We describe a generalization for regression of the concepts of precision and recall often used in classification. Using these new evaluation metrics we are able to focus the evaluation of predictive models on the cases that really matter for these applications. Our experiments indicate the advantages of the use of these new measures when comparing predictive models in the context of our target applications. ...|$|R
40|$|We {{propose a}} {{probabilistic}} interpretation of Benford’s law, which predicts the probability distribution of all digits in everyday-life numbers. Heuri- stically, {{our point of}} view consists in considering an everyday-life number as a <b>continuous</b> random <b>variable</b> taking <b>value</b> in an interval [0,A], whose maximum A is itself an everyday-life number. This approach can be linked to the chara- cterization of Benford’s law by scale-invariance, {{as well as to the}} convergence of a product of independent random variables to Benford’s law. It also allows to generalize Flehinger’s result about the convergence of iterations of Cesaro- averages to Benford’s la...|$|R
40|$|This paper {{proposes a}} {{superior}} Mixed Integer based hybrid Genetic Algorithm (MIGA) which inherits {{the advantages of}} binary and real coded Genetic Algorithm approach. The proposed algorithm is applied for the conventional generation cost minimization Optimal Power Flow (OPF) problem and for the Security Constrained Optimal Power Flow problem. Here, the main shortcoming with the conventional Genetic Algorithm, the ‘Hamming Cliff’ problem is addressed with Mixed Genetic Algorithm, which can overcome issues connected to the continuous search space. The proposed algorithm models the <b>continuous</b> <b>variables</b> using real <b>values</b> and discrete <b>variables</b> using binary <b>values.</b> A novel concept of Progressive filling is also presented here for Mixed Integer GA, which heightens the algorithm. The proposed procedure is compared with many conventional algorithms and validated on a test-bed of standard IEEE 30 bus system with and without valve-point loading effect...|$|R
30|$|All {{analyses}} were performed with SPSS 22 (IBM, New York, USA). The Kaplan–Meier method was used to generate actuarial survival curves. These were compared with the log-rank test. Multivariate analysis of prognostic factors was performed with the Cox proportional hazards model (forward stepwise data selection method; inclusion if univariate p value < 0.15; univariate analysis performed for all baseline parameters reported in the Results section). Age and Karnofsky performance status (KPS) were entered as <b>continuous</b> <b>variables.</b> A p <b>value</b> ≤ 0.05 was considered statistically significant. As a retrospective quality of care analysis, no approval from the Regional Committee for Medical and Health Research Ethics (REK Nord) was necessary. This research project was carried out according to our institutions’ guidelines and with permission to access the patients’ data.|$|R
40|$|Abstract. In {{discretization}} of a <b>continuous</b> <b>variable</b> its numerical <b>value</b> {{range is}} divided into a few intervals {{that are used in}} classification. For example, Naïve Bayes can benefit from this processing. A commonlyused supervised discretization method is Fayyad and Irani’s recursive entropy-based splitting of a value range. The technique uses mdl as a model selection criterion to decide whether to accept the proposed split. We argue that theoretically the method is not always close to ideal for this application. Empirical experiments support our finding. We give a statistical rule that does not use the ad-hoc rule of Fayyad and Irani’s approach to increase its performance. This rule, though, is quite time consuming to compute. We also demonstrate that a very simple Bayesian method performs better than mdl as a model selection criterion. ...|$|R
40|$|In diagnosis, when a {{hypothesis}} proposes a <b>variable's</b> <b>value,</b> several different lines of evidence may be considered; the different evidence must be arbitrated. The {{result of this}} arbitration consists of a single best estimate of the <b>variable</b> <b>value</b> and of a measure of that estimate's plausibility. The plausibility measure reflects the degree of agreement among the lines of evidence. This report describes heatx, a program for model-based diagnosis of nonlinear mechanisms with <b>continuous</b> <b>variables.</b> Previous work in model-based diagnosis has avoided arbitrating numeric evidence, often by representing <b>continuous</b> <b>variables</b> as discrete symbols (e. g., high, cold). Such restricted representation have had difficulty in diagnosing mechanisms with feedback or reconvergent fanout. Heatx represents numerical data explicitly in the hypotheses and in the inferencing procedures; it is thereby able to arbitrate evidence numerically. Heatx uses both nonlinear numerical simulations and approximate linear [...] ...|$|R
30|$|SPSS 15.0 {{software}} (SPSS Inc, Chicago, IL) {{was used}} for statistical analysis. Data were reported as mean ± SD for <b>continuous</b> <b>variables</b> and frequencies for categorical <b>variables.</b> P <b>values</b> of < 0.05 were considered statistically significant.|$|R
30|$|Data were {{presented}} in tables of frequency distribution for discrete variables, and using the average and standard deviation for <b>continuous</b> <b>variables.</b> The analysis of risk factors associated with infection in both groups (with or without infection) was made using the Student t test for <b>continuous</b> <b>variables</b> and the chi-squared test for discrete <b>variables.</b> The <b>value</b> of p ≤  0.05 was adopted as the significance level for all tests.|$|R
40|$|Soil {{properties}} are <b>continuous</b> <b>variables</b> whose <b>values</b> at any location {{can be expected}} to vary according to direction and distance of separation from neighboring samples. The spatial variability and variation of soil properties should be quantified for {{a better understanding of the}} influence of such factors as management and pollution, and finally for leading to more efficient management. Geostatistics provide descriptive tools such as variogram to characterize the spatial pattern of continuous soil attributes. This study addressed the spatial variability of soil properties at the regional scale using geostatistical method. The study was carried out in Wildlife Refugee of Karkhe in riparian forests of Karkhe river southwestern Iran. The soil was sampled in 2009 using 200 sampling point along parallel transects (perpendicular to the river). The distance between transects were 0. 5 km. The sampling procedure was hierarchically, we considered maximum distance between samples as 0. 5 km, but the samples were taken at 250 m, 100 m, 50 m, 10 m and 5 m at different locations of sampling. At each transect point, three 50 cm× 50 cm× 25 cm samples were taken for analyses at each sampling campaign. Soil bulk density, total nitrogen and C/N were analyzed usin...|$|R
40|$|Complex {{systems are}} often modeled as Boolean {{networks}} {{in attempts to}} capture their logical structure and reveal its dynamical consequences. Approximating the dynamics of <b>continuous</b> <b>variables</b> by discrete <b>values</b> and Boolean logic gates may, however, introduce dynamical possibilities that are not accessible to the original system. We show that large random networks of <b>variables</b> coupled through <b>continuous</b> transfer functions often fail to exhibit the complex dynamics of corresponding Boolean models in the disordered (chaotic) regime, even when each individual function {{appears to be a}} good candidate for Boolean idealization. A suitably modified Boolean theory explains the behavior of systems in which information does not propagate faithfully down certain chains of nodes. Model networks incorporating calculated or directly measured transfer functions reported in the literature on transcriptional regulation of genes are described by the modified theory. Comment: 7 pages, 1 table, 6 figures. Minor changes over previous version. accepted to PR...|$|R
30|$|For the {{analysis}} of the collected data, a series of Chi-square tests (χ 2) were performed to test the significance effect between two discrete variables. Fisher’s exact test was used for contingency tables that contain small expected values (< 5) in more than 20  % of the cells (i.e. only p value is reported). Cramers’ V 2 statistic was used to report the strength of association between discrete variables typically applied to 2  × n tables, which is conventionally considered to be low if <  0.04, medium if between 0.04 and 0.16, and high if >  0.16 [19]. Relative Deviations (RDs) were used to inform on the strength of association between the modalities of the two discrete variables. Relative deviations are calculated {{on the basis of the}} comparison between the observed and expected frequencies in each cell. By convention, there is a high positive or negative association when the absolute RD value is > 0.20. Only associations > 0.10 are described in the results section. Finally, analysis of variance tests (ANOVA) and correlations were used to test the effects on <b>continuous</b> <b>variables.</b> Post-hoc tests using Bonferroni correction were used to examine the relationships between the modalities of <b>continuous</b> <b>variables</b> (only p <b>value</b> is reported where the means are presented in tables).|$|R
30|$|Median {{and range}} {{were used to}} {{describe}} <b>continuous</b> <b>variables.</b> Pearson’s correlation coefficients and regression analysis were used to quantify and identify relations between <b>variables.</b> Two-sided P <b>values</b> below 0.05 were considered significant.|$|R
40|$|Background: Liver injury due to dengue viral {{infection}} is not uncommon. Acute liver injury is a severe complicating factor in dengue, predisposing to life-threatening hemorrhage, Disseminated Intravascular Coagulation (DIC) and encephalopathy. Therefore {{we sought to}} determine the frequency of hepatitis in dengue infection and to compare the outcome (length of stay, in hospital mortality, complications) between patients of Dengue who have mild/moderate (ALT 23 - 300 IU/L) v/s severe acute hepatitis (ALT 3 ̆e 300 IU/L). Methods: A Cohort study of inpatients with dengue {{viral infection}} done at Aga Khan University Hospital Karachi. All patients (≥ 14 yrs age) admitted with diagnosis of Dengue Fever (DF), Dengue Hemorrhagic Fever (DHF) or Dengue Shock Syndrome (DSS) were included. Chi square {{test was used to}} compare categorical variables and fischer exact test where applicable. Survival analysis (Cox regression and log rank) for primary outcome was done. Student t test was used to compare <b>continuous</b> <b>variables.</b> A p <b>value</b> of {{less than or equal to}} 0. 05 was taken as significant. Results: Six hundred and ninety nine patients were enrolled, including 87...|$|R
40|$|Three multidimensional {{visualization}} methods utilizing nested dimensions namely; trellis-like displays, mosaic {{plots and}} TempleMVV graphs are discussed and compared {{in respect to}} the insights they provide and their performance. These techniques are applicable {{when the number of}} dimensions is no larger than 10 to 20. Only mosaic plots and TempleMVV graphs, can be applied to datasets with large numbers of data points (millions or more). Trellis-like displays, consisting of collections of scatterplots of two <b>continuous</b> <b>variables</b> conditioned by <b>values</b> of other <b>variables,</b> cannot be used effectively for large datasets because of both performance and over-striking problems and the limitations of plotting "raw " data instead of statistical information. Although mosaic plots can be used to represent very large datasets, they can only display the sums of a response variable (or of count) not the means or other statistics. This limitation and a variety of display problems limit the types of insights that mosaic plots can provide. In order to make valid comparisons, all three methods are applied to the same dataset namely a census dataset involving 55, 000 persons. 1...|$|R
30|$|Data were {{collated}} in a secured {{data file}} and were analyzed {{thanks to the}} Excel software. Results were expressed as mean[*]±[*]standard deviation for <b>continuous</b> <b>variable</b> or as percentages for discrete <b>variables.</b> A p <b>value</b> <[*] 0.05 was considered as statistically significant.|$|R
40|$|Methods which yield {{closed form}} performability {{solutions}} for <b>continuous</b> <b>valued</b> <b>variables</b> are developed. The models {{are similar to}} those employed in performance modeling (i. e., Markovian queueing models) but are extended so as to account for variations in structure due to faults. In particular, the modeling of a degradable buffer/multiprocessor system is considered whose performance Y is the (normalized) average throughput rate realized during a bounded interval of time. To avoid known difficulties associated with exact transient solutions, an approximate decomposition of the model is employed permitting certain submodels to be solved in equilibrium. These solutions are then incorporated in a model with fewer transient states and by solving the latter, a closed form solution of the system's performability is obtained. In conclusion, some applications of this solution are discussed and illustrated, including an example of design optimization...|$|R
40|$|Abstract. Spatial {{autocorrelation}} must handle {{two kinds}} of geographic data. One is <b>continuous</b> <b>valued</b> <b>variables</b> in which the observations are real numbers. Another is nominal variables which consist {{of a set of}} discrete categories. The frequently used spatial autocorrelation statistic for nominal variable is “join-counts”, which deals with two categories that are often referred to as “black ” and “white”. However, three categories are also common case in present world. For example, as to land cover, the attribute of each parcel may be changed, unchanged or uncertain which represents parcels not belonging to the first two categories. In three valued logic, values could be true, false or unknown. This paper extended join-counts to trinary join-count. The trinary categories are referred as “black”, “white ” and “gray ” in this paper and the possible type of join...|$|R
