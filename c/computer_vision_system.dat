686|10000|Public
25|$|The {{organization}} of a <b>computer</b> <b>vision</b> <b>system</b> is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system {{of a larger}} design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a <b>computer</b> <b>vision</b> <b>system</b> also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.|$|E
25|$|The AIBO {{has seen}} much {{use as an}} {{inexpensive}} platform for artificial intelligence education and research, because it integrates a <b>computer,</b> <b>vision</b> <b>system,</b> and articulators in a package vastly cheaper than conventional research robots. One focal point for that development has been the Robocup Leagues.|$|E
25|$|Areas of {{artificial}} intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed {{understanding of these}} environments is required to navigate through them. Information about the environment {{could be provided by}} a <b>computer</b> <b>vision</b> <b>system,</b> acting as a vision sensor and providing high-level information about the environment and the robot.|$|E
40|$|Robots {{rely on the}} <b>computer</b> <b>vision</b> <b>systems</b> {{to obtain}} the {{environmental}} information. As a result, {{the accuracy of the}} <b>computer</b> <b>vision</b> <b>systems</b> is essential for the control of the robots. Many <b>computer</b> <b>vision</b> <b>systems</b> make use of markers of the well-designed patterns to calculate the system parameters. Undesirably, the noise exists universally, which decreases the calibration accuracy and consequently decreases the accuracy of the <b>computer</b> <b>vision</b> <b>systems.</b> In this paper, we propose a pattern modeling method to remove the noise by decreasing the degree of freedom of the total calibration markers to one. The theorem is proposed and proved. The proposed method can be readily adopted by different <b>computer</b> <b>vision</b> <b>systems,</b> e. g. structured light based <b>computer</b> <b>vision</b> <b>systems</b> and stereo <b>vision</b> based <b>systems.</b> IEE...|$|R
5000|$|While {{there are}} many {{manufacturers}} of 3D <b>computer</b> <b>vision</b> <b>systems,</b> these systems utilize either 3D scanning technologies or a moving camera to measure an object or scene. InspecVision Ltd. produces primarily 2D or 2&1/2D as well as 3D <b>computer</b> <b>vision</b> <b>systems</b> and its customers are often manufacturers of 2D and 3D components such as sheet metal components or gaskets or plastic mouldings. Furthermore, unlike other 3D or 2D <b>computer</b> <b>vision</b> <b>systems</b> InspecVision Ltd. have developed a technology which requires no moving parts, rather a single fixed ultra-high resolution digital camera is {{used to measure the}} entire object with a single image. These systems perform measurement times that are usually much less than a second. A completely static system also precludes the rigorous recalibration requirements of other moving camera or <b>computer</b> <b>vision</b> <b>systems.</b> In 2017 InspecVision re ...|$|R
25|$|A few <b>computer</b> <b>vision</b> <b>systems</b> use image {{acquisition}} hardware with active illumination or {{something other than}} visible light or both.|$|R
25|$|The {{interest}} points {{obtained from}} the scale-adapted Laplacian blob detector or the multi-scale Harris corner detector with automatic scale selection are invariant to translations, rotations and uniform rescalings in the spatial domain. The images that constitute the input to a <b>computer</b> <b>vision</b> <b>system</b> are, however, also subject to perspective distortions. To obtain interest points that are more robust to perspective transformations, a natural approach is to devise a feature detector that is invariant to affine transformations.|$|E
5000|$|... rectifyStereoImages, native MATLAB {{function}} in <b>Computer</b> <b>Vision</b> <b>System</b> Toolbox ...|$|E
50|$|In a <b>computer</b> <b>vision</b> <b>system,</b> several {{pre-processing}} {{steps are}} required.|$|E
25|$|Most <b>computer</b> <b>vision</b> <b>systems</b> use visible-light cameras passively viewing a {{scene at}} frame rates of at most 60 {{frames per second}} (usually far slower).|$|R
40|$|This paper {{describes}} a framework which uses augmented reality {{for evaluating the}} performance of mobile <b>computer</b> <b>vision</b> <b>systems.</b> <b>Computer</b> <b>vision</b> <b>systems</b> use primarily image data to interpret the surrounding world, e. g. to detect, classify and track objects. The performance of mobile <b>computer</b> <b>vision</b> <b>systems</b> acting in unknown environments is inherently difficult to evaluate since, often, obtaining ground truth data is problematic. The proposed novel framework exploits the possibility to add virtual agents into a real data sequence collected in an unknown environment, thus {{making it possible to}} efficiently create augmented data sequences, including ground truth, to be used for performance evaluation. Varying the content in the data sequence by adding different virtual agents is straightforward, making the proposed framework very flexible. The method has been implemented and tested on a pedestrian detection system used for automotive collision avoidance. Preliminary results show that the method has potential to replace and complement physical testing, for instance by creating collision scenarios, which are difficult to test in reality...|$|R
40|$|In {{this paper}} state-of-art of <b>computer</b> <b>vision</b> <b>systems</b> and algorythms is presented. The algorythms {{advantages}} and disadvatages, solved and unsolved tasks are discribed. The topics {{of the future}} work are circumscribed. ? ?????? ???????? ????? ??????????? ?????? ? ?????????? ???????????? ??????, ??????? ?? ??????? ? ?????? ???????, ???????? ? ?????????? ??????. ???????? ??????????? ??????? ???????????? ? ???? ???????...|$|R
5000|$|L.A.S.E.R. Tag, a <b>computer</b> <b>vision</b> <b>system</b> {{paired with}} a {{projector}} that enables one to write on walls using a high-power laser pointer.|$|E
50|$|The Visual Turing Test (VTT) {{unlike the}} Turing Test has a query engine system which interrogates a <b>computer</b> <b>vision</b> <b>system</b> in the {{presence}} of a human co-ordinator.|$|E
50|$|The feature {{concept is}} very {{general and the}} choice of {{features}} in a particular <b>computer</b> <b>vision</b> <b>system</b> may be highly dependent on the specific problem at hand.|$|E
40|$|Abstract. A fully {{integrated}} development tool for <b>computer</b> <b>vision</b> <b>systems</b> {{has been built}} in the frameworkofthispaper. There are many applications that help the user in the design of such systems, using graphical interfaces and function libraries. Even in some cases, the nal source code can be generated by these applications. This paper goes a step beyond � it allows the development of <b>computer</b> <b>vision</b> <b>systems</b> from a distributed environment. Besides, and as a distinctive characteristic with regard to other similar utilities, the system is able to automatically optimize task scheduling and assignment, depending on the available hardware. ...|$|R
40|$|In this paper, {{we propose}} a Confidence-driven {{architecture}} to control trade-off between precision and latency of <b>computer</b> <b>vision</b> <b>systems</b> dynamically. It {{is important for}} <b>vision</b> <b>systems</b> to achieve data stream interpretation accurately without latency. However, there is a trade-off: a long computation time is require...|$|R
40|$|Central to the {{development}} of <b>computer</b> <b>vision</b> <b>systems</b> is the collection and use of annotated images spanning our visual world. Annotations may include information about the identity, spatial extent, and viewpoint of the objects present in a depicted scene. Such a database is useful for the training and evaluation of <b>computer</b> <b>vision</b> <b>systems.</b> Motivated by the availability of images on the Internet, we introduced a webbased annotation tool that allows online users to label objects and their spatial extent in images. To date, we have collected over 400 000 annotations that span a variety of different scene and object classes. In this paper, we show the contents of th...|$|R
5000|$|... 1980: Steve Mann {{creates the}} first {{wearable}} computer, a <b>computer</b> <b>vision</b> <b>system</b> with text and graphical overlays on a photographically mediated scene. [...] See EyeTap. See Heads Up Display.|$|E
50|$|The {{organization}} of a <b>computer</b> <b>vision</b> <b>system</b> is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system {{of a larger}} design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a <b>computer</b> <b>vision</b> <b>system</b> also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.|$|E
50|$|The {{patented}} technology {{consists of a}} network of (overhead and/or underwater) cameras that are connected to a <b>computer</b> <b>vision</b> <b>system</b> that uses multiple overlapping views (epipolar geometry, fundamental matrix, essential matrix, etc.) to detect motionless bathers {{below the surface of the}} water in swimming pools.|$|E
40|$|This chapter {{describes}} a framework which uses augmentation techniques for performance evaluation of mobile <b>computer</b> <b>vision</b> <b>systems.</b> <b>Computer</b> <b>vision</b> <b>systems</b> use primarily image data {{to interpret the}} surrounding world, e. g. to detect, classify and track objects. The performance of mobile <b>computer</b> <b>vision</b> <b>systems</b> acting in unknown environments is inherently difficult to evaluate since, often, obtaining ground truth data is problematic. The proposed novel framework exploits the possibility to add new agents into a real data sequence collected in an unknown environment, thus {{making it possible to}} efficiently create augmented data sequences, including ground truth, to be used for performance evaluation. Varying the content in the data sequence by adding different agents or changing the behavior of an agent is straightforward, making the proposed framework very flexible. A key driver for using augmentation techniques to address <b>computer</b> <b>vision</b> performance is that the <b>vision</b> <b>system</b> output may be sensitive to the background data content. The method has been implemented and tested on a pedestrian detection system used for automotive collision avoidance. Results show that the method has potential to replace and complement physical testing, for instance by creating collision scenarios, which are difficult to test in reality, in particular in a real traffic environment...|$|R
40|$|The {{evolution}} of perceptual organization in biological vision, and its necessity in advanced <b>computer</b> <b>vision</b> <b>systems,</b> {{arises from the}} characteristic that perception, the extraction of meaning from sensory input, is an intelligent process. This is particularly so for high order organisms and, analogically, for more sophisticated computational models. The role of perceptual organization in <b>computer</b> <b>vision</b> <b>systems</b> is explored. This is done from four vantage points. First, {{a brief history of}} perceptual organization research in both humans and <b>computer</b> <b>vision</b> is offered. Next, a classificatory structure in which to cast perceptual organization research to clarify both the nomenclature and the relationships among the many contributions is proposed. Thirdly, the perceptual organization work in <b>computer</b> <b>vision</b> {{in the context of this}} classificatory structure is reviewed. Finally, the array of computational techniques applied to perceptual organization problems in <b>computer</b> <b>vision</b> is surveyed...|$|R
40|$|This is {{a review}} of the current {{existing}} literature concerning the inspection of fruits and vegetables with the application of <b>computer</b> <b>vision,</b> where the techniques most used to estimate various properties related to quality are analyzed. The objectives of the typical applications of such systems include the classification, quality estimation according to the internal and external characteristics, supervision of fruit processes during storage or the evaluation of experimental treatments. In general, <b>computer</b> <b>vision</b> <b>systems</b> do not only replace manual inspection, but can also improve their skills. In conclusion, <b>computer</b> <b>vision</b> <b>systems</b> are powerful tools for the automatic inspection of fruits and vegetables. In addition, the development of such systems adapted to the food industry is fundamental to achieve competitive advantages...|$|R
5000|$|Digital {{graffiti}} is the act {{of creating}} graffiti art using a <b>computer</b> <b>vision</b> <b>system.</b> Various groups and companies have pioneered digital graffiti since technology advances made it possible. Most notably is the Graffiti Research Lab based in the US with their L.A.S.E.R. Tag system.|$|E
50|$|The AIBO {{has seen}} much {{use as an}} {{inexpensive}} platform for artificial intelligence education and research, because it integrates a <b>computer,</b> <b>vision</b> <b>system,</b> and articulators in a package vastly cheaper than conventional research robots. One focal point for that development has been the Robocup Leagues.|$|E
50|$|In 2005 DARPA Grand Challenge - Adrian {{was on the}} Stanford’s {{winning team}} with Sebastian Thrun, Mike Montemerolo, Gary Bradski and others. Designed a <b>computer</b> <b>vision</b> <b>system</b> {{that contributed to the}} winning of the race. The winning vehicle, called “Stanley” is {{currently}} exhibited in the Smithsonian Institution.|$|E
40|$|Vergauwen M., Pollefeys M., Van Gool L., ''A stereo <b>vision</b> <b>system</b> {{for support}} of {{planetary}} surface exploration'', Lecture notes in computer science, vol. 2095, pp. 298 - 312 (Proceedings second international workshop on <b>computer</b> <b>vision</b> <b>systems</b> - ICVS 2001, July 7 - 8, 2001, Vancouver, Canada). status: publishe...|$|R
5000|$|InspecVision Ltd. - A {{manufacturer}} of <b>computer</b> <b>vision</b> inspection <b>systems</b> ...|$|R
40|$|A fully {{integrated}} development tool for <b>computer</b> <b>vision</b> <b>systems</b> {{has been built}} in the framework of this paper. There are many applications that helps the user in the design of such systems, using graphical interfaces and function libraries. Even in some cases, the find source code can be generated by these applications. This paper goes a step beyond;it allows the development of <b>computer</b> <b>vision</b> <b>systems</b> from a distributed environment. Besides, and as distinctive characteristic with regard to other similar utilities, the system is able to automatically optimize task scheduling and assignment, depending on the available hardware. This work was supported by the Spanish Government through CICYT in the framework of project TAP- 96 - 0629 -C 04 - 01...|$|R
50|$|YrWall is a Digital Graffiti Wall {{developed}} by event company Luma, where designs are created {{on a large}} wall using a modified spray paint can. The can contains no paint, instead it has an IR light which is tracked by a <b>computer</b> <b>vision</b> <b>system</b> and the image immediately back-projected onto the wall.|$|E
50|$|A {{more recent}} {{development}} is the binocular <b>computer</b> <b>vision</b> <b>system,</b> such as that introduced to the US market in model year 2013 by Subaru. These systems have front-facing video cameras mounted {{on either side of}} the rear view mirror and use digital processing to extract depth information from the parallax between the two cameras' views.|$|E
50|$|Areas of {{artificial}} intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed {{understanding of these}} environments is required to navigate through them. Information about the environment {{could be provided by}} a <b>computer</b> <b>vision</b> <b>system,</b> acting as a vision sensor and providing high-level information about the environment and the robot.|$|E
40|$|Abstract. Progress in {{statistical}} {{learning in}} recent years has enabled comput-ers to recognize objects with near-human ability. However, recent studies have revealed particular drawbacks in current <b>computer</b> <b>vision</b> <b>systems</b> which sug-gest there exist considerable differences between the way these systems function compared with human visual cognition. Major differences are that: 1) current <b>computer</b> <b>vision</b> <b>systems</b> learn high-level notions directly from the low-level fea-ture space, which makes them sensitive to low-level characteristics changing. 2) typical <b>computer</b> <b>vision</b> <b>systems</b> learn visual concepts discriminatively instead of encoding the knowledge necessary to produce a visual representation of the class. In this paper, we introduce a framework referred as Logical Vision which is demonstrated on learning visual concepts constructively and symbolically. It first constructively extracts logical facts of mid-level features, then generative Meta-Interpretive Learning technique is applied to learn high-level notions because it is capable of learning recursions, inventing predicates and so on. Owing to its symbolic representation paradigm, in our implementation, Logical Vision is fully implemented in Prolog apart from low-level image feature extraction primitives. Experiments are conducted on learning shapes (e. g. triangles, quadrilaterals, etc.), regular polygons and right-angle triangles. These demonstrates that learning vi-sual concepts constructively and symbolically is effective. ...|$|R
25|$|As a {{scientific}} discipline, <b>computer</b> <b>vision</b> {{is concerned with}} the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, <b>computer</b> <b>vision</b> seeks to apply its theories and models for the construction of <b>computer</b> <b>vision</b> <b>systems.</b>|$|R
40|$|As <b>computer</b> <b>vision</b> based <b>systems</b> like lane {{tracking}}, face {{tracking and}} obstacle detection mature an enhanced range of driver assistance systems are becoming feasible. This paper introduces {{a list of}} core competencies required for a driver assistance system, the issue of building in robustness is highlighted in contrast to leaving such considerations to a later product development phase. We then demonstrate how these issues may be addressed in driver assistance systems based primarily on <b>computer</b> <b>vision.</b> The underlying <b>computer</b> <b>vision</b> <b>systems</b> are discussed followed by {{an example of a}} driver support application for lane keeping based on force-feedback through the steering wheel. ...|$|R
