10000|7328|Public
5000|$|Normality test using Jarque-Bera test, Shapiro-Wilk test, and <b>Chi-square</b> <b>test</b> methods.|$|E
5000|$|<b>Chi-Square</b> <b>test</b> for {{exploring}} significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.|$|E
50|$|Common {{statistical}} misconceptions {{are challenged}} by the interface. For example, users can perform a <b>chi-square</b> <b>test</b> on a two-by-two table, but they are asked whether the data are from a cohort (perspective) or case-control (retrospective) study before delivering the result. Both processes produce a <b>chi-square</b> <b>test</b> result but more emphasis is put on the appropriate statistic for the inference, which is the odds ratio for retrospective studies and relative risk for prospective studies.|$|E
50|$|Just as the Wilson {{interval}} mirrors Pearson's <b>chi-squared</b> <b>test,</b> the Wilson interval with continuity correction {{mirrors the}} equivalent Yates' <b>chi-squared</b> <b>test.</b>|$|R
5000|$|For {{samples of}} a {{reasonable}} size, the G-test and the <b>chi-squared</b> <b>test</b> {{will lead to}} the same conclusions. However, the approximation to the theoretical chi-squared distribution for the G-test is better than for the Pearson's <b>chi-squared</b> <b>test.</b> In cases where [...] for some cell case the G-test is always better than the <b>chi-squared</b> <b>test.</b>|$|R
50|$|Cochran-Mantel-Haenszel <b>chi-squared</b> <b>test.</b>|$|R
5000|$|Expected cell frequencies: The {{expected}} cell {{frequencies of}} a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, {{the validity of}} the <b>chi-square</b> <b>test</b> is violated.|$|E
50|$|Raju, N. S., Drasgow, F., & Slinde, J. A. (1993). An {{empirical}} {{comparison of}} the area methods, lord&#39;s <b>chi-square</b> <b>test,</b> and the mantel-haenszel technique for assessing differential item functioning. Educational and Psychological Measurement, 53(2), 301-314.|$|E
50|$|The <b>chi-square</b> <b>test</b> uses as its {{criterion}} the sum, over predefined groups, of the squared {{difference between}} the increases of the empirical distribution and the estimated distribution, weighted by {{the increase in the}} estimate for that group.|$|E
5000|$|Goodness of fit: Kolmogorov-Smirnov <b>test,</b> <b>chi-squared</b> <b>test,</b> Shapiro-Wilk test, Lilliefors test, Anderson-Darling test, Cramér-von Mises {{statistic}} ...|$|R
5000|$|A <b>chi-squared</b> <b>test,</b> {{also written}} as [...] test, is any {{statistical}} hypothesis test wherein the sampling {{distribution of the}} test statistic is a chi-squared distribution when the null hypothesis is true. Without other qualification, 'chi-squared test' often is used as short for Pearson's <b>chi-squared</b> <b>test.</b>|$|R
40|$|A location-dispersion test f o r 2 x k contingency, tables is proposed. The {{asymptotic}} {{distributions of}} the proposed test are obtained both under the null and alternative hypotheses, and also its power and efficiency are studied. The proposed test is compared with several other <b>chi-squared</b> <b>tests</b> by Monte Carlo studies and it is shown that the test is superior to Pearson 2 ̆ 7 s <b>chi-squared</b> <b>test,</b> Nair 2 ̆ 7 s location and dispersion tests and to the cumulative <b>chi-squared</b> <b>test</b> in many cases...|$|R
50|$|Statistics is an {{analytical}} program performing {{many of the}} statistical calaculations ordinally found in FORTRAN driven SPSS programs of the time. Calculations performed by the program included mean, variance, standard deviation, Pearson correlation, normal distribution, <b>Chi-square</b> <b>test,</b> and T-Test.|$|E
5000|$|In the output, the [...] "block" [...] line {{relates to}} <b>Chi-Square</b> <b>test</b> {{on the set}} of {{independent}} variables that are tested and included in the model fitting. The [...] "step" [...] line relates to <b>Chi-Square</b> <b>test</b> on the step level while variables included in the model step by step. Note that in the output a step chi-square, {{is the same as the}} block chi-square since they both are testing the same hypothesis that the tested variables enter on this step are non-zero. If you were doing stepwise regression, however, the results would be different. Using forward stepwise selection, researchers divided the variables into two blocks (see METHOD on the syntax following below). LOGISTIC REGRESSION VAR=grade ...|$|E
5000|$|The {{derivation}} of the TDT {{shows that}} one should only use the heterozygous parents (total number b+c).The TDT tests whether the proportions b/(b+c) and c/(b+c) are compatible with probabilities (0.5, 0.5).This hypothesis can be tested using a binomial (asymptotically <b>chi-square)</b> <b>test</b> with one degree of freedom: ...|$|E
30|$|Mortality of O. nubilalis and A. bipunctata {{was tested}} for {{significant}} differences using a two-sided Cochran-Mantel-Haenszel <b>chi-squared</b> <b>test</b> for O. nubilalis bioassay data and a one-sided Cochran-Mantel-Haenszel <b>chi-squared</b> <b>test</b> for the A. bipunctata bioassay data. All analyses {{were carried out}} using the statistics software R [18].|$|R
40|$|The <b>chi-squared</b> <b>test</b> of Markov chain lumpability {{is shown}} to operate {{reliably}} under a corrected derivation of the degrees of freedom. The test is used to screen out lumping schemes that corrupt the Markov property and give rise to higher order dependence. Time series Markov chain Lumpability <b>Chi-squared</b> <b>tests...</b>|$|R
30|$|POSAS {{scores were}} {{compared}} by McNemar’s paired <b>chi-squared</b> <b>test.</b>|$|R
5000|$|The table above {{shows the}} Omnibus Test of Model Coefficients based on <b>Chi-Square</b> <b>test,</b> that {{implies that the}} overall model is {{predictive}} of re-arrest (we’re concerned about row three—“Model”): (4 degrees of freedom) = 41.15, p < [...]001, and the null can be rejected. Testing the null that the Model, or the group of independent variables that are taken together, does not predict the likelihood of being re-arrested. This result means that the model of expecting re-arrestment is more suitable to the data.|$|E
5000|$|Log-linear {{analysis}} {{is a technique}} used in statistics {{to examine the relationship}} between more than two categorical variables. The technique is used for both hypothesis testing and model building. In both these uses, models are tested to find the most parsimonious (i.e., least complex) model that best accounts for the variance in the observed frequencies. (A Pearson's <b>chi-square</b> <b>test</b> could be used instead of log-linear analysis, but that technique only allows for two of the variables to be compared at a time.) ...|$|E
50|$|A {{study in}} genetic {{variants}} that regulate lipid metabolism {{and determine the}} susceptibility to dyslipidemia in Japanese individuals revealed that UBE2Z, together with ZPR1 and Interleukin-6R, may be important loci for hypertriglyceridemia. Moreover, in a GWAS among 2247 Japanese individuals, 29 polymorphisms that were previously identified as susceptible loci for coronary artery disease were investigated to identify a correlation of these loci to chronic kidney disease. This GWAS meta-analysis revealed through a <b>chi-square</b> <b>test</b> that rs46522 on the UBE2Z gene {{was significantly related to}} chronic kidney disease.|$|E
5000|$|... #Subtitle level 2: Example <b>chi-squared</b> <b>test</b> for {{categorical}} data ...|$|R
50|$|Cramér's V - {{a measure}} of {{correlation}} for the <b>chi-squared</b> <b>test.</b>|$|R
5000|$|... #Subtitle level 2: <b>Chi-squared</b> <b>test</b> for {{variance}} {{in a normal}} population ...|$|R
50|$|The {{concept of}} volcano plot can be {{generalized}} to other applications, where the x axis is related to a measure ofthe strength of a statistical signal, and y axis is related to {{a measure of the}} statistical significance of the signal.For example, in a genetic association case-control study, such as Genome-wide association study,a point in a volcano plot represents a single-nucleotide polymorphism.Its x value can be the odds ratio and its y value can be -log10 of the p value from a Chi-square testor a <b>Chi-square</b> <b>test</b> statistic.|$|E
5000|$|There {{must be a}} {{sufficient}} amount of appropriate data available to build a conceptual model and validate a model. Lack of appropriate data is often the reason attempts to validate a model fail. [...] Data should be verified {{to come from a}} reliable source. A typical error is assuming an inappropriate statistical distribution for the data. [...] The assumed statistical model should be tested using goodness of fit tests and other techniques. Examples of goodness of fit tests are the Kolmogorov-Smirnov test and the <b>chi-square</b> <b>test.</b> Any outliers in the data should be checked.|$|E
5000|$|In {{contrast}} to permutation tests, the reference distributions for many popular [...] "classical" [...] statistical tests, {{such as the}} t-test, F-test, z-test, and χ2 test, are obtained from theoretical probability distributions.Fisher's exact test {{is an example of}} a commonly used permutation test for evaluating the association between two dichotomous variables. When sample sizes are very large, the Pearson's <b>chi-square</b> <b>test</b> will give accurate results. For small samples, the chi-square reference distribution cannot be assumed to give a correct description of the probability distribution of the test statistic, and in this situation the use of Fisher's exact test becomes more appropriate.|$|E
5000|$|... #Subtitle level 2: Example: Pearson's <b>chi-squared</b> <b>test</b> versus {{an exact}} test ...|$|R
25|$|Pearson's <b>chi-squared</b> <b>test.</b> A {{hypothesis}} test using normal approximation for discrete data.|$|R
30|$|On {{the basis}} of a <b>chi-squared</b> <b>test</b> for {{equality}} of regression coefficients.|$|R
50|$|The support {{calculation}} {{comes from}} testing a null {{hypothesis that the}} contrast set support is equal across all groups (i.e., that contrast set support is independent of group membership). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If {{there is a difference}} in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a <b>chi-square</b> <b>test</b> comparing the observed frequency count to the expected count.|$|E
5000|$|Columbia University Ph.D. {{candidates}} Bernd Beber and Alexandra Scacco {{have released}} an analysis {{concluding that the}} patterns {{of the last two}} digits of the election results contain characteristics of human manipulation. The province vote totals possess oddities, including too many sevens in the last digits and too few fives, as well as too few non-adjacent ending digits. The authors of the report concluded that the probability of such oddities occurring naturally is only one chance in 200. After their publication, a mathematician noted that the authors had made a computational error, and that the correct probability is lower, 0.13%; this correction was confirmed by Beber and Scacco. [...] However, Zach at AlchemyToday pointed out that the tests suffer from issues of post-hoc test selection and concluded that by Pearson's <b>chi-square</b> <b>test</b> the results did not meet confidence intervals for being non-random.|$|E
5000|$|Tests of Individual Parameters {{shown on}} the [...] "variables in the {{equation}} table", which Wald test (W=(b/sb)2, where b is β estimation and sb is its standard error estimation [...] ) that is testing whether any individual parameter equals zero [...] You can, if you want, do an incremental LR <b>chi-square</b> <b>test.</b> That, in fact, {{is the best way}} to do it, since the Wald test referred to next is biased under certain situations. When parameters are tested separately, by controlling the other parameters, we see that the effects of GPA and PSI are statistically significant, but the effect of TUCE is not. Both have Exp(β) greater than 1, implying that the probability to get [...] "A" [...] grade is greater than getting other grade depends upon the teaching method PSI and a former grade average GPA.|$|E
50|$|Pearson's <b>chi-squared</b> <b>test.</b> A {{hypothesis}} test using normal approximation for discrete data.|$|R
50|$|Similar themes appear when {{comparing}} Fisher's exact <b>test</b> with Pearson's <b>chi-squared</b> <b>test.</b>|$|R
5000|$|<b>Chi-squared</b> <b>test</b> of {{goodness}} of fit of observed data to hypothetical distributions ...|$|R
