8|10000|Public
5000|$|Modern {{members of}} the Lisp {{programming}} language family such as Clojure, Scheme and Common Lisp support macro systems to allow syntactic abstraction. Other programming languages such as Scala also have macros, or very similar metaprogramming features (for example, Haskell has Template Haskell, and OCaml has MetaOCaml). These can allow a programmer to eliminate boilerplate code, abstract away tedious function call sequences, implement new control flow structures, and implement Domain Specific Languages (DSLs), which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the <b>clarity</b> <b>of</b> <b>the</b> <b>code</b> by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to [...] "more traditional" [...] programming languages such as Python, C or Java.|$|E
40|$|Top-down {{backtracking}} language processors {{are highly}} modular, can handle ambiguity, and {{are easy to}} implement with clear and maintainable code. However, a widely-held, and incorrect, view is that top-down processors are in-herently exponential for ambiguous grammars and cannot accommodate left-recursive productions. It {{has been known for}} many years that exponential complexity can be avoided by memoization, and that left-recursive productions can be accommodated through a variety of techniques. However, until now, memoization and techniques for handling left recursion have either been presented independently, or else attempts at their integration have compromised modularity and <b>clarity</b> <b>of</b> <b>the</b> <b>code...</b>|$|E
40|$|Language-processors, {{that are}} {{constructed}} using top-down recursivedescent search with backtracking parsing technique, are highly modular, can handle ambiguity, and are easy to implement with clear and maintainable code. However, a widely-held, and incorrect, view is that topdown processors are inherently exponential for ambiguous grammars and cannot accommodate left-recursive productions. It {{has been known for}} many years that exponential complexity can be avoided by memoization, and that left- recursive productions can be accommodated through a variety of techniques. However, until now, memoization and techniques for handling left-recursion have either been presented independently, or else attempts at their integration have compromised modularity and <b>clarity</b> <b>of</b> <b>the</b> <b>code</b> – this leads {{to the fact that there}} exists no perfect environment for investigating many NLP-related theories. This thesis solves these shortcomings by proposing a new combinator-parsing algorithm, which i...|$|E
30|$|Design {{elements}} of codes, which increase <b>clarity</b> and unambiguity <b>of</b> <b>the</b> <b>code’s</b> prescriptions, increase compliant decision-making.|$|R
50|$|Guttenberg, Jack A., S. Becker., L. Snyder. ANDERSON'S OHIO LAW OF PROFESSIONAL CONDUCT. LexisNexus (2007 ed.), (2009 ed.), (2012 ed. forthcoming) - The {{work is a}} new {{authoritative}} {{source of}} professional conduct in <b>the</b> state <b>of</b> Ohio. On February 1, 2007, <b>the</b> Ohio Rules <b>of</b> Professional Conduct replaced <b>the</b> Ohio <b>Code</b> <b>of</b> Professional Responsibility, a set of standards that governed professional conduct since 1970. In this work, Guttenberg and his co-authors from Cleveland-Marshall College <b>of</b> Law describe <b>the</b> new standards and explain which rules differ from previous standards. While noticing some important features <b>of</b> <b>the</b> new rules, the author reveals how changes in the rules improve <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> former <b>Code's</b> substantive standards. <b>The</b> book covers such issues as unauthorized practice of law; lawyer discipline; conflicts of interest; client confidentiality; responsibility to clients; and responsibilities in adversarial and adjudicative settings.|$|R
40|$|Since the 1993 {{court case}} of Daubert v. Merrell Dow Pharmaceuticals, Inc. <b>the</b> {{subjective}} nature <b>of</b> toolmark comparison {{has been questioned}} by attorneys and law enforcement agencies alike. This {{has led to an}} increased drive to establish objective comparison techniques with known error rates, much like those that DNA analysis is able to provide. This push has created research in which the 3 -D surface profile of two different marks are characterized and the marks 2 ̆ 7 cross-sections are run through a comparative statistical algorithm to acquire a value that is intended to indicate <b>the</b> likelihood <b>of</b> a match between the marks. The aforementioned algorithm has been developed and extensively tested through comparison of evenly striated marks made by screwdrivers. However, this algorithm has yet to be applied to quasi-striated marks such as those made by <b>the</b> shear edge <b>of</b> slip-joint pliers. <b>The</b> results <b>of</b> this algorithm 2 ̆ 7 s application to <b>the</b> surface <b>of</b> copper wire will be presented. Objective mark comparison also extends to comparison of toolmarks made by firearms. In an effort to create objective comparisons, microstamping of firing pins and breech faces has been introduced. This process involves placing unique alphanumeric identifiers surrounded by a radial <b>code</b> on <b>the</b> surface <b>of</b> firing pins, which transfer to the cartridge 2 ̆ 7 s primer upon firing. Three different guns equipped with microstamped firing pins were used to fire 3000 cartridges. These cartridges are evaluated based on <b>the</b> <b>clarity</b> <b>of</b> their alphanumeric transfers and <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> radial <b>code</b> surrounding <b>the</b> alphanumerics...|$|R
40|$|A {{prototype}} scheduler {{for space}} experiments originally programmed in a dialect of LISP {{using some of}} the more traditional techniques of that language, was recast using an object-oriented LISP, Common LISP with Flavors on the Symbolics. This object-structured version was in turn partially implemented in Ada. The Flavors version showed a decided improvement in both speed of execution and readability of code. The recasting into Ada involved various practical problems of implementation as well as certain challenges of reconceptualization in going from one language to the other. Advantages were realized, however, in greater <b>clarity</b> <b>of</b> <b>the</b> <b>code,</b> especially where more standard flow of control was used. This exercise raised issues about the influence of programming language on the design of flexible and sensitive programs such as schedule planners, and called attention to the importance of factors external to the languages themselves such as system embeddedness, hardware context, and programmer practice...|$|E
40|$|We use knot {{count and}} path count metrics to {{identify}} which routines in the Level 1 basic linear algebra subroutines (BLAS) might benefit from code restructuring. We then consider how logical restructuring and the improvements in the facilities available from successive versions of Fortran have allowed us to improve {{the complexity of the}} code as measured by knot count, path count and cyclomatic complexity, and the user interface of one of the identified routines which computes the Euclidean norm of a vector. With these reductions in complexity we hope that we have contributed to improvements in the maintainability and <b>clarity</b> <b>of</b> <b>the</b> <b>code.</b> Software complexity metrics and the control graph are used to quantify and provide a visual guide {{to the quality of the}} software, and the performance of two Fortran code restructuring tools is reported. Finally, we give some indication of the cost of the extra numerical robustness offered by the BLAS routine over the use of new Fortran 90 intrinsic functions...|$|E
40|$|Our {{objective}} is {{the design of}} a face recognition system for its use in controlled environments. For that reason, different tasks will be performed, that chronologically can be stated as: The study of the basis of pattern recognition, and especially the statistical pattern recognition. The study of the main feature selection techniques, focusing in principal component analysis (PCA), and the basic classifiers, as the nearest neighbour and Parzen classifiers. The study of the main combinations techniques, as we will try {{to take advantage of the}} availability of sets of faces to make better-founded decisions The evaluation of the main problems, like pose variation or light changes. The choice and posterior adaptation of these different general tools to our needs, like the use of modifications of the Parzen classifier. The design of the main algorithm, focusing on the differences between the two working modes, identifying (when we try to recognize a person among the different people in our Data collection Cut out faces Normaliza-tionFeature selection PCA / LDA projectionModel selection Represen-tation of the classes UpdatingClassifier selection k-NN ParzenTraining Parameter optimiza-tion Testing Identifica-tion / verifica-tion of groups of faces 14 system) and verifying (when we know a priori the id of the person and we try to assure its identity), and the use of single images or sets of faces. The implementation of the algorithms in C, trying to become familiar with the development environment and considering the possible optimization but maintaining the <b>clarity</b> <b>of</b> <b>the</b> <b>code.</b> The testing of the whole application using faces from different databases and from real data; that is, captured in real scenarios and in non optimal condition...|$|E
40|$|Software {{refactoring}} is {{the process}} by which a sequence of incremental transforma-tions is performed on a body <b>of</b> source <b>code</b> with <b>the</b> aim <b>of</b> improving its <b>clarity</b> and reducing its complexity whilst ensuring that external behavior is preserved. Long-term maintenance is frequently cited as being the most expensive activ-ity in a software system’s life-cycle. Traditional software development ideologies typically dictate that <b>the</b> design <b>of</b> a software system precedes its implementa-tion. However, what usually occurs over <b>the</b> course <b>of</b> software development is that <b>the</b> requirements <b>of</b> <b>the</b> client and the environment in which the system is expected to operate changes. New functionality has to be added, and old ones deemed redundant or unsuitable are removed or modified. <b>The</b> <b>clarity</b> and com-plexity <b>of</b> <b>the</b> <b>code</b> often suffers as an inevitable consequence of this, leading to higher financial and temporal requirements for software maintenance and further evolution. Refactoring {{can be used as a}} preventive maintenance technique to help ameliorate <b>the</b> effects <b>of</b> this software decay as it enables a more continuou...|$|R
40|$|There is {{an ongoing}} need to {{optimize}} construction materials and reduce <b>the</b> size <b>of</b> elements required within <b>the</b> structural systems <b>of</b> high-rise buildings. Minimizing <b>the</b> size <b>of</b> <b>the</b> vertical structural elements, without compromising <b>the</b> economic feasibility <b>of</b> projects and limiting their significant share on tall buildings’ floor plans, is a consistent challenge. <b>The</b> use <b>of</b> composite structural elements, such as combining concrete and steel, along with higher grade materials within each, is a viable solution. Currently, concrete filled tubes (CFT) or concrete filled continuous caissons built-up by welding heavy plates are the common structural solutions. Their main drawbacks include high costs, the need for skilled labor, complex connections, and requiring welding conditions for heavy plates, such as preheating and repairing. Composite megacolumns considered in this research are defined as vertical structural systems {{with more than one}} hot-rolled steel section, longitudinal rebar and ties embedded in concrete, and they are subject to significant vertical loads and secondary bending moments from wind and seismic actions. They are believed to be a convenient solution in terms of structural behavior, cost, and constructability for <b>the</b> design <b>of</b> tall buildings, including towers over 300 meters tall. Although codes and specifications do consider composite structural elements, they do not offer specific provisions on <b>the</b> design <b>of</b> composite sections with two or more encased steel sections (AISC 2010 Specifications for instance). <b>The</b> lack <b>of</b> knowledge on <b>the</b> axial, bending, and shear behavior of composite megacolumns, along with <b>the</b> resulting lack <b>of</b> <b>clarity</b> in <b>the</b> <b>codes,</b> leads to <b>the</b> need for experimental performance tests. These tests, and the resulting findings, suggest a simplified design approach and help develop numerical methods to describe the designs and to validate the results. The laboratory tests took place between February and September 2015 within CABR Laboratories and <b>the</b> Laboratories <b>of</b> Tsinghua University, Beijing. The column specimens’ overall layout and geometry have been based on suggested sections, from MKA and others, of representative full scale composite columns considered for high-rise buildings. Overall dimensions <b>of</b> <b>the</b> representative full scale columns considered for this testing program are 1, 800 by 1, 800 millimeters, with a height of 9 meters at the Lobby level (base <b>of</b> <b>the</b> tower) and 4. 5 meters at the typical floor. The laboratory tests consisted of two sets of tests that attempt to define the axial load and moment (P-M) interaction curves <b>of</b> <b>the</b> representative columns at failure. Static tests were accomplished by applying 0...|$|R
50|$|Codota helps {{developers}} find typical Java code examples {{by analyzing}} millions of code snippets available on {{sites such as}} GitHub and StackOverflow. Codota ranks these examples by criteria such as commonality <b>of</b> <b>the</b> <b>coding</b> patterns, credibility <b>of</b> <b>the</b> origin and <b>clarity</b> <b>of</b> <b>the</b> code.The Codota plugin for the IntelliJ IDEA and Android Studio IDEs allows developers to get code examples for using Java and android APIs without having to leave their editor.|$|R
40|$|We {{present the}} {{development}} of the Adjoint of the Global Eulerian–Lagrangian Coupled Atmospheric (A-GELCA) model that consists of the National Institute for Environmental Studies (NIES) model as an Eulerian three-dimensional transport model (TM), and FLEXPART (FLEXible PARTicle dispersion model) as the Lagrangian Particle Dispersion Model (LPDM). The forward tangent linear and adjoint components of the Eulerian model were constructed directly from the original NIES TM code using an automatic differentiation tool known as TAF (Transformation of Algorithms in Fortran; [URL]), with additional manual pre- and post-processing aimed at improving transparency and <b>clarity</b> <b>of</b> <b>the</b> <b>code</b> and optimizing the performance of the computing, including MPI (Message Passing Interface). The Lagrangian component did not require any code modification, as LPDMs are self-adjoint and track a significant number of particles backward in time in order to calculate the sensitivity of the observations to the neighboring emission areas. The constructed Eulerian adjoint was coupled with the Lagrangian component at a time boundary in the global domain. The simulations presented in this work were performed using the A-GELCA model in forward and adjoint modes. The forward simulation shows that the coupled model improves reproduction of the seasonal cycle and short-term variability of CO 2. Mean bias and standard deviation for five of the six Siberian sites considered decrease roughly by 1 ppm when using the coupled model. The adjoint of the Eulerian model was shown, through several numerical tests, to be very accurate (within machine epsilon with mismatch around to ± 6 e − 14) compared to direct forward sensitivity calculations. The developed adjoint of the coupled model combines the flux conservation and stability of an Eulerian discrete adjoint formulation with the flexibility, accuracy, and high resolution of a Lagrangian backward trajectory formulation. A-GELCA will be incorporated into a variational inversion system designed to optimize surface fluxes of greenhouse gases...|$|E
40|$|In {{this thesis}} we study the {{implementation}} of program transformations at a high abstraction level. We believe this leads to better productivity of the transformation developer. A program transformation system is a computer program which main goal is the transformation of programs. There are several reasons for generating a program from another one. For instance to obtain an optimised version or to improve the <b>clarity</b> <b>of</b> <b>the</b> <b>code</b> for maintenance purposes. The specification of program transformation requires the specification of {{how and when to}} transform. For the specification of how to transform, rewriting is a natural and elegant paradigm to describe modifications to a program. Term rewriting is a computational model based on rewrite rules. A rewrite rule represents a single stepwise modification to a program. The specification of when to transform refers to the identification of all the enabling conditions of a particular transformation to take place. To find all information for activating an optimisation, a program transformation requires the aid of program analysis techniques. How and when to transform are known as program transformation and analysis respectively. Typically this tasks are separately implemented. For {{the implementation of}} program transformations we use Stratego; a domain-specific programming language for the specification of program transformation systems based on strategic rewriting. Stratego differentiates from a pure rewriting system in the use of an strategy (a Stratego program) to control the rewriting process. We focus on the implementation of data-flow optimisations. A data-flow optimisation requires to collect information which is valid on any possible execution path of a program. This information is exploited to perform optimisations to a program. Simple rewrite systems only based on rewrite rules can only access the information available in the term that is being transformed. However data-flow optimisations need to collect information which is located at different points of a program, thus a data-flow optimisation requires context-sensitive information for its realisation. Dynamic rules (a Stratego extension) are designed to overcome this need. Dynamic rules are generated at run-time and can access information available from their definition context. A dynamic rule application allows to access information from a different program point. Our implementation of data-flow optimisations use dynamic rules to provide contex-sensitive information. We have introduced operations on dynamic rules which enabled us to describe data-flow optimisations such as constant-propagation, copy-propagation, common sub-expression elimination and dead code elimination at a high level of abstraction. A nice feature of our implementation is that integrates analysis and transformation into one task. We have defined generic data-flow frameworks for data-flow optimisations. The frameworks capture the similarities of data-flow optimisations and allow to be parameterized with different dynamic rules and strategies. The strategy parameters are applied at certain stages of the transformation to tune a transformation. The generic propagation strategies can combine different analyses and transformations by combining elements from several one-issue transformations. When two or more transformations share specific transformation features, it is possible to define a combination of transformations. Thus, a super-optimiser is presented which combines renaming, constant propagation, copy propagation and common sub-expression elimination...|$|E
50|$|The {{characteristic}} {{above all}} affect <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> liquid output which {{is dependent on}} the volumetric throughout rate, where a higher flow rate will result in a poor liquid clarity. Another characteristic that influences <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> liquid output is the differential speed. A low differential speed results in a better clarity therefore aiding in the separation process. The G-Force also plays a role in <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> liquid discharge. Higher G-force results in an increase in <b>the</b> separation <b>of</b> <b>the</b> solid particles from the liquid and yields a better clarity.|$|R
50|$|<b>The</b> {{quality and}} <b>clarity</b> <b>of</b> <b>the</b> film sound is notable.|$|R
5000|$|Clear Creek was {{so named}} on account <b>of</b> <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> water it contains.|$|R
5000|$|Characteristic <b>of</b> Mantegna is <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> surface, <b>the</b> {{precision}} <b>of</b> an [...] "archaeological" [...] reproduction <b>of</b> <b>the</b> architectonical details, and <b>the</b> elegance <b>of</b> <b>the</b> martyr's posture.|$|R
5000|$|References and {{citations}} {{for which}} any instructions {{in the content}} maybe required to fulfill <b>the</b> traceability and <b>clarity</b> <b>of</b> <b>the</b> document ...|$|R
50|$|Power's {{ability to}} produce music that mimicked <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> sampled {{recordings}} was highly valued by producers within the Native Tongues.|$|R
50|$|The {{event was}} also {{made into a}} Storm Stories episode, and is notable for <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> video {{coverage}} it received.|$|R
25|$|If {{a stereo}} {{binocular}} microscope has been used, a final assessment using a 10x loupe is performed {{before the final}} judgment is made on <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> stone. The grader first decides <b>the</b> <b>clarity</b> category <b>of</b> <b>the</b> diamond: none (FL, or IF for a blemish), minute (VVS), minor (VS), noticeable (SI), or obvious (I). The decision is then made on <b>the</b> grade <b>of</b> <b>the</b> diamond.|$|R
30|$|The {{authors would}} like to thank the {{anonymous}} reviewers for their careful work and comments that helped to improve <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> paper.|$|R
3000|$|... [3]. Additionally, the {{available}} CT component with high-resolution capabilities can provide additional diagnostic <b>clarity</b> <b>of</b> <b>the</b> metabolically active pathology and may detect additional, non-active pathologies.|$|R
5000|$|In clarification, {{the total}} solids {{recovered}} in the solid cake measure <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> effluent indirectly. This indirect relationship {{is shown in}} Equation 7, ...|$|R
50|$|It was amended July 14, 2009. The changes improve <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> wording <b>of</b> <b>the</b> bill, but do {{not change}} <b>the</b> meaning <b>of</b> <b>the</b> bill.|$|R
5000|$|Prior {{to being}} elected to public office, Fowler {{was an avid}} {{fisherman}} who would wade into the Patuxent River and make note <b>of</b> <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> water. After noticing <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> water slowly diminishing, Fowler chose to run for Calvert County Commissioner in 1970 and make <b>the</b> health <b>of</b> <b>the</b> Patuxent River a key issue. After serving over a decade as county commissioner, Fowler {{was elected to the}} Maryland Senate, where he remained in until his retirement from public office in the mid 1990s.|$|R
5000|$|The bonus {{tracks were}} removed from the 2006 remastered reissues, because, {{according}} to the band, they didn't honour <b>the</b> [...] "conceptual <b>clarity</b> <b>of</b> <b>the</b> original statements".|$|R
50|$|Glass fusion {{is rarely}} used in art therapy. However, Somer and Somer suggest that {{properties}} of glass provide metaphors {{useful in the}} therapy process, including <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> glass, <b>the</b> act <b>of</b> breaking, and reflections seen in glass.|$|R
40|$|<b>The</b> purpose <b>of</b> <b>the</b> {{research}} is to identify which aspects <b>of</b> <b>the</b> communication skills <b>of</b> <b>the</b> postgraduate student <b>of</b> Primary Education Study Program {{that needs to be}} improved. <b>The</b> subjects <b>of</b> <b>the</b> research were 30 students of Postgraduate Student of Primary Education Study Program, State University of Yogyakarta, Indonesia, who took Mathematics course from September to December, 2014. The instruments used to collect the data were in form of Mathematics questions/problems. There were 3 times tests over a period of September – December 2014, each consists of 3 questions. There were 4 aspects that were assessed, namely: (1) the accuracy, coherently, and <b>clarity</b> <b>of</b> <b>the</b> reasons used in answering the question; (2) <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> drawing/illustrations used; (3) The appropriateness and <b>the</b> completeness <b>of</b> mathematical models/equations used, and (4) <b>the</b> accuracy <b>of</b> <b>the</b> phrase used in answering the question. This research shows: (1) The mathematical communication skills <b>of</b> <b>the</b> postgraduate student <b>of</b> Primary Education Study Program, Yogyakarta State University, who were <b>the</b> subject <b>of</b> this research are classified as "medium"; (2) The accuracy, coherently, and <b>clarity</b> <b>of</b> <b>the</b> reasons used in answering the question and <b>the</b> accuracy <b>of</b> <b>the</b> phrase used in answering the question tend to be in <b>the</b> category <b>of</b> "medium" constantly; (3) <b>The</b> <b>clarity</b> <b>of</b> <b>the</b> drawing/illustrations used and the appropriateness and <b>the</b> completeness <b>of</b> mathematical models/equations used has a tendency {{to be included in the}} high category. Thus, aspects that still need to be improved mainly are: (1)) The accuracy, coherently, and <b>clarity</b> <b>of</b> <b>the</b> reasons used in answering the question; and (2) <b>The</b> accuracy <b>of</b> <b>the</b> phrase used in answering the question. Key words: communication, mathematics, postgraduate, primary, educatio...|$|R
50|$|This is a {{stunning}} project. <b>The</b> <b>clarity</b> <b>of</b> <b>the</b> building scheme {{and the way}} it relates to the surrounding context are impressive in a modern civic landmark.|$|R
5000|$|Blue Shawnee Creek {{is named}} for <b>the</b> main stream <b>of</b> which this creek is a branch, and [...] "blue" [...] {{indicates}} <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> water, reflecting the sky.|$|R
5000|$|Though stealth {{technology}} is declared to be invisible to radar, all officially disclosed applications <b>of</b> <b>the</b> technology can only reduce <b>the</b> size and/or <b>clarity</b> <b>of</b> <b>the</b> signature detected by radar.|$|R
30|$|<b>The</b> imaging {{geometry}} <b>of</b> missile-borne circular-scanning SAR {{is illustrated}} in Figure 1. Symbols are listed as follows. For <b>clarity</b> <b>of</b> <b>the</b> illustration, some symbols are not labeled in Figure 1.|$|R
25|$|In <b>the</b> summer <b>of</b> 2008, <b>the</b> show ran a shortened, {{modified}} version {{in order for}} the Earth Globe to be refurbished. The refurbishment was to install a new LED video system, improving <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> video. <b>The</b> content <b>of</b> <b>the</b> video was not changed.|$|R
30|$|The {{turbidity}} {{could be}} determined by <b>the</b> <b>clarity</b> <b>of</b> <b>the</b> solution after mixing 0.5  g each <b>of</b> <b>the</b> control and test samples in 20  mL of deionized water (Snowden et al. 2012).|$|R
