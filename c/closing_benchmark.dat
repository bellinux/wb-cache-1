0|83|Public
2500|$|In the {{framework}} of “Positive Agenda”, Working Groups were established on 8 chapters (“3-Right of Establishment and Freedom to Provide Services”, “6-Company Law”, “10-Information Society and Media”, “18-Statistics”, “23-Judiciary and Fundamental Rights”, “24-Justice, Freedom and Security”, “28-Consumer and Health Protection” and “32-Financial Control”). [...] The “Positive Agenda” kick-off meeting was held on 17 May 2012 in Ankara with the participation of Stefan Füle, EU Commissioner for Enlargement and European Neighbourhood Policy. As {{a result of the}} Working Groups meetings held so far, a total of four <b>closing</b> <b>benchmarks</b> were confirmed to have been met by Turkey in three chapters (Company Law, Consumer and Health Protection and Financial Control chapters).|$|R
40|$|After 1979, {{the rate}} at which new firms are listed on the major U. S. stock exchanges {{increases}} sharply, asset growth rates of new lists are high, but their profitability declines and remains low {{for at least five years}} after listing. New lists also become less likely to survive, primarily because of delisting for poor performance. Overall, market prices reflect the volatile dynamics of new list fundamentals. Thus, for the full 1926 to 2000 period and the 1973 to 2000 Nasdaq period, value-weight and equalweight new list returns are <b>close</b> to <b>benchmark</b> returns. For the high action 1980 to 2000 period, equalweight new list returns are low, but value-weight returns are again <b>close</b> to <b>benchmark</b> returns. * Graduate School of Business, University of Chicago (Fama), and Tuck School of Business, Dartmouth College (French). We gratefully acknowledge the helpful comments of Frank Easterbrook, Kenneth Lehn, Jonathan Macey, Richard Roll, Hans Stoll, and seminar participants at UCLA. The market [...] ...|$|R
40|$|Until {{its recent}} end, the Spanish {{construction}} industry enjoyed {{a decade of}} unprecedented growth, absorbing labour and capital from other {{sectors of the economy}} in huge amounts. Using the input [...] output methodology, we compare the picture for different countries in order to seek patterns and identify possible distortions associated with the extraordinary growth of the construction sector in Spain. We find that the sector is plainly oversized in Spain in comparison to its <b>closest</b> <b>benchmarks,</b> namely a group of OECD advanced economies. The available input [...] output data for Spain, covering several years, also provide evidence of a process of 'deformation' of the economy. On this basis, we provide an initial estimate of the impact a hypothetical adjustment in the industry would have on total income and employment in the economy. Copyright The Author 2010. Published by Oxford University Press on behalf of the Cambridge Political Economy Society. All rights reserved., Oxford University Press. ...|$|R
40|$|The {{characterization}} of network community structure has profound implications in several scientific areas. Therefore, testing the algorithms developed {{to establish the}} optimal division of a network into communities is a fundamental problem in the field. We performed here a highly detailed evaluation of community detection algorithms, which has two main novelties: 1) using complex <b>closed</b> <b>benchmarks,</b> which provide precise ways to assess whether the solutions generated by the algorithms are optimal; and, 2) A novel type of analysis, based on hierarchically clustering the solutions suggested by multiple community detection algorithms, which allows to easily visualize how different are those solutions. Surprise, a global parameter that evaluates {{the quality of a}} partition, confirms the power of these analyses. We show that none of the community detection algorithms tested provide consistently optimal results in all networks and that Surprise maximization, obtained by combining multiple algorithms, obtains quasi-optimal performances in these difficult benchmarks. Comment: 13 pages, 8 figures, 1 table. Scientific Reports (in press...|$|R
50|$|Any {{component}} of a computer system can carry an SPI rating, for network or other bandwidth only devices {{it is a simple}} conversion process from bandwidth to SPI. For CPU or other computational devices, a <b>closed,</b> undocumented, <b>benchmark</b> is used. The most common areas of an IP camera server that to use an SPI rating are the CPU, Network card or interface device, and RAID Subsystem.|$|R
40|$|In November 2003, the Term Investment Option (TIO) program {{became an}} {{official}} cash management {{tool of the}} U. S. Treasury Department. Through TIO, the Treasury lends funds to banks for a set number of days at an interest rate determined by a single-rate auction. One reason why the Treasury introduced TIO {{was to try to}} earn a market rate of return on its excess cash balances. This article studies 166 TIO auctions from November 2003 to February 2006 to determine how TIO interest rates have compared with market rates. The author investigates the spread between TIO rates and rates on mortgage-backed-security repos, a <b>close</b> <b>benchmark</b> for TIO rates. He finds that aside from offerings with very short term lengths, the Treasury receives an interest rate on TIO auctions comparable to market rates. He also documents a negative relationship between an auction's size and the spread between TIO and repo rates. Furthermore, the Treasury's announcement and auctioning of funds on the same day does not adversely affect rate spreads, a finding that suggests that banks are indifferent to more advance notice of TIO auctions. Auctions; Government securities; Interest rates; Repurchase agreements...|$|R
50|$|In 1991, IronMind began {{certifying}} people able to <b>close</b> the company’s <b>benchmark</b> Captains of Crush No. 3 gripper; certification on the Captains of Crush No. 4 {{was added}} in 1998 and certification on the Captains of Crush No. 3.5 was added in 2008.|$|R
40|$|Productivity of {{the company}} depends on a leader 2 ̆ 7 s attempt to control staff via two-way communication. Binus University as a {{university}} that relies on IT-based communication has several advantages: cost, distance, and time reduced. However {{there are also a}} few problems, such as unreadable nonverbal communication and lack of formality and respect between subordinates and superiors. This study aims to determine vertical and horizontal communication in Binus University which form leadership style based on company local wisdom so that the company may become an institution of higher education that counts. In addition, study aims to determine how corporate culture may establish the company local wisdom in Binus University. Study used descriptive-qualitative method by doing interview and observation to some key informants. Results show that Binus University implements vertical and horizontal flow of information for communication openly and equally within the university. Leadership in Binus always embeds culture that becomes values of Binusian which are composed of 5 points: trust in God, sense of belonging, sense of <b>closing,</b> <b>benchmarking,</b> and continuous improvement in every meeting between leaders and employees. Binus University has also implemented three components forming company local wisdom, namely transformational leadership, culture and corporate structure, and effective knowledge transfer method...|$|R
40|$|We {{discuss the}} {{calculation}} of the double occupancy using Dynamical Mean-Field Theory (DMFT) in finite dimensions. The double occupancy can be determined from the susceptibility of the auxiliary impurity model or from the lattice susceptibility. The former method underestimates, whereas the latter overestimates the double occupancy. We illustrate this for the square-lattice Hubbard model. We propose an approach for which both methods lead to identical results by construction and which resolves this ambiguity. This self-consistent dual boson scheme results in a double occupancy that is numerically <b>close</b> to <b>benchmarks</b> available in the literature. Comment: 5 pages, 5 figure...|$|R
40|$|We {{formulate}} the resource-constrained {{project scheduling}} problem as a satisfiabil-ity problem and adapt a satisfiability solver {{for the specific}} domain of the problem. Our solver is lightweight and shows good performance both in finding feasible solutions and in proving lower bounds. Our numerical tests allowed us to <b>close</b> several <b>benchmark</b> instances of the RCPSP {{that have never been}} closed before by proving tighter lower bounds and by finding better feasible solutions. Using our method we solve optimally more instances of medium and large size from the benchmark library PSPLIB and do it faster compared to any other existing solver...|$|R
50|$|Metro's {{performance}} {{improved in}} 2011, exceeding performance benchmarks for six consecutive months from June to November {{the first time}} this had been achieved since December 2008. Since April 2012, the punctuality figures have been consistently outperforming the benchmark, while the delivery figures have either exceeded or were very <b>close</b> to the <b>benchmark</b> throughout 2012 and 2013.|$|R
40|$|In this paper, we model a {{developing}} economy in which individual decisions about education and migration are constrained by capital market imperfections (liquidity constraints). We examine the joint impact of brain drain and international remittances on human capital accumulation in the emigration country. We derive the condition {{under which the}} emigration {{of the most talented}} workers stimulates the economy-wide average stock of human capital in the sending country (compared to the <b>closed</b> economy <b>benchmark).</b> Such a BBD outcome (beneficial brain drain) is obtained (i) when the return to education is high compared to the costs of education and migration and (ii) when remittances received by each young are important. Unlike recent papers in that literature, the BBD cannot be obtained if emigration rates are small. info:eu-repo/semantics/publishe...|$|R
40|$|A {{suite of}} 86 {{criticality}} benchmarks has been recently implemented in MCNP{trademark} {{as part of}} the nuclear data validation effort. These benchmarks have been run using two sets of MCNP continuous-energy neutron data: ENDF/B-VI based data through Release 2 (ENDF 60) and the ENDF/B-V based data. New evaluations were completed for ENDF/B-VI for a number of the important nuclides such as the isotopes of H, Be, C, N, O, Fe, Ni, {sup 235, 238 }U, {sup 237 }Np, and {sup 239, 240 }Pu. When examining the results of these calculations for the five manor categories of {sup 233 }U, intermediate-enriched {sup 235 }U (IEU), highly enriched {sup 235 }U (HEU), {sup 239 }Pu, and mixed metal assembles, we find the following: (1) The new evaluations for {sup 9 }Be, {sup 12 }C, and {sup 14 }N show no net effect on k{sub eff}; (2) There is a consistent decrease in k{sub eff} for all of the solution assemblies for ENDF/B-VI due to {sup 1 }H and {sup 16 }O, moving k{sub eff} further from the benchmark value for uranium solutions and <b>closer</b> to the <b>benchmark</b> value for plutonium solutions; (3) k{sub eff} decreased for the ENDF/B-VI Fe isotopic data, moving the calculated k{sub eff} further from the benchmark value; (4) k{sub eff} decreased for the ENDF/B-VI Ni isotopic data, moving the calculated k{sub eff} <b>closer</b> to the <b>benchmark</b> value; (5) The W data remained unchanged and tended to calculate slightly higher than the benchmark values; (6) For metal uranium systems, the ENDF/B-VI data for {sup 235 }U tends to decrease k{sub eff} while the {sup 238 }U data tends to increase k{sub eff}. The net result depends on the energy spectrum and material specifications for the particular assembly; (7) For more intermediate-energy systems, the changes in the {sup 235, 238 }U evaluations tend to increase k{sub eff}. For the mixed graphite and normal uranium-reflected assembly, a large increase in k{sub eff} due to changes in the {sup 238 }U evaluation moved the calculated k{sub eff} much <b>closer</b> to the <b>benchmark</b> value. (8) There is little change in k{sub eff} for the uranium solutions due to the new {sup 235, 238 }U evaluations; and (9) There is little change in k{sub eff} for the {sup 239 }Pu metal assemblies, but a decrease in k{sub eff} for the solution assemblies, moving them <b>closer</b> to the <b>benchmark</b> value...|$|R
3000|$|The {{following}} observations {{can be made}} in Figure 9. First, the experimental, simulation, and model-generated {{analytical results}} closely match across all protocols. Second, as a general trend the delay performance improves with the amount of knowledge leveraged on topological locality. Both PRMPL and DVRPLC achieve significantly better delay compared to the other protocols and very <b>close</b> to BSDBR <b>benchmark</b> delay, because they are able to capture multi-scale topological localities in human postural movements using the cost parameters [...]...|$|R
40|$|This {{article is}} <b>closed</b> access. <b>Benchmarking</b> is a {{relatively}} new quality concept. It is the systematic search for best practices that leads to superior performance. This paper explores the definitions, scope, types, and applications of benchmarking. Various literature sources have been examined to establish a common understanding. Benchmarking is seen within the context of total quality management (TQM) as an accelerator toward achieving TQM by learning from others who have demonstrated excellence. Furthermore, this paper argues that it is a necessary tool that exposes internal activities of competitive organizations to external organizations, a competitive potential currently not fully used. A wide range of business successes achieved through benchmarking are cited, especially in manufacturing and service industries. These concepts are briefly discussed {{within the context of the}} construction industry, with the objective of provoking a debate on the applicability of the concept in construction...|$|R
40|$|The {{derivation}} {{of general}} performance benchmarks {{is important in}} the design of highly optimized heat engines and refrigerators. To obtain them, one may model phenomenologically the leading sources of irreversibility ending up with results that are model independent, but limited in scope. Alternatively, one can take a simple physical system realizing a thermodynamic cycle and assess its optimal operation from a complete microscopic description. We follow this approach in order to derive the coefficient of performance at maximum cooling rate for any endoreversible quantum refrigerator. At striking variance with the universality of the optimal efficiency of heat engines, we find that the cooling performance at maximum power is crucially determined by the details of the specific system-bath interaction mechanism. A <b>closed</b> analytical <b>benchmark</b> is found for endoreversible refrigerators weakly coupled to unstructured bosonic heat baths: an ubiquitous case study in quantum thermodynamics...|$|R
40|$|Electronic {{modifications}} within Ru-based olefin metathesis precatalysts {{have provided}} {{a number of new}} complexes with significant differences in reactivity profiles. So far, this aspect has not been studied for neutral 16 VE allenylidenes. The first synthesis of electronically altered complexes of this type is reported. Following the classical dehydration approach (vide infra) modified propargyl alcohols were transformed to the targeted allenylidene systems in the presence of PCy 3 . The catalytic performance was investigated in RCM reaction (ring <b>closing</b> metathesis) of <b>benchmark</b> substrates such as diallyltosylamide ( 6 ) and diethyl diallylmalonate ( 7 ) ...|$|R
40|$|This study {{presents}} an effective method of blindly classifying {{large amounts of}} gene expression data into biologically meaningful groups {{using a combination of}} independent component analysis (ICA) and clustering techniques. Specifically, we show that the genes can be classified blindly into several groups based solely on their expression profiles. These groups have a very <b>close</b> correspondence with <b>benchmarks</b> obtained by studies using domain knowledge. These results suggest that ICA can be a very useful pre-processing tool in blind gene classification, rather than using the resulting sources as the final model profiles. © 2005 IEEE...|$|R
40|$|The {{problem with}} peer review {{today is that}} there is so much {{research}} being produced that there are not enough experts with enough time to peer-review it all. As we look to address this problem, issues of standards and hierarchy remain unsolved. Stevan Harnad wonders whether crowd-sourced peer review could match, exceed, or come <b>close</b> to the <b>benchmark</b> of the current system. He predicts crowdsourcing will indeed be able to provide a supplement to the classical system, hopefully improving efficiency and accuracy, but not a substitute for it...|$|R
40|$|Mutual {{funds are}} key {{contributors}} to the globalization of financial markets {{and one of the}} main sources of capital flows to emerging economies. This study provides an overview of the performance of debt scheme of mutual fund of Reliance, and Birla Sunlife with the help of Sharpe Index after calculating Net Asset Values and Standard Deviation. This study reveals that returns on Debt Schemes are <b>close</b> to <b>Benchmark</b> return (Crisil Composite Debt Fund Index: 4. 34 %) and Risk Free Return: 6 % (average adjusted for last five year). The Sharpe’s Index shows that the return of the selected Debt Fund Schemes is less than even from Risk-free-return rate and the Benchmark Index. Performance of Debt Scheme of Reliance is better than the performance of Debt Scheme of Birla Sunlife on the basis of data studied in this report. The average returns of about 60 % selected schemes are more than the average market return or Benchmark return...|$|R
40|$|Abstract. Multiscale design {{dealing with}} a 2 -scale {{material}} and product system is implemented by employing the probabilistic distribution matching probabilistic analytical target cascading method (PATC-PCE) in this paper. PATC-PCE allows design autonomy at each scale subsystem by formulating the multiscale design system as a multilevel design structure. The probabilistic distribution matching strategy in PATC-PCE can quantify the stochastic interrelated responses accurately enough. Comparative study on a multiscale bracket design problem shows that the results obtained by our method are very <b>close</b> to the <b>benchmark</b> values. PATC-PCE is demonstrated to be highly effective and applicable on multi-scale design...|$|R
40|$|Endowment payouts {{have become}} an {{increasingly}} important component of universities’ revenues in recent decades. We test two leading theories of endowment payouts: (1) universities smooth endowment payouts, or (2) universities use endowments as self-insurance against financial shocks. In contrast to both theories, endowments actively reduce payouts relative to their stated payout policies following negative, but not positive, shocks. This asymmetric behavior is consistent with “endowment hoarding,” especially among endowments with values <b>close</b> to the <b>benchmark</b> value {{at the start of}} the university president’s tenure. We also document the effect of negative endowment shocks on university operations, including personnel cuts. ...|$|R
40|$|This {{paper is}} {{concerned}} with probabilistic analysis of initial member stress in geometrically imperfect regular lattice structures with periodic boundary conditions. Spatial invariance of the corresponding statistical parameters is shown to arise on the Born-von Kármán domains. This allows analytical treatment of the problem, where the parameters of stress distribution are obtained in a <b>closed</b> form. Several <b>benchmark</b> problems with beam- and plate-like lattices are considered, {{and the results are}} verified by the direct Monte–Carlo simulations. Behaviour of the standard deviation as a function of lattice repetitive cell number is investigated, and dependence on the lattice structural redundancy is pointed out...|$|R
40|$|We {{develop a}} model of the {{behavior}} of bidders in simultaneous ascending auctions based on two principles: principle of surplus maximization and principle of bid minimization. These principles lead to models of both price dynamics and equilibration, leading to disequlibrium structural equations {{that can be used for}} estimating bidder values. The intention behind the development of this methodology is to provide an auctioneer a method of extracting information during an auction about possible <b>closing</b> prices. We <b>benchmark</b> the performance of the model with data from experimental auctions and then apply it to the UK UMTS or Third Generation Mobile Auction...|$|R
50|$|When a test {{is started}} from WorldBench, the {{benchmark}} manager begins running the test script. This script {{is a series}} of commands—menu requests, keystrokes, and mouse clicks—that cause the application to step through typical tasks. For example, in Microsoft Office, the script starts by unpacking the version of Office embedded on the WorldBench CD. Then it opens the included test documents, copies and pastes text, changes the documents’ formatting, checks spelling, and undertakes other tasks that are likely to be performed using Office. When Office has finished all the steps in the script, the <b>benchmark</b> <b>closes</b> Office.|$|R
40|$|Abstract Background Total quality {{management}} (TQM) {{has a great}} potential to address quality problems {{in a wide range}} of industries and improve the organizational performance. The growing need to take initiatives by hospitals in countries like India and Iran to improve the service quality and reduce wastage of resources has inspired the authors to develop a survey instrument to measure health care quality and performance in the two countries. Methods Based on the Baldrige health care criteria for performance excellence 2009 - 2010 and the guidelines proposed by the American Hospitals Association for hospitals in pursuit of excellence, compared health care services in three countries. The data are collected from the capital cities and their nearby places in India and Iran. Using ANOVAs, three groups in quality planning and performance have been compared. Result Results showed there is significantly difference between groups and in no case the hospitals from India and Iran are found scoring <b>close</b> to the <b>benchmarks.</b> The average scores of Indian and Iranian hospitals on different constructs of the IHCQPM model are compared with the major results achieved by the recipients of the MBNQ award. Conclusion In no case the hospitals from India and Iran are found scoring <b>close</b> to the <b>benchmarks</b> (Baldrige health care criteria for performance excellence 2009 - 2010 and the guidelines proposed by the American Hospitals Association for hospitals). These results suggested to health care services more attempt to achieve high quality in management and performance. </p...|$|R
40|$|In {{order to}} {{anticipate}} future space shielding requirements, NASA has initiated {{an effort to}} formulate computational methods to simulate radiation effects in space. As part of the program, numerical transport algorithms {{have been developed for}} the deterministic Boltzman equation describing galactic cosmic ray (GCR) interactions with matter. It thus becomes necessary to assess the accuracy of proposed deterministic algorithms. For this reason, analytical benchmark solutions to mathematically tractable galactic cosmic ray equations have recently been obtained. Even though these problems involve simplifying assumptions of the associated physics, they still contain the essential features of the basic transport processes. The solutions obtained are features of the basic transport processes. The solutions obtained are compared to results from numerical algorithms in order to ensure proper coding and to provide a measure of the accuracy of the numerical methods used in the algorithm. For the first time, mathematical methods have been applied to the galactic ion transport (GIT) equations in the straight ahead approximation with constant nuclear properties. The approach utilizes a Laplace transforms inversion yielding a <b>closed</b> form <b>benchmark</b> solution which is also computationally efficient...|$|R
30|$|Based {{on these}} results, the three {{techniques}} {{with the best}} results—WSCS-based, MWSCS-based, and Cluster-WSCS-based—do not have differences which are statistically significant, based on their MAP results. The results also indicate that these three techniques have better MAP results than the benchmark (with statistically significant differences). The CF-Jaccard, CF-Communities, and the Cluster-MWSCS-based techniques have results very <b>close</b> to the <b>benchmark</b> (without statistically significant differences {{when compared to the}} benchmark) but less than the three techniques with the best results—WSCS-based, the MWSCS-based, and Cluster-WSCS-based (with differences statistically significant when compared to these three). The CF-Pearson and CF-Cosine techniques have poor accuracy (with statistically significant differences when compared to all other techniques).|$|R
40|$|We {{consider}} a regulated monopolist that faces {{a continuum of}} markets (e. g., [0, 1]) although a finite number k of prices must be charged. As {{a function of the}} integer k, we provide the optimal profit policy defined as the optimal market segmentation and the discriminatory prices. The social welfare is maximized for three prices, but the result is far below the benchmark, the welfare under the Ramsey price. As this price implies no profit, we study regulatory policies that yield a welfare <b>closer</b> to the <b>benchmark,</b> but compatible with "reasonable " profits. We show that a small amount of price discrimination can substantially enhanced the welfare when the monopolist is subject to an average price constraint...|$|R
50|$|Unfortunately, <b>Benchmark</b> <b>closed</b> {{its doors}} in December 2004, leaving Horan without a label {{throughout}} 2005 and going into the Winter NAMM trade show in January 2006. For several years, however, numerous musicians played shows {{in the lobby of}} the Anaheim Marriott Hotel after the trade show ended each day, and that year, Horan's set was seen by Hans-Peter Wilfer, head of Warwick bass guitars. In a conversation following the show, Wilfer invited Horan (already a Warwick endorsee to the extent of receiving discounts on gear and special assistance on repairs) to play at the Warwick booth during the Musikmesse trade show in Frankfurt am Main, Germany in March 2006...and offered to build him a custom bass.|$|R
40|$|Calculation of noncovalent {{interactions}} in set of complexes containing halogen atom (called Halogensx 10) {{is the main}} subject of this thesis. Wide variety of semiempirical quantum mechanical (SQM) methods (AM 1, RM 1, PM 3, PM 6 and SCC-DFTB augmented with empirical correction for dispersion interaction and hydrogen bonding (D 3 H 4)) have been tested and their accuracy was discussed. The SCC-DFTB-D 3 H 4 method showed the advantage of no artificial behaviour for other types of interactions with exception of halogen bonding which is underestimated. Specific correction for halogen bonding {{was designed as a}} purely empirical fix of serious underestimation of interaction energy in halogen bonded complexes that yields results for validation set very <b>close</b> to <b>benchmark</b> interaction energies with average error is 0. 3 kcal/mol ~ 6 % of the interaction energy. As a benchmark method the CCSD(T) extrapolated to the CBS limit, according to Helgaker's scheme with rather large correlation consistent basis sets has been used. The results presented herein can be used in the rational design of halogenated ligands as well as in research of another halogen enriched compounds...|$|R
40|$|International audienceThis paper {{deals with}} the job-shop {{scheduling}} problem with sequence-dependent setup times. We propose a new method to solve the makespan minimization problem to optimality. The method is based on iterative solving via branch and bound decisional versions of the problem. At each node of the branch and bound tree, constraint propagation algorithms adapted to setup times are performed for domain filtering and feasibility check. Relaxations based on the traveling salesman problem with time windows are also solved to perform additional pruning. The traveling salesman problem is formulated as an elementary shortest path problem with resource constraints and solved through dynamic programming. This method allows to <b>close</b> previously unsolved <b>benchmark</b> instances of the literature and also provides new lower and upper bounds...|$|R
40|$|Ground {{deformation}} occurring on {{the southern}} flank of Mt Etna volcano during the JulyAugust 2001 eruption was monitored by GPS measurements along an EW profile crossing the fissure system. This profile was measured eight times during the eruption, using the 'stop and go' semi-kinematic technique. Horizontal and vertical displacements between GPS surveys are reported for each station. The most significant event is a deformation episode occurring {{during the first week}} of the eruption, between 2527 July. Displacements were measured on <b>benchmarks</b> <b>close</b> to the eruptive fissure and the tensile 1989 fracture. Data inversions for measured displacements were performed using the Okada model. The model shows the narrowing of the 2001 dyke accompanied by a dextral dislocation along an east-dipping fault, parallel to the 1989 fracture...|$|R
40|$|In this paper, we {{consider}} {{the performance of the}} Noisy Gradient Descent Bit Flipping (NGDBF) algorithm under re-decoding of failed frames. NGDBF is a recent algorithm that uses a non-deterministic gradient descent search to decode low-density parity check (LDPC) codes. The proposed re-decode procedure obtains improved performance because the perturbations are independent at each re-decoding phase, therefore increasing the likelihood of successful decoding. We examine the benefits of re-decoding for an LDPC code from the IEEE 802. 3 an standard, and find that {{only a small fraction of}} re-decoded frames are needed to obtain significant performance benefits. When re-decoding is used, the NGDBF performance is very <b>close</b> to a <b>benchmark</b> offset min-sum decoder for the 802. 3 an code. Comment: 4 pages, 8 figure...|$|R
40|$|International audienceHearing Protection Device (HPD) rated {{attenuation}} {{is measured}} using the Real Ear Attenuation at Threshold (REAT) method specified in Standard 4869 - 1. This statistical method assumes optimal fitting and is applied under laboratory conditions {{to predict the}} hearing protector performance for an individual wearer. The rated attenuation is therefore generally higher than that measured in the field. A consequence {{is the emergence of}} commercially available systems, which offer the capability of individual fit testing of hearing protectors in the field to control the attenuation actually received by the wearer. The {{purpose of this paper is}} to assess the suitability of these systems. Three commercially available systems dedicated to earplugs were used under laboratory conditions to assess the performance of pre-formed, foam or custom-molded earplugs for at least 20 test subjects. Results were compared with attenuations for the same group of subjects. Two of these systems ensure mean attenuations <b>close</b> to <b>benchmark</b> values and individual comparisons are acceptable for these systems, although discrepancies with respect to benchmark values can be wide. These systems can therefore be used to validate a choice of hearing protection as long as a large but acceptable safety margin is considered. They are also quick and easy to use, and can contribute to worker training and motivation...|$|R
40|$|Energy and {{macronutrient}} {{intake of}} ultra-endurance runners (UER n= 74; control (CON) n= 12) during a 5 -days 225 km multi-stage ultra-marathon (MSUM) {{in the heat}} (Tmax 32 - 40 ˚C), were determined through dietary recall interview and analysed by dietary analysis software. Body mass (BM) and urinary ketones were determined pre- and post-stage. Recovery, appetite and gastrointestinal symptoms were monitored daily. Pre-stage BM, total daily energy (overall mean: 3348 kcal/day), protein (1. 5 g/kgBM/day), carbohydrate (7. 5 g/kgBM/day) and fat (1. 4 g/kgBM/day) intakes did not differ between stages in UER. CON presented a daily macronutrient profile <b>closer</b> to <b>benchmark</b> recommendations than UER. Carbohydrate intake pre-stage (102 g), during running (24 g/h) and immediately post-stage (1. 7 g/kgBM), and protein intake post-stage (0. 3 g/kgBM) did not differ between stages, and were below benchmark recommendations {{in the majority of}} UER. Post-stage urinary ketones increased in UER as competition progressed (Stage 1 : 16 % vs. Stage 5 : 32 %). Gastrointestinal distresses and appetite suppression were reported by 85 % and 72 % of UER, respectively, along the MSUM. Correlations between subjective symptomology, energy and carbohydrate intakes were observed in UER (P< 0. 05). Sub-optimal macronutrient profile, carbohydrate intake, and recovery nutrition throughout the MSUM suggests energy quantity and quality may be compromised in ultra-runners along competition; indicating that specialised nutritional education may be beneficial in this population...|$|R
