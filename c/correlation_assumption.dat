30|115|Public
30|$|Such an {{approximation}} {{helps to}} represent the ergodic sum rates in (8) with an exact Doppler analysis (without high channel <b>correlation</b> <b>assumption).</b>|$|E
40|$|Some {{temporal}} Bell inequalities are deduced {{under the}} assumption of realism and perfect correlation. No locality condition is needed. When the system is macroscopic, the perfect <b>correlation</b> <b>assumption</b> substitutes the noninvasive measurability hypothesis advanteousgely. The new inequalities are violated quantically. This violation is clearly more severe than the similar violation {{in the case of}} ordinary Bell inequalities. Some microscopic and macroscopic situations in which these inequalities could be tested are considered. Comment: 8 page...|$|E
40|$|The {{method of}} {{generalized}} estimating equations (GEE) {{is a popular}} tool for analysing longitudinal (panel) data. Often, the covariates collected are time-dependent in nature, for example, age, relapse status, monthly income. When using GEE to analyse longitudinal data with time-dependent covariates, crucial assumptions about the covariates are necessary for valid inferences to be drawn. When those assumptions do not hold or cannot be verified, Pepe and Anderson (1994, Communications in Statistics, Simulations and Computation 23, 939 – 951) advocated using an independence working <b>correlation</b> <b>assumption</b> in the GEE model as a robust approach. However, using GEE with the independence <b>correlation</b> <b>assumption</b> may lead to significant efficiency loss (Fitzmaurice, 1995, Biometrics 51, 309 – 317). In this article, we propose a method that extracts additional information from the estimating equations that are excluded by the independence assumption. The method always includes the estimating equations under the independence assumption and the contribution from the remaining estimating equations is weighted according to the likelihood of each equation being a consistent estimating equation and the information it carries. We apply the method to a longitudinal study of {{the health of a}} group of Filipino children...|$|E
3000|$|... [13]. This is {{a method}} for the {{analysis}} of clustered matched-pair data that adjusts for multiple units within a cluster, yet avoids <b>correlation</b> <b>assumptions</b> among and within clusters and also avoids distributional assumptions (modification of the McNemar test). The level of statistical significance was set at p[*]<[*] 0.05.|$|R
40|$|Abstract In this paper, we {{describe}} the <b>correlation</b> <b>assumptions</b> made by different power analysis methods and evaluate {{the impact on the}} accuracy of total power dissipation calculation {{as well as of the}} power dissipated by individual signals. Industrial circuits and applications are used. The results show that some assumptions cause inaccuracies of more than 100 % for certain circuit types. ...|$|R
40|$|In this paper, {{we provide}} a new {{analytical}} model {{for evaluating the}} blocking performance of dynamic lightpath establishment in multifiber wavelength-routed networks. By adopting the simple link-independent model together with the wavelength <b>correlation</b> <b>assumptions,</b> we manage to achieve a good balance between analytical accuracy and computational complexity. Extensive numerical {{results show that the}} proposed model can quickly produce accurate analytical results under different traffic loads and in different networks...|$|R
40|$|Some {{temporal}} Bell inequalities are deduced {{under the}} assumption of realism and perfect correlation. No locality condition is needed. The different measurements on the system refer to three external parameter values. In the case where the measured observable does not commute with the Hamiltonian, they also can refer to three measurement times. When the system is a macroscopic one, the perfect <b>correlation</b> <b>assumption</b> substitutes advantegeously the noninvasive measurability hypothesis. Some microscopic and macroscopic situations where these inequalities coud be tested are considered...|$|E
40|$|The {{conventional}} independence assumption {{made for}} the evolving speech spectra {{is replaced by a}} strong <b>correlation</b> <b>assumption,</b> which then leads to a new stochastic model. This model implements a nonlinear interpolation between the lower and upper bounds of the joint probability distributions. The advantage of the new model over other correlation-based modelling approaches is that it has a low parameter complexity, the same as that in models based on the independenceassumption. Experiments on a speaker-independent E-set database show the effectiveness of this new modelling approach...|$|E
40|$|In {{this paper}} I follow Abel and Kotlikoff 1994 {{non-parametric}} approach based on consumption cohort data {{to test for}} intergenerational altruism among Italian households. The Italian socio-economic framework represents an interesting ground to test for the Barro’s 1974 model given the stronger family linkages usually present among Italian households. All tests reject altruism. Further, I evaluate how restrictive is the assumption of a zero correlation between the clan’s Euler errors and the demographic structure of the clan. I find {{no evidence of any}} major role played by the age composition of the clan and conclude that the zero <b>correlation</b> <b>assumption</b> is reasonable. ...|$|E
40|$|We {{analyze the}} Solvency II {{standard}} formula (SF) for capital risk aggregation {{in relation to}} the treatment of operational risk (OR) capital. We show that the SF implicitly assumes that the correlation between OR and the other risks is very high: a situation that seems to be at odds with both the empirical evidence and the view of most industry participants. We also show that this formula, which somehow obscures the <b>correlation</b> <b>assumptions,</b> gives different insurance companies different benefits for diversification effects in relation to OR. Unfortunately, these benefits are based on the relative weights of the six basic capital components and not on any risk-related metric. Hence, contrary to what has been claimed, the SF does give diversification benefits (although minor ones) in relation to OR. Further, since the SF does not treat the correlation between OR and the other risks explicitly, it provides no incentive to gather data regarding this effect. Given all these considerations, for the time being, we recommend the adoption of the well-known linear aggregation formula, using low-to-moderate <b>correlation</b> <b>assumptions</b> between OR and the other risk...|$|R
40|$|Abstract — In this paper, {{we provide}} a new {{analytical}} model {{for evaluating the}} blocking performance of dynamic lightpath establishment in multifiber wavelength-routed networks. By adopting the simple link-independent model together with the wavelength <b>correlation</b> <b>assumptions,</b> we manage to achieve a good balance between analytical accuracy and computational complexity. Extensive numerical {{results show that the}} proposed model can quickly produce accurate analytical results under different traffic loads and in different networks. I...|$|R
40|$|Abstract. We discuss {{rationales}} for kinetic descriptions of traffic dynamics and present {{a class of}} new models of Vlasov-Fokker-Planck type. These models incorporate (nonlocal and time-delayed) braking and acceleration terms which are consistent with realistic time scales. <b>Correlation</b> <b>assumptions</b> are made such that braking and acceleration terms depend only on macroscopic densities and the relative speeds {{with respect to the}} average speed; the braking term includes lane change probabilities, and reasonable assumptions on the dependencies of these probabilities on the traffic situation lead to multivalued fundamental diagrams, consistent with traffic observations...|$|R
40|$|Eyeglass {{is a kind}} {{of common}} interfering factor in face {{recognition}} and analysis. A statistical learning method is presented to remove eyeglass from an input facial image for this problem. First, training samples are collected from facial images wearing eyeglasses and their corresponding facial images without eyeglasses. Then, a model of multi-variable linear regression is established based on linear <b>correlation</b> <b>assumption</b> between two sets of samples. Finally, the parameters matrix is solved by which the eyeglasses can be removed from an input facial image. Experimental results have demonstrated that the proposed algorithm is efficient and practical. The method is easy to be realized without any auxiliary equipment...|$|E
40|$|We study Principal Component Analysis (PCA) in {{a setting}} where {{a part of the}} corrupting noise is data-dependent and, as a result, the noise and the true data are correlated. Under a bounded-ness {{assumption}} on the true data and the noise, and a simple assumption on data-noise correlation, we obtain a nearly optimal sample complexity bound for the most commonly used PCA solution, singular value decomposition (SVD). This bound is a significant improvement over the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where this "correlated-PCA" problem was first studied; and it holds under a significantly weaker data-noise <b>correlation</b> <b>assumption</b> than the one used for this earlier result. Comment: made significant changes to paper. The completed new version is available at [URL]...|$|E
40|$|We {{formulate}} {{the problem}} of estimating the state in real-time of multiple continuous-time linear dynamical systems over a shared Additive White Gaussian Noise (AWGN) channel without channel feedback and characterize the optimal encoders and decoder for minimum mean-squared error (MMSE) estimation. The first analyses are for a a single linear system being observed over an AWGN channel with a power constraint. One optimal encoder is a scaled innovation encoder and the decoder a scaling of the channel output. We then study the same problem for multiple linear systems communicating over a shared AWGN channel and characterize the optimal encoders and decoder. Finally the encoders and decoder are characterized in closed form for two identical linear sources with some correlation. We bound the optimal costs without the <b>correlation</b> <b>assumption.</b> I...|$|E
40|$|We {{consider}} large random matrices with {{a general}} slowly decaying correlation among its entries. We prove universality of the local eigenvalue statistics and optimal local laws for the resolvent away from the spectral edges, generalizing the recent result of [arXiv: 1604. 08188] to allow slow correlation decay and arbitrary expectation. The main novel tool is a systematic diagrammatic control of a multivariate cumulant expansion. Comment: Updated version with further relaxed <b>correlation</b> <b>assumptions</b> (finite order power law decay), relaxed conditions outside the spectrum, and notes relating pre-cumulants to Wick polynomials. 38 pages, 1 figur...|$|R
40|$|The {{market for}} {{derivatives}} such as first-to-default baskets and CDO tranches on portfolios of corporate credit exposures (bonds, loans, default swaps, etc.) has grown rapidly in recent years. Various models for capturing portfolio correlation effects have been introduced, with Default Time models becoming {{the most widely}} used. While attractive for their relative simplicity and ability, in some cases, to allow fast computation of hedge ratios, there is increasing concern around the limitations and implications of these models. This paper uses simulation to {{study the effects of}} credit migration and <b>correlation</b> <b>assumptions</b> underlying the models for valuation of derivatives on credit portfolios...|$|R
40|$|This paper {{analyses}} {{the impact}} of using different <b>correlation</b> <b>assumptions</b> between lines of business when estimating the risk-based capital reserve, the Solvency Capital Requirement (SCR), under Solvency II regulations. A case study is presented and the SCR is calculated according to the Standard Model approach. Alternatively, the requirement is then calculated using an Internal Model based on a Monte Carlo simulation of the net underwriting result at a one-year horizon, with copulas being used to model the dependence between lines of business. To address {{the impact of}} these model assumptions on the SCR we conduct a sensitivity analysis. We examine changes in the correlation matrix between lines of business and address the choice of copulas. Drawing on aggregate historical data from the Spanish non-life insurance market between 2000 and 2009, we conclude that modifications of the <b>correlation</b> and dependence <b>assumptions</b> {{have a significant impact on}} SCR estimation. Solvency II, Solvency Capital Requirement, Standard Model, Internal Model, Monte Carlo simulation, Copulas JEL classification:C 53...|$|R
40|$|Solvency II Directive {{introduced}} a new framework {{in order to develop}} new risk management practices to manage risk and to define a minimum capital requirement. To this aim, Commission Delegated Regulation provided {{the final version of the}} Standard Formula. Capital requirement is obtained via a modular structure where each source of risk must be first measured and then aggregated under a linear <b>correlation</b> <b>assumption.</b> As results of main Quantitative Impact Studies have shown, Premium and Reserve risks represent a key driver for Non-Life insurers. In this regard, we focus here on the valuation of the capital requirement for this specific sub-module. Some inconsistencies of the approach provided by Solvency II will be highlighted. We show indeed that some assumptions of the Standard Formula may lead to an underestimation of the capital requirement for small insurers...|$|E
40|$|We {{introduce}} {{a method to}} repair an image which has been stamped by an identigram or a watermark. Our method {{is based on the}} cross-channel correlation which assures the co-occurrence of image discontinuities and correlation of color distributions across different color channels of an image. Using blind source separation, we find the transformation of color space which separates the structures of identigram and that of the original image into two different individual color channels. To repair the image contents in the corrupted channel, we formulate the problem using Bayes ’ rule where the prior and the likelihood probabilities are defined based on the cross-channel <b>correlation</b> <b>assumption.</b> We compare our results with results from inpainting and texture synthesis-based hole filling techniques. Our results are pleasable for real-world examples and have the maximum PSNR for synthetic examples. 1...|$|E
40|$|Abstract. In {{this paper}} we extend certain {{correlation}} inequalities for vector-valued Gaussian random variables due to Kolmogorov and Rozanov. The in-equalities {{are applied to}} sequences of Gaussian random variables and Gaussian processes. For sequences of Gaussian random variables satisfying a <b>correlation</b> <b>assumption,</b> we prove a Borel-Cantelli lemma, maximal inequalities and sev-eral laws of large numbers. This extends results of Beśka and Ciesielski and of Hytönen and the author. In {{the second part of}} the paper we consider a certain class of vector-valued Gaussian processes which are α-Hölder continuous in p-th moment. For these processes we obtain Besov regularity of the paths of order α. We also obtain estimates for the moments in the Besov norm. In par-ticular, the results are applied to vector-valued fractional Brownian motions. These results extend earlier work of Ciesielski, Kerkyacharian and Roynette and of Hytönen and the author. 1...|$|E
40|$|The paper {{present the}} main {{customer}} {{reasons for the}} choice to practice rural tourism activities. The survey targeted focused on analysis and opinion on the situation of people {{at a time of}} rural tourism activity overall, assessed primarily through the prsurvey, nonparametric <b>correlation,</b> statistic <b>assumptions...</b>|$|R
40|$|We examine {{consumer}} {{response to}} label information using a hypothetical choice experiment on red leaf lettuce attribute bundles. Using survey responses, several mixed logit models with random parameters and varying <b>correlation</b> <b>assumptions</b> are estimated that provide estimated of marginal utilities (and marginal values) of various attributes related to general health claims, specific nutrition and health claims, certification logos, and certified organic claims (relative {{to the conventional}} reference group) for this fresh produce product. We find that consumers distinguish between labeling claims, and that attribute bundling effects are present, suggesting the results from main effects (linear) models may be misleading. Furthermore, the results imply that consumers may value both privately and publicly appropriable benefits of alternative technologies, such as organic production. Choice experiment, Conditional distribution, Preference heterogeneity, Produce labels, Random parameters, Food Consumption/Nutrition/Food Safety,...|$|R
40|$|Abstract—We have {{recently}} proposed a correlated model to pro-vide a Gaussian mixture {{representation of the}} cardiovascular sig-nals, with promising results in identifying rhythm disturbances. The approach provides a transformation of the data into a set of integrable Gaussians distributed over time. Looking into the model from a new joint modeling perspective, {{it is capable of}} assembling a filtered estimation, and can be used to derive temporal information of the waveforms. In this paper, we present a step-by-step deriva-tion of the joint model putting <b>correlation</b> <b>assumptions</b> together to conclude a minimal joint description for a pair of ECG–ABP signals. We then probe novel applications of this model, includ-ing Kalman filter based denoising and fiducial point detection. In particular, we use the joint model for denoising and employ the de-noised signals for pulse transit time (PTT) estimation. We analyzed more than 70 h of data from 76 patients from the MIMIC databas...|$|R
40|$|This paper {{proposes a}} highly {{consistent}} estimation, the two stage generalized Cochrane-Orcutt transformation (2 SGAR) estimation for cointegration, the spurious regression, fractional cointegration and the fractional spurious regression via AR (k) approximation, eventhough the error terms are unknown in practice. The conver-gent rates of estimators of the spurious regression and the fractional cointegration are improved by using our methodology {{when compared to}} the existing estimating methods. Moreover, relaxing the no <b>correlation</b> <b>assumption</b> of regressors and error terms for above mentioned regressions, our procedure can also result in consistent estimators. We further compare our new estimator to other three estimators, OLS, GLS correction (GLSC) and feasible GLS (FGLS) by Monte Carlo studies. The evidence on simulation shows that the powerfulness and usefulness of our estimation, because our estimation can produce more accurate estimators for the most considered regressions...|$|E
40|$|Abstract — The {{performance}} of hybrid micro-diversity, {{in the form}} of generalized selection combining (GSC), and macrodiversity is presented for lognormal shadowed Rayleigh fading channels. The GSC-augmented macrodiversity consists of K ports in a cell site, each port carrying N microscopic diversity antennas. The macroscopic diversity involves selecting the port with the highest long-term local mean SNR among the K ports, and the GSC uses n strongest signals of the N branch received signals from that port for processing. We derive analytical expressions for error probability and outage for systems employing this hybrid scheme. The expressions are valid for any configurations of K, N, n. In microcell systems substantial correlation could exist among the ports in contrast to the zero <b>correlation</b> <b>assumption</b> of macrocell; results are also shown for correlated lognormal shadowed Rayleigh channels. Extensive simulation is carried out to validate the analytical expressions derived. I...|$|E
40|$|The Author(s) 2009. This {{article is}} {{published}} with open access at Springerlink. com Abstract In this paper we extend certain correlation inequalities for vector-valued Gaussian random variables due to Kolmogorov and Rozanov. The inequalities {{are applied to}} sequences of Gaussian random variables and Gaussian processes. For sequences of Gaussian random variables satisfying a <b>correlation</b> <b>assumption,</b> we prove a Borel-Cantelli lemma, maximal inequalities and several laws of large numbers. This extends results of Beśka and Ciesielski and of Hytönen and the author. In {{the second part of}} the paper we consider a certain class of vector-valued Gaussian processes which are α-Hölder continuous in p-th moment. For these processes we obtain Besov regularity of the paths of order α. We also obtain estimates for the moments in the Besov norm. In particular, the results are applied to vector-valued fractional Brownian motions. These results extend earlier work of Ciesielski, Kerkyacharian and Roynette and of Hytönen and the author...|$|E
40|$|The Vasicek {{single factor}} model of {{portfolio}} credit loss is generalized to include correlated stochastic exposures and loss rates. The new model can accommodate any distribution and <b>correlation</b> <b>assumptions</b> {{for the loss}} and exposure rates of individual credits and will produce a closed-form approximation for an asymptotic portfolio’s conditional loss rate. Revolving exposures are modeled as draws against committed lines of credit. Draw rates and loss rates on defaulted credits are random variables with known probability distributions. Dependence among defaults, individual exposures, and loss rates are modeled using a single common Gaussian factor. A closed-form expression for an asymptotic portfolio’s inverse cumulative conditional loss rate {{is used to calculate}} a portfolio’s unconditional loss rate distribution, estimate economic capital allocations, and analyze portfolio loss rate characteristics. Positive correlation in individual credit exposures and loss rates increases systematic risk. As a consequence, portfolio loss rate distributions exhibit wider ranges and greater skewness...|$|R
30|$|The {{elements}} of this set can be assumed to be isolated time–frequency points as in degenerate unmixing estimation technique (DUET)[15, 26] or to form a time–frequency box as in time–frequency ratio of mixtures (TIFROM)[16] and time–frequency <b>CORRelation</b> (TIFCORR)[27]. <b>Assumption</b> 1 is often reasonable thanks to the sparseness of the time–frequency representation of the sources, especially when this number of sources is moderate.|$|R
40|$|A {{previously}} published guideline for MIMO antenna arrays is refuted. The influence of radiation efficiency on diversity gain and MIMO capacity of wireless communications systems is investigated through simulations and measurements using a reverberation chamber. Integrated antennas on a portable device have efficiencies {{low enough to}} disallow typical inter-element <b>correlation</b> <b>assumptions.</b> Both diversity gain and MIMO capacity depend {{on the number of}} antennas, SNR and efficiency in a complex way. When the efficiency of antennas is considered, certain system capacity losses are predicted and measured. These losses may be recovered through using more receive elements than commonly recommended or through the addition of a smaller number of more efficient antennas. This work {{was supported in part by}} the Fundación Séneca, the R&D unit of the Autonomous Region of Murcia (Spain) under project references TIC-TEC 06 / 01 - 0003, 07 / 02 - 0005 and 05746 /PI/ 07, and in part by the Spanish National R&D Programme through TEC 2007 / 63470 /TCM...|$|R
40|$|The {{dynamics}} of an asymmetric kinetic Ising model is studied. Two schemes {{for improving the}} existing mean-field description are proposed. In the first scheme, we derive the formulas for instantaneous magnetization, equal-time correlation, and time-delayed correlation, considering the correlation between different local fields. To derive the time-delayed correlation, we emphasize that the small <b>correlation</b> <b>assumption</b> adopted in previous work [M. Mézard and J. Sakellariou, J. Stat. Mech., L 07001 (2011) ] is in fact not required. To confirm the inference efficiency of our method, we perform extensive simulations on single instances with either temporally constant external driving fields or sinusoidal external fields. In the second scheme, we develop an improved mean-field theory for instantaneous magnetization prediction utilizing {{the notion of the}} cavity system in conjunction with a perturbative expansion approach. Its efficiency is numerically confirmed by comparison with the existing mean-field theory when partially asymmetric couplings are present. Comment: 16 pages, 5 figures, Journal of Statistical Mechanics: Theory and Experiment (in press...|$|E
40|$|Multi-asset options exhibit {{sensitivity}} to {{the correlations between the}} underlying assets and these correlations are notoriously unstable. Moreover, some of these options such as the digital outperformance options, have a cross-gamma that changes sign depending on the relative evolution of the underlying assets. In this paper, I present a model to price digital outperformance options when there is uncertainty about correlation, but it is assumed to lie within a certain range. Under the assumption that assets prices follow a Geometric Brownian motion with constant instantaneous volatilities I present an analytic expression for the price of the digital outperformance option under the constant <b>correlation</b> <b>assumption,</b> as well as the partial differential equation corresponding to the uncertain correlation model. The comparison of the prices obtained using both models shows that there is no constant correlation which allows attaining the price obtained under the uncertain correlation model. This fact shows that it can be dangerous to assume a constant instantaneous correlation for products with a cross-gamma that changes sign. Digital outperformance options, uncertain correlation, cross-gamma...|$|E
40|$|Variable {{selection}} is a challenging issue in statistical applications {{when the number}} of predictors $p$ far exceeds the number of observations $n$. In this ultra-high dimensional setting, the sure independence screening (SIS) procedure was introduced to significantly reduce the dimensionality by preserving the true model with overwhelming probability, before a refined second stage analysis. However, the aforementioned sure screening property strongly relies {{on the assumption that the}} important variables in the model have large marginal correlations with the response, which rarely holds in reality. To overcome this, we propose a novel and simple screening technique called the high-dimensional ordinary least-squares projection (HOLP). We show that HOLP possesses the sure screening property and gives consistent variable selection without the strong <b>correlation</b> <b>assumption,</b> and has a low computational complexity. A ridge type HOLP procedure is also discussed. Simulation study shows that HOLP performs competitively compared to many other marginal correlation based methods. An application to a mammalian eye disease data illustrates the attractiveness of HOLP. Comment: To appear in JRSS-...|$|E
40|$|This paper {{exploits}} the endogenous default function {{framework of}} Das and Sundaram (2007) {{to develop an}} approach for modeling correlated default on binomial trees usually used for pricing equity options. We show how joint default contracts may be valued on these trees. The model accommodates different <b>correlation</b> <b>assumptions</b> and practical implementation considerations. Credit portfolio characteristics are examined within the model and found {{to be consistent with}} stylized empirics. Risk premia for default are computable and shown to be relatively higher for poor quality firms. Equity volatility is shown to impact correlated credit loss distributions substantially. Two kinds of default dependence are explored, one coming from default intensity correlations, and the other from further conditional dependence in defaults after accounting for intensity correlations (residual copula correlation). Both are found to impact credit loss distributions, though the absence of either makes these distributions less sensitive to correlation assumptions; on balance intensity correlations are more critical...|$|R
40|$|The {{distributed}} fusion {{state estimation}} problem is addressed for sensor network systems with random state transition matrix and random measurement matrices, which provide a unified framework to consider some network-induced random phenomena. The process noise {{and all the}} sensor measurement noises {{are assumed to be}} one-step autocorrelated and different sensor noises are one-step cross-correlated; also, the process noise and each sensor measurement noise are two-step cross-correlated. These <b>correlation</b> <b>assumptions</b> cover many practical situations, where the classical independence hypothesis is not realistic. Using an innovation methodology, local least-squares linear filtering estimators are recursively obtained at each sensor. The distributed fusion method is then used to form the optimal matrix-weighted sum of these local filters according to the mean squared error criterion. A numerical simulation example shows the accuracy of the proposed distributed fusion filtering algorithm and illustrates some of the network-induced stochastic uncertainties that can be dealt with in the current system model, such as sensor gain degradation, missing measurements, and multiplicative noise...|$|R
30|$|In addition, the {{proposed}} correlation-based inference methodology can also act supplementary to any existing or future {{approach that is}} based on <b>correlation</b> <b>assumptions</b> in frequency domain. Studies like [20] and [21] aim at accelerating the overall spectrum sensing procedure by assuming the occupancy of adjacent frequency bins correlated. However, both works do not deal with practical considerations regarding the methodology for obtaining full knowledge on the respective correlation coefficients values. Similar issues hold even in more advanced studies, like [19], where correlated occupancy is manually validated by observation on adjacent bins for a limited frequency range. To shed some more light on these assumptions, our methodology delves into the correlated behavior of neighboring frequency bins in a more educated and realistic manner and, thus, allows for any future work to exploit relevant observations with higher confidence. Additionally, it drives future approaches towards reflecting reality in a more accurate fashion, as well as avoiding assumptions like those made in [22] regarding the absence of any correlation in the occupancy of neighboring frequency bins.|$|R
