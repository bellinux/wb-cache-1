12|48|Public
5000|$|The <b>coincidence</b> <b>factor</b> is the {{reciprocal}} {{of the diversity}} factor. However, differing sources define the simultaneity factor to be identical to either the <b>coincidence</b> <b>factor</b> or the diversity factor. The International Electrotechnical Commission defines the coincidence and simultaneity factors identically with the diversity factor being the [...] Since the only change in definition {{is to take the}} inverse, all one needs to know is if the factor is greater than or less than one.|$|E
40|$|Abstract—In {{this paper}} a {{top-down}} approach to generating load profiles of end-consumers is described. The {{focus is on}} small consumers connected to the low voltage (LV) grid, e. g. a single household. The algorithm is based on either given measurements or synthetically generated bottom-up load profiles {{as well as a}} known average load profile. The data are statistically analysed and described in cumulative probability functions (CDF). By employing random numbers that conform to the CDF, both active and reactive power high resolution load profiles can be generated. The presented method is fast-acting and is therefore appropriate for generating large numbers of load profiles. In contrast to a bottom-up approach, it is also applicable if no or only little information about the consumer is available. Index Terms—top-down load profile generator, household, <b>coincidence</b> <b>factor,</b> stochastic behaviour, statistical analysis. I...|$|E
40|$|Electric {{vehicles}} (EVs) {{are becoming}} a significant element in the electricity system to help meet the environmental targets and can aid the network operator to shift the peak demand in the future. Therefore, {{it is important to}} evaluate the future economic impacts resulting from the increasing EV penetration to the national network. However, the potential contribution of EVs to network investment is difficult to quantify. Although we can measure the reduced capacity during peak demand for a specified low voltage network, it is difficult to quantify the impact at the national level. This paper uses a typical LV network to analyze the investment savings at this voltage level and extrapolates the results to cover the entire UK network by considering the <b>coincidence</b> <b>factor,</b> peak shaving percentage etc. From the demonstration, it can be concluded that semi-urban areas and low voltage networks will have the greatest benefit from a reduction in required investment compared with other areas and voltage levels...|$|E
40|$|A Monte Carlo method {{based on}} the GEANT 4 toolkit has been {{developed}} to correct the full-energy peak (FEP) efficiencies of a high purity germanium (HPGe) detector equipped with a low background shielding system, and moreover evaluated using summing peaks in a numerical way. It is found that the FEP efficiencies of ^ 60 Co, ^ 133 Ba and ^ 152 Eu can be improved up to 18 % by taking the calculated true summing <b>coincidence</b> <b>factors</b> (TSCFs) correction into account. Counts of summing coincidence γ peaks in the spectrum of ^ 152 Eu can be well reproduced using the corrected efficiency curve within an accuracy of 3 %. Comment: replace with the accepted versio...|$|R
40|$|Aim of {{this work}} is the {{numerical}} calculation of the true <b>coincidence</b> correction <b>factors</b> by means of Monte-Carlo simulation techniques. For this purpose, the Monte Carlo computer code PENELOPE was used and the main program PENMAIN was properly modified in order to include {{the effect of the}} true coincidence phenomenon. The modified main program that takes into consideration the true coincidence phenomenon was used for the full energy peak efficiency determination of an XtRa Ge detector with relative efficiency 104 % and the results obtained for the 1173 keV and 1332 keV photons of 60 Co were found consistent with respective experimental ones. The true <b>coincidence</b> correction <b>factors</b> were calculated as the ratio of the full energy peak efficiencies was determined from the original main program PENMAIN and the modified main program PENMAIN. The developed technique was applied for 57 Co, 88 Y, and 134 Cs and for two source-to-detector geometries. The results obtained were compared with true <b>coincidence</b> correction <b>factors</b> calculated from the "TrueCoinc" program and the relative bias was found to be less than 2 %, 4 %, and 8 % for 57 Co, 88 Y, and 134 Cs, respectively...|$|R
40|$|Abstract: Under the {{assumption}} that the firm has better information than the regulator we analyze a model for electricity distribution pricing where distribution basic cost are independently calculated, but the monopolist is allowed to set discriminatory prices in terms of consumers demand functions being subject to some constraints to induce the convergence of the firm solution to the social optimum. Two classes of constraints are analyzed, a price cap as proposed by Laffont and Tirole (1996), and a physical cap with prices restricted to be between the marginal cost and the stand-alone cost. The physical cap is a specific application for electricity distribution, using biases of the power <b>coincidence</b> <b>factors</b> used as a criteria for cost assignation, restricting the firm to balance the power distributed and the peak power sold. The model is calibrated with Chilean data, being demonstrated that through the proposed model an increment in the benefit of the firm, the consumers, and consequently, in social welfare, can be achieved. Among other factors, the magnitude of the achieved benefit depends on the price elasticity of the involved demands. ...|$|R
30|$|The inverse {{solution}} {{procedure that}} enables {{the identification of the}} defect position in a beam from the resonance frequency was exploited. Resonance frequency shifts of a power spectrum due to defects in a longitudinally vibrating beam when both ends are free were investigated by both numerical and experimental analysis. Calculation by a transfer matrix method showed that the frequency shift was large when the defect position coincides with a node of vibration and that no shift occurs when it coincides with a loop of vibration. The frequency shift could be approximated by a sinusoidal curve. Calculation results agreed well with those of the experiment in which artificial round holes were drilled as the defect model. Experimental equations predicting the amount of the frequency shift in function of the defect position were obtained. In the inverse procedure, the defect position was determined by comparing the resonance frequencies between the experimental and estimated power spectra so that the <b>coincidence</b> <b>factor</b> S(x) became a minimum. The results showed the validity of the proposed method to identify the defect positions of fewer than two predominant defects.|$|E
40|$|Abstract — Similarity {{between two}} spike trains is {{generally}} estimated using a ‘coincidence factor’. This factor relies on counting coincidences of firing-times for spikes {{in a given}} time window. However, in cases where there are significant fluctuations in membrane voltages, this uni-dimensional view is not sufficient. Results in this paper show that a two-dimensional approach taking both firing-time and the magnitude of spikes is necessary to determine similarity between spike trains. It is observed {{that the difference between}} the lower-bound limit of faithful behaviour and the reference inter-spike interval (ISI) reduces with the increase in the ISI of the input spike train. This indicates that spike trains generated by two highly-varying currents have a high <b>coincidence</b> <b>factor</b> thus indicating higher similarity – a limitation imposed due to a one-dimensional comparison approach. These results are analysed based on the responses of a Hodgkin-Huxley neuron, where the synaptic input induces fluctuations in the output membrane voltage. The requirement for a two-dimensional analysis is further supported by a clustering algorithm which differentiates between two visually-distinct responses as opposed to coincidence-factor. Index Terms—coincidence-factor, fluctuations, comparison, synaptic stimuli, membrane voltage...|$|E
40|$|This paper {{presents}} {{the lessons learned}} from a smart wash pilot, conducted with 24 employees of distribution system operator Enexis, who were equipped with an energy computer, smart washing machine, photovoltaic panels and smart meter. The pilot goal was to gain experience and knowledge about the application of these smart grid technologies in practice. Launched end of 2010 {{this was one of}} the first pilots focusing on consumer involvement. During the pilot consumers were encouraged to shift their load in order to match local electricity supply; to do their laundry while electricity is generated by their own photovoltaic panels. This paper contains an extensive analysis of the data the pilot brought forward and proposes an evaluation method to evaluate the washing machine load potential for smart grid integration. The <b>coincidence</b> <b>factor</b> of the daily load profiles and average load profiles of the participants is studied in order to define the significance of the results. Furthermore the paper elaborates on how the lessons learned during this pilot contributed to the design of a new smart grid pilot launched December 2012 in the Netherlands, `Your Energy Moment', focusing on demand side management of over 250 households...|$|E
40|$|Two {{important}} {{parameters for}} utilizing Ko-standardization method namely, absolute peak efficiency at reference position and peak-to-total ratio at different geometrical positions using standard point sources and HPGe were experimentally determined. <b>Coincidence</b> correction <b>factor,</b> C, for reference position and certain nuclides were also calculated and all almost equal to one. The importance and {{implication of this}} work to the K 0 -standardization method are presented. Other essential nuclear parameters which have to be experimentally determined or obtained from literature are also presented...|$|R
50|$|During {{the long}} years in {{opposition}} the party's support had grown steadily and in 1987 it attained the best parliamentary election result in its history. Harri Holkeri became the party's first Prime Minister since Paasikivi. During Holkeri's time in office, the Finnish economy suffered a downturn, precipitated by a <b>coincidence</b> of <b>factors,</b> and the 1991 parliamentary election {{resulted in a}} loss. The party continued in the government as a junior partner until the 2003 parliamentary election, after which it spent {{four years in the}} opposition.|$|R
40|$|As the {{demonstration}} of eco-communities, energy planning {{becomes more and more}} important for university campus. However, insufficient energy use data accumulation has been a significant barrier against fully understanding of energy use characteristics, and demand load features of campus buildings, which usually provide the basic support for energy planning from the demand side. A methodology to reveal the features of demand load and energy use of campus building was developed for this purpose. As a case study, both the long-term and real-time data of the electricity, heating, and water usage of a Norwegian university campus were analyzed by the descriptive statistics. On this base, coincidence characteristics of energy and water usage of the entire campus were analyzed, and individual coincidental rates to the campus were also quantified accordingly. The <b>coincidence</b> <b>factors</b> were calculated to be at high levels, which implied that the campus buildings’ usage of energy was quite similar to that of water. Finally, the individual coincidental contribution to total campus energy use was analyzed by the cluster analysis, to identify those buildings with the large potential of operation optimization. The results from this study could be used for the energy planning of cities and other urban energy systems...|$|R
40|$|In this paper, we {{describe}} the development, implementation and application of a novel mathematical procedure devoted to formulating the daily load profiles of off-grid consumers in rural areas. The procedure aims at providing such profiles as input data for the design process of off-grid systems for rural electrification. Indeed, daily load profiles represent an essential input for off-grid systems capacity planning methods based on steady-state energy simulation and lifetime techno-economic analyses, and {{for the analysis of}} the logics to control the energy fluxes among the different system components. Nevertheless, no particular attention has been devoted so far in the scientific literature as regards specific approaches for daily load profiles estimates for rural consumers. In order to contribute to covering this gap, we developed a new mathematical procedure taking into consideration the specific features of rural areas. The procedure is based on a set of data that can be surveyed and/or assumed in rural areas, and it relies on a stochastic bottom-up approach with correlations between the different load profile parameters (i. e. load factor, <b>coincidence</b> <b>factor</b> and number of consumers) in order to build up the coincidence behavior of the electrical appliances. We have implemented the procedure in a software tool (LoadProGen) which can eventually support the off-grid systems design process for rural electrification. Finally, we have applied the procedure to a case study in order to clarify the proposed approach...|$|E
40|$|The {{reliability}} and precision {{of the timing}} of spikes in a spike train is {{an important aspect of}} neuronal coding. We investigated reliability in thalamocortical relay (TCR) cells in the acute slice and also in a Morris-Lecar model with several extensions. A frozen Gaussian noise current, superimposed on a DC current, was injected into the TCR cell soma. The neuron responded with spike trains that showed trial-to-trial variability, due to amongst others slow changes in its internal state and the experimental setup. The DC current allowed to bring the neuron in different states, characterized by a well defined membrane voltage (between - 80 and - 50 mV) and by a specific firing regime that on depolarization gradually shifted from a predominantly bursting regime to a tonic spiking regime. The filtered frozen white noise generated a spike pattern output with a broad spike interval distribution. The <b>coincidence</b> <b>factor</b> and the Hunter and Milton measure were used as reliability measures of the output spike train. In the experimental TCR cell as well as the Morris-Lecar model cell the reliability depends on the shape (steepness) of the current input versus spike frequency output curve. The model also allowed to study the contribution of three relevant ionic membrane currents to reliability: a T-type calcium current, a cation selective h-current and a calcium dependent potassium current in order to allow bursting, investigate the consequences of a more complex current-frequency relation and produce realistic firing rates. The reliability of the output of the TCR cell increases with depolarization. In hyperpolarized states bursts are more reliable than single spikes. The analytically derived relations were capable to predict several of the experimentally recorded spike features...|$|E
40|$|Lighting {{measures}} is one {{effective strategy}} for reducing energy use in commercial buildings. Reductions in lighting energy have secondary effects on cooling/heating energy consumption and peak HVAC requirements; in general, they increase the heating and decrease cooling {{requirements of a}} building. Net change in a building`s annual and peak energy requirements, however, is difficult to quantify and depends on building characteristics, operating conditions, climate. This paper characterizes impacts of lighting/HVAC interactions on annual and peak heating/cooling requirements of prototypical US commercial buildings through computer simulations using DOE- 2. 1 E building energy analysis program. Ten building types of two vintages and nine climates are chosen to represent the US commercial building stock. For each combination, a prototypical building is simulated with two lighting power densities, and resultant changes in heating and cooling loads are recorded. Simple concepts of Lighting Coincidence Factors are {{used to describe the}} observed interactions between lighting and HVAC requirements. (<b>Coincidence</b> <b>Factor</b> (CF) is ratio of changes in HVAC loads to those in lighting loads, where load is either annual or peak load). The paper presents tables of lighting CF for major building types and climates. These parameters can be used for regional or national cost/benefit analyses of lighting- related policies and utility DSM programs. Using Annual CFs and typical efficiencies for heating and cooling systems, net changes in space conditioning energy use from a lighting measure can be calculated. Similarly, Demand CFs can be used to estimate the changes in HVAC sizing, which can then be converted to changes in capital outlay using standard-design curves; or they can be used to estimate coincident peak reductions for the analysis of the utility`s avoided costs. Results from use of these tables are meaningful only when they involve a significantly large number of buildings...|$|E
40|$|Production {{systems are}} usually {{organised}} in departments consisting of machines, each one characterised by specific patterns of energy demand over time. This work proposes an analytical approach, {{based on the}} application of Queuing Theory, to model the power request and the consequent energy use in a production system. Despite the industrial context addressed, the model may be easily applied to small units (e. g., civil buildings) and other energy sources (e. g., thermal energy), thus giving more relevance to the approach proposed. The model can efficiently support green-field cases, particularly avoiding or integrating the traditional assumptions, such as load and <b>coincidence</b> <b>factors</b> (usually employed to determine the contractual electrical power), which provide a static view of the power needs of the system. In fact, the proposed queuing model considers the arrivals as the statistical distribution of the switch-on of machines and service completions as the statistical distribution of the processing times at the machines themselves, thus offering a dynamic view of the power loads. Therefore, the model may be helpful while assessing the contract with the energy supplier or planning the production schedule of plants with significant energy-related constraints, including plant services. A numerical example shows {{the application of the}} proposed approach and its results are compared to those determined by the traditional design methodolog...|$|R
40|$|AbstractDual-color {{fluorescence}} cross-correlation {{analysis is}} {{a powerful tool for}} probing interactions of different fluorescently labeled molecules in aqueous solution. The concept is the selective observation of coordinated spontaneous fluctuations in two separate detection channels that unambiguously reflect the existence of physical or chemical linkages among the different fluorescent species. It has previously been shown that the evaluation of cross-correlation amplitudes, i. e., <b>coincidence</b> <b>factors,</b> is sufficient to extract essential information about the kinetics of formation or cleavage of chemical or physical bonds. Confocal fluorescence coincidence analysis (CFCA) (Winkler et al., Proc. Natl. Acad. Sci. U. S. A. 96 : 1375 – 1378, 1999) emphasizes short analysis times and simplified data evaluation and is thus particularly useful for screening applications or measurements on live cells where small illumination doses need to be applied. The recent use of two-photon fluorescence excitation has simplified dual- or multicolor measurements by enabling the simultaneous excitation of largely different dye molecules by a single infra-red laser line (Heinze et al., Proc. Natl. Acad. Sci. U. S. A. 97 : 10377 – 10382, 2000). It is demonstrated here that a combination of CFCA with two-photon excitation allows for minimization of analysis times for multicomponent systems down to some hundreds of milliseconds, while preserving all known advantages of two-photon excitation. By introducing crucial measurement parameters, experimental limits for the reduction of sampling times are discussed for the special case of distinguishing positive from negative samples in an endonucleolytic cleavage assay...|$|R
40|$|In {{positron}} {{emission tomography}} (PET) imaging, statistical iterative reconstruction (IR) techniques appear particularly promising since they can provide accurate system model. The system model matrix which describes the relationship between image space and projection space {{is important to the}} image quality. It contains some factors such as geometrical component and blurring component. The blurring component is usually described by point spread function (PSF). A PSF matrix derived from the single photon incidence response function is studied. And then an IR method based on the system matrix containing the PSF is developed. More specifically, the gamma photon incidence on a crystal array is simulated by Monte Carlo (MC) simulation, and then the single photon incidence response functions are calculated. Subsequently, the single photon incidence response functions is used to compute the <b>coincidence</b> blurring <b>factor</b> according to the physical process of PET coincidence detection. Through weighting the ordinary system matrix response by the <b>coincidence</b> blurring <b>factors,</b> the IR system matrix containing PSF is finally established. Using this system matrix, the image is reconstructed by ordered subset expectation maximization (OSEM) algorithm. The experimental results show that the proposed system matrix can obviously improve the image radial resolution, contrast and noise property. Furthermore, the simulated single gamma-ray incidence response function only depends on the crystal configuration, so the method could be extended to any PET scanners with the same detector crystal configuration. Comment: 21 pages, 14 figures, 5 table...|$|R
40|$|<b>Coincidence</b> <b>factor</b> {{take part}} in the friendships. It takes a long process and full of twistsand turns in {{achieving}} the goal. This friendships process and symbols which arerepresentated in Hugo film. Movies is one form of visualization of human thoughttowards surrounding reality. In this case, the authors formulate the problem asfollows: "How Friendship Representated in the Hugo film?". The use of mark in thefilm can affect someone thaught in represent certain meaning, in this case the meaningthat representated is the mening that contained in friendships symbols. This kind of research is a qualitative descriptive research with semiotic data analysistechniques. Researchers use Triangle Meaning Charles Peirce to reveal the meaning offriendship representations which is contained in the signs that are in the movie Hugo. Data collection techniques used are observation, documentation and FGD (FocusGroup Discussion). This research aims to find out any kind of friendship sign thatcontained in Hugo Film, by analysing the signs in the films. Based on the analysis, {{it can be seen that}} friendship in the film Hugo divided into 3 groups: utility, pleasure and virtue. (1) Utility, is a friendship because of the benefits,in this film Hugo friendship with Isabelle provides benefits not only for the two ofthem, but also for the people around him. (2) Pleasure, is friendship for pleasure, inthis film the friendship between Hugo and Isabelle provides enjoyable experience forthem through the adventure in solving the mystery of the automaton. (3) Virtue, truefriendship, in this film the friendship Hugo with Isabella not only just needed eachother and share the fun but also good to know and understand each other through joyand sorrow. Based on the results of the Focus Group Discussion group discussion(FGD) shows that friendship in the Hugo film consists of two types of friendshipswhich is fiendship because of the benefits and true friendship as seen on Hugofriendship with Isabelle. Keywords : Representation, Friendship, semiotic, Hugo Movie...|$|E
40|$|The {{aim of the}} {{experiments}} reported in this thesis {{was to investigate the}} multisensory interactions taking place between vision and audition. The focus is on the modulatory role of the temporal coincidence and semantic congruency of pairs of auditory and visual stimuli. With regards to the temporal <b>coincidence</b> <b>factor,</b> whether, and how, the presentation of a simultaneous sound facilitates visual target perception was tested using the equivalent noise paradigm (Chapter 3) and the backward masking paradigm (Chapter 4). The results demonstrate that crossmodal facilitation can be observed in both visual detection and identification tasks. Importantly, however, the results also reveal that the sound not only had to be presented simultaneously, but also reliably, with the visual target. The suggestion is made that the reliable co-occurrence of the auditory and visual stimuli provides observers with the statistical regularity needed to assume that the visual and auditory stimuli likely originate from the same perceptual event (i. e., that they in some sense 'belong together'). The experiments reported in Chapters 5 through 8 were designed to investigate the role of semantic congruency on audiovisual interactions. The results of {{the experiments}} reported in Chapter 5 revealed that the semantic context provided by the soundtrack that a person happens to be listening to can modulate his/her visual conscious perception in the binocular rivalry situation. In Chapters 6 - 8, the timecourse of audiovisual semantic interactions were investigated using categorization, detection, and identification tasks on visual pictures. The results suggested that when the presentation of the sound leads the presentation of a picture by more than 240 ms, it induces a crossmodal semantic priming effect. In addition, when the presentation of the sound lags a semantically-congruent picture by about 300 ms, it enhances performance, presumably by helping to maintain the visual representation in short-term memory. The results indicate that audiovisual semantic interactions constitute a heterogeneous group of phenomena. A crossmodal type-token binding framework is proposed to account for the parallel processing of the spatiotemporal and semantic interactions of multisensory inputs. The suggestion is that the congruent information in the type and token representation systems would integrate, and they finally bind into a unified multisensory object representation. This thesis is not currently available on ORA...|$|E
40|$|How safe was 2004 ?; Analysis of the {{decrease}} in number of road deaths in 2004. In 2004 there was a sharp {{decrease in the number}} of road deaths compared with 2003. This reduction of 19 % is spectacular: the number fell from 1088 in 2003 to 881 in 2004. This is the largest reduction ever in the Netherlands. Three questions need to be answered. Is the data correct? What was the role of coincidence? What really happened? SWOV investigated these three aspects, and the results were as follows: 1. The data is correct. They are just as reliable as in other years. Nothing was found to indicate that a lower registration rate caused the numerical reduction. 2. Coincidence can have played an important role in the difference between 2003 and 2004. A simple trend analysis shows that, with reasonable assumptions about the role of coincidence, the difference of 207 deaths consists of five components (the numbers in brackets give an indication of the size of the component) : - there was a random peak in 2003 (48), - explicable circumstances made 2003 less safe (16), - a decreasing trend (29), - there was a random drop in 2004 (48), and - explicable circumstances made 2004 safer (66). These 66 deaths account for 6. 6 % of the trend. These figures are very uncertain because the <b>coincidence</b> <b>factor</b> cannot be calculated exactly. To do this, a model that sufficiently accurately predicts the expected number of road deaths is necessary. In comparison with the trend, the peak in 2004 is not larger than the exceptions in previous years. In other words, 2004 is less exceptional than the difference with 2003 implies. 3. No explanatory influence factors were found to explain {{the decrease}} in 2004. Explanations were either too small, unresearchable, or too speculative. For example, the effect of extra enforcement possibly contributed considerably to the decreasing trend, but is insufficient to explain 2004. In general, meteorological effects have been empirically studied, but are not convincing enough to explain {{the decrease in}} 2004. What remains is that the growth in kilometres travelled seems to have stagnated: the numbers of delivery vans has stabilized and the numbers of kilometres travelled in lease cars has declined. This could explain a decrease of 6. 6 % road deaths, but it is speculative. This subject is an interesting starting point for a study by the Road Safety Planning department of SWOV...|$|E
40|$|Interest in {{biofuels}} surged in {{the late}} 1970 s and early 1980 s in response to high oil prices but waned by the mid 1980 s as oil prices plummeted and remained relatively low for almost 25 years. However, a <b>coincidence</b> of several <b>factors</b> has caused a recent resurgence in interest and growing global production of ethanol and biodiesel. Thes...|$|R
50|$|It is {{not well}} {{understood}} whether the lesions are precursors of breast cancer or only indication of increased risk, for most types of lesions the chance of developing breast cancer is nearly {{the same in the}} affected and unaffected breast (side) indicating only <b>coincidence</b> of risk <b>factors.</b> For atypical lobular hyperplasia there is high incidence of ipsilateral breast cancers indicating a possible direct carcinogenetic link.|$|R
40|$|A {{laboratory}} {{exercise for}} calculation of true <b>coincidence</b> summing correction <b>factors</b> {{as well as}} calculating the effect of deviations between sample and standard source (filling height) was developed. This laboratory exercise was held in a masters course in nuclear chemistry the first time during fall 2013. The aim of the exercise was to high-light the importance of correcting for biases due to different systematic effects in gamma spectrometric measurements...|$|R
50|$|The Carlton Association {{ceased to}} exist in 1993, as many of its {{original}} members were battle-weary and wanted to pursue other interests. Nevertheless, the Carlton Association has left Melbourne with a significant legacy, which has been interpreted in several ways. Founding members believe that the group’s success was due to a <b>coincidence</b> of <b>factors</b> in Melbourne during the 1960s and 70s. At this time, there was a renewed appreciation of the historic and architectural value of Carlton’s Victorian-era housing, spearheaded by {{the increasing number of}} young professionals and academics who were purchasing terraces in the suburb and restoring and renovating them. It was also an era of protest and direct action - a period in which citizens were questioning the authority of public decision-makers.|$|R
40|$|The {{second part}} of an intercomparison of the {{coincidence}} summing correction methods is presented. This exercise concerned three volume sources, filled with liquid solution. The same experimental spectra, decay scheme and photon emission intensities were used by all the participants. The results were expressed as <b>coincidence</b> summing correction <b>factors</b> for several energies of 152 Eu and 134 Cs, and different source-to-detector distances. They are presented and discussed. JRC. D. 4 -Nuclear physic...|$|R
40|$|The <b>coincidence</b> summing {{correction}} <b>factors</b> estimated with penEasy, a steering {{program for}} the Monte Carlo simulation code PENELOPE, and with penEasy-eXtended, an in-house modified version of penEasy, are presented and discussed for 152 Eu and 134 Cs in volume sources. The geometries and experimental data were obtained from an intercomparison study organized by the International Committee for Radionuclide Metrology (ICRM). A significant improvement in the results calculated with PENELOPE/penEasy was obtained when X-rays {{are included in the}} 152 Eu simulations. Postprint (published version...|$|R
40|$|Graduation date: 2005 Structural {{design codes}} have evolved {{continuously}} since modern codes were established. The allowable stress design format {{has been widely}} used since the late 191 h century. During the past two decades, probability-based limit states design concepts have evolved and are increasingly being used for various material and structural types. Limit state design, such as LRFD in the U. S., has requirements to ensure that structures perform satisfactorily under various loads and load combinations and that properly designed structures have reliable and consistent safety levels. Performance-based design concepts recently have gained interest among designers and researchers {{as an alternative to}} traditional (strength) design procedures. Performance-based engineering procedures require reliable predictions of structural response in order to quantify and limit damage to acceptable levels during the service-life of the structure. The objective {{of this study is to}} develop new contributions in performance-based design of engineered woodframe structures. Specially, fragility curves are developed for structures subjected to various natural hazards (and combinations of hazards) and new site-specific snow load models and hazard models are developed for use in probabilistic design. To accomplish these objectives, fragility curves are developed for assessing probabilistic response of engineered woodframe structures under wind, snow and earthquake hazards. The fragility curves developed herein can be used to develop performance-based design guidelines for woodframe structures built in high hazard regions as well as to provide information on which to base structural safety or expected structural (and economic loss) assessments. Probabilistic snow load models and snow hazard curves also are developed in this study. Updated snow load models can be used in code calibration studies and in the development of next-generation partial safety factors. The snow hazard curves can be used in a number of reliability-based design and performance-based design applications including assessment of partial safety factors for limit states design, evaluation of load combinations (<b>coincidence)</b> <b>factors</b> considering multiple hazards, evaluation of failure probabilities (by convolving with fragility curves for different performance levels, and development of risk-based assessment procedures for structures (and inventories of structures) under extreme snow loading...|$|R
40|$|This paper {{examines}} the Green Energy Act (GEA) {{and the economic}} circumstances that enabled the bill to become law in Ontario. An analysis of electrical power research, planning, and recommendations over the past forty years was conducted. The findings reveal {{that a variety of}} changes led to the approval of the GEA, including an environmentally conscious value shift and the economic recession, and the <b>coincidence</b> of these <b>factors</b> allowed forty years of government funded energy research to culminate in a publicly supported piece of legislation...|$|R
40|$|A {{comparison}} of the coincidence summing correction methods is presented Since {{there are several ways}} for computing these corrections, each method has advantages and drawbacks that could be compared. This part of the comparison was restricted to point sources The same experimental spectra. decay scheme and photon emission intensities were used by all the participants. The results were expressed as <b>coincidence</b> summing correction <b>factors</b> for several energies of Eu- 152 and Cs- 134, and three source-to-detector distances They are presented and discussed (C) 2010 Elsevier Ltd. All rights reserve...|$|R
40|$|The {{central point}} to be made about the move up in oil prices is that it {{reflects}} structural rather than cyclical issues. That {{is to say that}} higher prices are not the result of a random <b>coincidence</b> of short-term <b>factors</b> that could easily go away again. Instead, the move represents a significant structural shift upwards from the circumstances of the 1990 s. Over the past two years, that shift has become reflected in longer-term market values, as is seen in the time curve for West Texas Intermediate (WTI) crude oil prices...|$|R
40|$|The aim of {{the study}} was to check for {{equivalence}} of computer codes that can perform calculations of true <b>coincidence</b> summing correction <b>factors.</b> All calculations were performed for a set of well-defined detector and sample parameters, without any reference to empirical data. For a p-type detector model the application of different codes resulted in satisfactory agreement in the calculated correction factors. For high-efficiency geometries in combination with an n-type detector and a radionuclide emitting abundant X-rays the results were scattered. JRC. D. 4 -Standards for Nuclear Safety, Security and Safeguard...|$|R
30|$|Organizational {{concepts}} {{for mass}} events {{are supposed to}} be robust to the occurrence of single perturbations (‘single points of failure’). This in itself, however, does not exclude the possibility that the coincidence or interaction of problems can cause a systemic failure. When certain factors have amplifying effects on other factors (or there are even feedback loops), this can create systemic instabilities. We learn from this that, in order to reach a resilient organization of mass events (and actually any complex system), it is not sufficient to ensure the robustness of each contributing factor. One must also study their interaction effects, to guarantee that the overall organization is resilient to the <b>coincidence</b> of unfavorable <b>factors</b> as much as possible.|$|R
40|$|The {{analysis}} presented herein {{addresses the}} issue of social and religious diversity within the Catholic Church and its influence on voter turnout and Sejm election results in Poland. The paper covers election results from 2001 to 2007. Both organizational-institutional characteristics and social-religious characteristics of the Church have been taken into account when assessing {{the impact of the}} Church on regional differences in political support for selected political factions in 2005. The impact of each factor on the support level for a given party or political orientation in a regional (spatial) context was assessed {{on the basis of the}} degree of <b>coincidence</b> of the <b>factors</b> of interest, measured using the coefficient of correlation...|$|R
40|$|The thesis {{offers an}} {{interpretation}} of Paul's political thought and political theology {{in the context of}} current philosophy. The first part presents a methodological basis of the work: the concept of political theology is conceived as a methodological tool that enables us to concentrate on interrelations and mutual effects of religion and politics and to expose implicit or explicit political meanings and implications of religious ideas. The second methodological subchapter deals with Max Weber's approach to "economic ethics of world religions": Weber concentrates on historical crossroads and switches which are a result of random chain of <b>coincidences</b> and <b>factors</b> forming a specific relation to values (Wertbeziehung) which enables us to understand further historical development. Paul's missionary activity and theology is seen as such historical crossroad with far- reaching social consequences. Another part elaborately deals with the most important and influential interpretations of Paul in current non-Christian philosophy: Jacob Taubes, Alain Badiou, Giorgio Agamben. The attention is given not only to presentation of their interpretations but also to utilizations and interpretative gaps, which could be observed in the way these authors read and understand Paul. Though for different reasons, for all of them [...] ...|$|R
40|$|A {{series of}} Monte Carlo neutron {{calculations}} for a combined gamma/passive neutron coincidence counter has been performed. This type of detector, {{part of a}} suite of non-destructive assay instruments utilised for {{the enforcement of the}} Euratom nuclear safeguards within the European Union, is to be used for high accuracy measurements of the plutonium content of nuclear materials. The multi-purpose Monte Carlo N-Particle (MCNP) code version 4 B has been used to model in detail the neutron coincidence detector and to investigate the leakage self-multiplication of PuO 2 and Mixed U-Pu Oxide (MOX) reference samples used to calibrate the instrument. The MCNP calculations have been used together with a neutron coincidence counting interpretative model to determine characteristic parameters of the detector. A comparative study to both experimental and previous numerical results has been performed. Sensitivity curves of the variation of the detector's efficiency, epsilon, to the (alpha,n), to spontaneous fission neutron emission rate ratio, alpha, and to the reals <b>coincidence</b> gate utilisation <b>factor,</b> fr, are presented. Once bias-corrected, the trends of the real coincidence counts rate as a function of sample mass for three types of sample could be matched within 0. 33 % of experimental results. This result confirms the possible use of MCNP to calculate response trends accurately {{for a wide variety of}} source materials, given a limited experimental calibration set. Sources of the inaccuracy in the calculations have not yet been fully investigated, because of the vast parameter space to be investigated, but values of the <b>coincidence</b> gate utilisation <b>factor</b> derived directly from the MCNP data have been found to be overestimated by about 8. 2 %. JRC. E-Institute for Transuranium Elements (Karlsruhe...|$|R
