443|3147|Public
5|$|Developments in AspectJ have {{revealed}} {{the potential to}} incorporate just-in-time compilation into the execution of aspect-oriented code to address performance demands. At run-time, an aspect weaver could translate aspects in a more efficient manner than traditional, static weaving approaches. Using AspectJ on a Java Virtual Machine, dynamic weaving of aspects at run-time {{has been shown to}} improve <b>code</b> <b>performance</b> by 26%. While some implementations of just-in-time virtual machines implement this capability through a new virtual machine, some implementations can be designed to use features that already exist in current virtual machines. The requirement of a new virtual machine is contrary to one of the original design goals of AspectJ.|$|E
5000|$|Software {{performance}} analysis - techniques to monitor <b>code</b> <b>performance,</b> including instrumentation ...|$|E
5000|$|His second book, Zen of Assembly Language Volume 1: Knowledge (1990), {{focused on}} writing {{efficient}} assembly code for the 16-bit 8086 processor, but was released after the 80486 CPU was already available. In addition to assembly-level optimization, the book focused on {{parts of the}} system that silently affect <b>code</b> <b>performance,</b> which he called [...] "cycle eaters." [...] A key point of Zen of Assembly Language is that performance must always be measured, and the book included a tool called the Zen Timer to check if theoretical code optimizations actually worked. Volume 2 was never published.|$|E
3000|$|..., we next {{looked at}} n {{selection}} under different user bandwidth probability density distributions {{by using the}} algorithm described in Sections 3.2 and 4 to optimize n, then comparing the <b>coding</b> <b>performance</b> results to actual <b>coding</b> <b>performance</b> produced using other configurations. For this experiment, we conducted six tests using three different bandwidth probability density distributions with multiple users.|$|R
3000|$|In this experiment, we analyze <b>coding</b> <b>performance</b> {{when using}} the {{optimized}} coding configuration determined through our method, and compare these results against the <b>coding</b> <b>performance</b> when using other coding configurations. This experiment continues to use the “Mobile” and “Bus” (CIF format) video sets as the test objects. Coding configuration optimization is mainly reflected in QP [...]...|$|R
40|$|In this paper, {{we propose}} a new {{seamless}} bitstream switching scheme {{to improve the}} <b>coding</b> <b>performance</b> of H. 264 SP-frames for rate adaptation. Our method removes {{one of the two}} re-quantization blocks in the SP-frame encoders so as to significantly improve <b>coding</b> <b>performance.</b> The seamless switching property of SP-frames is retained by properly restructuring the primary and secondary switching frame codecs. Experimental results show that our proposed scheme achieves close <b>coding</b> <b>performance</b> to that of regular H. 264 P-frames and significantly better performance than that of SP-frames. The proposed method also provides the advantage of using a single secondary switching bitstream for both switching-up and switching-down processes...|$|R
50|$|Developments in AspectJ have {{revealed}} {{the potential to}} incorporate just-in-time compilation into the execution of aspect-oriented code to address performance demands. At run-time, an aspect weaver could translate aspects in a more efficient manner than traditional, static weaving approaches. Using AspectJ on a Java Virtual Machine, dynamic weaving of aspects at run-time {{has been shown to}} improve <b>code</b> <b>performance</b> by 26%. While some implementations of just-in-time virtual machines implement this capability through a new virtual machine, some implementations can be designed to use features that already exist in current virtual machines. The requirement of a new virtual machine is contrary to one of the original design goals of AspectJ.|$|E
5000|$|Training for LOSA experts {{includes}} two sessions: education in procedural protocols, and TEM concepts and classifications. A LOSA trainee is taught to find data {{first and then}} code them later for both sessions, during which a crew member must exhibit [...] "LOSA Etiquette" [...] - [...] ability to notify the pilot {{as to why he}} or she was not able to detect an error or threat after a flight. The pilot's responsibilities include his or her opinions on what safety issues could have had an adverse impact on their operations. A LOSA trainee must then record the specific responses of the pilot and thereafter <b>code</b> <b>performance</b> using behavioral markers. The order of the recording is as follows: a) record visible threats; b) identify error types, crew's responses, and specific outcomes; and c) use CRM behavioral markers to rate crew.|$|E
30|$|The decay s {{increases}} as the channel/channel <b>code</b> <b>performance</b> improves.|$|E
40|$|In this paper, a {{reversible}} integer-to-integer {{wavelet transform}} based on non-separable 2 D allpass filters is proposed for lossless image coding. The number of rounding operations {{included in the}} reversible wavelet transform is reduced by using non-separable 2 D allpass filters, thus {{it is expected to}} get better <b>coding</b> <b>performance.</b> The lossless <b>coding</b> <b>performance</b> of the proposed reversible wavelet transform is evaluated and compared with the conventional separa-ble wavelet transforms. It is shown from the experimental results that the proposed non-separable 2 D reversible wavelet tranform can achieve better lossless <b>coding</b> <b>performance</b> than the conven-tional separable wavelet transforms, including the D- 5 / 3 wavelet tranform in the JPEG 2000...|$|R
50|$|The <b>coding</b> <b>performance</b> of EZW {{has since}} been {{exceeded}} by SPIHT and its many derivatives.|$|R
5000|$|... #Caption: A Study in Keith is {{a musical}} live <b>coding</b> <b>performance</b> in Impromptu by Andrew Sorensen.|$|R
40|$|This {{research}} {{focuses on}} evaluating and enhancing the performance of an in-house, structured, 2 D CFD code- GHOST, on modern commodity clusters. The basic philosophy of this work is to optimize the cache performance of the code by splitting up the grid into smaller blocks and carrying out the required calculations on these smaller blocks. This in turn leads to enhanced <b>code</b> <b>performance</b> on commodity clusters. Accordingly, this work presents a discussion along with {{a detailed description of}} two techniques: external and internal blocking, for data access optimization. These techniques have been tested on steady, unsteady, laminar, and turbulent test cases and the results are presented. The critical hardware parameters which influenced the <b>code</b> <b>performance</b> were identified. A detailed study investigating the effect of these parameters on the <b>code</b> <b>performance</b> was conducted and the results are presented. The modified version of the cod...|$|E
40|$|In this work, an {{improved}} union bound {{on the performance}} of turbo codes is evaluated using a new union bound presented by Viterbi et al., (1998). The characterization of turbo <b>code</b> <b>performance</b> is provided considering both the fully-interleaved channel and correlated Rice slow-fading channel. A uniform interleaver is assumed: this strategy allows one to derive the average performance of the turbo coding scheme by analysis. Of interest is the improvement of the bound for values of E b/N 0 near and below R 0 (i. e., the "computational cutoff rate"), region where the conventional union bound becomes useless. This improvement allows a better estimate of the <b>code</b> <b>performance</b> in conjunction with the AWGN channel and with the fast-fading fully-interleaved channel. Moreover, it allows one to determine a tighter bound to the actual <b>code</b> <b>performance</b> as far as the slow fading Rician channel is concerne...|$|E
40|$|We present {{methods to}} {{estimate}} <b>code</b> <b>performance</b> and decoder complexity from the code rate, block size, and word-error rate, for families of related low-density parity-check (LDPC) codes. Performance estimates are generally {{within a couple}} tenths of a decibel of results determined by simulation; estimates of complexity (and hence decoder speed) are generally within 10 percent. Experimental data {{show that there is}} a trade-off between complexity and <b>code</b> <b>performance</b> determined by the design of the LDPC code, and that each 1 dB (26 percent) of increased complexity is worth about a 0. 1 -dB reduction in the required signal-to-noise ratio. I...|$|E
40|$|The {{orthogonal}} space-time coded {{continuous phase}} modulation (OST-CPM) system shows attractive performance over fading MIMO channels. In this paper, the Chernoff bound on pair-wise error probability (PWEP) is studied for two transmit antennas over spatially correlated quasi-static Rayleigh-fading channel. The maximum likelihood sequence detection (MLSD) algorithm {{is applied to}} the OST-CPM system. Approximate bound for high signal-to-noise ratio (SNR) is derived to evaluate the encoding performance in correlated channel. The effects of correlation coefficient matrices on the <b>coding</b> <b>performance</b> are simulated. Both analytical and simulation results show that the <b>coding</b> <b>performance</b> of this system decreases as the fading coefficients between the antennas increases. And the penalty on the <b>coding</b> <b>performance</b> increases a lot in fully correlated channel...|$|R
5000|$|NIME—academic {{and artistic}} conference on {{advances}} in music technology, sometimes featuring live <b>coding</b> <b>performances</b> and research presentations ...|$|R
40|$|Abstract—This paper {{presents}} a novel high-efficient hybrid openclose loop based fine granularity scalable (HOCFGS) coding framework supporting different decoding complexity applications. The open-loop motion compensation for inter-pictures is introduced to efficiently exploit the temporal correlation among adjacent pictures for both base layer and FGS enhancement layer within a wide bit-rate range. An efficient rate-distortion optimized macro-block mode decision rule {{is used to}} reduce drifting error and achieve comparable <b>coding</b> <b>performance</b> at the lowest bit-rate point (base layer) with non-scalable coding. An approach like MPEG- 4 FGS coding with close-loop motion compensation only at base layer is used for some inter-pictures to stop the drifting error propagation. Furthermore, to achieve better <b>coding</b> <b>performance</b> for these inter-pictures, block based progressive fine granularity scalable (BLPFGS) is introduced, in which leaky prediction is used to generate high-quality reference for FGS enhancement layer. In BLPFGS coding, efficient bit-plane coding and de-blocking techniques are investigated to improve the <b>coding</b> <b>performance</b> for FGS enhancement layer, especially at low bit-rate points. The <b>coding</b> <b>performance</b> for the proposed method is verified by integrating it into MPEG SVC reference software. I...|$|R
40|$|Borrowing from {{wireless}} communications, a "diversity" optical code-division multiple-access (O-CDMA) {{scheme for}} optical wireless is studied in this paper. The scheme transmits multiple copies of an optical code per data bit of one for improving <b>code</b> <b>performance</b> and spectral efficiency. An approximate performance-analytical model is also formulated for quick computation and comparison with a conventional O-CDMA scheme. Under {{the assumptions of}} same code power and hardware complexity, our study finds an important application of the diversity scheme, which circumvents a heavy code-weight requirement (for achieving good <b>code</b> <b>performance)</b> in conventional O-CDMA by transmitting multiple copies of lower weight codes with little penalty in spectral efficiency and without depletion in code cardinality (i. e., the number of possible subscribers. ...|$|E
40|$|Legacy <b>code</b> <b>performance</b> {{has failed}} {{to keep up with}} that of modern {{hardware}}. Many new hardware features remain under-utilised, with the majority of code bases still unable to make use of accelerated or heterogeneous architectures. Code maintainers now accept that they can no longer rely solely on hardware improvements to drive <b>code</b> <b>performance,</b> and that changes at the software engineering level need to be made. The principal focus of the work presented in this thesis is an analysis of the changes legacy Inertial Confinement Fusion (ICF) codes need to make in order to efficiently use current and future parallel architectures. We discuss the process of developing a performance model, and demonstrate the ability of such a model to make accurate predictions about <b>code</b> <b>performance</b> for code variants on a range of architectures. We build on the knowledge gained from such a process, and examine how Particle-in-Cell (PIC) codes must change in order to move towards the required levels of portable and future-proof performance needed to leverage the capabilities of modern hardware. As part of this investigation, we present an OpenCL port of the legacy code EPOCH, as well as a fully featured mini-app representing EPOCH. Finally, as a direct consequence of these investigations, we directly apply these performance optimisations to the production version EPOCH, culminating in a speedup of over 2 x for the core algorithm...|$|E
40|$|A new {{decoding}} algorithm for geometrically uniform trellis codes {{exploiting the}} group {{structure of the}} codes is presented. The algorithm complexity {{does not depend on}} the number of states of the trellis describing the <b>code.</b> <b>Performance</b> of the algorithm are evaluated through simulation and its complexity is compared to the Viterbi algorithm...|$|E
40|$|In wavelet-based {{image coding}} {{the choice of}} wavelets is crucial and determines the <b>coding</b> <b>performance.</b> Current {{techniques}} use computationally intensive search procedures to find the optimal basis (type, order and tree). In this paper, we show that searching for optimal wavelet does not always offer a substantial improvement in <b>coding</b> <b>performance</b> over "good" standard wavelets. We propose some guidelines for determining the need {{to search for the}} "optimal" wavelets based on the statistics of the image to be coded. In addition, we propose an adaptive wavelet packet decomposition algorithm based on the local transform gain of each stage of the decomposition. The proposed algorithm provides a good <b>coding</b> <b>performance</b> at a substantially reduced complexity. 1 Introduction Visual communication is becoming increasingly important with applications in several areas such as, digital television transmission, teleconferencing, multimedia communications, transmission and storage of remote sensing i [...] ...|$|R
40|$|Since wavelet filters {{composed}} of a single complex allpass filter satisfy both of the conditions of orthonormality and symmetry, {{it is expected to}} obtain better <b>coding</b> <b>performance</b> than biorthogonal wavelets in image coding. This paper proposes an effective implementation of the orthonormal symmetric wavelets {{composed of}} a complex allpass filter for image coding. Firstly, an implementation of irreversible real-to-real wavelets for lossy coding is presented by realizing the complex allpass filter. Then, the reversible integer-to-integer wavelets is realized by utilizing the invertible implementation of complex allpass filter. Finally, the <b>coding</b> <b>performance</b> of the orthonormal symmetric wavelets is evaluated and compared with the D- 9 / 7 and D- 5 / 3 wavelets. It is shown from the experimental results that the allpass-based orthonormal symmetric wavelets can achieve better <b>coding</b> <b>performance</b> than the D- 9 / 7 and D- 5 / 3 wavelets...|$|R
30|$|This paper {{proposes a}} {{representative}} pixel (RP) extraction and colorization algorithm for the colorization-based digital image coding. In {{order to achieve}} low computing cost and high image <b>coding</b> <b>performance,</b> multiple reduced images are generated by colorization error minimizing method, and the RPs are extracted from these reduced images. Numerical examples show that the proposed algorithm can extract RPs and recover a color image fast and effectively comparing with other colorization-based algorithms, and numerical {{results show that the}} proposed algorithm achieves higher <b>coding</b> <b>performance</b> than JPEG/JPEG 2000.|$|R
40|$|Abstract — In this paper, {{we provide}} {{necessary}} and sufficient {{conditions for a}} column-weight-three LDPC code to correct three errors when decoded using Gallager A algorithm. We then provide a construction technique which results in a code satisfying the above conditions. We also provide numerical assessment of <b>code</b> <b>performance</b> via simulation results. I...|$|E
40|$|An {{analysis}} is presented of several factors influencing {{the performance of}} a parallel implementation of the UCLA atmospheric general circulation model (AGCM) on massively parallel computer systems. Several modificaitons to the original parallel AGCM code aimed at improving its numerical efficiency, interprocessor communication cost, load-balance and issues affecting single-node <b>code</b> <b>performance</b> are discussed...|$|E
40|$|Abstract—A {{class of}} {{pseudo-random}} compound error-correcting codes, called Generalized Low-Density (GLD) Parity-Check codes, has been pro-posed recently. As a generalization of Gallager’s Low-Density Parity-Check (LDPC) codes, GLD codes are also asymptotically {{good in the}} sense of minimum distance criterion and can be effectively decoded based on iter-ative soft-input soft-output (SISO) decoding of individual constituent codes. The <b>code</b> <b>performance</b> and decoding complexity of GLD code are heav-ily dependent on the employed SISO decoding algorithm. In this paper, we show that Max-Log-MAP is an attractive SISO decoding algorithm for GLD coding scheme, considering the trade-off between performance and complexity in the practical implementations. A normalized Max-Log-MAP is presented to improve the GLD <b>code</b> <b>performance</b> significantly compared with using conventional Max-Log-MAP. Moreover, we propose two tech-niques, decoding task scheduling and reduced search Max-Log-MAP, to effectively reduce the decoding complexity without any performance degra-dation. I...|$|E
40|$|A {{software-based}} {{implementation of}} the image codec specified in the emerging JPEG- 2000 standard is discussed. The run-time complexity and <b>coding</b> <b>performance</b> of this implementation are also briefly analyzed. 1. INTRODUCTION Digital imagery is pervasive in our world today. Hence, standards for the efficient representation and interchange of such information are essential. JPEG 2000 [1] is a new image compression standard currently being developed by the International Organization for Standardization (ISO). In this paper, we describe JasPer, {{one of the first}} implementations of this newly emerging standard. We also analyze the run-time complexity and <b>coding</b> <b>performance</b> of the implementation. By studying code execution profiles for typical coding scenarios, we gain improved insight into how one might develop a fast software implementation. Also, we examine the impact of using fixed-point (instead of floating-point) arithmetic on <b>coding</b> <b>performance.</b> 2. JPEG 2000 JPEG 2000 supports lossy [...] ...|$|R
40|$|Embedded {{quantization}} is {{a mechanism}} employed by many lossy image codecs to progressively refine the distortion of a (transformed) image. Currently, {{the most common}} approach {{to do so in}} the context of wavelet-based image coding is to couple uniform scalar deadzone quantization (USDQ) with bitplane coding (BPC). USDQ+BPC is convenient for its practicality and has proved to achieve competitive <b>coding</b> <b>performance.</b> But the quantizer established by this scheme does not allow major variations. This paper introduces a multistage quantization scheme named general embedded quantization (GEQ) that provides more flexibility to the quantizer. GEQ schemes can be devised for specific decoding rates achieving optimal <b>coding</b> <b>performance.</b> Practical approaches of GEQ schemes achieve <b>coding</b> <b>performance</b> similar to that of USDQ+BPC while requiring fewer quantization stages. The performance achieved by GEQ is evaluated in this paper through experimental results carried out in the framework of modern image coding systems...|$|R
30|$|As {{mentioned}} before, multimedia DVC is a {{new video}} coding paradigm which {{makes it possible to}} shift complexity from an encoder to a decoder. Two signals can be independently coded in the encoder and then reconstructed with a prediction of the cross-correlation between them in the decoder. Slepian and Wolf proved that <b>coding</b> <b>performance</b> can be improved with decoder-side prediction. Wyner and Ziv developed several lossy DVC systems based on the Slepian-Wolf information theory [1, 2]. They also proposed a novel SI generation algorithm to improve DVC <b>coding</b> <b>performance</b> [16 – 18].|$|R
40|$|This paper {{discusses}} FORTRAN optimizations {{that the}} user can perform manually {{at the source}} code level to improve object <b>code</b> <b>performance.</b> It makes use of descriptive examples within {{the text of the}} paper for explanatory purposes. The paper defines key areas in writing a FORTRAN program and recommends ways to improve efficiency in these areas...|$|E
30|$|Rahnavard et al. [14], {{first of}} all, {{presented}} a distribution-based approach. They introduced UEP at the LT encoding stage and designed a non-uniformly degree distribution such that lower layer symbols can be selected with higher probability. With achieving unequal recovery of different layers, the altered distribution weakens the <b>code</b> <b>performance</b> {{and results in}} a larger overhead.|$|E
40|$|The {{focus of}} the work is on the {{performance}} study, design and weight distribution analysis of convolutional turbo code and turbo product codes. It includes investigation of vital components that influence convolutional turbo <b>code</b> <b>performance,</b> such as constituent encoders and the interleaver. This work begins with the investigation of various iterative decoding techniques, and performance and complexity comparison...|$|E
40|$|Multiple {{reference}} frame selection {{adopted by the}} state-of-art H. 264 video compression standard offers substantial performance gain. The temporal search range control, as a consequence, is crucial for maintaining the <b>coding</b> <b>performance</b> with minimum complexity. In this paper, we investigate {{the relationships between the}} {{reference frame}} buffer utilization and the optimal search range. A content-adaptive algorithm is proposed to control the search range dynamically during the encoding process. Experimental results show that our algorithm can rapidly adapt to the video characteristics and effectively reduce the complexity with negligible <b>coding</b> <b>performance</b> penalty. © 2006 IEEE...|$|R
40|$|Recently, an {{efficient}} video coding method at {{low bit rate}} using Matching Pursuits (MP) has been proposed. The MP coding method represents a signal in an approximate form using a dictionary. Therefore, <b>coding</b> <b>performance</b> depends greatly on the dictionary. In this paper, we introduce a video coding method that employs motion compensation and MP using a dynamic learning dictionary. The dictionary of the proposed method is renewed at each frame by using encoded information. Simulation {{results show that the}} <b>coding</b> <b>performance</b> of MP can be improved by applying the dynamic learning dictionary...|$|R
30|$|In practice, the <b>coding</b> <b>performance</b> is {{determined}} by the capacity of the correlation channel that approaches the Slepian–Wolf bound used by sophisticated turbo or low-density parity check (LDPC) codes.|$|R
