4400|149|Public
25|$|Since {{this ideal}} {{selector}} contains the unknown density function ƒ, it {{cannot be used}} directly. The many different varieties of data-based bandwidth selectors arise from the different estimators of the AMISE. We concentrate on two classes of selectors which {{have been shown to}} be the most widely applicable in practice: smoothed <b>cross</b> <b>validation</b> and plug-in selectors.|$|E
25|$|A more {{straightforward}} {{way to use}} kernel {{machines for}} deep learning was developed for spoken language understanding. The main idea {{is to use a}} kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by <b>cross</b> <b>validation.</b>|$|E
2500|$|Smoothed <b>cross</b> <b>validation</b> (SCV) is {{a subset}} of a larger class of <b>cross</b> <b>validation</b> techniques. The SCV {{estimator}} differs from the plug-in estimator in the second term ...|$|E
3000|$|... 10 -fold <b>cross</b> <b>validations.</b> Accomplished by {{partitioning}} {{the original}} sample into a training set and a test set in rotation.|$|R
3000|$|... > and use <b>cross</b> <b>validations</b> to fix model {{parameters}} like {{number of}} Gaussian processes, vocabulary size, temporal step sizes and so on.|$|R
30|$|Since we {{are doing}} multi-fold <b>cross</b> <b>validations</b> in our experiments, we use the micro-average of F measure for the final {{classification}} results. This is done by adding the classification results for all documents across all five folds before computing the final P, R, and the F.|$|R
2500|$|The [...] in R {{implements}} the plug-in {{and smoothed}} <b>cross</b> <b>validation</b> selectors (amongst others). This dataset (included {{in the base}} distribution of R) contains ...|$|E
2500|$|It {{has been}} {{established}} that the plug-in and smoothed <b>cross</b> <b>validation</b> selectors (given a single pilot bandwidth G) both converge at a relative rate of Op(n−2/(d+6)) [...] i.e., both these data-based selectors are consistent estimators.|$|E
2500|$|A common {{choice is}} a Gaussian kernel, {{which has a}} single {{parameter}} '. The best combination of C and [...] is often selected by a grid search with exponentially growing sequences of C and ', for example, [...] Typically, each combination of parameter choices is checked using <b>cross</b> <b>validation,</b> and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization {{can be used to}} select C and ' , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.|$|E
5000|$|StrategyEdit section which {{describes}} the validation rules {{to be applied}} - typically these will be <b>cross</b> field <b>validations</b> ...|$|R
30|$|The {{position}} of soil {{units in the}} capability classification matrix, productivity scores of capability classes by climate zones, and multiplication factors {{to account for the}} water-storing capacities by climate zones were first defined by expert judgment and then refined and completed through an iterative analytical process with a series of <b>cross</b> <b>validations</b> using regression analysis (GWR) with the validation dataset as dependent variable (see below for details on the validation process).|$|R
40|$|International audienceIn this paper, the Gauss {{process is}} {{proposed}} for application on landslide displacement analysis and prediction with dynamic <b>crossing</b> <b>validation.</b> The prediction problem using noisy observations is first introduced. Then the Gauss process method is proposed for modeling non-stationary series of landslide displacements {{based on its}} ability to model noisy data. The monitoring displacement series of the New Wolong Temple Landslide is comparatively studied with other methods as an instance to implement the strategy of the Gauss process for predicting landslide displacement. The dynamic <b>crossing</b> <b>validation</b> method is adopted to manage the displacement series so as to give more precise predictions. Different covariance functions are illustrated to give predictive results which show that different covariance functions result in varying levels of prediction accuracy. Comparisons with other methods are also discussed in this study. The results show that the Gauss process can perform better than the RBF network and the SVM methods in this problem in view of the trends according to the original data. Finally, the landslide criterion is given for creep-typed slopes that landslide event would occur imminently if the cross angle at the intersection point of displacement curve changes more than 45...|$|R
5000|$|Smoothed <b>cross</b> <b>validation</b> (SCV) is {{a subset}} of a larger class of <b>cross</b> <b>validation</b> techniques. The SCV {{estimator}} differs from the plug-in estimator in the second term ...|$|E
5000|$|Validating models {{by using}} random subsets (bootstrapping, <b>cross</b> <b>validation)</b> ...|$|E
5000|$|Find a heuristically optimal number k of nearest neighbors, {{based on}} RMSE. This is done using <b>cross</b> <b>validation.</b>|$|E
40|$|In {{this paper}} we report results for {{recognizing}} colorectal NBI endoscopic images by using features extracted from {{convolutional neural network}} (CNN). In this comparative study, we extract features from different layers from different CNN models, and then train linear SVM classifiers. Experimental results with 10 -fold <b>cross</b> <b>validations</b> show that features from first few convolution layers are enough to achieve similar performance (i. e., recognition rate of 95 %) with non-CNN local features such as Bag-of-Visual words, Fisher vector, and VLAD. Comment: 5 pages, FCV 201...|$|R
40|$|In a {{well defined}} model system which {{exhibits}} strong overfitting we study {{the method of}} <b>cross</b> [...] <b>validation.</b> Both <b>cross</b> [...] <b>validation</b> and training use the available data, however exclusively. Therefore the question arises, how the examples should be partitioned between <b>cross</b> [...] <b>validation</b> and training. We show that small fractions of examples are sufficient to achieve good results with cross-validation. The definition of an optimal fraction is discussed. I. Introduction The ability to learn functions from examples {{is one of the}} major advantages of artificial neural networks. While the existence of general function approximators is already well established, the practical realization is still not completely understood. Current realizations require, in most cases, a lot of experience and sometimes even folkloristic tricks. All too often overfitting can occur in the learning process. Overfitting denotes the phenomenon that the net learns too many details from the examples and neglects its gene [...] ...|$|R
40|$|Abstract In {{this paper}} {{we present a}} novel {{framework}} for classification of the different kind of tissues in intravascular ultrasound (IVUS) data. We describe a normalized reconstruction process for IVUS images from radio frequency (RF) signals. The reconstructed data is {{described in terms of}} texture based features and feeds an ECOC-Adaboost learning process. In the same manner, the RF signals are characterized using Autoregressive models, and classified with a similar learning process. A comparison is performed among these techniques using two different <b>cross</b> <b>validations</b> schemes: 50 rounds of 90 - 10 -Holdout and Leave One Patient Out obtaining very promising results. ...|$|R
50|$|Non-exhaustive <b>cross</b> <b>validation</b> methods do not compute {{all ways}} of {{splitting}} the original sample. Those methods are approximations of leave-p-out cross-validation.|$|E
5000|$|Dave Kerby (2003) {{showed that}} unit weights compare well with {{standard}} regression, doing {{so with a}} <b>cross</b> <b>validation</b> study—that is, he derived beta weights in one sample and applied them to a second sample. The outcome of interest was suicidal thinking, and the predictor variables were broad personality traits. In the <b>cross</b> <b>validation</b> sample, the correlation between personality and suicidal thinking was slightly stronger with unit-weighted regression (r = [...]48) than with standard multiple regression (r = [...]47).|$|E
50|$|Different {{strategies}} are available, including random generator sequences, Factorial DOEs, Orthogonal and Iterative Techniques, as like as D-Optimal or <b>Cross</b> <b>Validation.</b> Monte Carlo and Latin hypercube {{are available for}} robustness analysis.|$|E
40|$|Identifying cancer {{molecular}} {{patterns with}} high accuracy from high-dimensional proteomic profiles presents {{a challenge for}} statistical learning and oncology research. In this study, we develop a nonnegative principal component analysis and propose a nonnegative principal component analysis based support vector machine with a sparse coding to conduct effective feature selection and high-performance proteomic pattern classification. We demonstrate the superiority of our algorithm by comparing it with six peer algorithms on four benchmark proteomic tumor profiles under 100 trials of 50 % holdout <b>cross</b> <b>validations.</b> We also rigorously show that the over-fitting problem associated with support vector machines can be overcome by nonnegative principa...|$|R
40|$|The paper {{focuses on}} issues in {{experimental}} design for identification of nonlinear multivariable systems. Perturbation signal design is analyzed for a hybrid model structure consisting of linear and neural network structures. Input signals, designed {{to minimize the}} effects of nonlinearities during the linear model identification for the multivariable case, have been proposed and its properties have been theoretically established, The superiority of the proposed perturbation signal and the hybrid model has been demonstrated through extensive <b>cross</b> <b>validations.</b> The utility of the obtained models for control has also been proved through a case study involving MPC of a nonlinear multivariable neutralization plant...|$|R
40|$|Abstract. We present our {{experience}} {{in creating a}} novel unsupervised clustering algorithm for situation-aware pattern extraction from usage logs. The algorithm automatically estimates near-optimal number of clusters and cluster centroids. It models situation {{by taking advantage of}} sensors. 5 -fold <b>cross</b> <b>validations</b> using real-world data show that the algorithm delivers higher accuracy than existing algorithms with much lower complexity. As a result it is the first clustering algorithm that can be practically deployed on mobile handheld devices in the real world. We also describe the research problems in situation-aware personalization that must be addressed before users can benefit from the learning algorithms and speculate on possible approaches to the solutions...|$|R
5000|$|... {{internal}} validation or cross-validation (actually, while extracting data, <b>cross</b> <b>validation</b> {{is a measure}} of model robustness, the more a model is robust (higher q2) the less data extraction perturb the original model); ...|$|E
5000|$|Where [...] {{is the sum}} of squared {{prediction}} errors. These {{errors are}} estimated based on <b>cross</b> <b>validation.</b> In the <b>cross</b> <b>validation</b> procedure, the set of support points is mapped to [...] subsets. Then the approximation model is built by removing subset [...] from the support points and approximating the subset model output [...] using the remaining point set. This means that the model quality is estimated only at those points which are not used to build the approximation model. Since the prediction error is used instead of the fit, this approach applies to regression and even interpolation models.|$|E
5000|$|It {{has been}} {{established}} that the plug-in and smoothed <b>cross</b> <b>validation</b> selectors (given a single pilot bandwidth G) both converge at a relative rate of Op(n−2/(d+6)) [...] i.e., both these data-based selectors are consistent estimators.|$|E
40|$|Robust cancer {{molecular}} pattern identification from microarray {{data not}} only plays {{an essential role}} in modern clinic oncology, but also presents a challenge for statistical learning. Although principal component analysis (PCA) is a widely used feature selection algorithm in microarray analysis, its holistic mechanism prevents it from capturing the latent local data structure in the following cancer molecular pattern identification. In this study, we investigate the benefit of enforcing non-negativity constraints on principal component analysis (PCA) and propose a nonnegative principal component (NPCA) based classification algorithm in cancer molecular pattern analysis for gene expression data. This novel algorithm conducts classification by classifying meta-samples of input cancer data by support vector machines (SVM) or other classic supervised learning algorithms. The meta-samples are low-dimensional projections of original cancer samples in a purely additive meta-gene subspace generated from the NPCA-induced nonnegative matrix factorization (NMF). We report strongly leading classification results from NPCA-SVM algorithm in the cancer molecular pattern identification for five benchmark gene expression datasets under 100 trials of 50 % hold-out <b>cross</b> <b>validations</b> and leave one out <b>cross</b> <b>validations.</b> We demonstrate superiority of NPCA-SVM algorithm by direct comparison with seven classification algorithms: SVM, PCA-SVM, KPCA-SVM, NMF-SVM, LLE-SVM, PCA-LDA and k-NN, for the five cancer datasets in classification rates, sensitivities and specificities. Our NPCA-SVM algorithm overcomes the over-fitting problem associative with SVM-based classifications for gene expression data under a Gaussian kernel. As a more robust high-performance classifier, NPCA-SVM can be used to replace the general SVM and k-NN classifiers in cancer biomarker discovery to capture more meaningful oncogenes...|$|R
40|$|Description This {{package is}} useful in finding and validating {{predictive}} gene signature for classifying low risk versus high risk patients in early phase clinical trials. The primary end point is survival, and classification of cancer patients into low risk or high risk groups is mainly based on median cutoff,but others {{can be considered as}} well. It can also accommodate the prognostic factors if any. Both statistical and machine learning techniques are integrated as validating suit. The package can be used to perform the analysis using the entire samples and {{can also be used to}} carryout large scale <b>cross</b> <b>validations.</b> For the first instance, package reduces larger gene expression matrix to smaller version using supervised principle components analysis. Later entire validation procedure can be performed using reduced gene expression matrix with various types of validation schemes...|$|R
40|$|The Superposing Significant Interaction Rules (SSIR) {{method is}} revised and implemented. The method {{is a simple}} {{combinatorial}} procedure, which deals with in situ generated rules among a dichotomized congeneric molecular family, selecting the most probabilistically relevant ones. The mere counting {{of the number of}} relevant rules attached to new compounds generates a molecular ranking useful for database filtering, refinement and prediction. The algorithm only needs for a symbolic molecular representation and this allows for mining the database in a confidential manner. Third parties will not know the real compounds that are on the way to be worked out. The procedure is tested for a complete series of substituted amino acids. Areas under the receiver operating characteristic (AU-ROC) are always greater than 0. 9 for all the following tried protocols: training, leave-one out, balanced leave-two-out and 5 -fold <b>cross</b> <b>validations</b> and, finally, a stochastic series of calculations combined with a randomization test...|$|R
50|$|Generalized <b>Cross</b> <b>Validation</b> is {{so named}} becauseit uses a formula to {{approximate}} the errorthat would {{be determined by}} leave-one-out validation.It is just an approximation but works well in practice.GCVs were introduced by Craven andWahba and extended by Friedman for MARS.|$|E
50|$|The {{backward}} pass uses generalized <b>cross</b> <b>validation</b> (GCV) {{to compare the}} performance of model subsets in order to choose the best subset: lower values of GCV are better. The GCV is a form ofregularization:it trades off goodness-of-fit against model complexity.|$|E
50|$|Langille et al {{tested the}} {{accuracy}} of this genome prediction step using leave-one-out <b>cross</b> <b>validation</b> on the input set of sequenced genomes. Additional tests examined sensitivity to errors in phylogenetic inference, lack of genomic data, and {{the accuracy of}} the confidence intervals on gene content.|$|E
40|$|Several {{electrically}} large {{phased array}} feed (PAF) reflector systems are modeled {{to examine the}} mechanism of multiple reflections between parabolic reflectors and low-and high-scattering feeds giving rise to frequency-dependent patterns and impedance ripples. The PAF current is expanded in physics-based macro domain basis functions (CBFs), while the reflector employs the physical optics (POs) equivalent current. The reflector-feed coupling is systematically accounted for through a multiscattering Jacobi approach. An FFT expands the reflector radiated field {{in only a few}} plane waves, and the reflector PO current is computed rapidly through a near-field interpolation technique. The FEKO software is used for several <b>cross</b> <b>validations,</b> and the convergence properties of the hybrid method are studied for several representative examples showing excellent numerical performance. The measured and simulated results for a 121 -element Vivaldi PAF, which is installed on the Westerbork Synthesis Radio Telescope, are in very good agreement...|$|R
40|$|Viral hashtags {{spread across}} a large {{population}} of Internet users very quickly. Previous studies use features mostly in an aggregate sense to predict the popularity of hashtags, for example, {{the total number of}} hyperlinks in early tweets adopting a tag. Since each tweet is time stamped, many aggregate features can be decomposed into fine-grained time series such as a series of numbers of hyperlinks in early adopting tweets. This research utilizes frequency domain tools to analyze these time series. In particular, we apply scalogram analysis to study the series of adoption time lapses and the series of mentions and hyperlinks in early adopting tweets. Besides continuous wavelet transforms (CWTs), we also use fast wavelet transforms (FWTs) to analyze the time series. Through experiments with two sets of tweets collected in different seasons, out-of-sample <b>cross</b> <b>validations</b> show that wavelet spectral features can generally improve the prediction performance, and discrete FWT yields results as good as the more complicated CWT-based methods with scalogram analysis...|$|R
40|$|Protein–peptide {{interactions}} {{are essential for}} all cellular processes including DNA repair, replication, gene-expression, and metabolism. As most protein–peptide {{interactions are}} uncharacterized, it is cost effective to investigate them computationally as the first step. All existing approaches for predicting protein–peptide binding sites, however, are based on protein structures {{despite the fact that}} the structures for most proteins are not yet solved. This article proposes the first machine-learning method called SPRINT to make Sequence-based prediction of Protein–peptide Residue-level Interactions. SPRINT yields a robust and consistent performance for 10 -fold <b>cross</b> <b>validations</b> and independent test. The most important feature is evolution-generated sequence profiles. For the test set (1056 binding and non-binding residues), it yields a Matthews’ Correlation Coefficient of 0. 326 with a sensitivity of 64 % and a specificity of 68 %. This sequence-based technique shows comparable or more accurate than structure-based methods for peptide-binding site prediction. SPRINT is available as an online server at: [URL] Sciences, School of Information and Communication TechnologyFull Tex...|$|R
