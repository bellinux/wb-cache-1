33|13|Public
5000|$|Existing <b>concordance</b> <b>software</b> isn’t well {{equipped}} for HTML or XML files.|$|E
5000|$|The corpus and its {{annotations}} {{are provided}} {{according to the}} specifications of ISO/TC 37 SC4's Linguistic Annotation Framework. By using a freely provided transduction tool (ANC2Go), the corpus and user-chosen annotations is provided in multiple formats, including CoNLL IOB format, the XML format conformant to the XML Corpus Encoding Standard (XCES) (usable with the British National Corpus's XAIRA search engine), a UIMA-compliant format, and formats suitable for input {{to a wide variety}} of <b>concordance</b> <b>software.</b> Plugins to import the annotations into General Architecture for Text Engineering (GATE) are also available.|$|E
40|$|This study {{involved}} {{the creation of}} a corpus of children’s literature spanning 5. 5 million words. Using <b>concordance</b> <b>software,</b> the corpus was able to show the most frequent words and collocations. These will be of interest both to literary researchers in the genre of children’s literature and also teachers and applied linguists working with adult students of English...|$|E
40|$|The <b>Software</b> <b>Concordance</b> {{project is}} {{extending}} {{the concept of}} literate programming with research on how modern document and hypermedia services can improve software development environments. The <b>Software</b> <b>Concordance</b> editor is both a syntax-recognizing Java program editor and an XML document editor. It has a uniform document model, based on XML, that lets Java source code documents include both hyperlinks and inline multimedia documentation. The next step for the <b>Software</b> <b>Concordance</b> project is to enhance the editor with versioned hypermedia services {{in order to support}} sophisticated analysis and visualization of relationships among software documents. This article discusses three important issues for versioned hypermedia in this context: identification of the versioning infrastructure that will be required; exploration of how versioning information can be used to detect non-conforming documents; and the extension of versioned hypermedia to handle software configuration management...|$|R
40|$|The <b>Software</b> <b>Concordance</b> is a {{hypermedia}} {{software development}} environment exploring how document technology and versioned hypermedia can improve software document management. The <b>Software</b> <b>Concordance's</b> central tool is a document editor that integrates program analysis and hypermedia services for both source code and multimedia documentation in XML. The editor allows developers to embed inline multimedia documentation, including images and audio clips, into their program sources and to bind them to any program fragment. Web style hyperlinks are also supported. The developers {{are able to}} move seamlessly between source code and the documentation that describes its motivation, design, correctness and use without disrupting the integrated program analysis services, which include lexing, parsing and type checking...|$|R
40|$|Since {{the source}} code {{is only one}} of many types of {{documents}} that must be maintained as a software system evolves, modern software development could be improved by better interoperability between source code and other software documents. The <b>Software</b> <b>Concordance</b> is a prototype programming environment that uses hypermedia services and a uniform document model to achieve this interoperability. The <b>Software</b> <b>Concordance</b> editor allows developers to enhance their inline documentation with multimedia objects and hyperlinks, while still supporting advanced program analysis including lexing, parsing and type checking. This paper motivates the need for environments like the <b>Software</b> <b>Concordance,</b> describes the design and implementation of its program editor, and discusses lessons learned while creating it. The system is based on a uniform, Web compatible document model for both program source code and non-program software documents and a corresponding API. Important technical problems addressed by this research include creating a persistent representation of program lexemes that the analysis suite considers to be ephemeral (such as keywords), providing a simple incremental parsing system, and embedding multimedia in source code without disrupting program analysis. Based on insight gained in the process of implementing the document API, a new approach to the design of a more suitable program analysis infrastructure is suggested...|$|R
40|$|What {{happens to}} writing instructors’ {{feedback}} when {{they use a}} common rubric and an online tool to respond to student papers in a first-year composition course at a large state university in the United States? To investigate this question, we analyze the 118, 611 comments instructors made when responding to 17, 433 student essays. Using <b>concordance</b> <b>software</b> to quantify teachers’ use of rubric terms, we found instructors were primarily concerned with global, substantive, higher-order concerns—such as responding to students’ rhetorical situations, use of reason, and organization—rather than lower-order concerns about grammar or formatting. Given past research has determined teachers overemphasize lower-order concerns such as grammar, mechanics, and punctuation (Connors and Lunsford, 1988, Lunsford and Lunsford, 2008, Moxley and Joseph, 1989, Moxley and Joseph, 1992, Schwartz, 1984, Sommers, 1982 and Stern and Solomon, 2006), these results may suggest {{the possibility of a}} generational shift when it comes to response to student writing. Aggregating teacher commentary, student work, and peer review responses via digital tools and employing <b>concordance</b> <b>software</b> to identify big-data patterns illuminates a new assessment practice for Writing Program Administrators—the practice of Deep Assessment...|$|E
40|$|As checklists {{developed}} for textbook evaluation are question-able {{in terms of}} reliability and validity, other ways are being sought to bring about more systematic, efficient and objective evaluation instruments, which can provide greater insight into the strengths and weak-nesses of textbooks. With this in mind, the researchers explored the abilities of WordSmith 3. 0, a <b>concordance</b> <b>software,</b> in providing some insights into the structure of textbooks. This study will provide findings on data WordSmith 3. 0 generates automatically and semi-automatically, and how this information {{could be used in}} the evaluation of textbooks...|$|E
40|$|Multinational {{corporations}} (MNCs) {{have over}} the past decades increasingly expanded to foreign markets, changing the international operating environment and requiring the MNCs to more efficiently transfer knowledge within and between functions {{in order to stay}} competitive. Language is a critical factor for knowledge transfer and many MNCs have adopted a common corporate language. It has however been debated to what extent a shared corporate language enables effective communication between various units. This study aims to understand if a shared language is adequate in helping overcome barriers for knowledge transfer and by extension identify and analyse other potential factors that can be barriers or facilitators for knowledge transfer in a setting of a common corporate language. The study has been based on 13 semi-structured interviews and five surveys with respondents within two different MNCs in Sweden and in Singapore. The data has been analysed by using a novel method applied to the field of international business (IB) studies, that of corpus linguistics and the use of a <b>concordance</b> <b>software,</b> AntConc. The use of the <b>concordance</b> <b>software</b> enables the findings of this qualitative study to be data driven. The results confirm that a shared language is not in itself enough to ensure successful knowledge transfer. The findings further show how several barriers and facilitators present in a multilingual setting remains in a monolingual setting. A shared language setting also appears to partially change the dynamics of barriers and facilitators for knowledge transfer as it sheds light on additional barriers and facilitators for knowledge transfer that the contemporary MNC faces. MSc in International Business and Trad...|$|E
40|$|The <b>Software</b> <b>Concordance</b> {{project is}} {{examining}} how hypermedia technology can provide improved tools {{for managing the}} full range of documents produced by the software life cycle. The project's aim is to help software developers better maintain conformance between these many documents as they and the software that they describe change over time. This research requires solutions to open problems {{in a number of areas}} including document representation and formatting (especially for program source code), fine-grained version control for tree-structured documents, the categorization of relationships among software documents, and the analysis and visualization of document relationships. The <b>Software</b> <b>Concordance</b> prototype will support Java programs and XML documents and will provide tools for defining, maintaining, and analyzing document relationships. 1 Introduction Much of the advance of the human condition is based on our ability, through word-of-mouth, written documents, and other [...] ...|$|R
40|$|AbstractData-Driven Learning (DDL) is an {{approach}} {{in which the}} language learners are also research workers whose learning is driven by access to linguistic data (Johns, 1991 : 2). This essay aims at evaluating {{the potential of the}} DDL approach using <b>concordance</b> compiling <b>software</b> in language teaching in Taiwan. The advantages and disadvantages of using concordancing in the classroom and the values and limitations of using small corpora are discussed. A CALL program is explored in language teaching, particularly in the teaching of J. K. Rowling's uses of the preposition in Harry Potter and the Philosopher's Stone...|$|R
40|$|Abstract. In {{software}} development projects, process execution typically lacks automated guidance and support, and process models remain rather abstract. The environment is sufficiently dynamic that unforeseen situations can occur due to various events {{that lead to}} potential aberrations and process governance issues. To alleviate this problem, a dynamic exception handling approach for software engineering processes is presented that incorporates event detection and processing facilities and semantic classification capabilities with a dynamic process-aware information system. A scenario is used to illustrate how this approach supports exception handling with different levels of available contextual knowledge in <b>concordance</b> with <b>software</b> engineering environment relations to the development process and the inherent dynamicity of such relations...|$|R
40|$|Ankara : The Program of Teaching English as a Foreign Language Bilkent University, 2015. Thesis (Master's) [...] Bilkent University, 2015. Includes bibliographical {{references}} leaves 83 - 91. This {{study investigated}} {{the effectiveness of the}} use of a <b>concordance</b> <b>software</b> and concordance lines as a pedagogical tool to learn the target vocabulary of a text book. The {{purpose of the study was}} to compare the effects of corpus-aided vocabulary instruction with traditional vocabulary teaching methods. This study also examined the extent to which students used the target vocabulary in paragraph writing exercises. Students’ perception as to the use of concordance lines in their vocabulary learning was explored as well. Eighty-two students from four intermediate level EFL classes at Karadeniz Technical University School of Foreign Languages participated in the study. The quantitative data were collected through the administration of three tests, three writing assignments and a student questionnaire. The statistical analysis of the test results revealed that results revealed that using concordance lines in vocabulary instruction was more effective and yielded higher scores when compared to traditional vocabulary instruction with the text book. Additionally, it was found that using concordance lines in learning the target vocabulary produced similar results when compared to using a text book in less controlled paragraph writing exercises. The analysis of the student questionnaire showed that the students had positive perception about using concordance lines in learning English vocabulary. Key words: Corpus-aided language pedagogy, corpus-based approach, concordance lines, <b>concordance</b> <b>software,</b> vocabulary instruction, English vocabulary learning. Kazaz, İlknurM. S...|$|E
40|$|This paper {{discusses}} {{the use of}} corpus-based activities in the EFL classroom. Starting with Ellis’s premise that what learners can find out for themselves is better remembered than what they are simply told, it argues that analyzing and interpreting concordance lines, and drawing conclusions about certain linguistic aspects is not only motivational and engaging, {{but it is also}} very beneficial for language learners. The application of the corpus-based approach is illustrated by an exercise {{that can be used to}} teach derivative word forms, which is an important aspect of language teaching and learning, as it is one of the main ways of increasing one’s vocabulary, and studies on L 2 acquisition have shown that learners have problems recognizing and producing derivatives. Key words: corpora, <b>concordance</b> <b>software,</b> English language teaching, corpus-based activities, derivative word forms. ...|$|E
40|$|The {{research}} {{presented in}} this paper aimed to investigate the linguistic and discourse characteristics of Mr. Obama?s press conferences held after the G- 20 summits in London in 2009, in Toronto in 2010 and in Cannes in 2011. The President?s speeches were divided into two separate parts: (1) the first part during which he spoke about his plans to deepen international connections and the second during which he answered the questions of journalists from different nationalities? question/answer session ?considered as spontaneous speech text and labeled a TC, LC and CC. Using <b>concordance</b> <b>software</b> the number of types and tokens and the type-token ratio were calculated. The results show that although the number of tokens decreases chronically, in terms of the qualitative analysis all speeches display similar characteristics regarding vocabulary variation...|$|E
40|$|The <b>Software</b> <b>Concordance</b> {{project is}} {{addressing}} the software document management problem {{by providing a}} fine-grained version control model for software documents and their relationships using hypermedia versioning. A set of tools needed to maintain, visualize and analyze software documents is being constructed. This short paper presents research issues, initial results and a scheme for using hypermedia versioning and time stamps to automate detection of possible semantic non-conformance among software artifacts...|$|R
30|$|The Amsterdam-gated dynamic cardiac phantom (AGATE, Vanderwilt techniques, Boxtel, The Netherlands) was {{successively}} {{filled with}} a solution of 123 I alone, 99 mTc alone, and a mixture of 123 I and 99 mTc. A total of 12 datasets was acquired with each commercially available CZT camera (DNM 530 c, GE Healthcare and DSPECT, Biosensors International) using both energy windows (99 mTc or 123 I) with ejection fraction set to 33, 45, and 60 %. End-diastolic (EDV) and end-systolic (ESV) volumes, ejection fraction (LVEF), and regional wall motion and thickening (17 -segment model) were assessed using Cedars-Sinai QGS <b>Software.</b> <b>Concordance</b> between single- and dual-isotope acquisitions was tested using Lin’s concordance correlation coefficient (CCC) and Bland–Altman plots.|$|R
40|$|Several {{researchers}} have explored {{the use of}} hypermedia technology in software development environments (SDEs). However, existing hypermedia-based SDEs have only limited support for the evolutionary aspects of software projects. On the other hand, commercial software configuration management systems (SCMs) have had noticeable success in helping developers manage system evolution. While researchers in the hypermedia community acknowledged the need for strong version control support in their systems, they are still far from achieving this goal. The <b>Software</b> <b>Concordance</b> (SC) project is developing a SDE to experiment {{with the use of}} versioned hypermedia services for managing software documents and their logical relationships. This paper describes our versioned hypermedia framework in which hypermedia services are built on top of a SCM system and provides uniform version control supports for both software documents and their relationships...|$|R
40|$|The {{presence}} of Arabic loans in Swahili has not become subject of reliable corpus-based analyses so far. The influence of Arabic language on Swahili can be investigated on literary sources, but reference to whether writers are Muslims {{or not is}} essential for their differentiation. This article intends to investigate the {{presence of}} Arabic in contemporary prose texts written by Tanzanian authors from Zanzibar and from mainland. The electronic corpus has two sets, Tanzanian corpus and Zanzibarian corpus respectively which are almost equal in size. The reference list of Arabic loans has been extracted from two published sources. Using Concordance, a <b>concordance</b> <b>software</b> for text analysis, the frequency of words representing grammatical classes are tested. Differences in the two corpora are indicated, {{as well as some}} shared occurrences of items of Arabic origin (mostly adverbs and conjunctions) ...|$|E
40|$|Interest in ESL learner {{language}} has gained momentum since the 1990 s with {{the generation of}} learner corpora, development of robust <b>Concordance</b> <b>software</b> {{and the establishment of}} the principles of the corpus –-linguistic methodology. All these innovations have empowered researchers to investigate not only the frequent but also the idiosyncratic features of different language phenomena in learner language. This corpus-based content analysis stydy was an attempt to explore the phenomena of creativity and unnaturalness in the use of phrasal verbs in an ESL context. Findings revealed that albeit the ESL learners were competent enough in creating compositional phrasal verbs, hence creative, they often produced unusual forms in their attempt to use and create idiomatic phrasal verbs. Materials developers and teachers are, therefore, recommended to provide materials and learning activities that would enable ESL learners to more effectively acquire phrasal verbs in general and idiomatic combinations in particular...|$|E
40|$|The {{present study}} deals with English {{question}} tags {{of the same}} polarity and their Czech translation equivalents. It focuses on question tags {{from the point of}} view of their structure, formation, polarity and intonation and describes their various discourse functions. Since English question tags do not have an analogous construction in Czech, they provide an interesting construction for comparison. The discourse functions of the question tags differ according to the type of the main clause to which they are appended, i. e. declarative, imperative, interrogative or incomplete. These functions, together with the focus on the means of their translation into Czech, are analysed in detail in the empirical part of the study. As the question tags of the same polarity are not very frequent linguistic phenomenon, seventeen texts and their translations were needed to gather 102 instances by means of ParaConc, a parallel <b>concordance</b> <b>software,</b> and The British National Corpus...|$|E
40|$|Several {{analysis}} {{software packages}} for myocardial blood flow (MBF) quantification from cardiac PET studies exist, {{but they have}} not been compared using concordance analysis, which can characterize precision and bias separately. Reproducible measurements are needed for quantification to fully develop its clinical potential. Fifty-one patients underwent dynamic Rb- 82 PET at rest and during adenosine stress. Data were processed with PMOD and FlowQuant (Lortie model). MBF and myocardial flow reserve (MFR) polar maps were quantified and analyzed using a 17 -segment model. Comparisons used Pearson's correlation ρ (measuring precision), Bland and Altman limit-of-agreement and Lin's concordance correlation ρc = ρ·C b (C b measuring systematic bias). Lin's concordance and Pearson's correlation values were very similar, suggesting no systematic bias between software packages with an excellent precision ρ for MBF (ρ =  0. 97, ρc =  0. 96, C b =  0. 99) and good precision for MFR (ρ =  0. 83, ρc =  0. 76, C b =  0. 92). On a per-segment basis, no mean bias was observed on Bland-Altman plots, although PMOD provided slightly higher values than FlowQuant at higher MBF and MFR values (P < . 0001). <b>Concordance</b> between <b>software</b> packages was excellent for MBF and MFR, despite higher values by PMOD at higher MBF values. Both software packages can be used interchangeably for quantification in daily practice of Rb- 82 cardiac PET...|$|R
40|$|Abstract:- The {{purpose of}} this work is to present the design and {{implementation}} of a secure web framework that allows the creation and application of customized metric systems related to diagnoses based on medical images. The image studies are obtained from a Picture Archiving and Communications System (PACS) that was developed at the Medical Informatics and Radiological Diagnosis Centre (IMEDIR Research Centre); the web application is designed with the architectonic design pattern Model View Controller (MVC); and we opted for Java Server Pages (JSP), Servlets and Java DataBase Connectivity (JDBC) as implementation technology and deployment platform. Extensible Mark-Up Language (XML) is used to store the information related to {{the composition of the}} metrics, and to save the results of the application of these metrics to each image. The X-Rays were selected at random and proceed from patients at the Orthopedic Surgery Service of the Juan Canalejo Hospital and the Modelo Hospital at A Coruña. The system was validated by a comparative study, in which we used our tool, as well as an already validated software application, to apply the same metric system to a series of 29 digitalized total hip X-Rays. By testing our framework against previously validated methods, we obtain a measurement method that is not only in <b>concordance</b> with preceding <b>software</b> tools, but presents the advantages of secure access through the web, flexibility in the development of new metric systems, and integration with a PACS...|$|R
40|$|Abstract Background Quantification of {{lung tissue}} via {{analysis}} of computed tomography (CT) scans is increasingly common for monitoring disease progression and for planning of therapeutic interventions. The current study evaluates the quantification of human lung tissue mass by software {{analysis of a}} CT to physical tissue mass measurements. Methods Twenty-two ex vivo lungs were scanned by CT and analyzed by commercially available software. The lungs were then dissected into lobes and sublobar segments and weighed. Because sublobar boundaries are not visually apparent, a novel technique of defining sublobar segments in ex vivo tissue was developed. The tissue masses were then compared to measurements by the software analysis. Results Both emphysematous (n[*]=[*] 14) and non-emphysematous (n[*]=[*] 8) bilateral lungs were evaluated. Masses (Mean[*]±[*]SD) as measured by dissection were 651 [*]±[*] 171 [*]g for en bloc lungs, 126 [*]±[*] 60 [*]g for lobar segments, and 46 [*]±[*] 23 [*]g for sublobar segments. Masses as measured by software analysis were 598 [*]±[*] 159 [*]g for en bloc lungs, 120 [*]±[*] 58 [*]g for lobar segments, and 45 [*]±[*] 23 [*]g for sublobar segments. Correlations between measurement methods was above 0. 9 for each segmentation level. The Bland-Altman analysis found limits of agreement at the lung, lobe and sublobar levels to be − 13. 11 % to − 4. 22 %, – 13. 59 % to 4. 24 %, and – 45. 85 % to 44. 56 %. Conclusion The degree of <b>concordance</b> between the <b>software</b> mass quantification to physical mass measurements provides substantial evidence that the software method represents an appropriate non-invasive means to determine lung tissue mass. </p...|$|R
40|$|Abstract: As checklists {{developed}} for textbook evaluation are question-able {{in terms of}} reliability and validity, other ways are being sought to bring about more systematic, efficient and objective evaluation instruments, which can provide greater insight into the strengths and weak-nesses of textbooks. With this in mind, the researchers explored the abilities of WordSmith 3. 0, a <b>concordance</b> <b>software,</b> in providing some insights into the structure of textbooks. This study will provide findings on data WordSmith 3. 0 generates automatically and semi-automatically, and how this information {{could be used in}} the evaluation of textbooks. Key words: textbook evaluation, material evaluation, ESL textbooks, con-cordance software Textbook evaluation can be carried out for selection purposes or to determine the effectiveness of textbooks while they are being used. Evaluating a textbook during the selection process is known as predictive evaluation. It focuses on the potential value of the textbook. Ellis (1997) calls this an evaluation designed t...|$|E
40|$|The {{research}} {{presented in}} this paper aimed to investigate the linguistic and discourse characteristics of narratives produced by student-teachers in an ELT department. Thirty-four students from the ELT Department of Inonu University participated in the study. Each was asked to write two stories about an experience in which they were made angry (in English) and an experience in which they made someone angry (in Turkish). A total of 68 narrative texts were collected. Using <b>concordance</b> <b>software</b> the number of types and tokens and the type-token ratio were calculated; and each sentence in the text was rated as simple, coordinate, complex and coordinate+complex by the researcher to reveal the syntactic richness. The analyses suggest that although the participants? Turkish texts are richer than the English ones in terms of lexical richness, there is high parallelism between the texts in terms of syntactic richness. We believe that relatively poor vocabulary in the target language is the main reason for participants to prefer writing in their native language...|$|E
40|$|This thesis {{deals with}} existential {{quantifier}} some and its compound forms someone, somebody and something. The English pronoun some has two basic functions: pronominal and determinative. In Czech the grammatical category of definiteness is missing. The {{lack of the}} category of definiteness {{is reflected in the}} use of the translation equivalents that are manifold. In many cases there are no explicit correlates. The pronunciation of some in determinative or pronominal function is also the subject of this paper as well as the occurence of some in assertive or non-assertive contexts. The use of someone and somebody differs stylistically. The English pronominal compounds are also compared with the Czech translation correlates. By means of ParaConc, a parallel <b>concordance</b> <b>software,</b> the total number of 105 examples of some, 30 examples of something, 30 examples of someone and 9 instances of somebody was gathered. The collected material is based on both American and British English, the main sources being A Widow for One Year by John Irving and The Holy Thief by Ellis Peters...|$|E
40|$|Abstract Background The {{impact of}} {{increased}} energy resolution of cadmium–zinc–telluride (CZT) cameras on {{the assessment of}} left ventricular function under dual-isotope conditions (99 mTc and 123 I) remains unknown. The Amsterdam-gated dynamic cardiac phantom (AGATE, Vanderwilt techniques, Boxtel, The Netherlands) was successively filled with a solution of 123 I alone, 99 mTc alone, and a mixture of 123 I and 99 mTc. A total of 12 datasets was acquired with each commercially available CZT camera (DNM 530 c, GE Healthcare and DSPECT, Biosensors International) using both energy windows (99 mTc or 123 I) with ejection fraction set to 33, 45, and 60 %. End-diastolic (EDV) and end-systolic (ESV) volumes, ejection fraction (LVEF), and regional wall motion and thickening (17 -segment model) were assessed using Cedars-Sinai QGS <b>Software.</b> <b>Concordance</b> between single- and dual-isotope acquisitions was tested using Lin’s concordance correlation coefficient (CCC) and Bland–Altman plots. Results There {{was no significant difference}} between single- or simultaneous dual-isotope acquisition (123 I and 99 mTc) for EDV, ESV, LVEF, or segmental wall motion and thickening. Myocardial volumes using single- (123 I, 99 mTc) and dual-isotope (reconstructed using both 123 I and 99 mTc energy windows) acquisitions were, respectively, the following: EDV (mL) 88 [*]±[*] 27 vs. 89 [*]±[*] 27 vs. 92 [*]±[*] 29 vs. 90 [*]±[*] 26 for DNM 530 c (p[*]=[*]NS) and 82 [*]±[*] 20 vs. 83 [*]±[*] 22 vs. 79 [*]±[*] 19 vs. 77 [*]±[*] 20 for DSPECT (p[*]=[*]NS); ESV (mL) 40 [*]±[*] 1 vs. 41 [*]±[*] 2 vs. 41 [*]±[*] 2 vs. 42 [*]±[*] 1 for DNM 530 c (p[*]=[*]NS) and 37 [*]±[*] 5 vs. 37 [*]±[*] 1 vs. 35 [*]±[*] 3 vs. 34 [*]±[*] 2 for DSPECT (p[*]=[*]NS); LVEF (%) 52 [*]±[*] 14 vs. 51 [*]±[*] 13 vs. 53 [*]±[*] 13 vs. 51 [*]±[*] 13 for DNM 530 c (p[*]=[*]NS) and 52 [*]±[*] 16 vs. 54 [*]±[*] 13 vs. 54 [*]±[*] 14 vs. 54 [*]±[*] 13 for DSPECT (p[*]=[*]NS); regional motion (mm) 6. 72 [*]±[*] 2. 82 vs. 6. 58 [*]±[*] 2. 52 vs. 6. 86 [*]±[*] 2. 99 vs. 6. 59 [*]±[*] 2. 76 for DNM 530 c (p[*]=[*]NS) and 6. 79 [*]±[*] 3. 17 vs. 6. 81 [*]±[*] 2. 75 vs. 6. 71 [*]±[*] 2. 50 vs. 6. 62 [*]±[*] 2. 74 for DSPECT (p[*]=[*]NS). The type of camera significantly impacted only on ESV (p[*]<[*] 0. 001). Conclusions The new CZT cameras yielded similar results for the assessment of LVEF and regional motion using different energy windows (123 I or 99 mTc) and acquisition types (single vs. dual). With simultaneous dual-isotope acquisitions, the presence of 123 I did not impact on LVEF assessment within the 99 mTc energy window for either CZT camera...|$|R
40|$|The aim of {{this study}} is to {{investigate}} formality and contextuality in weblogs. It investigates the main formality and contextuality features principally focusing on the F-score proposed by Heylighen and Dewale (1999). The primary material comes from the online blog directory Technorati and from Blogeries. com. The method was a corpus analysis using the <b>concordance</b> <b>software</b> Wordsmith in order to carry out a linguistic analysis of the data. The main findings confirm previous findings that blogs are mainly separated into thematic and Personal blogs. In addition, the more focused the author on imparting information, the higher the F-score and consequently the lower the contextuality of the text. Authors who focused on their personal lives produced more contextual texts with lower F-scores. The study suggests that whilst the F-score is a good indicator of formality, it should not be considered an absolute indication of formality in the traditional sense, rather it should be seen as an indication of a text’s contextuality and be used as a basis from which further investigation can be developed...|$|E
40|$|Parallel <b>concordance</b> <b>software</b> {{provides}} a gen ral purpose tool that permits {{a wide range}} of investigations of translated texts, from the analysis of bilingual terminology and phraseology to the study of alternative translations of a single text. This paper outlines the main features of a Windows concordancer, ParaConc, focussing on alignment of parallel (translated) texts, general search procedures, identification of translation equivalents, and the furnishing of basic frequency information. ParaConc accepts up to four parallel texts, which might be four different languages or an original text plus three different translations. A semi-automatic alignment utility is included in the program to prepare texts that are not already pre-aligned. Simple text searches for words or phrases can be performed and the resulting concordance lines can be sorted according to the alphabetical order of the words surrounding the searchword. More complex searches are also possible, including context searches, searches based on regular expressions, and word/part-of-speech searches (assuming that the corpus is tagged for POS). Corpus frequency and collocate frequency information can be obtained. The program includes features for highlighting potential translations, including an automatic component “Hot words, ” which uses frequency information to provide information about possible translations of the searchword...|$|E
40|$|This paper {{presents}} {{the reader with}} the AraConc, software, which has been devised to extract concordances and frequency lists in Arabic by R. Abbès within the SILAT research team. (The SILAT group, ‘Systèmes d’information, Ingénierie et Linguistique Arabe, Terminologie’, {{is included in the}} ICAR Lab, CNRS/Université Lumière-Lyon 2 and ENS-LSH.). AraConc is one of the analyzers based on the DIINAR. 1 knowledge database (DIctionnaire INformatisé de l’ARabe, version 1). First, the authors introduce the functions included in the software, which are based on the specific structures of Arabic texts (many of which are shared by Semitic languages of the same group). Second, they deal with the difficulties encountered in the pre-treatment of texts, i. e. : the fact that usual Arabic script is ‘unvowelled’, the agglutinative structure of Arabic word-forms and the various morphological variations encountered. Third, they show the inadequacy of surface search techniques, and the subsequent interest of a far-fetching morpho-syntactic analysis centred on the specific structures of the language, {{when it comes to the}} building of a real and thorough <b>concordance</b> <b>software.</b> Examples of how AraConc operates will also be given...|$|E
40|$|Keyword {{analysis}} {{is a method}} used in corpus linguistics, in which keywords are generated automatically from the text {{against the background of}} a reference corpus serve as the starting point of the analysis of the text. In this thesis, keywords are retrieved from a literary text and analyzed, with a focus on their function as style markers. The theoretical part of this work centers around the description of corpus stylistics, the role of keywords in corpus stylistics and previous findings in the field by other researchers, including linguistic analyses of literary texts. This chapter is complemented by a methodological part which describes the software used in the research and its individual tools. The keyword analysis method is tested by comparing the seven novels of the Harry Potter series to a reference corpus - a subcorpus of the British National Corpus comprising fiction for children and teenage readers. The first one hundred keywords generated using the <b>concordance</b> <b>software</b> AntConc are further divided into subgroups of grammatical and stylistic words, and lexical words, which are listed together with proper nouns. The analyzed keywords illustrate {{the ways in which the}} explored text differs from comparable works of children's literature in both lexis and grammar. The concluding part evaluates [...] ...|$|E
40|$|This study {{investigates the}} {{narratives}} risk disclosures {{of the four}} British financial institutions that were adversely affected during the 2008 banking crisis. This investigation uses content analysis {{with the aid of}} <b>Concordance</b> <b>software</b> to explore the risk disclosure of these companies for the period 1998 - 2008. Risk disclosures in the Business Review sections of the annual reports of the banks were analysed into their historic and forward looking contents, and current firm performance was measured with respect to earnings per share (EPS) and dividend, while future performance was measured by growth in EPS, and positive time lag in EPS. Consistent with its predictions, the study found a significant negative relationship between the extent of historic narrative disclosures and current and future firm performance, and a significant positive relationship between forward looking narrative risk disclosures and both current and future firm performance. Additional analysis shows that optimistic and pessimistic narrative risk disclosures are not significant in explaining current and future firm performance for these firms. Findings from this study are important for users of the financial statements and regulators because they highlight an opportunity to detect warning signals for companies at risk of collapse. The study adds to the growing number of empirical investigations that provide alternative approaches to understanding business failures...|$|E
40|$|The {{purpose of}} the present work was to analyse the English gerund and its Czech {{translation}} equivalents. The gerund and its translation equivalents were gathered from two parallel literary texts: Kazuo Ishiguro's An Artist of the Floating World, translated by Jiří Hanuš, and Joanne K. Rowling's Harry Potter and the Philosopher's Stone, translated by Vladimír Medek. For the research we used ParaConc, <b>concordance</b> <b>software</b> for multilingual parallel corpora. The resulting excerpts were then analysed from the linguistic point of view in order to provide a contrastive description of the English gerund and its Czech equivalent expressions; as a background texts were used comprehensive English and Czech grammar books. The study aimed to illuminate two related areas; namely the types of Czech translation equivalents of gerunds and the correspondence of the types of Czech translation equivalents with various syntactic functions of the gerunds. The analysis provided details about the regularities that occur in the translations of the various syntactic functions of the gerund. It was also proved that the translation of the English gerund into Czech is closely associated with the interpretation of the analytic language into the syntactic language; the confrontation of the language types may be used as background to [...] ...|$|E
40|$|The {{purpose of}} this study is to {{investigate}} how genders are represented in selected sports news during the 2008 Olympics. The sports news of the 2008 Olympics from The Star online newspaper were selected as the corpus of the study, focusing on reports of selected female-appropraite sports. These female-appropriate sports were classified by Vincent et al. (2002) based on categories proposed by Metheny (1965) and Kane (1988). A discourse analysis was conducted to investigate how gender is portrayed in today’s sports media. A <b>concordance</b> <b>software</b> was used to study the data to obtain terms referring to male and female athletes. Visual images were also analysed to examine how the media portray male and female athletes on the selected articles. Results indicate that female athletes were under-represented in terms of word and photograph count. Although women were competing in the so-called ‘female-appropriate’ sports, they were negatively portrayed compared to their male counterparts. Some of the terms used for women tend to focus on appearance rather than their achievement and skills. Interestingly, the photographs attributed to male athletes were more likely to be featured as being competitive compared to the female counterparts. This study tends to support the notion that gender bias and stereotyping still exist in today’s newspapers reporting of sports...|$|E
40|$|Social {{networking}} sites (SNS) provide {{adolescents with}} opportunities for content generation {{on a wide}} range of social issues, providing unique insight into the psychosocial development of adolescence. We explored SNS webpages viewed by a random sample of adolescents during the initial uptake of SNS use (2005) to describe their general language use. Adolescents aged 14 to 17 with home Internet access were recruited using list-assisted random digit dialing methods. All SNS (MySpace) webpages viewed by participants were captured, and a large, structured set of texts (text corpus) was created from the profiles and message boards therein. Using <b>concordance</b> <b>software,</b> word frequency and keyword associations were analyzed. The 346 participants viewed approximately 28, 000 MySpace pages, yielding a 1, 147, 432 -word text corpus. Profile sections presented information about the content creator, while message boards focused more on short conversations with recipients. The most common content word was the term love. Profile owners would profess their love for activities, such as dancing, partying, or shopping, followed by their love for family, friends, and significant others. SNS offer teens an opportunity to describe and share feelings about people, places, and things connected to a range of activities and social contacts within their online and offline environments. Better understanding of SNS can offer strategies to adolescents and health care providers for insight into what connects young people in a community...|$|E
