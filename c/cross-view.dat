130|4|Public
5000|$|... #Caption: <b>Cross-view</b> of {{classical}} {{details in the}} entrance portico ...|$|E
50|$|Anaglyph {{images are}} {{much easier to}} view than either {{parallel}} (diverging) or crossed-view pairs stereograms. However, these side-by-side types offer bright and accurate color rendering, not easily achieved with anaglyphs. Recently, <b>cross-view</b> prismatic glasses with adjustable masking have appeared, that offer a wider image on the new HD video and computer monitors.|$|E
5000|$|Loreo 3D Lens in a Cap (Hong Kong), an {{accessory}} device, which incorporates {{a pair of}} small closely spaced lens, and a simple mirror box as an attachment for many modern SLR digital cameras. The latest version has 25mm wider angle lenses. Loreo also makes currently, a <b>cross-view</b> 35mm film only, 3D CAMERA, (model 321) which takes [...] "deeper" [...] stereo images, with a wider mirror system, sold with a folding print viewer included.|$|E
50|$|Prismatic glasses make <b>cross-viewing</b> easier {{as well as}} over/under-viewing possible, {{examples}} include the KMQ viewer.|$|R
40|$|Abstract. The current process {{hiding and}} {{detection}} technologies are analyzed, and {{the mechanism of}} <b>cross-views</b> discrepancy utilized by Strider GhostBuster are studied in detail, and based on Hardware-assisted Virtual Machine (HVM) a new framework for process detection is proposed, namely HCPD, whose effectiveness and integrality are verified through experiments...|$|R
2500|$|The {{establishment}} of the Ministry of Culture, as it notes, Mr. Lee gathered the various cultural groups in Singapore to stage a series of concerts (Aneka Ragam Ra’ayat) wherein <b>cross-viewing</b> of others’ ethnic performances was advocated. The success of these concerts sparked {{the idea of a}} National Theatre thus in that same year, the building was commissioned.|$|R
5000|$|On {{the forward}} {{section of the}} body, the {{mounting}} of the left-side (driver) windshield was changed from sloping inward to vertically mounted, matching the other half. In a change from its mass-transit counterpart, the quad headlights were deleted in favor of dual headlights (though quad headlights later became an option ordered by some operators). In order to properly meet regulations for school buses, the Phantom was fitted with larger sideview mirrors along with front <b>cross-view</b> mirrors. In addition, {{the design of the}} roof cap was modified to accommodate the red warning lamps fitted to school buses (with the ability to fit amber lights, for Phantom school buses sold outside of California) ...|$|E
40|$|Abstract. Recently, {{more and}} more {{approaches}} are emerging to solve the <b>cross-view</b> matching problem where reference samples and query sam-ples are from different views. In this paper, inspired by Graph Embed-ding, we propose a unified framework for these <b>cross-view</b> methods called <b>Cross-view</b> Graph Embedding. The proposed framework can not only reformulate most traditional <b>cross-view</b> methods (e. g., CCA, PLS and CDFE), but also extend the typical single-view algorithms (e. g., PCA, LDA and LPP) to <b>cross-view</b> editions. Furthermore, our general frame-work also facilitates {{the development of new}} <b>cross-view</b> methods. In this paper, we present a new algorithm named <b>Cross-view</b> Local Discrimi-nant Analysis (CLODA) under the proposed framework. Different from previous <b>cross-view</b> methods only preserving inter-view discriminant in-formation or the intra-view local structure, CLODA preserves the local structure and the discriminant information of both intra-view and inter-view. Extensive experiments are conducted to evaluate our algorithms on two <b>cross-view</b> face recognition problems: face recognition across poses and face recognition across resolutions. These real-world face recogni-tion experiments demonstrate that our framework achieves impressive performance in the <b>cross-view</b> problems. ...|$|E
40|$|<b>Cross-view</b> retrieval, {{which focuses}} on searching images as {{response}} to text queries or vice versa, has received increasing attention recently. <b>Cross-view</b> hashing is to efficiently solve the <b>cross-view</b> retrieval problem with binary hash codes. Most existing works on <b>cross-view</b> hashing exploit multi-view embedding method to tackle this problem, which inevitably causes the information loss in both image and text domains. Inspired by the Generative Adversarial Nets (GANs), this paper presents a new model that is able to Turn <b>Cross-view</b> Hashing into single-view hashing (TUCH), thus enabling the information of image to be preserved as much as possible. TUCH is a novel deep architecture that integrates a language model network T for text feature extraction, a generator network G to generate fake images from text feature and a hashing network H for learning hashing functions to generate compact binary codes. Our architecture effectively unifies joint generative adversarial learning and <b>cross-view</b> hashing. Extensive empirical evidence shows that our TUCH approach achieves state-of-the-art results, especially on text to image retrieval, based on image-sentences datasets, i. e. standard IAPRTC- 12 and large-scale Microsoft COCO...|$|E
40|$|This study problematizes the {{literacy}} practices in a fourth grade suburban classroom. Drawing on sociocultural and poststructural theories {{of language and}} literacy, this study examines the teacher-student interactions and student-student interactions within classroom literacy events. This study argues for the need for progressive pedagogy as it examines how the very practices that are implemented to support student difference also serve to marginalize opportunities for student participation within the dominant discourses that shape the classroom culture. Using Fairclough 2 ̆ 7 s three-dimensional model of critical discourse analysis (Fairclough, 1992, 1995), this study examines the interactions through moment-by-moment analysis of critical moments and contrastive cases to gain perspective on how students 2 ̆ 7 literacy identities were constructed in this classroom. The use of critical discourse analysis helped to make visible both the dominant discourses that were operating {{in the classroom and}} how they contributed to the shaping of student literacy identities. ^ The use of critical moments as a unit of analysis in this study arose from the tensions that occurred within the analysis of many literacy events, between the teachers and 3 focal students that were considered to be struggling literacy learners within the classroom. The critical moments also highlighted the tensions that occurred between the students and the dominant discourses of educational reform and differentiated instruction as they were enacted through literacy practices and teacher-student interactions. This tension, enacted as resistance, positioned the students as agentic in the construction of their own literacy identities rather than subject to the teacher 2 ̆ 7 s construction of them as struggling literacy learners and also made visible how the students contributed to the knowledge of what counted as literacy in this culture. Major themes stood out as the critical moments were <b>cross-viewed,</b> which revealed the issues of authority, agency, choice, competition, and differentiated instruction as major constructs within and across the interactions. This study demonstrates how students 2 ̆ 7 resistance to the discourses disrupted the ideologies, particularly within the discourse of differentiated instruction as students agentically constructed their literacy identities in opposition to what counted as literacy. ...|$|R
40|$|Due to the {{significant}} reduction in computational cost and storage, hashing techniques have gained increasing interests in facilitating large-scale <b>cross-view</b> retrieval tasks. Most <b>cross-view</b> hashing methods are developed by assuming that data from different views are well paired, e. g., text-image pairs. In real-world applications, however, this fully-paired multiview setting may not be practical. The more practical yet challenging semi-paired <b>cross-view</b> retrieval problem, where pairwise correspondences are only partially provided, has less been studied. In this paper, we propose an unsupervised hashing method for semi-paired <b>cross-view</b> retrieval, dubbed semi-paired discrete hashing (SPDH). In specific, SPDH explores the underlying structure of the constructed common latent subspace, where both paired and unpaired samples are well aligned. To effectively preserve the similarities of semi-paired data in the latent subspace, we construct the <b>cross-view</b> similarity graph {{with the help of}} anchor data pairs. SPDH jointly learns the latent features and hash codes with a factorization-based coding scheme. For the formulated objective function, we devise an efficient alternating optimization algorithm, where the key binary code learning problem is solved in a bit-by-bit manner with each bit generated with a closed-form solution. The proposed method is extensively evaluated on four benchmark datasets with both fully-paired and semi-paired settings and the results demonstrate the superiority of SPDH over several other state-of-the-art methods in term of both accuracy and scalability...|$|E
40|$|The {{challenge}} of person re-identification (re-id) is to match individual {{images of the}} same person captured by different non-overlapping camera views against significant and unknown <b>cross-view</b> feature distortion. While {{a large number of}} distance metric/subspace learning models have been developed for re-id, the <b>cross-view</b> transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i. e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs <b>cross-view</b> adaptation by automatically measuring camera correlation from <b>cross-view</b> visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating <b>cross-view</b> adaptation by CRAFT. Comment: To Appear in IEEE Transactions on Pattern Analysis and Machine Intelligence, 201...|$|E
30|$|Second, we {{analyzed}} a {{sensitivity of the}} number of training subjects on the recognition accuracy when we adopt the state-of-the-art <b>cross-view</b> gait recognition, i.e., CNN-based method.|$|E
40|$|Abstract—In {{some real}} world applications, like {{information}} retrieval and data classification, we often confront {{with the situation}} that the same semantic concept can be expressed using different views with similar information. Thus, how to obtain a certain Semantically Consistent Patterns (SCP) for <b>cross-view</b> data, which embeds the complementary information from different views, is of great importance for those applications. However, the heterogeneity among <b>cross-view</b> representations brings a significant challenge on mining the SCP. In this paper, we propose a general framework to discover the SCP for <b>cross-view</b> data. Specifically, aiming at building a feature-isomorphic space among different views, a novel Isomorphic Relevant Redundant Transformation (IRRT) is first proposed. The IRRT linearly maps multiple heterogeneous low-level feature spaces to a high-dimensional redundant feature-isomorphic one, which we name as mid-level space. Thus, much more complementary information from different views can be captured. Furthermore, to mine the semantic consistency among the isomorphic representations in the mid-level space, we propose a new Correlation-based Joint Feature Learning (CJFL) model to extract a unique high-level semantic subspace shared across the feature-isomorphic data. Consequently, the SCP for <b>cross-view</b> data can be obtained. Comprehensive experiments on three datasets demonstrate the advantages of our framework in classification and retrieval. Index Terms—Cross-view, cross-media, shared subspace learning, heterogeneous data, dimensionality reduction F...|$|E
40|$|In this paper, {{we propose}} a new {{approach}} for match-ing images observed in different camera views with com-plex <b>cross-view</b> transforms {{and apply it to}} person re-identification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of <b>cross-view</b> transforms. The visual fea-tures of an image pair from different views are first lo-cally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering <b>cross-view</b> transforms. They are jointly learned by utilizing sparsity-inducing norm and information theoretical regularization. This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach. 1...|$|E
40|$|Existing methods on video-based action {{recognition}} {{are generally}} view-dependent, i. e., performing recognition {{from the same}} views seen in the training data. We present a novel multiview spatio-temporal AND-OR graph (MST-AOG) representation for <b>cross-view</b> action recognition, i. e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of <b>cross-view</b> actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG {{takes advantage of the}} 3 D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3 D information and is based on 2 D video input. A new Multiview Action 3 D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for <b>cross-view</b> action recognition on 2 D videos. 1...|$|E
40|$|View {{difference}} {{is an important}} issue to deal with for robust <b>cross-view</b> gait recognition, and a view transformation model (VTM) is a popular approach for improving the accuracy degradation caused by the view difference. In this paper, we focus on the VTM with a matrix factorization process and describe our solution for <b>cross-view</b> gait recognition using the VTM. To evaluate the efficiency of our solutions, we performed experiments against publicly available large population dataset. We also report the resultsMathematics and Computer Science : Proceedings of Annual Workshop on Mathematics and Computer Science, held at Josai University on March 25 in 2014 / edited by Masatoshi IIDA, Manabu INUMA, Kiyoko NISHIZAW...|$|E
40|$|<b>Cross-view</b> video {{understanding}} {{is an important}} yet under-explored area in computer vision. In this paper, we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of <b>cross-view</b> scenes. Our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge. The proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs. Quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions. Comment: Accepted by AAAI 201...|$|E
30|$|As general {{approaches}} to <b>cross-view</b> gait recognition, {{a family of}} VTM [6, 32, 33] have been widely studied and a singular value decomposition-based VTM [5] is the most basic one among them. We then exploit it as a baseline for the generative approach.|$|E
30|$|The {{outline of}} the paper is as follows: Section 2 {{introduces}} existing gait databases, while Section 3 addresses the construction of our dataset. The performance evaluation for <b>cross-view</b> gait recognition is described in Section 4, and Section 5 presents our conclusions and discusses future work.|$|E
40|$|Existing {{techniques}} for 3 D action recognition {{are sensitive to}} viewpoint variations because they extract features from depth images which are viewpoint dependant. In contrast, we directly process pointclouds for <b>cross-view</b> action recognition from novel unknown and unseen views. We propose Histogram of Oriented Principal Components (HOPC) descriptor which is robust to noise, viewpoint, scale and ac-tion speed variations. HOPC is extracted at a point by projecting all three eigenvectors of the pointcloud within a local spatio-temporal support volume, scaled by their corresponding eigenvalues, on the vertices of a regular dodecahedron. HOPC {{is also used to}} detect Spatio-Temporal Keypoints (STK) in 3 D point-cloud sequences so that HOPC descriptors from only the relevant regions are used for action recognition. Finally, we propose a global descriptor that is extracted from the normalized spatio-temporal distribution of STKs in 4 -D. STK-Distribution and local HOPC descriptors were evaluated for <b>cross-view</b> human action recognition on the Northwestern-UCLA and UWA 3 D Multiview Activity II datasets. Comparison with six <b>cross-view</b> action recognition algorithms shows that both descriptors individually outperform all existing techniques. Since HOPC and STK-Distribution capture complimentary information, their combination significantly improves classification accuracy which is over 22 % higher than the nearest competitor on both datasets...|$|E
40|$|Nearest {{neighbor}} search methods {{based on}} hashing have attracted considerable attention for effective and efficien-t large-scale similarity search in computer vision and in-formation retrieval community. In this paper, we study {{the problems of}} learning hash functions {{in the context of}} multi-modal data for <b>cross-view</b> similarity search. We put forward a novel hashing method, which is referred to Collective Matrix Factorization Hashing (CMFH). CMFH learns u-nified hash codes by collective matrix factorization with la-tent factor model from different modalities of one instance, which can not only supports <b>cross-view</b> search but also in-creases the search accuracy by merging multiple view in-formation sources. We also prove that CMFH, a similarity-preserving hashing learning method, has upper and lower boundaries. Extensive experiments verify that CMFH sig-nificantly outperforms several state-of-the-art methods on three different datasets. 1...|$|E
30|$|Using our dataset, {{we carried}} out a {{statistically}} reliable performance comparison of <b>cross-view</b> gait recognition by various approaches. Moreover, we confirmed the effectiveness of our dataset with the largest population for recent CNN-based approaches to gait recognition which generally requires {{a large number of}} training samples but achieves the state-of-the-art performance.|$|E
30|$|In addition, we {{evaluate}} various {{approaches to}} gait recognition which are robust against view angles. By using our dataset, we can fully exploit a state-of-the-art method requiring {{a large number}} of training samples, e.g., CNN-based <b>cross-view</b> gait recognition method, and we validate effectiveness of such a family of the methods.|$|E
40|$|Learning {{an ideal}} metric {{is crucial to}} many tasks in {{computer}} vision. Diverse feature representations may combat this problem from different aspects; as visual data objects described by multiple features can be decomposed into multiple views, thus often provide complementary information. In this paper, we propose a <b>cross-view</b> fusion algorithm {{that leads to a}} similarity metric for multiview data by systematically fusing multiple similarity measures. Unlike existing paradigms, we focus on learning distance measure by exploiting a graph structure of data samples, where an input similarity matrix can be improved through a propagation of graph random walk. In particular, we construct multiple graphs with each one corresponding to an individual view, and a <b>cross-view</b> fusion approach based on graph random walk is presented to derive an optimal distance measure by fusing multiple metrics. Our method is scalable to a large amount of data by enforcing sparsity through an anchor graph representation. To adaptively control the effects of different views, we dynamically learn view-specific coefficients, which are leveraged into graph random walk to balance multiviews. However, such a strategy may lead to an over-smooth similarity metric where affinities between dissimilar samples may be enlarged by excessively conducting <b>cross-view</b> fusion. Thus, we figure out a heuristic approach to controlling the iteration number in the fusion process in order to avoid over smoothness. Extensive experiments conducted on real-world data sets validate the effectiveness and efficiency of our approach. Yang Wang, Wenjie Zhang, Lin Wu, Xuemin Lin and Xiang Zha...|$|E
40|$|In this paper, {{we propose}} a novel {{approach}} of learning mid-level filters from automatically discovered patch clus-ters for person re-identification. It is well motivated by our study on what are good filters for person re-identification. Our mid-level filters are discriminatively learned for iden-tifying specific visual patterns and distinguishing persons, and have good <b>cross-view</b> invariance. First, local patches are qualitatively measured and classified with their discrim-inative power. Discriminative and representative patches are collected for filter learning. Second, patch clusters with coherent appearance are obtained by pruning hierarchi-cal clustering trees, and a simple but effective <b>cross-view</b> training strategy is proposed to learn filters that are view-invariant and discriminative. Third, filter responses are in-tegrated with patch matching scores in RankSVM training. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK 01 dataset. The learned mid-level features are complementary to existing handcrafted low-level features, and improve the best Rank- 1 matching rate on the VIPeR dataset by 14 %. 1...|$|E
40|$|UnrestrictedMulti-view video {{sequences}} {{consist of}} a set of monoscopic video sequences captured at the same time by cameras at different locations and angles. These sequences contain 3 -D information {{that can be used to}} deliver new 3 -D multimedia services. Due to the amount of data, it is important to efficiently compress these multi-view sequences to deliver more accurate 3 -D information.; Since the captured frames by adjacent cameras have similar contents, <b>cross-view</b> redundancy can be exploited for disparity compensation. Typically both temporal and <b>cross-view</b> correlations are exploited in multi-view video coding (MVC), so that a frame can use as a reference the previous frame in time in the same view and/or a frame at the same time from an adjacent view, thus leading to a 2 -D dependency problem. The disparity of an object depends primarily on its depth in the scene, which can lead to lack of smoothness in the disparity field. These complex disparity fields are further corrupted by the brightness variations between views captured by different cameras. We propose several solutions to solve these problems in block based predictive coding in MVC.; Firstly, the 2 -D dependency problem is addressed in Chapter 2. We use the monotonicity property and the correlation between anchor and non-anchor quantizers to reduce the complexity in data collection of an optimization based on the Viterbi algorithm. The proposed bit allocation achieves 0. 5 dB coding gains as compared to MVC with fixed QP.; In Chapter 3, we propose an illumination compensation (IC) model to compensate local illumination mismatches. With about 64 % additional complexity for IC, 0. 3 - 0. 8 dB gains are achieved in <b>cross-view</b> prediction. IC techniques are extended to compensate illumination mismatches both in temporal and <b>cross-view</b> prediction.; In Chapter 4, we seek to enable compensation based on arbitrarily-shaped regions, while preserving an essentially block-based compensation architecture. To do so, we propose tools for implicit block-segmentation and predictor selection. Given two candidate block predictors, segmentation is applied to the difference of predictors. Then a weighted sum of predictors in each segment is selected for prediction. Simulation results show 0. 1 - 0. 4 dB gains as compared to the standard quad tree approach in H. 264 /AVC...|$|E
30|$|This paper {{describes}} the world’s largest gait database with wide view variation, the “OU-ISIR gait database, multi-view large population dataset (OU-MVLP)”, and {{its application to}} a statistically reliable performance evaluation of vision-based <b>cross-view</b> gait recognition. Specifically, we construct a gait dataset that includes 10, 307 subjects (5114 males and 5193 females) from 14 view angles ranging 0 ° − 90 °, 180 ° − 270 °.|$|E
30|$|However, the {{availability}} of gait recognition for uncooperative subjects induces problematic covariates, including view angle, walking speed, clothing, surface, carrying status, shoe, and time elapse. Therefore, for further progress, {{it is essential that}} gait recognition is more robust against these covariates. In this paper, we focus on the view angle {{which is one of the}} most important covariates and hence deal with <b>cross-view</b> gait recognition.|$|E
30|$|First, we {{evaluated}} performances of existing approaches to <b>cross-view</b> gait recognition: generative approaches and discriminative approaches. In addition, we considered two settings: recognition with a cooperative gallery subject (i.e., cooperative setting) {{and with an}} uncooperative gallery subject (i.e., uncooperative setting). View angles of the gallery GEIs are the same among enrolled subjects in cooperative setting, whereas they may differ for each subject in uncooperative setting.|$|E
30|$|From {{the above}} results, {{it has been}} {{confirmed}} that {{the accuracy of the}} <b>cross-view</b> gait recognition was increased significantly with {{an increase in the number}} of training subjects for CNN-based methods which requires a large training data, especially in the case of large view difference. Thus, it indicated the importance of our dataset, OU-ISIR MVLP, with a large subject number and large view variation.|$|E
40|$|We {{propose a}} Predictable Dual-View Hashing (PDH) {{algorithm}} which embeds proximity of data samples {{in the original}} spaces. We create a <b>cross-view</b> hamming space {{with the ability to}} compare information from previously incomparable domains with a notion of ‘predictability’. By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms. 1...|$|E
40|$|Finding {{relationships}} between multiple views of data is essential both for exploratory analysis and as pre-processing for predictive tasks. A prominent {{approach is to}} apply variants of Canonical Correlation Analysis (CCA), a classical method seeking correlated components between views. The basic CCA is restricted to maximizing a simple dependency criterion, correlation, measured directly between data coordinates. We introduce a new method that finds dependent subspaces of views directly optimized for the data analysis task of neighbor retrieval between multiple views. We optimize mappings for each view such as linear transformations to maximize <b>cross-view</b> similarity between neighborhoods of data samples. The criterion arises directly from the well-defined retrieval task, detects nonlinear and local similarities, is able to measure dependency of data relationships rather than only individual data coordinates, and is related to well understood measures of information retrieval quality. In experiments we show the proposed method outperforms alternatives in preserving <b>cross-view</b> neighborhood similarities, and yields insights into local dependencies between multiple views. Comment: 9 pages, 15 figures. Submitted for ICLR 2016; the authors contributed equall...|$|E
40|$|Existing {{techniques}} for 3 D action recognition {{are sensitive to}} viewpoint variations because they extract features from depth images which are viewpoint dependent. In contrast, we directly process pointclouds for <b>cross-view</b> action recognition from unknown and unseen views. We propose the Histogram of Oriented Principal Components (HOPC) descriptor that is robust to noise, viewpoint, scale and action speed variations. At a 3 D point, HOPC is computed by projecting the three scaled eigenvectors of the pointcloud within its local spatio-temporal support volume onto the vertices of a regular dodecahedron. HOPC is also used {{for the detection of}} Spatio-Temporal Keypoints (STK) in 3 D pointcloud sequences so that view-invariant STK descriptors (or Local HOPC descriptors) at these key locations only are used for action recognition. We also propose a global descriptor computed from the normalized spatio-temporal distribution of STKs in 4 -D, which we refer to as STK-D. We have evaluated the performance of our proposed descriptors against nine existing techniques on two <b>cross-view</b> and three single-view human action recognition datasets. The Experimental results show that our techniques provide significant improvement over state-of-the-art methods...|$|E
40|$|In this paper, {{we address}} the problem of <b>cross-view</b> image geo-localization. Specifically, we aim to {{estimate}} the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for <b>cross-view</b> image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations...|$|E
40|$|This paper {{presents}} a general multi-view feature extrac-tion approach {{that we call}} Generalized Multiview Analysis or GMA. GMA has all the desirable properties required for <b>cross-view</b> classification and retrieval: it is supervised, it allows generalization to unseen classes, it is multi-view and kernelizable, it affords an efficient eigenvalue based solu-tion and is applicable to any domain. GMA exploits {{the fact that most}} popular supervised and unsupervised feature ex-traction techniques are the solution of a special form of a quadratic constrained quadratic program (QCQP), which can be solved efficiently as a generalized eigenvalue prob-lem. GMA solves a joint, relaxed QCQP over different fea-ture spaces to obtain a single (non) linear subspace. Intu-itively, GMA is a supervised extension of Canonical Cor-relational Analysis (CCA), which is useful for <b>cross-view</b> classification and retrieval. The proposed approach is gen-eral and has the potential to replace CCA whenever clas-sification or retrieval is the purpose and label information is available. We outperform previous approaches for text-image retrieval on Pascal and Wiki text-image data. We re-port state-of-the-art results for pose and lighting invariant face recognition on the MultiPIE face dataset, significantly outperforming other approaches. 1...|$|E
