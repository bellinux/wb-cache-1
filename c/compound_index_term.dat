0|10000|Public
40|$|In {{this paper}} we {{address the problem of}} <b>compound</b> noun <b>indexing</b> that is about {{segmenting}} or decomposing compound nouns into promising <b>index</b> <b>terms.</b> <b>Compound</b> nouns as <b>index</b> <b>terms</b> that usually subscribe to specific notions tend to increase the precision of retrieval performance. The use of the component nouns of a <b>compound</b> noun as <b>index</b> <b>terms,</b> on the other hand, may improve the recall performance, but can decrease the precision. Our proposed method to handle compound nouns with a goal to increase the recall while preserving the precision computes the relevance of the colnponcnt nouns of a compound noun to the document content by coinparing the document sets that are snpported by the component nouns and the terms of the document. The operational content of a term is represented as the proba- bilistic distribution of the term over the document set...|$|R
40|$|We {{developed}} a fully automated Information Retrieval System which uses advanced {{natural language processing}} techniques to enhance the effectiveness of traditional key-word based document retrieval. In early experiments with the standard CACM- 3204 collection of abstracts, the augmented system has displayed capabilities that made it clearly superior to the purely statistical base system. 1. OVERALL DESIGN Our information retrieval system consists of a traditional statistical backbone (Harman and Candela, 1989) augmented with various natural language processing components that assist the system in database processing (stemming, indexing, word and phrase clustering, selectional restrictions), and translate a user's information request into an effective query. This design is a careful compromise between purely statistical non-linguistic approaches and those requiring rather accomplished (and expensive) semantic analysis of data, {{often referred to as}} 'conceptual retrieval'. The conceptual retrieval systems, though quite effective, are not yet mature enough to be considered in serious information retrieval applications, the major problems being their extreme inefficiency and the need for manual encoding of domain knowledge (Mauldin, 1991). In our system the database text is first processed with a fast syntactic parser. Subsequently certain types of phrases are extracted from the parse lxees and used as <b>compound</b> <b>indexing</b> <b>terms</b> in addition to single-word terms. The extracted phrases are statistically analyzed as syntactic contexts in order to discover a variety of similarity links between smaller subphrases and words occurring in them. A further filtering process maps these similarity links onto semantic relations (generalization, specialization, synonymy, etc.) after which they are used to transform user's request into a search query. The user's natural language request is also parsed, and all <b>indexing</b> <b>terms</b> occurring in them are identified. Next, certain highly ambiguous (usually single-word) terms are dropped, provided that they also occur as elements in some compound terms. For example, "natural " is deleted from a query already containing "natural language " becaus...|$|R
40|$|Abstract — Agentcities is {{a network}} of FIPA {{compliant}} agent platforms that constitute a distributed environment to demonstrate the potential of autonomous agents. One of {{the aims of the}} project is the development of a network architecture to allow the integration of platforms based on different technologies and models. It provides basic white pages and yellow pages services to allow the dynamic discovery of the hosted agents and the services they offer. An important outcome is the exploitation of the capability of agent-based applications to adapt to rapidly evolving environments. This is particularly appropriate to dynamic societies where agents act as buyers and sellers negotiating their goods and services, and composing simple services offered by different providers into new <b>compound</b> services. <b>Index</b> <b>Terms</b> — Autonomous agent, agent-mediated electronic commerce, agent security, ontology, web services, servic...|$|R
40|$|Identifying {{candidate}} <b>index</b> <b>terms</b> from Korean documents poses unique {{problems for}} which the methods used to index English documents are inappropriate. In particular, {{in this paper we}} address the problem of <b>compound</b> noun <b>indexing</b> that involves segmenting or decomposing com-pound nouns into promising <b>index</b> <b>terms.</b> <b>Compound</b> nouns as <b>index</b> <b>terms</b> that usually subscribe to the specific notion tend to increase the precision of retrieval perfor-mance. The indiscrete use of component nouns of a com-pound noun as <b>index</b> <b>terms,</b> on the other hand, may improve the recall performance, but can lead to loss of precision. Our proposed method of handling compound nouns with the goal of preserving the precision while attaining the recall computes the relevance of component nouns of a compound noun to the document content by comparing the document sets that are supported by the component nouns and the terms of the document. Comparison metric is divergence of two information sources. The operational content of a term is represented as the probabilistic distribution of the term over the docu-ment set. Experiments with a set of 1, 000 documents show that the proposed method out-performs other known methods, including even manual indexing, for thirty sample queries. 1...|$|R
40|$|In {{this paper}} we explore H. 264 /AVC {{operating}} in intraframe mode to compress a mixed image, i. e. composed of text, graphics and pictures. Even though mixed contents (compound) documents usually {{require the use of}} multiple compressors, we apply a single compressor for both text and pictures. For that, distortion is taken into account differently between text and picture regions. Our approach is to use a segmentation-driven adaptation strategy to change the H. 264 / AVC quantization parameter on a macroblock by macroblock basis, i. e. we deviate bits from pictorial regions to text in order to keep text edges sharp. We show results of a segmentation driven quantizer adaptation method applied to compress documents. Our reconstructed images have better text sharpness compared to straight unadapted coding, at negligible visual losses on pictorial regions. Our results also highlight the fact that H. 264 /AVC-INTRA outperforms coders such as JPEG- 2000 as a single coder for <b>compound</b> images. <b>Index</b> <b>Terms</b> — <b>Compound</b> document coding, segmentationdriven image coding, H. 264 /AV...|$|R
40|$|Can one hide an averse {{food in a}} flavorful food so {{that the}} averse food is not perceptible? Here we take a {{statistical}} signal process-ing approach to show how to optimally design a food additive (ei-ther using pure flavor compounds or natural ingredients) {{to act as a}} steganographic key for this food steganography problem. We use a synthesis-based model of olfaction that has emerged in the psychol-ogy literature and the percept known as olfactory white acts as an intermediate signal in our approach. The problem decomposes into predictive analytics and prescriptive analytics components. In the predictive component, we learn a mapping from the space of physic-ochemical descriptors of flavor compounds to the space of percep-tual odor descriptors through multivariate regression with nuclear norm regularization. In the prescriptive component, we find optimal mixtures of compounds or foods to make the averse food impercep-tible in the flavorful food by posing and solving an inverse problem with non-negativity constraints. We demonstrate the proposed ap-proach on real-world physicochemical and olfactory perception data for <b>compounds</b> in food. <b>Index</b> <b>Terms</b> — olfactory signal processing, steganography 1...|$|R
40|$|Phrase {{browsing}} applications {{provide information}} seekers {{with access to}} text content via structured lists of <b>index</b> <b>terms.</b> The <b>index</b> <b>terms,</b> which may be identified {{by a variety of}} techniques, are phrases that have been automatically extracted from full text documents. Browsing applications support interactive navigation of <b>index</b> <b>terms</b> and provide direct access to the original documents via the <b>index</b> <b>terms.</b> Term...|$|R
50|$|Many {{journals}} and databases provides access (also) to <b>index</b> <b>terms</b> made by authors to the articles being published or represented. The relative quality of indexer-provided <b>index</b> <b>terms</b> and author provided <b>index</b> <b>terms</b> {{is of interest}} to research in information retrieval. The quality of both kinds of <b>indexing</b> <b>terms</b> depends, of course, on the qualifications of provider. In general authors have difficulties providing <b>indexing</b> <b>terms</b> that characterizes his document relative to the other documents in the database. Author keywords {{are an integral part}} of literature.|$|R
40|$|AbstractThe {{purpose of}} this study is to reveal the {{characteristics}} of the terminological structure formed by the <b>index</b> <b>terms</b> of junior-high school, high school and university-level textbooks. We first identify the types of concept of <b>index</b> <b>terms,</b> and then uncover the conceptual structure that underlies <b>index</b> <b>terms.</b> We found that, as the school level progresses, (1) the balance of concept types of <b>index</b> <b>terms</b> shift from concrete objects to their behaviours or features, (2) <b>index</b> <b>terms</b> as a whole shifts from a set of fragmented terms to a single indirectly-related group, (3) the core part of the <b>index</b> <b>terms</b> comes to be occupied by terms which represent concepts other than concrete objects...|$|R
40|$|In {{this study}} we propose {{statistical}} models to model the indexing of textual documents by human indexers in a hypertext environment. Previous research revealed that hypertext links connecting text indexed with the same subject <b>index</b> <b>term</b> assigned by a human indexer offer users {{an effective way to}} access information due to the structure, focus, and imbedded intelligence of a manually assigned subject index. We propose that for a document collection with an existing well developed subject index, the contingent relationship between subject <b>index</b> <b>terms</b> and words in the text can be used by a Bayesian inference rule in predicting which subject <b>index</b> <b>term</b> is relevant given the words occurring in a document. It was hypothesized that the indexing models (1) will be able to predict what <b>index</b> <b>terms</b> a human indexer will use, (2) serve as decision aids, and (3) will help users in information retrieval tasks. This problem area is approached by (1) direct comparison between the <b>index</b> <b>terms</b> assigned by the indexing models and those assigned by a human indexer The overlap between the two sets of <b>index</b> <b>terms</b> was measured by the proportion of <b>index</b> <b>terms</b> in the intersection for each set. (2) expert ratings on the relevance of <b>index</b> <b>terms</b> assigned by different means. The ratings as well as the quality of an <b>index</b> <b>term</b> (determined by the magnitude of the rating) were statistically analyzed. Potential errors made by the human indexer and the indexing models were estimated. (3) a controlled laboratory experiment in which users 2 ̆ 7 performance in information retrieval tasks using different indices was evaluated. Results obtained revealed that (1) a significant overlap exists between the <b>index</b> <b>terms</b> assigned by the indexing models and those by a human indexer, (2) <b>index</b> <b>terms</b> suggested by the indexing models received significantly higher ratings than <b>index</b> <b>terms</b> randomly selected and the estimated number of errors made by the indexing models and the human indexer were similar, and (3) users performed better or equally well in information retrieval tasks using the subject index assigned by the indexing models than when using the index prepared by the human indexer. ...|$|R
5000|$|Let: [...] be the {{occurrence}} matrix [...] be {{the occurrence}} matrix without the <b>index</b> <b>term</b> [...] and [...] be density of [...] Then: The discrimination {{value of the}} <b>index</b> <b>term</b> [...] is: ...|$|R
40|$|This thesis {{discusses}} {{the problems and}} the methods of finding relevant information in large collections of documents. The contribution of this thesis to this problem is to develop better content analysis methods {{which can be used}} to describe document content with <b>index</b> <b>terms.</b> <b>Index</b> <b>terms</b> can be used as meta-information that describes documents, and that is used for seeking information. The main point of this thesis is to illustrate the process of developing an automatic indexer which analyses the content of documents by combining evidence from word frequencies and evidence from linguistic analysis provided by a syntactic parser. The indexer weights the expressions of a text according to their estimated importance for describing the content of a given document {{on the basis of the}} content analysis. The typical linguistic features of <b>index</b> <b>terms</b> were explored using a linguistically analysed text collection where the <b>index</b> <b>terms</b> are manually marked up. This text collection is referred to as an <b>index</b> <b>term</b> corpus. Specific features of the <b>index</b> <b>terms</b> provided the basis for a linguistic term-weighting scheme, which was then combined with a frequency-based term-weighting scheme. The use of an <b>index</b> <b>term</b> corpus like this as training material is a new method of developing an automatic indexer. The results of the experiments were promising...|$|R
40|$|The {{invention}} {{relates to}} a feature weighing computation method {{based on a}} structural constraint in a Chinese information retrieval. The method comprises the following steps: a. carrying out structuring processing to inquiry and obtaining a structuring inquiry result, wherein the structuring processing comprises one or a plurality of following steps: splitting words, carrying out part-of-speech tagging to splitted works, carrying out shallow parsing to inquiry or carrying out parsing to inquiry; b. determining an <b>index</b> <b>term</b> according to the structuring inquiry result and then determining an inquiry-context property set of the <b>index</b> <b>term</b> according to the structuring inquiry result which is adjacent to the <b>index</b> <b>term</b> and positioned in a word list; c. computing the weighing value of each property in the inquiry-context property set; d. combining the weighing values of all properties into property values of the <b>index</b> <b>terms</b> through a first composite function; and f. combining the property values of the <b>index</b> <b>terms</b> by a second composite function and obtaining the <b>index</b> <b>term</b> weighing. The method can compute the weighing accurately no matter whether the <b>index</b> <b>terms</b> exist in the word list or not. 本发明是有关于一种中文信息检索中基于结构约束的特征权重计算方法，包括以下步骤：a. 对查询进行结构化处理，得到结构化查询结果；结构化处理包括：分词、对切分出的词进行词性标注、对查询进行浅层句法分析或对查询进行句法分析中一个或几个；b. 根据述结构化查询结果确定索引词，然后根据与所述索引词相邻并位于词列表中的结构化查询的结果，确定所述索引词的查询—上下文属性集；c. 计算查询—上下文属性集中每个属性的权重值；d. 通过第一组合函数将各个属性的权重值组合成所述索引词的属性值；e. 使用第二组合函数对所述索引词的属性值组合，得到所述索引词权重。无论索引词是否在词列表中，本发明的方法都能准确计算出其权重。Department of ComputingInventor name used in this publication: 陆永邦, Lu YongbangTitle in Traditional Chinese: 中文信息檢索中基於結構約束的索引詞權重計算方法Chin...|$|R
5000|$|The {{index of}} a book may report any number of {{references}} for a given <b>index</b> <b>term,</b> and thus may be coded as a multimap from <b>index</b> <b>terms</b> to any number of reference locations or pages.|$|R
50|$|An optimal <b>index</b> <b>term</b> is {{one that}} can {{distinguish}} two different documents {{from each other and}} relate two similar documents. On the other hand, a sub-optimal <b>index</b> <b>term</b> can not distinguish two different document from two similar documents.|$|R
40|$|In {{this paper}} the GK# {{model for the}} {{construction}} of thesaurus classes based on fuzzy semantic association measure between <b>index</b> <b>terms</b> and concepts (thesaurus classes) is presented. The association measure is obtained on the basis of fuzzy semantic relations between <b>index</b> <b>terms,</b> and it is used to cluster <b>index</b> <b>terms</b> into concepts. A hierarchical algorithm is introduced which runs on a simple numerical example. # 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved...|$|R
40|$|This paper {{describes}} a technique for automatically creating an index for handwritten notes captured as digital ink. No text recognition is performed. Rather, a dictionary of possible <b>index</b> <b>terms</b> is built by clustering groups of ink strokes corresponding roughly to words. Terms whose distribution varies significantly across note pages are {{selected for the}} index. An index page containing the <b>index</b> <b>terms</b> is created, and terms are hyper-linked back to their original location in the notes. Further, <b>index</b> <b>terms</b> occurring in a note page are highlighted to aid browsing. 1...|$|R
50|$|Each record {{contains}} a bibliographic citation, abstract, <b>index</b> <b>terms</b> from the Thesaurus of Psychological <b>Index</b> <b>Terms,</b> keywords, classification categories, population information, the geographical {{location of the}} research population, and cited references for journal articles, book chapters, and books, mainly from 2001 to present. Records of books include the book's table to contents.|$|R
40|$|Most {{information}} retrieval systems use stopword lists and stemming algorithms. However, {{we have found}} that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results. We present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding <b>indexing</b> <b>terms</b> that do not. In two different domains, relevancy signatures produced better results than the simple <b>indexing</b> <b>terms.</b> These experiments suggest that stopword lists and stemming algorithms may remove or conflate many words {{that could be used to}} create more effective <b>indexing</b> <b>terms.</b> Introduction Most {{information retrieval}} systems use a stopword list to prevent common words from being used as <b>indexing</b> <b>terms.</b> Highly frequent words, such as determiners and prepositions, are not considered to be content words because they appear in virtually every document. Stopword lists are almost univer [...] ...|$|R
40|$|When {{professional}} indexers independently assign {{terms to}} a given document, the term sets generally differ between indexers. Studies of inter-indexer consistency measure the percentage of matching <b>index</b> <b>terms,</b> {{but none of them}} consider the semantic relationships that exist amongst these terms. We propose to represent multiple-indexers data in a vector space and use the cosine metric as a new consistency measure that can be extended by semantic relations between <b>index</b> <b>terms.</b> We believe that this new measure is more accurate and realistic than existing ones and therefore more suitable for evaluation of automatically extracted <b>index</b> <b>terms...</b>|$|R
40|$|Model {{the query}} as a {{sequence}} of input observations (<b>index</b> <b>terms),</b> • Model the doc as a discrete HMM composed of distribution of N-gram parameters • The relevance measure,, can be estimated by the N-gram probabilities of the <b>index</b> <b>term</b> sequence for the query,, predicted by the doc – A generative model for IR Q Nn qqqqQ [...] 21...|$|R
40|$|International audienceMost Information {{retrieval}} systems {{represent a}} query, also a document, as {{a bag of}} <b>indexing</b> <b>terms</b> without any relation between each other. This bag-based representation causes a problem for specialists when they deal with a specific domain like medical one. We present {{an alternative to the}} bag of <b>indexing</b> <b>terms</b> representation depending on semantic query structuring, in order to fulfill this need of precision in a specific domain. This structure of a query is obtained by grouping <b>indexing</b> <b>terms</b> using pre-defined categories called dimensions. These dimensions represent the different aspects that could appear in a query or a document. By using this notion, the relevant document to a given query should not only has a maximum number of shared <b>indexing</b> <b>terms</b> but also have a similar structure. Experimental results show precision improvement related to the granularity of dimensions and its distribution over the whole corpus...|$|R
50|$|An <b>index</b> <b>term,</b> subject term, subject heading, or descriptor, in {{information}} retrieval, {{is a term}} that captures {{the essence of the}} topic of a document. <b>Index</b> <b>terms</b> make up a controlled vocabulary for use in bibliographic records. They {{are an integral part of}} bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a search engine. A popular form of keywords on the web are tags which are directly visible and can be assigned by non-experts. <b>Index</b> <b>terms</b> can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with subject indexing or automatically with automatic indexing or more sophisticated methods of keyword extraction. <b>Index</b> <b>terms</b> can either come from a controlled vocabulary or be freely assigned.|$|R
5000|$|... {{one or more}} {{preferred}} <b>index</b> <b>terms</b> (at most one in each natural language) ...|$|R
40|$|A {{method of}} drawing <b>index</b> <b>terms</b> from text is presented. The {{approach}} uses no stop list, stemmer, or other language-and domain-specific component, allowing operation {{in any language}} or domain with only trivial modification. The method uses n-gram counts, achieving a function similar to, but more general than, a stemmer. The generated <b>index</b> <b>terms,</b> which the author calls “highlights, ” are suitable for identifying the topic for perusal and selection. An extension is also described and demonstrated which selects <b>index</b> <b>terms</b> to represent a subset of documents, distinguishing them from the corpus. Some experimental results are presented, showing operation in English, Spanish, German, Georgian, Russian, and Japanese...|$|R
40|$|This paper {{explores the}} {{effectiveness}} of <b>index</b> <b>terms</b> more complex than single words in conventional information retrieval systems. Retrieval is performed in two phases. In the first phase, a conventional retrieval method (the Okapi system) is used {{and in the second}} phase, complex <b>index</b> <b>terms</b> such as syntactic relations and single words with part of speech information are introduced to rerank the results of the first phase. The effectiveness of the different types of <b>index</b> <b>terms</b> are evaluated through experiments, in which the TREC- 7 test collection and 50 queries are used. The experiments show that retrieval effectiveness was improved for 32 out of 50 queries...|$|R
40|$|As an {{information}} retrieval system, PubMed® aims at providing efficient {{access to documents}} cited in MEDLINE®. For this purpose, it relies on matching representations of documents, as provided by authors and indexers to user queries. In this paper, we describe the growth of author keywords in biomedical journal articles and present a comparative study of author keywords and MeSH® <b>indexing</b> <b>terms</b> assigned by MEDLINE indexers to PubMed Central Open Access articles. A similarity metric is used to assess automatically the relatedness between pairs of author keywords and <b>indexing</b> <b>terms.</b> A set of 300 pairs is manually reviewed to evaluate the metric and characterize the relationships between author keywords and <b>indexing</b> <b>terms.</b> Results show that author keywords are increasingly available in biomedical articles and that over 60 % of author keywords {{can be linked to}} a closely related <b>indexing</b> <b>term.</b> Finally, we discuss the potential impact of this work on indexing and terminology development...|$|R
5000|$|... 2000 - Additional IV <b>Index</b> <b>terms</b> were added: 60, 90, 120, 150, 180, 360, 720 ...|$|R
5000|$|SSA (Salient Semantic Analysis) which <b>indexes</b> <b>terms</b> using salient {{concepts}} {{found in}} their immediate context.|$|R
5000|$|Extraction {{indexing}} involves taking words {{directly from}} the document. It uses natural language and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as <b>index</b> <b>terms.</b> A stop-list containing common words (such as [...] "the", [...] "and") would be referred to and such stop words would be excluded as <b>index</b> <b>terms.</b>|$|R
40|$|Experimenting with {{different}} mathematical objects for text representation {{is an important}} step of building text classification models. In order to be efficient, such objects of a formal model, like vectors, have to reasonably reproduce language-related phenomena such as word meaning inherent in <b>index</b> <b>terms.</b> We introduce an algorithm for sense-based semantic ordering of <b>index</b> <b>terms</b> which approximates Cruse’s description of a sense spectrum. Following semantic ordering, text classification by support vector machines can benefit from semantic smoothing kernels that regard semantic relations among <b>index</b> <b>terms</b> while computing document similarity. Adding expansion terms to the vector representation can also improve effectiveness. This paper proposes a new kernel which discounts less important expansion terms based on lexical relatedness. ...|$|R
40|$|This paper {{presents}} a concret model of comparative quality educational process from an university, based on particular indicators system. In {{the same time}} is presented a comparation between traditional education system and education based on total quality system [...] education, quality, competitivity, <b>compound</b> <b>indexes,</b> hierarchy...|$|R
40|$|Abstract—Thai {{language}} is {{considered as a}} non-segmented language where words are a string of symbols without explicit word boundaries, and also the structure of written Thai {{language is}} highly ambiguous. This problem causes an indexing technique has become a main issue in Thai text retrieval. To construct an inverted index for Thai texts, an <b>index</b> <b>terms</b> extraction technique is usually required to segment texts into <b>index</b> <b>term</b> schemes. Although <b>index</b> <b>terms</b> can be specified manually by experts, this process is very time consuming and labor-intensive. Word segmentation {{is one of the}} many techniques that are used to automatically extract <b>index</b> <b>terms</b> from Thai texts. However, most of the word segmentation techniques require linguistic knowledge and the preparation of these approaches is time consuming. An n-gram based approach is another automatic <b>index</b> <b>terms</b> extraction method that is often used as indexing technique for Asian languages including Thai. This approach is language independent which does not require any linguistic knowledge or dictionary. Although the n-gram approach out performs many indexing techniques for Asian languages in term of retrieval effectiveness, the disadvantage of n-gram approach is it suffers from large storage space and long retrieval time. In this paper we present the frequent max substring mining to extract <b>index</b> <b>terms</b> from Thai texts. Our method is language-independent and it does not rely on any dictionary or language grammatical knowledge. Frequent max substring mining is based on text mining that describes a process of discovering useful information or knowledge from unstructured texts. This approach uses the analysis of frequent max substring sets to extract all long and frequently-occurred substrings. We aim to employ the frequent max substring mining algorithm to address the drawback of n-gram based approach by keeping only frequent max substrings to reduce disk space requirement for storing <b>index</b> <b>terms</b> and to reduce the retrieval time in order to deal with the rapid growth of Thai texts. I...|$|R
40|$|Abstract. This paper {{presents}} a new method that aims at improving semantic indexing while {{reducing the number}} of <b>indexing</b> <b>terms.</b> <b>Indexing</b> <b>terms</b> are determined using a minimum redundancy cut in a hierarchy of conceptual hypernyms provided by an ontology (e. g. WordNet, EDR). The results of some information retrieval experiments carried out on several standard document collections using the EDR ontology are presented, illustrating the benefit of the method. ...|$|R
40|$|The {{extraction}} of the keywords that characterize each document {{in a given}} collection {{is one of the}} most important components of an Information Retrieval system. In this article, we propose to apply shallow parsing, implemented by means of cascades of finite-state transducers, to extract complex <b>index</b> <b>terms</b> based on an approximate grammar of Spanish. The effectiveness of the <b>index</b> <b>terms</b> extracted has been evaluated through the CLEF collection. ...|$|R
40|$|Automatic image {{annotation}} is {{an attractive}} approach for enabling convenient access to images found {{in a variety of}} documents. Since image captions and relevant discussions found in the text can be useful for summarizing the content of images, {{it is also possible that}} this text can be used to generate salient <b>indexing</b> <b>terms.</b> Unfortunately, this problem is generally domainspecific because <b>indexing</b> <b>terms</b> that are useful in one domain can be ineffective in others. Thus, we present a supervised machine learning approach to image annotation utilizing non-lexical features 1 extracted from image-related text to select useful terms. We apply this approach to several subdomains of the biomedical sciences and show that we are able to reduce the number of ineffective <b>indexing</b> <b>terms.</b> ...|$|R
