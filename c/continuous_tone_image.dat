33|567|Public
50|$|A <b>continuous</b> <b>tone</b> <b>image</b> is {{one where}} each color {{at any point in}} the image is {{reproduced}} as a single tone, and not as discrete halftones, such as one single color for monochromatic prints, or a combination of halftones for color prints.|$|E
5000|$|After proper drying and curing, {{the plate}} can be inked and printed. The dual {{exposures}} produce an [...] "etched" [...] polymer plate with {{many thousands of}} indentations of varying depth which hold ink, which in turn are transferred as a <b>continuous</b> <b>tone</b> <b>image</b> to a sheet of paper. Depending on the quality, the resulting print may look similar to or {{the same as those}} produced with the traditional photogravure process, though without the same amount of three-dimensional depth {{on the surface of the}} plate, and arguably, the print itself.|$|E
40|$|Digital halftoning {{is used to}} {{reproduce}} a <b>continuous</b> <b>tone</b> <b>image</b> with a printer. One of these halftoning algorithms, error diffusion, suffers from certain artifacts. One of these artifacts is commonly denoted as worms. We propose a simple measure for detection of worm artifacts. The proposed measure is evaluated by a psychophysical experiment, where 4 images were reproduced using 5 different error diffusion algorithms. The results indicate a high correlation between the predicted worms and perceived worms. 1...|$|E
50|$|TIFF/IT defines {{image file}} formats for {{encoding}} colour <b>continuous</b> <b>tone</b> picture <b>images,</b> colour line art <b>images,</b> high resolution <b>continuous</b> <b>tone</b> <b>images,</b> monochrome <b>continuous</b> <b>tone</b> <b>images,</b> binary picture images, binary line art images, screened data, {{and images of}} composite final pages.|$|R
5000|$|Screening and {{adjustment}} of a <b>continuous</b> <b>tone</b> of <b>images</b> such as photographs ...|$|R
50|$|The {{most common}} <b>continuous</b> <b>tone</b> <b>images</b> are digital {{photographs}} every single pixel {{of which can}} take a continuous range of colors depending on the quantity of captured radiance. On the other hand, at a microscopic level, developed black-and-white photographic film consists of only two colors, and not an infinite range of <b>continuous</b> <b>tones.</b> For details, see film grain. Therefore, film is a halftone medium.|$|R
40|$|A new inverse halftoning {{algorithm}} {{for restoring}} a <b>continuous</b> <b>tone</b> <b>image</b> from a given error diffusion halftone image is presented. The algorithm {{is based on}} a novel anisotropic deconvolution strategy [14, 15]. The linear model of error diffusion halftoning proposed by Kite et al. [19] is exploited. It approximates error diffusion as the sum of the convolution of the original grayscale image with a speciÞckernelandcolored random noise. Under this model the inverse halftoning can be therefore formulated as a special deconvolution problem. The deconvolution is performed following the RI-RWI (regularized inverse-regularized Wiener inverse) scheme [14] and exploiting the recently introduced anisotropic LPA-ICI estimator [15]. This adaptive varying scale estimator, based on the directional local polynomial approximation (LPA) technique and the intersection of conÞdence intervals (ICI) scale selection algorithm, allows near optimal edge adaptation. As a result, the reconstructed <b>continuous</b> <b>tone</b> <b>image</b> presents smooth areas faithful to the unknown original and yet preserves all the details found in the halftone. Conventional inverse-halftoning algorithms often produce estimates that are either oversmooth (loss of details) or still noisy. Simulation experiments conÞrm the state-of-the-art performance of the proposed algorithm, both visually and in mean-squared-error sense. 1...|$|E
40|$|Cataloged from PDF {{version of}} article. In this paper, a novel inverse halftoning method is {{proposed}} {{to restore a}} <b>continuous</b> <b>tone</b> <b>image</b> from a given half-tone image. A set theoretic formulation is used where three sets are defined using the prior information about the problem. A new spacedomain projection is introduced assuming the halftoning is performed using error diffusion, and the error diffusion filter kernel is known. The space-domain, frequency-domain, and space-scale domain projections are used alternately to obtain a feasible solution for the inverse halftoning problem which {{does not have a}} unique solution...|$|E
40|$|In this paper, a novel inverse halftoning {{method is}} {{proposed}} {{to restore a}} <b>continuous</b> <b>tone</b> <b>image</b> from a given half-tone image. A set theoretic formulation is used where three sets are defined using the prior information about the problem. A new space-domain projection is introduced assuming the halftoning is performed using error diffusion, and the error diffusion filter kernel is known. The space-domain, frequency-domain, and space-scale domain projections are used alternately to obtain a feasible solution for the inverse halftoning problem which {{does not have a}} unique solution...|$|E
50|$|Vectorization {{is usually}} {{inappropriate}} for <b>continuous</b> <b>tone</b> <b>images</b> such as portraits. The result is often poor. For example, many different image tracing algorithms {{were applied to}} a 25 kB JPEG image. The resulting vector images are at least a factor of ten larger and may have pronounced posterization effects when {{a small number of}} colors are used.|$|R
50|$|A Woodburytype {{is both a}} {{printing}} process and the print that it produces. In technical terms, the process is a photomechanical rather than a photographic one, because sensitivity to light plays {{no role in the}} actual printing. The process produces very high quality <b>continuous</b> <b>tone</b> <b>images</b> in monochrome, with surfaces that show a slight relief effect. Essentially, a Woodburytype is a molded copy of an original photographic carbon print.|$|R
40|$|<b>Continuous</b> <b>tone</b> <b>images</b> must be halftoned to be {{displayed}} on binary output {{devices such as}} printers. The ordered dither algorithm is a popular approach to halftoning. This algorithm uses a threshold matrix to approximate gray scale values. The arrangement of thresholds in the matrix determines texture artifacts introduced into the halftoned image. Thus, the challenge of research in ordered dithering {{is to find a}} matrix that results in the least visible texture artifacts...|$|R
40|$|AbstractThe spatial {{variation}} of cell size in a functionally graded cellular structure is achieved using error diffusion to convert a <b>continuous</b> <b>tone</b> <b>image</b> into binary form. Effects of two control parameters, greyscale value and resolution on the resulting cell size measures were investigated. Variation in cell edge length was greatest for the Voronoi connection scheme, particularly at certain parameter combinations. Relationships between these parameters and cell size {{were identified and}} applied to an example, where the target was to control the minimum and maximum cell size. In both cases there was an 8 % underestimation of cell area for target regions...|$|E
40|$|Different {{advantages}} {{stem from}} the digital restoration of documents of high cultural value. In this paper different techniques to digital restoration of antique documents are reported. In particular, we {{address the problem of}} virtual restoration of photographic prints. We propose a classification of defects related to their origin presenting also some non linear techniques able to restore the more diffused ones. In particular we briefly report some restoration results of water blotches, foxing and creases with some more details for a novel methodology to obtain a <b>continuous</b> <b>tone</b> <b>image</b> starting from an halftoned one, without no knowledge of the original halftoning technique. ...|$|E
40|$|The spatial {{variation}} of cell size in a functionally graded cellular structure is achieved using error diffusion to convert a <b>continuous</b> <b>tone</b> <b>image</b> into binary form. Effects of two control parameters, greyscale value and resolution on the resulting cell size measures were investigated. Variation in cell edge length was greatest for the Voronoi connection scheme, particularly at certain parameter combinations. Relationships between these parameters and cell size {{were identified and}} applied to an example, where the target was to control the minimum and maximum cell size. In both cases there was an 8 % underestimation of cell area for target regions...|$|E
40|$|This paper {{describes}} a unique method of recognizing common patterns among various images. It {{makes use of}} signatures of images which enables it to overcome the problem of different features ordering and appearance of noisy and less expressive features. It discusses two different methods of finding common set of data among various signatures. It has been then concluded that the proposed algorithm would be very suitable for <b>continuous</b> <b>tone</b> <b>images</b> with high contrast patterns. 1...|$|R
40|$|In {{this report}} we analyse the image {{reconstruction}} accuracy when using different orthogonal basis functions as the kernel for a reversible image transform. In particular {{we examine the}} Discrete Cosine Transform(DCT), Discrete Tchebichef Transform(DTT), Haar Transform, and Walsh-Hadamard Transforms(WHT). We {{have found that the}} DCT provides the greatest energy compactness properties for <b>continuous</b> <b>tone</b> images(such as photographs). For images demonstrating rapid gradient variations the Haar Transform performs significantly better {{than any of the other}} transform we have analysed, although its performance on <b>continuous</b> <b>tone</b> <b>images</b> is substantially worse than either the DCT or DTT. The WHT performs poorly on either image type...|$|R
50|$|Richard Puckett, an American photographer, {{announced}} in the March/April 2012 issue of View Camera magazine a chrysotype process that uses ascorbate (vitamin C) with ammonium ferric oxalate to print out fine-grained, <b>continuous</b> <b>tone</b> gold <b>images.</b>|$|R
40|$|The {{applicability}} of fairly standard image processing techniques to processing and analyzing large geologic data sets in addressed. Image filtering techniques {{were used to}} interpolate between gravity station locations to produce a regularly spaced data array that preserves detail in areas with good coverage, and that produces a <b>continuous</b> <b>tone</b> <b>image</b> rather than a contour map. Standard image processing techniques were used to digitally register and overlay topographic and gravity data, and the data were displayed in ways that emphasize subtle but pervasive structural features. The potential of the methods is illustrated through a discussion of linear structures that appear in the processed data between the midcontinent gravity high and the Appalachians...|$|E
40|$|We {{describe}} and implement an algorithmic framework for memory efficient, `on-the-fly' halftoning in a progressive transmission environment. Instead {{of a conventional}} approach which repeatedly recalls the <b>continuous</b> <b>tone</b> <b>image</b> from memory and subsequently halftones it for display, the proposed method achieves significant memory efficiency by storing only the halftoned image and updating it in response to additional information received through progressive transmission. Thus the method requires only a single frame-buffer of bits for storage of the displayed binary image and no additional storage {{is required for the}} contone data. The additional image data received through progressive transmission is accommodated through in-place updates of the buffer. The method is thus particularly advantageous for high resolution bi-level displays where it can result in significant savings in memory. The propose...|$|E
40|$|Abstract: In {{this paper}} we are {{simultaneously}} concerned with methods for decomposing grey scale microscope images and with methods for verifying the correctness of these decompositions. One such method is resynthesis. Resynthesis js {{viewed as a}} procedure whereby an analyzed scene can be reconstituted and subjerted to an analysis by human (informal) methods to determine the information preservation of the process. Several algorithms are presented for different ways of resynthesizing a decomposed image from its morphological decomposition analysis. In attempting to do pattern recognition with computers on <b>continuous</b> <b>tone</b> <b>image</b> sources of complex structure, one encounters the problem of decomposing the images for scene analysis. When one‘s goal in such pattern recognition is more than to assign the image to {{one of a number}} (usually small) of distinct classes, it become...|$|E
40|$|In this paper, a novel wavelet-based {{approach}} to recover <b>continuous</b> <b>tone</b> <b>images</b> from halftone images is presented. Wavelet decomposition of the halftone image facilitates {{a series of}} spatial and frequency selective processing to preserve most of the original image contents while eliminating the halftone noise. Furthermore, optional non-linear ltering can be applied as a post-processing stage to create the nal aesthetic contone image. This approach lends itself to practical applications since it is independent of parameter estimation and hence universal to all types of halftoned images, including those obtained by scanning printed halftones...|$|R
50|$|Many of the {{vectorization}} {{programs will}} group same-color pixels into lines, curves, or outlined shapes. If each possible color is grouped {{into its own}} object, there can be {{an enormous number of}} objects. Instead, the user is asked to select a finite number of colors (usually less than 256), the image is reduced to using that many colors (this step is color quantization), and then the vectorization is done on the reduced <b>image.</b> For <b>continuous</b> <b>tone</b> <b>images</b> such as photographs, the result of color quantization is posterization. Gradient fills will also be posterized.|$|R
50|$|JPEG XR (abbr. for JPEG {{extended}} range) is a still-image compression {{standard and}} file format for <b>continuous</b> <b>tone</b> photographic <b>images,</b> based on technology originally developed and patented by Microsoft {{under the name}} HD Photo (formerly Windows Media Photo). It supports both lossy and lossless compression, and is the preferred image format for Ecma-388 Open XML Paper Specification documents.|$|R
40|$|Digital halftoning quantizes a {{grayscale}} image tone bit per pixel for display and printing on binary devices. It {{is a crucial}} technique used in digital printers to convert a <b>continuous</b> <b>tone</b> <b>image</b> into a pattern {{of black and white}} dots. Halftoning is used since printers have a limited availability of inks and cannot reproduce all the color intensities in a continuous image. Error Diffusion is an algorithm in halftoning that iteratively quantizes pixels in a neighborhood dependent fashion. Halftones are created through a process called dithering. The standard error diffusion algorithm was introduced by Floyd and Steinberg. Though it has the advantage of producing high visual quality images at low cost,still it suffers from the problem of introducing worm-like artifacts in smooth regions. To overcome such problem, a chaotic and edge enhanced error diffusion method for image enhancement is proposed...|$|E
40|$|A {{generalization}} of periodic clustered-dot halftones is proposed, wherein {{the phase of}} the halftone spots is modulated using a secondary signal. The process is accomplished by using an analytic halftone threshold function that allows halftones to be generated with controlled phase variation in {{different regions of the}} printed page. The method {{can also be used to}} modulate the screen frequency, albeit with additional constraints. Visible artifacts are minimized/eliminated by ensuring the continuity of the modulation in phase. Limitations and capabilities of the method are analyzed through a quantitative model. The technique can be exploited for two applications that are presented in this paper: a) embedding watermarks in the halftone image by encoding information in phase or in frequency and b) modulating the screen frequency according to the frequency content of the <b>continuous</b> <b>tone</b> <b>image</b> in order to improve spatial and tonal rendering. Experimental performance is demonstrated for both applications...|$|E
40|$|Abstract—A {{generalization}} of periodic clustered-dot halftones is proposed, wherein {{the phase of}} the halftone spots is modulated using a secondary signal. The process is accomplished by using an analytic halftone threshold function that allows halftones to be gen-erated with controlled phase variation in {{different regions of the}} printed page. The method {{can also be used to}} modulate the screen frequency, albeit with additional constraints. Visible artifacts are minimized/eliminated by ensuring the continuity of the modulation in phase. Limitations and capabilities of the method are analyzed through a quantitative model. The technique can be exploited for two applications that are presented in this paper: a) embedding wa-termarks in the halftone image by encoding information in phase or in frequency and b) modulating the screen frequency according to the frequency content of the <b>continuous</b> <b>tone</b> <b>image</b> in order to improve spatial and tonal rendering. Experimental performance is demonstrated for both applications. Index Terms—Clustered-dot halftones, continuous phase modu-lation, halftone watermarking, spatial/tonal rendering. I...|$|E
40|$|We {{propose a}} level-successive {{encoding}} scheme for the compression of <b>continuous</b> <b>tone</b> <b>images.</b> The compressed bit stream is partitioned into individual segments corresponding to successive bits and arranged {{in order of}} decreasing significance of the bits. Bit-plane scalability is achieved by recursively partitioning the image into an LSB bit-plane and remaining higher order bits and encoding the LSB bit-plane using the remaining bits as context. This allows the scheme to achieve excellent compression performance by exploiting both spatial and inter-level correlations. We compare the proposed scheme {{with a number of}} scalable and non-scalable lossless image compression algorithms to benchmark its performance. Results indicate that the level-embedded compression incurs only a small penalty in compression efficiency over conventional lossless schemes while offering the benefit of easy bit-plane scalability...|$|R
40|$|Watermarking is {{becoming}} increasingly important for content control and authentication. Watermarking seamlessly embeds data in media that provide additional information about that media. Unfortunately, watermarking schemes {{that have been developed}} for <b>continuous</b> <b>tone</b> <b>images</b> cannot be directly applied to halftone images. Many of the existing watermarking methods require characteristics that are implicit in <b>continuous</b> <b>tone</b> <b>images,</b> but are absent from halftone images. With this in mind, it seems reasonable to develop watermarking techniques specific to halftones that are equipped to work in the binary image domain. In this thesis, existing techniques for halftone watermarking are reviewed and improvements are developed to increase performance and overcome their limitations. Post-halftone watermarking methods work on existing halftones. Data Hiding Cell Parity (DHCP) embeds data in the parity domain instead of individual pixels. Data Hiding Mask Toggling (DHMT) works by encoding two bits in the 2 x 2 neighborhood of a pseudorandom location. Dispersed Pseudorandom Generator (DPRG), on the other hand, is a preprocessing step that takes place before image halftoning. DPRG disperses the watermark embedding locations to achieve better visual results. Using the Modified Peak Signal-to-Noise Ratio (MPSNR) metric, the proposed techniques outperform existing methods by up to 5 - 20 %, depending on the image type and method considered. Field programmable gate arrays (FPGAs) are ideal for solutions that require the flexibility of software, while retaining the performance of hardware. Using VHDL, an FPGA based halftone watermarking engine was designed and implemented for the Xilinx Virtex XCV 300. This system was designed for watermarking pre-existing halftones and halftones obtained from grayscale images. This design utilizes 99 % of the available FPGA resources and runs at 33 MHz. Such a design could be applied to a scanner or printer at the hardware level without adversely affecting performance...|$|R
50|$|Such processes, as {{pioneered by}} Firmin Gillot {{represent}} a prototyping and experimental stage between the manual and process printing eras and {{are characterized by}} their utilization of various hand-originated textures and photographically transferred tones or outlines, which when combined with other color plates produced in a like manner could produce <b>continuous</b> <b>tone</b> color <b>images</b> unlike those found in similar technologies such as chromolithography.|$|R
40|$|Abstract—In this paper, {{we address}} issues {{concerning}} bilevel image compression using JPEG 2000. While JPEG 2000 {{is designed to}} compress both bilevel and <b>continuous</b> <b>tone</b> <b>image</b> data using a single unified framework, there exist significant limitations with respect to its use in the lossless compression of bilevel imagery. In particular, substantial degradation in image quality at low resolutions severely limits the resolution scalable features of the JPEG 2000 code-stream. We examine these effects and present two efficient methods to improve resolution scalability for bilevel imagery in JPEG 2000. By analyzing the sequence of rounding operations performed in the JPEG 2000 lossless compression pathway, we introduce a simple pixel assignment scheme that improves image quality for commonly occurring types of bilevel imagery. Additionally, we develop a more general strategy based on the JPIP protocol, which enables efficient interactive access of compressed bilevel imagery. It may be noted that both proposed methods are fully compliant with Part 1 of the JPEG 2000 standard. Index Terms—Bilevel image compression, binary image compression, JPEG 2000, JPIP, resolution scalability...|$|E
40|$|Some 600, 000 {{discrete}} Bouguer gravity {{estimates of}} the continental United States were spatially filtered to produce a <b>continuous</b> <b>tone</b> <b>image.</b> The filtered data were also digitally painted in color coded form onto a shaded relief map. The resultant image is a colored shaded relief map where the hue and saturation of a given image element {{is controlled by the}} value of the Bouguer anomaly. Major structural features (e. g., midcontinent gravity high) are readily discernible in these data, as are a number of subtle and previously unrecognized features. A linear gravity low that is approximately 120 to 150 km wide extends from southeastern Nebraska, at a break in the midcontinent gravity high, through the Ozark Plateau, and across the Mississippi embayment. The low is also aligned with the Lewis and Clark lineament (Montana to Washington), forming a linear feature of approximately 2800 km in length. In southeastern Missouri the gravity low has an amplitude of 30 milligals, a value that is too high to be explained by simple valley fill by sedimentary rocks...|$|E
40|$|Halftoning {{breaks the}} <b>continuous</b> <b>tone</b> <b>image</b> into {{a pattern of}} black dots and makes an {{illusion}} of a continuous image to the observer. Halftoned image is a binary version of the continuous toned image. Halftoning techniques have been widely accepted for the printing of newspapers, magazines as well as fax machines and printers. ^ Inverse halftoning is the reconstruction of continuous toned image from its halftoned version. The need for inverse halftoning arises from the need to manipulate the images, rotation or zooming etc In this thesis we deal with inverse halftoning of images halftoned by error diffusion algorithms. ^ Analysing the spectrum the low frequencies of the halftoned and original image are similar, but the halftoned image contains quantization noise in the higher frequencies. So {{the first step in}} our inverse halftoning method is the removal of high frequency quantization noise by Gaussian lowpass filtering. The second stage of our inverse halftoning algorithm is the Projection Onto Convex Sets (POCS) algorithm which alternatively projects the image onto the two convex sets, repeats and converges in the intersection of the two convex sets. (Abstract shortened by UMI.) ...|$|E
40|$|A {{study has}} been {{completed}} which quantifies the amount of misregistration allowable on a <b>continuous</b> <b>tone</b> multi-color <b>image.</b> By the use of color separation filters and a misregistration device, the actual threshold limits of registration have been derived for each individual color. The correlation between the subjective, visual impression, and the actual numerical quantity have been examined. As a result the specific threshold limits have been determined {{and can be used}} for any <b>continuous</b> <b>tone</b> imaging application...|$|R
40|$|Ordered dither is {{a popular}} method for printing/displaying <b>continuous</b> <b>tone</b> <b>images</b> on bi-level dev-ices such as laser {{printers}}, ink-jet printers and liquid crystal flat panels; {{the success of this}} algorithm comes from its simplicity and its non recursive nature. A major contention about this technique is its poor performance for rendering sharp edges. To alleviate this problem, we propose a new half-toning method called the dynamic ordered dither algorithm; it is an adaptive scheme based on the standard ordered dither operation preceded by an adaptive high pass filter. The dynamic ordered dither algorithm yields a far better edge rendition while keeping the good properties of the standard dithering process, namely the preservation of the object location, the good color reproduction and the possibility to use parallel archi-tectures for processing pictures in real time...|$|R
40|$|Abstract- Leakage {{reduction}} {{plays an}} important role in power consumption in many of the systems like image sensors. Adaptive bulk biasing control scheme is used to reduce the leakage during the standby mode of operation in the systems. Advanced WDR image sensors used to take images especially in situations where the light enters a premise from different angles, i. e. where both the dark and bright areas are there in the camera field of view that provides <b>continuous</b> <b>tone</b> <b>images.</b> Every pixel is to be converted in to halftone pixels in any of the conventional press, in order to print the image. Here a halftone pixel is generated using Floyd-Steinberg algorithm. The adaptive bulk biasing control scheme provides 21 % power reduction as compared to any other standard systems...|$|R
