0|10000|Public
5000|$|Peripheral {{equipment}} comprising subscribers' line circuits, junction terminations {{and other}} units providing {{a variety of}} special functions such as <b>coin</b> and fee <b>checking</b> <b>on</b> <b>coin</b> box calls.|$|R
40|$|Integrated Iris and MacSim {{to create}} a GPU/CPU {{simulator}} with on-chip network simulator support • Implementing functional <b>correctness</b> <b>checking</b> <b>on</b> the emulator backend and binary instrumentation via GPU Ocelot, a PTX emulator and dynamic translator to various backend processors • Extended GPU Ocelot to output instrumented data for interfacing to H-base database for storage of performance data • Added a trace generator to expose kernel parameters and GPU utilization via /proc interface Research: Lab of Computer Architecture at UT Austin under Professo...|$|R
40|$|Many {{sequential}} {{applications are}} difficult to parallelize because of unpredictable control flow, indirect data access, and inputdependent parallelism. These difficulties led us to build a software system for behavior oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading {{just part of the}} source code, or a profiling tool examining merely one or few executions. The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculation, critical-path minimization, and value-based <b>correctness</b> <b>checking.</b> <b>On</b> a recently acquired multicore, multi-processor PC, the BOP system reduced the end-to-end execution time by integer factors for a Lisp interpreter, a data compressor, a language parser, and a scientific library, with no change to the underlying hardware or operating system...|$|R
40|$|This report {{describes}} the formal specification and verification of a "Transit-Node", an abstraction of a routing {{component of a}} communication network. First, an informal definition of the Transit-Node, initially proposed within the RACE project SPECS, is formally described using the Lotos language. Then, it is verified following a model-based approach: the Lotos specification is translated into a finite LTS, and its <b>correctness</b> is <b>checked</b> <b>on</b> this model. Practically, all the verifications has been performed using Caesar-Ald' ebaran, a toolbox for the verification of Lotos programs. This work {{was carried out in}} the framework of the French project VTT (Verification, Types and Time), those goal was to compare and evaluate different verification methods on a same case study. Introduction Formal verification is a part of system design those purpose is to prove statically, on a system description, some of the properties expected on its run-time behaviour. Therefore, it offers an at [...] ...|$|R
40|$|With {{current trends}} towards more complex {{software}} system {{and use of}} higher level languages, a monitoring technique is of increasing importance for the areas such as performance enhancement, dependability, <b>correctness</b> <b>checking</b> and so <b>on.</b> In this paper, we present a formal specification-based online monitoring technique. The key idea of our technique {{is to build a}} linking system, which connects a specification animator and a program debugger. The required information about dynamic behaviors of the formal specification and concrete implementation of a target system is obtained from the animator and the debugger. Based on those information, the judgement on the consistency of the concrete implementation with the formal specification will be provided. Not embedding any instrumentation code into the target system, our monitoring technique will not alter the dynamic behavior of the target system. Animating the formal specification, rather than annotating the target system with extra formal specifications, our monitoring technique separates the implementation-dependent description of the monitored objects and the formal requirement specification of them. 1...|$|R
40|$|The {{possibility}} of applying machine learning is {{considered for the}} classification of malicious requests to a Web application. This approach excludes the use of deterministic analysis systems (for example, expert systems), {{and based on the}} application of a cascade of neural networks or perceptrons on an approximate model to the real human brain. The main idea of the work is to enable to describe complex attack vectors consisting of feature sets, abstract terms for compiling a training sample, controlling the quality of recognition and classifying each of the layers (networks) participating in the work, with the ability to adjust not the entire network, But {{only a small part of}} it, in the training of which a mistake or inaccuracy crept in.  The design of the developed network can be described as a cascaded, scalable neural network.  The developed system of intrusion detection uses a three-layer neural network. Layers can be built independently of each other by cascades. In the first layer, for each class of attack recognition, there is a corresponding network and <b>correctness</b> is <b>checked</b> <b>on</b> this network. To learn this layer, we have chosen classes of things that can be classified uniquely as yes or no, that is, they are linearly separable. Thus, a layer is obtained not just of neurons, but of their microsets, which can best determine whether is there some data class in the query or not. The following layers are not trained to recognize the attacks themselves, they are trained that a set of attacks creates certain threats. This allows you to more accurately recognize the attacker's attempts to bypass the defense system, as well as classify the target of the attack, and not just its fact. Simple layering allows you to minimize the percentage of false positives. </p...|$|R
5000|$|Code in C#for <b>checking</b> <b>correctness</b> of RUT can {{be found}} on the here: http://www.vesic.org/english/blog/c-sharp/verifying-chilean-rut-code-tax-number/ and in Clojure for <b>checking</b> <b>correctness</b> of RUT {{can be found}} on the here: https://github.com/daplay/chileno ...|$|R
40|$|Abstract. This {{article focuses}} on how to assess {{projects}} implementing business processes and IT systems on compliance with an Enterprise Architecture that provides constraints and high-level solutions. First, the core elements of Enterprise Architecture compliance testing are presented. Second, we discuss the testing process and four types of compliance <b>checks</b> (<b>correctness</b> <b>check,</b> justification check, consistency check and completeness check). Finally, an empirical case is reported in which a real-life project has been tested on conformance. The results show that our approach works. Furthermore, to increase the objectivity of compliance testing, operationalization of EA prescriptions is recommended...|$|R
40|$|Abstract: In {{this paper}} a process {{model for the}} {{development}} of system requirements specifications for railway systems is introduced. Demands of the approval of system requirements specifications, which arise from recent European railway standards, are taken into account. The aim is to obtain a system specification, which is unambiguous and easy to understand for all parties involved and in which safety aspects are considered in detail. Correlations between the development of a precise system specification, the performance of safety relevant <b>correctness</b> <b>checks</b> and the performance of risk analysis are presented. Especially the identification, specification and formalisation of safety requirements are treated with regard to <b>correctness</b> <b>checks</b> referred to safety aspects by using model checking. It is also demonstrated how different techniques of risk analysis can be supported by a system model in diagrams of the Unified Modelling Language (UML). This work has been developed in close co-operation with the Institute of Railway Systems Engineering and Traffic Safety (IfEV), Technical University of Braunschweig, Germany within the scope of the projec...|$|R
40|$|Abstract. Many {{implementations}} of cryptographic algorithms {{have shown}} to be susceptible to fault attacks. For some of them, countermeasures against specific fault models have been proposed. However, for symmetric algorithms like AES, {{the main focus of}} available countermeasures lies on performance so that their achieved error detection rates are rather low or not determinable at all. Even worse, those error detection rates only apply to specific parts of the cipher. In this paper we present a way to achieve a constantly higher error detection rate throughout the whole algorithm while assuming a much stronger adversary model than in previous papers. Furthermore, we propose solutions for two very important, unsolved questions: First, how to do secure and efficient table lookups in redundant algebras. Second, how to implement secure <b>correctness</b> <b>checks</b> to verify the result in a scenario where the adversary can manipulate comparisons. Our paper is therefore the first one to construct a sound and continuous AES fault countermeasure with an attacker-independent minimum error detection rate. Keywords: Fault attacks, countermeasure, AES, EAN+B codes, redundant table lookups, secure <b>correctness</b> <b>checks...</b>|$|R
40|$|In {{this paper}} a process {{model for the}} {{development}} of system requirements specifications for railway systems is introduced. Demands of the approval of system requirements specifications, which arise from recent European railway standards, are taken into account. The aim is to obtain a system specification, which is unambiguous and easy to understand for all parties involved and in which safety aspects are considered in detail. Correlations between the development of a precise system specification, the performance of safety relevant <b>correctness</b> <b>checks</b> and the performance of risk analysis are presented. Especially the identification, specification and formalisation of safety requirements are treated with regard to <b>correctness</b> <b>checks</b> referred to safety aspects by using model checking. It is also demonstrated how different techniques of risk analysis can be supported by a system model in diagrams of the Unified Modelling Language (UML). This work has been developed in close co-operation with the Institute of Railway Systems Engineering and Traffic Safety (IfEV), Technical University of Braunschweig, Germany within the scope of the project SafeRail (see [URL]...|$|R
3000|$|... cFor a {{detailed}} description of the α-algorithm and a proof of its <b>correctness</b> please <b>check</b> the link: [URL] [...]...|$|R
40|$|This paper {{introduces}} {{a system for}} automatic evaluation of correctness and originality of source codes submitted by students enrolled in courses dealing with computer programming. Automatic <b>correctness</b> <b>checking</b> consists of searching for plagiarisms in assignments submitted earlier and checking the correct implementation of algorithms. User interface is implemented as a Moodle module using its Plagiarism API. The complete system is published with GPLv 3 license; therefore other learning institutions can use it as well...|$|R
40|$|We {{propose a}} set of {{patterns}} for structuring a system to be dependable by design. The key idea is to localize the system’s most critical requirements into small, reliable parts called trusted bases. We describe two instances of trusted bases: (1) the end-toend check, which localizes the <b>correctness</b> <b>checking</b> of a computation to end points of a system, and (2) the trusted kernel, which ensures the safety of {{a set of}} resources with a small core of a system...|$|R
40|$|Dynamic {{instrumentation}} is {{the process}} of modifying a program’s binary instructions on the fly while the program executes. This technique is used {{in a wide variety of}} tools for performance analysis, profiling, coverage analysis, <b>correctness</b> <b>checking,</b> and testing. Instrumenting applications generated from C++ sources reveals new complications, mainly caused by the C++ exception handling mechanism. This paper presents problems, solutions and experiences specific to dynamic instrumentation of multi-threaded C++ applications for HP-UX running on IA- 64 processors. 1...|$|R
40|$|<b>Check</b> <b>correctness</b> of the {{implementation}} given the specification • Static verification – <b>Check</b> <b>correctness</b> without executing the program – E. g. static type systems, theorem provers • Dynamic verification – <b>Check</b> <b>correctness</b> by executing the program – E. g. unit tests, automatic testing • Automatic verification – Push-button verification 2 How to get the specification • Need machine-readable specification for automatic verification (not just comments) • Different variants: – Eiffel‘s „Design by Contract“ • With new across construct even features quantifier...|$|R
40|$|A brief {{overview}} of several recent model-based testing and verification projects that I’ve been involved in. These include our book on “Practical Model-Based Testing”, the Jumble mutation analysis tool for JUnit tests, unit testing of Z specifications with positive and negative tests, <b>correctness</b> <b>checking</b> of the JStar parallel programming language using SMT solvers, {{and work with the}} Whiley verified programming language in collaboration with Victoria University of Wellington. The common theme is getting computers to automate more of the checking for errors in our programs...|$|R
40|$|Abstract – This paper {{presents}} an integrated approach to verification and testing automation of UML projects. It consists of automatic model creation from UML specifications {{in the formal}} language of basic protocols, model’s verification by the means of VRS technology and automatic tests generation in TTCN language using TAT. The actuality of this task arises from necessity of software functionality’s <b>correctness</b> <b>checking,</b> including verification and testing, but there is lack of industrial technologies which allow integrating these two activities. Results of the developed approach piloting are also described. I...|$|R
40|$|Abstract Graphical {{workflow}} modeling tools, such as UML and DAG, {{can facilitate}} users to express workflow process logic, but lack of abilities {{to carry out}} simula-tion and <b>correctness</b> <b>checking.</b> In this paper, based on Petri net, we propose a service composition oriented Grid workflow model and its related six elementary workflow patterns: sequence, condition, iteration, concurrency, synchronization, and trigger-ing. In addition, we present our Grid workflow analysis approaches on three aspects: workflow reachability verification, workflow deadlock verification, and workflow op-timization. The experimental results show that our workflow verification and opti-mization mechanisms are feasible and efficient...|$|R
40|$|By {{considering}} Sudoku as a language, where a Sudoku puzzle is {{an instance}} of the language, {{we are able to}} apply meta-model-based technologies for the implementation of Sudoku, including <b>correctness</b> <b>checking</b> of a puzzle and solving strategies. The description of Sudoku includes not only the structure of Sudoku, but also covers constraints, textual representation, graphical representation, and behaviour (transformation and execution). We have created a meta-model-based description of Sudoku and use this as a basis for a comparative implementation in Eclipse with various plug-ins, and in Visual Studio with DSL-tools. ...|$|R
40|$|We {{extend the}} offline memory <b>correctness</b> <b>checking</b> scheme {{presented}} by Blum et. al [BEG 91] {{to develop an}} offline checker that can detect attacks by active adversaries. We introduce the concept of incremental multiset hashes, and detail one example: MSet-XOR MAC, which uses a secret key, and is efficient as updating the hash costs a few hash and XOR operations. Using multiset hashes as our underlying cryptographic tool, we introduce a primitive, bag integrity checking, to explain offline integrity checking; we demonstrate how this primitive {{can be used to}} build cryptographically secure integrity checking schemes for random access memories and disks. Recen...|$|R
40|$|This work {{develops}} a framework {{derived from the}} proof carrying code approach to ensure that code received and executed by an agent does not violate {{the requirements of the}} recipient. Unlike the proof carrying code approach, our framework requires the code recipient to perform the necessary <b>correctness</b> <b>checks.</b> The advantages of our framework include the specification and verification of policy driven properties, less code to be shipped to the recipient and the ability to use a variety of verification tools. This paper describes the general framework and presents a few examples which illustrate its potential uses...|$|R
40|$|Existing {{computational}} {{solutions for}} stepwise <b>correctness</b> <b>checking</b> of free-response solution schemes consisting of equations only consider providing qualitative feedbacks. Hence, this research intends {{to propose a}} computational model of a typical stepwise <b>correctness</b> <b>checking</b> of a scheme of student-constructed responses normally (usually) performed by a human examiner with the provision of quantitative feedbacks. The responses are worked solutions on solving linear algebraic equations in one variable. The proposed computational model comprises of computational techniques of key marking processes, and has enabled a marking engine prototype, which has been developed based on the model, to perform stepwise <b>correctness</b> <b>checking</b> and scoring of the response of each step in a working scheme and of the working scheme as well. The assigned numeric score of each step, or analytical score, serves as a quantitative feedback to inform students {{on the degree of}} correctness of the response of a particular step. The numeric score of the working scheme, or overall score, indicates the degree of correctness of the whole working scheme. Existing computational solutions that are currently available determine response correctness based on mathematical equivalence of expressions. In this research, the degree of correctness of an equation is based on the structural identicalness of the constituting mathtokens, which is evaluated using a correctness measure formulated in this research. The experimental verification shows that the evaluation of correctness by the correctness measure is comparable to human judgment on correctness. The computational model is formalized mathematically by basic concepts from Multiset Theory, while the process framework is supported by basic techniques and processes problem of this research. The data used are existing worked solutions on solving linear algebraic equations in one variable from a previous pilot study as well as new sets of test of correctness shows that the computational model is able to generate the expected output. Hence, the underlying computational techniques of the model can be regarded as correct. The agreement between the automated and the manual marking methods were analysed in terms of the agreement between the correctness scores. The method agreement analyses were conducted in two phases. The analysis in Phase I involved a total of 561 working schemes which comprised of 2021 responses and in Phase II a total of 350 working schemes comprising of 1385 responses were used. The analyses involved determining the percent agreement, degree of correlation and degree of agreement between the automated and manual scores. The accuracy of the scores was determined by calculating the average absolute errors present in the automated scores, which are calibrated by the average mixed errors. The results show that both the automated analytical scores and the automated overall scores exhibited high percent agreement, high correlation, high degree of agreement and small average absolute and mixed errors. It can be inferred that the automated scores are comparable with manual scores and that the stepwise <b>correctness</b> <b>checking</b> and scoring technique of this research agrees with the human marking technique. Therefore, the computational model of stepwise quantitative assessment is a valid and reliable model to be used in place of a human examiner to check and score responses to similar questions used in this research for both formative and summative assessment settings...|$|R
5000|$|... #Caption: <b>Checking</b> <b>correctness</b> of a {{timestamp}} {{generated by}} a time stamping authority (TSA) ...|$|R
40|$|Abstract. This paper {{describes}} {{the integration of}} Squolem, Quantified Boolean Formulas (QBF) solver, with the interactive theorem prover HOL Light. Squolem generates certificates of validity {{which are based on}} witness functions. The certificates are checked in HOL Light by con-structing proofs based on these certificates. The presented approach al-lows HOL Light users to prove larger valid QBF problems than before and provides <b>correctness</b> <b>checking</b> of Squolem’s outputs based on the LCF approach. An error in Squolem was discovered thanks to the integration. Experiments show that the feasibility of the integration is very sensitive to implementation of HOL Light and used inferences. This resulted in improvements in HOL Light’s inference system. ...|$|R
40|$|Following {{previous}} work on displaying static data dependences and experience with large sets of dependence displaying strategies, {{we developed a}} tool for visualizing dynamic data dependences. Our prototype {{is based on a}} modified Lisp interpreter and this paper presents our evaluation of its application to a highly complex AI program. This permitted us to build efficient visualizations and to evaluate the benefits of using dynamic dependences for program understanding, debuging and <b>correctness</b> <b>checking.</b> In this paper, we present our prototype, detailing especially the different visualizations we introduced to allow users to deal with hard to understand programs, and we discuss our findings working with dynamic dependencies. 1...|$|R
40|$|Corpus {{annotation}} {{practices for}} most major languages have moved the focus, within the NLP research community, on low-level linguistic analysis. This trend {{is reinforced by}} the relative failure of grammar based applications and the need for fast and robust document processing for tasks as varied as information indexing or language <b>correctness</b> <b>checking.</b> This paper describes a set of tools for morpho-syntactic annotation of Spanish texts, based both on well known public domain tools (emerging from MULTEXT) and proprie-tary technologies (as Constraint Grammars). Besides, a com-plete bunch of new specific modules ranging from morpho-logical analyzers to form, typographical and morpho-syntactic checkers have been integrated in this NLP tool. 1. Overvie...|$|R
40|$|Digital's Alpha-based DECchip 21164 {{processor}} was verified extensively {{prior to}} fabrication of silicon. This simulation-based verification effort used implementation-directed, pseudorandom exercisers which were supplemented with implementation-specific, hand-generated tests. Special emphasis {{was placed on}} the tasks of checking for correct operation and functional coverage analysis. Coverage analysis shows where testing is incomplete, under the assumption that untested logic often contains bugs. Correctness checkers are various mechanisms (both during and after simulation) that monitor a test to determine if it was successful. This paper details the coverage analysis and <b>correctness</b> <b>checking</b> techniques that were used. We show how our methodology and its implementation was successful, and we discuss the reasons why this methodolog...|$|R
50|$|He {{is known}} only through {{inscriptions}} <b>on</b> <b>coins.</b> His name appears <b>on</b> <b>coins</b> minted c. AD 30-60, paired {{with the name}} Volisios, who {{is thought to have}} been an ally or co-rulers.|$|R
50|$|The coins of the Pandyas, which {{bore the}} fish symbols, were termed as 'Kodandaraman' and 'Kanchi' Valangum Perumal'. The Chola {{standing}} and the seated king type coins had the titles 'Bhutala Ellamthalai', 'Parasurama', 'Kulasekhara'. Apart from these, 'Ellamthalaiyanam' was seen <b>on</b> <b>coins</b> {{which had the}} standing king {{on one side and}} the fish on the other. 'Samarakolahalam' and 'Bhuvanekaviram' were found <b>on</b> the <b>coins</b> having a Garuda, 'Konerirayan' <b>on</b> <b>coins</b> having a bull and 'Kaliyugaraman' <b>on</b> <b>coins</b> that depict a pair of feet.|$|R
50|$|Spalagadames was {{mentioned}} <b>on</b> <b>coins</b> {{in the name}} of king Vonones, as the son of Spalahores, and later <b>on</b> <b>coin</b> of Spalirises as king, where he is again introduced as the son of Spalahores.|$|R
40|$|Abstract. Behavioural {{protocols}} are {{beneficial to}} Component-Based Software Engineering and Service-Oriented Computing as they foster automatic procedures for discovery, composition, composition <b>correctness</b> <b>checking</b> and adaptation. However, resulting composition models (e. g., orchestrations or adaptors) often contain redundant or useless parts yielding the state explosion problem. Mechanisms {{to reduce the}} state space of behavioural composition models are therefore required. While reduction techniques are numerous, e. g., in the process algebraic framework, none is suited to compositions where provided/required services correspond to transactions of lower-level individual event based communications. In this article we address this issue through {{the definition of a}} dedicated model and reduction techniques. They support transactions and are therefore applicable to service architectures. ...|$|R
40|$|Abstract. GPU based {{computing}} {{has made}} significant strides in recent years. Unfortunately, GPU program optimizations can introduce subtle concurrency errors, and so incisive formal bug-hunting methods are essential. This paper presents a new formal bug-hunting method for GPU programs that combine barriers and atomics. We present an algorithm called conflict-directed delay-bounded scheduling algorithm (CD) that exploits the occurrence of conflicts among atomic synchronization commands to trigger the generation of alternate schedules; these alternate schedules are executed in a delay-bounded manner. We formally describe CD, and present two <b>correctness</b> <b>checking</b> methods, one based on final state comparison, {{and the other on}} user assertions. We evaluate our implementation on realistic GPU benchmarks, with encouraging results. ...|$|R
40|$|HP Caliper is an {{architecture}} for software developer tools {{that deal with}} executable (binary) programs. It provides a common framework that allows building {{of a wide variety}} of tools for doing performance analysis, profiling, coverage analysis, <b>correctness</b> <b>checking,</b> and testing. HP Caliper uses a technology known as dynamic instrumentation, which allows program instructions to be changed on-the-fly with instrumentation probes. Dynamic instrumentation makes HP Caliper easy to use: It requires no special preparation of an application, supports shared libraries, collects data for multiple threads, and has low intrusion and overhead. This paper describes HP Caliper for HP-UX, running on the IA- 64 (Itanium) processor. It describes Caliper’s architecture, dynamic instrumentation algorithm, and the experiences gathered during its implementation. 1...|$|R
40|$|This report {{describes}} {{a framework for}} achieving flexible scheduling in the Real-Time Specification for Java (RTSJ), and provides verification of its operation by modelling it as a system of timed automata in the UPPAAL model checker. The proposed approach is a two-level scheduling mechanism where the first level is the RTSJ priority scheduler and the second level is under application control. Minimum, backward-compatible changes to the RTSJ specification are discussed. The only assumptions made are that the RTSJ supports a native thread model, and that the underlying real-time operating system supports pre-emptive priority-based dispatching of threads with changes to priorities having immediate effect. The framework model is described and its <b>correctness</b> <b>checked.</b> 1...|$|R
50|$|This is {{the form}} under which Ēl/Cronus appears <b>on</b> <b>coins</b> from Byblos from {{the reign of}} Antiochus IV (175-164 BCE) four spread wings and two folded wings, leaning on a staff. Such images {{continued}} to appear <b>on</b> <b>coins</b> until after the time of Augustus.|$|R
