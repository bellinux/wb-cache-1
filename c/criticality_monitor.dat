1|17|Public
40|$|The need to {{prioritize}} maintenance activities and investments based on asset criticality and associated risk is seen increasingly {{as important in}} industry. However, proper use of criticality in developing maintenance strategies and plans is still at a nascent stage in most organisations. A review of industrial practices showed that criticality is considered as {{more or less a}} static quantity that is not updated with sufficient frequency as the operating environment changes. This paper examines an electricity distribution network operator (DNO) to show the need to model the changing nature of criticality and ensure an optimal maintenance strategy and plan, aligned to business needs. A Dynamic Criticality Based Maintenance (DCBM) methodology is proposed to identify factors affecting and influencing changes to <b>criticality,</b> <b>monitor</b> and update asset criticality and exploit the dynamic criticality to optimise maintenance decisions. Asset criticality was calculated using network performance, safety, environmental integrity, maintenance cost among other factors as the consequence categories for asset failure. The criticality for each asset (such as transformer circuit breakers, busbars etc.) is calculated as a weighted sum of the impact of supply loss on each of the consequence categories. Variations in some factors such as electricity demand influences changes in asset criticality with time and therefore criticality is modelled as a dynamic process, which is a function of time in addition to other factors. The performance measure used for the maintenance plan is based on the utility network reliability (quality of service) which is measured in terms of Customer Interruptions (CI) and Customer Minutes Lost (CML). The performance targets (for CIs & CMLs) and standard service levels for DNOs are given in the UK's Office of Gas and Electricity Markets (OFGEM). The results showed evidence of changing asset criticality in the network. The benefit of reviewing maintenance plans based on changing criticality was also highlighted...|$|E
40|$|Monitoring {{of higher}} {{actinides}} (HA [...] includes neptunium, plutonium, americium, and curium) during {{the separation of}} used nuclear fuel {{has been identified as}} a critical research area in the U. S. Advanced Fuel Cycle Initiative (AFCI). Recycling of used fuel by chemically separating it into uranium, fission products, and HA would be the first step in this new fuel cycle. The Material Protection, Accounting, and Control (MPAC) are necessary for materials accounting, <b>criticality</b> <b>monitoring,</b> and assurance of proliferation resistance. The objective of the MPAC project is to develop technology to detect and accurately measure quantities of higher actinides in used fuel assemblies and processing systems without taking frequent samples. Process systems may include separations batches, pipelines, storage tanks, and fuel fabrication equipment. A variety of measurements may be combined to calculate flow rates of actinide elements with a to be- determined precision...|$|R
40|$|International audienceThe {{dependability}} of open multi-agent {{systems is}} a particular concern, notably because of their main characteristics as decentralization and no single point of control. This paper describes an approach to increase the availability of such systems through a technique of fault tolerance known as agent replication, and to increase their reliability through a mechanism of agent interaction regulation called law enforcement mechanism. Therefore, we combine two frameworks: one for law enforcement, named XMLaw, and another for agent adaptive replication, named DimaX, in which the decision of replicating an agent {{is based on a}} dynamic estimation of its criticality. Moreover, we will describe how we can reuse some of the information expressed by laws in order to help at the estimation of agent criticality, thus providing a better integration of the two frameworks. At the end of the paper, we recommend a means to specify <b>criticality</b> <b>monitoring</b> variation through a structured argumentation approach that documents the rationale around the decisions of the law elements derivation...|$|R
40|$|This Compliance Program Guidance Package {{identifies}} the regulatory guidance and industry {{codes and standards}} addressing radiation protection equipment, instrumentation, and support facilities considered to be appropriate for radiation protection at the Monitored Geologic Repository (MGR). Included are considerations relevant to radiation monitoring instruments, calibration, contamination control and decontamination, respiratory protection equipment, and general radiation protection facilities. The scope of this Guidance Package does not include design guidance relevant to <b>criticality</b> <b>monitoring,</b> area radiation monitoring, effluent monitoring, and airborne radioactivity monitoring systems since they {{are considered to be}} the topics of specific design and construction requirements (i. e., ''fixed'' or ''built-in'' systems). This Guidance Package does not address radiation protection design issues; it addresses the selection and calibration of radiation monitoring instrumentation {{to the extent that the}} guidance is relevant to the operational radiation protection program. Radon and radon progeny monitoring instrumentation is not included in the Guidance Package since such naturally occurring radioactive materials do not fall within the NRC's jurisdiction at the MGR...|$|R
30|$|While {{these works}} propose models and {{evaluate}} I 2 C integration, they explore different domains with {{different levels of}} <b>criticality.</b> E-health <b>monitoring</b> systems, {{the focus of our}} study, has specific constraints not dealt with in the aforementioned works including the load generated by the sensors. Authors in [11] propose a model to represent and evaluate the security of information flow in IoT systems integrated with cloud, using a medical application as an example. They analyzed how service provider availability affects the security of the information flow. Similarly, the authors in [11] use a medical application as a case study for their proposed model analyzing the security of the information flow in IoT systems integrated with cloud infrastructures. In [12], the authors propose a framework that enables multiple applications to share IoT computational devices for health monitoring. The use case scenario in [13] is a wearable IoT architecture for health care systems. In [14], the authors propose stochastic models to represent a health service relying on mobile cloud computing infrastructure (i.e. cloud infrastructure, wireless communication and mobile device). Experiments were conducted considering scenarios with different wireless communication channels (Wi-Fi and 4 G), different battery discharge rates, and different timeouts.|$|R
40|$|AbstractDecarbonisation of {{existing}} infrastructure systems requires a dynamic roll-out of technology at an unprecedented scale. The potential disruption in supply of critical materials could endanger such {{a transition to}} low-carbon infrastructure and, by extension, compromise energy security more broadly because low carbon technologies are reliant on these materials {{in a way that}} fossil-fuelled energy infrastructure is not. Criticality is currently defined as the combination of the potential for supply disruption and the exposure of a system of interest to that disruption. We build on this definition and develop a dynamic approach to quantifying <b>criticality,</b> which <b>monitors</b> the change in criticality during the transition towards a low-carbon infrastructure goal. This allows us to assess the relative risk of different technology pathways to reach a particular goal and reduce the probability of being ‘locked in’ to currently attractive but potentially future-critical technologies. To demonstrate, we apply our method to criticality of the proposed UK electricity system transition, with a focus on neodymium. We anticipate that the supply disruption potential of neodymium will decrease by almost 30 % by 2050; however, our results show the criticality of low carbon electricity production increases ninefold over this period, as a result of increasing exposure to neodymium-reliant technologies...|$|R
40|$|Abstract—When {{operating}} in volatile environments, service-based systems (SBSs) that are dynamically composed from component services must be monitored {{in order to}} guarantee timely and successful delivery of outcomes in response to user requests. However, monitoring consumes resources and very often impacts {{on the quality of}} the SBSs being monitored. Such resource and system costs need to be considered in formulating monitoring strategies for SBSs. The critical path of a composite SBS, i. e., the execution path in the service composition with the maximum execution time, is of particular importance in cost-effective monitoring as it determines the response time of the entire SBS. In volatile operating environments, the critical path of an SBS is probabilistic, as every execution path can be critical with a certain probability, i. e., its criticality. As such, it is important to estimate the criticalities of different execution paths when deciding which parts of the SBS to monitor. Furthermore, cost-effective monitoring also requires management of the trade-off between the benefit and cost of monitoring. In this paper, we propose CriMon, a novel approach to formulating and evaluating monitoring strategies for SBSs. CriMon first calculates the criticalities of the execution paths and the component services of an SBS and then, based on those criticalities, generates the optimal monitoring strategy considering both the benefit and cost of monitoring. CriMon has two monitoring strategy formulation methods, namely local optimisation and global optimisation. In-lab experimental results demonstrate that the response time of an SBS can be managed cost-effectively through CriMon-based monitoring. The effectiveness and efficiency of the two monitoring strategy formulation methods are also evaluated and compared. Index Terms—Service-based system, web service, QoS, response time, <b>monitoring,</b> <b>criticality,</b> cost of <b>monitoring,</b> value of monitoring Ç...|$|R
40|$|Abstract — Given the {{criticality}} {{of energy}} awareness in wireless networks, {{it has become}} essential to devise an improved definition of the network lifetime at the system design stage. The new definition must capture the life profile of the network while accounting for its functionality and specific design parameters. This paper presents the notion of network durability, which captures the spatiotemporal life/death patterns of devices in a wireless network by examining the time evolution of spatial patterns according to which devices are progressively forced to exit the network having exhausted their energy resource. Using network durability, we show how networks can satisfy different levels of <b>monitoring</b> <b>criticality,</b> even when they exhibit the same conventionally defined lifetime. Finally, as an example application, we consider a heterogeneous location-aware modulation scheme where the proposed durability model is effectively employed to characterize the network lifetime...|$|R
40|$|Abstract: Any real {{project is}} very complex. Many {{of the issues}} of project {{management}} are too difficult to be solved by mathematical programming. So the project tasks are very often undertaken {{on the basis of}} extremely informal planning. Project activities are also much non-routine in nature as compared to production. This poses problems to devise a structured method and to work in a structured way. This paper presents a structured approach for execution, monitoring and control of project activities at operational level. Here we have proposed subjective evaluation as a means to determine index for activity <b>criticality</b> for selective <b>monitoring</b> and control, index for readiness level for starting execution of an activity, and a structured way to sort out issues for execution of activities. The scope of this paper is limited to operational aspects of task execution and does not include purchase of materials, recruitment of human resources, and finance...|$|R
40|$|Any real {{project is}} very complex. Many {{of the issues}} of project {{management}} are too difficult to be solved by mathematical programming. So the project tasks are very often undertaken {{on the basis of}} extremely informal planning. Project activities are also much non-routine in nature as compared to production. This poses problems to devise a structured method and to work in a structured way. This paper presents a structured approach for execution, monitoring and control of project activities at operational level. Here we have proposed subjective evaluation as a means to determine index for activity <b>criticality</b> for selective <b>monitoring</b> and control, index for readiness level for starting execution of an activity, and a structured way to sort out issues for execution of activities. The scope of this paper is limited to operational aspects of task execution and does not include purchase of materials, recruitment of human resources, and finance Key words: Project management; Critical activity; Scheduling; Monitoring; Decision-makin...|$|R
40|$|Safety issues {{involved}} in the disposal of nuclear wastes in space as a complement to mined geologic repositories are examined {{as part of an}} assessment of the feasibility of nuclear waste disposal in space. General safety guidelines for space disposal developed in the areas of radiation exposure and shielding, containment, accident environments, <b>criticality,</b> post-accident recovery, <b>monitoring</b> systems and isolation are presented for a nuclear waste disposal in space mission employing conventional space technology such as the Space Shuttle. The current reference concept under consideration by NASA and DOE is then examined in detail, with attention given to the waste source and mix, the waste form, waste processing and payload fabrication, shipping casks and ground transport vehicles, launch site operations and facilities, Shuttle-derived launch vehicle, orbit transfer vehicle, orbital operations and space destination, and the system safety aspects of the concept are discussed for each component. It is pointed out that future work remains in the development of an improved basis for the safety guidelines and the determination of the possible benefits and costs of the space disposal option for nuclear wastes...|$|R
40|$|Messina's {{historical}} heritage rebuilding {{after the earthquake}} of 1908 was almost exclusively directed both to ensure the static safety of structures and casings and giving new dignity through a rich aesthetic language and preserving the historical identity of the urban fabric. This approach did not keep into account all the buildings energetic response and the optimization of occupants comfort. From here, the need to prepare swift and exhaustive methodology of analysis of the building performance level {{both in terms of}} conservative asset than energy performance. Once you have identified the overall <b>criticalities</b> through the <b>monitoring</b> campaign, the scenery is enriched by data derived from indoor comfort perception, obtained by administering an appropriate survey. In this paper it was analyzed the Palazzo dei Leoni's energy vulnerability, the current seat of the Province of Messina, subject of reconstruction in 1914 after the earthquake and chosen as a case study. The validity of the suggested screening methodology lies in potential applicability to any fine building, subject to postconflict or post-seismic, needing an energy retrofit, supported by a preliminary investigation of the building's potential decays performance and of the level of indoor comfort...|$|R
40|$|Recently {{evidence}} has accumulated that many neural networks exhibit self-organized criticality. In this state, activity is similar across temporal scales {{and this is}} beneficial with respect to information flow. If subcritical, activity can die out, if supercritical epileptiform patterns may occur. Little is known about how developing networks will reach and stabilize <b>criticality.</b> Here we <b>monitor</b> the development between 13 and 95 days in vitro (DIV) of cortical cell cultures (n = 20) and find four different phases, related to their morphological maturation: An initial low-activity state (< 19 DIV) {{is followed by a}} supercritical (< 20 DIV) and then a subcritical one (< 36 DIV) until the network finally reaches stable criticality (< 58 DIV). Using network modeling and mathematical analysis we describe the dynamics of the emergent connectivity in such developing systems. Based on physiological observations, the synaptic development in the model is determined by the drive of the neurons to adjust their connectivity for reaching on average firing rate homeostasis. We predict a specific time course for the maturation of inhibition, with strong onset and delayed pruning, and that total synaptic connectivity should be strongly linked to the relative levels of excitation and inhibition. These results demonstrate that the interplay between activity and connectivity guides developing networks into criticality suggesting that this may be a generic and stable state of many networks in vivo and in vitro...|$|R
40|$|We give a {{comprehensive}} {{account of a}} complex systems approach to large blackouts caused by cascading failure. Instead {{of looking at the}} details of particular blackouts, we study the statistics, dynamics and risk of series of blackouts with approximate global models. North American blackout data suggests that the frequency of large blackouts is governed by a power law. This result is consistent with the power system being a complex system designed and operated near criticality. The power law makes the risk of large blackouts consequential and implies the need for nonstandard risk analysis. Power system overall load relative to operating limits is a key factor affecting the risk of cascading failure. Blackout models and an abstract model of cascading failure show that there are critical transitions as load is increased. Power law behavior can be observed at these transitions. The critical loads at which blackout risk sharply increases are identifiable thresholds for cascading failure and we discuss approaches to computing the proximity to cascading failure using these thresholds. Approximating cascading failure as a branching process suggests ways to compute and <b>monitor</b> <b>criticality</b> by quantifying how much failures propagate. Inspired by concepts from self-organized criticality, we suggest that power system operating margins evolve slowly to near criticality and confirm this idea using a blackout model. Mitigation of blackout risk should take care to account for counter-intuitive effects in complex self-organized critical systems. For example, suppressing small blackouts could lead the system to be operated closer to the edge and ultimately increase the risk of large blackouts. ...|$|R
40|$|A <b>criticality</b> {{approach}} to <b>monitoring</b> cascading failure risk and failure propagation in transmission systems Abstract — We consider {{the risk of}} cascading failure of electric power transmission systems as overall loading is increased. There is evidence from both abstract and power systems models of cascading failure {{that there is a}} critical loading at which the risk of cascading failure sharply increases. Moreover, as expected in a phase transition, at the critical loading there is a power tail in the probability distribution of blackout size. (This power tail is consistent with the empirical distribution of North American blackout sizes.) The importance of the critical loading is that it gives a reference point for determining the risk of cascading failure. Indeed the risk of cascading failure can be quantified and monitored by finding the closeness to the critical loading. This paper suggests and outlines ways of detecting the closeness to criticality from data produced from a generic blackout model. The increasing expected blackout size at criticality can be detected by computing expected blackout size at various loadings. Another approach uses branching process models of cascading failure to interpret the closeness to the critical loading in terms of a failure propagation parameter λ. We suggest a statistic for λ that could be applied before saturation occurs. The paper concludes with suggestions for a wider research agenda for measuring the closeness to criticality of a fixed power transmission network and for studying the complex dynamics governing the slow evolution of a transmission network. Index Terms — blackouts, power system security, stochastic processes, branching process, cascading failure, reliability, risk analysis, complex system, phase transition. I...|$|R
40|$|Embedded {{systems have}} proliferated into diverse and complex {{critical}} applications with stringent reliability and timeliness requirements. Guaranteeing reliability {{in the presence}} of increasing complexity of embedded systems have necessitated a multitude of architectural designs including integrated modular architectures and architectural designs for robustness by minimizing inter-component failure dependencies. In the software development cycle, the system integration architect occupies a key position between the domain-specialist, designing the algorithms and the high-level logical design, and the individual software component developers. In essence, the system architect refines the logical design into concrete software components, while facilitating high-level properties such as timing, dependency management, and fault-tolerance. Existing tools for the systems architect include architecture description languages and model-checking tools, which specify and verify the architectural designs. However, there is a gap between the architectural principles and the actual implementations developed by individual software developers. Low-level software errors, particularly in languages like C and C++, such as dangling pointer dereferences and array bounds errors, violate architectural properties. Recent research on debugging tools focus on best-effort approaches to detecting low-level programming errors. However, {{there is a need for}} tools that guarantee that high-level architectural properties are enforced in the component implementation. Failure to verify these properties in the actual code have caused two critical disasters in recent years, the satellite Phobos I and the Ariane V rocket. The primary contribution of this dissertation is to design a system to analyze individual components and guarantee high-level architectural properties in the system using static analysis. In particular, we verify two key properties: (a) memory isolation and (b) safe value propagation paths from non-core to core components communicating using shared memory. safe exchange of data between components of different <b>criticalities</b> through run-time <b>monitoring.</b> Our solution combines language and library usage restrictions on the C language with a suite of compiler analyses to statically guarantee these properties. In doing so, we incur minimal (often zero) run-time overhead and do not require garbage collection, making our approach very attractive for embedded systems. We have examined different critical systems and embedded benchmarks and shown that our language restrictions are expressive enough for embedded systems while enabling statically guaranteeing high-level architectural properties. Finally, we show that we can verify other related architectural properties by extending our static analysis techniques...|$|R

