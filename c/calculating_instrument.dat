7|78|Public
25|$|The sector, a <b>calculating</b> <b>instrument</b> {{used for}} solving {{problems}} in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, {{was developed in}} the late 16th century and found application in gunnery, surveying and navigation.|$|E
25|$|It was {{not simply}} a more {{powerful}} <b>calculating</b> <b>instrument</b> that placed the reality of white flight beyond a high hurdle of proof seemingly required for policy makers to consider taking action. Also instrumental were new statistical methods developed by Emerson Seim for disentangling deceptive counter-effects that had resulted when numerous cities reacted to departures of a wealthier tax base by annexation. In other words, central cities had been bringing back their new suburbs, such that families that had departed from inner cities were not even being counted as having moved from the cities.|$|E
50|$|An equatorium (plural, equatoria) is {{an astronomical}} <b>calculating</b> <b>instrument.</b> It {{can be used}} for finding the {{positions}} of the Moon, Sun, and planets without calculation, using a geometrical model to represent the position of a given celestial body.|$|E
40|$|This {{paper is}} based on a {{doctoral}} thesis about studying <b>calculating</b> <b>instruments</b> and deals with the very familiar primary school notion: the carried-number. We develop this notion in three ways: the history of <b>calculating</b> <b>instruments</b> and their mechanisation; a mathematical study of this notion within the place-value system; and an analysis of experimental data from an investigation of student and teacher understanding. For many students and teachers the notion of carried-number appears to be undeveloped mathematically...|$|R
50|$|During World War II, Felsenthal {{produced}} over 75% of {{the plastic}} <b>calculating</b> <b>instruments</b> used by the Army Air Forces, and over 90% of those used by the Navy Bureau of Aeronautics. After the war, it introduced a variety of plastic items for commercial and technical purposes. Located in Chicago, it was known as G. Felsenthal & Sons in the 1940s, G. Felsenthal & Sons, Inc. in the 1950s, Felsenthal Plastics Inc. in 1971, and Felsenthal Instrument Co. in 1972.|$|R
40|$|This {{book was}} {{produced}} by George K. Thiruvathukal for the American Institute of Physics to promote interest in the interdisciplinary publication, Computing in Science and Engineering. It accompanied a limited edition set of playing cards {{that is no longer}} available (except in PDF). This book features a set of 54 significant computers by era/category, including ancient <b>calculating</b> <b>instruments,</b> pre-electronic mechanical calculators and computers, electronic era computers, and modern computing (minicomputers, maniframes, personal computers, devices, and gaming consoles) ...|$|R
50|$|The sector, a <b>calculating</b> <b>instrument</b> {{used for}} solving {{problems}} in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, {{was developed in}} the late 16th century and found application in gunnery, surveying and navigation.|$|E
50|$|It was {{not simply}} a more {{powerful}} <b>calculating</b> <b>instrument</b> that placed the reality of white flight beyond a high hurdle of proof seemingly required for policy makers to consider taking action. Also instrumental were new statistical methods developed by Emerson Seim for disentangling deceptive counter-effects that had resulted when numerous cities reacted to departures of a wealthier tax base by annexation. In other words, central cities had been bringing back their new suburbs, such that families that had departed from inner cities were not even being counted as having moved from the cities.|$|E
50|$|The sector, {{also known}} as a {{proportional}} compass or military compass, was a major <b>calculating</b> <b>instrument</b> in use {{from the end of the}} sixteenth century until the nineteenth century. It is an instrument consisting of two rulers of equal length joined by a hinge. A number of scales are inscribed upon the instrument which facilitate various mathematical calculations. It was used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots. Its several scales permitted easy and direct solutions of problems in gunnery, surveying and navigation. The sector derives its name from the fourth proposition of the sixth book of Euclid, where it is demonstrated that similar triangles have their like sides proportional. It has four parts, two legs with a pivot (the articulation), a quadrant and a clamp (the curved part at the end of the leg) that enables the compass to function as a gunner's quadrant.|$|E
40|$|During {{ground-based}} {{assembly and}} upon {{exposure to the}} space environment, optical surfaces accumulate both particles and molecular condensibles, inevitably resulting in degradation of optical instrument performance. Currently, this performance degradation (and the resulting end-of-life instrument performance) cannot be predicted with sufficient accuracy using existing software tools. Optical design codes exist to <b>calculate</b> <b>instrument</b> performance, but these codes generally assume uncontaminated optical surfaces. Contamination models exist which predict approximate end-of-life contamination levels, but the optical effects of these contamination levels can not be quantified without detailed information about the optical constants and scattering properties of the contaminant. The problem is particularly pronounced in the extreme ultraviolet (EUV, 300 - 1, 200 A) and far (FUV, 1, 200 - 2, 000 A) regimes {{due to a lack}} of data and a lack of knowledge of the detailed physical and chemical processes involved. Yet it is in precisely these wavelength regimes that accurate predictions are most important, because EUV/FUV instruments are extremely sensitive to contamination...|$|R
40|$|Humans rely {{extensively}} on material culture {{when they are}} thinking, including when {{they are involved in}} reasoning tasks that require creative solutions. Examples are measuring devices like compasses and barometers; external memory aids like calendars, books, and maps; <b>calculating</b> <b>instruments</b> like abaci and slide rules; and highly specialized tools like imaging software. Even a brief look around one’s desk suffices to indicate that humans are surrounded by artifacts that are specifically designed to perform a variety of cog-nitive tasks. Why do humans rely so {{extensively on}} external tools? What are the kinds of cognitive tasks that material culture helps them to accom-plish? How are they instrumental in helping them complete such tasks? This entry of the extended cognition literature will look at these questions in more detail and pay special attention to their relevance to creativity, starting out by consider-ing different ways in which material culture enhances cognition and then briefly reviewing three models of extended cognition. This entry ends by outlining practical applications of the extended cognition literature for innovators...|$|R
3000|$|... separately, {{and this}} is called a phase calibrator. A pulse is {{inserted}} at the front end and produces many frequency components after a band-pass filter. The phases are measured by the tone signal that is produced by the pulse at near-signal frequencies, and the delay produced in the instruments can also be <b>calculated.</b> This <b>instrument</b> is, of course, kept at a constant temperature. However, in reality, it is rather difficult to determine [...]...|$|R
40|$|This article {{presents}} multiplying devices {{from the middle}} of the 19 th century, which are based on the so called Theorem of Slonimsky. With his theorem Slonimsky succeeded in solving the problem of tens carry in a simple device with no gears. He did it in a surprising way, but at the same time the multiplying device is an interesting and impressive example of application and usage of a rather complicated statement in numerical mathematics. The inventor and the base problem Hayyim Selig Slonimsky 1 (* 1810 in Byelostok or Belostok, Russian Empire, now Bialystok, Poland, † 1904 in Warsaw) was a knowledgeable Talmudist and author of science books about mathematics and astronomy in Hebrew for Jewish people. As a publisher he produced the science magazine Hazefirah (Ha-Tsefira) from 1862 on, also in Hebrew, which continued until 1931. Furthermore Slonimsky invented at least two calculating aids, an adding device and a multiplying device. The article in the Leipziger Illustrierte Zeitung from 1845 [2] mentions a third <b>calculating</b> <b>instrument,</b> similar to an abacus. It is worth noting that the clockmaker and inventor of a calculating machine Abraham Jacob Stern was his father-in-law. For a better understanding of what is new in Slonimsky's invention, I consider it useful to repeat briefly some historic methods for tens carry in multiplying devices. In his Rabdologia John Napier (1550 – 1617) broke a simple multiplying table into vertical columns. With these stripes, mounted on square rods which were called Napier's rods or bones, multiplication tables for any multiplicand may be composed (fig. 1, to ease comparison here I continuously use the multiplicand 274). Figures of the partial products are arranged in triangles so that the user may obtain the required product only digit by digit by diagonally addition from right to left [8, 15]. Fig. 1 : Napier's rods, here used in Theutometer (Germany), about 1910, shortene...|$|E
40|$|The paper {{starts with}} an {{overview}} about the TS-X data take command generation. The general approach of <b>calculating</b> particular <b>instrument</b> settings for each data acquisition is presented. Selected details are presented allowing {{a better understanding}} of presented results. The data take planning and commanding especially in the commissioning phase is resumed. The second part concentrates on the SAR system performance, i. e. obtained verification results are presented together with the resulting improvements in the data take commanding...|$|R
40|$|At Lisbon in the 1630 s a Jesuit of English birth, Ignace Stafford (1599 – 1642), wrote a thick {{treatise}} on <b>calculating</b> <b>instruments,</b> among them Gunter's sector, {{while he was}} a teacher of the 'Aula da Esfera' attached to the Society's college. Gunter had published his book on a sector equipped with recently developed logarithmic scales at London in 1623. While Stafford may have been the first person to routinely deal with logarithms on the Iberian peninsula, he {{does not seem to be}} the first one introducing the use of the sector. It appears that earlier versions of this instrument had some currency already under the name of 'pantometra'. This article is based on a close reading of Stafford's arithmetical treatise that seeks to make visible different kinds of distances in communicating knowledge, as well as the interplay of distancing and approaching. Paying attention to the various kinds of distances involved, will contribute to a better understanding of the historical contingencies that shaped this particular work on arithmetic and mathematical instruments. As a conclusion, I briefly consider the impact of varying distances on the circulation of knowledge...|$|R
30|$|A further {{adjustment}} is also made as the SILC data contains some private sector social protection instruments such as redundancy payments and some private pensions. Amounts {{over and above}} state social protection <b>instruments,</b> <b>calculated</b> using the tax-benefit model, are transferred into market income variables.|$|R
40|$|This paper {{examines}} {{the impact of}} contracting complementarity across product development systems in the automobile industry. While most empirical research on contracting assumes that each governance choice is independent of each other, cross-system integration and incentive problems within product development suggest the potential for complementarities across contracting choices within the firm. This paper develops and implements an instrumental variables estimator which allows us to distinguish contracting complementarity from firm-level fixed effects in governance choice. Taking advantage of system-level determinants of vertical integration to <b>calculate</b> <b>instruments</b> for system-to-system contracting complementarity, we establish three findings. First, contracting choices are “clustered”: the probability of vertical integration for each automobile system increases in number of other systems that are vertically integrated. Second, instrumental variables and reduced-form estimates suggest that contracting complementarity, rather than unobserved firm-level factors, are the drivers of this correlation. Finally, the degree of correlation in governance is sensitive to the underlying contracting and technology environment, and contracting complementarity seems to be highest for those interactions where integration and coordination are most critical. While we interpret these findings cautiously, {{the results suggest that}} assuming away contracting complementarity may be problematic in contexts where coordination activities are both important and difficult to monitor...|$|R
5000|$|Water vapor can dilute other {{gases in}} air and thus change {{the amount of}} [...] in a column above {{the surface of the}} Earth, so often column-average dry-air mole {{fractions}} (X) are reported instead. To <b>calculate</b> this, <b>instruments</b> may also measure O, which is diluted similarly to other gases, or the algorithms may account for water and surface pressure from other measurements. Clouds may interfere with accurate measurements so platforms may include instruments to measure clouds. Because of measurement imperfections and errors in fitting signals to obtain X, space-based observations may also be compared with ground-based observations such as those from the TCCON.|$|R
50|$|This {{method is}} used for {{measuring}} electrophoretic mobility and then <b>calculating</b> zeta potential. <b>Instruments</b> to apply the method are commercially available from several manufacturers. The last set of calculations requires information on viscosity and dielectric permittivity of the dispersion medium. Appropriate electrophoresis theory is also required. Sample dilution is often {{necessary in order to}} eliminate particle interactions.|$|R
50|$|The wire {{from the}} ECU {{to the third}} {{injector}} is also connected to the main instrument. The main <b>instrument</b> <b>calculates</b> the fuel consumption based on the injection pulses duration. The fuel consumption is used to help getting an accurate presentation of the fuel level in the fuel tank and to calculate average fuel consumption in SID.|$|R
40|$|AbstractMicroscopy-based {{fluorescence}} {{resonance energy}} transfer (FRET) experiments measure {{donor and acceptor}} intensities by isolating these signals {{with a series of}} optical elements. Because this filtering discards portions of the spectrum, the observed FRET efficiency is dependent on the set of filters in use. Similarly, observed FRET efficiency is also affected by differences in fluorophore quantum yield. Recovering the absolute FRET efficiency requires normalization for these effects to account for differences between the donor and acceptor fluorophores in their quantum yield and detection efficiency. Without this correction, FRET is consistent across multiple experiments only if the photophysical and instrument properties remain unchanged. Here we present what is, to our knowledge, the first systematic study of methods to recover the true FRET efficiency using DNA rulers with known fluorophore separations. We varied optical elements to purposefully alter observed FRET and examined protein samples to achieve quantum yields distinct from those in the DNA samples. Correction for <b>calculated</b> <b>instrument</b> transmission reduced FRET deviations, which can facilitate comparison of results from different instruments. Empirical normalization was more effective but required significant effort. Normalization based on single-molecule photobleaching was the most effective depending on how it is applied. Surprisingly, per-molecule γ-normalization reduced the peak width in the DNA FRET distribution because anomalous γ-values correspond to FRET outliers. Thus, molecule-to-molecule variation in gamma has an unrecognized effect on the FRET distribution that must be considered to extract information on sample dynamics from the distribution width...|$|R
5000|$|Reverend Nathan Brown of the American Baptist Mission, {{referring}} to the opening up of the tombs of Ahom kings in Charaideo, wrote:"The tomb of King Gadadhar at Soraideo, as nearly as we could <b>calculate</b> without <b>instruments,</b> was ninety feet high, and so natural in its appearance that a stranger would scarcely have suspected it to be anything more than an ordinary hill...Thirteen of these royal tombs were dug open during my residence in Assam, and I was told in the flowery language of the country, that when King Gadadhar's tomb was opened 'the backs of three elephants were broken {{with the weight of}} the treasures it contained', meaning simply that three elphants were well loaded down." ...|$|R
40|$|This work {{deals with}} {{measurement}} capacity and dissipation factor of capacitor with real dielectric in frequency range 20 Hz to 30 MHz. Liquid and solid dielectrics are measured by LCR instruments and by test fixtures, both from company Agilent. Relative permittivity and loss number are <b>calculated</b> for different <b>instruments</b> settings. The general {{aim is to}} specify the influence of parameters as frequency, voltage, integration time and thickness of material on the uncertainties of primary and secondary quantities for measurement...|$|R
5000|$|A and B are the {{respective}} instrument constants of each oscillator. Their values {{are determined by}} calibrating with two substances of the precisely known densities ρ1 and ρ2. Modern <b>instruments</b> <b>calculate</b> and store the constants A and B after the two calibration measurements, which are mostly performed with air and water. They employ suitable measures to compensate various parasitic influences on the measuring result, e.g. {{the influence of the}} sample’s viscosity and the non-linearity caused by the measuring instrument’s finite mass.|$|R
5|$|Mr Accum acquaints the Patrons and Amateurs of Chemistry that he {{continues}} to give private Courses of Lectures on Operative and Philosophical Chemistry, Practical Pharmacy and the Art of Analysis, {{as well as to}} take Resident Pupils in his House, and that he keeps constantly on sale in as pure a state as possible, all the Re-Agents and Articles of Research made use of in Experimental Chemistry, together with a complete Collection of Chemical Apparatus and <b>Instruments</b> <b>calculated</b> to Suit the conveniences of Different Purchasers.|$|R
50|$|A sucrose {{solution}} with {{an apparent}} specific gravity (20°/20 °C) of 1.040 would be 9.99325 °Bx or 9.99359 °P while the representative sugar body, the International Commission for Uniform Methods of Sugar Analysis (ICUMSA), which favors {{the use of}} mass fraction, would report the solution strength as 9.99249%. Because {{the differences between the}} systems are of little practical significance (the differences are less than the precision of most common instruments) and wide historical use of the Brix unit, modern <b>instruments</b> <b>calculate</b> mass fraction using ICUMSA official formulas but report the result as °Bx.|$|R
40|$|The {{sensitivity}} and dynamic range of {{optical coherence tomography}} (OCT) are <b>calculated</b> for <b>instruments</b> utilizing two common interferometer configurations and detection schemes. Previous researchers recognized that the performance of dual-balanced OCT instruments is severely limited by beat noise, which is generated by incoherent light backscattered from the sample. However, beat noise has been ignored in previous calculations of Michelson OCT performance. Our measurements of instrument noise confirm the presence of beat noise even in a simple Michelson interferometer configuration with a single photodetector. Including this noise, we calculate the dynamic range {{as a function of}} OCT light source power, and find that instruments employing balanced interferometers and balanced detectors can achieve a sensitivity up to six times greater than those based on a simple Michelson interferometer, thereby boosting image acquisition speed by the same factor for equal image quality. However, this advantage of balanced systems is degraded for source powers greater than a few milliwatts. We trace the concept of beat noise back to an earlier paper [J. Opt. Soc. Am. 52, 1335 (1962) ]...|$|R
40|$|Objectives: Key {{diagnostic}} decisions often turn on {{measurement of}} change in pain intensity after diagnostic anesthetic blocks. This study aimed to introduce a new direct measure pain intensity change and compare it with percent change as calculated from the traditional preprocedure and postprocedure pain visual analog scales. Methods: Shoulder pain patients enrolled in a diagnostic accuracy study comparing clinical variables with image-guided local anesthetic injections were assessed with both the traditional preprocedure and postprocedure visual analog scales and the new direct method. Percent change in pain intensity was <b>calculated</b> with both <b>instruments</b> and were compared using statistical methods. The percentage pain reduction used to classify patients as responders was 80...|$|R
40|$|This paper {{presents}} a virtual instrument developed in LabView for modeling generating a distorting voltage by adding harmonics with different level and phase {{to the base}} signal. The other virtual instrument models the single-phase power system and calculates the power and energy proper to the harmonics. The virtual instrument can be enlarged very easy for three-phase power system. Developing proper conditional circuits for current and voltage acquisition the virtual instrument can be modify to measure real data. The designed virtual <b>instrument</b> <b>calculates</b> from the acquired data the active and reactive power, the power factor and the frequency and level of the harmonics in case of nonsinusoidal signals...|$|R
50|$|James William Hunter of Thurston FRSE (1783-1844) was a Scottish landowner, {{inventor}} {{and agricultural}} improver. His main {{claim to fame}} is the improvement to the mechanical Odometer in 1827, creating a single-handed and single-wheeled device, setting {{a series of three}} 100 tooth cogs against 101 tooth cogs, attached to a wheel of circumference either 6 or 10 feet. This created a very convenient apparatus for land measurement, and is still the basis for modern day mechanical surveying odometers. The larger version was attached to the rear of a carriage and was the first known <b>instrument</b> <b>calculating</b> total vehicle distance travelled in a precise and visually clear way.|$|R
40|$|Background: To {{optimize}} response rates, it {{is important}} to have brief, comprehensive instruments. Aims: We have developed and validated a short form of an instrument for measuring students' perceptions of teachers' competencies to encourage students' reflective learning in small groups (the STERLinG). Methods: Based on statistical and content criteria, the original 36 -item STERLinG was reduced to 15 items: three scales with five items each. This mini-STERLinG was validated. Confirmatory factor analysis was performed and internal consistencies were <b>calculated.</b> Results: The <b>instrument</b> was completed by 501 respondents (63 %). The original instrument structure was confirmed with 62. 6 % explained variance. Reliabilities were high with 0. 91 for the entire mini-STERLinG and 0. 87, 0. 85 and 0. 81 for its subscales. Conclusions: The mini-STERLinG was found to be a feasible, valid and reliable instrument. ...|$|R
50|$|A {{fiber optic}} sensor is a sensor that uses optical fiber either as the sensing element ("intrinsic sensors"), or {{as a means}} of {{relaying}} signals from a remote sensor to the electronics that process the signals ("extrinsic sensors"). Fibers have many uses in remote sensing. Depending on the application, fiber may be used because of its small size, or because no electrical power is needed at the remote location, or because many sensors can be multiplexed along the length of a fiber by using light wavelength shift for each sensor, or by sensing the time delay as light passes along the fiber through each sensor. Time delay can be determined using a device such as an optical time-domain reflectometer and wavelength shift can be <b>calculated</b> using an <b>instrument</b> implementing optical frequency domain reflectometry.|$|R
40|$|An {{experimental}} set-up to measure low-energy (below 1 keV) sputtering of materials is described. The materials to be bombarded represent ion thruster components {{as well as}} insulators used in the stationary plasma thruster. The sputtering {{takes place in a}} 9 inch diameter spherical vacuum chamber. Ions of argon, krypton and xenon are used to bombard the target materials. The sputtered neutral atoms are detected by a secondary neutral mass spectrometer (SNMS). Samples of copper, nickel, aluminum, silver and molybdenum are being sputtered initially to calibrate the spectrometer. The base pressure of the chamber is approximately 2 x 10 (exp - 9) Torr. the primary ion beam is generated by an ion gun which is capable of delivering ion currents in the range of 20 to 500 nA. The ion beam can be focused to a size approximately 1 mm in diameter. The mass spectrometer is positioned 10 mm from the target and at 90 deg angle to the primary ion beam direction. The ion beam impinges on the target at 45 deg. For sputtering of insulators, charge neutralization is performed by flooding the sample with electrons generated from an electron gun. Preliminary sputtering results, methods of <b>calculating</b> the <b>instrument</b> response function of the spectrometer and the relative sensitivity factors of the sputtered elements will be discussed...|$|R
60|$|When the King of Cashmere had quitted her {{presence}} the evening before, he had resolved {{that the sun}} should not set again without the princess becoming his wife, and at daybreak proclamation of his intention was made throughout the town, {{by the sound of}} drums, trumpets, cymbals, and other <b>instruments</b> <b>calculated</b> to fill the heart with joy. The Princess of Bengal was early awakened by the noise, but she did not for one moment imagine that it {{had anything to do with}} her, till the Sultan, arriving as soon as she was dressed to inquire after her health, informed her that the trumpet blasts she heard were part of the solemn marriage ceremonies, for which he begged her to prepare. This unexpected announcement caused the princess such terror that she sank down in a dead faint.|$|R
40|$|The Extreme Environment Diffractometer is {{a neutron}} {{time of flight}} instrument, {{designed}} {{to work with a}} constant field hybrid magnet capable of reaching fields over 26 T, unprecedented in neutron science; however, the presence of the magnet imposes both spatial and technical limitations on the surrounding instrument components. In addition to the existing diffraction and small angle neutron scattering modes, the instrument will operate also in an inelastic scattering mode, as a direct time of flight spectrometer. In this paper we present the Monte Carlo ray tracing simulations, the results of which illustrate the performance of the instrument in the inelastic scattering mode. We describe the focussing neutron guide and the chopper system of the existing instrument and the planned design for the instrument upgrade. The neutron flux, neutron spatial distribution, divergence distribution and energy resolution are <b>calculated</b> for standard <b>instrument</b> configuration...|$|R
5000|$|When {{central vision}} is compromised, {{as in the}} case of macular scotoma, {{patients}} develop an eccentric or extra-foveal vision, normally with unstable fixation. The retinal area used by eccentric viewers to substitute the foveal vision is known as the Preferred Retinal Locus (PRL) In Microperimetry systems, the fundus (eye) is imaged in real time, while an eye tracker compensates eye movements during stimuli projection, allowing correct matching between expected and projected stimulus position onto the retina. Simultaneously, the eye tracker plots the retinal movement during fixation attempt defining the PRL zone as well as fixation stability. Some microperimetry <b>instruments</b> <b>calculate</b> 2 different PRL zones during the examination. To create the fundus image an infrared telecamera is used, {{as in the case}} of the [...] "Nidek-MP1", or a Scanning Laser Ophthalmoscope (SLO), {{as in the case of}} the [...] "Centervue-MAIA".|$|R
