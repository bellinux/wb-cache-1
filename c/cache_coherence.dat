1005|33|Public
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance <b>cache</b> <b>coherence</b> systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.|$|E
25|$|The {{bandwidth}} ceilings are bandwidth diagonals placed {{below the}} idealized peak bandwidth diagonal. Their existence {{is due to}} the lack of some kind of memory related architectural optimization, such as <b>cache</b> <b>coherence,</b> or software optimization, such as poor exposure of concurrency (that in turn limit bandwidth usage).|$|E
2500|$|PCI {{originally}} included optional {{support for}} write-back <b>cache</b> <b>coherence.</b> [...] This required support by cacheable memory targets, which {{would listen to}} two pins from the cache on the bus, SDONE (snoop done) and SBO# (snoop backoff).|$|E
40|$|Digital Equipment Corporation Extensive caching is a {{key feature}} of the Echo {{distributed}} file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed write-back) of all cached information. Echo specifies ordering constraints on this write-behind, enabling applications to store and maintain consistent data structures in the file system even when crashes or network faults prevent some writes from being completed. In this paper we describe the Echo <b>cache’s</b> <b>coherence</b> and ordering semantics, show how they can improve the performance and consistency of applications, and explain how they are implemented. We also discuss the general problem of reliably notifying applications and users when write-behind is 10 SE we addressed this problem {{as part of the}} Echo design, but did not find a fully satisfactory solution...|$|R
40|$|Extensive caching is a {{key feature}} of the Echo {{distributed}} file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed write-back) of all cached information. Echo specifies ordering constraints on this write-behind, enabling applications to store and maintain consistent data structures in the file system even when crashes or network faults prevent some writes from being completed. In this paper we describe the Echo <b>cache's</b> <b>coherence</b> and ordering semantics, show how they can improve the performance and consistency of applications, and explain how they are implemented. We also discuss the general problem of reliably notifying applications and users when write-behind is lost; we addressed this problem {{as part of the}} Echo design but did not find a fully satisfactory solution. Contents 1 Introduction 1 2 Design motivation 2 3 Coherence and ordering semantics 4 3. 1 Ordering constraints : : : : : : : : : : : : : : : : [...] ...|$|R
50|$|Two 64-bit {{operating}} systems {{run on the}} server nodes of the appliance: Oracle Linux version 5.5 or Solaris 11. All servers have an installed cluster configuration of Oracle WebLogic Server and distributed memory <b>cache</b> Oracle <b>Coherence.</b> To run Java applications on a machine there is a choice of HotSpot or JRockit. Management of the appliance {{is available in the}} Oracle Enterprise Manager toolset, which is also pre-installed in the appliance. A transaction monitor Tuxedo is optionally supplied.|$|R
2500|$|Stanford DASH was a cache {{coherent}} multiprocessor {{developed in}} the late 1980s by a group led by Anoop Gupta, John L. Hennessy, Mark Horowitz, and Monica S. Lam at Stanford University. It was based on adding a pair of directory boards designed at Stanford to up to 16 SGI IRIS 4D Power Series machines and then cabling the systems in a mesh topology using a Stanford-modified version of the Torus Routing Chip. The boards designed at Stanford implemented a directory-based <b>cache</b> <b>coherence</b> protocol allowing Stanford DASH to support distributed shared memory for up to 64 processors. Stanford DASH was also notable for both supporting and helping to formalize weak memory consistency models, including release consistency. Because Stanford DASH was the first operational machine to include scalable <b>cache</b> <b>coherence,</b> it influenced subsequent computer science research {{as well as the}} commercially available SGI Origin 2000. Stanford DASH is included in the 25th anniversary retrospective of selected papers from the International Symposium on Computer Architecture ...|$|E
50|$|<b>Cache</b> <b>coherence</b> is {{provided}} by the memory controllers. Each memory controller has a <b>cache</b> <b>coherence</b> engine. The Alpha 21364 uses a directory <b>cache</b> <b>coherence</b> scheme where part of the memory is used to store Modified, Exclusive, Shared, Invalid (MESI) coherency data.|$|E
5000|$|... hardware, such as <b>cache</b> <b>coherence</b> {{circuits}} {{and network}} interfaces ...|$|E
40|$|Abstract—Prohibitive {{simulation}} {{time with}} pre-silicon design models and unavailability of proprietary target applications make microprocessor design very tedious. The framework proposed {{in this paper}} is the first attempt to automatically generate synthetic benchmark proxies for real world multithreaded applications. The framework includes metrics that characterize {{the behavior of the}} workloads in the shared <b>caches,</b> <b>coherence</b> logic, out-of-order cores, interconnection network and DRAM. The framework is evaluated by generating proxies for the workloads in the multithreaded PARSEC benchmark suite and validating their fidelity by comparing the microarchitecture dependent and independent metrics to that of the original workloads. The average error in IPC is 4. 87 % and maximum error is 10. 8 % for Raytrace in comparison to the original workloads. The average error in the power-per-cycle metric is 2. 73 % with a maximum of 5. 5 % when compared to original workloads. The representativeness of the proxies to that of the original workloads in terms of their sensitivity to design changes is evaluated by finding the correlation coefficient between the trends followed by the synthetic and the original for design changes in IPC, which is 0. 92. A speedup of four to six orders of magnitude is achieved by using the synthetic proxies over the original workloads...|$|R
40|$|This {{document}} {{is a short}} review about the MESI protocol simulator. This simulator is a tool {{which is used to}} teach the <b>cache</b> memory <b>coherence</b> on the computer systems with hierarchical memory system. The MESI protocol is a well known method to the maintenance of the information coherence in the memory system. The MESI simulator is also used to explain the process of the cache memory location (in multilevel cache memory systems). In this paper, we explain the MESI protocol and we show how the simulator works. Besides, we present some classroom practices carried out by using the MESI simulator...|$|R
5000|$|Replicated and {{partitioned}} {{data management}} and caching services - At its core Oracle Coherence {{is a highly}} scalable and fault-tolerant distributed <b>cache</b> engine. <b>Coherence</b> uses a specialized scalable protocol and many inexpensive computers to create a cluster which can be seamlessly expanded to add more memory, processing power or both. As a result, Coherence has no {{single point of failure}} and transparently fails over if a cluster member fails. When a Coherence server is added or removed the cluster automatically re-balances to share the workload. As a result, Coherence provides a highly available and predictably horizontally scalable infrastructure for managing application data.|$|R
50|$|In {{computer}} engineering, directory-based <b>cache</b> <b>coherence</b> {{is a type}} of <b>cache</b> <b>coherence</b> mechanism, where directories {{are used}} to manage caches in place of snoopy methods due to their scalability. Snoopy bus-based methods scale poorly due to the use of broadcasting. These methods can be used to target both performance and scalability of directory systems.|$|E
5000|$|Following {{are a few}} {{advantages}} and disadvantages of the directory based <b>cache</b> <b>coherence</b> protocol: ...|$|E
5000|$|AMD Fusion Compute Link (Onion) - {{interfaces}} to CPU cache {{and coherent}} system memory (see <b>cache</b> <b>coherence)</b> ...|$|E
40|$|This paper {{addresses}} {{the problem of}} evaluating the performance of multiprocessor with shared memory and private <b>caches</b> executing Invalidate <b>Coherence</b> Protocols. The model is grounded in queuing network theory and includes bus interference, cache interference, and main memory interference. The method of the Imbedded Markov Chains is used. The highest and lowest performance characteristics are calculated for both equilibrium and transient states. Key words...|$|R
50|$|When {{processor}} P1 {{has obtained}} a lock and processor P2 is also {{waiting for the}} lock, P2 will keep incurring bus transactions in attempts to acquire the lock. When a processor has obtained a lock, all other processors which also wish to obtain the same lock keep trying to obtain the lock by initiating bus transactions repeatedly until they get hold of the lock. This increases the bus traffic requirement of test-and-set significantly. This slows down all other traffic from <b>cache</b> and <b>coherence</b> misses. It slows down the overall section, since the traffic is saturated by failed lock acquisition attempts. Test-and-test-and-set is an improvement over TSL since it does not initiate lock acquisition requests continuously.|$|R
50|$|Database {{operations}} like Insert, Update and Delete {{can also}} be performed in TopLink. The changes made to the database {{are reflected in the}} Oracle <b>Coherence</b> <b>cache.</b> In Java Persistence API, an entity is a persistence class.Using TopLink, a number of performance features for writing large amounts of data can be implemented. Batch writing, stored procedure support, parameter binding, statement ordering and other features are offered to satisfy database constraints.|$|R
50|$|However, {{there are}} a few limits on the {{scalability}} of SMP due to <b>cache</b> <b>coherence</b> and shared objects.|$|E
50|$|As memory {{architectures}} {{increase in}} complexity, maintaining <b>cache</b> <b>coherence</b> becomes a greater problem than simple connectivity. Fireplane represents a substantial advance over previous interconnects in this aspect. It combines both snoopy cache and point-to-point directory-based models {{to give a}} two-level <b>cache</b> <b>coherence</b> model. Snoopy buses are used primarily for single buses with small numbers of processors; directory models are used for larger numbers of processors. Fireplane combines both, to give a scalable shared memory architecture.|$|E
50|$|Few years later, in 1992, Daniel Lenoski from Stanford {{university}} {{published a}} paper that proposes some interesting {{advances in the}} <b>cache</b> <b>coherence</b> protocols for Directory-based systems. In 1996, he (i.e. Lenoski) proposed SGI Origin 2000, which was a family of server computers that employs directory based <b>cache</b> <b>coherence.</b> The design was proposed in paper published in 1996. After that, Origin 3000 was proposed in July 2000. And it finally was discontinued two years later, in June 2002.|$|E
40|$|Abstract — We {{describe}} a RISC-based architecture for moving tiny light-weight threads instead of {{data in the}} multicore context. We assume the architecture to consist of homogeneous cores that are connected with an on-chip network. A sparse 3 D torus is considered as a network delivering enough bandwidth. Besides rather ordinary ALU capabilities, each core maintains a rather large set of threads, and has separate memory and instruction caches. For the data <b>cache,</b> we avoid <b>coherence</b> problems by partitioning the memory and assuming that {{a portion of the}} main memory can be accessed only via one specific data cache. Consequently, we need to move light-weight hardwaresupported threads between the cores in the on-chip network. Compared to approaches where all memory locations are accessible via each <b>cache,</b> we avoid <b>coherence</b> problems and do less loads to caches. The price is an additional network between the cores and possible inefficiencies due to moving threads between the cores. As the threads in each core are used to hide memory access and thread moving latencies, we characterize requirements for the amount of tiny threads. 1...|$|R
40|$|In {{this paper}} {{we present a}} new {{continuous}} multiresolution approach which has been developed for the interactive visualization of meshes in real-time applications. Our interest is to offer an efficient solution which considers submeshes, textures, normals and bones for skeletal animations. The model {{has been designed to}} give view-independent continuous levels-of-detail and uses triangle strips for exploiting vertex <b>cache</b> and <b>coherence</b> for minimizing bus traffic. Furthermore, its data structures allow for an efficient extraction process where all unnecessary information is eliminated and also for progressive transmission. A new simplification strategy has also been developed, which preserves appearance and attributes. In the results section we present different images to show the visual quality obtained with this simplification method, as well as a study of the storage and rendering costs. © 2009 Elsevier Inc. All rights reserved...|$|R
40|$|A shared-bus shared-memory {{multiprocessor}} {{based on}} multithreaded CPUs is evaluated against different solutions for <b>cache</b> and <b>coherence</b> protocols. Multithreaded architectures have been intensively studied for DSM multiprocessors, where memory latencies {{are a major}} factor in limiting performance. They can be interesting also for bus-based multiprocessors, since processor speed are increasing at a much faster rate than memory. In these systems, not only pure parallel workloads, but also workloads consting of both parallel and sequential applications are the typical object of user demand for processing power. The aim of this work is to investigate the relations among workloads of this kind, multithreaded processors, sharedbus architecture, coherence schemes, and thereby the consequences on performance. We have used an enhanced trace-driven approach that takes into consideration both user and kernel references. Results show that multithreaded processors can play an effective role in boostin [...] ...|$|R
50|$|The Firefly <b>cache</b> <b>coherence</b> {{protocol}} is the schema {{used in the}} DEC Firefly multiprocessor workstation, {{developed by}} DEC Systems Research Center. This protocol is a 3 State Write Update <b>Cache</b> <b>Coherence</b> Protocol. Unlike the Dragon protocol, the Firefly protocol updates the Main Memory {{as well as the}} Local caches on Write Update Bus Transition. Thus the Shared Clean and Shared Modified States present in case of Dragon Protocol, are not distinguished between in case of Firefly Protocol.|$|E
50|$|Coherence Protocols apply <b>cache</b> <b>coherence</b> in {{multiprocessor}} systems. The {{intention is}} that two clients must never see different {{values of the}} same shared data.|$|E
50|$|Arvind's {{research}} interests include verification of large-scale digital systems using Guarded Atomic Actions, Memory Models and <b>Cache</b> <b>Coherence</b> Protocols for parallel architectures and languages.|$|E
40|$|Hierarchical {{storage of}} web pages in proxy server and client browser <b>caches</b> {{introduce}} <b>coherence</b> problems, which require cache management techniques which are both accurate and computationally efficient. We suggest that current approaches, {{such as the}} most common Least Recently Used (LRU) technique, are inadequate for future network loads as they do not incorporate the dynamics of document selection and modification. We propose {{the use of an}} intelligent, adaptive cache management technique to overcome the coherence problem by using document life histories to optimise cache performance. This work addresses the use of damped exponential smoothing to model accurately the frequency of file requests and modifications, in order to predict the future value of cached files. Finally, we make a mathematical analysis of LRU in comparison with our technique, showing how and why the use of document life histories is a more effective cache management technique without imposing major computational ove [...] ...|$|R
40|$|The {{speed of}} {{processors}} increases {{much faster than}} the memory access time. This makes memory accesses expensive. To meet this problem, cache hierarchies are introduced to serve the processor with data. However, the effectiveness of caches depends {{on the amount of}} locality in the application’s memory access pattern. The behavior of various programs differs greatly in terms of cache miss characteristics, access patterns and communication intensity. Therefore a computer built for many different computational tasks potentially benefits from dynamically adapting to the varying needs of the applications. This thesis shows that a cc-NUMA multiprocessor with data migration and replication optimizations efficiently exploits the temporal locality of algorithms. The performance of the self-optimizing system is similar to a system with a perfect initial thread and data placement. Data locality optimizations are not for free. Large <b>cache</b> line <b>coherence</b> protocols improve spatial locality but yield increases in false sharing misses for man...|$|R
5000|$|Oracle Event Processing (OEP) {{provides}} {{read access}} to Oracle NoSQL Database via the NoSQL Database cartridge. Once the cartridge is configured, CQL queries {{can be used}} toquery the data.Oracle Semantic Graph has developed a Jena Adapter for Oracle NoSQL Database [...] to store large volumes of RDF data (as triplets/quadruplets). This adapter enables fast access to graph data stored in Oracle NoSQL Database via SPARQL queries. An integration with Oracle Coherence has been provided that allows Oracle NoSQL Database {{to be used as}} a <b>cache</b> for Oracle <b>Coherence</b> applications, also allowing applications to directly access cached data from Oracle NoSQL Database.|$|R
50|$|The Dragon <b>cache</b> <b>coherence</b> {{protocol}} is the schema {{used in the}} Xerox Dragon multiprocessor workstation, {{developed by}} Xerox PARC. This protocol uses a write-back policy.|$|E
50|$|The {{ideas for}} cache-coherent {{synchronization}} of photos and videos {{to a local}} application cache {{can be traced back}} to work in <b>Cache</b> <b>Coherence</b> in shared memory Multiprocessing.|$|E
5000|$|Therefore, {{in order}} to satisfy Transaction Serialization, and hence achieve <b>Cache</b> <b>Coherence,</b> the {{following}} condition along with the previous two mentioned in this section must be met: ...|$|E
40|$|Most {{shared memory}} systems {{maximize}} performance by unpredictably resolving memory races. Unpredictable memory races {{can lead to}} nondeterminism in parallel programs, which can suffer from hard-toreproduce hiesenbugs. We introduce Calvin, a shared memory model capable of executing in a conventional nondeterministic mode when performance is paramount and a deterministic mode when execution repeatability is important. Unlike prior hardware proposals for deterministic execution, Calvin exploits the flexibility of a memory consistency model weaker than sequential consistency. Specifically, Calvin logically orders memory operations into strata that are compatible with the Total Store Order (TSO). Calvin is also designed {{with the needs of}} future power-aware processors in mind, and does not require any speculation support. We develop a Calvin-MIST implementation that uses an unordered coalescing write <b>cache,</b> multiplewrite <b>coherence</b> protocol, and delayed (timebomb) invalidations while maintaining TSO compatibility. Results show that Calvin-MIST can execute workloads in conventional mode at speeds comparable to a conventional system (providing compatibility) or execute deterministically for a modest average slowdown of less than 20 % (when determinism is valued). 1...|$|R
40|$|The cache {{replacement}} {{policy is}} one of the factors that determines the effectiveness of cache memories. In this paper, we study the impact of incorporating the <b>cache</b> block <b>coherence</b> state information in the Random replacement policy in a shared [...] memory multiprocessor. We assign replacement priority to each cache block within a set based on its state. To reduce the probability of replacing a recently accessed block and to adapt to the program's access patterns, we also associate with each set an MRU (Most Recently Used) [...] state. The MRU [...] state causes the lowest replacement priority to be assigned to the blocks in the same state as the MRU [...] state. Our evaluations indicate that, with the appropriate priority assignment and a set associativity size less than 16, the proposed policy can outperform the Random and Random & Invalid policies and, in some cases, can even outperform the LRU policy. 1. Introduction Most general [...] purpose processors use a cache memory to reduce the average memory [...] ...|$|R
40|$|This paper investigates {{optimized}} synchronization {{techniques for}} shared memory on-chip multiprocessors (CMPs) based on network-on-chip (NoC) and targeted at future mobile systems. The proposed solution {{is based on}} the idea of locally performing synchronization operations requiring continuous polling of a shared variable, thus, featuring large contentions (e. g., spin locks and barriers). A hardware (HW) module, the synchronization-operation buffer (SB), has been introduced to queue and to manage the requests issued by the processors. By using this mechanism, we propose a spin lock implementation requiring a constant number of network transactions and memory accesses per lock acquisition. The SB also supports an efficient implementation of barriers. Experimental validation has been carried out by using GRAPES, a cycle-accurate performance/power simulation platform for multiprocessor systems-on-chip (MPSoCs). Two different architectures have been explored to prove that the proposed approach is effective independently from <b>caches</b> and <b>coherence</b> schemes adopted. For an eight-processor target architecture, we show that the SB-based solution achieves up to 50 % performance improvement and 30 % energy saving with respect to synchronization based on the caching of the synchronization variables and directory-based coherence protocol. Furthermore, we prove the scalability of the proposed approach when the number of processors increases...|$|R
