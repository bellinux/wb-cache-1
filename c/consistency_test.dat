482|933|Public
25|$|In {{doing so}} the master {{constraint}} programme has been satisfactorily {{tested in a}} number of model systems with non-trivial constraint algebras, free and interacting field theories. The master constraint for LQG was established as a genuine positive self-adjoint operator and the physical Hilbert space of LQG was shown to be non-empty, an obvious <b>consistency</b> <b>test</b> LQG must pass to be a viable theory of quantum General relativity.|$|E
5000|$|The Lynch test is a <b>consistency</b> <b>test</b> {{of journalists}} {{covering}} the Israeli-Arab conflict. According to Nahum Barnea, {{winner of the}} Israel Prize, Israeli journalists who fail to criticize Arab terrorism fail the lynch test. According to author Kenneth Levin, this is a [...] "rare instance of Israeli media self-scrutiny." [...] The term came after the 2000 Ramallah lynching, in which an Arab mob beat to death two Israeli reservists who had mistakenly entered Ramallah.|$|E
50|$|In psychometrics and psychophysics, {{the term}} {{accuracy}} is interchangeably used with validity and constant error. Precision is {{a synonym for}} reliability and variable error. The {{validity of a measurement}} instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal <b>consistency</b> <b>test</b> like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population.|$|E
40|$|AbstractConstraint {{propagation}} is {{an elementary}} method {{for reducing the}} search space of combinatorial search and optimization problems which has {{become more and more}} important in the last decades. The basic idea of constraint propagation is to detect and remove inconsistent variable assignments that cannot participate in any feasible solution through the repeated analysis and evaluation of the variables, domains and constraints describing a specific problem instance. The contribution of this paper is twofold. The first contribution is a description of efficient constraint propagation methods also known as <b>consistency</b> <b>tests</b> for the disjunctive scheduling problem (DSP) which is a generalization of the classical job shop scheduling problem (JSP). Applying an elementary constraint based approach involving a limited number of search variables, we will derive <b>consistency</b> <b>tests</b> that ensure 3 -b-consistency. We will further present and analyze both new and classical <b>consistency</b> <b>tests</b> which to some extent are generalizations of the aforementioned <b>consistency</b> <b>tests</b> involving a higher number of variables, but still can be implemented efficiently with a polynomial time complexity. Further, the concepts of energetic reasoning and shaving are analyzed and discussed. The other contribution is a classification of the <b>consistency</b> <b>tests</b> derived according to the domain reduction achieved. The particular strength of using <b>consistency</b> <b>tests</b> is based on their repeated application, so that the knowledge derived is propagated, i. e., reused for acquiring additional knowledge. The deduction of this knowledge can be described as the computation of a fixed point. Since this fixed point depends upon the order of the application of the tests, we first derive a necessary condition for its uniqueness. We then develop a concept of dominance which enables the comparison of different <b>consistency</b> <b>tests</b> as well as a simple method for proving dominance. An extensive comparison of all <b>consistency</b> <b>tests</b> is given. Quite surprisingly, we will find out that some apparently stronger <b>consistency</b> <b>tests</b> are subsumed by apparently weaker ones. At the same time an open question regarding the effectiveness of energetic reasoning is answered...|$|R
40|$|This paper {{considers}} {{a class of}} <b>consistency</b> <b>tests</b> for the specification of heteroskedastic and risk models. The tests are related to other procedures such as the conditional moment tests of Newey and Tauchen, Hausman's tests, White's tests, the variable addtion Lagrange multiplier tests of Engle and Pagan, and the residual analysis of Pagan and Hall. The power of the <b>consistency</b> <b>tests</b> {{in the presence of}} local departures is analyzed and the risk premia model of Engle, Lilien and Robins is re-assessed. ...|$|R
30|$|Scenario 9 : using {{multiple}} MCDM methods {{is not recommended}} as both {{the internal and external}} <b>consistency</b> <b>tests</b> yielded lower coefficients for the MCDM methods.|$|R
5000|$|... 1. Cut a {{piece of}} {{drinking}} straw to fit. The length depends on individual tastes—longer reeds make a lower sound and shorter reeds a higher sound.2. Cut the corners off the top.3. Flatten the reed {{in order to make}} two sharp edges.4. Wrap {{a piece of}} fine grain sandpaper around the index finger or the instrument(to make a curved surface).5. Gently sand the flat part of the straw. Count how many strokes you execute oneach side to maintain <b>consistency.</b> <b>Test</b> and sand as needed. Generally, a softer,thinner reed will be easier to produce sounds initially, but a certain degree offirmness is required to hit higher notes and manipulate the sound fully.|$|E
5000|$|The {{majority}} {{proceeded to}} explain the history and merits of the [...] "internal consistency test." [...] The test considers the hypothetical situation in which every state adopted the tax structure in question and then asks whether interstate commerce would be at a disadvantage to intrastate commerce. The majority applied the internal <b>consistency</b> <b>test</b> to the hypothetical situation, paralleling Maryland's tax scheme, in which every state imposed a 1.25% income tax on residents' income earned within the state, a 1.25% income tax on residents' income earned in other states, and a 1.25% income tax on income earned by nonresidents within the state. In this case, intrastate income would only be taxed once at 1.25% but interstate income would always be taxed by both states involved (1.25% {{by the state of}} residence and 1.25% in the state where the income was earned). The majority, quoting West Lynn Creamery, Inc. v. Healy, thus found that [...] "Maryland's tax scheme is inherently discriminatory and operates as a tariff...which is fatal because tariffs are 'the paradigmatic example of a law discriminating against interstate commerce.'" ...|$|E
3000|$|Scenario 7 : using {{multiple}} MCDM methods {{is not recommended}} because while the external <b>consistency</b> <b>test</b> yielded higher coefficients for MCDM methods than the Intuitive approach, the internal <b>consistency</b> <b>test</b> yielded lower coefficients for the MCDM methods, and if the argument made under Scenario 3 is verified (i.e., internal <b>consistency</b> <b>test</b> carries more weight than external <b>consistency</b> <b>test),</b> this conclusion holds. However, this is also subject to further research, which may also consider how higher or lower the coefficients are; [...]...|$|E
40|$|The {{potential}} of gel <b>consistency</b> <b>tests</b> {{in the evaluation}} of sorghum food quality was investigated. Gelspread of cooled thin porridges exhibitedsignificent cultivar differences and was affected by season, available soil moisture, dehulling and grinding methods. Gel spread was negatively associated with corneousness of the gram andparticle size index of the flour. It was also associated with the roti and ugali properties assessed by taste panelists. The flow of cold flour-KOH gels in test tubes varied among cultivars and deserves more investigations. The value of gel <b>consistency</b> <b>tests</b> in sorghum quality improvement programs is discussed...|$|R
40|$|We {{describe}} {{the implementation of}} a search for gravitational waves from compact binary coalescences in LIGO and Virgo data. This all-sky, all-time, multidetector search for binary coalescence has been used to search data taken in recent LIGO and Virgo runs. The search is built around a matched filter analysis of the data, augmented by numerous signal <b>consistency</b> <b>tests</b> designed to distinguish artifacts of non- Gaussian detector noise from potential detections. We demonstrate the search performance using Gaussian noise and data from the fifth LIGO science run and demonstrate that the signal <b>consistency</b> <b>tests</b> are capable of mitigating the effect of non-Gaussian noise and providing a sensitivity comparable to that achieved in Gaussian noise...|$|R
40|$|Responses of inattentive or {{inconsistent}} {{subjects in}} stated-choice (SC) surveys {{can lead to}} imprecise or biased estimates. Several SC studies have investigated inconsistency {{and most of these}} studies dropped subjects who were inconsistent. However, none of these studies reported who is more likely to fail <b>consistency</b> <b>tests.</b> We investigated the effect of the personal characteristics and task complexity on preference inconsistency in eight different SC surveys. We found that white, higher-income and better-educated female subjects were less likely to fail <b>consistency</b> <b>tests.</b> Understanding the characteristics of subjects who are inattentive to the choice task may help in designing and pre-testing instruments that work effectively for a wider range of subjects. Copyright © 2009 John Wiley & Sons, Ltd. ...|$|R
3000|$|Scenario 8 : using {{multiple}} MCDM methods {{is not recommended}} as the internal <b>consistency</b> <b>test</b> yielded lower coefficients for the MCDM methods while the external <b>consistency</b> <b>test</b> yielded equal coefficients; and [...]...|$|E
3000|$|Scenario 6 : using {{multiple}} MCDM methods {{is not recommended}} as the internal <b>consistency</b> <b>test</b> yielded equal coefficients for both MCDM methods and Intuitive approach while the external <b>consistency</b> <b>test</b> yielded lower coefficients for the MCDM methods; [...]...|$|E
3000|$|Scenario 3 : using {{multiple}} MCDM methods is preferred, because: (1) the internal <b>consistency</b> <b>test</b> yielded higher {{coefficients for the}} MCDM methods while the external <b>consistency</b> <b>test</b> yielded lower coefficients, (2) However, Hypothesis 2 results (i.e., all three correlation coefficient methods results) showed {{a high degree of}} consistency between the MCDM methods when the same SME set of priorities are analyzed, which could lead to arguing that the internal <b>consistency</b> <b>test</b> (i.e., testing the results of the same MCDM method across all SMEs) may carry more weight than the internal consistency test; hence the Scenario 3 recommendation. Nevertheless, this argument calls for further research; [...]...|$|E
40|$|We {{introduce}} {{a method for}} conducting a targeted, coherent search for compact binary coalescences. The search is tailored {{to be used as}} a follow-up to electromagnetic transients such as gamma-ray bursts. We derive the coherent search statistic for Gaussian detector noise and discuss the benefits of a coherent, multidetector search over coincidence methods. To mitigate the effects of nonstationary data, we {{introduce a}} number of signal <b>consistency</b> <b>tests,</b> including the null signal-to-noise ratio, amplitude consistency, and several χ 2 tests. We demonstrate the search performance on Gaussian noise and on data from LIGO’s fourth science run and verify that the signal <b>consistency</b> <b>tests</b> are capable of removing the majority of noise transients, giving the search an efficiency comparable to that achieved in Gaussian noise...|$|R
40|$|Abstract. This {{paper is}} an {{empirical}} application that combines averting behavior with contingent valuation data. <b>Consistency</b> <b>tests</b> are performed incorporating alternative heteroscedastic {{structures in the}} bivariate probit models {{by taking advantage of}} the different information content that characterizes each data source. We look at three covariates not yet examined in the literature when combining stated and revealed preferred data to explain the variance in the models: income, the bid in the contingent valuation questionnaire, and the distance between the bid and the averting expenditures with drinking water. The models estimated include between and within data sources heteroscedasticity. The results obtained allow the combination of the two data sources under a common preference structure. Key words: averting behavior, combination of data sets, <b>consistency</b> <b>tests...</b>|$|R
40|$|We {{present the}} details of a method for {{conducting}} a targeted, coherent search for compact binary coalescences. The search is tailored {{to be used as a}} followup to electromagnetic transients such as Gamma Ray Bursts. We derive the coherent search statistic for Gaussian detector noise and discuss the benefits of a coherent, multi-detector search over coincidence methods. To mitigate the effects of non-stationary data, we introduce a number of signal <b>consistency</b> <b>tests,</b> including the null SNR, amplitude consistency and several χ^ 2 tests. We demonstrate the search performance on Gaussian noise and on data from LIGO's fourth science run and verify that the signal <b>consistency</b> <b>tests</b> are capable of removing the majority of noise transients and the search gives an efficiency comparable to that achieved in Gaussian noise. Comment: 21 pages, 14 figure...|$|R
40|$|Abstract—The <b>consistency</b> <b>test</b> {{is one of}} the {{critical}} components both in AHP and ANP. It is necessary to make sure if the judgment result is accuracy and reliable. This paper stated a specific process of <b>consistency</b> <b>test</b> in ANP with group judgment under intuitionistic fuzzy environment. A two steps de-fuzzification technique with intuitionistic fuzzy number reduction and generalized mean computation is proposed to apply in this study. The group consistency is also fully tested in two stages. This proposed process exposes that it is comprehensive and feasible. Besides, the application of maximum eigenvalue threshold method, a new <b>consistency</b> <b>test</b> index to check for the consistency, is an advantage because it reduces a lot of operations. Index Terms — Consistency testing, group judgment, analytic network process, intuitionistic fuzzy. I...|$|E
30|$|To {{check the}} {{consistency}} of the fuzzy pairwise comparison matrices, we used the <b>consistency</b> <b>test</b> auxiliary linear programming mentioned in Eqs. (9) through (15).|$|E
3000|$|Finally, we {{conducted}} a <b>consistency</b> <b>test</b> for the obtained weights. The final results about the 16 subindex weight ϖ [...] m [...] n [...] are presented in Table  7.|$|E
40|$|Starting from a variational {{formulation}} of the Kardar‐Parisi‐Zhang (KPZ) equation, we point out some strong constraints and <b>consistency</b> <b>tests,</b> to be fulfilled by real‐space discretization schemes. In the light of these findings, the mainstream opinion on the relevance of Galilean invariance and the fluctuation—dissipation theorem (peculiar of 1 D) is challenged. Peer Reviewe...|$|R
40|$|After {{the present}} {{establishment}} of CP-Violation in Bd-physics, <b>consistency</b> <b>tests</b> of unitarity in the Standard Model {{and the search}} of new phenomena are compulsory. I illustrate the way to look for T-violation, without contamination of absorptive parts, in correlated decays in B-factories. Bs-mixing and penguin-mediated Bs-decays are of prime importance in hadronic machines to look for new physics...|$|R
40|$|In {{this paper}} {{we present a}} way to {{automate}} <b>consistency</b> <b>tests</b> between two different kinds of software description techniques using PROLOG. Especially, we focus on hierarchical state transition diagrams and interaction sequences both of which specify aspects of a system's dynamics. In contrast to the similar graphical representations of UML our description techniques come with clear semantics which {{serve as the basis}} for all <b>consistency</b> <b>tests.</b> After a brief discussion of the involved description techniques, we present a specification for an example application. Then, we evolve an PROLOG algorithm to automatically test an actual implementation as given by a state automaton to corresponding requirements as opposed by a given set of interaction sequences. Keywords [...] - Testing, Specification, Behavior, Interaction, Description Technique, I. INTRODUCTION The development of software systems is a difficult and error prone. This is even true for small or medium-sized system which base on comp [...] ...|$|R
3000|$|Scenario 2 : using {{multiple}} MCDM methods is preferred over the Intuitive approach {{given that the}} internal <b>consistency</b> <b>test</b> yielded higher coefficients for the MCDM methods versus equal coefficients for the external consistency test; [...]...|$|E
40|$|We {{discuss a}} <b>consistency</b> <b>test</b> {{which makes it}} {{possible}} to discriminate unknown nuclear background lines from neutrinoless double beta decay with only one isotope. By considering both the transition to the ground state and to the first excited $ 0 ^+$ state, a sufficiently large detector can reveal if neutrinoless double beta decay or some other nuclear physics process is at work. Such a detector could therefore simultaneously provide a <b>consistency</b> <b>test</b> for a certain range of Majorana masses and be sensitive to lower values of the effective Majorana mass. Comment: 1 + 12 pages, 4 figures; v 2 : discussion enhanced, figures improved, matches journal versio...|$|E
40|$|We {{study the}} {{coupling}} of chiral bosons to external electromagnetic fields. It is observed that a naive gauging procedure leaves the gauge invariant chirality condition {{incompatible with the}} field equations. We propose {{the use of this}} feature as a <b>consistency</b> <b>test</b> to select the appropriate way to perform the gauge coupling. We verify that among all the possible gauging schemes, only the coupling of gauge fields with chiral currents passes the <b>consistency</b> <b>test.</b> As an application, we use this gauging scheme to show how the introduction of a gauge field becomes necessary in order to sold together a right and a left chiral boson...|$|E
40|$|The Collaboratory for the Study of Earthquake Predictability (CSEP) aims to prospectively test {{time-dependent}} earthquake probability forecasts {{on their}} consistency with observations. To compete, time-dependent seismicity models are calibrated on earthquake catalog data. But catalogs contain much observational uncertainty. We study {{the impact of}} magnitude uncertainties on rate estimates in clustering models, on their forecasts and on their evaluation by CSEP’s <b>consistency</b> <b>tests.</b> First, we quantify magnitude uncertainties. We find that magnitude uncertainty is more heavy-tailed than a Gaussian, such as a double-sided exponential distribution, with scale parameter νc = 0. 1 − 0. 3. Second, we study the impact of such noise on the forecasts of a simple clustering model which captures the main ingredients of popular short term models. We prove that the deviations of noisy forecasts from an exact forecast are power law distributed in the tail with exponent α = (aνc) − 1, where a is the exponent of the productivity law of aftershocks. We further prove that the typical scale of the fluctuations remains sensitively dependent on the specific catalog. Third, we study how noisy forecasts are evaluated in CSEP <b>consistency</b> <b>tests.</b> Noisy forecasts are rejected more frequently than expected for a given confidence limit. The Poisson assumption of the <b>consistency</b> <b>tests</b> is inadequate for shortterm forecast evaluations. To capture the idiosyncrasies of each model together with any propagating uncertainties, the forecasts need to specify the entire likelihood distribution of seismic rates...|$|R
30|$|Validation: This crucial {{procedure}} assesses {{how significant}} and relevant the various compiled statistics and estimates {{are in the}} various food groups. The rationality and <b>consistency</b> <b>tests</b> within and across time frames are important to establish {{the validity of the}} information presented. Various challenges that lurk around the corner include double counting and benchmarking against inappropriate/misleading industry metrics, including metrics that are ill defined.|$|R
50|$|Ideally, {{all tests}} are {{executed}} across all non-production test environments. To ensure accuracy and <b>consistency,</b> <b>testing</b> {{should be performed}} in the most complete, production-like environment possible. Strategies for increasing test environment stability include virtualization software (for dependencies your organization can control and image) service virtualization (for dependencies beyond your scope of control or unsuitable for imaging), and test data management.|$|R
40|$|In {{order to}} solve the problem that the {{accuracy}} of sensor data is reducing due to zero offset and the stability is decreasing in wireless sensor networks, a novel algorithm is proposed based on <b>consistency</b> <b>test</b> and sliding-windowed variance weighted. The internal noise is considered to be the main factor of the problem in this paper. And we can use <b>consistency</b> <b>test</b> method to diagnose whether the mean of sensor data is offset. So the abnormal data is amended or removed. Then, the result of fused data can be calculated by using sliding window variance weighted algorithm according to normal and amended data. Simulation results show that the misdiagnosis rate of the abnormal data can be reduced to 3 % by using improved <b>consistency</b> <b>test</b> with the threshold set to [0. 05, 0. 15], so the abnormal sensor data can be diagnosed more accurately and the stability can be increased. The accuracy of the fused data can be improved effectively when the window length is set to 2. Under the condition that the abnormal sensor data has been amended or removed, the proposed algorithm has better performances on precision compared with other existing algorithms. In order {{to solve the}} problem that the accuracy of sensor data is reducing due to zero offset and the stability is decreasing in wireless sensor networks, a novel algorithm is proposed based on <b>consistency</b> <b>test</b> and sliding-windowed variance weighted. The internal noise is considered to be the main factor of the problem in this paper. And we can use <b>consistency</b> <b>test</b> method to diagnose whether the mean of sensor data is offset. So the abnormal data is amended or removed. Then, the result of fused data can be calculated by using sliding window variance weighted algorithm according to normal and amended data. Simulation results show that the misdiagnosis rate of the abnormal data can be reduced to 3 % by using improved <b>consistency</b> <b>test</b> with the threshold set to [0. 05, 0. 15], so the abnormal sensor data can be diagnosed more accurately and the stability can be increased. The accuracy of the fused data can be improved effectively when the window length is set to 2. Under the condition that the abnormal sensor data has been amended or removed, the proposed algorithm has better performances on precision compared with other existing algorithms...|$|E
40|$|The {{object of}} this paper is to propose a <b>consistency</b> <b>test</b> for an {{individual}} involved in collective choice process. Collective choice processes considered in the paper are those that transform individuals `tastes' - which reflect the self-interested view point of the individuals - into (social) ranking of alternatives. In addition to her tastes, an individual has values about the way by which collective decision should be made. We distinguish two categories of such values. First, there are end-values that restrict the class of social rankings that the individual considers ethically acceptable. Second there are aggregation-values that specify the way by which the social ranking should depend upon the individuals tastes. The <b>consistency</b> <b>test</b> stands on an hypothetical operation of universalization of the individual tastes to everyone. Five illustrations of the potential usefulness of our approach for interpreting social choice theory and welfare economics are proposed. These illustrations deal with utilitarian aggregation in the presence of income inequality aversion, the so-called `ethics of responsibility' and the aggregation of individual ranking of opportunity sets based on their freedom of choice. A discussion of the relevance of the <b>consistency</b> <b>test</b> for addressing the problem of `laundering' individual preferences is also provided. ...|$|E
40|$|Mechanisms for {{the early}} {{electron}} transfer steps in the bacterial photosynthetic reaction center are discussed. An internal <b>consistency</b> <b>test</b> is described which places a real constraint on the unknown parameters. A resulting possible paradox is described, together with proposed electric field measurements and an alternate mechanism...|$|E
40|$|This lecture {{discusses}} the Higgs boson {{sectors of the}} SM and the MSSM, in particular {{in view of the}} recently discovered particle at ∼ 125. 5 GeV. It also covers their connection to electroweak precision physics and the implications for the <b>consistency</b> <b>tests</b> of the respective model. Comment: 30 pages, 17 figures. Lecture given at SUSSP 69, August 2012, St. Andrews, U...|$|R
40|$|Pulsed-laser {{polymerization}} {{results are}} presented for N-isopropyl acrylamide (NIPAM) in water; the data show the overtones that satisfy one of the PLP <b>consistency</b> <b>tests.</b> Apparent propagation rate coefficients k(p,app) were measured; these decrease with increasing concentrations of initiator, monomer and polymer, sugeestina significant association between the various species involved. The PLP data also qualitatively show the presence of low rates of termination and transfer...|$|R
40|$|The precise {{measurement}} {{of the mass of}} the top quark is one of the central goals of the Fermilab Tevatron. The top quark mass is a fundamental parameter of nature and also serves as an important input to <b>consistency</b> <b>tests</b> of the Standard Model and models beyond it [1, 2]. Within the Standard Model it provides indirect contraints on the Higgs boson mass. jet energy scal...|$|R
