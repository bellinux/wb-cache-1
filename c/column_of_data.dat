16|10000|Public
5000|$|Use the [...] {{option to}} sort {{on a certain}} column. For example, use [...] "" [...] to sort on the second column). In old {{versions}} of sort, the [...] option made the program sort on the second <b>column</b> <b>of</b> <b>data</b> ( [...] for the third, etc.). This usage is deprecated. $ cat zipcode Adam 12345 Bob 34567 Joe 56789 Sam 45678 Wendy 23456 [...] $ sort -k 2n zipcode Adam 12345 Wendy 23456 Bob 34567 Sam 45678 Joe 56789 ...|$|E
5000|$|The {{shuffling}} {{method is}} a very common form of data obfuscation. It {{is similar to the}} substitution method but it derives the substitution set from the same <b>column</b> <b>of</b> <b>data</b> that is being masked. In very simple terms, the data is randomly shuffled within the column. However, if used in isolation, anyone with any knowledge of the original data can then apply a [...] "What If" [...] scenario to the data set and then piece back together a real identity. The shuffling method is also open to being reversed if the shuffling algorithm can be deciphered.|$|E
50|$|Column-orientation has {{a number}} of advantages. If a search is being done for items {{matching}} a particular value in a <b>column</b> <b>of</b> <b>data,</b> only the storage objects corresponding to that data column within the table need to be accessed. A. A traditional row-based database would have to read the whole table, top to bottom. Another advantage is that when indexed correctly, a value {{that would have to be}} stored once in each row of data in a traditional database is stored only once, and in SAP IQ, an n-bitindex is used to access the data. Nbit and tiered indexing is used to allow for increased compression and fast, incremental batch loads.|$|E
5000|$|Re-perform {{important}} calculations, such as verifying <b>columns</b> <b>of</b> <b>data</b> {{that are}} formula driven; ...|$|R
5000|$|Consolidation of {{multiple}} databases {{into a single}} data base and identifying redundant <b>columns</b> <b>of</b> <b>data</b> for consolidation or elimination ...|$|R
3000|$|Three <b>columns</b> <b>of</b> <b>data</b> are {{processed}} in decision block, where the PCM signal is reconstructed {{as shown in}} Figure  6 a. SIs are generated from the previous PCM samples (Figure  6 b), {{and one of the}} possible intervals [...]...|$|R
5000|$|L1+Shared Memory:On-chip {{memory that}} can be used either to cache data for {{individual}} threads (register spilling/L1 cache) and/or to share data among several threads (shared memory). This 64 KB memory can be configured as either 48 KB of shared memory with 16 KB of L1 cache, or 16 KB of shared memory with 48 KB of L1 cache.Shared memory enables threads within the same thread block to cooperate, facilitates extensive reuse of on-chip data, and greatly reduces off-chip traffic.Shared memory is accessible by the threads in the same thread block. It provides low-latency access (10-20 cycles) and very high bandwidth (1,600 GB/s) to moderate amounts of data (such as intermediate results in a series of calculations, one row or <b>column</b> <b>of</b> <b>data</b> for matrix operations, a line of video, etc.). David Patterson says that this Shared Memory uses idea of local scratchpad ...|$|E
40|$|FORPRINT {{computer}} program prints FORTRAN-coded output files on most non-Postscript printers with such extra features as control of fonts for Epson and Hewlett Packard printers. Rewrites data to printer and inserts correct printer-control codes. Alternative uses include ability to separate data or ASCII file during printing {{by use of}} editing software to insert " 1 " in first <b>column</b> <b>of</b> <b>data</b> line that starts new page. Written in FORTRAN 77...|$|E
40|$|This {{document}} {{describes a}} convention for compressing FITS binary tables that is {{modeled after the}} FITS tiled-image compression method (White et al. 2009) {{that has been in}} use for about a decade. The input table is first optionally subdivided into tiles, each containing an equal number of rows, then every <b>column</b> <b>of</b> <b>data</b> within each tile is compressed and stored as a variable-length array of bytes in the output FITS binary table. All the header keywords from the input table are copied to the header of the output table and remain uncompressed for efficient access. The output compressed table contains the same number and order of columns as in the input uncompressed binary table. There is one row in the output table corresponding to each tile of rows in the input table. In principle, each <b>column</b> <b>of</b> <b>data</b> can be compressed using a different algorithm that is optimized for the type of data within that column, however in the prototype implementation described here, the gzip algorithm is used to compress every column. Comment: Proposed FITS Convention: [URL] v 1. 0, 28 October 2010, 6 page...|$|E
5000|$|Fields/Columns: Items {{can also}} have {{additional}} fields <b>of</b> information. This <b>data</b> can be shown as <b>columns</b> <b>of</b> <b>data</b> in the outline or as fields in the second pane (see 'Layout' below). Some outliners also allow the user to create custom fields and/or filter on fields.|$|R
50|$|To a user, SAP IQ {{looks just}} like any {{relational}} DBMS with a SQL-based language layer accessible via ODBC/JDBC drivers. However, inside, Sybase IQ is a column-oriented DBMS, which stores data tables as sections <b>of</b> <b>columns</b> <b>of</b> <b>data</b> rather than as rows <b>of</b> <b>data</b> like most transactional databases.|$|R
50|$|The package {{included}} a reference card that fit around the Commodore's function keys, customizable colors, and scrolling 80-column width display. The maximum file size is 23 pages, but multiple files can be chained for printing. OMNIWRITER can import up to 240 <b>columns</b> <b>of</b> <b>data</b> from Microsoft Multiplan.|$|R
40|$|Three of the {{arguments}} made by Temin (2008) in his review of Great Depressions of the Twentieth Century are demonstrably wrong: that {{the treatment of the}} data in the volume is cursory; that the definition of great depressions is too general and, in particular, groups slow growth experiences in Latin America in the 1980 s with far more severe great depressions in Europe in the 1930 s; and that the book is an advertisement for the real business cycle methodology. Without these three arguments — which are the results of obvious conceptual and arithmetical errors, including copying the wrong <b>column</b> <b>of</b> <b>data</b> from a source — his review says little more than that he does not think it appropriate to apply our dynamic general equilibrium methodology to the study of great depressions, and he does not like the conclusion that we draw: that a successful model of a great depression {{needs to be able to}} account for the effects of government policy on productivity. ...|$|E
40|$|In {{these data}} each row gives {{information}} for one unordered pair of players, named here player one (p 1) and player two (p 2). The show played for two seasons. The first <b>column</b> <b>of</b> <b>data,</b> season, indicates {{the season in}} which the pair played. The columns headed p 1. g (gender M/F), p 1. e (ethnicity White/Other), p 1. a (age in years) andp 1. f (player played ‘foe ’ TRUE/FALSE) give data on player one. There are corresponding columns for player two. The pair in any particular row were either eliminated at round one or two, or survived to round three. The variable rnd gives the round each pair reached and cash the total cash (in US$ 1000) the pair played for when they played ‘friend or foe’. Thus the 225 ’th pair were one white man aged 43 and one non-white woman aged 26. They were eliminated at round 1. The man played ‘friend ’ and lost all of the $ 1500 they had accumulated to the woman, who played ‘foe’...|$|E
30|$|For each sample, small nickel sphere {{standard}} {{was used to}} calibrate the VSM machine. Each time, the Teflon stick holding the thin film sample was saddled, and {{the direction of the}} stick was fixed where the magnetic moment was observed to be maximum. All experiments were carried out in nighttime to minimize noises, and the VSM was situated in a clean room at ground floor far from any noises. Substrate contribution to magnetic moments (m) was subtracted from the magnetic moments of substrate plus thin film. This subtraction was carried out by first determining the slope of the m vs. H curve as obtained for the thin film plus substrate. Then, a straight line equation (m − m 1) = slope (H − H 1) was written, and magnetic moment values corresponding to each magnetic field values were obtained from this equation. These values of magnetic moments due to substrate diamagnetic behavior were written in a column, and then this column values were then subtracted from the <b>column</b> <b>of</b> <b>data</b> for original magnetic moment values for thin film plus substrate. Thus, the magnetic moment values achieved represent the thin film material magnetic moment only.|$|E
5000|$|NMF has an {{inherent}} clustering property, i.e., it automatically clusters the <b>columns</b> <b>of</b> input <b>data</b> [...] It is this property that drives most applications of NMF.|$|R
5000|$|... where Yi• is {{the mean}} of the ith row <b>of</b> the <b>data</b> table, Y•j is the mean <b>of</b> the jth <b>column</b> <b>of</b> the <b>data</b> table, and Y•• is the overall mean <b>of</b> the <b>data</b> table.|$|R
50|$|Semantic mapping {{is similar}} to the auto-connect feature <b>of</b> <b>data</b> mappers with the {{exception}} that a metadata registry can be consulted to look up data element synonyms. For example, if the source system lists FirstName but the destination lists PersonGivenName, the mappings will still be made if these data elements are listed as synonyms in the metadata registry. Semantic mapping is only able to discover exact matches between <b>columns</b> <b>of</b> <b>data</b> and will not discover any transformation logic or exceptions between columns.|$|R
40|$|The Self Describing Data Sets (SDDS) file {{protocol}} is {{the basis}} for a powerful and expanding toolkit of generic programs. These programs are used for simulation postprocessing, graphics, data preparation, program interfacing, and experimental data analysis. This document describes Version 1. 30 of the SDDS commandline toolkit. Those wishing to write programs using SDDS should consult the Application Programmer’s Guide for SDDS Version 1. 5 [1]. The first section of the present document is shared with this reference. This document does not describe SDDS-compliant EPICS applications, of which they are many. Some of these will be covered in a separate manual. 1 Why Use Self-Describing Files? Before answering the question posed by the title of this section, it is necessary to define what a self-describing file is. As used here, data in self-describing files has the following attributes: • The data is accessed by name and by class. For example, one might ask for “the <b>column</b> <b>of</b> <b>data</b> called X”, or “the array of data called Y”. Self-describing data is not accessed by position in a file; e. g., one would not ask for “the third column of data”. • Various attributes of the data that may be necessary to using it are available. For example...|$|E
40|$|This paper {{introduces}} a new method for 2 D image compression whose quality is demonstrated through accurate 3 D reconstruction using structured light techniques and 3 D reconstruction from multiple viewpoints. The method {{is based on}} two discrete transforms: 1) A one-dimensional Discrete Cosine Transform (DCT) is applied to each row of the image. 2) The output from the previous step is transformed again by a one-dimensional Discrete Sine Transform (DST), which is applied to each <b>column</b> <b>of</b> <b>data</b> generating new sets of high-frequency components followed by quantization of the higher frequencies. The output is then divided into two parts where the low-frequency components are compressed by arithmetic coding and the high frequency ones by an efficient minimization encoding algorithm. At decompression stage, a binary search algorithm is used to recover the original high frequency components. The technique is demonstrated by compressing 2 D images up to 99 % compression ratio. The decompressed images, which include images with structured light patterns for 3 D reconstruction and from multiple viewpoints, are of high perceptual quality yielding accurate 3 D reconstruction. Perceptual assessment and objective quality of compression are compared with JPEG and JPEG 2000 through 2 D and 3 D RMSE. Results show that the proposed compression method is superior to both JPEG and JPEG 2000 concerning 3 D reconstruction, and with equivalent perceptual quality to JPEG 2000...|$|E
40|$|Twentieth Century are demonstrably wrong: {{that the}} {{treatment}} of the data in the volume is cursory; that the definition of great depressions is too general and, in particular, groups slow growth experiences in Latin America in the 1980 s with far more severe great depressions in Europe in the 1930 s; and that the book is an advertisement for the real business cycle methodology. Without these three arguments — which are the results of obvious conceptual and arithmetical errors, including copying the wrong <b>column</b> <b>of</b> <b>data</b> from a source — his review says little more than that he does not think it appropriate to apply our dynamic general equilibrium methodology to the study of great depressions, and he does not like the conclusion that we draw: that a successful model of a great depression {{needs to be able to}} account for the effects of government policy on productivity. ______________________________________________________________________ All of the data used in this paper are available at www. greatdepressionsbook. com and at www. econ. umn. edu/~tkehoe. We have benefited from extensive discussions with Hal Cole and Lee Ohanian. We gratefully acknowledge financial support from the National Science Foundation for support and thank John Dalton and Kevin Wiseman for excellent research assistance. The views expressed herein are those of the authors and not necessarily those of the Federal Reserve Bank of Minneapolis or th...|$|E
40|$|Geometrical {{models to}} explore and {{represent}} asymmetric proximity data are usually classified in two classes: distance models and scalar product models. In this paper we focalize on scalar product models, emphasizing some relationships and showing possibilities to incorporate external information that can help the analysis of proximities between rows and <b>columns</b> <b>of</b> <b>data</b> matrices. In particular it is 7 pointed out how some of these model apply {{to the analysis of}} skew-symmetry with external information...|$|R
50|$|Some {{operations}} used specific memory locations (those locations {{were not}} reserved {{and could be}} used for other purposes). Read a card stored the 80 <b>columns</b> <b>of</b> <b>data</b> from a card into memory locations 001-080. Index registers 1, 2 and 3 were in memory locations 087-089, 092-094 and 097-099 respectively. Punch a card punched the contents of memory locations 101-180 into a card. Write a line printed the contents of memory locations 201-332.|$|R
5000|$|... {{data level}} (individual <b>columns</b> and {{attributes}} <b>of</b> <b>data</b> stores) ...|$|R
40|$|Echelon is a {{high-level}} language {{which can be}} used to write efficient programs for Fleet. It is closely matched with Fleet’s abilities, yet is expressive enough for humans to write programs in by hand. 1 An Echelon: The Basic Unit of Computation The basic unit of computation in Echelon is the echelon (plural: echelons). It consists of a table-like structure of data having a fixed, finite width and either a fixed height or unbounded height: We draw the items in a row in a staggered configuration in order to emphasize {{the fact that they are}} processed in strictly left-to-right order (think of the echelon as moving upwards). This encourages the programmer to arrange the columns of her echelons so that their left-to-right order matches the order in which they are processed. We use the same shading color for each column to indicate that each <b>column</b> <b>of</b> <b>data</b> must be treated identically. As we will see later in this document, the Echelon compiler compiles an operation on a w-wide echelon into a requeueing UCB-AM 28 printed on August 14, 2007 page 1 of 10 UCB-AM 28 instruction loop of w instructions. For this reason, w must be known at compile time and every w th item must be handled by the same instruction. By contrast, echelons may be of either fixed height known at compile time (“finite echelons”) or of unbounded height (“infinite echelons”). These correspond to finite and infinite requeue counts on instructions, as we will also see later. ...|$|E
40|$|Abstract In this paper, we {{prove that}} Optimal Tuple Merge (OTM) is NP-Complete. OTM arises withinthe context of {{relational}} query language extensions to query and manipulate metadata {{as well as}} data. Such extensions include {{the ability to create}} dynamic output schemas from the inputdata. This flexibility is necessary for truly schema independent restructuring, however many null values may be introduced into the resultant data. Many of these &quot;artifical &quot; null values canbe subsequently merged away. We prove that the optimal merging case, in which the resulting relation contains as few tuples as possible, results in an NP-Complete problem. Fortunately,we can characterize when an optimal (and unique) merge is easy to obtain, and identify at least one class of practically relevant relations where OTM is efficiently solvable. 1 Introduction Recent frameworks for relational interoperability provide flexible methods for querying and re-structuring both metadata and data in canonical relations. This flexibility is necessary to satisfy the demands for schema independent queries that arise when combining semantically similar data frommultiple, distinct sources. Applications of relational interoperability are widespread, and include support for Federated Information Systems (FIS), encompassing the ability to create and managemediators, global schemas, and metadata dictionaries. Furthermore, recent extension to the relational query languages SQL and the RA enable real-time interoperability in a large federation ofrelational databases [3]. However, the added flexibility provided by frameworks for relational interoperability comeswith a price. One required capability is the ability to promote a <b>column</b> <b>of</b> <b>data</b> to relational attributes (column headings). In order that this &quot;transpose-like &quot; operation on relations has a con-sistent semantics for any relational table, many null values may be introduced into the result. We would ideally like to merge these &quot;artificial &quot; null values as much as possible to obtain a relationwith as few tuples as possible...|$|E
40|$|This {{csv file}} {{contains}} the raw {{center of pressure}} data measured in centimeter (cm). They were collected on an AMTI force plate across two days from 18 participants. Participant 1 - 11 participated on day 1. Participant 12 - 18 participated on day 2. Participant 17 discontinued mid study and the data was excluded from analysis. Each column is a time series evenly distributed across the duration of a trial (60 seconds). Each participant took part in 6 trials: 3 trials for the far condition and 3 trials for the near condition. Each trial yielded two columns of data for a participant. One <b>column</b> <b>of</b> <b>data</b> recorded the movement in the mediolateral (ML) axis, and the other column recorded the movement in the anterior-posterior axis (AP). The AP and ML axes act as coordinates for the x-y location {{of the center of}} pressure on the force plate. The columns in the csv file are ordered as thus: The first three columns are about P 1 (Participant 1) for the ML axis in the far condition. The next three columns are about P 2 for the ML axis in the far condition. The same pattern goes on until we reached Participant 18. Then the sequence repeats for the AP axis in the far condition, then for the ML axis in the near condition, and finally for the AP axis in the near condition. This dataset contains the raw center of pressure data collected on the Enrichment Voyage (www. semesteratsea. org) on an AMTI (Advanced Mechanical Technology, Inc.) force plate. The data was collected across two days from 18 participants. There were two conditions (the near condition and the far condition) split evenly across six trials in a randomized order. In the trials of the near condition, participants stood on the force plate with their hands comfortably on their sides with their shoes on. They maintained their gaze on a tripod located 50 cm from their heel {{for the duration of the}} trial. In the trials of the far condition, everything remains the same, except that the tripod was removed, and participants were instructed to look at the horizon. Trials were 60 seconds long...|$|E
5000|$|Expressions, {{which can}} produce either scalar values, or tables {{consisting}} <b>of</b> <b>columns</b> and rows <b>of</b> <b>data</b> ...|$|R
40|$|We {{describe}} an interoperability challenge that arose in Haiti, identify {{the parameters of}} a general problem in crisis data management, and present a protocol called Tablecast {{that is designed to}} address the problem. Tablecast enables crisis organizations to publish, share, and update tables <b>of</b> <b>data</b> in real time. It allows rows and <b>columns</b> <b>of</b> <b>data</b> to be merged from multiple sources, and its incremental update mechanism is designed to support offline editing and data collection. Tablecast uses a publish/subscribe model; the format is based on Atom and employs PubSubHubbub to distribute updates to subscribers...|$|R
40|$|Metadata is {{data that}} {{describes}} other data or resources. It {{has a defined}} number of named elements that convey meaning. Medical data are complex to process. For example, in the Primary Care Data Quality (PCDQ) renal programme, we need to collect over 300 variables {{because there are so}} many possible causes of renal disease. These variables are not just single <b>columns</b> <b>of</b> <b>data</b> [...] all are extracted as code plus date, while others are code-date-value. Metadata has the potential to improve the reliability of processing large datasets...|$|R
40|$|Authors:Brian Brown Date: 27 th November 1981 Brief Description:Data were {{recorded}} from Rod Smallwoood's arm on the 27 th November 1981; the dot matrix image {{which shows the}} ulna and radius bones. We made a 'radiotherapy type' mould of the arm and then put drawing pins through the plastic (pin head inwards) as electrodes. There two sets of data. One is recorded from the arm {{and the other is}} with saline filling the mould. The data were published in: D. C. Barber, B. H. Brown, and I. L. Freeston, "Imaging spatial distributions of resistivity using applied potential tomography", Electronics Letters, 19 (22) : 933 - 935, 1983 [URL] License:Creative Commons Artistic License (with Attribution) Attribution Requirement:Use or presentation of these data reference this publication: D. C. Barber, B. H. Brown, and I. L. Freeston, "Imaging spatial distributions of resistivity using applied potential tomography", Electronics Letters, 19 (22) : 933 - 935, 1983 Format:Data are handwritten and scanned into the linked pdf file. The adjacent drive/receive data sets for both the Uniform(Saline) and Arm data and these are included in the attached Excel file. The are 6 columns of data in the xls file. The first three are for the uniform case and give the two reciprocal data sets and the mean of the two. Columns 4 - 6 are for the arm. I did a quick reconstruction using columns 3 and 6 as ref and data respectively and it looked OK. Methods:The pdf file that is attached shows the line printer output of the data we recorded from Rod Smallwoood's arm on the 27 th November 1981 and the dot matrix image which shows the ulna and radius bones. We made a 'radiotherapy type' mould of the arm and then put drawing pins through the plastic (pin head inwards) as electrodes. There two sets of data. One is recorded from the arm and the other is with saline filling the mould. The pdf file also shows my plot of the XY position of the electrodes. Now the data set on the line printer is a complete data set i. e. Drive 1 / 2 then 1 / 3 then 1 / 4 etc for every combination. I could only find the print out for one of the data sets. However, I found my notebook with the adjacent drive/receive data set and this is page 7 of the pdf file. I have extracted the adjacent drive/receive data sets for both the Uniform(Saline) and Arm data and these are included in the attached Excel file. The are 6 columns of data in the xls file. The first three are for the uniform case and give the two reciprocal data sets and the mean of the two. Columns 4 - 6 are for the arm. I did a quick reconstruction using columns 3 and 6 as ref and data respectively and it looked OK. The first <b>column</b> <b>of</b> <b>data</b> is 104 point as follows. Drive 1 / 2 receive 3 / 4 Drive 1 / 2 receive 4 / 5 etc Drive 1 / 2 receive 16 / 1 Drive 2 / 3 receive 4 / 5 Drive 2 / 3 receive 5 / 6 etc Drive 2 / 3 receive 16 / 1 Drive 4 / 5 receive 6 / 7 Drive 4 / 5 receive 7 / 8 etc Drive 4 / 5 receive 16 / 1 etc etc Drive 14 / 15 receive 16 / 1 The second column is the other reciprocal set. I think these data are the ones used to produce the image in the Electronics Letters paper of 1983 - page 1 of my pdf file...|$|E
40|$|In the {{traditional}} Guttman-Harris type image analysis, a transformation {{is applied to}} the data matrix such that each <b>column</b> <b>of</b> the transformed <b>data</b> matrix is the best least squares estimate <b>of</b> the corresponding <b>column</b> <b>of</b> the <b>data</b> matrix from the remaining columns. The model is scale free. However, it assumes (1) that the correlation matrix is basic and (2) that the data matrix is free of measurement errors. In this paper a more generalized model is developed that does not require these two assumptions. Computational procedures are suggested. (Author...|$|R
2500|$|Rows {{beginning}} with an exclamation mark contains only comments. The row {{beginning with}} the hash symbol indicates {{that in this case}} frequencies are in megahertz (MHZ), S-parameters are listed (S), magnitudes are in dB log magnitude (DB) and the system impedance is 50 Ohm (R 50). There are 9 <b>columns</b> <b>of</b> <b>data.</b> <b>Column</b> 1 is the test frequency in megahertz in this case. Columns 2, 4, 6 and 8 are the magnitudes of , , [...] and [...] respectively in dB. Columns 3, 5, 7 and 9 are the angles of , , [...] and [...] respectively in degrees.|$|R
50|$|The Lite version {{features}} descriptive statistics, {{hypothesis testing}} (t-tests, chi-square, 1-way ANOVA with post-hocs, non-parametric tests), distribution testing, linear regression, correlation and {{a selection of}} basic graphing functions. It is limited to 254 rows and 12 <b>columns</b> <b>of</b> <b>data,</b> and does not offer import or export functions. The Pro version features advanced analyses (unlimited rows and columns, import and export, more means of analyses (2-way ANOVA, logistic and nonlinear regression, principal component analysis and multidimensional scaling, time series, power/sample-size, factorial design), more types of graph, word processing functions and options for formatting of results.|$|R
5000|$|Rows {{beginning}} with an exclamation mark contains only comments. The row {{beginning with}} the hash symbol indicates {{that in this case}} frequencies are in megahertz (MHZ), S-parameters are listed (S), magnitudes are in dB log magnitude (DB) and the system impedance is 50 Ohm (R 50). There are 9 <b>columns</b> <b>of</b> <b>data.</b> <b>Column</b> 1 is the test frequency in megahertz in this case. Columns 2, 4, 6 and 8 are the magnitudes of , , [...] and [...] respectively in dB. Columns 3, 5, 7 and 9 are the angles of , , [...] and [...] respectively in degrees.|$|R
50|$|This {{easter egg}} {{is in the}} built in System Manager of the HP 200LX. This 'easter egg' is {{probably}} more of a development tool than an easter egg, but, in any case, the user may display the function by first pressing the blue &... key to start 'More Applications'. The user may then hold down ALT while pressing F9 4 times, followed by F10 once. As long as the ALT key is held down, the user will observe <b>columns</b> <b>of</b> <b>data</b> about System Manager compliant (.EXM) programs registered with the System Manager, along with other arcane program information.|$|R
