5|26|Public
50|$|Scale effects {{resulting}} from <b>centralized</b> <b>acquisition</b> purchase centres {{in the food}} supply chain favor large players such as big retailers or distributors in the food distribution market. This {{is due to the}} fact that they can utilize their strong market power and financial advantage over smaller players. Having both strong market power and greater access to the financial credit market meant that they can impose barriers to entry and cement their position in the food distribution market. This would result in a food distribution chain that is characterized by large players on one end and small players choosing niche markets to operate in on the other end. The existence of smaller players in specialized food distribution markets could be attributed to their shrinking market share and their inability to compete with the larger players due to the scale effects. Through this mechanism, globalization has displaced smaller role players.Another mechanism troubling the specialized food distribution markets is the ability of distribution chains to possess their own brand. Stores with their own brand are able to combat price wars between competitors by lowering the price of their own brand, thus making consumers more likely to purchase goods from them.|$|E
40|$|Establish a multi-agency {{vehicle to}} {{coordinate}} CADD and GIS activities within the Department of Defense {{and with other}} participating governmental (federal, state and local) agencies, and the private sector. This includes setting standards, promoting system integration, supporting <b>centralized</b> <b>acquisition,</b> and providing assistance for the installation/district, training, operation, and maintenance of and facilities management systems. 05 / 05 / 200...|$|E
40|$|For {{the purpose}} of tracing the {{migration}} of radioactive materials in the environment {{it is essential to}} monitor meteorological and radiological parameters by a unified measurement system, because of the strong correlation of meteorological and radiological parameters. The ultimate goal is the prevention of radioactivity-induced diseases and disorders caused by radioactivity in both human population and the environment. A unified meteorological and radiological monitoring system can be readily implemented by using the organization and communication infrastructure of HYPERION technology. This would ensure an automatic and <b>centralized</b> <b>acquisition</b> of all relevant parameters...|$|E
5000|$|Providing <b>centralized</b> <b>acquisitions</b> and {{technical}} processing of all prints and non print library materials {{which includes the}} ordering, receipt, processing and distribution of new materials in shelf-ready condition; ...|$|R
40|$|This {{paper is}} an {{analysis}} of a community college district's attempt to introduce computer technology into the operation of its five libraries. In {{spite of the fact}} that the conversion from the Dewey Decimal Classification system to the Library of Congress Classification (LCC) system, which initiated the effort, began about nine years ago, the basic causes of failure are as relevant today as they were then because they are rooted in the minds of those responsible for them: librarians, computer specialists and institutional executives. Involved in the project were five libraries serving the district's five campuses, a <b>centralized</b> <b>acquisitions</b> and processing unit (referred to here as library technical services or LTS) responsible for ordering and cataloging materials for the district's five libraries, and the district's computer center. published or submitted for publicatio...|$|R
40|$|Wireless sensor {{networks}} (WSNs) {{facilitate a}} new paradigm to structural identification and monitoring for civil infrastructure. Conventional structural monitoring systems based on wired sensors and <b>centralized</b> data <b>acquisition</b> systems are costly for installation as well as maintenance. WSNs have emerged as a technology that can overcome such difficulties, making deployment of a dense array of sensors on large civil structures both feasible and economical. However, as opposed to wired sensor networks in which <b>centralized</b> data <b>acquisition</b> and processing is common practice, WSNs require decentralized computing algorithms to reduce data transmission due to the limitation associated with wireless communication. In this paper, the stochastic subspace identification (SSI) technique is selected for system identification, and SSI-based decentralized system identification (SDSI) is proposed to be implemented in a WSN composed of Imote 2 wireless sensors that measure acceleration. The SDSI is tightly scheduled in the hierarchical WSN, and its performance is experimentally verified in a laboratory test using a 5 -story shear building model. ⓒ 2015 by the authors; licensee MDPI, Basel, Switzerlandopen 0...|$|R
40|$|International audienceThe CODALEMA {{experiment}} aims {{to study}} the radio-detection of Ultra High Energy Cosmic Rays in the energy range of 1017 eV. Spread over an area of 0. 25 km 2, the original device hosted at Nançay (France) has mainly benefited of an array of short dipoles, connected by cables up to a <b>centralized</b> <b>acquisition</b> room. Since 2010, a major evolution has been initiated to add 60 autonomous radio-detection stations, covering a surface of 1. 5 km 2. This enlarged configuration should help refine the studies {{and serve as a}} bench test for the mastery of autonomous detection. The main characteristics of this new mode of operation is presented in the light of recent results obtained by the original CODALEMA setup...|$|E
40|$|The {{purpose of}} the CODALEMA experiment, {{installed}} at the Nançay Radio Observatory (France), is to study the radio-detection of ultra-high energy cosmic rays in the energy range of 10 ^ 16 - 10 ^ 18 eV. Distributed over an area of 0. 25 km^ 2, the original device uses in coincidence an array of particle detectors {{and an array of}} short antennas, with a <b>centralized</b> <b>acquisition.</b> A new analysis of the observable in energy for radio is presented from this system, taking into account the geomagnetic effect. Since 2011, a new array of radio-detectors, consisting of 60 stand-alone and self-triggered stations, is being deployed over an area of 1. 5 km^ 2 around the initial configuration. This new development leads to specific constraints to be discussed in term of recognition of cosmic rays and in term of analysis of wave-front. Comment: 13 pages, 12 figures Proceeding of the ARENA 2012 conference, Erlangen, Germany To be published in AI...|$|E
40|$|The OBELIX {{spectrometer}} {{is dedicated}} to meson spectroscopy and to antinucleon-nucleon and antinucleon-nucleus interaction studies. The time-of-flight (TOF) apparatus consists of two concentric arrays (30 + 90) of plastic scintillators. The OBELIX online architecture is designed to allow <b>centralized</b> data <b>acquisition</b> as well as independent data taking for each detector. The TOF online system is described. Software and hardware aspects are discussed. This system is being employed in test runs on the TOF apparatus and, integrated with other components of the whole detector, in beam tests...|$|R
25|$|This {{accumulated}} know-how {{resulted in}} the creation of the ULA Information and Documentation System (SIDULA). It was originally designed to be a comprehensive library management system that would physically <b>centralize</b> all the <b>acquisitions,</b> cataloguing and loans. With the evolution in the University networks, it was ultimately redesigned to work as a client–server system.|$|R
40|$|The {{concept of}} smart {{microgrid}} (SMG) is {{widely recognized as}} one of the most promising and enabling technologies for smart grids. In this context, the development of integrated and decentralized frameworks for executing complex control and monitoring applications is still in its infancy and needs to be researched. To address this issue, this paper conceptualizes a self-organizing computing framework, based on self-organizing agents, for solving the fundamental control and monitoring problems of an SMG without the need for <b>centralized</b> data <b>acquisition</b> and processing. Simulation results obtained for an 18 -bus test network are presented and discussed in order to demonstrate the significance and validity of the proposed new framework...|$|R
40|$|Structural {{monitoring}} systems used in practice employ conventional cables to communicate sensor measurements to a <b>centralized</b> data <b>acquisition</b> unit. Cabled based systems have high installation costs and leave wires vulnerable to ambient signal noise corruption. In addressing these inherent drawbacks, a modular wireless monitoring system is proposed. Such a system promises lower capital and installation costs simultaneously ensuring reliable communication between sensing units. A proof-of-concept sensing unit {{has been designed}} and fabricated using standard integrated circuit components and wireless modem technology. Employing an enhanced RISC microcontroller, the sensing unit has powerful computational capabilities for data aggregation and processing. The sensing unit is flexible in its design by allowing any analog sensor to be used. Two MEMS based accelerometers are considered for this study...|$|R
40|$|The Human Exploration Demonstration Project (HEDP) is {{an ongoing}} task at the NASA's Ames Research Center to address the {{advanced}} technology requirements necessary to implement an integrated working and living environment for a planetary surface habitat. The integrated environment consists of life support systems, physiological monitoring of project crew, a virtual environment work station, and <b>centralized</b> data <b>acquisition</b> and habitat systems health monitoring. The HEDP is an integrated technology demonstrator, {{as well as an}} initial operational testbed. There are several robotic systems operational in a simulated planetary landscape external to the habitat environment, to provide representative work loads for the crew. This paper describes the evolution of the HEDP from initial concept to operational project; the status of the HEDP after two years; the final facilities composing the HEDP; the project's role as a NASA Ames Research Center systems technology testbed; and the interim demonstration scenarios that have been run to feature the developing technologies in 1993...|$|R
30|$|Structural {{integration}} is often conceptualized as a binary construct contrasting, for instance, organizationally distinct vs. non-distinct (Puranam et al. 2009) or aligned vs. non-aligned functions, such as R&D activities (Paruchuri et al. 2006). This {{type of information}} can be retrievable from annual reports or press releases. Survey scales have also been used to capture structural integration. For example, Zollo and Singh (2004) used a survey-based approach to assess {{the extent to which}} systems, procedures, and products were aligned or <b>centralized</b> after an <b>acquisition.</b> Similarly, Bauer and Matzler (2014) used surveys to measure sociocultural integration, integration of production, marketing integration, and systems integration in their study.|$|R
40|$|Aiming at special {{demanding}} {{of weather}} monitoring in power system, an application project was designed basing on new type distributed and smart {{automatic weather station}} (AWS) Defects such as fixed hardware configuration and <b>centralized</b> data <b>acquisition</b> are overcome in new AWS. With distributed data acquisition node, AWS can extend and add new sensors for disaster forecasting of lightning, strong gale, icing of transmission line and insulators contamination flashover in power system {{and it is also}} convenient for evaluation of wind energy, solar energy and for power load forecasting. Using protective power supply to each node, AWS has high performance of lightning protection, which will not let system breakdown for any node crash. Three-stage automatic self-checking for node state is included, which can make failure diagnosis efficient. Data quality checking node is created in new AWS. It can provide data self-checking function and improve reliability of data from unattended AWS. Finally, application models of monitoring data are introduced. New type AWS is fit for system with many sensors or large amount of data flow. And this project can supply accurate, adequate and real-time data for weather disaster prevention and other applications in power system...|$|R
40|$|Understanding {{the dynamic}} {{behavior}} of civil engineering structures {{is important to}} adequately resolve problems related to structural vibration. The dynamic properties of a structure are commonly obtained by conducting a modal survey {{that can be used}} for model updating, design verification, and improvement of serviceability. However, particularly for large-scale civil structures, modal surveys using traditional wired sensor systems can be quite challenging to carry out due to difficulties in cabling, high equipment cost, and long setup time. Smart sensor networks (SSN) offer a unique opportunity to overcome such difficulties. Recent advances in sensor technology have realized low-cost smart sensors with on-board computation and wireless communication capabilities, making deployment of a dense array of sensors on large civil structures both feasible and economical. However, as opposed to wired sensor networks in which <b>centralized</b> data <b>acquisition</b> and processing are a common practice, the SSN requires decentralized algorithms due to the limitation associated with wireless communication; to date such algorithms are limited. This paper proposes a new decentralized hierarchical approach for modal analysis that reliably determines the global modal properties and can be implemented on a network of smart sensors. The efficacy of the proposed approach is demonstrated through several numerical examples...|$|R
40|$|Abstract: This work {{describes}} {{the design and}} implementation of an embedded system that provides <b>centralized</b> control, data <b>acquisition</b> and environment monitoring to particle detectors. The system was designed for low power consumption, allowing the use of renewable energies as power sources. The system runs an open source operating system, allowing to write applications that are independent of the particular hardware implementation used on the detectors. Communications are provided using an Ethernet network within the detector, and 802. 11 n wireless radio links for communications with a centralized data storage and control server. An implementation of this system is being deployed {{as part of the}} AMIGA muon counters. This implementation and its performance verification, which were carried out by ITeDA, are also discussed in this work...|$|R
40|$|Wireless {{smart sensor}} {{networks}} (WSSN) facilitate {{a new paradigm}} for structural health monitoring (SHM) of civil infrastructure. Conventionally, SHM systems employing wired sensors and <b>centralized</b> data <b>acquisition</b> {{have been used to}} characterize the state of a structure; however, widespread implementation has been limited due to high costs and difficulties in installation. WSSN offer a unique opportunity to overcome such difficulties. Recent developments have realized low-cost, smart sensors with on-board computation and wireless communication capabilities, making deployment of a dense array of sensors on large civil structures both economical and feasible. Wireless smart sensors (WSS) have shown their tremendous potential for SHM in recent full-scale bridge monitoring examples. However, structural damage identification using on-board computation capability in a WSSN, a primary objective of SHM, has yet to reach its full potential. This paper presents full-scale validation of a damage identification strategy using a decentralized network of Imote 2 nodes on a historic steel truss bridge. A total of 24 WSS nodes with 144 sensor channels are deployed on the bridge to validate the developed damage identification software. The performance of this decentralized damage identification strategy is demonstrated on the WSSN by comparing its results with those from the traditional centralized approach, as well as visual inspection. close...|$|R
40|$|In {{spite of}} the {{remarkable}} safety record of the nuclear industry as a whole, recent public concern over {{the potential impact of}} the industry's accelerated growth has prompted ERDA to expand its emergency response procedures. The Atmospheric Release Advisory Capability, ARAC, is a computer communications system designed to enhance the existing emergency response capability of ERDA nuclear facilities. ARAC will add at least two new functions to this capability: <b>centralized,</b> real-time data <b>acquisition</b> and storage, and simulation of the long range atmospheric transport of hazardous materials. To perform these functions, ARAC employs four major sub-systems or facilities: the site facility, the central facility, the global weather center and the regional model. The system has been under development {{for the past two years}} at the Lawrence Livermore Laboratory of the University of California. (auth...|$|R
40|$|There is a {{need for}} {{reliable}} monitoring systems to follow the evolution of the behavior of structures over time. Deflections and rotations are values that reflect the overall structure behavior. This paper presents an innovative approach to the measurement of long-term deformations of bridges by use of inclinometers. High precision electronic inclinometers can be used to follow effectively long-term rotations without disruption of the traffic. In addition to their accuracy, these instruments have proven to be sufficiently stable over time and reliable for field conditions. The Mentue bridges are twin 565 m long box-girder post-tensioned concrete highway bridges under construction in Switzerland. The bridges are built by the balanced cantilever method over a deep valley. The piers are 100 m high and the main span is 150 m. A <b>centralized</b> data <b>acquisition</b> system was installed in one bridge during its construction in 1997. Every minute, the system records the rotation and temperature at a number of measuring points. The simultaneous measurement of rotations and concrete temperature at several locations gives a clear idea of the movements induced by thermal conditions. The system will be used in combination with a hydrostatic leveling setup to follow the long-term behavior of the bridge. Preliminary results show that the system performs reliably and that the accuracy of the sensors is excellent. Comparison of the evolution of rotations and temperature indicate that the structure responds to changes in ai...|$|R
40|$|Several {{distributed}} coordinated precoding methods {{exist in}} the downlink multicell MIMO literature, many of which assume perfect knowledge of received signal covariance and local effective channels. In this work, we let the notion of channel state information (CSI) encompass this knowledge of covariances and effective channels. We analyze what local CSI is required in the WMMSE algorithm for distributed coordinated precoding, and study how this required CSI can be obtained in a distributed fashion. Based on pilot-assisted channel estimation, we propose three CSI acquisition methods with different tradeoffs between feedback and signaling, backhaul use, and computational complexity. One of the proposed methods is fully distributed, meaning that it only depends on over-the-air signaling but requires no backhaul, and results in a fully distributed joint system when coupled with the WMMSE algorithm. Naively applying the WMMSE algorithm together with the fully distributed CSI acquisition results in catastrophic performance however, and therefore we propose a robustified WMMSE algorithm based on the well known diagonal loading framework. By enforcing properties of the WMMSE solutions with perfect CSI onto the problem with imperfect CSI, the resulting diagonally loaded spatial filters are shown to perform significantly better than the naive filters. The proposed robust and distributed system is evaluated using numerical simulations, and shown to perform well compared with benchmarks. Under <b>centralized</b> CSI <b>acquisition,</b> the proposed algorithm performs on par with other existing centralized robust WMMSE algorithms. When evaluated {{in a large scale}} fading environment, the performance of the proposed system is promising. QC 20150521 </p...|$|R
40|$|Wireless Smart Sensor Networks (WSSNs) {{facilitates}} a {{new paradigm}} to structural identification and monitoring for civil infrastructure. Conventionally, wired sensors and central data acquisition systems {{have been used to}} characterize the state of the structure, which is quite challenging due to difficulties in cabling, long setup time, and high equipment and maintenance costs. WSSNs offer a unique opportunity to overcome such difficulties. Recent advances in sensor technology have realized low-cost, smart sensors with on-board computation and wireless communication capabilities, making deployment of a dense array of sensors on large civil structures both feasible and economical. However, as opposed to wired sensor networks in which <b>centralized</b> data <b>acquisition</b> and processing are common practice, WSSNs require decentralized algorithms due to the limitation associated with wireless communication; to date such algorithms are limited. This research develops new decentralized algorithms for structural identification and monitoring of civil infrastructure. To increase performance, flexibility, and versatility of the WSSN, the following issues are considered specifically: (1) decentralized modal analysis, (2) efficient decentralized system identification in the WSSN, and (3) multimetric sensing. Numerical simulation and laboratory testing are conducted to verify the efficacy of the proposed approaches. The performance of the decentralized approaches and their software implementations are validated through full-scale applications at the Irwin Indoor Practice Field in the University of Illinois at Urbana-Champaign and the Jindo Bridge, a 484 meter-long cable-stayed bridge located in South Korea. This research provides a strong foundation on which to further develop long-term monitoring employing a dense array of smart sensors. The software developed in this research is opensource and is available at: [URL]...|$|R
40|$|At the College of Staten Island (CSI) Library-CUNY, {{the library}} {{has access to}} over 160 {{different}} electronic resources. A concerted effort started in 2016 to start collecting relevant voluntary product accessibility template (VPAT) statements from new and current vendors and integrate these new practices into acquisition and electronic resources (ER) workflows. The paper will discuss the responsibilities of purchasing agents in libraries, acquisition or ER librarians, in regard to understanding disability law and how these legal mandates apply when investigating, acquiring, and maintaining electronic resources. Relevant tools will be discussed, in particular the use of VPATs and WCAG 2. 0 guidelines {{that can be used}} when evaluating digital resources for Section 508 compliance. A VPAT repository was started using <b>Centralized</b> Online Resource <b>Acquisitions</b> and Licensing (CORAL) tool, by the CSI library. The benefits and the limitations of these evaluation tools will be discussed, as well as the sharing of current processes used at other libraries in determining the accessibility of e-resources. The concept of universal design (UD) and how to incorporate UD into better purchasing decisions for ER products will be introduced...|$|R
40|$|At the Department for {{comparative}} {{medicine at}} NTNU, di erent instruments from di erent vendors are providing measurements related to cardiac function for physiological research in large animals. There {{is a need}} for a <b>centralized</b> tool for <b>acquisition,</b> calibration and synchronization of the di erent measurements sources. The purpose of this thesis is to develop a system that can ful ll these requirements. Functionality for calculating and comparing di erent parameters related to cardiac function will also be included in the system. An important motivation for the development of this system is to allow for comparison of invasive and non-invasive measurements. Invasive measurements produces more accurate indicators of cardiac function, but is often not available in the clinical setting. Non-invasive measurements is more readily available, but is less accurate. The system will have a possibility for comparing parameters calculated from invasive and non-invasive measurements, and thus aiding the development of more accurate measures of cardiac function based on non-invasive measurements. The system will be developed in close cooperation with a team of doctors, and will be tested out during operations on living pigs...|$|R
5000|$|The first {{professional}} Director in several years, {{and the first}} modern one, Kellam found the library in a chaotic state upon taking up his appointment; government publications and periodicals lay uncatalogued in the basement, faculty members were allowed to keep books indefinitely and academic departments had continued to purchase books and periodicals, but had generally neither cataloged them properly nor made them available to others. The student assistants who operated the library during nights and weekends were poorly trained, poorly supervised and often disorganized. To rectify the situation, Kellam <b>centralized</b> the <b>acquisition</b> of periodicals, {{increased the number of}} professional librarians on staff, nearly doubled the number of volumes in the collection and organized the library into five departments: circulation, reference (documents), order (books and periodicals), cataloging and periodicals (check-in, binding, exchanges). Each department was headed by a professional librarian. As a result, hours of service increased during evenings and weekends, faculty members were allowed to only keep borrowed materials for a year before having to renew them and a training program was instituted for student assistants. A browsing room opened in 1936 and the collection grew to 50,000 volumes in 1937. The University Archives were begun in the University's 50th anniversary year in 1939, and expenditures increased to $10,000 the same year; the number of current periodicals received by the library nearly doubled from 400 to 700. However, by the time Kellam left on August 31, 1939, the D. H. Hill Library building was found to be cramped and functionally obsolete. The library was no longer {{in the center of the}} campus, as the campus had expanded west over the previous decade. As Kellam concluded, [...] "...the present library building was planned from an artistic point of view and not for efficiency and without benefit of a librarian's advice. The same mistake should not be repeated." ...|$|R
40|$|During {{this first}} Quarter of the Project, {{a team of}} five {{individuals}} was formed to characterize aphron drilling fluids, with the ultimate objectives to gain acceptance for this novel technology and decrease the costs of drilling mature and multiple-pressure formations in oil and gas wells. Aphron drilling fluids are very high low-shear-rate viscosity fluids laden with specially designed microbubbles, or ''aphrons. '' The focus of the Project is to develop some understanding of the aphron structure and how aphrons and base fluid behave under downhole conditions. Four tasks were begun during this Quarter. All of these focus {{on the behavior of}} aphrons: (a) Aphron Visualization - to evaluate various methods of measuring bubble size distribution, especially Acoustic Bubble Spectroscopy (ABS), in aphron drilling fluids at elevated pressure; (b) Fluid Density - to investigate the effects of pressure, temperature and chemical composition on the survivability of aphrons; (c) Aphron Air Diffusivity - to determine the rate of loss of air from aphrons during pressurization; and (d) Pressure Transmissibility - to determine whether aphron networks (similar to foams) in fractures and pore networks reduce fracture propagation. The project team installed laboratory facilities and purchased most of the equipment required to carry out the tasks described above. Then work areas were combined to permit <b>centralized</b> data <b>acquisition</b> and communication with internal and external file servers, and electronic and hard copy filing systems were set up to be compatible with ISO 9001 guidelines. Initial feasibility tests for all four tasks were conducted, which led to some modification of the experimental designs so as to enable measurements with the required accuracy and precision. Preliminary results indicate that the Aphron Visualization, Aphron Air Diffusivity and Pressure Transmissibility tasks should be completed on time. The Fluid Density task, on the other hand, has some fundamental problems that may preclude realization of its objectives; alternative experimental approaches and methods of analysis will be explored during the next Quarter...|$|R
40|$|This {{dissertation}} {{studies the}} context-aware application with its proposed algorithms at client side. The required context-aware infrastructure {{is discussed in}} depth to illustrate that such an infrastructure collects the mobile user’s context information, registers service providers, derives mobile user’s current context, distributes user context among context-aware applications, and provides tailored services. The approach proposed tries {{to strike a balance}} between the context server and mobile devices. The context <b>acquisition</b> is <b>centralized</b> at the server to ensure the reusability of context information among mobile devices, while context reasoning remains at the application level. Hence, a <b>centralized</b> context <b>acquisition</b> and distributed context reasoning are viewed as a better solution overall. ^ The context-aware search application is designed and implemented at the server side. A new algorithm is proposed to take into consideration the user context profiles. By promoting feedback on the dynamics of the system, any prior user selection is now saved for further analysis such that it may contribute to help the results of a subsequent search. On the basis of these developments at the server side, various solutions are consequently provided at the client side. A proxy software-based component is set up for the purpose of data collection. This research endorses the belief that the proxy at the client side should contain the context reasoning component. Implementation of such a component provides credence to this belief in that the context applications are able to derive the user context profiles. Furthermore, a context cache scheme is implemented to manage the cache on the client device in order to minimize processing requirements and other resources (bandwidth, CPU cycle, power). Java and MySQL platforms are used to implement the proposed architecture and to test scenarios derived from user’s daily activities. ^ To meet the practical demands required of a testing environment without the impositions of a heavy cost for establishing such a comprehensive infrastructure, a software simulation using a free Yahoo search API is provided as a means to evaluate the effectiveness of the design approach in a most realistic way. The integration of Yahoo search engine into the context-aware architecture design proves how context aware application can meet user demands for tailored services and products in and around the user’s environment. The test results show that the overall design is highly effective, providing new features and enriching the mobile user’s experience through a broad scope of potential applications. ...|$|R
40|$|The Closed Environmental Research Chamber (CERC) at the NASA Ames Research Center {{was created}} to {{investigate}} both components and complete systems for life support of advanced space exploration missions. This facility includes a Main Chamber, an Airlock, a Sample Transfer Lock, a Vacuum System, an Air Recompression System, a dedicated control room and a pit area for housing supporting and environmental control systems. The Main Chamber provides 310 sq ft of internal working/living space on two levels. It is planned that the CERC will be a human-rated facility for habitation simulation under mass balance closure conditions. The internal pressure will be variable over the range of 14. 7 psia to 5 psia with accompanying capability for variation in atmosphere composition to maintain the oxygen partial pressure at 160 mm Hg. The CERC will be provided with a core set of primary life support subsystems for temperature and humidity control, C 02 removal and trace contaminant control. Interfacing with external life support technology test b~ds with be provided, along with connection to <b>centralized,</b> microprocessor-based data <b>acquisition</b> and control systems. This paper will discuss {{the current status of}} the CERC facility and show how it is being used to address the advanced technology requirements necessary to implement an integrated working and living environment for a planetary habitat. In particular, it will be shown how the CERC, along with a human-powered centrifuge, a planetary terrain simulator and advanced displays and a virtual reality capability will work together to develop and demonstration applicable technologies for future planetary habitats. Artificial intelligence and expert system programming techniques will be used extensively to provide an automated environment for a 4 -person crew. There will be several robotic mechanisms performing exploration tasks external to the habitat that will be controlled through the virtual environment to provide representative workloads for the crew. Finally, there will be a discussion of how effective are innovative new multidisciplinary test facilities to the investigation of the wide range of human and machine problems inherent in exploration missions...|$|R
40|$|Reorganization of the Army and {{creation}} of the Defense Intelligence Agency by Secretary of Defense Robert McNamara lead {{to the creation of}} the Area Intelligence Analysis Agency. For 20 years before 1962, the concept of technical intelligence had evolved in the US Army. Originally the Army Technical Services were charged with producing intelligence about German and Japanese weapons and about organizations analogous to the Army Technical Services in the German and Japanese armed forces. The Army Technical Services were bureaus within the Headquarters, Department of the Army, which supplied weapons, equipment, and services to the Army, managed the careers of officers in a particular branch (like the Quartermaster Corps, the Chemical Corps, and several medical-related branches in the Army Medical Department), trained specialists, and organized and trained special purpose military units. Each was headed by a general officer with a headquarters in Washington called the office of the chief (Like the Office of the Surgeon General). The Technical Services were: Service Name / Title of Chief / Abbreviation for Headquarters Chemical Corps / Chief Chemical Officer / OCCO Corps of Engineers / Chief of Engineers / OCE Army Medical Service / The Surgeon General* / OTSG Ordnance Corp / Chief of Ordnance / OCO Quartermaster Corps / Quartermaster General / OQMG Signal Corps / Chief Signal Officer / OCSO Transportation Corps / Chief of Transporation / OCT *The Surgeon General, US Army, should not be confused with the Surgeon General of the United States who is the head of the Public Health Service. After World War, Army intelligence efforts changed focus but remained interested in foreign weapons and equipment and technical services organizations in foreign armys. Since there was no national-level military intelligence agency, the Army Technical Services intelligence organizations were assigned to produce various kinds of strategic intelligence. For example, the Transportation Corps produced intelligence concerning railroads, inland waterways, and highways with assistance from its contractor Georgetown University. The Signal Corps produced intelligence on civilian communications and electrical networks with assistance from its contractor the Radio Corporation of America. The Board of Engineers for Rivers and Harbors of the Corps of Engineers produced intelligence about foreign ports. The Beach Erosion Board of the Corps of Engineers produced intelligence about potential landing beaches. The Corps of Engineers produced intelligence about terrain with assistance from its contractor the Military Geography Branch of the US Geological Survey. The Army Medical Services produced intelligence about public health and the public health systems in foreign countries. McNamara decided to modernize and centralize the way the Army does business. The Army was required to devise plans for <b>centralizing</b> weapons <b>acquisition,</b> military personnel management, training, etc. This required the abolishment and or radical reorganization of the Army Technical Servces. The Army Medical Services and the Corps of Engineers survived with greatly reduced responsibilities. The McNamara reforms of the Army are described in detail in From Root to McNamara: Army Organization and Administration (1975) by James E. Hewes, Jr., an official history produced by the Army Center for Military History which is available at: [URL] Because the Army Technical Services were being abolished, their intelligence functions had to be reassigned.. It was decided to divide the intelligence responsibilities of the old Army Technical Services between two new Army intelligence Agencies: The Army Foreign Science and Technology Center (FSTC) which was responsible for the intelligence about weapons and equipment, and The Area Analysis Intelligence Agency (AAIA) which took over the other intelligence responsibilities of the old Army technical services. That decision was formalized in in a Reorganization Planning Directive, “Department of the Army Reorganization Planning Directive 381 - 2, Technical, Area Analysis, and Order of Battle That decision Intelligence Production, 18 May 1962 ” which is available at: [URL] This file contains detailed instructions for disentangling the intelligence activities of the Army technical services to create the Army Area Intelligence Agency (AAIA), a special purpose military organization subordinate to the Army Map Service within the Corps of Engineers. The Medical Information and Intelligence Agency (MIIA) of the Army Medical Services was to be collocated with the AAIA and to work closely with it. The AAIA was to have about 720 workers, including personnel from contractors, and a budget of approximately 8 million dollars. Enclosure 2 to Tab B (Page 32 of this PDF document), describes the mission of the AAIA as follows: [AAIA] is responsible for planning, developing, and implementing a balanced intelligence program within the area of Army responsibility for the following fields of interest: a. Transportation, b. Military Geography, c. Telecommunications, [and] d. Military Resources. Such a program contains an all source capability and will include initiating and guiding the collection of intelligence information, organizing and maintaining intelligence, and producing and disseminating intelligence in whatever form required for Army, joint, and national intelligence use in R 2 ̆ 6 D, operational and logistical planning, training, and missile support. The Agency will receive and integrate health and sanitation data produced by the Medical Information and Intelligence Agency, Office of the Surgeon General. The organization chart of the AAIA in Enclosure 2 to Tab D (on Page 51 of this PDF document) shows that the AAIA would have three department: (1) A Transportation Department with four divisions: The Highways Division The Railroad Division The Ports Division The Inland Waterways Division (2) A Military Industrial Department two divisions: The Telecommunions Division The Miliary Resources Division (3) An Environment Department with three divisons: The Terrain Division The Urban Areas Division The Coast and Landing Beaches Division A short time after AAIA was organized, it was nationalized when it was absorbed by the Defense Intelligence Agency (DIA). At that time the Medical Information and Intelligence Agency was also absorbed by DIA as well. ...|$|R
40|$|Modern seismic {{networks}} {{have grown to}} become increasingly complex infrastructures, composed of hundreds of devices and data streams scattered over wide geographic regions. Among the components of such networks are heterogeneous seismic and environmental sensors, digitizers, data loggers, data collection servers, wired and wireless communication hardware, and other devices and software subsystems charged with different data handling tasks, such as continuous data storage or analysis. In order to be effectively managed, a seismic network therefore needs a tiered software application. This application encompasses tasks that range from the low-level (hardware monitoring for failure detection) to the mid-level (data quality control) to the high-level (managing the final output of the network: recorded events, waveforms, and parametric data). At the same time such an application should provide a centralized and easy-to-use graphical user interface (GUI). Over the past two decades, several institutions and commercial companies have devoted great efforts {{to the development of}} software tools to manage and <b>centralize</b> the data <b>acquisition</b> and analysis for regional to global seismic networks. Among the most valuable products worth mentioning are: Earthworm, an open-source real-time seismic management system developed by the U. S. Geological Survey (Johnson et al. 1995); Antelope, a commercial real-time system for environmental data collection, developed by Boulder Real Time Technologies (BRTT 2008); and the more recent SeisComP (Hanka et al. 2000), an open-source tool for real-time data acquisition and analysis developed by the German Research Centre for Geosciences (GFZ-Potsdam). Although well-suited for real-time data collection and analysis, these systems do not currently provide advanced features for managing the infrastructure of a seismic network, such as state-of-health monitoring of the instrumentation or tracking all the network appliances. Trying to fill this gap, Instrumental Software Technologies (ISTI 2008) has recently developed SeisNetWatch (SeisNetWatch 2008), a tool for monitoring and controlling the data quality and the status of several types of data loggers and real-time seismic management systems. This desktop- and Web-accessible tool features a core system and a user interface written in Java, plus several “agents” each interacting with a particular piece of hardware or system. During the development of the Irpinia Seismic Network (ISNet) in southern Italy (Weber et al. 2007), we decided to address our needs of hardware monitoring and data management by developing our own solution, a Web-based application called SeismNet Manager. The application is designed as a graphical front-end to ISNet for internal and external users of the network, as well as its administrators, with an interface that is simple to use. SeismNet Manager leverages an instrument database and a seismic database {{to keep track of the}} hardware components that comprise the network (such as stations, servers, devices) and the data they produce (such as recorded waveforms and events). The application, universally accessible through a Web browser, fulfills the following needs: • to keep a detailed inventory of the multiple components that constitute a seismic network, including stations, sensors, data loggers, network hardware, generic hardware, data servers, and communication links; • to maintain a historical record of the installations and of the configuration details, as well as of the mutual connections of said components; • to perform real-time monitoring of some of the devices (hardware state and “health” problems, quality of the output) for alerting network operators of problems and complementing the seismic data; • to manage the seismic data produced by the network, obtained either through automatic data retrieval procedures or manual insertion by administrators (detected events, seismic recordings, parametric information) and to perform some routine tasks on returned data, including inspection, filtering, picking, and flagging. • to offer a Web-based interface that lets data consumers or network operators insert, edit, search, download and visualize all the available information (as tables, graphs, maps, waveform plots, and 3 D renderings). To accomplish these goals, which are not specific to ISNet but are shared by most seismic networks, we made use of opensource technological solutions such as Linux (Debian 2008), PostgreSQL (PostgreSQL 2008), and Tomcat (Tomcat 2008). Flexibility and configurability was a priority, so that we could tailor SeismNet Manager to the specific needs and actual hardware of different networks and could manage multiple networks. At the same time, SeismNet Manager is not designed as a “be-all do-all” system performing every task needed in a seismic network, some of which are better left to specialized and standard software packages. For instance, in ISNet the continuous data acquisition and storage from the stations and the real-time seismic data processing for seismic early warning are implemented elsewhere, as discussed below. SeismNet Manager is thus built on top of the various elements and subsystems already operating in a network...|$|R

