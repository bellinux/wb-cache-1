34|104|Public
5000|$|Direct-mapped caches have faster {{access time}} than set-associative caches. However, for a direct-mapped cache if {{multiple}} cache {{blocks in the}} memory map to same cache-line they end up evicting each other when anyone of them is accessed. This {{is known as the}} <b>cache</b> <b>conflict</b> problem. This problem is resolved by increasing the associativity of the cache. But {{there is a limit to}} which associativity can be increased owing to the complexity in its implementation. Thus, for solving the <b>cache</b> <b>conflict</b> problem for a cache with limited associativity victim cache is employed.|$|E
50|$|Copker in Network and Distributed System Security Symposium (NDSS) 2014 {{presented}} another cache-based solution against cold-boot attacks for public-key cryptographic computations. Public-key cryptographic algorithms such as RSA, requires {{much more}} memory space than symmetric encryption algorithms such as AES. Copker employs the WB (Write-Back) cache mode to keep data in caches. The WB cache mode {{is the most}} common mode; that is, modified data are not synchronized from caches into the RAM until explicit or implicit write-back operations. RSA computations are conducted with private keys in memory under the WB cache mode; then, sensitive private keys are kept in caches. Then, Copker finished the following mechanisms to prevent the sensitive private keys from being synchronized into RAM. (1) Eliminate heap variables, and only static variables are used in the computations; switching stack to pre-allocated data variables. So all used data variables are stored in reserved address space within caches, - no <b>cache</b> <b>conflict</b> among these data. (2) Task scheduling and kernel preemption are disabled. So, the RSA computations are not suspended; otherwise, the states of suspended tasks may be swapped into RAM. (3) All other cores are forced to enter the No-Fill cache mode. In such mode, read misses do not cause cache replacement (data are read either from another core that holds the newest copy of the data, or directly from RAM), and write misses access RAM directly. Hence, the cores sharing caches with the Copker core that are executing RSA computations, are forced to enter the no-fill mode, so that they cannot evict Copkerâ€™s caches.|$|E
40|$|<b>Cache</b> <b>conflict</b> misses {{can cause}} severe {{degradation}} in application performance. Previous {{research has shown}} that for many scientific applications majority of cache misses are due to conflicts in cache. Although, conflicts in cache are a major concern for application performance {{it is often difficult to}} eliminate them completely. Eliminating conflict misses requires detailed knowledge of the cache replacement policy and the allocation of data in memory. This information is usually not available to the compiler. As such, the compiler has to resort to applying heuristics to try and minimize the occurrence of conflict misses. In this paper, we present a probabilistic method of estimating <b>cache</b> <b>conflict</b> misses for setassociative caches. We present a set of experiments evaluating the model and discuss the implications of the experimental results. ...|$|E
40|$|Abstract. Sac is a {{functional}} array processing language particularly designed with numerical applications in mind. In this field the runtime performance of programs critically {{depends on the}} efficient utilization of the memory hierarchy. <b>Cache</b> <b>conflicts</b> due to limited set associativity are one relevant source of inefficiency. This paper describes the realization of an optimization technique which aims at eliminating <b>cache</b> <b>conflicts</b> by adjusting the data layout of arrays to specific access patterns and cache configurations. Its effect on cache utilization and runtime performance is demonstrated by investigations on the PDE 1 benchmark. ...|$|R
40|$|Loop fusion {{improves}} data locality {{and reduces}} synchronization in data-parallel applications. However, loop fusion {{is not always}} legal. Even when legal, fusion may introduce loop-carried dependences which prevent parallelism. In addition, performance losses result from <b>cache</b> <b>conflicts</b> in fused loops. In this paper, we present new techniques to: (1) allow fusion of loop nests {{in the presence of}} fusion-preventing dependences, (2) maintain parallelism and allow the parallel execution of fused loops with minimal synchronization, and (3) eliminate <b>cache</b> <b>conflicts</b> in fused loops. We describe algorithms for implementing these techniques in compilers. The techniques are evaluated on a 56 -processor KSR 2 multiprocessor and on a 16 -processor Convex SPP- 1000 multiprocessor. The results demonstrate performance improvements for both kernels and complete applications. The results also indicate that careful evaluation of the profitability of fusion is necessary as more processors are used. [...] ...|$|R
40|$|Loop fusion {{improves}} data locality {{and reduces}} synchronization in data-parallel applications. However, loop fusion {{is not always}} legal. Even when legal, fusion may introduce loop-carried dependences which reduce parallelism. In addition, performance losses result from <b>cache</b> <b>conflicts</b> in fused loops. We present new, systematic techniques which: (1) allow fusion of loop nests {{in the presence of}} fusion-preventing dependences, (2) allow parallel execution of fused loops with minimal synchronization, and (3) eliminate <b>cache</b> <b>conflicts</b> in fused loops. We evaluate our techniques on a 56 -processor KSR 2 multiprocessor, and show improvements of up to 20 % for representative loop nest sequences. The results also indicate a performance tradeoff as more processors are used, suggesting careful evaluation of the profitability of fusion. 1 Introduction The performance of data-parallel applications on cachecoherent shared-memory multiprocessors is significantly affected by data locality and by the cost [...] ...|$|R
40|$|Loop tiling is an e#ective {{optimizing}} transformation {{to boost}} the memory performance of a program, especially for dense matrix scientific computations. The magnitude and stability of the achieved performance improvements is heavily dependent on the appropriate selection of tile sizes. Many existing tile selection algorithms try to find tile sizes which eliminate self-interference <b>cache</b> <b>conflict</b> misses, maximize cache utilization, and minimize cross-interference <b>cache</b> <b>conflict</b> misses. These techniques depend heavily on the actual layout of the arrays in memory. Array padding, an e#ective data layout optimization technique, is therefore incorporated by many algorithms to help loop tiling stablize its e#ectiveness by avoiding "pathological" array sizes. In this paper we examine several such combined algorithms in terms of cost-benefit trade-o#s, and introduce a new algorithm. The preliminary experimental results show that more precise and costly tile selection and array padding [...] ...|$|E
40|$|The {{performance}} of parallel programs {{has suffered from}} memory access latencies induced by cache misses. In this paper, to investigate the causes of these cache misses, data parallel applications were executed on shared memory multiprocessors. The experiment showed that <b>cache</b> <b>conflict</b> misses occupied most of the cache misses. This {{was due to the}} cross interference among the grains composed of the part of data arrays. To address this problem, a tailored grain size was devised from the underlying cache architecture. Besides the interference among grains, cache performance was sensitive to the way data were constructed. To make data structure for exhibiting good cache behavior, a stride merging-arrays method was presented. This method entailed the reduction of <b>cache</b> <b>conflict</b> misses and reduced the useless prefetches in cache lines with multiple words. Simulation results show that these techniques may enhance the {{performance of}} parallel applications due to the improved cache performance...|$|E
30|$|The <b>cache</b> <b>conflict</b> is {{high for}} the small {{configuration}} with 1 [*]KB cache. The direct-mapped cache, backed up with a low-latency main memory, performs better than the method cache. When high-latency memories are used, the method cache performs better than the direct-mapped cache. This is expected as the long latency for a transfer is amortized when more data (the whole method) is filled in one request.|$|E
40|$|Simultaneous {{multithreading}} (SMT) is {{an architectural}} technique {{in which the}} processor issues multiple instructions from multiple threads each cycle. While SMT {{has been shown to}} be effective on scientific workloads, its performance on database systems is still an open question. In particular, database systems have poor cache performance, and the addition of multithreading has the potential to exacerbate <b>cache</b> <b>conflicts.</b> This paper examines database performance on SMT processors using traces of the Oracle database management system. Our research makes three contributions. First, it characterizes the memory-system behavior of database systems running on-line transaction processing and decision support system workloads. Our data show that while DBMS workloads have large memory footprints, there is substantial data reuse in a small, cacheable "critical" working set. Second, we show that the additional data <b>cache</b> <b>conflicts</b> caused by simultaneousmultithreaded instruction scheduling can be n [...] ...|$|R
40|$|The {{performance}} of both serial and parallel implementations of matrix multiplication is highly sensitive to memory system behavior. False sharing and <b>cache</b> <b>conflicts</b> cause traditional column-major or row-major array layouts to incur high variability in memory system performance as matrix size varies. This paper investigates {{the use of}} recursive array layouts to improve performance and reduce variability...|$|R
40|$|We {{address the}} problem of <b>cache</b> <b>conflicts</b> in loop nests. <b>Cache</b> <b>conflicts</b> degrade performance, {{particularly}} for locality-enhancing transformations, which rely on retaining reusable data in the cache to improve performance. We present a new technique called cache partitioning which eliminates conflicts by logically dividing the cache into a number of partitions and adjusting the array layout in memory to map different arrays to different partitions. One-dimensional cache partitioning, in which partitions contain contiguous data from each array, is described first, followed by multidimensional partitioning, in which the data in each partition need not be contiguous. The elimination of <b>conflicts</b> with <b>cache</b> partitioning enables accurate modelling of the benefit of locality-enhancing transformations. A new model is presented which is based on the sweep ratio, defined as the ratio of the amounts of data traffic between the cache and memory before and after applying a locality-enhancing transf [...] ...|$|R
40|$|Abstract: Loop fusion is {{recognized}} as an effective transformation for improving memory hierarchy performance. However, unconstrained loop fusion can lead to poor performance because of increased register pressure and <b>cache</b> <b>conflict</b> misses. In this paper, we present a cache-conscious analytical model for profitable loop fusion. We use this model to tune fusion parameters for different architectures through empirical search. Experiments on four different platforms {{for a set of}} applications show significant speedup over fully optimized code generated by state-of-the-art commercial compilers...|$|E
40|$|Code {{positioning}} is {{a well-known}} compiler optimization aiming at {{the improvement of the}} instruction cache behavior. A contiguous mapping of code fragments in memory avoids overlapping of cache sets and thus decreases the number of <b>cache</b> <b>conflict</b> misses. We present a novel cache-aware code positioning optimization driven by worst-case execution time (WCET) information. For this purpose, we introduce a formal cache model based on a conflict graph which is able to capture a broad class of cache architectures. This cache model is combined with a formal WCET timing model, resulting in a <b>cache</b> <b>conflict</b> graph weighted with WCET data. This conflict graph is then exploited by heuristics for code positioning of both basic blocks and entire functions. Code positioning is able to decrease the accumulated cache misses for a total of 18 real-life benchmarks by 15. 5 % on average for an automotive processor featuring a 2 -way setassociative cache. These cache miss reductions translate to average WCET reductions by 6. 1 %. For direct-mapped caches, even larger savings of 18. 8 % (cache misses) and 9. 0 % (WCET) were achieved. Categories and Subject Descriptors D. 3. 4 [Programming Languages]: Compilers; Optimization; C. 3 [Real-time and embedded systems]; B. 3. ...|$|E
40|$|The {{performance}} of applications on largescale shared-memory multiprocessors depends {{to a large}} extent on cache behavior. Cache conflicts among array elements in loop nests degrade performance and reduce the effectiveness of locality-enhancing optimizations. In this paper, we describe a new technique for reducing <b>cache</b> <b>conflict</b> misses. The technique, called cache partitioning, logically divides cache capacity into equal parts, and allocates arrays in memory such that each array maps into a separate partition in the cache. We present experimental results from KSR and SGI machines to demonstrate the effectiveness of cache partitioning in eliminating cache conflicts and in realizing the full benefit of locality-enhancing techniques for both sequential and parallel execution...|$|E
40|$|Instruction cache {{performance}} {{is important to}} instruction fetch eciency and overall processor performance. The layout of an executable has a substantial eect on the cache miss rate and the instruction working set size during execution. This means that the performance of an executable can be improved by applying a code-placement algorithm that minimizes instruction <b>cache</b> <b>conflicts</b> and improves spatial locality. We describe an algorithm for procedure placement, one type of code placement, that signicantly diers from previous approaches {{in the type of}} information used to drive the placement algorithm. In particular, we gather temporal-ordering information that summarizes the interleaving of procedures in a program trace. Our algorithm uses this information along with cache conguration and procedure size information to better estimate the conflict cost of a potential procedure ordering. It optimizes the procedure placement for single level and multilevel caches. In addition to reducing instruction <b>cache</b> <b>conflicts,</b> the algorithm simultaneously minimizes the instruction working set size of the program. We compare the performance of our algorithm with a particularly successful procedure-placement algorithm and show noticeable improvement...|$|R
40|$|Matrix {{multiplication}} is {{an important}} kernel in linear algebra algorithms, {{and the performance of}} both serial and parallel implementations is highly dependent on the memory system behavior. Unfortunately, due to false sharing and <b>cache</b> <b>conflicts,</b> traditional column-major or row-major array layouts incur high variability in memory system performance as matrix size varies. This paper investigates the use of recursive array layouts for improving the performance of parallel recursive matrix multiplication algorithms [...] ...|$|R
40|$|Careful page mapping {{has been}} shown in the past to be {{effective}} for reducing <b>cache</b> <b>conflicts</b> on both uniprocessor and Uniform Memory Access (UMA) multiprocessors. This paper extends previous page-mapping schemes to Cache-Coherent Non-Uniform Memory Access (CC-NUMA) multiprocessors. These extensions maintain the program's data-task affinity, which is important to CC-NUMA, while reducing <b>cache</b> set <b>conflicts</b> by carefully selecting the page frames. Using an execution-driven superscaler multiprocessor simulator, we find that a simplistic application of page-coloring performs worse than bin-hopping by 10 - 45 %, while by hashing the page color with part of the MID bits, pagecoloring can perform closely to bin-hopping. 1 Introduction Cache-Coherent Non-Uniform Memory Access (CCNUMA) multiprocessors become increasingly attractive as an architecture which provides a transparent access to local and remote memories and a good scalability. Systems based on this architecture include research prototy [...] ...|$|R
40|$|In this dissertation, a novel SIMD {{extension}} called Modified MMX (MMMX) for multimedia computing is presented. Specifically, the MMX {{architecture is}} enhanced with the extended subwords and the matrix register file techniques. The extended subwords technique uses SIMD registers that are {{wider than the}} packed format used to store the data. The extended subwords technique avoids data type conversion overhead and increases parallelism in SIMD architectures. This is because promoting the subwords of the source SIMD registers to larger subwords {{before they can be}} processed and demoting the results again before they can be written back to memory incurs conversion overhead. The matrix register file technique allows to load data that is stored consecutively in memory into a column of the register file, where a column corresponds to the corresponding subwords of different registers. In other words, this technique provides both row-wise as well as column-wise accesses to the media register file. It is a useful approach for matrix operations that are common in multimedia processing. In addition, in this work, new and general SIMD instructions addressing the multimedia application domain are investigated. It does not consider an ISA that is application specific. For example, special-purpose instructions are synthesized using a few general-purpose SIMD instructions. The performance of the MMMX architecture is compared to the performance of the MMX/SSE architecture for different multimedia applications and kernels using the sim-outorder simulator of the SimpleScalar toolset. Additionally, three issues related to the efficient implementation of the 2 D Discrete Wavelet Transform (DWT) on general-purpose processors, in particular the Pentium 4, are discussed. These are 64 K aliasing, <b>cache</b> <b>conflict</b> misses, and SIMD vectorization. 64 K aliasing is a phenomenon that happens on the Pentium 4, which can degrade performance by an order of magnitude. It occurs if two or more data items whose addresses differ by a multiple of 64 K need to be cached simultaneously. There are also many <b>cache</b> <b>conflict</b> misses in the implementation of vertical filtering of the DWT, if the filter length exceeds the number of cache ways. In this dissertation, techniques are proposed to avoid 64 K aliasing and to mitigate <b>cache</b> <b>conflict</b> misses. Furthermore, the performance of the 2 D DWT is improved by exploiting the data-level parallelism using the SIMD instructions supported by most general-purpose processors. Electrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|Procedure Positioning is a {{well known}} {{compiler}} optimization aiming at {{the improvement of the}} instruction cache behavior. A contiguous mapping of procedures calling each other frequently in the memory avoids overlapping of cache lines and thus decreases the number of <b>cache</b> <b>conflict</b> misses. In standard literature, these positioning techniques are guided by execution profile data and focus on an improved average-case performance. We present two novel positioning optimizations driven by worst-case execution time (WCET) information to effectively minimize the programâ€™s worst-case behavior. WCET reductions by 10 % on average are achieved. Moreover, a combination of positioning and the WCET-driven Procedure Cloning optimization proposed in [14] is presented improving the WCET analysis by 36 % on average. 1...|$|E
40|$|Abstract. Loop fusion is {{recognized}} as an effective program transformation for improving memory hierarchy performance. However, unconstrained loop fusion can lead to poor performance because of increased register pressure and <b>cache</b> <b>conflict</b> misses. The complex interaction between different levels of the memory hierarchy with the input program makes {{it very difficult to}} always make the right choice in fusing loops. In this paper, we present a cache-conscious analytical model for profitable loop fusion to be used with a constrained weighted fusion algorithm. We then extend the model to show its effectiveness {{in the context of an}} empirical tuning framework. A preliminary evaluation of the model is presented using hand experiments on four applications. ...|$|E
40|$|In {{a typical}} {{commercial}} multi-core processor, the last level cache (LLC) {{is shared by}} two or more cores. Existing {{studies have shown that}} the shared LLC is beneficial to concurrent query processes with commonly shared data sets. However, the shared LLC can also be a performance bottleneck to concurrent queries, each of which has private data structures, such as a hash table for the widely used hash join operator, causing serious <b>cache</b> <b>conflicts.</b> We show that <b>cache</b> <b>conflicts</b> on multi-core processors can significantly degrade overall database performance. In this paper, we propose a hybrid system method called MCC-DB for accelerating executions of warehouse-style queries, which relies on the DBMS knowledge of data access patterns to minimize LLC conflicts in multicore systems through an enhanced OS facility of cache partitioning. MCC-DB consists of three components: (1) a cacheaware query optimizer carefully selects query plans in order to balance the numbers of cache-sensitive and cache-insensitive plans; (2) a query execution scheduler makes decisions to corun queries with an objective of minimizing LLC conflicts; and (3) an enhanced OS kernel facility partitions the shared LLC according to each queryâ€™s cache capacity need and locality strength. We have implemented MCC-DB by patching the three components in PostgreSQL and Linux kernel. Our intensive measurements on an Intel multi-core system with warehouse-style queries show that MCC-DB can reduce query execution times by up to 33 %. 1...|$|R
40|$|Abstract. We present three {{algorithms}} for Cholesky factorization using minimum block {{storage for}} a distributed memory (DM) environment. One of the distributed square block packed (SBP) format algorithms performs similar to ScaLAPACK PDPOTRF, and our algorithm with iteration overlapping typically outperforms it by 15 â€“ 50 % for small and medium sized matrices. By storing the blocks contiguously, we get better performing BLAS operations. Our DM algorithms are not sensitive to <b>cache</b> <b>conflicts</b> and thus give smooth and predictable performance. We also investigate {{the intricacies of}} using rectangular full packed (RFP) format with ScaLAPACK routines and point out some advantages and drawbacks. ...|$|R
40|$|Multi-core and multithreaded {{processors}} present both {{opportunities and}} challenges {{in the design of}} database query processing algorithms. Previous work has shown the potential for performance gains, but also that, in adverse circumstances, multithreading can actually reduce performance. This paper examines the performance of a pipeline of hashjoin operations when executing on multithreaded and multicore processors. We examine the optimal number of threads to execute and the partitioning of the workload across those threads. We then describe a buffer-management scheme that minimizes <b>cache</b> <b>conflicts</b> among the threads. Additionally we compare the performance of full materialization of the output at each stage in the pipeline versus passing pointers between stages. 1...|$|R
40|$|Loop tiling is an {{effective}} optimizing transformation to boost the memory performance of a program, especially for dense matrix scientific computations. The magnitude and stability of the achieved performance improvements is heavily dependent on the appropriate selection of tile sizes. Many existing tile selection algorithms try to find tile sizes which eliminate self-interference <b>cache</b> <b>conflict</b> misses, maximize cache utilization, and minimize cross-interference <b>cache</b> <b>conflict</b> misses. These techniques depend heavily on the actual layout of the arrays in memory. Array padding, {{an effective}} data layout optimization technique, is therefore incorporated by many algorithms to help loop tiling stablize its effectiveness by avoiding "pathological" array sizes. In this paper we examine several such combined algorithms in terms of cost-benefit trade-offs, and introduce a new algorithm. The preliminary experimental results show that more precise and costly tile selection and array padding algorithms may not be justified by the resulting performance improvements since such improvements may also be achieved by much simpler and therefore less expensive strategies. The key issues in finding a good tiling algorithms are (1) to identify critical performance factors and (2) to develop corresponding performance models that allow predictions at a suffcient level of accuracy. Following this insight, we have developed a new tiling algorithm that performs better than previous algorithms in terms of execution time and stability, and generates code with a performance comparable to the best measured algorithm. Experimental results on two standard benchmark kernels for matrix multiply and LU factorization show that the new algorithm is orders of magnitude faster than the best previous algorithm without sacrificing stability and execution speed of the generated code...|$|E
40|$|Analyzing and {{optimizing}} {{program memory}} performance is a pressing problem in high-performance computer architectures. Currently, software solutions addressing the processormemory performance gap include compiler- or programmerapplied optimizations like data structure padding, matrix blocking, and other program transformations. Compiler optimization can be effective, {{but the lack}} of precise analysis and optimization frameworks makes it impossible to confidently make optimal, rather than heuristic-based, program transformations. Imprecision is most problematic in situations where hard-to-predict cache conflicts foil heuristic approaches. Furthermore, the lack of a general framework for compiler memory performance analysis makes it impossible to understand the combined effects of several program transformations. The Cache Miss Equation (CME) framework discussed in this paper addresses these issues. We express memory reference and <b>cache</b> <b>conflict</b> behavior in terms of sets of equations. The [...] ...|$|E
40|$|High {{performance}} architectures depend {{heavily on}} efficient multi-level memory hierarchies {{to minimize the}} cost of accessing data. This dependence will increase with the expected increases in relative distance to main memory. There {{have been a number}} of published proposals for <b>cache</b> <b>conflict</b> -avoidance schemes. In this paper we investigate the design and performance of conflict-avoiding cache architectures based on polynomial modulus functions, which earlier research has shown to be highly effective at reducing conflict miss ratios. We examine a number of practical implementation issues and present experimental evidence to support the claim that pseudo-randomly indexed caches are both effective in performance terms and practical from an implementation viewpoint. Keywords: novel cache architectures, pseudo-random hash functions, conflict-avoidance. 1 Introduction On current projections the next 10 years could see CPU clock frequencies increase by a factor of twenty whereas DRAM row-addr [...] ...|$|E
40|$|Instruction cache aware {{compilation}} {{seeks to}} lay out a program in memory {{in such a way}} that <b>cache</b> <b>conflicts</b> between procedures are minimized. It does this through profile-driven knowledge of procedure invocation patterns. On a multithreaded architecture, however, more conflicts may arise between threads than between procedures on the same thread. This research examines opportunities for the compiler to optimize instruction cache layout on a multithreaded architecture. We examine scenarios where (1) the compiler has knowledge about multiple programs that will be or are likely to be co-scheduled, and where (2) the compiler has no knowledge at compile time of which applications will be co-scheduled. We present solutions for both environments...|$|R
40|$|Algorithms which access memory {{regularly}} {{are typical}} for scientific computing, image processing and multimedia. <b>Cache</b> <b>conflicts</b> are often responsible for performance degradation, {{but can be}} avoided by an adequate placement of data in memory. The huge search space for such compile time placements is systematically reduced until we arrive at a class of very simple mappings, well known from data distribution onto processors in parallel computing. The choice of parameters is then guided by a cost function which reects the tradeoff between additional instruction overhead and reduced miss penalty. We show by experiment that when keeping the overhead low, a considerable speedup can be achieved...|$|R
40|$|The common {{approach}} to reduce <b>cache</b> <b>conflicts</b> is to in-crease the associativity. From a dynamic power perspective this associativity {{comes at a}} high cost. In this paper we present miss ratio performance and a dynamic power com-parison for set-associative caches, a skewed cache and also for a new organization proposed, the elbow cache. The el-bow cache extends the skewed cache organization with a relocation strategy for conflicting blocks. We show that these skewed designs significantly reduce the conflict problems while consuming up to 56 % less dy-namic power than a comparably performing 8 -way set as-sociative cache. We believe {{this to be the}} strongest case in favor of skewed caches presented so far...|$|R
40|$|We {{address the}} problem of {{improving}} the data cache performance of numerical applications [...] specifically, those with blocked (or tiled) loops. We present DAT, a data alignment technique utilizing arraypadding, to improve program performance through minimizing <b>cache</b> <b>conflict</b> misses. We describe algorithms for selecting tile sizes for maximizing data cache utilization, and computing pad sizes for eliminating self-interference conflicts in the chosen tile. We also present a generalization of the techinque to handle applications with several tiled arrays. Our experimental results comparing our technique with previous published approaches on machines with different cache configurations show consistently good performance on several benchmark programs, for a variety of problem sizes. 1 Introduction The growing disparity between processor and memory speeds makes the efficient utilization of cache memory a critical factor in determining program performance. While locality in instruction refe [...] ...|$|E
40|$|Multithreaded {{processors}} {{are used}} to tolerate long memory latencies. By executing threads loaded in multiple hardware contexts, an otherwise idle processor can keep busy, thus increasing its utilization. However, the larger size of a multi-thread working set can {{have a negative effect}} on <b>cache</b> <b>conflict</b> misses. In this paper we evaluate the two phenomena together, examining their combined effect on execution time. The usefulness of multiple hardware contexts depends on: program data locality, cache organization and degree of multiprocessing. Multiple hardware contexts are most effective on programs that have been optimized for data locality. For these programs, execution time dropped with increasing contexts, over widely varying architectures. With unoptimized applications, multiple contexts had limited value. The best performance was seen with only two contexts, and only on uniprocessors and small multiprocessors. The behavior of the unoptimized applications changed more noticeably with [...] ...|$|E
40|$|Prior {{knowledge}} of the target application leads to new optimization and customization opportunities in embedded system design. Such techniques often lead to design solutions that are better in terms of performance, area, or power. We present a technique that analyzes a given application and statically estimates the number of data cache misses for different associativity values, which is then used in performance and energy estimates. The technique consists of an initial analysis of array reference pairs and determining <b>cache</b> <b>conflict</b> frequency, followed by combining the conflict estimate for all references in a loop nest taken together, incorporating the given associativity value. This analytical estimation is orders of magnitude faster than simulation based techniques and is independent of the data size, leading to significant savings in time {{in the early stages}} of embedded system design where decisions on hardware/software trade-offs and architectural customization are taken. ...|$|E
40|$|Locking cache {{lines in}} hard {{real-time}} systems {{is a common}} means to ensure timing predictability of data references and to lower bounds on worst-case execution time, especially in a multi-tasking environment. Growing processing demand on multi-tasking real-time systems can be met by employing scalable multi-core architectures, like the recently introduced tile-based architectures. This paper studies the use of cache locking on massive multi-core architectures with private caches {{in the context of}} hard real-time systems. In shared cache architectures, a single resource is shared among all the tasks. However, in scalable cache architectures with private <b>caches,</b> <b>conflicts</b> exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. Our work extends the cache-unawar...|$|R
40|$|In a direct-mapped {{instruction}} cache, all {{instructions that}} have the same memory address modulo the cache size, share a common and unique cache slot. Instruction <b>cache</b> <b>conflicts</b> can be partially handled at linked time by procedure placement. Pettis and Hansen give in [1] an algorithm that reorders procedures in memory by aggregating them in a greedy fashion. The Gloy and Smith algorithm [2] greatly decreases the number of conflict-misses but increases the code size by allowing gaps between procedures. The latter contains two main stages: the cache-placement phase assigns modulo addresses to minimizes cache-conflicts; the memoryplacement phase assigns final memory addresses under the modulo placement constraints, and minimizes the code size expansion. In this paper: (1) we state the NP-completenes...|$|R
40|$|Prefetching is {{a widely}} used consumer-initiated {{mechanism}} to hide communication latency in sharedmemory multiprocessors. However, prefetching is inapplicable or insufficient for some communication patterns such as irregular communication, pipelined loops, and synchronization. For these cases, a combination of two fine-grain, producer-initiated primitives (referred to as remote-writes) is better able to reduce the latency of communication. This paper demonstrates experimentally that remote writes provide significant performance benefits in cache-coherent sharedmemory multiprocessors with and without prefetching. Further, the combination of remote writes and prefetching is able to eliminate most of the memory system overhead in the applications, except misses due to <b>cache</b> <b>conflicts.</b> 1 Introduction In traditional cache-coherent, shared-memory multiprocessors, interprocessor data movement primarily occurs {{in response to a}} read operation by the consumer of the data value, and is usually a [...] ...|$|R
