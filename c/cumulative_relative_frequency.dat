18|6251|Public
40|$|The {{issue of}} {{flooding}} {{that occurred in}} the city of Palembang negative impacts that can not be ignored because it involves the loss and suffering that is both morally and materially, for example, is the impact of stagnant water can cause damage to road infrastructure and complementary buildings, disrupting traffic on these roads or even shut down traffic movement if the puddle is happening is quite high and takes a long time. Moreover, the city of Palembang was a center of trade, services, {{and is one of the}} icons that became a tourist attraction in South Sumatra that can disrupt economic activity in the city of Palembang. The analysis of extreme rainfall intensity on Palembang was done by calculating the <b>cumulative</b> <b>relative</b> <b>frequency</b> of rainfall which is greater than 50 mm/hr. Spatial pattern of extreme rainfall based on the amount of the most extreme hourly rainfall at 3 stations in Palembang were done using Surfer software. The research results showed the greatest rainfall intensity occurred in Palembang with the <b>cumulative</b> <b>relative</b> <b>frequency</b> of rainfall intensity > 50 mm/hr in between the years 2005 to 2014 with 8, 1...|$|E
40|$|Previous {{studies have}} shown that ratings of SEM (Strong Experiences of Music) {{strongly}} relate to ratings of physical reactions {{at the conclusion of the}} music. As physical reactions are aroused by fluctuating acoustical features of the music, it has been suggested that ratings of physical reactions for each music passage differ. In this light this study makes an attempt to clarify the relationship between overall and partial evaluations of physical reactions in listening to music. Four representatives of partial evaluations of physical reactions were calculated and it was hypothesized that one of them corresponded to overall evaluations of physical reactions. The four representatives considered were the mean, the median, the maximum, and the upper values of <b>cumulative</b> <b>relative</b> <b>frequency</b> distribution of partial evaluations. Absolute values of differences between overall evaluations and partial representatives of physical reactions were calculated, and then averaged to indicate degrees of correspondence. As a result, R 80, the 80 % value of <b>cumulative</b> <b>relative</b> <b>frequency</b> distribution, strongly corresponded to overall evaluations. This suggests that an overall evaluation is determined rather based on high partial evaluations than by averaging all partial evaluations. This study is expected to contribute to the quantitative clarification of SEM in the future. 1...|$|E
40|$|Normal {{values have}} been {{established}} for haemoglobin and plasma calcium, inorganic phosphate, magnesium, iron and copper {{in the blood of}} Angora goats maintained in the Cape Midlands of South Africa. With the exception of the values obtained for haemoglobin, the data collected for these determinations present slightly to considerably skewed distribution curves. Normal values {{have been established}} by using <b>cumulative</b> <b>relative</b> <b>frequency</b> polygons constructed from these data. The journals have been scanned in colour with a HP 5590 scanner; 600 dpi. Adobe Acrobat v. 11 was used to OCR the text and also for the merging and conversion to the final presentation PDF-format...|$|E
30|$|The VGC {{data points}} in the {{frequency}} table represent the coordinates of the VGC curve. As in an ROC curve the origin of a VGC curve per definition is “ 0 ”. The data points are arranged according to the <b>cumulative</b> (<b>relative)</b> <b>frequencies</b> of the corresponding categories. The last point includes all decisions and therefore is “ 1 ” [16].|$|R
50|$|One {{way is to}} use the <b>relative</b> <b>cumulative</b> <b>frequency</b> Fc as an estimate.|$|R
40|$|Indices that {{quantify}} structural diversity {{have been}} adopted by research to specify particular attributes at forest stand level. Nevertheless, their employment {{has yet to be}} satisfactory and only a few studies have considered different attributes simultaneously to quantify forest structural complexity. This thesis investigates the structural complexity in a forest stand where coppice thinning treatments have occurred. A diversity index based on the degree of departure from a theoretic completely even distribution of frequencies was tested both on the height of the “tree crown centre” and on the diameter at breast height (DBH) values. For this purpose, <b>cumulative</b> <b>relative</b> <b>frequencies</b> were used. This part of the research was integrated with a point pattern analysis by following a specific approach to investigate relationships between tree stem locations and a given stand structural attribute such as the height of the “tree crown centre”. For this reason, distance-based and correlation-based functions were used. The tested indices showed a clear ability to quantify the differences between observed stand components. Since structural complexity is a quantifiable concept and is related to ecological processes governing forest ecosystems, this research could be a useful contribution to the quantification of forest biodiversity in a perspective of sustainable forest management, with particular reference to the traditional coppice management system...|$|R
40|$|The normal ranges for {{red cell}} counts, haemoglobin, blood sugar and blood urea {{nitrogen}} have been calculated for apparently healthy dogs {{emanating from the}} immediate surroundings of Onderstepoort. The statistical method involved makes use of <b>cumulative</b> <b>relative</b> <b>frequency</b> curves. A note is appended {{on the construction of}} these curves and the application of the "distribution-free tolerance limits test" to the results obtained. The journals have been scanned in colour with a HP 5590 scanner; 600 dpi. Adobe Acrobat v. 11 was used to OCR the text and also for the merging and conversion to the final presentation PDF-format...|$|E
40|$|We {{present a}} new global method for the {{identification}} of hotspots in conservation and ecology. The method {{is based on the}} identification of spatial structure properties through <b>cumulative</b> <b>relative</b> <b>frequency</b> distributions curves, and is tested with two case studies, the identification of fish density hotspots and terrestrial vertebrate species diversity hotspots. Results from the frequency distribution method are compared with those from standard techniques among local, partially local and global methods. Our approach offers the main advantage to be independent from the selection of any threshold, neighborhood, or other parameter that affect most of the currently available methods for hotspot analysis. The two case studies show how such elements of arbitrariness of the traditional methods influence both size and location of the identified hotspots, and how this new global method can be used for a more objective selection of hotspots...|$|E
40|$|Abstract: This paper {{presents}} the results obtained {{in a study of}} environmental noise pollution in Alexandria city. Alexandria is a coastal city. It is the second largest city in Egypt. Thirty seven sampling sites were selected to measure the noise level at three main streets in the city (Elgish Street, Horreya Avenue and Circular Highway). The minimum noise levels recoded at Elgish Street, Horreya Avenue and Circular Highway were 58. 4, 48. 6 and 40. 2 dB, respectively. The maximum values were close to 101 dB. The long-term annoyance was calculated based on the day-evening-night level (L DEN). The mean values of L DEN were more accurate if the day and night time is divided into three intervals. Relative frequency distribution and <b>cumulative</b> <b>relative</b> <b>frequency</b> distribution are used for assessing noise levels. The noise level exceeding 10 % of the measurement time (L 10...|$|E
40|$|Three pigeons {{performed}} on two-component multiple variable-interval variable-interval schedules of reinforcement. There were two independent variables: component duration and the <b>relative</b> <b>frequency</b> of reinforcement in a component. The component duration, which {{was always the}} same in both components, was varied over experimental conditions from 2 to 180 sec. Over these conditions, the <b>relative</b> <b>frequency</b> of reinforcement in a component was either 0. 2 or 0. 8 (± 0. 03). As the component duration was shortened, the <b>relative</b> <b>frequency</b> of responding in a component approached a value equal to the <b>relative</b> <b>frequency</b> of reinforcement in that component. When the <b>relative</b> <b>frequency</b> of reinforcement was varied over conditions in which the component duration was fixed at 5 sec, the <b>relative</b> <b>frequency</b> of responding in a component closely approximated the <b>relative</b> <b>frequency</b> of reinforcement in that component. That is, the familiar matching relationship, obtained previously only with concurrent schedules, was obtained in multiple schedules with a short component duration...|$|R
50|$|The <b>cumulative</b> <b>relative</b> {{survival}} rate for {{all age groups}} and histology follow-up was 60%, 52%, and 32% at 5 years, 10 years, and 20 years, respectively, with children doing better than adults.|$|R
2500|$|Benford's {{law says}} that this should be the {{opposite}} due to the <b>relative</b> <b>frequency</b> of a specific digit a {{in the first place}} of a number is Fa = log10 The digit 1 occurs with a <b>relative</b> <b>frequency</b> of about [...]30, the digit 2 with a <b>relative</b> <b>frequency</b> of about [...]18, and the digit 9 finally with about [...]05.|$|R
40|$|To {{automate}} the multiresolution {{procedure of}} Kuhl and Wilson for modeling and simulating arrival processes that exhibit long-term trends and nested periodic effects (such as daily, weekly, and monthly cycles), {{we present a}} statistical-estimation method that involves the following steps at each resolution level corresponding to a basic cycle: (a) transforming the <b>cumulative</b> <b>relative</b> <b>frequency</b> of arrivals within the cycle (for example, the percentage of all arrivals {{as a function of}} the day of the week within the weekly cycle) to obtain a statistical model with normal, constantvariance responses; (b) fitting a specially formulated polynomial to the transformed responses; (c) performing a likelihood ratio test to determine the degree of the fitted polynomial; and (d) fitting a polynomial of the degree determined in (c) to the original (untransformed) responses. An example demonstrates web-based software that implements this flexible approach to handling complex arrival processes. ...|$|E
40|$|Normal {{values for}} the various serum protein {{fractions}} of Merino sheep in South Africa have been established. Use was made of cellulose acetate membrane electrophoresis by the microzone technique and after processing the data these were plotted on <b>cumulative</b> <b>relative</b> <b>frequency</b> curves. Some sex differences in the values for albumin and globulins are apparent. The albumin fraction {{was found to be}} higher in rams and wethers than in ewes. Age differences are also apparent particularly with respect to the values for total serum protein and albumin which are lower in lambs and young animals than in adult sheep. The precision of the microzone technique has permitted a re-appraisal of the albumin: globulin ratio in ovine blood. The journals have been scanned in colour with a HP 5590 scanner; 600 dpi. Adobe Acrobat v. 11 was used to OCR the text and also for the merging and conversion to the final presentation PDF-format...|$|E
40|$|To {{automate}} the multiresolution {{procedure of}} Kuhl et al. for modeling and simulating arrival processes that may exhibit a long-term trend, nested periodic phenomena (such as daily and weekly cycles), or {{both types of}} effects, we formulate a statistical-estimation method that involves the following steps at each resolution level corresponding to a basic cycle: (a) transforming the <b>cumulative</b> <b>relative</b> <b>frequency</b> of arrivals within the cycle (for example, the percentage of all arrivals {{as a function of}} the time of day within the daily cycle) to obtain a statistical model with approximately normal, constant-variance responses; (b) fitting a specially formulated polynomial to the transformed responses; (c) performing a likelihood ratio test to determine the degree of the fitted polynomial; and (d) fitting to the original (untransformed) responses a polynomial of the same form as in (b) with the degree determined in (c). A comprehensive experimental performance evaluation involving 100 independent replications of eight selected test processes demonstrates the accuracy and flexibility of the automated multiresolution procedure...|$|E
40|$|Nine pigeons {{were used}} in two {{experiments}} in which a response was reinforced if a variable-interval schedule had assigned a reinforcement and if the response terminated an interresponse time within a certain interval, or class, of interresponse times. One such class was scheduled on one key, and a second class was scheduled on a second key. The procedure was, therefore, a two-key concurrent paced variable-interval paced variable-interval schedule. In Exp. I, the lengths of the two reinforced interresponse times were varied. The <b>relative</b> <b>frequency</b> of responding on a key approximately equalled the relative reciprocal {{of the length of}} the interresponse time reinforced on that key. In Exp. II, the <b>relative</b> <b>frequency</b> and <b>relative</b> magnitude of reinforcement were varied. The <b>relative</b> <b>frequency</b> of responding on the key for which the shorter interresponse time was reinforced was a monotonically increasing, negatively accelerated function of the <b>relative</b> <b>frequency</b> of reinforcement on that key. The <b>relative</b> <b>frequency</b> of responding depended on the relative magnitude of reinforcement in approximately the same way as it depended on the <b>relative</b> <b>frequency</b> of reinforcement. The <b>relative</b> <b>frequency</b> of responding on the key for which the shorter interresponse time was reinforced depended on the lengths of the two reinforced interresponse times and on the <b>relative</b> <b>frequency</b> and <b>relative</b> magnitude of reinforcement {{in the same way as}} the <b>relative</b> <b>frequency</b> of the shorter interresponse time depended on these variables in previous one-key concurrent schedules of reinforcement for two interresponse times...|$|R
40|$|Pigeons pecked {{for food}} in a two-key procedure. A {{concurrent}} variable-interval variable-interval schedule of reinforcement for two classes of interresponse times was arranged on each key. A visual stimulus set the occasion for potential reinforcement of the four operant classes: shorter and longer interresponse times on left and right keys. In Exp. I, the <b>relative</b> <b>frequency</b> of respones on a key equalled the <b>relative</b> <b>frequency</b> of reinforcement on that key. In Exp. II, the <b>relative</b> <b>frequency</b> of an interresponse time equalled the relative reciprocal of its length. In Exp. III, the <b>relative</b> <b>frequency</b> of an interresponse time was a monotonically increasing function of its <b>relative</b> <b>frequency</b> of reinforcement. These functions relating the <b>relative</b> <b>frequency</b> of an interresponse time to its relative length and to its <b>relative</b> <b>frequency</b> of reinforcement {{were the same as}} if there had been no second key. Also, the distribution of responses between keys was independent of the <b>relative</b> <b>frequency</b> of an interresponse time on either key. Experiment IV replicated Exp. I except that choices between keys were controlled by a stimulus that signalled the availability of reinforcement on the right key. A comparison of Exp. I and IV suggested that the <b>relative</b> <b>frequency</b> of an interresponse time on one key generally was independent of behavior on the other key, but that the number of responses per minute on a key did depend on behavior on the other key...|$|R
50|$|Popper's propensities, {{while they}} are not <b>relative</b> <b>frequencies,</b> are yet {{defined in terms of}} <b>relative</b> <b>frequency.</b> As a result, they face many of the serious {{problems}} that plague frequency theories. First, propensities cannot be empirically ascertained, on this account, since the limit of a sequence is a tail event, and is thus independent of its finite initial segments. Seeing a coin land heads every time for the first million tosses, for example, tells one nothing about the limiting proportion of heads on Popper's view. Moreover, the use of <b>relative</b> <b>frequency</b> to define propensity assumes the existence of stable <b>relative</b> <b>frequencies,</b> so one cannot then use propensity to explain the existence of stable <b>relative</b> <b>frequencies,</b> via the Law of large numbers.|$|R
40|$|As such, {{the use of}} ∆E LAB {{to specify}} color {{tolerances}} of ink has been common in the graphic arts. However, {{there is no easy}} way to assess color difference quantitatively between two complex images, e. g., a pictorial color proof and its corresponding press sheet. Consequently, the practice of qualifying a color proofing system which conforms to a known printing condition or verifying printed products that match a contract proof remains visual and subjective. This paper uses a standard press characterization target (IT 8. 7 / 3 basic block) to quantify pictorial color image difference by colorimetry. Colorimetric measurement conditions and ∆E(LAB) calculations adhere to the ANSI CGATS. 5 - 1993 standard. Comparison of color images with the same colorant conditions and images with different colorant conditions were investigated. The quantitative analysis mainly focuses on the use of <b>cumulative</b> <b>relative</b> <b>frequency</b> (CRF) of the ∆E distribution. We learned that a visual match between two color images can be specified by means of the CRF curve and its derived statistics. In addition, a visual match between two color images imposes a tighter colorimetric tolerance than process conformance as specified by solid ink density and dot gain...|$|E
40|$|Functional status {{assessments}} mainly {{concentrate on}} difficulty in performance and/or dependency on other people. Very few assessments document the perceived {{problems of the}} patient regarding the assessed activities. This study evaluates the added value of measuring perceived problems using a parallel assessment method: the Rehabilitation Activities Profile. The study population consisted of 57 stroke patients living at home, six months after stroke. Thirty-three were receiving therapy (physiotherapy or rehabilitation day care). The patients indicated that they perceived a number of problems, especially on the items ’walking’, ’using transport’, ’leisure activities ’ and ’relation with friends/acquaintances’. The perceived problem scores did not relate in a uniform way to the disability scores. The perceived problem scores were dichotomized. The proportions of persons with perceived problems showed {{statistically significant differences between}} the therapy group and the nontherapy group for 15 out of 21 items (p < 0. 02). The <b>cumulative</b> <b>relative</b> <b>frequency</b> distributions showed that the therapy group indicated significantly more problems than the nontherapy group (p < 0. 01). It was concluded that it is at least as important to measure perceived problems as it is to measure disabilities...|$|E
40|$|The {{probability}} density function (PDF) of wind speed {{is important in}} numerous wind energy applications. A large {{number of studies have}} been published in scientific literature related to renewable energies that propose the use of a variety of PDFs to describe wind speed frequency distributions. In this paper a review of these PDFs is carried out. The flexibility and usefulness of the PDFs in the description of different wind regimes (high frequencies of null winds, unimodal, bimodal, bitangential regimes, etc.) is analysed for a wide collection of models. Likewise, the methods that have been used to estimate the parameters on which these models depend are reviewed and the degree of complexity of the estimation is analysed in function of the model selected: these are the method of moments (MM), the maximum likelihood method (MLM) and the least squares method (LSM). In addition, a review is conducted of the statistical tests employed to see whether a sample of wind data comes from a population with a particular probability distribution. With the purpose of cataloguing the various PDFs, a comparison is made between them and the two parameter Weibull distribution (W. pdf), which has been the most widely used and accepted distribution in the specialised literature on wind energy and other renewable energy sources. This comparison is based on: (a) an analysis of the degree of fit of the continuous cumulative distribution functions (CDFs) for wind speed to the <b>cumulative</b> <b>relative</b> <b>frequency</b> histograms of hourly mean wind speeds recorded at weather stations located in the Canarian Archipelago; (b) an analysis of the degree of fit of the CDFs for wind power density to the <b>cumulative</b> <b>relative</b> <b>frequency</b> histograms of the cube of hourly mean wind speeds recorded at the aforementioned weather stations. The suitability of the distributions is judged from the coefficient of determination R 2. Amongst the various conclusions obtained, it can be stated that the W. pdf presents a series of advantages with respect to the other PDFs analysed. However, the W. pdf cannot represent all the wind regimes encountered in nature such as, for example, those with high percentages of null wind speeds, bimodal distributions, etc. Therefore, its generalised use is not justified and {{it will be necessary to}} select the appropriate PDF for each wind regime in order to minimise errors in the estimation of the energy produced by a WECS (wind energy conversion system). In this sense, the extensive collection of PDFs proposed in this paper comprises a valuable catalogue. Probability density distribution Wind power density Coefficient of determination Moments method Maximum likelihood method Least squares method...|$|E
5000|$|Benford's {{law says}} that this should be the {{opposite}} due to the <b>relative</b> <b>frequency</b> of a specific digit a {{in the first place}} of a number is Fa = log10+ 1)/a The digit 1 occurs with a <b>relative</b> <b>frequency</b> of about [...]30, the digit 2 with a <b>relative</b> <b>frequency</b> of about [...]18, and the digit 9 finally with about [...]05.|$|R
50|$|Find the <b>frequencies,</b> <b>relative</b> <b>{{frequency}},</b> <b>cumulative</b> frequency etc. as required.|$|R
40|$|A pigeon's pecking {{at each of}} two {{or three}} {{simultaneously}} available red keys was reinforced at different frequencies with a conditioned reinforcer, an orange key, on which 25 pecks resulted in a presentation of grain. Pecking was occasionally punished with a period of no reinforcement during which each key was dark. Both with two and with three keys, the <b>relative</b> <b>frequency</b> of pecking on a key was equal to the <b>relative</b> <b>frequency</b> of reinforcement obtained by pecks on that key. Also, the absolute frequency of pecking on each key was a linear function with zero intercept of the absolute frequency of reinforcement associated with that key. The slope of this function varied with the number of available keys; it was steeper with two than with three. The <b>relative</b> <b>frequency</b> of switching from any key (two successive pecks on different keys) approximated a linear function with zero intercept and slope slightly greater than 1. 0 of the total <b>relative</b> <b>frequency</b> of reinforcement associated with the keys to which the bird could switch. However, the <b>relative</b> <b>frequency</b> of switching to a particular key often showed systematic irregularities. The invariance in these data is the equality between the <b>relative</b> <b>frequency</b> of pecks on one {{of two or three}} keys and the <b>relative</b> <b>frequency</b> of reinforcement associated with that key...|$|R
40|$|The recent {{paper by}} Bartolino et al. (Popul Ecol 53 : 351 - 359, 2011) {{presents}} a new method to objectively select hotspots using <b>cumulative</b> <b>relative</b> <b>frequency</b> distribution (CRFD) curves. This method {{is presented as}} being independent from the selection of any threshold and, therefore, less arbitrary than traditional approaches. We argue that this method, albeit mathematically sound, is based on likewise arbitrary decisions regarding threshold selection. Specifically, {{the use of the}} CRFD curve approach requires the occurrence of two criteria for the method to be applied correctly: the selection of a 45 ° tangent to the curve, and the need to consider the highest relative value of the study parameter corresponding to a 45 ° slope tangent to the curve. Using two case studies (dealing with species richness and abundance of a particular species), we demonstrate that these two criteria are really unrelated to the underlying causes that shape the spatial pattern of the phenomena under study, but rather related to sampling design and spatial scale; hence, one could likewise use different but valid criteria. Consequently, the CRFD curve approach is based on the selection of a pre-defined threshold that has little, if any, ecological justification, and that heavily influences the final hotspot selection. Therefore, we conclude that the CRFD curve approach itself is not necessarily better and more objective than any of the global methods typically used for hotspot identification. Indeed, mathematical and/or statistical approaches should not be viewed as a panacea to solve conservation problems, but rather used in combination with biological, practical, economic and social considerations. Peer Reviewe...|$|E
40|$|In industry, {{simulation}} {{is one of}} {{the most}} widely used probabilistic modeling tools for modeling highly complex systems. Major sources of complexity include the inputs that drive the logic of the model. Effective simulation input modeling requires the use of accurate and efficient input modeling procedures. This research focuses on nonstationary arrival processes. The fundamental stochastic model on which this study is conducted is the nonhomogeneous Poisson process (NHPP) which has successfully been used to characterize arrival processes where the arrival rate changes over time. Although a number of methods exist for modeling the rate and mean value functions that define the behavior of NHPPs, one of the most flexible is a multiresolution procedure that is used to model the mean value function for processes possessing long-term trends over time or asymmetric, multiple cyclic behavior. In this research, a statistical-estimation procedure for automating the multiresolution procedure is developed that involves the following steps at each resolution level corresponding to a basic cycle: (a) transforming the <b>cumulative</b> <b>relative</b> <b>frequency</b> of arrivals within the cycle to obtain a linear statistical model having normal residuals with homogeneous variance; (b) fitting specially formulated polynomials to the transformed arrival data; (c) performing a likelihood ratio test to determine the degree of the fitted polynomial; and (d) fitting a polynomial of the degree determined in (c) to the original (untransformed) arrival data. Next, an experimental performance evaluation is conducted to test the effectiveness of the estimation method. A web-based application for modeling NHPPs using the automated multiresolution procedure and generating realizations of the NHPP is developed. Finally, a web-based simulation infrastructure that integrates modeling, input analysis, verification, validation and output analysis is discussed...|$|E
40|$|Purpose: Lean mass is an {{important}} component of health because of its multifaceted role in the body. Of particular concern are the effects of muscle mass loss due to aging. Lean mass index (LMI), calculated as lean mass/height 2, and fat-free mass index (FFMI) are used to assist in determining a healthy lean mass. The exact FFMI a healthy individual should have to be considered clinically healthy is unclear. Three population-based studies have been done to establish FFMI percentiles, but none were with American subjects. The {{purpose of this study was}} to develop LMI percentiles for females of different age groups, to compare the LMI values among these groups, and to compare the FFMI of this cohort to previous studies. Methods: Participants included 762 women, 18 to 75 yrs of age, who had DXA body composition testing at the Fitness Institute of Texas. LMI was calculated for each participant. The women were split into age groups of 18 - 22 (G 1), 23 - 39 (G 2), and 40 + (G 3) yrs of age. <b>Cumulative</b> <b>relative</b> <b>frequency</b> was used to determine the LMI percentiles for each group and a univariate ANOVA was used to compare the LMI of the three groups. FFMI percentiles were developed to compare with previously published studies using age groupings of 18 - 34 and 35 - 59 yrs of age. Results: The LMI percentiles for each age group are shown in Figure 1. LMI increased significantly between each age group: G 1 (15. 2 ± 1. 8 kg/ m 2) 3 ̆c G 2 (15. 7 ± 1. 9 kg/ m 2) 3 ̆c G 3 (16. 1 ± 2. 0 kg/ m 2) (...|$|E
50|$|If five {{consecutive}} years are multiplied, the resulting figure would {{be known as}} <b>cumulative</b> <b>relative</b> survival (CRS). It {{is analogous to the}} five-year overall survival rate, but {{it is a way of}} describing cancer-specific risk of death over five years after diagnosis.|$|R
40|$|Patients with {{diabetes}} mellitus (DM) {{often have a}} positive result on exercise testing despite a normal coronary arteriogram, which indicates that exercise-induced ST depression is not always an accurate indicator {{of the presence of}} coronary artery disease (CAD) in such patients. The present study evaluated the usefulness of the post-exercise systolic blood pressure (SBP) response for the detection of CAD in 47 consecutive patients with DM. Significant stenotic lesions were detected by angiography in 25 patients; 18 of these had true positive (TP) exercise testing results, and 7 had false negative (FN) results. No significant stenotic lesions were detected in the remaining 22 patients and of these 10 had true negative (TN) exercise testing results, and 12 had false positive (FP) results. The SBP ratio (SBP after 3 min of recovery divided by the SBP at peak exercise) was significantly higher in patients with coronary stenoses than in those without. Analysis of the <b>relative</b> <b>cumulative</b> <b>frequency</b> revealed that a SBP ratio greater than 0. 87 was associated with significant stenoses. The sensitivity, specificity, and accuracy of ST change combined with a SBP ratio greater than 0. 87 for detecting stenoses in patients with DM were 68 %, 82 %, and 74 %, respectively. These results suggest that calculating the SBP ratio, in combination with monitoring for ST depression, improves the accuracy of treadmill exercise testing for the detection of CAD in patients with DM...|$|R
40|$|Background: Intraoral lesions {{clinically}} suspicious for cancer/precancer {{should be}} biopsied and diagnosed histopathologically. We evaluated whether {{the frequency of}} oral cancer (OC) cases diagnosed in Puerto Rico (PR) is disproportionately high <b>relative</b> to the <b>frequency</b> of persons with histopathologic diagnoses that would have appeared clinically suspicious for OC/precancer at biopsy. Methods: All pathology reports for oral (ICD-O- 3 C 01 -C 06) soft tissue biopsies generated during 1 / 2004 – 5 / 2005 by seven PR and two New York City (NYC) pathology laboratories were reviewed. The analysis was restricted to persons diagnosed with invasive oral squamous cell carcinoma (OSCC), epithelial dysplasia, or hyperkeratosis/epithelial hyperplasia (HK/EH), i. e., diagnoses associated with lesions clinically suspicious for OC/ precancer. The OC <b>relative</b> <b>frequency</b> measured the percentage of persons diagnosed with OSCC among persons with OSCC, dysplasia, or HK/EH. OC <b>relative</b> <b>frequencies</b> for PR and NYC laboratories were compared. Results: Overall, the OC <b>relative</b> <b>frequency</b> was 67 % in PR and 40 % and 4 % in the NYC general and oral pathology laboratories, respectively (each p < 0. 001). In PR, the OC <b>relative</b> <b>frequency</b> was highest for males (80 %). When OC <b>relative</b> <b>frequencies</b> were stratified by pathology laboratory type (general/oral) and compared across PR and NYC, age/gender-specific OC <b>relative</b> <b>frequencies</b> were always higher in PR; however, differences were consistently statistically significant for males only. Conclusion: A disparity in the OC <b>relative</b> <b>frequency</b> exists in PR vs. NYC indicating a shortfall in biopsying potentially precancerous oral lesions in PR. PR residents with intraoral lesions suspicious for oral cancer/precancer {{are most likely to}} be biopsied only after developing an invasive OC...|$|R
40|$|Purpose: Lean mass is {{indicative}} of an individual’s overall health, as low levels are associated with issues including sarcopenia and increased hospital length of stay. Converting lean mass to a lean mass index (LMI) allows for comparison of individuals of different heights. Four population-based {{studies have been done}} to establish fat free mass index (FFMI) percentiles. However, none have had American subjects, {{with the exception of a}} small study that utilized an uncommon body composition method. Therefore, the purposes of this study were: 1) to establish a LMI for American adult males, 2) to compare the results to previous studies, and 3) to compare LMI between age categories of this population. Methods: Subjects were 642 men, 18 to 75 yrs of age (x = 29. 6 ± 12. 4 yrs), who had DXA body composition testing at the Fitness Institute of Texas. LMI was calculated as lean mass/height 2. <b>Cumulative</b> <b>relative</b> <b>frequency</b> analysis was performed to create indices for three age categories: 18 - 22 (G 1), 23 - 39 (G 2), and 40 + (G 3) yrs of age. LMI values for each age category were compared using ANOVA. To compare the results to those of existing studies, FFMI percentiles were calculated using age groups of 18 - 34 and 34 - 59 yrs. Results: Age group LMI percentile graphs were created. Mean LMI was higher in G 2 (19. 6 ± 2. 3 kg/m 2) and G 3 (19. 8 ± 2. 3 kg/m 2) than in G 1 (18. 5 ± 2. 1 kg/m 2) (P 3 ̆c 0. 05), but there was no difference between G 2 and G 3 (P 3 ̆e 0. 05). FFMI percentile results from this study were similar to previous studies, except the FFMI values above the 50 th percentile were greater in the 35 - 59 yr olds in the current study, as shown in Figure 1. Conclusions: The LMI percentile graphs, generated from a large cohort, provide a validated reference for health care professionals and clients. Contrary to expectations, FFMI did not decrease with age for this population. The FFMI percentile values were similar to those of previously published studies...|$|E
40|$|Smoothing {{techniques}} {{are designed to}} improve the accuracy of equating functions. The main purpose of this dissertation was to propose a new statistic (CS) and compare it to existing model selection strategies in selecting smoothing parameters for polynomial loglinear presmoothing (C) and cubic spline postsmoothing (S) for mixed-format tests under a random groups design. For polynomial loglinear presmoothing, CS was compared to seven existing model selection strategies in selecting the C parameters: likelihood ratio chi-square test (G 2), Pearson chi-square test (PC), likelihood ratio chi-square difference test (G 2 diff), Pearson chi-square difference test (PCdiff), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Consistent Akaike Information Criterion (CAIC). For cubic spline postsmoothing, CS was compared to the ± 1 standard error of equating (± 1 SEE) rule. In this dissertation, both the pseudo-test data, Biology long and short, and Environmental Science long and short, and the simulated data were {{used to evaluate the}} performance of the CS statistic and the existing model selection strategies. For both types of data, sample sizes of 500, 1000, 2000, and 3000 were investigated. In addition, No Equating Needed conditions and Equating Needed conditions were investigated for the simulated data. For polynomial loglinear presmoothing, mean absolute difference (MAD), average squared bias (ASB), average squared error (ASE), and mean squared errors (MSE) were computed to evaluate the performance of all model selection strategies based on three sets of criteria: <b>cumulative</b> <b>relative</b> <b>frequency</b> distribution (CRFD), relative frequency distribution (RFD), and the equipercentile equating relationship. For cubic spline postsmoothing, the evaluation of different model selection procedures was only based on the MAD, ASB, ASE, and MSE of equipercentile equating. The main findings based on the pseudo-test data and simulated data were as follows: (1) As sample sizes increased, the average C values increased and the average S values decreased for all model selection strategies. (2) For polynomial loglinear presmoothing, compared to the results without smoothing, all model selection strategies always introduced bias of RFD and significantly reduced the standard errors and mean squared errors of RFD; only AIC reduced the MSE of CRFD and MSE of equipercentile equating across all sample sizes and all test forms; the best CS procedure tended to yield an equivalent or smaller MSE of equipercentile equating than the AIC and G 2 diff statistics. (3) For cubic spline postsmoothing, both the ± 1 SEE rule and the CS procedure tended to perform reasonably well in reducing the ASE and MSE of equipercentile equating. (4) Among all existing model selection strategies, the ± 1 SEE rule in postsmoothing tended to perform better than any of the seven existing model selection strategies in presmoothing in terms of the reduction of random error and total error; (5) pseudo-test data and the simulated data tended to yield similar results. The limitations of the study and possible future research are discussed in the dissertation...|$|E
40|$|Brand {{color is}} one way to {{represent}} the identity of the corporation and for the company to advertise itself. The purpose of brand color is to make customers perceive the color, recognize the brand, and ultimately buy products manufactured by the corporation. However, {{it is not easy to}} produce the same color through different print media applications because of different processes of color reproduction technology. In the print industry, corporations have controlled brand color tolerance to insure its consistency. However, there is some question as to how the print industry actually performs in maintaining this tolerance through reproduction. The goal of this research is to find out what variability exists in commercial packaging and magazine advertising. CIE LAB (color space proposed by the CIE to attempt a perceptually uniform color space) was used to measure the difference in both media. For the methodology, brand colors based on the hue circle were selected for sampling and measurement. Seven colors were chosen for commercial packaging: Kodak yellow, Fuji green, Coca Cola red, Sunkist orange, Chips Ahoy cyan, Pepsi deep blue, and Alpha Bits cereal magenta. Two samples, Kodak yellow and Fuji green, were chosen for examples of magazine colors. The instrument, an X-Rite spectrophotometer, model 528, 500 series, was calibrated in D 50 / 2 for the measurement. The value of each color was acquired by the mean of three measurements. The samples were measured from ten Wegman 2 ̆ 7 s grocery stores near the Rochester area and the RIT bookstore over a period of five months. For the magazine advertisement, only two colors, Kodak yellow and Fuji green, were selected, because the advertisement of the other brand colors were not readily found in magazines. The magazines in the sample were photo - related magazines and published in the past five years. The measurement period was from May 13 to September 20, 2003. Three different methods are used for the analysis of results: a*b* slice plot with constant L value, Lab axis plot from Dr. Granger, and the CRF (<b>Cumulative</b> <b>Relative</b> <b>Frequency)</b> curves. As a result, each brand color has different performance in terms of color matching. For commercial packages, Sunkist orange has the smallest color difference, whereas Kodak yellow has the largest color difference. Post Alpha Bits magenta and Chip 2 ̆ 7 s Ahoy cyan were close to Sunkist orange, which means that they have a relatively small color difference. Pepsi 2 ̆ 7 s deep blue was close to Kodak yellow, which means Pepsi has a relatively large color difference. For the magazine advertising, the variation is much greater than that of the packages measured. According to the findings, each brand color for commercial packaging shows different performance in terms of color matching. The color match should be ΔE 3 ̆c 2. From the aspect of the 90 th percentile of samples for each brand color, none of samples is ΔE 3 ̆c 2. Assuming the expectation of the print service providers is that the sample for brand color should fall within acceptable tolerances, alternative hypothesis (Ha) is accepted. With respect to the measurement of the accurate reproduction of brand colors, the actual performance of print services providers shows variation in color reproduction that exceeds acceptable tolerances for color matching. For the samples of solid brand color, seven brand colors are selected based on the hue circle. Samples of each brand color show the different variation in terms of ΔE and move toward different CIE LAB axes. This result shows that controlling the variation of brand color should consider the nature of the variations. From the aspect of color matching, the variation of brand color for magazine advertisement is much greater than that for film packages. This is primarily due to the wider range of substrates and the print process technology and process color application inherent in publication printing. Thus, it is also concluded that the expectation of the companies for specifying color should be different for each type of application...|$|E
40|$|A {{well known}} result {{states that the}} set of numbers in base r in which the digits i occur with <b>relative</b> <b>frequency</b> pi for i = 0, [...] ., r − 1 {{is a set of}} Hausdorff {{dimension}}−(1 / log r) ∑r− 1 i= 0 pi logpi. For instance, decimal numbers in which only the digits 1 and 6 occur, both with <b>relative</b> <b>frequencies</b> 12, have Hausdorff dimension log 2 / log 10. In this paper we generalize this result to the situation where one prescribes the <b>relative</b> <b>frequencies</b> of groups of digits in the expansion. For example, suppose we require that in the decimal expansion digits from { 0, 1, 2 } occur with <b>relative</b> <b>frequency</b> 12, and also that digits from{ 3, 4, [...] ., 9 } occur with this <b>relative</b> <b>frequency.</b> Our result shows that the Hausdorff dimension of this set is (log 2 + 12 log 3 + 1 2 log 7) / log 10. Actually, we take a much more general geometric viewpoint, considering subsets of Moran fractals specified by prescribing the <b>relative</b> <b>frequencies</b> of groups of symbols in their codings. We determine the Hausdorff dimension of such sets, and moreover give necessary and sufficient conditions for such a set to have positive Hausdorff measure in its dimension. Mathematics Subject Classification: 28 A 80, 28 A 78 1...|$|R
5000|$|... #Subtitle level 2: <b>Relative</b> <b>frequencies</b> {{of letters}} in other {{languages}} ...|$|R
5000|$|... #Subtitle level 2: <b>Relative</b> <b>frequencies</b> {{of letters}} in the English {{language}} ...|$|R
