22|66|Public
5000|$|Modelization {{tool set}} (<b>clipping</b> <b>plane,</b> 2D projection, {{measurement}} tool, colors, view shot,…) ...|$|E
50|$|Here n {{stands for}} normal {{of the current}} <b>clipping</b> <b>plane</b> (pointed away from interior).|$|E
50|$|Now to find {{intersection}} point with the clipping window we calculate value of dot product. Let pE {{be a point}} on the <b>clipping</b> <b>plane</b> E.|$|E
50|$|<b>Clipping</b> <b>planes</b> {{are used}} in 3D {{computer}} graphics {{in order to prevent}} the renderer from calculating surfaces at an extreme distance from the viewer. The plane is perpendicular to the camera, a set distance away (the threshold), and occupies the entire viewport. Used in real-time rendering, <b>clipping</b> <b>planes</b> can help preserve processing for objects within clear sight.|$|R
5000|$|... some {{material}} parameters were removed, including back-face parameters and user defined <b>clip</b> <b>planes.</b>|$|R
5000|$|... #Caption: A view frustum, with near- and far- <b>clip</b> <b>planes.</b> Only {{the shaded}} volume is rendered.|$|R
50|$|Often, {{objects are}} so far away {{that they do not}} {{contribute}} significantly to the final image. These objects are thrown away if their screen projection is too small. See <b>Clipping</b> <b>plane.</b>|$|E
5000|$|One {{method of}} {{speeding}} up the shadow volume geometry calculations is to utilize existing parts of the rendering pipeline {{to do some of}} the calculation. For instance, by using homogeneous coordinates, the w-coordinate may be set to zero to extend a point to infinity. This should be accompanied by a viewing frustum that has a far <b>clipping</b> <b>plane</b> that extends to infinity in order to accommodate those points, accomplished by using a specialized projection matrix. This technique reduces the accuracy of the depth buffer slightly, but the difference is usually negligible. See 2002 paper [...] "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated Rendering", C. Everitt and M. Kilgard, for a detailed implementation.|$|E
5000|$|This {{approach}} has problems when the eye itself is inside a shadow volume (for example, {{when the light}} source moves behind an object). From this point of view, the eye sees the back face of this shadow volume before anything else, and this adds a −1 bias to the entire stencil buffer, effectively inverting the shadows. This can be remedied by adding a [...] "cap" [...] surface {{to the front of}} the shadow volume facing the eye, such as at the front <b>clipping</b> <b>plane.</b> There is another situation where the eye may be in the shadow of a volume cast by an object behind the camera, which also has to be capped somehow to prevent a similar problem. In most common implementations, because properly capping for depth-pass can be difficult to accomplish, the depth-fail method (see below) may be licensed for these special situations. Alternatively one can give the stencil buffer a +1 bias for every shadow volume the camera is inside, though doing the detection can be slow.|$|E
40|$|The {{concept of}} <b>clipping</b> <b>planes</b> {{is well known}} in {{computer}} graphics and can be used to create cut-away views. But clipping against just analytical defined planes is not always suitable for communicating every aspect of such visualization. For example, in hand-drawn technical illustrations, artists tend to communicate the difference between a cut and a model feature by using non-regular, sketchy cut lines instead of straight ones. To enable this functionality in computer graphics, this paper presents a technique for applying 2. 5 D clip-surfaces in real-time. Therefore, the <b>clip</b> <b>plane</b> equation is extended with an additional offset map, which can be represented by a texture map that contains height values. Clipping is then performed by varying the <b>clip</b> <b>plane</b> equation with respect to such an offset map. Further, a capping technique is proposed that enables the rendering of caps onto the clipped area to convey the impression of solid material. It avoids a re-meshing of a solid polygonal mesh after clipping is performed. Our approach is pixel precise, applicable in real-time, and takes fully advantage of graphics accelerators...|$|R
50|$|OpenGL ES 1.1 added {{features}} such as mandatory support for multitexture, better multitexture support (including combiners and dot product texture operations), automatic mipmap generation, vertex buffer objects, state queries, user <b>clip</b> <b>planes,</b> and greater control over point rendering.|$|R
40|$|ABSTRACT: This article {{presents}} a simple technique for splitting up a panoramic range image into {{a set of}} 2 [1 / 2]D representations. The proposed technique consists of three stages. First, a spherical dis-cretization map is generated. Second, main surface orientations are extracted together with their corresponding histogram of distances. Each one of these histograms is used to define {{the position of a}} pro-jection plane as well as two associated <b>clipping</b> <b>planes.</b> Finally, data points bounded by <b>clipping</b> <b>planes</b> are mapped onto the correspond-ing projection plane defining a classical 2 [1 / 2]D range image. This last stage—projection—is applied as many times as main orientations in the spherical discretization map. Experimental results with a pano...|$|R
40|$|Visualization via direct volume {{rendering}} is {{a potentially}} very powerful technique for exploring and interacting with {{large amounts of}} scientific data. However, the available two-dimensional (2 D) interfaces make three-dimensional (3 D) manipulation with such data very difficult. Many usability problems during interaction in turn discourage {{the widespread use of}} volume rendering as a scientific tool. In this paper, we present a more in-depth investigation into one specific interface aspect, i. e., the positioning of a <b>clipping</b> <b>plane</b> within volume-rendered data. More specifically, we propose three different interface prototypes that have been realized with the help of wireless vision-based tracking. These three prototypes combine aspects of 2 D graphical user interfaces with 3 D tangible interaction devices. They allow to experience and compare different user interface strategies for performing the <b>clipping</b> <b>plane</b> interaction task. They also provide a basis for carrying out user evaluations in the near future...|$|E
40|$|Abstract. Several 3 D {{rendering}} {{techniques have}} been developed in which part of the final image {{is the result of}} rendering from a virtual camera whose position in the scene differs from that of the primary camera. In these situations, there is usually a planar surface, such as the reflecting plane of a mirror, that can be considered the physical boundary of the recursively rendered image. In order to avoid artifacts that can arise when rendered geometry penetrates the boundary plane {{from the perspective of the}} virtual camera, an additional <b>clipping</b> <b>plane</b> must be added to the standard six-sided view frustum. However, many 3 D graphics processors cannot support an extra <b>clipping</b> <b>plane</b> natively, or require that vertex and fragment shaders be augmented to explicitly perform the additional clipping operation. This paper discusses a technique that modifies the projection matrix in such a way that the conventional near plane of the view frustum is repositioned to serve as the generally oblique boundary <b>clipping</b> <b>plane.</b> Doing so avoids the performance penalty and burden of developing multiple shaders associated with user-defined clipping planes by keeping the total number of clipping planes at six. The near plane is moved without affecting the four side planes, but the conventional far plane is inescapably destroyed. We analyze the effect on the far plane as well as the related impact on depth buffer precision and present a method for constructing the optimal oblique view frustum. 1...|$|E
40|$|Understanding big {{and complex}} {{scientific}} data {{is still an}} immature topic. It involves studying visualization methods to faithfully represent data, on the one hand, and designing interfaces that truly assist users with data analysis, on the other hand. In an earlier study, we developed guidelines for choosing display environment for four specific, but common, data analysis tasks: identification and judgment of the size, shape, density, and connectivity of objects in a volume. The results showed that using the fish tank virtual reality (VR) system was significantly more accurate at judging the shape, density, and connectivity of objects and significantly faster than the immersive Head-mounted display VR system. Based on those results, we asked the question {{whether or not the}} user performance could be further improved by adding tangible elements into the fish tank VR system. We propose several different interface prototypes of a <b>clipping</b> <b>plane</b> that have been realized with the help of wireless vision-based tracking. These prototypes allow to experience and evaluate those user interface strategies for performing the <b>clipping</b> <b>plane</b> function. An experimental study is carried out to quantitatively measure the added value of these tangible interfaces. The result shows that the inclusion of a tangible frame for controlling a virtual <b>clipping</b> <b>plane</b> and the correspondent 2 D intersection image into the basic fish tank VR system significantly improve the user performance for the shape, size and the connectivity task...|$|E
50|$|As the {{distance}} between near and far <b>clip</b> <b>planes</b> increases {{and in particular the}} near plane is selected near the eye, the greater the likelihood exists that z-fighting between primitives will occur. With large virtual environments inevitably there is an inherent conflict between the need to resolve visibility in {{the distance}} and in the foreground, so for example in a space flight simulator if you draw a distant galaxy to scale, you will not have the precision to resolve visibility on any cockpit geometry in the foreground (although even a numerical representation would present problems prior to z-buffered rendering). To mitigate these problems, z-buffer precision is weighted towards the near <b>clip</b> <b>plane,</b> {{but this is not the}} case with all visibility schemes and it is insufficient to eliminate all z-fighting issues.|$|R
50|$|In {{computer}} graphics, one of {{the most}} common matrices used for orthographic projection can be defined by a 6-tuple, (left, right, bottom, top, near, far), which defines the <b>clipping</b> <b>planes.</b> These planes form a box with the minimum corner at (left, bottom, -near) and the maximum corner at (right, top, -far).|$|R
5000|$|Approaching {{the distant}} plane to {{restrict}} scene such as far <b>clip</b> <b>plane,</b> thus increasing {{the accuracy of}} the depth buffer, or reducing the distance at which objects are visible in a scene. Increasing the number of bits allocated to the depth buffer, which is possible at the expense of memory for the stencil buffer. Staying away polygons, which could be inconsistent with the requirements elaborated scene.|$|R
40|$|Clipping is a fast, common {{technique}} for resolving occlusions. It only requires simple interaction, is easily understandable, and thus {{has been very}} popular for volume exploration. However, a drawback of clipping is that the technique indiscriminately cuts through features. Illustrators, for example, consider the structures {{in the vicinity of}} the cut when visualizing complex spatial data and make sure that smaller structures near the <b>clipping</b> <b>plane</b> are kept in the image and not cut into fragments. In this paper we present a new technique, which combines the simple clipping interaction with automated selective feature preservation using an elastic membrane. In order to prevent cutting objects near the <b>clipping</b> <b>plane,</b> the deformable membrane uses underlying data properties to adjust itself to salient structures. To achieve this behaviour, we translate data attributes into a potential field which acts on the membrane, thus moving the problem of deformation into the soft-body dynamics domain. This allows us to exploit existing GPU-based physics libraries which achieve interactive frame rates. For manual adjustment, the user can insert additional potential fields, as well as pinning the membrane to interesting areas. We demonstrate that our method can act as a flexible and non-invasive replacement of traditional clipping planes...|$|E
40|$|Volume {{rendering}} of 3 D anatomical and medical data would be valuable in medical diagnosis and surgical planning. In this paper, we investigated the visualization of the segmented heart {{obtained by the}} cross-sectional data from the Visible Human Project, and proposed an accelerated rendering method {{to speed up the}} original ray-casting rendering method. To provide a satisfactory visualization quality on the shape and boundary of cardiac tissues, we designed the transfer function and assigned an appreciate opacity and color value for each kind of tissue. In interactive visualization, we adopt the ray casting algorithm of volume rendering and modify it to accelerate the rendering speed. We also provide the fundamental rotation and zooming operations in the visualization platform, and further implement an interface to display three regular views of the volume data. Furthermore, a <b>clipping</b> <b>plane</b> tool is provided to crop the heart at arbitrary point of view. 1...|$|E
40|$|International audienceIn {{the context}} of {{scientific}} data analysis, we propose to compare a remote collaborative manipulation technique with a single user manipulation technique. The manipulation task consists in positioning a <b>clipping</b> <b>plane</b> in order to perform cross-sections of scientific data which show several points of interest located inside this data. For the remote collaborative manipulation, we have chosen to use the 3 -hand manipulation technique proposed by Aguerreche et al. which is very suitable with a remote manipulation of a plane. We ran two experiments to compare the two manipulation techniques with some participants located in two different countries. These experiments showed that the remote collaborative manipulation technique was significantly more efficient than the single user manipulation when the three points of interest were far from each other inside the scientific data and, consequently, when the manipulation task was harder and required more precision. When the three points of interest were close from each other, there was not {{significant difference between the}} two manipulation techniques...|$|E
50|$|Volumetric {{lighting}} requires two {{components: a}} light space shadow map, and a depth buffer. Starting at the near <b>clip</b> <b>plane</b> of the camera, {{the whole scene}} is traced and sampling values are accumulated into the input buffer. For each sample, it is determined if the sample is lit by the source of light being processed using the shadow map as a comparison. Only lit samples will affect final pixel color.|$|R
40|$|Abstract—We {{describe}} {{a series of}} experiments that compare 2 D displays, 3 D displays, and combined 2 D/ 3 D displays (orientation icon, ExoVis, and <b>clip</b> <b>planes)</b> for relative position estimation, orientation, and volume of interest tasks. Our results indicate that 3 D displays can be very effective for approximate navigation and relative positioning when appropriate cues, such as shadows, are present. However, 3 D displays are not effective for precise navigation and positioning except possibly in specific circumstances, for instance when good viewing angles or measurement tools are available. For precise tasks in other situations, orientation icon and ExoVis displays were better than strict 2 D or 3 D displays (displays consisting exclusively of 2 D or 3 D views). The combined displays had as good or better performance, inspired higher confidence, and allowed natural, integrated navigation. <b>Clip</b> <b>plane</b> displays were not effective for 3 D orientation because users could not easily view more than one 2 D slice {{at a time and}} had to frequently change the visibility of individual slices. Major factors contributing to display preference and usability were task characteristics, orientation cues, occlusion, and spatial proximity of views that were used together...|$|R
40|$|This work {{extends the}} metaphor of a see-through {{interface}} embodied in Magic LensesTM to 3 D environments. We present two new see-through visualization techniques: jlat lenses in 3 D and volumetric lenses. We discuss implementation concerns for platforms that have programmer accessible hardware <b>clipping</b> <b>planes</b> and show several examples of each visualization technique. We also examine composition of multiple lenses in 3 D environments, which strengthens the flat lens metaphor, but may have no meaningful semantics {{in the case of}} volumetric lenses...|$|R
40|$|Medical {{illustrations}} {{have been}} used for a long time for teaching and communicating information for diagnosis or surgery planning. Illustrative visualization systems create methods and tools that adapt traditional illustration techniques to enhance the result of renderings. Clipping the volume is a popular operation in volume rendering for inspecting the inner parts, though it may remove some information of the context that is worth preserving. In this paper we present a new editing technique based on the use of clipping planes, direct structure extrusion, and illustrative methods, which preserves the context by adapting the extruded region to the structures of interest of the volumetric model. We will show that users may interactively modify the <b>clipping</b> <b>plane</b> and edit the structures to highlight, in order to easily create the desired result. Our approach works with segmented volume models and non-segmented ones. In the last case, a local segmentation is performed on-the-fly. We will demonstrate the efficiency and utility of our method. Peer ReviewedPostprint (published version...|$|E
40|$|Figure 1 : Visualization of swirl {{and tumble}} flow using a {{combination}} of direct color-mapping, streamlines, isosurfaces, texture-based flow visualization and slicing. (Left) visualizing swirl flow using 3 D streamlines and texture-based flow visualization on an isosurface, (middle-left) a <b>clipping</b> <b>plane</b> is applied to reveal occluded flow structures, (middle-right) an isosurface and 3 D streamlines visualize tumble motion, and (right) the addition of texture-based flow visualization on a color-mapped slice. We investigate two important, common fluid flow patterns from computational fluid dynamics (CFD) simulations, namely, swirl and tumble motion typical of automotive engines. We study and visualize swirl and tumble flow using three different flow visualization techniques: direct, geometric, and texture-based. When illustrating these methods side-by-side, we describe the relative strengths and weaknesses of each approach within a specific spatial dimension and across multiple spatial dimensions typical of an engineer’s analysis. Our study is focused on steady-state flow. Based on this investigation we offer perspectives on where and when thes...|$|E
40|$|This paper {{describes}} a real-time shadow generation algorithm for polygonal environments illuminated by movable point light sources. The main {{goal is to}} quickly {{reduce the number of}} hidden shadow volumes, by using a technique of volumetric shadow rendering using stencil buffers with a modified BSP tree, i. e. a simplified version of a SVBSP (Shadow Volume BSP) tree. It also provides new easy-to-implement approaches to improvement techniques used in shadow volume algorithms, such as silhouette detection {{to reduce the number of}} redundant shadow polygons and the computation of capping polygons to handle cases where the shadow volumes are clipped by the eye-view near <b>clipping</b> <b>plane.</b> Such hybrid approach solves important limitations on the original shadow rendering algorithm, as well as achieves real-time frame rates when using modest size scenes (about 500 shadow polygons), according to measurements performed on personal computers using current graphics hardware. Per-phase timing results from the implementation are provided along the text and compared with those of the standard algorithm...|$|E
50|$|The use of <b>clipping</b> <b>planes</b> {{can result}} in a detraction from the realism of a scene, as the viewer may notice that {{everything}} at the threshold is not rendered correctly or seems to (dis)appear spontaneously. The addition of fog—a variably transparent region of color or texture just before the clipping plane—can help soften the transition between what should be in plain sight and opaque, and what should be beyond notice and fully transparent, and therefore {{does not need to be}} rendered.|$|R
40|$|We compare 2 D/ 3 D {{combination}} displays to displays with 2 D and 3 D views alone. Combination displays {{we consider}} are: orientation icon (i. e., separate windows), in-place methods (e. g., <b>clip</b> and cutting <b>planes),</b> {{and a new}} method called ExoVis. We specifically analyze performance differences (i. e. time and accuracy) for 3 D orientation and relative position tasks. Empirical results show that 3 D displays are effective for approximate navigation and relative positioning whereas 2 D/ 3 D combination displays (orientation icon and ExoVis) are useful for precise orientation and position tasks. Combination 2 D/ 3 D displays had as good or better performance as 2 D displays. <b>Clip</b> <b>planes</b> were not effective for a 3 D orientation task, but may be useful when only one slice is needed...|$|R
40|$|Twenty-five years ago, Crow {{published}} the shadow volume approach for determining shadowed regions in a scene. A decade ago, Heidmann described a hardware-accelerated stencil buffer-based shadow volume algorithm. Unfortunately hardware-accelerated stenciled shadow volume techniques {{have not been}} widely adopted by 3 D games and applications {{due in large part}} to the lack of robustness of described techniques. This situation persists despite widely available hardware support. Specifically what has been lacking is a technique that robustly handles various "hard" situations created by near or far <b>plane</b> <b>clipping</b> of shadow volumes. We describe a robust, artifact-free technique for hardware-accelerated rendering of stenciled shadow volumes. Assuming existing hardware, we resolve the issues otherwise caused by shadow volume near and far <b>plane</b> <b>clipping</b> through a combination of (1) placing the conventional far <b>clip</b> <b>plane</b> "at infinity", (2) rasterization with infinite shadow volume polygons via homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth clamping, a new rasterization feature provided by NVIDIA's GeForce 3, preserves existing depth precision by not requiring the far plane to be placed at infinity. We also propose two-sided stencil testing to improve the efficiency of rendering stenciled shadow volumes. Comment: 8 pages, 5 figure...|$|R
40|$|Finding good {{representational}} {{images for}} 3 D object exploration {{is a highly}} subjective problem in the cognitive field. The “best” or “good” definitions do not depend on any metric. We have explained the VKL distance concept and introduced a novel view descriptor called vSKL distance for finding “good” representational images. The image generation is done by projecting the surfaces of 3 D objects onto the screen or any planar surface. The projection process depends on parameters such as camera position, camera vector, up vector, and <b>clipping</b> <b>plane</b> positions. In this work we present a technique to find such camera positions that the 3 D object is projected in “good” or “best” way where those subjective definitions are mapped to Information Theoretical distances. We compared greedy view selection integrated vSKL with two well known techniques: VKL and VMI. vSKL performs {{very close to the}} other two, hence face coverage perturbation is minimal, but it is 3 to 4 times faster. Furthermore, the saliency information is conveyed to users with generated images...|$|E
40|$|Figure 1. A 128 -sided 2 D bound (orange) of the {{perspective}} projection of a sphere, with construction lines (white). We deliberately chose a wide {{field of view}} to illustrate that the projection is an ellipse and not a disk. We show how to efficiently compute 2 D polyhedral bounds of the (elliptic) perspective projection of a 3 D sphere that has been clipped to the near plane. For the special case of a 2 D axis-aligned bounding box, the algorithm is especially efficient. This has applications for bounding the screen-space effect of an emitter under deferred shading, bounding the kernel in density estimation during image space photon mapping, and bounding the pixel extent of objects for ray casting. Our solution is designed to elegantly handle the case where the sphere crosses the near <b>clipping</b> <b>plane</b> and efficiently handle all cases on vector processors. In addition to the algorithm, we provide implementations of two common applications: light-tile classification in C++ and expanding an attribute array of spheres (encoded as points and radii) into polygons that cover their silhouettes as a GLSL geometry shader...|$|E
40|$|In {{the context}} of {{scientific}} data analysis, we propose to compare a remote collaborative manipulation technique with a single user manipulation technique. The manipulation task consists in positioning a <b>clipping</b> <b>plane</b> in order to perform cross-sections of scientific data that show several points of interest located inside these data. For the remote collaborative manipulation, we have chosen to use the 3 -hand manipulation technique proposed by Aguerreche et al. [1], which is very suitable with a remote manipulation of a plane. We ran two experiments to compare the two manipulation techniques with some participants located in two different countries. These experiments {{has shown that the}} remote collaborative manipulation technique was significantly more efficient than the single user manipulation when the 3 points of interest were far apart inside the scientific data and, consequently, when the manipulation task was more difficult and required more precision. When the 3 points of interest were close together, there was not significant difference between the two manipulation techniques. Categories andSubjectDescriptors H. 5. 3 [Informations Interfaces and Presentation]...|$|E
40|$|We {{describe}} a CSG rendering algorithm that requires no {{evaluation of the}} CSG tree beyond normalization and pruning. It renders directly from the normalized CSG tree and primitives described (to the graphics system) by their facetted boundaries. It behaves correctly {{in the presence of}} user defined, "near" and "far" <b>clipping</b> <b>planes.</b> It has been implemented on standard graphics workstations using Iris GL and OpenGL graphics libraries. Modestly sized models can be evaluated and rendered at interactive (less than a second per frame) speeds. We have combined the algorithm with an existing B-rep based modeller to provide interactive rendering of incremental updates to large models...|$|R
40|$|We {{present a}} system for interactively {{producing}} exploded views of 3 D architectural environments such as multi-story buildings. These exploded views allow viewers to simultaneously see {{the internal and external}} structures of such environments. To create an exploded view we analyze the geometry of the environment to locate individual stories. We then use <b>clipping</b> <b>planes</b> and multipass rendering to separately render each story of the environment in exploded form. Our system operates at the graphics driver level and therefore can be applied to existing OpenGL applications, such as first-person multiplayer video games, without modification. The resulting visualization allows users to understand the global structure of architectural environments and to observe the actions of dynamic characters and objects interacting within such environments...|$|R
40|$|We {{propose a}} simple {{modification}} {{to the classical}} polygon rasterization pipeline that enables exact, efficient raycasting of bounded implicit surfaces {{without the use of}} a global spatial data structure or bounding hierarchy. Our algorithm requires two descriptions for each object: a (possibly non-convex) polyhedral bounding volume, and an implicit equation (including, optionally, a number of <b>clipping</b> <b>planes).</b> Unlike conventional raycasters, the modified pipeline is unidirectional and operates in immediate mode, making hardware implementation feasible. We discuss an extension to the OpenGL state machine that enables immediate-mode raycasting while making no modification to OpenGL's architecture for high-performance polygon rendering. A software simulation of our algorithm generates scenes of visual fidelity equal to those produced by a conventional raycaster, and superior to those produced by a polygon rasterizer, significantly faster than either existing method alone...|$|R
