8|107|Public
50|$|A mass used to {{calibrate}} a weighing scale {{is sometimes called}} a calibration mass or <b>calibration</b> <b>weight.</b>|$|E
40|$|The work is {{justified}} {{from the standpoint}} of system analysis using multimedia factors to improve rehabilitation patients at different stages of the disease. The first time the formation of the methodology vector priorities specialists in various fields of <b>calibration</b> <b>weight</b> judgments are used according to their professional expertise...|$|E
40|$|An {{experiment}} was designed (a) {{to examine the}} weight losses caused by Sitophilus feeding on maize cultures which contained different proportions (0 %, 5 % and 15 %) of broken grains and (b) to assess and compare the efficiency of different weight loss assessment methods on the different cultures. The weight loss assessment methods examined were the Count and Weigh, the Converted Percentage Damage, the Simple and Multiple Thousand Grain Mass (TGM) methods and the Standard Volume Weight (SVW) methods by direct comparison and by reference to a baseline <b>calibration.</b> <b>Weight</b> losses in cultures containing different proportions of broken grains were not significantly different. Further work is recommended using pests with secondary status...|$|E
3000|$|... dev, {{from the}} {{development}} database {{are used to}} train the <b>calibration</b> <b>weights,</b> i.e. the intercept and regression coefficient of the logistic regression model, and subsequently these <b>calibration</b> <b>weights</b> can then be used to calibrate scores from the test database. The pooled procedure for calculating the <b>calibration</b> <b>weights</b> was adopted (refer to [19] for details) in this paper. For a detailed tutorial on logistic regression calculation in converting a score to an interpretable likelihood ratio, refer to [12].|$|R
5000|$|Kott, P. (2006). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonresponse and {{coverage}} errors. Survey Methodology, 133-142 ...|$|R
40|$|<b>Calibration</b> <b>weighting</b> is a {{methodology}} under which probability-sample weights are adjusted {{in such a}} way that when applied to survey data they can produce model-unbiased estimators for a number of different target variables. This paper briefly reviews the history of <b>calibration</b> <b>weighting</b> before the term was coined and some major developments since then. A change in the definition of a calibration estimator is recommended. This change expands the class to include such special cases as, 1, randomization-optimal estimators, and, 2, randomization-consistent estimators incorporating local polynomial regression. Although originally developed as a method for reducing sampling errors, <b>calibration</b> <b>weighting</b> has also been applied to adjust for unit nonresponse and for coverage errors. A variant of the jackknife variance estimator proposed here should prove computationally convenient for these applications...|$|R
40|$|A {{new snow}} layer probing device with a {{rectangular}} section {{is described in}} the paper. This device allows receiving snow samples with the undisturbed structures of snow layers, and more accurately reflects the actual value of the snow water equivalent. Application of a new snow probing device in snow measurements provides {{a new way to}} organize the monitoring dynamics of snow density layer variability, to estimate density of certain snow types in the snow cover, including the total or vertically averaged one, and to visualize the features of the snow stratigraphy structure. Snow layer probing device with a rectangular section as opposed to the cylindrical weight snow devices has constructive advantages. It also can be used in the <b>calibration</b> <b>weight</b> snow measurement devices that will ensure the comparability of the snow shooting results made in different climatic zones. </p...|$|E
40|$|In the {{introductory}} chapter {{of this work}} is caught organizational structure of the national metrology system in the Czech Republic and its links to international organizations. There is indicated the basic terminology of metrology, particularly {{in the area of}} classification instruments. The following sections approaching the issue of measurement uncertainties, their classification, sources of uncertainty determined by the type A and B, their specifics and calculation. The above linked area already dealing with themselves calibrations, first of all calibration weights, classification of weights according accuracy classes, established procedures, and finally determining uncertainty in calibration weights. Then, immediately followed by a chapter dealing with calibration balances, performed tests and measurement uncertainties. The main part is of course directed towards the application of acquired knowledge to practical examples, thus performing the <b>calibration</b> <b>weight</b> class F 2 using a high-precision weights, both in the premises of the Technical University in Brno, both in the laboratory weighing the Czech Metrological Institute. Further calibration was performed school balances Ohaus Explorer EX 224...|$|E
40|$|Main abstract: Fluctuation scaling {{reports on}} all {{processes}} producing a data set. Some fluctuation scaling relationships, {{such as the}} Horwitz curve, follow exponential dispersion models which have useful properties. The mean-variance method applied to Poisson distributed data is a special case of these properties allowing the gain of a system to be measured. Here, a general method is described for investigating gain (G), dispersion (β), and process (α) in any system whose fluctuation scaling follows a simple exponential dispersion model, a segmented exponential dispersion model, or complex scaling following such a model locally. When gain and dispersion cannot be obtained directly, relative parameters, GR and βR, may be used. The method was demonstrated on data sets conforming to simple, segmented, and complex scaling. These included mass, fluorescence intensity, and absorbance measurements and specifications for classes of calibration weights. Changes in gain, dispersion, and process were observed in the scaling of these data sets in response to instrument parameters, photon fluxes, mathematical processing, and <b>calibration</b> <b>weight</b> class. The process parameter which limits the type of statistical process that can be invoked to explain a data set typically exhibited 0 4 possible. With two exceptions, <b>calibration</b> <b>weight</b> class definitions only affected β. Adjusting photomultiplier voltage while measuring fluorescence intensity changed all three parameters (0 <α< 0. 8; 0 <βR< 3; 0 <GR< 4. 1). The method provides a framework for calibrating and interpreting uncertainty in chemical measurement allowing robust compar ison of specific instruments, conditions, and methods. Supporting information abstract: On first inspection, fluctuation scaling data may appear to approximate a straight line when log transformed. The data presented in figure 5 of the main text gives a reasonable approximation to a straight line and for many purposes this would be sufficient. The {{purpose of the study}} of fluorescence intensity was to determine whether adjusting the voltage of a photomultiplier tube while measuring a fluorescent sample changes the process (α), the dispersion (β) and/or the gain (G). In this regard, the linear model established that PMT setting affects more than the gain. However, a detailed analysis beginning with testing for model mis-specification provides additional information. Specifically, Poisson behavior is only seen over a limited wavelength range in the 600 V and 700 V data sets...|$|E
5000|$|Kott, P., & Chang, T. (2010). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonignorable unit nonresponse. Journal of the American Statistical Association, 105, 1265-1275.|$|R
40|$|Background and Purpose. The {{purpose of}} this study was to {{evaluate}} the reli-ability cf measurements of weight distribution among the wheels of wheelchairs using a commercial balance testing system. Reliable data may be useful in the wheelchair evaluation and adjustment process. Subjects. Three male full-time manual wheelchair users aged 30, 26, and 2 7 years with cervical spinal cord injuries 7. 5, 65, and 10 years in duration participated. Metbods. <b>Calibration</b> <b>weights,</b> unoccupied wheelchairs, and occupied wheelchairs were repeatedly placed o n the force transducers of the balance testing system to obtain measure-ments o f weight distribution. Results The intraclass correlation coeficients of the measurements were. 99 for <b>calibration</b> <b>weights,.</b> 96 for unoccupied wheelchairs, and. 98 for wheelchairs occupied by the subjects. Conclusion and Discussion. The described use of this instrumentation appears to generate reliable measure-ments o f static weight distribution. With further testing, this system may provide useful information related to manual wheelchair prescription and adjustment...|$|R
40|$|Fractional {{hot deck}} imputation, {{considered}} in Fuller and Kim (2005), is extended to multivariate missing data. The joint {{distribution of the}} study items is nonparametrically estimated using a discrete approximation, where the discrete transformation also serves to define imputation cells. The procedure first estimates the probabilities for the cells and then imputes real observations for missing items. <b>Calibration</b> <b>weighting</b> is used to reduce the imputation variance. Replication variance estimation is discussed...|$|R
40|$|DE 29917940 U UPAB: 20000301 NOVELTY - For each channel {{there is}} a holder (9) {{accepting}} a test volume or container (2) holding it. For each holder {{there is a}} load cell (1) to weigh the container and/or the sample volume. DETAILED DESCRIPTION - Preferred Features: Each holder rests directly on its load cell, or is connected via a rod or lever system to it. Adjacent rods or levers seen from the holders, include an angle greater than zero, and/or are arranged in different planes. Spacing between adjacent load cells is the same as, or greater than, that of adjacent channels. Weighing cells include a strain gauge or operate {{on the principle of}} electromagnetic force balance. A container holder (8) moves between sample-holding and weighing positions. The holder (8) {{is at the end of}} an arm with a <b>calibration</b> <b>weight</b> receptacle, used when the holder is in the sample holding position. The holder has two jaws gripping or releasing containers. Load cells (1) and holder (8) move relatively to one another. Movements are provided pneumatically, by stepper motors and/or servomotors. Weighing cells are surrounded by a separate casing having an opening into which a rod is guided, connecting weighing cells with allocated holders (9). A sensor measures sample volume indirectly. A compressed air sensor and/or a sample fluid temperature sensor and/or an air temperature sensor are included. A computer controls operations and calculations. The containers taper towards their openings. USE - To test the accuracy of multichannel pipettes. ADVANTAGE - The accuracy of multichannel pipettes can be tested in accordance with quality assurance requirements and national standards of accuracy. The new machine mainly tests automatically, shortening the time required. Test results are automatically calculated and presented...|$|E
40|$|A {{platform}} comprising four individual sections {{has been}} designed and built to determine a dairy cow’s weight, hooves’ position, the duration each hoof is on the section, and the stride length. The developed hardware and software is geared towards building a complete system to detect lameness in cattle, the ultimate aim of the project. Each section is an independent unit and consists of four ASB 1000 shearbeam load cells, an AD 7193 which is a 24 -bit sigma-delta analogue-to-digital converter (ADC), and an ATmega 328 microcontroller. The AD 7193 ADC communicates with the microcontroller via the serial peripheral interface (SPI). Because each section contains its own microcontroller, an Arduino Mega 2560 {{has been used as}} the master microcontroller. This handles communication between the computer and all the sections. The master and sections communicate on a RS- 485 half-duplex bus. The load cell values are transmitted from the master microcontroller to the computer via serial communication. The individual load cell value is then recorded and further processed where the data can be plotted, and the cow’s average weight, stride length, hooves’ position and duration can be calculated. The user also has the ability to render the data to a video file and to split cow data. Laboratory testing was conducted to find the accuracy of the sections using a laser cut jig and a 20 kg point load <b>calibration</b> <b>weight.</b> It was found that the X-position mean error is 1. 0 ± 2. 2 mm, the Y-position mean error is 0. 8 ± 1. 8 mm, and the total weight on the section has a maximum error of 0. 4 %. The mainframe to which the sections bolt to is 3000 mm long and 540 mm wide, while the individual sections measure 650 mm long by 500 mm wide. When the platform is assembled, the platform is 100 mm high and has a walking surface width of 400 mm. The platform sections are adjustable between the ranges of 700 ± 50 mm to find the optimal stride length. The platform has been galvanized for protection against the elements. Experimental field testing was conducted at Massey Dairy Farm Number 1 where the signal signatures of 60 cows were recorded for further analysis. The recorded data was used as the basis for all the software tools that were developed; more field testing would be required to make the software more robust to different cow behaviours to see whether cow’s weight, hoof position, duration of each hoof and stride length can be successfully and accurately calculated...|$|E
40|$|We {{extend the}} problem of obtaining an {{estimator}} for the finite population mean parameter incorporating complete auxiliary information through calibration estimation in survey sampling but considering a functional data framework. The functional <b>calibration</b> sampling <b>weights</b> of the estimator are obtained by matching the calibration estimation problem with the maximum entropy on the mean principle. In particular, the calibration estimation is viewed as an infinite dimensional linear inverse problem following {{the structure of the}} maximum entropy on the mean approach. We give a precise theoretical setting and estimate the functional <b>calibration</b> <b>weights</b> assuming, as prior measures, the centered Gaussian and compound Poisson random measures. Additionally, through a simple simulation study, we show that our functional calibration estimator improves its accuracy compared with the Horvitz-Thompson estimator...|$|R
40|$|Sometimes {{benchmark}} constraints in a calibration problem {{cannot be}} met {{if there are}} range restric-tions on the <b>calibration</b> <b>weights.</b> There are various approaches to this problem that involve either allow-ing the benchmark constraints to be adjusted within a specified tolerance, or to determine a minimal lin-ear adjustment of the benchmark constraints. In this paper we propose an optimization problem that explicitly incorporates into the objective function {{a measure of the}} amount by which the benchmark con-straints are missed...|$|R
40|$|Given a {{randomly}} drawn sample, <b>calibration</b> <b>weighting</b> {{can provide}} double {{protection against the}} selection bias resulting from unit nonresponse. This means that if either an assumed linear prediction model or an implied unit selection model holds, the resulting estimator will be asymptotically unbiased in some sense. The functional form of the selection model when using linear alibration adjustment is dubious. The authors discuss an alternative, nonlinear calibration-weighting procedure and software that can, among other things, implicitly estimate a logistic-response model. " (author's abstract...|$|R
40|$|Weighting {{procedures}} are commonly applied in surveys {{to compensate for}} nonsampling errors such as nonresponse errors and coverage errors. Two types of weight-adjustment {{procedures are}} commonly used {{in the context of}} unit nonresponse: (i) nonresponse propensity <b>weighting</b> followed by <b>calibration,</b> also known as the two-step approach and (ii) nonresponse <b>calibration</b> <b>weighting,</b> also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented...|$|R
40|$|Parametric {{fractional}} imputation {{is proposed}} {{as a general}} tool for missing data analysis. Using fractional weights, the observed likelihood can be approximated by the weighted mean of the imputed data likelihood. Computational efficiency can be achieved using the idea of importance sampling and <b>calibration</b> <b>weighting.</b> The proposed imputation method provides efficient parameter estimates for the model parameters specified in the imputation model and also provides reasonable estimates for parameters that {{are not part of}} the imputation model. Variance estimation is discussed and results from a limited simulation study are presented. Copyright 2011, Oxford University Press. ...|$|R
50|$|Phillip S. Kott (born 1952) is an American statistician. He {{has worked}} in the field of survey {{statistics}} for more than 25 years, and is regarded as a leader in this field. His areas of expertise include survey sampling design, analysis of survey data, and <b>calibration</b> <b>weighting,</b> among other areas. He revolutionized sampling design and estimation strategies with the Agricultural Resource Management Survey, which uses survey information more efficiently. He has taught at George Mason University, and USDA Graduate School. He is currently an Associate Editor for the Journal of Official Statistics and the scientific journal Survey Methodology.|$|R
40|$|Although {{survey data}} are {{sometimes}} weighted by their selection weights, {{it is often}} preferable to use auxiliary information available on the whole population to improve estimation. <b>Calibration</b> <b>weighting</b> (Deville and Sarndal, 1992, Journal of the American Statistical Association 87 : 376 - 382) {{is one of the}} most common methods of doing this. This method adjusts the selection weights so that known population totals for the auxiliary variables are reproduced exactly, while ensuring that the calibrated weights are as close as possible to the original sampling weight. The simplest example of calibration is poststratification. This is the special case where the auxiliary variable is a single categorical variable. General calibration extends this to deal with more than one auxiliary variable and allows the user to include both categorical and numerical variables. A typical example might occur in a population survey, where the selection weights could be calibrated to ensure that the sample weighted by the <b>calibration</b> <b>weights</b> has exactly the same distribution as the population on variables such as age, sex, and region. Many packages have routines for calibration. SAS has the macro CALMAR; GenStat has the procedure SVCALIBRATE; and R has the function calibrate. However, no such routine is publicly available in Stata. I will introduce a user-written Stata program for calibration and will also discuss a simple extension to show how it can incorporate a nonresponse correction. I will also briefly discuss the program's strengths and limitations when compared to rival packages. ...|$|R
50|$|As Agriculture Commissioner, Perry was {{responsible}} for promoting the sale of Texas farm produce to other states and foreign nations, and for supervising the <b>calibration</b> of <b>weights</b> and measures, such as gasoline pumps and grocery store scales.|$|R
40|$|Calibration estimation, {{which can}} be roughly {{described}} as adjusting the original design weights to incorporate the known population totals of the auxiliary variables, has become very popular in sample surveys. The <b>calibration</b> <b>weights</b> are chosen to minimize a given distance measure while satisfying a set of constraints related to the auxiliary variable information. Under simple random sampling, Chen and Qin (1993) suggested that the calibration estimator maximizing the constrained empirical likelihood can make efficient use of the auxiliary variables. We extend the result to unequal probability sampling and propose an algorithm to implement the proposed procedure. Asymptotic properties of the proposed calibration estimator are discussed. The proposed method is extended to the stratified sampling. Results from a limited simulation study are presented...|$|R
40|$|In survey statistics, {{the usual}} {{technique}} for estimating a population total consists in summing appropriately weighted variable {{values for the}} units in the sample. Different weighting systems exit: sampling weights, GREG <b>weights</b> or <b>calibration</b> <b>weights</b> for example. In this article, we propose to use the inverse of conditional inclusion probabilities as weighting system. We study examples where an auxiliary information enables to perform an a posteriori stratification of the population. We show that, in these cases, exact computations of the conditional weights are possible. When the auxiliary information consists in the knowledge of a quantitative variable for all the units of the population, then we show that the conditional weights can be estimated via Monte-Carlo simulations. This method is applied to outlier and strata-Jumper adjustments...|$|R
40|$|The use of nonresponse <b>calibration</b> <b>weighting</b> is {{considered}} {{in a complete}} design-based frameworkto account for the cases in which nonresponse is a fixed characteristic of the units, just like the interest variable. Approximate expressions of design-based bias and variance of the calibration estimator are derived and some estimators of the sampling variance are proposed. The choice of auxiliary variables is discussed from theoretical and practical point of view. The results of an extensive simulation study demonstrate how {{the reliability of the}} procedure is mainly determined by the capability of selecting auxiliary variables {{in such a way that}} their relationship with the interest variable is similar for both the respondent and nonrespondent sub-populations. auxiliary variables, calibration estimator, variance estimator, simulation study. ...|$|R
30|$|In {{order to}} elicit a stretch reflex {{the force of}} the pull and {{subsequent}} strech velocity of the tibial translation must be sufficient to activate the proprioceptors. The velocity of the pulls was generated as fast as possible by the tester (JA). For the force magnitude, Friemert et al. (2005 b) demonstrated that a force of[*]≥[*] 140  N and stretch velocity of 30  mm/s would increase the chance to elicit stretch reflex to 100  %. In a pilot experiment, the force of pull on the handle of the KT- 2000 measured using a strain gauge. The force output of the KT- 2000 was determined statically with <b>calibration</b> <b>weights</b> within the range of forces expected to be generated by the user. Similarly, the position output of the KT- 2000 was calibrated using ceramic test gauge blocks.|$|R
40|$|Abstract: Work {{with sample}} surveys often makes {{extensive}} use of measures of size. Two prominent examples are the use of “probability proportional to size ” sampling; and use of size measures in adjustment of survey weights through, e. g., ratio estimation, post-stratification or <b>calibration</b> <b>weighting.</b> However, many survey applications use size variables that are imperfect approximations to the idealized size measures that would produce optimal efficiency results. This paper explores the effects that alternative size measures may have on the efficiency of some standard design-estimator pairs. Principal {{emphasis is placed on}} numerical results of a simulation study that uses size measures and economic variables available through the Quarterly Census of Employment and Wages of the Bureau of Labor Statistics. Key words: measures of size; ratio estimation; regression estimation; sampling with probabilities proportional to size; unequal-probability sampling. 1...|$|R
40|$|In the {{presence}} of a missing response, reweighting the complete case subsample by the inverse of nonmissing probability is both intuitive and easy to implement. When the population totals of some auxiliary variables are known and when the inclusion probabilities are known by design, survey statisticians have developed calibration methods for improving efficiencies of the inverse probability weighting estimators and the methods can be applied to missing data analysis. Model-based calibration has been proposed in the survey sampling literature, where multidimensional auxiliary variables are first summarized into a predictor function from a working regression model. Usually, one working model is being proposed for each parameter of interest and results in different sets of <b>calibration</b> <b>weights</b> for estimating different parameters. This paper considers calibration using multiple working regression models for estimating a single or multiple parameters. Contrary to a common belief that overfitting hurts efficiency, we present three rather unexpected results. First, when the missing probability is correctly specified and multiple working regression models for the conditional mean are posited, calibration enjoys an oracle property: the same semiparametric efficiency bound is attained as if the true outcome model is known in advance. Second, when the missing data mechanism is misspecified, calibration can still be a consistent estimator when any one of the outcome regression models is correctly specified. Third, a common set of <b>calibration</b> <b>weights</b> can be used to improve efficiency in estimating multiple parameters of interest and can simultaneously attain semiparametric efficiency bounds for all parameters of interest. We provide connections of a wide class of calibration estimators, constructed based on generalized empirical likelihood, to many existing estimators in biostatistics, econometrics and survey sampling and perform simulation studies to show that the finite sample properties of calibration estimators conform well with the theoretical results being studied. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Due to {{unbalanced}} speed-density observations, the one-regime traffic {{fundamental diagram}} and speed-density relationship models using {{least square method}} (LSM) cannot reflect actual conditions under congested/jam traffic. In that case, it is inevitable to adopt the weighted least square method (WLSM). This paper used freeway Georgia State Route 400 observation data and proposed 5 weight determination methods except the LSM to analyse 5 wellknown one-regime speed-density models {{to determine the best}} calibrating models. The results indicated that different one-regime speed-density models have different best calibrating models, for Greenberg, it was possible to find a specific weight using LSM, which is similar for Underwood and Northwestern Models, but different for that one known as 3 PL model. An interesting case is the Newell 2 ̆ 7 s Model which fits well with two distinct <b>calibration</b> <b>weights.</b> This paper can make contribution to calibrating a more precise traffic fundamental diagram...|$|R
40|$|Unit nonresponse {{is often}} a problem in sample surveys. It arises when {{the values of the}} survey {{variable}} cannot be recorded for some sampled units. In this paper, the use of nonresponse <b>calibration</b> <b>weighting</b> to treat nonresponse is considered in a complete design-based framework. Nonresponse is viewed as a fixed characteristic of the units. The approach is suitable in environmental and forest surveys when sampled sites cannot be reached by field crews. Approximate expressions of design-based bias and variance of the calibration estimator are derived and design-based consistency is investigated. Choice of auxiliary variables to perform calibration is discussed. Sen–Yates–Grundy, Horvitz-Thompson, and jackknife estimators of the sampling variance are proposed. Analytical and Monte Carlo results demonstrate the validity of the procedure when the relationship between survey and auxiliary variables is similar in respondent and nonrespondent strata. An application to a forest survey performed in Northeastern Italy is considered...|$|R
40|$|The {{first data}} from the ATLAS {{detector}} at the Large Hadron Collider (LHC) is due to be collected later this year. This first phase will {{play a vital role}} in understanding the detector and its response, in-situ. Jet reconstruction is important for identifying new physics as well as making precision measurements of standard model physics. The fine granularity of the ATLAS calorimeters can be used to gain information about a jet's shape and the classification of energy deposits, which allows a better estimate of the jet energy to be made and in particular correction for the non-compensating nature of the calorimeter using so-called <b>calibration</b> <b>weights.</b> The classification algorithm and weights are presently calculated using simulation. In this presentation we describe an important step in the validation of ATLAS's jet calibration using charged tracks reconstructed in the inner detector and their inter-calibration with the clusters reconstructed in the calorimeters...|$|R
40|$|Maximum entropy {{and minimum}} {{cross-entropy}} estimation are applica- ble {{when faced with}} ill-posed estimation problems. I introduce a Stata command that estimates a probability distribution using a maximum entropy or minimum cross-entropy criterion. I show how this command {{can be used to}} calibrate survey data to various population totals. Copyright 2010 by StataCorp LP. maxentropy, maximum entropy, minimum cross-entropy, survey <b>calibration,</b> sample <b>weights...</b>|$|R
40|$|Quantitative expert {{judgement}} {{is used in}} many areas of risk analysis to provide assessments of uncertainty. A leading method is Cooke’s classical model, which provides a way of weighting experts depending on their performance in answering so-called calibration questions. The moment method for {{expert judgement}} combination is an alternative to Cooke’s method which provides a different way of calibrating experts. It has better theoretical properties in that it uses a strictly proper scoring rule, that is, an expert who wishes to maximise his score can only do so by stating what he actually believes. The method also requires the specification of <b>weights</b> for the <b>calibration</b> process. In this paper we consider a special case of the moment model that scores location and variability assessments. We provide an approach to setting the <b>calibration</b> <b>weights</b> and repeat the comparison of this moment model with Cooke’s classical model we conducted earlier for this special case...|$|R
40|$|This article {{describes}} a two-step calibration-weighting scheme for a stratified simple {{random sample of}} hospital emergency departments. The first step adjusts for unit nonresponse. The second increases the statistical efficiency of most estimators of interest. Both use a measure of emergency-department size and other useful auxiliary variables contained in the sampling frame. Although many survey variables are roughly a linear function of the measure of size, response is better modeled {{as a function of}} the log of that measure. Consequently the log of size is a calibration variable in the nonresponse-adjustment step, while the measure of size itself is a calibration variable in the second calibration step. Nonlinear calibration procedures are employed in both steps. We show with 2010 DAWN data that estimating variances as if a one-step <b>calibration</b> <b>weighting</b> routine had been used when there were in fact two steps can, after appropriately adjusting the finite-population correct in some sense, produce standard-error estimates that tend to be slightly conservative...|$|R
30|$|Using the tumor-to-liver-ratio (TLR) {{obviously}} removes {{some of the}} SUV limitations, i.e. possible inaccuracies regarding actually injected dose, scanner <b>calibration,</b> {{and patient}} <b>weight</b> index (either actual body weight, lean body mass [14], or body surface area [15]).|$|R
40|$|<b>Calibration</b> <b>weighting</b> {{has been}} usefully {{employed}} {{to adjust for}} unit nonresponse. Generalized cal-ibration allows to distinguish among auxiliary variables between those that are useful to model unit nonresponse (instrumental or model variables) {{and those that are}} used in the calibration constraints (calibration variables). Since model variables need only be known on the respondents, generalized calibration offers a particularly useful tool to deal with nonignorable nonresponse. Response to a survey is the outcome of a complex process that involves several aspects: we assume that a part (or all) of such a process may be measured by unobservable variables. Latent variable models can be employed to extract either continuous constructs (latent trait models) or categorical ones (latent class models) from a set of dichotomous/ordered manifest variables. We propose to use such constructs as instrumental variables in the generalized calibration procedure. This allows to include variables of interest among the set of manifest variables. The properties of the proposed methodology are illustrated, then it is tested on a series of simulation studies and finally applied to adjust estimates from the Italian Survey of Households Income and Wealth...|$|R
