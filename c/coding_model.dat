232|7355|Public
50|$|In the United States, {{statutory}} law cannot be copyrighted and is freely accessible and copyable by anyone. When a standards organization develops a new <b>coding</b> <b>model</b> {{and it is}} not yet accepted by any jurisdiction as law, it is still the private property of the standards organization and the reader may be restricted from downloading or printing the text for offline viewing. For that privilege, the <b>coding</b> <b>model</b> must still be purchased as either printed media or a CD-ROM. Once the <b>coding</b> <b>model</b> has been accepted as law, it loses copyright protection and may be freely obtained at no cost.|$|E
5000|$|The base S5 encoder {{compresses}} multichannel audio {{information by}} downmixing the f-channel signal to g channels and produces sparse spatial data {{according to an}} inverse <b>coding</b> <b>model,</b> which approaches the localization and ambiance of the original signal. The inverse <b>coding</b> <b>model</b> directly constructs an upmix of h channels from the audio downmix and its associated spatial data.|$|E
5000|$|Simple <b>coding</b> <b>model</b> {{simplifies}} {{the task of}} partitioning a complex system and aids in testing ...|$|E
5000|$|... 1 September 2006ISO/IEC 18004:2006 Information technologyAutomatic (now withdrawn) Defines QR code 2005 symbols, an {{extension}} of QR <b>code</b> <b>model</b> 2. Does not specify how to read QR <b>code</b> <b>model</b> 1 symbols, or require this for compliance.|$|R
30|$|In this section, we {{introduce}} {{and discuss}} the <b>CoDe</b> <b>model</b> for projecting mortality using the concepts of delay and compression. We then briefly describe the Lee-Carter (LC) model, and outline the qualitative and quantitative methods we use to compare the <b>CoDe</b> <b>model</b> with the Lee-Carter (LC) model.|$|R
40|$|Examples {{are given}} from {{comparisons}} of analyses based on (1) <b>code</b> <b>models,</b> (2) {{finite element models}} and (3) full scale tests to failure of three bridges. The analyses based on the <b>code</b> <b>models</b> gave very conservative results, while the finite element models could better predict the real behaviour. MAINLIN...|$|R
5000|$|The independent-spike <b>coding</b> <b>model</b> of {{neuronal}} firing {{claims that}} each individual action potential, or [...] "spike", {{is independent of}} each other spike within the spike train.|$|E
5000|$|The rate <b>coding</b> <b>model</b> of {{neuronal}} firing communication {{states that}} as the intensity of a stimulus increases, the frequency or rate of action potentials, or [...] "spike firing", increases. Rate coding is sometimes called frequency coding.|$|E
50|$|The NEC is also {{available}} as a restricted, digitized <b>coding</b> <b>model</b> that can be read online but not saved, copied and pasted, or printed, free of charge on certain computing platforms that support the restricted viewer software.|$|E
40|$|Several recent {{results in}} machine {{learning}} have established formal connections between autoencoders [...] -artificial neural network models {{that attempt to}} reproduce their inputs [...] -and other <b>coding</b> <b>models</b> like sparse <b>coding</b> and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear <b>coding</b> <b>models.</b> We provide an intuitive interpretation {{of the behavior of}} these <b>coding</b> <b>models</b> and demonstrate this intuition using small, artificial datasets with known distributions...|$|R
40|$|We re-introduce the <b>coded</b> <b>model</b> of {{fault-tolerant}} computation {{in which}} the input and output of a computational device are treated as words in an errorcorrecting code. A computational device correctly computes a function in the <b>coded</b> <b>model</b> if its input and output, once decoded, are a valid input and output of the function. In the <b>coded</b> <b>model,</b> {{it is reasonable to}} hope to simulate all computational devices by devices whose size is greater by a constant factor but which are exponentially reliable even if each of their components can fail with some constant probability. We consider fine-grained parallel computations in which each processor has a constant probability of producing the wrong output at each time step. We show that any parallel computation that runs for time t on w processors can be performed reliably on a faulty machine in the <b>coded</b> <b>model</b> using w log O(1) w processors [...] ...|$|R
5000|$|AWS CloudFormation {{provides}} a declarative template-based Infrastructure as <b>Code</b> <b>model</b> for configuring AWS.|$|R
50|$|In 2015, Lisa Barrett and W. Kyle Simmons (2015) {{proposed}} the Embodied Predictive Interoception <b>Coding</b> <b>model,</b> {{a framework that}} unifies Bayesian active inference principles with a physiological framework of corticocortical connections. Using this model, they posited that agranular visceromotor cortices are responsible for generating predictions about interoception, thus, defining the experience of interoception.|$|E
50|$|Notable codecs {{produced}} by Nellymoser include the Asao Codec used in Adobe Flash, speech codec used in Microsoft's Xbox Live. Speech codecs originated from MIT Lincoln Laboratory and {{were based on}} McAulay and Quatieri's sinusoidal transform <b>coding</b> <b>model.</b> The Asao product family is based on Nellymoser's proprietary Scalable Projective Transform Coding technology (SPTC).|$|E
5000|$|The {{original}} {{version of the}} problem is defined for only a single point [...] and its noisy observation. Often, a single point can have more than one sparse representation with similar data fitting errors. In the collaborative sparse <b>coding</b> <b>model,</b> more than one observation of the same point is available. Hence, the data fitting error is defined as the sum of the [...] norm for all points.|$|E
40|$|Although the {{topological order}} {{is known as}} a quantum order in quantum many-body systems, it seems {{that there is not a}} {{one-to-one}} correspondence between topological phases and quantum phases. As a well-known example, it has been shown that all one-dimensional (1 D) quantum phases are topologically trivialspt. By such a fact, it seems a challenging task to understand when a quantum phase transition between different topological models necessarily reveals different topological classes of them. In this paper, we make an attempt to consider this problem by studying a phase transition between two different quantum phases which belong to a universal topological phase. We define a Hamiltonian which describes an interpolation between the toric <b>code</b> <b>model</b> with Z_ 2 topological order and the color <b>code</b> <b>model</b> with Z_ 2 × Z_ 2 topological order on a hexagonal lattice. We show such a model is exactly mapped to many copies of 1 D quantum Ising model in transverse field by rewriting the Hamiltonian in a new complete basis. Consequently, we show that the universal topological phase of the color <b>code</b> <b>model</b> and the toric <b>code</b> <b>model</b> reflects in the 1 D nature of the phase transition. We also consider the expectation value of Wilson loops by a perturbative calculation and show that behavior of the Wilson loop captures the non-topological nature of the quantum phase transition. The result on the point of phase transition also show that the color <b>code</b> <b>model</b> is strongly robust against the toric <b>code</b> <b>model.</b> Comment: 15 pages, 5 figures, minor revisions, references added, accepted for publication in Physical Review...|$|R
50|$|June 2000ISO/IEC 18004:2000 Information technologyAutomatic (now withdrawn) Defines QR <b>code</b> <b>models</b> 1 and 2 symbols.|$|R
2500|$|Legal: October 4, 1926: Introduction {{of the new}} civil <b>code</b> <b>modeled</b> {{after the}} Swiss civil code ...|$|R
50|$|In SIT's formal <b>coding</b> <b>model,</b> {{candidate}} {{interpretations of}} a stimulus {{are represented by}} symbol strings, in which identical symbols refer to identical perceptual primitives (e.g., blobs or edges). Every substring of such a string represents a spatially contiguous part of an interpretation, so that the entire string {{can be read as}} a reconstruction recipe for the interpretation and, thereby, for the stimulus. These strings then are encoded (i.e., they are searched for visual regularities) to find the interpretation with the simplest code.|$|E
50|$|Although visual stimuli are {{fundamentally}} multi-interpretable, the human visual system usually {{has a clear}} preference for only one interpretation. To explain this preference, SIT introduced a formal <b>coding</b> <b>model</b> starting from {{the assumption that the}} perceptually preferred interpretation of a stimulus is the one with the simplest code. A simplest code is a code with minimum information load, that is, a code that enables a reconstruction of the stimulus using a minimum number of descriptive parameters. Such a code is obtained by capturing a maximum amount of visual regularity and yields a hierarchical organization of the stimulus in terms of wholes and parts.|$|E
50|$|The two popular models {{claiming}} {{to explain the}} WSE are the interactive activation model (IAM) and the dual-route <b>coding</b> <b>model</b> (DRC) Neither of these models takes attention into account; This is a relationship looked into through research on the WSE. Evidence shows that the WSE persists without an observer’s conscious awareness of the word presented, which implies that attention is neither necessary for WSE nor involved in this phenomenon. However, attentional focus has been demonstrated to modulate the WSE which agrees with recent neurophysiological data explaining that attention, in fact, modulates early stages of word processing.|$|E
5000|$|Program visualization: To allow {{tools to}} reverse-engineer legible Codecharts from plain source <b>code</b> <b>modeling</b> their design ...|$|R
40|$|<b>Code</b> <b>model</b> {{checking}} is {{a rapidly}} advancing research topic. However, apart from very constrained scenarios (e. g., verification of device drivers by Slam), the <b>code</b> <b>model</b> checking tools are not {{widely used in}} general software development process. We believe {{that this could be}} changed if the developers could use the tools in the same way they already use testing tools. In this paper, we present the UnitCheck tool, which enhances standard unit testing of Java <b>code</b> with <b>model</b> checking. A developer familiar with unit testing can apply the tool on standard unit test scenarios and benefit from the exhaustive traversal performed by a <b>code</b> <b>model</b> checker, which is employed inside UnitCheck. The UnitCheck plugin for Eclipse presents the checking results in a convenient way known from unit testing, while providing also a verbose output for the expert users...|$|R
30|$|To project {{mortality}} using {{delay and}} compression, {{we will use}} the compression and delay (<b>CoDe)</b> <b>model.</b> The <b>CoDe</b> <b>model</b> is a non-linear parametric mortality model for modeling mortality and forecasting the full age range that distinguishes between mortality delay and mortality compression (de Beer and Janssen 2016). We refer to our <b>model</b> as <b>CoDe</b> 2.1, {{as it is a}} slight adaptation of the CoDe 2.0 version (de Beer et al. 2017)—which was in turn a slight adaptation of the original <b>CoDe</b> <b>model</b> (de Beer and Janssen 2016). The main difference between CoDe 2.0 and CoDe 2.1 is that we followed the principle of parsimony and limited the number of time-varying parameters as much as possible, as this may be expected to result in more robust projections.|$|R
5000|$|The {{correlation}} <b>coding</b> <b>model</b> of neuronal firing {{claims that}} correlations between action potentials, or [...] "spikes", within a spike train may carry additional information {{above and beyond}} the simple timing of the spikes. Early work suggested that correlation between spike trains can only reduce, and never increase, the total mutual information present in the two spike trains about a stimulus feature. However, this was later demonstrated to be incorrect. Correlation structure can increase information content if noise and signal correlations are of opposite sign. Correlations can also carry information not present in the average firing rate of two pairs of neurons. A good example of this exists in the pentobarbital-anesthetized marmoset auditory cortex, in which a pure tone causes {{an increase in the number}} of correlated spikes, but not an increase in the mean firing rate, of pairs of neurons.|$|E
50|$|Lossless JPEG is {{actually}} a mode of operation of JPEG. This mode exists because the discrete cosine transform (DCT) based form cannot guarantee that encoder input would exactly match decoder output. Unlike the lossy mode {{which is based on}} the DCT, the lossless coding process employs a simple predictive <b>coding</b> <b>model</b> called differential pulse code modulation (DPCM). This is a model in which predictions of the sample values are estimated from the neighboring samples that are already coded in the image. Most predictors take the average of the samples immediately above and {{to the left of the}} target sample. DPCM encodes the differences between the predicted samples instead of encoding each sample independently. The differences from one sample to the next are usually close to zero. A typical DPCM encoder is displayed in Fig.1. The block in the figure acts as a storage of the current sample which will later be a previous sample.|$|E
40|$|In this paper, {{we propose}} a bilevel sparse <b>coding</b> <b>model</b> for coupled feature spaces, where {{we aim to}} learn {{dictionaries}} for sparse modeling in both spaces while enforcing some desired relationships between the two signal spaces. We first present our new general sparse <b>coding</b> <b>model</b> that relates signals from the two spaces by their sparse representations and the corresponding dictionaries. The learning algorithm is formulated as a generic bilevel optimization problem, which is solved by a projected first-order stochastic gradient descent algorithm. This general sparse <b>coding</b> <b>model</b> {{can be applied to}} many specific applications involving coupled feature spaces in computer vision and signal processing. In this work, we tailor our general model to learning dictionaries for compressive sensing recovery and single image super-resolution to demonstrate its effectiveness. In both cases, the new sparse <b>coding</b> <b>model</b> remarkably outperforms previous approaches in terms of recovery accuracy. 1...|$|E
30|$|The <b>CoDe</b> <b>model</b> {{has less}} {{parameters}} {{to describe the}} shape of the age pattern, but more parameters to describe the changes in the age pattern, provides extrapolation to higher ages, allows to estimate the modal age at death, does not assume the exponential decline of rates across all ages, decomposes the delay and compression effect, and can serve as a diagnostic tool. While the LC model provides a better fit at younger ages, the <b>CoDe</b> <b>model</b> provides a better fit at older ages. The LC model consistently projects a slowdown of mortality delay and thus of the increase in life expectancy at birth, whereas the <b>CoDe</b> <b>model</b> can project a continuation of delay and thus a steady increase in life expectancy.|$|R
40|$|The primary {{implementations}} of AspectJ to date {{are based}} on a compile- or load-time weaving process that produces Java byte code. Although this implementation strategy has been crucial to the adoption of AspectJ, it faces inherent performance constraints that stem from a mismatch between Java byte code and AspectJ semantics. We discuss these mismatches and show their performance impact on advice dispatch, and we present a machine <b>code</b> <b>model</b> that can be targeted by virtual machine JIT compilers to alleviate this inefficiency. We also present an implementation based on the Jikes RVM which targets this machine <b>code</b> <b>model.</b> Performance evaluation with a set of micro benchmarks shows that our machine <b>code</b> <b>model</b> provides improved performance over translation of advice dispatch to Java byte code...|$|R
30|$|The tool {{automatically}} extracts source <b>code</b> <b>models</b> of {{a target}} system from its version control platform in predefined time intervals.|$|R
40|$|Recently sparse coding {{have been}} highly {{successful}} in image classification mainly {{due to its}} capability of incorporating the sparsity of image representation. In this paper, we propose an improved sparse <b>coding</b> <b>model</b> based on linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform (SIFT) descriptors. The novelty is the simultaneous non-convex and non-negative characters added to the sparse <b>coding</b> <b>model.</b> Our numerical experiments show that the improved approach using non-convex and non-negative sparse coding is superior than the original ScSPM[1] on several typical databases...|$|E
30|$|The H. 264 /AVC design {{adopts a}} multidirectional spatial {{prediction}} model to reduce spatial redundancy, where neighboring pixels {{are used as}} a prediction for the samples in a data block to be encoded. In this paper, a recursive prediction scheme and an enhanced (block-matching algorithm BMA) prediction scheme are designed and integrated into the state-of-the-art H. 264 /AVC framework to provide a new intra <b>coding</b> <b>model.</b> Extensive experiments demonstrate that the coding efficiency can be on average increased by 0.27 [*]dB with comparison {{to the performance of}} the conventional H. 264 <b>coding</b> <b>model.</b>|$|E
30|$|Sparse coding model-based image local {{semantic}} concept representation has the following advantages: First, sparse <b>coding</b> <b>model</b> map image features high-dimensional space, compared with low-dimensional vector, and high-dimensional vector is {{more conducive to}} image classification. Second, each feature point in the image is represented {{by a number of}} base vectors in an over-complete dictionary, which reduces the quantization error and makes the image more accurate. Finally, the non-zero coefficients of sparse representation actually reveal the classification relationship of signals. Therefore, it is very advantageous for scene classification task to obtain image representation using sparse <b>coding</b> <b>model.</b>|$|E
5000|$|C {{language}} <b>coded</b> <b>models</b> with spice format: As an open-source project, Ngspice allows {{new models}} {{to be linked}} to the sources and compiled.|$|R
40|$|Stateless <b>code</b> <b>model</b> {{checking}} is {{an effective}} verification technique, which is more applicable than stateful model checking to the software world. Existing stateless model checkers support the verification of neither LTL formulae nor the information flow security properties. This paper proposes a distributed stateless <b>code</b> <b>model</b> checker (DSCMC) designed based on the Actor model, and has the capability of verifying code written in different programming languages. This tool is implemented using Erlang, which is an actor-based programming language. DSCMC is able to detect deadlocks, livelocks, and data races automatically. In addition, the tool can verify information flow security and the properties specified in LTL. Thanks to its actor-based architecture, DSCMC provides {{a wide range of}} capabilities. The parallel architecture of the tool exploiting the rich concurrency model of Erlang is suited to the time-intensive process of stateless <b>code</b> <b>model</b> checking...|$|R
50|$|BECP {{focuses on}} three key {{building}} energy <b>code</b> areas: <b>model</b> <b>code</b> development, adoption, and compliance.|$|R
