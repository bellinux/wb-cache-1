10000|10000|Public
25|$|Foreground pixels {{are grouped}} using 2D {{connected}} <b>component</b> <b>analysis.</b>|$|E
25|$|Principal <b>component</b> <b>analysis.</b> The {{method of}} fitting a linear {{subspace}} to multivariate data by minimising the chi distances.|$|E
25|$|Principal <b>component</b> <b>analysis</b> is used {{to study}} large data sets, such as those {{encountered}} in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, {{in the field of}} psychometrics. In Q methodology, the eigenvalues of the correlation matrix determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal <b>component</b> <b>analysis</b> {{can be used as a}} method of factor analysis in structural equation modeling.|$|E
40|$|This paper {{presents}} a method called quadtree principal <b>components</b> <b>analysis</b> for facial expression classification. The quadtree principal <b>components</b> <b>analysis</b> {{is an image}} transformation that {{takes its name from}} the quadtree partition scheme on which it is based. The quadtree principal <b>components</b> <b>analysis</b> method implements a global-local decomposition of the input face image. This solves the problems associated with the existing principal <b>components</b> <b>analysis</b> and local principal <b>components</b> <b>analysis</b> methods when applied to facial expression classification...|$|R
40|$|The {{essence of}} {{principal}} <b>components</b> <b>analysis</b> {{and the problem}} of dimension reduction are described. A method of principal components calculation is presented, which is based on the covariance matrix eigenvalues determination. Practical implementations of principal <b>components</b> <b>analysis</b> are described, which are based on QR-algorithm. Application of principal <b>components</b> <b>analysis</b> in space images classification for the reduction of training samples dimension is discussed...|$|R
30|$|A {{combination}} of methods for image subtraction and principal <b>components</b> <b>analysis</b> and {{the division of}} images and principal <b>components</b> <b>analysis</b> were used to achieve exact results since the application of only one method limits the precision of the results. Principal <b>components</b> <b>analysis</b> cannot be considered {{for the purpose of}} research because information on LULC change will not appear for only one component (Trincsi et al., 2014; Gong et al. 2015).|$|R
25|$|Several {{important}} {{problems can}} be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm {{is based on the}} singular value decomposition. The corresponding tool in statistics is called principal <b>component</b> <b>analysis.</b>|$|E
25|$|Multilayer kernel {{machines}} (MKM) {{are a way}} {{of learning}} highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal <b>component</b> <b>analysis</b> (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of the deep learning architecture.|$|E
25|$|The {{mixture of}} helium and oxygen with a 0% {{nitrogen}} content is generally known as Heliox. This is frequently {{used as a}} breathing gas in deep commercial diving operations, where it is often recycled to save the expensive helium <b>component.</b> <b>Analysis</b> of two-component gases is much simpler than three-component gases.|$|E
40|$|This paper applies {{a hidden}} Markov {{model to the}} problem of Attention Deficit Hyperactivity Disorder (ADHD) {{diagnosis}} from resting-state functional Magnetic Resonance Image (fMRI) scans of subjects. The proposed model considers the temporal evolution of fMRI voxel activations in the cortex, cingulate gyrus, and thalamus regions of the brain {{in order to make a}} diagnosis. Four feature dimen- sionality reduction methods are applied to the fMRI scan: voxel means, voxel weighted means, principal <b>components</b> <b>analysis,</b> and kernel principal <b>components</b> <b>analysis.</b> Using principal <b>components</b> <b>analysis</b> and kernel principal <b>components</b> <b>analysis</b> for dimensionality reduction, the proposed algorithm yielded an accu- racy of 63. 01 % and 62. 06 %, respectively, on the ADHD- 200 competition dataset when differentiating between healthy control, ADHD innattentive, and ADHD combined types...|$|R
5000|$|Exploratory factor <b>analysis</b> versus {{principal}} <b>components</b> <b>analysis</b> ...|$|R
40|$|Principal <b>components</b> <b>analysis</b> {{relates to}} the {{eigenvalue}} distribution of Wishart matrices. Given few observations and very many variables this distribution maps to eigenvalue statistics in the Gaussian orthogonal ensemble. Principal components selection can then be based on existing analytical results. Principal <b>components</b> <b>analysis</b> Random matrix theory...|$|R
25|$|The {{approach}} above {{belongs to}} the model-based approach. Another appearance-based approach recognizes individuals through binary gait silhouette sequences. For example, silhouette sequences of full gait cycles can be treated as 3D tensor samples, and multilinear subspace learning, such as the multilinear principal <b>component</b> <b>analysis,</b> can be employed to learning features for classification.|$|E
25|$|A 2007 {{study by}} Bauchet, which {{utilised}} about 10,000 autosomal DNA SNPs arrived at similar results. Principal <b>component</b> <b>analysis</b> clearly identified four widely dispersed groupings, corresponding to Africa, Europe, Central Asia and South Asia. PC1 separated Africans {{from the other}} populations, PC2 divided Asians from Europeans and Africans, whilst PC3 split Central Asians apart from South Asians.|$|E
25|$|Independent <b>component</b> <b>analysis</b> (ICA) {{is another}} signal {{processing}} solution that separates different signals that are statistically independent in time. It is primarily used to remove artifacts such as blinking, eye muscle movement, facial muscle artifacts, cardiac artifacts, etc. from MEG and EEG signals {{that may be}} contaminated with outside noise. However, ICA has poor resolution of highly correlated brain sources.|$|E
40|$|Abstract—The {{medical data}} {{statistical}} analysis often requires the using of some special techniques, {{because of the}} particularities of these data. The principal <b>components</b> <b>analysis</b> and the data clustering are two statistical methods for data mining very useful in the medical field, the first one as a method to decrease the number of studied parameters, and the second one as a method to analyze the connections between diagnosis and the data about the patient’s condition. In this paper we investigate the implications obtained from a specific data analysis technique: the data clustering preceded by a selection of the most relevant parameters, made using the principal <b>components</b> <b>analysis.</b> Our assumption was that, using the principal <b>components</b> <b>analysis</b> before data clustering- in order to select and to classify only the most relevant parameters – the accuracy of clustering is improved, but the practical results showed the opposite fact: the clustering accuracy decreases, with a percentage approximately equal with the percentage of information loss reported by the principal <b>components</b> <b>analysis.</b> Keywords—Data clustering, medical data, principal <b>components</b> <b>analysis.</b> I...|$|R
40|$|Objective To {{explore the}} {{characteristics}} of regional distribution of cancer deaths in Shandong Province with the principle <b>components</b> <b>analysis.</b> Methods The principle <b>components</b> <b>analysis</b> with co-variance matrix for age-adjusted mortality rates and percentages of 20 types of cancer in 22 counties （cities） were carried out using SAS Software. Results Over 90...|$|R
5000|$|... #Subtitle level 2: Exploratory factor <b>analysis</b> versus {{principal}} <b>components</b> <b>analysis</b> ...|$|R
25|$|The SVD is also applied {{extensively}} to {{the study}} of linear inverse problems, and is useful in the analysis of regularization methods such as that of Tikhonov. It is widely used in statistics where it is related to principal <b>component</b> <b>analysis</b> and to Correspondence analysis, and in signal processing and pattern recognition. It is also used in output-only modal analysis, where the non-scaled mode shapes can be determined from the singular vectors. Yet another usage is latent semantic indexing in natural language text processing.|$|E
25|$|A 2014 {{study by}} Carmi et al. {{published}} by Nature Communications found that Ashkenazi Jewish population originates from mixing between Middle Eastern and European peoples. According to the authors, that mixing likely occurred some 600–800 years ago, followed by rapid growth and genetic isolation (rate per generation 16–53%;). The {{study found that}} all Ashkenazi Jews descent from around 350 individuals, and that the principal <b>component</b> <b>analysis</b> of common variants in the sequenced AJ samples, confirmed previous observations, namely, the proximity of Ashkenazi Jewish cluster to other Jewish, European and Middle Eastern populations".|$|E
25|$|Two {{types of}} tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them {{decomposes}} a tensor into a sum of rank-1 tensors, {{which is called}} a tensor rank decomposition. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is {{referred to in the}} literature as the higher-order SVD (HOSVD) or Tucker3/TuckerM. In addition, multilinear principal <b>component</b> <b>analysis</b> in multilinear subspace learning involves the same mathematical operations as Tucker decomposition, being used in a different context of dimensionality reduction.|$|E
50|$|This decorrelation {{is related}} to {{principal}} <b>components</b> <b>analysis</b> for multivariate data.|$|R
5000|$|... in {{principal}} <b>components</b> <b>analysis</b> (PCA) {{when there}} are many supplementary variables; ...|$|R
40|$|PCA in high dimensions. • Sparsity of {{principal}} components. • Consistent estimation and minimax theory. • Feasible algorithms using convex relaxation. Principal <b>Components</b> <b>Analysis</b> • I have iid data points X 1, [...] .,Xn on p variables. • p may be large, so I {{want to use}} principal <b>components</b> <b>analysis</b> (PCA) for dimension reduction...|$|R
25|$|Principal {{component}} regression (PCR) is {{used when}} the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal <b>component</b> <b>analysis</b> then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.|$|E
25|$|In image processing, {{processed}} {{images of}} faces {{can be seen}} as vectors whose components are the brightnesses of each pixel. The dimension of this vector space is the number of pixels. The eigenvectors of the covariance matrix associated with a large set of normalized pictures of faces are called eigenfaces; {{this is an example of}} principal <b>component</b> <b>analysis.</b> They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.|$|E
25|$|A 2014 {{study by}} Carmi et al. {{published}} by Nature Communications {{found that the}} Ashkenazi Jewish population originates from an even mixture between Middle Eastern and European peoples. According to the authors, that mixing likely occurred some 600–800 years ago, followed by rapid growth and genetic isolation (rate per generation 16–53%;). The study found that all Ashkenazi Jews descent from around 350 individuals, about half of whom were Middle Eastern and half were European, and that all Ashkenazi Jews {{are related to the}} point of being no more than 30th cousins. The principal <b>component</b> <b>analysis</b> of common variants in the sequenced AJ samples, confirmed previous observations, namely, the proximity of Ashkenazi Jewish cluster to other Jewish, European and Middle Eastern populations.|$|E
40|$|The aim of {{this article}} is the {{approximation}} of the principal <b>components</b> <b>analysis</b> (PCA) of a stationary function, defined on, by the PCA of a stationary series. For this, we use a discretization of the real line. We study the behavior of the approximation when the step of discretization decreases. Principal <b>components</b> <b>analysis</b> Stationary random function Frequency domain...|$|R
5000|$|Distinctiveness of {{descriptors}} {{is measured}} by summing the eigenvalues of the descriptors, obtained by the Principal <b>components</b> <b>analysis</b> of the descriptors normalized by their variance. This corresponds {{to the amount of}} variance captured by different descriptors, therefore, to their distinctiveness. PCA-SIFT (Principal <b>Components</b> <b>Analysis</b> applied to SIFT descriptors), GLOH and SIFT features give the highest values.|$|R
40|$|We {{propose a}} method for spatial {{principal}} <b>components</b> <b>analysis</b> that has two important advantages over the method that Wartenberg (1985) proposed. The first advantage is that, contrary to Wartenberg's method, our method has a clear and exact interpretation: it produces a summary measure (component) that itself has maximum spatial correlation. Second, an easy and intuitive link {{can be made to}} canonical correlation analysis. Our spatial canonical correlation analysis produces summary measures of two datasets (e. g., each measuring a different phenomenon), and these summary measures maximize the spatial correlation between themselves. This provides an alternative weighting scheme as compared to spatial principal <b>components</b> <b>analysis.</b> We provide example applications of the methods and show that our variant of spatial canonical correlation analysis may produce rather different results than spatial principal <b>components</b> <b>analysis</b> using Wartenberg’s method. We also illustrate how spatial canonical correlation analysis may produce different results than spatial principal <b>components</b> <b>analysis.</b> ...|$|R
500|$|Such a two-good {{world is}} a {{theoretical}} simplification {{because of the difficulty}} of graphical analysis of multiple goods. If we are interested in one good, a composite score of the other goods can be generated using different techniques. [...] Furthermore, the production model can be generalised using higher-dimensional techniques such as Principal <b>Component</b> <b>Analysis</b> (PCA) and others.|$|E
500|$|Specimens of Tatuidris are small, about [...] {{in total}} length, but {{specimens}} can vary greatly in size, with larger specimens being {{twice as large}} as the smaller ones. Size variability within trap catches (possibly same colonies) may be considerable. For example, workers from one collection catch in Nicaragua varied 30% in size. It is still unclear whether intra-colony size variation is due to the presence of morphological worker castes (e.g. minor and major castes) or continuous size variability. Principal <b>component</b> <b>analysis</b> (PCA) revealed that most variability among specimens is related to size, with shape explaining little of total variation.|$|E
2500|$|Covariation among traits (scatterplots and correlations, {{principal}} <b>component</b> <b>analysis)</b> ...|$|E
40|$|AbstractIn {{allusion to}} the {{shortage}} of traditional analytic hierarchy process in the determination of weight coefficient, a threat evaluation model {{based on the principle}} <b>components</b> <b>analysis</b> and analytic hierarchy process was proposed in the article. The subjective and objective factors were comprehensive considered. The importance of each index in index layer obtained by the principal <b>components</b> <b>analysis</b> method was used to gain the judge matrix in the analytic hierarchy process to improve the veracity and rationality of the judge matrix. The simulation proved that the threat assessment result of anti-warship missiles to warship obtained through the threat assessment algorithm based on AHP and the principal <b>components</b> <b>analysis</b> was objective and reasonable...|$|R
40|$|In {{this report}} we {{address the problem}} of skin {{fluorescence}} in feature extraction from Raman spectra of skin lesions. We apply a highly automated neural network method for suppressing skin fluorescence from Raman spectrum of skin lesions before dimension reduction with principal <b>components</b> <b>analysis.</b> By applying the background suppression, the effect of outlier spectrum in the principal <b>components</b> <b>analysis</b> was greatly reduced...|$|R
40|$|We thank Denis de Crombrugghe for useful {{comments}} and discussions. We propose {{a method for}} spatial principal <b>components</b> <b>analysis</b> that has two important advantages over the method that Wartenberg (1985) proposed. The first advantage is that, contrary to Wartenberg’s method, our method has a clear and exact interpretation: it produces a summary measure (component) that itself has maximum spatial correlation. Second, an easy and intuitive link {{can be made to}} canonical correlation analysis. Our spatial canonical correlation analysis produces summary measures of two datasets (e. g., each measuring a different phenomenon), and these summary measures maximize the spatial correlation between themselves. This provides an alternative weighting scheme as compared to spatial principal <b>components</b> <b>analysis.</b> We provide example applications of the methods and show that our variant of spatial canonical correlation analysis may produce rather different results than spatial principal <b>components</b> <b>analysis</b> using Wartenberg’s method. We also illustrate how spatial canonical correlation analysis may produce different results than spatial principal <b>components</b> <b>analysis...</b>|$|R
