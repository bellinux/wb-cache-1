16|10000|Public
25|$|Recent {{approaches}} {{consider the}} use of high-quality microscopy images over which a statistical classification algorithm is used to perform automated cell detection and counting as an image analysis task. Generally performs with a <b>constant</b> <b>error</b> <b>rate</b> as an off-line (batch) type process. A range of image classification techniques can be employed for this purpose.|$|E
30|$|Figure  2 b {{indicates}} that the optimal number of trees is approximately 330 for the forest with 330 trees, which has the minimum OOB error rate along with a <b>constant</b> <b>error</b> <b>rate</b> 0.16. From Fig.  2 a, six key factors (Q, C M, Q D etc.) can be selected by the mean magnitude value, which will be utilized in the SVMs modeling process afterward.|$|E
40|$|Randomised {{techniques}} allow {{very big}} language models {{to be represented}} succinctly. However, being batch-based they are unsuitable for modelling an unbounded stream of language whilst maintaining a <b>constant</b> <b>error</b> <b>rate.</b> We present a novel randomised language model which uses an online perfect hash function to efficiently deal with unbounded text streams. Translation experiments over a text stream show that our online randomised model matches the performance of batch-based LMs without incurring the computational overhead associated with full retraining. This opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the Web each day. ...|$|E
40|$|Digital {{technology}} in space-bound, nanometer scale systems create {{the need for}} reliable computation {{in the face of}} a small but finite <b>error</b> <b>rate.</b> We present a designerdriven, semi-automated technique for synthesizing fault tolerance in finite state machines. This is done with lower implementation overhead in terms of both machine redundancy and overall logic complexity than conventional methods. We further show that it is possible to generate efficient encodings that inherently provide tolerance to <b>constant</b> <b>error</b> <b>rates</b> in excess of one error per cycle and, through the use of a high level description language (HDL), can be performed with turn-key simplicity. The technique is demonstrated in the design of a practical multi-threaded microprocessor engineered specifically for real-time and high bandwidth control. Comparison between the basic and fault tolerant versions shows that as large as 18 % <b>error</b> <b>rates</b> can be recovered with minimal space overhead and almost no additional delay. ...|$|R
40|$|We {{consider}} {{the problem of}} constructing efficient locally decodable codes {{in the presence of}} a computationally bounded adversary. Assuming the existence of one-way functions, we construct efficient locally decodable codes with positive information rate and low (almost optimal) query complexity which can correctly decode any given bit of the message from <b>constant</b> channel <b>error</b> <b>rate</b> ρ. This compares favorably to our state of knowledge locally-decodable codes without cryptographic assumptions. For all our constructions, the probability for any polynomial-time adversary, that the decoding algorithm incorrectly decodes any bit of the message is negligible in the security parameter...|$|R
40|$|Initial code {{acquisition}} of direct-sequence spread-spectrum signals is typically based on serial multidwell hypothesis tests {{to limit the}} costs of mobile terminals. The procedure that searches for the correct code and its actual time offset usually adopts a user-oriented quality criterion based on <b>constant</b> <b>error</b> <b>rates.</b> The false-alarm and miss detection probabilities can be theoretically evaluated {{by means of the}} (recently introduced) class of generalized Q (GQ) functions. In this correspondence, we show that the GQ functions constitute a useful tool not only for analytic performance analysis, but also for the optimized design of initial, code synchronization systems. Some examples of application of two-dwell (search/verification) procedures are provided. The mathematical problem consists of the minimization of an objective function (i. e., the mean acquisition time) depending on four parameters (two testing durations and two thresholds) with two constraints (the probabilities of miss detection and false alarm). In particular, we have implemented and analyzed the convergence of the steepest descent and the Newton-Raphson numerical algorithms. The computational cost of the method and the effect of multipath Rayleigh channels are also discussed. The optimized acquisition procedure has evidenced a significant reduction of the mean duration of serial tests in comparison with (suboptimum) previous attempts...|$|R
40|$|This work is {{concerned}} with phrasing the concepts of fault-tolerant quantum computation {{within the framework of}} disordered systems, Bernoulli site percolation in particular. We show how the so-called "threshold theorems" on the possibility of fault-tolerant quantum computation with <b>constant</b> <b>error</b> <b>rate</b> can be cast as a renormalization (coarse-graining) of the site percolation process describing the occurrence of errors during computation. We also use percolation techniques to derive a trade-off between the complexity overhead of the fault-tolerant circuit and the threshold error rate. Comment: 4 pages, 2 eps figures; revtex 4; based on talk given at the Simons Conference on Quantum and Reversible Computation, Stony Brook NY, May 28 - 31; minor typographical change...|$|E
40|$|One of {{the core}} {{challenges}} of decision research is to identify individuals' decision strategies without influencing decision behavior by the method used. Br"oder and Schiffer (2003) suggested a method to classify decision strategies based on a maximum likelihood estimation, comparing the probability of individuals' choices given {{the application of a}} certain strategy and a <b>constant</b> <b>error</b> <b>rate.</b> Although this method was shown to be unbiased and practically useful, it obviously does not allow differentiating between models that make the same predictions concerning choices but different predictions for the underlying process, which is often the case when comparing complex to simple models or when comparing intuitive and deliberate strategies. An extended method is suggested that additionally includes decision times and confidence judgments in a simultaneous Multiple-Measure Maximum Likelihood estimation. In simulations, it is shown that the method is unbiased and sensitive to differentiate between strategies if the effects on times and confidence are sufficiently large...|$|E
40|$|Abstract—Over {{the last}} few decades, most {{quantitative}} measures of VLSI performance have improved by many orders of magnitude; this has been achieved by the unabated scaling of the sizes of MOSFETs. However, scaling also exacerbates noise and reliability issues, thus posing new challenges in circuit design. Reliability becomes a major concern due to many and often correlated factors, such as parameter variations and soft errors. Existing reliability evaluation tools focus on algorithmic development at the logic level that usually uses a <b>constant</b> <b>error</b> <b>rate</b> for gate failure and thus leads to approximations {{in the assessment of}} a VLSI circuit. This paper proposes a more accurate and scalable approach that utilizes a transistor-level stochastic analysis for digital fault modeling. It accounts for very detailed measures, including the probability of failure of individual transistors, the topology of logic gates, timing sequences and the applied input vectors. Simulation results are provided to demonstrate both the efficiency and the accuracy of the proposed approach...|$|E
40|$|This paper {{introduces}} {{a hybrid of}} antithetic sampling and scrambled digital nets. Antithetic sampling typically affects the <b>constant</b> in the <b>error</b> <b>rate,</b> not always for the better. A form of local antithetic sampling, in conjunction with scrambled net sampling reduces the root mean squared <b>error</b> <b>rate</b> of scrambled net sampling from O(n − 3 / 2 +ɛ) to O(n − 3 / 2 − 1 /d+ɛ) in d dimensions. The method can also {{be viewed as a}} hybrid of scrambled nets with monomial cubature...|$|R
40|$|A poster to {{show the}} Spectrum {{efficiency}} gains {{as a result of}} the adaptive transmit power control in fixed terrestrial links at 38 GHZ. Adaptive transmit power control can be used to improve the spectrum efficiency of terrestrial point to point fixed links by limiting the transmit power to that required to maintain a <b>constant</b> bit <b>error</b> <b>rate</b> (BER) regardless of the propagation conditions. This results in a reduced transmit power being used during clear sky conditions, lowering the interference resulting from the ATPC link. This improves the frequency reuse factor associated with a given band and geographic area, providing a spectrum efficiency gain...|$|R
40|$|Selection of {{the best}} set of scales is {{problematic}} when developing signal driven approaches for pixel-based image segmentation. Often, different possibly conflicting criteria need to be fulfilled {{in order to obtain}} the best tradeoff between uncertainty (variance) and location accuracy. The optimal set of scales depends on several factors: the noise level present in the image material, the prior distribution of the different types of segments, the class-conditional distributions associated with each type of segment as well as the actual size of the (connected) segments. We analyse, theoretically and through experiments, the possibility of using the overall and class-conditional <b>error</b> <b>rates</b> as criteria for selecting the optimal sampling of the linear and morphological scale spaces. It is shown that the overall <b>error</b> <b>rate</b> is optimised by taking the prior class distribution in the image material into account. However, a uniform (ignorant) prior distribution ensures <b>constant</b> class-conditional <b>error</b> <b>rates.</b> Consequently, we advocate for a uniform prior class distribution when an uncommitted, scaleinvariant segmentation approach is desired...|$|R
40|$|We {{show how}} to {{efficiently}} reconstruct an original DNA sequence of length n with high probability from log ffi n erroneous copies (for some ffi ! 2), assuming the sequence itself is random and errors are random with <b>constant</b> <b>error</b> <b>rate</b> 1 =C. Key words: DNA, sequencing, alignment, errors, Kolmogorov complexity. 1 Introduction DNA sequencing {{is a key}} step and a major bottleneck in the Human Genome Project. It is relatively slow and expensive (~$ 1 per base with current techniques). Since the human genome comprises no less than 3 billion bases, the development of faster and cheaper sequencing methods {{is crucial to the}} project. Certain technologies promise the ability to obtain long DNA sequences fast but with lots of errors. In single-molecule DNA sequencing, the DNA strand is passed by a cutter, that cuts off a single base at a time, which then flows down a microscopic tube at high speed past an optical device. This excites the individual molecule and reads off which type of base it is. [...] ...|$|E
40|$|This work {{considers}} locally decodable {{codes in}} the computationally bounded channel model. The computationally bounded channel model, introduced by Lipton in 1994, views the channel as an adversary which {{is restricted to}} polynomial-time computation. Assuming the existence of IND-CPA secure public-key encryption, we present a construction of public-key locally decodable codes, with constant codeword expansion, tolerating <b>constant</b> <b>error</b> <b>rate,</b> with locality O(λ), and negligible probability of decoding failure, for security parameter λ. Hemenway and Ostrovsky gave a construction of locally decodable codes in the public-key model with constant codeword expansion and locality O(λ 2), but their construction had two major drawbacks. The keys in their scheme were proportional to n, {{the length of the}} message, and their schemes were based on the Φ-hiding assumption. Our keys are of length proportional to the security parameter instead of the message, and our construction relies only on the existence of IND-CPA secure encryption rather than on specific number-theoretic assumptions. Our scheme also decreases the locality from O(λ 2) to O(λ). Our construction can b...|$|E
30|$|Random forest technique, {{proposed}} by Breiman [19], {{is one of}} the most recent and most promising machine learning techniques, well known for its capability to identify significant variables from a set of them. In this method, numerous trees are attempted by randomly selecting some observations from the original data set with replacement, and then searching over a randomly selected subset of covariates at each split [20, 21]. To examine whether attempted numbers of trees are adequate to reach reasonably stable outputs, the out-of-bag (OOB) error rate parameter is used. The best number of trees has the minimum error rate and a <b>constant</b> <b>error</b> <b>rate</b> nearby. In order to identify the relative importance of each variable from a set of them, a mean decrease Gini IncNodePurity diagram can be produced using the R-package [22]. By using this diagram, a node purity value for every variable (node of a tree) can be determined by means of the Gini index [21]. The higher the value of node purity, the higher the importance of a variable is. Breiman [19] can be followed for further details on this technique.|$|E
50|$|As an example, the {{spacecraft}} Cassini-Huygens, launched in 1997, contains two identical flight recorders, each with 2.5 gigabits of memory {{in the form}} of arrays of commercial DRAM chips. Thanks to built-in EDAC functionality, spacecraft's engineering telemetry reports the number of (correctable) single-bit-per-word errors and (uncorrectable) double-bit-per-word errors. During the first 2.5 years of flight, {{the spacecraft}} reported a nearly <b>constant</b> single-bit <b>error</b> <b>rate</b> of about 280 errors per day. However, on November 6, 1997, during the first month in space, the number of errors increased by more than a factor of four for that single day. This was attributed to a solar particle event that had been detected by the satellite GOES 9.|$|R
50|$|A major {{disadvantage}} of using IMUs for navigation {{is that they}} typically suffer from accumulated error. Because the guidance system is continually integrating acceleration with respect to time to calculate velocity and position (see dead reckoning), any measurement errors, however small, are accumulated over time. This leads to 'drift': an ever-increasing difference between where the system thinks it is located and the actual location. Due to integration a <b>constant</b> <b>error</b> in acceleration results in a linear error in velocity and a quadratic error growth in position. A <b>constant</b> <b>error</b> in attitude <b>rate</b> (gyro) results in a quadratic error in velocity and a cubic error growth in position.|$|R
40|$|This study {{presents}} a {{reliability and availability}} analysis of human-machine systems with human errors and common-cause failures. The systems incorporate elements of several commonly used redundant configurations: standby, k-out-of-n, majority voting with imperfect voter, and parallel. The analysis considers systems with <b>constant</b> human <b>error</b> <b>rates,</b> increasing human <b>error</b> <b>rates</b> and general human <b>error</b> <b>rates,</b> and with arbitrarily distributed failed system repair times. The method of linear ordinary differential equations for general system failure rates is presented to obtain the general expressions of the steady state availability for various types of system repair time distributions, such as Gamma, Weibull, lognormal, exponential and Rayleigh distributions. A method which combines the inclusion supplementary variables technique and the method of stages is developed to perform time-dependent system availability analysis for systems with both time-dependent human <b>error</b> <b>rates</b> and failed system repair rates. Generalized expressions for such relevant system performance indices as the system reliability, steady state availability, time-dependent system availability, {{mean time to failure}} and system variance of time to failure are presented. The impact of human error, common-cause failure, failed system repair policy and the elements of redundant configurations on the values of the afore-said system performance indices is demonstrated by means of plots...|$|R
40|$|We give {{improved}} pseudorandom generators (PRGs) for Lipschitz {{functions of}} low-degree polynomials over the hypercube. These are {{functions of the}} form psi(P(x)), where P is a low-degree polynomial and psi is a function with small Lipschitz constant. PRGs for smooth functions of low-degree polynomials have {{received a lot of}} attention recently and {{play an important role in}} constructing PRGs for the natural class of polynomial threshold functions. In spite of the recent progress, no nontrivial PRGs were known for fooling Lipschitz functions of degree O(log n) polynomials even for <b>constant</b> <b>error</b> <b>rate.</b> In this work, we give the first such generator obtaining a seed-length of (log n) Õ(d^ 2 /eps^ 2) for fooling degree d polynomials with error eps. Previous generators had an exponential dependence on the degree. We use our PRG to get better integrality gap instances for sparsest cut, a fundamental problem in graph theory with many applications in graph optimization. We give an instance of uniform sparsest cut for which a powerful semi-definite relaxation (SDP) first introduced by Goemans and Linial and studied in the seminal work of Arora, Rao and Vazirani has an integrality gap of exp(Ω((log log n) ^ 1 / 2)). Understanding the performance of the Goemans-Linial SDP for uniform sparsest cut is an important open problem in approximation algorithms and metric embeddings and our work gives a near-exponential improvement over previous lower bounds which achieved a gap of Ω(log log n) ...|$|E
40|$|Graduation date: 1979 Misclassification {{introduced}} by fallible measurements affects {{the estimate of}} the proportion in a class {{as well as the}} comparison of proportions in different classes. In this thesis the magnitude of effects of misclassification and the importance of misclassification error rates on estimates of proportions and variances of these estimates were examined. Studies on the values of the false negative and false positive rates associated with medical screening were reviewed to determine typical levels of error rates occurring in practice. A lack of a consistent relationship between these rates was found and the common assumption of a small <b>constant</b> <b>error</b> <b>rate</b> in all groups being compared was violated in almost all-studies. An attempt was made to determine how robust the usual statistical procedure for analyzing a given set of data is against these classification errors. The study was carried out for the case of two independent binomial samples (very common in epidemiologic research) with the conditional model (Fisher's exact test) considered in detail under various error rates. Substantial effects of misclassification on the estimation of parameters as well as on hypothesis testing showed the importance of estimating the values of the misclassification rates in a particular study. The randomized response technique was used to estimate error rates and the prevalence rate, π, for a situation where the true classification can only be obtained directly from the respondent, but the response has a stigmatizing nature. An unbiased estimate of π was obtained along with an expression for the variance of π. Formulas for sample size determination for fixed cost and fixed variance problems are given...|$|E
30|$|Despite the advantages, SVMs models {{lack the}} {{capability}} of detecting the contributing factors {{and the use of}} all the variables as input makes the estimation inefficient. As suggested by Yu et al. [17], variable selection procedure is needed prior to the SVMs estimation. Meanwhile, by selecting variables it is able to solve the over-fitting issues. Hence, random forest was employed to select the contributing factors, as it is well known for selecting significant contributing variables from a set of factors [27]. The strategy of random forest is that every tree is built with several factors, so a particular tree grows from a bootstrap aggregate sample, part of the cases is discarded and they will not be used {{in the development of the}} trees. The left-out cases are called Out-Of-Bag (OOB) data. The OOB data turn to validate the built trees with an unbiased error estimate as well as the important level estimations of variables. To test whether the attempted numbers of trees are sufficient to reach relatively stable results, the plot of OOB error rate against various tree numbers is developed. The optimal number of trees is the one having the minimum OOB error rate along with a nearly <b>constant</b> <b>error</b> <b>rate.</b> A wrapper MATLAB file interface to C code used in R package random forest [28] was employed to select the contributing factors. The tool provides the ‘mean decrease in Gini index’ method to select contributing variables. A higher magnitude implies a higher variable importance. Hassan et al. [27] chose several variables with higher scores (approximately 50 % of the scores for all variables in total) than the remaining variables. In this paper, the variables which score higher than the mean score of all the variables were selected for the modeling process.|$|E
40|$|BACKGROUND: Performing {{laparoscopic surgery}} {{involves}} a complex cascade of cognitive skills, which may inherently have a <b>constant</b> technical <b>error</b> <b>rate.</b> We assess generic and specific minor and major <b>error</b> <b>rates</b> in laparoscopic cholecystectomies (LCs) performed by consultant surgeons. METHODS: Checklists of generic (11) and specific technical minor (six) and major events (eight) were devised for LCs. Two experienced surgeons assessed each full-length operation blindly and independently. RESULTS: A total of 37 LCs were performed by eight consultants. There were no major intraoperative or postoperative complications. Mean inter-rater reliability was kappa = 0. 91 (range 0. 80 - 0. 98) {{for each of}} the <b>error</b> categories. <b>Error</b> <b>rates</b> were generic (27 / 407) 6. 6 %, minor (59 / 222) 26. 6 %, and major (8 / 296) 2. 7 %, respectively. There was a significant statistical difference between the minor error group and the other groups, p <or= 0. 05. CONCLUSIONS: Performing laparoscopic surgery may always have a background technical <b>error</b> <b>rate.</b> Our present study demonstrates a migration of surgical technical errors in expert laparoscopic surgeons. The surgeons migrate technically when they execute a high rate of procedure-specific minor errors. However, {{when it comes to the}} major fundamental aspects of the operation, they dynamically adapt and migrate away from performing major technical errors. We aim to continue the study to increase cases, assess trainees as well, and also explore other factors that may affect the surgeon when executing surgical technical tasks...|$|R
40|$|Purpose – The {{purpose of}} this {{research}} is to incorporate the exponentiated Weibull testing-effort functions into software reliability modeling and to estimate the optimal software release time. Design/methodology/approach – This paper suggests a software reliability growth model based on the non-homogeneous Poisson process (NHPP) which incorporates the exponentiated Weibull (EW) testing-efforts. Findings – Experimental results on actual data from three software projects are compared with other existing models which reveal that the proposed software reliability growth model with EW testing-effort is wider and effective SRGM. Research limitations/implications – This paper presents a SRGM using a <b>constant</b> <b>error</b> detection <b>rate</b> per unit testing-effort. Practical implications – Software reliability growth model is one of the fundamental techniques to assess software reliability quantitatively. The results obtained in this paper will be useful during the software testing process. Originality/value – The present scheme has a flexible structure and may cover many of the earlier results on software reliability growth modeling. In general, this paper also provides a framework in which many software reliability growth models can be described...|$|R
40|$|International Telemetering Conference Proceedings / October 09 - 11, 1973 / Sheraton Inn Northeast, Washington, D. C. This paper {{reviews the}} data <b>rate,</b> <b>error</b> <b>rate,</b> and {{signal-to-noise}} ratio relationship for various uncoded M-ary digital amplitude modulation (AM), phase modulation (PM), and combined AM-PM systems. These signal systems have the common virtue that expanding {{the number of}} possible signals to be transmitted increases the data rate but not the bandwidth. The increased data rate generally requires an increased signal-to-noise ratio to maintain <b>constant</b> <b>error</b> probability performance. Thus, these systems use power to conserve bandwidth. A general treatment of the <b>error</b> <b>rate</b> of M-ary digital AM-PM permits development of a simple yet accurate expression which approximates the increase in average signal-to-noise ratio (over that of binary phase shift keying) required for <b>constant</b> <b>error</b> performance. This equation provides insight into why arrays differ in their signal-to-noise ratio requirements...|$|R
40|$|An {{approximate}} membership {{data structure}} is a randomized data structure representing a set which supports membership queries. It {{allows for a}} small false positive error rate but has no false negative errors. Such data structures were first introduced by Bloom in the 1970 s and have since had numerous applications, mainly in distributed systems, database systems, and networks. The algorithm of Bloom (known as a Bloom filter) is quite effective: it can store an approximation of a set S of size n by using only ≈ 1. 44 n log 2 (1 /ε) bits while having false positive error ε. This is within a constant factor of the information-theoretic lower bound of n log 2 (1 /ε) for storing such sets. Closing this gap is an important open problem, as Bloom filters are widely used in situations where storage is at a premium. Bloom filters have another property: they are dynamic. That is, they support the iterative insertions of up to n elements. In fact, if one removes this requirement, there exist static data structures that receive the entire set at once and can almost achieve the information-theoretic lower bound; they require only (1 + o(1)) n log 2 (1 /ε) bits. Our main result is a new lower bound for the space requirements of any dynamic approximate membership data structure. We show that for any constant ε > 0, any such data structure that achieves false positive error rate of ε must use at least C(ε) · n log 2 (1 /ε) memory bits, where C(ε) > 1 depends only on ε. This shows that the information-theoretic lower bound cannot be achieved by dynamic data structures for any <b>constant</b> <b>error</b> <b>rate.</b> © 2013 Society for Industrial and Applied Mathematics...|$|E
40|$|We {{consider}} {{the task of}} interactive communication {{in the presence of}} adversarial errors and present tight bounds on the tolerable error-rates {{in a number of different}} settings. Most significantly, we explore adaptive interactive communication where the communicating parties decide who should speak next based on the history of the interaction. Braverman and Rao [STOC' 11] show that non-adaptively one can code for any <b>constant</b> <b>error</b> <b>rate</b> below 1 / 4 but not more. They asked whether this bound could be improved using adaptivity. We answer this open question in the affirmative (with a slightly different collection of resources) : Our adaptive coding scheme tolerates any error rate below 2 / 7 and we show that tolerating a higher error rate is impossible. We also show that in the setting of Franklin et al. [CRYPTO' 13], where parties share randomness not known to the adversary, adaptivity increases the tolerable error rate from 1 / 2 to 2 / 3. For list-decodable interactive communications, where each party outputs a constant size list of possible outcomes, the tight tolerable error rate is 1 / 2. Our negative results hold even if the communication and computation are unbounded, whereas for our positive results communication and computation are polynomially bounded. Most prior work considered coding schemes with linear amount of communication, while allowing unbounded computations. We argue that studying tolerable error rates in this relaxed context helps to identify a setting's intrinsic optimal error rate. We set forward a strong working hypothesis which stipulates that for any setting the maximum tolerable error rate is independent of many computational and communication complexity measures. We believe this hypothesis to be a powerful guideline for the design of simple, natural, and efficient coding schemes and for understanding the (im) possibilities of coding for interactive communications...|$|E
40|$|The {{purpose of}} this thesis was twofold: to {{investigate}} the effects of categorized versus uncategorized material on selective attention and to test predictions derived from Filter Theory (Broadbent, 1958), Response Selection Theory (Deutsch and Deutsch, 1963), and Attenuation Theory (Treisman, 1969). Subjects performed a dichotic-listening task in which they shadowed a list of words presented to one ear (i. e., relevant message) while trying to ignore a simultaneously presented list of words on the other ear (i. e., irrelevant message). Lists were 16 words in length and consisted of either categorized words (C) or uncategorized words (U) presented {{at a rate of}} one word per second. Four conditions were generated by using all pairings of C lists and U lists for relevant versus irrelevant messages: U-U, U-C, C-U and C-C. Note that the left-most symbol designates the relevant message and the right-most symbol designates the irrelevant message. Subjects received two presentations of each of the four conditions. Measurements of pupil size were taken twice (9 sec and 5 sec) before the presentation of each dichotic trial (i. e., baseline measures) and at six positions (1, 4, 7, 10, 13 and 16) in the word lists (i. e., trial measures). Since each subject received eight experimental trials (two trials in each of four conditions), there were a total of four baseline measurements and twelve trial measurements for each condition. In each condition the four baseline measurements were averaged and the two trial measurements were averaged at each of the six positions. The mean baseline was subtracted from each of the six position means in each condition. These mean difference scores were used as the basis for one analysis. Shadowing errors (i. e. omissions of relevant words, mispronunciations of relevant words, or intrusions of irrelevant words) were scored by quadrants separately {{for each of the four}} conditions. The first quadrant consisted of the first through fourth words, the second quadrant consisted of the fifth through eighth words and so on. Error scores were then converted to percents and used as the bass for a second analysis. A 4 by 6 ANOVA with repeated measures on both factors (condition and position, respectively) was used to analyze the pupil size data. The results indicated that pupil size decreased across serial position in a similar fashion for all conditions. Furthermore, pupil size did not differ significantly among the four conditions. A 4 by 4 ANOVA with repeated measures on both factors (condition and quadrant, respectively) was used to analyze the error rate data. The results indicated in an interaction between condition and quadrant. The C-U and C-C conditions resulted in a relatively <b>constant</b> <b>error</b> <b>rate</b> across quadrants, while the U-U and U-C conditions exhibited an increasing error rate across quadrants. The results of the two analyses are discussed in terms of their implications for Filter Theory, Response Selection Theory and Attenuation Theory...|$|E
40|$|International audienceIn {{the present}} {{functional}} {{magnetic resonance imaging}} experiment, study participants performed a dynamic tracking task in a precision grip con¢guration. The precision level of the force control was varied while the mean force level of 5 N was kept <b>constant.</b> Contrasts cancelling <b>error</b> <b>rate</b> di¡erences between the conditions showed activation of nonprimary motor areas and other frontal structures in response to increasing precision constraints when the precision of force control could still be increased, and of right primary and associative parietal areas when the precision of the produced force control reached its maximum. These results suggest that the network of frontal and parietal areas, usually working together in ¢ne control of dexterity tasks, can be di¡erentially involved when environmental constraints become very high. NeuroReport 16 : 1271 ^ 127...|$|R
40|$|In this paper, several {{techniques}} {{for reducing the}} search complexity of beam search for continuous speech recognition task are proposed. Six heuristic methods for pruning are described and {{the parameters of the}} pruning are adjusted to keep <b>constant</b> the word <b>error</b> <b>rate</b> while reducing the computational complexity and memory demand. The evaluation of the effect of each pruning method is performed in Mixture Stochastic Trajectory Model (MSTM). MSTM is a segment-based model using phonemes as the speech units. The set of tests in a speaker-dependent continuous speech recognition task shows that using the pruning methods, a substantial reduction of 67 # of search effort is obtained in term of number of hypothesised phonemes during the search. All proposed techniques are independent of the acoustic models and therefore are applicable to other acoustic modeling techniques...|$|R
40|$|This paper {{reviews the}} data <b>rate,</b> <b>error</b> <b>rate,</b> and {{signal-to-noise}} ratio relationship for various uncoded M-ary digital amplitude modulation (AM), phase modulation (PM), and combined AM-PM systems. These signal systems have the common virtue that expanding {{the number of}} possible signals to be transmitted increases the data rate but not the bandwidth. A general treatment of the <b>error</b> <b>rate</b> of M-ary digital AM-PM permits development of a simple yet accurate expression which approximates the increase in average signal-to-noise ratio (over that of binary phase shift keying) required for <b>constant</b> <b>error</b> performance. This equation provides insight into why arrays differ in their signal-to-noise ratio requirements...|$|R
40|$|A {{challenging}} {{problem in}} offering high-speed data service over wireless {{is to protect}} data over the error-proned fading channel in an effective way (high-bandwidth efficiency). In this paper, we propose a bandwidth-efficient error correction scheme, namely the variable rate adaptive bit interleaved coded modulation (ABICM), for the wireless mobile channel. The code rate and modulation level are varied according to the current channel state to exploit the time-varying nature of the wireless channel. Design challenges to achieve symbol-by-symbol adaptation are addressed. In particular, we address the criteria to choose the family of component codes for the ABICM system. We propose a multilevel puncturing scheme that solves the problem of symbol-by-symbol adaptive puncturing and interleaving. Equivalent distance spectrum for variable rate symbol-by-symbol adaptive codes are introduced and analytical bounds on adaptive codes are derived that enable us to determine the optimal adaptation thresholds. Two operation modes, namely the constant throughput and the <b>constant</b> bit <b>error</b> <b>rate</b> (BER) controls, are proposed. It is found that there are significant gains relative to fixed-rate coding in terms of signal-to-noise ratio (SNR) and throughput. It is {{also found that the}} ABICM scheme is essentially not degraded when used with small interleaving depths. This makes the ABICM very suitable for real-time applications...|$|R
40|$|Dissolution {{rates were}} {{calculated}} {{for a range of}} grain sizes of anorthite and biotite dissolved under far from equilibrium conditions at pH 3, T = 20 degrees C. Dissolution rates were normalized to initial and final BET surface area, geometric surface area, mass and (for biotite only) geometric edge surface area. <b>Constant</b> (within <b>error)</b> dissolution <b>rates</b> were only obtained by normalizing to initial BET surface area for biotite. The normalizing term that gave the smallest variation about the mean for anorthite was initial BET surface area. In field studies, only current (final) surface area is measurable. In this study, final geometric surface area gave the smallest variation for anorthite dissolution rates and final geometric edge surface area for biotite dissolution rates. (c) 2005 Published by Elsevier B. V...|$|R
40|$|This thesis {{consists}} of three parts, each of independent interest, yet tied together by the problem of matching with mismatches. In the first chapter, we present a motivated exposition of a new randomized algorithm for indexed matching with mismatches which, for <b>constant</b> <b>error</b> (substitution) <b>rates,</b> locates a substring of length m within a string of length n faster than existing algorithms {{by a factor of}} O(m/ log(n)). The second chapter turns from this theoretical problem to an entirely practical concern: delta compression of executable code. In contrast to earlier work which has either generated very large deltas when applied to executable code, or has generated small deltas by utilizing platform and processor-specific knowledge, we present a naïve approach — that is, one which does not rely upon any external knowledge — which nevertheless constructs deltas of size comparable to those produced by a platformspecific approach. In the course of this construction, we utilize the result from the first chapter, although it is of primary utility only when producing deltas between very similar executables. The third chapter lies between the horn and ivory gates, being both highly interesting from a theoretical viewpoint and of great practical value. Using the algorithm for matching with mismatches from the first chapter, combined with error correcting codes, we give a practical algorithm for “universal” delta compression (often called “feedback-free file synchronization”) which can operate in the presence of multiple indels and a large number of substitutions. </p...|$|R
40|$|Many {{inference}} {{techniques for}} multivariate data analysis {{assume that the}} rows of the data matrix are realizations of independent and identically distributed random vectors. Such an assumption will be met, for example, if the rows of the data matrix are multivariate measurements {{on a set of}} independently sampled units. In the absence of an independent random sample, a relevant {{question is whether or not}} a statistical model that assumes such row exchangeability is plausible. One method for assessing this plausibility is a statistical test of row covariation. Maintenance of a <b>constant</b> type I <b>error</b> <b>rate</b> regardless of the column covariance or matrix mean can be accomplished with a test that is invariant under an appropriate group of transformations. In the context of a class of elliptically contoured matrix regression models (such as matrix normal models), I show that there are no non-trivial invariant tests if the number of rows is not sufficiently larger than the number of columns. Furthermore, I show that even if the number of rows is large, there are no non-trivial invariant tests that have power to detect arbitrary row covariance in the presence of arbitrary column covariance. However, we can construct biased tests that have power to detect certain types of row covariance that may be encountered in practice...|$|R
40|$|Second-order asymptotics of {{channel coding}} under a <b>constant</b> <b>error</b> {{constraint}} is discussed. The optimum second-order transmission <b>rate</b> with a <b>constant</b> <b>error</b> constraint ǫ is obtained {{by using the}} information spectrum method. We also clarify that the Gallager bound does not give the optimum evaluation in the second-order asymptotics...|$|R
40|$|A bit <b>error</b> <b>rate</b> test on a {{transceiver}} is {{accelerated by}} adding a phase offset to data phase encoding and decoding in the transceiver and by mapping bit <b>error</b> <b>rate</b> test results from an elevated <b>error</b> <b>rate</b> condition to a normal <b>error</b> <b>rate</b> condition for the transceiver. The elevated <b>error</b> <b>rate</b> is accomplished by adjusting the phase of the phase encoder and decoder with {{the value of the}} phase offset so that the encoded data transmission signal is not as robust against noise as it normally would be. Noise {{in the form of an}} interference signal is introduced during the transmission, and the bit <b>error</b> <b>rate</b> is measured after the receiver has decoded the signal. The bit <b>error</b> <b>rate</b> (BER) data with an elevated propensity for error is mapped against bit <b>error</b> <b>rate</b> data for normal operations. A mapping function is built to map BERE (bit <b>error</b> <b>rate</b> elevated) data�data from the elevated <b>error</b> <b>rate</b> condition for data encoding, to BERN (normal bit <b>error</b> <b>rate)</b> data�data from the normal <b>error</b> <b>rate</b> condition for data encoding. The BERE data is mapped to BERN data using the mapping function so that the BER performance of the transceiver may be measured using far fewer test sequences of digital bits. Georgia Institute Of Technolog...|$|R
