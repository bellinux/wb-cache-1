26|871|Public
50|$|Significant {{advances}} in lunar theory {{made by the}} Arab astronomer, Ibn al-Shatir (1304-1375). Drawing on the observation that {{the distance to the}} Moon did not change as drastically as required by Ptolemy's lunar model, he produced a new lunar model that replaced Ptolemy's crank mechanism with a double epicycle model that reduced the <b>computed</b> <b>range</b> of distances of the Moon from the Earth. A similar lunar theory, developed some 150 years later by the Renaissance astronomer Nicolaus Copernicus, had the same advantage concerning the lunar distances.|$|E
5000|$|If {{the signal}} being {{dithered}} is to undergo further processing, {{then it should}} be processed with a triangular-type dither that has an amplitude of two quantisation steps; for example, so that the dither values <b>computed</b> <b>range</b> from, say, −1 to +1, or 0 to 2. This is the [...] "lowest power ideal" [...] dither, in {{that it does not}} introduce noise modulation (which would manifest as a constant noise floor), and completely eliminates the harmonic distortion from quantisation. If a colored dither is used instead at these intermediate processing stages, then frequency content may [...] "bleed" [...] into other frequency ranges that are more noticeable, which could become distractingly audible.|$|E
40|$|Abstract: Many of ADAS {{applications}} such as pedestrian and vehicle detection are using stereo vision. By computing thewell-known disparity the range of objects ahead of the car can be determined. According to the <b>computed</b> <b>range,</b> {{it is possible to}} iden-tify whether this object is in a collision position or not. Based on the chain code algorithm a new disparity computation method is introduced. The proposed method enhances obviously the delivered disparity values as well as the object segmentation results by integrating the segmentation into the disparity assignment step. ...|$|E
40|$|In {{interval}} computations, usually, once we {{prove that}} a problem of <b>computing</b> the exact <b>range</b> is NP-hard, then it later {{turns out that the}} problem of <b>computing</b> this <b>range</b> with a given accuracy is also NP-hard. In this paper, we provide a general explanation for this phenomenon. Formulation of the problem. One of the main problems of interval computations is, given a function f(x 1, [...] ., xn) and n intervals xi, to <b>compute</b> the <b>range</b> f(x 1, [...] ., xn) of possible values of f when xi ∈ xi. In interval computations, many subclasses of this general problem are NP-hard: e. g., the problem of <b>computing</b> the <b>range</b> of a quadratic function, the problem of <b>computing</b> the <b>range</b> of the solution to the system of linear questions with linear coefficients, the problem of <b>computing</b> the <b>range</b> of variance with interval data, etc.; see, e. g., [1, 2]. Usually, once we prove {{that a problem}} of <b>computing</b> the exact <b>range</b> is NP-hard, then it later turns out that the problem of <b>computing</b> this <b>range</b> with a given accuracy is also NP-hard. In theory of computing in general, it is possible that a problem is NP-hard but its approximation is easy to solve; several packing and scheduling problems have this property; see, e. g., [3]. In this paper, we provide a general explanation why in interval computations, the introduction of approximations does not make th...|$|R
50|$|CUA {{was more}} than just an attempt to rationalise DOS {{applications}} — {{it was part of a}} larger scheme to bring together, rationalise and harmonise the overall functions of software and hardware across IBM's entire <b>computing</b> <b>range</b> from microcomputers to mainframes. This is perhaps partly why it was not completely successful.|$|R
40|$|We {{introduce}} {{new methods of}} equivalence checking and simulation based on <b>Computing</b> <b>Range</b> Reduction (CRR). Given a combinational circuit $N$, the CRR problem is to compute the set of outputs that disappear from the range of $N$ if a set of inputs of $N$ is excluded from consideration. Importantly, in many cases, range reduction can be efficiently found even if <b>computing</b> the entire <b>range</b> of $N$ is infeasible. Solving equivalence checking by CRR facilitates generation of proofs of equivalence that mimic a "cut propagation" approach. A limited version of such an approach has been successfully used by commercial tools. Functional verification of a circuit $N$ by simulation {{can be viewed as}} a way to reduce the complexity of <b>computing</b> the <b>range</b> of $N$. Instead of finding the entire range of $N$ and checking if it contains a bad output, such a <b>range</b> is <b>computed</b> only for one input. Simulation by CRR offers an alternative way of coping with the complexity of range computation. The idea is to exclude a subset of inputs of $N$ and <b>compute</b> the <b>range</b> reduction caused by such an exclusion. If the set of disappeared outputs contains a bad one, then $N$ is buggy. Comment: The difference of this version from the previous one (i. e. version number 2) is twofold. First, I improved the readability of the paper. Second, I removed the claim that equivalence checking by <b>computing</b> <b>range</b> reduction is noise-insensitive. The final result is indeed noise-insensitive but the presence of noise may drastically slow down an algorithm computing this resul...|$|R
40|$|Abstract. This {{paper offers}} a formal {{investigation}} of the measurement principle of time-of-flight (TOF) 3 D cameras using correlation of amplitude-modulated continuous-wave signals. These sen-sors can provide both depth maps and IR intensity pictures simultaneously and in real-time. We examine {{the theory of the}} data acquisition in detail. The variance of the range measurements is de-rived in a concise way and we show that the <b>computed</b> <b>range</b> follows an Offset Normal distribution. The impact of quantization of that distribution is discussed. All theoretically investigated errors like the behavior of the variance, depth bias, saturation and quantization effects are supported by experimental results...|$|E
30|$|During a pass of a {{satellite}} over an SLR station, the laser range measurements are collected. After the pass, the predicted range trend is fitted to the measured range values {{by adjusting the}} time bias and range bias estimations. As the next step, the range residuals are formed by calculating {{the difference between the}} observed and predicted (<b>computed)</b> <b>range</b> values, O-C. In the course of this process, fitting functions (orbital function or low degree polynomials) are used in order to remove the systematic trends from the distribution of the range residual data. The final step of the post-processing is the formation of the so-called normal points (NP).|$|E
40|$|Given that n voters report {{only the}} first r (1 ≤ r < m) ranks of their linear {{preference}} rankings over m alternatives, the likelihood of implementing Borda outcome is investigated. The information contained in the first r ranks is aggregated through a Borda-like method, namely the r-Borda rule. Monte-Carlo simulations are run to detect changes in the likelihood of r-Borda winner(s) {{to coincide with the}} original Borda winner(s) as a function of m, n and r. The voters ’ preferences are generated through the Impartial Anonymous and Neutral Culture Model, where both the names of the alternatives and voters are immaterial. It is observed that, for a given r, the likelihood of choosing the Borda winner decreases down to zero independent of n as m increases within the <b>computed</b> <b>range</b> of parameter values, 1 ≤ m, n ≤ 30. Fo...|$|E
40|$|This paper {{presents}} a statistical learning method for <b>computing</b> <b>range</b> data as an initial {{solution to the}} environment modeling problem {{in the context of}} mobile robotics. Unlike other methods that are based on a set of geometric primitives, our method <b>computes</b> dense <b>range</b> maps of locations in the environment using only intensity images and very limited amount of range data as an input. This is achieved by exploiting the following assumptions: 1) the observed range and intensity images are correlated and, 2) variations of pixels in the range and intensity images are related to the values elsewhere in the image(s). These variations can be efficiently captured by the neighborhood system of a Markov Random Field (MRF). Experimental results show the feasibility of our method...|$|R
40|$|One of {{the main}} {{problems}} of interval computations is <b>computing</b> the <b>range</b> of a given function over given intervals. It is known {{that there is a}} general algorithm for <b>computing</b> the <b>range</b> of computable functions over computable intervals. However, if we take into account that often in practice, not all possible combinations of the inputs are possible (i. e., that there are constraints), then it becomes impossible to have an algorithm which would always <b>compute</b> this <b>range.</b> In this paper, we explain that the main reason why range estimation under constraints is not always computable is that constraints may introduce discontinuity [...] and all computable functions are continuous. Specifically, we show that if we restrict ourselves to computably continuous constraints, the problem of range estimation under constraints remains computable...|$|R
40|$|A {{study was}} made of a {{polynomial}} filter for <b>computing</b> <b>range</b> rate information from CSM VHF range data. The filter's performance during the terminal phase of the rendezvous is discussed. Two modifications of the filter were also made and tested. A manual terminal rendezvous was simulated and desired accuracies were achieved for vehicles on an intercept trajectory, except for short periods following each braking maneuver when the estimated range rate was initially in error by {{the magnitude of the}} burn...|$|R
40|$|The {{record of}} planktonic {{foraminifer}} abundances at Site 662 {{during the late}} Pliocene (~ 1. 7 - 2. 1 Ma) was examined to determine variations in estimated sea-surface temperature (SST). We compared the results to SST estimates from a late Pleistocene record (~ 1. 5 - 200 ka) from nearby piston core RC 24 - 7. Within the primary orbital band (~ 20 - 100 k. y.), the cold-season responses of both equatorial Atlantic records are dominated by the precessional period, and the <b>computed</b> <b>range</b> of variability is quite similar. This {{is in contrast to}} the evolution of the dominant climatic response from 41 to 100 k. y. at high northern latitudes between the late Pliocene and the late Pleistocene. The orbital-band SST response in this region of greatest divergence in the equatorial Atlantic has not changed appreciably between the late Pliocene and the late Pleistocene...|$|E
40|$|International audienceIn {{this paper}} we perform an {{analytical}} and numerical study of Extreme Value distributions in discrete dynamical {{systems that have}} a singular measure. Using the block maxima approach described in Faranda et al. [2011] we show that, numerically, the Extreme Value distribution for these maps can be associated to the Generalised Extreme Value family where the parameters scale with the information dimension. The numerical analysis are performed on a few low dimensional maps. For the middle third Cantor set and the Sierpinskij triangle obtained using Iterated Function Systems, experimental parameters show a very good agreement with the theoretical values. For strange attractors like Lozi and Hénon maps a slower convergence to the Generalised Extreme Value distribution is observed. Even in presence of large statistics the observed convergence is slower if compared with the maps which have an absolute continuous invariant measure. Nevertheless and within the uncertainty <b>computed</b> <b>range,</b> the results are in good agreement with the theoretical estimates...|$|E
40|$|This paper aims to empirically {{investigate}} {{the extent of}} difficulty in implementing Borda outcomes when n voters report only the first r ranks of their linear preference rankings over m alternatives. The information contained in the first r ranks is aggregated through a Bordalike method, namely the r-Borda rule. Monte-Carlo experiments are run to detect changes in {{the likelihood of the}} r-Borda winner to coincide with the original Borda winner as a function of m, n and r. The voters ' preferences are generated through Impartial Anonymous and Neutral Culture model where both the names of the alternatives and voters are immaterial. For a given r, the likelihood of choosing the Borda winner decreases towards zero independently of n as m increases within the <b>computed</b> <b>range</b> of parameter values, 1 ≤m,n≤ 30. We show empirically that this probability is inversely proportional to m, and determine the constant of proportionality for two different types of likelihood that we consider. 1...|$|E
40|$|The {{interest}} of organisations in becoming more environmentally sustainable by adopting green ICT solution is constantly growing. More and more initiatives {{have been proposed}} on energy efficiency <b>computing,</b> <b>ranging</b> from hardware to software solutions [10]. Accordingly, energy efficiency has become an important issue for industry as energy consumption of ICT systems grows and related energy footprint into realise important financial savings while decreasing {{the environmental impact of}} their systemsâ€”in terms of, for example, greenhouse gas emissions, e-waste and heat generation [9]...|$|R
40|$|This paper {{examines}} a novel method we {{have developed}} for <b>computing</b> <b>range</b> data {{in the context of}} mobile robotics. Our objective is to <b>compute</b> dense <b>range</b> maps of locations in the environment, but to do this using intensity images and very limited range data as input. We develop a statistical learning method for inferring and extrapolating range data from a combination of a single video intensity image and a limited amount of input range data. Our methodology is to compute the relationship between the observed range data and the variations in the intensity image, and use this to extrapolate new range values. These variations can be efficiently captured by the neighborhood system of a Markov Random Field (MRF) without making any strong assumptions about the kind of surfaces in the world. Experimental results show the feasibility of our method...|$|R
40|$|This paper {{describes}} a Vision-based Adaptive Cruise Control (ACC) system {{which uses a}} single camera as input. In particular we discuss how to <b>compute</b> <b>range</b> and range-rate from a single camera and discuss how the imaging geometry affects the range and range rate accuracy. We determine the bound on the accuracy given a particular configuration. These bounds in turn determine what steps {{must be made to}} achieve good performance. The system has been implemented on a test vehicle and driven on various highways over thousands of miles...|$|R
40|$|Abstract — A {{successful}} {{grasp of}} an object can be guaranteed when the object can never escape from the surrounding fingers during the entire grasping execution. Ability to capture an object clearly contributes to the robustness and success of grasping tasks. Object concavity is a useful geometric property allowing objects to be captured with only few fingers. In particular, certain concave objects may be captured using two fingers by appropriately placing the fingers close to some pair of opposite concave sections. Based on this intuitive idea, we {{address the problem of}} capturing concave polygonal objects with two disc-shaped fingers. We present an approach for computing a range of distance such that the two fingers can move away from a given immobilizing grasp but still prevent the object from escaping; when within this <b>computed</b> <b>range,</b> it is guaranteed under the frictionless contact assumption that the fingers can move toward each other to bring the object to the given immobilizing grasp. The proposed approach is implemented and preliminary result is presented. I...|$|E
40|$|International audiencePolynomial ranges are {{commonly}} used for numerically solving polynomial systems with interval Newton solvers. Often ranges are computed using the convex hull property of the tensorial Bernstein basis, which is exponential size in the number n of variables. In this paper, we consider methods to compute tight bounds for polynomials in n variables by solving two linear programming problems over a polytope. We formulate a polytope defined as the convex hull of the coefficients {{with respect to the}} tensorial Bernstein basis, and we formulate several polytopes based on the Bernstein polynomials of the domain. These Bernstein polytopes can be defined by a polynomial number of halfspaces. We give the number of vertices, the number of hyperfaces, and the volume of each polytope for n= 1, 2, 3, 4, and we compare the <b>computed</b> <b>range</b> widths for random n-variate polynomials for n⩽ 10. The Bernstein polytope of polynomial size gives only marginally worse range bounds compared to the range bounds obtained with the tensorial Bernstein basis of exponential size...|$|E
40|$|Abstract. This {{paper offers}} a formal {{investigation}} of the measurement principle of time-of-flight (TOF) 3 D cameras using correlation of amplitude-modulated continuous-wave signals. These sensors can provide both depth maps and IR intensity pictures simultaneously and in real-time. We examine {{the theory of the}} data acquisition in detail. The variance of the range measurements is derived in a concise way and we show that the <b>computed</b> <b>range</b> follows an Offset Normal distribution. The impact of quantization of that distribution is discussed. All theoretically investigated errors like the behavior of the variance, depth bias, saturation and quantization effects are supported by experimental results. Copyright 2008 Society of Photo-Optical Instrumentation Engineers. This paper will be published in Optical Engineering (OE) and is made available as an electronic preprint with permission of SPIE. One print or electronic copy may be made for personal use only. Systematic or multiple reproduction, distribution to multiple locations via electronic or other means, duplication of any material in this paper for a fee or for commercial purposes, or modification of the content of the paper are prohibited. 1...|$|E
40|$|Context-aware {{middleware}} encompasses uniform abstractions {{and reliable}} services for common operations, supports {{for most of}} the tasks involved in dealing with context, and thus simplifying the development of context-aware applications. In this paper 1, we address some key issues of a middleware for context-aware ubiquitous <b>computing,</b> <b>ranging</b> from design considerations of a unified sensing framework, formal modeling and representation of the real world, pluggable reasoning engines for high-level contexts, and context delivery-runtime service composition mechanisms. Our implementation experience indicates that a comprehensive approach throughout the system layers results in a flexible and reusable middleware architecture...|$|R
50|$|End-user <b>computing</b> can <b>range</b> in {{complexity}} from users simply clicking {{a series}} of buttons, to writing scripts in a controlled scripting language, {{to being able to}} modify and execute code directly.|$|R
5000|$|EA = time to <b>compute</b> {{effective}} address, <b>ranging</b> from 5 to 12 cycles.|$|R
40|$|Image geo-localization is an {{important}} problem with many applica-tions such as augmented reality and navigation. The most common ways to geo-localize an image are to use its meta-data such as GPS or to match it against a geo-tagged database. When neither of those is available, {{it is still possible}} to apply shadow analysis to determine the camera heading for outdoor images. This could be useful prun-ing the search space in geo-localization applications, for example by removing roads with incompatible orientations from a database such as Open Street Map. In this paper, we develop a novel interactive method for deducing the global heading of a query image using the shadows in it. We start by constructing a model of the sun-earth system to determine all shadows possible at a given approximate lat-itude, and compare shadows within the query to those possible under the model to determine the range of possible headings. We demon-strate this on 54 query images with known ground truth, and show that in 52 cases the ground truth lies in the <b>computed</b> <b>range.</b> 1...|$|E
30|$|The nmax was {{varied from}} 2.2 to 2.9, while the miminum {{refractive}} index (nmin) was adjusted {{to keep the}} OT constant as (a) 24, (b) 25, and (c) 26 μ m. The <b>computed</b> <b>range</b> of nmax was limited by the experimental capability to obtain high refractive indices (keeping PS as a possible reference material) and the adjusted values of nmin to keep the same OT of all the structures. Figure 1 a,b,c demonstrates that for each OT, one can find a particular value of nmax at which the profile corresponding to the higher value of OPBG changes. For example, in Figure 1 b, the largest OPBG for nmax range of 2.25 to 2.45, the Bragg-type profile {{has to be the}} preferred choice. For 2.45 < nmax < 2.57, the sinusoidal profile has the largest OPBG, but the Gaussian profile prevails for nmax > 2.57. A similar behavior is observed for higher OTs (Figure 1 c). For the OT of 24 μ m, the Bragg-type profile fails to demonstrate any OPBG (Figure 1 a). Although the Gaussian structure shows the largest OPBG, the corresponding value of nmax is also very high.|$|E
40|$|TOPEX/POSEIDON (T/P) is a {{joint mission}} of United States' National Aeronautics and Space Administration (NASA) and French Centre National d'Etudes Spatiales (CNES) design {{launched}} August 10, 1992. It carries two radar altimeters which alternately share a common antenna. There are two project designated verification sites, a NASA site off the coast at Pt. Conception, CA and a CNES site near Lampedusa Island in the Mediterranean Sea. Altimeter calibration and validation for T/P is performed over these highly instrumented sites by comparing the spacecraft's altimeter radar range to <b>computed</b> <b>range</b> based on in situ measurements which include the estimated orbit position. This paper presents selected results of orbit determination over each of these sites to support altimeter verification. A short arc orbit determination technique is used to estimate a locally accurate position determination of T/P from less than one revolution of satellite laser ranging (SLR) data. This technique is relatively insensitive to gravitational and non-gravitational force modeling errors and is demonstrated by covariance analysis and by comparison to orbits determined from longer arcs of data and other tracking data types, such as Doppler Orbitography and Radiopositioning Integrated by Satellite (DORIS) and Global Positioning System Demonstration Receiver (GPSDR) data...|$|E
40|$|An {{expression}} was derived {{for the time}} transformation t - tau, where t is coordinate time {{in the solar system}} barycentric space-time frame of reference and tau is proper time obtained from a fixed atomic clock on earth. This transformation is suitable for use in the computation of high-precision earth-based range and Doppler observables of a spacecraft or celestial body located anywhere in the solar system; it can also be used in obtaining computed values of very long baseline interferometry data types. The formulation for <b>computing</b> <b>range</b> and Doppler observables, which is an explicit function of the transformation t - tau, is described briefly...|$|R
40|$|This article {{presents}} alternative way of distance measurement to the preceding vehicle. Investigations were {{conducted for the}} purpose of assessing the possibility of using automatic determination of a car plate size in a photograph picture, and basing on that <b>computing</b> <b>range</b> to a vehicle. The first part describes the research methodology relating to angular measurements made with a still camera, The second part includes the method employed to automatically detect area occupied by a registration plate in a picture of a vehicle. The final part contains the mechanism of calculating distance to a car and the results of the investigation which justify using described method as a tool for range measurement...|$|R
5000|$|The studentized <b>range</b> <b>computed</b> {{from a list}} x1, ..., xn {{of numbers}} is given by the {{formulas}} ...|$|R
40|$|In {{this paper}} we perform an {{analytical}} and numerical study of Extreme Value distributions in discrete dynamical {{systems that have}} a singular measure. Using the block maxima approach described in Faranda et al. [2011] we show that, numerically, the Extreme Value distribution for these maps can be associated to the Generalised Extreme Value family where the parameters scale with the information dimension. The numerical analysis are performed on a few low dimensional maps. For the middle third Cantor set and the Sierpinskij triangle obtained using Iterated Function Systems, experimental parameters show a very good agreement with the theoretical values. For strange attractors like Lozi and Hènon maps a slower convergence to the Generalised Extreme Value distribution is observed. Even in presence of large statistics the observed convergence is slower if compared with the maps which have an absolute continuous invariant measure. Nevertheless and within the uncertainty <b>computed</b> <b>range,</b> the results are in good agreement with the theoretical estimates. The existence of extreme value laws for dynamical systems preserving an absolutely continuous invariant measure or a singular continuous invariant measure has been re-cently proven if strong mixing properties or exponential hitting time statistics on balls are satisfied. In our previous work we have shown that there exists an algorithmic way 1 a...|$|E
40|$|Abstract: The Changjiang Estuary nearby Shanghai {{is one of}} {{the largest}} and the most {{complicated}} estuaries in China. The flow exhibits obvious 3 D structure in the Changjiang Estuary due to the complex topography, tide and runoff, and the pollutant transportation is also complicated in the region. The south branch of the Changjiang Estuary is the main outfall channel to the East China Sea, and the runoff of south branch is about 95 percent of total runoff of the Changjiang Estuary. Based on POM Model, the three-dimensional flow and mass transportation mathematical model with an orthogonal curvilinear coordinate in the horizontal direction and sigma coordinate in the vertical direction for the south branch of the Changjiang Estuary is established in this paper. The basic equations are discretized by the finite difference method and the process splitting skill (the numerical computational process is divided to external mode and internal mode) is applied in this model. The <b>computed</b> <b>range</b> in south branch of the Changjiang Estuary is 91. 70 kilometer in length and 1328. 81 km 2 in area. The three dimensional flow field and polluted scope by Shidongkou waste discharge outlet of Shanghai city are simulated, and the computational results mimic the observed data well. This work is benefited for guidance of waste pollution control in the Changjiang Estuary...|$|E
40|$|Continuous {{tracking}} of geodetic satellites using the Satellite Laser Ranging technique has provided unprecedented opportunity in long- to medium-wavelength gravity field modelling. Numerous gravity field {{models have been}} derived from such observations and have been made freely available to the science community for research purpose. The accuracy {{of most of the}} latest gravity field models in terms of precise orbit determination is currently at cm level. Improvement in the Earth gravity field modelling is anticipated as quantitative and qualitative data (in particular from low earth orbit satellites) become available in the future. Such expectations require that the accuracy and precision of existing gravity field models be assessed. The validation of gravity field models in terms of satellite orbit determination is often based on the difference between the observed and <b>computed</b> <b>range.</b> The resulting range residuals are considered an important index when determining the accuracy of the gravity models and hence the satellite orbits. In this contribution we investigate the general improvement in gravity field modelling over a period of 15 years. The orbit accuracy of twelve gravity field models (both satellite-only and combined models) were assessed by analysing seven months of data from LAGEOS 1 and 2 using HartRAO analysis software. Results show that the gravity field models developed over the years have improved by at least a factor of 2 since 1990, considering improvement in O-C residuals. [URL]...|$|E
40|$|Query {{imprecision}} may be frequently {{caused by}} the nature of many applications including location based services. The problem of capably <b>computing</b> <b>range</b> aggregates in a multidimensional space is examined in this paper when the query location is unsure. Specifically, for a query point P whose location is uncertain and a set X of points in a multi-dimensional space, we want to calculate the aggregate over the subset S of X such that for q each  ∈ S, P has at least probability θ within the distance γ to q.  Novel efficient techniques are proposed to solve the problem following the filtering-and-verification paradigm. Especially, to efficiently and proficiently remove data points from verification two novel filtering techniques are propose...|$|R
30|$|The Timedomain PulsON 220 UWB sensors [16], {{used for}} the experiment, operate with a center {{frequency}} of 4.7 GHz and a bandwidth (10 dB radiated) of 3.2 GHz at - 12.8 dB EIRP. Pulse repetition frequency was 9.6 MHz. The measured quantity is the distance estimate between the sensor nodes at a certain sampling rate (500 ms in our case). These sensors are interfaced via Ethernet using the user datagram protocol (UDP) controlled from a laptop as shown in Figure 2. The locations of these sensor nodes were accurately measured. Note that all nodes were located at a same height of 1.13 m, with the ceiling being at 3 m. Timedomain PulsON 220 UWB node <b>computed</b> a <b>range</b> estimate using a proprietary time-of-arrival (TOA) estimator, whose implementation is not public. The experiment of <b>computing</b> <b>ranges</b> between robot's node {{and the rest of}} nodes was performed 700 times per pair, composing the database described in [11]. Notice that some nodes were located inside neighboring rooms and hence those measurements were in non-line-of-sight (NLOS) conditions for the whole (or part of the) trajectory of the robot.|$|R
40|$|One of {{the main}} {{problems}} of interval computations is to <b>compute</b> the <b>range</b> y of the given function f(x 1, [...] ., xn) under interval uncertainty. Interval computations started with the invention of straightforward interval computations, when we simply replace each elementary arithmetic operation in the code for f with the corresponding operation from interval arithmetic. In general, this technique only leads to an enclosure Y ⊇ y for the desired range, but in the important case of single use expressions (SUE), in which each variable occurs only once, we get the exact range. Thus, for SUE expressions, there exists a feasible (polynomial-time) algorithm for <b>computing</b> the exact <b>range.</b> We show that in the complex-valued case, <b>computing</b> the exact <b>range</b> is NP-hard even for SUE expressions. Moreover, it is NP-hard even for such simple expressions as the product f(z 1, [...] ., zn) = z 1 · [...] . · zn...|$|R
