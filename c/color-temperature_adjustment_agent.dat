0|35|Public
50|$|Another {{commonly}} used treatment is cement based solidification and stabilization. Cement is used {{because it can}} treat a range of hazardous wastes by improving physical characteristics and decreasing the toxicity and transmission of contaminants. The cement produced is categorized into 5 different divisions, depending on its strength and components. This process of converting sludge into cement might include the addition of pH <b>adjustment</b> <b>agents,</b> phosphates, or sulfur reagents to reduce the settling or curing time, increase the compressive strength, or reduce the leach ability of contaminants.|$|R
3000|$|... is {{a complex}} of {{institutions}} and norms which secure, {{at least for a}} certain period, the <b>adjustment</b> of individual <b>agents</b> and social groups to the overarching principle of the accumulation regime (Hirst and Zeitlin [...]...|$|R
30|$|The initial {{steady state}} has a DB system. In period one, unexpectedly, an economy shifts {{gradually}} towards a DC system. The gradual transition means that for all cohorts living already before t= 1, initial capital is computed and implied in the DC system but the pensions allocated according to the DB rules are honored without any <b>adjustments.</b> Furthermore, <b>agents</b> {{who were at the}} age of j= 30 or older at t= 1 also continue to receive DB pension benefits. 7 The obligatory contribution rate τ is kept the same as in the initial steady state and DB system.|$|R
40|$|We review {{theoretical}} and empirical analyses {{of the costs of}} inflation. Part 2 of the paper examines microeconomic models in which inflation is perfectly anticipated, and viewed as the only distortion in the economy or as one of many distortionary taxes, {{which may or may not}} be chosen optimally. Part 3 turns to stochastic models with a more macroeconomic orientation, in which inflation is imperfectly perceived, or where real costs of price <b>adjustment</b> cause <b>agents</b> not to adjust fully in the presence of inflation. Part 4 discusses empirical work, which largely focuses on the relationship between the level of inflation, its variability, its unpredictability, and variation in relative prices. Fluctuations; Inflation; Welfare Costs...|$|R
40|$|This paper studies a model where {{consumption}} has two components, {{one that}} is costlessly adjustable and one that involves an <b>adjustment</b> cost. <b>Agents</b> have time-separable neoclassical preferences over the two components of consumption. We show that aggregating over a heterogeneous population of such agents implies dynamics identical to those of a representative consumer economy with habit formation utility. In particular, aggregate consumption is a slow-moving average of past consumption levels, and risk aversion is ampli…ed because the marginal utility of wealth is determined by excess consumption over the prior commitment level. In certain cases, the representative consumer’s preferences and consumption dynamics coincide exactly with the exponential form of habit proposed by Constantinides (1990). Hence, consumption commitments provide simple, non-psychological micro-foundations for “habit. ...|$|R
40|$|Medications {{to treat}} peptic ulcer disease are used widely {{and may have}} adverse effects on renal function. Similarly, renal {{dysfunction}} may alter the pharmacokinetics of this diverse group of medications resulting in dosage <b>adjustments.</b> The older <b>agents,</b> antacids and sucralfate, allow absorption of cations (calcium, magnesium and aluminum) which may result in toxicity. Newer medications (H 2 blockers and omeprazole) appear to have fewer side effects and be better tolerated with appropriate dosage adjustments...|$|R
40|$|This paper {{introduces}} {{the idea of}} “robust political economy. ” In the context of political economic systems, “robustness” refers to a political economic arrangement's ability to produce social welfare-enhancing outcomes {{in the face of}} deviations from ideal assumptions about individuals' motivations and information. Since standard assumptions about complete and perfect information, instantaneous market <b>adjustment,</b> perfect <b>agent</b> rationality, political actor benevolence, etc., rarely, if ever actually hold, a realistic picture and accurate assessment of the desirability of alternative political economic systems requires an analysis of alternative systems' robustness. The Mises-Hayek critique of socialism forms the foundation for investigations of robustness that relax ideal informational assumptions. The Buchanan-Tullock public choice approach complements this foundation in forming the basis for investigations of robustness that relax ideal motivational assumptions. Copyright Springer Science + Business Media, Inc. 2006 Austrian economics, Public choice, Robustness,...|$|R
40|$|In {{this thesis}} work {{oxidative}} dehydrogenation (ODH) of ethane to ethane over MoV oxide catalyst was investigated. The {{influence of the}} preparation techniques and different reaction conditions were studied thoroughly. It {{was found that the}} precipitation method for the catalyst preparation using variable pH produces a more active catalyst at pH values of 3 to 3. 5. Slurry temperature and calcination temperature are also very important parameters which affect the selectivity pattern of the products. This selectivity pattern was found to be further influenced by reaction temperature, pressure, GHSV and ethane-oxygen ratio in the feed. The influence of the V: Mo ratio on the performance of the catalyst for the ODH was investigated by several characterisation techniques, such as BET, XRD, XPS, TEM, SEM, EDX coupled with catalytic performance tests in a fixed bed reactor. The optimum V: Mo ratio was found to be 0. 25 : 1 (i. e., MoV 0. 40). At this ratio, the oxidation state of vanadium with respect to total vanadium concentration (V 5 +/Vtotal) is at an optimum in terms of the adsorption strength of the desired products. It was further fine-tuned by investigating the influence of reaction conditions. An improvement on the most active MoV oxide catalyst for the ODH reaction was developed with the addition of oxalic acid as the vanadium dissolution and pH <b>adjustment</b> <b>agent.</b> Addition of oxalic acid influenced the catalytic properties {{in a variety of ways}} as observed from characterisation and reaction results. Addition of either a smaller amount or an excess amount compared with the optimal amount has determental impact on the activity of the catalyst. Further catalytic activities were tested by the addition of different types of supports (e. g., ZrO 2, TiO 2, Nb 2 O 5, SiO 2, and AlO 3) into the MoV oxide catalytic system. The alumina support was extensively tested with different amounts onto the base MoV oxide for the ethane ODH to ethane...|$|R
40|$|The {{document}} attached {{has been}} archived {{with permission from}} the Australian Dental Association. An external link to the publisher’s copy is included. BACKGROUND: This study investigated {{the extent to which}} a coating of 10 % silver fluoride (AgF) on discs of glass jonomer cements (GIGs) would enhance the release of fluoride ion into eluting solutions at varying pH. MATERIALS AND METHODS: Forty discs each of Fuji LX, Fuji VII and of Vitrebond were prepared in a plastic mould. Twenty discs of each material were coated for 30 seconds with a 10 % solution of AgF. Five discs each of coated and uncoated material were placed individually in 4 m 1 of differing eluant solutions. The eluant solutions comprised deionized distilled water (DDW) and three separate acetate buffered solutions at pH 7, pH 5 and pH 3. After 30 minutes the discs were removed and placed in five vials containing 4 m 1 of the various solutions for a further 30 minutes. This was repeated for further intervals of time up to 216 hours, and all eluant solutions were stored. Fluoride concentrations in the eluant solutions were estimated using a fluoride specific electrode, with TISAB IV as a metal ion complexing and ionic concentration <b>adjustment</b> <b>agent.</b> Cumulative fluoride release patterns were determined from the incremental data. RESULTS: The coating of AgF greatly enhanced the level of fluoride ion release from all materials tested. Of the uncoated samples, Vitrehond released the greater concentrations of fluoride ion, followed by Fuji VII. However, cumulative levels of fluoride released from coated samples of the GICs almost matched those from coated Vitrebond. CONCLUSIONS: It was concluded that a coating of 10 % AgF on GICs and a resin modified GIC greatly enhanced the concentration of fluoride released from these materials. This finding might be applied to improving protection against recurrent caries, particularly in high caries risk patients, and in the atraumatic restorative technique (ART) of restoration placement...|$|R
40|$|Software agents "live" in {{changing}} environments. Perception {{and actions of}} agents need to adapt dynamically to new situations. This paper is concerned with meta-agent protocols, an approach to support the modular and portable implementation of various kinds of agent systems. Meta-agent protocols are derived from object-based reflective systems that allow access {{to the state and}} structure of a program during its execution. A meta level interface to the internal representation of agents can provide support for introspection and <b>adjustment</b> of <b>agents.</b> Meta-agent protocols result in a clear separation between application level and meta level (e. g. dynamic communication protocols, dynamic modification of behaviour, fault tolerance, monitoring, dynamic performance optimization) in agent systems which leads to modular as well as portable application components. 1 Introduction The Internet is a dynamic and distributed system. It is constructed from open services built around a standard commu [...] ...|$|R
40|$|Message {{passing by}} means of public key {{encryption}} is {{described in terms of}} doxastic dynamic logic. A secret message from a to b can have as eect that b learns something new from a, but it can also cause a change in the real world, when the contents of the message forces b to cease trusting a. As a tool for analysing secret message passing (and much else besides) we develop a framework that allows changes of states conditioned by beliefs about those states. 1 Introduction We present a model for describing how changes in the world get reected in multi-agent doxastic settings. One of the applications is the description of the results of message passing, where messages can be public announcements, limited access announcements, or secret communications. Message exchanges can result in belief updates, but also in changes of attitude towards the sender. Our model can also deal with <b>adjustments</b> of <b>agents</b> to a changing reality, collaborations between agents to achieve common goals, and much [...] ...|$|R
40|$|Repeated {{measurements}} designs, occur {{frequently in}} the assessment of exposure to toxic chemicals. This thesis deals with the possibilities of using mixed effects models for occupational exposure assessment and in the analysis of exposure response relationships. The model enables simultaneous estimation of both the variance components of exposure (between- and within-subject) and the unbiased regression coefficients for determinants of exposure. Implications are relevant to grouping strategies, hazard control, risk assessment and over-exposure assessment. In an Israeli cohort of industry workers sampled over a year, the geometric standard deviations representing variation of exposure between workers (after <b>adjustment</b> for <b>agent</b> and factory) and within workers were 3. 1 and 3. 0, respectively. These values may be used, as crude estimates of exposure variability to obtain an interval estimate of mean exposure. In studies among Dutch rubber manifactoring workers exposed to inhalable particulate and rubber fumes and pig farmers exposed to bacterial endotoxins, exposure determinants reduced the random between-worker variance estimators between 59 - 100...|$|R
40|$|Abstract. The visual {{design of}} virtual agents {{presents}} developers {{with a very}} large number of choices. We conducted a series of studies using Amazon’s Mechanical Turk that demonstrate that there are no design universals for characters, optimal design of character proportion and rendering style depends on the task domain and user characteristics. Specifically, we found these <b>adjustments</b> to an <b>agent’s</b> appearance directly effected how users rated it based on whether it was discussing social or medical content. The results of this research aim to help create visual guidelines for the development of domain specific virtual agents...|$|R
40|$|This paper {{focuses on}} the {{delegation}} of bank managers of lending decisions to their agents typically subordinate employees of the bank. We assume that agents may base their decisions about lending to borrowers on decisions other banks have made about these same borrowers. Then we show that there exist some lazy or negligent agents who neither directly monitor the borrower nor imitate the other banks if managers use relative performance evaluations as incentive schemes. In addition, it is shown that the learning or <b>adjustment</b> process of <b>agents</b> exhibits cyclical dynamics. (Copyright: Elsevier) relative performance evaluation, imitation, negligence, random matching game, evolutionary game theory...|$|R
40|$|Software agents “live” in {{changing}} environments. Perception {{and actions of}} agents need to adapt dynamically to new situations. This paper is concerned with meta-agent protocols, an approach to support the modular and portable implementation of various kinds of agent systems. Meta-agent protocols are derived from object-based reflective systems that allow access {{to the state and}} structure of a program during its execution. A meta level interface to the internal representation of agents can provide support for introspection and <b>adjustment</b> of <b>agents.</b> Meta-agent protocols result in a clear separation between application level and meta level (e. g. dynamic communication protocols, dynamic modification of behaviour, fault tolerance, monitoring, dynamic performance optimization) in agent systems which leads to modular as well as portable application components. Collaboration between Human and Artificial Societies Collaboration between Human and Artificial Societies Look Inside Chapter Metrics Downloads 88 Provided by Bookmetrix MyCopy Softcover Edition 24. 99 EUR/USD/GBP/CHF Reference tools Export citation Add to Papers Other actions About this Book Reprints and Permissions Share Share this content on Facebook Share this content on Twitter Share this content on LinkedI...|$|R
40|$|The sharp raise of {{the price}} of {{agricultural}} commodities between 2006 and 2008 seems to have a rationalization that goes beyond the mere interaction between supply and demand. Data evidence suggests that financial factors, rather than real determinants, {{played an important role in}} determining the dynamics of agricultural commodity prices. In particular, there seems to be a common source underlying food price changes and the financial markets dynamics. Evidence based on principal components supports the view that large fluctuations of food commodity prices can be related to portfolios <b>adjustments</b> of financial <b>agents.</b> We find robust evidence of a strong inverse correlation between financial markets’ returns and the movements of food commodity prices. Moreover, such an inverse relationship has clearly emerged during the recent financial crisis. ...|$|R
40|$|A {{system can}} be viewed from {{different}} perspectives, each focusing on a specific aspect such as availability, performance, security. Configurations reflect the manageable resources of the system, their attributes and organization which are necessary {{for the management of}} the system for each aspect. Thus, for management purposes a system is generally described through various partial configurations (also known as configuration fragments). To form a consistent system con-figuration, these independently developed configuration fragments need to be integrated together. The integration of configuration fragments is a challenging task. This is mainly due to over-lapping entities (different logical representations of the same system resource) in the configuration fragments and/or complex relationships among the entities of the different configuration fragments. At runtime the system may be reconfigured to meet certain/new requirements or in response to performance degradations. These changes may lead to inconsistency as some changes may violate the constraints between entities. Maintaining the consistency and adjusting the system configuration at runtime is another challenging task. In our research, we propose to handle these two important issues in an integrated manner. We define a model-based framework for configuration management. We use the Unified Modeling Language (UML) and its profiling mechanism for representing the configuration fragments. Using model weaving and model trans-formation techniques, we propose a solution for the integration of configuration fragments targeting specific system properties. To handle runtime changes, we propose a configuration validation and adjustment solution to check and preserve the consistency of the system configuration. We introduce a partial validation technique in which the runtime reconfigurations are checked against a reduced set of consistency rules instead of the complete set of rules and the reconfigurations are applied only if they are safe, i. e. they preserve the configuration consistency. For handling the changes that violate the consistency rules, we propose an adjustment technique to automatically resolve (if possible) the inconsistencies. This is achieved by propagating the changes in the configuration according to the system constraints following the possible impacts of the configuration entities on each other. Some heuristics are used to reduce the complementary changes and to limit the propagation. We evaluate the complexity of our adjustment technique and conduct experiments to evaluate its efficiency. The Service Availability Forum middleware is used as an application domain in the examples throughout this thesis; however the proposed solutions are applicable in more general settings. We present proofs of concepts using different technologies. We use the Eclipse Modeling Framework (EMF) and Papyrus for implementing the UML profiles. The Atlas Model Weaver (AMW) and Atlas Transformation Language (ATL) are used to integrate the configuration fragments, and we also use the APIs of the Object Constraint Language (OCL) in the Eclipse environment and the Microsoft Z 3 constraint solver to develop a prototype tool of our partial validation and <b>adjustment</b> <b>agents...</b>|$|R
50|$|Clark {{points out}} here that the latter {{strategy}} of catching the ball {{as opposed to the}} former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action <b>adjustment</b> between the <b>agent</b> and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.|$|R
40|$|We {{investigate}} American {{options in}} a multiple prior setting of continuous time and determine optimal exercise strategies form {{the perspective of}} an ambiguity averse buyer. The multiple prior setting relaxes the presumption of a known distribution of the stock price process and captures the idea of incomplete information of the market data leading to model uncertainty. Using the theory of (reflected) backward stochastic differential equations {{we are able to}} solve the optimal stopping problem under multiple priors and identify the particular worst-case scenario in terms of the worst-case prior. By means of the analysis of exotic American options we highlight the main difference to classical single prior models. This is characterized by a resulting endogenous dynamic structure of the worst-case scenario generated by model <b>adjustments</b> of the <b>agent</b> due to particular occurring events that change the agent’s beliefs. optimal stopping for exotic American options, uncertainty aversion, multiple priors, robustness, (reflected) BSDEs...|$|R
40|$|This paper {{characterizes the}} optimal labor income taxes in an {{environment}} where individual labor supply choices are subject to <b>adjustment</b> frictions. <b>Agents</b> incur a fixed cost of adjusting their hours of work in response to changes in their idiosyncratic wages or their tax rates. This fixed cost {{can be thought of as}} the cost of searching for a new job in an economy where hours are constrained within the firm. I derive a formula that characterizes the optimal long-run progressive tax schedule in this economy. Adjustment frictions generate endogenously an extensive margin of labor supply conditional on participation. In addition to the standard intensive margin disincentive effects of taxes, the optimal schedule takes into account their effects on the option value of adjusting hours of work, and therefore depends on several new elasticities and marginal social welfare weights. I then evaluate the quantitative magnitude of these novel theoretical effects and show that for a given intensive margin labor supply elasticity, the optimal long-run tax schedule is less progressive than a frictionless model would predict, because an increase in progressivity raises the dispersion of individual incomes around their desired values. The welfare miscalculations by wrongly assuming a frictionless economy can be large, and are decreasing in the size of the intensive margin labor income elasticity. The insights of this paper apply more broadly to models where fixed costs interact with non-linear policy instruments to yield long-run aggregate effects...|$|R
40|$|This paper {{presents}} a new framework for analyzing and designing no-regret algorithms for dynamic (possibly adversarial) systems. The proposed framework generalizes the popular online convex optimization framework and extends {{it to its}} natural limit allowing it to capture a notion of regret that is intuitive for more general problems such as those encountered in game theory and variational inequalities. The framework hinges on a special choice of a system-wide loss function we have developed. Using this framework, we prove that a simple update scheme provides a no-regret algorithm for monotone systems. While previous results in game theory prove individual agents can enjoy unilateral no-regret guarantees, our result proves monotonicity sufficient for guaranteeing no-regret when considering the <b>adjustments</b> of multiple <b>agent</b> strategies in parallel. Furthermore, to our knowledge, {{this is the first}} framework to provide a suitable notion of regret for variational inequalities. Most importantly, our proposed framework ensures monotonicity a sufficient condition for employing multiple online learners safely in parallel. Comment: 23 pages, 6 figure...|$|R
40|$|This paper {{presents}} a new quarterly macroeconometric {{model of the}} Belgian economy. It is intended to contribute to existing analytical work covering the specific transmission mechanisms of the euro area monetary policy in the Belgian economy. It also contributes to the forecast exercises and to their risk analysis. Finally it {{is also used to}} analyse the consequences of specific Belgian shocks. The model is small-scale and based on recent macroeconomic theory. The model's dynamics not only allow for the lagged <b>adjustments</b> from economic <b>agents</b> due to transaction costs to be taken into consideration, but also for agents to anticipate future developments and policy reactions. In simulations, expectation formation can be assumed either to be model consistent or to be generated by VAR-based extrapolations. On the basis of a few diagnostic simulations it is shown {{that in the long run}} the model converges to its steady state, defined by the underlying economic theory. 2 WORKING PAPER No. 4 - MA [...] ...|$|R
40|$|This paper {{investigates the}} {{implications}} of an intertemporally dependent demand structure {{in the market for}} an exhaustible resource. The demand structure is derived endogenously in a partial equilibrium model where price-taking users combine the resource with capital to produce final output. It is shown that, when demand for the resource is modelled explicitly as a derived demand, the common assumption of an intertemporally independent sequence of instantaneous demand relationships is justified if there is a perfect rental market for capital equipment and there are no internal adjustment costs associated with either adding to the stock of equipment or subtracting from it. In the more realistic case where changes in capital stocks incur adjustment costs, the time-profile of demand is determined by the underlying programme of (dis) investment. In this case, changes in the resource price over time induce a lagged demand response. The paper goes on to study the properties of an intertemporal competitive equilibrium, with emphasis on the demand structure with underlying <b>adjustment</b> costs. <b>Agents</b> are assumed to transact on perfectly functioning forward markets, so expectations about the resource price are always fulfilled a post, The principal results to emerge from the analysis are that the resource utilization rate declines to zero in the long term as processes that use the resource in question are replaced by others; however, the precise time-path of substitution depends sensitively on the nature of adjustment costs. For example, if adjustment costs exhibit non-convexities, substitution away from processes that use the resource occurs in "pulses", in contrast to the smooth substitution programme under convex adjustment costs...|$|R
40|$|Multiagent Systems Engineering (MaSE) Methodology {{is one of}} old {{object-oriented}} methodology {{which supports}} the development process and is established based {{on the development of}} the object-oriented software engineering methods and their <b>adjustment</b> with the <b>agent</b> view. Some characteristics of the agent like autonomy, creativity and preactivated are not paid attention. The agents are supposed as simple software processes which cooperate to obtain a certain goal. There are two basic phases in MaSE: analysis and design. The analysis phase concentrates on specializing the agent’s roles, their duties and interactions. In design phase, matters such as diagrams and conversations class are introduced. The all steps in MaSE are implemented by graphic tools, agent Tool. This tool (agent Tool) covers all the steps of MaSE methodology design and analysis. In this article, we have covered the MaSE methodology based on a practical experience. The reason of choosing the chain store system is that it has the necessary characteristics like customer and seller autonomic agent technology and it is easier to identify and understand the analysis and design steps...|$|R
40|$|AbstractThis study {{experiments}} with the manufacturing efficiency by layout change of a factory {{by means of}} agent-based autonomous production scheduling, using the virtual factory on a multi-agent simulation system. As infrastructure software for agent based simulation, the artisoc(c) is used. In this virtual factory, three types of agents are equipped. Users can alter a configuration such as input new jobs, adjusting a machine setting, etc, with monitoring conditions of agents. As a result, by <b>adjustment</b> of the <b>agent's</b> behavior with shop floor detail, the assembly schedule becomes more effective. The experiment is carried out to show that local negotiations contribute to global optimization when resources in the factory are effectively distributed and shared. In this paper, the effectiveness of job-list cleanup method is shown. In addition, the scheduling influence is simulated by the communication range of agents. A part agent chooses a machine, by {{the length of a}} job list and the conveyance cost. But the communication cost between agents increases with the size of the communication range. From experimental results, when extending the communication range simply, the conclusion is reached that optimization did not necessarily result in progress...|$|R
40|$|Abstract. Policies {{are being}} {{increasingly}} used for controlling {{the behavior of}} complex multi-agent systems. The use of policies allows administrators to regulate agent behavior without changing source code or requiring the consent or cooperation of the agents being governed. However, policy-based control can sometimes encounter difficulties when applied to agents that act in pervasive environments characterized by frequent and unpredictable changes. In such cases, we cannot always specify policies a priori to handle any operative run time situation, but instead require continuous <b>adjustments</b> to allow <b>agents</b> to behave in a contextually appropriate manner. To address these issues, some policy approaches for governing agents in pervasive environments specify policies {{in a way that}} is both context-based and semantically-rich. Two approaches have been used in recent research: an ontology-based approach that relies heavily on the expressive features of Description Logic (DL) languages, and a rule-based approach that encodes policies as Logic Programming (LP) rules. The aim of this paper is to analyze the emerging directions for the specification of semantically-rich context-based policies, highlighting their advantages and drawbacks. Based on our analysis we describe a hybrid approach that exploits the expressive capabilities of both DL and LP approaches. 1...|$|R
40|$|Tardive {{dyskinesia}} is {{a potentially}} irreversible syndrome of involuntary hyperkinetic movements {{that occur in}} predisposed persons receiving extended neuroleptic (antipsychotic) drug therapy. It is usually characterized by choreoathetoid dyskinesias in the orofacial, limb, and truncal regions, but subtypes of this syndrome may include tardive dystonia and tardive akathisia. Although the mechanisms underlying the pathogenesis and pathophysiology of this disorder are unproven, altered dopaminergic functions will likely {{play a role in}} any explanation of it. Tardive dyskinesia develops in 20 % of neuroleptic-treated patients, but high-risk groups such as the elderly have substantially higher rates. Risk factors include age, female sex, affective disorders, and probably those without psychotic diagnoses, including patients receiving drugs with antidopaminergic activity for nausea or gastrointestinal dysfunction for extended periods. Total drug exposure is positively correlated with tardive dyskinesia risk. Management strategies include a careful evaluation of both the psychiatric and neurologic states, a broad differential diagnosis, and <b>adjustment</b> of neuroleptic <b>agents</b> to the lowest effective dose that controls psychosis and minimizes motor side effects. No drug therapy is uniformly safe and effective for treating this disorder. A favorable long-term outcome of improvement or resolution correlates with younger age, early detection, lower drug exposure, and duration of follow-up...|$|R
40|$|The {{purpose of}} this study was to {{evaluate}} the therapeutic options for diabetes treatment and their potential side effects, in addition to analyzing the risks and benefits of tight glycemic control in patients with diabetic kidney disease. For this review, a search was performed using several pre-defined keyword combinations and their equivalents: &# 8220;diabetes kidney disease&# 8221; and &# 8220;renal failure&# 8221; in combination with &# 8220;diabetes treatment&# 8221; and &# 8220;oral antidiabetic drugs&# 8221; or &# 8220;oral hypoglycemic agents. &# 8221; The search was performed in PubMed, Endocrine Abstracts and the Cochrane Library from January 1980 up to January 2015. Diabetes treatment in patients with diabetic kidney disease is challenging, in part because of progression of renal failure-related changes in insulin signaling, glucose transport and metabolism, favoring both hyperglycemic peaks and hypoglycemia. Additionally, the decline in renal function impairs the clearance and metabolism of antidiabetic agents and insulin, frequently requiring reassessment of prescriptions. The management of hyperglycemia in patients with diabetic kidney disease is even more difficult, requiring <b>adjustment</b> of antidiabetic <b>agents</b> and insulin doses. The health team responsible for the follow-up of these patients should be vigilant and prepared to make such changes; however, unfortunately, there are few guidelines addressing the nuances of the management of this specific population...|$|R
40|$|Inflammatory {{diseases}} of the respiratory tract are characterized by changes in rheological properties of the phlegm and lower of the mucociliary clearance. <b>Adjustment</b> of mucoregulatory <b>agents</b> is of a special significance in treatment of {{diseases of}} the lower respiratory tract in children. Aim: to assess efficacy of carbocysteine lysine salt monohydrate as a mucokinetic agent in children with respiratory tract diseases. Patients and methods : 65 children (31 girls and 34 boys) aged from 5 to 16 years old with acute respiratory tract diseases received treatment in Belgorod pediatric out-patient clinic №  4 were included into the study. The results of the clinical follow-up of these children are shown in the article. Results : carbocysteine lysine salt monohydrate {{was found to be}} effective and safe in treatment of acute and chronic inflammatory {{diseases of the}} respiratory tract in children. The authors observed quicker convalescence of the patients and possibility of combination of this drug with other medicines used in pediatric practice. Conclusions : the above-mentioned drug when used in combination with antibacterial agents intensifies penetration of the latter into the bronchial secretion and bronchial mucous membrane thereby increasing their efficacy. The drug does not have toxicity, is well-tolerated even when prolonged using and can be recommended for treatment of cough in children both under out- and in-patients conditions. Keywords : children, respiratory tract diseases, carbocysteine. </p...|$|R
40|$|Policies {{are being}} {{increasingly}} used for controlling {{the behavior of}} complex multi-agent systems. The use of policies allows administrators to specify both agent permissions and duties without changing source code or requiring the consent or cooperation of the agents being governed. However, policy-based control can encounter difficulties when applied to agents that act in pervasive environments characterized by frequent and unpredictable changes. In this case, policies cannot be all specified a priori to face any operative run time situation, but require continuous <b>adjustments</b> to allow <b>agents</b> to behave in a contextually appropriate manner. Current approaches to policy representation have been restrictive in many ways, as they typically follow a subject-centric model, which assigns agent permissions and obligations {{on the basis of}} agent role/identity information. However, in the new pervasive scenario the roles/identities of interacting agents may not be known a-priori and most important, may not be informative or sufficiently trustworthy. We claim that the design of policy-based agent systems for pervasive environments requires a paradigm shift from subject-centric to contextcentric policy models. This paper discusses some issues concerning the specification and enforcement of contextdriven policies and presents a novel context-based policy approach that considers context as a first-class principle to guide both policy specification and enforcement. In this perspective, “context ” explicitly appears in the specification of security policies and context changes trigger the evaluation process of applicable agent permissions and obligations. 1...|$|R
40|$|Background: DUE (Drug Utilization Evaluation) {{studies can}} help {{identify}} and correct {{problems associated with}} irrational use of drugs. Considering lack of data regarding how rational vancomycin is being used, we evaluated this DUE study in a referral infectious center to evaluate compliance with guidelines in terms of rational use of this valuable antibiotic. Methods: This retrospective study was done for 6 months from March to September 2012 at Razi hospital, an educational hospital affiliated to Mazandaran University of Medical Sciences. Data including patients’ demographics, vancomycin dose, kidney function assessment, dose adjustments, sampling and culture were collected. Based on the HICPAC (Hospital Infection Control Practices Advisory Committee) and Up-to-date 2012 advices, the concordance of practice with standard guidelines was assessed. Results: One hundred and forty six medical records were reviewed in this study. Fever and shortness of breath were the most common symptoms {{at the time of}} initiation of vancomycin. Skin infections, lower respiratory tract infection and septicemia were the most common initial diagnosis of patients. Sampling was done in almost one-third of patients. Most of patient with a specific order were received vancomycin in half an hour. Considering the indication, Vancomycin was administered appropriately in 58 percent of patients. Conclusion: Vancomycin was used irrationally in a great proportion of patients. The main observed drawbacks were empiric use of vancomycin without subsequent <b>adjustment</b> of antimicrobial <b>agent</b> according to culture and sensitivity data and lack of paying enough attention to calculation of creatinine clearance and dosage adjustment. </p...|$|R
40|$|Repeated {{measurements}} designs, occur {{frequently in}} the assessment of exposure to toxic chemicals. This thesis deals with the possibilities of using mixed effects models for occupational exposure assessment and in the analysis of exposure response relationships. The model enables simultaneous estimation of both the variance components of exposure (between- and within-subject) and the unbiased regression coefficients for determinants of exposure. Implications are relevant to grouping strategies, hazard control, risk assessment and over-exposure assessment. In an Israeli cohort of industry workers sampled over a year, the geometric standard deviations representing variation of exposure between workers (after <b>adjustment</b> for <b>agent</b> and factory) and within workers were 3. 1 and 3. 0, respectively. These values may be used, as crude estimates of exposure variability to obtain an interval estimate of mean exposure. In studies among Dutch rubber manifactoring workers exposed to inhalable particulate and rubber fumes and pig farmers exposed to bacterial endotoxins, exposure determinants reduced the random between-worker variance estimators between 59 - 100 %. Interestingly, the random within-worker variability was reduced only in the pig farmer data set, by 25 %, when specific work activities that varied over time were accounted for. Results of linear regression and mixed models were compared. In rubber manufacturing, coefficients were similar, but fewer factors affecting exposure were statistically significant due to the high correlation between repeated measurements. In a cohort of benzene-workers, both time related factors and a non-time related factor (e. g. job task) were found to affect the mean exposure significantly. The random between workers variance was highly affected by job task. Time related factors (warm month, pay day, day of the week), were found to be responsible for the high random within-worker variance, which was more than two times higher than the between-worker variance. Grouping strategies in occupational health should result in a small between-worker variance. The within-worker variance often varies greatly. Consequently, in simulated data based on real data we found that it is common to obtain a zero or negative ANOVA estimate of the between-worker variance. We evaluated an approach proposed earlier to use an upper confidence bound when the estimate is negative, and found that this method has three main disadvantages: the estimator can remain negative, performs poorly with two repeated measures per worker, and the method can be extremely sensitive to small changes in the data. Our alternative estimator incorporates "plugging in" of an estimator in which the observed mean squares replaces the expected values and this offers a solution to these problems. Exposure assessment {{plays an important role in}} a valid exposure-response evaluation in epidemiology. In a study among Dutch bakers mixed modeling was used in two procedures: firstly to estimate exposure based on specific exposure detreminants, and secondly for exposure-response relationship where the estimated variance components were used as a scaling factor to avoid possible exposure-respons attenuation. The shape of the relationship between sensitization and exposure, was found to be a quadratic function...|$|R
40|$|In the introduction, I {{have stated}} the {{motivation}} for the model I will present in this thesis; trust is an important economic asset and may be vital to industrialization, {{which in turn is}} crucial for successful development and increased standards of living in the long run. In the following chapters I will present a model of industrial production and investment, where trust enters the production function and greatly influences investment decisions. In chapter 2, the basic model is presented, firstly, by introducing the economy in which the model unfolds, secondly by presenting the economic agents that enter the model, and lastly, by deducing the <b>agents</b> <b>adjustment</b> and actions. The simple model shows that when successful industrialization is contingent on trust, there are multiple equilibria; the economy may reach either of two possible solutions; one a low trust poverty trap, the other a preferred solution of sustained growth and development. Chapter 3 is an extension of the simple model. The model in this chapter concerns the same economy, and addresses the problems that may arise when there is asymmetric information, and when various types of business contacts differ with respect to trustworthiness. Again two equilibria are possible outcomes. However, it is shown that there are several qualified interpretations of the model; depending on how the population is composed, the predictions of the model will vary. In chapter 3. 3, I examine an economy that consists of producers and predators. The economy may find itself in a crime induced low-trust poverty trap, if the ratio of producers to predators is too low. Conversely, if the ratio is favourable, the economy heads towards a benign equilibrium in which the economy will experience sustained growth and development. The other interpretation is presented in chapter 3. 4; in an economy of advanced producers and cottage producers, the model predicts either coordination failure and a subsequent poverty trap, or a big push out of poverty. Chapter 4 is an extension to the model as interpreted in chapter 3. 4, but the asymmetries of information are less prominent. The predictions of this chapter are that coordination failure (or the big push effect) may occur even when information about business contacts trustworthiness is more readily available. Chapter 5 offers a presentation of the general conclusions, predictions and policy implications of the thesis...|$|R
40|$|Two studies {{examined}} the pharmacokinetics of indinavir and rifabutin when coadministered in healthy subjects. Rifabutin, which induces {{the expression of}} cytochrome P 450 (CYP) 3 A, and indinavir, which inhibits that enzyme system, are frequently coadministered in patients infected with HIV. The second study was undertaken to determine if altering the dose of rifabutin coadministered with indinavir would minimize the drug interaction observed in the first study. Two studies, each with a three-period crossover design, were performed. In study 1, standard doses of rifabutin and indinavir (300 mg of rifabutin qd and 800 mg indinavir q 8 h) were administered as monotherapy (with placebo to the other drug) or in combination to 10 volunteers for 10 days. In study 2, 150 mg qd of rifabutin together with 800 mg q 8 h of indinavir, 300 mg qd of rifabutin alone, or 800 mg q 8 h of indinavir alone was administered to 14 volunteers for 10 days. In study 1, the geometric mean ratio (GMR) (90 % confidence interval [CI]) of the AUC((0 - 8 h)) of indinavir, coadministered with rifabutin 300 mg qd compared to indinavir alone (with rifabutin placebo), was 0. 66 (0. 56, 0. 77), while that of the AUC((0 - 24 h)) of rifabutin, coadministered with indinavir compared to rifabutin alone (with indinavir placebo), was 2. 73 (1. 99, 3. 77). In study 2, the GMR (90 % CI) of the AUC((0 - 8 h)) of indinavir, coadministered with rifabutin 150 mg qd compared to indinavir alone, was 0. 68 (0. 60, 0. 76), while that of the AUC((0 - 24 h)) of rifabutin, when rifabutin 150 mg qd was coadministered with indinavir compared to rifabutin 300 mg qd alone, was 1. 54 (1. 33, 1. 79). For both studies 1 and 2, indinavir and rifabutin administered alone or in combination were generally well tolerated. No clinical or laboratory adverse experience was serious. These data demonstrate the important pharmacokinetic interactions between indinavir and rifabutin when they are coadministered. Indeed, these observations {{formed the basis for}} the subsequent ACTG 365 study that explored dose <b>adjustments</b> for these <b>agents</b> in combination regimens to preserve the sustained antiviral activity of indinavir in the absence of adverse events as a result of elevated circulating levels of rifabutin...|$|R
40|$|Early {{goal-directed}} {{therapy is}} a term {{used to describe the}} guidance of intravenous fluid and vasopressor/inotropic therapy by using cardiac output or similar parameters in the immediate post-cardiopulmonary bypass in cardiac surgery patients. Early recognition and therapy during this period may result in better outcome. In keeping with this aim in the cardiac surgery patients, we conducted the present study. The study included 30 patients of both sexes, with EuroSCORE &# 8805; 3 undergoing coronary artery bypass surgery under cardiopulmonary bypass. The patients were randomly divided into two groups, namely, control and early goal-directed therapy (EGDT) groups. All the subjects received standardized care; arterial pressure was monitored through radial artery, central venous pressure through a triple lumen in the right internal jugular vein, electrocardiogram, oxygen saturation, temperature, urine output per hour and frequent arterial blood gas analysis. In addition, cardiac index monitoring using FloTrac and continuous central venous oxygen saturation using PreSep was used in patients in the EGTD group. Our aim was to maintain the cardiac index at 2. 5 - 4. 2 l/min/m 2, stroke volume index 30 - 65 ml/beat/m 2, systemic vascular resistance index 1500 - 2500 dynes/s/cm 5 /m 2, oxygen delivery index 450 - 600 ml/min/m 2, continuous central venous oximetry more than 70 &#x 0025;, stroke volume variation less than 10 &#x 0025;; in addition to the control group parameters such as central venous pressure 6 - 8 mmHg, mean arterial pressure 90 - 105 mmHg, normal arterial blood gas analysis values, pulse oximetry, hematocrit value above 30 &#x 0025; and urine output more than 1 ml/kg/h. The aims were achieved by altering the administration of intravenous fluids and doses of inotropic or vasodilator agents. Three patients were excluded from the study and the data of 27 patients analyzed. The extra volume used (330 &#x 00 B 1; 160 v/s 80 &#x 00 B 1; 80 ml, P = 0. 043) number of <b>adjustments</b> of inotropic <b>agents</b> (3. 4 &#x 00 B 1; 1. 5 v/s 0. 4 &#x 00 B 1; 0. 7, P = 0. 026) in the EGDT group were significant. The average duration of ventilation (13. 8 &#x 00 B 1; 3. 2 v/s 20. 7 &#x 00 B 1; 7. 1 h), days of use of inotropic agents (1. 6 &#x 00 B 1; 0. 9 v/s 3. 8 &#x 00 B 1; 1. 6 d), ICU stay (2. 6 &#x 00 B 1; 0. 9 v/s 4. 9 &#x 00 B 1; 1. 8 d) and hospital stay (5. 6 &#x 00 B 1; 1. 2 v/s 8. 9 &#x 00 B 1; 2. 1 d) were less in the EGDT group, compared to those in the control group. This study is inconclusive with regard to the beneficial aspects of the early goal-directed therapy in cardiac surgery patients, although a few benefits were observed...|$|R

