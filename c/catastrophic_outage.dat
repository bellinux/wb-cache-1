3|18|Public
40|$|Cloud {{services}} inevitably fail: machines lose power, networks become disconnected, pesky software bugs cause sporadic crashes, and so on. Unfortunately, {{failure recovery}} itself is often faulty; e. g. recovery can accidentally recursively replicate small failures to other machines until the entire cloud service fails in a <b>catastrophic</b> <b>outage,</b> amplifying a small cold into a contagious deadly plague! We propose that failure recovery should be engineered foremost {{according to the}} maxim of primum non nocere, that it “does no harm. ” Accordingly, we must consider the system holistically when failure occurs and recover only when observed activity safely allows for it...|$|E
40|$|Stellar {{growth in}} use of mobile {{platforms}} for business users, combined with rapid business growth {{in developing countries}} that have much higher utilization of mobile access have put cloud back-end hosted mobile applications in a sweet spot. By using a cloud mobile combination, computationally intensive services can be delivered right to the consumer anywhere, anytime. Two unmet challenges in developing such applications are managing the development of applications for heterogeneous mobile platforms with equivalent functionality, and maintaining a portable back-end to mitigate any <b>catastrophic</b> <b>outage</b> (such as the recent outage of Amazon EC 21.) This article discusses the mobile application aspects of the MobiCloud [1] approach for rapidly developing cloud-mobile hybrid applications for heterogeneous mobile front-ends and cloud back-ends. MobiCloud exploits the features of Domain Specific Languages (DSLs) to address the difficulty of programming for multiple mobile platforms. It also helps to overcome some {{of the limitations of}} mobile platforms by pairing with cloud back-ends...|$|E
40|$|Digital {{technologies}} {{continue to}} entangle themselves more deeply in {{our everyday lives}} in various ways; however, two key aspects {{can be seen as}} particularly dominant: an increasing recognition of the materiality of the digital and the role of ‘big data’ in controlling—indeed, structuring—us. The emergence of ubiquitous computing {{in the form of the}} Internet of Things that connects devices and their users physically within cybernetic networks can be seen through the increasing popularity of wearable devices and ‘smart’ home technologies. Meanwhile, the operations of big data reconfigure human subjects into ‘users’, defined by quantification and shaped by algorithmic processes. Undergoing such datafication, we are interpellated—often voluntarily, occasionally through coercion—into systems that leave us prone to surveillance by corporations, governmental agencies and cybercriminals. Recent macro-events—the crippling cyberattack on the UK’s National Health Service (May 2017), the <b>catastrophic</b> <b>outage</b> of British Airways’ IT infrastructure (May 2017) and the shadowy role played by data mining/analysis companies in the US Presidential Election and Brexit Referendum in the second half of 2016 —have demonstrated how vulnerable today’s digital citizens are. This chapter considers seven publications from 2016 that reflect these concerns with materiality and datafication in various ways, first surveying three major essay collections that seek to explore longstanding issues or stimulate new reflections on our immersions within digital culture. Discussion then moves to examine ‘media archaeological’ approaches to computing, in Matthew Kirschenbaum’s literary history of the word processor and Tung-Hui Hu’s prehistory of the cloud—both offering new insights into everyday computational technologies that provoke a reconsideration of our interactions with them. The chapter then turns to quantification, by examining Deborah Lupton’s analysis into the ways in which digital self-tracking has coalesced around the Quantified Self movement, and to the risks to our lives and liberties through the increasing dominance of big data in analysing and controlling social policy interrogated by Cathy O’Neil...|$|E
50|$|Hurricane Ike, another <b>catastrophic</b> power <b>outage</b> in Ohio.|$|R
50|$|Hurricanes Irene and Isabel, {{two other}} <b>catastrophic</b> power <b>outages</b> on the Mid-Atlantic coast.|$|R
25|$|This view of {{software}} quality on a linear continuum {{has to be}} supplemented by the identification of discrete Critical Programming Errors. These vulnerabilities may not fail a test case, {{but they are the}} result of bad practices that under specific circumstances can lead to <b>catastrophic</b> <b>outages,</b> performance degradations, security breaches, corrupted data, and myriad other problems (Nygard, 2007) that make a given system de facto unsuitable for use regardless of its rating based on aggregated measurements. A well-known example of vulnerability is the Common Weakness Enumeration, a repository of vulnerabilities in the source code that make applications exposed to security breaches.|$|R
2500|$|Software quality {{measurement}} quantifies to {{what extent}} a software program or system rates along each of these five dimensions. An aggregated measure of software quality can be computed through a qualitative or a quantitative scoring scheme or a mix of both and then a weighting system reflecting the priorities. This view of software quality being positioned on a linear continuum is supplemented by the analysis of [...] "critical programming errors" [...] that under specific circumstances can lead to <b>catastrophic</b> <b>outages</b> or performance degradations that make a given system unsuitable for use regardless of rating based on aggregated measurements. Such programming errors found at the system level represent up to 90% of production issues, whilst at the unit-level, even if far more numerous, programming errors account for less than 10% of production issues. As a consequence, code quality without {{the context of the}} whole system, as W. Edwards Deming described it, has limited value.|$|R
40|$|Extreme weather events, many {{of which}} are climate change related, are {{occurring}} with increasing frequency and intensity and causing <b>catastrophic</b> <b>outages,</b> reminding the need to enhance the resilience of power systems. This paper proposes a proactive operation strategy to enhance system resilience during an unfolding extreme event. The uncertain sequential transition of system states driven by the evolution of extreme events is modeled as a Markov process. At each decision epoch, the system topology is used to construct a Markov state. Transition probabilities are evaluated according to failure rates caused by extreme events. For each state, a recursive value function, including a current cost and a future cost, is established with operation constraints and intertemporal constraints. An optimal strategy is established by optimizing the recursive model, which is transformed into a mixed integer linear programming by using the linear scalarization method, with the probability of each state as the weight of each objective. The IEEE 30 -bus system, the IEEE 118 -bus system, and a realistic provincial power grid are used to validate the proposed method. The results demonstrate that the proposed proactive operation strategies can reduce the loss of load due to the development of extreme events. postprin...|$|R
40|$|System {{restoration}} is {{an integral}} part of the overall defense system against <b>catastrophic</b> <b>outages.</b> The nature of system restoration problem involves status assessment, optimization of generation capability and load pickup. The optimization problem needs to take into numerous practical considerations and, therefore, it cannot be formulated as one single optimization problem. The other critical consideration for the development of decision support tools is its generality, i. e., the tools should be portable from a system to another with minimal customization. This presentation will provide a comprehensive methodology for construction of system restoration strategies. The strategy adopted by each power system differs, depending on the system characteristics and policies. A new method based on the concept of "generic restoration milestones" and "generic restoration actions" has been developed. A specific restoration strategy can be synthesized by a combination of the milestones and actions based on the actual system conditions. The decision support tool is expected to reduce the restoration time, thereby improving the system reliability. published_or_final_versionThe 6 th International Conference on Electrical and Electroincs Engineering (ELECO 2009), Bursa, Turkey, 5 - 8 November 2009. In Proceedings of the International Conference on Electrical and Electronics Engineering, 2009, p. I 8 -I 1...|$|R
40|$|Existing maximum flow {{algorithms}} use one processor for all calculations or one processor per vertex in a graph {{to calculate}} the maximum possible flow through a graph's vertices. This is not suitable for practical implementation. We extend the max-flow work of Goldberg and Tarjan to a distributed algorithm to calculate maximum flow where the number of processors {{is less than the}} number of vertices in a graph. Our algorithm is applied to maximizing electrical flow within a power network where the power grid is modeled as a graph. Error detection measures are included to detect problems in a simulated power network. We show that our algorithm is successful in executing quickly enough to prevent <b>catastrophic</b> power <b>outages...</b>|$|R
40|$|Abstract — Existing maximum flow {{algorithms}} use one processor for all calculations or one processor per vertex in a graph {{to calculate}} the maximum possible flow through a graph’s vertices. This is not suitable for practical implementation. We extend the max-flow work of Goldberg and Tarjan to a distributed algorithm to calculate maximum flow where the number of processors {{is less than the}} number of vertices in a graph. Our algorithm is applied to maximizing electrical flow within a power network where the power grid is modeled as a graph. Error detection measures are included to detect problems in a simulated power network. We show that our algorithm is successful in executing quickly enough to prevent <b>catastrophic</b> power <b>outages.</b> Index Terms — Fault Injection, FT Algorithms, FT Communication, maximum flow, power system. A...|$|R
40|$|This paper {{provides}} a comprehensive state-of-the-art overview on power infrastructure defense systems. A {{review of the}} literature on the subjects of critical infrastructures, threats to the power grids, defense system concepts, and the special protection systems is reported. The proposed Strategic Power Infrastructure Defense (SPID) system methodology is a real-time, wide-area, adaptive protection and control system involving the power, communication, and computer infrastructures. The SPID system performs the failure analysis, vulnerability assessment, and adaptive control actions to avoid <b>catastrophic</b> power <b>outages.</b> This paper also includes a new concept for bargaining by multiagents to identify the decision options to reduce the system vulnerability. The concept of a flexible configuration of the wide-area grid is substantiated with an area-partitioning algorithm. A 179 -bus system is used to illustrate the area partitioning method that is intended to minimize the total amount of load shedding. Keywords—Critical infrastructures, defense systems, intelligent systems, multiagent technology, power infrastructure security...|$|R
40|$|The {{increasing}} demand for electricity {{over the last}} few decades has not been followed by adequate growth in electric infrastructure. As a result, the reliability and safety of the electric grids are facing tremendously growing pressure. Large blackouts in the recent past indicate that sustaining system reliability and integrity turns out to be more and more difficult due to reduced transmission capacity margins and increased stress on the system. Due to the heavy loading conditions that occur when the system is under stress, the protection systems are susceptible to mis-operation. It is under such severe situations that the network cannot afford to lose its critical elements like the main generation units and transmission corridors. In addition to the slow but steady variations in the network structure over a long term, the grid also experiences drastic changes during the occurrence of a disturbance. One of the main reasons why protection relays mis-operate is due to the inability of the relays to adjust to the evolving network scenario. Such failures greatly compound the severity of the disturbance, while diminishing network integrity leading to <b>catastrophic</b> system-wide <b>outages.</b> With the advancement of Wide Area Measurement Systems (WAMS), it is no...|$|R
40|$|Generation {{scheduling}} in restructured {{electric power}} systems {{is critical to}} maintain the stability and security of a power system and economical operation of the electricity market. However, new generation scheduling problems (GSPs) are emerging under critical or new circumstances, such as generator starting sequence and black-start (BS) generator installation problems in power system restoration (PSR), and generation operational planning considering carbon dioxide (CO 2) emission regulation. This dissertation proposes new optimization techniques to investigate these new GSPs that do not fall into the traditional categories. Resilience and efficient recovery are critical and desirable features for electric power systems. Smart grid technologies are expected to enable a grid to be restored from major outages efficiently and safely. As a result, power system restoration is increasingly important for system planning and operation. In this dissertation, the optimal generator start-up strategy is developed to provide the starting sequence of all BS or non-black-start (NBS) generating units to maximize the overall system generation capability. Then, based on the developed method to estimate the total restoration time and system generation capability, the optimal installation strategy of blackstart capabilities is proposed for system planners to develop the restoration plan and achieve an efficient restoration process. Therefore, a new decision support tool for system restoration has been developed to assist system restoration planners and operators to restore generation and transmission systems in an on-line environment. This tool is able to accommodate rapidly changing system conditions {{in order to avoid}} <b>catastrophic</b> <b>outages.</b> Moreover, to achieve the goal of a sustainable and environment-friendly power grid, CO 2 mitigation policies, such as CO 2 cap-and-trade, help to reduce consumption in fossil energy and promote a shift to renewable energy resources. The regulation of CO 2 emissions for electric power industry to mitigate global warming brings a new challenge to generation companies (GENCOs). In a competitive market environment, GENCOs can schedule the maintenance periods to maximize their profits. Independent System Operator 2 ̆ 7 s (ISO) functionality is also considered from the view point of system reliability and cost minimization. Considering these new effects of CO 2 emission regulation, GENCOs need to adjust their scheduling strategies in the electricity market and bidding strategies in CO 2 allowance market. This dissertation proposes a formulation of the emission-constrained GSP and its solution methodology involving generation maintenance scheduling, unit commitment, and CO 2 cap-and-trade. The coordinated optimal maintenance scheduling and CO 2 allowance bidding strategy is proposed to provide valuable information for GENCOs 2 ̆ 7 decision makings in both electricity and CO 2 allowance markets. By solving these new GSPs with advanced optimization techniques of Mixed Integer Linear Programming (MILP) and Mixed Integer Bi-level Liner Programming (MIBLP), this dissertation has developed the highly efficient on-line decision support tool and optimal planning strategies to enhance resilience and sustainability of the electric power grid...|$|R
40|$|Abstract—The {{collection}} and prompt analysis of synchrophasor measurements {{is a key}} step towards enabling the future smart power grid, in which grid management applications would be deployed to monitor and react intelligently to changing conditions. The potential exists to slash inefficiencies and to adaptively reconfigure the grid to take better advantage of renewables, coordinate and share reactive power, and {{to reduce the risk}} of <b>catastrophic</b> large-scale <b>outages.</b> However, to realize this potential, a number of technical challenges must be overcome. We describe a continuously active, timely monitoring framework that we have created, architected to support a wide range of grid-control applications in a standard manner designed to leverage cloud computing. Cloud computing systems bring significant advantages, including an elastic, highly available and cost-effective compute infrastructure well-suited for this application. We believe that by showing how challenges of reliability, timeliness, and security can be addressed while leveraging cloud standards, our work opens the door for wider exploitation of the cloud by the smart grid community. This paper characterizes a PMU-based state-estimation application, explains how the desired system maps to a cloud architecture, identifies limitations in the standard cloud infrastructure relative to the needs of this usecase, and then shows how we adapt the basic cloud platform options with sophisticated technologies of our own to achieve the required levels of usability, fault tolerance, and parallelism. I...|$|R
40|$|AbstractThe Smart Grid is a {{relatively}} new effort for the way we distribute energy within the country. While it is best known for its capability to allow end users to sell energy generated from renewable sources such as wind and sun back to the power company, it also has another component that deals with the capability to react automatically to disturbances that can cause large scale power outages. As part of our work at Los Alamos National Laboratory, intelligent control of flexible manufacturing systems research was applied to controlling an electric power grid model using the sensors and technology that is expected to be available with the introduction of the Smart Grid. The goal is to use the automated equipment that will eventually be in place for the implementation of the Smart Grid and build a controller that can avoid such <b>catastrophic</b> power <b>outages</b> such as the 2003 outage over the north east part of the country. The power system of the City of Los Alamos was modeled and used as a test system. The states of the models were initialized with data collected from their SCADA system. Then artificial disturbances were induced to test the system. The controller was able to react to the disturbances in real time with no human command. The controller was able to consider the financial consequences to its actions by optimizing for the least overall loss of revenue. This work is conceptual in nature with the goal of introducing research from the manufacturing control community to applications in the control of the electric power grid now possible with the introduction of the technology associated with the Smart Grid...|$|R
40|$|Thesis (Ph. D.), School of Electrical Engineering and Computer Science, Washington State UniversityCyber {{intrusions}} into substations of a {{power grid}} {{are a source of}} vulnerability since most substations are unmanned and with limited protection of the cyber and physical security. In the worst case, simultaneous cyber intrusions into multiple substations can lead to severe cascading events, causing <b>catastrophic</b> power <b>outages.</b> In addition, substation communication protocols do not include cyber security features in their original standard. Generic Object Oriented Substation Event (GOOSE) contains the circuit breaker trip command whereas Sampled Measured Value (SMV) includes measured analog values such as currents and voltages. Due to the importance of substation automation multicast messages, IEC 62351 standards proposed the authentication method as a primary security measure for GOOSE and SMV messages since they required fast transmission time. However, performance testing for the application of the authentication method to GOOSE and SMV is in an early stage, and there is presently no solution to detection of the GOOSE and SMV related error, anomaly and intrusion. Cyber security technologies for anomaly detection at a substation are in an early stage of development. Technologies to detect anomalies for substation automation multicast protocols and applications are critically needed. This dissertation is concerned with anomaly detection in the computer network environment of a substation. The proposed integrated Anomaly Detection System (ADS) contains host- and network-based anomaly detection systems for the substations, and simultaneous anomaly detection for multiple substations. Potential scenarios of simultaneous intrusions into the substations have been simulated using a substation automation testbed based on the IEEE 39 and modified IEEE 118 -bus systems. The host-based anomaly detection considers temporal anomalies in the substation facilities. The malicious behaviors of substation automation based on multicast messages are incorporated in the proposed network-based anomaly detection. The proposed impact evaluation method can help operators find the most critical substation among the anomaly detected substations. In addition, the proposed simultaneous intrusion detection method is able to identify the same type of attacks at multiple substations and their locations. The result is a new integrated tool for detection and mitigation of cyber intrusions at a single substation or multiple substations {{of a power}} grid. School of Electrical Engineering and Computer Science, Washington State Universit...|$|R
40|$|Transformers are {{the most}} {{expensive}} and critical asset in any electrical power network. Their failure results in long interruption of power supply with consequent loss of reliability and revenue. Understanding and detection of the failure mechanism helps in avoiding <b>catastrophic</b> failures, unplanned <b>outages</b> and improving the power system reliability. Oil impregnated paper (OIP) and pressboards form the main soild insulation in a transformer. Life of the transformer is governed mostly by the life of OIP insulation. Until recently, {{it was thought that}} ageing of the OIP insulation in power transformer and its eventual failure, is mainly a function of temperature and electrical stresses. However, it has now been realized that the moisture causes rapid degradation of OIP and needs a special attention. Considering its practical relevance, this research program was formulated with goals: (i) to study the ageing of OIP insulation under temperature and moisture stresses, (ii) to seek correlation between diagnostic ageing indices and end-of-life (EOL) and (iii) to develop a life model for OIP considering moisture along with the thermal stress. Observing that working with actual transformers or even the prototypes are rather inordinately expensive, experiments were conducted with paper strips immersed in oil in test tubes with paper to oil ratio kept same as that in power transformers. In order to cater for the statistical nature of the phenomena, adequate numbers of test specimens were employed (25 numbers for each experiment). Experiments were conducted for two years at temperatures 90 °C, 110 °C & 120 °C and moisture 1 %, 2 % & 3 %. Following the literature, the degree of polymerization (DP) was chosen as the primary index for ageing. As measurement of DP is not only destructive, but also impractical on most of the working transformers, with an aim to develop suitable diagnostic indices for ageing, 2 -furfural (2 -FAL) and oxides of carbon (CO and CO 2) were also measured. Empirical relation between ageing and amount of stresses and time have been deduced for the relevant range. Limiting value of these indices to prescribe the end-of-life, as well as, their correlation with DP have been worked out and reported. In order to bring the role of moisture explicitly, based on earlier work on multi-stress ageing, a multiplicative power law supplementing the Arrhenius factor is envisaged. Accordingly, a phenomenological combined stress model involving the time to failure, temperature, and moisture content is deduced. Based on the experimental results, this model is statistically validated and the values of parameters appearing in the model is obtained. Thus the combined stress model enables one to estimate the life of OIP insulation at any temperature and moisture under synergy. In summary, this work through experimental and analytical approach has contributed to the evaluation of the aging of OIP insulation used in power transformers under the combined action of moisture and temperature...|$|R

