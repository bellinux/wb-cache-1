28|11|Public
50|$|The <b>conditionality</b> <b>{{principle}}</b> is a Fisherian {{principle of}} statistical inference that Allan Birnbaum formally defined and studied in his 1962 JASA article. Informally, the <b>conditionality</b> <b>principle</b> {{can be taken}} as the claim that experiments which were not actually performed are statistically irrelevant.|$|E
50|$|The {{arguments}} given above can {{be viewed}} as following the spirit of the <b>conditionality</b> <b>principle</b> of statistical inference, although they express a more generalized notion of conditionality which do not require the existence of an ancillary statistic. The <b>conditionality</b> <b>principle</b> however, already in its original more restricted version, formally implies the likelihood principle, a result famously shown by Birnbaum. CLs does not obey the likelihood principle, and thus such considerations may only be used to suggest plausibility, but not theoretical completeness from the foundational point of view. (The same however can be said on any frequentist method if the <b>conditionality</b> <b>principle</b> is regarded as necessary).|$|E
5000|$|Birnbaum {{proved that}} the {{likelihood}} principle follows from two more primitive and seemingly reasonable principles, the <b>conditionality</b> <b>principle</b> and the sufficiency principle. The <b>conditionality</b> <b>principle</b> says that if an experiment is chosen by a random process independent of the states of nature , then only the experiment actually performed is relevant to inferences about [...] The sufficiency principle says that if [...] is a sufficient statistic for , and if in two experiments with data [...] and [...] we have [...] , then the evidence about [...] given by the two experiments is the same.|$|E
40|$|The {{paper by}} Mayo claims {{to provide a}} new {{clarification}} and critique of Birnbaum's argument for showing that sufficiency and <b>conditionality</b> <b>principles</b> imply the likelihood principle. However, much of the arguments go back to arguments made thirty to forty years ago. Also, the main contention in the paper, that Birnbaum's arguments are not valid, seems to rest on a misunderstanding. [arXiv: 1302. 7021]Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Birnbaum's theorem, {{that the}} {{sufficiency}} and <b>conditionality</b> <b>principles</b> entail the likelihood principle, has engendered {{a great deal}} of controversy and discussion since the publication of the result in 1962. In particular, many have raised doubts as to the validity of this result. Typically these doubts are concerned with the validity of the principles of sufficiency and conditionality as expressed by Birnbaum. Technically it would seem, however, that the proof itself is sound. In this paper we use set theory to formalize {{the context in which the}} result is proved and show that in fact Birnbaum's theorem is incorrectly stated as a key hypothesis is left out of the statement. When this hypothesis is added, we see that sufficiency is irrelevant, and that the result is dependent on a well-known flaw in conditionality that renders the result almost vacuous...|$|R
40|$|Many modern {{adaptive}} designs apply {{an analysis}} where p-values from different stages are weighted together to an overall hypothesis test. One merit of this combination {{approach is that}} the design can be made very flexible. However, combination tests violate the sufficiency and <b>conditionality</b> <b>principles.</b> As a consequence, combination tests may lead to absurd conclusions, such as 'proving' a positive effect while the average effect is negative. We explore the possibility of modifying the test so that such illogical conclusions are no longer possible. The dual test requires both the weighted combination test and a nave test, ignoring the adaptations, to be statistically significant. The {{result is that the}} flexibility and type I error level control of the combination test are preserved, while the nave test adds a safeguard against unconvincing results. The dual test is, by construction, at least as conservative as the combination test. However, many design changes will not lead to any power loss. A typical situation where the combination approach can be used is two-stage sample size reestimation (SSR). For this case, we give a complete specification of all sample size modifications for which the two tests are equally powerful. We also study the overall power loss for some suggested SSR rules. Rules based on conditional power generally lead to ignorable power loss while a decision analytic approach exhibits clear discrepancies between the two tests...|$|R
50|$|The <b>conditionality</b> <b>principle</b> {{makes an}} {{assertion}} about an experiment E {{that can be}} described as a mixture of several component experiments Eh where h is an ancillary statistic (i.e. a statistic whose probability distribution does not depend on unknown parameter values). This means that observing a specific outcome x of experiment E is equivalent to observing the value of h and taking an observation xh from the component experiment Eh.|$|E
5000|$|Conditionality Principle: If E is any {{experiment}} {{having the}} form of a mixture of component experiments Eh, then for each outcome [...] of E, ... the evidential meaning of any outcome x of any mixture experiment E is {{the same as that of}} the corresponding outcome xh of the corresponding component experiment Eh, ignoring the over-all structure of the mixed experiment (See Birnbaum 1962). An illustration of the <b>conditionality</b> <b>principle,</b> in a bioinformatics context, is given by Barker (2014).|$|E
50|$|A {{debtor country}} {{is invited to}} a {{negotiation}} meeting with its Paris Club creditors when it has concluded an appropriate programme with the International Monetary Fund (IMF) that demonstrates {{that the country is}} not able to meet its external debt obligations and thus needs a new payment arrangement with its external creditors (<b>conditionality</b> <b>principle).</b> Paris Club creditors link the debt restructuring to the IMF programme because the economic policy reforms are intended to restore a sound macroeconomic framework that will lower the probability of future financial difficulties.|$|E
40|$|By {{using the}} direct and inverse {{binomial}} experiments it is shown {{that there is a}} situation where Birnbaum's basic axiom of mathematical equivalence and the likelihood principle is a tautology. This observation disqualifies Birnbaum's proof of the likelihood principle based on the axioms of mathematical equivalence and conditionality. The implication of this disproof of Birnbaum's argument for Bayesian statistics is briefly discussed. Likelihood <b>principle</b> <b>conditionality</b> sufficiency Bayesian statistics prior distribution...|$|R
40|$|Anderson (2013) {{referred}} to {{various aspects of}} permutation p-values. We agree {{with most of the}} notions and ideas heuristically discussed there; however, we present a more formal discussion to confirm and be more precise. The approach adopted in Pesarin (2001), Pesarin (2013) and Pesarin & Salmaso (2010 a, 2010 b) considered permutation tests in light of the <b>conditionality</b> and sufficiency <b>principles</b> of inference, and we borrow freely from those citations...|$|R
40|$|We {{explore the}} meaning of {{information}} about quantities of interest. Our approach is divided in two scenarios: the analysis of observations and the planning of an experiment. First, we review the Sufficiency, <b>Conditionality</b> and Likelihood <b>principles</b> and how they relate to trivial experiments. Next, we review Blackwell Sufficiency and show that sampling without replacement is Blackwell Sufficient for sampling with replacement. Finally, we unify the two scenarios presenting {{an extension of the}} relationship between Blackwell Equivalence and the Likelihood Principle. CNPqCNPqFAPESPFAPES...|$|R
40|$|Helland (1995) argues through {{example that}} the <b>Conditionality</b> <b>Principle</b> {{does not always}} apply and that {{unconditional}} analyses are sometimes more informative than conditional ones. We try to counter his arguments. KEY WORDS: Conditionality Principle; Likelihood Principle; Bayesian statistics. In Simple Counterexamples Against the <b>Conditionality</b> <b>Principle</b> (Helland 1995) Helland purports {{to show that the}} <b>Conditionality</b> <b>Principle</b> does not have universal application. The argument is made through several ex- Supported by NSF Grant DMS- 9300137 amples. Here we examine two of those examples in more detail and argue that the conclusion is not warranted. Example 1 Quoting from Helland: "As a small part of a larger medical experiment, two individuals (1 and 2) have been on a certain diet for some time, and by taking samples at the beginning of {{and at the end of}} that period some response like change in blood cholesterol levels is measured. For the individual h(h = 1; 2), the measured response i [...] ...|$|E
40|$|Ancillary {{statistics}} {{are divided into}} two logically distinct types: those determined by the experimental design and those determined by the mathematical modelling of the problem. I t is pointed out that, {{in the class of}} inference problems where our purpose is to gain information or insight into the nature of a chance set-up, a weakened <b>conditionality</b> <b>principle</b> when applied first removes the possibility of deriving the likelihood principle. Since to some extent a <b>conditionality</b> <b>principle</b> must be applied in experiment definition, it is argued that this is a necessary first step if full acceptance of the likelihood axiom is to be avoided...|$|E
40|$|In this {{discussion}} we demonstrate that fiducial distributions provide a natural {{example of an}} inference paradigm that does not obey Strong Likelihood Principle while still satisfying the Weak <b>Conditionality</b> <b>Principle.</b> [arXiv: 1302. 7021]Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|A {{comparison}} {{is made of}} the two concepts, generalized likelihood (from Bjørnstad, 1996) and Royall's measure of empirical statistical evidence in prediction problems, here called evidential likelihood, applied to survey sampling when a population model is assumed. As shown by Bjørnstad (1996), the likelihood principle based on the generalized likelihood is implied by <b>conditionality</b> and sufficiency <b>principles,</b> generalizing the fundamental result from Birnbaum (1962). The main {{difference between the two}} likelihood concepts is that the generalized likelihood is a basis for statistical inference containing all available statistical information, while the evidential likelihood concentrates on the empirical evidence per se which does not contain all available statistical information about the variable to be predicted. One can regard the evidential likelihood as the sample evidence of the generalized likelihood, changing the prior distribution of the population total to the generalized likelihood after the data have been obtained...|$|R
40|$|Published with {{permission}} from The Central Statistical Office of Poland. A comparison is made of the two concepts, generalized likelihood (from Bjørnstad, 1996) and Royall's measure of empirical statistical evidence in prediction problems, here called evidential likelihood, applied to survey sampling when a population model is assumed. As shown by Bjørnstad (1996), the likelihood principle based on the generalized likelihood is implied by <b>conditionality</b> and sufficiency <b>principles,</b> generalizing the fundamental result from Birnbaum (1962). The main {{difference between the two}} likelihood concepts is that the generalized likelihood is a basis for statistical inference containing all available statistical information, while the evidential likelihood concentrates on the empirical evidence per se which does not contain all available statistical information about the variable to be predicted. One can regard the evidential likelihood as the sample evidence of the generalized likelihood, changing the prior distribution of the population total to the generalized likelihood after the data have been obtained...|$|R
40|$|Sustainable {{groundwater}} {{development is}} fundamental {{in order to}} provide universal access to safe drinking water. This docu-ment, The Code of Practice for Cost Effective Boreholes provides a basis for the realisation of economical and sustainable access to safe water. The term “cost-effective ” means optimum value for money invested over the long term. Boreholes are drilled to function for a lifespan of 20 to 50 years. Thus, the lowest cost is not always the most cost-effective, particularly if construction quality is compromised to save money. Cheap drilling or poor construction quality can lead to premature failure of the well or contamination of the water supply. Boreholes that are subse-quently abandoned by the users are clearly not cost-effective. The Code of Practice sets out nine principles that relate directly to the practicalities of borehole construction (see below). They should be adhered to {{in order to provide}} cost-effective bore-holes. Each principle is broken down into sub-principles which recommend procedures to be followed and call for the defini-tion of and adherence to minimum standards. The Code of Practice thus provides a framework to analyse {{the strengths and weaknesses of}} existing policies and practices. It is intended to be used as the foundation for the development of national protocols for cost-effective borehole provision. It pro-vides a basis for stakeholders to examine whether they are working in accordance with international practices, and it can be used by donors to examine funding <b>conditionalities.</b> Nine <b>Principles</b> for Cost-Effective Boreholes Principle 1 Professional Drilling Enterprises and Consultants: Construction of drilled water wells and supervision is under-taken by professional and competent organisations which ad-here to national standards and are regulated by the public sec-tor...|$|R
40|$|Deborah Mayo {{claims to}} have refuted Birnbaum's {{argument}} that the Likelihood Principle is a logical consequence of the Sufficiency and Conditionality Principles. However, this claim fails because her interpretation of the <b>Conditionality</b> <b>Principle</b> is different from Birnbaum's. Birnbaum's proof cannot be so readily dismissed. [arXiv: 1302. 7021]Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|SUMMARY. D. Basu’s {{approach}} to (survey) sampling from finite populations is well known. Basu {{adhered to the}} likelihood principle and to the <b>conditionality</b> <b>principle.</b> He gradually became ardently Bayesian, and eventually rejected randomization in the data analysis stage. He accepted the predictive modelling approach as (empirical) Bayesian procedures. The present article reviews recent developments, specially in the model based approach, in light of Basu’s philosophy. 1...|$|E
40|$|We {{present a}} simple, illustrative example of Fisher’s <b>Conditionality</b> <b>Principle</b> in {{statistical}} pattern recognition. We observe training data {(Xi,Yi,Zi) } n i= 1 {{with which to}} learn the discriminant boundary. At classification time, we observe the to-beclassified feature vector X with true-but-unobserved class label Y. WedonotobservetheZassociated with X, andthecollection {Zi} is ancillary for the discriminant boundary. Nonetheless, {Zi} is essential for optimal classification. KEY WORDS: Ancillary; Classification; Control variate...|$|E
40|$|Accession {{negotiations}} to the EU since 2004 brought significant changes to European enlargement customary law and exacerbated the reliance of the Commission on conditionality {{to impose its}} leverage on present and prospective member states. The subsequent development of European norms in the pre-accession phase was transposed onto current member states {{and led to the}} edification of a Normative Empire. This research reformulated the concept of Normative Empire while resting on factual and contemporary evidence. It investigated why the increasingly significant role in <b>conditionality</b> of the <b>principle</b> of independence of the judiciary contributed to the metamorphosis of the EU into a Normative Empire. The argumentation of this research rested on the study of Bulgaria, Croatia and Romania. In addition to their geographical kinship, these three cases share issues of rampant corruption, notably in the political and judicial structures, which remain the main obstacles to their accession or full membership. The analysis of the Commission's influence in judicial reforms during the pre and post-accession phases was supported by a thorough study of the Cooperation and Verification Mechanism and the progress reports from 2004 till present. In conclusion, the Commission's post-accession monitoring in Bulgaria and [...] ...|$|R
40|$|The author studies {{foreign aid}} policy within a principal-agent framework. He shows {{that one reason}} for foreign aid's poor overall record may be a moral hazard problem that shapes the aid {{recipient}} s incentive to undertake structural reform. The model's basic prediction is a two-way relationship: Disbursements of foreign aid are guided (in part) by {{the needs of the}} poor. Anticipating this, recipients have little incentive to improve the welfare of the poor. Preliminary econometric work shows that the data support this hypothesis. In <b>principle,</b> <b>conditionality</b> could partly solve this problem, but only if the donor can make a binding commitment to increase disbursements in good relative to bad states. Without such a commitment technology, aid disbursements remain guided by the needs of the poor and recipient countries maintain a low effort to reduce poverty. Contrary to the conventional wisdom found in the aid literature, the author shows that the welfare of all parties might be improved by using tied project aid or by delegating part of the aid budget to an (international) agency with less aversion to poverty. Development Economics&Aid Effectiveness,School Health,Economic Adjustment and Lending,Environmental Economics&Policies,Economic Theory&Research...|$|R
40|$|Cox and Mayo (7 (II), this volume) {{make the}} {{following}} bold assertion: It {{is not uncommon}} to see statistics texts argue that in frequentist theory one is faced with the following dilemma, either to deny the appropriateness of conditioning on the precision of the tool chosen by the toss of a coin, or else to embrace the strong likelihood principle which entails that frequentist sampling distributions are irrelevant to inference once the data are obtained. This is a false dilemma: Conditioning is warranted in achieving objective frequentist goals, and the <b>conditionality</b> <b>principle</b> coupled with sufficiency does not entail the strong likelihood principle. The “dilemma ” argument is therefore an illusion. Given how widespread is the presumption that the (weak) <b>conditionality</b> <b>principle</b> (CP) plus the sufficiency principle (SP) entails (and is entailed by) the (strong) likelihood principle (LP), and given the dire consequence for error statistics that follows from assuming it is true, some justification for our dismissal is warranted. The discussion of the three principles (in 7 (II)) sets the stage for doing this. The argument purporting to show that CP + S...|$|E
40|$|This is an invited {{contribution}} to the discussion on Professor Deborah Mayo's paper, "On the Birnbaum argument for the strong likelihood principle," to appear in Statistical Science. Mayo clearly demonstrates that statistical methods violating the likelihood principle need not violate either the sufficiency or <b>conditionality</b> <b>principle,</b> thus refuting Birnbaum's claim. With the constraints of Birnbaum's theorem lifted, we revisit the foundations of statistical inference, focusing on some new foundational principles, the inferential model framework, and connections with sufficiency and conditioning. [arXiv: 1302. 7021]Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Every {{experiment}} or {{observational study}} {{is made in}} a context. This context is being explicitly considered in this book. To do so, a conceptual variable is defined as any variable which can be defined by (a group of) researchers in a given setting. Such variables are classified. Sufficiency and ancillarity are defined conditionally on the context. The <b>conditionality</b> <b>principle,</b> the sufficiency principle and the likelihood principle are generalized, and a tentative rule for when one should not condition on an ancillary is motivated by examples. The theory {{is illustrated by the}} case where a nuisance parameter {{is a part of the}} context, and for this case, model reduction is motivated. Model reduction is discussed in general from the point of view that there exists a mathematical group acting upon the parameter space. It is shown that a natural extension of this discussion also gives a conceptual basis from which essential parts of the formalism of quantum mechanics can be derived. This implies an epistemological basis for quantum theory, a kind of basis that has also been advocated by part of the quantum foundation community in recent years. Born's celebrated formula is shown to follow from a focused version of the likelihood principle together with some reasonable assumptions on rationality connected to experimental evidence. Some statistical consequences of Born's formula are sketched. The questions around Bell's inequality are approached by using the <b>conditionality</b> <b>principle</b> for each observer. The objective aspects of the world are identified with the ideal inference results upon which all observers agree (epistemological objectivity). Comment: 98 pages; invited as a Springer Brie...|$|E
40|$|Although {{minority}} protection {{was one of}} the Copenhagen {{political criteria}} and thus was {{at the core of the}} <b>conditionality</b> <b>principle</b> presupposing a fair assessment of the candidate countries’ progress towards accession on the merits, the Commission simultaneously promoted two contradicting approaches to the issue throughout the whole duration of the pre–accession process. They included, on the one hand, de facto assimilation (prohibited by art. 5 (2) of the Framework Convention for the Protection of National Minorities) and, on the other hand, cultural autonomy, bringing to life a complicated web of partly overlapping – partly contradicting standards. This paper is dedicated to outlining the main differences between the two key approaches to minority protection espoused by the Commission in the course of the latest enlargements’ preparation...|$|E
40|$|There is {{consensus}} that payments for biodiversity services are a promising conservation tool, yet {{the implementation of}} applied schemes has been lagging behind. This paper explores some reasons why potential biodiversity buyers may hesitate. It describes {{the case of an}} unsuccessful attempt to establish a community conservation concession in the village of Setulang (East Kalimantan, Indonesia) to safeguard a biologically valuable area from predatory logging. Potential biodiversity donors did not engage in this payments-for-environmental-services scheme mainly because of their limited time horizon and uneasiness about the <b>conditionality</b> <b>principle.</b> Other complicating factors included overlapping land claims, and the diagnosis of the externality at hand. We conclude that new investment modalities and attitudes are needed if potential biodiversity buyers are to exploit the advantages of this innovative tool. We also provide some tangible recommendations on factors to consider when designing a compensation scheme for conservation at the community level...|$|E
40|$|When testing {{large numbers}} of null hypotheses, one needs to assess the {{evidence}} against the global null hypothesis {{that none of the}} hypotheses is false. Such evidence typically is based on the test statistic of the largest magnitude, whose statistical significance is evaluated by permuting the sample units to simulate its null distribution. Efron (2007) has noted that correlation among the test statistics can induce substantial interstudy variation in the shapes of their histograms, which may cause misleading tail counts. Here, we show that permutation-based estimates of the overall significance level also can be misleading when the test statistics are correlated. We propose that such estimates be conditioned on a simple measure of the spread of the observed histogram, and we provide a method for obtaining conditional significance levels. We justify this conditioning using the <b>conditionality</b> <b>principle</b> described by Cox and Hinkley (1974). Application of the method to gene expression data illustrates the circumstances when conditional significance levels are needed...|$|E
40|$|EU {{candidate}} countries must prove their respect {{for democracy and}} the rule of law to be eligible for EU membership. The Commission administers their accession processes following the principle of conditionality. This paper examines how domestic conditions and different aspects of the <b>conditionality</b> <b>principle</b> affect policy outcomes. It reviews the arguments made in the literature on EU conditionality and applies them to the policy areas of minority rights and the fight against corruption in Croatia and Macedonia. Both countries have been subjected to the Commission’s conditionality while their democratic achievements differ substantially. Thereby, the two countries offer a fruitful ground to evaluate the lessons drawn from the 2004 - 07 enlargement. While previous studies have remained quite unclear about the relative importance of domestic and EU-related determinants of effective conditionality, I argue that domestic influences vary strongly across the researched policy areas. In comparison, the political-legal instruments of the Commission show clear impacts on policies in {{candidate countries}}. Material incentives offered by the EU are only effective within the early phases of the accession process...|$|E
40|$|This thesis {{develops}} {{techniques for}} adjusting for selection bias using Gaussian process models. Selection bias {{is a key}} issue both in sample surveys and in observational studies for causal inference. Despite recently emerged techniques for dealing with selection bias in high-dimensional or complex situations, use of Gaussian process models and Bayesian hierarchical models in general has not been explored. Three approaches are developed for using Gaussian process models to estimate the population mean of a response variable with binary selection mechanism. The first approach models only the response with the selection probability being ignored. The second approach incorporates the selection probability when modeling the response using dependent Gaussian process priors. The third approach uses the selection probability as an additional covariate when modeling the response. The third approach requires knowledge of the selection probability, while the second approach can be used even when the selection probability is not available. In addition to these Gaussian process approaches, {{a new version of}} the Horvitz-Thompson estimator is also developed, which follows the <b>conditionality</b> <b>principle</b> and relates to importance sampling for Monte Carlo simulations. Simulation studies and the analysis of an example due to Kang and Schafer show that the Gaussia...|$|E
40|$|A {{number of}} {{available}} legal instruments {{have the potential}} to contribute to the elaboration of an EU minority protection standard. These instruments, however, are mostly limited to guaranteeing simple nondiscrimination, which is not enough to ensure minority protection stricto sensu. The lack of any viable internal minority protection standard did not prevent the European Union from treating minority protection as one of the key elements of the pre-accession process leading to the Eastern enlargement, reinforcing the internal-external competence divide and reducing the effectiveness of minority protection in the European Union. Although minority protection was one of the Copenhagen political criteria—and thus {{at the core of the}} <b>conditionality</b> <b>principle</b> presupposing a fair assessment of the candidate countries’ progress on the merits—the Commission clearly used minority protection in a discriminatory way, tolerating the standard of assimilation in one group of candidate countries (Latvia, Estonia) and backing cultural autonomy in others. Thus, alongside the internal toleration or simple denial of minority problems in the European Union, the Commission simultaneously promoted two contradicting approaches in external relations: de facto assimilation, which is prohibited by article 5 (2) of the Framework Convention for the Protection of National Minorities, and cultural autonomy, which brings to life a complicated web of partly overlapping, partly contradicting standards...|$|E
40|$|As {{part of the}} {{international}} presence in the Western Balkans, the European Union has adopted sanctions, brokered political agreements, launched its first-ever police and military missions and directed economic, legal and administrative reforms to eradicate {{the root causes of}} instability. Yet, despite the comprehensive nature of its involvement, the EU’s strategies have been marked by confusion, its actions by concurrent or competing mandates of other international organisations. As a result, the returns on its investments are dwindling, at a time when nation-building in the region has entered its final stages with the separation of Serbia and Montenegro, the search for bringing an end to {{the international}} governance of Bosnia-Herzegovina and a final status for Kosovo. As the Western Balkans still contain a genuine security threat, there is a real imperative now to move the region as a whole from the stage of international protectorates and weak states to the stage of accession to the euro-atlantic organisations to which they aspire. This book presents legal and political ways and means to restructure the international effort to see the defining processes through. Under the leadership of the EU, only a ‘tough love’ strategy based on a firm but fair application of the <b>conditionality</b> <b>principle</b> can lead to the integration of the Western Balkans into the European mainstream. Promotores: H. G. Schermers, C. Hillion, H. NeuholdWith summary in DutchCopyright Steven Blockmans, Brussels, 200...|$|E
40|$|In {{order to}} {{functionally}} interpret differentially expressed genes or other discovered features, researchers seek to detect enrichment {{in the form}} of overrepresentation of discovered features associated with a biological process. Most enrichment methods treat the p-value as the measure of evidence using a statistical test such as the binomial test, Fisher 2 ̆ 7 s exact test or the hypergeometric test. However, the p-value is not interpretable as a measure of evidence apart from adjustments in light of the sample size. As a measure of evidence supporting one hypothesis over the other, the Bayes factor (BF) overcomes this drawback of the p-value but lacks the minimax optimality of the normalized maximum likelihood (NML) of recent minimum description length methodology. On the basis of either of two NMLs, the strength of evidence for enrichment may be measured by the discrimination information (DI) in the data that favors the alternative hypothesis over the null hypothesis. One of the NMLs, the normalized maximum conditional likelihood (NMCL), is supported by the <b>conditionality</b> <b>principle.</b> We assessed measures of evidence derived from the two NMLs, two BFs and the p-value for one-sided and two-sided hypothesis comparisons using a gene expression data set from an experiment on a breast cancer cell line. These measures, for most GO terms, give the same results for the two-sided hypothesis comparison. However, they do not agree as well for the one-sided hypothesis comparison, in which case the DI based on the NMCL cannot be closely approximated by any of the faster methods...|$|E
40|$|ABSTRACT. As yet, no general {{agreement}} {{has been reached}} on whether the Bayesian or the frequentist (Neyman-Pearson, NP) approach to statistics is to be preferred. Whereas Bayesians adhere to coherence conditions of de Finetti, Savage, and others, frequentists do not consider these conditions normative and deliberately and knowingly violate them. Hence further arguments, bringing more clarity on the disagreements, are warranted. Providing such arguments, by refining the coherence conditions, {{is the purpose of}} this paper. It invokes recent arguments from the economic literature demonstrating that some seemingly self-evident principles for dynamic decision making have a surprising implication for static decisions: They imply Bayesianism. These principles are forgone-event independence (independence of past counterfactual events, often called consequentialism in decision theory and known as the <b>conditionality</b> <b>principle</b> in statistics), dynamic consistency (what is optimal at some given time point is independent of the time point at which that is decided), and two other conditions. Thus, a more sensitive diagnostic tool is obtained for identifying the disagreements between Bayesians and frequentists. If a frequentist does not mind violating Bayesian coherence, a Bayesian can now ask a follow-up question: Which of the dynamic principles will the frequentist give up? The debate may lead either to Bayesianism or to better implementations of non-Bayesian models in dynamic decision situations and to better non-Bayesian methods for updating information. The diagnostic tool sheds new light on NP hypothesis testing. NP theory requires that statistical procedures are laid down before data are observed. It adheres to dynamic consistency but violates forgone-event independence. Forgone-event independence, however, is so natural that NP practitioners adhere to it and observe the data before deciding on a statistical procedure. They are thus led into violations of dynamic consistency...|$|E
40|$|An {{essential}} component of inference based on familiar frequentist notions, such as p-values, significance and confidence levels, is the relevant sampling distribution. This feature results in violations of a principle known as the strong likelihood principle (SLP), {{the focus of this}} paper. In particular, if outcomes x^* and y^* from experiments E_ 1 and E_ 2 (both with unknown parameter θ) have different probability models f_ 1 (·),f_ 2 (·), then even though f_ 1 (x^*;θ) =cf_ 2 (y^*;θ) for all θ, outcomes x^* and y^* may have different implications for an inference about θ. Although such violations stem from considering outcomes other than the one observed, we argue this does not require us to consider experiments other than the one performed to produce the data. David Cox [Ann. Math. Statist. 29 (1958) 357 - 372] proposes the Weak <b>Conditionality</b> <b>Principle</b> (WCP) to justify restricting the space of relevant repetitions. The WCP says that once it is known which E_i produced the measurement, the assessment should be in terms of the properties of E_i. The surprising upshot of Allan Birnbaum's [J. Amer. Statist. Assoc. 57 (1962) 269 - 306] argument is that the SLP appears to follow from applying the WCP in the case of mixtures, and so uncontroversial a principle as sufficiency (SP). But this would preclude the use of sampling distributions. The goal {{of this article is to}} provide a new clarification and critique of Birnbaum's argument. Although his argument purports that [(WCP and SP) entails SLP], we show how data may violate the SLP while holding both the WCP and SP. Such cases also refute [WCP entails SLP]. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
