22|255|Public
5000|$|<b>Cache</b> <b>buffer</b> sizes can {{be larger}} than what the 32-bit kernel space allows, {{potentially}} increasing I/O performance.|$|E
50|$|This is {{the first}} Barracuda family using the ATA/IDE interface. The hard drives were {{available}} in capacities between 6.8 GB and 28.2 GB, with a 512 KB <b>cache</b> <b>buffer</b> and an ATA/66 interface.|$|E
50|$|Available with SATA/150 and ATA/100 interfaces, the Barracuda Serial ATA V {{is one of}} {{the first}} hard drives to feature a SATA interface. Capacities range from 30 GB to 120 GB, with a 2 MB <b>cache</b> <b>buffer.</b>|$|E
40|$|<b>Buffer</b> <b>caches</b> are {{commonly}} used in servers {{to reduce the number}} of slow disk accesses or network messages. These <b>buffer</b> <b>caches</b> form a multilevel <b>buffer</b> <b>cache</b> hierarchy. In such a hierarchy, second-level <b>buffer</b> <b>caches</b> have different access patterns from first-level <b>buffer</b> <b>caches</b> because accesses to a second-level are actually misses from a first-level. Therefore, commonly used cache management algorithms such as the Least Recently Used (LRU) replacement algorithm that work well for single-level <b>buffer</b> <b>caches</b> may not work well for second-level. This paper investigates multiple approaches to effectively manage second-level <b>buffer</b> <b>caches.</b> In particular, it reports our research results in 1) second-level <b>buffer</b> <b>cache</b> access pattern characterization, 2) a new local algorithm called Multi-Queue (MQ) that performs better than nine tested alternative algorithms for second-level <b>buffer</b> <b>caches,</b> 3) a set of global algorithms that manage a multilevel <b>buffer</b> <b>cache</b> hierarchy globally and significantly improve second-level <b>buffer</b> <b>cache</b> hit ratios over corresponding local algorithms, and 4) implementation and evaluation of these algorithms in a real storage system connected with commercial database servers (Microsoft SQL Server and Oracle) running industrial-strength online transaction processing benchmarks...|$|R
40|$|Abstract—Buffer caches are {{commonly}} used in servers {{to reduce the number}} of slow disk accesses or network messages. These <b>buffer</b> <b>caches</b> form a multilevel <b>buffer</b> <b>cache</b> hierarchy. In such a hierarchy, second-level <b>buffer</b> <b>caches</b> have different access patterns from first-level <b>buffer</b> <b>caches</b> because accesses to a second-level are actually misses from a first-level. Therefore, commonly used cache management algorithms such as the Least Recently Used (LRU) replacement algorithm that work well for single-level <b>buffer</b> <b>caches</b> may not work well for second-level. This paper investigates multiple approaches to effectively manage second-level <b>buffer</b> <b>caches.</b> In particular, it reports our research results in 1) second-level <b>buffer</b> <b>cache</b> access pattern characterization, 2) a new local algorithm called Multi-Queue (MQ) that performs better than nine tested alternative algorithms for second-level <b>buffer</b> <b>caches,</b> 3) a set of global algorithms that manage a multilevel <b>buffer</b> <b>cache</b> hierarchy globally and significantly improve second-level <b>buffer</b> <b>cache</b> hit ratios over corresponding local algorithms, and 4) implementation and evaluation of these algorithms in a real storage system connected with commercial database servers (Microsoft SQL Server and Oracle) running industrial-strength online transaction processing benchmarks. Index Terms—Cache memories, storage hierarchy, storage management. ...|$|R
50|$|The POGL {{team has}} {{collaborated with the}} ImageMagick team to add PerlMagick APIs that allow GPUs and ImageMagick to share <b>cache</b> <b>buffers</b> via C {{pointers}} - optimizing performance for FBOs and VBOs - for use with loading and saving textures and GPGPU data transfer.|$|R
5000|$|K: Kernel KA: Kernel Access KC: Kernel Cache KCB: Kernel <b>Cache</b> <b>Buffer</b> KCBW: Kernel <b>Cache</b> <b>Buffer</b> Wait KCC: Kernel Cache Control file KCCB: Kernel Cache Control file Backup KCCCF: Kernel Cache Copy Flash {{recovery}} area KCCDC: Kernel cache Control file Copy KCP: Kernel Cache transPortable tablespace KCR: Kernel Cache Redo KCT: Kernel Cache insTance KD: Kernel Data KG: Kernel Generic KGL: Kernel Generic library cache KGLJ: Kernel Generic library cache Java KJ: Kernel Locking KK: Kernel Compilation KQ: Kernel Query KS: Kernel Service(s) KSB: Kernel Service Background KSM: Kernel Service Memory KSR: Kernel Service Reliable message KSU: Kernel Service User KSUSE: Kernel Service User SEssion KSUSECON: Kernel Service User SEssion CONnection KSUSEH: Kernel Service User SEssion History KT: Kernel Transaction(s) KTU: Kernel Transaction Undo KX: Kernel Execution KXS: Kernel eXecution Sql KZ: Kernel Security K2: Kernel Distributed Transactions ...|$|E
50|$|Hard disks may default {{to using}} their own {{volatile}} write cache to buffer writes, which greatly improves performance while introducing a potential for lost writes. (Tools such as hdparm -F will instruct the HDD controller to flush the on-drive write <b>cache</b> <b>buffer.)</b> The performance impact of turning caching off is so large that even the normally conservative FreeBSD community rejected disabling write caching by default in FreeBSD 4.3.|$|E
50|$|In {{computer}} storage, {{disk buffer}} (often ambiguously called disk cache or <b>cache</b> <b>buffer)</b> is the embedded memory {{in a hard}} disk drive (HDD) acting as a buffer between {{the rest of the}} computer and the physical hard disk platter that is used for storage. Modern hard disk drives come with 8 to 256 MiB of such memory, and solid-state drives come with up to 1 GB of cache memory.|$|E
40|$|This paper proposes <b>buffer</b> <b>cache</b> architecture. The {{proposed}} model {{consists of}} three units – main <b>buffer</b> <b>cache</b> unit, pre-fetch unit and LRU Evict Unit. An algorithm is proposed to retain the evicted entry from main <b>buffer</b> <b>cache</b> unit in the LRU Evict Unit thereby giving it a second chance of access. On subsequent access in the LRU Evict Unit, the entry is promoted to the prefetch unit to accommodate more entries in the main <b>buffer</b> <b>cache</b> unit. On access in the pre-fetch unit, an entry is fetched into the main <b>buffer</b> <b>cache.</b> The LRU replacement policy is used in all the units. The proposed model is compared with <b>buffer</b> <b>cache</b> architecture with LRU replacement algorithm. The performance is comparable for sequential input where as 3 % improvement in performance is seen for random input...|$|R
40|$|Abstract—Due {{to recent}} {{advances}} in semiconductor technologies, storage class RAMs (SCRAMs) such as FRAM and PRAM are emerging rapidly. Since SCRAMs are nonvolatile and byte-accessible, there are attempts to use these SCRAMs as part of nonvolatile <b>buffer</b> <b>caches.</b> A nonvolatile <b>buffer</b> <b>cache</b> provides improved consistency of file systems by absorbing write I/Os as well as improved performance. In this paper, we discuss the optimality of cache replacement algorithms in nonvolatile <b>buffer</b> <b>caches</b> and present a new algorithm called NBM (Nonvolatile-RAM-aware <b>Buffer</b> <b>cache</b> Management). NBM has three salient features. First, it separately exploits read and write histories of block references, and thus it estimates future references of each operation more precisely. Second, NBM guarantees the complete consistency of write I/Os since all dirty data are cached in nonvolatile <b>buffer</b> <b>caches.</b> Third, metadata lists are maintained separately from cached blocks. This allows more efficient management of volatile and nonvolatile <b>buffer</b> <b>caches</b> based on read and write histories, respectively. Trace-driven simulations show that NBM improves the I/O performance of file systems significantly compared to the NVLRU algorithm that is {{a modified version of}} LRU to hold dirty blocks in nonvolatile <b>buffer</b> <b>caches...</b>|$|R
40|$|Modern servers pay a {{heavy price}} in block access time on diskbound workloads when the working set {{is greater than the}} size of the local <b>buffer</b> <b>cache.</b> We provide a {{mechanism}} for cooperating servers to coordinate and share their local <b>buffer</b> <b>caches.</b> The coordinated <b>buffer</b> <b>cache</b> can handle working sets on the order of the aggregate cache memory, greatly improving performance on diskbound workloads. This facility is provided with minimal communication overhead, no penalty for local cache hits, and without any explicit kernel support...|$|R
50|$|Since the caches {{intermediate}} accesses {{to memory}} addresses, data written to different addresses may reach the peripherals' memory or registers {{out of the}} program order, i.e. if software writes data to an address and then writes data to another address, the cache write buffer does not guarantee that the data will reach the peripherals in that order. It {{is the responsibility of the}} software to include memory barrier instructions after the first write, to ensure that the <b>cache</b> <b>buffer</b> is drained before the second write is executed.|$|E
50|$|The {{disk buffer}} is {{physically}} distinct from {{and is used}} differently from the page cache typically kept by the operating system in the computer's main memory. The disk buffer {{is controlled by the}} microcontroller in the hard disk drive, and the page cache is controlled by the computer to which that disk is attached. The disk buffer is usually quite small, ranging between 8 and 256 MiB, and the page cache is generally all unused main memory. While data in the page cache is reused multiple times, the data in the disk buffer is rarely reused. In this sense, the terms disk cache and <b>cache</b> <b>buffer</b> are misnomers; the embedded controller's memory is more appropriately called disk buffer.|$|E
40|$|Abstract We {{present an}} exact {{analysis}} of the superposition of address streams into a <b>cache</b> <b>buffer</b> which is managed according to a Least Recently Used (LRU) replacement policy. Each of the streams {{is characterized by a}} stack depth distribution, and we seek the cache hit ratio for each stream, when the combined, or superposed, stream is applied to a shared LRU cache. In this paper the combining process is taken to be a Bernoulli switching process. This problem arises in a number of branches of computer science, particularly in database systems and processor architecture. Previously, a number of approximation techniques of various complexities have been proposed for the solution of this problem. The main contribution of this paper is the description of an exact technique. We evaluate the performance of the exact and an approximate technique on realistic data, both in a lab environment and a large database installation. The results allow comparisons of the techniques, and provide insight into the validity of the Bernoulli switching assumption. 1 Introduction We consider models which can be used to predict the behavior of a <b>cache</b> <b>buffer</b> when it is subjected to the combining, or superposition, of I streams of requests, each of which has a known stack depth distribution. This <b>cache</b> <b>buffer</b> is assumed to be managed according a Least Recently Used (LRU) replacement policy. While a combined trace of the I streams, or timestamped traces of each of the streams, contains the complete information neede...|$|E
40|$|Recent {{results in}} the Rio project at the University of Michigan show that it is {{possible}} to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three designs for integrating reliable memory into databases: non-persistent database <b>buffer</b> <b>cache,</b> persistent database <b>buffer</b> <b>cache,</b> and persistent database <b>buffer</b> <b>cache</b> with protection. Non-persistent <b>buffer</b> <b>caches</b> use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent <b>buffer</b> <b>caches</b> use a memory interface to reliable memory by mapping it into the database address space. This places reliable memory under complete database contr [...] ...|$|R
40|$|Abstract: Storage {{management}} {{is important for}} overall performance of the system. Slow access time of disk storage is crucial for performance of large scale computer systems. Two important components of storage management are <b>buffer</b> <b>cache</b> and disk layout management. The <b>buffer</b> <b>cache</b> is used to store disk pages in memory {{to speed up the}} access to them. <b>Buffer</b> <b>cache</b> management algorithms require careful hand-tuning for good performance. Aself-tuning algorithm automatically manages to clean the page activity in the <b>buffer</b> <b>cache</b> management algorithm by monitoring the I/O activities. <b>Buffer</b> <b>cache</b> management algorithm is used for global data structure and it is protected by lock. Lock can because contention is helpful in reducing the throughput of the multi-processing system. Currently used solution for eliminating lock contention will directly degrade hit ratio of the <b>buffer</b> <b>cache</b> will result in poor performance in the I/O bound. The new approach, multi-region cache eliminates the lock contention without affecting the hit ratio of <b>buffer</b> <b>cache.</b> Disk layout approach improves the disk I/O efficiency, called Overwrite, and is optimized for sequential me /so from a single file. A new disk layout approach, called HyLog. HyLog achieves performance comparable to the best of existing disk layout approaches in most cases...|$|R
40|$|This project {{deals with}} the {{performance}} analysis of the Linux <b>buffer</b> <b>cache</b> while running an Oracle OLTP workload. The Linux <b>buffer</b> <b>cache</b> was studied and tests were conducted to gather <b>buffer</b> <b>cache</b> hit rates and test run times. The results of this analysis have lead {{to a better understanding}} of the complex operations of this system and may help to inspire further research on this topic. 2 Table of Content...|$|R
40|$|Abstract:- In this paper, we propose, design, implement, and {{evaluate}} a proxy caching system for MPEG- 4 video streaming services for heterogeneous users. In our system, a video stream {{is divided into}} blocks for efficient use of the <b>cache</b> <b>buffer</b> and the bandwidth. A proxy retrieves a block from a server, deposits it in its local <b>cache</b> <b>buffer,</b> and provides a requesting client with the block in time. It maintains the cache with limited capacity by replacing unnecessary cached blocks with a newly retrieved one. It also prefetches video blocks {{that are expected to}} be required in the near future. In addition, the proxy server adjusts the quality of block appropriately, because clients are heterogeneous, in terms of the available bandwidth, end-system performance, and so on. Through evaluations, we proved that our proxy caching system could provide users with a continuous and high-quality video streaming service in a heterogeneous environment. Key-Words:- video streaming service, proxy caching, quality adaptation, MPEG- 4, prototype, evaluation...|$|E
40|$|The proxy {{mechanism}} {{widely used}} in WWW systems offers low-delay and scalable delivery of data {{by means of a}} “proxy server. ” By applying a proxy mechanism to video streaming systems, high-quality and low-delay video distribution can be accomplished without imposing extra load on the system. We have proposed proxy caching mechanisms to accomplish the high-quality and highly interactive video streaming services. In our proposed mechanisms, a video stream is di-vided into blocks for efficient use of the <b>cache</b> <b>buffer</b> and the bandwidth. A proxy retrieves a block from a server, deposits it in its local <b>cache</b> <b>buffer,</b> and provides a requesting client with the block in time. It maintains the cache with limited capacity by replacing unnec-essary cached blocks with a newly retrieved block. The proxy cache server also prefetches video blocks that are expected to be required in the near future in order to avoid cache misses. In addition, the quality of cached video data can be adapted appropriately in the proxy to cope with the client-to-client heterogeneity, in terms of the available bandwidth, end-system performance, and user preferences on the perceived video quality. Further...|$|E
40|$|SUMMARY With {{the growth}} of {{computing}} power {{and the proliferation of}} the Internet, video streaming services become widely deployed. In this paper, we propose, design, implement, and evaluate a proxy caching system for MPEG- 4 video streaming services. With our system, the high-quality, low-delay, and scalable video distribution can be accomplished. In our system, a video stream is divided into blocks for efficient use of the <b>cache</b> <b>buffer</b> and the bandwidth. A proxy retrieves a block from a server, deposits it in its local <b>cache</b> <b>buffer,</b> and provides a requesting client with the block in time. It maintains the cache with limited capacity by replacing unnecessary cached blocks with a newly retrieved block. The proxy cache server also prefetches video blocks that are expected to be required in the near future in order to avoid cache-misses. In addition, the proxy server adjusts the quality of cached or retrieved video block appropriately, because clients are heterogeneous, in terms of the available bandwidth, end-system performance, and user preferences on the perceived video quality. Through evaluations conducted from several performance aspects, we proved that our proxy caching system can provide users with a continuous and high-quality video streaming service in a heterogeneous environment. key words: video streaming service, proxy caching, quality adjustment, MPEG- 4 1...|$|E
40|$|Abstract: Recently, byte-accessible NVRAM (nonvolatile RAM) {{technologies}} such as PRAM and FeRAM are advancing rapidly and there are attempts to use these NVRAMs as part of <b>buffer</b> <b>caches.</b> A nonvolatile <b>buffer</b> <b>cache</b> provides improved consistency of file systems by absorbing write I/Os as well as improved performance. In this paper, we discuss the optimality of cache replacement algorithms in nonvolatile <b>buffer</b> <b>caches</b> and present a new algorithm called NBM (NVRAM-aware <b>Buffer</b> <b>cache</b> Management). NBM has three salient features. First, it separately exploits read and write histories of block references, and thus it estimates future references of each operation more precisely. Second, NBM guarantees the complete consistency of write I/Os since all dirty data are cached in NVRAM. Third, metadata lists are maintained separately from cached blocks. This allows more efficient management of volatile and nonvolatile <b>buffer</b> <b>caches</b> based on read and write histories, respectively. Trace-driven simulations show that NBM improves the I/O performance of file systems significantly compared to the NVLRU algorithm that is {{a modified version of}} LRU to hold dirty blocks in NVRAM...|$|R
40|$|Under a {{replacement}} policy, existing operating systems identify and maintain {{most frequently used}} storage data in <b>buffer</b> <b>caches</b> located in main memory, aiming at low-latency I/O data accesses. However, replacement policies can also strongly affect energy consumptions of various connected storage devices, {{which has not been}} a consideration in the design and implementation of <b>buffer</b> <b>cache</b> management. In this paper, we present a system framework for an energy-aware <b>buffer</b> <b>cache</b> replacement, called PS-BC (power-saving <b>buffer</b> <b>cache).</b> By considering several critical factors affecting system energy consumption, PS-BC can effectively improve system energy efficiency, while it is able to flexibly incorporate conventional performance-oriented <b>buffer</b> <b>cache</b> replacement policies for different performance objectives. Our experimental studies based on a trace-driven simulation show that the PS-BC framework embedded with the CLOCK replacement policy can achieve an energy saving rate of up to 32. 5 % with a minimal overhead for various workloads...|$|R
40|$|Abstract. A <b>buffer</b> <b>cache</b> {{mechanism}} is usually employed in modern operating system {{to enhance the}} performance that is limited by slow secondary storage. In this paper, we present {{the implementation of a}} trace-driven simulator for <b>buffer</b> <b>cache</b> schemes that consider DRAM/PRAM hybrid main memory and flash memory based storages. The goal of simulator is to analyze the legacy <b>buffer</b> <b>cache</b> schemes by measuring the number of write operations on PRAM and the number of erase operations on flash memory...|$|R
40|$|A web {{geographical}} {{information system}} is a typical service-intensive application. Tile prefetching and cache replacement can improve cache hit ratios by proactively fetching tiles from storage and replacing the appropriate tiles from the high-speed <b>cache</b> <b>buffer</b> without waiting for a client's requests, which reduces disk latency and improves system access performance. Most popular prefetching strategies consider only the relative tile popularities to predict which tile should be prefetched or consider only a single individual user's access behavior to determine which neighbor tiles need to be prefetched. Some studies show that comprehensively considering all users' access behaviors and all tiles' relationships in the prediction process can achieve more significant improvements. Thus, this work proposes a new global user-driven model for tile prefetching and cache replacement. First, based on all users' access behaviors, a type of expression method for tile correlation is designed and implemented. Then, a conditional prefetching probability can be computed based on the proposed correlation expression mode. Thus, some tiles to be prefetched can be found by computing and comparing the conditional prefetching probability from the uncached tiles set and, similarly, some replacement tiles {{can be found in}} the <b>cache</b> <b>buffer</b> according to multi-step prefetching. Finally, some experiments are provided comparing the proposed model with other global user-driven models, other single user-driven models, and other client-side prefetching strategies. The results show that the proposed model can achieve a prefetching hit rate in approximately 10. 6 % ~ 110. 5 % higher than the compared methods...|$|E
40|$|Abstract—Steady {{improvements}} in storage capacities and CPU clock speeds intensify the performance bottleneck at the I/O subsystem of modern computers. Caching data can efficiently short circuit costly delays associated with disk accesses. Recent {{studies have shown}} that disk I/O performance gains provided by a <b>cache</b> <b>buffer</b> do not scale with cache size. Therefore, new algorithms have to be investigated to better utilize <b>cache</b> <b>buffer</b> space. Predictive prefetching and caching solutions have been shown to improve I/O performance in an efficient and scalable manner in simulation experiments. However, most predictive prefetching algorithms have not yet been implemented in realworld storage systems due to two main limitations: first, the existing prefetching solutions are unable to self regulate based on changing I/O workload; second, excessive number of unneeded blocks are prefetched. Combined, these drawbacks make predictive prefetching and caching a less attractive solution than the simple LRU management. To address these problems, in this paper we propose an automatic prefetching and caching system (or APACS for short), which mitigates all of these shortcomings through three unique techniques, namely: (1) dynamic cache partitioning, (2) prefetch pipelining, and (3) prefetch buffer management. APACS dynamically partitions the buffer cache memory, used for prefetched and cached blocks, by automatically changing buffer/cache sizes in accordance to global I/O performance. The adaptive partitioning scheme implemented in APACS optimizes cache hit ratios, which subsequently accelerates application execution speeds. Experimental results obtained from trace-driven simulations show that APACS outperforms the LRU cache management and existing prefetching algorithms by an average of over 50 %...|$|E
40|$|Large storage {{systems are}} often shared among {{a mix of}} {{services}} with different performance needs, some having latency or throughput needs. These services may tolerate different various levels of performance degradation below certain performance goal, while others {{may not be able}} to tolerate any degradation. Providing and maintaining performance guarantees requires coordination at each component along the request path. This paper focuses on guaranteeing performance at the buffer <b>cache.</b> <b>Buffer</b> caches are often seen as unpredictable because the performance is dependent upon previous accesses, and because of asymmetric execution times for read/write operations. This research aims at providing pedictable and guaranteeable, performance for services sharing a storage system. By providing controls over the throughput, latenc...|$|E
40|$|In {{this paper}} we propose a novel cache {{management}} mechanism termed the Content-Based <b>Buffer</b> <b>Cache.</b> The Content-Based <b>Buffer</b> <b>Cache</b> (CBBC) attempts to maintain a single copy of any block in memory according to its contents. In the presence of repeated content, this mechanism increases the effective size of the <b>buffer</b> <b>cache.</b> Overheads for maintaining this extra state information are small and bounded, providing an overall system performance improvement. Additionally, we eliminate writes to blocks where the new and old content are the same, reducing pressure on the I/O subsystems in the presence of these “Silent Writes”. We have logged traces of block-level disk access for a group of workstations over a several month period using a modified Linux kernel designed to boot off of an iSCSI target. We have analyzed single client access, as well as multiple client access to distinct logical disks using a unified block cache. There is significant replication of content and significant numbers of “Silent Writes ” within a single workstation trace, improving the Content-Based <b>Buffer</b> <b>Cache</b> read hit rate as much as 80 % over the traditional <b>buffer</b> <b>cache</b> design. We have also found that there is significant sharing of content between disks, which benefits content-based caching performance {{in the presence of a}} unified cache. For our workloads, these results indicate that content-based <b>buffer</b> <b>caches</b> dramatically improve I/O performance when used to manage a cluster of similar storage. ...|$|R
40|$|Because of {{the slow}} access time of disk storage, storage {{management}} {{is crucial to}} the performance of many large scale computer systems. This thesis studies performance issues in <b>buffer</b> <b>cache</b> management and disk layout management, two important components of storage management. The <b>buffer</b> <b>cache</b> stores popular disk pages in memory to speed up the access to them. <b>Buffer</b> <b>cache</b> management algorithms used in real systems often have many parameters that require careful hand-tuning to get good performance. A self-tuning algorithm is proposed to automatically tune the page cleaning activity in the <b>buffer</b> <b>cache</b> management algorithm by monitoring the I/O activities of the <b>buffer</b> <b>cache.</b> This algorithm achieves performance comparable to the best manually tuned system. The global data structure used by the <b>buffer</b> <b>cache</b> management algorithm is protected by a lock. Access to this lock can cause contention which can significantly reduce system throughput in multi-processor systems. Current solutions to eliminate lock contention decrease the hit ratio of the <b>buffer</b> <b>cache,</b> which causes poor performance when the system is I/O-bound. A new approach, called the multi-region cache, is proposed. This approach eliminates lock contention, maintains the hit ratio of the <b>buffer</b> <b>cache,</b> and incurs little overhead. Moreover, this approach can be applied to most <b>buffer</b> <b>cache</b> management algorithms. Disk layout management arranges the layout of pages on disks to improve the disk I/O efficiency. The typical disk layout approach, called Overwrite, is optimized for sequential I/Os from a single file. Interleaved writes from multiple users can significantly decrease system throughput in large scale systems using Overwrite. Although the Log-structured File System (LFS) is optimized for such workloads, its garbage collection overhead can be expensive. In modern and future disks, because of the much faster improvement of disk transfer bandwidth over disk positioning time, LFS performs much better than Overwrite in most workloads, unless the disk is close to full. A new disk layout approach, called HyLog, is proposed. HyLog achieves performance comparable to the best of existing disk layout approaches in most cases...|$|R
40|$|To {{bridge the}} {{increasing}} processor-disk performance gap, <b>buffer</b> <b>caches</b> {{are used in}} both storage clients (e. g. database systems) and storage servers {{to reduce the number}} of slow disk accesses. These <b>buffer</b> <b>caches</b> need to be managed effectively to deliver the performance commensurate to the aggregate <b>buffer</b> <b>cache</b> size. To address this problem, two paradigms have been proposed recently to collaboratively manage these <b>buffer</b> <b>caches</b> together: the hierarchy-aware caching maintains the same I/O interface and is fully transparent to the storage client software, and the aggressively-collaborative caching trades off transparency for performance and requires changes to both the interface and the storage client software. Before storage industry starts to implement collaborative caching in real systems, it is crucial to find out whether sacrificing transparency is really worthwhile, i. e., how much can we gain by usin...|$|R
40|$|DE 10102159 A UPAB: 20021031 NOVELTY - The method {{involves}} {{writing a}} definition data block for current data after a delay {{less than or}} equal to the maximum size of the bit <b>cache.</b> <b>Buffer</b> information is written into the bit stream indicating where the start of the output data for the second encoder for the current section of the input signal is in relation to the definition data block. DETAILED DESCRIPTION - The method involves writing a block of data from a first encoder (12) into the scalable data stream if present and, if output data from a second encoder (14) with a bit cache function for a preceding section of the input signal for the second encoder is present, writing the second encoder's output data after the block for the output data from the first encoder. If data re present for the current section of the second encoder, the output data are written after the data for the preceding section. A definition data block for current data is written after a delay {{less than or equal}} to the maximum size of the bit <b>cache.</b> <b>Buffer</b> information is written into the bit stream indicating where the start of the output data for the second encoder for the current section of the input signal is in relation to the definition data block. INDEPENDENT CLAIMS are also included for the following: (1) an encoder, (2) a scalable encoder, (3) a device for producing a scalable data stream (4) a method of decoding a scalable data stream. USE - For generating or decoding scalable data steam. ADVANTAGE - Reduced transmission delays are achieved for an encoder with a bit cache function...|$|E
40|$|International audience—In this work, a {{heterogeneous}} cellular network with caching enabled {{at the small}} cell aggregators is analyzed. The caching technology is enabled to help offload some of the popular traffic requests onto the cache. In this framework, a network congestion game is formulated to study the evolution of traffic when every user as an independent decision maker decides its choice of communication, i. e, via the macro cell or the caching enabled small cells. To study {{the effect of the}} cache on the network delay, a numerical simulation is implemented. The file popularity is modeled using the well known Zipf-distribution and the network delay is studied {{as a function of the}} <b>cache</b> <b>buffer</b> size and network traffic. In summary, these results can be utilized by network operators to deploy infrastructure efficiently...|$|E
40|$|In this paper, we {{investigate}} mechanisms for the video streaming system where proxies cooperate to provide users with low-latency and high-quality services under heterogeneous and mobile environment where hosts have different capabilities and dynamically change their locations. The proxy {{is capable of}} adapting incoming or cached video data to user's demand by means of transcoders and filters. With such proxies, heterogeneous QoS requirements on the delivered stream can be fully satisfied by preparing high-quality video data in the local <b>cache</b> <b>buffer</b> and adjust them to the requirements. On receiving a request from a user, the proxy first checks the cache. If no appropriate data is available, it retrieves the video data of the satisfactory quality from the video server or proxies nearby. The proxy communicates with the others and finds an appropriate one for data retrieval by {{taking into account the}} transfer delay and the video quality. We propose a cooperative video caching mechanism for the video streaming system and evaluate the performance in terms of the delay introduced and the video quality...|$|E
40|$|<b>Buffer</b> <b>caches</b> in {{operating}} systems keep active file blocks in memory to reduce disk accesses. Related {{studies have focused}} on minimizing buffer misses and the resulting performance degradation. However, the side effects and performance implications of accessing the data in <b>buffer</b> <b>caches</b> (i. e. <b>buffer</b> <b>cache</b> hits) have been ignored. In this paper, we show that accessing <b>buffer</b> <b>caches</b> can cause serious performance degradation on multicores, particularly with shared last level caches (LLCs). There are two reasons for this problem. First, data objects in files normally have weaker localities than data objects in virtual memory spaces. Second, due to the shared structure of LLCs on multicore processors, an application accessing the data in a <b>buffer</b> <b>cache</b> may flush the to-be-reused data of its co-running applications from the shared LLC and significantly slow down these applications. The paper proposes a <b>buffer</b> <b>cache</b> design called Selected Region Mapping Buffer (SRM-buffer) for multicore systems to address effectively the cache pollution problem caused by OS buffer. SRM-buffer improves existing OS buffer management with an enhanced page allocation policy that carefully selects mapping physical pages upon buffer misses. For a sequence of blocks accessed by an application, SRMbuffer allocates physical pages that are mapped to a selected region consisting of a small portion of sets in the LLC. Thus, when these blocks are accessed, cache pollution is effectively limited within the small cache region. We have implemented a prototype of SRM-buffer into the Linux kernel, and tested it with extensive workloads. Performance evaluation shows SRM-buffer can improve system performance and decrease the execution times of workloads by up to 36 %...|$|R
5000|$|Cache space, {{including}} CPU cache and MMU <b>cache</b> (translation lookaside <b>buffer)</b> ...|$|R
5000|$|Shares a <b>buffer</b> <b>cache</b> among instances, {{by using}} the Global Cache ...|$|R
