7|0|Public
50|$|Aureal's 3D audio {{technology}} was originally developed by Crystal River Engineering for NASA's Virtual Environment Workstation Project (VIEW). Crystal River later commercialized the technology {{with a series}} of products including the <b>Convolvotron</b> and the Acoustetron. Aureal acquired Crystal River in May 1996 and rebranded the technology A3D.|$|E
40|$|Crystal River Engineering was {{originally}} featured in Spinoff 1992 with the <b>Convolvotron,</b> a high speed digital audio processing system that delivers three-dimensional sound over headphones. The <b>Convolvotron</b> {{was developed for}} Ames' research on virtual acoustic displays. Crystal River is a now a subsidiary of Aureal Semiconductor, Inc. and they together develop and market the technology, which is a 3 -D (three dimensional) audio technology known commercially today as Aureal 3 D (A- 3 D). The technology has been incorporated into video games, surround sound systems, and sound cards...|$|E
40|$|This {{document}} {{was developed to}} accompany an audio cassette that demonstrates work in three-dimensional auditory displays, developed at the Ames Research Center Aerospace Human Factors Division. It provides a text version of the audio material, and covers the theoretical and technical issues of spatial auditory displays in greater depth than on the cassette. The technical procedures used {{in the production of}} the audio demonstration are documented, including the methods for simulating rotorcraft radio communication, synthesizing auditory icons, and using the <b>Convolvotron,</b> a real-time spatialization device...|$|E
40|$|Ames Research Center {{research}} into virtual reality {{led to the}} development of the <b>Convolvotron,</b> a high speed digital audio processing system that delivers three-dimensional sound over headphones. It consists of a two-card set designed for use with a personal computer. The Convolvotron's primary application is presentation of 3 D audio signals over headphones. Four independent sound sources are filtered with large time-varying filters that compensate for motion. The perceived location of the sound remains constant. Possible applications are in air traffic control towers or airplane cockpits, hearing and perception research and virtual reality development...|$|E
40|$|The {{contribution}} of interaural time differences (ITDs) to the localization of virtual sound sources {{with and without}} head motion was examined. Listeners estimated the apparent azimuth, elevation and distance of virtual sources presented over headphones. Stimuli (3 sec., white noise) were synthesized from minimum-phase representations of nonindividualized head-related transfer functions (HRTFs); binaural magnitude spectra were derived from the minimum phase estimates and ITDs were represented as a pure delay. During dynamic conditions, listeners were encouraged to move their heads; head position was tracked and stimuli were synthesized in real time using a <b>Convolvotron</b> to simulate a stationary external sound source. Two synthesis conditions were tested: (1) both interaural level differences (ILDs) and ITDs correctly correlated with source location and head motion, (2) ITDs correct, no ILDs (flat magnitude spectrum). Head movements reduced azimuth confusions primarily when interaural cues were correctly correlated, although a smaller effect was also seen for ITDs alone. Externalization was generally poor for ITD-only conditions and was enhanced by head motion only for normal HRTFs. Overall the data suggest that, while ITDs alone can provide a significant cue for azimuth, the errors most commonly associated with virtual sources are reduced by location-dependent magnitude cues...|$|E
40|$|This paper {{presents}} {{preliminary data}} from a study examining the relative contribution of interaural time differences (ITDs) and interaural level differences (ILDs) to the localization of virtual sound sources both with and without head motion. The listeners' task was to estimate the apparent direction and distance of virtual sources (broadband noise) presented over headphones. Stimuli were synthesized from minimum phase representations of nonindividualized directional transfer functions; binaural magnitude spectra were derived from the minimum phase estimates and ITDs were represented as a pure delay. During dynamic conditions, listeners were encouraged to move their heads; {{the position of the}} listener's head was tracked and the stimuli were synthesized in real time using a <b>Convolvotron</b> to simulate a stationary external sound source. ILDs and ITDs were either correctly or incorrectly correlated with head motion: (1) both ILDs and ITDs correctly correlated, (2) ILDs correct, ITD fixed at 0 deg azimuth and 0 deg elevation, (3) ITDs correct, ILDs fixed at 0 deg, 0 deg. Similar conditions were run for static conditions except that none of the cues changed with head motion. The data indicated that, compared to static conditions, head movements helped listeners to resolve confusions primarily when ILDs were correctly correlated, although a smaller effect was also seen for correct ITDs. Together with the results for static conditions, the data suggest that localization tends to be dominated by the cue that is most reliable or consistent, when reliability is defined by consistency over time as well as across frequency bands...|$|E
40|$|This paper gives HRTF {{magnitude}} data in numerical {{form for}} 43 frequencies between 0. 2 [...] - 12 kHz, {{the average of}} 12 studies representing 100 different subjects. However, no phase data {{is included in the}} tables; group delay simulation would need to be included in order to account for ITD. In 3 -D sound applications intended for many users, we want might want to use HRTFs that represent the common features of a number of individuals. But another approach might be to use the features of a person who has desirable HRTFs, based on some criteria. (One can sense a future 3 -D sound system where the pinnae of various famous musicians are simulated.) A set of HRTFs from a good localizer (discussed in Chapter 2) could be used if the criterion were localization performance. If the localization ability of the person is relatively accurate or more accurate than average, it might be reasonable to use these HRTF measurements for other individuals. The <b>Convolvotron</b> 3 -D audio system (Wenzel, Wightman, and Foster, 1988) has used such sets particularly because elevation accuracy is affected negatively when listening through a bad localizers ears (see Wenzel, et al., 1988). It is best when any single nonindividualized HRTF set is psychoacoustically validated using a 113 statistical sample of the intended user population, as shown in Chapter 2. Otherwise, the use of one HRTF set over another is a purely subjective judgment based on criteria other than localization performance. The technique used by Wightman and Kistler (1989 a) exemplifies a laboratory-based HRTF measurement procedure where accuracy and replicability of results were deemed crucial. A comparison of their techniques with those described in Blauert (1983), Shaw (1974), Mehrgardt and Mellert (1977), Middlebrooks, Makous, and Gree [...] ...|$|E

