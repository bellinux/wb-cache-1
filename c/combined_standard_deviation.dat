9|10000|Public
40|$|The {{clustering}} {{problem has been}} widely studied because it arises in many knowledge management oriented applications. It aims at identifying the distribution of patterns and intrinsic correlations in data sets by partitioning the data points into similarity clusters. Traditional clustering algorithms use distance functions to measure similarity centroid, which subside the influences of data points. Hence, in this article a novel non-distance based clustering algorithm is proposed which uses <b>Combined</b> <b>Standard</b> <b>Deviation</b> (CSD) as measure of similarity. The performance of CSD based K-means approach, called K-CSD clustering algorithm, is tested on synthetic data sets. It compared favorably to widely used K-means clustering algorithm. Key words: Clustering algorithm; <b>combined</b> <b>standard</b> <b>deviation...</b>|$|E
40|$|In {{multispectral}} imaging system, one of {{the most}} important tasks is to accurately reconstruct the spectral reflectance from system responses. We propose such a new method by combing three most frequently used techniques, i. e., wiener estimation, pseudo-inverse, and finite-dimensional modeling. The weightings of these techniques are calculated by minimizing the <b>combined</b> <b>standard</b> <b>deviation</b> of both spectral errors and colorimetric errors. Experimental results show that, in terms of color difference error, the performance of the proposed method is better than those of the three techniques. It is found that the simple averaging of the reflectance estimates of these three techniques can also yield good color accuracy. Institute of Textiles and ClothingAuthor name used in this publication: John H. XinAuthor name used in this publication: Si-Jie Sha...|$|E
40|$|The grazing {{incidence}} X-ray reflectivity {{is used to}} determine the multilayer thickness of GaAs/AlAs supperlattice. The measurement process includes the fitting model and the measurement conditions (different powers of 45 kV × 40 mA, 40 kV × 40 mA and 35 kV × 40 mA, different step sizes of 0. 005 °, 0. 008 ° and 0. 010 °, and different times per step of 1 s, 2 s, 3 s). In order to obtain the valid measurement process, the <b>combined</b> <b>standard</b> <b>deviation</b> is used as the normal of the fitting results selection. As a result, the measurement condition of 0. 008 ° step size and 2 s time per step with the power 40 kV × 40 mA is selectable with the operation stability of facilities and smaller error...|$|E
40|$|The Vud {{element of}} the Cabibbo-Kobayashi-Maskawa quark mixing matrix has {{traditionally}} been determined from the analysis of data in nuclear superallowed 0 + -> 0 + transitions, neutron decay, and pion beta decay. After providing a new test of the conserved vector current hypothesis, we present here a new independent determination of |Vud| from a set of five T = 1 / 2 nuclear mirror transitions. The extracted value, |Vud| = 0. 9719 +/- 0. 0017, is at 1. 2 <b>combined</b> <b>standard</b> <b>deviations</b> from the value obtained from superallowed 0 + -> 0 + transitions and has a precision comparable to the value obtained from neutron decay experiments. status: publishe...|$|R
40|$|The Vud {{element of}} the Cabibbo-Kobayashi-Maskawa quark mixing matrix has {{traditionally}} been determined from the analysis of data in nuclear superallowed 0 + to 0 + transitions, neutron decay and pion beta decay. We show here that this element can independently be determined from nuclear mirror transitions. The extracted value, |Vud| = 0. 9719 +- 0. 0017, is at 1. 2 <b>combined</b> <b>standard</b> <b>deviations</b> from the value obtained in superallowed 0 + to 0 + transitions and has a similar precision than the value obtained from neutron decay experiments. We discuss some prospects to improve its precision through experiments in nuclear mirror transitions. Comment: 4 pages, 2 figure...|$|R
40|$|Abstract — A {{non-parametric}} image blur {{measure is}} presented. The measure {{is based on}} edge analysis and is suitable for various image processing applications. The proposed measure for any edge point is obtained by <b>combining</b> the <b>standard</b> <b>deviation</b> of the edge gradient magnitude profile {{and the value of}} the edge gradient magnitude using a weighted average. The <b>standard</b> <b>deviation</b> describes the width of the edge, and its edge gradient magnitude is also included to make the blur measure more reliable. Moreover, the value of the weight is related to image contrast and can be calculated directly from the image. Experiments on natural scenes indicate that the proposed technique can effectively describe the blurriness of images in image processing applications. Keywords—blur measure; edge analysis; image contrast calculation I...|$|R
30|$|Known-group {{validity}} {{was assessed}} by comparing the mean IQualiV-OG- 21 subscales scores between groups of patients with Eastern Cooperative Oncology Group Performance Status (ECOG-PS) values of 0 – 1 versus 2 – 4. A t test was used for these comparisons. Additionally, Cohen’s effect size (ES) was calculated to indicate {{the magnitude of the}} differences between groups, defined as the difference in scores between the ECOG-PS 0 – 1 and 2 – 4 groups divided by the <b>combined</b> <b>standard</b> <b>deviation</b> of both groups. Effect size values of < 0.2, 0.2 – 0.49, 0.5 – 0.79 and ≥ 0.8 were classified as negligible, small, moderate and large differences, respectively (Fayers and Machin 2007). We hypothesized that the physical, functional and overall domains would be the most discriminative. For convergent validity, the IQualiV-OG- 21 scores were compared with the corresponding scores on the FACT-G and FACIT-sp- 12 instruments. Correlation coefficients ≥ 0.4 were considered adequate (Fayers and Machin 2007).|$|E
40|$|International audienceThe 8 main tidal {{constituents}} {{were computed}} using a finite element, hydrodynamic ocean tide model over the South Indian Ocean region. The discretization of the domain {{is of the}} order of 100 km over the deep ocean and a few hundred meters near the coast. Such refinement in the grid resolution enables wave propagation and damping on the continental shelves to be solved correctly. The model used the GEBCO 1 -minute global bathymetric grid which was improved with updated topographic data. The model solutions show good agreement with in-situ observations and Topex-Poseidon altimeter measurements and are significantly better than previously published solutions. We obtain a <b>combined</b> <b>standard</b> <b>deviation</b> of 1. 4 cm for differences of our new regional model against independent observations compared to about 2. 5 cm for the other tide models. The greatest improvements are found around the Kerguelen Islands, around Antarctica and beneath the Amery Ice Shelf and {{can be explained by the}} high grid resolution used and the particular attention given to the accuracy of the bathymetry in those regions...|$|E
40|$|BACKGROUND: Evidence of a {{relationship}} between stressful life events and the onset of autoimmune diseases is not univocal {{and there are no}} meta-analyses in the literature on the question. AIM: To look for differences in the number and type of stressful life events in the premorbid period between patients with autoimmune diseases and healthy subjects. METHOD: Review of the literature in PubMed and Scopus (January 1963 -May 2015). INCLUSION CRITERIA: We included retrospective case-control studies that compared patients diagnosed with autoimmune disorders and controls regarding the incidence of stressful events occurring before diagnosis, and investigated said events with validated questionnaires. EFFECT-SIZE INDEXES: By random effect meta-analysis, two independent researchers calculated effect-size indexes as the difference between the means of the clinical groups and the control group in relation to the <b>combined</b> <b>standard</b> <b>deviation.</b> RESULTS: The database searches produced 2490 articles, 14 of which were selected (3201 patients). Analysis showed a moderate but significant mean effect-size index [d= 0. 63, p< 0. 01], suggesting that autoimmune disorders are effectively associated with major stressful events in the premorbid period. The relationship between stressful events and autoimmune disease was weaker in studies with a high proportion of female subjects [β=- 0. 004, p< 0. 01] and stronger in studies that considered a longer interval between stressors and onset of disease [β= 0. 16, p< 0. 01]. CONCLUSIONS: The results of this meta-analysis suggest that stressors may {{play an important role in}} the etiopathogenesis of autoimmune disorders. Only prospective studies can provide more certain inference about the causality of this relationshi...|$|E
40|$|About 300 {{experiments}} {{have tried to}} determine {{the value of the}} Newtonian gravitational constant, G, so far, but large discrepancies in the results have made it impossible to know its value precisely. The weakness of the gravitational interaction and the impossibility of shielding the effects of gravity make it very difficult to measure G while keeping systematic effects under control. Most previous experiments performed were based on the torsion pendulum or torsion balance scheme as in the experiment by Cavendish in 1798, and in all cases macroscopic masses were used. Here we report the precise determination of G using laser-cooled atoms and quantum interferometry. We obtain the value G= 6. 67191 (99) x 10 ^(- 11) m^ 3 kg^(- 1) s^(- 2) with a relative uncertainty of 150 parts per million (the <b>combined</b> <b>standard</b> uncertainty is given in parentheses). Our value differs by 1. 5 <b>combined</b> <b>standard</b> <b>deviations</b> from the current recommended value of the Committee on Data for Science and Technology. A conceptually different experiment such as ours helps to identify the systematic errors that have proved elusive in previous experiments, thus improving the confidence in the value of G. There is no definitive relationship between G and the other fundamental constants, and there is no theoretical prediction for its value, against which to test experimental results. Improving the precision with which we know G has not only a pure metrological interest, but is also important because of the key role that G has in theories of gravitation, cosmology, particle physics and astrophysics and in geophysical models. Comment: 3 figures, 1 tabl...|$|R
30|$|<b>Standard</b> <b>deviations</b> between {{parallel}} test assemblies {{are usually}} lower for test assemblies using sets of five test specimens as required in OECD 313 than for tests using a single test specimen as allowed for TS- 2 experiments since differences in surfaces of test specimens are compensated {{to a certain}} extent if wood samples are <b>combined.</b> Relative <b>standard</b> <b>deviations</b> of tebuconazole concentrations in the eight eluate fractions (i.e. samples obtained after the defined immersion periods) from parallel experiments were 11 % (3 % to 24 %) in TS- 2 experiments using two test specimens per test assembly and 6 % (3 % to 10 %) in OECD 313 experiments. Relative <b>standard</b> <b>deviations</b> of copper concentrations in the corresponding eluate fractions of parallel experiments were 14 % (6 % to 42 %) in TS- 2 experiments and 2 % (1 % to 5 %) in OECD 313 experiments. The maximum <b>standard</b> <b>deviations</b> for the TS- 2 experiments were observed for the first eluate fractions that were obtained after 2 h of immersion. This observation probably reflects concentration differences {{on the surface of the}} test specimens at the beginning of the test.|$|R
40|$|This paper {{examines}} three {{alternative measures}} of {{exchange rate risk}} {{that could be used}} to develop a risk-based capital requirement for banks with foreign-exchange exposure. One measure, the <b>standard</b> <b>deviation</b> of the portfolio, is constructed under the assumption that exchange rate changes are distributed normally. While this measure is widely used in a variety of financial applications, it is subject to the criticism that it fails to capture well the behavior of exchange rate changes in the tails of their density function. A second possible measure is developed that <b>combines</b> the <b>standard</b> <b>deviation</b> and a method used by the Bank of England to assess foreign exchange exposure. This measure fails to represent the tail behavior and correlation patterns of exchange rates. The third measure uses nonparametric methods to determine capital requirements. The third measure does not suffer from the deficiencies of the other two: it allows for a rich pattern of exchange rate correlations and for non-normal characteristics in the tails of the density function.; Because of the generality of the nonparametric method, it is used to quantitatively assess the deficiencies of the other two measures. In a sample of simulated portfolios of marks, yen, and sterling, it is shown that the <b>standard</b> <b>deviation</b> measure is likely to yield capital requirements that are too small relative to the nonparametric measure. The second measure behaves on average like the <b>standard</b> <b>deviation</b> measure but the capital requirement is more erratic: it generates too much capital for some portfolios and too little capital for others in larger proportions than the <b>standard</b> <b>deviation</b> measure. Bank capital; Foreign exchange...|$|R
40|$|Background:High impact {{physical}} activity (PA) {{is thought to}} improve skeletal health, but its relation to other health outcomes are unclear. We investigated associations between PA impact magnitude and body mass index (BMI) in older adults. Methods:Data {{were taken from the}} Cohort for Skeletal Health in Bristol and Avon (COSHIBA), Hertfordshire Cohort Study, and MRC National Survey of Health and Development. Vertical acceleration peaks from 7 -day hip-worn accelerometer recordings were used to classify PA as low (0. 5 < g < 1. 0 g), medium (1 < g < 1. 5 g), or higher (≥ 1. 5 g) impact. Cohort-specific associations of low, medium, and higher impact PA with BMI were examined using linear regressions and estimates combined using random-effects meta-analysis. Results:A total of 1182 participants (mean age = 72. 7 years, 68 % female) were included. Low, medium, and higher impact PA were inversely related to BMI in initial models. After adjustment for confounders and other impacts, low, but not medium or higher, impacts were inversely related to BMI (− 0. 31, p <. 001 : overall <b>combined</b> <b>standard</b> <b>deviation</b> change in BMI per doubling in the number of low impacts). In adjusted analyses of body composition measured by dual-energy X-ray absorptiometry in COSHIBA, low, but not medium or higher, impacts were inversely related to total body fat mass (− 0. 19, p <. 001) and android:gynoid fat mass ratio (− 0. 16, p =. 01), whereas high impact PA was weakly and positively associated with lean mass (0. 05, p =. 06). Conclusions:Greater exposure to PA producing low magnitude vertical impacts was associated with lower BMI and fat mass at older age. Low impact PA may help reduce obesity risk in older adults. </p...|$|E
40|$|In offset printing, {{dampening}} solution {{is used to}} create a good balance in the process. If too much water is transferred to the paper, the sheet can change its size between the printing units, due to water absorption, and cause a problem with the colour register. This phenomenon is usually referred to as fanout. In this degree project, an investigation was made to see if the paper dimensions changed through its way in the sheet-fed printing process. The instrument Luchs Register Measuring Systems (Lynx) was used, and a method for measuring if the paper changed its dimensions with this instrument, was developed. Paper qualities with three different grammages were used, 90, 130 and 250 gsm. This investigation showed that all paper qualities changed their size with widening in the gripper edge in the range of 10 - 70 µm and in the trailing edge the increase was 10 - 130 µm. The elongations of the papers were in the range of 10 - 300 µm. The papers with lowest grammage changed more than the heavier. To see if the print had been affected of the widening and elongation, print quality parameters like relative contrast, dot gain and mottle were correlated with the Lynx data from the sheets. The group of papers that gave correlations were in 130 gsm. The sheets had visual doubling and the <b>combined</b> <b>standard</b> <b>deviation</b> from the Lynx marks K 3, K 5 and K 21 correlated with dot gain. When the variations increased so did the dot gain and this indicates that the doubling was due to the widening. There was also a correlation between the standard deviation from K 3 and Mottle. The sheets widened with an average of 30 µm in the gripper edge and since there probably were doubling due to widening it also affected the Mottle values. What the widening depends on is hard to tell. Since widening was so small, it could be due to water absorption, papers being ironed out or maybe the sheets have been flattened out. It probably needs a more detailed investigation to find out what causes the widening. Further investigations about how print quality is affected by the register accuracy of a printing machine should include a print form with measuring areas close to the Lynx marks. The measuring areas should contain fine hairlines, negative text printed with at least two colours and some pictures to evaluate together with standard measuring should give a good knowledge about the subject...|$|E
40|$|Initially, the {{statistic}} {{benefit of}} flood predictions obtained by combining reliable regional and scarce site hydrometric data is pointed out. Then the mathematical equations for <b>combining</b> mean and <b>standard</b> <b>deviation</b> logarithms of regional and site data are exposed, {{as well as}} the necessary expressions for desired predictions, based on Student's t distribution. Later two numerical applications are described, the first one based in Carrizal hydrometric station located in Santiago River in Nayarit and the second one which makes use of five water gauging stations in Tempoal River in Veracruz. Finally, a conclusion is formulated pointing out the simplicity of that method and the accuracy of its predictions...|$|R
40|$|This {{research}} {{investigates the}} capability of the multitemporal RADARSAT Fine-Beam C-HH SAR imagery for extracting landuse/land-cover information in the rural-urban fringe of the Greater Toronto Area (GTA) using various image processing techniques and classification algorithms. Five-date RADARSAT fine-beam SAR images were acquired during May to August in 2002. The major landuse/land-cover classes were high-density built-up areas, low-density built-up areas, roads, forests, parks, golf courses, water and three types of agricultural lands. These ten classes were chosen to characterize the complex landuse/land-cover types in the rural-urban fringe of the GTA. The results demonstrated that, for identifying landuse/land-cover classes, five-date raw SAR imagery yielded very poor result due to speckles. The best result was achieved for <b>combined</b> Mean, <b>Standard</b> <b>Deviation</b> and Correlation texture images using artificial neural networks (ANN) (overall accuracy: 89. 7 % and Kappa: 0. 886). These high accuracies indicated that RADARSAT fine-beam SAR {{has the potential for}} operational landuse/land-cover mapping in urban environments...|$|R
40|$|Abstract: This paper {{investigates the}} {{application}} of widely used K-Medoids based clustering algorithm on data collected through CoMon facility for the PlanetLab testbed. The averaged values of various metrics in passively collected slice-centric data has been considered for clustering purposes. Various groups of slices, depicting similar resource usage patterns {{have been identified in}} original data set. These clusters have been represented in reduced dimensional space formed by first two principal components of original data set. In order to capture variations in pattern of resource usage by various slices at a PlanetLab node, clustering of <b>standard</b> <b>deviations</b> of various metrics have also been carried out. Further, <b>combining</b> averaged and <b>standard</b> <b>deviation,</b> clustering has also been performed on index of dispersion computed from the original data set. It has been found that K-medoid based clustering can effectively split the original data space into various sub-spaces of different resource usage behaviour of slices. Thus, it can lead to better resource management and control in publicly available testbeds...|$|R
40|$|The most {{frequent}} computation performed in forensic toxicology is probably Widmark’s equation. Despite its wide use in estimating {{the number of}} drinks consumed or the blood or breath alcohol concentration, the computation rarely includes an estimate of its uncertainty. The equation includes at least seven random uncertain variables. Uncertainty estimates for Widmark calculations can easily be determined using the general method of error propagation. Widmark also developed an equation for estimating uncertainty. Assuming reasonable values for the variables and their uncertainty, an example resulting in 10. 4 drinks yielded a <b>combined</b> uncertainty (<b>standard</b> <b>deviation)</b> of 1. 3 drinks (CV= 12. 3 %) with the general error propagation method. Estimating the blood alcohol concentration yielded 0. 120 g/ 100 ml with an uncertainty of 0. 0255 g/ 100 ml (CV= 21. 2 %). Employing the method developed by Widmark yielded a larger uncertainty estimate of 1. 6 drinks (CV= 15. 4 %) resulting from the inappropriate application of the error propagation method. Including reliable estimates of uncertainty should enhance the legal admissibility and confidence in Widmark estimates...|$|R
40|$|The {{upcoming}} GNSS Galileo, {{with its}} new satellite geometry and frequency plan, will not only bring many benefits for navigation and positioning but also help to improve ionosphere delay estimation. This paper investigates ionosphere estimation with Galileo and compares it with the results from GPS-only and <b>combined</b> GPS-Galileo. The <b>standard</b> <b>deviation</b> of the Vertical Total Electron Content (VTEC) at a certain location can improve significantly by 40 per cent. Various Galileo configurations are considered to assess the differences in frequency plan and signals to be used. The IGS network, which is involved in producing the current IGS Global Ionosphere Map (GIM) {{is used in the}} simulation for more realistic results. A more accurate GIM will in its turn improve navigation and positioning performance. Aerospace Engineerin...|$|R
40|$|Abstract. We give a local {{central limit theorem}} {{for simple}} random walks on Z d, {{including}} Gaussian error estimates. The detailed proof <b>combines</b> <b>standard</b> large <b>deviation</b> techniques with Cramér-Edgeworth expansions for lattice distributions. A simple random walk on Z d is a sequence of independent identically distributed random variables (steps) on Z d. We here suppose that the steplength is bounded, that is, the measure describing a single step has bounded support, and {{our goal is to}} give local approximations for the sum of the first n steps of the walk. Of course the Central Limit Theorem applies, and there also exist local CLTs, see for example Bhattacharya/Rao, [1]. In our work on local approximations for polymer measures (see the joint paper with E. Bolthausen [2]), we considered weakly self-avoiding walks as perturbations of simple walks. In order to obtain good bounds for the perturbations we thus needed good bounds for the simple walks as well. In particular, we needed Gaussian tail estimates, which were not provided by [1]. Therefore we proved a version of...|$|R
40|$|Uncertainty is an {{inherent}} property of all measurements. The magni-tude of this uncertainty {{will determine the}} number of meaningful dig-its that should be reported in a measurement result. Several statistical arguments are considered providing evidence that three digit truncated results are more appropriate than two since the first significant digit of the <b>combined</b> uncertainty (<b>standard</b> <b>deviation)</b> in breath alcohol measurement is found in the third decimal place. Probably, the most compelling reason for reporting three digits is the significant reduction in combined uncertainty compared with the use of two digits. For a breath alcohol concentration of 0. 089 g/ 210 L, the combined uncertainty for two digit results is∼ 0. 0042 g/ 210 L, com-pared with 0. 0031 g/ 210 L for three digit results. The historical practice of reporting two digit truncated results in forensic breath alcohol analy-sis has been largely based on the use of analog scale instruments with 0. 01 g/ 210 L scale resolution. With today’s modern digital instrumenta-tion, this practice should be reconsidered. While the focus of this paper is on breath alcohol analysis, the general principles will apply to any quantitative analytical measurement...|$|R
40|$|We {{present a}} novel physics-based {{retrieval}} method to infer total column mixing ratios of methane (XCH 4) {{and carbon dioxide}} (XCO 2) from space-born short-wavelength infrared (SWIR) Earth-radiance observations over the cloud-covered ocean. In nadir observing geometry in the SWIR spectral range backscattering at the ocean surface is negligible. Hence, space-borne radiance measurements of ocean scenes generally do not provide sufficient signal level to retrieve XCO 2 and XCH 4. Our approach specifically targets cloudy GOSAT ocean sounding to provide sufficient radiance signal in nadir soundings in ocean areas. Currently, exploiting space-borne SWIR sounding in ocean relies on observation in sun glint geometry, observing the specular solar reflection at the ocean surface. The glint observation mode requires cloud-free conditions and a suitable observation geometry, severely limiting their number and geographical coverage. The proposed method {{is based on the}} existing RemoTeC algorithm [e. g. Butz et al., 2011; Guerlet et al., 2013] that is extensively used to retrieve CH 4 and CO 2 columns from GOSAT SWIR measurements over land. For ocean pixels, we describe light scattering by clouds and aerosols by a single-layer water cloud with Gaussian height distribution. We infer the height and the geometrical thickness of the cloud layer jointly with the droplet size and the number density and the column abundances of CO 2, CH 4 and H 2 O. The CO 2 and CH 4 column product is validated with ground-based total column measurements performed at 8 stations from the TCCON network that are geographically close to an ocean coastline. For the TCCON site with the most robust statistics (Lauder, New Zealand), we find a retrieval bias of 0. 36 % for XCH 4 <b>combined</b> with a <b>standard</b> <b>deviation</b> of retrieval errors of 1. 12 %. For XCO 2, the bias is 0. 51 % <b>combined</b> with a <b>standard</b> <b>deviation</b> of 1. 03 %. Averaged over all TCCON sites, our retrievals are biased - 0. 01 % for XCO 2 and - 0. 32 % for XCH 4. The <b>standard</b> <b>deviation</b> of station biases amounts to 0. 45 % for XCO 2 and 0. 35 % for XCH 4...|$|R
40|$|Estimation of the quantile, mean, and {{variability}} of populations is generally {{carried out by}} means of sample estimates. Given normality of the parent population, the distribution of sample mean and sample variance is straightforward. However, when normality cannot be assumed, inference is usually based on approximations {{through the use of}} the Central Limit theorem. Furthermore, the data generated from many real populations may be naturally bounded; i. e., weights, heights, etc. Thus, a normal population, with its infinite bounds, may not be appropriate, and the distribution of any specified quantile, such as the median, is not obvious. Using Bayesian analysis and maximum entropy, procedures are developed which produce distributions for any specified quantile, the mean, and <b>combined</b> mean and <b>standard</b> <b>deviation.</b> These methods require no assumptions on the form of the parent distribution or the size of the sample and inherently make use of existing bounds...|$|R
40|$|PURPOSE: To {{correctly}} evaluate realistic treatment {{plans in}} terms of absorbed dose to the clinical target volume (CTV), equivalent uniform dose (EUD), and tumor control probability (TCP) {{in the presence of}} execution (random) and preparation (systematic) geometric errors. MATERIALS AND METHODS: The dose matrix is blurred with all execution errors to estimate the total dose distribution of all fractions. To include preparation errors, the CTV is randomly displaced (and optionally rotated) many times with respect to its planned position while computing the dose, EUD, and TCP for the CTV using the blurred dose matrix. Probability distributions of these parameters are computed by combining the results with the probability of each particular preparation error. We verified the method by comparing it with an analytic solution. Next, idealized and realistic prostate plans were tested with varying margins and varying execution and preparation error levels. RESULTS: Probability levels for the minimum dose, computed with the new method, are within 1 % of the analytic solution. The impact of rotations depends strongly on the CTV shape. A margin of 10 mm between the CTV and planning target volume is adequate for three-field prostate treatments given the accuracy level in our department; i. e., the TCP in a population of patients, TCP(pop), is reduced by less than 1 % due to geometric errors. When reducing the margin to 6 mm, the dose must be increased from 80 to 87 Gy to maintain the same TCP(pop). Only in regions with a high-dose gradient does such a margin reduction lead to a decrease in normal tissue dose for the same TCP(pop). Based on a rough correspondence of 84 % minimum dose with 98 % EUD, a margin recipe was defined. To give 90 % of patients at least 98 % EUD, the planning target volume margin must be approximately 2. 5 Sigma + 0. 7 sigma - 3 mm, where Sigma and sigma are the <b>combined</b> <b>standard</b> <b>deviations</b> of the preparation and execution errors. This recipe corresponds accurately with 1 % TCP(pop) loss for prostate plans with clinically reasonable values of Sigma and sigma. CONCLUSION: The new method computes in a few minutes the influence of geometric errors on the statistics of target dose and TCP(pop) in clinical treatment plans. Too small margins lead to a significant loss of TCP(pop) that is difficult to compensate for by dose escalatio...|$|R
40|$|The Persistent Scatterer Interferometry Wide Area Product (PSI-WAP) {{based on}} ERS 1 & ERS 2 radar data {{has been used}} in the seismic active area of Atalanti Faulting Zone (Central Greece) to {{spatially}} and temporally study the ground deformation for the period 1992 - 2003. The observed LOS velocity field, with values ranging between - 0. 5 to - 5. 0 mm/yr, <b>combined</b> with small <b>standard</b> <b>deviation</b> velocity values reveals an almost linear type of ground deformation. The most intense subsidence was associated to alluvia deposits and man made activities (intense water pumping). Differential motions along the main faulting zones have also been clearly identified. GPS results reflected a similar pattern of motions (subsidence) with the identified interferometric image. The recorded seismicity in the area is not significant for the PSI-WAP period. The micro-seismic activity (M< 3) is mainly confined peripherally and does not seem to confidently affect the observed ground deformation...|$|R
40|$|The {{investigation}} {{aimed at}} studying {{the nature of}} distribution of means of internal assessment scores awarded by different teacher-training institutions {{in each of the}} eight courses of Bachelor of Education (B. Ed.) Programme run by Himachal Pradesh University for the years 2008, 2009 and 2010. The sample for the study included all the candidates who were enrolled in B. Ed. Programme in different teachertraining institutions affiliated to Himachal Pradesh University and passed their B. Ed. examination during the sessions 2007 - 08, 2008 - 09 and 2009 - 10. The means of internal assessment marks awarded by different teacher-training institutions in each of the eight courses for the three years were computed. The results revealed that (a) The overall tendency of awarding internal assessment marks remained almost the same for all the eight courses {{in each of the three}} years; (b) The <b>combined</b> means, <b>standard</b> <b>deviations,</b> minimum scores and maximum scores were very nearly the same for all the eight courses in each of the three years; (c) Only a nominal number of students were awarded an internal assessment score of 13, 14, 15 or 16 and majority of students were awarded a score of 17 or above in each of the three years; (d) The colleges have been too liberal in awarding internal assessment marks to students in each of the three years; and (e) An increasing trend emerged in awarding internal assessment marks from 2008 to 2010...|$|R
40|$|We {{report on}} design details and first results {{obtained}} with the transportable absolute gravimeter MPG- 2 ("Max-Planck-Gravimeter"). It is developed as an {{evolution of the}} stationary device MPG- 1, completed in 2007. The MPG- 2 is built on a common scheme where {{the position of a}} freely falling object is monitored. The setup consists of a ballistic block, an interferometer and the electronics. Free fall drops can be repeated every 10 s with the <b>standard</b> <b>deviation</b> close to 30 mu gal. A one-day gravity observation gives a result with a <b>standard</b> <b>deviation</b> of the mean of less than 5 mu gal. A prototype of the MPG- 2 took part in the ECAG- 2007. New measurements at the reference gravity station "Bad Homburg", Germany confirmed the declared <b>combined</b> <b>standard</b> uncertainty of 50 mu gal...|$|R
30|$|Granulocytes are {{evaluated}} {{with a score}} of 0.71 (<b>standard</b> <b>deviation</b> 0.49), lymphocytes with a score of 2.86 (<b>standard</b> <b>deviation</b> 0.38), macrophages with a score of 1.86 (<b>standard</b> <b>deviation</b> 0.38), foreign-body giant cells with a score of 0.29 (<b>standard</b> <b>deviation</b> 0.49), vascularization with a score of 1.71 (<b>standard</b> <b>deviation</b> 0.49), fibrosis with a score of 2.29 (<b>standard</b> <b>deviation</b> 0.49) and wear debris (mainly microparticles of PE, bone fragments, no necrosis, extensive lymphocyte infiltration) with a score of 1.14 (<b>standard</b> <b>deviation</b> 0.69).|$|R
30|$|Granulocytes are {{evaluated}} {{with a score}} of 2.43 (<b>standard</b> <b>deviation</b> 0.53), lymphocytes with a score of 1.43 (<b>standard</b> <b>deviation</b> 0.79), macrophages with a score of 2.43 (<b>standard</b> <b>deviation</b> 0.53), foreign-body giant cells with a score of 1.29 (<b>standard</b> <b>deviation</b> 0.95), vascularization with a score of 2.71 (<b>standard</b> <b>deviation</b> 0.49), fibrosis with a score of 2.43 (<b>standard</b> <b>deviation</b> 0.79) and wear debris (mainly microparticles of PE, bone fragments, no necrosis, 1 / 7 with extensive lymphocyte infiltration) with score of 1.57 (<b>standard</b> <b>deviation</b> 1.27).|$|R
30|$|Granulocytes are {{expressed}} {{with a score}} of 1.29 (<b>standard</b> <b>deviation</b> 0.49), lymphocytes with a sore of 1.71 (<b>standard</b> <b>deviation</b> 0.95), macrophages with a score of 2.71 (<b>standard</b> <b>deviation</b> 0.49), foreign-body giant cells with a score of 1.57 (<b>standard</b> <b>deviation</b> 0.79), vascularization with a score of 1.29 (<b>standard</b> <b>deviation</b> 0.49), fibrosis with a score of 1.43 (<b>standard</b> <b>deviation</b> 0.98) and wear debris (mainly macroparticles of PE and PMMA, bone fragments, few necrosis, 1 / 7 with extensive lymphocyte infiltration) with a score of 2.0 (<b>standard</b> <b>deviation</b> 0.82).|$|R
50|$|However, {{the sample}} <b>standard</b> <b>deviation</b> is not {{unbiased}} {{for the population}} <b>standard</b> <b>deviation</b> - see unbiased estimation of <b>standard</b> <b>deviation.</b>|$|R
30|$|There are no granulocytes in the periprosthetic {{membrane}} after semiquantitative {{evaluation of}} the examined tissue samples (mean value 0.0, <b>standard</b> <b>deviation</b> 0.0). Lymphocytes are expressed with a score of 0.73 (<b>standard</b> <b>deviation</b> 0.47), macrophages with a score of 2.91 (<b>standard</b> <b>deviation</b> 0.30), foreign-body giant cells with a score of 2.64 (<b>standard</b> <b>deviation</b> 0.50), vascularization with a score of 1.0 (<b>standard</b> <b>deviation</b> 0.0), fibrosis with a score of 2.09 (<b>standard</b> <b>deviation</b> 1.30) and wear debris (mainly macroparticles of polyethylene (PE) and bone cement (PMMA), no non-implant material, few necrosis, scattered lymphocytes) with a score of 2.18 (<b>standard</b> <b>deviation</b> 0.75).|$|R
3000|$|... ‘Effective {{training}} program schedule for customers’ {{has also been}} observed having the lowest <b>standard</b> <b>deviation</b> and lowest variance. ‘Awareness level of customers’ has been observed having the highest <b>standard</b> <b>deviation</b> and highest variance. Variance and <b>standard</b> <b>deviation</b> measure variability within a distribution. <b>Standard</b> <b>deviation</b> indicates how much, on average, each of the values in the distribution deviates from {{the mean of the}} distribution. Higher <b>standard</b> <b>deviation</b> means having large variations in the data, and lower <b>standard</b> <b>deviation</b> means having small variations in the data. Lower <b>standard</b> <b>deviation</b> will indicate the reliability of data. Variance is the average squared deviations of the mean.|$|R
5000|$|... where [...] {{represents}} the <b>standard</b> <b>deviation</b> {{of the function}} , [...] {{represents the}} <b>standard</b> <b>deviation</b> of , [...] represents the <b>standard</b> <b>deviation</b> of , and so forth.|$|R
30|$|For {{each of the}} 16 {{aforementioned}} features (13 MFCCs, 3 FFT), {{we calculated}} the mean, <b>standard</b> <b>deviation</b> (std), mean <b>standard</b> <b>deviation</b> (mean std), and <b>standard</b> <b>deviation</b> of <b>standard</b> <b>deviation</b> (std std) over all frames. This led {{to a total of}} 64 features and 8 rhythmic features.|$|R
50|$|Based on {{records from}} 1975 to 1986 from {{approximately}} 5500 Carora cows in 28 herds, the average milk yield was 2433 kg. in 244 {{days with a}} <b>standard</b> <b>deviation</b> of 1062 kg. Average lactation length was 267 days with a <b>standard</b> <b>deviation</b> of 64. Age at first calving was 37.5 months with a <b>standard</b> <b>deviation</b> of 6.6. Dry period was 128 days with a <b>standard</b> <b>deviation</b> of 89. Calving interval was 405 days with a <b>standard</b> <b>deviation</b> of 60. Body weight was 426 kg, with a sta with <b>standard</b> <b>deviation</b> of 31.|$|R
