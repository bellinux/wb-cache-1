8|13|Public
25|$|The sorting routine uses two subroutines, heapify and siftDown. The {{former is}} the common in-place heap {{construction}} routine, while {{the latter is a}} <b>common</b> <b>subroutine</b> for implementing heapify.|$|E
5000|$|The LZMA SDK {{comes with}} the BCJ / BCJ2 {{preprocessor}} included, so that later stages are able to achieve greater compression: For x86, ARM, PowerPC (PPC), IA-64 Itanium, and ARM Thumb processors, jump targets are normalized before compression by changing relative position into absolute values. For x86, this means that near jumps, calls and conditional jumps (but not short jumps and conditional jumps) are converted from the machine language [...] "jump 1655 bytes backwards" [...] style notation to normalized [...] "jump to address 5554" [...] style notation; all jumps to 5554, perhaps a <b>common</b> <b>subroutine,</b> are thus encoded identically, making them more compressible.|$|E
5000|$|The sorting routine uses two subroutines, [...] and [...] The {{former is}} the common in-place heap {{construction}} routine, while {{the latter is a}} <b>common</b> <b>subroutine</b> for implementing [...] [...] (Put elements of a in heap order, in-place) procedure heapify(a, count) is (start is assigned the index in a of the last parent node) (the last element in a 0-based array is at index count-1; find the parent of that element) start ← iParent(count-1) [...] while start ≥ 0 do (sift down the node at index start to the proper place such that all nodes below [...] the start index are in heap order) siftDown(a, start, count - 1) (go to the next parent node) start ← start - 1 (after sifting down the root all nodes/elements are in heap order) [...] (Repair the heap whose root element is at index start, assuming the heaps rooted at its children are valid) procedure siftDown(a, start, end) is root ← start [...] while iLeftChild(root) ≤ end do (While the root has at least one child) child ← iLeftChild(root) (Left child of root) swap ← root (Keeps track of child to swap with) [...] if aswap < achild swap ← child (If there is a right child and that child is greater) if child+1 ≤ end and aswap < achild+1 swap ← child + 1 if swap = root (The root holds the largest element. Since we assume the heaps rooted at the [...] children are valid, this means that we are done.) return else swap(aroot, aswap) root ← swap (repeat to continue sifting down the child now) ...|$|E
40|$|Three {{versions}} of rigid lid programs are presented: one for near field simulation; the second for far field unstratified situations; {{and the third}} for stratified basins, far field simulation. The near field simulates thermal plume areas, and the far field version simulates larger receiving aquatic ecosystems. Since these versions have many <b>common</b> <b>subroutines,</b> a unified testing is provided, with main programs for the three possible conditions listed...|$|R
5000|$|The {{ability for}} {{multiple}} users to quickly load {{and use a}} collection of <b>common</b> reentrant <b>subroutines,</b> which are available in shared virtual memory.|$|R
40|$|Abstract-A program {{package is}} {{provided}} {{for analysis of}} kinetic mechanisms on personal computers. KINAL consists of four programs called DIFF, SENS, PROC and YRED. These require similar input data and use <b>common</b> <b>subroutines.</b> DIFF solves stiff differential equations and SENS computes the local concentration sensitivity matrix. PROC generates the rate sensitivity matrix or the quasi-stationary sensitivity matrix from concentration data or uses a matrix computed by SENS and extracts the kinetic information inherent in sensitivity matrices by principal component analysis. Finally, YRED provides suggestions {{for the elimination of}} species from the reaction mechanism...|$|R
40|$|This paper {{introduces}} a new architecture for automating the generalization of program {{structure and the}} recognition of common patterns {{in the area of}} malware analysis. By using massively parallel processing on large malware program sets we can recognize common code sequences, such as loop constructs, if-then-else structures, and subroutine calls. We can also recognize <b>common</b> <b>subroutine</b> sequences. The Concordia architecture generalizes the recognized elements so they can be collected into invariant forms. The invariant forms can be used by the analyst to understand the program being analyzed. The invariant forms {{can also be used to}} classify large numbers of programs automatically...|$|E
40|$|The Bernstein-Sato {{polynomial}} (or global b-function) is {{an important}} invariant in singularity theory, which can be computed using symbolic methods {{in the theory of}} D-modules. After surveying algorithms for computing the global b-function, we develop a new method to compute the local b-function for a single polynomial. We then develop algorithms that compute generalized Bernstein-Sato polynomials of Budur-Mustata-Saito and Shibuta for an arbitrary polynomial ideal. These lead to computations of log canonical thresholds, jumping coefficients, and multiplier ideals. Our algorithm for multiplier ideals simplifies that of Shibuta and shares a <b>common</b> <b>subroutine</b> with our local b-function algorithm. The algorithms we present have been implemented in the D-modules package of the computer algebra system Macaulay 2. Comment: 16 pages, to appear in ISSAC 201...|$|E
40|$|This work {{contributes}} to throughput calculation for real-time multiprocessor applications experiencing dynamic workload variations. We {{focus on a}} method to predict the system throughput when processing an arbitrarily long data frame given the meta-characteristics of the workload in that frame. This {{is useful for different}} purposes, such as resource allocation or dynamic voltage scaling in embedded systems. An accurate enough analysis is not trivial when two factors are combined: parallelism and dynamic workload variations. In earlier work, two analysis methods showed good accuracy for several application examples, but no comparative experiments were carried out. In this work, we contribute new propositions to the theoretical basis of the previous methods. Based on these propositions, we remove a potential problem in a <b>common</b> <b>subroutine</b> and propose a new analysis method. We compare the methods experimentally. The new method provides a significant reduction of the throughput prediction error, up to 12 %...|$|E
5000|$|Another idiom is {{to shift}} {{parameters}} off of [...] This is especially <b>common</b> when the <b>subroutine</b> takes only one argument or for handling the [...] argument in object-oriented modules.|$|R
40|$|Abstract—Subsequence {{similarity}} {{search is}} one of the most <b>common</b> <b>subroutines</b> in time series data mining algorithms. According to previous studies, Dynamic Time Warping (DTW) distance is the best distance measurement in many domains. However, the high computational complexity of DTW distance makes it a critical bottleneck in many subsequence similarity search applications. In some applications, the performance of software implementation still could not meet the high requirements of applications. Under the circumstance, some hardware implementations of DTW-based algorithms were proposed in the data mining community, using GPUs and FPGAs. In this paper, we propose a full system implementation for subsequence similarity search on AMD heterogeneous computing platform, including complete normalization pre-processing, two kinds of improved lower bound for pruning, and a novel segmented parallel DTW calculation process, which efficiently utilizes the capacity of CPU and GPU on the platform. Our work achieves one to two orders of magnitude speedup compared to software implementation and several times speedup to other GPU or FPGA implementations...|$|R
50|$|The XL1 phase {{contains}} a large set of plug-ins, notably , that provide <b>common</b> abstractions like <b>subroutine,</b> data type and variable declaration and definition, {{as well as}} basic structured programming statements, like conditionals or loops.|$|R
40|$|Abstract This paper {{introduces}} Concordia [4], a new architecture for automating {{generalization of}} program structures {{and recognition of}} common patterns for malware analysis. By using massively parallel processing on large malware program sets Concordia can recognize common code sequences, such as loop constructs, if-then-else structures, and subroutine calls, as well as <b>common</b> <b>subroutine</b> sequences. The Concordia architecture generalizes the recognized elements for collection into invariant forms. The invariant forms {{can be used by}} an analyst to understand a particular program. These invariant forms can be used to classify large numbers of programs automatically. Concordia gathers and organizes expertise from malware specialists in a way that can be used by organizations to recognize and respond to malware threats. Recent trends have shown that threats are becoming more focused and specific, which means that standard, signature-based approaches to malware will no longer be effective. Research and development in new malware analysis technology is required to respond to this trend. The end result could be characterized as a “Google c ○ for Malware”, for rapidly recognizing malicious software...|$|E
40|$|Sparse matrix-vector {{multiplication}} (shortly SpM×V) {{is one of}} most <b>common</b> <b>subroutines</b> in numerical linear algebra. The {{problem is}} that the memory access patterns during SpM×V are irregular, and utilization of the cache can suffer from low spatial or temporal locality. Approaches to improve the performance of SpM×V are based on matrix reordering and register blocking. These matrix transformations are designed to handle randomly occurring dense blocks in a sparse matrix. The efficiency of these transformations depends strongly on the presence of suitable blocks. The overhead of reorganization of a matrix from one format to another is often of the order of tens of executions of SpM×V. For this reason, such a reorganization pays off only if the same matrix A is multiplied by multiple different vectors, e. g., in iterative linear solvers. This paper introduces an unusual approach to accelerate SpM×V. This approach can be combined with other acceleration approaches and consists of three steps: 1) dividing matrix A into non-empty regions, 2) choosing an efficient way to traverse these regions (in other words, choosing an efficient ordering of partial multiplications), 3) choosing the optimal type of storage for each region. All these three steps are tightly coupled. The first step divides the whole matrix into smaller parts (regions) that can fit in the cache. The second step improves the locality during multiplication due to better utilization of distant references. The last step maximizes the machine computation performance of the partial multiplication for each region. In this paper, we describe aspects of these 3 steps in more detail (including fast and time-inexpensive algorithms for all steps). Our measurements prove that our approach gives a significant speedup for almost all matrices arising from various technical areas...|$|R
40|$|HAL accomplishes three {{significant}} objectives: (1) increased readability, {{through the}} use of a natural two-dimensional mathematical format; (2) increased reliability, by providing for selective recognition of <b>common</b> data and <b>subroutines,</b> and by incorporating specific data-protect features; (3) real-time control facility, by including a comprehensive set of real-time control commands and signal conditions. Although HAL is designed primarily for programming on-board computers, it is general enough to meet nearly all the needs in the production, verification and support of aerospace, and other real-time applications...|$|R
40|$|The Mission Analysis Evaluation and Space Trajectory Operations program {{known as}} MAESTRO is described. MAESTRO is an all FORTRAN, block style, {{computer}} {{program designed to}} perform various mission control tasks. This manual is a guide to MAESTRO, providing individuals the capability of modifying the program to suit their needs. Descriptions are presented {{of each of the}} subroutines descriptions consist of input/output description, theory, subroutine description, and a flow chart where applicable. The programmer's manual also contains {{a detailed description of the}} <b>common</b> blocks, a <b>subroutine</b> cross reference map, and a general description of the program structure...|$|R
5000|$|All input/output requests, {{whether by}} the MTS job program itself or by a program running under MTS, is done using a <b>common</b> set of <b>subroutine</b> calls (GETFD, FREEFD, READ, WRITE, CONTROL, GDINFO, ATTNTRP, ...). The same subroutines are used {{no matter what}} program is doing the I/O {{and no matter what}} type of file or device is being used (typewriter or {{graphics}} terminal, line printer, card punch, disk file, magnetic and paper tape, etc.). No knowledge of the format or contents of system control blocks is required to use these subroutines. Programs may use specific characteristics of a particular device, but such programs will be somewhat less device independent.|$|R
40|$|A {{hypothetical}} {{turbofan engine}} simplified simulation with a multivariable control and sensor failure detection, isolation, and accommodation logic (HYTESS II) is presented. The digital program, written in FORTRAN, is self-contained, efficient, realistic and easily used. Simulated engine dynamics were developed from linearized operating point models. However, essential nonlinear effects are retained. The simulation {{is representative of}} the hypothetical, low bypass ratio turbofan engine with an advanced control and failure detection logic. Included is a description of the engine dynamics, the control algorithm, and the sensor failure detection logic. Details of the simulation including block diagrams, variable descriptions, <b>common</b> block definitions, <b>subroutine</b> descriptions, and input requirements are given. Example simulation results are also presented...|$|R
5000|$|A {{subroutine}} may {{be written}} {{so that it}} expects to obtain one or more data values from the calling program (its parameters or formal parameters). The calling program provides actual values for these parameters, called arguments. Different programming languages may use different conventions for passing arguments: The subroutine may return a computed value to its caller (its return value), or provide various result values or output parameters. Indeed, a <b>common</b> use of <b>subroutines</b> is to implement mathematical functions, in which {{the purpose of the}} subroutine is purely to compute one or more results whose values are entirely determined by the parameters passed to the subroutine. (Examples might include computing the logarithm of a number or the determinant of a matrix.) ...|$|R
40|$|NASA JPL {{has been}} {{creating}} a code in FORTRAN called KINETICS {{to model the}} chemistry of planetary atmospheres. Recently {{there has been an}} effort to introduce Message Passing Interface (MPI) into the code so as to cut down the run time of the program. There has been some implementation of MPI into KINETICS; however, the code could still be more efficient than it currently is. One way to increase efficiency is to send only certain variables to all the processes when an MPI subroutine is called and to gather only certain variables when the subroutine is finished. Therefore, all the variables that are used in three of the main subroutines needed to be investigated. Because of the sheer amount of code that there is to comb through this task was given as a ten-week project. I have been able to create flowcharts outlining the <b>subroutines,</b> <b>common</b> blocks, and functions used within the three main subroutines. From these flowcharts I created tables outlining the variables used in each block and important information about each. All this information will be used to determine how to run MPI in KINETICS in the most efficient way possible...|$|R

