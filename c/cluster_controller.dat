29|38|Public
5000|$|The VMware Broker is an {{optional}} component that provides an AWS-compatible interface for VMware environments and physically {{runs on the}} <b>Cluster</b> <b>Controller.</b> The VMware Broker overlays existing ESX/ESXi hosts and transforms Eucalyptus Machine Images (EMIs) to VMware virtual disks. The VMware Broker mediates interactions between the <b>Cluster</b> <b>Controller</b> and VMware and can connect directly to either ESX/ESXi hosts or to vCenter Server.|$|E
5000|$|... 3274 <b>cluster</b> <b>controller</b> (different models {{could be}} channel-attached or remote via BSC or SDLC {{communication}} lines, and had between eight and 32 co-ax ports) ...|$|E
5000|$|The <b>Cluster</b> <b>Controller</b> (CC) {{is written}} in C and acts as the front end for a cluster within a Eucalyptus cloud and {{communicates}} with the Storage Controller and Node Controller. It manages instance (i.e., virtual machines) execution and Service Level Agreements (SLAs) per cluster.|$|E
3000|$|... save, {{the super}} {{controller}} calculates cost functions of each scheme by (16) and selects {{the scheme of}} minimum cost function, generates load transferring strategy, then sends sleeping messages and load transferring strategy to the selected <b>cluster</b> <b>controllers.</b> The <b>cluster</b> <b>controllers</b> which receive sleeping messages transfer all loads based on load transferring strategy from super controller and go to sleep.|$|R
30|$|Network topologies: the {{simulation}} {{is carried out}} in the topology as follows: there are four <b>cluster</b> <b>controllers</b> and one super controller in HybridFlow, which is to, m= 2,n= 2, respectively labeled in CC 11,CC 12,CC 21,CC 22. We assume {{the simulation}} is event-based, and when the network works, <b>cluster</b> <b>controllers</b> receive packet-in messages with M/M/ 1 /m queuing theory.|$|R
50|$|The {{original}} IBM Network Control Program {{ran on the}} 3705-I {{and supported}} access to older devices by application programs using Telecommunications Access Method (TCAM). With the advent of Systems Network Architecture (SNA), NCP was enhanced to connect <b>cluster</b> <b>controllers</b> (such as the IBM 3270) to application programs using TCAM and later to application programs using Virtual Telecommunications Access Method (VTAM). Subsequent versions of NCP were released to run on the IBM 3704, IBM 3705-II, IBM 3725. IBM 3720, or IBM 3745 Communications Controllers, all of which SNA defined as a SNA Physical Unit Type 4 (PU4). A PU4 usually had SDLC links to remote <b>cluster</b> <b>controllers</b> (PU1/PU2) or to other PU4s. Polling and addressing of the <b>cluster</b> <b>controllers</b> was performed by the NCP without mainframe intervention.|$|R
50|$|IaaS service {{components}} Cloud Controller, <b>Cluster</b> <b>Controller,</b> Walrus, Storage Controller, and VMware Broker are configurable as redundant {{systems that are}} resilient to multiple types of failures. Management state of the cloud machine is preserved and reverted to normal operating conditions {{in the event of}} a hardware or software failure.|$|E
5000|$|The Storage Controller (SC) {{is written}} in Java and is the Eucalyptus {{equivalent}} to AWS EBS. It communicates with the <b>Cluster</b> <b>Controller</b> and Node Controller and manages Eucalyptus block volumes and snapshots to the instances within its specific cluster. If an instance requires writing persistent data to memory outside of the cluster, it would need to write to Walrus, which is available to any instance in any cluster.|$|E
50|$|A {{telecommunication}} {{control unit}} (TCU), line control unit, or terminal control unit (although terminal control unit may also refer to a terminal <b>cluster</b> <b>controller)</b> is a Front-end processor for mainframes and some minicomputers which supports attachment {{of one or more}} telecommunication lines. TCUs free processors from handling the data coming in and out of RS-232 ports. The TCU can support multiple terminals, sometimes hundreds. Many of these TCUs can support RS-232 when it is required, although there are other serial interfaces as well.|$|E
5000|$|PU2 nodes are <b>cluster</b> <b>controllers</b> running {{configuration}} {{support programs}} such as IBM 3174, IBM 3274, or the IBM 4701 or IBM 4702 Branch Controller ...|$|R
50|$|IBM Storwize V7000 Gen2 and Gen2 turbo, each a {{technology}} upgrade with increased throughput {{and number of}} drives support: 720 slots per single controller or 3040 per <b>clustered</b> <b>controller.</b>|$|R
3000|$|... where C is current queuing length matrix, MA is largest {{queue length}} matrix, and M is the {{starting}} threshold in M-N policy sleeping management algorithm. When situation 1 happens, the below algorithm will not be carried out. <b>Cluster</b> <b>controllers</b> {{wait for the next}} cycle to report loadnotice signaling. When situation 2 happens, it shows that the system is in the light traffic condition and {{there is no need to}} make all <b>cluster</b> <b>controllers</b> opened. Closing some <b>cluster</b> <b>controllers</b> enables to reduce the energy consumption of system. From (14), we can see that when M becomes larger, the threshold of starting the algorithm is bigger; the possibility of starting the algorithm is bigger, then the possibility of closing controllers is bigger; more controllers are to be closed in the system. In this case, energy consumption declines, but when more controllers are in sleeping mode, time delay of transferring sleeping controllersâ€™ packet-in messages increase. In a word, when M becomes bigger, energy consumption decreases and time delay increases. On the contrary, energy consumption increases and time delay decreases. We can conclude that there is fundamental tradeoff between energy consumption and time delay with a different M. The simulation which is shown in Section 4 also verifies the accuracy of the above analysis.|$|R
5000|$|Uniscope was a {{registered}} trade mark {{for a set of}} Sperry Univac dumb terminal products. The trademark was applied for October 13, 1969. Several models were produced: the Uniscope 100, Uniscope 200, Uniscope 300, the UTS 400, the UTS 10, the UTS 20, the UTS 30, the UTS 40 and the color UTS 60. The UTS 10, UTS 20, UTS 30, UTS 40 and the color UTS 60 were [...] "intelligent terminals" [...] powered by 8-bit microprocessors, predecessor of today's powerful chips that run today's PCs. There was also the UTS 4000 <b>cluster</b> <b>controller</b> and terminal line, and the SVT-1120. Various models supported 16x64, 12x80, and 24x80 display formats. The UTS 4000 line had a COBOL compiler available that made it possible to do local processing in the <b>cluster</b> <b>controller,</b> and the UTS 60 was also capable of being programmed. This line of terminals roughly paralleled the similar IBM product, the IBM 3270. The UTS-400-TE was specialized terminal that had a powerful text editing program burned into firmware intended at first to allow for the editing of simple copy such as that for a newspaper, and later adapted as a prototype word processor with 8" [...] floppy disks and driving Letter Quality daisy wheel printers.|$|E
5000|$|With {{the advent}} of Advanced Peer-to-Peer Networking (APPN), routing {{functionality}} was {{the responsibility of the}} computer as opposed to the router (as with TCP/IP networks). Each computer maintained a list of Nodes that defined the forwarding mechanisms. A centralized node type known as a Network Node maintained Global tables of all other node types. APPN stopped the need to maintain Advanced Program-to-Program Communication (APPC) routing tables that explicitly defined endpoint to endpoint connectivity. APPN sessions would route to endpoints through other allowed node types until it found the destination. This is similar to the way that routers for the Internet Protocol and the Netware Internetwork Packet Exchange protocol function. (APPN is also sometimes referred to PU2.1 or Physical Unit 2.1. APPC, also sometime referred to LU6.2 or Logical Unit 6.2, was the only protocol defined to APPN networks, but was originally one of many protocols supported by VTAM/NCP, along with LU0, LU1, LU2 (3270 Terminal), and LU3. APPC was primarily used between CICS environments, as well as database services, because it contact protocols for 2-phase commit processing). Physical Units were PU5 (VTAM), PU4 (37xx), PU2 (<b>Cluster</b> <b>Controller).</b> A PU5 was the most capable and considered the primary on all communication. Other PU devices requested a connection from the PU5 and the PU5 could establish the connection or not. The other PU types could only be secondary to the PU5. A PU2.1 added the ability to a PU2.1 to connect to another PU2.1 in a peer-to-peer environment.) ...|$|E
3000|$|... {{dispenses}} {{one message}} to the <b>cluster</b> <b>controller</b> which has least messages at this moment; this operation leads to the change of load distribution of the cluster. Secondly, <b>cluster</b> <b>controller</b> CC [...]...|$|E
5000|$|VMware NSX Controller uses Paxos-based {{algorithm}} within NSX <b>Controller</b> <b>cluster.</b>|$|R
40|$|Software-Defined Networking (SDN) {{potentially}} {{can improve}} the flexibility and management of Wireless Sensor Networks (WSNs). To investigate the impact of SDN on WSN, in this thesis, we consider three Software-Defined Wireless Sensor Network (SD-WSN) frameworks, namely SDN-WISE, SDWN-ONOS, and TinySDN. After comparing these frameworks, the performance of TinySDN is evaluated in three different scenarios: homogeneous, heterogeneous, and dynamic networks. The Collection Tree Protocol (CTP) {{is used for the}} evaluations, and is subsequently compared with Rime data collection protocol in Contiki. Our performance evaluation is based on four metrics: Packet Delivery Ratio (PDR), packet duplication, duty cycle, and delay. Our results show that the PDR for WSN and SD-WSN is relatively similar, that is, around 0. 98 to 1 for homogeneous and heterogeneous networks, whereas in a dynamic network, the PDR of WSN decreases from 0. 98 to 0. 9. Compared to the WSN, the SD-WSN reduces both the average delay of SDN sensor node and the time the SDN sensor node is active to send packets in homogeneous and heterogeneous networks. However, implementing a centralized controller in a dynamic network may cause the SD-WSN performance decrease, which is indicated by the increase ratio of average delay from 2. 03 to 2. 3, whereas the increase ratio of average delay for the WSN is only around 1. 6 to 1. 98. The packet duplication level increases by 33 % in the dynamic network when the number of SDN sensor nodes increases from 10 to 15. The performance of SD-WSN in heterogeneous, homogeneous, and dynamic networks is relatively worse than WSN in terms of packet duplication and Rx duty cycle. SD-WSN, in addition, is not optimally implemented for dynamic conditions as the activity changes from the SDN sensor nodes will significantly affect the performance of SD-WSN. To reduce the load on the centralized <b>controller,</b> the <b>clustering</b> <b>controllers</b> are deployed to distribute the load on multiple controllers. The result shows that packet duplication and average delay in a dynamic network can be reduced by 13 % and 57 %, respectively. <b>Clustering</b> <b>controllers</b> provide more stability in terms of Rx duty cycle compared to before using <b>clustering</b> <b>controllers.</b> Embedded System...|$|R
50|$|RG-62 is a 93 Î© {{coaxial cable}} {{originally}} used in mainframe computer {{networks in the}} 1970s and early 1980s (it was the cable used to connect IBM 3270 terminals to IBM 3274/3174 terminal <b>cluster</b> <b>controllers).</b> Later, some manufacturers of LAN equipment, such as Datapoint for ARCNET, adopted RG-62 as their coaxial cable standard. The cable has the lowest capacitance per unit-length {{when compared to other}} coaxial cables of similar size. Capacitance is the enemy of square-wave data transmission (in particular, it slows down edge transitions), and this is a much more important factor for baseband digital data transmission than power handling or attenuation.|$|R
3000|$|HybridFlow {{consists}} of a super controller and x*y cluster controllers, and we set X={ 1,...,x} which denotes the set of x clusters, Y={ 1,...y} which determines the set of y cluster controllers in one cluster, and <b>cluster</b> <b>controller</b> j in i cluster is denoted as <b>cluster</b> <b>controller</b> [...]...|$|E
3000|$|... {{enable to}} load balance between active <b>cluster</b> <b>controller.</b> The polling least-connection {{algorithm}} is as follows.|$|E
3000|$|... also {{dispenses}} {{one message}} to least message controller. In this way, extra messages from <b>cluster</b> <b>controller</b> CC [...]...|$|E
40|$|Process {{variation}} and signal integrity issues are increasing in the late-silicon era. In particular, they adversely affect {{the timing and}} power characteristics of the global clock signal. In an attempt to eliminate the global clock while preserving the synchronous design style, an automated procedure, de-synchronization, has been proposed [1]. This paper will introduce and investigate two methods of reducing the overhead of the de-synchronization approach. These methods â€“ latch <b>controller</b> <b>clustering</b> and <b>controller</b> simplification through timing analysis â€“ have been successfully applied {{on a number of}} benchmark circuits. 1...|$|R
40|$|Asynchronous and latency-insensitive {{circuits}} offer {{a similar}} form of elasticity that tolerates {{variations in the}} delays or the latencies of the computation and communication resources of a system. This flexibility comes {{at the expense of}} including a control layer that synchronizes the flow of information. This paper proposes a method for reducing the complexity of the control layer by <b>clustering</b> <b>controllers</b> with similar functionality. The approach reduces the control layer and the number of elastic buffers to a significantly smaller elastic skeleton that preserves the performance of the system. The method also takes into account layout information, thus avoiding optimizations that can be physically unfeasible. The experimental results indicate that drastic reductions in the complexity of the control can be obtained. Postprint (author's final draft...|$|R
40|$|Based on {{the design}} of string inverters, a high {{performance}} gird-connected photovoltaic (PV) power plant is presented. The plant consists of grid-connected single phase inverters, intelligent <b>cluster</b> <b>controllers</b> (ICC) and user-friendly monitoring software. The feed-forward control strategy based on grid voltage and load current is proposed to improve inverter's dynamic response. With an optimal maximum power point tracking (MPPT) algorithm, the maximum power output of PV modules is achieved by using forth-back voltage increment perturbation. A Sandia frequency shift based strategy which increases the anti-islanding ability of grid-connected PV power system is also discussed. The proposed system can simultaneously monitor all the inverters on-site of PV modules. The verification has been made on the experimental units. The results show that the plant has excellent capability of high efficiency, intelligent maintenance and integrated management...|$|R
3000|$|... uses polling least-connection {{algorithm}} {{to determine}} the number of load distribution to other cluster controllers in the same cluster which is called Packet_in_oneiÌ‡. Firstly, <b>cluster</b> <b>controller</b> CC [...]...|$|E
40|$|Coauthored by Jean-Marie Choffray and Fabio Lucarelli, GenRX is a {{versatile}} graphic software designed for both simple and complex, {{individual or group}} decision analysis. It makes use of an exclusive interactive voting technology, comprising a set of individual numeric keypads and a <b>cluster</b> <b>controller</b> connected to a computer...|$|E
30|$|AsterixDB {{may also}} be used for {{processing}} of streaming data. One of the main design goals of AsterixDB has been to create a â€˜one size fits a bunchâ€™ architecture, where one technology can be utilized in multiple use cases without gluing multiple technologies together [45]. AsterixDB has a flexible data model and query language (AQL) for describing, querying, and analysing semi-structured data [13]. Algebricks is a data model-agnostic layer in AsterixDB for parallel query processing and optimization [46]. It pushes jobs into AsterixDBâ€™s Hyracks run time as distributed acyclic graphs (DAG). The implementation architecture of AsterixDB consists of a <b>cluster</b> <b>controller,</b> metadata controller, and node controllers [45]. The <b>cluster</b> <b>controller</b> accepts AQL statements pushed from clients (over HTTP), which are distributed as job descriptions to Hyracks data flow engine [47], and node controllers [45].|$|E
30|$|In this paper, we {{have studied}} the method of {{switching}} off <b>cluster</b> <b>controllers</b> based on HybridFlow architecture to improve energy efficiency. First, we use queuing theory to model the operation procedure of controllers, formulate the energy consumption management issue as a 0 - 1 integer linear programming model. Through turning off the redundant controllers when the system is in the scenario of light traffic, the total energy consumption of the whole system can be cut down. Then, we present the processing of energy consumption management algorithm with M-N sleeping policy in details. Simulation results show that proposed algorithm exhibits better energy efficiency but introduces extra time delay. In further study, we need to improve energy efficiency, and reduce the time delay. Meanwhile, {{we need to find}} the relationship between energy efficiency, time delay and threshold M,N.|$|R
30|$|Sleeping {{controllers}} {{detect the}} number of queuing length. When queuing length surpasses threshold N which is the threshold of setting up sleeping controllers, sleeping controllers awaken themselves and continue to deal with messages. Otherwise, <b>cluster</b> <b>controllers</b> are still in sleeping mode. When N becomes larger, the threshold of setting up sleeping controllers increases and sleeping controllers have a longer sleeping time, which reduces energy consumption. But in this case, packet-in messages queuing in the sleeping controllers will wait for a longer time, which gives rise to more time delay. In a word, when N becomes bigger, energy consumption decreases and time delay decreases. On the contrary, energy consumption increases and time delay decreases. We can conclude that there is also fundamental tradeoff between energy consumption and time delay with a different N. The simulation shown in Section 4 also verifies {{the accuracy of the}} above analysis. The algorithm and the actions performed on each of its steps are shown as follows.|$|R
40|$|This paper {{discusses}} the adaptive <b>cluster</b> formation and <b>controller</b> selection from {{a set of}} access nodes, given the RAN characteristics, the resource situation and the per slice QoS requirements, Subsequently, the adaptive placement of Radio Resource Management (RRM) functionalities to the RAN nodes and the interactions among functionalities are determined on per slice basis. By {{taking into account the}} slice requirements, the backhaul/access channel conditions and the traffic load, a central management entity assigns RRM functionalities to the controllers with different levels of centralization {{in order to meet the}} per Slice KPIs (throughput, reliability, latency). The <b>controller,</b> <b>cluster</b> and RRM split configuration problems are formulated and interpreted as three dependent graph-based sub-problems, where low complexity heuristic approaches, requiring low signalling, are proposed in this framework...|$|R
40|$|In {{this paper}} {{we present a}} <b>cluster</b> <b>controller</b> design of a mesh of {{helicopters}} which, produces the trajectory to be tracked in terms of desired acceleration. As an intermediate step for the formation flying of a cluster of helicopters an individual helicopter controller is designed which accepts the trajectory to be tracked in terms of desired accelerations. The design uses {{the fact that the}} linear position dynamics of a helicopter are slow compared to the attitude dynamics. Next we discuss the design of a higher level <b>cluster</b> <b>controller</b> assuming simple double integrator dynamics. The results show that we need leader information to achieve mesh stability in case of point masses. The higher level mesh controller is then used for simulation of a formation flight of a cluster of helicopters. It is observed that the intuition about mesh stability obtained with simple double integrator dynamics is carried over to the case of helicopters. link_to_subscribed_fulltex...|$|E
30|$|The {{methodology}} for {{management of resources}} in Eucalyptus is predominantly reliant upon establishing a control structure between nodes, such that one cluster is managed by one second-tier controller, which is managed by a centralized cloud controller. In the case of Eucalyptus, there are five controller types: cloud controller, <b>cluster</b> <b>controller,</b> block-based storage controller (EBS), bucket-based storage controller (S 3), and node controller. The cloud controller is responsible for managing attributes of the cloud, such as the registration of controllers, access control management, as well as facilitating user interaction through command-line and, in some cases, web-based interfacing. The <b>cluster</b> <b>controller</b> is responsible for managing a cluster of node controllers, which entails transmission of control messages for instantiation of virtual machine images and other necessities required for compute nodes. Block-based storage controllers provide an abstract interface for creation of storage blocks, which are dynamically allocated virtual storage devices that can be utilized as persistent storage. Bucket-based storage controllers are not allocated as block-level devices, but instead are treated as containers by which files, namely virtual machine images, may be stored. Node controllers are responsible for hosting virtual machine instances and for facilitating remote access via RDP [22], SSH [23], VNC [24], and other remote access protocols.|$|E
30|$|The {{controller}} enables {{to communicate}} with other controllers which {{are in the same}} cluster directly, and controllers which belong to different clusters connect with others via super controller. Under such a communicated mechanism, network status is synchronized between all the cluster controllers, such as global view information, topological information, and routing paths. Super controller is a special device which has the following function modules: forwarding, storage, calculation, timing, etc. Its main functions are to collect the load status from cluster controllers, correlatively calculate and forward. In addition, super controller connects the cluster controllers from different clusters respectively and monitors the load status of each <b>cluster</b> <b>controller.</b>|$|E
40|$|Software Defined Networking (SDN) is an {{emerging}} promising paradigm for network management {{because of its}} centralized network intelligence. However, the centralized control architecture of the software-defined networks (SDNs) brings novel challenges of reliability, scalability, fault tolerance and interoperability. In this paper, we proposed a novel <b>clustered</b> distributed <b>controller</b> architecture in the real setting of SDNs. The distributed cluster implementation comprises of multiple popular SDN controllers. The proposed mechanism is evaluated using a real world network topology running {{on top of an}} emulated SDN environment. The result shows that the proposed distributed <b>controller</b> <b>clustering</b> mechanism is able to significantly reduce the average latency from 8. 1 % to 1. 6 %, the packet loss from 5. 22 % to 4. 15 %, compared to distributed <b>controller</b> without <b>clustering</b> running on HP Virtual Application Network (VAN) SDN and Open Network Operating System (ONOS) controllers respectively. Moreover, proposed method also shows reasonable CPU utilization results. Furthermore, the proposed mechanism makes possible to handle unexpected load fluctuations while maintaining a continuous network operation, even when there is a controller failure. The paper is a potential contribution stepping towards addressing the issues of reliability, scalability, fault tolerance, and inter-operability...|$|R
40|$|Unlike pre-programmed controlling, {{real-time}} {{control of}} a 6 -DOF manipulator is a complex task which involves physical interaction with a traditional manual <b>controller</b> <b>clustered</b> with numerous joysticks. This take lot of joystick movements {{in order to bring}} the robotic arm to a position followed by an action with the end-effector. Using the latest technology in human computer interaction, i. e. Leap Motion, we can reduce the complexity of this task to minimum effort by making the robotic arm mimic human hand in real time...|$|R
40|$|In {{distributed}} SDN architectures, {{the network}} is controlled by a <b>cluster</b> of multiple <b>controllers.</b> This distributed approach permits to meet the scalability and reliability requirements of large operational networks. Despite that, a logical centralized view of the network state should be guaranteed, enabling the simple development of network applications. Achieving a consistent network state requires a consensus protocol, which generates control traffic among the controllers whose timely delivery is crucial for network performance. We focus on the state-of-art ONOS controller, designed to scale to large networks, based on a <b>cluster</b> of self-coordinating <b>controllers.</b> In particular, we study the inter-controller control traffic due to the adopted consistency protocols. Based on real traffic measurements and {{the analysis of the}} adopted consistency protocols, we develop some empirical models to quantify the traffic exchanged among the controllers, depending on the considered shared data structures, the current network state (e. g. topology) and the occurring network events (e. g. flow or host addition). Our models provide a formal tool to be integrated into the design and dimension the control network interconnecting the controllers. Our results are of paramount importance for the proper design of large SDN networks, in which the control plane is implemented in-band and cannot exploit dedicated network resources...|$|R
