0|1397|Public
30|$|Masci et al. [80]: A {{convolutional}} {{neural network}}(CNN) {{is a type}} of feed-forward artificial neural network where the individual neurons respond to overlapping regions in the visual field. In <b>CNN,</b> convolutional <b>layer</b> <b>performs</b> a 2 D filtering between input images, x and a bank of filters, w producing another set of images, h. A nonlinear activation function is applied to h just as for standard multilayer networks. Pooling layer reduces the dimensionality of the input by a constant factor and also undertakes feature selection. The input images are tiled in nonoverlapping subregions from which only one output value (max. or avg.) is extracted. Subsequent, fully connected <b>layer</b> <b>performs</b> a linear combination of the input vector with a weight matrix. Max-pooling convolutional neural networks (MPCNNs) perform feature extraction and classification jointly. With 7 % error rate, MPCNN performed much better than SVM for seven defects in cold strips. In CNN, the number of free parameters does not grow proportionally with the input dimensions and therefore performs better in terms of many benchmarks.|$|R
40|$|In this paper, {{we study}} the dynamic range of {{multi-layer}} cellular neural networks (CNN's) by using Lyapunov functions. A theorem {{is presented to}} guarantee the existence of equilibrium point of multi-layer CNN. A theorem on globally stable equilibrium point of multi-layer CNN is given. Multi-layer CNN's are used to model some functions of nitric oxide (NO) in nervous systems. In a 2 -layer CNN model, the first <b>CNN</b> <b>layer</b> implements synaptic events such as image processing tasks. During these synaptic events artificial NO sources are triggered by the outputs of the first <b>CNN</b> <b>layer.</b> In the 2 nd <b>CNN</b> <b>layer,</b> a NO di#usion model is implemented. The output of the 2 nd <b>CNN</b> <b>layer</b> functions as a feedback, which mimics the actions of NO to synaptic events, to the first <b>CNN</b> <b>layer.</b> This kind of feedback from the 2 nd <b>CNN</b> <b>layer</b> introduces "plasticity" into the CNN synaptic law of the first <b>CNN</b> <b>layer.</b> We improve the NO diffusion model in the 2 nd <b>CNN</b> <b>layer</b> by introducing the third <b>CNN</b> <b>layer,</b> which feedbacks {{the output of the}} first <b>CNN</b> <b>layer</b> to the NO diffusion model in the second <b>CNN</b> <b>layer.</b> As an application of CNN NO model, we use it to improve the performance of edge detection CNN...|$|R
30|$|Firstly, {{we produce}} n feature vectors {{generated}} by <b>CNNs</b> <b>layer</b> of pool 5 (for City Centre and New College dataset, n {{is equal to}} 2474 and 2146, respectively) using Caffe [25].|$|R
40|$|The {{objective}} {{of this paper is}} the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different <b>CNN</b> <b>layers</b> improved by average/max pooling on the feature maps. Our observation suggests that the Conv 5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various <b>CNN</b> <b>layers</b> is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving {{the state of the art}} on a number of benchmarks to a large margin. Comment: 9 pages. It will be submitted to an appropriate journa...|$|R
30|$|Fischer et al. [48] {{reported}} that, given feature positions, descriptors {{extracted from}} <b>CNN</b> <b>layer</b> have better matchability compared to SIFT [11]. More recently, Schonberger et al. [49] {{also showed that}} CNN-based learned local features such as LIFT [17], Deep-Desc [50], and ConvOpt [51] have higher recall compared to SIFT [11] but still cannot outperform its variants, e.g., DSP-SIFT [16] and SIFT-PCA [52].|$|R
40|$|We {{present a}} simple yet {{effective}} neural network architecture for image recognition. Unlike the previous state-of-the-art neural networks which usually have very deep architectures, we build networks that are shallower but can achieve better performances on three competitive benchmark datasets, i. e., CIFAR- 10 / 100 and ImageNet. Our architectures are built using Gradually Updated Neural Network (GUNN) layers, which {{differ from the}} standard Convolutional Neural Network (<b>CNN)</b> <b>layers</b> in the way their output channels are computed: the <b>CNN</b> <b>layers</b> compute the output channels simultaneously while GUNN layers compute the channels gradually. By adding the computation ordering to the channels of CNNs, our networks are able to achieve better accuracies while using fewer layers and less memory. The architecture design of GUNN is guided by theoretical results and verified by empirical experiments. We set new records on the CIFAR- 10 and CIFAR- 100 datasets and achieve better accuracy on ImageNet under similar complexity with the previous state-of-the-art methods...|$|R
40|$|Convolutional neural {{networks}} (CNNs) are revolutionizing {{a variety of}} machine learning tasks, but they present significant computational challenges. Recently, FPGA-based accelerators have been proposed to improve the speed and efficiency of CNNs. Current approaches construct a single processor that computes the <b>CNN</b> <b>layers</b> one at a time; this single processor is optimized to maximize the overall throughput at which the collection of layers are computed. However, this approach leads to inefficient designs because the same processor structure is used to compute <b>CNN</b> <b>layers</b> of radically varying dimensions. We present a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, {{each of which is}} tailored for a different subset of the <b>CNN</b> convolutional <b>layers.</b> Using the same FPGA resources as a single large processor, multiple smaller specialized processors result in increased computational efficiency and lead to a higher overall throughput. Our design methodology achieves 1. 51 x higher throughput than {{the state of the art}} approach on evaluating the popular AlexNet CNN on a Xilinx Virtex- 7 FPGA. Our projections indicate that the benefit of our approach increases with the amount of available FPGA resources, already growing to over 3 x over the state of the art within the next generation of FPGAs...|$|R
30|$|Hou et al. [7] {{were the}} pioneers to {{consider}} using features generated by <b>CNNs</b> <b>layers</b> for visual LCD. They use a public pre-trained CNNs model, Places CNNs, {{trained on the}} scene-centric dataset Places [26] with over 2.5 million images of 205 scene categories, as an efficient whole-image descriptor generator for LCD. They comprehensively compared the performance of Places <b>CNNs</b> model’s all <b>layers</b> by using the euclidean distance as the similarity measurement. Their work demonstrated that the pool 5 layer provides the best image descriptors {{in terms of both}} detection accuracy and dimension of feature among all Places CNNs descriptors.|$|R
30|$|Against the background, in {{this paper}} we provide two {{solutions}} to address the above two challenges. Firstly, we explicitly provide matching range of candidate images to prevent images matching with their adjacent images. Meanwhile, we get better performance than state-of-the-art algorithms by adapting the matching range. Secondly, we provide a efficient feature compression method to reduce the dimension of feature generated by <b>CNNs</b> <b>layers,</b> which boosts real-time performance with marginal performance loss.|$|R
40|$|We {{address the}} problem of contour {{detection}} via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different <b>CNN</b> <b>layers</b> and verify their performance on BSDS 500. Comment: 2 pages. arXiv admin note: substantial text overlap with arXiv: 1412. 685...|$|R
30|$|Second layer: Neurons in {{the second}} <b>layer</b> <b>perform</b> fuzzification.|$|R
5000|$|The {{data link}} <b>layer</b> <b>performs</b> three vital {{services}} for the PCIe express link: ...|$|R
40|$|This paper {{studies the}} emotion {{recognition}} from musical {{tracks in the}} 2 -dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one <b>CNN</b> <b>layer</b> followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval 2015 emotion in music' dataset. We achieved an RMSE of 0. 202 for arousal and 0. 268 for valence, {{which is the best}} result reported on this dataset. Comment: Accepted for Sound and Music Computing (SMC 2017...|$|R
40|$|Convolutional Neural Networks (CNN) are {{the most}} popular of deep network models due to their {{applicability}} and success in image processing. Although plenty of effort has been made in designing and training better discriminative CNNs, little is yet known about the internal features these models learn. Questions like, what specific knowledge is coded within <b>CNN</b> <b>layers,</b> and how can it be used for other purposes besides discrimination, remain to be answered. To advance in the resolution of these questions, in this work we extract features from <b>CNN</b> <b>layers,</b> building vector representations from CNN activations. The resultant vector embedding is used to represent first images and then known image classes. On those representations we perform an unsupervised clustering process, with the goal of studying the hidden semantics captured in the embedding space. Several abstract entities untaught to the network emerge in this process, effectively defining a taxonomy of knowledge as perceived by the CNN. We evaluate and interpret these sets using WordNet, while studying the different behaviours exhibited by the <b>layers</b> of a <b>CNN</b> model according to their depth. Our results indicate that, while top (i. e., deeper) layers provide the most representative space, low layers also define descriptive dimensions. This work was partially supported by the IBM/BSC Technology Center for Supercomputing (Joint Study Agreement, No. W 156463), by the Spanish Government through Programa Severo Ochoa (SEV- 2015 - 0493), by the Spanish Ministry of Science and Technology through TIN 2015 - 65316 -P project and by the Generalitat de Catalunya (contracts 2014 -SGR- 1051). Peer ReviewedPostprint (author's final draft...|$|R
30|$|However, when we {{actually}} use these features generated by <b>CNNs</b> <b>layers</b> {{in a practical}} environment, two challenges appear. Firstly, the adjacent images in the dataset of LCD might have more resemblance than the images that really form the loop closure, so the algorithm tends to identify the adjacent images as loop closure, which is certainly not preferred. Secondly, the feature matching is computationally intensive because the dimension of features generated by CNNs may be very large, and LCD may have to compare the current image to {{a large amount of}} pre-captured images in order to decide whether the robot returns to previously visited positions. This can not satisfy strong request for real-time performance in robotic applications.|$|R
40|$|Recently, deep {{learning}} {{approach has been}} used widely {{in order to enhance}} the recognition accuracy with different application areas. In this paper, both of deep convolutional neural networks (CNN) and support vector machines approach were employed in human action recognition task. Firstly, 3 D CNN approach was used to extract spatial and temporal features from adjacent video frames. Then, support vector machines approach was used in order to classify each instance based on previously extracted features. Both of the number of <b>CNN</b> <b>layers</b> and the resolution of the input frames were reduced to meet the limited memory constraints. The proposed architecture was trained and evaluated on KTH action recognition dataset and achieved a good performance...|$|R
30|$|We aim at {{providing}} the deep neural architecture {{with the ability}} to adapt to new subject cases, assisting doctors with efficient patient-specific analysis and treatment selection, without forgetting its former knowledge. Our methodology is based on a new network retraining approach which extends the work in [5, 19]. This approach uses clustering [26] of trained system internal representations, in particular, of the neurons’ outputs at the last fully connected <b>CNN</b> <b>layer</b> (denoted, in vector form, as F in Fig.  5), or at the last hidden RNN layer (let us denote them, in vector form, as u, and consider them feeding the output units o). We use the centres of these clusters as knowledge extracted from the data-driven supervised training of the DNN architecture.|$|R
40|$|We {{present a}} method for skin lesion {{segmentation}} for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach {{is based on a}} Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network 1 × 1 convolution layers to add capacity to the network and (iii) state-of-art super-resolution upsampling of predictions using subpixel <b>CNN</b> <b>layers.</b> We reported a mean IOU score of 0. 642 on the validation set provided by the organisers...|$|R
40|$|We {{introduce}} our {{method and}} system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several pose-specific deep {{convolutional neural network}} (CNN) models to generate multiple pose-specific features. 3 D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, <b>CNN</b> <b>layer</b> selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPA's CS 2 and NIST's IJB-A in both verification and identification (i. e. search) tasks. Comment: WACV 201...|$|R
2500|$|Typically, neurons are {{organized}} in layers. Different <b>layers</b> may <b>perform</b> {{different kinds of}} transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. [...] In artificial networks with multiple hidden layers, the initial layers might detect primitives (e.g. the pupil in an eye, the iris, eyelashes, etc..) and their output is fed forward to deeper <b>layers</b> who <b>perform</b> more abstract generalizations (e.g. [...] eye, mouth).... and so on until the final <b>layers</b> <b>perform</b> the complex object recognition (e.g. face).|$|R
3000|$|Phase I of the {{algorithm}} is {{the training of}} CNN C and S Layers. Supervised learning is used to train C and S layers. A gradient descend method is used to update the weights in all the layers. Phase II of {{the algorithm}} is only used when new training samples are available. The issue is to incorporate the information available from the new samples into the trained network. This issue is solved by the Phase II step of the algorithm. In this phase output, O^z of the last <b>CNN</b> <b>layer</b> is tapped and reweighted or updated using Eq. (3) to get new vector O^ztk for each training sample. In Phase III step of the algorithm, layer F_ 6 [...] is trained with O^ztk as training vectors for the classification task.|$|R
40|$|We {{address the}} problem of contour {{detection}} via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different <b>CNN</b> <b>layers</b> and obtain comparable performances to the state-of-the-art on BSDS 500. Comment: 9 pages, 3 figure...|$|R
30|$|To {{overcome}} this problem, pre-trained CNN feature extraction method is proposed in recent years. CNN features, extracted from different <b>CNN</b> <b>layers,</b> have different characteristics {{in describing the}} object [24]. The CNN features from deeper layer contains more high-level semantic information, which {{can be seen as}} structural information, have more distinguishing capabilities and thus is effective facing the situation when intra-class appearance variation occurs. However, the features from deep layer have very low spatial resolution so that it cannot fit the task in generic visual tracking, which aim to indicate the location of target. On the other hand, CNN features from earlier layer contain more fine-grained information, which means the more the discriminative capabilities, the more effective in locating the target. But with the less semantic information, features from earlier layer are more sensitive to intra-class appearance variations.|$|R
40|$|At present, {{designing}} convolutional {{neural network}} (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified {{from a handful of}} existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose <b>CNN</b> <b>layers</b> using Q-learning with an ϵ-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks...|$|R
40|$|Abstract — This paper {{describes}} {{the design and}} the implementation of an embedded system based on multiple FPGAs {{that can be used}} to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more <b>CNN</b> <b>layers.</b> The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates. Keywords-component; Multi-FPGA; Embedded Syseim...|$|R
40|$|International audienceConvolutional Neural Network (CNN) {{techniques}} improved {{accuracy and}} robustness of machine vision systems {{at the price}} of a very high computational cost. This motivated multiple research efforts to investigate the applicability of approximate computing and more particularly, fixed point-arithmetic for CNNs. In all this approaches, a recurrent problem is that the learned parameters in deep <b>CNN</b> <b>layers</b> have a significantly lower numerical dynamic range when compared to the feature maps. This problem prevents from using of a low bit-width representation in deep layers. In this paper, we demonstrate that using the TanH activation function is way to prevent this issue. To support this demonstration, three benchmark CNN models are trained with the TanH function. These models are then quantized using the same bit-width across all the layers. Efficiency of this method is demonstrated on an FPGA based accelerator, by inferring CNNs with the minimal amount of logic elements...|$|R
40|$|Previous {{models for}} video {{captioning}} often use the output from a specific layer of a Convolutional Neural Network (CNN) as video features. However, the variable context-dependent semantics {{in the video}} may make it more appropriate to adaptively select features from the multiple <b>CNN</b> <b>layers.</b> We propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task. A novel attention mechanism is developed, that adaptively and sequentially focuses on different <b>layers</b> of <b>CNN</b> features (levels of feature "abstraction"), {{as well as local}} spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube 2 Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics. Comment: Accepted to AAAI 201...|$|R
40|$|Recent {{advances}} in 3 D sensing technologies {{make it possible}} to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3 D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The <b>CNN</b> <b>layer</b> learns low-level translationally invariant features which are then given as inputs to multiple, fixed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. ...|$|R
40|$|Deformable part models (DPMs) and {{convolutional}} {{neural networks}} (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper, {{we show that}} a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) <b>CNN</b> <b>layer.</b> From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running {{an order of magnitude}} faster...|$|R
40|$|We {{conduct an}} {{in-depth}} exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study {{different ways of}} performing spatial and temporal pooling, feature normalization, choice of <b>CNN</b> <b>layers</b> as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches {{that have been used}} till now. We evaluate our approach on the challenging TRECVID MED' 14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED' 14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED' 14 from 34. 95 % to 38. 74 %. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF- 101 dataset...|$|R
30|$|For each NN layer or {{mechanism}} (e.g., word embedding, dropout, RNN/LSTM/BLSTM, and CNN; see Sect.  4.1), we optimized its different parameters. In {{the word}} embedding layer, the parameter was the hidden node {{number in the}} hidden layer, which was checked between 500 and 1300 with a 100 hidden-node jump. In the <b>CNN</b> <b>layer,</b> the parameters were the filter size, which was evaluated between two and seven with a one-step jump, {{and the number of}} filters, which was checked between 500 and 1300 with a 100 hidden-node jump. The max pooling layer was set to be global. Last, in the RNN/LSTM layer (used by all LSTM variants; Sect.  4.1), the memory unit number was tested between 500 and 1300 with a 100 hidden-node jump. For all the models, we used 35 epochs and 64 trajectories as the batch size. For each of the evaluated parameters, its best value was selected using the validation set, as described in Sect.  5.|$|R
40|$|We {{introduce}} {{the concept of}} dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image {{is based on the}} rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling <b>CNN</b> <b>layer</b> allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance...|$|R
40|$|We {{describe}} a joint model for intent detection and slot filling based on convolutional neural networks (CNN). The proposed architecture can {{be perceived as}} a neural network (NN) ver-sion of the triangular CRF model (TriCRF), in which the in-tent label and the slot sequence are modeled jointly and their dependencies are exploited. Our slot filling component is a globally normalized CRF style model, as opposed to left-to-right models in recent NN based slot taggers. Its features are automatically extracted through <b>CNN</b> <b>layers</b> and shared by the intent model. We show that our slot model component generates state-of-the-art results, outperforming CRF signifi-cantly. Our joint model outperforms the standard TriCRF by 1 % absolute for both intent and slot. On a number of other domains, our joint model achieves 0. 7 - 1 %, and 0. 9 - 2. 1 % absolute gains over the independent modeling approach for intent and slot respectively. Index Terms — Joint modeling, slot filling, convolutional neural network, triangular CR...|$|R
40|$|Neural {{networks}} have {{shown to be}} a practical way of building a very complex mapping between a pre-specified input space and output space. For example, a convolutional neural network (CNN) mapping an image into one of a thousand object labels is approaching human performance in this particular task. However the mapping (neural network) does not automatically lend itself to other forms of queries, for example, to detect/reconstruct object instances, to enforce top-down signal on ambiguous inputs, or to recover object instances from occlusion. One way to address these queries is a backward pass through the network that fuses top-down and bottom-up information. In this paper, we show a way of building such a backward pass by defining a generative model of the neural network's activations. Approximate inference of the model would naturally {{take the form of a}} backward pass through the <b>CNN</b> <b>layers,</b> and it addresses the aforementioned queries in a unified framework...|$|R
40|$|Convolutional neural {{networks}} (CNN) are extensions to deep {{neural networks}} (DNN) {{which are used}} as alternate acoustic models with state-of-the-art performances for speech recognition. In this paper, CNNs are used as acoustic models for speech activity detection (SAD) on data collected over noisy radio communication channels. When these SAD models are tested on audio recorded from radio channels not seen during training, there is severe performance degradation. We attribute this degradation to mismatches between the two dimensional filters learnt in the initial <b>CNN</b> <b>layers</b> and the novel channel data. Using {{a small amount of}} supervised data from the novel channels, the filters can be adapted to provide significant improvements in SAD performance. In mismatched acoustic conditions, the adapted models provide significant improvements (about 10 - 25 %) relative to conventional DNN-based SAD systems. These results illustrate that CNNs have a considerable advantage in fast adaptation for acoustic modeling in these settings. Index Terms — Convolutional neural networks, Speech activity detection, Neural network adaptatio...|$|R
40|$|This {{paper is}} focused on {{studying}} the view-manifold structure in the feature spaces implied by the different layers of Convolutional Neural Networks (CNN). There are several questions that this paper aims to answer: Does the learned CNN representation achieve viewpoint invariance? How does it achieve viewpoint invariance? Is it achieved by collapsing the view manifolds, or separating them while preserving them? At which layer is view invariance achieved? How can {{the structure of the}} view manifold at each layer of a deep convolutional neural network be quantified experimentally? How does fine-tuning of a pre-trained CNN on a multi-view dataset affect the representation at each layer of the network? In order to answer these questions we propose a methodology to quantify the deformation and degeneracy of view manifolds in <b>CNN</b> <b>layers.</b> We apply this methodology and report interesting results in this paper that answer the aforementioned questions. Comment: This paper accepted in ICLR 2016 main conferenc...|$|R
