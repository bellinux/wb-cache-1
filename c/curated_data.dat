197|294|Public
25|$|<b>Curated</b> <b>data</b> are {{available}} at FlyBase.|$|E
25|$|Traditionally, {{the basic}} level of {{annotation}} is using BLAST for finding similarities, and then annotating genomes based on homologues. More recently, additional information {{is added to}} the annotation platform. The additional information allows manual annotators to deconvolute discrepancies between genes that are given the same annotation. Some databases use genome context information, similarity scores, experimental data, and integrations of other resources to provide genome annotations through their Subsystems approach. Other databases (e.g. Ensembl) rely on both <b>curated</b> <b>data</b> sources as well as a range of software tools in their automated genome annotation pipeline. Structural annotation consists of the identification of genomic elements, primarily ORFs and their localisation, or gene structure. Functional annotation consists of attaching biological information to genomic elements.|$|E
50|$|<b>Curated</b> <b>data</b> are {{available}} at FlyBase.|$|E
5000|$|To <b>curate</b> <b>data</b> in the literature, {{the authors}} chose {{to focus on}} the {{analytical}} methods, experimental results, resources, and nomenclature. If there was insufficient data in the papers, users will see [...] "Not specified" [...] in these sections.|$|R
50|$|Additional {{monitoring}} companies use crawlers and spidering {{technology to}} find keyword reference. (See also: Semantic analysis, Natural language processing.) Basic implementation involves <b>curating</b> <b>data</b> from social media {{on a large}} scale and analyzing the results to make sense out of it.|$|R
50|$|COSMIC is {{an online}} {{database}} of somatically acquired mutations found in human cancer. Somatic mutations {{are those that}} occur in non-germline cells that are not inherited by children. COSMIC, an acronym of Catalogue Of Somatic Mutations In Cancer, <b>curates</b> <b>data</b> from papers in the scientific literature and large scale experimental screens from the Cancer Genome Project at the Sanger Institute. The database is freely available without restriction via its website.|$|R
5000|$|... #Caption: The {{process of}} {{database}} compilation and curationThe <b>curated</b> <b>data</b> may comprise a process from practical experience and literature review to web {{publication of the}} database ...|$|E
5000|$|<b>Curated</b> <b>data</b> for MobiDB is {{obtained}} from DisProt database giving information and disorder annotation manually extracted from literature. In order to complement disorder annotation, MobiDB features additional annotations from external sources: ...|$|E
50|$|The <b>curated</b> <b>data</b> {{store is}} {{available}} for use by others as git repository. All software produced by the project is available under open source licenses; (see opentreeoflife.github.io for links to the code, data, and documentation).|$|E
50|$|The {{following}} {{organisms are}} currently supported within the BioGRID, {{and each has}} <b>curated</b> interaction <b>data</b> available {{according to the latest}} statistics.|$|R
40|$|<b>Curating</b> Research <b>Data,</b> Volume Two: A Handbook of Current Practice guides you {{across the}} data {{lifecycle}} through the practical strategies and techniques for <b>curating</b> research <b>data</b> {{in a digital}} repository setting. The data curation steps for receiving, appraising, selecting, ingesting, transforming, describing, contextualizing, disseminating, and preserving digital research data are each explored, and then supplemented with detailed case studies written by more than forty international practitioners from national, disciplinary, and institutional data repositories. The steps in this volume detail the sequential actions that you might take to <b>curate</b> a <b>data</b> set from receiving the data (Step 1) to eventual reuse (Step 8). Data curators, archivists, research data management specialists, subject librarians, institutional repository managers, and digital library staff will benefit from these current and practical approaches to data curation...|$|R
50|$|QTLs: RGD's staff {{manually}} <b>curates</b> <b>data</b> for rat {{and human}} QTLs {{from the literature}} where such publications exist or from records directly submitted by researchers. Mouse QTL records, including Mammalian Phenotype (MP) ontology assignments, are imported directly from MGI. For rat and human QTLs, curation includes assigning MP and disease ontology annotations. QTL positions are automatically assigned based on the genomic positions of peak and/or flanking markers or single nucleeotide polymorphisms (SNPs). QTL records link to information about related strains, candidate genes, associated markers and related QTLs.|$|R
50|$|The NCI-Nature <b>curated</b> <b>data</b> is {{gathered}} from published research literature and reviewed by expert scientists before publication. Evidence codes {{are assigned to}} each molecular interaction, which allows users to evaluate {{the reliability of the}} interactions or to search for interactions identified by particular experimental techniques.|$|E
50|$|Wolfram Mathematica {{includes}} {{collections of}} <b>curated</b> <b>data</b> provided {{for use in}} computations. Mathematica is also integrated with Wolfram Alpha, an online service which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra).|$|E
50|$|The modern {{versions}} of these libraries {{as used in}} most software are presented as multidimensional distributions of probability or frequency, where the peaks correspond to the dihedral-angle conformations considered as individual rotamers in the lists. Some versions are based on very carefully <b>curated</b> <b>data</b> and are used primarily for structure validation, while others emphasize relative frequencies in much larger data sets and are the form used primarily for structure prediction, such as the Dunbrack rotamer libraries.|$|E
40|$|Abstract. Annotating {{datasets}} with metadata is {{an important}} part of organizing and <b>curating</b> <b>data.</b> However, it is a time consuming process and often not done in a rigorous fashion. In this paper, we propose a new approach to annotating datasets through the use of reconstructed provenance. A detailed survey of the related work in this area is given. Additionally, we provide an overview of our approach for both reconstructing provenance and using that provenance to automatically annotate datasets with metadata. This approach leverages existing work in AI planning and change detection algorithms...|$|R
40|$|The Arabidopsis Information Resource (TAIR) is {{a web-based}} {{community}} database {{for the model}} plant Arabidopsis thaliana. It provides an integrated view of genes, sequences, proteins, germplasms, clones, metabolic pathways, gene expression, ecotypes, polymorphisms, publications, maps and community information. TAIR is developed and maintained by collaboration between software developers and biologists. Biologists provide specification and use cases for the system, acquire, analyse and <b>curate</b> <b>data,</b> interact with users and test the software. Software developers design, implement and test the database and software. In this review, we briefly describe how TAIR was built and is being maintained...|$|R
50|$|Some {{social media}} {{monitoring}} and analytics companies use calls to data providers each time an end-user develops a query. Others archive and index social media posts to provide end users with on-demand access to historical data and enable methodologies and technologies leveraging network and relational data. Additional monitoring companies use crawlers and spidering technology to find keyword references, known as semantic analysis or natural language processing. Basic implementation involves <b>curating</b> <b>data</b> from social media {{on a large}} scale and analyzing the results to make sense out of it.|$|R
5000|$|Users submit queries and {{computation}} requests via a text field. Wolfram Alpha then computes {{answers and}} relevant visualizations from a knowledge base of curated, structured data {{that come from}} other sites and books. The site [...] "uses a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review." [...] The <b>curated</b> <b>data</b> makes Alpha different from semantic search engines, which index {{a large number of}} answers and then try to match the question to one.|$|E
50|$|HighChem is a {{scientific}} software {{company based in}} Bratislava, Slovakia, which focuses on developing mass spectrometry software and cloud solutions for data processing and the identification of compounds. In addition to software and cloud solutions, HighChem builds <b>curated</b> <b>data</b> collections of experimental mass spectra, fragmentation patterns, quantum chemically calculated fragments and organism specific biochemical data. Its products are used by over 2,000 customers in pharmaceutical, biomarker, forensic, environmental protection, food control and related applications. HighChem is affiliated with Thermo Fisher Scientific.|$|E
50|$|The term <b>curated</b> <b>data</b> {{refers to}} information, that may {{comprise}} {{the most sophisticated}} computational formats for structured data, scientific updates, and curated knowledge, that has been composed and prepared under the regulation {{of one or more}} experts considered to be qualified to engage in such an activity The implication is that the resulting database is of high quality. The contrast is with data which may have been gathered through some automated process or using particularly low or inexpert unsupported data quality and possibly untrustworthy. Some of the most common examples include: CTD and UNIPROT.|$|E
5000|$|... 2010 - The ViS Research Institute was founded. Initial work {{included}} developing advanced {{tools and}} procedures for harvesting and <b>curating</b> structured <b>data</b> about ongoing clinical research and center capabilities.|$|R
30|$|The {{limitations}} of this exploration offer several opportunities for further research in acquiring and <b>curating</b> <b>data</b> for analysis, identifying and using network metrics for innovation relationships, and utilizing research results and visualizations to accomplish change. Relationships among individuals {{are one of the}} key elements - but not the sole factor that defines community, and our analysis identifies metropolitan areas as communities in which relational capital may be an indicator for network orchestration. We recognize that observable relational capital of the business thread in the Triple Helix may include both direct and indirect relationships and that it is one of several mutually interdependent factors.|$|R
40|$|More {{information}} about the Data Curation Network {{can be found on}} our website: [URL] report contains the results of an event designed to identify researcher priorities, needs in <b>curating</b> <b>data</b> and the activities they take in support of curation. The event was held at the University of Michigan Library on November 18, 2016 and was attended by 18 researchers and data managers from across campus. This event was conducted {{as a part of the}} Data Curation Network project which includes The University of Minnesota (lead), the University of Michigan, the University of Illinois, Cornell University, Penn State University, Washington University in St. Louis...|$|R
50|$|The Comparative Toxicogenomics Database, {{helps to}} {{understand}} {{about the effects}} of environmental compounds on human health by integrating data from curated scientific literature to describe biochemical interactions with genes and proteins, and links between diseases and chemicals, and diseases and genes or proteins. CTD contains <b>curated</b> <b>data</b> defining cross-species chemical-gene/protein interactions and chemical- and gene-disease associations to illuminate molecular mechanisms underlying variable susceptibility and environmentally influenced diseases. These data deliver insights into complex chemical-gene and protein interaction networks. One of the main sources in this Database is curated information from OMIM.|$|E
50|$|The simpliest way {{to perform}} gene {{annotation}} relies on homology based search tools, like BLAST, {{to search for}} homologous genes in specific databases, the resulting information is then used to annotate genes and genomes. However, nowadays more and more additional information {{is added to the}} annotation platform. The additional information allows manual annotators to deconvolute discrepancies between genes that are given the same annotation. Some databases use genome context information, similarity scores, experimental data, and integrations of other resources to provide genome annotations through their Subsystems approach. Other databases (e.g. Ensembl) rely on both <b>curated</b> <b>data</b> sources as well as a range of different software tools in their automated genome annotation pipeline.|$|E
50|$|Traditionally, {{the basic}} level of {{annotation}} is using BLAST for finding similarities, and then annotating genomes based on homologues. More recently, additional information {{is added to}} the annotation platform. The additional information allows manual annotators to deconvolute discrepancies between genes that are given the same annotation. Some databases use genome context information, similarity scores, experimental data, and integrations of other resources to provide genome annotations through their Subsystems approach. Other databases (e.g. Ensembl) rely on both <b>curated</b> <b>data</b> sources as well as a range of software tools in their automated genome annotation pipeline. Structural annotation consists of the identification of genomic elements, primarily ORFs and their localisation, or gene structure. Functional annotation consists of attaching biological information to genomic elements.|$|E
40|$|The {{situation}} of university libraries {{in front of}} the current problematic {{situation of}} radical changes in the library profession and of intense general economic crisis is described. Internet, digitalization, mobile web, social networks and other technologies require librarians to diversify in many ways, {{and at the same time}} these technologies disconnect them from the main flow of information, leaving their classical role as intermediaries. Their survival should be based on cooperation with other departments and institutions, and assuming new tasks such as advising researchers, <b>curating</b> <b>data,</b> and promoting open data, open access and repositories, and more...|$|R
40|$|Some {{content from}} the {{presentation}} {{was presented with}} George Alter at the 8 th International Digital Curation Conference (January 2013). This presentation, given at SciDataCon 2016 in Denver, Colorado on September 12, 2016, discusses how the Inter-university Consortium for Political and Social Research (ICPSR), a social and behavioral science data archive, shares sensitive <b>data,</b> by: 1) <b>curating</b> <b>data</b> to limit disclosure, 2) using secure technologies to reduce risk, and 3) training and informing users through data use agreements. The presentation also discusses future opportunities and challenges with respect to sensitive and disclosive data, including privacy budgets, transactional data, geospatial data, and researcher credentialing...|$|R
5000|$|In space science, RAL builds {{components}} for, {{and tests}} satellites, {{as well as}} receiving, analysing and <b>curating</b> the <b>data</b> collected by those spacecraft. Satellite missions in which RAL has a significant role include: ...|$|R
50|$|The Biological General Repository for Interaction Datasets (BioGRID) is a curated {{biological}} {{database of}} protein-protein interactions, genetic interactions, chemical interactions, and post-translational modifications created in 2003 (originally {{referred to as}} simply the General Repository for Interaction Datasets (GRID) by Mike Tyers, Bobby-Joe Breitkreutz, and Chris Stark at the Lunenfeld-Tanenbaum Research Institute at Mount Sinai Hospital. It strives to provide a comprehensive curated resource for all major model organism species while attempting to remove redundancy to create a single mapping of data. Users of The BioGRID can search for their protein or publication of interest and retrieve annotation, as well as <b>curated</b> <b>data</b> as reported, by the primary literature and compiled by in house large-scale curation efforts. The BioGRID is hosted in Toronto, Ontario, Canada and Dallas, Texas, United States and is partnered with the Saccharomyces Genome Database. The BioGRID is funded by the BBSRC, NIH, and CIHR. BioGRID {{is a member of}} the International Molecular Exchange Consortium (IMEx).|$|E
50|$|The BioGRID was {{originally}} published and released as simply the General Repository for Interaction Datasets but was later renamed to the BioGRID {{in order to}} more concisely describe the project, and help distinguish it from several GRID Computing projects with a similar name. Originally separated into organism specific databases, the newest version now provides a unified front end allowing for searches across several organisms simultaneously. The BioGRID was developed initially as a project at the Lunenfeld-Tanenbaum Research Institute at Mount Sinai Hospital but has since expanded to include teams at the Institut de Recherche en Immunologie et en Cancérologie at the Université de Montréal, the Lewis-Sigler Institute for Integrative Genomics at Princeton University, and the Wellcome Trust Center for Cell Biology at the University of Edinburgh. The BioGRID's original focus was on curation of binary protein-protein and genetic interactions, but has expanded over several updates to incorporate curated post-translational modification data, chemical interaction data, and complex multi-gene/protein interactions. Moreover, on a monthly basis, the BioGRID continues to expand <b>curated</b> <b>data</b> and also develop and release new tools, data from comprehensive targeted curation projects, and perform targeted scientific analysis.|$|E
5000|$|High-throughput {{analyses}} of ESTs often encounter similar data management challenges. A first challenge is that tissue provenance of EST libraries {{is described in}} plain English in dbEST. This {{makes it difficult to}} write programs that can unambiguously determine that two EST libraries were sequenced from the same tissue. Similarly, disease conditions for the tissue are not annotated in a computationally friendly manner. For instance, cancer origin of a library is often mixed with the tissue name (e.g., the tissue name [...] "glioblastoma" [...] indicates that the EST library was sequenced from brain tissue and the disease condition is cancer). With the notable exception of cancer, the disease condition is often not recorded in dbEST entries. The TissueInfo project was started in 2000 to help with these challenges. The project provides <b>curated</b> <b>data</b> (updated daily) to disambiguate tissue origin and disease state (cancer/non cancer), offers a tissue ontology that links tissues and organs by [...] "is part of" [...] relationships (i.e., formalizes knowledge that hypothalamus is part of brain, and that brain is part of the central nervous system) and distributes open-source software for linking transcript annotations from sequenced genomes to tissue expression profiles calculated with data in dbEST.|$|E
5000|$|Comparative_Toxicogenomics_Database (CTD) is {{a public}} website and {{research}} tool that <b>curates</b> scientific <b>data</b> describing relationships between chemicals/drugs, genes/proteins, diseases, taxa, phenotypes, GO annotations, pathways, and interaction modules; CTD illuminates how environmental chemicals affect human health.|$|R
40|$|A {{major goal}} of cancer genome {{sequencing}} {{is to identify}} mutations or other somatic alterations that can be targeted by selective and specific drugs. dGene is an annotation tool designed to rapidly identify genes belonging to one of ten druggable classes that are frequently targeted in cancer drug development. These classes were comprehensively populated by combining and manually <b>curating</b> <b>data</b> from multiple specialized and general databases. dGene was used by The Cancer Genome Atlas squamous cell lung cancer project, and here we further demonstrate its utility using recently released breast cancer genome sequencing data. dGene {{is designed to be}} usable by any cancer researcher without the need for suppor...|$|R
50|$|The company {{launched}} Wolfram Alpha, {{an answer}} engine on 16 May 2009. It brings {{a new approach}} to knowledge generation and acquisition that involves large amounts of <b>curated</b> computable <b>data</b> in addition to semantic indexing of text.|$|R
