81|86|Public
2500|$|The {{problem of}} global {{serializability}} {{has been a}} quite intensively researched subject in the late 1980s and early 1990s. Commitment ordering (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of strong strict two phase locking (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the Two-phase commit protocol (2PC) [...] ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic global deadlock resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in orders of magnitude increases in both centralized databases' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between <b>centralized</b> <b>computing</b> and distributed one over fast, low-latency local networks (e.g., Infiniband). These, together with progress in database vendors' distributed solutions (primarily the popular SS2PL with 2PC based, a de facto standard that allows interoperability among different vendors' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), workflow management systems, and database replication technology, in most cases have provided satisfactory and sometimes better information technology solutions without multi database atomic distributed transactions over databases with different concurrency control (bypassing the problem above). As a result, {{the sense of urgency}} that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control [...] types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also Distributed serializability in Serializability). With the proliferation of the Internet, Cloud computing, Grid computing, small, portable, powerful computing devices (e.g., smartphones), and sophisticated systems management the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).|$|E
5000|$|<b>Centralized</b> <b>computing</b> facilities: PC-LAN and {{an array}} of Sun Workstations.|$|E
5000|$|... #Subtitle level 2: Distinction between {{diskless}} {{nodes and}} <b>centralized</b> <b>computing</b> ...|$|E
50|$|OBServer: HTML5 AJAX web {{application}} for uploading content, creating smart playlists, managing users, assigning users to timeslots and for scheduling music. Decentralized file storage with <b>centralized</b> cloud <b>computing</b> management.|$|R
40|$|Previous {{studies have}} {{established}} {{the impact of}} information technology on interdepartmental cooperation. However, the move from distributed <b>computing</b> to <b>centralize</b> <b>computing</b> {{as a result of}} cloud adoption is having an impact on the current type and level of coordination in workplaces. This study attempts to identify factors influencing the type of coordination strategy and the moderating effect of cloud computing usage. Specifically the study contends that cloud computing usage strengthens the effect of goal alignment and task interdependence on the cooperative intensity between the IT department and business line. Hopefully, data from an online survey of managers and their subordinates will be used to test hypothesis advanced in this study...|$|R
40|$|Abstract- In {{this paper}} we present the {{architectural}} concepts of highly complex {{knowledge based system}} dubbed as the Universal Knowledge Processing System (UKPS) could {{be thought of as}} a Wisdom Machine (WM) capable of solving any problem simple or difficult with the help of its knowledge bases. The Universal Knowledge Processing Machine is in turn formed of several Knowledge Processing Systems (KPS) connected together through a global network The paper also discusses the key block level components which go on to make level and needs further investigation in the cycle of evolution and development. The architecture of the knowledge processing systems over the decades has changed in the philosophy of computing in organization from the mainframe-dominated, <b>centralize</b> <b>computing</b> system...|$|R
50|$|Diskless nodes {{can be seen}} as a {{compromise}} between fat clients (such as ordinary personal computers) and <b>centralized</b> <b>computing,</b> using central storage for efficiency, but not requiring centralized processing, and making efficient use of the powerful processing power of even the slowest of contemporary CPUs, which would tend to sit idle for much of the time under the <b>centralized</b> <b>computing</b> model.|$|E
5000|$|The Informatics Branch {{provides}} <b>centralized</b> <b>computing</b> {{and telecommunications}} services {{to meet the}} Atmospheric Environment Programs' objectives, support other departmental operations, and assist other approved users. It operates and maintains the supercomputer facility in Dorval and various telecommunications networks.|$|E
50|$|A {{collective}} term encompassing both thin client computing, and its technological predecessor, text terminals (which are text-only), is <b>centralized</b> <b>computing.</b> Thin clients and text terminals can both require powerful central processing {{facilities in the}} servers, in order to perform all significant processing tasks {{for all of the}} clients.|$|E
40|$|In many {{biomedical}} applications, {{several researchers}} need to collaborate on extracting or validating models from empirical and simulation data. Often these collaborators do not reside {{at the same}} location, making collaboration and consultation difficult and costly. We present a system, TDE, which integrates sophisticated data exploration and telecoltaboration capabilities. It runs in a heterogeneous distributed computing environment, supporting {{a wide variety of}} displays around a <b>centralized</b> <b>compute</b> server. It offers the users customizable views of the data. Pointing and cursor linking are based in n-dimensional object space, rather than screen space. We demonstrate TDE's teleeollaborative data exploration facilities in three biomedical applications: user-asslsted, boundary-based segmentation of an embryo heart, multi-spectral segmentation of thyroid tissue, and volume probing of a CT scan...|$|R
40|$|In {{the last}} couple of decades, network {{connected}} systems have gradually replaced <b>centralized</b> parallel <b>computing</b> machines. To provide smooth operation of network applications, the underlying system has to provide so-called basic services. One of the most crucial services is to provide a transparent access to data lik...|$|R
40|$|We {{investigate}} {{the use of}} a thin-client based configuration in providing students with universal access to a <b>centralized,</b> graphical <b>computing</b> environment. The primary goal is to enable students to work effectively from arbitrary locations and computing platforms, while always interacting with the consistent environment seen in tightly controlled labs...|$|R
50|$|Decentralized {{computing}} is {{the allocation}} of resources, both hardware and software, to each individual workstation, or office location. In contrast, <b>centralized</b> <b>computing</b> exists when the majority of functions are carried out, or obtained from a remote centralized location. Decentralized computing is a trend in modern-day business environments. This {{is the opposite of}} <b>centralized</b> <b>computing,</b> which was prevalent during the early days of computers. A decentralized computer system has many benefits over a conventional centralized network. Desktop computers have advanced so rapidly, that their potential performance far exceeds the requirements of most business applications. This results in most desktop computers remaining idle (in relation to their full potential). A decentralized system can use the potential of these systems to maximize efficiency. However, it is debatable whether these networks increase overall effectiveness.|$|E
50|$|Some organisations use {{a hybrid}} client model partway between <b>centralized</b> <b>computing</b> and {{conventional}} desktop computing, {{in which some}} applications (such as web browsers) are run locally, while other applications (such as critical business systems) are run on the terminal server. One way to implement this is simply by running remote desktop software on a standard desktop computer.|$|E
50|$|Another {{disadvantage}} is {{that central}} computing {{relies heavily on}} the quality of administration and resources provided to its users. Should the central computer be inadequately supported by any means (e.g. size of home directories, problems regarding administration), then your usage will suffer greatly. The reverse situation, however, (i.e., a system supported better than your needs) {{is one of the key}} advantages to <b>centralized</b> <b>computing.</b>|$|E
40|$|The {{torsional}} braid experiment {{has been}} interfaced with a <b>centralized</b> hierarchical <b>computing</b> system for data acquisition and data processing. Such a system, when {{matched by the}} appropriate upgrading of the monitoring techniques, provides high resolution thermomechanical spectra of rigidity and damping, and their derivatives with respect to temperature...|$|R
40|$|Many {{scientific}} and engineering applications use models extracted from and validated by empirical and simulation data. Data extraction, validation, and simulation are rarely performed by an individual, {{but rather by}} a group of collaborators. Often these collaborators do not reside at the same location, making collaboration and consultation difficult and costly. Unfortunately, current scientific visualization and data exploration environments do not yet provide telecollaborative facilities. We describe a Telecollaborative Data Exploration system (TDE) which combines sophisticated scientific data exploration tools with telecollaborative capabilities. All exploration tools available to one user are also available to two or more users working on the same problem at other sites. TDE runs in a heterogeneous distributed computing environment, supporting a wide variety of displays around a <b>centralized</b> <b>compute</b> server. It offers the users customizable views of the data. Pointing and cursor linki [...] ...|$|R
40|$|A large, highly ranked public {{university}} implemented {{a requirement for}} all incoming undergraduates to own a laptop computer starting in fall, 2000. To control increased expenditures for information technology, this requirement has shifted some {{of the cost of}} technology to students by decreasing the need for <b>centralized</b> general-purpose <b>computing</b> laboratories. At the same time, a shift towards <b>centralized</b> academic <b>computing</b> support occurred. This shift was away from information technology resources, services and support based in individual departments. This shift, engineered by the newly formed office of the Chief Information Officer (CIO), was envisioned to generate cost savings through economies of scale. The educational impact of the laptop requirement is starting to be felt, but adoption is not widespread in daily classroom use. Envisioned cost savings have not yet become apparent. However, laptop ownership has enabled some new classroom activities, and helped to reinforce the leading-edge image of the university. ORGANIZATION BACKGROUN...|$|R
50|$|A {{relatively}} new method of <b>centralized</b> <b>computing,</b> hosted computing, solves {{many of the}} problems associated with traditional distributed computing systems. By centralizing processing and storage on powerful server hardware located in a data center, rather than in a local office, it relieves organizations of the many responsibilities in owning and maintaining an information technology system. These services are typically delivered on a subscription basis by an application service provider (ASP).|$|E
50|$|The {{expansion}} of the Internet during the 1990s brought about {{a new class of}} <b>centralized</b> <b>computing,</b> called Application Service Providers (ASP). ASPs provided businesses with the service of hosting and managing specialized business applications, with the goal of reducing costs through central administration and through the solution provider's specialization in a particular business application. Two of the world's pioneers and largest ASPs were USI, which was headquartered in the Washington, DC area, and Futurelink Corporation, headquartered in Irvine, California.|$|E
50|$|<b>Centralized</b> <b>{{computing}}</b> is computing done at {{a central}} location, using terminals that {{are attached to}} a central computer. The computer itself may control all the peripherals directly (if they are physically connected to the central computer), {{or they may be}} attached via a terminal server. Alternatively, if the terminals have the capability, {{they may be able to}} connect to the central computer over the network. The terminals may be text terminals or thin clients, for example.|$|E
40|$|Abstract. Cloud {{computing}} {{can improve}} corporate IT operational efficiency and reduce maintenance costs on enterprise PC terminal after <b>centralizing</b> <b>computing</b> and IT services. However, computing resources centralization makes security control and management particularly complicated. Without moderate isolation and security policy, one computer infected by virus {{can cause the}} whole network paralysis in cloud computing. It is absolutely necessary to develop "cloud management system " in cloud computing. In this paper, we analysis cloud management system architectures, build a cloud management system conceptual model, and design cloud management system deployment. We also implement cloud management system integrating a variety of technologies including host virtualization technology, virtual host automatic backup management, virtual host blocking and audit technique, and cloud computing access control etc. The cloud management system can guarantee the security of cloud computing, and provide a more economical and more reasonable solution to the cloud service users. 1...|$|R
30|$|The {{nature of}} problem P 1 {{is the same}} as that of P 2, and consequently, the upper bound and {{feasible}} <b>centralized</b> solution are <b>computed</b> in the same way as BRR.|$|R
30|$|The Mobile Cloud Computing Forum {{considers}} MCC as “an infrastructure {{where both}} the data storage {{and the data}} processing happen outside of the mobile device” [17]. In [18], MCC {{is defined as a}} new model for mobile applications: “it will be transferred to a <b>centralized</b> and powerful <b>computing</b> platform in the cloud”.|$|R
50|$|Data Management Platforms: A data {{management}} platform (DMP) is a <b>centralized</b> <b>computing</b> system for collecting, integrating and managing large sets of structured and unstructured data from disparate sources. Personalized marketing enabled by DMPs, {{is integral to}} consumers receiving relevant, timely, engaging, and personalized messaging and advertisements that resonate with their unique needs and wants. Growing number of DMP software options are available including Adobe Systems Audience Manager and Core Audience (Marketing Cloud) to Oracle-acquired BlueKai, Sitecore Experience Platform and X+1.|$|E
50|$|He was {{the founder}} and {{director}} of the Computer Applications & Research Center (CARC) at the University of Connecticut's School of Engineering. In 1981 the center was created to support the school's growing need for <b>centralized</b> <b>computing</b> research and development services. After his death the center was renamed to Taylor L. Booth Center for Computer Applications and Research or in its shorter form the Booth Research Center. In 2002 the Booth Research Center (BRC) and the Advanced Technology Institute (ATI), another center at the School of Engineering, merged into the Booth Engineering Center for Advanced Technology (BECAT).|$|E
5000|$|This {{technology}} is becoming popular in schools as {{it allows the}} school to provide pupils access to computers without purchasing or upgrading expensive desktop machines. Improving access to computers becomes less costly as thin client machines can be older computers that are no longer suitable for running a full desktop OS. Even a relatively slow CPU with as little as 128 MB of RAM can deliver excellent performance as a thin client. In addition, the use of <b>centralized</b> <b>computing</b> resources means that more performance can be gained for less money through upgrades to a single server rather than across a fleet of computers.|$|E
40|$|Log {{analytics}} are a bedrock {{component of}} running many of today’s Internet sites. Application and click logs {{form the basis}} for tracking and analyzing customer behaviors and preferences, and they form the basic inputs to ad-targeting algorithms. Logs are also critical for performance and security monitoring, debugging, and optimizing the large compute infrastructures that make up the compute “cloud”, thousands of machines spanning multiple data centers. With current log generation rates on the order of 1 – 10 MB/s per machine, a single data center can create tens of TBs of log data a day. While bulk data processing has proven to be an essential tool for log processing, current practice transfers all logs to a <b>centralized</b> <b>compute</b> cluster. This not only consumes large amounts of network and disk bandwidth, but also delays the completion of time-sensitive analytics. We present an in-situ MapReduce architecture that mines data “on location”, bypassing the cost and wait time of this store-first-query-later approach. Unlike current approaches, our architecture explicitly supports reduced data fidelity, allowing users to annotate queries with latency and fidelity requirements. This approach fills an important gap in current bulk processing systems, allowing users to trade potential decreases in data fidelity for improved response times or reduced load on end systems. We report on the design and implementation of our in-situ MapReduce architecture, and illustrate how it improves our ability to accommodate increasing log generation rates. ...|$|R
40|$|It is {{discussed}} the Linux PC cluster prototype at Petersburg Nuclear Physics Institute (Russia). It was enumerated {{the reasons why}} such a type of computing installation is profitable now and will be more attractive in nearest future for <b>centralized</b> general purpose <b>computing</b> in scientific applications. INTRODUCTION Last year (1997) we were faced the problem to upgrade <b>centralized</b> general purpose <b>computing</b> facility at HEP Division at PNPI. In looking for appropriate decision we took into account several hardware platforms, namely: Intel Pentium II as most wide spread microprocessor and DEC Alpha as most powerful microprocessor {{in a range of}} last years. For some comparison please see fig. 1. The figure 1 shows that two microprocessor lines have almost same power: the close frequency the close power. That means we have to consider other microprocessor features. A binary program size for DEC Alpha is 1. 5 - 3 times more than same binary program for Pentium due to RISC structure of Alpha mic [...] ...|$|R
40|$|Abstract – To {{overcome}} the traffic growth predicted by current ATM research programs in Europe and the US, we propose {{a new model}} to avoid conflicts based on small speed regulations, as depicted in the ERASMUS project [6], with interval conflict constraints as in [2]. After a first conflict detection phase, a <b>centralized</b> solver <b>computes</b> new RTAs to dynamically adjust the flight plans during the flight, taking operational costs for airlines and for ATC into account. The resolution would be iteratively performed over a rolling horizon to handle the uncertainties inherent to trajectory prediction. The described model is currently being implemented using Constraint Programming and Local Search as opti-mization techniques. Simulations {{will be carried out}} with Europe-wide traffic data...|$|R
50|$|As of 2007, <b>centralized</b> <b>computing</b> is {{now coming}} back into fashion - to a certain extent. Thin clients {{have been used for}} many years by {{businesses}} to reduce total cost of ownership, while web applications are becoming more popular because they can potentially be used on many types of computing device without any need for software installation. Already, however, there are signs that the pendulum is swinging back again, away from pure centralization, as thin client devices become more like diskless workstations due to increased computing power, and web applications start to do more processing on the client side, with technologies such as AJAX and rich clients.|$|E
50|$|Throughout {{the late}} 1960s and the 1970s, {{computer}} terminals were multiplexed onto large institutional mainframe computers (<b>centralized</b> <b>computing</b> systems), which in many implementations sequentially polled the terminals {{to see whether}} any additional data was available or action was requested by the computer user. Later technology in interconnections were interrupt driven, {{and some of these}} used parallel data transfer technologies such as the IEEE 488 standard. Generally, computer terminals were utilized on college properties in much the same places as desktop computers or personal computers are found today. In the earliest days of personal computers, many were in fact used as particularly smart terminals for time-sharing systems.|$|E
50|$|The {{very first}} {{computers}} {{did not have}} separate terminals as such; their primitive input/output devices were built in. However, soon it {{was found to be}} extremely useful for multiple {{people to be able to}} use a computer at the same time, for reasons of cost - early computers were very expensive, both to produce and maintain, and occupied large amounts of floor space. The idea of <b>centralized</b> <b>computing</b> was born. Early text terminals used electro-mechanical teletypewriters, but these were replaced by cathode ray tube displays (as found in 20th century televisions and computers). The text terminal model dominated computing from the 1960s until the rise to dominance of home computers and personal computers in the 1980s.|$|E
40|$|Abstracts. As {{the concept}} has been {{recommended}} in recent years, cloud computing {{more and more}} into sight. <b>Centralized</b> its <b>computing</b> and storage to the network, let local application procedures and client simple to only a support scripting browser, the performance of personal computer to minimize, maximize function,on {{the resources of the}} distribution of teaching resources integration way is needed. This paper expounds the concept and characteristics of cloud computing,and then it puts forward how to manage the teaching resources of college by cloud computing, in order to base the teaching resources sharing mechanism which relies on cloud technology, integrate the teaching resources, share the resources based on data security, promote the construction of digital teaching resources and the high quality teaching resources sharing...|$|R
40|$|The {{capability}} to query the topology of spatial regions {{is fundamental to}} today&# 039;s <b>centralized</b> spatial <b>computing</b> systems, like spatial databases and GIS. By contrast, this paper explores decentralized algorithms for computing the topology of spatial regions in wireless sensor networks. The approach generates global topological information about regions, using only the local knowledge of nodes and their immediate network neighbors aggregated up through spatial boundary structures. Using three basic boundary structures (boundary nodes, boundary cycles, and boundary orientation), a family of decentralized algorithms is defined that can respond efficiently to snapshot queries about the topology of spatial regions, including containment and adjacency queries. The communication complexity of the algorithm is O(n) for realistic inputs. Empirical investigation {{of the performance of}} the approach, using simulation, also confirms the efficiency, scalability, and robustness of this approach...|$|R
40|$|In 1980 the Ontario Cancer Treatment and Research Foundation {{embarked}} upon {{an ambitious}} program to introduce computing in its seven original cancer centres. Considerable expansion of its computer facilities {{to handle the}} Ontario Cancer Registry was also required. This paper describes {{the current status of}} the program which involves a combination of <b>centralized</b> and distributed <b>computing</b> using an IBM mainframe, Honeywell minicomputers and IBM microcomputers in local area networks. Included are brief descriptions of the major application areas...|$|R
