35|10000|Public
30|$|Variable <b>Costs</b> <b>of</b> <b>Sampling.</b>|$|E
40|$|In nature females {{can usually}} choose among males only {{sequential}}ly. Recent models of sequential choice predict that, with increasing <b>costs</b> <b>of</b> <b>sampling,</b> selectivity for preferred males should decline. In our tests female sticklebacks were highly {{selective in their}} sequential choice between dull and bright males when costs were low. With increasing time and energy costs of moving between males, they reduced their selectivity. In particular, when the females had to swim against a current, dull males when met became highly acceptable. The females' response to increased <b>costs</b> <b>of</b> <b>sampling</b> shows that they {{make the kind of}} economic decisions predicted by models of sequential choice. The strength of sexual selection by female choice therefore depends on the spatial structure of the population...|$|E
30|$|The {{application}} of these methods faces similar challenges in these very similar contexts: ensuring the representativeness and adequacy of the sample, and again keeping the <b>costs</b> <b>of</b> <b>sampling</b> under control. Below we discuss the different methodologies and outcomes of different empirical experiences conducted in Italy and Spain, paying {{particular attention to the}} “Centre sampling technique” (CS), which {{has proved to be a}} very effective approach for addressing the Italian situation with many immigrants not registered in the Anagrafe.|$|E
40|$|An urn {{contains}} N balls, labelled 1, [...] .,N. Optimal stopping {{rules are}} considered for payoff functions f(k, m) where f(k, m) is the reward when stopping after k draws, {{and the largest}} number seen by then is m. f(k, m) is assumed nondecreasing im for each k. We show: (i) For any horizon n [less-than-or-equals, slant] N, under optimal stopping, sampling without replacement yields a larger expected value than sampling with replacement. (ii) A sufficient condition, both when sampling with or without replacement, for the optimal stopping rule {{to be of the}} form t =inf{k:Mk [greater-or-equal, slanted]qk} for some constants qk, where Mk is the maximal label [Delta](k, m) = f(k, m + 1) - f(k, m) be nonincreasing in k for each m. Better sufficient conditions are given, and several e such as reward minus <b>cost</b> <b>of</b> <b>sampling,</b> or discounted rewards, are considered. Some limiting results, as N [...] >[infinity], and prophet inequality considerations are included for the example where the payoff is reward minus <b>cost</b> <b>of</b> <b>sampling.</b> Sampling without replacement optimal stopping <b>cost</b> <b>of</b> observation discounting prophet inequality...|$|R
40|$|This paper {{considers}} {{the choice of}} the most profitable sampling frequency for a feedback quality control system. It is assumed that the disturbance can be adequately modeled by a first-order Integrated Moving Average (IMA (0, 1, 1)) process. The cost model includes two terms, one for the <b>cost</b> <b>of</b> being off-target and one for the <b>cost</b> <b>of</b> <b>sampling</b> and adjustment. An analytical solution is obtained and the sensitivity of parameters examined...|$|R
40|$|A {{model is}} {{developed}} consisting <b>of</b> all the <b>costs</b> incurred when fraction-defective control charts are utilized to control current production: <b>cost</b> <b>of</b> <b>sampling,</b> <b>cost</b> <b>of</b> not detecting {{a change in}} the process, <b>cost</b> <b>of</b> false indication <b>of</b> change, and <b>cost</b> <b>of</b> re-adjusting detected changes. It is suggested that such control charts can be utilized in an optimal way, if those combinations <b>of</b> <b>sample</b> size, frequency <b>of</b> <b>sampling</b> and extent <b>of</b> control limits from process average will be used that provide the minimum total cost. It is further suggested that, by applying any specific cost data to this model, these optimal decision values can be easily evaluated by computer enumeration. ...|$|R
40|$|In {{this paper}} {{we present a}} method for {{sampling}} items that are checked on a pass/fail basis, {{with a view to}} a claim being made about the success/failure rate for the purposes of promoting a company???s product/service. Attention is paid to the appropriate use of statistical phrases for the claims and this leads to the development of Bayesian credible intervals. The hypergeometric distribution is used to calculate successive stopping rules so that the <b>costs</b> <b>of</b> <b>sampling</b> can be minimised. Extensions to the sampling procedure are considered so as to allow the potential for stronger and weaker claims to be made as sampling progresses. The relationship between the true error rate and the probabilities of making correct claims is discussed...|$|E
40|$|In {{educational}} effectiveness research, often multilevel {{data analyses}} are used because research units (most frequently, pupils or teachers) are studied that are nested in groups (schools and classes). This hierarchical data structure complicates designing the study because the structure {{has to be}} taken into account when approximating the accuracy of estimation and the power for statistical testing which should be sufficient to reach meaningful conclusions. Accuracy and power, both referred to as efficiency, can be optimized by carefully choosing the number of units to sample at each of the levels, taking into account the available resources and <b>costs</b> <b>of</b> <b>sampling</b> at these levels. We complement the findings that are found in the literature with regard to designing multilevel studies and propose a simulation approach that can be used to help making study-specific decisions. status: publishe...|$|E
30|$|The {{lack of an}} {{exhaustive}} list of the target population (and the inaccessibility of the Anagrafe) has led researchers in Italy to turn to alternative sampling methods which allow including different definitions of migrants (foreign-born) and, in particular, undocumented migrants. In the case of Spain, the financial and administrative barriers to draw and implement samples from the Padrón has also led some researchers to apply alternative sampling methods. The application of these methods faces similar challenges in both countries: ensuring the representativeness and unbiased nature of the sample, and again keeping the <b>costs</b> <b>of</b> <b>sampling</b> under control, due in particular to high attrition rates. In this context, and particularly for the case of Italy, the estimates based on the CS methodology {{seem to be the}} most accurate and reliable and the response rates are also satisfactory (Fasani, 2008; Mecatti & Migliorati, 2003; Blangiardo, 2008).|$|E
40|$|The <b>costs</b> <b>of</b> a {{clinical}} chemistry laboratory {{in a district}} general hospital were studied. The system used has certain advantages over the conventional Cooper Lybrand method. The time taken by technicians to perform tests was more variable than expected and the <b>cost</b> <b>of</b> <b>sample</b> collection was higher than process-cost for many tests. Indirect costs (overheads) were greater than direct costs and there were potential economies of scale. The most time-consuming part {{of this study was}} collecting the <b>cost</b> <b>of</b> chemicals and other disposables...|$|R
40|$|Abstract. - Experimental {{research}} in both medical sciences and material science rely {{in many situations}} on a reduced quantity of available data, due to limited number of patients or high <b>costs</b> <b>of</b> <b>samples.</b> Some statistical data manipulation methods are discussed regarding their applicability, information content, value and limits. A special attention {{is given to the}} extreme values eliminated by the GRUBBS test. The necessity of the elimination of the extreme values is demonstrated. ...|$|R
40|$|An {{expected}} {{cost model}} for the fraction defective control chart for the case where there are several out-of-control states is developed. A procedure is presented for determining the sample size, control limit or critical region, and interval between samples which minimizes the expected <b>cost</b> <b>of</b> control per unit of product. The expected <b>cost</b> <b>of</b> control consists <b>of</b> the <b>cost</b> <b>of</b> <b>sampling,</b> the <b>cost</b> <b>of</b> investigating and possibly correcting the process when an out-of-control condition is indicated, and the <b>cost</b> <b>of</b> producing defective product. Direct search techniques are used to optimize the expected cost function. Numerical examples and sensitivity analysis of the model are presented. ...|$|R
40|$|Minimum {{price is}} not the only {{objective}} that companies pursue when sourcing their materials. Selecting the best supplier entails looking for the best quality as well as the most reliable delivery. This work suggests a Multi-Criteria objective function that linearly aggregates a number of Taguchi loss functions, which represent the criteria of price, quality, and delivery. We initially recommend a framework to represent the market and then generate test data to represent the different market scenarios. We introduce randomness into this framework in order to achieve a highly realistic assumption. This study then employs the Optimal Computation Budget Allocation (OCBA) algorithm to choose the best supplier. OCBA solutions are benchmarked against the deterministic solution to check OCBA’s ability to find the optimal solution. OCBA solutions are also compared to an Equal Allocation (EA) algorithm to verify their effectiveness in terms of minimizing the <b>costs</b> <b>of</b> <b>sampling.</b> ...|$|E
40|$|The {{analysis}} of {{volatile organic compounds}} [VOCs] is an attractive approach {{to the discovery of}} potential cancer biomarkers due to its non-invasive nature and potential low <b>costs</b> <b>of</b> <b>sampling</b> and analysis. Solid phase microextraction [SPME] {{is one of the main}} extraction techniques used to date for the collection of VOCs from both in vivo and in vitro samples in studies of potential biomarkers of various types of cancer. It offers simplicity of use, compatibility with both gas-chromatography [GC] and liquid-chromatography [LC] separation techniques and relatively lower costs. Development of the SPME method includes several important considerations: selection of the sampling mode, type of fiber and holder, optimisation of incubation, extraction and desorption conditions, and finally the use of an appropriate calibration procedure. This review summarizes and discusses the particular parameters of the SPME method development used by researchers to date for VOCs collection, from various biological matrices, in search of potential biomarkers of cancer...|$|E
40|$|Female choice, {{identified}} as {{a major force in}} sexual selection theory, has recently been demonstrated in a number of species. These tests concentrated on simultaneous choice situations although females have to compare males sequentially in most territorial species, which is the more demanding task. Here it is shown that female three-spined sticklebacks, Gasterosteus aculeatus L., rate sequentially presented males according to their brightness. With increasing <b>costs</b> <b>of</b> <b>sampling</b> the females become less choosy. Furthermore, a male's attractiveness has a significant effect on the female's rating of the next male; a given male is rated higher when preceded by a duller male than by a brighter one and vice versa. Female sticklebacks use a stochastic decision rule in sequential mate choice that is attuned to the attractiveness of the present and previously encountered male. This demonstration of a "previous male effect" not only indicates an efficient mechanism for finding the best of a number of males but also extends the applicability of sexual selection theory...|$|E
40|$|We {{present a}} model of online music sharing that {{incorporates}} economic and technological incentives to sample, purchase, and pirate. Contrary to conventional wisdom, we find that lowering the <b>cost</b> <b>of</b> <b>sampling</b> music will propel more consumers to purchase music online as the total <b>cost</b> <b>of</b> evaluation and acquisition decreases. Attempts to prevent sampling will be counterproductive in the long run. Sharing technologies erode the superstar phenomenon widely prevalent in the music business. Extensive empirical investigations, based on surveys and Billboard ranking charts, {{lend support to the}} economic model and validate the key results. ...|$|R
40|$|In {{this paper}} we {{proposed}} a dynamic programming procedure to develop an optimal sequential sampling plan. A suitable cost model is employed for depicting the <b>cost</b> <b>of</b> <b>sampling,</b> accepting or rejecting the lot. This model is based on sequential approach. A sequential iterative approach is used for modeling the <b>cost</b> <b>of</b> different decisions in each stage. In addition a backward recursive algorithm is developed that {{can be adapted to}} a computer program. At the end of this paper a numerical example is solved to show how this model works.   </p...|$|R
40|$|Pork {{producers}} in Kansas were surveyed {{to determine their}} attitudes regarding onfarm Salmonella testing and to provide estimates ofthe costs ofcollecting hide, fecal, or blood samples from live pigs. Veterinarians and Cooperative Extension Service personnel were cited most frequently as the most preferred groups for monitoring and verification. Results ofthe survey indicate that pork producers {{may be willing to}} conduct on-farm Salmonella testing, if they can recover the <b>costs</b> <b>of</b> <b>sample</b> collection. The sampling costs ranged from $ 1. 76 to $ 4. 72 per pig, depending on the method <b>of</b> <b>sample</b> collection...|$|R
40|$|This paper derives optimal {{rules for}} {{sequential}} mastery tests. In a sequential mastery test, {{the decision is}} to classify a subject as a master or a nonmaster or to continue sampling and administering another random item. The framework of minimax sequential decision theory (minimum information approach) is used; that is, optimal rules are obtained by minimizing the maximum expected losses associated with all possible decision rules at each stage of sampling. The main advantage {{of this approach is}} that <b>costs</b> <b>of</b> <b>sampling</b> can be explicitly taken into account. The binomial model is assumed for the probability of a correct response given the true level of functioning, and threshold loss is adopted for the loss function involved. Monotonicity conditions are derived, conditions sufficient for optimal rules to be in the form of sequential cutting scores. The paper concludes with the description of a simulation study in which the minimax sequential strategy is compared with other procedures that exist for similar classification decision problems in the literature...|$|E
40|$|The {{cork oak}} (Quercus suber L.) is an {{evergreen}} oak {{that has the}} ability to produce a continuous layer of cork tissue which regenerates after being removed. Cork oak stands can be diverse in structure. Young stands are often regularly spaced, whereas older stands usually show clustering and can be mixed with other species. Farmers assessing cork value use a zigzag sampling procedure within a stand. In this study we compare zigzag sampling with two other sampling methods, fixed-radius plot sampling and n-tree distance sampling, using a model for the <b>costs</b> <b>of</b> <b>sampling.</b> We used data from two cork oak stands in Portugal as well as data from six types of simulated stands. We found that zigzag is the poorest sampling method, as in most situations it produces estimators with larger bias and larger standard errors than that produced by the other two procedures. Fixed-radius plot sampling and n-tree distance sampling produce comparable results; however, fixed-radius plot sampling is preferred because it produces unbiased estimator...|$|E
40|$|Time-series of high {{throughput}} {{gene sequencing}} data intended for gene regulatory network (GRN) inference are often short {{due to the}} high <b>costs</b> <b>of</b> <b>sampling</b> cell systems. Moreover, experimentalists lack a set of quantitative guidelines that prescribe the minimal number of samples required to infer a reliable GRN model. We study the temporal resolution of data vs quality of GRN inference in order to ultimately overcome this deficit. The evolution of a Markovian jump process model for the Ras/cAMP/PKA pathway of proteins and metabolites in the G 1 phase of the Saccharomyces cerevisiae cell cycle is sampled {{at a number of}} different rates. For each time-series we infer a linear regression model of the GRN using the LASSO method. The inferred network topology is evaluated in terms of the area under the precision-recall curve AUPR. By plotting the AUPR against the number of samples, we show that the trade-off has a, roughly speaking, sigmoid shape. An optimal number of samples corresponds to values on the ridge of the sigmoid...|$|E
40|$|In this article, we {{consider}} sequential {{estimation of the}} end points of the support based on the extreme values when the underlying distribution has a bound support. Some sequential fixed-width confidence intervals are proposed. Stopping rules based on the range are proposed and the estimation procedures based on them are shown to be asymptotically efficient. The results of numerical simulations are presented. Moreover, the sequential point estimation problem is considered under squared loss plus <b>cost</b> <b>of</b> <b>sampling...</b>|$|R
40|$|In {{practical}} utilization <b>of</b> stratified random <b>sampling</b> scheme, {{the investigator}} meets {{a problem to}} select a sample that maximizes the precision of a finite population mean under cost constraint. An allocation <b>of</b> <b>sample</b> size becomes complicated when more than one characteristic is observed from each selected unit in a sample. In many real life situations, a linear <b>cost</b> function <b>of</b> a <b>sample</b> size nh {{is not a good}} approximation to actual <b>cost</b> <b>of</b> <b>sample</b> survey when traveling cost between selected units in a stratum is significant. In this paper, sample allocation problem in multivariate stratified random sampling with proposed cost function is formulated in integer nonlinear multiobjective mathematical programming. A solution procedure is proposed using extended lexicographic goal programming approach. A numerical example is presented to illustrate the computational details and to compare the efficiency of proposed compromise allocation...|$|R
40|$|A utility-function {{approach}} to optimal spatial sampling design {{is a powerful}} way to quantify what "optimality" means. The emphasis then should be to capture all possible contributions to utility, including scientific impact and the <b>cost</b> <b>of</b> <b>sampling.</b> The resulting sampling plan should contain a component of designed randomness that would allow for a non-parametric design-based analysis if model-based assumptions were in doubt. [arXiv: 1509. 03410]Comment: Published at [URL] in the Bayesian Analysis ([URL] by the International Society of Bayesian Analysis ([URL]...|$|R
40|$|A {{methodology}} for optimized contaminated land investigation (OCLI) is described that balances {{the uncertainty of}} measurements against the cost of taking the measurements and the financial losses that may arise from misclassification of the land. Uncertainty from the sources of both field sampling and chemical analysis is estimated using existing techniques, based on the taking of duplicated samples. The actual <b>costs</b> <b>of</b> <b>sampling</b> and analysis and the expected costs that could arise from either `false positive¿ or `false negative¿ classification of areas of land were estimated. A loss function was constructed that calculates the expectation of financial loss that will arise for a given uncertainty of measurement. The function shows a clear minimum value of cost at an optimal value of uncertainty. Application of this OCLI technique to two case studies demonstrated this minimum value. Below the optimum value of uncertainty, the costs increased due to higher measurement costs. Above the optimum, the costs increased due to increasing risk of factors such as unnecessary remediation or potential litigation over undetected contamination. Many areas for further development of OCLI are identified, but the technique is demonstrated as a useful new approach to judging fitness-for-purpose of such measurement...|$|E
40|$|A techno-economical {{evaluation}} of the processing result of waste sorting plants should at least provide a realistic assessment of the recovery yields of valuable materials and of the qualities of the obtained products. This practical data is generated by weighing all the output products and sampling these products. Due to the technological complexity of sorting plants, for example, lightweight packaging waste treatments plants and the high expenditures concerning time and <b>costs</b> <b>of</b> <b>sampling</b> with subsequent manual sorting for quality determination, usually only final products undergo such an investigation. Thereby, the transferability of the results depends decisively on the boundary conditions (extent, throughput of the plant, process parameterization). Given that the process is too complex, not all relevant information of the process steps can be determined by sampling. By model calculations and/or adjustment of reasonable assumptions, information concerning weak points in the process can be identified, {{which can be used}} for further plant optimization. For the example of the recovery of beverage cartons from co-collected and mechanically recovered mixtures of lightweight packaging waste, a methodical approach for the assessment of processing results will be presented. </p...|$|E
40|$|It is well {{established}} that fish are sensitive indicators of environmental degradation and offer the major advantage of integrating the direct and indirect effects of stress over large scales of space and time. Nevertheless, the use of fish communities as indicators of environmental quality is highly challenging, therefore fish community {{has been one of}} the most neglected aspect of lake ecological monitoring. This paper gives an overview on fish-based assessment methods in Europe. By now, 15 Member States have finalised fish-based lake assessment systems, five of these assessment systems have been recently intercalibrated in the Alpine and Northern region, while Intercalibration is still ongoing in the Central-Baltic region. In contrary, several countries of the Mediterranean region have currently renounced the use of fish in lake assessment (mainly due to a low species richness, dominance of invasive taxa, and high <b>costs</b> <b>of</b> <b>sampling),</b> this opinion being strongly debated within region. This paper seeks to answer questions: How lake fish ecological assessment systems are built and used across Europe? Which pressures are assessed and are the pressure-response relationships tested? What are the main lessons and challenges of the lake fish methods` development and harmonization process...|$|E
30|$|Samples to be {{considered}} may be new samples designed {{or they may be}} archival samples or samples that did not quite achieve the appropriate standards in a previous round of testing. The overriding considerations are return on investment (likelihood to get the job, likelihood to get good money out of job if accepted), availability of machinery and raw material within specified time frame, time availability and <b>cost</b> <b>of</b> <b>sampling.</b> The initial decision-making regarding whether a sample should be put into manufacture is illustrated in Figure  5 b.|$|R
40|$|The {{success of}} {{site-specific}} nematode management {{depends on a}} grower or advisor being {{able to afford to}} make a map of an infestation that is accurate enough for management decisions. The spatial dependence of nematode infestations and correlation of soil attributes with nematode density were assessed to investigate the scale <b>of</b> <b>sampling</b> required to obtain correlated observations of density and the use of soils data to reduce the <b>cost</b> <b>of</b> <b>sampling.</b> Nematodes and soil were sampled on a 76. 2 × 76. 2 -m grid in two irrigated corn (Zea mays) fields for 2 years. Nematodes of each of three species were found in 36 % to 77 % of the cores from a field. Spatial dependence was detected for 10 of 16 distributions, and 22 % to 67 % of the variation in density within a field could be attributed to spatial correlation. Density was correlated to distances of 115 to 649 m in the directions of 0, 45, 90, and 135 ° from the crop row, and distances varied with direction. Correlations between nematode density and soil attributes were inconsistent between species and fields. These results indicate a potential for mapping nematode infestations for site-specific management, but provide no evidence for reducing the <b>cost</b> <b>of</b> <b>sampling</b> by substituting soils data for nematode counts when making a map...|$|R
50|$|The {{band had}} {{intended}} to use a number <b>of</b> <b>samples</b> from American films, but the <b>cost</b> <b>of</b> clearing these <b>samples</b> led them to using primarily British samples.|$|R
40|$|Biased and {{subjective}} {{choices in the}} variable selection processes used in ecological studies commonly lead researchers to reach misleading conclusions regarding patterns of biodiversity response to disturbances. Nevertheless, {{little attention has been}} given to these processes in the majority of studies published to date. Here, we assess the extent to which variables commonly employed in ecological studies correspond to those deemed to be most important by researchers of the same studies. Specifically, we examined both biodiversity (response) and environmental (explanatory) metrics from a comprehensive literature review and compared their use with their relative importance, according to a survey with the studies’ authors. We used the literature concerning land use change effects on dung beetles as our study case. Our results highlight marked disparities between researchers opinion and their choice of variables in published papers. We suggest that these disparities are due to the high <b>costs</b> <b>of</b> <b>sampling</b> and processing some variables, as well as to logistical constraints and researchers own bias. If current practices and these discrepancies persist then our understanding of the biodiversity consequences of land-use change will remain compromised, while further undermining our confidence in the results of ecological studies...|$|E
40|$|In this paper, {{we develop}} an {{expected}} cost {{model for a}} process whose mean is controlled by an Xc̅hart and whose variance is controlled by an R chart. The expected cost comprises the fixed and variable <b>costs</b> <b>of</b> <b>sampling,</b> the cost of investigating and correcting the process when at least one control chart indicates that the process parameters have shifted, {{and the cost of}} producing defective units. We use a search procedure to determine the sample size, interval between samples and control limits for both charts that minimize the expected cost. Optimal solutions to numerical examples are presented. A sensitivity analysis of the model is performed. In addition, we find the optimal interval between samples and the expected cost for several examples with large shifts in the mean and variance where Shewhart's heuristic design is used in place of the optimal design. Comparison of the expected cost of the optimal design to the expected cost of Shewhart's design shows an increase in expected cost of only 0. 4 to 8. 2 percent for the latter design. But other situations are discussed and examples presented which indicate that the optimal design is preferred. ...|$|E
40|$|The {{design of}} control charts in {{statistical}} quality control addresses the optimal selection of the design parameters (such as the sampling frequency and the control limits) and includes sensitivity analysis with respect to system parameters (such as the various process parameters and the economic <b>costs</b> <b>of</b> <b>sampling).</b> The advent of more complicated control chart schemes has necessitated the use of Monte Carlo simulation in the design process, especially {{in the evaluation of}} performance measures such as average run length. In this paper, we apply two gradient estimation procedures [...] -perturbation analysis and the likelihood ratio/score function method [...] -to derive estimators {{that can be used in}} gradient-based optimization algorithms and in sensitivity analysis when Monte Carlo simulation is employed. We illustrate the techniques on a general control chart that includes the Shewhart chart and the exponentially-weighted moving average chart as special cases. Simulation examples comparing the estimators with each other and with "brute force" finite differences demonstrate the possibility of significant variance reduction in settings of practical interest. {{statistical quality control}}, control charts, average run length, sensitivity analysis, economic design problem, Monte Carlo simulation, perturbation analysis, likelihood ratio/score function method...|$|E
40|$|This paper {{develops}} a theory and methodology for estimation of Gini index such that both <b>cost</b> <b>of</b> <b>sampling</b> and estimation error are minimum. Methods in which sample size is fixed in advance, cannot minimize estimation error and sampling cost {{at the same}} time. In this article, a purely sequential procedure is proposed which provides an estimate <b>of</b> the <b>sample</b> size required to achieve a sufficiently smaller estimation error and lower <b>sampling</b> <b>cost.</b> Characteristics <b>of</b> the purely sequential procedure are examined and asymptotic optimality properties are proved without assuming any specific distribution of the data. Performance of our method is examined through extensive simulation study...|$|R
40|$|This is a {{study of}} Bayesian data worth {{analysis}} in environmental clean-up applications. Its focus is on calculating the worth of simultaneously taking several (soil) samples in a small homogeneous area and on finding the optimal number <b>of</b> <b>samples</b> to take, by relating the reduction in risk cost from sampling, that is, data worth, to the <b>cost</b> <b>of</b> the <b>samples.</b> Even though the <b>cost</b> <b>of</b> one <b>sample</b> may be higher than the risk cost reduction it provides, this study shows that several samples may be cost-efficient. This is mainly due to two factors: {{one is that the}} unit sample cost often decreases as the number <b>of</b> <b>samples</b> increase; another, more important, factor is that the data worth <b>of</b> several <b>samples</b> typically is higher than the worth <b>of</b> fewer <b>samples.</b> Copyright # 2006 John Wiley & Sons, Ltd...|$|R
40|$|Advantages {{of dried}} blood spot include low invasiveness, ease and low <b>cost</b> <b>of</b> <b>sample</b> collection, transport, and storage. We used tandem mass {{spectrometry}} (LC-MS/MS) to determine phenobarbital levels on dried blood spot specimens and compared this methodology to commercially available particle enhanced turbidimetric inhibition immunoassay (PETINIA) in plasma/serum samples. The calibration curve in matrix using D 5 -phenobarbital as internal standard was linear in the phenobarbital concentration range of 19 ̆ 6100 mg/L (correlation coefficient 0. 9996). The coefficients {{of variation in}} blood spots ranged 2. 299 ̆ 66. 71...|$|R
