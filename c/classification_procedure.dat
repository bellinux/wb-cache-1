878|707|Public
500|$|Even before IQ {{tests were}} invented, there were {{attempts}} to classify people into intelligence categories by observing their behavior in daily life. Those {{other forms of}} behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of [...] "intelligence" [...] used in a particular case and on the reliability and error of estimation in the <b>classification</b> <b>procedure.</b>|$|E
5000|$|... passed <b>classification</b> <b>procedure</b> (three tests, {{covering}} {{visual and}} graphic abilities, spatial perception and general knowledge) ...|$|E
50|$|In {{statistical}} classification, the Fisher kernel, {{named after}} Ronald Fisher, {{is a function}} that measures the similarity of two objects {{on the basis of}} sets of measurements for each object and a statistical model. In a <b>classification</b> <b>procedure,</b> the class for a new object (whose real class is unknown) can be estimated by minimising, across classes, an average of the Fisher kernel distance from the new object to each known member of the given class.|$|E
30|$|<b>Classification</b> <b>procedures</b> define how {{subject matter}} {{instances}} (GSE projects) are systematically {{assigned to the}} categories of each dimension (Wheaton 1968). Qualitative <b>classification</b> <b>procedures</b> are based on scales while quantitative <b>classification</b> <b>procedures</b> are based on ratio scales (Wheaton 1968). The three taxonomies use nominal scales.|$|R
50|$|<b>Classification</b> <b>Procedures</b> are {{objective}} {{measurement techniques}} {{used to assign}} a PANOSE number to a font.|$|R
40|$|In {{this paper}} we tested two digital <b>classifications</b> <b>procedures</b> (supervised and automatic) {{in order to}} {{distinguish}} crop areas from other targets and evaluate the supervised classification results with the target's NDVI patters. The study area covers part of Ipuã municipality in São Paulo State, where land use were mostly agriculture, with corn, sorghum and sugar cane, and other areas with grassland and gallery forest. The digital image <b>classification</b> <b>procedures</b> are useful to separate and quantify agriculture areas. To separate different agricultural targets this present classification techniques and sensor spatial and spectral resolution are not appropriate. Pages: 79 - 8...|$|R
5000|$|A Chief Classifier is {{appointed}} to handle each event. As {{it relates to}} the event, their role {{is similar to that of}} Head of Classification. They are in charge direction, administration, co-ordination and implementation of classification during that sporting event. [...] If a sportsperson misses their <b>classification</b> <b>procedure</b> or is non-cooperative, it is up to the Chief Classifier to determine if the sportsperson had a valid reason and to reschedule their classification if they accept said reason.|$|E
5000|$|Even before IQ {{tests were}} invented, there were {{attempts}} to classify people into intelligence categories by observing their behavior in daily life. Those {{other forms of}} behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of [...] "intelligence" [...] used in a particular case and on the reliability and error of estimation in the <b>classification</b> <b>procedure.</b>|$|E
5000|$|Some {{classification}} {{rules are}} static functions. Others can be computer programs. A computer classifier can {{be able to}} learn or can implement static classification rules. For a training data-set, the true labels yj are unknown, but it is a prime target for the <b>classification</b> <b>procedure</b> that the approximation [...] as well as possible, where the quality of this approximation needs to be judged {{on the basis of the}} statistical or probabilistic properties of the overall population from which future observations will be drawn.|$|E
50|$|The GALEN {{tools and}} {{technologies}} {{were used in}} France {{for the development of}} the French <b>classification</b> of <b>procedures</b> <b>Classification</b> Commune des Actes Médicaux (CCAM).|$|R
40|$|The {{results from}} the U. S. corn/soybeans {{exploratory}} experiment which was completed during FY 1980 are summarized. The experiment consisted of two parts: the <b>classification</b> <b>procedures</b> verification test and the simulated aggregation test. Evaluations of labeling, proportion estimation, and aggregation procedures are presented...|$|R
40|$|In {{this paper}} {{we aim to}} analyze how {{sensible}} is the informal worker profile to distinct <b>classification</b> <b>procedures,</b> {{as well as to}} analyze the sensibility of the link between informality and welfare to the same procedures. A disaggregated analysis by employers and employees is one contribution of the paper, which also innovates using information on being included in an official tax payer administrative file (CNPJ) to one of the <b>classification</b> <b>procedures</b> employed to the employers. Two out of three procedures share very similar results in terms of informal worker profile and the link between informality and welfare. These are the procedure based on social security contribution, and the one based on CNPJ for employers and based on legal labor contracts for employees...|$|R
5000|$|Robert Weber notes: [...] "To make valid inferences {{from the}} text, it is {{important}} that the <b>classification</b> <b>procedure</b> be reliable in the sense of being consistent: Different people should code the same text in the same way". The validity, inter-coder reliability and intra-coder reliability are subject to intense methodological research efforts over long years.Neuendorf suggests that when human coders are used in content analysis two coders should be used. Reliability of human coding is often measured using a statistical measure of intercoder reliability or [...] "the amount of agreement or correspondence among two or more coders".|$|E
5000|$|Given {{a set of}} transformations [...] {{obtained}} from congealing many images of a certain category, the authors extend their classifier to the case where only one training [...] example of a new category [...] is allowed. Applying all the transformations [...] sequentially to , we create an artificial data training set for [...] This artificial data set can be made larger by borrowing transformations from not only one, but many already known categories. Once this data set is obtained, , a test instance of , can be classified as in the normal <b>classification</b> <b>procedure.</b> The key assumption here is that categories are similar enough that the transforms from one {{can be applied to}} another.|$|E
50|$|The REACH <b>classification</b> <b>procedure</b> {{consists}} of two basic phases. In phase one the hazards intrinsic to the material are determined, without any consideration of how the material might be used or encountered {{in the work place}} or by a consumer. In phase two the risk of harmful exposure is considered along with procedures that can mitigate exposure. Both GaAs and InP are in phase 1 evaluation. The principal exposure risk occurs during substrate preparation where grinding and polishing generate micron-size particles of GaAs and InP. Similar concerns apply to wafer dicing to make individual devices. This particle dust can be absorbed by breathing or ingestion. The increased ratio of surface area to volume for such particles increases their chemical reactivity.|$|E
5000|$|... #Article: International <b>Classification</b> of <b>Procedures</b> in Medicine ...|$|R
40|$|An {{evaluation}} of the corn and soybean proportion-estimation accuracy and dot labeling accuracy of the Simulated Aggregation Test, U. S. Corn and Soybean Exploratory Experiment, is presented. These results are in turn compared with the corn and soybean proportion-estimation accuracy and dot labeling accuracy of the <b>Classification</b> <b>Procedures</b> Verification Test...|$|R
40|$|Abstract: The {{problem of}} the lack of answer in {{questions}} of survey is usually dealt with different estimation and <b>classification</b> <b>procedures</b> from the answers to other questions. In this document, the results of applying fuzzy control methods for the vote-one of the variables with bigger lack of answer in opinion polls- are presented...|$|R
40|$|Every {{automatic}} <b>classification</b> <b>procedure,</b> {{defined by}} a classifier and an optimization procedure, has (a set of) problems for which it performs better than other proce-dures. The most simple of these problems we call the prob-lem archetype of the <b>classification</b> <b>procedure.</b> In this paper we present 2 D examples of these problem archetypes for 14 standard classifiers. It gives insight in the focus and robust-ness of the classification procedures 1. 1...|$|E
40|$|A {{pilot study}} {{has been carried out}} to test the {{effectiveness}} and feasibility of a multi-level <b>classification</b> <b>procedure</b> in handling a large number of vegetation species and aims to improve the classification accuracy. In this study, the research question is whether a selected vegetation index has its significance towards a specific vegetation species group. Ten vegetation indices extracted from in-situ hyperspectral remote sensing data has been applied and one-way ANOVA analysis (with significance level, α = 0. 01, 0. 05, and 0. 1) has been carried out to evaluate significant difference of vegetation indices in pair-wise vegetation species. In the multi-level <b>classification</b> <b>procedure,</b> vegetation species were classified continuously from one level to the next level until a good classification result has been achieved. Results indicated that multi-level <b>classification</b> <b>procedure</b> has effectiveness in handling vegetation species where accuracy has been improved from the first classification level to the second level. The study also has highlighted the significance of selected vegetation indices subsets toward different vegetation species in the multi-level <b>classification</b> <b>procedure</b> where the highest accuracy has been given by the selection in dedicated vegetation species groups...|$|E
30|$|Step 4 : Use a K-nearest neighbour(KNN)classifier {{to assign}} each element {{to a class}} for the input radar signals, to perform the <b>classification</b> <b>procedure.</b>|$|E
40|$|In {{this paper}} {{international}} markets are segmented {{and in order}} to eliminate the redundancy of the twenty-five socio-economic indicators, a factor analysis has been performed. In the space of factors which has been considered three hierarchical <b>classification</b> <b>procedures</b> are successively applied to the coordinates of 94 countries. Results are compared and discussed...|$|R
40|$|A {{statistical}} {{pattern recognition}} approach is presented for the on-line transient stability evaluation of electric power systems. The classifier {{that is used}} implements the Bayes' decision rule for classification. A flexible point estimates method is used to provide accurate values of the transient energy statistics, required in the <b>classification</b> <b>procedures.</b> © 1988...|$|R
40|$|The {{problem of}} the lack of answer in {{questions}} of survey is usually dealt with different estimation and <b>classification</b> <b>procedures</b> from the answers to other questions. In this document, the results of applying fuzzy control methods for the vote -one of the variables with bigger lack of answer in opinion polls- are presented...|$|R
40|$|Finbar’s Hotel is a multi-author {{book which}} was written by seven {{contemporary}} Irish authors. A novelty aspect {{of the book is}} that the names of the seven authors are known, but the assignment of authors to the seven chapters is unknown; each chapter has only one author. A model-based <b>classification</b> <b>procedure</b> is developed to determine the authorship of each chapter of the book. Vocabulary distributions are modelled using Sichel distributions which are estimated using text samples from the chapters and from books by the seven authors. Constraints are introduced into the model-based <b>classification</b> <b>procedure</b> so that a one-to-one matching of chapters and authors is achieved. A comparison of the maximum likelihood and Bayesian approaches to implementing the model-based <b>classification</b> <b>procedure</b> is completed. Finally, we give an answer to the question “Who wrote each chapter of Finbar’s Hotel?...|$|E
40|$|In {{this paper}} an Adaptive Bayesian Contextual <b>classification</b> <b>procedure</b> that {{utilizes}} both spectral and spatial interpixel dependency contexts in estimation of statistics and classification is proposed. Essentially, this classifier is the constructive coupling of an adaptive <b>classification</b> <b>procedure</b> and a Bayesian contextual <b>classification</b> <b>procedure.</b> In this classifier, the joint prior probabilities {{of the classes}} of each pixel and its spatial neighbors are modeled by the Markov Random Field. The estimation of statistics and classification are performed in a recursive manner to allow {{the establishment of the}} positive feedback process in a computationally efficient manner. Experiments with real hyperspectral data show that, starting with a small training sample set, this classifier can reach classification accuracies similar to that obtained by a pixelwise MLC with a very large training sample set. Additionally, classification maps are produced which have significantly less speckle error...|$|E
40|$|In this paper, {{we present}} a texture <b>classification</b> <b>procedure</b> that makes use of a blind {{deconvolution}} approach. Specifically, the texture is modeled as the output of a linear system driven by a binary excitation. We show that features computed from one-dimensional slices extracted from the two-dimensional autocorrelation function (ACF) of the binary excitation allows representing the texture for rotation-invariant classification purposes. The two-dimensional classification problem is thus reconduced to a more simple one-dimensional one, {{which leads to a}} significant reduction of the <b>classification</b> <b>procedure</b> computational complexity...|$|E
30|$|This {{paper is}} {{organised}} as follows. In Section 2, the theory underlying fractal sets is introduced, and {{the materials and}} methods employed in this work are presented. In Section 3, the results obtained in the characterisation and <b>classification</b> <b>procedures</b> are shown and discussed. In Section 4, the conclusions are summarised, as well as possible future works.|$|R
40|$|Difficult pattern {{recognition}} problems involving large class sets and noisy input {{can be solved}} by a multiple classifier system, which allows simultaneous use of arbitrary feature descriptors and <b>classification</b> <b>procedures.</b> Independent decisions by each classifier can be combined by methods of the highest rank, Borda count, and logistic regression, resulting in substantial improvement in overall correctness. ...|$|R
40|$|This Management Memo {{provides}} information to state departments about actions to take during electrical emergencies. This memo includes: background notification caution about other sources emergency stage <b>classifications</b> <b>procedures</b> for each emergency stage related memos contact information Background During periods in which electrical demand puts strains on the electric {{systems of the}} state’s utilities, the California Independent Syste...|$|R
40|$|A {{sequential}} nonparametric pattern <b>classification</b> <b>procedure</b> is presented. The method presented is {{an estimated}} {{version of the}} Wald sequential probability ratio test (SPRT). This method utilizes density function estimates, and the density estimate used is discussed, including a proof of convergence in probability of the estimate to the true density function. The <b>classification</b> <b>procedure</b> proposed makes use {{of the theory of}} order statistics, and estimates of the probabilities of misclassification are given. The procedure was tested on discriminating between two classes of Gaussian samples and on discriminating between two kinds of electroencephalogram (EEG) responses...|$|E
40|$|Objective:This {{dissertation}} {{developed an}} automatic <b>classification</b> <b>procedure,</b> {{as an example}} of a novel tool for an informationist, which extracts information from published abstracts, classifies abstracts into their "fields of study," and then determines the researcher's "field of study" and "level of activity. " Method: This dissertation compared a domain expert's method of classification and an automatic <b>classification</b> <b>procedure</b> on a random sample of 101 medical researchers (derived from a potential list of 305 medical researchers) and their associated abstracts. Design: The study design is a retrospective, cross-sectional, inter-rater agreement study, designed to compare two classification methods (i. e., automatic <b>classification</b> <b>procedure</b> and domain expert). The study population consists of University of Pittsburgh, School of Medicine, Department of Medicine (DOM) professionals who (1) have published at least one article listed in PubMed® as first or last author and/or (2) are the primary investigator for at least one grant listed in CRISP. Main outcome measures: Three outcome measures were derived from the domain expert's versus automatic categorization procedure: (1) an abstract's "field of study," (2) a researcher's "field of study" and (3) a researcher's "level of activity and field of study. " Results: Kappa showed moderate agreement between automatic and domain expert classification for the abstracts' "field of study" (Kappa = 0. 535, n = 504, p <. 000). Kappa showed moderate agreement between automatic and domain expert classification of the researcher's "field of study" (Kappa = 0. 535, n = 101, p <. 000). Kappa showed good agreement between automatic and domain expert classification of the researcher's "level of activity and field of study" (Kappa = 0. 634, n = 101, p <. 000). Conclusion: The study suggests that an automatic library <b>classification</b> <b>procedure</b> can provide rapid classification of medical research abstracts into their "fields of study. " The <b>classification</b> <b>procedure</b> can also process multiple abstracts' "fields of study" and classify their associated medical researchers into their "field of study" and "level of activity and field of study. " The <b>classification</b> <b>procedure,</b> used as a tool by an informationist, can be used as the basis for new services...|$|E
30|$|Visual {{appearance}} sorting {{with a fine}} gradation {{of visual}} characteristics and thus {{a large number of}} different classes could be reproduced in relatively good approximation with the tested two-stage <b>classification</b> <b>procedure.</b>|$|E
50|$|Unlike frequentist <b>procedures,</b> Bayesian <b>classification</b> <b>procedures</b> {{provide a}} natural way of {{taking into account}} any {{available}} information about the relative sizes of the sub-populations associated with the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, {{in the days before}} Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.|$|R
40|$|This paper {{presents}} a general notion of Mahalanobis distance for functional data that extends the classical multivariate concept to {{situations where the}} observed data are points belonging to curves generated by a stochastic process. More precisely, a new semi-distance for functional observations that generalize the usual Mahalanobis distance for multivariate datasets is introduced. For that, the development uses a regularized square root inverse operator in Hilbert spaces. Some of the main characteristics of the functional Mahalanobis semi-distance are shown. Afterwards, new versions of several well known functional <b>classification</b> <b>procedures</b> are developed using the Mahalanobis distance for functional data {{as a measure of}} proximity between functional observations. The performance of several well known functional <b>classification</b> <b>procedures</b> are compared with those methods used in conjunction with the Mahalanobis distance for functional data, with positive results, through a Monte Carlo study and the analysis of two real data examples...|$|R
40|$|Supervised <b>classification</b> <b>procedures</b> are {{developed}} {{and applied to}} synthetic aperture radar (SAR) {{in order to identify}} their various earth terrain components. An implementation of the maximum a posteriori (MAP) and the maximum likelihood (ML) algorithms are presented. These two techniques need a statistic model for the conditional distribution of the polarimetric complex data. Many previous studies used the classical Rayleigh distribution to characterize the earth terrain, but this model doesn’t yield a good result for heterogeneous backscattering media. This study applies a new model based on the K-distribution. This distribution, based on the physical definition of the texture and its mathematical representation, will be shown as rigorous model to describe amplitudes and intensities of the backscattering signal. We also use Markov fields to enhance the results of the <b>classifications.</b> These <b>classification</b> <b>procedures</b> have been applied to the Flevoland site (Holland) and Landes forest (France) SAR images, supplied by the Je...|$|R
