717|249|Public
25|$|The main {{categories}} of networks are acyclic or feedforward neural networks (where the signal passes {{in only one}} direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks {{can be applied to}} the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or <b>competitive</b> <b>learning.</b>|$|E
25|$|Pattern {{separation}} {{is the ability}} to differentiate one memory from other stored memories. Pattern separation begins in the dentate gyrus, a section of the hippocampus involved in memory formation and retrieval. Granule cells in the dentate gyrus process sensory information using <b>competitive</b> <b>learning,</b> and relay a preliminary representation to form place fields. Place fields are extremely specific, as they are capable of remapping and adjusting firing rates in response to subtle sensory signal changes. This specificity is critical for pattern separation, as it distinguishes memories from one another.|$|E
5000|$|Self-organization such as {{self-organizing}} maps, <b>competitive</b> <b>learning</b> ...|$|E
5000|$|It {{leads to}} the {{development}} of more discoveries as compared to <b>competitive</b> or individualistic <b>learning</b> approaches.|$|R
5000|$|At Arts York, {{there are}} 2 classes (often combined) of {{approximately}} 30 students. Admission to the Arts York Drama program is extremely <b>competitive.</b> Students <b>learn</b> the metrics of theater both through academic and practical training, {{and are expected}} to study multiple aspects of theater arts including: ...|$|R
40|$|We {{describe}} the Fourier basis, a linear value function approximation {{scheme based on}} the Fourier series. We empirically demonstrate that it performs well compared to radial basis functions and the polynomial basis, the two most popular fixed bases for linear value function approximation, and is <b>competitive</b> with <b>learned</b> proto-value functions...|$|R
5000|$|There {{are three}} basic {{elements}} to a <b>competitive</b> <b>learning</b> rule: ...|$|E
50|$|<b>Competitive</b> <b>learning</b> {{is a form}} of {{unsupervised}} learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, <b>competitive</b> <b>learning</b> works by increasing the specialization of each node in the network. It is well suited to finding clusters within data.|$|E
5000|$|Draft Report [...] "Some <b>Competitive</b> <b>Learning</b> Methods" [...] (contains {{descriptions}} of several related algos) ...|$|E
40|$|Prediction is {{a complex}} notion, and {{different}} predictors (such as people,computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of <b>competitive</b> on-line <b>learning</b> can benefit from the kinds of prediction that are now foreign to it...|$|R
40|$|Prediction is {{a complex}} notion, and {{different}} predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of <b>competitive</b> on-line <b>learning</b> can benefit from the kinds of prediction that are now foreign to it. The standard goal for predictor in learning theory is to incur a small loss for a given loss function measuring {{the discrepancy between the}} predictions and the actual outcomes. <b>Competitive</b> on-line <b>learning</b> concentrates on a “relative ” version of this goal: the predictor is to perform almost as well as the best strategies in a given benchmark class of prediction strategies. Such predictions can be interpreted as decisions made by a “small ” decision maker (i. e., one whose decisions do not affect the future outcomes). Predictions, or probability forecasts, considered in the foundations o...|$|R
40|$|International audienceWe {{present a}} first step towards anticipatory machine {{improvisation}} systems. The proposed system, based on fundamentals of music cognition, is a multi-agent memory-based collaborative and <b>competitive</b> reinforcement <b>learning</b> architecture, capable of live interaction with a musician or a music score. Results demonstrate the ability to model long-term stylistic planning and need for much less training data than reported in previous works...|$|R
50|$|Here is {{a simple}} <b>competitive</b> <b>learning</b> {{algorithm}} to find three clusters within some input data.|$|E
50|$|Models and {{algorithms}} {{based on}} the principle of <b>competitive</b> <b>learning</b> include vector quantization and self-organizing maps (Kohonen maps).|$|E
50|$|Vector {{quantization}} {{is based}} on the <b>competitive</b> <b>learning</b> paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.|$|E
40|$|Brand {{orientation}} is an approach {{in which the}} process of the organization revolve around the creation, development, and protection of brand identity in an ongoing interaction with the target group with the aim of achieving lasting <b>competitive</b> advantages. <b>Learning</b> to see intangible values and symbols as resouces is the necessary step in brand orienation [...] . Case studies Nestlé, DuPont, Tetra Pak, Volvo [...] ...|$|R
40|$|We {{describe}} the Fourier Basis, a linear value function approximation {{scheme based on}} the Fourier Series. We empirically evaluate its properties, and demonstrate that it performs well compared to Radial Basis Functions and the Polynomial Basis, the two most popular fixed bases for linear value function approximation, and is <b>competitive</b> with <b>learned</b> Proto-Value Functions even though no extra experience or computation is required. ...|$|R
5000|$|Mao took {{keyboard}} and vocal lessons since youth, and {{has gone on}} to <b>learn</b> <b>competitive</b> ballroom dancing such as Latin dance and dancesport. She studied performing arts at Central Academy of Drama.|$|R
5000|$|... the {{foundations}} of neural network research: <b>competitive</b> <b>learning,</b> self-organizing maps, instars, and masking fields (for classification), outstars (for spatial pattern learning), avalanches (for serial order learning and performance), gated dipoles (for opponent processing); ...|$|E
5000|$|IEEE 1998 Industry Applications Society Best Paper Prize, for the {{technical}} competence {{displayed in the}} paper entitled [...] "A novel <b>competitive</b> <b>learning</b> neural network based acoustic transmission system for oil-well monitoring" [...] October 1998.|$|E
50|$|Such cumulative, <b>competitive</b> <b>learning</b> {{was also}} {{accomplished}} more effectively by private tutors using individual tuition, specially prepared manuscripts, and graded examples and problems, {{than it was}} by college lecturers teaching large classes at {{the pace of the}} mediocre.|$|E
40|$|This paper, aims {{to address}} the ability of self-organising {{networks}} to automatically extract and correspond landmark points using only topological relations derived from <b>competitive</b> hebbian <b>learning.</b> We discuss, how the Growing Neural Gas (GNG) algorithm {{can be used for}} the automatic extraction and correspondence of nodes in a set of objects, which are then used to built statistical human brain MRI and hand gesture models. ...|$|R
40|$|It {{has been}} shown empirically that the XCS {{classifier}} system solves typical classification problems in a machine <b>learning</b> <b>competitive</b> way. However, until now, no learning time estimate has been available for the system. This paper introduces a time estimate that bounds the learning time of XCS until maximally accurate classifiers are found. We assume a domino convergence model in which each attribute is successively specialized to the correct value. It is shown that learning time in XCS scales polynomial in problem length and exponential {{in the order of}} problem difficulty and thus in a machine <b>learning</b> <b>competitive</b> way...|$|R
40|$|This paper {{introduces}} Competitive Neural Trees (CNeT) for pattern classification. The CNeT performs {{hierarchical classification}} and employ <b>competitive</b> unsupervised <b>learning</b> at the node level. The generalization {{ability of the}} CNeT is guaranteed by forward pruning, which is an inherent part of the learning process. Different search methods for the CNeT are introduced and used for training and recall. The influence of different search methods {{on the performance of}} the CNeT is experimentally evaluated...|$|R
50|$|According to Johnson and Johnson's meta-analysis, {{students}} in cooperative learning settings {{compared to those}} in individualistic or <b>competitive</b> <b>learning</b> settings, achieve more, reason better, gain higher self-esteem, like classmates and the learning tasks more and have more perceived social support.|$|E
5000|$|<b>Competitive</b> <b>Learning</b> {{is usually}} {{implemented}} with Neural Networks that contain a hidden layer which is {{commonly known as}} “competitive layer”. Every competitive neuron is described by a vector of weights [...] and calculates the similarity measure between the input data [...] and the weight vector [...]|$|E
5000|$|Cooperative {{learning}} may be {{contrasted with}} competitive and individualistic learning. The key {{difference between these}} teaching approaches is the way students' learning goals are structured. The goal structure specifies {{the ways in which}} students will interact with each other and the teacher during the instructional session. Within cooperative situations, individuals seek outcomes that are beneficial to themselves and beneficial to all other group members. In <b>competitive</b> <b>learning</b> students work against each other to achieve an academic goal such as a grade of [...] "A" [...] that only one or a few students can attain. Finally, in individualistic learning students work by themselves to accomplish learning goals unrelated to those of the other students.In cooperative and individualistic learning, student efforts are evaluated on a criteria-referenced basis while in <b>competitive</b> <b>learning</b> teachers grade in a norm-referenced basis.|$|E
40|$|Although the {{detection}} of invariant structure in a given set of input patterns is vital to many recognition tasks, connectionist learning rules {{tend to focus on}} directions of high variance (principal components). The prediction paradigm is often used to reconcile this dichotomy; here we suggest a more direct approach to invariant learning based on an anti-Hebbian learning rule. An unsupervised two-layer network implementing this method in a <b>competitive</b> setting <b>learns</b> to extract coherent depth information from random-dot stereograms. ...|$|R
50|$|The Forest City Velodrome runs several {{programs}} designed to encourage recreational cycling and develop <b>competitive</b> cyclists. <b>Learn</b> to ride programs introduce new riders to track cycling. Organized drills help cyclists hone their skills. Frequent recreational sessions give riders of various skill levels time to ride on the track for fun, fitness and training. Periodic race nights develop racing skills and give spectators the chance to learn about and enjoy track cycling events. In 2013, the track hosted the Ontario Provincial Track Championships.|$|R
2500|$|Johnson {{began his}} work on social {{interdependence}} theory and the appropriate use of cooperative, competitive, and individualistic efforts in the mid-1960s. At that time, elementary, secondary, and university teaching was dominated by <b>competitive</b> and individualistic <b>learning.</b> [...] Social Darwinism, with its premise that students must be taught {{to survive in a}} “dog-eat-dog” world dominated educational thought, although it was being challenged by individualistic learning largely based on B. F. Skinner’s work on programmed learning and behavioral modification. In the 60's and 70's, while cooperative learning was generally ignored by educators, Johnson challenged the prevailing competitive and individualistic practices by presenting the theory and research on cooperative, <b>competitive,</b> and individualistic <b>learning,</b> creating operational procedures for cooperative learning and appropriate competition, and implementing cooperative learning in schools and universities.|$|R
50|$|Companies with {{spread out}} {{distribution}} chains use e-learning for staff training {{and development and}} to bring customers information about the latest product developments. Continuing professional development (CPD) can deliver regulatory compliance updates and staff development of valuable workplace skills. For effectiveness and <b>competitive</b> <b>learning</b> performance, scoring systems are designed to give live feedback on decision-making in complex (mobile) learning scenarios.|$|E
50|$|There {{are many}} reasons why {{competitors}} tend to achieve less than they would if they were working cooperatively. And {{there have also been}} lots of studies making a claim that cooperative learning is more effective than <b>competitive</b> <b>learning</b> and individualistic efforts. But studies also show that competition and individualistic efforts can be constructive and should be encouraged when they are appropriately structured.|$|E
5000|$|Austin College was {{profiled}} {{in all three}} editions of renowned education writer and editor Loren Pope's book Colleges That Change Lives, which distinguishes schools having two essential elements: [...] "A familial sense of communal enterprise that gets students heavily involved in cooperative rather than <b>competitive</b> <b>learning,</b> and a faculty of scholars devoted to helping young people develop their powers, mentors who often become their valued friends." ...|$|E
2500|$|Rowing {{occurs at}} Townsville & JCU Rowing Club and Riverway Rowing Club. Both clubs cater to <b>competitive</b> masters, social, <b>learn</b> to row and school-based rowing programs. In 2009 the Townsville & JCU club won its first Queensland Club Premiership and in 2010 Riverway club claimed theirs.|$|R
40|$|Martinetz and Schulten {{proposed}} {{the use of}} a <b>Competitive</b> Hebbian <b>Learning</b> (CHL) rule to build Topology Representing Networks. From a set of units and a data distribution, a link is created {{between the first and second}} closest units to each datum, creating a graph which preserves the topology of the data set. However, one has to deal with finite data distributions generally corrupted with noise, for which CHL may be unefficient. We propose a more robust approach to create a topology representing graph, by considering the density of the data distribution. ...|$|R
40|$|Crockett, Spear and Sunder [2005 b] propose an {{algorithm}} whereby boundedly rational agents {{with standard}} neoclassical preferences <b>learn</b> <b>competitive</b> equilibrium in a repeated static exchange economy. In this paper a laboratory market is instituted {{to examine the}} hypothesis that people {{are at least as}} sophisticated as these agents. The adopted market institution strongly restricts the space of agent actions, facilitating the identification of decision rules. Evidence for <b>learning</b> <b>competitive</b> equilibrium is mixed due to strong heterogeneity in decision-making. Some subjects clearly demonstrate the ability to learn across periods. However, a majority exhibit little evidence of learning, and many are, in fact, simply content to satisfice, though the opportunity to do better was fairly straightforward. The presence of satisficers permits noncompetitive outcomes within this particular market institution, but I conjecture learners may stimulate convergence to competitive equilibrium in others...|$|R
