20|52|Public
5000|$|... #Caption: The New Tong Wen Tang {{serves the}} Traditional-Simplified Chinese <b>character</b> <b>conversion</b> {{function}} for Mozilla Firefox.|$|E
50|$|Besides the Traditional-Simplified Chinese <b>character</b> <b>conversion</b> and zooming function, it {{can also}} {{directly}} input converted content of web pages into the clipboard.|$|E
50|$|He {{is one of}} the key {{technical}} {{contributors to}} the Unicode specifications, being the primary author or co-author of Bi-directional Algorithm (used worldwide to display Arabic and Hebrew text), Collation (used for sorting and searching), Normalization, Scripts, Text segmentation, Identifiers, Regular Expressions, Compression, <b>Character</b> <b>Conversion,</b> and Security.|$|E
5000|$|For the {{convenience}} of English writers and readers the Old Norse characters not used in English are commonly replaced with English ones. This can lead to ambiguity and confusion. Diacritics may be removed (á → a, ö → o). The following <b>character</b> <b>conversions</b> also take place: ...|$|R
5000|$|<b>Character</b> {{encoding}} <b>conversion</b> between ASCII, UTF-8, and UTF-16 formats ...|$|R
50|$|The Unicode Security Considerations report {{recommends}} {{this character}} {{as a safe}} replacement for unmappable characters during <b>character</b> set <b>conversion.</b>|$|R
50|$|Conversion is {{done through}} {{a set of}} <b>character</b> <b>conversion</b> tables that may be edited by administrators. To provide an {{alternative}} means to harmonize the characters when the server-side converters fail to work properly, a special template was created to manually convert characters and article titles in one specific page.|$|E
50|$|The budget also ensures {{automatic}} {{adoption of}} {{measures in the}} 2013 federal budget regarding capital cost allowance, lifetime capital gains exemptions, dividend tax credit on ineligible dividends, corporate and trust loss trading, leveraged life insurance arrangements, <b>character</b> <b>conversion</b> transactions, and other federal legislative tax changes. The high income threshold for the top personal marginal tax rate increased from $500,000 to $509,000 {{as a result of}} indexing.|$|E
5000|$|The CTB {{was based}} on a shared library concept in an era when the Mac OS did not include a shared library system. Instead, the CTB wrote its own driver manager layer, the Communications Resource Manager. The Resource Manager was {{responsible}} for installing and managing the various drivers, or [...] "Tools" [...] that provided various functions within the CTB system. There were three primary types of Tools, each with their own associated Manager: the Connection Manager handled the communications drivers that opened channels to remote services, the Terminal Manager managed Tools that implemented the <b>character</b> <b>conversion</b> and command string interpretation needed to support any sort of terminal emulator, and the File Transfer Manager did the same for any sort of file transfer protocol.|$|E
5000|$|Since {{many letters}} are {{distinguished}} from others solely by a dot {{above or below}} the main <b>character,</b> the <b>conversions</b> frequently used the same letter or number with an apostrophe added before or after (e.g. 3 is used to represent [...] ).|$|R
40|$|Even {{today in}} Twenty First Century Handwritten {{communication}} {{has its own}} stand {{and most of the}} times, in daily life it is globally using as means of communication and recording the information like to be shared with others. Challenges in handwritten characters recognition wholly lie in the variation and distortion of handwritten characters, since different people may use different style of handwriting, and direction to draw the same shape of the characters of their known script. This paper demonstrates the nature of handwritten <b>characters,</b> <b>conversion</b> of handwritten data into electronic data, and the neural network approach to make machine capable of recognizing hand written characters. Comment: 4 pages, 1 Figure, ISSN: 0975 - 888...|$|R
40|$|Challenges in {{handwritten}} characters {{recognition is}} due to the variation and distortion of handwritten characters, since different people use different style and way of draw the same shape of the characters. This paper demonstrates the nature of handwritten <b>characters,</b> <b>conversion</b> of handwritten data into electronic data, and the neural network approach to make machine capable of recognizing hand written characters. This motivates the use of Genetic Algorithms for the problem. In order to prove this, we made a pool of images of characters. We converted them to graphs. The graph of every character was intermixed to generate new or unique styles intermediate between the styles of parent character. Character recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing...|$|R
40|$|This report {{explores the}} {{possibility}} of creating a Pinyin <b>Character</b> <b>conversion</b> system which is more dynamic in nature. This proposed system will undergo learning stages to build up its knowledge database. Subsequently, rules will be formulated {{to assist in the}} conversion stage. Learning will take place throughout the entire life-cycle of the system. It takes place in the knowledge building stage when the system started out as a naïve system and later on, continues to learn through the mistakes that it makes during application of the rules. The purpose of this system is to break away from the conventional pure statistical approach to pinyin <b>character</b> <b>conversion</b> and to build a more intelligent system...|$|E
40|$|This thesis {{describes}} a new method of voice conversion, which aims at <b>character</b> <b>conversion</b> based on eigenvoice GMM (EV-GMM) approach. Using an eigenvoice space built from 273 speakers and speech samples {{of three different}} characters created by a single skilled voice actor/actress, the conversion can generate {{the voices of the}} three characters from an arbitrary speaker, while keeping the speaker identity. Listening tests were carried out by presenting two kinds of synthetic voices; before and after the <b>character</b> <b>conversion.</b> The results showed that listeners, both native and non-native speakers, can perceive well the character voice difference as what was intended by experimenters. It was also shown that this difference was perceived well even when F 0 difference between the two was very small, which indicates better performance of our method in <b>character</b> <b>conversion</b> compared to the general F 0 -based conversion. Further, acoustic comparison between different characters in two cases of the voice actor and the proposed method was made. Results showed that the proposed method can realize acoustically valid modification between different characters. 報告番号:; 学位授与年月日: 2012 - 09 - 27; 学位の種別: 修士; 学位の種類: 修士(情報理工学); 学位記番号:; 研究科・専攻: 情報理工学系研究科電子情報学専...|$|E
40|$|A set of <b>character</b> <b>conversion</b> {{utilities}} for {{tapes were}} develope {{a couple of}} years ago for use on Sperry 1100. They have since been modified and noted bugs removed. Further a couple of routines have been added to read files written on PDP- 11 system. These are reported. Also added is an absolute to scratch only the tape header labels...|$|E
40|$|Statistical {{method is}} a good way for pinyin to Chinese <b>characters</b> <b>conversion</b> and has gotten {{preferable}} conversion rate. However, there are still several percent words cannot be converted correctly with the method. This paper presents an error correction approach based on grammatical and semantic rules. According to the conversion results and neighboring information obtained from pinyin to Chinese characters using statistical method, we build a knowledge base consists of phrase rules, syntactic rules and semantic rules. By analyzing the syntactic structure of sentences, we check the semantic correction at some local part of speech node. This method is used for error correction as a post processing method under the assumption of localized error point at preliminary experiment. The experiments prove that the correct conversion rate is improved based on rule method...|$|R
50|$|Kermit is a {{computer}} file transfer/management protocol {{and a set of}} communications software tools primarily used {{in the early years of}} personal computing in the 1980s; it provides a consistent approach to file transfer, terminal emulation, script programming, and <b>character</b> set <b>conversion</b> across many different computer hardware and OS platforms.|$|R
40|$|We study {{a number}} of natural {{language}} decipherment problems using unsupervised learning. These include letter substitution ciphers, <b>character</b> code <b>conversion,</b> phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most often fail on the first try, so we describe techniques for understanding errors and significantly increasing performance. ...|$|R
40|$|Recent {{commentary}} {{has described}} founders 2 ̆ 7 stock as tax-advantaged because it converts founders 2 ̆ 7 compensation income into capital gains. In this paper we describe various founders 2 ̆ 7 stock strategies that offer this <b>character</b> <b>conversion</b> and then analyze whether they are, on the whole, tax advantageous. While the founders 2 ̆ 7 stockstrategies favorably convert {{the character of}} the founders 2 ̆ 7 income, they simultaneously turn the company 2 ̆ 7 s compensation deductions into non-deductions. Whetherfounders 2 ̆ 7 stock is tax-advantaged overall depends on whether the benefit of the founders 2 ̆ 7 <b>character</b> <b>conversion</b> outweighs the cost of the company 2 ̆ 7 s lost deductions. We use various hypothetical to illustrate this tradeoff. We conclude that founders 2 ̆ 7 stock is likely to be significantly tax-advantaged only in those cases where the start up company shows great promise early on but ultimately never develops into a profitable enterprise. Even in that subset of cases where founders 2 ̆ 7 stock turns out to be tax-advantaged, the advantage exists only because of the tax law 2 ̆ 7 s overly harsh treatment of net operating losses. Therefore, whatever tax advantage that exists for founders 2 ̆ 7 stock is best viewed as a partial move towards the optimal treatment of tax losses, not as a stand-alone tax benefit that needs to be eliminated...|$|E
40|$|We {{address the}} problem of {{statistical}} language modeling in the context of PinYin to Chinese (PTC) conversion, a similar problem to speech recognition but without acoustic recognition step. Inputted phonetic syllables were first segmented and converted into word lattice, which was then scored within a Source-Channel framework in order to find the most probable Chinese sentence. In particular, we discuss the use of a Whole Sentence Maximum Entropy (WSME) model, an expressive framework for constructing language models with diverse features. Experiment showed WSME model trained with d 2 -ngrams and word triggers achieved a 20 % reduction in perplexity and a 11. 05 % reduction in <b>character</b> <b>conversion</b> error over a baseline trigram...|$|E
40|$|Conference Name: 2011 International Conference on Asian Language Processing, IALP 2011. Conference Address: Penang, Malaysia. Time:November 15, 2011 - November 17, 2011. With {{the growth}} of {{exchange}} activities between four regions of cross strait, the problem to correctly convert between Traditional Chinese (TC) and Simplified Chinese (SC) {{become more and more}} important. Numerous one-to-many mappings and term usage differences {{make it more difficult to}} convert from SC to TC. This paper proposed a novel simplified-traditional Chinese <b>character</b> <b>conversion</b> model based on log-linear models, in which features such as language models and lexical semantic consistency weighs are integrated. When estimating lexical semantic consistency weighs, cross-language word-based semantic spaces were used. Experiments were conducted and the results show that the proposed model achieve better performance. ? 2011 IEEE...|$|E
5000|$|Lín Jìnyì ( [...] / [...] ), editor. 1984. [...] Kanji denpō kōdō henkan hyō <b>character</b> {{telegraph}} code <b>conversion</b> table (In Japanese). Tokyo: KDD Engineering & Consulting.|$|R
50|$|Prior to GBK which {{includes}} {{both traditional and}} simplified <b>characters,</b> <b>conversion</b> between Chinese and Taiwanese charsets was complicated by the need of transcribing text between the two variants of Chinese, as one charset cover many of the other's characters only in its own variant. The conversion between traditional and simplified Chinese is usually problematic, because the simplification of some traditional forms merged two or more different characters into one simplified form. The traditional to simplified (many-to-one) conversion is technically simple. The opposite conversion often results in a data loss when converting to GB 2312: in mapping one-to-many when assigning traditional glyphs to the simplified glyphs, some characters will inevitably be the wrong choices {{in some of the}} usages. Thus simplified to traditional conversion often requires usage context or common phrase lists to resolve conflicts. This issue is less of a problem with newer standards such as GBK, GB18030 and Unicode which have separate code points for both simplified and traditional characters.|$|R
40|$|Objetivo: conocer los resultados de la generalización de la colecistectomía laparoscópica en Cuba. Métodos: se realizó un estudio retrospectivo y descriptivo a través de una encuesta completada por 16 grupos de cirugía de mínimo acceso de hospitales universitarios en 9 provincias del país para definir carácter, conversiones, conducta ante litiasis de la vía biliar, morbilidad y mortalidad perioperatoria. Resultados: se obtuvieron datos relacionados con 56 878 intervenciones realizadas, desde los inicios de la actividad en esos servicios, hasta noviembre de 2007, y se definió carácter, conversiones, conducta ante litiasis de la vía biliar, morbilidad y mortalidad perioperatoria. El acceso laparoscópico se usó en 80, 7 % de los casos, electivo en 97, 1 %, con índices de conversión de 1, 4 %, morbilidad de 0, 58 % y mortalidad de 0, 10 %. Predominó, ante el hallazgo de litiasis coledociana, el convertir y explorar de forma convencional. Conclusiones: los resultados de la generalización de la colecistectomía laparoscópica en Cuba son excelentes, pero es necesario {{extender}} su uso como arsenal en el tratamiento de las complicaciones de la litiasis biliar. Objective: to {{know the}} results from the standardization of the laparoscopic cholecystectomy in Cuba. Methods: a descriptive and retrospective study was conducted be means of a survey completed by 16 groups of minimal access surgery from university hospital in 9 provinces of our country to define the <b>character,</b> <b>conversions,</b> behavior in face of biliary tract lithiasis, perioperative morbidity and mortality. Results: it was possible to obtain data related to 56 878 surgical interventions performed from the onset of activity in these centers up to November, 2007 defining the <b>character,</b> <b>conversions,</b> behavior in face of a biliary tract lithiasis and perioperative morbidity and mortality. The laparoscopic approach was used in the 80. 7 % of cases, elective in the 97. 1 %, with conversion rates of 1. 4 %, morbidity of 0. 58 % and mortality of 0. 10 %. There was predominance of conversion and exploration of conventional for in face of the choledochal lithiasis finding. Conclusions: results of standardization of laparoscopic cholecystectomy in Cuba are excellent, but it is necessary to extend its use as a tool in the treatment of the biliary lithiasis complications...|$|R
40|$|One of {{the popular}} input systems is based on Chinese phonetic symbols. Designing such kind of a syllable-to-character (STC) input system {{involves}} two major issues, namely, fault tolerance handling and homonym resolution. In this paper, the fault tolerance mechanism is constructed {{on the basis of}} a user-defined confusing set and a modified bucket indexing scheme is incorporated so as to satisfy real-time requirement. Meanwhile the homonym resolution is handled by binding force and heuristic selection rules. Both the system performance and tolerance ability are justified with real corpus in terms of searching speed and <b>character</b> <b>conversion</b> accuracy rate. Experimental results show that the proposed scheme can achieve 93. 54 % accuracy for zero-error syllable inputs and 80. 13 % for zero-tone syllable inputs. Furthermore both robustness and tolerance of the proposed system are proved for high input error rates. ...|$|E
40|$|Parsing is an {{expensive}} operation that can degrade XML processing performance. A survey of four representative XML parsing models—DOM, SAX, StAX, and VTD—reveals their suitability for different types of applications. B roadly used in database and networking applications, the Extensible Markup Language is the de facto standard for the interoperable document format. As XML becomes widespread, it is critical for application developers to understand the opera-tional and performance characteristics of XML processing. As Figure 1 shows, XML processing occurs in four stages: parsing, access, modification, and serialization. Although parsing {{is the most expensive}} operation, 1 there are no detailed studies that compare the processing steps and associated overhead costs of different parsing models, tradeoffs in accessing and modifying parsed data, and XML-based applications ’ access and modification requirements. Figure 1 also illustrates the three-step parsing process. The first two steps, <b>character</b> <b>conversion</b> and lexical analysis, are usually invariant among dif-ferent parsing models, while the third step, syntactic analysis, creates data representations based on the parsing model used. To help developers make sensible choices for their target applications, we compared the data representations of four representative parsing models: document object model (DOM; www. w 3. org/DOM), simple API for XML (SAX; www. saxproject. org), streaming API for XML (StAX...|$|E
40|$|Collocations, the {{combination}} of specific words are quite useful linguistic resources for NLP in general. The {{purpose of this paper}} is to show their usefulness, exemplifying an application to Kanji character decision processes for Japanese word processors. Unlike recent trials of automatic extraction, our collocations were collected manually through many years of intensive investigation of corpus. Our collection procedure consists of (1) finding a proper combination of words in a corpus and (2) recollecting similar combinations of words, incited by it. This procedure, which depends on human judgment and the enrichment of data by association, is effective for remedying the sparseness of data problem, although the arbitrariness of human judgment is inevitable. Approximately seventy two thousand and four hundred collocations were used as word co-occurrence restriction data for deciding Kanji characters in the processing of Japanese word processores. Experiments have shown that the collocation data yield 8. 9 % higher fraction of Kana-to-Kanji <b>character</b> <b>conversion</b> accuracy than the system which uses no collocation data and 7. 0 % higher, than a commercial word processor software of average performance. The meanings of words in a natural sentence are mutually bound. The most crucial problem for any kind of NLP is to disambiguate the meanings of each word in th...|$|E
50|$|In February 2005, {{the project}} servers moved to CVSNT version 2.5 and began {{unstable}} releases {{of a new}} server incorporating server plugins, ACLs, filename <b>character</b> set <b>conversion</b> as well as client/server autodiscovery. In October 2005, the first commercial CVS Suite was released, incorporating non-GPL addins and clients for CVSNT. In November 2005, CVSNT was enhanced to incorporate {{the features of the}} CVSNT SJIS project.|$|R
2500|$|In Strafford v. Pell Clayt. 151, pl. 276, 1650, {{a similar}} action in trover as in Holdsworth's Case failed against a carrier of {{chattels}} {{for this was}} a declaration of a trover, and [...] "supposeth a losing of goods, where the carrier hath them by delivery." [...] Neither of these actions had anything to differentiate them from the old action of detinue, because both {{were based on a}} nonfeasance, before the <b>character</b> of <b>conversion</b> had been adequately realized.|$|R
50|$|Anything - if {{you like}} the system but you have an old {{favorite}} <b>character...</b> convert it. <b>Conversions</b> are encouraged but due to copyright issues Kevin doesn't want any conversions that have a basis copyrighted by someone else posted on the Palladium Forums.|$|R
40|$|The {{objectives}} {{of the present study}} are: (1) To synthesize iron catalysts: Fe/MoO{sub 3 }, and Fe/Co/MoO{sub 3 } employing two distinct techniques: Pyrolysis with organic precursors and Co-precipitation of metal nitrates; (2) To investigate the magnetic character of the catalysts before and after exposure to CO and CO+H{sub 2 } by (a) Mossbauer study of Iron (b) Zerofield Nuclear Magnetic Resonance study of Cobalt, and (c) Magnetic character of the catalyst composite; (3) To study the IR active surface species of the catalyst while stimulating (CO [...] Metal, (CO+H{sub 2 }) [...] Metal) interactions, by FTIR Spectroscopy; and (4) To analyze the catalytic <b>character</b> (<b>conversion</b> efficiency and product distribution) in both direct and indirect liquefaction Process and (5) To examine the correlations between the magnetic and chemical characteristics. This report presents the results of our investigation on (a) the effect of metal loading (b) the effect of intermetallic ratio and (c) the effect of catalyst preparation procedure on (i) the magnetic character of the catalyst composite (ii) the IR active surface species of the catalyst and (iii) the catalytic yields for three different metal loadings: 5 %, 15 %, and 25 % (nominal) for three distinct intermetallic ratios (Fe/Co = 0. 3, 1. 5, 3. 0) ...|$|E
40|$|Columbia University Libraries {{is working}} on a {{large-scale}} project, funded by the Ford Foundation grant, to permanently preserve and make accessible the archives of the International Fellowships Program. Active in 2001 - 2013, the IFP offered fellowships for post-graduate study to social justice leaders from underserved communities in Asia, Africa, Latin America, Russia, and the Middle East. The IFP records included a substantial digital component: 3. 6 TB from 22 countries, in 245 file formats, 10 languages and 7 non-Roman character sets. This presentation focuses on metadata and ingest issues we faced when processing this major born-digital acquisition, and on procedural and technological solutions we adopted. The only descriptive metadata on the file level was contained in file names and directory paths, so these were retained as an originalName metadata element in AIP METS file. Files from each office were sorted into three groups by desired access level (online, reading room, and embargoed until 2075). Archivematica software was used to create the Submission Information Packages (SIPs) and subsequently transform them into Archival Information Packages (AIPs). One or more SIPs were created for each access group, depending on the directory size. We developed a formula to calculate if a group of files was small enough to fit in one SIP. Audiovisual materials, databases, emails, and compressed files were addressed separately. Processing included <b>character</b> <b>conversion</b> and format normalization. Access restrictions and SIP-specific descriptive metadata for each package were entered manually. AIPs were transferred to preservation storage in BagIt format...|$|E
40|$|This paper {{presents}} an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction {{refers to the}} spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless {{they are in the}} list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学 生 'student ' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based <b>character</b> <b>conversion</b> using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20 % (from 8. 9 % to 7. 12 %) over the previous state-of-the art based on a noisy channel model. ...|$|E
5000|$|When the kernel {{receives}} control, {{it gets a}} pointer to {{a structure}} as parameter. This structure is passed by the bootloader and contains information about the hardware, {{the path to the}} registry file, kernel parameters containing boot preferences or options that change the behavior of the kernel, path of the files loaded by the bootloader ( [...] Registry hive, nls for <b>character</b> encoding <b>conversion</b> and vga font). The definition of this structure can be retrieved by using the kernel debugger or downloading it from the Microsoft symbol database.|$|R
50|$|Most {{literature}} of the 1930s, 40s, and early 50s presented lesbian life as tragedy, ending with either the suicide of the lesbian <b>character</b> or her <b>conversion</b> to heterosexuality. This was required so that the authorities did not declare the literature obscene. Furthermore, the Hays Code, which was in operation from 1930 until 1967, prohibited the depiction of homosexuality in all Hollywood films.|$|R
50|$|Originally {{the game}} was {{intended}} for release as {{an expansion of the}} Spycraft 2.0 ruleset, following up the first two Origin of the Species electronic products that added character creation options for a variety of standard fantasy and classical Greek non-humans to the ruleset. During the development process, the decision was made by Crafty Games to instead take the opportunity to significantly streamline the game mechanics in response to user feedback and create the ruleset referred to as Mastercraft. This permitted the release of Fantasy Craft as a standalone product under the Open Gaming Licence, including full rules for <b>character</b> generation, <b>conversion</b> mechanics for monsters found in the System Resource Document, and the tools for creating, populating and presenting user-generated campaign settings to players.|$|R
