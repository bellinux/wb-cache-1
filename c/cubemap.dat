13|4|Public
50|$|Direct3D 10.1 sets a {{few more}} image quality {{standards}} for graphics vendors, and gives developers more control over image quality. Features include finer control over anti-aliasing (both multisampling and supersampling with per sample shading and application control over sample position) and more flexibilities {{to some of the}} existing features (<b>cubemap</b> arrays and independent blending modes). Direct3D 10.1 level hardware must support the following features: Multisampling has been enhanced to generalize coverage based transparency and make multisampling work more effectively with multi-pass rendering, better culling behavior - Zero-area faces are automatically culled; this affects wireframe rendering only, independent blend modes per render target, new sample-frequency pixel shader execution with primitive rasterization, increased pipeline stage bandwidth, both colour and depth/stencil MSAA surfaces can now be used with CopyResource as either a source or destination, MultisampleEnable only affects line rasterization (points and triangles are unaffected), and is used to choose a line drawing algorithm. This means that some multisample rasterization from Direct3D 10 are no longer supported, Texture Sampling - sample_c and sample_c_lz instructions are defined to work with both Texture2DArrays and TextureCubeArrays use the Location member (the alpha component) to specify an array index, support for TextureCubeArrays.|$|E
40|$|Abstract — This paper {{proposes a}} novel six-face spherical map, isocube, that fully {{utilizes}} the <b>cubemap</b> hardware built in most GPUs. Unlike the <b>cubemap,</b> the proposed isocube uniformly samples the unit sphere (uniformly distributed) and all samples span the same solid angle (equally important). Its mapping computation contains {{only a small}} overhead. By feeding the <b>cubemap</b> hardware with the six-face isocube map, the isocube can exploit all built-in texturing operators tailored for the <b>cubemap</b> and achieve a very high frame rate. In addition, we develop an anisotropic filtering that compensates aliasing artifacts due to texture magnification. This filtering technique extends the existing hardware anisotropic filtering and can be applied to, not just the proposed isocube, but also other texture mapping applications...|$|E
40|$|We {{present a}} new {{efficient}} method for <b>cubemap</b> filtering which alleviates the edge seam artifacts that occur with standard <b>cubemap</b> filtering methods. This approach works with current cubemapping hardware since {{it does not}} require rarely supported texture border hardware used in other approaches [1]. Despite the fact that cube maps are defined on the spherical domain, standard <b>cubemap</b> filtering techniques perform filtering independently on each cube face. The main problem with this approach is that no information is propagated across edges, thus creating undesirable discontinuities along the cube face edges. A limitation of nearly all cubemapping hardware which makes the seam problem substantially worse {{is the fact that the}} bilinear texel filtering is not able to fetch across cube faces thus producing a hard seam artifact. The seam problem also causes aliasing artifacts. These two compounding problems limit the usefulness o...|$|E
5000|$|Texturing: 1D/2D/3D textures, <b>cubemaps,</b> multi-texturing, texture arrays, texture rectangles, {{render to}} texture, texture {{coordinate}} generation, texture combiners, mipmapping and mipmaps generation, anisotropic filtering, compressed textures, depth textures, integer textures, non normalized textures, texture buffers, multisample textures.|$|R
40|$|We {{present a}} new method to compute {{interactive}} reflections on curved objects. The approach creates virtual reflected objects which are blended into the scene. We use a {{property of the}} reflection geometry which allows us to efficiently and accurately find the point of reflection for every reflected vertex, using only reflector geometry and normal information. This reflector information is stored {{in a pair of}} appropriate <b>cubemaps,</b> thus making it available during rendering. The implementation presented achieves interactive rates on reasonablysized scenes. In addition,we introduce an interpolation method to control the accuracy of our solution depending on the required frame rate. ...|$|R
40|$|We {{present an}} {{automatic}} software based approach for building extremely high resolution panoramic mosaics from images captured by an off-the-shelf pan tilt zoom camera. Using numerous zoomed-in images {{from such a}} camera, existing mosaicing algorithms could theoritically build gigapixel images. However {{the large number of}} images that must be processed makes such approaches impractical. Our stratified approach for high resolution panoramic imagery, first constructs a coarse panorama of the scene and then adds in detail from a zooming camera, in a top-down multiresolution fashion. Our approach uses both feature based and direct intensity based image alignment methods. Both the geometric calibration (intrinsic parameters and radial distortion) as well as the photometric calibration and alignment is done automatically. Our fully calibrated panoramas are represented as multi-resolution pyramids of <b>cubemaps.</b> We align hundreds of images captured within a 1 - 12 X zoom range and show results from two datasets captured from cameras placed in an uncontrolled outdoor scene. 1...|$|R
40|$|Figure 1 : Our scale-sensitive {{approach}} allows navigation between {{scales in}} 3 D scenes. Here, the user navigates {{from thousands of}} kilometres above Earth’s surface to come to rest inside a jug on a table only centimetres in diameter. We present a comprehensive system for multiscale navigation of 3 -dimensional scenes, and demonstrate our approach on multiscale datasets such as the Earth. Our system incorporates a novel imagebased environment representation which we {{refer to as the}} <b>cubemap.</b> Our <b>cubemap</b> allows consistent navigation at various scales, as well as real-time collision detection without pre-computation or prior knowledge of geometric structure. The <b>cubemap</b> is used to improve upon previous work on proximal object inspection (HoverCam), and we present an additional interaction technique for navigation which we call look-and-fly. We believe that our approach to the navigation of multiscale 3 D environments offers greater flexibility and ease of use than mainstream applications such as Google Earth and Microsoft Virtual Earth, and we demonstrate our results with this system...|$|E
40|$|One of {{the main}} goals of {{anywhere}} augmentation {{is the development of}} automatic algorithms for scene acquisition in augmented reality systems. In this paper, we present Envisor, a system for online construction of environment maps in new locations. To accomplish this, Envisor uses vision-based frame to frame and landmark orientation tracking for long-term, drift-free registration. For additional robustness, a gyroscope / compass orientation unit can optionally be used for hybrid tracking. The tracked video is then projected into a <b>cubemap</b> frame by frame. Feedback is presented to the user to help avoid gaps in the <b>cubemap,</b> while any remaining gaps are filled by texture diffusion. The resulting environment map can be used for a variety of applications, including shading of virtual geometry and remote presence...|$|E
40|$|In this dissertation, {{we first}} {{introduce}} an equal-area spherical map, HEALPix, which is borrowed from astrophysics. Its associated sample pattern is uniformly distributed over the sphere surface. Then we discuss its application in environment mapping. Although the HEALPix representation is more balanced {{than the traditional}} <b>cubemap,</b> it cannot utilize the built-in hardware operations like <b>cubemap,</b> and the mipmapping construction is more complicated. Therefore, its rendering speed is not {{comparable to that of}} the <b>cubemap.</b> This motivates us to invent a new six-face spherical map, called isocube. Unlike <b>cubemap,</b> isocube is an equal-area mapping, i. e., each texel spans the same area and is equally important. Due to the six-face representation, isocube can fit nicely into the <b>cubemap</b> hardware and hence can fully exploit the hardware operations tailored for <b>cubemap.</b> In addition, its mapping computation only involves a small overhead. Therefore the look-up speed for isocube is very fast. Next we discuss another application of HEALPix map, the dynamic environment sequence sampling. Previous work only considers the static case where the environment map is approximated by finite directional light sources. In the dynamic case, the individual regeneration of samples for each frame may introduce abrupt, changes in the rendering animation. To handle this temporal inconsistency problem, we propose spherical q 2 -tree based on IIEALPix map. The adaptiveness of the quadtree can suppress the abrupt changes between consecutive frames, and hence a rather smooth rendering can be produced. This method, however, generates sample pattern independently for each environment frame, and therefore it may still cause unexpected, lumps in some situations. To fully utilize the temporal coherence in the sequence, we present a global sampling approach which treats the dynamic environment sequence as a all volume and performs sampling in the volume. The volumetric sampling adapts the sample number according to each frame and restricts the temporal/spatial changes within small subdivided volumes, and hence ensures a smooth sampling sequence. Within the framework, we present a volumetric importance metric and develop a binary-quad tree to perform stratification. Experimental results show that our volumetric sampling method can generate light samples with a better temporal consistency. Spherical map is the foundation for many applications in computer graphics, such as environment mapping, precomputed radiance transfer, radiosity and image-based rendering. For these applications that involve intensive computation on spherical domain, it is generally desirable to employ equal-area and uniform spherical maps as the underlying parameterization. However, equal-area spherical maps are less exploited in graphics. Wan, Liang. "August 2007. "Adviser: Tien-Tsin Wong. Source: Dissertation Abstracts International, Volume: 69 - 02, Section: B, page: 1121. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2007. Includes bibliographical references (p. 118 - 123). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstract in English and Chinese. School code: 1307...|$|E
40|$|Recently, several {{algorithms}} {{have been}} introduced that enable realtime performance for many lights in applications such as games. In this paper, we explore the use of hardware-supported virtual <b>cubemap</b> shadows to efficiently implement high-quality shadows from hundreds of light sources in real time and within a bounded memory footprint. In addition, we explore the utility of ray tracing for shadows from many lights and present a hybrid algorithm combining ray tracing with cube maps to exploit their respective strengths. Our solution supports real-time performance with hundreds of lights in fully dynamic high-detail scenes. Copyrigh...|$|E
40|$|Figure 1 : A {{cylindrical}} {{projection of}} an environment map constructed using Envisor {{with a camera}} on a tripod. One important component of modeling new scenes is the acqui-One of the main goals of anywhere augmentation is the develop- sition of an environment map. As an image-based representation ment of automatic algorithms for scene acquisition in augmented of the light distribution around a single position, environment maps reality systems. In this paper, we present Envisor, a system for have many uses in AR systems. Most commonly, {{they can be used}} online construction of environment maps in new locations. To ac- for realistic shading of virtual geometry [1, 8, 12] for more seamcomplish this, Envisor uses vision-based frame to frame and land- less integration of virtual objects into the physical scene. They are mark orientation tracking for long-term, drift-free registration. For also useful for remote presence applications [25], as a simple way additional robustness, a gyroscope / compass orientation unit can of representing a remote environment, e. g. as a backdrop in a teleoptionally be used for hybrid tracking. The tracked video is then collaboration system, or in low-bandwidth first-person interfaces projected into a <b>cubemap</b> frame by frame. Feedback is presented like QuickTime VR models [18]. to the user to help avoid gaps in the <b>cubemap,</b> while any remain-In this paper, we present Envisor, a system for the automatic, oning gaps are filled by texture diffusion. The resulting environment line construction of environment maps using a hand-held or headma...|$|E
40|$|This work {{deals with}} the problem of {{accurately}} rendering mirror reflections on curved surfaces in real-time. While planar mirrors do not pose a problem in this area, non-planar surfaces are nowadays rendered mostly using environment mapping, which is a method of approximating the reflections well enough for the human eye. However, this approach may not be suitable for applications such as CAD systems. Accurate mirror reflections can be rendered with ray tracing methods, but not in real-time and therefore without offering interactivity. This work examines existing approaches to the problem and proposes a new algorithm for computing accurate mirror reflections in real-time using accelerated searching for intersections with depth profile stored in <b>cubemap</b> textures. This algorithm has been implemented using OpenGL and tested on different platforms...|$|E
40|$|Standard video {{encoders}} {{developed for}} conventional narrow field-of-view video are widely applied to 360 video as well, with reasonable results. However, while this approach commits arbitrarily to {{a projection of}} the spherical frames, we observe that some orientations of a 360 video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip's visual content and its compressibility at different rotations of a <b>cubemap</b> projection. Given a novel video, our learning-based approach efficiently infers the most compressible direction in one shot, without repeated rendering and compression of the source video. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360 compression has substantial potential [...] "good" rotations are typically 8 - 10...|$|E
40|$|Figure 1 : Complex {{lighting}} effects like soft shadows require transport matrices {{that have}} a very high rank or dimensionality. However, within local blocks, the dimensionality is much lower. This paper analyzes these effects theoretically. One practical application is to all-frequency relighting of high-resolution images; (a) shows a 1024 × 1024 image, lit by a 6 × 32 × 32 <b>cubemap,</b> rendered interactively. We achieve this result by using our framework to adaptively subdivide the image into local patches. The adaptive subdivision, and zoom-ups to show the full resolution are in (b). Our main contribution is a theoretical analysis of how light transport dimensionality varies with patch size. For complex shadows like these, dimensionality is essentially linear in patch area (c). Blockwise or Clustered Principal Component Analysis (CPCA) is commonly used to achieve real-time rendering of shadows and glossy reflections with precomputed radiance transfer (PRT). The vertices or pixels are partitioned into smaller coherent regions, and light transport in each region is approximated by a locally lowdimensional subspace using PCA. Many earlier techniques such as surface light field and reflectance field compression use a similar paradigm. However, there has been no clear theoretical understanding of how light transport dimensionality increases with local patc...|$|E
40|$|Abstract — In {{networked}} virtual environments, videoconferences or chatting {{over the}} Internet users are often graphically represented by virtual characters. Modeling realistic virtual heads of users suitable for animation implies a heavy artistic effort and resource cost. This paper introduces {{a system that}} generates a 3 D model of a real human head with a little human intervention. The system receives five input orthogonal photographs of the human head and a generic template 3 D model. It requires manual annotation of 94 feature points on each photograph. The same set of feature points must be selected on the template model in a preprocessing step that is done only once. Computing process consists of two phases: a morphing and a coloring phase. In the morphing phase the template model is morphed in two steps using a Radial Basis Function (RBF) to take a shape similar {{to the shape of}} the real human head. In the coloring phase the deformed model is colored using the input photographs based on a <b>cubemap</b> projection, which leads to a realistic appearance of the model while allowing for a real-time performance. We show the use of the output model by automatically copying facial motions from the template model to the deformed model, while preserving the compliance of the motion to the MPEG- 4 FBA standard. I...|$|E
40|$|Fig. 1. Images {{produced}} by ray tracing two scenes while using the proposed ray-caching textures for real-time rendering (a) -(c), and without our caching mechanism (d). The {{quality of the}} resulting images increases as {{the resolution of the}} proposed caching structures increases. Notice that caching textures having 512 × 512 texels (c) generated images equivalent to the standard ray tracing (d). As can be seen in Table I for the images on the bottom, our approach has the advantage of producing results like (a) -(c) at 42 fps, while (d) is rendered at 12 fps. Abstract—Ray tracing allows the rendering of scenes with very complex light interactions. It is {{based on the idea that}} reflection, refraction and shadows can be modeled by recursively following the path that light takes as it bounces through an environment. However, despite its conceptual simplicity, tracing rays is a computationally intensive task. Also, optimizing memory management to increase efficiency is hard since coherent access in 3 D space would not generate coalescent memory patterns. We present a new caching-like strategy suitable for real-time ray tracing which is capable to store data generated in previous frames in such a way that coherent memory access is achieved while data is reused by subsequent frames. By storing light bounce results of previously traced rays in a <b>cubemap</b> attached to each scene object, we show that it is possible to explore the efficient memory sampling mechanism provided by the graphics hardware to increase frame rate. Our approach is suitable for static scenes and may prevent deep interactions of rays with the scene as well as enable synchronous computation of rays in parallelized architectures, and it can be easily integrated to any existing ray tracing solutions. Keywords-ray tracing; cache memory; cubemap; real time. I...|$|E

