10000|10000|Public
5|$|A Museum of <b>Computing</b> {{was opened}} in 2001 in Žirmūnai by the Lithuanian {{software}} company Sintagma, showcasing {{the history of}} Lithuanian <b>computing</b> science and hardware production. It {{was based on a}} museum opened in 1985 by Sigma, one of the leading computer manufacturers in the former Soviet Union. The museum's exhibits include EV-80, the first Soviet vacuum tube <b>computing</b> machine manufactured by Sigma, and a copy of the IBM 604.|$|E
5|$|Grid <b>computing</b> is {{the most}} {{distributed}} form of parallel <b>computing.</b> It makes use of computers communicating over the Internet {{to work on a}} given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed <b>computing</b> typically deals only with embarrassingly parallel problems. Many distributed <b>computing</b> applications have been created, of which SETI@home and Folding@home are the best-known examples.|$|E
5|$|In {{computer}} science research, Plan 9 {{has been used}} as a grid <b>computing</b> platform and as a vehicle for research into ubiquitous <b>computing</b> without middleware.|$|E
5000|$|An {{arithmetic}} circuit <b>computes</b> a polynomial in {{the following}} natural way. An input gate <b>computes</b> the polynomial it is labeled by. A sum gate [...] <b>computes</b> {{the sum of the}} polynomials <b>computed</b> by its children (a gate [...] is a child of [...] if the directed edge [...] is in the graph). A product gate <b>computes</b> the product of the polynomials <b>computed</b> by its children. Consider the circuit in the figure, for example: the input gates <b>compute</b> (from left to right) [...] and [...] the sum gates <b>compute</b> [...] and [...] and the product gate <b>computes</b> ...|$|R
5000|$|... 3 <b>Compute</b> Nodes: Dell R175 {{each with}} 32 CPU cores/ <b>compute</b> node (96 in total), 128GB RAM/ <b>compute</b> node (384GB in total), 600GB Secondary Memory/ <b>compute</b> node (1.8TB in total) ...|$|R
5000|$|A Q32 <b>compute</b> drawer {{contains}} 32 <b>compute</b> cards, each water cooled.A [...] "midplane" [...] (crate) contains 16 Q32 <b>compute</b> drawers for a {{total of}} 512 <b>compute</b> nodes, electrically interconnected in a 5D torus configuration (4x4x4x4x2). Beyond the midplane level, all connections are optical. Racks have two midplanes, thus 32 <b>compute</b> drawers, {{for a total}} of 1024 <b>compute</b> nodes, 16,384 user cores and 16 TB RAM.|$|R
5|$|The Jean Jennings Bartik <b>Computing</b> Museum at Northwest Missouri State University in Maryville, Missouri is {{dedicated}} to the history of <b>computing</b> and Bartik's career.|$|E
5|$|Bob Muglia is {{currently}} the Chief Executive Officer of Snowflake <b>Computing,</b> a cloud-based data-warehousing startup. He joined the company in June 2014, a couple {{years after it was}} founded in 2012. Snowflake <b>Computing</b> came out of stealth mode that October.|$|E
5|$|Fellow of the Association for <b>Computing</b> Machinery, 1994.|$|E
5000|$|Measurements - <b>Compute</b> {{some value}} of the meshVolume - <b>compute</b> the 3D volume of a mesh (discrete {{volumetric}} integral)Surface Area - <b>compute</b> the surface area of a mesh (discrete surface integral)Collision Detection - determine if two complex meshes in motion have collidedFitting - construct a parametric surface (NURBS, bicubic spline) by fitting it to a given meshPoint-Surface Distance - <b>compute</b> distance from a point to the meshLine-Surface Distance - <b>compute</b> distance from a line to the meshLine-Surface Intersection - <b>compute</b> intersection of line and the meshCross Section - <b>compute</b> the curves created by a cross-section of a plane through a meshCentroid - <b>compute</b> the centroid, geometric center, of the meshCenter-of-Mass - <b>compute</b> the center of mass, balance point, of the meshCircumcenter - <b>compute</b> {{the center of a}} circle or sphere enclosing an element of the meshIncenter - <b>compute</b> the center of a circle or sphere enclosed by an element of the mesh ...|$|R
3000|$|... nonoverlapping {{blocks and}} <b>compute</b> the DCT on each block. Then <b>compute</b> the HVS model {{properties}} as said in Section 2.1 and <b>compute</b> [...]...|$|R
40|$|Objective Comparative {{evaluation}} of ground-glass opacity using conventional high-resolution <b>computed</b> tomography technique and volumetric <b>computed</b> tomography by 64 -row multi-slice scanner, verifying advantage of volumetric acquisition and post-processing technique allowed by 64 -row CT scanner. Methods Thirty-four patients, in which was assessed ground-glass opacity pattern by previous high-resolution <b>computed</b> tomography during a clinical-radiological follow-up for their lung disease, were studied {{by means of}} 64 -row multi-slice <b>computed</b> tomography. Comparative {{evaluation of}} image quality was done by both CT modalities. Results It was reported good inter-observer agreement (k value 0. 78 – 0. 90) in detection of ground-glass opacity with high-resolution <b>computed</b> tomography technique and volumetric <b>Computed</b> Tomography acquisition with moderate increasing of intra-observer agreement (k value 0. 46) using volumetric <b>computed</b> tomography than high-resolution <b>computed</b> tomography. Conclusions In our experience, volumetric <b>computed</b> tomography with 64 -row scanner shows good accuracy in detection of ground-glass opacity, providing a better spatial and temporal resolution and advanced post-processing technique than high-resolution <b>computed</b> tomography...|$|R
5|$|Parallel <b>computing</b> {{is closely}} related to {{concurrent}} computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel <b>computing,</b> a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent <b>computing,</b> the various processes often do not address related tasks; when they do, as is typical in distributed <b>computing,</b> the separate tasks may have a varied nature and often require some inter-process communication during execution.|$|E
5|$|Sechin, A.; Parallel <b>Computing</b> in Photogrammetry. GIM International. #1, 2016, pp.21–23.|$|E
5|$|The fast Fourier {{transform}} is an algorithm for rapidly <b>computing</b> the discrete Fourier transform. It is used {{not only}} for calculating the Fourier coefficients but, using the convolution theorem, also for <b>computing</b> the convolution of two finite sequences. They in turn are applied in digital filters and as a rapid multiplication algorithm for polynomials and large integers (Schönhage-Strassen algorithm).|$|E
40|$|Maximal label search {{algorithms}} to <b>compute</b> {{perfect and}} minimal elimination orderings. (English summary) SIAM J. Discrete Math. 23 (2008 / 09), no. 1, 428 – 446. Summary: “Many graph search algorithms use a vertex labeling to <b>compute</b> an ordering of the vertices. We examine such algorithms which <b>compute</b> a peo (perfect elimination ordering) of a chordal graph and corresponding algorithms which <b>compute</b> an meo (minimal elimination ordering) of a non-chordal graph, an ordering used to <b>compute</b> a minimal triangulation of the input graph. We express all known peo-computing search algorithms as instances of a generic algorithm called MLS (maximal label search) and generalize Algorithm MLS into CompMLS, which can <b>compute</b> any peo. We then extend these algorithms to versions which <b>compute</b> an meo and likewise generalize all known meo-computing search algorithms. We show {{that not all}} minimal triangulations can be <b>computed</b> by such a graph search, and, more surprisingly, that all these search algorithms <b>compute</b> {{the same set of}} minimal triangulations, even though the <b>computed</b> meos ar...|$|R
3000|$|... [...]. Therefore, to <b>compute</b> d(r), it is {{sufficient}} to <b>compute</b> dout(r). In the following, we <b>compute</b> dout(r) for the COSTBC with a 2 -hop network.|$|R
5000|$|<b>Computed</b> {{tomography}} scan, including high-resolution <b>computed</b> tomography ...|$|R
5|$|The {{project has}} pioneered {{the use of}} {{graphics}} processing units (GPUs), PlayStation3s, Message Passing Interface (used for <b>computing</b> on multi-core processors), and some Sony Xperia smartphones for distributed <b>computing</b> and scientific research. The project uses statistical simulation methodology that is a paradigm shift from traditional <b>computing</b> methods. As part of the client–server model network architecture, the volunteered machines each receive pieces of a simulation (work units), complete them, and return them to the project's database servers, where the units are compiled into an overall simulation. Volunteers can track their contributions on the Folding@home website, which makes volunteers' participation competitive and encourages long-term involvement.|$|E
5|$|The two {{projects}} {{also differ}} significantly in their <b>computing</b> power and host diversity. Averaging about 6,650 teraFLOPS from a host base of {{central processing units}} (CPUs), graphics processing units (GPUs), and PS3s, Folding@home has nearly 108 times more <b>computing</b> power than Rosetta@home.|$|E
5|$|Turing Award, Association for <b>Computing</b> Machinery, 1968.|$|E
3000|$|... will <b>compute</b> an encryption, <b>compute</b> its partial {{decryption}} {{and finally}} will {{send to the}} verifier. This will assure that the user indeed used its private decryption key over the encrypted value. Next, the verifier will also <b>compute</b> its partial decryption and will <b>compute</b> the decrypted value privately.|$|R
40|$|International audienceMany graph search {{algorithms}} use a vertex labeling to <b>compute</b> an {{ordering of}} the vertices. We examine such algorithms which <b>compute</b> a peo (perfect elimination ordering) of a chordal graph, and corresponding algorithms which <b>compute</b> an meo (minimal elimination ordering) of a non-chordal graph, an ordering used to <b>compute</b> a minimal triangulation of the input graph. We express all known peo-computing search algorithms as instances of a generic algorithm called MLS (Maximal Label Search) and generalize Algorithm MLS into CompMLS, which can <b>compute</b> any peo. We then extend these algorithms to versions which <b>compute</b> an meo, and likewise generalize all known meo-computing search algorithms. We show {{that not all}} minimal triangulations can be <b>computed</b> by such a graph search, and, more surprisingly, that all these search algorithms <b>compute</b> {{the same set of}} minimal triangulations, even though the <b>computed</b> meos are different. Finally, we present a complexity analysis of these algorithms...|$|R
30|$|In summary, HW_v 1 is {{designed}} with separate modules, including GFFm, ΦA, E, F_sub, M, and P, to execute various computations in stage 1. In this case, two GFFm modules <b>compute</b> Eqs. (59) and (60) in parallel, and two ΦA modules <b>compute</b> Eqs. (64) and (65) in parallel. The F_sub module <b>computes</b> Eqs. (55)–(58), the M module <b>computes</b> Eqs. (61)–(63), and finally P module <b>computes</b> Eqs. (66) and (67).|$|R
25|$|In 2007 {{the term}} cloud <b>computing</b> came into popularity, which is conceptually {{similar to the}} {{canonical}} Foster definition of grid <b>computing</b> (in terms of <b>computing</b> resources being consumed as electricity is from the power grid) and earlier utility <b>computing.</b> Indeed, grid <b>computing</b> is often (but not always) associated with the delivery of cloud <b>computing</b> systems as exemplified by the AppLogic system from 3tera.|$|E
25|$|Applications to {{reversible}} <b>computing</b> and quantum <b>computing,</b> including {{limits to}} <b>computing.</b>|$|E
25|$|Green <b>Computing</b> Initiative - GCI {{offers the}} Certified Green <b>Computing</b> User Specialist (CGCUS), Certified Green <b>Computing</b> Architect (CGCA) and Certified Green <b>Computing</b> Professional (CGCP) certifications.|$|E
40|$|This paper {{speeds up}} Brent's {{algorithms}} for various high-precision computations {{in the power}} series ring C[[t]]. If it takes time 3 to <b>compute</b> a product then it takes time roughly 5 : 6 to <b>compute</b> a reciprocal; roughly 8 : 2 to <b>compute</b> a quotient or a logarithm; roughly 6 : 5 to <b>compute</b> a square root; roughly 9 to <b>compute</b> both a square root and a reciprocal square root; and roughly 10 : 4 to <b>compute</b> an exponential. The same ideas apply to approximate computations in R, Q p, etc...|$|R
25|$|In April 1983 <b>Compute!</b> {{published}} Scriptor, a {{word processor}} written by staff writer Charles Brannon in BASIC and assembly language, as a type-in program for the Atari 8-bit family. In January 1984 version 1.0 of his new word processor SpeedScript appeared in <b>Compute!'s</b> Gazette for the Commodore 64 and VIC-20. 1.1 appeared in <b>Compute!'s</b> Second Book of Commodore 64, 2.0 on Gazette Disk in May 1984, and 3.0 in <b>Compute!</b> in March and April 1985. Corrections that updated 3.0 to 3.1 appeared in May 1985, and the full version appeared in a book published by <b>Compute!,</b> SpeedScript: The Word Processor for the Commodore 64 and VIC-20. A 3.2 update appeared in the December 1985 <b>Compute!</b> and January 1986 <b>Compute!</b> Disk and again later in the May 1987 <b>Compute!'s</b> Gazette issue with three additional utilities.|$|R
5000|$|The X6-8 <b>compute</b> server uses eight-socket servers that consume 5 RU {{in height}} and have greater memory {{capacity}} than the X6-2. Whereas each X6-2 <b>compute</b> server contains 44 <b>compute</b> cores, the X6-8 server contains 144 <b>compute</b> cores. This allows large database workloads to easily [...] "scale-up" [...] within a <b>compute</b> server while still supporting Exadata’s [...] "scale-out" [...] expandability across multiple servers. The larger memory {{capacity of the}} X6-8 also favors in-memory database and very large OLTP, consolidation, and DW workloads. Like the X6-2, the Exadata X6-8 base configuration has 2 <b>compute</b> servers and 3 storage servers, but consumes a [...] "Half Rack" [...] of space.Additional <b>compute</b> and storage servers may be added until the rack is full.|$|R
25|$|Scientific <b>computing</b> {{includes}} {{applied mathematics}} (especially numerical analysis), <b>computing</b> science (especially high-performance <b>computing),</b> and mathematical modelling in a scientific discipline.|$|E
25|$|The UNSW <b>COMPUTING</b> ProgComp – Since 1997, The School of Computer Science and Engineering (UNSW <b>COMPUTING)</b> {{has run the}} UNSW <b>COMPUTING</b> ProgComp. This {{competition}} has the overall aim of raising awareness amongst high school students of the craft of programming and {{to encourage students to}} develop and apply their <b>computing</b> knowledge and skills.|$|E
25|$|DNA <b>computing</b> uses DNA, {{biochemistry}} {{and molecular}} biology, {{instead of the}} traditional silicon-based computer technologies. DNA <b>computing,</b> or, more generally, molecular <b>computing,</b> is a fast-developing interdisciplinary area. Research and development in this area concerns theory, experiments and applications of DNA <b>computing.</b> DNA <b>computing</b> is fundamentally similar to parallel <b>computing</b> in that it {{takes advantage of the}} many different molecules of DNA to try many different possibilities at once. Leonard Adleman of the University of Southern California initially pioneered this field in 1994. Adleman demonstrated a proof-of-concept use of DNA as a form of computation which solved the seven-point Hamiltonian path problem.|$|E
50|$|An upgrade from 2 to 64 {{sources for}} <b>compute</b> commands, {{improving}} <b>compute</b> parallelism and execution priority control. This enables finer-grain control over load-balancing of <b>compute</b> commands including superior game-engine integration.|$|R
5000|$|We {{have now}} <b>computed</b> [...] for all [...] {{that are needed}} to <b>compute</b> [...] However, {{this has led to}} {{additional}} suspended recursions involving [...] We proceed and <b>compute</b> these values.|$|R
5000|$|Asynchronous <b>compute</b> queue for {{overlapping}} of <b>compute</b> {{and graphics}} workloads ...|$|R
