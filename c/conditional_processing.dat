11|19|Public
50|$|DITA {{includes}} extensive {{metadata elements}} and attributes, both at topic level and within elements. Conditional text allows filtering or styling content based on attributes for audience, platform, product, and other properties. The <b>conditional</b> <b>processing</b> profile (.ditaval file) {{is used to}} identify which values are {{to be used for}} <b>conditional</b> <b>processing.</b>|$|E
5000|$|... where {k1} {{modifier}} next to {{the destination}} operand encodes the use of opmask register k1 for <b>conditional</b> <b>processing</b> and updates to destination, and {z} modifier (encoded by EVEX.z) provides {{the two types of}} masking (merging and zeroing), with merging as default when no modifier is attached.|$|E
50|$|JSSS {{lacks the}} various CSS {{selector}} features, supporting only simple tag name, class and id selectors. On the other hand, {{since it is}} written using a complete programming language, stylesheets can include highly complex dynamic calculations and <b>conditional</b> <b>processing.</b> (In practice, however, this can be achieved as Dynamic CSS by using JavaScript to modify the stylesheets applicable to the document at runtime.)Because of this JSSS was often used {{in the creation of}} DHTML.|$|E
50|$|The JavaServer Pages Standard Tag Library (JSTL) is a {{component}} of the Java EE Web application development platform. It extends the JSP specification by adding a tag library of JSP tags for common tasks, such as XML data <b>processing,</b> <b>conditional</b> execution, database access, loops and internationalization.|$|R
40|$|Superscalar {{processing}} is {{the latest}} in a long series of innovations aimed at producing ever-faster microprocessors. By exploiting instruction-level parallelism, superscalar processors are capable of executing more than one instruction in a clock cycle. This paper discusses the microarchitecture of superscalar processors. We begin with a discussion of the general problem solved by superscalar processors: converting an ostensibly sequential program into a more parallel one. The principles underlying this process, and the constraints that must be met, are discussed. The paper then provides a description of the specific implementation techniques used in the important phases of superscalar processing. The major phases include: i) instruction fetching and <b>conditional</b> branch <b>processing,</b> ii) the determination of data dependences involving register values, iii) the initiation, or issuing, of instructions for parallel execution, iv) the communication of data values through memory via loads and [...] ...|$|R
40|$|We {{report on}} a {{mechanism}} for semantic and prag-matic interpretation that {{has been designed to}} take advantage of the generally compositional na-ture of semantic analysis, without unduly con-straining the order in which pragmatic decisions are made. To achieve this goal, we introduce the idea of a conditional interpretation: one that de-pends upon a set of assumptions about subsequent pragmatic <b>processing.</b> <b>Conditional</b> interpretations are constructed compositionally according to a set of declaratively specified interpretation rules. The mechanism can handle a wide range of pragmatic phenomena and their interactions. ...|$|R
50|$|The {{main feature}} of BMDFM {{is to provide}} a {{conventional}} programming paradigm at the top level, so-called transparent dataflow semantics. A user understands BMDFM as a virtual machine, which runs all statements of an application program in parallel having all parallelization and synchronization mechanisms fully transparent. The statements of an application program are normal operators, which any single threaded program might consist of - they include variable assignments, <b>conditional</b> <b>processing,</b> loops, function calls, etc.|$|E
40|$|ABSTRACT Description: This paper {{explains}} how to conditionally execute statements in open code {{without having to}} wrap the code in a macro. This is accomplished by combining the macro function sysfunc with the data step function ifc. The result is that parameterized include programs gain the power of <b>conditional</b> <b>processing</b> — %if [...] . %then — for which macros are usually used...|$|E
40|$|SAS ® {{has some}} {{powerful}} tools to handle conditional program flow {{but you may}} not want all that power or the learning curve that goes with it. You may only want to turn on special code for debugging, error handling or in special cases. The RUN statement, with its optional argument CANCEL, can help create <b>conditional</b> <b>processing</b> routines without the need for more involved SAS tools...|$|E
40|$|The use of RFID tag which {{identifies}} a {{thing and}} an object will be expanded with progress of ubiquitous society, {{and it is}} necessary to study how to construct RFID network system as a social infrastructure like the Internet. First, this paper proposes the virtualization method of RFID tag network system to enable the same physical RFID network system to be used by multiple different service systems. The system virtualization not only reduces the system cost but also can dramatically reduce the installation space of physical readers and the operation cost. It is proposed that all equipments in the RFID network system except RFID tag could be shared with the conventional virtual technologies. Then, this paper proposes the <b>conditional</b> tag ID <b>processing</b> and the efficient tag ID transmission method which can greatly reduce the processing time and processing load in RFID tag Infrastructure network The <b>conditional</b> tag ID <b>processing</b> allows that tag ID is valid only at a certain time zone of day or in a certain area. The efficient tag ID transmission method uses the virtual network address of the service center {{as a part of the}} ID of an RF tag, which allows the direct ID forwarding to the service center. Comment: 12 page...|$|R
5000|$|Within {{microprocessors}} {{and other}} logic devices, collections of bit fields called [...] "flags" [...] {{are commonly used}} to control or to indicate the intermediate state or outcome of particular operations. Microprocessors typically have a status register that is composed of such flags, used to indicate various post-operation conditions, for example an arithmetic overflow. The flags can be read and used to decide subsequent operations, such as in <b>processing</b> <b>conditional</b> jump instructions. For example, a je (Jump if Equal) instruction in the x86 assembly language {{will result in a}} jump if the Z (zero) flag was set by some previous operation.|$|R
40|$|Fine-grain {{parallelism}} {{offered by}} VLIW and superscalar processors can be effectively exploited in computational intensive loops. In this paper {{we present a}} new Software Pipelining technique and show how an efficient VLIW code can be automatically generated using a hamiltonian recurrence in the dependence graph. The dependence graph, extended with a scheduling recurrence, describes {{the characteristics of the}} schedule: the number of functional units required and the efficiency achieved (in terms of parallelism) can be known prior to generating the physical scheduling. In this paper we consider single nested loops without <b>conditionals</b> and multifunctional <b>processing</b> units with a unit latency; extensions to multi-cycle and specific processing units are straightforward...|$|R
40|$|Dealing with {{a single}} {{variable}} is all well and good, but {{there comes a time}} when you have to deal with a big list of variables. That’s where array comes in. Array in SAS ® allows you to group a bunch of variables for the same process. The huge block of the repetitious statements and redundant calculation codes can be reduced to just a few lines. With arrays you can simplify the coding in many cases. Sometimes, arrays can even accomplish tasks which are not easily done with other methods. Array, besides its iterative and <b>conditional</b> <b>processing,</b> can be very powerful. This paper introduces some advanced array applications in data manipulations from data search, count consecutive days, LOCF, find and replace, shift, leading to a more complicated efficient process. These array applications will be presented with ten practical examples. The purpose is to explore new applications of arrays over alternative strategies. The paper is appropriate for intermediate SAS programmers with array experience...|$|E
40|$|The {{successful}} {{comprehension of}} a utility conditional (i. e., an ‘if p, then q’ statement where p and/or q is valued {{by one or}} more agents) requires the construction of a mental representation of the situation described by that conditional, and integration of this representation with prior context. In an eye-tracking experiment, we examined the time course of integrating conditional utility information into the broader discourse model. Specifically, the experiment determined whether readers were sensitive, during rapid heuristic processing, to the congruency between the utility of the consequent clause of a conditional (positive or negative) and a reader’s subjective expectations based on prior context. On a number of eye-tracking measures we found that readers were sensitive to conditional utility; conditionals for which the consequent utility mismatched that which would be anticipated on the basis of prior context resulted in processing disruption. Crucially, this sensitivity emerged on measures which are accepted to indicate early processing within the language comprehension system, and suggests that the evaluation of a conditional’s utility informs the early stages of <b>conditional</b> <b>processing...</b>|$|E
40|$|The initial {{efforts of}} e-health have fallen {{far short of}} expectations. They were buoyed by the hype and {{excitement}} of the Internet craze but limited by their {{lack of understanding of}} important market and environmental factors. E-health now recognizes that legacy systems and processes are important, that there is a technology adoption process that needs to be followed, and that demonstrable value drives adoption. Initial e-health transaction solutions have targeted mostly low-cost problems. These solutions invariably are difficult to integrate into existing systems, typically requiring manual interfacing to supported processes. This limitation in particular makes them unworkable for large volume providers. To meet the needs of these providers, e-health companies must rethink their approaches, appropriately applying technology to seamlessly integrate all steps into existing business functions. E-automation is a transaction technology that automates steps, integration of steps, and information communication demands, resulting in comprehensive automation of entire business functions. We applied e-automation to create a billing management solution for clinical reference laboratories. Large volume, onerous regulations, small margins, and only indirect access to patients challenge large laboratories’ billing departments. Couple these problems with outmoded, largely manual systems and it becomes apparent why most laboratory billing departments are in crisis. Our approach has been to focus on the most significant and costly problems in billing: errors, compliance, and system maintenance and management. The core of the design relies on <b>conditional</b> <b>processing,</b> a “universal ” communications interface, and ASP technologies. The result i...|$|E
40|$|New {{methods are}} {{illustrated}} for online nonlinear estimation {{applied to the}} lateral deflection of an elastic beam on board measurements of angular rates and angular accelerations. The development of the filter equations, together with practical issues of their numerical solution as developed from global linearization by nonlinear output injection are contrasted with the usual method of the extended Kalman filter (EKF). It is shown how nonlinear estimation due to gyroscopic coupling can be implemented as an adaptive covariance filter using off-the-shelf Kalman filter algorithms. The effect of the global linearization by nonlinear output injection is to introduce a change of coordinates in which only the process noise covariance is to be updated in online implementation. This {{is in contrast to}} the computational approach which arises in EKF methods arising by local linearization with respect to the current <b>conditional</b> mean. <b>Processing</b> refinements for nonlinear estimation based on optimal, nonlinear interpolation between observations are also highlighted. In these methods the extrapolation of the process dynamics between measurement updates is obtained by replacing a transition matrix with an operator spline that is optimized off-line from responses to selected test inputs...|$|R
40|$|Title: Assessment {{of using}} {{equipment}} for functional training of floorball women Objectives: The {{aim of this}} work is to compile selection of useful functional exercises with exercise equipment of extralegue floorball women team and also to get expert considerations of conditional trainer and assessment of exercises by Borg scale of floorball women. Methods: For the data collection and information acquisitions have been used following methods: questionnaire interviews by players of team and oral communication during an interview with <b>conditional</b> trainer. <b>Processing</b> of the questionnaires conducted in Microsoft Excel, the results are expressed through charts. Results: According to the assessment of conditional trainer has each selected functional exercise with equipment benefits for floorball players. It was found that floorball players do not considered functional training with the equipment as too exhausting, because they evaluated all exercises the average mark of 4 on the Borg scale. We can see some differences {{in the evaluation of}} players belonging to the national team, compared with players who had experience only with the league matches. We do not see a big difference in player's rating by age category and length of floorball practice. Keywords: functional training, exercise equipment, [...] ...|$|R
40|$|A {{decomposition}} of the Brier skill score {{shows that the}} performance of judgmental forecasts depends on seven components: environmental predictability, fidelity of the information system, match between environment and forecaster, reliability of information acquisition, reliability of information <b>processing,</b> <b>conditional</b> bias, and unconditional bias. These components {{provide a framework for}} research on the forecasting process. Selected literature addressing each component is reviewed, and implications for improving judgmental forecasting are discussed. KEY WORDS Judgmental forecasting Brier skill score Lens model equation Bias Reliability In any field requiring judgmental forecasts, the performance of professional forecasters depends jointly on (1) the environment about which forecasts are made, (2) the information system that brings data about the environment to the forecaster, and (3) the cognitive syste...|$|R
40|$|As {{biometric}} {{technology is}} increasingly deployed, {{it will be}} common to replace parts of operational systems with newer designs. The cost and inconvenience of reacquiring enrolled users when a new vendor solution is incorporated makes this approach difficult and many applications will require to deal with information from different sources regularly. These interoperability problems can dramatically affect the performance of biometric systems and thus, {{they need to be}} overcome. Here, we describe and evaluate the ATVS-UAM fusion approach submitted to the quality-based evaluation of the 2007 BioSecure Multimodal Evaluation Campaign, whose aim was to compare fusion algorithms when biometric signals were generated using several biometric devices in mismatched conditions. Quality measures from the raw biometric data are available to allow system adjustment to changing quality conditions due to device changes. This system adjustment is referred to as quality-based <b>conditional</b> <b>processing.</b> The proposed fusion approach is based on linear logistic regression, in which fused scores tend to be log-likelihood-ratios. This allows the easy and efficient combination of matching scores from different devices assuming low dependence among modalities. In our system, quality information is used to switch between different system modules depending on the data source (the sensor in our case) and to reject channels with low quality data during the fusion. We compare our fusion approach to a set of rule-based fusion schemes over normalized scores. Results show that the proposed approach outperforms all the rule-based fusion schemes. We also show that with the quality-based channel rejection scheme, an overall improvement of 25 % in the equal error rate is obtained. © 2010 IEEE...|$|E
40|$|Contemporary {{psychology}} has linked humans’ societal {{nature to}} the need to feel part of a community. McMillan and Chavis (1986) conceptualised a sense of community (SoC), upon four factors: 1) membership; 2) influence; 3) integration and fulfilment of need; and, 4) shared emotional connection. SoC has been explored in a variety of contexts identifying distinct correlated outcome-variables. For adults and young people alike, a strong SoC is positively related to outcomes in personal performance, health, and well-being. Recently, consideration has focused on schools as key ‘educational communities’ where pupils develop a SoC and learn the rules of society. Key educational legislation within the UK emphasises the importance placed upon developing community cohesion within schools. Within educational settings SoC has been shown to relate strongly with this aim, as well as correspond with positive outcomes in both pupil performance and well-being. However, research has lacked focus upon the mechanisms involved in developing a positive SoC within young people. This has meant that schools, as moral agents in facilitating young people’s formation of SoC, are yet ill informed as to how they can help in this process. This study accessed the experiences of 777 pupils in the South of England (Mage= 13. 34 years, %Male= 52) of their schools as communities. Ratings of pupil’s SoC were explored in relation to educational outcomes (Attainment, Attendance, Academic Self-concept) and measures of well-being (Self-Esteem, Life-satisfaction, Loneliness). Additional attention was paid to the hypothesised role of achievement motivation as an underlying mechanism between SoC and outcomes. Further, individual’s levels of Narcissism were explored as a potential personality level moderator. Correlation analyses indicated strong links between increased levels of SoC within school and multiple positive outcomes. <b>Conditional</b> <b>processing</b> models showed achievement motivation, notably via Mastery-Approach goals, to mediate all relationships, with Narcissism having a limited moderating effect...|$|E
40|$|We {{define a}} new gene, SRD 1, {{involved}} in the processing of pre-rRNA to mature rRNA. The SRD 1 gene was identified by selecting for second-site suppressors of the previously described rrp 1 - 1 mutation. The rrp 1 - 1 mutation causes temperature-sensitive growth, a <b>conditional</b> defect in <b>processing</b> of 27 S pre-rRNA to mature 25 S rRNA, and a nonconditional increase in sensitivity to several aminoglycoside antibiotics. All srd 1 alleles identified are recessive and apparently specific to the rrp 1 - 1 mutation. Although a mutation of SRD 1 suppresses the pre-rRNA processing defect, drug sensitivity and thermolethality of a point mutation of RRP 1, it is unable to suppress a rrp 1 -disruption allele. We suggest that the SRD 1 gene product either interacts with or regulates the RRP 1 product...|$|R
40|$|The Simon effect {{refers to}} the {{observation}} that responses to a relevant stimulus dimension are faster and moreaccurate when the stimulus and response spatially correspond than when they do not, even though stimulusposition is irrelevant. Recent findings {{have suggested that the}} Simon effect can be strongly modulated by priorpractice with a spatially incompatible mapping and by correspondence sequence. Although practice is thought toinfluence <b>conditional</b> stimulus–response (S–R) <b>processing,</b> leaving response priming through the unconditionalroute unaffected, sequential effects are thought to represent trial-by-trial adaptations that selectively involveunconditional S–R processing. In the present study, we tested this assumption by assessing the effects of correspondencesequence both when the Simon task alone was performed and when it was preceded by a spatialcompatibility task with either incompatible (Experiments 1 – 2) or compatible (Experiment 2) instructions. Theobservation that practice and correspondence sequence co-occur and exert additive effects strongly demonstratesthat the two factors affect different processing routes...|$|R
40|$|Recent {{studies have}} shown the {{existence}} of two qualitatively distinct groups of people based on how they judge {{the probability of a}} conditional statement. The present study was designed to test whether these differences are rooted in distinctive means of <b>processing</b> <b>conditional</b> statements and whether they are linked to differences in general intelligence. In the study, each of 120 participants completed three separate cognitive tasks involving the <b>processing</b> of abstract <b>conditional</b> statements [...] the probability-of-conditionals task, the conditional truth table task, and the conditional inference task [...] in addition to completing a test of general intelligence (AH 4). The results showed a number of predicted effects: People responding with conditional (rather than conjunctive) probabilities on the first task were higher in cognitive ability, showed reasoning patterns more consistent with a suppositional treatment of the conditional, and showed a strongly "defective" truth table pattern. The results include several novel findings and post challenges to contemporary psychological theories of conditionals. 13 page(s...|$|R
40|$|In {{this paper}} the author {{describes}} about superscalar processor and its architecture. A superscalar architecture {{is one in}} which several instructions can be initiated simultaneously and executed independently. pipelining allows several instructions to be executed at the same time, but they have to be in different pipeline stages at a given moment. Superscalar architectures include all features of pipelining but, in addition, there can be several instructions executing simultaneously in the same pipeline stage. They have the ability to initiate multiple instructions during the same clock cycle. Superscalar processing {{is the latest in a}} long series of innovations aimed at producing ever-faster microprocessors. By exploiting instruction-level parallelism, superscalar processors are capable of executing more than one instruction in a clock cycle. This paper discusses the microarchitecture of superscalar processors. We begin with a discussion of the general problem solved by superscalar processors: converting an ostensibly sequential program into a more parallel one. The principles underlying this process, and the constraints that must be met, are discussed. The paper then provides a description of the specific implementation techniques used in the important phases of superscalar processing. The major phases include: i) instruction fetching and <b>conditional</b> branch <b>processing,</b> ii) the determination of data dependences involving register values, iii) the initiation, or issuing, of instructions for parallel execution, iv) the communication of data values through memory via loads and stores, and v) committing the process state in correct order so that precise interrupts can be supported. Examples of recent superscalar microprocessors, the MIPS R 10000, the DE...|$|R
50|$|The ALU {{is capable}} of {{performing}} two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that {{is capable of}} performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated <b>conditional</b> statements and <b>processing</b> boolean logic.|$|R
40|$|ABSTRACT While the {{effectiveness}} {{and benefits of}} statin medications have been demonstrated in numerous studies, adherence to statin therapy is still less than optimal. Psychosocial factors are attributed {{to a variety of}} health behaviors, but the study of their impact on statin medication adherence is limited. This study aimed to (1) investigate the effect of self-regulatory mechanisms impacting patients’ adherence to statin medications, (2) assess the effect of a regulatory fit intervention on the behavior, and (3) investigate how optimism {{may play a role in}} adhering to statin medications. Adults taking a statin medication were recruited from a Midwestern University health system and were randomized into two study groups. Each study group was primed with messages that were framed either as promotion or prevention; another randomization followed into three groups for each. Patients were primed with implementation intentions framed as promotion or prevention and one group served as the control receiving no implementation intentions. The Regulatory Focus and Fit Theories were used to design the intervention. The behavior of statin medication adherence and patient psychosocial factors were assessed via a series of questions delivered in two questionnaires two weeks apart. <b>Conditional</b> indirect <b>processing</b> models were designed to test the relationships between the psychosocial factors and the behavior of statin medication adherence. A total of 326 patients completed both surveys. Patients’ prevention compared to promotion orientation positively and significantly impacted the behavior of statin medication adherence directly and indirectly via behavioral intentions and motivational intensity. At weak patient psychosocial factors, the Fit intervention resulted in higher statin medication adherence levels compared to the non-fit or control groups. Patients’ optimism levels positively impacted the behavior of statin medication adherence by significantly influencing behavioral intentions and motivational intensity, when levels of self-efficacy and outcome expectations were low or moderate. Patient psychosocial factors present an area of opportunity to explain the behavior of medication adherence and design interventions that will motivate patients to successfully engage in medication adherence. Future studies are needed to further investigate the effect of these and other psychosocial factors in a variety of medications and populations...|$|R
40|$|Abstract We {{consider}} {{the problem of}} training a conditional random field (CRF) to max-imize per-label predictive accuracy on a training set, an approach motivated by the principle of empirical risk minimization. We give a gradient-based procedurefor minimizing an arbitrarily accurate approximation of the empirical risk under a Hamming loss function. In experiments with both simulated and real data, ouroptimization procedure gives significantly better testing performance than several current approaches for CRF training, especially in situations of high label noise. 1 Introduction Sequence labeling, the task of assigning labels y = y 1, [...] ., yL to an input sequence x = x 1, [...] ., xL, isa machine learning problem of great theoretical and practical interest that arises in diverse fields such as computational biology, computer vision, and natural language <b>processing.</b> <b>Conditional</b> randomfields (CRFs) are a class of discriminative probabilistic models designed specifically for sequence labeling tasks [1]. CRFs define the conditional distribution Pw(y | x) {{as a function of}} featuresrelating labels to the input sequence. Ideally, training a CRF involves finding a parameter set w that gives high accuracy when labelingnew sequences. In some cases, however, simply finding parameters that give the best possible accuracy on training data (known as empirical risk minimization [2]) can be difficult. In particular, ifwe wish to minimize Hamming loss, which measures the number of incorrect labels, gradient-based optimization methods cannot be applied directly. 1 Consequently, surrogate optimization problems,such as maximum likelihood or maximum margin training, are solved instead...|$|R
40|$|Fine-grain {{parallelism}} {{available in}} VLIW and superscalar processors can be mainly exploited in computational intensive loops. Aggressive scheduling techniques {{are required to}} fully exploit this parallelism. In this paper we present a new Software Pipelining technique based on Graph Traverse Scheduling, a parallelizing technique originally proposed for multiprocessor systems that generates parallel threads automatically using a hamiltonian recurrence in the dependence graph of the loop. Explicit synchronizations required in multiprocessors to guarantee data dependences are now substituted in the VLIW architecture by the correct allocation of loop operations and nop-operations in the lock-step execution. The technique proposed here shows how an efficient VLIW code can be automatically generated using a hamiltonian recurrence in the dependence graph. The NP-hardness of the scheduling problem is restricted here to a problem of smaller size than related techniques. The dependence graph, extended with a scheduling recurrence, describes {{the characteristics of the}} schedule: the number of functional units required and the efficiency achieved, in terms of parallelism, can be known prior to generate the physical scheduling. In this paper we consider single-nested loops without <b>conditionals</b> and multifunctional <b>processing</b> units with a unit latency; extensions for multi-cycle processing units are straightforward. Finally we also show how other Software Pipelining techniques can be interpreted by means of scheduling recurrences in the dependence graph. In this sense our technique encompasses these other techniques since the schedules they obtaine can be also achieved with the technique described here...|$|R
30|$|Deblocking filter {{which is}} often {{referred}} to as a loop filter is the final stage of the decoding process. DF subfunction reduces the blocking effect that is introduced by encoding the process at block boundaries. Comparatively high complexity of the subfunction is in consensus. Even after a tremendous effort in speed optimization of the filtering algorithms, the filter can easily account for one-third of the computational complexity of a decoder [35]. The complexity is mainly based on the high adaptivity of the filter, which requires <b>conditional</b> and decisional <b>processing</b> on the block edge and sample levels, thus, there are many conditional branches in the filter which leads to excessive power consumption. At the same time, for a macroblock, the vertical filter begins from left-most edge and is followed from left to right by the three vertical edges; besides, the horizontal filter begins from top edge, and is followed by the three internal horizontal edges from top to bottom. Amount of relevant and candidate pixels should be loaded into the memory, this leads to additional power consumption either. Scalable energy can be achieved by classifying the filtering process into three levels, including full, half, and rough filtering. Among these, full filtering operation means that overall branch filtering is implemented for the macroblock. And, half filtering represents the operation reduced in computational complexity, which can be achieved by taking into account the fact that the image area in past frames is already filtered, and thereby optimizing or omitting the filtering process accordingly. For the rough filtering, skip operation is used with low quality degradation, while the lowest power consumption of the DF subfunction is required in this mode.|$|R
40|$|Machine {{learning}} (ML) is {{the study}} of representations and algorithms used for building functions that improve their behavior with experience. Today, researchers in many domains are applying ML to solve their problems when conventional programming techniques have proven insufficient. The first such were simple, computing the setting of only a single output variable. More recently, structured learning is adding its own set of challenges, and the specifications designed for such learning based programs are not scaling well, nor are the programs they represent. This thesis introduces Learning Based Programming (LBP), the study of programming language formalisms that directly support programs that learn their representations from data. An LBP framework embodies a set of design principles that ensures learning based programs designed under its formalism are composable, prepared for an infinite feature space, and not necessarily dependent on any particular learning or inference algorithms, among other things. We submit that adherence to these principles is necessary to enable the implementation of learning based programs that scale beyond today???s implementations. To ensure independence from learning and inference algorithms, we present the Constrained Conditional Model (CCM) and demonstrate that it is general enough to subsume the majority of models described in the literature. We then present Learning Based Java (LBJ), our first attempt at implementing an LBP framework which supports an important subset of CCMs. LBJ is a discriminative modeling language that abstracts away implementation details of feature extraction, learning, and inference. It also features a First Order Logic inspired syntax for expressing constraints between independently trained classifiers. LBJ has already been used successfully in a variety of Natural Language Processing tasks. We evaluate LBJ with both a comprehensive questionnaire and case studies. A variety of participants responded to the questionnaire, including varying levels of experience in ML and LBJ. They also applied LBJ on different tasks ranging in complexity. Simpler tasks tended to lead to a positive evaluation of LBJ, while those who required more advanced ML techniques, especially structured representations, found LBJ lacking. The case studies came to a similar conclusion while uncovering situations in which LBJ???s syntactic sugar could have encoded specifications more succinctly, as well as specific pieces of syntax that attempted to patch LBJ???s limitations. The results of our evaluation show that LBJ leaves something to be desired; in particular, it cannot naturally specify an arbitrary CCM. As such, we present our second offering in the LBP line: a general purpose programming language called the Constrained <b>Conditional</b> Model <b>Processing</b> language and designed from the ground up to support CCMs. We also present a formal semantics for CCMP specified in the language of rewriting logic and consider several interesting test cases. CCMP is a robust and flexible solution for structured learning techniques at both training-time and inference-time...|$|R

