1881|158|Public
25|$|Lee, S. and Honavar, V. (2017). Self-Discrepancy <b>Conditional</b> <b>Independence</b> Test. In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
25|$|The {{difference}} between the two expressions is the <b>conditional</b> <b>independence</b> of the variables from any of their non-descendants, given the values of their parent variables.|$|E
2500|$|... are equivalent: that is they impose {{exactly the}} same <b>conditional</b> <b>independence</b> requirements.|$|E
40|$|Markov random fields {{provide a}} compact {{representation}} of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these <b>conditional</b> <b>independences</b> to factorize a Gibbs distribution into {{a set of}} factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of <b>conditional</b> <b>independences</b> that is true only for a certain assignment of its conditioning set; in contrast to <b>conditional</b> <b>independences</b> that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for <b>conditional</b> <b>independences.</b> Comment: 7 page...|$|R
40|$|In an {{independence}} model, the triplets {{that represent}} <b>conditional</b> <b>independences</b> between singletons are called elementary. It {{is known that}} the elementary triplets represent the independence model unambiguously under some conditions. In this paper, we show how this representation helps performing some operations with independence models, such as finding the dominant triplets or a minimal independence map of an independence model, or computing the union or intersection {{of a pair of}} independence models, or performing causal reasoning. For the latter, we rephrase in terms of <b>conditional</b> <b>independences</b> some of Pearl's results for computing causal effects...|$|R
40|$|Building higher-dimensional copulas is {{generally}} {{recognized as a}} difficult problem. Regular-vines using bivariate copulas provide a flexible class of high-dimensional dependency models. In large dimensions, the drawback of the model is the exponentially increasing complexity. Recognizing some of the <b>conditional</b> <b>independences</b> is a possibility for {{reducing the number of}} levels of the pair-copula decomposition, and hence to simplify its construction Aas et al (2009). The idea of using <b>conditional</b> <b>independences</b> was already performed under elliptical copula assumptions Hanea, Kurowicka and Cooke (2006), Kurowicka and Cooke (2002) {{and in the case of}} DAGs in a recent work Bauer, Czado and Klein (2011). We provide a method which uses some of the <b>conditional</b> <b>independences</b> encoded by the Markov network underlying the variables. We give a theorem which under some graph conditions makes possible to derive pair-copula decomposition of the probability density function associated to a Markov network. As the underlying Markov network is usually unknown, we first have to discover it from the sample data. Using our results published in Szantai and Kovacs (2008) and Kovacs and Szantai (2010 a) we will show how to derive a multidimensional copula model exploiting the information on <b>conditional</b> <b>independences</b> hidden in the sample data. Comment: the paper will be presented at the 4 th Workshop on Vine Copula Distributions and Applications, May 11 - 12, 2011, Technische Universitat Muenche...|$|R
2500|$|In January 1991, a {{referendum}} {{was held in}} the Crimean Oblast, and voters approved restoring the Crimean Autonomous Soviet Socialist Republic. However, after the dissolution of the Soviet Union less than a year later, the Autonomous Republic of Crimea was formed as a constituent entity of independent Ukraine, with a slight majority of Crimean voters approving Ukrainian independence in a December referendum. â€” 67.5% of the total Crimean electorate voted, and 54.2% said yes. On 5 May 1992, the Crimean legislature declared <b>conditional</b> <b>independence,</b> but {{a referendum}} to confirm the decision was never held amid opposition from Kiev, elected president of Crimea [...] Yuriy Meshkov, was replaced by Kiev appointed Anatoliy Franchuk, which was done with the intent to reign in Crimean aspirations of autonomy. The Verkhovna Rada voted to grant Crimea [...] "extensive home rule" [...] during the dispute.|$|E
50|$|Conditional {{dependence}} {{is different}} from <b>conditional</b> <b>independence.</b> In <b>conditional</b> <b>independence</b> two events (which may be dependent or not) become independent given the occurrence of a third event.|$|E
5000|$|Extended axiomatic {{foundations}} {{of information and}} valuation algebras: The concept of <b>conditional</b> <b>independence</b> is basic for information algebras and a new axiomatic foundation of information algebras, based on <b>conditional</b> <b>independence,</b> extending the old one (see above) is available: https://arxiv.org/abs/1701.02658 ...|$|E
40|$|We {{introduce}} a new modelling representation, the Decision Event Graph (DEG), for asymmetric multistage decision problems. The DEG explicitly encodes <b>conditional</b> <b>independences</b> and has additional significant advantages over other representations of asymmetric decision problems. The colouring of edges {{makes it possible to}} identify <b>conditional</b> <b>independences</b> on decision trees, and these coloured trees serve as a basis for the construction of the DEG. We provide an efficient backward-induction algorithm for finding optimal decision rules on DEGs, and work through an example showing the efficacy of these graphs. Simplifications of the topology of a DEG admit analogues to the sufficiency principle and barren node deletion steps used with influence diagrams...|$|R
40|$|A {{variety of}} {{statistical}} graphical {{models have been}} defined to represent the <b>conditional</b> <b>independences</b> underlying a random vector of interest. Similarly, many different graphs embedding various types of preferential independences, as for example <b>conditional</b> utility <b>independence</b> and generalized additive independence, have more recently started to appear. In this paper we define a new graphical model, called a directed expected utility network, whose edges depict both probabilistic and utility <b>conditional</b> <b>independences.</b> These embed a very flexible class of utility models, much larger than those usually conceived in standard influence diagrams. Our graphical representation, and various transformations of the original graph into a tree structure, are then used to guide fast routines for the computation of a decision problem's expected utilities. We show that our routines generalize those usually utilized in standard influence diagrams' evaluations under much more restrictive conditions. We then proceed {{with the construction of}} a directed expected utility network to support decision makers in the domain of household food security...|$|R
40|$|Three {{representations}} of the <b>conditional</b> <b>independences</b> due to Mendelian segregation of genes in a pedigree are proposed. The computational costs of performing calculations using the technique of peeling with each of these representations is compared by considering the weights of triangulations of the graph produced by each representation...|$|R
5000|$|Stochastic factors {{affecting}} an observation show strong <b>conditional</b> <b>independence.</b>|$|E
5000|$|... are equivalent: that is they impose {{exactly the}} same <b>conditional</b> <b>independence</b> requirements.|$|E
50|$|Ancestral graphs {{are used}} to depict <b>conditional</b> <b>independence</b> {{relations}} between variables in Markov models.|$|E
50|$|A {{fundamental}} tool in graphical {{analysis is}} d-separation, which allows researchers to determine, by inspection, whether the causal structure implies that {{two sets of}} variables are independent given a third set. In recursive models without correlated error terms (sometimes called Markovian), these <b>conditional</b> <b>independences</b> represent all of the model's testable implications.|$|R
40|$|The {{study of}} {{causality}} or causal inference - {{how much a}} given treatment causally affects a given outcome in a population - goes way beyond correlation or association analysis of variables, and is critical in making sound data driven decisions and policies in a multitude of applications. The gold standard in causal inference is performing "controlled experiments", which often is not possible due to logistical or ethical reasons. As an alternative, inferring causality on "observational data" based on the "Neyman-Rubin potential outcome model" has been extensively used in statistics, economics, and social sciences over several decades. In this paper, we present a formal framework for sound causal analysis on observational datasets that are given as multiple relations and where the population under study is obtained by joining these base relations. We study a crucial condition for inferring causality from observational data, called the "strong ignorability assumption" (the treatment and outcome variables should be independent in the joined relation given the observed covariates), using known <b>conditional</b> <b>independences</b> that hold in the base relations. We also discuss how {{the structure of the}} <b>conditional</b> <b>independences</b> in base relations given as graphical models help infer new <b>conditional</b> <b>independences</b> in the joined relation. The proposed framework combines concepts from databases, statistics, and graphical models, and aims to initiate new research directions spanning these fields to facilitate powerful data-driven decisions in today's big data world...|$|R
40|$|Graphical models {{provide a}} {{framework}} for exploration of multivariate dependence patterns. The connection between graph and statistical model is made by identifying the vertices of the graph with the observed variables and translating the pattern of edges in the graph into a pattern of <b>conditional</b> <b>independences</b> that is imposed on the variables' joint distribution. Focusing on Gaussian models, we review classical graphical models. For these models the defining <b>conditional</b> <b>independences</b> are equivalent to vanishing of certain (partial) correlation coefficients associated with individual edges that are absent from the graph. Hence, Gaussian graphical model selection can be performed by multiple testing of hypotheses about vanishing (partial) correlation coefficients. We show and exemplify how this approach allows one to perform model selection while controlling error rates for incorrect edge inclusion. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
5000|$|Doob's <b>conditional</b> <b>independence</b> property: If [...] are conditionally {{independent}} given , then [...] (equivalently, [...] ).|$|E
50|$|A set {{of rules}} {{governing}} statements of <b>conditional</b> <b>independence</b> have been derived from the basic definition.|$|E
5000|$|Lee, S. and Honavar, V. (2017). Self-Discrepancy <b>Conditional</b> <b>Independence</b> Test. In: Conference on Uncertainty in Artificial Intelligence (UAI-17).|$|E
40|$|CUI networks: A {{graphical}} representation for <b>conditional</b> utility <b>independence</b> We introduce CUI networks, a compact {{graphical representation}} of utility functions over multiple attributes. CUI networks model multiattribute utility functions using the well studied and widely applicable utility independence concept. We show how <b>conditional</b> utility <b>independence</b> {{leads to an}} effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes {{can be used to}} calculate joint utility and to find the utility maximizing alternative. We discuss the applicability of <b>conditional</b> utility <b>independence</b> compared to other well known independence conditions, and contrast our new representation with previous graphical preference modeling. ...|$|R
40|$|In this paper, we {{evaluate}} {{a method for}} analyzing microarray data. The method {{is an attempt to}} learn regulatory interactions between genes from gene expression data. It is based on a Bayesian network, which is a mathematical tool for modeling <b>conditional</b> <b>independences</b> between stochastic variables. We review the dynamic nature of interacting genes, and explain how to model them using such a network...|$|R
40|$|<b>Independence</b> of <b>Conditionals</b> (IC) has {{recently}} been proposed as a basic rule for causal structure learning. If a Bayesian network represents the causal structure, its Conditional Probability Distributions (CPDs) should be algorithmically independent. In this paper we compare IC with causal faithfulness (FF), stating that only those <b>conditional</b> <b>independences</b> that are implied by the causal Markov condition hold true. The latter is a basic postulate in common approaches to causal structure learning. The common spirit of FF and IC is to reject causal graphs for which the joint distribution looks â€˜non-genericâ€™. The difference lies {{in the notion of}} genericity: FF sometimes rejects models just because one of the CPDs is simple, for instance if the CPD describes a deterministic relation. IC does not behave in this undesirable way. It only rejects a model when there is a non-generic relation between different CPDs although each CPD looks generic when considered separately. Moreover, it detects relations between CPDs that cannot be captured by <b>conditional</b> <b>independences.</b> IC therefore helps in distinguishing causal graphs that induce the same <b>conditional</b> <b>independences</b> (i. e., they belong to the same Markov equivalence class). The usual justification for FF implicitly assumes a prior that is a probability density on the parameter space. IC can be justified by Solomonoffâ€™s universal prior, assigning non-zero probability to those points in parameter space that have a finite description. In this way, it favours simple CPDs, and therefore respects Occamâ€™s razor. Since Kolmogorov complexity is uncomputable, IC is not directly applicable in practice. We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms...|$|R
5000|$|... where f is the {{conditional}} density of V given Z. Now, suppose that given Z, V is conditionally independent of W. This assumption is called <b>conditional</b> <b>independence</b> assumption or selection on observables. Intuitively, this condition means that Z {{is a good}} predictor of V so that once conditioned on Z, V has no systematic dependence on W. Under the <b>conditional</b> <b>independence</b> assumption, the asymptotic variance of the two-step estimator is: ...|$|E
5000|$|C. van Putten and J.H. van Schuppen, Invariance {{properties}} of the <b>conditional</b> <b>independence</b> relation, Ann. Probab. 13 (1985), 934 - 945.|$|E
5000|$|The {{last step}} follows from an {{application}} of the Bayes' rule and the <b>conditional</b> <b>independence</b> of [...] and [...] given [...]|$|E
40|$|Learning the {{structure}} of graphical models is an important task, but one of considerable difficulty when latent variables are involved. Because <b>conditional</b> <b>independences</b> using hidden variables cannot be directly observed, one has to rely on alternative methods to identify the d-separations that define the graphical structure. This paper describes new distribution-free techniques for identifying d-separations in continuous latent variable models when non-linear dependencies are allowed among hidden variables. 1...|$|R
40|$|This {{paper is}} {{focusing}} on the computational aspects of using graphs in analysing log-linear models, but graphical models also have other advantages. By the graphs, {{the structure of a}} given model is visually represented. The <b>conditional</b> <b>independences</b> can be read directly of the graphs, and thus the graphs facilitate communication between statistician and researcher. Some textbooks on graphical models are Cox and Wermuth (1996), Edwards (1995), Lauritzen (1996) and Whittaker (1990) ...|$|R
40|$|The paper {{gives an}} {{operator}} algebras {{model for the}} <b>conditional</b> monotone <b>independence,</b> introduced by T. Hasebe. The construction is used to prove an embedding result for the N. Muraki's monotone product of C*-algebras. Also, the formulas from the definition of <b>conditional</b> monotone <b>independence</b> are used to define the monotone product of maps which is shown to preserve complete positivity, a similar to {{the results from the}} case of free products...|$|R
50|$|Independence {{can be seen}} as {{a special}} kind of <b>conditional</b> <b>independence,</b> since {{probability}} {{can be seen as}} a kind of conditional probability given no events.|$|E
50|$|The {{difference}} between the two expressions is the <b>conditional</b> <b>independence</b> of the variables from any of their non-descendants, given the values of their parent variables.|$|E
5000|$|Maximum {{conditional}} independence: if {{the hypothesis}} can be {{cast in a}} Bayesian framework, try to maximize <b>conditional</b> <b>independence.</b> This is the bias used in the Naive Bayes classifier.|$|E
40|$|Abstract. Multivariate Gaussian {{graphical}} {{models are}} {{defined in terms}} of Markov properties, i. e., <b>conditional</b> <b>independences</b> associated with the underlying graph. Thus, model selection can be performed by testing these <b>conditional</b> <b>independences,</b> which are equivalent to specified zeroes among certain (partial) correlation coefficients. For concentration graphs, covariance graphs, acyclic directed graphs, and chain graphs (both LWF and AMP), we apply Fisherâ€™s z-transformation, Ë‡ SidÃ¡kâ€™s correlation inequality, and Holmâ€™s step-down procedure, to simultaneously test the multiple hypotheses obtained from the Markov properties. This leads to a simple method for model selection that controls the overall error rate for incorrect edge inclusion. In practice, we advocate partitioning the simultaneous p-values into three disjoint sets, a significant set S, an indeterminate set I, and a non-significant set N. Then our SIN model selection method selects two graphs, a graph whose edges correspond to the union of S and I, and a more conservative graph whose edges correspond to S only. Prior information about the presence and/or absence of particular edges can be incorporated readily. 1...|$|R
40|$|In this paper, {{we study}} {{different}} concepts of <b>conditional</b> belief functions <b>independence</b> {{in the context}} of the transferable belief model. We especially clarify the relationships between the concepts of conditional non-interactivity, irrelevance and doxastic <b>independence.</b> <b>Conditional</b> non-interactivity is defined by the 'mathematical' property useful for computation considerations and corresponds to decomposionality of the belief functions. Conditional irrelevance is defined by a 'common sense' property based on conditioning. <b>Conditional</b> doxastic <b>independence</b> is deFIned by a particular form of irrelevance, the one preserved under Dempster's rule of combination. SCOPUS: cp. pinfo:eu-repo/semantics/publishe...|$|R
50|$|A crucial {{problem of}} multivariate {{statistics}} is finding (direct-)dependence structure underlying the variables contained in high-dimensional contingency tables. If {{some of the}} <b>conditional</b> <b>independences</b> are revealed, then even the storage of the data {{can be done in}} a smarter way (see Lauritzen (2002)). In order to do this one can use information theory concepts, which gain the information only from the distribution of probability, which can be expressed easily from the contingency table by the relative frequencies.|$|R
