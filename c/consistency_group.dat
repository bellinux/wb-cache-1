7|197|Public
50|$|The <b>consistency</b> <b>group</b> (CG) term is {{used for}} {{grouping}} several LUNs together {{in order to ensure}} write-order consistency over several volumes. This {{is used for}} example with a database that stores its data and journal on different logical drives. These logical drives must be kept in-sync on the replica if data-consistency needs to be preserved. Other examples are multi-volume file systems such as ZFS or Windows' Dynamic Disks.|$|E
5000|$|Global Mirror is an IBM {{technology}} that provides data replication over extended distances between two sites for business continuity and disaster recovery. If adequate bandwidth exists, Global Mirror provides an {{recovery point objective}} (RPO) of as low as 3-5 seconds [...] between the two sites at extended distances with no performance impact on the application at the primary site. It replicates the data asynchronously and also forms a <b>consistency</b> <b>group</b> at a regular interval allowing a clean recovery of the application.|$|E
40|$|Initial Psychological Experimentation In {{the first}} experiment, {{subjects}} were asked to imagine that they work for the US Forest Service and were assigned the task of learning to predict years {{in which there is a}} severe risk of forest fire danger in the fall. Four concepts had to be learned in the experiment [...] one concept in each of four phases. All subjects learned the same 3 background concepts in phases 1 - 3. Then, for phase 4, they were divided into two groups (the logical <b>consistency</b> <b>group</b> and the feature <b>consistency</b> <b>group)</b> to learn one of two separate concepts which depended on the background concepts. We investigate learning a set of causally related concepts from examples. We show that human subjects make fewer errors and learn more rapidly when the set of concepts is logically consistent. We compare the results of these subjects to subjects learning equivalent concepts that share sets of relevant features, but are not logically consistent. We present a shared-task neural [...] ...|$|E
5000|$|The {{implementation}} of <b>consistency</b> <b>groups</b> {{is not limited}} to FlashCopy. Global Mirror for IBM System z series (formerly known as XRC or eXtended Remote Copy) also creates <b>consistency</b> <b>groups</b> to asynchronously mirror disk data from one site to another over any distance [...]|$|R
5000|$|FlashCopy <b>consistency</b> <b>groups</b> {{are used}} in a single-site {{scenario}} {{in order to create}} a time-consistent copy of data that can then be backed-up and sent off site, or in a multi-site Global Mirror for ESS implementation to force time consistency at the remote site.|$|R
5000|$|FlashCopy Version 2 {{introduced}} {{the ability to}} flash individual data sets and then added support for “consistency groups”. FlashCopy <b>consistency</b> <b>groups</b> {{can be used to}} help create a consistent point-in-time copy across multiple volumes, and even across multiple ESSs, thus managing the consistency of dependent writes.|$|R
40|$|This paper {{studies the}} {{possible}} benefit {{that can be}} obtained by introducing variability into the robotic control of trajectories used to train hindlimb locomotion in adult spinal mice. The spinal cords of adult female Swiss-Webster mice were completely transected at a mid-thoracic level. Fourteen days post-transection, the spinal mice were robotically trained to step {{in the presence of a}} 5 -HT agonist, quipazine, for a period of six weeks. In this pilot study nine animals were divided into three groups, each receiving a different control strategy: a fixed training trajectory (Group A), a variable training trajectory without interlimb coordination imposed (Group B) and a variable training trajectory with hindlimb bilateral coordination imposed (Group C). Preliminary results indicate that Group A recovers more slowly than the two groups receiving variable modes of robotic training. Groups B and C show higher levels of recovery than Group A in terms of the number of steps performed during testing sessions, as well as in their step periodicity and shape <b>consistency.</b> <b>Group</b> C displays a higher incidence of alternating stepping than Group B. These results indicate that variable trajectory robotic training paradigms may be more effective than fixed trajectory paradigms in promoting robust post-injury stepping behavior. Furthermore, it appears that the inclusion of interlimb coordination is an important contribution to successful training...|$|E
40|$|Many {{distributed}} systems for wide­area networks {{can be built}} conveniently, and operate efficiently and correctly, using a weak <b>consistency</b> <b>group</b> communication mechanism. This mechanism organizes a set of principals into a single logical entity, and provides methods to multicast messages to the members. A weak consistency distributed system allows the principals in the group to differ {{on the value of}} shared state at any given instant, as long as they will eventually converge to a single, consistent value. A group containing many principals and using weak consistency can provide the reliability, performance, and scalability necessary for wide­area systems. I have developed a framework for constructing group communication systems, for classifying existing distributed system tools, and for constructing and reasoning about a particular group communication model. It has four components: message delivery, message ordering, group membership, and the application. Each component may have a different implementation, so that the group mechanism can be tailored to application requirements. The framework supports a new message delivery protocol, called timestamped anti­entropy, which provides reliable, eventual message delivery; is efficient; and tolerates most transient processor and network failures. It can be combined with message ordering implementations that provide ordering guarantees ranging from unordered to total, causal delivery. A new group membership protocol completes the set, providing temporarily inconsistent membership views resilient to up to k simultaneous principal failures. The Refdbms distributed bibliographic database system, which has been constructed using this framework, is used as an example. Refdbms databases can be replicated on many different sites, using the group communication system described here...|$|E
40|$|Databases are an {{important}} part of today's IT infrastructure: both companies and state institutions rely on database systems to store most of their important data. As we are more and more dependent on database systems, securing this key facility is now a priority. Because of this, research on fault-tolerant database systems is of increasing importance. One way to ensure the fault-tolerance of a system is by replicating it. Replication is a natural way to deal with failures: if one copy is not available, we use another one. However implementing consistent replication is not easy. Database replication is hardly a new area of research: the first papers on the subject are more than twenty years old. Yet how to build an efficient, consistent replicated database is still an open research question. Recently, a new approach to solve this problem has been proposed. The idea is to rely on some communication infrastructure called group communications. This infrastructure offers some high-level primitives that can help in the design and the implementation of a replicated database. While promising, this approach to databasereplication is still in its infancy. This thesis focuses on group communication-based database replication and strives to give an overall understanding of this topic. This thesis has three major contributions. In the structural domain, it introduces a classification of replication techniques. In the qualitative domain, an analysis of fault-tolerance semantics is proposed. Finally, in the quantitative domain, a performance evaluation of group communication-based database replication is presented. The classification gives an overview of the different means to implement databasereplication. Techniques described in the literature are sorted using this classification. The classification highlights structural similarities of techniques originating from different communities (database community and distributed systems community). For each category of the classification, we also analyse the requirements imposed on the database component and group communication primitives that are needed to enforce <b>consistency.</b> <b>Group</b> communication-based database replication implies building a system from two different components: a databasesystem and a group communication system. Fault-tolerance is an end-to-end property: a system built from two components tends to be as fault-tolerant as the weakest component. The analysis of fault-tolerance semantics show what fault-tolerance guarantee is ensured by group communication based replication techniques. Additionally a new fault-tolerance guarantee, group-safety, is proposed. Group-safety is better suited to group communication-based database replication. We also show that group-safe replication techniques can offer improved performance. Finally, the performance evaluation offers a quantitative view of group communication based replication techniques. The performance of group communication techniques and classical database replication techniques is compared. The way those different techniques react to different loads is explored. Some optimisation of group communication techniques are also described and their performance benefits evaluated. ...|$|E
40|$|A log-based {{mechanism}} is described for restoring consistent states to replicated data objects after failures. Preserving a causal form of consistency {{based on the}} notion of virtual time is focused upon in this report. Causal consistency has been shown to apply to a variety of applications, including distributed simulation, task decomposition, and mail delivery systems. Several mechanisms have been proposed for implementing causally consistent recovery, most notably those of Strom and Yemini, and Johnson and Zwaenepoel. The mechanism proposed here differs from these in two major respects. First, a roll-forward style of recovery is implemented. A functioning process is never required to roll-back its state in order to achieve consistency with a recovering process. Second, the mechanism does not require any explicit information about the causal dependencies between updates. Instead, all necessary dependency information is inferred from the orders in which updates are logged by the object servers. This basic recovery technique appears to be applicable to forms of consistency other than causal consistency. In particular, it is shown how the recovery technique can be modified to support an atomic form of <b>consistency</b> (<b>grouping</b> <b>consistency).</b> By combining <b>grouping</b> <b>consistency</b> with casual consistency, it may even be possible to implement serializable consistency within this mechanism...|$|R
2500|$|The CCI {{model was}} {{proposed}} as a consistency management in collaborative editing systems. Under the CCI model, three <b>consistency</b> properties are <b>grouped</b> together: ...|$|R
50|$|In 2008, Thistle Hotels {{announced}} that it is currently undergoing a re-branding of £100 million {{in order to maintain}} <b>consistency</b> across the <b>group.</b>|$|R
5000|$|The CCI {{model was}} {{proposed}} as a consistency management in collaborative editing systems. Under the CCI model, three <b>consistency</b> properties are <b>grouped</b> together: ...|$|R
40|$|State-of-the-art {{middleware}} like. NET or J 2 EE offers transparent {{access to}} distributed objects and services but {{lack of support}} for <b>consistency</b> between <b>groups</b> of objects. Distributed Shared Memory (DSM) is an interesting alternative to build distributed applications because it maintains <b>consistency</b> among object <b>groups.</b> Although DSM {{has been used in}} the past only for special number crunching programs we believe it is a good foundation to simplify the development of multi-player games and virtual reality applications. In this paper we present the relevant parts of our transactional DSM operating system (OS). Subsequently, we describe the gaming framework we have built on top of our DSM system. Finally we evaluate the framework with a sample game...|$|R
40|$|Acknowledgement: We {{would like}} to {{acknowledge}} the contributions of the Mode <b>Consistency</b> Working <b>Group,</b> through whose efforts these guidelines were developed. The members of the Working Group provided the breadth of knowledge and varied points of view so that this document would be useful to all decennial programs. The Working Group was chaired by Kathleen M. Styles (DMD) and included Census Bureau participants Sarah Brad...|$|R
40|$|Abstract—The {{consistency}} test {{is one of}} the critical components both in AHP and ANP. It is necessary to make sure if the judgment result is accuracy and reliable. This paper stated a specific process of {{consistency test}} in ANP with group judgment under intuitionistic fuzzy environment. A two steps de-fuzzification technique with intuitionistic fuzzy number reduction and generalized mean computation is proposed to apply in this study. The <b>group</b> <b>consistency</b> is also fully tested in two stages. This proposed process exposes that it is comprehensive and feasible. Besides, the application of maximum eigenvalue threshold method, a new consistency test index to check for the consistency, is an advantage because it reduces a lot of operations. Index Terms — <b>Consistency</b> testing, <b>group</b> judgment, analytic network process, intuitionistic fuzzy. I...|$|R
40|$|We {{study the}} paper of Xu [Z. Xu, On {{consistency}} of the weighted geometric mean complex judgement matrix in AHP, European Journal of Operational Research 126 (2000) 683 - 687] for the <b>group</b> <b>consistency</b> in analytic hierarchy process of multicriteria decision-making. The purpose of this note is threefold. First, we point out the questionable results in this paper. Second, for three by three comparison matrices, we provide a patchwork for his method. Third, we constructed a counter example to show that in general his method is wrong. Numerical examples are provided to illustrate our findings. If there are four or more alternatives, then we may advise researchers to ignore his results to avoid questionable estimation of <b>group</b> <b>consistency.</b> ...|$|R
5000|$|Indeed, local {{consistency}} is only {{relative to}} the <b>consistency</b> of <b>groups</b> of variables. For example, arc consistency guarantees that every consistent evaluation of a variable can be consistently extended to another variable. However, when a single value of a variable is extended to two other variables, {{there is no guarantee}} that these two values are consistent with each other. For example, [...] may be consistent with [...] and with , but these two evaluations may not be consistent with each other.|$|R
40|$|Background: Numerous {{types of}} {{clustering}} like single linkage and K-means {{have been widely}} studied and applied {{to a variety of}} scientific problems. However, the existing methods are not readily applicable for the problems that demand high stringency. Methods: Our method, self <b>consistency</b> <b>grouping,</b> i. e. SCG, yields clusters whose members are closer in rank to each other than to any member outside the cluster. We do not define a distance metric; we use the best known distance metric and presume that it measures the correct distance. SCG does not impose any restriction on the size or the number of the clusters that it finds. The boundaries of clusters are determined by the inconsistencies in the ranks. In addition to the direct implementation that finds the complete structure of the (sub) clusters we implemented two faster versions. The fastest version is guaranteed to find only the clusters that are not subclusters of any other clusters and the other version yields the same output as the direct implementation but does so more efficiently. Results: Our tests have demonstrated that SCG yields very few false positives. This was accomplished by introducing errors in the distance measurement. Clustering of protein domain representatives by structural similarity showed that SCG could recover homologous groups with high precision...|$|R
40|$|We {{consider}} the least-square regression problem with regularization by a block 1 -norm, i. e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, {{referred to as}} the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly {{referred to as the}} Lasso. In this paper, we study the asymptotic model <b>consistency</b> of the <b>group</b> Lasso. We derive necessary and sufficient conditions for the <b>consistency</b> of <b>group</b> Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied...|$|R
40|$|Abstract Background Numerous {{types of}} {{clustering}} like single linkage and K-means {{have been widely}} studied and applied {{to a variety of}} scientific problems. However, the existing methods are not readily applicable for the problems that demand high stringency. Methods Our method, self <b>consistency</b> <b>grouping,</b> i. e. SCG, yields clusters whose members are closer in rank to each other than to any member outside the cluster. We do not define a distance metric; we use the best known distance metric and presume that it measures the correct distance. SCG does not impose any restriction on the size or the number of the clusters that it finds. The boundaries of clusters are determined by the inconsistencies in the ranks. In addition to the direct implementation that finds the complete structure of the (sub) clusters we implemented two faster versions. The fastest version is guaranteed to find only the clusters that are not subclusters of any other clusters and the other version yields the same output as the direct implementation but does so more efficiently. Results Our tests have demonstrated that SCG yields very few false positives. This was accomplished by introducing errors in the distance measurement. Clustering of protein domain representatives by structural similarity showed that SCG could recover homologous groups with high precision. Conclusions SCG has potential for finding biological relationships under stringent conditions. </p...|$|R
50|$|This is {{one factor}} that {{plays a role in}} how issues are addressed. When there are a group of {{individuals}} involved in communication, decisions, and change, with the lack of <b>consistency</b> in the <b>group,</b> most of the tasks fail to complete. This phenomenon occurs in situations such as policy change, rulings and procedures.|$|R
40|$|Abstract Judgment {{aggregation}} theory, which {{concerns the}} translation of individual judgments on logical propositions into consistent group judgments, has shown that <b>group</b> <b>consistency</b> generally cannot be guaranteed if each proposition is treated independently from the others. Developing the right method of abandoning independence is thus a high-priority goal. However, little {{work has been done}} in this area outside of a few simple approaches. To fill the gap, we compare four methods based on distance metrics between judgment sets. The methods generalize the premise-based and sequential priority approaches to judgment aggregation, as well as distance-based preference aggregation. They each guarantee <b>group</b> <b>consistency</b> and implement a range of distinct functions with different properties, broadening the available tools for social choice. A central result is that only one of these methods (not previously considered in the literature) satisfies three attractive properties for all reasonable metrics. ...|$|R
40|$|Introduction: <b>Consistency</b> of {{relativity}} <b>group</b> with Heisenberg commutation relations Question: What is the condition on a relativity {{group for the}} Heisenberg commutation relations to hold for all states related by the group (on a flat manifold) The Weyl-Heisenberg group m m s m 1, m m, is a matrix group with manifold 2 m...|$|R
30|$|From {{estimates}} of the dissimilarities, the strains were grouped using hierarchical UPGMA method (Unweighted Pair-Group Mean Average) with the test of bootstrap (1000 times) to evaluate the <b>consistency</b> of the <b>group</b> (Efron & Tibshirani, 1993). A second analysis was performed using the pooling method of Tocher. All these tests were performed using the Genes program (Cruz, 2001).|$|R
50|$|Global Mirror {{is based}} on IBM Copy Services functions: Global Copy and FlashCopy. Global mirror {{periodically}} pauses updates of the primary volumes and swaps change recording bitmaps. It then uses the previous bitmap to drain updates from the primary volumes to the secondaries. After all primary updates have been drained, the secondary volumes are used as the source for a FlashCopy to tertiary volumes at the recovery site. This ensures that the tertiary copy of the volumes has point-in-time <b>consistency.</b> By <b>grouping</b> many volumes into one Global Mirror session multiple volumes may be copied to the recovery site simultaneously while maintaining point-in-time consistency across those volumes.|$|R
40|$|We {{start from}} the fact that the {{discriminated}} is almost absent from the theory of discrimination in economics. Indeed, while the reasons that make one discriminates another are well established, not much has been said about how the discriminated will react. Putting together concepts such as psychological <b>consistency</b> and <b>group</b> identity, we propose a frame that sums up some strategies that a discriminated can pursue. We end up analysing the conditions under which an individual would consider to get away from the groups he is assimilated with in order to avoid discrimination, at the expense of his psychological consistency and of his group related identity...|$|R
40|$|Statistical {{procedures}} for variable selection have become integral elements in any analysis. Successful procedures {{are characterized by}} high predictive accuracy, yielding interpretable models while retaining computational efficiency. Penalized methods that perform coefficient shrinkage {{have been shown to}} be successful in many cases. Models with correlated predictors are particularly challenging to tackle. We propose a penal-ization procedure that performs variable selection while clustering groups of predictors automatically. The oracle properties of this procedure including <b>consistency</b> in <b>group</b> identification are also studied. The proposed method compares favorably with existing selection approaches in both prediction accuracy and model discovery, while retaining its computational efficiency. Supplemental materials are available online...|$|R
30|$|Synthetic femora {{implanted}} {{with short}} stems fractured at {{a significantly higher}} torque (27.1 vs. 24.2  Nm, p =  0.03) and angle (30.3 ° vs. 22.3 °, p =  0.002) than those implanted with long stems. Fracture patterns {{of the two groups}} were different, but showed remarkable <b>consistency</b> within each <b>group.</b> These characteristic fracture patterns were closely replicated in the pair of cadaveric femora.|$|R
40|$|As {{the first}} stage of power system {{restoration}} after a blackout, an optimal black-start scheme is very important for speeding up the whole restoration procedure. Up to now, much research work has been done on generating or selecting an optimal black-start scheme by a single round of decision-making. However, less attention has been paid for improving the final decision-making results through a multiple-round decision-making procedure. In the group decision-making environment, decision-making results evaluated by different black-start experts may differ significantly with each other. Thus, the consistency of black-start decision-making results could be deemed as an important indicator in assessing the black-start group decision-making results. Given this background, an intuitionistic fuzzy distance-based method is presented to analyse the <b>consistency</b> of black-start <b>group</b> decision-making results. Moreover, the weights of black-start indices as well as the weights of decision-making experts are modified in order to optimise the <b>consistency</b> of black-start <b>group</b> decision-making results. Finally, an actual example is served for demonstrating the proposed method. ...|$|R
40|$|Joint {{action and}} group agency {{have emerged as}} focuses of {{attention}} in recent social theory and philosophy but they have rarely been connected with one another. The argument {{of this article is}} that whereas joint action involves people acting together to achieve any sort of result, group agency requires them to act together for the achievement of one result in particular: the construction of a centre of attitude and agency that satisfies the usual constraints of consistency and rationality in adequate measure. The main discovery in the recent theory of group agency is that this result is not easily achieved; no regular voting procedure will ensure, for example, that a group of individually consistent agents will display <b>consistency</b> in <b>group</b> judgments...|$|R
40|$|Locking is a {{standard}} technique in traditional distributed computing and database systems to ensure data integrity by prohibiting concurrent conflicting updates on shared data objects. Operational transformation is an innovative technique invented by groupware research for consistency maintenance in real-time group editors. In this paper, we will examine and explore the complementary roles of locking and operational transformation in consistency maintenance. A novel optional locking scheme is proposed and integrated with operation transformation to maintain both generic and context-specific consistency in a distributed, interactive, and collaborative environment. The integrated optional locking and operational transformation technique is fully distributed, highly responsive, non-blocking, and capable of avoiding locking overhead in the most common case of collaborative editing. Keywords: Locking, operational transformation, <b>consistency</b> maintenance, <b>group</b> editors, groupware, distribute [...] ...|$|R
40|$|In this {{presentation}} I shall {{explore the}} question of whether or not it is defensible to grant legal group rights through international instruments and national legal systems. I shall proceed in the following way. First, I shall briefly examine the conceptual <b>consistency</b> between <b>group</b> rights and the framework of rights discourse, and I shall conclude that it is conceptually possible to include group rights in ordinary rights talk. Secondly, I shall explore what the basic requirements would be for the recognition of a group right. I shall suggest that the use of rights discourse bears a number of conceptual as well as normative constraints that carry important practical consequences, and that these constraints must be understood as conditions of admissibility for any group right...|$|R
40|$|Previous {{research}} {{has noted that}} novice drivers are at greatest risk of an accident. One reason that has been reported {{for this is that}} they have not developed the optimum visual search strategies of their more experienced counterparts. One might expect that new drivers might be taught the appropriate visual skills while learning to drive, though this requires instructors to have introspection into their own visual skills before they can be passed on to the student. In addition novice drivers should be able to acquire the instructed skills. This study used an image-based questionnaire to assess driving instructors’ and novice drivers’ priority ratings for attending to different areas of the driving scene across nine scenarios. It was predicted that if instructors and novices have introspection into the relative importance of these different areas, there should be agreement across the sample of participants. Additionally it was considered important to assess which areas of the visual scene are important across all different scenarios and which areas change in priority with a change in scenario. Results showed that for both groups the opinions regarding visual field prioritisation were highly consistent when compared to chance. Despite the rating <b>consistencies,</b> <b>group</b> differences were found, across all scenarios with “Rear View Mirrors” being the visual field with the most frequent observed group differences. Certain categories (“Road Ahead” and “Mirrors”) were highly ranked across all scenarios, while other categories were more scenario specific. We conclude that both groups have insight into some elements of visual search. However, in many occasions the prioritisation was different between driving instructors and novice drivers. It appears that during the learning process the novice drivers did not adopt the prioritisation strategies seen in driving instructors. This has important implications for the teaching of visual skills in driving...|$|R
40|$|Atomic {{broadcast}} {{is a group}} communication service that enables a team of distributed processes to keep replicated data `consistent', despite concurrency, communication uncertainty, failures and recoveries. We investigate possible meanings for replicated data `consistency' in timed asynchronous systems, subject to crash/performance process failures and omission /performance communication failures which may partition correct team members into isolated parallel groups. We propose three different replica <b>consistency</b> specifications: <b>group</b> agreement, majority agreement and strict agreement and give examples of atomic broadcast protocols that implement these specifications. The interface issues between the underlying membership services and the broadcast protocols that provide the above semantics are also addressed. 1 Introduction A basic method for achieving high availability of a computing service is to implement it {{by a team of}} replicated servers which run on distinct processors. Atomica [...] ...|$|R
40|$|This project investigates how to quantitatively measure {{equity in}} small student groups. We follow several student groups to operationalize how {{discourse}} may be equitable or inequitable. The groups {{came from a}} two week, pre-college program that prepares first generation and deaf/hard-of-hearing students to major in a STEM field. In the program, students focus on improving their metacognitive skills and cultural preparation for college life within a context of model building. We use three methods to measure equity. First, we look at speaking time: who talks, when, and to whom. Second, we look to segment student discourse by analyzing <b>consistency</b> of <b>group</b> speaking time. Third, we analyze group equality over time changes. We analyze these methods to see how effective they are at capturing equity in group discourse. Comment: 4 pages, submitted to PERC 201...|$|R
