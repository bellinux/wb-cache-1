254|398|Public
2500|$|During read {{streaming}} {{into the}} CPU, a custom prefetch instruction, extended data <b>cache</b> <b>block</b> touch (xDCBT) prefetches data {{directly to the}} L1 data cache of the intended core, which skips putting the data in the L2 cache to avoid thrashing the L2 cache. Writes streaming from each core skip the L1 cache, due to its no-write allocation (avoids thrashing of high-bandwidth, transient, write-only data streams on the L1 cache), and goes directly to the L2 cache.|$|E
50|$|However, in {{the case}} of the Write Back policy, the changed <b>cache</b> <b>block</b> will be updated in the lower-level {{hierarchy}} only when the <b>cache</b> <b>block</b> is evicted. Writing back every <b>cache</b> <b>block</b> which is evicted is not efficient. Therefore, we use the concept of a Dirty bit attached to each <b>cache</b> <b>block.</b> The dirty bit is made high whenever the <b>cache</b> <b>block</b> is modified and during eviction, only the blocks with dirty bit high will be written to the lower-level hierarchy and then the dirty bit is cleared. In this policy, there is data losing risk as the only valid copy is stored in the cache and therefore need some correction techniques to be implemented.|$|E
5000|$|BusUpgr: Snooped {{request that}} {{indicates}} {{that there is a}} write request to a <b>Cache</b> <b>block</b> made by another processor but that processor already has that <b>Cache</b> <b>block</b> resident in its Cache.|$|E
3000|$|Yan and Zhang analyze {{a shared}} {{instruction}} cache on a dual core system that executes two threads [47]. To restrict {{the set of}} conflicting <b>cache</b> <b>blocks,</b> they introduce the category always-except one hit for level 2 <b>cache</b> <b>blocks.</b> Assuming threads [...]...|$|R
40|$|SpMV), or y = y +A x, {{which is}} an {{important}} and ubiquitous computational kernel. Prior work indicates that <b>cache</b> <b>blocking</b> of SpMV is extremely important for some matrix and machine combinations, with speedups as high as 3 x. In this paper we present a new, more compact data structure for <b>cache</b> <b>blocking</b> for SpMV {{and look at the}} general question of when and why performance improves. <b>Cache</b> <b>blocking</b> appears to be most e#ective when simultaneously 1) the vector x does not fit in cache 2) the vector y fits in cache 3) the non zeros are distributed throughout the matrix and 4) the non zero density is su#ciently high. In particular we find that <b>cache</b> <b>blocking</b> does not help with band matrices no matter how large x and y are since the matrix structure already lends itself to the optimal access pattern...|$|R
50|$|The {{original}} Pentium 4 processor had a four-way set associative L1 data {{cache of}} 8 KiB in size, with 64-byte <b>cache</b> <b>blocks.</b> Hence, there are 8 KiB / 64 = 128 <b>cache</b> <b>blocks.</b> The {{number of sets}} {{is equal to the}} number of <b>cache</b> <b>blocks</b> divided by the number of ways of associativity, what leads to 128 / 4 = 32 sets, and hence 25 = 32 different indices. There are 26 = 64 possible offsets. Since the CPU address is 32 bits wide, this implies 21 + 5 + 6 = 32, and hence 21 bits for the tag field.|$|R
50|$|When a bus {{transaction}} {{occurs to}} a specific <b>cache</b> <b>block,</b> all snoopers must snoop the bus transaction. Then the snoopers look up their corresponding cache tag to check whether {{it has the same}} <b>cache</b> <b>block.</b> In most cases, the caches do not have the <b>cache</b> <b>block</b> since a well optimized parallel program doesn’t share much data among threads. Thus the cache tag lookup by the snooper is usually an unnecessary work for the cache who does not have the <b>cache</b> <b>block.</b> But the tag lookup disturbs the cache access by a processor and incurs additional power consumption.|$|E
5000|$|If the <b>cache</b> <b>block</b> {{is in the}} Shared Modified {{state when}} a write (PrWr) occurs and the shared line is asserted, a bus update (BusUpd) is {{generated}} to update the other <b>cache</b> <b>block.</b>|$|E
50|$|Processor Write (PrWr): This {{happens when}} the {{processor}} completes a successful write on a certain <b>cache</b> <b>block</b> placed in its cache. This makes the processor to be the latest to update the <b>cache</b> <b>block.</b>|$|E
5000|$|CACHESIZE (Embedded DOS 6-XL only) : Maximum {{number of}} 512-byte <b>cache</b> <b>blocks</b> {{dynamically}} allocated from system pool.|$|R
40|$|International audienceThis paper proposes {{and studies}} a {{hardware-based}} adaptive controlled migration strategy for managing distributed L 2 caches in chip multiprocessors. Building on an area-efficient shared cache design, the proposed scheme dynamically migrates <b>cache</b> <b>blocks</b> to <b>cache</b> banks that best minimize the average L 2 access latency. <b>Cache</b> <b>blocks</b> are continuously monitored and {{the locations of}} the optimal corresponding cache banks are predicted to effectively alleviate the impact of non-uniform cache access latency. By adopting migration alone without replication, the exclusiveness of <b>cache</b> <b>blocks</b> is maintained, thus further optimizing the cache miss rate. Simulation results using a full system simulator demonstrate that the proposed controlled migration scheme outperforms the shared caching strategy and compares favorably with previously proposed replication schemes...|$|R
5000|$|CACHETTL (Embedded DOS 6-XL only) : Maximum time in ms before unused <b>cache</b> <b>blocks</b> are {{returned}} to system pool.|$|R
5000|$|If the <b>cache</b> <b>block</b> {{is in the}} Shared Clean {{state when}} a write (PrWr) occurs and the shared line is asserted, a bus update (BusUpd) is {{generated}} to update the other <b>cache</b> <b>block</b> and the state changes to Shared Modified.|$|E
50|$|The {{processor}} with the <b>cache</b> <b>block</b> in Sm state {{is responsible for}} updating memory when the <b>cache</b> <b>block</b> is replaced. But if the main memory is updated whenever a bus update transaction occurs, {{there is no need}} for separate Sm and Sc states, and the protocol can afford a single shared state. However, this causes a lot more memory transactions which can slow down the system, especially when multiple processors are writing to the same <b>cache</b> <b>block.</b>|$|E
50|$|Invalid (I) - <b>Cache</b> <b>block</b> is invalid.|$|E
40|$|Abstract. We present new {{performance}} {{models and}} more compact data structures for <b>cache</b> <b>blocking</b> {{when applied to}} sparse matrix-vector multiply (SpM×V). We extend our prior models by relaxing {{the assumption that the}} vectors fit in cache and find that the new models are accurate enough to predict optimum block sizes. In addition, we determine criteria that predict when <b>cache</b> <b>blocking</b> improves performance. We conclude with architectural suggestions that would make memory systems execute SpM×V faster. ...|$|R
50|$|In {{order to}} {{conserve}} memory, and system calls, iovec file operations {{are used to}} flush multiple <b>cache</b> <b>blocks</b> in a single call.|$|R
50|$|Theoretically, {{coherence}} can {{be performed}} at the load/store granularity. However, in practice it is generally performed at the granularity of <b>cache</b> <b>blocks.</b>|$|R
5000|$|If the <b>cache</b> <b>block</b> is in Shared Clean state, and it {{receives}} a bus read (BusRd) or a bus update (BusUpd) {{it continues to}} retain its state as the value is still shared. However, {{in the case of}} an Update, it will update the value in the <b>cache</b> <b>block.</b>|$|E
50|$|Processor Write Miss (PrRdMiss): This {{happens when}} the {{processor}} fails to write to a <b>cache</b> <b>block</b> from its cache, and needs to fetch the block from the memory or another cache and then write to it. This again makes the processor to be the latest to update the <b>cache</b> <b>block.</b>|$|E
5000|$|Shared(S): The <b>cache</b> <b>block</b> is valid, {{clean and}} may reside in {{multiple}} caches.|$|E
50|$|Data is {{transferred}} between memory and <b>cache</b> in <b>blocks</b> of fixed size, called cache lines or <b>cache</b> <b>blocks.</b> When a <b>cache</b> line is copied from memory into the cache, a cache entry is created. The cache entry {{will include the}} copied data {{as well as the}} requested memory location (called a tag).|$|R
40|$|Abstract—Yield {{enhancement}} {{through the}} acceptance of partially good chips is a well-known technique [1], [2], [3]. In this paper, we derive a yield model for single-chip VLSI processors with partially good on-chip cache. Also, we investigate how the yield enhancement of VLSI processors with on-chip CPU cache relates {{with the number of}} acceptable faulty <b>cache</b> <b>blocks,</b> the percentage of the cache area with respect to the whole chip area, and various manufacturing process parameters as defect densities and the fault clustering parameter. One of the main conclusions is that the maximum effective yield is achieved by accepting as good, caches with {{a very small number of}} faulty <b>cache</b> <b>blocks.</b> One of the main conclusions is that the maximum effective yield is achieved by accepting as good, processor chips containing caches with a very small number of faulty <b>cache</b> <b>blocks.</b> Index Terms—Fault tolerance, on-chip CPU caches, partially good chips, yield enhancement. æ...|$|R
40|$|Trace-driven {{simulations}} of numerical Fortran programs {{are used to}} study {{the impact of the}} parallel loop scheduling strategy on data prefetching in a shared memory multiprocessor with private data caches. The simulations indicate that to maximize memory performance it is important to schedule blocks of consecutive iterations to execute on each processor, and then to adaptively prefetch singleword <b>cache</b> <b>blocks</b> to match the number of iterations scheduled. Prefetching multiple single-word <b>cache</b> <b>blocks</b> on a miss reduces the miss ratio by approximately 5 to 30 percent compared to a system with no prefetching. In addition, the proposed adaptive prefetching scheme further reduces the miss ratio while significantly reducing the false sharing among <b>cache</b> <b>blocks</b> compared to nonadaptive prefetching strategies. Reducing the false sharing causes fewer coherence invalidations to be generated, and thereby reduces the total network traffic. The impact of the prefetching and scheduling strategies on th [...] ...|$|R
5000|$|Valid-Exclusive(V): The <b>cache</b> <b>block</b> is valid, {{clean and}} only resides in one cache.|$|E
50|$|Each <b>cache</b> <b>block</b> {{resides in}} one of the four states: exclusive-clean, shared-clean, shared-modified and modify.|$|E
5000|$|Sharer Node: One or many node {{which are}} sharing {{a copy of}} the <b>cache</b> <b>block.</b>|$|E
5000|$|No source file {{content is}} stored in the <b>block</b> <b>cache,</b> only <b>blocks</b> that consist of {{probably}} random data.|$|R
50|$|Computer memory caches usually {{operate on}} blocks of memory that consist of several {{consecutive}} words. These units are customarily called <b>cache</b> <b>blocks,</b> or, in CPU caches, cache lines.|$|R
30|$|A common {{method for}} {{avoiding}} data caches is an onchip memory called scratchpad memory, which is under program control. This program managed memory entails {{a more complicated}} programming model although it can be automatically partitioned [50, 51]. A similar approach for time-predictable caching is to lock <b>cache</b> <b>blocks.</b> The control of the cache locking [52] and the allocation of data in the scratchpad memory [53, 54] can be optimized for the WCET. A comparison between locked <b>cache</b> <b>blocks</b> and a scratchpad memory {{with respect to the}} WCET can be found in [55].|$|R
5000|$|PrRdHit: Processor side {{request to}} read a <b>cache</b> <b>block</b> that already resides in the cache.|$|E
5000|$|Flush: Request that {{indicates}} that a whole <b>cache</b> <b>block</b> is being written back to the memory.|$|E
5000|$|On a BusRd, the <b>cache</b> <b>block</b> is flushed {{onto the}} bus and state changes to Shared.|$|E
40|$|We present new {{performance}} {{models and}} a new, more compact data structure for <b>cache</b> <b>blocking</b> {{when applied to}} the sparse matrixvector multiply (SpMV) operation, y # y +A x. Prior work indicates that <b>cache</b> <b>blocked</b> SpMV performs very well for some matrix and machine combinations, yielding speedups as high as 3 x. We look at the general question of when and why performance improves, finding that <b>cache</b> <b>blocking</b> is most e#ective when simultaneously 1) x does not fit in cache, 2) y fits in cache, 3) the non-zeros are distributed throughout the matrix, and 4) the non-zero density is su#ciently high. We extend our prior performance models, which bounded performance by assuming x and y fit in cache, to consider these classes of matrices. Unlike our prior model, the updated models are accurate enough {{to use as a}} heuristic for predicting the optimum block sizes. We conclude with architectural suggestions that would make processor and memory systems more amenable to SpMV...|$|R
50|$|Suppose a direct-mapped L1 <b>cache</b> with <b>blocks</b> A, B {{pointing}} to the same set. It is linked to a 2 entry fully associative victim <b>cache</b> with <b>blocks</b> C, D in it.|$|R
40|$|We {{consider}} {{the problem of}} building high-performance implementations of sparse matrix-vector multiply (SpM×V), or y = y+A ·x, which is an important and ubiquitous computational kernel. Prior work indi-cates that <b>cache</b> <b>blocking</b> of SpM×V is extremely important for some matrix and machine combinations, with speedups as high as 3 x. In this paper we present a new, more compact data structure for <b>cache</b> <b>blocking</b> for SpM×V {{and look at the}} general question of when and why performance improves. <b>Cache</b> <b>blocking</b> appears to be most effective when simultaneously 1) the vector x does not fit in cache 2) the vector y fits in cache 3) the non zeros are distributed throughout the matrix and 4) the non zero density is sufficiently high. In particular we find that <b>cache</b> <b>blocking</b> does not help with band matrices no matter how large x and y are since the matrix structure already lends itself to the optimal access pattern. Prior work on performance modeling assumed that the matrices were small enough so that x and y fit in the cache. However when this is not the case, the optimal block sizes picked by these models may have poor performance motivating us to update these performance models. In contrast, the optimum block sizes predicted by the new performance models generally match the measured optimum block sizes and therefore the models {{can be used as a}} basis for a heuristic to pick the block size...|$|R
