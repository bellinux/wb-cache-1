19|96|Public
3000|$|... dev, {{from the}} {{development}} database {{are used to}} train the <b>calibration</b> <b>weights,</b> i.e. the intercept and regression coefficient of the logistic regression model, and subsequently these <b>calibration</b> <b>weights</b> can then be used to calibrate scores from the test database. The pooled procedure for calculating the <b>calibration</b> <b>weights</b> was adopted (refer to [19] for details) in this paper. For a detailed tutorial on logistic regression calculation in converting a score to an interpretable likelihood ratio, refer to [12].|$|E
40|$|Background and Purpose. The {{purpose of}} this study was to {{evaluate}} the reli-ability cf measurements of weight distribution among the wheels of wheelchairs using a commercial balance testing system. Reliable data may be useful in the wheelchair evaluation and adjustment process. Subjects. Three male full-time manual wheelchair users aged 30, 26, and 2 7 years with cervical spinal cord injuries 7. 5, 65, and 10 years in duration participated. Metbods. <b>Calibration</b> <b>weights,</b> unoccupied wheelchairs, and occupied wheelchairs were repeatedly placed o n the force transducers of the balance testing system to obtain measure-ments o f weight distribution. Results The intraclass correlation coeficients of the measurements were. 99 for <b>calibration</b> <b>weights,.</b> 96 for unoccupied wheelchairs, and. 98 for wheelchairs occupied by the subjects. Conclusion and Discussion. The described use of this instrumentation appears to generate reliable measure-ments o f static weight distribution. With further testing, this system may provide useful information related to manual wheelchair prescription and adjustment...|$|E
40|$|In the {{introductory}} chapter {{of this work}} is caught organizational structure of the national metrology system in the Czech Republic and its links to international organizations. There is indicated the basic terminology of metrology, particularly {{in the area of}} classification instruments. The following sections approaching the issue of measurement uncertainties, their classification, sources of uncertainty determined by the type A and B, their specifics and calculation. The above linked area already dealing with themselves calibrations, first of all <b>calibration</b> <b>weights,</b> classification of weights according accuracy classes, established procedures, and finally determining uncertainty in <b>calibration</b> <b>weights.</b> Then, immediately followed by a chapter dealing with calibration balances, performed tests and measurement uncertainties. The main part is of course directed towards the application of acquired knowledge to practical examples, thus performing the calibration weight class F 2 using a high-precision weights, both in the premises of the Technical University in Brno, both in the laboratory weighing the Czech Metrological Institute. Further calibration was performed school balances Ohaus Explorer EX 224...|$|E
5000|$|Kott, P. (2006). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonresponse and {{coverage}} errors. Survey Methodology, 133-142 ...|$|R
50|$|A mass used to {{calibrate}} a weighing scale {{is sometimes called}} a calibration mass or <b>calibration</b> <b>weight.</b>|$|R
40|$|<b>Calibration</b> <b>weighting</b> is a {{methodology}} under which probability-sample weights are adjusted {{in such a}} way that when applied to survey data they can produce model-unbiased estimators for a number of different target variables. This paper briefly reviews the history of <b>calibration</b> <b>weighting</b> before the term was coined and some major developments since then. A change in the definition of a calibration estimator is recommended. This change expands the class to include such special cases as, 1, randomization-optimal estimators, and, 2, randomization-consistent estimators incorporating local polynomial regression. Although originally developed as a method for reducing sampling errors, <b>calibration</b> <b>weighting</b> has also been applied to adjust for unit nonresponse and for coverage errors. A variant of the jackknife variance estimator proposed here should prove computationally convenient for these applications...|$|R
40|$|Sometimes {{benchmark}} constraints in a calibration problem {{cannot be}} met {{if there are}} range restric-tions on the <b>calibration</b> <b>weights.</b> There are various approaches to this problem that involve either allow-ing the benchmark constraints to be adjusted within a specified tolerance, or to determine a minimal lin-ear adjustment of the benchmark constraints. In this paper we propose an optimization problem that explicitly incorporates into the objective function {{a measure of the}} amount by which the benchmark con-straints are missed...|$|E
40|$|Calibration estimation, {{which can}} be roughly {{described}} as adjusting the original design weights to incorporate the known population totals of the auxiliary variables, has become very popular in sample surveys. The <b>calibration</b> <b>weights</b> are chosen to minimize a given distance measure while satisfying a set of constraints related to the auxiliary variable information. Under simple random sampling, Chen and Qin (1993) suggested that the calibration estimator maximizing the constrained empirical likelihood can make efficient use of the auxiliary variables. We extend the result to unequal probability sampling and propose an algorithm to implement the proposed procedure. Asymptotic properties of the proposed calibration estimator are discussed. The proposed method is extended to the stratified sampling. Results from a limited simulation study are presented...|$|E
40|$|In survey statistics, {{the usual}} {{technique}} for estimating a population total consists in summing appropriately weighted variable {{values for the}} units in the sample. Different weighting systems exit: sampling weights, GREG weights or <b>calibration</b> <b>weights</b> for example. In this article, we propose to use the inverse of conditional inclusion probabilities as weighting system. We study examples where an auxiliary information enables to perform an a posteriori stratification of the population. We show that, in these cases, exact computations of the conditional weights are possible. When the auxiliary information consists in the knowledge of a quantitative variable for all the units of the population, then we show that the conditional weights can be estimated via Monte-Carlo simulations. This method is applied to outlier and strata-Jumper adjustments...|$|E
5000|$|Kott, P., & Chang, T. (2010). Using <b>calibration</b> <b>weighting</b> {{to adjust}} for nonignorable unit nonresponse. Journal of the American Statistical Association, 105, 1265-1275.|$|R
40|$|The work is {{justified}} {{from the standpoint}} of system analysis using multimedia factors to improve rehabilitation patients at different stages of the disease. The first time the formation of the methodology vector priorities specialists in various fields of <b>calibration</b> <b>weight</b> judgments are used according to their professional expertise...|$|R
40|$|Fractional {{hot deck}} imputation, {{considered}} in Fuller and Kim (2005), is extended to multivariate missing data. The joint {{distribution of the}} study items is nonparametrically estimated using a discrete approximation, where the discrete transformation also serves to define imputation cells. The procedure first estimates the probabilities for the cells and then imputes real observations for missing items. <b>Calibration</b> <b>weighting</b> is used to reduce the imputation variance. Replication variance estimation is discussed...|$|R
40|$|We {{extend the}} problem of obtaining an {{estimator}} for the finite population mean parameter incorporating complete auxiliary information through calibration estimation in survey sampling but considering a functional data framework. The functional calibration sampling weights of the estimator are obtained by matching the calibration estimation problem with the maximum entropy on the mean principle. In particular, the calibration estimation is viewed as an infinite dimensional linear inverse problem following {{the structure of the}} maximum entropy on the mean approach. We give a precise theoretical setting and estimate the functional <b>calibration</b> <b>weights</b> assuming, as prior measures, the centered Gaussian and compound Poisson random measures. Additionally, through a simple simulation study, we show that our functional calibration estimator improves its accuracy compared with the Horvitz-Thompson estimator...|$|E
30|$|In {{order to}} elicit a stretch reflex {{the force of}} the pull and {{subsequent}} strech velocity of the tibial translation must be sufficient to activate the proprioceptors. The velocity of the pulls was generated as fast as possible by the tester (JA). For the force magnitude, Friemert et al. (2005 b) demonstrated that a force of[*]≥[*] 140  N and stretch velocity of 30  mm/s would increase the chance to elicit stretch reflex to 100  %. In a pilot experiment, the force of pull on the handle of the KT- 2000 measured using a strain gauge. The force output of the KT- 2000 was determined statically with <b>calibration</b> <b>weights</b> within the range of forces expected to be generated by the user. Similarly, the position output of the KT- 2000 was calibrated using ceramic test gauge blocks.|$|E
40|$|In the {{presence}} of a missing response, reweighting the complete case subsample by the inverse of nonmissing probability is both intuitive and easy to implement. When the population totals of some auxiliary variables are known and when the inclusion probabilities are known by design, survey statisticians have developed calibration methods for improving efficiencies of the inverse probability weighting estimators and the methods can be applied to missing data analysis. Model-based calibration has been proposed in the survey sampling literature, where multidimensional auxiliary variables are first summarized into a predictor function from a working regression model. Usually, one working model is being proposed for each parameter of interest and results in different sets of <b>calibration</b> <b>weights</b> for estimating different parameters. This paper considers calibration using multiple working regression models for estimating a single or multiple parameters. Contrary to a common belief that overfitting hurts efficiency, we present three rather unexpected results. First, when the missing probability is correctly specified and multiple working regression models for the conditional mean are posited, calibration enjoys an oracle property: the same semiparametric efficiency bound is attained as if the true outcome model is known in advance. Second, when the missing data mechanism is misspecified, calibration can still be a consistent estimator when any one of the outcome regression models is correctly specified. Third, a common set of <b>calibration</b> <b>weights</b> can be used to improve efficiency in estimating multiple parameters of interest and can simultaneously attain semiparametric efficiency bounds for all parameters of interest. We provide connections of a wide class of calibration estimators, constructed based on generalized empirical likelihood, to many existing estimators in biostatistics, econometrics and survey sampling and perform simulation studies to show that the finite sample properties of calibration estimators conform well with the theoretical results being studied. Comment: Published in at [URL] the Statistical Science ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Given a {{randomly}} drawn sample, <b>calibration</b> <b>weighting</b> {{can provide}} double {{protection against the}} selection bias resulting from unit nonresponse. This means that if either an assumed linear prediction model or an implied unit selection model holds, the resulting estimator will be asymptotically unbiased in some sense. The functional form of the selection model when using linear alibration adjustment is dubious. The authors discuss an alternative, nonlinear calibration-weighting procedure and software that can, among other things, implicitly estimate a logistic-response model. " (author's abstract...|$|R
40|$|Weighting {{procedures}} are commonly applied in surveys {{to compensate for}} nonsampling errors such as nonresponse errors and coverage errors. Two types of weight-adjustment {{procedures are}} commonly used {{in the context of}} unit nonresponse: (i) nonresponse propensity <b>weighting</b> followed by <b>calibration,</b> also known as the two-step approach and (ii) nonresponse <b>calibration</b> <b>weighting,</b> also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented...|$|R
40|$|Parametric {{fractional}} imputation {{is proposed}} {{as a general}} tool for missing data analysis. Using fractional weights, the observed likelihood can be approximated by the weighted mean of the imputed data likelihood. Computational efficiency can be achieved using the idea of importance sampling and <b>calibration</b> <b>weighting.</b> The proposed imputation method provides efficient parameter estimates for the model parameters specified in the imputation model and also provides reasonable estimates for parameters that {{are not part of}} the imputation model. Variance estimation is discussed and results from a limited simulation study are presented. Copyright 2011, Oxford University Press. ...|$|R
40|$|Due to {{unbalanced}} speed-density observations, the one-regime traffic {{fundamental diagram}} and speed-density relationship models using {{least square method}} (LSM) cannot reflect actual conditions under congested/jam traffic. In that case, it is inevitable to adopt the weighted least square method (WLSM). This paper used freeway Georgia State Route 400 observation data and proposed 5 weight determination methods except the LSM to analyse 5 wellknown one-regime speed-density models {{to determine the best}} calibrating models. The results indicated that different one-regime speed-density models have different best calibrating models, for Greenberg, it was possible to find a specific weight using LSM, which is similar for Underwood and Northwestern Models, but different for that one known as 3 PL model. An interesting case is the Newell 2 ̆ 7 s Model which fits well with two distinct <b>calibration</b> <b>weights.</b> This paper can make contribution to calibrating a more precise traffic fundamental diagram...|$|E
40|$|The {{first data}} from the ATLAS {{detector}} at the Large Hadron Collider (LHC) is due to be collected later this year. This first phase will {{play a vital role}} in understanding the detector and its response, in-situ. Jet reconstruction is important for identifying new physics as well as making precision measurements of standard model physics. The fine granularity of the ATLAS calorimeters can be used to gain information about a jet's shape and the classification of energy deposits, which allows a better estimate of the jet energy to be made and in particular correction for the non-compensating nature of the calorimeter using so-called <b>calibration</b> <b>weights.</b> The classification algorithm and weights are presently calculated using simulation. In this presentation we describe an important step in the validation of ATLAS's jet calibration using charged tracks reconstructed in the inner detector and their inter-calibration with the clusters reconstructed in the calorimeters...|$|E
40|$|Quantitative expert {{judgement}} {{is used in}} many areas of risk analysis to provide assessments of uncertainty. A leading method is Cooke’s classical model, which provides a way of weighting experts depending on their performance in answering so-called calibration questions. The moment method for {{expert judgement}} combination is an alternative to Cooke’s method which provides a different way of calibrating experts. It has better theoretical properties in that it uses a strictly proper scoring rule, that is, an expert who wishes to maximise his score can only do so by stating what he actually believes. The method also requires the specification of weights for the calibration process. In this paper we consider a special case of the moment model that scores location and variability assessments. We provide an approach to setting the <b>calibration</b> <b>weights</b> and repeat the comparison of this moment model with Cooke’s classical model we conducted earlier for this special case...|$|E
50|$|Phillip S. Kott (born 1952) is an American statistician. He {{has worked}} in the field of survey {{statistics}} for more than 25 years, and is regarded as a leader in this field. His areas of expertise include survey sampling design, analysis of survey data, and <b>calibration</b> <b>weighting,</b> among other areas. He revolutionized sampling design and estimation strategies with the Agricultural Resource Management Survey, which uses survey information more efficiently. He has taught at George Mason University, and USDA Graduate School. He is currently an Associate Editor for the Journal of Official Statistics and the scientific journal Survey Methodology.|$|R
50|$|As Agriculture Commissioner, Perry was {{responsible}} for promoting the sale of Texas farm produce to other states and foreign nations, and for supervising the <b>calibration</b> of <b>weights</b> and measures, such as gasoline pumps and grocery store scales.|$|R
40|$|An {{experiment}} was designed (a) {{to examine the}} weight losses caused by Sitophilus feeding on maize cultures which contained different proportions (0 %, 5 % and 15 %) of broken grains and (b) to assess and compare the efficiency of different weight loss assessment methods on the different cultures. The weight loss assessment methods examined were the Count and Weigh, the Converted Percentage Damage, the Simple and Multiple Thousand Grain Mass (TGM) methods and the Standard Volume Weight (SVW) methods by direct comparison and by reference to a baseline <b>calibration.</b> <b>Weight</b> losses in cultures containing different proportions of broken grains were not significantly different. Further work is recommended using pests with secondary status...|$|R
40|$|The {{validity}} of design-based inference is {{not dependent on}} any model assumption. However, {{it is well known}} that estimators derived through design-based theory may be inefficient for the estimation of population totals when the design weights are weakly related to the variables of interest and have widely dispersed values. We propose estimators that have the potential to improve the efficiency of any estimator derived under the design-based theory. Our main focus is limited to the improvement of the Horvitz [...] Thompson estimator, but we also discuss the extension to calibration estimators. The new estimators are obtained by smoothing design or <b>calibration</b> <b>weights</b> using an appropriate model. Our approach to inference requires the modelling of only one variable, the weight, and it leads to a single set of smoothed weights in multipurpose surveys. This is to be contrasted with other model-based approaches, such as the prediction approach, in which it is necessary to postulate and validate a model for each variable of interest leading potentially to variable-specific sets of weights. Our proposed approach is first justified theoretically and then evaluated through a simulation study. Copyright 2008, Oxford University Press. ...|$|E
40|$|Bayesian {{methods for}} {{inference}} on finite population means and other parameters by using sample survey data face hurdles {{in all three}} phases of the inferential procedure: the formulation of a likelihood function, {{the choice of a}} prior distribution and the validity of posterior inferences under the design-based frequentist framework. In the case of independent and identically distributed observations, the profile empirical likelihood function of the mean and a non-informative prior on the mean can be used as the basis for inference on the mean and the resulting Bayesian empirical likelihood intervals are also asymptotically valid under the frequentist set-up. For complex survey data, we show that a pseudo-empirical-likelihood approach can be used to construct Bayesian pseudo-empirical-likelihood intervals that are asymptotically valid under the design-based set-up. The approach proposed compares favourably with a full Bayesian analysis under simple random sampling without replacement. It is also valid under general single-stage unequal probability sampling designs, unlike a full Bayesian analysis. Moreover, the approach is very flexible in using auxiliary population information and can accommodate two scenarios which are practically important: incorporation of known auxiliary population information for the construction of intervals by using the basic design weights; calculation of intervals by using <b>calibration</b> <b>weights</b> based on known auxiliary population means or totals. Copyright (c) 2010 Royal Statistical Society. ...|$|E
40|$|Summary. Bayesian {{methods for}} {{inference}} on finite population means and other parameters by using sample survey data face hurdles {{in all three}} phases of the inferential procedure: the formulation of a likelihood function, {{the choice of a}} prior distribution and the validity of posterior inferences under the design-based frequentist framework. In the case of independent and identically distributed observations, the profile empirical likelihood function of the mean and a non-informative prior on the mean can be used as the basis for inference on the mean and the resulting Bayesian empirical likelihood intervals are also asymptotically valid under the frequentist set-up. For complex survey data, we show that a pseudo-empirical-likelihood approach can be used to construct Bayesian pseudo-empirical-likelihood intervals that are asymptotically valid under the design-based set-up. The approach proposed compares favourably with a full Bayesian analysis under simple random sampling without replacement. It is also valid under general single-stage unequal probability sampling designs, unlike a full Bayesian analysis. Moreover, the approach is very flexible in using auxiliary population information and can accommodate two scenarios which are practically important: incorporation of known auxiliary population information for the construction of intervals by using the basic design weights; calculation of intervals by using <b>calibration</b> <b>weights</b> based on known auxiliary population means or totals...|$|E
40|$|The use of nonresponse <b>calibration</b> <b>weighting</b> is {{considered}} {{in a complete}} design-based frameworkto account for the cases in which nonresponse is a fixed characteristic of the units, just like the interest variable. Approximate expressions of design-based bias and variance of the calibration estimator are derived and some estimators of the sampling variance are proposed. The choice of auxiliary variables is discussed from theoretical and practical point of view. The results of an extensive simulation study demonstrate how {{the reliability of the}} procedure is mainly determined by the capability of selecting auxiliary variables {{in such a way that}} their relationship with the interest variable is similar for both the respondent and nonrespondent sub-populations. auxiliary variables, calibration estimator, variance estimator, simulation study. ...|$|R
40|$|A {{new snow}} layer probing device with a {{rectangular}} section {{is described in}} the paper. This device allows receiving snow samples with the undisturbed structures of snow layers, and more accurately reflects the actual value of the snow water equivalent. Application of a new snow probing device in snow measurements provides {{a new way to}} organize the monitoring dynamics of snow density layer variability, to estimate density of certain snow types in the snow cover, including the total or vertically averaged one, and to visualize the features of the snow stratigraphy structure. Snow layer probing device with a rectangular section as opposed to the cylindrical weight snow devices has constructive advantages. It also can be used in the <b>calibration</b> <b>weight</b> snow measurement devices that will ensure the comparability of the snow shooting results made in different climatic zones. </p...|$|R
40|$|Abstract: Work {{with sample}} surveys often makes {{extensive}} use of measures of size. Two prominent examples are the use of “probability proportional to size ” sampling; and use of size measures in adjustment of survey weights through, e. g., ratio estimation, post-stratification or <b>calibration</b> <b>weighting.</b> However, many survey applications use size variables that are imperfect approximations to the idealized size measures that would produce optimal efficiency results. This paper explores the effects that alternative size measures may have on the efficiency of some standard design-estimator pairs. Principal {{emphasis is placed on}} numerical results of a simulation study that uses size measures and economic variables available through the Quarterly Census of Employment and Wages of the Bureau of Labor Statistics. Key words: measures of size; ratio estimation; regression estimation; sampling with probabilities proportional to size; unequal-probability sampling. 1...|$|R
40|$|Although {{survey data}} are {{sometimes}} weighted by their selection weights, {{it is often}} preferable to use auxiliary information available on the whole population to improve estimation. Calibration weighting (Deville and Sarndal, 1992, Journal of the American Statistical Association 87 : 376 - 382) {{is one of the}} most common methods of doing this. This method adjusts the selection weights so that known population totals for the auxiliary variables are reproduced exactly, while ensuring that the calibrated weights are as close as possible to the original sampling weight. The simplest example of calibration is poststratification. This is the special case where the auxiliary variable is a single categorical variable. General calibration extends this to deal with more than one auxiliary variable and allows the user to include both categorical and numerical variables. A typical example might occur in a population survey, where the selection weights could be calibrated to ensure that the sample weighted by the <b>calibration</b> <b>weights</b> has exactly the same distribution as the population on variables such as age, sex, and region. Many packages have routines for calibration. SAS has the macro CALMAR; GenStat has the procedure SVCALIBRATE; and R has the function calibrate. However, no such routine is publicly available in Stata. I will introduce a user-written Stata program for calibration and will also discuss a simple extension to show how it can incorporate a nonresponse correction. I will also briefly discuss the program's strengths and limitations when compared to rival packages. ...|$|E
40|$|When we {{estimate}} the population total for a survey variable or variables, calibration forces the weighted estimates of certain covariates to match known or alternatively estimated population totals called benchmarks. Calibration {{can be used}} to correct for sample-survey nonresponse, or for coverage error resulting from frame undercoverage or unit duplication. The quasi-randomization theory supporting its use in nonresponse adjustment treats response as an additional phase of random sampling. The functional form of a quasi-random response model is assumed to be known, its parameter values estimated implicitly through the creation of <b>calibration</b> <b>weights.</b> Unfortunately, calibration depends upon known benchmark totals while the covariates in a plausible model for survey response may not be the benchmark covariates. Moreover, it may be prudent to keep the number of covariates in a response model small. We use calibration to adjust for nonresponse when the benchmark model and covariates may differ, provided the number of the former is at least as great as that of the latter. We discuss the estimation of a total for a vector of survey variables that do not include the benchmark covariates, but that may include some of the model covariates. We show how to measure both the additional asymptotic variance due to the nonresponse in a calibration-weighted estimator and the full asymptotic variance of the estimator itself. All variances are determined with respect to the randomization mechanism used to select the sample, the response model generating the subset of sample respondents, or both. Data from the U. S. National Agricultural Statistical Service's 2002 Census of Agriculture and simulations are used to illustrate alternative adjustments for nonresponse. The paper concludes with some remarks about adjustment for coverage error. Copyright 2008, Oxford University Press. ...|$|E
40|$|Main abstract: Fluctuation scaling {{reports on}} all {{processes}} producing a data set. Some fluctuation scaling relationships, {{such as the}} Horwitz curve, follow exponential dispersion models which have useful properties. The mean-variance method applied to Poisson distributed data is a special case of these properties allowing the gain of a system to be measured. Here, a general method is described for investigating gain (G), dispersion (β), and process (α) in any system whose fluctuation scaling follows a simple exponential dispersion model, a segmented exponential dispersion model, or complex scaling following such a model locally. When gain and dispersion cannot be obtained directly, relative parameters, GR and βR, may be used. The method was demonstrated on data sets conforming to simple, segmented, and complex scaling. These included mass, fluorescence intensity, and absorbance measurements and specifications for classes of <b>calibration</b> <b>weights.</b> Changes in gain, dispersion, and process were observed in the scaling of these data sets in response to instrument parameters, photon fluxes, mathematical processing, and calibration weight class. The process parameter which limits the type of statistical process that can be invoked to explain a data set typically exhibited 0 4 possible. With two exceptions, calibration weight class definitions only affected β. Adjusting photomultiplier voltage while measuring fluorescence intensity changed all three parameters (0 <α< 0. 8; 0 <βR< 3; 0 <GR< 4. 1). The method provides a framework for calibrating and interpreting uncertainty in chemical measurement allowing robust compar ison of specific instruments, conditions, and methods. Supporting information abstract: On first inspection, fluctuation scaling data may appear to approximate a straight line when log transformed. The data presented in figure 5 of the main text gives a reasonable approximation to a straight line and for many purposes this would be sufficient. The {{purpose of the study}} of fluorescence intensity was to determine whether adjusting the voltage of a photomultiplier tube while measuring a fluorescent sample changes the process (α), the dispersion (β) and/or the gain (G). In this regard, the linear model established that PMT setting affects more than the gain. However, a detailed analysis beginning with testing for model mis-specification provides additional information. Specifically, Poisson behavior is only seen over a limited wavelength range in the 600 V and 700 V data sets...|$|E
40|$|Unit nonresponse {{is often}} a problem in sample surveys. It arises when {{the values of the}} survey {{variable}} cannot be recorded for some sampled units. In this paper, the use of nonresponse <b>calibration</b> <b>weighting</b> to treat nonresponse is considered in a complete design-based framework. Nonresponse is viewed as a fixed characteristic of the units. The approach is suitable in environmental and forest surveys when sampled sites cannot be reached by field crews. Approximate expressions of design-based bias and variance of the calibration estimator are derived and design-based consistency is investigated. Choice of auxiliary variables to perform calibration is discussed. Sen–Yates–Grundy, Horvitz-Thompson, and jackknife estimators of the sampling variance are proposed. Analytical and Monte Carlo results demonstrate the validity of the procedure when the relationship between survey and auxiliary variables is similar in respondent and nonrespondent strata. An application to a forest survey performed in Northeastern Italy is considered...|$|R
40|$|Maximum entropy {{and minimum}} {{cross-entropy}} estimation are applica- ble {{when faced with}} ill-posed estimation problems. I introduce a Stata command that estimates a probability distribution using a maximum entropy or minimum cross-entropy criterion. I show how this command {{can be used to}} calibrate survey data to various population totals. Copyright 2010 by StataCorp LP. maxentropy, maximum entropy, minimum cross-entropy, survey <b>calibration,</b> sample <b>weights...</b>|$|R
40|$|This article {{describes}} a two-step calibration-weighting scheme for a stratified simple {{random sample of}} hospital emergency departments. The first step adjusts for unit nonresponse. The second increases the statistical efficiency of most estimators of interest. Both use a measure of emergency-department size and other useful auxiliary variables contained in the sampling frame. Although many survey variables are roughly a linear function of the measure of size, response is better modeled {{as a function of}} the log of that measure. Consequently the log of size is a calibration variable in the nonresponse-adjustment step, while the measure of size itself is a calibration variable in the second calibration step. Nonlinear calibration procedures are employed in both steps. We show with 2010 DAWN data that estimating variances as if a one-step <b>calibration</b> <b>weighting</b> routine had been used when there were in fact two steps can, after appropriately adjusting the finite-population correct in some sense, produce standard-error estimates that tend to be slightly conservative...|$|R
