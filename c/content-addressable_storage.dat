19|12|Public
2500|$|In {{the real}} world the key [...] could be a hash of a file's content rather than a hash of a file's name to provide <b>content-addressable</b> <b>storage,</b> so that {{renaming}} of the file does not prevent users from finding it.|$|E
50|$|<b>Content-addressable</b> <b>storage,</b> also {{referred}} to as associative storage or abbreviated CAS, is a mechanism for storing information that can be retrieved based on its content, not its storage location. It is typically used for high-speed storage and retrieval of fixed content, such as documents stored for compliance with government regulations. Roughly speaking, <b>content-addressable</b> <b>storage</b> is the permanent-storage analogue to content-addressable memory.|$|E
50|$|Centera, {{debuted in}} 2002. The {{technology}} called <b>content-addressable</b> <b>storage,</b> was developed at Filepool, acquired by EMC Corporation in 2001.|$|E
50|$|Arvados Keep is an {{open source}} <b>content-addressable</b> {{distributed}} <b>storage</b> system. It is designed for large-scale, computationally intensive data science work such as storing and processing genomic data.|$|R
50|$|Permabit’s first product, Permabit Enterprise Archive (originally {{known as}} Permeon) was a multi-PB scalable, <b>content-addressable,</b> scale-out <b>storage</b> product, first {{launched}} in 2004. Enterprise Archive utilized in-house developed technologies {{in the areas}} of capacity optimization, WORM, storage management and data protection.|$|R
50|$|BitVault is a <b>content-addressable</b> {{distributed}} <b>storage</b> system, {{developed by}} Microsoft Research in China. BitVault uses peer-to-peer technology {{to distribute the}} tasks of storing and managing data. As such, there is no central authority responsible for management of the system. Rather, it is self-managing, provides high availability, reliability and scales up in a self-organizing manner, with low administrative overhead, which is almost constant irrespective {{of the size of}} the distributed overlay network.|$|R
5000|$|Magnetic {{disk storage}} with special {{software}} protection against overwriting, erasure, and editing; delivers security similar to optical write-once, read-many media. This category includes <b>content-addressable</b> <b>storage.</b>|$|E
5000|$|... git-annex is a {{distributed}} file synchronization {{system which}} uses <b>content-addressable</b> <b>storage</b> for files it manages. It relies on Git and symbolic links to index their filesystem location.|$|E
5000|$|Camlistore is {{a recent}} project to bring the {{advantages}} of <b>content-addressable</b> <b>storage</b> [...] "to the masses". It {{is intended to be}} used {{for a wide variety of}} use cases, including distributed backup; a snapshotted-by-default, version-controlled filesystem; and decentralised, permission-controlled filesharing.|$|E
40|$|In this paper, {{we report}} the design, {{implementation}} and experimental results of Machine Bank, a system engineered towards the popular shared-lab scenario, where users outnumber available PCs and may get different PCs in different sessions. Machine Bank {{allows users to}} preserve their entire working environment across sessions. Each client runs virtual machine, which is saved to and reinstantiated from a <b>content-addressable</b> backend <b>storage.</b> We carefully designed lightweight hooks at client side that implements caching and tracking logics to improve reinstantiation speed {{as well as to}} remove unnecessary network and disk traffic. Our detailed evaluation demonstrates that these techniques are effective, and the overall performance fits well with the shared-lab usage. 1...|$|R
50|$|Camlistore (<b>Content-Addressable</b> Multi-Layer Indexed <b>Storage)</b> {{is a set}} of {{open source}} formats, protocols, and {{software}} for modeling, storing, searching, sharing and synchronizing data. Influenced by the relative short time blogging and storage platforms typically stay around compared to family heirlooms passed along such as photograph albums. Unlike many social online storage systems that default to public sharing, Camlistore requires personal hosting and sharing defaults to private.|$|R
40|$|Abstract. Content-addressability is a {{fundamental}} feature of human memory underlying many associative information retrieval tasks. In contrast to location-based memory devices, content-addressable memories require complex interactions between memory elements, which makes conventional computation paradigms difficult. Here we present a molecular computational model of <b>content-addressable</b> information <b>storage</b> and retrieval which makes use of the massive interaction capability of DNA molecules in a reaction chamber. This model {{is based on the}} “hypernetwork” architecture which is an undirected hypergraph of weighted edges. We describe the theoretical basis of the hypernetwork model of associative memory and its realization in DNA-based computing. A molecular algorithm is derived for automatic storage of data into the hypernetwork, and its performance is examined on an image data set. In particular, we study the effect of the hyperedge cardinality and error tolerance on the associative recall performance. Our simulation results demonstrate that short DNA strands in a vast number can be effective in some pattern information processing tasks whose implementation is within reach of current DNA nanotechnology. ...|$|R
5000|$|In {{the real}} world the key [...] could be a hash of a file's content rather than a hash of a file's name to provide <b>content-addressable</b> <b>storage,</b> so that {{renaming}} of the file does not prevent users from finding it.|$|E
50|$|OCFA {{consists}} of a back end for the Linux platform, it uses a PostgreSQL database for data storage, a custom <b>Content-addressable</b> <b>storage</b> or CarvFS based data repository and a Lucene index. The front end for OCFA {{has not been made}} publicly available due to licensing issues.|$|E
5000|$|Content-addressable: Each {{individually}} accessible unit {{of information}} is selected based {{on the basis of}} (part of) the contents stored there. <b>Content-addressable</b> <b>storage</b> can be implemented using software (computer program) or hardware (computer device), with hardware being faster but more expensive option. Hardware content addressable memory is often used in a computer's CPU cache.|$|E
40|$|We propose and {{demonstrate}} {{the use of}} phase images for <b>content-addressable</b> holographic data <b>storage.</b> Use of binary phase-based data pages with 0 and p phase changes, produces uniform spectral distribution at the Fourier plane. The absence of strong DC component at the Fourier plane and more intensity of higher order spatial frequencies facilitate better recording of higher spatial frequencies, and improves the discrimination capability of the contentaddressable memory. This improves {{the results of the}} associative recall in a holographic memory system, and can give low number of false hits even for small search arguments. The phase-modulated pixels also provide an opportunity of subtraction among data pixels leading to better discrimination between similar data pages...|$|R
40|$|Neural {{networks}} used as content-addressable memories show unequaled retrieval {{and speed}} capabilities in problems srreh as vision and pattern recognition. We propose a new {{implementation of a}} VLSI fully interconnected neural network with only two binary memory points per synapse. The small area of single synaptic cells allows implementation of neural networks with hundreds of neurons. Classical learning algorithms like the Hebb’s rule show a poor storage capacity, especially in VLSI neural networks where {{the range of the}} synapse weights is limited by the number of memory points contained in each connectiorq we propose a new algorithm for programming a Hopfield neuraf network as a high-storage <b>content-addressable</b> memory. The <b>storage</b> capacity obtained with this algorithm is very promising for pattern recognition applications...|$|R
30|$|The privacy {{protection}} of the WMSN is a challenging research hotspot {{due to the lack}} of related Internet of Things (IoT) standards [4, 5]. Blockchain is usually used as a basic decentralized technology for encrypting digital currency, such as Bitcoin and Ethereum. In general, IoT is a centralized distributed structure. The centralized IoT data management and access control model has many problems, especially the scalability issues of IoT systems, forcing users to trust third-party intermediaries to manage their data [14]. But the literature [7] presented a novel decentralized privacy-preserving access control model based on blockchain technology in IoT. So, for protecting privacy data and sensitive data on the opened IoT of WMSN, it provided us a decentralized and secure technical guarantee example [7]. But there is still a problem because that many blockchain technologies such as Bitcoin and Ethereum do not provide decentralized data storage capabilities. So, we must borrow the InterPlanetary File System (IPFS) because it provides us a high-throughput <b>content-addressable</b> block <b>storage</b> model with <b>content-addressable</b> hyperlinks [15], which forms a generic Melker-DAG. IPFS is well integrated with blockchain technology [3, 5, 7, 14, 16]. Therefore, the decentralized security and {{privacy protection}} model based on IPFS data storage technology has become one of the research hotspots of the WMSN Internet of Things.|$|R
5000|$|... git-annex uses git {{to index}} files {{but does not}} store them in the git history. Instead a Symbolic link {{representing}} and linking to the probably large file is committed. git-annex manages a <b>content-addressable</b> <b>storage</b> for the files under its control. A separate git branch logs the location of every file. Thus users can clone a git-annex repository and then decide for every file whether to make it locally available.|$|E
50|$|NEC HYDRAstor is a disk-based grid {{storage system}} with data {{deduplication}} for backups and archiving, developed by NEC Corporation. A HYDRAstor storage {{system can be}} composed of multiple nodes, starting from one up to 100+ nodes. Each node contains standard hardware including disk drives, CPU, memory and network interfaces and is integrated with the HYDRAstor software into a single storage pool. HYDRAstor software incorporates multiple features of distributed storage systems: <b>content-addressable</b> <b>storage,</b> global data deduplication, variable block size, Rabin fingerprinting, erasure codes, data encryption and load balancing.|$|E
50|$|Camlistore's <b>content-addressable</b> <b>storage</b> uses a {{mutation}} model {{based on}} GPG signed claims called Permanodes. Permanodes provide modeling, storing, searching, sharing & syncing {{data in the}} post-PC era. Camlistore {{can be thought of}} as Git for general personal information storage: a user's Camlistore is the master repository. Camlistore is graph-based rather than a working tree in data model terminology: Entities are permanodes and relations between them are claims on the permanodes. Entities are named by their cryptographic hashes. Search is enabled over permanodes & the claims. While Camlistore can store files like a traditional filesystem, it specializes in storing higher-level objects (pictures, videos).|$|E
40|$|HYDRAstor is a scalable, {{secondary}} storage solution {{aimed at the}} enterprise market. The system consists of a back-end architectured as a grid of storage nodes built around a distributed hash table; and a front-end consisting of a layer of access nodes which implement a traditional file system interface and can be scaled in number for increased performance. This paper concentrates on the back-end which is, to our knowledge, the first commercial implementation of a scalable, high-performance <b>content-addressable</b> secondary <b>storage</b> delivering global duplicate elimination, per-block user-selectable failure resiliency, selfmaintenance including automatic recovery from failures with data and network overlay rebuilding. The back-end programming model {{is based on an}} abstraction of a sea of variable-sized, content-addressed, immutable, highly-resilient data blocks organized in a DAG (directed acyclic graph). This model is exported with a low-level API allowing clients to implement new access protocols and to add them to the system on-line. The API has been validated with an implementation of the file system interface. The critical factor for meeting the design targets has been the selection of proper data organization based on redundant chains of data containers. We present this organization in detail and describe how it is used to deliver required data services. Surprisingly, the most complex to deliver turned out to be on-demand data deletion, followed (not surprisingly) by the management of data consistency and integrity...|$|R
40|$|The {{class of}} {{recurrent}} networks known as attractor networks {{is known to}} exhibit behaviors relevant to modeling human memory processes – notably <b>content-addressable</b> memory, <b>storage</b> of repeated inputs as stable patterns (under Hebbian learning), and maintenance of information (as activity) over time. In addition, these networks provide a natural account {{of the effect of}} similarity on interference in recall. However when looked at in finer detail there are some ways in which traditional attractor networks fail as models of human short-term memory. In particular, information in human short-term memory decays over time unless it is rehearsed, rather than remaining indefinitely. Also, under Hebbian learning traditional attractor networks have particular trouble learning correlated patterns. Here we investigate some variations on the classic framework which make it more appropriate for modeling human STM. We show (1) how adjusting the threshold of continuously-valued units can lead to networks which maintain activity information temporarily, but decay over time, (2) how noise in learning and/or input leads the similarity structure of the set of stored patterns to be reflected in the distribution of recall errors, and (3) how adding a timedelayed anti-correlative component to the learning rule provides robustness against highly correlated patterns and varying levels of input. These ideas have been incorporated in a model of serial recall that explains many aspects of human behavior on that task, and have also been used in newer simulations that learn temporal properties of the environment...|$|R
40|$|Highly {{reliable}} storage systems ensure {{protection of}} application data from loss. Reliability {{comes at the}} cost of expensive resources such as storage and bandwidth. In addition, challenges in data management are exacerbated due to the exponentially growing volumes of data in today 2 ̆ 7 s digital world. <b>Content-addressable</b> <b>storage</b> systems mitigate some of these problems by providing automatic mechanisms for detecting and deleting duplicate data in the storage infrastructure. ^ In this thesis, we provide several solutions that simplify the process of storage management. Specifically, we provide solutions that improve the scalability, performance and usability of highly-reliable <b>content-addressable</b> <b>storage</b> systems. We present an object partitioning technique called fingerdiff that improves duplicate elimination and scalability. Next, we present a block-level system called BLESS that enables primary storage applications such as file systems to continuously and transparently protect their data on a highly reliable content-addressable store. We evaluate BLESS using various workloads and demonstrate its efficacy in improving duplicate elimination, while reducing data protection costs. ...|$|E
40|$|The CernVM-File System (CernVM-FS) {{provides}} a scalable, reliable and low-maintenance software distribution service. It {{was developed to}} assist scientific collaborations to deploy software on the (geographically) distributed computing infrastructures used to run data processing applications. CernVM-FS is implemented as a POSIX read-only file system in user space (a FUSE module). Files and directories are hosted on standard web servers and mounted in the universal namespace /cvmfs. Internally, CernVM-FS uses <b>content-addressable</b> <b>storage</b> and Merkle trees {{in order to maintain}} file data and meta-data. CernVM-FS uses outgoing HTTP connections only, thereby it avoids most of the firewall issues of other network file systems. It transfers data and meta-data on demand and verifies data integrity by cryptographic hashes...|$|E
40|$|The fuzzy file block {{matching}} technique (fuzzy matching for short), {{was first}} proposed for opportunistic use of Content Addressable Storage. Fuzzy matching aims {{to increase the}} hit ratio in the <b>content-addressable</b> <b>storage</b> providers, and thus can improve the performance of underlying distributed file storage systems by potentially saving significant network bandwidth and reducing file transmission costs. Fuzzy matching employs shingling to represent the fuzzy hashing of file blocks for similarity detection, and error-correcting information to reconstruct the canonical content of a file block from some similar blocks. In this paper, we present the implementation details of fuzzy matching and a very basic evaluation of its performance. In particular, we show that fuzzy matching can recover new versions of GNU Emacs source from older versions. ...|$|E
40|$|We {{present the}} Deep Store archival storage architecture, a {{large-scale}} storage system that stores immutable data efficiently and reliably {{for long periods}} of time. Archived data is stored across a cluster of nodes and recorded to hard disk. The design differentiates itself from traditional file systems by eliminating redundancy within and across files, distributing content for scalability, associating rich metadata with content, and using variable levels of replication based on the importance or degree of dependency of each piece of stored data. We evaluate the foundations of our design, including PRESIDIO, a virtual <b>content-addressable</b> <b>storage</b> framework with multiple methods for inter-file and intra-file compression that effectively addresses the data-dependent variability of data compression. We measure content and metadata storage efficiency, demonstrate the need for a variabledegree replication model, and provide preliminary results for storage performance. 1...|$|E
40|$|Distributed <b>content-addressable</b> <b>storage</b> systems use self-verifying data {{to protect}} data {{integrity}} and to enable graceful scaling. One feature commonly missing from these systems, however, {{is the ability}} to identify the owner of a piece of data in a non-repudiable manner. While a solution that associates a certificate with each block of data is conceptually simple, researchers have traditionally claimed that the cost of creating and maintaining certificates is too great. In this paper, we demonstrate that systems can, in fact, efficiently map data to its owner in a secure and nonrepudiable fashion. To reduce the cost of creating and maintaining certificates, we extend the traditional contentaddressable interface to allow the aggregation of many small data blocks into larger containers. The aggregation is performed in a way that also supports self-verifying data at the granularity of the block and container, fine-granularity access, and incremental updates. We describe two prototype implementations and present preliminary performance results from deployments on PlanetLab and a local cluster. ...|$|E
40|$|Abstract: NAND flash-based {{solid-state}} drives (SSDs) {{are increasingly}} being deployed in storage systems at different levels such as buffer-caches and even secondary storage. However, the poor reliability and performance offered by these SSDs for write-intensive workloads continues to be their key shortcoming. Several solutions based on traditionally popular notions of temporal and spatial locality help reduce write traffic for SSDs. However, another form of locality- value locality- has remained completely unexplored. Value locality implies that certain data items (i. e., “values, ” not just logical addresses) {{are likely to be}} accessed preferentially. Given evidence for the presence of significant value locality in real-world workloads, we design CA-SSD which employs <b>content-addressable</b> <b>storage</b> (CAS) to exploit such locality. Our CA-SSD design employs enhancements primarily in the flash translation layer (FTL) with minimal additional hardware, suggesting its feasibility. Using three real-world workloads with content information, we devise statistical characterizations of two aspects of value locality- value popularity and temporal value locality- that form the foundation of CA-SSD. We observe that CA-SSD is able to reduce average response times by about 59 - 84 % compared to traditional SSDs. Even for workloads with little or no value locality, CA-SSD continues to offer comparable performance to a traditional SSD. Our findings advocate adoption of CAS in SSDs, paving the way {{for a new generation of}} these devices. ...|$|E
40|$|Scalable, highly {{reliable}} distributed systems supporting data deduplication have recently become popular for storing backup and archival data. One {{of the important}} requirements for backup storage {{is the ability to}} delete data selectively. Unlike in traditional storage systems, data deletion in distributed systems with deduplication is a major challenge because deduplication leads to mul-tiple owners of data chunks. Moreover, system config-uration changes often due to node additions, deletions and failures. Expected high performance, high availabil-ity and low impact of deletion on regular user operations additionally complicate identification and reclamation of unnecessary blocks. This paper describes a deletion algorithm for a scal-able, <b>content-addressable</b> <b>storage</b> with global dedupli-cation. The deletion is concurrent: user reads and writes can proceed in parallel with deletion with only mi-nor restrictions established to make reclamation feasible. Moreover, our approach allows for deduplication of user writes during deletion. We extend traditional distributed reference counting to deliver a failure-tolerant deletion that accommodates not only deduplication, but also the dynamic nature of a scalable system and its physical re-source constraints. The proposed algorithm has been ver-ified with an implementation in a commercial dedupli-cating storage system. The impact of deletion on user operations is configurable. Using a default setting that grants deletion maximum 30 % of system resources run-ning the deletion reduces end performance by not more that 30 %. This impact can be reduced to less than 5 % when deletion is given only minimal resources. ...|$|E

