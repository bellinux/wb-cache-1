22|543|Public
40|$|Contrary {{to recent}} claims, the Long Short Term Memory {{is not the}} only neural network which learns a context {{sensitive}} language. Both Simple Recurrent Network and Sequential <b>Cascaded</b> <b>Network</b> are able to generalize beyond training data but by utilizing di#erent dynamics. Di#erences in performance and dynamics are discussed. Keywords: Recurrent neural network, language, prediction. ...|$|E
40|$|Abstract. We derive {{analytical}} {{expressions of}} local codim- 1 -bifurcations for a fully connected, additive, discrete-time RNN, where we regard the external inputs as bifurcation parameters. The {{complexity of the}} bifurcation diagrams obtained increases exponentially {{with the number of}} neurons. We show that a three-neuron <b>cascaded</b> <b>network</b> can serve as a universal oscillator, whose amplitude and frequency can be completely controlled by input parameters. ...|$|E
40|$|Large pose {{variations}} {{remain to}} be a challenge that confronts real-word face detection. We propose a new cascaded Convolutional Neural Network, dubbed the name Supervised Transformer Network, to address this challenge. The first stage is a multi-task Region Proposal Network (RPN), which simultaneously predicts candidate face regions along with associated facial landmarks. The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns. The second stage, which is a RCNN, then verifies if the warped candidate regions are valid faces or not. We conduct end-to-end learning of the <b>cascaded</b> <b>network,</b> including optimizing the canonical positions of the facial landmarks. This supervised learning of the transformations automatically selects the best scale to differentiate face/non-face patterns. By combining feature maps from both stages of the network, we achieve state-of-the-art detection accuracies on several public benchmarks. For real-time performance, we run the <b>cascaded</b> <b>network</b> only on regions of interests produced from a boosting cascade face detector. Our detector runs at 30 FPS on a single CPU core for a VGA-resolution image...|$|E
40|$|We {{consider}} many-body quantum systems dissipatively {{coupled by}} a <b>cascade</b> <b>network,</b> i. e. a setup in which interactions are mediated by unidirectional environmental modes propagating through a linear optical interferometer. In particular {{we are interested}} in the possibility of inducing different effective Hamiltonian interactions by properly engineering an external dissipative network of beam- splitters and phase-shifters. In this work we first derive the general structure of the master equation for a symmetric class of translation-invariant <b>cascade</b> <b>networks.</b> Then we show how, by tuning the parameters of the interferometer, one can exploit interference effects to tailor a large variety of many-body interactions. Comment: 11 pages, 9 figure...|$|R
30|$|Finally, we {{point out}} that using two {{different}} (frequency non-degenerate) resonator modes has the advantage that the interaction between control and signal inputs is phase insensitive which greatly simplifies the design and analysis of <b>cascaded</b> <b>networks</b> of such switches.|$|R
40|$|Online social {{networks}} enable collectives of users {{to create and}} share content at scale. The diffusion of content through the network, and the resulting information cascades, are phenomena that have been widely investigated on various platforms, which facilitate information diffusion using diverse technical mechanisms, user interfaces and incentives. This paper focuses on Tumblr, an online microblogging social network with a core 'reblogging' functionality that allows information to diffuse across its network by appearing on multiple user blogs. The formation of any <b>cascade</b> <b>network</b> is visible as a list of reblogging events attached as notes to each appearance of the post in the cascade. In this paper, we examine <b>cascade</b> <b>networks</b> on Tumblr, recreated from the series of diffusion events, and analyse them from structural and temporal perspectives. To achieve this, we utilise a cascade construction model that create <b>cascade</b> <b>networks,</b> overcoming problems {{of a lack of}} contextual information and missing/degraded data. Finally, we compare cascades in Tumblr with those appearing on other social network platforms. Our analysis shows that popular content on Tumblr creates 'large' cascades that are deep, branching into a large number of separate and long paths, having a consistent number of reblogs at each depth and at each given time...|$|R
40|$|We derive {{analytical}} {{expressions of}} local codimension- 1 bifurcations for a fully connected, additive, discrete-time recurrent neural network (RNN), where we regard the external inputs as bifurcation parameters. The {{complexity of the}} bifurcation diagrams obtained increases exponentially {{with the number of}} neurons. We show that a three-neuron <b>cascaded</b> <b>network</b> can serve as a universal oscillator, whose amplitude and frequency can be completely controlled by input parameters. Key words: bifurcation manifolds, input space, dynamics, recurrent neural network...|$|E
40|$|Neural network {{language}} models (NNLM) {{have become}} an increasingly popular choice for large vocabulary continuous speech recognition (LVCSR) tasks, due to their inherent gener-alisation and discriminative power. This paper present two tech-niques to improve performance of standard NNLMs. First, the form of NNLM is modelled by introduction an additional out-put layer node to model the probability mass of out-of-shortlist (OOS) words. An associated probability normalisation scheme is explicitly derived. Second, a novel NNLM adaptation method using a <b>cascaded</b> <b>network</b> is proposed. Consistent WER reduc-tions were obtained on a state-of-the-art Arabic LVCSR task over conventional NNLMs. Further performance gains were also observed after NNLM adaptation...|$|E
40|$|In this paper, {{we report}} and {{describe}} the derivation of an analytical approach for the calculation of the input impedance of a non-identical <b>cascaded</b> <b>network</b> of ‘n ’ discrete components based on knowing the propagation constant and the characteristic impedance of each component. These parameters may be obtained analytically or experimentally in a complex form. The developed equation allows {{the study of the}} effects of network irregularity, load impedance mismatching and the mismatching of the impedance of cascaded segments of the whole network. It is then applied to calculate the impedance of a multi-segment communication channel at a range of frequencies and validated against a standard equation...|$|E
5000|$|Image {{impedance}} is {{a similar}} concept to the characteristic impedance used {{in the analysis of}} transmission lines. In fact, in the limiting case of a chain of <b>cascaded</b> <b>networks</b> where the size of each single network is approaching an infinitesimally small element, the mathematical limit of the image impedance expression is the characteristic impedance of the chain. That is, ...|$|R
40|$|Cascade Correlation (Cascor) {{has proved}} to be a {{powerful}} method for training neural networks. Cascor, however, has been shown not to generalise well on regression and some classification problems. A new <b>Cascade</b> <b>network</b> algorithm employing Progressive RPROP (Casper), is proposed. Casper, like Cascor, is a constructive learning algorithm which builds <b>cascade</b> <b>networks.</b> Instead of using weight freezing and a correlation measure to install new neurons, however, Casper uses a variation of RPROP to train the whole network. Casper is shown to produce more compact networks, which generalise better than Cascor. INTRODUCTION The Cascade Correlation algorithm (Fahlman and Lebiere, 1990) is a very powerful method for training artificial neural networks. Cascor is a constructive algorithm which begins training with a single input layer connected directly to the output layer. Neurons are added one at time to the network and are connected to all previous hidden and input neurons, producing a cascade [...] ...|$|R
40|$|A general compact {{formula for}} {{multi-port}} matching efficiency {{in the presence}} of an arbitrary number of <b>cascaded</b> <b>networks</b> is introduced. A comparison between the decoupling efficiency and multi-port matching efficiency is presented. Similarly, a compact formula for decoupling efficiency in a cascaded chain is provided. The presented compact formulas are quite advantageous for optimization of radiation efficiencies by designing a proper matching network...|$|R
40|$|Continuous-valued {{recurrent}} {{neural networks}} can learn mechanisms for processing context-free languages. The dynamics of such networks is usually based on damped oscillation around fixed points in state space and {{requires that the}} dynamical components are arranged in certain ways. It is shown that qualitatively similar dynamics with similar constraints hold for a n b n c n, a context-sensitive language. The additional difficulty with a n b n c n, compared with the context-free language a n b n, consists of "counting up" and "counting down" letters simultaneously. The network solution is to oscillate in two principal dimensions, one for counting up and one for counting down. This study focuses on the dynamics employed by the Sequential <b>Cascaded</b> <b>Network,</b> {{in contrast with the}} Simple Recurrent Network, and the use of Backpropagation Through Time. Found solutions generalize well beyond training data, however, learning is not reliable. The contribution of this [...] ...|$|E
40|$|In [5], a new {{incremental}} cascade {{network architecture}} has been presented. This paper discusses {{the properties of}} such cascade networks and investigates their generalization abilities under the particular constraint of small data sets. The evaluation is done for cascade networks consisting of local linear maps using the MackeyGlass time series prediction task as a benchmark. Our results indicate that to bring the potential of large networks {{to bear on the}} problem of extracting information from small data sets without running the risk of overfitting, deeply <b>cascaded</b> <b>network</b> architectures are more favorable than shallow broad architectures that contain the same number of nodes. 1 Introduction For many real-world applications, a major constraint for the successful learning from examples is the limited number of examples available. Thus, methods are required, that can learn from small data sets. This constraint makes the problem of generalization particularly hard. If the number of adjus [...] ...|$|E
40|$|Estimating crowd {{count in}} densely crowded scenes is an {{extremely}} challenging task due to non-uniform scale variations. In this paper, we propose a novel end-to-end <b>cascaded</b> <b>network</b> of CNNs to jointly learn crowd count classification and density map estimation. Classifying crowd count into various groups is tantamount to coarsely estimating the total count in the image thereby incorporating a high-level prior into the density estimation network. This enables the layers in the network to learn globally relevant discriminative features which aid in estimating highly refined density maps with lower count error. The joint training is performed in an end-to-end fashion. Extensive experiments on highly challenging publicly available datasets show that the proposed method achieves lower count error and better quality density maps {{as compared to the}} recent state-of-the-art methods. Comment: Accepted at AVSS 2017 (14 th International Conference on Advanced Video and Signal Based Surveillance...|$|E
40|$|We {{present a}} new {{incremental}} <b>cascade</b> <b>network</b> architecture based on error minimization combined with "Local linear maps" (LLM) as cascaded units. The {{performance of the}} network is achieved by several layers of LLMs that are trained in a strictly feed-forward manner and one after the other. The properties of this and related cascade architectures are discussed. We report on extensive benchmarking results for various classification tasks, and time series prediction, and compare them with other results reported in the literature. Direct cascading is proposed as a promising approach to introduce context information in the approximation process. 1 Introduction Neural networks offer {{a wide range of}} architectural possibilities. So far, mainly shallow, broad architectures have been considered, while there has been very little research on the kind of architecture which is the focus of the present paper, namely narrow, but deeply <b>cascaded</b> <b>networks.</b> One of the few exceptions is the work of Fahl [...] ...|$|R
50|$|The oncotecture of {{a cancer}} {{derives from the}} gene-expression {{profiles}} (gene activity) of that cancer. This reveals the quantities and identities of the associated proteins. The relevant proteins are those involved in regulating cell growth and division via signalling pathways whereby one protein changes the behavior of potentially many others. These proteins in turn activate other genes, resulting in a <b>cascading</b> <b>network</b> of activity.|$|R
40|$|Forecasting of {{wave height}} is {{necessary}} in {{a large number of}} ocean coastal activities. Recently, neural networks are used for prediction and approximation of wave heights in sea and ocean due to their great convergence rate. In this paper a <b>cascade</b> correlation neural <b>network</b> is used for prediction of wave heights at given times due to the useful capability of this network for prediction and approximation. Results of different prediction for 500 data points in <b>cascade</b> correlation neural <b>network</b> are compared with those of the M. L. P. (Multi-layer Perceptron) neural network. These results show that <b>cascade</b> correlation <b>network</b> has larger convergence rate compared with M. L. P. network. Also various simulations show that the <b>cascade</b> correlation <b>network</b> has better performance with α= 0. 005 (Learning-rate), sigmoid activation function for hidden units and linear activation function for output units...|$|R
40|$|The cascade-correlation {{method of}} Fahlman and Lebiere is an inventive mix of powerful, largely {{independent}} features that {{work together to}} incrementally construct a neural network. We identify aspects of their learning algorithm upon which the cascade-correlation approach does not fundamentally depend, and analyze their strengths and weaknesses. We use four problems from the literature to empirically study these features. Evidence indicates that, apart from the <b>cascaded</b> <b>network</b> topology, the most powerful algorithmic feature of cascade-correlation is patience. Correlation is sometimes a valuable heuristic also, while the use of quickprop learning rules and the freezing of trained weights are of less value, and can often be detrimental to effective learning. (Submitted to Neural Networks, August 1991.) #################################### + This work was partially supported by NSF Grant IRI- 9002413 and ONR Grant N 00014 - 90 -J- 1941 and was inspired by research conducted with N [...] ...|$|E
40|$|A {{new method}} is {{presented}} for designing space compactors for either deterministic testing or pseudorandom testing. A tree of elementary gates (AND, OR, NAND, NOR) {{is used to}} combine the outputs of the circuit-under-test (CUT) {{in a way that}} zero-aliasing is guaranteed with no modification of the CUT. The elementary-tree is synthesized by adding one gate at a time without introducing redundancy. The end result is a <b>cascaded</b> <b>network,</b> CUT followed by space compactor, that is irredundant and has fewer outputs than the CUT alone. All faults in the CUT and space compactor can be tested. Only the outputs of the space compactor need to be observed during testing. Experimental results are surprising; they show that very high compaction ratios can be achieved with zero-aliasing elementary-tree space compactors. Compared with parity trees and other space compactor designs that have been proposed, the method presented here requires less overhead and yet guarantees zero-aliasing. 1...|$|E
40|$|Evolution {{of neural}} networks, or neuroevolution, {{has been a}} {{successful}} approach to many low-level control problems such as pole balancing, vehicle control, and collision warning. However, certain types of problems – such as those involving strategic decision-making – have remained difficult for neuroevolution to solve. This paper evaluates the hypothesis that such problems are difficult because they are fractured: The correct action varies discontinuously as the agent moves from state to state. A method for measuring fracture using the concept of function variation is proposed, and based on this concept, two methods for dealing with fracture are examined: neurons with local receptive fields, and refinement based on a <b>cascaded</b> <b>network</b> architecture. Experiments in several benchmark domains {{are performed to evaluate}} how different levels of fracture affect the performance of neuroevolution methods, demonstrating that these two modifications improve performance significantly. These results form a promising starting point for expanding neuroevolution to strategic tasks. 1...|$|E
40|$|We {{consider}} coordination in <b>cascade</b> <b>networks</b> {{and construct}} sequences of polar codes that achieve {{any point in}} a special region of the empirical coordination capacity region. Our design combines elements of source coding to generate actions with the desired type with elements of channel coding to minimize the communication rate. Moreover, we bound the probability of malfunction of a polar code for empirical coordination. Possible generalizations and open problems are discussed. QC 2012060...|$|R
40|$|AbstractBased on the {{principle}} of discrete Hopfield neural network, the paper proposes a <b>cascade</b> Hopfield neural <b>network</b> controller model and applied in a miniature inchworm robot locomotion process. According to the robot moving modes in one cycle, the <b>cascade</b> Hopfield neural <b>network</b> model with three neural nodes was set up, the weight factors and thresholds of the networks had been designed. The convergence results prove the <b>cascade</b> Hopfield neural <b>network</b> controller is suitable for the orderly continuous moving process of an inchworm robot...|$|R
5000|$|Scattering {{transfer}} parameters, like scattering parameters, {{are defined}} in terms of incident and reflected waves. The difference is that T-parameters relate the waves at port 1 to the waves at port 2 whereas S-parameters relate the reflected waves to the incident waves. In this respect T-parameters fill the same role as ABCD parameters and allow the T-parameters of <b>cascaded</b> <b>networks</b> to be calculated by matrix multiplication of the component networks. T-parameters, like ABCD parameters, can also be called transmission parameters. The definition is, ...|$|R
40|$|This thesis aims to {{evaluate}} four {{artificial neural network}} architectures, each of which implements the sensory-motor mapping in an embodied, situated, and autonomous agent set up to reach a goal area in one out of six systematically varied T-maze environments. In order to reach the goal the agent has to turn either {{to the left or}} to the right in each junction in the environment, depending on the placement of previously encountered light sources. The evaluation is broken down into (i) measuring the reliability of the agents' capacity to repeatedly reach the goal area, (ii) analyzing how the agents work, and (iii) comparing the results to related work on the problem. Each T-maze constitutes an instance of a broad class of problems known as delayed response tasks, which are characterized by a significant (and typically varying) delay between a stimulus and the corresponding appropriate response. This thesis expands this notion to include, besides simple tasks, repeated and multiple delayed response tasks. In repeated tasks, the agent faces several stimulus-delay-response sequences after each other. In multiple tasks, the agent faces several stimuli before the delay and the corresponding appropriate responses. Even if simple at an abstract level, these tasks raise some of the fundamental issues within cognitive science and artificial intelligence such as whether or not an internal objective world model is necessary and/or suitable to achieve the appropriate behavior. For such reasons, these problems also constitute an interesting base for evaluating alternative ideas within these fields. The work leads to several interesting insights. Firstly, purely reactive controllers (as represented by a feed-forward network) may be sufficient, in interaction with the environment, to solve both simple and repeated delayed response tasks. Secondly, an extended sequential <b>cascaded</b> <b>network</b> that selectively replaces its own sensory-motor mapping achieves significantly better performance than the other networks. This indicates that selective replacement of the sensory-motor mapping may be more powerful than both modulation (as represented by a simple recurrent network) and replacement in each step (as represented by a standard sequential <b>cascaded</b> <b>network).</b> Thirdly, this thesis demonstrates that even reactive controllers may contribute to behavior, which, from an observer's point of view, may seem to require an internal rational capacity, i. e. the ability to represent and explore alternatives internally...|$|E
40|$|Monocular depth estimation, {{which plays}} a key role in {{understanding}} 3 D scene geometry, is fundamentally an ill-posed problem. Existing methods based on deep convolutional neural networks (DCNNs) have examined this problem by learning convolutional networks to estimate continuous depth maps from monocular images. However, we find that training a network to predict a high spatial resolution continuous depth map often suffers from poor local solutions. In this paper, we hypothesize that achieving a compromise between spatial and depth resolutions can improve network training. Based on this "compromise principle", we propose a regression-classification <b>cascaded</b> <b>network</b> (RCCN), which consists of a regression branch predicting a low spatial resolution continuous depth map and a classification branch predicting a high spatial resolution discrete depth map. The two branches form a cascaded structure allowing the classification and regression branches to benefit from each other. By leveraging large-scale raw training datasets and some data augmentation strategies, our network achieves top or state-of-the-art results on the NYU Depth V 2, KITTI, and Make 3 D benchmarks...|$|E
40|$|Degrees {{of freedom}} (DoF) {{of a network}} build a new scaling law characterizing the {{scalability}} of capacity at high signal-to-noise region. In this paper, we extend our recent work from <b>cascaded</b> <b>network</b> to the general K-hop layered network. The main framework {{is based on the}} assumption of layered TDD, where all nodes at each layer work with the same on/off status. By this approach we decompose the DoF analysis into two steps: 1) apply the result of cascaded networks; 2) analyze / design the transmission of each hop. The upper and lower bounds on DoF are deduced. By viewing the network as cascaded X channels, we find an inner bound of the DoF region, applicable to many message topologies. The detail of message splitting is demonstrated. Finally ultimate analysis shows if the number of antennas/nodes at each relay layer goes to infinity, the lower bound reaches the upper bound. As a by-product, when K > 2 the network can alleviate the effect of TDD with the increase of relay antennas/nodes...|$|E
40|$|We {{study the}} {{dissipative}} dynamics {{and the formation}} of entangled states in driven <b>cascaded</b> quantum <b>networks,</b> where multiple systems are coupled to a common unidirectional bath. Specifically, we identify the conditions under which emission and coherent reabsorption of radiation drives the whole network into a pure stationary state with non-trivial quantum correlations between the individual nodes. We illustrate this effect in more detail for the example of cascaded two-level systems, where we present an explicit preparation scheme that allows one to tune the whole network through "bright" and "dark" states associated with different multi-partite entanglement patterns. In a complementary setting consisting of cascaded non-linear cavities, we find that two cavity modes can be driven into a non-Gaussian entangled dark state. Potential realizations of such <b>cascaded</b> <b>networks</b> with optical and microwave photons are discussed. Comment: 20 pages, 7 figures, version as accepted by NJ...|$|R
40|$|Driving point {{impedance}} synthesis for <b>cascade</b> of lossless <b>network</b> {{sections and}} gyrator reduction. Prepared at Lewis Research Center. "November 1969. "Cover title. Includes bibliographical references (p. 63). Driving point impedance synthesis for <b>cascade</b> of lossless <b>network</b> sections and gyrator reduction. Mode of access: Internet...|$|R
40|$|Generalization {{performance}} in recurrent neural networks {{is enhanced by}} <b>cascading</b> several <b>networks.</b> By discretizing abstractions induced in one network, other networks can operate on a coarse symbolic level with increased performance on sparse and structural prediction tasks. The level of systematicity exhibited by the <b>cascade</b> of recurrent <b>networks</b> is assessed {{on the basis of}} three language domains. (C) 2004 Elsevier B. V. All rights reserved...|$|R
40|$|In this work, a short-open {{calibration}} {{technique is}} extended for field theory-based parametric extraction of planar discontinuities with nonuniform feed {{lines in the}} platform of full-wave method-of-moments. As a planar discontinuity is analyzed {{with respect to the}} planes of impressed sources, each individual nonuniform feed line is modeled as a general error box relying on the perfect short- and open-circuit standards. By calibrating out all the error boxes associated with all the nonuniform feed lines, the core discontinuity section can be deembedded based on the <b>cascaded</b> <b>network</b> theorem. Since each individual feed line is modeled as a unified two-port circuit instead of a transmission line section, the presented technique provides an advantageous feature in deembedding various planar circuits that may be fed by nonuniform lines, as met in the high-density and high-integrated circuits. After theoretical description is made on the deembedding procedure, a microstrip-line slit discontinuity with varied feed lines is numerically characterized. Extracted equivalent circuit parameters are at first confirmed by the Sonnet em simulator in the uniform case and then substantially demonstrated in various nonuniform cases. © 2007 IEEE. link_to_subscribed_fulltex...|$|E
40|$|Von Neumann {{projections}} are {{the main}} operations by which information can be extracted from the quantum to the classical realm. They are however static processes that do not adapt to the states they measure. Advances {{in the field of}} adaptive measurement have shown that this limitation can be overcome by "wrapping" the von Neumann projectors in a higher-dimensional circuit which exploits the interplay between measurement outcomes and measurement settings. Unfortunately, the design of adaptive measurement has often been ad hoc and setup-specific. We shall here develop a unified framework for designing optimized measurements. Our approach is two-fold: The first is algebraic and formulates the problem of measurement as a simple matrix diagonalization problem. The second is algorithmic and models the optimal interaction between measurement outcomes and measurement settings as a <b>cascaded</b> <b>network</b> of conditional probabilities. Finally, we demonstrate that several figures of merit, such as Bell factors, can be improved by optimized measurements. This leads us to the promising observation that measurement detectors which [...] -taken individually [...] -have a low quantum efficiency can be be arranged into circuits where, collectively, the limitations of inefficiency are compensated for...|$|E
40|$|LAN, {{quality of}} service, {{deterministic}} services In this paper and its sequel [1] we study {{the problem of}} allocating resources in single hub and cascaded 802. 12 networks. We show {{that the use of}} the 802. 12 high priority mechanism when combined with admission control, allows the network to provide small, deterministic delay bounds in large, <b>cascaded</b> <b>network</b> topologies with potentially many hundreds of nodes. The allocation scheme proposed is based on a time frame concept that takes advantage of the properties of the Demand Priority medium access protocol to provide much tighter delay bounds than given by the time frame itself. The first part of the work is to analyse relevant network performance parameters and their dependencies. In the second part, we describe the scheduling model and define the admission control conditions used to provide deterministic service guarantees. Experimental results received with a UNIX kernel based implementation in a standard 802. 12 test network confirm our theoretical results for network parameters, throughput and delay bounds. In this paper, the single hub topology is analysed. In the sequel of this paper, the network parameters are derived for cascaded 802. 12 networks which allow the admission control conditions to be applied to the topologies...|$|E
2500|$|Scattering {{transfer}} parameters, like scattering parameters, {{are defined}} in terms of incident and reflected waves. [...] The difference is that T-parameters relate the waves at port 1 to the waves at port 2 whereas S-parameters relate the reflected waves to the incident waves. [...] In this respect T-parameters fill the same role as ABCD parameters and allow the T-parameters of <b>cascaded</b> <b>networks</b> to be calculated by matrix multiplication of the component networks. [...] T-parameters, like ABCD parameters, can also be called transmission parameters. [...] The definition is, ...|$|R
40|$|Abstract: A {{programmed}} {{algorithm is}} presented for the synthesis and optimisation of networks implemented with multiplexer universal logic modules. The algorithm attempts level by level optimisation selecting the control variables {{that result in}} minimum number of continuing branches. <b>Cascaded</b> <b>networks,</b> if realisable, are always found and given preference over tree net-works, though mixtures of cascade and tree con-figurations are permitted. The algorithm is programmed in Fortran and tested for single and double control variable modules. In theory, the program {{can be used for}} any number of variables for completely and incompletely specified func-tions. ...|$|R
40|$|An {{enhancement}} of nodal noise analysis of electrical networks is presented by computing noise sensitivities {{with respect to}} network parameters. The noise figure and spot noise parameters sensitivities of the reduced two-port network are obtained from the partial inversion of the nodal admittance matrix. Analytical formulas are derived to calculate first- and second-order noise sensitivities directly, rather than using a computer-time intensive perturbation method. Expressions for noise sensitivities of <b>cascaded</b> <b>networks</b> are also given. Numerical results for an actively matched amplifier are shown to illustrate {{the advantages of the}} proposed technique...|$|R
