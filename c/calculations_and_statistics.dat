10|10000|Public
30|$|For data synthesis, the {{survival}} and success {{data of the}} individual studies were pooled by the weighted mean method and 95  % confidence intervals were calculated as estimations of variance. A meta-analysis was not possible due {{to the structure of}} the underlying survival success data as simple percentages without a measure of variance and without control groups in most studies. The survival/success data were weighted both based on patients and units (either implants/teeth/prostheses). Because the studies showed large differences in follow-up times, these were standardized by calculating annual failure rates by dividing the success and survival data through the follow-up time in years. All spreadsheet <b>calculations</b> <b>and</b> <b>statistics</b> were made with the Microsoft Excel program.|$|E
40|$|During the Toga/Coare {{experiment}} in the tropical west Pacific, a cloud radiation {{experiment in}}volving measurement from the NASA ER- 2 high altitude aircraft and DC- 8 airborne laboratory was performed. Observations include multispectral visible/IR and microwave radiometric imaging, active lidar profiling and radiation flux measurements. On a number of missions coordinated flights were made for simultaneous cloud observations with the ER- 2 remote sensing and DC- 8 below cloud and in cloud measurements. Since the field experiment there has been systematic analysis of the observations. Intercomparison of retrieved measurements will be presented. These include the relation between visible bi-directional reflectivity and infrared emissivity of tropical cirrus. Upper troposphere microwave water vapor retrievals and cirrus optical parameters have been correlated. The relation observed short wave flux divergence to retrieved cloud thickness and type has been studied. <b>Calculations</b> <b>and</b> <b>statistics</b> for the short and long wave heating profiles of observed clouds have been derived. A summary of key results will be presented...|$|E
40|$|Indonesian Bankings profit growth {{during the}} year 2004 - 2009 had been {{constantly}} fluctuating with a down tendency each year, but the amount of deposits and total assets always went up. This anomaly can {{be influenced by the}} factors, some of which were used as variables in this study. This study uses secondary data from banks financial reports such as BI rate and Blanket Guarantee Rate, during the first quarter of 2006 to the fourth quarter 2009. The purposive sampling was employed by ranking the criteria based on the largest number of diposits and total assets with a share of at least 5 % of all commercial banks in Indonesia. The analysis was done by performing mathematical <b>calculations</b> <b>and</b> <b>statistics</b> from various financial ratios that reflect the growth rate of deposits products and their distribution within the reference rate. It was found that only two variables can significantly affect the banks NIM ratio, deposits and moderation of saving. Another results showed that moderation of savings become the most dominant variable...|$|E
40|$|Abstract. We adopt rapid {{prototyping}} method {{to develop the}} system, depend on the {{rapid prototyping}} creation tool to achieve the design and implementation of management system of university sports scores, aim at simplifying the <b>calculation</b> <b>and</b> <b>statistics</b> of sports scores, realize the informatization management, and deliver the cumbersome <b>calculation</b> <b>and</b> <b>statistics</b> work to computer to accomplish automatically...|$|R
50|$|From {{the autumn}} of 1962 he was again professor, head of the {{department}} of probability <b>calculation</b> <b>and</b> mathematical <b>statistics</b> (successor to professor Onicescu).|$|R
5000|$|In banks, for {{managing}} and processing of accounts, bookkeeping, foreign-currency <b>and</b> interest <b>calculations,</b> amortization plans <b>and</b> <b>statistics</b> ...|$|R
40|$|Lensometers and keratometers yield powers along {{perpendicular}} meridians even if {{the principal}} meridians of the lens and the cornea are oblique. From each such instrument, multiple raw data represented on optical crosses require conversion to determine elementary statistics. Calculations for research decisions need to be authentic. Principles common to meridians generalize formulaic methods for oblique meridians. Like a lens or a cornea, matrix latent quantities are represented on a matrix cross. Our problem {{is to determine the}} matrix whose cross represents quantities on the optical cross. All measurements on an optical cross that include corneal and lens powers and oblique meridians can be considered. Once determined, a portfolio of matrix calculations applies and is justified for ophthalmic calculation. Matrices can be unique and, like a cornea before it is measured, contain latent observations. Asymmetric power component matrices quantify a deviation of a corneal surface from smoothness and toricity. Entries may identify those measurements causing irregular astigmatism that may stem from surgical or other external intervention. Irregular astigmatism is detected primarily from significant measurements in the paraxial range. Measurements are assimilated with matrix factors in a holistic way in order to support choices with <b>calculations</b> <b>and</b> <b>statistics...</b>|$|E
40|$|The aim of {{this paper}} is to show the {{advantages}} of GIS in monitoring and improving flood response management in Albania. A full statistic overview of the last flooding occured in the region of Shkodra will be presented. The delicate area balances as far as water management is concerned, have turned into repetitive problematic that have become endemic to the region. The flooding in 2010 and 2011 due to heavy rain, snow melting and hydropower management caused a strong impact in the socio-economic life of the population. According to the last statistics, numbers referring to population displacement, house inundation, property damages seems to be a growing concern for the State Emergency Service. This scenario involves the role of the government institutions in both planning and the operational contexts. Uncoordinated measures between emergency groups, delayed actions from the hydropower specialists, the lack of updated geoinformation followed by a limited remote control occur due to a continuous distant approach created toward GIS technology in our country. As a solution to this scenario it will be presented a concrete platform based on <b>calculations</b> <b>and</b> <b>statistics</b> of dam capacity, allowed water levels, maximum rainfall levels, climate factors, population density and movements. GIS carries the potential for flood plain management, flood mapping and forecasting, also population education and awareness. Geospatial information and remot...|$|E
40|$|Copyright © 2014 H. Abelman and S. Abelman. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in anymedium, provided the originalwork is properly cited. Lensometers and keratometers yield powers along perpendicular meridians even if the principal meridians of the lens and the cornea are oblique. From each such instrument, multiple raw data represented on optical crosses require conversion to determine elementary statistics. Calculations for research decisions need to be authentic. Principles common tomeridians generalize formulaic methods for oblique meridians. Like a lens or a cornea, matrix latent quantities are represented on a matrix cross. Our problem {{is to determine the}} matrix whose cross represents quantities on the optical cross. All measurements on an optical cross that include corneal and lens powers and oblique meridians can be considered. Once determined, a portfolio of matrix calculations applies and is justified for ophthalmic calculation. Matrices can be unique and, like a cornea before it is measured, contain latent observations. Asymmetric power component matrices quantify a deviation of a corneal surface from smoothness and toricity. Entries may identify those measurements causing irregular astigmatism that may stem from surgical or other external intervention. Irregular astigmatism is detected primarily from significant measurements in the paraxial range. Measurements are assimilated with matrix factors in a holistic way in order to support choices with <b>calculations</b> <b>and</b> <b>statistics.</b> 1...|$|E
40|$|International audienceWith the {{increasing}} number of protein structures available, {{there is a need for}} tools capable of automating the comparison of ensembles of structures, a common requirement in structural biology and bioinformatics. PSSweb is a web server for protein structural statistics. It takes as input an ensemble of PDB files of protein structures, performs a multiple sequence alignment <b>and</b> computes structural <b>statistics</b> for each position of the alignment. Different optional functionalities are proposed: structure superposition, Cartesian coordinate <b>statistics,</b> dihedral angle <b>calculation</b> <b>and</b> <b>statistics,</b> <b>and</b> a cluster analysis based on dihedral angles. An interactive report is generated, containing a summary of the results, tables, figures and 3 D visualization of superposed structures...|$|R
40|$|Background: The {{quantitative}} analysis of metabolic fluxes, i. e., in vivo activities of intracellular enzymes and pathways, provides key information on biological systems in systems biology and metabolic engineering. It {{is based on}} a comprehensive approach combining (i) tracer cultivation on C- 13 substrates, (ii) C- 13 labelling analysis by mass spectrometry and (iii) mathematical modelling for experimental design, data processing, flux <b>calculation</b> <b>and</b> <b>statistics.</b> Whereas the cultivation and the analytical part is fairly advanced, a lack of appropriate modelling software solutions for all modelling aspects in flux studies is limiting the application of metabolic flux analysis...|$|R
50|$|In 1948, {{after the}} reform of {{education}} in all degrees, he was appointed {{head of the department}} of probability <b>calculation</b> <b>and</b> mathematical <b>statistics</b> at the Faculty of mathematics and physics of Bucharest University, then as professor head of department of applied mathematics.|$|R
40|$|International audienceNetworks are classic but under-acknowledged {{figures of}} journalistic storytelling. Who is {{connected}} to whom and by which means? Which organizations receive support from which others? What resources or information circulate through which channels and which intermediaries enable and regulate their flows? These are all customary stories and lines of inquiry in journalism {{and they all have}} to do with networks. Additionally, the recent spread of digital media has increasingly confronted journalists with information coming not only in the traditional form of statistic tables, but also of relational databases. Yet, journalists have so far made little use of the analytical resources offered by networks. To address this problem in this chapter we examine how " visual network exploration " may be brought to bear in the context of data journalism in order to explore, narrate and make sense of large and complex relational datasets. We borrow the more familiar vocabulary of geographical maps to show how key graphical variables such as position, size and hue can be used to interpret and characterise graph structures and properties. We illustrate this technique by taking as a starting point a recent example from journalism, namely a catalogue of French information sources compiled by Le Monde's The Decodex. We establish that good visual exploration of networks is an iterative process where practices to demarcate categories and territories are entangled and mutually constitutive. To enrich investigation we suggest ways in which the insights of the visual exploration of networks can be supplemented with simple <b>calculations</b> <b>and</b> <b>statistics</b> of distributions of nodes and links across the network. We conclude with reflection on the knowledge-making capacities of this technique and how these compare to the insights and instruments that journalists have used in the Decodex project – suggesting that visual network exploration is a fertile area for further exploration and collaborations between data journalists and digital researchers...|$|E
40|$|Due {{to limited}} sources available, Chinese {{scholars}} {{so far are}} slightly behind the Western scholars {{in the study of}} Chinese export porcelain. It is sincerely hope that the current in-depth analysis of Kraak porcelain will contribute to portray a more comprehensive picture of Chinese export porcelain. Kraak porcelain is a type of Chinese export porcelain with unique features which was produced {{in the second half of}} the sixteenth century for European market. It was the earliest type of export porcelain of some scale and had distinct characteristics. In this attempt to understand the rich cultural significance of Kraak porcelain, interdisciplinary research methodologies have been integrated. They include documentary and textual studies (history), <b>calculations</b> <b>and</b> <b>statistics</b> (economics), archaeological typology and stylistic analysis (art history and archaeology). The aim of the thesis is to carry out a detailed study of Kraak porcelain, in particular specimen from datable contexts and archaeological material, so as to better understand the provenance of Kraak porcelain, its period of production and changes at different stages. Based on the results of the above examinations, a preliminary study of the sale and distribution of Kraak porcelain has been carried out [...] -an area which has been neglected. Related issues such as order placing, different requirements of patrons and export routes are also discussed. 范夢園. Adviser: Harold Kar-Leung Mok. Source: Dissertation Abstracts International, Volume: 73 - 03, Section: A, page:. Thesis (Ph. D.) [...] Chinese University of Hong Kong, 2010. Includes bibliographical references (p. 248 - 267). Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [201 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstracts in Chinese and English. Fan Mengyuan...|$|E
40|$|Every {{man needs}} {{a house for}} shelter and as a {{gathering}} place {{as well as the}} ongoing activities of the family, as well as investment goods. Today the house functions slightly changed, from the original just as a place to stay, this time the home is also required to be able to bring in satisfaction and greater benefits for its owners as a strategic location, building a good, solid, and comfortable environment. In other words not just shelter but must be adequately housed. This study on the analysis of the factors - factors that influence the decision of the selection of type 60 in the city of Semarang. This research was conducted in the Mega Housing Residence in the city of Semarang. The {{purpose of this study was}} to determine whether the price suitability, facilities, consumer spending, the number of family members and other types of home prices affect the decision of selecting the type 60. This research is quantitative. Quantitative research is research that uses mathematical <b>calculations</b> <b>and</b> <b>statistics.</b> The study population was consumers who purchase and occupy housing complex Mega Residence Semarang with a total sample of 72 respondents. The analysis method is validity, reliability and Binary Logistic Regression analysis. The results showed that: (1) Suitability price positive and significant impact on home purchases 60 Type Perceptions better price will increase the probability of purchase of house type 60 (2) Perception Facilities positive and significant impact on the purchase of a home type facility Perception 60 better would increase the probability of purchase of house type 60 (3) Expenditures significant negative effect on house purchases 60 type of greater spending will decrease the probability of purchase of house type 60 (4) Number of family members does not significantly influence the purchase of type 60 (5) prices of other types of significant negative effect on the purchase price of the type 60 Perceptions better substitution would decrease the probability of purchase of house type 60...|$|E
40|$|The aim of {{the paper}} is to value the {{progress}} in the European social and territorial cohesion in terms of welfare of population. Research methodology {{is based upon the}} economic theory, analysis of literature, official documents and statistical data, author’s <b>calculations</b> <b>and</b> mathematical <b>statistics.</b> Only a minor degree of development and welfare level equalisation within the EU is stated. The EU new member states seem to face a prospect of remaining a poor periphery of Europe for an uncertain period of time...|$|R
40|$|It is not {{possible}} to understand the present or future climate unless scientists can account for the enormous and rapid cycles of glaciation that have taken place over the last million years, and which are expected to continue into the future. A great deal has happened in the theory of the ice ages over the last decade, and it is now widely accepted that ice ages are driven by changes in the Earth's orbit. The study of ice ages is very inter-disciplinary, covering geology, physics, glaciology, oceanography, atmospheric science, planetary orbit <b>calculations</b> astrophysics <b>and</b> <b>statistics...</b>|$|R
30|$|The {{technical}} guideline for Europe describes {{the basic principles}} for fire risk assessment principles and performance based design. This includes fire safety engineering design, analytical approaches <b>and</b> <b>calculation,</b> design fires <b>and</b> <b>statistics</b> (Ostman 2010). While the design guide document is focused on timber design, the guidance on performance based solutions {{can be applied to}} all building types. This allows a designer to propose an engineered solution using fire risk assessment principles to comply with the prescriptive regulations.|$|R
40|$|The article {{presents}} {{the hypothesis that}} the increase in the intermediate consumption caused by the growth of material costs due to innovation and modernization temporarily reduces the level of economic efficiency. This leads to further multiple increases of final product volume, which, in turn, increases the region’s economic potential and competitiveness, provides its enhanced participation in inter-regional and international exchange processes. The authors consider the main factors and conditions that determine the variation of these parameters in the market economy and discuss the intermediate and final product, export, import and others. The results of the analysis, <b>calculations</b> <b>and</b> <b>statistics</b> reveal and confirm the possibility of their measurement with the use of interindustry balances. The study aims at assessing the quantitative interdependence of intermediate and final products growth based on the analysis of interregional and international exchange in the system coordination to macroeconomic indicators of the socio-economic development of the region during the periods of planned and market economy. Another purpose is to determine the directions of the region’s participation in the division of labour and implementation of innovations in order to increase of gross domestic product. On the example of the development of Bashkortostan economy, the authors have revealed that a greater involvement of the region in the foreign economic processes led to the increase in the flow of exports and imports and the simultaneous reduction of production export to other regions of the Russian Federation and import from them. At the same time, the region has focused mainly on the export of fuel and energy products. The proportions of intermediate and final products in the period under review have changed by increasing the final products. That confirms the improvement of conditions for the growth of the region’s competitiveness. On the example of oil-extracting and oil processing industries, the authors have proved that the modernization and introduction of innovative approaches to the organization of production are an effective tool for multiple increases of the final product volume, despite the increase in intermediate products...|$|E
40|$|We {{continue}} {{the debate on}} anisotropic but scaling turbulence {{and its effect on}} aircraft measurements of turbulence (cf. Lindborg et al., 2010 a, b); hereafter LTNCG 1, LTNCG 2). We revisit the repeatedly presented back-of-the-envelope <b>calculation</b> <b>and</b> discuss wind <b>statistics</b> on real isobars. We then discuss theoretical and empirical evidence that a k&minus; 5 / 3 horizontal wind spectrum could extend out to planetary scales...|$|R
40|$|Abstract Background The {{quantitative}} analysis of metabolic fluxes, i. e., in vivo activities of intracellular enzymes and pathways, provides key information on biological systems in systems biology and metabolic engineering. It {{is based on}} a comprehensive approach combining (i) tracer cultivation on 13 C substrates, (ii) 13 C labelling analysis by mass spectrometry and (iii) mathematical modelling for experimental design, data processing, flux <b>calculation</b> <b>and</b> <b>statistics.</b> Whereas the cultivation and the analytical part is fairly advanced, a lack of appropriate modelling software solutions for all modelling aspects in flux studies is limiting the application of metabolic flux analysis. Results We have developed OpenFLUX as a user friendly, yet flexible software application for small and large scale 13 C metabolic flux analysis. The application is based on the new Elementary Metabolite Unit (EMU) framework, significantly enhancing computation speed for flux calculation. From simple notation of metabolic reaction networks defined in a spreadsheet, the OpenFLUX parser automatically generates MATLAB-readable metabolite and isotopomer balances, thus strongly facilitating model creation. The model can be used to perform experimental design, parameter estimation and sensitivity analysis either using the built-in gradient-based search or Monte Carlo algorithms or in user-defined algorithms. Exemplified for a microbial flux study with 71 reactions, 8 free flux parameters and mass isotopomer distribution of 10 metabolites, OpenFLUX allowed to automatically compile the EMU-based model from an Excel file containing metabolic reactions and carbon transfer mechanisms, showing it's user-friendliness. It reliably reproduced the published data and optimum flux distributions for the network under study were found quickly (Conclusion We have developed a fast, accurate application to perform steady-state 13 C metabolic flux analysis. OpenFLUX will strongly facilitate and enhance the design, <b>calculation</b> <b>and</b> interpretation of metabolic flux studies. By providing the software open source, we hope it will evolve with the rapidly growing field of fluxomics. </p...|$|R
40|$|In 2004, Garcia-Berthou and Alcaraz {{published}} "Incongruence between test <b>statistics</b> <b>and</b> P {{values in}} medical papers," {{a critique of}} statistical errors that received {{a tremendous amount of}} attention. One of their observations was that the final reported digit of p-values in articles published in the journal Nature departed substantially from the uniform distribution that they suggested should be expected. In 2006, Jeng critiqued that critique, observing that the statistical analysis of those terminal digits had been based on comparing the actual distribution to a uniform continuous distribution, when digits obviously are discretely distributed. Jeng corrected the <b>calculation</b> <b>and</b> reported <b>statistics</b> that did not so clearly support the claim of a digit preference. However delightful it may be to read a critique of statistical errors in a critique of statistical errors, we nevertheless found several aspects of the whole exchange to be quite troubling, prompting our own meta-critique of the analysis...|$|R
40|$|We {{show how}} to {{accelerate}} relativistic hydrodynamics simulations using graphic cards (graphic processing units, GPUs). These improvements are of highest relevance e. g. {{to the field}} of high-energetic nucleus-nucleus collisions at RHIC and LHC where (ideal and dissipative) relativistic hydrodynamics is used to calculate the evolution of hot and dense QCD matter. The results reported here are based on the Sharp And Smooth Transport Algorithm (SHASTA), which is employed in many hydrodynamical models and hybrid simulation packages, e. g. the Ultrarelativistic Quantum Molecular Dynamics model (UrQMD). We have redesigned the SHASTA using the OpenCL computing framework to work on accelerators like graphic processing units (GPUs) as well as on multi-core processors. With the redesign of the algorithm the hydrodynamic calculations have been accelerated by a factor 160 allowing for event-by-event <b>calculations</b> <b>and</b> better <b>statistics</b> in hybrid <b>calculations.</b> Comment: Details <b>and</b> discussions added, replaced with accepted version. (15 pages, 8 figures, 2 listings...|$|R
40|$|This paper {{describes}} a proof-of-concept study undertaken {{to determine the}} usefulness of provider-beneficiary mean geographic distances as a screening tool in Medicare fraud, waste and abuse detection activities. Tech-niques included geocoding provider and beneficiary addresses, calculating geographic distances between pro-vider-beneficiary pairs, <b>and</b> creating summary <b>statistics</b> <b>and</b> automated screening procedures. We used ESRI’s ArcView Desktop GIS software for street-level geocoding, Gazetteer files publicly available from the U. S. Census Bureau for zip code-level geocoding, and Base SAS for all data transfer activities, distance <b>calculations,</b> summary <b>statistics</b> <b>and</b> screening. SAS procedures included PROC DBF, PROC MEANS and PROC SUMMARY. The DATA Step was also used for <b>calculations</b> <b>and</b> screening. Outcomes suggest that larger-than-average mean pro-vider-beneficiary geographic distances are a good indicator of potentially anomalous provider activity and that the methodology developed is {{for the most part}} time-efficient and cost-effective...|$|R
40|$|The {{students}} appearing for placements {{and various}} other exams like GRE, GMAT, GATE prepare themselves by appearing for mock tests and searching for the study material from different websites. For this they require an internet connection, which {{is not possible to}} access everywhere. Today, the smart phones have replaced desktop PC’s and laptops in many manners. There are offline android applications available for aptitude tests but there exists none that provides the test for all sections with random set of questions, timer for each test, tutorials, score <b>calculation</b> <b>and</b> maintaining <b>statistics</b> in one single application. This paper provides a comparison of the existing aptitude applications available in Android market along with the idea of developing an application that overcomes the flaws of the existing applications. This application has been developed {{within the framework of the}} current study that provides its users with a fast, effective and efficient learning environment, thanks to today’s various mobile devices. ...|$|R
40|$|International audienceThere is a {{need for}} water {{distribution}} network models to better assess water quality for safety and security in case of contamination intrusion. One critical enhancement needed is to take into account imperfect mixing at junctions. This paper completes previous work by conducting LES <b>and</b> DNS CFD <b>calculation</b> <b>and</b> laboratory experiments. <b>Statistics</b> were compiled on large-size networks in France and Germany to serve to design a test rig and supply CFD cases for the numerical simulation. The main influencing factors were identified to be: the inter-Tee distance and three of the Reynolds numbers of the inlets/outlets. A lookup table and interpolation methods are used to model inside a one-dimensional model. This is implemented in the transport module of Porteau software...|$|R
40|$|The {{author has}} {{identified}} the following significant results. The {{integration of the}} available methods provided the analyst with the unified scanner analysis package (USAP), the flexibility and versatility of which was superior to many previous integrated techniques. The USAP consisted of three main subsystems; (1) a spatial path, (2) a spectral path, and (3) a set of analytic classification accuracy estimators which evaluated the system performance. The spatial path consisted of satellite and/or aircraft data, data correlation analyzer, scanner IFOV, and random noise model. The output of the spatial path was fed into the analytic classification and accuracy predictor. The spectral path consisted of laboratory and/or field spectral data, EXOSYS data retrieval, optimum spectral function <b>calculation,</b> data transformation, <b>and</b> <b>statistics</b> <b>calculation.</b> The output of the spectral path was fended into the stratified posterior performance estimator...|$|R
40|$|Turkey {{is located}} in an {{important}} geographical location, {{in terms of the}} epidemiology of vector-borne diseases, linking Asia and Europe. Cutaneous leishmaniasis (CL) is one of the endemic diseases in a Turkey and according to the Ministry Health of Turkey, 45 % of CL patients originate from Şanlıurfa province located in southeastern Turkey. Herein, the epidemiological status of CL, caused by L. tropica, in Turkey was examined using multilocus microsatellite typing (MLMT) of strains obtained from Turkish and Syrian patients. A total of 38 cryopreserved strains and 20 Giemsa-stained smears were included in the present study. MLMT was performed using 12 highly specific microsatellite markers. Delta K (ΔK) <b>calculation</b> <b>and</b> Bayesian <b>statistics</b> were used to determine the population structure. Three main populations (POP A, B and C) were identified and further examination revealed the presence of three subpopulations for POP B and C. Combined analysis was performed using the data of previously typed L. tropica strains and Mediterranean and Şanlıurfa populations were identified. This finding suggests that the epidemiological status of L. tropica is more complicated than expected when compared to previous studies. A new population, comprised of Syrian L. tropica samples, was reported {{for the first time in}} Turkey, and the data presented here will provide new epidemiological information for further studies...|$|R
40|$|An {{inexpensive}} microcomputer, programmed inhouse, {{was linked}} to a commercially installed coronary care computer-monitoring system in order to expand the latter's capability and utility. While the commercial unit was only able to monitor and store real-time electrocardiographic and hemodynamic data, the combined system enabled the estimation of mass infarcted muscle, the measurement of left ventricular function, the determination of prognosis, <b>calculation</b> of <b>statistics</b> <b>and</b> the generation of clinical summaries on each patient. The feasibility of this computer interaction in a community hospital setting was demonstrated and the resulting additional diagnostic and therapeutic parameters were found relevant to medical management on the coronary care ward...|$|R
40|$|We present Monte Carlo {{studies of}} charge {{expectation}} values and charge fluctuations for quasi-particles in the quantum Hall system. We {{have studied the}} Laughlin wave functions for quasi-hole and quasi-electron, and also Jain’s definition of the quasi-electron wave function. The considered systems consist of from 50 to 200 electrons, and the filling fraction is 1 / 3. For all quasi-particles our calculations reproduce well the expected values of charge; − 1 / 3 times the electron charge for the quasi-hole, and 1 / 3 for the quasi-electron. Regarding fluctuations in the charge, our results for the quasi-hole and Jain quasi-electron {{are consistent with the}} expected value zero in the bulk of the system, but for the Laughlin quasi-electron we find small, but significant, deviations from zero throughout the whole electron droplet. We also present Berry phase <b>calculations</b> of charge <b>and</b> <b>statistics</b> parameter for the Jain quasi-electron, calculations which supplement earlier studies for the Laughlin quasi-particles. We find that the statistics parameter is more well behaved for the Jain quasi-electron than it is for the Laughlin quasi-electron. Supported by The Norwegian Research Council...|$|R
40|$|This {{thesis is}} {{oriented}} to the reliability theory and diagnostics. It involves brief explanation of terms {{that exists in}} a branches of technical devices reliability. The next part contains determining the reliability indexes, <b>calculations</b> of probabilities <b>and</b> <b>statistics</b> for electronic devices. I do mention the different methods of reliability diagnostics. The three most used of them are described. First {{of them is the}} reliability block model. By this I describe the serial reliability model, the parallel reliability model and the combination of them. Using this method is the simple way to show how each component affects reliability of the whole system. The second method is FMEA (Failure Mode and Effects Analysis) which is used mostly in the begining of the system designing. The third method is FTA (Fault tree analysis) {{which is one of the}} classic deductive methods. The FTA method is a graphical reliability model of a given system. By the end of this thesis, process of calculating the failure probability and the other reliability indexes of a given komponent (preamplifier with operational amplifier) is shown...|$|R
40|$|There are 38975 Fermat pseudoprimes (base 2) up to 10 11, 101629 up to 10 12 and 264239 up to 10 13 : we {{describe}} the <b>calculations</b> <b>and</b> give some <b>statistics.</b> The numbers were generated {{by a variety of}} strategies, the most important being a back-tracking search for possible prime factorisations, and the computations checked by a sieving technique. 1 Introduction A (Fermat) pseudoprime (base 2) is a composite number N with the property that 2 N Γ 1 j 1 mod N. For background on pseudoprimes and primality tests in general we refer to Bressoud [1], Brillhart et al [2], Koblitz [4], Ribenboim [12] and [13] or Riesel [14]. Previous tables of pseudoprimes were computed by Pomerance, Selfridge and Wagstaff [11]. We have shown that there are 38975 pseudoprimes up to 10 11, 101629 up to 10 12 and 264239 up to 10 13; all have at most 9 prime factors. Let P (X) denote the number of pseudoprimes less than X and let P (d; X) denote the number with exactly d prime factors. In [...] ...|$|R
50|$|In Windows 7, {{separate}} programmer, statistics, unit conversion, date <b>calculation</b> <b>and</b> worksheets modes were added. Tooltips were removed. Furthermore, Calculator's interface was revamped for {{the first}} time since its introduction. The base conversion functions were moved to the programmer mode <b>and</b> <b>statistics</b> functions were moved to the statistics mode. Switching between modes does not preserve the current number, clearing it to 0.|$|R
40|$|At {{the present}} time the {{continuous}} present changes in the environment demand the opportune participation of a qualified management, capable of encumber to the organization toward the productivity and the excellence. The theories on the management of companies have oscillated among the scientific and basically quantitative method proposed by Taylor (1911) and a dynamic, creative style that that according to Hernández (2002), they constitute some {{of the limits of}} the manager's profile, in which you/they stand out elements based on the manager's personality more than in the <b>calculations</b> <b>and</b> the <b>statistics,</b> considering the leadership like important element of the management. In both ends, it arises as objective to analyze the epistemology like doctrine of the foundations and methods of the scientific knowledge of the managerial formation, under a focus transdisciplinary and holistic. The used paradigm is the holistic, which faces all the elements and it constitutes an open system as Montero he/she refers (1999), in the one which the things and people are crisscross with all the organizations. It is demanded {{in the face of the}} existent necessities a formation managerial transdisciplinary, a vision shared toward the handling of a series of concepts that you/they allow him the effective and competitive boarding. The combination of both theories, as much the rational one as the administration of the individual's training or managerial Formation, they will allow the obtaining of in agreement results then with the new demands outlined to the management, in times of evolving toward the sustainable and sustainable development...|$|R
40|$|Abstract: The {{evaluation}} of interactive systems {{has been an}} active subject of research for many years. Many methods and tools have been proposed {{but most of them}} do not take architectural specificities of agent-based interactive systems into account. In addition, electronic informers are popular evaluation tools but current ones have often some limits. In order to solve these problems, we propose an electronic informer to evaluate agent-based interactive systems. This tool captures interaction events occurred in agent-based interactive systems and then, based on such captured data, it realizes treatments such as <b>calculations,</b> <b>statistics</b> <b>and</b> generates Petri Nets (PNs) to assist evaluators in evaluating three aspects of the system: user interface, non-functional properties (e. g. response time, reliability, etc.) and user’s properties (e. g. abilities, preferences, etc.). The approach has been validated by applying it to evaluate an agent-based interactive system used for the supervision of urban transport network...|$|R
40|$|Abstract In 2004, Garcia-Berthou and Alcaraz {{published}} "Incongruence between test <b>statistics</b> <b>and</b> P {{values in}} medical papers," {{a critique of}} statistical errors that received {{a tremendous amount of}} attention. One of their observations was that the final reported digit of p-values in articles published in the journal Nature departed substantially from the uniform distribution that they suggested should be expected. In 2006, Jeng critiqued that critique, observing that the statistical analysis of those terminal digits had been based on comparing the actual distribution to a uniform continuous distribution, when digits obviously are discretely distributed. Jeng corrected the <b>calculation</b> <b>and</b> reported <b>statistics</b> that did not so clearly support the claim of a digit preference. However delightful it may be to read a critique of statistical errors in a critique of statistical errors, we nevertheless found several aspects of the whole exchange to be quite troubling, prompting our own meta-critique of the analysis. The previous discussion emphasized statistical significance testing. But there are various reasons to expect departure from the uniform distribution in terminal digits of p-values, so that simply rejecting the null hypothesis is not terribly informative. Much more importantly, Jeng found that the original p-value of 0. 043 should have been 0. 086, and suggested this represented an important difference because it {{was on the other side}} of 0. 05. Among the most widely reiterated (though often ignored) tenets of modern quantitative research methods is that we should not treat statistical significance as a bright line test of whether we have observed a phenomenon. Moreover, it sends the wrong message about the role of statistics to suggest that a result should be dismissed because of limited statistical precision when it is so easy to gather more data. In response to these limitations, we gathered more data to improve the statistical precision, and analyzed the actual pattern of the departure from uniformity, not just its test statistics. We found variation in digit frequencies in the additional data and describe the distinctive pattern of these results. Furthermore, we found that the combined data diverge unambiguously from a uniform distribution. The explanation for this divergence seems unlikely to be that suggested by the previous authors: errors in <b>calculations</b> <b>and</b> transcription. </p...|$|R
