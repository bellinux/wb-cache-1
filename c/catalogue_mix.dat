0|16|Public
5000|$|Editorial {{collaboration}} to {{the exhibition}} <b>catalogue</b> <b>Mixing</b> Memory and Desire, Museum of Art Lucerne.|$|R
50|$|Grand Central Translation is a DJ mix {{album of}} Grand Central Records back <b>catalogue,</b> <b>mixed</b> by Philadelphia's Qool DJ Marv.|$|R
50|$|Economies {{ranging from}} the United States to Cuba have been <b>catalogued</b> as <b>mixed</b> economies. The term {{is also used to}} {{describe}} the economies of countries which are referred to as welfare states, such as the Nordic countries. Governments in mixed economies often provide environmental protection, maintenance of employment standards, a standardized welfare system, and maintenance of competition.|$|R
5000|$|In 2010, Partridge {{announced}} that a follow-up to 'Rag And Bone Buffet' entitled 'Bric-a-Brac Breakfast' was in the pipeline and he asked XTC fans via his own APE Blog which tracks should be considered for inclusion on this new compilation. [...] As of 2016, this collection has not yet appeared, but a new re-release campaign was {{announced that}} would involve their entire <b>catalogue</b> being <b>mixed</b> in 5.1 surround sound by Steven Wilson and released in expanded editions beginning with their 1992 album Nonsuch in 2013.This was followed by Drums and Wires, Oranges & Lemons, Skylarking, and Black Sea, released yearly. In 2016, Andy Partridge said on Twitter that an XTC documentary was being made featuring interviews with all band members apart from Barry Andrews.|$|R
50|$|For a few years, {{the scheme}} was so widely adopted {{that it was}} {{referenced}} in pop songs. But it suffered when Tesco ceased to use it, {{as part of a}} price-cutting policy that became standard nationwide. To retain business, Green Shield allowed customers to buy gifts from the <b>catalogue</b> with a <b>mix</b> of stamps and cash, but soon the catalogue became cash-only, and the operation was re-branded as Argos. Stamps were withdrawn altogether in 1991.|$|R
40|$|What is {{the image}} of our country’s {{cultural}} heritage? Unfortunately it seemingly shows an image which is still fixed by taxonomic categories which, by forcedly separating cultural areas, products and actions, break up the unitarity that, even in local specificities, pervades our culture. In this regard, we should attempt to provide a fresh image, trying to regenerate it in the knowledge of that transformative process which, over time and in particular productive, socio-cultural and territorial contexts, has played a role in widening the very concept of cultural heritage. It is a question of adopting a change in perspective leading to actions addressed to shaping an image with which to enhance the parts and the whole, to form a cultural panorama that depicts places in their wide range of forms, expressions and productions. This can therefore provide the people with that sense of identity from which emerges the respect for cultural differences characterizing contemporaneity. The primary condition to depict this type of image may clearly be ascribed to the concept of freedom. Freedom to address different themes, genres, products, areas, events and so on, in a <b>cataloguing</b> <b>mix</b> that paints a picture where identity is also deduced from strong contrasts, from apparently incoherent light and shade. “Freedom of panorama” to be able to choose, represent and interpret following the standard norms, but irrespective of field restrictions and implemental prescriptions, even the highest expressions of cultural heritage. This paper intends to recommend some working hypotheses to paint the identity picture of some territorial areas by freely selecting, representing and interpreting the related tangible or intangible cultural heritage. A contemporary picture that also takes into account the multiculturality as central and unavoidable in describing cultural heritage as an open, living system...|$|R
50|$|On 21 May 2012, Prydz {{released}} his debut artist album as a 3 disc set through Virgin Records. Disc 1 {{is made up}} of new unreleased Pryda productions, which some fans may have heard as works in progress (including Shadows, Agag, Mighty Love, Allein and the intro edit of Pjanoo). Discs 2 and 3 bring together many of the classic tracks from the Pryda <b>catalogue,</b> sequenced and <b>mixed</b> by the man himself, including some of his special re-edits.|$|R
50|$|This LP {{was also}} {{released}} in New Zealand in stereo {{in time for}} Christmas 1966. The pressing plates were obtained from EMI (UK) and are identical to their export-release. The title on the record label reads Beatles IV, and the catalogue number is PCSM 6042.Beatles VI is available on CD as part of The Capitol Albums, Volume 2 box set in both stereo and mono <b>mixes</b> (<b>catalogue</b> number CDP 0946 3 57499 2 2.) In 2014, Beatles VI was issued on CD again, individually {{and as part of}} The U.S. Albums boxed set.|$|R
40|$|Background: The MYC {{transcription}} {{factors are}} known {{to be involved in the}} biology of many human cancer types. But little is known about the Myc/microRNAs cooperation in the regulation of genes at the transcriptional and post-transcriptional level. Methodology/Principal Findings: Employing independent databases with experimentally validated data, we identified several mixed microRNA/Transcription Factor Feed-Forward Loops regulated by Myc and characterized completely by experimentally supported regulatory interactions, in human. We then studied the statistical and functional properties of these circuits and discussed in more detail a few interesting examples involving E 2 F 1, PTEN, RB 1 and VEGF. Conclusions/Significance: We have assembled and characterized a <b>catalogue</b> of human <b>mixed</b> Transcription Factor/ microRNA Feed-Forward Loops, having Myc as master regulator and completely defined by experimentally verified regulatory interactions...|$|R
5000|$|The Beatles in Mono was {{released}} {{to reflect the}} fact that most of the Beatles' <b>catalogue</b> was originally <b>mixed</b> and released in the monophonic format. Stereo recordings were a fairly new concept for pop music in the 1960s and did not become standard until late in that decade. This explains why the Beatles' initial album releases were mixed for mono. By the late sixties, however, stereo recording for pop music was becoming more popular and, thus, the new standard. Therefore, the last few Beatles albums—Yellow Submarine, Abbey Road, and Let It Be—were mixed and released only in stereo. Many feel that the mono mixes reflect the true intention of the band. For example, in the case of Sgt. Pepper's Lonely Hearts Club Band, all the mono mixes were done together with the Beatles themselves, throughout the recording of the album, whereas the stereo mixes were done in only six days by Abbey Road personnel George Martin, Geoff Emerick and Richard Lush after the album had been finished, with none of the Beatles attending. George Harrison commented: ...|$|R
40|$|In {{regions with}} large, mature fault systems, a {{characteristic}} earthquake model {{may be more}} appropriate for modelling earthquake occurrence than extrapolating from a short history of small, instrumentally observed earthquakes using the Gutenberg–Richter scaling law. We illustrate how the geomorphology and geodesy of the Malawi Rift, a region with large seismogenic thicknesses, long fault scarps, and slow strain rates, {{can be used to}} assess hazard probability levels for large infrequent earthquakes. We estimate potential earthquake size using fault length and recurrence intervals from plate motion velocities and generate a synthetic catalogue of events. Since {{it is not possible to}} determine from the geomorphological information if a future rupture will be continuous (7. 4 ≤ M W ≤ 8. 3 with recurrence intervals of 1, 000 – 4, 300 years) or segmented (6. 7 ≤ M W ≤ 7. 7 with 300 – 1, 900 years), we consider both alternatives separately and also produce a <b>mixed</b> <b>catalogue.</b> We carry out a probabilistic seismic hazard assessment to produce regional- and site-specific hazard estimates. At all return periods and vibration periods, inclusion of fault-derived parameters increases the predicted spectral acceleration above the level predicted from the instrumental catalogue alone, with the most significant changes being in close proximity to the fault systems and the effect being more significant at longer vibration periods. Importantly, the results indicate that standard probabilistic seismic hazard analysis (PSHA) methods using short instrumental records alone tend to underestimate the seismic hazard, especially for the most damaging, extreme magnitude events. For many developing countries in Africa and elsewhere, which are experiencing rapid economic growth and urbanisation, seismic hazard assessments incorporating characteristic earthquake models are critical...|$|R
40|$|We {{present the}} DARTH FADER algorithm, a new wavelet-based method for {{estimating}} redshifts of galaxy spectra in spectral surveys {{that is particularly}} adept in the very low SNR regime. We use a standard cross-correlation method to estimate the redshifts of galaxies, using a template set built using a PCA analysis {{on a set of}} simulated, noise-free spectra. Darth Fader employs wavelet filtering to both estimate the continuum & to extract prominent line features in each galaxy spectrum. A simple selection criterion {{based on the number of}} features present in the spectrum is then used to clean the catalogue: galaxies with fewer than six total features are removed as we are unlikely to obtain a reliable redshift estimate. Applying our wavelet-based cleaning algorithm to a simulated testing set, we successfully build a clean catalogue including extremely low signal-to-noise data (SNR= 2. 0), for which we are able to obtain a 5. 1 % catastrophic failure rate in the redshift estimates (compared with 34. 5 % prior to cleaning). We also show that for a <b>catalogue</b> with uniformly <b>mixed</b> SNRs between 1. 0 & 20. 0, with realistic pixel-dependent noise, it is possible to obtain redshifts with a catastrophic failure rate of 3. 3 % after cleaning (as compared to 22. 7 % before cleaning). Whilst we do not test this algorithm exhaustively on real data, we present a proof of concept of the applicability of this method to real data, showing that the wavelet filtering techniques perform well when applied to some typical spectra from the SDSS archive. The Darth Fader algorithm provides a robust method for extracting spectral features from very noisy spectra. The resulting clean catalogue gives an extremely low rate of catastrophic failures, even when the spectra have a very low SNR. For very large sky surveys, this technique may offer a significant boost in the number of faint galaxies with accurately determined redshifts. Comment: 22 pages, 15 figures. Accepted for publication in Astronomy & Astrophysic...|$|R
40|$|International audienceContext. Accurate {{determination}} of the redshifts of galaxies comes from the identification of key lines in their spectra. Large sky surveys, and {{the sheer volume of}} data they produce, have made it necessary to tackle this identification problem in an automated and reliable fashion. Current methods attempt to do this with careful modelling of the spectral lines and the continua, or by employing a flux/magnitude or a signal-to-noise cut to the dataset in order to obtain reliable redshift estimates for the majority of galaxies in the sample. Aims. In this paper, we present the Darth Fader algorithm (denoised and automatic redshifts thresholded with a false detection rate), which is a new wavelet-based method for estimating redshifts of galaxy spectra. Automated, simple, and largely empirical, we highlight how the Darth Fader algorithm performs in the very low signal-to-noise regime, and demonstrate its effectiveness at removing catastrophic failures from the catalogue of redshift estimates. Methods. We present a new, nonparametric method for estimating and removing the continuum in noisy data that requires no a priori information about the galaxy properties. This method employs wavelet filtering based on a tuneable false detection rate (FDR) threshold, which effectively removes the noise in the spectrum, and extracts features at different scales. After removal of the continuum, the galaxy spectra are then cross-correlated with the eigentemplates, and a standard χ 2 -minimisation used to determine the redshift of the spectrum. FDR filtering is applied to the spectra a second time to determine the number of spectral features in each galaxy spectrum, and those with fewer than six total features are removed from the catalogue as we are unlikely to obtain a reliable and correct estimate of the redshift of such spectra. Results. Applying our wavelet-based cleaning algorithm on a simulated testing set, we can successfully build a clean catalogue including extremely low signal-to-noise data (S/N = 2. 0), for which we are able to obtain a 5. 1 % catastrophic failure rate in the redshift estimates (compared with 34. 5 % prior to cleaning). We also show that for a <b>catalogue</b> with uniformly <b>mixed</b> signal-to-noise ratios between 1. 0 and 20. 0, with realistic pixel-dependent noise, it is possible to obtain redshifts with a catastrophic failure rate of 3. 3 % after cleaning (as compared to 22. 7 % before cleaning). Whilst we do not test this algorithm exhaustively on real data, we present a proof of concept of the applicability of this method to real data, showing that the wavelet filtering techniques perform well when applied to some typical spectra from the Sloan Digital Sky Survey archive. Conclusions. The Darth Fader algorithm provides a robust method for extracting spectral features from very noisy spectra: spectra for which a reliable redshift cannot be measured are automatically identified and removed from the input data set. The resulting clean catalogue, although restricted in number, gives an extremely low rate of catastrophic failures, even when the spectra have a very low S/N. For very large sky surveys, this technique may offer a significant boost in the number of faint galaxies with accurately determined redshifts...|$|R
40|$|International audienceThe HYDROPTIMET {{case studies}} (9 ? 10 June 2000 Catalogne, 8 ? 9 September 2002 Cévennes and 24 ? 26 November 2002 Piémont) appear to {{encompass}} {{a sort of}} prototype flash-flood situations in the western Mediterranean attending to the relevant synoptic and mesoscale signatures identified on the meteorological charts. In Catalogne, the convective event was driven by a low-pressure system of relatively small dimensions developed over the mediterranean coast of Spain that moved into southern France. For Cévennes, the main circulation pattern was a synoptic-scale Atlantic low which induced a persistent southerly low-level jet (LLJ) over the western Mediterranean, strengthened by the Alps along its western flank, which guaranteed continuous moisture supply towards southern France where the long-lived, quasistationary convective system developed. The long Piémont episode, very representative of the most severe alpine flash flood events, shares some similarities with the Cévennes situation during its first stage {{in that it was}} controlled by a southerly moist LLJ associated with a large-scale disturbance located to the west. However, these circulation features were transient aspects and {{during the second half of}} the episode the situation was dominated by a cyclogenesis process over the Mediterranean which gave place to a mesoscale-size depression at surface that acted to force new heavy rain over the slopes of the Alps and maritime areas. That is, the Piémont episode can be <b>catalogued</b> as of <b>mixed</b> type with regard to the responsible surface disturbance, evolving from a large-scale pattern with remote action (like Cévennes) to a mesoscale pattern with local action (like Catalogne). A prominent mid-tropospheric trough or cut-off low can be identified in all events prior and during the period of heavy rain, which clearly served as the precursor agent for the onset of the flash-flood conditions and the cyclogenesis at low-levels. Being aware of the uncertainty in the representation of the upper-level disturbance and the necessity to cope with it within the operational context when attempting to issue short to mid-range numerical weather predictions of these high impact weather events, a systematic exploration of the predictability of the three selected case studies subject to uncertainties in the representation of the upper-level precursor disturbance is carried out in this paper. The study is based on an ensemble of mesoscale numerical simulations of each event with the MM 5 non-hydrostatic model after perturbing in a systematic way the upper-level disturbance, in the sense of displacing slightly this disturbance upstream/downstream along the zonal direction and intensifying/weakening its amplitude. These perturbations are guided by a previous application of the MM 5 -adjoint model, which consistently shows high sensitivities of the dynamical control of the heavy rain to the flow configuration about the upper-level disturbance on the day before, thus confirming the precursor characteristics of this agent. The perturbations are introduced to the initial conditions by applying a potential vorticity (PV) inversion procedure to the positive PV anomaly associated with the upper-level disturbance, and then using the inverted fields (wind, temperature and geopotential) to modify under a physically consistent balance the model initial fields. The results generally show that the events dominated by mesoscale low-level disturbances (Catalogne and last stage of the Piémont episode) are very sensitive to the initial uncertainties, such that the heavy rain location and magnitude are in some of the experiments strongly changed in response to the "forecast errors" of the cyclone trajectory, intensity, shape and translational speed. In contrast, the other situations (Cévennes and initial stage of the Piémont episode), dominated by a larger scale system wich basically acts to guarantee the establishment and persistence of the southerly LLJ towards the southern France-north Italy orography, exhibit much higher predictability. That is, the slight modifications in the LLJ direction and intensity encompassed by the ensemble of perturbed forecasts are less critical with respect to the heavy precipitation potential and affected area...|$|R
40|$|The {{proposed}} {{research is}} based on past years studies on innovation in biometric systems, {{which has led to}} verifying how, in all the biometry approach, exists a similar approach, defined in the research Biometry Paradigm, which {{is based on}} the identification and utilization of particular characteristics, having the properties to be univocal and measurable. Typical examples are fingerprint, iris, retina, hand geometry, face and so on. Starting from this consideration, we have tried to verify if a similar approach could be applied to valuable objects, such as banknotes, artworks, identification documents and, in general, to all the objects which have the necessity to be verified to fight against their counterfeiting. The research work was based on the identification of some characteristics, which can be measured in a non-destructive way, inside some notable case studies, such as banknotes, passports, drug packages, lithography, with the aim to create a method, starting from the past experiences in the biometric analysis, which allows to verify with a great accuracy objects’ authenticity. The methods developed in this research have been defined as Hylemetric Authentication, from Greek words ὕλη (hyle), for inanimate objects and μετρον (metros) for measurement. The study started from the analysis of the existing solutions adopted to put in secureness banknotes, documents, artworks’ certificates and drug packages. Starting from this analysis, in each case study, has been possible identify a set of different security systems and approach. In particular, it is possible define two categories of security approach:  Overt Authentication/Identification System, visible at human eye and verifiable in a simple manner by the final utilizer.  Covert Authentication/Identification System, not simply visible or visible under particular environmental conditions and/or using particular instrumentations. Typical examples of overt systems are holograms and intaglio printing on banknotes, shifting and changing inks, used on pharmacologic packages. Covert systems are typically ultraviolet inks, infrared patterns and so on, used always on banknotes. Sometimes, mixed systems are also proposed, such as 1 D and 2 D barcodes, which at first analysis can be considered as overt systems, but the necessity to use dedicated and sometimes also complex decoding systems, allows to <b>catalogue</b> it as <b>mixed</b> system. In fact, its presence is sometimes considered as proof of originality, but in reality the originality can be proven only analysing the content of the barcode, which sometimes results encrypted. Focusing on research case studying, banknotes presents both overt and covert solutions, whereas overt or mixed ones in general characterize drugs packages. Lithography and artworks in general have been authenticated using only paper certificate of authenticity, without any automatic or semi-automatic verification system. In any case, this approach can be categorized as overt, because the presence of a Certificate of Authenticity with signature and stamp on it is generally considered sufficient to be sure on the artwork originality. Starting from this state-of-art, the research has tried to verify the possibility to apply the biometric approach to these case studies, with the aim to enforce the authentication and anti-counterfeiting process with an innovative solution. For making this, the first step is the identification, for each case study, of at least one univocal characteristic, present on the analyzed object at priori (e. g. Banknotes security fibres), or added on it (e. g. white light speckles added on drug packages), which has the requested requirements to be categorized as Hylemetric Characteristic. In particular a characteristic, to be considered as usable in the Hylemetric approach, has to have the following basic properties:  Uniqueness: every objects should be identifiable and distinguishable from all others;  Consistency: feature vector should be verifiable by multiple parties over the lifetime of the object;  Conciseness: feature vector should be short and easily computable;  Robustness: {{it should be possible to}} verify the feature vector even if the object has been subjected to harsh treatment;  Resistance to Forgery: it should be very difficult and costly, or impossible for an adversary, to forge a document by coercing a second object to express the same feature vector as the original one. For any of the case study analyzed during the research course, it has been possible to identify proper characteristics which have all these properties. The acquisition of them changes from object to object, has in biometry paradigm, depending on the object physical characteristics, but the verification approach is similar for all the analyzed ones. In banknote case, we have used metallic security fibres as Hylemetric characteristic. They have the physical property to shine in the visible spectrum when illuminated by ultraviolet light at a precise wavelength. We have acquired the fibres distribution, creating a unique identification pattern based on them. This pattern has been encrypted and then coded in a bidimensional barcode, based on DataMatrix ECC 200 Standard. The proposed encryption has been made by means of Elliptic Curve, to reduce dimensions of pattern encrypted version and cope with DataMatrix maximum storage constraint. For artworks in general, we have made some experiments on lithography and then on oil and statue. The proposed approach has to modify the Certificate of Authenticity, introducing both new information on the paper version and a new digital version of it. All the modifications are based on the extraction of an Hylemetric characteristic from the artwork. In Lithography case, the typical stone impression leave a unique grain pattern, acquirable using a digital camera. For oil paints and statue using particular image manipulations and filters is possible from the acquired image obtaining a speckle-like pattern from the object structure. Analyzing the pharmacologic products packaging, we have decided to add on them the Hylemetric characteristic, using the so called White Light Speckle technique. We have used ultraviolet ink sprayed on a package’ particular area. In this way each package has been stamped with a different Speckle Pattern. In any studied cases, independently from the acquired characteristic, has been possible defining a Hylemetric Template, which allows to authenticate the related object. In some cases the defined template has been converted into a 2 D Barcode to be directly put on the object (e. g. banknotes, drug packages), or has been codified to be inserted in a Digital Certificate of Authenticity (e. g. artworks). In the case of barcodes, we have used both standard DataMatrix, or a new 2 D Barcode based on Computer Generated Holograms, defined HoloBarcode. In some case the barcode has been proposed to be put on the object using infrared ink, to maintain the original object aspect and to increase the system security as well. The usage of infrared ink starting from the necessity to put it on banknotes, plenty of visible and invisible security artefacts. The infrared band is empty on over the 80 % of the banknote surface in both sides and it is possible to find the proper area where to put the barcode. Otherwise, the introduction of a non-standard bidirectional barcode, based on CGH, has been proposed in such cases where the object to be analysed can be heavily manipulated and ruined. HoloBarcode has the great advantage to be highly resistant to loss of information (i. e. loss of part of the barcode area) and to offer the possibility to extract information also starting from only a little part of the barcode. However, the storage capacity of these barcodes is very poor, if compared with standard 2 D Barcodes with same dimensions. In conclusion, during this research we have analysed the state-of-art of security features applied to valuable objects. Then, starting from a biometric approach, has been proposed an Hylemetric paradigm, which allows to authenticate objects starting from their intrinsic characteristics. We have made tests on different type of objects, such as banknotes, passports, lithography, oil paints, drug packages, obtaining in any case the correct definition of a Hylemetric Procedure, similar in any case, and a Hylemetric Template to be used in verification phase. After that we have also proposed a new bi-dimensional barcode, based on Synthetic Holograms, to be used in particular cases instead of Standard ones to store the Hylemetric Template for offline verification activities...|$|R

