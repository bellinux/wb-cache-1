5|10000|Public
40|$|Employers {{are looking}} for {{reducing}} execution time and maintaining {{the quality of the}} projects that are the main objective of the projects. In this article, we focus on crashing projects by con-sidering different factors such as cost, time, quality and risk. For the proposed integer linear model, cost of conformance and <b>cost</b> <b>of</b> <b>non-conformance</b> are considered as parts of the costs of quality of deliverables in projects. The cost of conformance consists of the costs of training the project team, inspection and test of deliverables. The <b>cost</b> <b>of</b> <b>non-conformance</b> also includes costs of rework and scrap. Project risk management is one of the important aspects of the pro-jects. The present study also considers the impact of risks, which is highly applicable in projects {{with a high level of}} uncertainty. Results are presented using integer programming approach with the aim of minimizing the costs of the project...|$|E
40|$|Inefficiencies in {{the process}} chain result in bloated {{operational}} costs and ostensibly pass on to the customer. It is essential to streamline process costs and categorise them appropriately as either costs of conformance or <b>cost</b> <b>of</b> <b>non-conformance.</b> In this way, hidden costs due to inefficiencies in the system can be identified. A sustained continuous improvement programme to systematically eliminate waste can be employed to achieve lean manufacturing or “cleaner production”. A number of factors contribute to costs in a foundry from procurement of scrap to the delivery of a casting...|$|E
40|$|In {{order to}} improve quality, an {{organization}} {{must take into account}} the costs associated with achieving quality since the objective of continuous improvement programs is not only to meet customer requirements, but also to do it at the lowest, possible, cost. This can only obtained by reducing the costs needed to achieve quality, and the reduction of these costs is only possible if they are identified and measured. Therefore, measuring and reporting the cost of quality (CoQ) should be considered an important issue for achieving quality excellence. To collect quality costs an organization needs to adopt a framework to classify costs; however, there is no general agreement on a single broad definition of quality costs. CoQ is usually understood as the sum of conformance plus non-conformance costs, where cost of conformance is the price paid for prevention of poor quality (for example, inspection and quality appraisal) and <b>cost</b> <b>of</b> <b>non-conformance</b> is the cost of poor quality caused by product and service failure (for example, rework and returns). The objective {{of this paper is to}} give a survey of research articles on the topic of CoQ; it opens with a literature review focused on existing CoQ models; then, it briefly presents the most common CoQ parameters and the metrics (indices) used for monitoring CoQ. Finally, the use of CoQ models in practice, i. e., the implementation of a quality costing system and cost of quality reporting in companies is discussed, with emphasis in cases concerning manufacturing firms...|$|E
40|$|This {{investigation}} {{attempts to}} examine the influences of validation on pharmaceutical processes especially at a new manufacturing facility that has to meet international requirements, and fulfil a cost effective business strategy. At Aspen Pharmacare, a pharmaceutical organisation, there are two manufacturing facilities situated adjacent to each other, one new and one old. The new facility creates ideal opportunities to supply products to local and international markets. The investigation compares legal requirements from local and international regulatory authorities. Validation and qualification practices {{as well as the}} problems encountered during the different phases are discussed. Particular attention is given to the validation approach at the new Aspen facility. Problems and proposed solutions relating to the design review, installation, operational, and performance qualification are discussed. Validation of analytical methods for cleaning analysis, cleaning validation of equipment, and optimisation of some tablet manufacturing processes are described. Statistical evaluations of analytical results are included to find the optimum conditions for integrating new personnel with new processes and equipment. A business model reviews the <b>cost</b> <b>of</b> <b>non-conformances</b> <b>of</b> the enalapril maleate 10 mg tablets manufactured at the two manufacturing facilities. Finally the dissertation proves that validation is not only a regulatory requirement but that it also provides benefits such as adding value to the business, and ultimately reducing the <b>cost</b> <b>of</b> medicines...|$|R
5000|$|The {{interpretation}} <b>of</b> <b>non-conformance</b> {{depends on}} {{the purpose of the}} model: ...|$|R
40|$|The {{choice of}} {{procedure}} during public procurement is assumed to affect the overall result of the project. When studying the construction industry; delivery methods, payment systems and project specific variables may affect the end result. The question here is if project success is predicted by choice of procurement procedures. A survey was done on 222 road and railroad construction projects in Sweden between 2007 and 2010, collecting expected and actual empirical data on cost and time overruns and number <b>of</b> <b>non-conformances</b> from each project. The conclusion is that procurement procedures do not predict cost and time overrun nor do they predict number <b>of</b> <b>non-conformances</b> during inspection...|$|R
40|$|Variation in the {{manufactured}} {{geometry of}} engineering components is perpetually present in production. Random variation can arise due to slight differences in material properties, machines and tools, processes and even climatic {{conditions in the}} factory. To guarantee the functionality or quality of individual components, features are inspected to verify they conform to the tolerance limits imposed. It is undesirable to produce nonconforming features, due {{to the cost of}} reworking features or scrapping components. In practice, it is not always feasible to improve manufacturing capability (reduce variation), or design components to be less susceptible to variation; in such a situation the <b>cost</b> <b>of</b> <b>non-conformance</b> should be minimised. Optimal Mean Setting, a methodology to maximise profit from a production system where the manufacturing variation is often greater than a feature's tolerance limits, can be applied in these circumstances. Although the principle of Optimal Mean Setting dates back over 60 years, its application to engineering design is relatively undeveloped. A major part of this thesis was devoted to developing a robust, reliable and generalised framework to practice Optimal Mean Setting in engineering design. Errors were uncovered in previous attempts in the literature relating to Optimal Mean Setting of simple systems. Improvements to the maximum obtainable profit were also realised by implementing a new optimisation strategy to that developed in the literature. Another innovation developed in this thesis was the the application of copula function modelling to Optimal Mean Setting. Copulas allowed joint distributions to be created from non-parametric (or non family specific) feature variation distributions. This permitted Optimal Mean Setting to be applied to components with several quality characteristics where different distributions modelled the manufacturing variation. It also allowed the final geometry of a component to be modelled to access the distribution of performance of a batch of components. Numerical examples and the applications to real components are given...|$|E
40|$|The quality <b>cost</b> <b>of</b> <b>non-conformance</b> {{associated}} with first run production builds is typically {{more than five}} times that of later production runs. If a manufacturing organization is to gain market share and increase its profitability, it must explore methods of accelerating its learning curves through defect prevention. Current 2 ̆ 2 Transition to Production 2 ̆ 2 concept methodologies attempt with limited success to accelerate organizational learning through Design for Manufacturability (DFM), design phase dimensional management studies, manufacturing floor statistical methods (SPC, DOE, etc.), and various qualitative strategies. While each of these techniques are effective to some degree in reducing future nonconformances, an integrated, design-phase approach utilizing current technology is needed. ^ 2 ̆ 2 Virtual Process Capability 2 ̆ 2 (VPC) is a methodology for integrating statistical process capability knowledge directly into the hardware design phase, resulting in the improved performance and reduced product costs typically {{associated with}} mature product manufacturing. The intent behind the methodology is to realistically simulate the manufacture of hardware products by understanding their underlying model equations and the statistical distributions of each involved contributing parameter. Once each product has been simulated and an expected percentage defective has been estimated, mathematical programming and statistical quality engineering techniques are then utilized for improvement purposes. Data taken from the practical application of this methodology at Raytheon Aircraft has conservatively estimated that for each dollar invested ten are saved. ^ As a technical extension to this developed methodology, statistical insights and methods are provided as to how product and process improvement analysis is best accomplished. Included within this area of discussion is the statistical development and validation of improved measures for the more efficient detection of dispersion and mean effects than that of more traditional methods. Additionally, the use of mathematical programming techniques is creatively employed as an improved mechanism in the optimization of nominal-the-best type problems. ...|$|E
40|$|Cockpit {{alerting}} systems monitor potentially hazardous situations, {{both inside}} and outside the aircraft. When a hazard is projected to occur, the alerting system displays alerts and/or command decisions to the pilot. However, pilots have been observed to not conform to alerting system commands by delaying their response or by not following the automatic commands exactly. This non-conformance to the automatic alerting system can reduce its benefit. Therefore, a need exists to understand the causes and effects <b>of</b> pilot <b>non-conformance</b> in order to develop automatic alerting systems whose commands the pilots are more likely to follow. These considerations were examined through flight simulator evaluations of the collision avoidance task during closely spaced parallel approaches. This task provided a useful case-study because the effects <b>of</b> <b>non-conformance</b> can be significant, given the time-critical nature of the task. A preliminary evaluation of alerting systems identified non-conformance in over 40 % of the cases and a corresponding drop in collision avoidance performance. A follow-on experiment found subjects' alerting and maneuver selection criteria were consistent with different strategies than those used by automatic systems, indicating the pilot may potentially disagree with the alerting system if the pilot attempts to verify automatic alerts and commanded avoidance maneuvers. A final experiment found supporting automatic alerts with the explicit display of its underlying criteria resulted in more consistent subject reactions. In light of these experimental results, a general discussion <b>of</b> pilot <b>non-conformance</b> is provided. Contributing factors in pilot non-conformance include a lack of confidence in the automatic system and mismatches between the alerting system's commands and the pilots' own decisions based on the information available to them. The effects <b>of</b> <b>non-conformance</b> on system performance are discussed. Possible methods of reconciling mismatches are given, and design considerations for alerting systems which alleviate the problem <b>of</b> <b>non-conformance</b> are provided...|$|R
40|$|An ongoing {{production}} process produces products with quality characteristics following a known probability distribution. Two process states, in-control and out-of-control, are assumed. The process {{is subject to}} complete inspection. Whenever the quality characteristic of a product produced exceeds a pre-determined action limit, remedial action is taken to restore the process to the in-control state. In addition, the decision maker has a learning opportunity to improve the process by investing resources to identify and eliminate the causes <b>of</b> <b>non-conformance</b> to the target characteristic value. A learning action taken would reduce the probability of shifting from the in-control state to the out-of-control state. A cost model is developed in this paper to determine the optimal number of learning {{actions to be taken}} and the optimal action limit. The model gives insight into the tradeoff <b>of</b> <b>cost</b> <b>of</b> quality and <b>cost</b> <b>of</b> prevention. © 1996 Taylor & Francis Ltd. link_to_subscribed_fulltex...|$|R
40|$|One of the {{precepts of}} {{traditional}} models of software development is that detailed specifications, that is, what precisely the software should do, must precede {{the design and}} then creation of the code. This paper details scenarios <b>of</b> <b>non-conformance</b> to this model, and explores the issues that arise from {{the failure of the}} model...|$|R
40|$|Abstract: Oil and gas {{exploration}} and production (E&P) is the main activity for the offshore economy. Yet, this it is also responsible {{for many of the}} major environmental accidents. Environmental legislation acts strongly in this type of activity, through limitations in the exploration processes and control procedures for the generation, storage, transportation and disposal of waste, applying penalties in cases <b>of</b> <b>non-conformances,</b> such as oil spills at sea and air pollutants emissions. In this context, waste management is very important for these operations, because of the complexity and dangerousness with the waste generated, and the <b>cost</b> <b>of</b> their destination. This work presents the offshore waste management system, introducing the procedures and controls according the requirements of the Brazilian Guideline for Environmental Audits, DZ- 056 -R. 3...|$|R
40|$|This paper compares our {{proposed}} {{method of}} using the modified tolerance region (MTR) to the generalised distance (GD) approach for estimating the probability of non-conforming product. For correlated quality characteristics, the tolerance region needs to be redefined so that the model is more stringent in identifying potentially poor product quality, as perceived by customers. The paper shows that the MTR approach is more effective in estimating the proportion <b>of</b> <b>non-conformance</b> (PNC) for correlated quality characteristics {{as compared to the}} GD approach. We also compare the multivariate capability index computed using the PNC values obtained from both the GD and MTR methods. The estimation of PNC assists not only in the evaluation of process performance and capability but also in the quantification <b>of</b> <b>cost</b> <b>of</b> poor quality...|$|R
40|$|In {{this note}} we examine using Genzbretz and Miwa {{algorithms}} to improve estimation <b>of</b> proposition <b>of</b> <b>non-conformance</b> in multivariate normal distributions. This estimation {{is required in}} the procedure outlined in Abbasi and Niaki (Int J Adv Manuf Technol 50 (5 - 8) : 823 - 830, 2010) to determine process capability index of multivariate non-normal processes...|$|R
50|$|The EPA {{recognized}} Navistar's imminent non-compliance {{and created}} a system <b>of</b> <b>Non-Conformance</b> Penalties (NCPs) that included a $1,919 fine for every non-compliant engine that Navistar sold. To bridge the gap, Navistar began using EPA credits it had previously earned for being compliant in lieu of paying fines. In August 2012, Navistar stated they would run out of EPA credits soon. Only days earlier the EPA announced increased new penalties of $3,744 per engine.|$|R
50|$|He {{therefore}} {{argued that}} quality engineering should {{start with an}} understanding <b>of</b> quality <b>costs</b> in various situations. In much conventional industrial engineering, the quality costs are simply represented {{by the number of}} items outside specification multiplied by the <b>cost</b> <b>of</b> rework or scrap. However, Taguchi insisted that manufacturers broaden their horizons to consider cost to society. Though the short-term costs may simply be those <b>of</b> <b>non-conformance,</b> any item manufactured away from nominal would result in some loss to the customer or the wider community through early wear-out; difficulties in interfacing with other parts, themselves probably wide of nominal; or the need to build in safety margins. These losses are externalities and are usually ignored by manufacturers, which are more interested in their private costs than social costs. Such externalities prevent markets from operating efficiently, according to analyses of public economics. Taguchi argued that such losses would inevitably find their way back to the originating corporation (in an effect similar to the tragedy of the commons), and that by working to minimise them, manufacturers would enhance brand reputation, win markets and generate profits.|$|R
40|$|The saolc {{package is}} the {{reference}} {{software for the}} Structured Audio part of the MPEG- 4 Audio standard (ISO 14496 - 3 Section 5). saolc provides non-real-time decoding of Structured Audio bitstreams, and demonstrates the proper functioning of a normative SA decoder. The structure of saolc is documented for implementors who wish {{to make use of}} the reference software, beginning at a high-level overview and proceeding to a list of important data structures and a module-by-module description. Bugs, extensions, and areas <b>of</b> <b>non-conformance</b> to the paper specification are documented. This documentation augments the internal documentation provided by comments in the code. Content...|$|R
40|$|This article {{presents}} {{a framework for}} integrating manufacturing knowledge within a design environment. An overview {{of past and present}} practice in the area of design and manufacturing collaboration is given. ‘Strangers’ are classified according to their degree of deviation from past designs. The method described makes use <b>of</b> <b>non-conformance</b> data in order to enable preliminary assessment of the producibility of a new design. This is enabled by relating the majority of the features on ‘strange’ components to Standard Features within the confines of a component template. Features that do not fit this criterion are investigated for producibility by means of manufacturing simulation. These concepts are contained in a software based tool that is integrated into design softwar...|$|R
40|$|During the 80 ’s {{and early}} 90 ’s, {{high-level}} programming languages supported two different type-checking philosophies: static typing, and dynamic typing. Statically typed programming languages ensured that all programs conformed {{to a set}} of typing rules, which implied that programs could not “go wrong”. Non-conforming programs were rejected. Dynamically typed programming languages, in contrast, did not reject programs, but rather checked for “type errors ” during program execution. ML and Scheme were the prototypical respresentatives of the static and dynamic approachs to typing. Each typing philosophy had distinct advantages and disadvantages. Static typing does not require as much run-time type checking, and the detection <b>of</b> <b>non-conformance</b> supplies programmers with potentially valuable debugging information. The disadvantage of static typing, however, is that are some well-written, meaningfu...|$|R
40|$|There is {{increasing}} pressure on corporations for sustainability reporting. However, current patterns in {{corporate sustainability reporting}} are not well understood. Additional {{research is needed to}} identify the contents of current reports and to provide a basis for improvement. The aim of this research is to analyze the sustainable development reporting patterns of Dutch companies. A content analysis of Dutch sustainability reports was conducted. The findings show that the contents of Dutch sustainability reports vary widely. While some areas in these reports are well developed, others – {{such as the use of}} cross-cutting indicators, linking sustainability initiatives with broader public policy, future reporting directions, systematic presentation of data, and discussion <b>of</b> <b>non-conformances</b> – require significant improvement. Copyright © 2012 John Wiley & Sons, Ltd and ERP Environment...|$|R
40|$|Test and Evaluation (T&E) is {{becoming}} increasingly important in the current acquisition paradigm {{as a way to}} ensure that the military equipment user receives equipment that conforms to its requirements. However, T&E really has several different roles: • Testing – determines the degree <b>of</b> <b>non-conformance</b> to requirements <b>of</b> “as-delivered ” equip-ment. • Evaluation- determines the capability of "as-delivered " or “as-built ” equipment. In recent times, recognizing that the documented requirements do not generally represent the true needs of the user, the T&E role has expanded itself to attempt to ensure that “as-delivered ” equipment meets the needs of the user. This paper discusses the reasons for, the differences between, and how modern quality theory, Infor-mation Technology and Knowledge Management, can improve the various roles of T&E...|$|R
40|$|Part 9 : PLM and Collaborative Product DevelopmentInternational audienceDuring {{all phases}} {{in the product}} life cycle quality is a key {{influencing}} factor. The earlier a high quality level reached during the product lifecycle, the lower the amount of quality deviations, required changes, and the occurrence <b>of</b> <b>non-conformance</b> <b>costs</b> throughout mass production. Already in the prototype phase, an efficient and effective quality control loop is an important enabler for achieving {{a high level of}} product quality. This includes a quality process and specified interfaces to product development, production planning and overall quality management. Presenting an approach for the development of such a quality control loop including interfaces to associated processes in product development, production planning and quality management as well as a first examination of affected process parts in production planning is subject of this paper...|$|R
30|$|By {{reviewing}} both {{primary and}} support {{activities in the}} value chain, {{it was found that}} most elements determine the performance of food supply chains. In primary activities, inbound logistics, operations and outbound logistics enhance food supply chain operations. Inbound logistics are concerned with receiving the food or materials from suppliers, storing these externally sourced food or materials and handling them within the firms. Operations are concerned with the production or value added of products and services in departments within the firm. Outbound logistics are concerned with distributing the food to the final customer. In service industries, firms regularly monitor food supply chain operations, and remedial action is taken in the event <b>of</b> <b>non-conformance.</b> Marketing and sales preserve the required quantity and quality of the food in the available market to maintain stability in cold chains (Verbic, 2006).|$|R
50|$|Corrective {{actions are}} {{implemented}} {{in response to}} customer complaints, unacceptable levels <b>of</b> product <b>non-conformance,</b> issues identified during an internal audit, as well as adverse or unstable trends in product and process monitoring such as would be identified by statistical process control (SPC). Preventive actions are implemented {{in response to the}} identification of potential sources of non-conformity.|$|R
40|$|Open {{computational}} {{systems are}} systems composed of self-interested software agents {{that have been}} developed by different parties. Characteristic features are agent heterogeneity, con-flicting individual goals and a possibility <b>of</b> <b>non-conformance</b> to the specifications of the sys-tems. Consequently, the activity of such systems needs to be governed by a framework with a formal, declarative, verifiable and meaningful semantics. This thesis addresses this requirement by presenting an executable specification of open com-putational systems. This specification exhibits the following characteristics: • It adopts the perspective of an external observer, as opposed to an agent’s own perspective whereby it reasons about how it should act. Furthermore, it views computational systems as instances of normative systems, that is, it describes the permissions and obligations of the members of these systems, considering the possibility that the behaviour of the members may deviate from the ideal. • It explicitly represents the institutionalised powers of the member agents, a standard fea-ture of any norm-governed interaction. Moreover, it maintains the standard, long estab...|$|R
40|$|As is well known, process {{capability}} analysis {{for more than}} one quality variables is a complicated and sometimes contentious area with several quality measures vying for recognition. When these variables exhibit non-normal characteristics, the situation becomes even more complex. The aim {{of this paper is to}} measure Process Capability Indices (PCIs) for bivariate non-normal process using the bivariate Burr distribution. The univariate Burr distribution has been shown to improve the accuracy of estimates of PCIs for univariate non-normal distributions (see for example, [7] and [16]). Here, we will estimate the PCIs of bivariate non-normal distributions using the bivariate Burr distribution. The process of obtaining these PCIs will be accomplished in a series of steps involving estimating the unknown parameters of the process using maximum likelihood estimation coupled with simulated annealing. Finally, the Proportion <b>of</b> <b>Non-Conformance</b> (PNC) obtained using this method will be compared with those obtained from variables distributed under the bivariate Beta, Weibull, Gamma and Weibull-Gamma distributions...|$|R
40|$|This work revisits the setuid {{family of}} calls for {{privilege}} management that is implemented in several widely-used operating systems. Three {{of the four}} commonly used calls in the family are standardized by POSIX. The work investigates {{the current status of}} setuid and, in the process, challenges some assertions in prior work. It addresses three sets of questions with regards to the setuid family: (1) Is the POSIX standard indeed broken as prior work suggests? (2) Are implementations POSIX-compliant as claimed? (3) Are the wrapper functions that prior work proposes to circumvent issues with setuid calls correct and usable? Towards (1), the standards are expressed in a precise syntax that lends itself to a rigorous assessment of whether the standards are unambiguous and logically consistent descriptions of well-formed functions. Under some reasonable assumptions, two of the three functions that are standardized fit these criteria, which challenges assertions in prior work regarding the quality of the standard. In cases wherein the standard is broken, the problem is clearly characterized, and suggestions are given for fixing standard, but at the <b>cost</b> <b>of</b> backwards-compatibility. Towards (2), a state-space enumeration is performed as in prior work, and a discussion of the implications <b>of</b> <b>non-conformance</b> and differences in implementation is presented. Towards (3), some issues with prior wrappers are identified. The work proposes a new suite of wrapper functions which are designed with a different mindset from prior work, and provides both stronger guarantees with respect to atomicity and a clearer semantics for permanent and temporary changes in process identity. With a fresh approach, this work is a contribution to a well-established mechanism for privilege management...|$|R
40|$|The {{reliability}} of building processes is related {{not only to}} the level of performance required but also to their organizational structure. Conditions of turbulence in which constructors operate on site closely interact with the performance of the construction process and particularly with the risk <b>of</b> <b>non-conformance</b> <b>of</b> a building to the performances required. Interaction is characterised by the stability of the organisational structure or its capacity to adapt to environmental variability. A technical risk organisational factor analysis in construction processes can lead construction management to design organisational structures capable of reacting to environmental conditions in which "on site" production operations develop, and also able to increase the {{reliability of}} the building process. Working on organisational interfaces can lead not only to changes in detail design, but also in performances required to each operator, in order to obtain higher efficiency levels. Based on case studies conducted on sequential organisational structures of the building process, a set of organisational rules is proposed for implementation in experimental "lean construction" processes. Organisational rules based on risk organisational factors analysis can lead to the designing of a "lean" organizational structure of construction processes. KEY WORDS Construction, organisational design, sequential process, uncertainty 1 Associate Professor of Construction Management at the University of Calabria, Dipartimento di Strutture, Facolt di Ingegneria, Arcavacata di Rende (CS), Italy, lucat@fu. penteres. it Mecca 400 26 - 28 July 1999, University of California, Berkeley, CA, US...|$|R
40|$|Abstract:- As is well known, process {{capability}} analysis {{for more than}} one quality variables is a complicated and sometimes contentious area with several quality measures vying for recognition. When these variables exhibit non-normal characteristics, the situation becomes even more complex. The aim {{of this paper is to}} measure Process Capability Indices (PCIs) for bivariate non-normal process using the bivariate Burr distribution. The univariate Burr distribution has been shown to improve the accuracy of estimates of PCIs for univariate non-normal distributions (see for example, [7] and [16]). Here, we will estimate the PCIs of bivariate non-normal distributions using the bivariate Burr distribution. The process of obtaining these PCIs will be accomplished in a series of steps involving estimating the unknown parameters of the process using maximum likelihood estimation coupled with simulated annealing. Finally, the Proportion <b>of</b> <b>Non-Conformance</b> (PNC) obtained using this method will be compared with those obtained from variables distributed under the bivariate Beta, Weibull, Gamma and Weibull-Gamma distributions. Key-Words:- Process Capability Index (PCI), bivariate Burr distribution, simulated annealing algorithm, nonnorma...|$|R
40|$|Quality is an {{important}} aspect in today’s business, regardless {{of the nature of}} the industry, whether it is manufacturing or services and without exemption to large or small and medium-sized enterprises. In the context of the manufacturing industry, most products today are valued by more than one key quality characteristics and in many cases correlations exist among these quality characteristics. In the presence of correlation, evaluating quality using classical univariate techniques may provide inaccurate conclusion about the actual process performance. Furthermore, when a product or process quality is measured by a large number of key quality characteristics with different specifications type, evaluating the process performance can be a difficult task. A comprehensive review on past researches related to techniques of multivariate capability analysis and multivariate loss functions has been conducted. However, most of these models are insufficient in dealing with these aspects of multivariate process data: correlated variables, departure from normality and mixed type of specifications. The present research is motivated by the need to address these concerns. Subsequently, the core objective of this research is to answer the following questions: How can we measure the risk of quality failure for correlated multivariate quality characteristics effectively? Does the quality risk information assists in quality improvement efforts? In evaluating quality based on sample data, the work presented in this thesis considers both the multivariate normal and multivariate non-normal distributions. The Modified Tolerance Region (MTR) model and the Target Distance (TD) models are developed to evaluate the proportion <b>of</b> <b>non-conformance</b> in the presence of joint normal distributions with a combination of nominal-the-best (NTB) specifications type. When quality characteristic is governed by unilateral type of specifications such as smaller-the-better (STB) and larger-the-better (LTB), the assumption of normal distribution is ineffective. Largely, the distribution of data in unilateral type of specifications tends to cluster towards the extreme end of the specification length. Furthermore, the setting of process mean to the target mean value as done in the NTB type cannot be extended to unilateral type of specifications. Thus, in the presence of skewed marginal distributions in correlated data, the Gaussian copula approach and the zero-centred Target Distance (TD 0) model are introduced. The models developed in this study are demonstrated in numerical examples and compared to other existing models in the literature. The results obtained highlight that these models are very promising and are effective in estimating the performance of the multivariate process. An application of the TD 0 model to a medical case study is also included. When non-conformance is detected within the vicinity of the manufacturing site, costs associated to activities that attempt to rectify the non-conformance are categorised as internal failure costs. On the other hand, poor quality products which passed the engineering specification may leave undetected and received by the customer, which consequently gives rise to the external failure cost. External failure costs such as warranty claim, product recall, etc. can be difficult to estimate and are often neglected by manufacturers. In this thesis, it is shown that the internal failure costs and the external failure costs for multivariate processes can be estimated based on the probability <b>of</b> <b>non-conformance</b> and expected loss due to quality failure. The probability <b>of</b> <b>non-conformance</b> is evaluated using one of the presented models (MTR, TD, TD 0 or Gaussian copula) and the expected loss is estimated using multivariate loss functions. Numerical examples for cases of bilateral and unilateral type of specification are provided. Comparison to other model shows that the quality failure costs are greatly underestimated when correlation among quality characteristics is not considered. The final part of this thesis highlights the feasibility of the presented models for the quality improvement of processes with multivariate quality characteristics. A new 6 -step quality improvement framework i. e. Define, Measure, Analyse, Prioritise, Improve and Control (DMAPIC) is introduced. The DMAPIC framework integrates the prioritisation of unfavourable risk from the risk assessment process into the widely known problem-solving methodology DMAIC. Unacceptable risks are identified based on the estimated quality failure <b>costs.</b> The application <b>of</b> the DMAPIC framework is demonstrated using a case study conducted on an Australian manufacturing company. From the case study, it is evident that the prioritisation of risk in processes with multiple key quality characteristics provides {{an important}} advantage especially when resources are limited...|$|R
40|$|The {{hierarchical}} {{production planning}} and control has been disputed {{since the early}} 1990 s and a paradigm shift has emerged that highlights the role of humans in controlling job shops. However, most approaches to supply chain management still follow the conventional hierarchical approach, which establishes the interaction between supply partners at the management control level and does not consider decisional role for shopfloor control level. This study challenges this viewpoint. It discusses human contribution to collaborative supply-chain planning and scheduling within automotive industry. The development and execution of plans in a car manufacturing group and its suppliers are discussed. The research methodology involves an analysis of qualitative data from an ethnographic field study on planning and scheduling practices. Two scenarios of collaborative scheduling are analysed through which it is shown how the operational levels adapt their own strategies to cope with operational variations. The management control systems do not receive detailed information of what happens. They receive aggregate reports <b>of</b> <b>non-conformances</b> during the execution for which they develop compensatory plans {{and send them to}} the task control level and the story repeats from the beginning...|$|R
40|$|Purpose: The main aim of {{the paper}} is the {{evaluation}} of efficiency of working time of equipment in blast furnace department with the use of Overall Equipment Effectiveness (OEE) and PAMCO method. Design/methodology/approach: The investigation was made for blast furnace department in Polish steel plant. Two methods: OEE and PAMCO was used. The analysis covers the period of 7 years. The analysis was made based on different data: different types of time connected whit work of blast furnaces, interruption in blast furnace work, level of pig iron production, unit production time and quantity <b>of</b> <b>non-conformance</b> production. Findings: Performed research made it possible to determine the level of efficiency of blast furnace department. It can be concluded that working time in this department is used efficient. Factors that reduce efficiency are usually not-dependent on the company: mainly are connected with the situation on the steel market. Research limitations/implications: It is necessary to continue the research in order to assess individual blast furnaces. That analysis may enable to show if these units has the same level of efficiency and if they are affected by the same factors in the same way. Practical implications: Optimal utilization of capacity and working time of machines and equipment are important for the reduction <b>of</b> production <b>costs.</b> Use <b>of</b> OEE and PAMCO methods helps to assess the level of efficiency of working time and allow to find factors that has great importance for level of efficiency. Originality/value: Results of this analysis can be taken into consideration by blast furnace department under study. Analysis can help to assess the level of efficiency and find factors that influence on it...|$|R
40|$|To {{determine}} {{the nature of}} psychotropic medication routinely prescribed in Scottish PICUs, the medical notes and prescription charts of 75 patients from 10 out of 14 units were consulted. These included 55 males (73 %), the majority of whom were detained and had a diagnosis of schizophrenia. There was good conformance with mental health legislation treatment plans (T 2 /T 3 forms) with only five instances <b>of</b> <b>non-conformance</b> identified. Antipsychotics were prescribed to 60 patients (80 %) of which olanzapine and zuclopenthixol decanaote were the most common. The average number of regular psychotropic medicines prescribed per patient was two. In total, 14 drugs, encompassing 21 formulations, were prescribed for PRN use. Lorazepam and haloperidol were the most common PRN medicines. 76 PRN prescriptions (32 %) had the oral and IM doses written on the same line of the prescription chart and showed no allowance for bioavailability when detailing the maximum dose that could be administered. 16 patients (21 %) were prescribed more than one antipsychotic. Seven patients were prescribed high dose therapy, but this increased to 25 when PRN antipsychotics were included. This study provides information about psychotropic medication prescribing in Scottish PICUs and highlights areas where practice is sub-optimal...|$|R
40|$|Thesis (Master) [...] İzmir Institute of Technology, Architecture, İzmir, 2011 Includes bibliographical {{references}} (leaves: 76 - 81) Text in English;Abstract: Turkish and Englishxii, 85 leavesThis study aims {{to predict}} the issuance durations of occupancy permit applications using the delay causes defined in the permit process and reveal the most significant causes affecting {{the performance of the}} prediction. Artificial Neural Networks (ANN) is used for predicting the issuance durations of occupancy permit applications. The model is constructed {{to predict the}} issuance durations of least once rejected applications made to Izmir Konak Municipality during year 2008. Then, sensitivity analysis is carried out to detect the most significant delay causes affecting the issuance duration. Permit data are examined to reveal the delay causes of occupancy permit process. Six inputs are generated from the delay causes and used in ANN model: 1) Number of missing approval letters, 2) Number of missing payment documents, 3) Number <b>of</b> <b>non-conformances</b> <b>of</b> project to codes and regulations, 4) Number of all missing documents, 5) First permit application season, 6) First permit rejection season. Total issuance durations of the occupancy permit applications are used as the output parameters of the model. The results of the analysis indicate that the prediction accuracy of the model is 86 % and the number of missing approval letters, the number of missing payment documents, and the first application season are respectively the three most significant inputs affecting the prediction performance of the model. This study proves that the total issuance durations are so bound to the delay causes in the permit process that it can be learned and predicted by the ANN model and the occupancy permit process is required to be reengineered...|$|R
50|$|In certain {{markets and}} industries, CAPA may be {{required}} {{as part of the}} quality management system, such as the Medical Devices and Pharmaceutical industries in the United States. In this case, failure to adhere to proper CAPA handling is considered a violation of US Federal regulations on good manufacturing practices. As a consequence, a medicine or medical device can be termed as adulterated or substandard if the company has failed to investigate, record and analyse the root-cause <b>of</b> a <b>non-conformance,</b> and failed to design and implement an effective CAPA.|$|R
50|$|High {{transaction}} costs: The transaction <b>cost</b> <b>of</b> {{trading in}} a water market is the sum <b>of</b> the <b>cost</b> <b>of</b> obtaining information, search <b>cost</b> <b>of</b> finding willing traders, negotiation <b>cost</b> <b>of</b> achieving mutually beneficial trades, <b>cost</b> <b>of</b> effecting and registering trades, and <b>cost</b> <b>of</b> enforcing trade contracts. Increasing the geographic range of the trade and number of stakeholders involved tends to increase the transaction <b>cost</b> <b>of</b> the trade.|$|R
