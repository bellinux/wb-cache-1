15|38|Public
50|$|Because (like Thumb-1 and MIPS16) the <b>compressed</b> <b>instructions</b> {{are simply}} {{alternate}} encodings (aliases) for a selected subset of larger instructions, the compression {{can be implemented}} in the assembler, {{and it is not}} essential for the compiler to even know about it.|$|E
5000|$|The {{standard}} RISC-V ISA specifies {{that all}} instructions are 32 bits. This {{makes for a}} particularly simple implementation, but like other RISC processors with such an instruction encoding, results in larger code size than in other instruction sets.To compensate, RISC-V's [...] "32-bit" [...] instructions are actually 30 bits; [...] of the opcode space is reserved for an optional (but recommended) variable-length [...] "compressed" [...] instruction set, RVC, that includes 16-bit instructions. Like ARM's Thumb and the MIPS16, the <b>compressed</b> <b>instructions</b> are simply aliases for {{a subset of the}} larger instructions. Unlike ARM's Thumb or the MIPS compressed set, space was reserved from the beginning so there is no separate operating mode. Standard and <b>compressed</b> <b>instructions</b> may be intermixed freely. (letter [...] "C") ...|$|E
50|$|In {{the early}} 1990s, {{engineers}} at Hitachi {{found ways to}} compress RISC instruction sets so they fit in even smaller memory systems than CISC instruction sets. They developed a compressed instruction set for their SuperH series of microprocessors, introduced in 1992. The SuperH instruction set was later adapted for the ARM architecture's Thumb instruction set. <b>Compressed</b> <b>instructions</b> appeared in the ARM architecture, after ARM Holdings licensed SuperH patents {{as a basis for}} its Thumb instruction set.|$|E
40|$|<b>Compressed</b> <b>Instruction</b> Set {{architecture}} {{refers to}} support for coexistence of instructions of different instruction widths. By re-encoding {{the most frequently}} used instructions in lesser number of widths code density can be increased at the expense of some extra work in the decode cycle. In this project we have provided support for <b>Compressed</b> <b>Instruction</b> Set in an existing re-targetable compiler-simulator framework, EXPRESSION. We tested the generic implementation on the motorola PowerPC architecture. We decided upon a <b>Compressed</b> <b>Instruction</b> Set for the same, and executed our compiler optimization to estimate the improvements in code size and impact on performance. We found encouraging improvements of upto 20 % in the code size for not a setback i...|$|R
40|$|Available purely {{software}} based code attestation protocols {{have recently}} {{been shown to be}} cheatable. In this work we propose to upload <b>compressed</b> <b>instruction</b> code to make the code attestation protocol robust against a so called compresssion attack. The described secure code attestation protocol makes use of recently proposed microcontroller architectures for reading out <b>compressed</b> <b>instruction</b> code. We point out that the proposed concept only makes sense if the provided cost/benefit ratio for the aforementioned microcontroller is higher than an alternative hardware based solution requiring a tamperresistant hardware module. Comment: 7 page...|$|R
40|$|Abstract: Available purely {{software}} based code attestation protocols {{have recently}} {{been shown to be}} cheatable. In this work we propose to upload <b>compressed</b> <b>instruction</b> code to make the code attestation protocol robust against a so called compresssion attack. The described secure code attestation protocol makes use of proposed micro-controller architectures for reading out <b>compressed</b> <b>instruction</b> code. 1 Introduction 1 The evolution of the ubiquitous computing vision towards full-fledged real world appli-cations faces a diversity of new problems. Due to the fact that for many applications cost-efficient hardware is an issue, one can not guarantee that a code image that has been uploaded on a non-tamper resistant device will always run in a correct and un-manipulate...|$|R
5000|$|A {{prototype}} of RVC was tested. The prototype code was 20% smaller than an x86 PC and MIPS compressed code, and 2% larger than ARM Thumb-2 code. It also substantially reduced both the needed cache {{memory and the}} estimated power usage of the memory system. [...] The researcher intended to reduce the code's binary size for small computers, especially embedded computer systems. The prototype included 33 {{of the most frequently}} used instructions, recoded as compact 16-bit formats using operation codes previously reserved for the compressed set. The compression was done in the assembler, with no changes to the compiler. <b>Compressed</b> <b>instructions</b> omitted fields that are often zero, used small immediate values or accessed subsets (16 or 8) of the registers. [...] is very common and often compressible.|$|E
40|$|Code {{compression}} {{has been}} shown to be an effective technique to reduce code size in memory constrained embedded systems. It has also been used as a way to increase cache hit ratio, thus reducing power consumption and improving performance. This paper proposes an approach to mix static/dynamic instruction profiling in dictionary construction, so as to best exploit trade-offs in compression ratio/performance. <b>Compressed</b> <b>instructions</b> are stored as variable-size indices into fixed-size codewords, eliminating compressed code misalignments. Experimental results, using the Leon (SPARCv 8) processor and a program mix from MiBench and Mediabench, show that our approach halves the number of cache accesses and power consumption while produces compression ratios as low as 56 %...|$|E
40|$|This paper {{presents}} a compression scheme for embedded RISC microprocessors' code. The scheme achieves better compression ratios (around 0. 57) than other reported implementations, {{and as the}} Instruction Cache (ICache) holds <b>compressed</b> <b>instructions</b> its effective size is increased and the hit ratio is improved. Moreover, the processor remains unaware of the compression and its functionality is fully preserved. Other important benefits are the reduction in power consumption and improvement of performance brought about by instruction cache compression, which are still to be quantified. The scheme has required the resolutions of issues that arise from both memory and ICache data misalignment and from the compressed to uncompressed address space mapping. These resolutions are briefly described here...|$|E
40|$|An {{architecture}} is presented for a {{digital signal processor}} (DSP) intended for use in digital mobile phones. In this application, {{it is necessary to}} balance the requirement of high processing throughput with the demand of low power for extended battery lifetime. These requirements are addressed by a multi-level power reduction strategy, involving the use of a parallel asynchronous architecture, a configurable <b>compressed</b> <b>instruction</b> set, a large register file, the use of sign-magnitude arithmetic, and reduced support for interrupts. 1...|$|R
40|$|We {{propose a}} method for {{compressing}} programs in embedded processors where instruction memory size dominates cost. A post-compilation analyzer examines a program and replaces common sequences of instructions with a single instruction codeword. A microprocessor executes the <b>compressed</b> <b>instruction</b> sequences by fetching codewords from the instruction memory, expanding {{them back to the}} original sequence of instructions in the decode stage, and issuing them to the execution stages. We apply our technique to the PowerPC instruction set and achieve 30 % to 50 % reduction in size for SPEC CINT 95 programs. Keywords...|$|R
40|$|Code {{compression}} {{could lead}} to less overall system die area and therefore less cost. This is significant in the embedded system field where cost is very sensitive. In most of the recent approaches for code compression, only <b>instruction</b> ROM is <b>compressed.</b> Decompression is done between the cache and memory, and instruction cache is kept uncompressed. Additional saving could be achieved if the decompression unit is moved to between CPU and cache, and keep the <b>instruction</b> cache <b>compressed</b> as well. In this project we explored the issues for <b>compressing</b> the <b>instruction</b> cache. In the process, we constructed a high level implementation for a 64 -bit, 5 -stage pipeline MIPS like processor with <b>compressed</b> <b>instruction</b> cache. We developed a compression algorithm with instruction level random access within the compressed file. In addition we devised a branch compensation cache, a small cache mechanism to alleviate the unique branching penalties that branch prediction cannot reduce. 1...|$|R
40|$|Code {{compression}} {{is known}} as an effective technique to reduce instruction memory size on an embedded system. However, code compression can also be very effective in increasing processorto -memory bandwidth and hence provide increased system performance. In this paper we describe our design and design methodology of the first running prototype of a one-cycle code decompression unit that decompresses <b>compressed</b> <b>instructions</b> on-thefly. We describe in detail the architecture that enables decompression of multiple instructions in one cycle and we present the design methodologies and tools used. The stand-alone decompression unit does not require any modifications on the processor core. We observed up to 63 % performance increase with 25 % in average over {{a wide variety of}} applications running on the hardware prototype under various system configurations...|$|E
40|$|We {{investigate}} {{the feasibility of}} using instruction compression at some level in a multi-level memory hierarchy to increase memory system performance. For example, compressing at main memory means that main memory and the file system would contain <b>compressed</b> <b>instructions,</b> but upstream caches would see normal uncompressed instructions. Compression effectively increases the memory size and the block size reducing the miss rate {{at the expense of}} increased access latency due to decompression delays. We present a simple compression scheme using the most frequently used symbols and evaluate it with several other compression schemes. On a SPARC processor, our scheme obtained compression rirtio of 150 % for most programs. We analytically evaluate the impact of compression on the average memory access (ime for various memory systems and compression approaches. Our results show that feasibility of using compression is sensitive to the miss rates and miss penalties at the point of compression {{and to a lesser extent}} the amount of compression possible. For high performance workstations of today, compression already shows promise; as miss penalties increase in future, compression will only become more feasible...|$|E
40|$|Code {{compression}} techniques {{might be}} useful to meet code size constraints in embedded systems. In the average case, the impact of code compression on the performance is double-edged: on one side, the number of accesses to memory hierarchy is reduced because several instructions are coded in a single word, and {{this is likely to}} reduce the execution time; on the other side, the decompression penalty increases the processing time of <b>compressed</b> <b>instructions.</b> Nevertheless, experimental results show that the execution time might be lowered by code compression. In this paper, our goal is to analyze the impact of code compression on the estimated Worst-Case Execution Time of critical tasks that must meet at the same time code size constraints and timing deadlines. Changes in the access patterns to the instruction cache are indeed likely to alter the accuracy of the cache analysis within the process of determining the WCET. Experimental results show that, besides reducing the code size, our code compression scheme also improves the WCET estimates in most of the cases. 1...|$|E
40|$|In the {{embedded}} system design, memory {{is one of}} the most restricted resources. Code compression has been proposed as a solution to reduce the code size of applications for {{embedded system}}s. Data compression techniques are used to compress programs to reduce memory size. Most previous work <b>compresses</b> all <b>instructions</b> found in an executable, without taking into account the program execution profile. In this paper, a profile-driven code compression design methodology is proposed. Program profiling information can be used to help code compression to selectively <b>compress</b> non-critical <b>instructions,</b> such that the system performance degradation due to the decompression penalty is reduced. ...|$|R
5000|$|RISC-V {{does not}} support {{predication}} (the conditional execution of instructions) as its designers claim that CPUs without predication are easier to design, and optimizing compilers {{are less likely to}} mistakenly use predication where it should not be used. The designers claim that very fast, out-of-order CPU designs do predication anyway, by doing the comparison branch and conditional code in parallel, then discarding the unused path's effects. They also claim that even in simpler CPUs, predication is less valuable than branch prediction, which can prevent most stalls associated with conditional branches. Code without predication is larger, with more branches, but they also claim that a <b>compressed</b> <b>instruction</b> set (such as RISC-V's set [...] "C") solves that problem in most cases.|$|R
50|$|The <b>compress</b> {{and expand}} <b>instructions</b> match the APL {{operations}} of the same name. They use the opmask {{in a slightly different}} way from other AVX-512 <b>instructions.</b> <b>Compress</b> only saves the values marked in the mask, but saves them compacted by skipping and not reserving space for unmarked values. Expand operates in the opposite way, by loading as many values as indicated in the mask and then spreading them to the selected positions.|$|R
40|$|AbstractEmbedded systems {{often have}} severe memory {{constraints}} requiring careful encoding of programs. For example, smart cards {{have on the}} order of 1 K of RAM, 16 K of non-volatile memory, and 24 K of ROM. A virtual machine can be an effective approach to obtain compact programs but instructions are commonly encoded using one byte for the opcode and multiple bytes for the operands, which can be wasteful and thus limit the size of programs runnable on embedded systems. Our approach uses canonical Huffman codes to generate compact opcodes with custom-sized operand fields and with a virtual machine that directly executes this compact code. We present techniques to automatically generate the new instruction formats and the decoder. In effect, this automatically creates both an instruction set for a customized virtual machine and an implementation of that machine. We demonstrate that, without prior decompression, fast decoding of these virtual <b>compressed</b> <b>instructions</b> is feasible. Through experiments on Scheme and Java, we demonstrate the speed of these decoders. Java benchmarks show an average execution slowdown of 9 %. The reductions in size highly depend on the original bytecode and the training samples, but typically vary from 40 % to 60 %...|$|E
40|$|Endpoint {{devices for}} Internet-of-Things not {{only need to}} work under {{extremely}} tight power envelope of a few milliwatts, but {{also need to be}} flexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold(NT) operation can achieve higher energy efficiency, and the performance scalability can be gained through parallelism. In this paper we describe the design of an open-source RISC-V processor core specifically designed for NT operation in tightly coupled multi-core clusters. We introduce instruction-extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure towards the shared memory hierarchy. For typical data-intensive sensor processing workloads the proposed core is on average 3. 5 x faster and 3. 2 x more energy-efficient, thanks to a smart L 0 buffer to reduce cache access contentions and support for <b>compressed</b> <b>instructions.</b> SIMD extensions, such as dot-products, and a built-in L 0 storage further reduce the shared memory accesses by 8 x reducing contentions by 3. 2 x. With four NT-optimized cores, the cluster is operational from 0. 6 V to 1. 2 V achieving a peak efficiency of 67 MOPS/mW in a low-cost 65 nm bulk CMOS technology. In a low power 28 nm FDSOI process a peak efficiency of 193 MOPS/mW(40 MHz, 1 mW) can be achieved...|$|E
40|$|Code {{compression}} {{coupled with}} dynamic decompression {{is an important}} technique for both embedded and general-purpose microprocessors. Post-fetch decompression, in which decompression is performed after the <b>compressed</b> <b>instructions</b> have been fetched, allows the instruction cache to store compressed code but requires a highly efficient decompression implementation. We propose implementing post-fetch decompression using dynamic instruction stream editing (DISE), a programmable decoder [...] -similar in structure to those in many IA 32 processors [...] -that is used to add functionality to an application by injecting custom code snippets into its fetched instruction stream. A DISE implementation of post-fetch decompression naturally supports customized program-specific decompression dictionaries, enables parameterized decompression allowing similar instruction sequences to share dictionary entries, and uses no decompression-specific hardware. Cycle-level simulation of DISE decompression shows that it can reduce static program size by 35 % and execution time by 20 %. Parameterized decompression, a feature unique to DISE, accounts for 20 % of the code size reduction by making more effective use of the dictionary and allowing PC-relative branches {{to be included in}} compressed sequences. DISE-based compression can reduce total energy consumption by 10 % and the energy-delay product by as much as 20 %. Categories and Subject Descriptors B. 3 [Hardware]: Memory Structure...|$|E
40|$|Trace-driven {{simulations}} {{have been}} widely used in computer architecture for quantitative evaluations of new ideas and design prototypes. Efficient trace compression and fast decompression are crucial for contemporary workloads, as representative benchmarks grow in size and number. This article presents Stream-Based Compression (SBC), a novel technique for single-pass compression of address traces. The SBC technique <b>compresses</b> both <b>instruction</b> and data addresses by associating them with a particular instruction stream, that is, a block of consecutively executing <b>instructions.</b> The <b>compressed</b> <b>instruction</b> trace is a trace of instruction stream identifiers. The compressed data address trace encompasses the data address stride and the number of repetitions for each memoryreferencing instruction in a stream, ordered by the corresponding stream appearances in the trace. SBC reduces the size of SPEC CPU 2000 Dinero instruction and data address traces from 18 to 309 times, outperforming the best trace compression techniques presented in the open literature. SBC can be successfully combined with general-purpose compression techniques. The combined SBC-gzip compression ratio is from 80 to 35, 595, and the SBC-bzip 2 compression ratio is from 75 to 191, 257. Moreover, SBC outperforms other trace compression techniques when both decompression time and compression time are considered. This article also shows how the SBC algorithm can be modified for hardware implementation with very modest resources and only a minor loss in compression ratio...|$|R
40|$|VLIW {{architectures}} use {{very wide}} instruction words {{in conjunction with}} high bandwidth to the instruction cache to achieve multiple instruction issue. This report uses the TINKER experimental testbed to examine instruction fetch and instruction cache mechanisms for VLIWs. A <b>compressed</b> <b>instruction</b> encoding for VLIWs is defined, and a classification scheme for i-fetch hardware for such an encoding is introduced. Several interesting cache and i-fetch organizations are described and evaluated through trace-driven simulations. A new i-fetch mechanism using a silo cache is found {{to have the best}} performance. 1. Introduction VLIW architectures use very wide instruction words to achieve multiple instruction issue. These architectures require high bandwidth instruction fetch (i-fetch) mechanisms to transport instruction words from the cache to the execution pipeline. The complexity of the hardware support required for i-fetch is related to the type of instruction encoding used. In general, VLI [...] ...|$|R
40|$|Current {{mobile phone}} {{applications}} demand high performance from the DSP, {{and future generations}} are likely to require even greater throughput. However, {{it is important to}} balance these processing demands against the requirement of low power consumption for extended battery lifetime. A novel low-power digital signal processor (DSP) architecture CADRE (Configurable Asynchronous DSP for Reduced Energy) addresses these requirements through a multi-level power reduction strategy. A parallel architecture and configurable <b>compressed</b> <b>instruction</b> set meets the throughput requirements without excessive program memory bandwidth, while a large register file reduces the cost of data accesses. Sign-magnitude representation is used for data, to reduce switching activity within the datapath. Asynchronous design gives fine-grained activity control without the complexities of clock gating, and gives low electromagnetic interference. Finally, the operational model of the target application allows for a reduced interrupt structure, simplifying processor design by avoiding the need for exact exceptions...|$|R
40|$|Embedded systems {{often have}} strong memory {{constraints}} requiring careful encoding of programs. For example, smart cards {{have on the}} order of 1 K of RAM, 16 K of non-volatile memory, and 24 K of ROM. A virtual machine can be an effective approach to obtain compact programs but instructions are commonly encoded using one byte for the opcode and multiple bytes for the operands, which can be wasteful. We use another approach, using canonical Huffman codes to generate compact custom-sized opcodes and custom-sized operand fields along with a virtual machine that directly executes the encoded operations. We present techniques that automatically generate the opcodes and the decoder. In effect, this automatically creates both an instruction set for a customized virtual machine and an implementation of that machine. We demonstrate that, without prior decompression, fast decoding of these virtual <b>compressed</b> <b>instructions</b> is feasible. We also discuss the relevant difficulties in generating C code for such decoders, in particular the problem of efficient program memory access. Through experiments we demonstrate the speed of these decoders. Synthetic and Java benchmarks show an execution slowdown ranging from 10 % to 30 %, with an average of 9 % for good decoders. For the Java bytecode, the average overall compression factor is 60 %...|$|E
40|$|VLIW {{architectures}} use {{very wide}} instruction words {{in conjunction with}} high bandwidth to the instruction cache to achieve multiple instruction issue. One instruction fetch mechanism for VLIWs {{is the use of}} a banked instruction cache. Such a cache is intended for use with a <b>compressed</b> <b>instruction</b> encoding. A <b>compressed</b> encoding supports variable length VLIWs and thus has associated with it the difficulties in supporting variable length instructions. One of these is determining on every cycle the instruction fetch address (NextPC) for the following cycle. This report uses the TINKER experimental testbed to illustrate a mechanism that can be used by a banked instruction cache for NextPC computation. An algorithm for NextPC computation is outlined and associated hardware support is presented. Issues relating to the cycle time complexity of the proposed design are also addressed. 1 Introduction VLIW architectures use very wide instruction words to achieve multiple instruction issue. These [...] ...|$|R
40|$|Is it {{possible}} to <b>compress</b> <b>instruction</b> time into fewer school years without lowering education levels? A fundamental reform in Germany reduced the length of academic track schooling by one year, while increasing instruction hours in the remaining school years {{to provide students with}} a very similar core curriculum and the same overall instruction time. Using aggregated administrative data on the full population of students, we find that the reform increases grade repetition rates and lowers final grade point averages, without affecting graduation rates. The results suggest adverse reform effects on student performance, but the economic significance of the effects appears moderate. This is the preprint of an article published in Economics of Education Review 58 (2017), pp. 1 - 14, available online at: [URL] A previous version of the paper was circulated under the title 2 ̆ 01 cMoving up a Gear: The Impact of Compressing Instructional Time into Fewer Years of Schooling 2 ̆ 01 d as DIW Discussion Paper No. 1450...|$|R
40|$|<b>Compressing</b> the <b>instructions</b> of an {{embedded}} {{program is}} important for cost-sensitive low-power controloriented embedded computing. Several hardware decompression architectures have been proposed. In this paper, we present a method of decompressing programs using software. One interesting aspect of our method is that it relies on using a software-managed instruction cache under control of the decompressor. In addition, our solution is {{an order of magnitude}} faster than a previous software-managed decompression system. ...|$|R
40|$|This work {{describes}} {{a method that}} makes the Java instruction as small as possible. The proposed method, called "nibble coding", <b>compresses</b> two <b>instructions</b> into one byte. The experiment is carried out to compare Java byte code instruction and a modified code with the nibble coding. The result shows the proposed scheme achieves a much smaller instruction bandwidth than the ordinary byte-code counterpart. Key-Words: code compression, instruction bandwidth, Java byte cod...|$|R
40|$|This work {{describes}} a new {{investigation of the}} problem of instruction set design: How to make the program as small as possible. The proposed method, called "nibble coding", <b>compresses</b> two <b>instructions</b> into one byte. The experiment is carried out to compare conventional byte-code instruction and a typical 32 -bit machine code with the nibble coding. The result shows the proposed scheme achieves a smaller instruction bandwidth than a byte-code virtual machine and is much smaller than the conventional executable machine code...|$|R
40|$|VLIW {{architectures}} {{are popular}} in embedded systems because they offer high-performance processing at low cost and energy. The major problem with traditional VLIW designs {{is that they}} do not scale efficiently due to bottlenecks that result from centralized resources and global communication. Multicluster designs have been proposed to solve the scaling problem of VLIW datapaths, while much less work has been done on the control path. In this paper, we propose a distributed control path architecture for VLIW processors (DVLIW) to overcome the scalability problem of VLIW control paths. The architecture simplifies the dispersal of complex VLIW instructions and supports efficient distribution of instructions through a limited bandwidth interconnect, while supporting <b>compressed</b> <b>instruction</b> encodings. DVLIW employs a multicluster design where each cluster contains a local instruction memory that provides all intra-cluster control. All clusters have their own program counter and instruction sequencing capabilities, thus instruction execution is completely decentralized. The architecture executes multiple instruction streams at the same time, but these streams collectively function as a single logical instruction stream. Simulation results show that DVLIW processors reduce the number of cross-chip control signals by approximately two orders of magnitude while incurring a small performance overhead to explicitly manage the instruction streams. 1...|$|R
50|$|Recently, {{engineers}} have {{found ways to}} <b>compress</b> the reduced <b>instruction</b> sets so they fit in even smaller memory systems than CISCs. Examples of such compression schemes include ARM architecture's Thumb instruction set. In applications that do not need to run older binary software, compressed RISCs are growing to dominate sales.|$|R
40|$|Memory systems {{select from}} {{environmental}} stimuli those to encode permanently. Repeated stimuli separated by timed spaces without stimuli can initiate Long-Term Potentiation (LTP) and long-term memory (LTM) encoding. These processes occur in time scales of minutes, {{and has been}} demonstrated in many species. This study reports on using a specific timed pattern of three repeated stimuli separated by ten-minute spaces drawn from both behavioural and laboratory studies of LTP and LTM encoding. A technique was developed based on this pattern to test whether encoding complex information into LTM in students was possible using the pattern within {{a very short time}} scale. In an educational context, stimuli were periods of highly <b>compressed</b> <b>instruction,</b> and spaces were created through 10 minute distractor activities. Spaced learning in this form was used as the only means of instruction for a national curriculum Biology course, and led to very rapid LTM encoding as measured by the high-stakes test for the course. Remarkably, learning at a greatly increased speed and in a pattern that included deliberate distraction produced significantly higher scores than random answers (p < 0. 00001) and scores were not significantly different for experimental groups (one hour spaced learning) and control groups (four months teaching). Thus learning per hour of instruction, as measured by the test, was significantly higher for the spaced learning groups (p < 0. 00001). In a third condition, spaced learning was used to replace the end of course review for one of two examinations. Results showed significantly higher outcomes for the course using spaced learning (p < 0. 0005). The implications of these findings and further areas for research are briefly considered...|$|R
40|$|Abstract: Entropy coding is an {{efficient}} method for compressing information {{and it can}} be used to improve code density of processor architectures. However, optimal compression requires information of the probability of instructions, which, in general, is not available when processor is designed. However, when designing customizable processors, which are tailored for a given application, such information can be obtained at design time. In this paper, entropy coding is used to improve code density of application-specific programmable processors based on transport triggering paradigm. Three DSP applications are used as benchmark applications and Huffman coding is applied to <b>compress</b> the <b>instructions</b> of a processor architecture tailored for each benchmark. Key words: code compression, Huffman coding, customizable processor architecture, transport triggered architecture, discrete cosine transform, Viterbi decodin...|$|R
40|$|<b>Compressing</b> the <b>instructions</b> of an {{embedded}} {{program is}} important for cost-sensitive lowpower control-oriented embedded computing. A number of compression schemes have been proposed to reduce program size. However, the increased instruction density has an accompanying performance cost because the instructions must be decompressed before execution. In this paper, we investigate the performance penalty of a hardware-managed code compression algorithm recently introduced in IBM's PowerPC 405. This scheme {{is the first to}} combine many previously proposed code compression techniques, making it an ideal candidate for study. We find that code compression with appropriate hardware optimizations does not have to incur much performance loss. Furthermore, our studies show this holds for architectures {{with a wide range of}} memory configurations and issue widths. Surprisingly, we find that a performance increase over native code is achievable in many situations. Keywords: Compression, CodePack, Code [...] ...|$|R
40|$|In {{this paper}} {{we present a}} {{straightforward}} technique for <b>compressing</b> the <b>instruction</b> stream for programs that overcomes some {{of the limitations of}} earlier proposals. After code generation, the instruction stream is analysed for frequently used sequences of instructions from within the program's basic blocks. These patterns of multiple instructions are then mapped into single byte opcodes. This constitutes a compression of multiple, multi-byte operations onto a single byte. When compressed opcodes are detected during the instruction fetch cycle of program execution, they are expanded within the CPU into the original (multi-cycle) sequence of instructions. We only examine patterns within a program's basic block, so branch instructions and their targets are unaffected by this technique allowing compression to be decoupled from compilation. 1. Introduction Compilers are universally used for program development, due to the complexity of managing the large applications developed today. How [...] ...|$|R
