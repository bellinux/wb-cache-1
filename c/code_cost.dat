9|375|Public
5000|$|Hinton and Zelem [...] "derive an {{objective}} function for training autoencoders {{based on the}} minimum description length (MDL) principle". [...] "The description length of an input vector using a particular code {{is the sum of}} the <b>code</b> <b>cost</b> and reconstruction cost. They define this to be the energy of the code, for reasons that will become clear later. Given an input vector, they define the energy of a code to be the sum of the <b>code</b> <b>cost</b> and the reconstruction cost." [...] The true expected combined cost is"which has exactly the form of Helmholtz free energy".|$|E
50|$|In {{register}} machines, {{a common}} subexpression (a subexpression {{which is used}} multiple times with the same result value) can be evaluated just once and its result saved in a fast register. The subsequent reuses have no time or <b>code</b> <b>cost,</b> just a register reference. This optimization speeds simple expressions (for example, loading variable X or pointer P) as well as less-common complex expressions.|$|E
50|$|However, {{there are}} certain {{barriers}} to overcome for open design when compared to software development where there are mature and widely used tools available and the duplication and distribution of <b>code</b> <b>cost</b> next to nothing. Creating, testing and modifying physical designs is not quite so straightforward because of the effort, time and cost required to create the physical artefact; although with access to emerging flexible computer-controlled manufacturing techniques the complexity and effort of construction can be significantly reduced (see tools mentioned in the fab lab article).|$|E
50|$|After each prediction, {{the model}} is updated by {{adjusting}} the weights to minimize <b>coding</b> <b>cost.</b>|$|R
50|$|This {{demonstrates}} how encoding clocking or synchronization in a <b>code</b> <b>costs</b> channel capacity, and illustrates the trade-off.|$|R
3000|$|..., the {{solution}} provided by (9) {{corresponds to the}} choice of progressively assigning the less efficient coding mode (higher values of the <b>coding</b> <b>cost</b> k [...]...|$|R
40|$|An {{optimized}} {{cost and}} performance {{model for a}} phosphoric acid fuel cell power plant system was derived and developed into a modular FORTRAN computer <b>code.</b> <b>Cost,</b> energy, mass, and electrochemical analyses were combined to develop a mathematical model for optimizing the steam to methane ratio in the reformer, hydrogen utilization in the PAFC plates per stack. The nonlinear programming code, COMPUTE, was used to solve this model, in which the method of mixed penalty function combined with Hooke and Jeeves pattern search was chosen to evaluate this specific optimization problem...|$|E
40|$|Thesis (Master's) [...] University of Washington, 2015 - 12 Most office {{building}} construction relies on steel and concrete for mid-high rise {{office building}} applications. The {{primary goal of}} this thesis is to understand the implications of CLT and mass timber construction systems for mid-high rise office buildings in Seattle by developing a prototypical office building located on a specific site. This research thesis will focus on comparing this prototypical mass timber office building design to the same/similar design using industry standard construction materials for Seattle. The criteria for comparison will include <b>code,</b> <b>cost,</b> schedule and greenhouse gas emissions...|$|E
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. To investigate the role of national wage settlements {{in the process of}} wage inflation. Main Topics : Variables Settlements: title of agreement, date of implementation, date of settlement, standard weekly hours, time rate of minimum earnings level, basic weekly/hourly wage of top male/semi-skilled male/bottom male/female, arbitration, Government intervention, staged settlement <b>code,</b> <b>cost</b> of living clause. Agreements: title, main order heading, bargaining system at 1950 / 1973, date of change from one bargaining system to another, trade unions involved and operation dates, geographical area covered at 1950 / 1973, date of change of geographical area covered, number of workers covered at dates 1950, 1955, 1965, 1970, 1975, wage rate as percentage of standard weekly earnings, source of wage rate information. Retail Price Index: year/week of observation, RPI all items/foods/all except foods/all except seasonal foods...|$|E
40|$|We analyze a {{model with}} two {{software}} firms, quality improving coding expenditures and potential competition. The firms can publish {{parts of their}} software as open source. Publishing software implies positive spillovers and thus reduces the firms' <b>coding</b> <b>costs.</b> On the other hand there exist two negative effects. First, lower <b>coding</b> <b>costs</b> induce higher <b>coding</b> expenditures which decreases the firms' profits if their programs are substitutes. Second, open source encourages entry and increases the expenditures required to deter entry. The firms' optimal open source decisions balance these opposite effects. Open Source, Spillovers, Potential Entry...|$|R
40|$|Event {{detection}} is {{a typical}} application of wireless sensor networks. The existing approaches of event detection usually employ certain event models that are constructed with prior domain knowledge. The resulting event detection processes appear to be cost-inefficient, which require either intensive data exchanges among neighboring nodes or caching large columns of history data. In this paper, {{we focus on the}} issue of locating event in wireless sensor network without locations. This involves two tasks, namely detecting an event and identifying an area in the network where the event occurs. Motivated by the real system, we propose a model-free approach for event detection called CCD (<b>Coding</b> <b>Cost</b> based event Detection). <b>Coding</b> <b>cost</b> is a metric that quantifies the diversity of a set of sensor readings. Incorporated into the inherent data collection mechanism, CCD passively constructs a gradient map of <b>coding</b> <b>cost</b> throughout the network. An event is then detected where a change point of gradient appears and identifies the event pattern automatically. CCD is fully distributed and does not incur apparent communication overhead. We implement CCD and evaluate its performance with extensive experiments and simulations. The results demonstrate that CCD is accurate, scalable, and applicable to a variety of sensor networks. Â© 2011 IEEE...|$|R
40|$|Recent {{development}} in video coding research {{deals with the}} use of hierarchical and/or adaptive mesh for video representation. Concurrently, transmitted bit rates have to be reduced to adapt to the network available bandwidth. Some previous works deal with adaptive node sampling according to image content. However, adaptive hierarchical proposed approaches do not optimize a compromise between distortion and bitrate: the representation <b>coding</b> <b>cost</b> is often stated but not taken into account as a constraint. Compared to these methods, this paper proposes for considering an adaptive hierarchical mesh based representation whose splitting criterion optimizes both the <b>coding</b> <b>cost</b> and the image rendering. Jointly, node value optimization, adaptive quantization, cheap coding tree and a wavelet approach are presented. To illustrate our different proposed methods, experimental results are shown and compared to the JPEG picture coding format...|$|R
40|$|Abstract: The {{software}} testing community had much less awareness and debate about UML then software design and development communities, and {{has largely been}} absent as the modeling standard was developed. This is an important issue, because in many software development organizations, the cost of testing can {{account for more than}} 40 % of the total development cost for a software system. In software engineering, system modeling is the process of formulating a representation of a real system in an abstract way to understand its behavior. Software testing encourages reusing these models for testing purpose. This expedites the process of test case generation. The objective {{of this paper is to}} explore the possibility of using the UML for {{software testing}} and how much does development of <b>code</b> <b>cost</b> with and without the models. This paper emphasis on automation of software testing which is widely used in organizations to reduce manual effort and project cost. The testing process that encourages the automation of software testing is Model based testing i. e, MBT. This testing process is based on using the modeling techniques that are defined in Unified modeling language. UML structural and behavioral specification diagrams have been used by testing researchers for generation of test scenarios and test data...|$|E
40|$|Most risk {{analysis}} tools and techniques require {{the user to}} enter {{a good deal of}} information before they can provide useful diagnoses. In this paper, we describe an approach to enable the user to obtain a COTS glue code integration {{risk analysis}} with no inputs other than the set of glue <b>code</b> <b>cost</b> drivers the user submits to get a glue code integration effort estimate with the COnstructive COTS integration cost estimation (COCOTS) tool. The risk assessment approach is built on a knowledge base with 24 risk identification rules and a 3 -level risk probability weighting scheme obtained from an expert Delphi analysis. Each risk rule is defined as one critical combination of two COCOTS cost drivers that may cause certain undesired outcome if they are both rated at their worst case ratings. The 3 -level nonlinear risk weighting scheme represents the relative probability of risk occurring with respect to the individual cost driver ratings from the input. Further, to determine the relative risk impact, we use the productivity range of each cost driver in the risky combination to reflect the cost consequence of risk occurring. We also develop a prototype called COCOTS Risk Analyzer to automate our risk assessment method. The evaluation of our approach shows that it has done an effective job of estimating the relative risk levels of both small USC eservices and large industry COTS-based applications...|$|E
40|$|Context. Reports {{suggest that}} defects in <b>code</b> <b>cost</b> the US {{in excess of}} 50 billion per year to put right. Defect Prediction is an {{important}} part of Software Engineering. It allows developers to prioritise the code that needs to be inspected when trying to reduce the number of defects in code. A small change in the number of defects found will have a significant impact on the cost of producing software. Aims. The aim of this dissertation is to investigate the factors which a ect the performance of defect prediction models. Identifying the causes of variation in the way that variables are computed should help to improve the precision of defect prediction models and hence improve the cost e ectiveness of defect prediction. Methods. This dissertation is by published work. The first three papers examine variation in the independent variables (code metrics) and the dependent variable (number/location of defects). The fourth and fifth papers investigate the e ect that di erent learners and datasets have on the predictive performance of defect prediction models. The final paper investigates the reported use of di erent machine learning approaches in studies published between 2000 and 2010. Results. The first and second papers show that independent variables are sensitive to the measurement protocol used, this suggests that the way data is collected a ects the performance of defect prediction. The third paper shows that dependent variable data may be untrustworthy as there is no reliable method for labelling a unit of code as defective or not. The fourth and fifth papers show that the dataset and learner used when producing defect prediction models have an e ect on the performance of the models. The final paper shows that the approaches used by researchers to build defect prediction models is variable, with good practices being ignored in many papers. Conclusions. The measurement protocols for independent and dependent variables used for defect prediction need to be clearly described so that results can be compared like with like. It is possible that the predictive results of one research group have a higher performance value than another research group because of the way that they calculated the metrics rather than the method of building the model used to predict the defect prone modules. The machine learning approaches used by researchers need to be clearly reported in order to be able to improve the quality of defect prediction studies and allow a larger corpus of reliable results to be gathered...|$|E
5000|$|Entropic {{estimation}} is {{a method}} for revising beliefs when faced with new data. [...] It combines Bayes theorem and Occam's Razor to find a probabilistic model that minimizes the <b>coding</b> <b>costs</b> of both the data and the model, {{which in turn has}} high probability of generalizing correctly. It is a popular technique in the analysis of bioinformatic data.|$|R
5000|$|... where Î· is the {{learning}} rate (typically 0.002 to 0.01), y is the predicted bit, and (y â P(1)) is the prediction error. The weight update algorithm differs from backpropagation {{in that the}} terms P(1)P(0) are dropped. This is because {{the goal of the}} neural network is to minimize <b>coding</b> <b>cost,</b> not root mean square error.|$|R
40|$|AbstractâWe {{consider}} {{the problem of}} decomposing a video sequence into a superposition of (a given number of) moving layers. For this problem we propose an energy minimization approach based on <b>coding</b> <b>cost.</b> Our contributions affect both the model (what is minimized) and the algorithmic side (how it is minimized). The novelty of the <b>coding</b> <b>cost</b> model is {{the inclusion of a}} refined model of the image formation process, known as superresolution. This accounts for camera blur and area averaging arising in a physically plausible image formation process. It allows to extract sharp, high-resolution layers from the video sequence. The algorithmic framework is based on an alternating minimization scheme and includes the following innovations: (1) Instead of optimizing a video labeling we optimize the layer domains. This allows to regularize the shapes of the layers and a very elegant handling of occlusions. (2) We present an efficient, parallel algorithm for extracting super-resolved layers, based on TV-filtering. I...|$|R
40|$|For {{application}} in fractal coding we investigate image partitionings that are derived by a merge process {{starting with a}} uniform partition. At each merging step one would like to opt for the rate-distortion optimal choice. Unfortunately, this is computationally infeasible when efficient coders for the partition information are employed. Therefore, one has to use a model for estimating the <b>coding</b> <b>costs.</b> We discuss merging criteria that depend on variance or collage error and on the Euclidean length of the partition boundaries. Preliminary tests indicate that improved <b>coding</b> <b>costs</b> estimators may be of crucial importance {{for the success of}} our approach. 1 INTRODUCTION In fractal image compression the image to be coded is partitioned into image blocks called ranges. Each range is approximated by another image part, called domain, which is scaled geometrically and modified using an affine transformation for the grey values. The fractal code consists of the partitioning information and, f [...] ...|$|R
40|$|AbstractThe {{problem of}} non-distorting {{compression}} (or coding) of sequences of symbols is considered. For sequences of asymptotically zero empirical entropy, {{a modification of}} the LempelâZiv coding rule is offered whose <b>coding</b> <b>cost</b> is at most {{a finite number of}} times worse than the optimum. A combinatorial proof is offered for the well-known redundancy estimate of the LempelâZiv coding algorithm for sequences having a positive entropy...|$|R
40|$|AbstractâIn {{this paper}} we revisit the {{triangulated}} cubes of the Marching Cubes algorithm {{in order to}} regularize the connectivity of the iso-surface mesh i. e. to maximize the valence six vertices. This leads to both an enhancement of the mesh topology and a significant reduction of the connectivity <b>coding</b> <b>cost.</b> We prove {{the effectiveness of the}} approach by processing medical examples with various geometry and topology...|$|R
50|$|The Mobile Giving Foundation {{collects}} all money {{donated to}} charities through wireless carriers. Carriers currently partnered with the Mobile Giving Foundation pass 100% of donations {{through to the}} nonprofit. The Mobile Giving Foundation then takes 5% to 10% of the donations to cover messaging and short <b>code</b> <b>costs</b> and 90% to 95% of donations are {{passed on to the}} non-profit organizations running the campaigns.|$|R
5000|$|In a true job cost {{accounting}} system, a Budget {{is set up}} {{in advance of the}} job. As actual costs are accrued, they are compared to budgeted costs, to determine variances for each phase of each job. <b>Cost</b> <b>Codes</b> are used for each phase, allowing [...] "mini-budgets" [...] to be generated and tracked. In the construction industry, the Construction Specifications Institute (CSI) has established an industry standard <b>Cost</b> <b>Coding</b> system.job <b>costing</b> system consists of various cost driver that drives job cost, moreover it ...|$|R
40|$|A new {{approach}} for Embedded or Progressive Transmission (PT) image coding based on wavelet transform (WT) and binary position coding is presented. By separating the binary position information and amplitude information {{the positions of}} the amplitude range of interest are eciently coded using adaptive bit-plane encoding[2]. Bit allocation which takes into account of lossless coding gain is derived and optimal scanning scheme which minimizes the total <b>coding</b> <b>cost</b> is devised. 1...|$|R
40|$|In this paper, we {{introduce}} a new algorithm called `bits-back coding' that makes stochastic source codes efficient. For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword. Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parametersâ maximum-likelihood estimationâactually minimizes the bits-back <b>coding</b> <b>cost</b> when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum-likelihood estimationâthe generalized expectation-maximization algorithmâminimizes the bits-back <b>coding</b> <b>cost.</b> After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution {{can be used for}} bits-back coding. We illustrate the performance of bits-back coding using non-synthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol...|$|R
40|$|In this paper, we {{introduce}} a new algorithm called "bits-back coding" that makes stochastic source codes efficient. For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword. Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution {{can be used for}} bits-back coding. It turns out that a commonly used technique for determining parameters [...] - maximum likelihood estimation [...] - actually minimizes the optimal bitsback <b>coding</b> <b>cost.</b> A tractable approximation to maximum likelihood estimation [...] - incremental expectation maximization [...] - minimizes the bits-back <b>coding</b> <b>cost</b> as well. We illustrate the performance of bits-back coding first on a toy problem and then using real data with a binary Bayesian network that produces 2 60 possible codewords for each symbol. For both tasks, the rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol...|$|R
50|$|A solar-tracking 7 kW {{photovoltaic}} system {{would probably have}} an installed price well over $20,000 USD (with PV equipment prices currently falling at roughly 17% per year). Infrastructure, wiring, mounting, and NEC <b>code</b> <b>costs</b> may add up to an additional cost; for instance a 3120 watt solar panel grid tie system has a panel cost of $0.99/watt peak, but still costs ~$2.2/watt hour peak. Other systems of different capacity cost even more, let alone battery backup systems, which cost even more.|$|R
40|$|High order {{essentially}} non-oscillatory (ENO) finite difference schemes {{are applied}} to the 2 -D and 3 -D compressible Euler and Navier-Stokes equations. Practical issues, such as vectorization, efficiency of <b>coding,</b> <b>cost</b> comparison with other numerical methods, and accuracy degeneracy effects, are discussed. Numerical examples are provided which are representative of computational problems of current interest in transition and turbulence physics. These require both nonoscillatory shock capturing and high resolution for detailed structures in the smooth regions and demonstrate the advantage of ENO schemes...|$|R
40|$|Object-oriented coding in the MPEG- 4 {{standard}} {{enables the}} separate processing of foreground objects {{and the scene}} background (sprite). Since the background sprite only has to be sent once, transmission bandwidth can be saved. This paper shows {{that the concept of}} merging several views of a non-changing scene background into a single background sprite is usually not the most e#cient way to transmit the background image. We have found that the counter-intuitive approach of splitting the background into several independent parts can reduce the overall amount of data. For this reason, we propose an algorithm that provides an optimal partitioning of a video sequence into independent background sprites (a multi-sprite), resulting in a significant reduction of the involved <b>coding</b> <b>cost.</b> Additionally, our algorithm results in background sprites with better quality by ensuring that the sprite resolution has at least the final display resolution throughout the sequence. Even though our sprite generation algorithm creates multiple sprites instead of a single background sprite, it is fully compatible with the existing MPEG- 4 standard. The algorithm has been evaluated with several test-sequences, including the well-known Table-tennis and Stefan sequences. The total <b>coding</b> <b>cost</b> could be reduced by factors of about 2. 7 or even higher...|$|R
40|$|Object-oriented coding in the MPEG- 4 {{standard}} {{enables the}} separate processing of foreground objects {{and the scene}} background (sprite). Since the background sprite only has to be sent once, transmission bandwidth can be saved. We {{have found that the}} counter-intuitive approach of splitting the background into several independent parts can reduce the overall amount of data. Furthermore, we show that in the general case, the synthesis of a single background sprite is even impossible and that the scene background must be sent as multiple sprites instead. For this reason, we propose an algorithm that provides an optimal partitioning of a video sequence into independent background sprites (a multisprite), resulting in a significant reduction of the involved <b>coding</b> <b>cost.</b> Additionally, our sprite-generation algorithm ensures that the sprite resolution is kept high enough to preserve all details of the input sequence, which is a problem especially during camera zoom-in operations. Even though our sprite generation algorithm creates multiple sprites instead of only a single background sprite, it is fully compatible with the existing MPEG- 4 standard. The algorithm has been evaluated with several test sequences, including the well-known Table-tennis and Stefan sequences. The total <b>coding</b> <b>cost</b> for the sprite VOP is reduced by a factor of about 2. 6 or even higher, depending on the sequence...|$|R
40|$|Recently {{developed}} video coding schemes such as MPEG- 4 permit arbitrarily-shaped {{objects to}} be encoded as separate entities. We describe a shape coding algorithm that adapts the MPEG- 4 arbitrary shape coding strategies to a variable block size framework. The new technique, {{when used in}} conjunction with motion compensation schemes employing quad-trees as a coding mechanism, successfully integrates the shape and quadtree coding in a unified structure that minimises temporal correlation. Performance evaluation using MPEG- 4 test sequences shows a reduction of up to 16 % in the combined motion and shape <b>coding</b> <b>cost...</b>|$|R
40|$|This {{paper is}} {{concerned}} with the decoding technique and performance of multi-dimensional concatenated single-parity-check (SPC) code. A very efficient sub-optimal soft-in-soft-out decoding rule is presented for the SPC <b>code,</b> <b>costing</b> only 3 addition-equivalent-operations per information bit. Multi-dimensional concatenated coding and decoding principles are investigated. Simulation results of rate 5 / 6 and 4 / 5 3 -dimensional concatenated SPC codes are provided. Performance of BER = 10 - 4 to approximately 10 - 5 can be achieved by the MAP and Max-Log-MAP decoders, respectively, with Eb/N 0 only 1 and 1. 5 dB away from the theoretical limits. link_to_subscribed_fulltex...|$|R
50|$|A Standard <b>Cost</b> <b>Coding</b> System is {{a system}} of cost {{classification}} {{that can be used}} for benchmarking cost and quantities data. In the Norwegian oil and gas industry, NORSOK Z-014 developed as part of the NORSOK standards. ISO is also developing a Standard <b>Cost</b> <b>coding</b> System as an extension of NORSOK Z-014 under ISO 19008.|$|R
50|$|The extra-over cost of {{building}} to Code Level 3 was valued around Â£2000-3000. Additionally the <b>Code</b> assessment <b>cost</b> around Â£2000 for a small project. The total cost was typically under 5% of a standard build.|$|R
40|$|Although {{general purpose}} {{computation}} on GPU (GPGPU) {{seems to be}} a promising method for high-performance computing, current programming frameworks such as CUDA and OpenCL are difficult and not portable enough. Therefore, we propose a new frame-work MESI-CUDA for easier GPGPU programming. MESI-CUDA provides shared variables which can be accessed from both CPU and GPU. Our compiler translates userâs shared-memory-based pro-gram into a CUDA program automatically generating the memory allocation and data transfer code. The compiler also overlaps kernel executions and data transfers by optimizing the scheduling. The evaluation results show that programs using MESI-CUDA can achieve the performance close to hand-optimized CUDA programs, largely reducing userâs <b>coding</b> <b>cost...</b>|$|R
40|$|In an {{empirical}} analysis of sixty-five software maintenance projects {{in a large}} IBM COBOL transaction processing environment, the impacts of correctable software complexity upon project costs were estimated. Module size, procedure size, {{and the use of}} complex branching were all found to significantly affect software maintenance costs. It was estimated that projects involving maintenance of systems with greater underlying <b>code</b> complexity <b>cost</b> approximately 25 % more than otherwise identical projects dealing with less complex <b>code.</b> These <b>costs</b> are estimated to amount to several million dollars at this data site, suggesting that the aggregate cost across the industry may be extremely large. A generalizable model i...|$|R
