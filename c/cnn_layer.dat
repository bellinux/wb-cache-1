16|88|Public
40|$|In this paper, {{we study}} the dynamic range of {{multi-layer}} cellular neural networks (CNN's) by using Lyapunov functions. A theorem {{is presented to}} guarantee the existence of equilibrium point of multi-layer CNN. A theorem on globally stable equilibrium point of multi-layer CNN is given. Multi-layer CNN's are used to model some functions of nitric oxide (NO) in nervous systems. In a 2 -layer CNN model, the first <b>CNN</b> <b>layer</b> implements synaptic events such as image processing tasks. During these synaptic events artificial NO sources are triggered by the outputs of the first <b>CNN</b> <b>layer.</b> In the 2 nd <b>CNN</b> <b>layer,</b> a NO di#usion model is implemented. The output of the 2 nd <b>CNN</b> <b>layer</b> functions as a feedback, which mimics the actions of NO to synaptic events, to the first <b>CNN</b> <b>layer.</b> This kind of feedback from the 2 nd <b>CNN</b> <b>layer</b> introduces "plasticity" into the CNN synaptic law of the first <b>CNN</b> <b>layer.</b> We improve the NO diffusion model in the 2 nd <b>CNN</b> <b>layer</b> by introducing the third <b>CNN</b> <b>layer,</b> which feedbacks {{the output of the}} first <b>CNN</b> <b>layer</b> to the NO diffusion model in the second <b>CNN</b> <b>layer.</b> As an application of CNN NO model, we use it to improve the performance of edge detection CNN...|$|E
30|$|Fischer et al. [48] {{reported}} that, given feature positions, descriptors {{extracted from}} <b>CNN</b> <b>layer</b> have better matchability compared to SIFT [11]. More recently, Schonberger et al. [49] {{also showed that}} CNN-based learned local features such as LIFT [17], Deep-Desc [50], and ConvOpt [51] have higher recall compared to SIFT [11] but still cannot outperform its variants, e.g., DSP-SIFT [16] and SIFT-PCA [52].|$|E
40|$|This paper {{studies the}} emotion {{recognition}} from musical {{tracks in the}} 2 -dimensional valence-arousal (V-A) emotional space. We propose a method based on convolutional (CNN) and recurrent neural networks (RNN), having significantly fewer parameters compared with the state-of-the-art method for the same task. We utilize one <b>CNN</b> <b>layer</b> followed by two branches of RNNs trained separately for arousal and valence. The method was evaluated using the 'MediaEval 2015 emotion in music' dataset. We achieved an RMSE of 0. 202 for arousal and 0. 268 for valence, {{which is the best}} result reported on this dataset. Comment: Accepted for Sound and Music Computing (SMC 2017...|$|E
30|$|Firstly, {{we produce}} n feature vectors {{generated}} by <b>CNNs</b> <b>layer</b> of pool 5 (for City Centre and New College dataset, n {{is equal to}} 2474 and 2146, respectively) using Caffe [25].|$|R
40|$|The {{objective}} {{of this paper is}} the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different <b>CNN</b> <b>layers</b> improved by average/max pooling on the feature maps. Our observation suggests that the Conv 5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various <b>CNN</b> <b>layers</b> is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving {{the state of the art}} on a number of benchmarks to a large margin. Comment: 9 pages. It will be submitted to an appropriate journa...|$|R
40|$|We {{present a}} simple yet {{effective}} neural network architecture for image recognition. Unlike the previous state-of-the-art neural networks which usually have very deep architectures, we build networks that are shallower but can achieve better performances on three competitive benchmark datasets, i. e., CIFAR- 10 / 100 and ImageNet. Our architectures are built using Gradually Updated Neural Network (GUNN) layers, which {{differ from the}} standard Convolutional Neural Network (<b>CNN)</b> <b>layers</b> in the way their output channels are computed: the <b>CNN</b> <b>layers</b> compute the output channels simultaneously while GUNN layers compute the channels gradually. By adding the computation ordering to the channels of CNNs, our networks are able to achieve better accuracies while using fewer layers and less memory. The architecture design of GUNN is guided by theoretical results and verified by empirical experiments. We set new records on the CIFAR- 10 and CIFAR- 100 datasets and achieve better accuracy on ImageNet under similar complexity with the previous state-of-the-art methods...|$|R
30|$|We aim at {{providing}} the deep neural architecture {{with the ability}} to adapt to new subject cases, assisting doctors with efficient patient-specific analysis and treatment selection, without forgetting its former knowledge. Our methodology is based on a new network retraining approach which extends the work in [5, 19]. This approach uses clustering [26] of trained system internal representations, in particular, of the neurons’ outputs at the last fully connected <b>CNN</b> <b>layer</b> (denoted, in vector form, as F in Fig.  5), or at the last hidden RNN layer (let us denote them, in vector form, as u, and consider them feeding the output units o). We use the centres of these clusters as knowledge extracted from the data-driven supervised training of the DNN architecture.|$|E
40|$|We {{introduce}} our {{method and}} system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several pose-specific deep {{convolutional neural network}} (CNN) models to generate multiple pose-specific features. 3 D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, <b>CNN</b> <b>layer</b> selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPA's CS 2 and NIST's IJB-A in both verification and identification (i. e. search) tasks. Comment: WACV 201...|$|E
3000|$|Phase I of the {{algorithm}} is {{the training of}} CNN C and S Layers. Supervised learning is used to train C and S layers. A gradient descend method is used to update the weights in all the layers. Phase II of {{the algorithm}} is only used when new training samples are available. The issue is to incorporate the information available from the new samples into the trained network. This issue is solved by the Phase II step of the algorithm. In this phase output, O^z of the last <b>CNN</b> <b>layer</b> is tapped and reweighted or updated using Eq. (3) to get new vector O^ztk for each training sample. In Phase III step of the algorithm, layer F_ 6 [...] is trained with O^ztk as training vectors for the classification task.|$|E
40|$|Convolutional neural {{networks}} (CNNs) are revolutionizing {{a variety of}} machine learning tasks, but they present significant computational challenges. Recently, FPGA-based accelerators have been proposed to improve the speed and efficiency of CNNs. Current approaches construct a single processor that computes the <b>CNN</b> <b>layers</b> one at a time; this single processor is optimized to maximize the overall throughput at which the collection of layers are computed. However, this approach leads to inefficient designs because the same processor structure is used to compute <b>CNN</b> <b>layers</b> of radically varying dimensions. We present a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, {{each of which is}} tailored for a different subset of the <b>CNN</b> convolutional <b>layers.</b> Using the same FPGA resources as a single large processor, multiple smaller specialized processors result in increased computational efficiency and lead to a higher overall throughput. Our design methodology achieves 1. 51 x higher throughput than {{the state of the art}} approach on evaluating the popular AlexNet CNN on a Xilinx Virtex- 7 FPGA. Our projections indicate that the benefit of our approach increases with the amount of available FPGA resources, already growing to over 3 x over the state of the art within the next generation of FPGAs...|$|R
30|$|Hou et al. [7] {{were the}} pioneers to {{consider}} using features generated by <b>CNNs</b> <b>layers</b> for visual LCD. They use a public pre-trained CNNs model, Places CNNs, {{trained on the}} scene-centric dataset Places [26] with over 2.5 million images of 205 scene categories, as an efficient whole-image descriptor generator for LCD. They comprehensively compared the performance of Places <b>CNNs</b> model’s all <b>layers</b> by using the euclidean distance as the similarity measurement. Their work demonstrated that the pool 5 layer provides the best image descriptors {{in terms of both}} detection accuracy and dimension of feature among all Places CNNs descriptors.|$|R
30|$|Against the background, in {{this paper}} we provide two {{solutions}} to address the above two challenges. Firstly, we explicitly provide matching range of candidate images to prevent images matching with their adjacent images. Meanwhile, we get better performance than state-of-the-art algorithms by adapting the matching range. Secondly, we provide a efficient feature compression method to reduce the dimension of feature generated by <b>CNNs</b> <b>layers,</b> which boosts real-time performance with marginal performance loss.|$|R
40|$|Recent {{advances}} in 3 D sensing technologies {{make it possible}} to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3 D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The <b>CNN</b> <b>layer</b> learns low-level translationally invariant features which are then given as inputs to multiple, fixed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. ...|$|E
40|$|Deformable part models (DPMs) and {{convolutional}} {{neural networks}} (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are "black-box" non-linear classifiers. In this paper, {{we show that}} a DPM can be formulated as a CNN, thus providing a novel synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent (and at times novel) <b>CNN</b> <b>layer.</b> From this perspective, it becomes natural to replace the standard image features used in DPM with a learned feature extractor. We call the resulting model DeepPyramid DPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly outperforms DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running {{an order of magnitude}} faster...|$|E
30|$|For each NN layer or {{mechanism}} (e.g., word embedding, dropout, RNN/LSTM/BLSTM, and CNN; see Sect.  4.1), we optimized its different parameters. In {{the word}} embedding layer, the parameter was the hidden node {{number in the}} hidden layer, which was checked between 500 and 1300 with a 100 hidden-node jump. In the <b>CNN</b> <b>layer,</b> the parameters were the filter size, which was evaluated between two and seven with a one-step jump, {{and the number of}} filters, which was checked between 500 and 1300 with a 100 hidden-node jump. The max pooling layer was set to be global. Last, in the RNN/LSTM layer (used by all LSTM variants; Sect.  4.1), the memory unit number was tested between 500 and 1300 with a 100 hidden-node jump. For all the models, we used 35 epochs and 64 trajectories as the batch size. For each of the evaluated parameters, its best value was selected using the validation set, as described in Sect.  5.|$|E
40|$|We {{address the}} problem of contour {{detection}} via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different <b>CNN</b> <b>layers</b> and verify their performance on BSDS 500. Comment: 2 pages. arXiv admin note: substantial text overlap with arXiv: 1412. 685...|$|R
40|$|Convolutional Neural Networks (CNN) are {{the most}} popular of deep network models due to their {{applicability}} and success in image processing. Although plenty of effort has been made in designing and training better discriminative CNNs, little is yet known about the internal features these models learn. Questions like, what specific knowledge is coded within <b>CNN</b> <b>layers,</b> and how can it be used for other purposes besides discrimination, remain to be answered. To advance in the resolution of these questions, in this work we extract features from <b>CNN</b> <b>layers,</b> building vector representations from CNN activations. The resultant vector embedding is used to represent first images and then known image classes. On those representations we perform an unsupervised clustering process, with the goal of studying the hidden semantics captured in the embedding space. Several abstract entities untaught to the network emerge in this process, effectively defining a taxonomy of knowledge as perceived by the CNN. We evaluate and interpret these sets using WordNet, while studying the different behaviours exhibited by the <b>layers</b> of a <b>CNN</b> model according to their depth. Our results indicate that, while top (i. e., deeper) layers provide the most representative space, low layers also define descriptive dimensions. This work was partially supported by the IBM/BSC Technology Center for Supercomputing (Joint Study Agreement, No. W 156463), by the Spanish Government through Programa Severo Ochoa (SEV- 2015 - 0493), by the Spanish Ministry of Science and Technology through TIN 2015 - 65316 -P project and by the Generalitat de Catalunya (contracts 2014 -SGR- 1051). Peer ReviewedPostprint (author's final draft...|$|R
30|$|However, when we {{actually}} use these features generated by <b>CNNs</b> <b>layers</b> {{in a practical}} environment, two challenges appear. Firstly, the adjacent images in the dataset of LCD might have more resemblance than the images that really form the loop closure, so the algorithm tends to identify the adjacent images as loop closure, which is certainly not preferred. Secondly, the feature matching is computationally intensive because the dimension of features generated by CNNs may be very large, and LCD may have to compare the current image to {{a large amount of}} pre-captured images in order to decide whether the robot returns to previously visited positions. This can not satisfy strong request for real-time performance in robotic applications.|$|R
40|$|We {{introduce}} {{the concept of}} dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image {{is based on the}} rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling <b>CNN</b> <b>layer</b> allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance...|$|E
30|$|To {{solve this}} problem, many hand-crafted {{features}} {{have been used}} for visual tracking, such as Haar-like features, histogram of oriented gradient (HOG) features, local binary pattern (LBP), and scale-invariant feature transform (SIFT). However, these hand-crafted features are not robust for generic object tracking. Convolutional neural network (CNN) models which learn hierarchical features from raw images on large-scale dataset have been widely used to represent the appearance of the target. Ma et al. [24] exploit the features from hierarchical layers of CNN within a correlation filter-based framework for visual tracking, learn linear correlation filters on each <b>CNN</b> <b>layer,</b> and adopt a coarse-to-fine method to estimate the target location. Wang et al. [25] analyze CNN features from different layers and use a novel tracking method which jointly exploits two convolutional layers to mitigate the drift problem. Danelljan et al. [26] indicate that activations from the first convolutional layers achieve favorable tracking performance compared with the deeper layers within a discriminative correlation filter-based framework. In contrast to the traditional feature descriptors, CNN features contain more structural information, which is crucial to localize the target in an unknown frame.|$|E
40|$|We {{introduce}} {{the concept of}} "dynamic image", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of `rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data {{and to use the}} parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation {{is in the form of}} an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a <b>CNN</b> <b>layer.</b> This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance. Comment: 14 pages, 9 figures, 9 table...|$|E
40|$|Recently, deep {{learning}} {{approach has been}} used widely {{in order to enhance}} the recognition accuracy with different application areas. In this paper, both of deep convolutional neural networks (CNN) and support vector machines approach were employed in human action recognition task. Firstly, 3 D CNN approach was used to extract spatial and temporal features from adjacent video frames. Then, support vector machines approach was used in order to classify each instance based on previously extracted features. Both of the number of <b>CNN</b> <b>layers</b> and the resolution of the input frames were reduced to meet the limited memory constraints. The proposed architecture was trained and evaluated on KTH action recognition dataset and achieved a good performance...|$|R
40|$|We {{present a}} method for skin lesion {{segmentation}} for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach {{is based on a}} Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network 1 × 1 convolution layers to add capacity to the network and (iii) state-of-art super-resolution upsampling of predictions using subpixel <b>CNN</b> <b>layers.</b> We reported a mean IOU score of 0. 642 on the validation set provided by the organisers...|$|R
40|$|We {{address the}} problem of contour {{detection}} via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different <b>CNN</b> <b>layers</b> and obtain comparable performances to the state-of-the-art on BSDS 500. Comment: 9 pages, 3 figure...|$|R
40|$|Deep Convolutional Neural Networks (CNN) {{have shown}} great success in {{supervised}} classification {{tasks such as}} character classification or dating. Deep learning methods typically {{need a lot of}} annotated training data, which is not available in many scenarios. In these cases, traditional methods are often better than or equivalent to deep learning methods. In this paper, we propose a simple, yet effective, way to learn CNN activation features in an unsupervised manner. Therefore, we train a deep residual network using surrogate classes. The surrogate classes are created by clustering the training dataset, where each cluster index represents one surrogate class. The activations from the penultimate <b>CNN</b> <b>layer</b> serve as features for subsequent classification tasks. We evaluate the feature representations on two publicly available datasets. The focus lies on the ICDAR 17 competition dataset on historical document writer identification (Historical-WI). We show that the activation features trained without supervision are superior to descriptors of state-of-the-art writer identification methods. Additionally, we achieve comparable results in the case of handwriting classification using the ICFHR 16 competition dataset on historical Latin script types (CLaMM 16). Comment: ICDAR 2017 camera ready (fixed p@ 2 values, missing table references...|$|E
40|$|IEEE We {{introduce}} {{the concept of}} dynamic image, a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of and #x 0027;rank pooling and #x 0027;. The idea is to learn a ranking machine that captures the temporal evolution of the data {{and to use the}} parameters of the latter as a representation. We call the resulting representation dynamic image because it summarizes the video dynamics in addition to appearance. This powerful idea allows to convert any video to an image so that existing CNN models pre-trained with still images can be immediately extended to videos. We also present an efficient approximate rank pooling operator that runs two orders of magnitude faster than the standard ones with any loss in ranking performance and can be formulated as a <b>CNN</b> <b>layer.</b> To demonstrate the power of the representation, we introduce a novel four stream CNN architecture which can learn from RGB and optical flow frames as well as from their dynamic image representations. We show that the proposed network achieves state-of-the-art performance, 95. 5 % and 72. 5 % accuracy, in the UCF 101 and HMDB 51 respectively...|$|E
40|$|We present highly {{efficient}} algorithms for performing {{forward and backward}} propagation of Convolutional Neural Network (CNN) for pixelwise classification on images. For pixelwise classification tasks, such as image segmentation and object detection, surrounding image patches are fed into CNN for predicting the classes of centered pixels via forward propagation and for updating CNN parameters via backward propagation. However, forward and backward propagation was originally designed for whole-image classification. Directly applying it to pixelwise classification in a patch-by-patch scanning manner is extremely inefficient, because surrounding patches of pixels have large overlaps, which lead {{to a lot of}} redundant computation. The proposed algorithms eliminate all the redundant computation in convolution and pooling on images by introducing novel d-regularly sparse kernels. It generates exactly the same results as those by patch-by-patch scanning. Convolution and pooling operations with such kernels are able to continuously access memory and can run efficiently on GPUs. A fraction of patches of interest can be chosen from each training image for backward propagation by applying a mask to the error map at the last <b>CNN</b> <b>layer.</b> Its computation complexity is constant with respect to the number of patches sampled from the image. Experiments have shown that our proposed algorithms speed up commonly used patch-by-patch scanning over 1500 times in both forward and backward propagation. The speedup increases with the sizes of images and patches...|$|E
30|$|To {{overcome}} this problem, pre-trained CNN feature extraction method is proposed in recent years. CNN features, extracted from different <b>CNN</b> <b>layers,</b> have different characteristics {{in describing the}} object [24]. The CNN features from deeper layer contains more high-level semantic information, which {{can be seen as}} structural information, have more distinguishing capabilities and thus is effective facing the situation when intra-class appearance variation occurs. However, the features from deep layer have very low spatial resolution so that it cannot fit the task in generic visual tracking, which aim to indicate the location of target. On the other hand, CNN features from earlier layer contain more fine-grained information, which means the more the discriminative capabilities, the more effective in locating the target. But with the less semantic information, features from earlier layer are more sensitive to intra-class appearance variations.|$|R
40|$|At present, {{designing}} convolutional {{neural network}} (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified {{from a handful of}} existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose <b>CNN</b> <b>layers</b> using Q-learning with an ϵ-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks...|$|R
40|$|Abstract — This paper {{describes}} {{the design and}} the implementation of an embedded system based on multiple FPGAs {{that can be used}} to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more <b>CNN</b> <b>layers.</b> The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates. Keywords-component; Multi-FPGA; Embedded Syseim...|$|R
40|$|Deep Convolutional Neural Networks (CNNs) have {{recently}} evinced immense success for various image recognition tasks [11, 27]. However, {{a question of}} paramount importance is somewhat unanswered in deep learning research - is the selected CNN optimal for the dataset in terms of accuracy and model size? In this paper, we intend {{to answer this question}} and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size. We use two operations for architecture refinement, viz. stretching and symmetrical splitting. Stretching increases the number of hidden units (nodes) in a given <b>CNN</b> <b>layer,</b> while a symmetrical split of say K between two layers separates the input and output channels into K equal groups, and connects only the corresponding input-output channel groups. Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture. We empirically demonstrate the necessity of the two operations. We evaluate our approach on two natural scenes attributes datasets, SUN Attributes [16] and CAMIT-NSAD [20], with architectures of GoogleNet and VGG- 11, that are quite contrasting in their construction. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our architectural refinement algorithm. Our results substantiate the usefulness of the proposed method...|$|E
40|$|International audienceConvolutional Neural Network (CNN) {{techniques}} improved {{accuracy and}} robustness of machine vision systems {{at the price}} of a very high computational cost. This motivated multiple research efforts to investigate the applicability of approximate computing and more particularly, fixed point-arithmetic for CNNs. In all this approaches, a recurrent problem is that the learned parameters in deep <b>CNN</b> <b>layers</b> have a significantly lower numerical dynamic range when compared to the feature maps. This problem prevents from using of a low bit-width representation in deep layers. In this paper, we demonstrate that using the TanH activation function is way to prevent this issue. To support this demonstration, three benchmark CNN models are trained with the TanH function. These models are then quantized using the same bit-width across all the layers. Efficiency of this method is demonstrated on an FPGA based accelerator, by inferring CNNs with the minimal amount of logic elements...|$|R
40|$|Previous {{models for}} video {{captioning}} often use the output from a specific layer of a Convolutional Neural Network (CNN) as video features. However, the variable context-dependent semantics {{in the video}} may make it more appropriate to adaptively select features from the multiple <b>CNN</b> <b>layers.</b> We propose a new approach for generating adaptive spatiotemporal representations of videos for the captioning task. A novel attention mechanism is developed, that adaptively and sequentially focuses on different <b>layers</b> of <b>CNN</b> features (levels of feature "abstraction"), {{as well as local}} spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube 2 Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics. Comment: Accepted to AAAI 201...|$|R
40|$|We {{conduct an}} {{in-depth}} exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study {{different ways of}} performing spatial and temporal pooling, feature normalization, choice of <b>CNN</b> <b>layers</b> as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches {{that have been used}} till now. We evaluate our approach on the challenging TRECVID MED' 14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED' 14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED' 14 from 34. 95 % to 38. 74 %. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF- 101 dataset...|$|R
40|$|We {{describe}} a joint model for intent detection and slot filling based on convolutional neural networks (CNN). The proposed architecture can {{be perceived as}} a neural network (NN) ver-sion of the triangular CRF model (TriCRF), in which the in-tent label and the slot sequence are modeled jointly and their dependencies are exploited. Our slot filling component is a globally normalized CRF style model, as opposed to left-to-right models in recent NN based slot taggers. Its features are automatically extracted through <b>CNN</b> <b>layers</b> and shared by the intent model. We show that our slot model component generates state-of-the-art results, outperforming CRF signifi-cantly. Our joint model outperforms the standard TriCRF by 1 % absolute for both intent and slot. On a number of other domains, our joint model achieves 0. 7 - 1 %, and 0. 9 - 2. 1 % absolute gains over the independent modeling approach for intent and slot respectively. Index Terms — Joint modeling, slot filling, convolutional neural network, triangular CR...|$|R
40|$|Neural {{networks}} have {{shown to be}} a practical way of building a very complex mapping between a pre-specified input space and output space. For example, a convolutional neural network (CNN) mapping an image into one of a thousand object labels is approaching human performance in this particular task. However the mapping (neural network) does not automatically lend itself to other forms of queries, for example, to detect/reconstruct object instances, to enforce top-down signal on ambiguous inputs, or to recover object instances from occlusion. One way to address these queries is a backward pass through the network that fuses top-down and bottom-up information. In this paper, we show a way of building such a backward pass by defining a generative model of the neural network's activations. Approximate inference of the model would naturally {{take the form of a}} backward pass through the <b>CNN</b> <b>layers,</b> and it addresses the aforementioned queries in a unified framework...|$|R
