0|24|Public
50|$|Semantic {{compression}} is advantageous {{in information}} retrieval tasks, improving their effectiveness (in {{terms of both}} precision and recall). This is due to more precise descriptors (reduced effect of language diversity - limited language redundancy, a step towards a <b>controlled</b> <b>dictionary).</b>|$|R
50|$|The tool used {{to provide}} the {{cryptographic}} tests was John the Ripper (JtR). JtR was scaled by using named pipes to funnel a <b>controlled</b> <b>dictionary</b> (a set of keys to try) into an arbitrary number of JtR clients. Each client would take one key, encrypt it, and test it against a local copy of the hash(es). John the Ripper on CHAOS differed from Cisillia as it facilitated dictionary based brute-force attacks across {{a large number of}} algorithms, rather than an entire key-space driven brute-force attack across one or two algorithms.|$|R
40|$|Abstract. In {{recent years}} {{a variety of}} {{approaches}} in classifying the sen-timent polarity of texts have been proposed. While {{in the majority of}} approaches the determination of subjectivity or polarity-related term features is at the center, the number of publicly available dictionaries is rather limited. In this paper, we investigate the performance of com-bining lexical resources with machine learning-based classifier for the task of sentiment classification. We systematically analyze four different English and three different German polarity dictionaries as a resources for a sentiment-based feature selection. The evaluation results show that smaller but more <b>controlled</b> <b>dictionaries</b> used for feature selection per-form within a SVM-based classification setup equally good compared to the biggest available resources...|$|R
40|$|This Provisional PDF {{corresponds}} to the article as it appeared upon acceptance. Copyedited and fully formatted PDF and full text (HTML) versions will be made available soon. PubFocus: Semantic MEDLINE/PubMed citations analytics through integration of <b>controlled</b> biomedical <b>dictionaries</b> and ranking algorithm BMC Bioinformatics 2006, 7 : 424 doi: 10. 1186 / 1471 - 2105 - 7 - 42...|$|R
40|$|End-users {{should be}} able to {{identify}} the most relevant scientific and technical information resources. They must understand database structure, query techniques, <b>controlled</b> <b>dictionaries,</b> terminology and indexing schemes. In the present article, we review principal characteristics of the following information systems: bibliographic databases (Agricola, Agris, Cab Abstracts (CABI), Compendex, Inspec, Iconda, statistical collections Eurostat and Faostat, and technical standards and patents ISO and WIPO with regard to the field of wood sciences or processing of forest products. Differences in database fields and search characteristics (search platforms, Boolean logic, prefixes, suffixes, wildcards) are described. Special attention is placed on subject headings in each respective database: narrow keywords (Descriptors, Heading Words, Controlled Terms, Keywords) and broader categories (Subject Categories, Concept Codes, CABI Codes, Classification Codes). Word counts are shown for the terms with the highest frequency of occurrences. This review promotes the use of additional databases, encourages employment of structured vocabularies, which should result in improved search results...|$|R
50|$|In 1980, {{soon after}} the {{publication}} of the first Official Scrabble Players <b>Dictionary,</b> <b>control</b> of the national tournament passed to the National Scrabble Association. They continued to organize the tournament until 2008.|$|R
40|$|Interpretation and {{exploration}} of longitudinal clinical data {{is a major}} part of diagnosis, therapy, quality assessment, and clinical research, particularly for chronic patients. KNAVE-II is an intelligent interface to a distributed architecture specific to the tasks of query, knowledge-based interpretation, summarization, visualization, interactive exploration of large numbers of distributed time-oriented clinical data and dynamic sensitivity analysis of these data. The web-based architecture enables users (e. g., physicians) to query, visualize and explore clinical time-oriented databases. Both, the generation of context-sensitive interpretations (abstractions) of the time-stamped data, as well as the dynamic visual exploration of the raw data and the multiple levels of concepts abstracted from these data, are supported by runtime access to domain-specific knowledge bases, maintained by domain experts. KNAVE-II was designed according to a set of well-defined desiderata. The architecture enables exploration along both absolute (calendar-based) and relative (clinically meaningful) time-lines. The underlying architecture uses standardized vocabularies (such as a <b>controlled</b> <b>dictionary</b> for laboratory tests and physical observations), and predefined mappings to local data sources, for communication among its various components. Thus, the new framework enables users to access and explore multiple remote heterogeneous databases, without explicitly knowing thei...|$|R
5000|$|More recently, {{the word}} hangry—a {{portmanteau}} of 'hungry' and 'angry'—has {{been used to}} refer to an irritable state induced by lack of food. [...] Oxford <b>Dictionaries</b> (<b>controlled</b> by but more lax than the Oxford English Dictionary) added hangry on 27 August 2015.|$|R
5000|$|VEST Registry is {{a catalog}} of {{controlled}} vocabularies (such as authority files, classification systems, concept maps, <b>controlled</b> lists, <b>dictionaries,</b> ontologies or subject headings); metadata sets (metadata element sets, namespaces and application profiles); and tools (such as library management software, content management systems or document repository software). It is concerned primarily with collecting and maintaining a consistent set of metadata for each resource. The scope of the VEST Registry {{is to provide a}} clearing house for tools, metadata sets and vocabularies used in food, agriculture, development, fisheries, forestry and natural resources information management context.|$|R
40|$|Methodical {{recommendations}} «Introduction to linguistic studies» contains preface, {{plan of the}} themes, {{questions for}} the <b>control,</b> a short <b>dictionary</b> of linguistic terms, {{a list of the}} main and the recommended literature. Methodical recommendations is addressed to the students-philologists of bachelor and master studies and for the teachers of foreign languages...|$|R
50|$|Some {{examples}} of dream interpretation are: dreaming {{you are on}} a beach means you are facing negativity in your life, or a lion may represent a need to <b>control</b> others. Dream <b>dictionaries</b> typically hold interpretations ranging from A-Z. Dream dictionaries {{can be found in}} book form or on the internet.|$|R
500|$|One of {{his most}} famous {{contributions}} to statistics is sequential sampling. Friedman did statistical work at the Division of War Research at Columbia, {{where he and his}} colleagues came up with the technique. It later became, in the words of The New Palgrave Dictionary of Economics, [...] "the standard analysis of quality <b>control</b> inspection". The <b>dictionary</b> adds, [...] "Like many of Friedman’s contributions, in retrospect it seems remarkably simple and obvious to apply basic economic ideas to quality control; that however is a measure of his genius." ...|$|R
40|$|Enter the Swift {{future of}} iOS and OS X {{programming}} Beginning Swift Programming is your ideal {{starting point for}} creating Mac, iPhone, and iPad apps using Apple's new Swift programming language. Written by an experienced Apple developer and trainer, this comprehensive guide explains {{everything you need to}} know to jumpstart the creation of your app idea. Coverage includes data types, strings and characters, operators and functions, arrays and <b>dictionaries,</b> <b>control</b> flow, and looping, with expert guidance on classes, objects, class inheritance, closures, protocols, and generics. This succinct - y...|$|R
40|$|The {{theme of}} this thesis is {{development}} of multimedia mobile applications. Specifically, applications {{in the field of}} rehabilitation and re-education of patients suffering from aphasia. The aim of this thesis is to determine the application parameters, which in practice could be used by people, who suffer from aphasia and all those who assist them in therapy To this end, was developed the first version of Aphasia vocabulary. By the medium of Aphasia vocabulary was then conducted research survey at pacients with aphasia. The research assesses the suitability of various multimedia applications and <b>controls</b> aphasic <b>dictionary</b> while working with a speech therapist with the patient. It is a qualitative research based on the method of interviewing and participant observation at a standard therapeutic practice. The result of the research is a set of recommendations for the development of mobile applications for people with aphasia. Conclusions are useful in the creation of rehabilitation and re-education materials...|$|R
40|$|While {{remanufacturing}} is a-concept {{that has}} been around for some time, it is not a widely understood one. In fact, the word doesn't even appear in the current edition of the American Production and Inventory <b>Control</b> Society <b>Dictionary</b> [12]. It is not simply the repairing of a broken item, nor the "reconditioning " of a product: Remanufacturing is an industrial process in which wom-out products are restored to like-new condition. Through a series of industrial processes in a factory environment, a discarded product is completely disassembled. Usable parts are cleaned, refurbished, and put into inventory. Then the new product is reassembled from both old and, where necessary, new parts to produce a unit fully equivalent-and sometimes superior-in performance and expected lifetime to the original new product. In contrast, a repaired or rebuilt product normally retains its identity, and only those parts that have failed or are badly wom are replaced or serviced [7 l. As an example, The Trane Company, a leading air conditioning manufacturer, remanufactures air conditioning compressors and motors at...|$|R
5000|$|A {{variety of}} {{techniques}} based on artificial intelligence (AI) and {{natural language processing}} (NLP) have been applied to semantic processing, {{and most of them}} have relied on the use of auxiliary structures such as controlled vocabularies and ontologies. <b>Controlled</b> vocabularies (<b>dictionaries</b> and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries. Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries. Over the years, additional auxiliary structures of general interest, such as the large synonym sets of WordNet, have been constructed. [...] It was shown that concept search that is based on auxiliary structures, such as WordNet, can be efficiently implemented by reusing retrieval models and data structures of classical information retrieval. [...] Later approaches have implemented grammars to expand the range of semantic constructs. The creation of data models that represent sets of concepts within a specific domain (domain ontologies), and which can incorporate the relationships among terms, has also been implemented in recent years.|$|R
40|$|Iako razvoj specijalnih knjižnica započinje krajem 18. i početkom 19. stoljeća, 20. stoljeće se naziva stoljećem specijalnih knjižnica. Nakon kratkog pregleda specijalnih knjižnica, ističu se neki aspekti povijesti Knjižnice Hrvatskog državnog arhiva te pregled djelovanja knjižničara u arhivskim knjižnicama u Hrvatskoj. U drugom dijelu govori se o potrebnim znanjima i vještinama knjižničara / informacijskih stručnjaka zaposlenih u specijalnim knjižnicama. Cilj je rada ukazati na specifičnosti rada u knjižnicama pri arhivima, s posebnim osvrtom na dodatna znanja i vještine koje se očekuju od knjižničara u takvim knjižnicama. Although {{development}} of the special libraries began {{at the end of}} the 18 th and in the early 19 th century, the 20 th century is known as a century of special libraries. Specialisation of library collections and services led to establishment of special libraries, first in Great Britain in the first half of the 18 th century, then in the United States of America and Europe. In Croatia first special libraries were established at the beginning of the 19 th century. First special libraries differed by thematic collections. They gradually developed, and their principal task became serving to their institutions. Apart from meeting user needs, special libraries are research and development centers occupied with research and {{development of}} <b>controlled</b> <b>dictionaries</b> of specified disciplines, and information use, but also with their impact on scientific and professional work. Special libraries can be roughly divided into two groups. Corporate libraries are being established within companies or organizations to meet information needs of its employees. On the other side, many of archival or museum libraries, same as libraries of voluntary or professional associations, or libraries with regional tasks, do not serve a narrow population, but offer their services and open their collections to the general public. According to such missions, their goals and quality indicators differ. Characteristics of special archival libraries may be recognized in both groups. They tend to posses qualities of corporate libraries efficiency and quick delivery of required information to their clientele, and their collection building is based on actual user needs. From the other group they take over a quality of building comprehensive collections on specified subjects or with regional imprint, preserving collections for future generations. Publishing of bibliographies and catalogues and organization of cultural events will often be included in their tasks...|$|R
30|$|The {{compression}} ratios (CRs) and the SDG for {{all three}} coders (MP 3, AAC and ATFT) are shown in Table 1. All the coders were tested in the VBR mode. For the presented technique, VBR {{was the best way}} to present the performance benefit of using an adaptive decomposition approach. In ATFT, the type of the signal and the characteristics of the TF functions (type of <b>dictionary)</b> <b>control</b> the number of transformation parameters required to approximate the signal and thereby the compression ratio. The inherent variability introduced in the number of TF functions required to model a signal is one of the highlights of using ATFT. Hence we choose to present comparison of the coders in the VBR mode.|$|R
40|$|Motivation: Sites with {{substantive}} bioinformatics {{operations are}} challenged to build data processing and delivery infrastructure that provides reliable access and enables data integration. Locally generated data must be processed and stored such that relationships to external data sources can be presented. Consistency and comparability across data sets requires annotation with controlled vocabularies and, further, metadata standards for data representation. Programmatic {{access to the}} processed data should be supported to ensure the maximum possible value is extracted. Confronted with these challenges at the National Cancer Institute Center for Bioinformatics, we decided to develop a robust infrastructure for data management and integration that supports advanced biomedical applications. Results: We have developed an interconnected set of software and services called caCORE. Enterprise Vocabulary Services (EVS) provide <b>controlled</b> vocabulary, <b>dictionary</b> and thesaurus services. The Cancer Data Standards Repository (caDSR) provides a metadata registry for common data elements. Cancer Bioinformatics Infrastructure Objects (caBIO) implements an object-oriented model of the biomedical domain and provides Java, Simple Object Access Protocol and HTTP–XML application programming interfaces. caCORE {{has been used to}} develop scientific applications that bring together data from distinct genomic and clinical science sources. Availability: caCORE downloads and web interfaces can be accessed from links on the caCORE web sit...|$|R
40|$|Vocabulary is {{of vital}} {{importance}} for EFL learners particularly for its major contribution to reading comprehension. The lack of necessary support for learning vocabulary effectively explains why EFL university learners still have limited vocabulary knowledge. The integration {{of computer technology}} has offered language teachers and learners a variety of tools to assist in developing pedagogical practices and language learning. One of these tools is corpora, also known as data-driven learning (DDL). The {{purpose of this study}} is to examine whether DDL instruction has a significant effect on developing EFL Yemeni learners’ words meaning as well as the collocation of receptive vocabulary knowledge compared to the dictionary use. The participants in this study were 60 female second level English language students who were divided into an experimental group (DDL group) and a <b>control</b> group (<b>dictionary</b> group). The findings from the pretest result demonstrated comparability of the two groups in two aspects of vocabulary knowledge. However, the results from the posttest and the delayed posttest showed that the learning outcomes of the DDL group were significantly higher than the dictionary group. The study findings confirmed the substantial short and long term effects of DDL instructional method on vocabulary learning. Based on the study results, there is a great need for raising teachers’ and learners’ awareness of the effectiveness of the DDL method particularly in a Yemeni context where corpus use is still a novel method for learning...|$|R
40|$|Selected for Journal Publication from CITALA 12 Conference. This paper {{describes}} a technique for spelling and correcting Arabic text that provides different variables {{that can be}} controlled to give customized results based on {{the properties of the}} processed text. The proposed technique depends on dynamic <b>dictionaries</b> <b>controlled</b> and customized based on the input text categorization. In the research reported here we employ a statistical/corpus-based approach with data obtained from the Arabic Wikipedia and local Palestinian newspapers. Based on corpus statistics we constructed databases of words and their frequencies as single, double and triple expressions and used that as the infrastructure for our spelling and text correction technique. Our spelling technique builds on earlier work[7], but using new spelling variables and dynamic dictionaries based on categorized texts. We briefly report on the results of preliminary testing and analysis. While the results reported here are promising, they must be viewed as work in progress, still in need of more testing, refining, integration and deployment in real life settings...|$|R
40|$|Ankara : The Institute of Economic and Social Sciences of Bilkent Univ., 1995. Thesis (Master's) [...] Bilkent University, 1995. Includes bibliographical {{references}} leaves 55 - 59 The {{present study}} investigated the effects of monolingual dictionary training on Turkish EFL students' reading comprehension and vocabulary learning. Thirty-seven intermediate-level Turkish EFL preparatory students in the Department of English Language Teaching at Mustafa Kemal University participated in this study. The study considered two research questions. The first question concerned the effect of monolingual dictionary training on students' reading comprehension. The second research question investigated the effects of monolingual dictionary training on students' vocabulary learning. To answer the research questions, the students {{were randomly assigned to}} one of three groups: one experimental and two control groups. The experimental group, the Dictionary Training group (DT), received special training on the use of a monolingual dictionary. One of the <b>control</b> groups, <b>Dictionary</b> group (D), had access to a monolingual dictionary, but received no training. The other control group, the Guessing group (G), had neither training with nor access to a dictionary. To gather the data, a pretest-posttest procedure was followed. The data were analysed using ANOVA procedures. In order to measure students' reading comprehension, a multiple-choice test based on two reading passages was used. There were a total of 12 questions on the test. Vocabulary learning was tested in two ways: vocabulary production (recall) and vocabulary selection (recognition). In both these tests, the same 16 vocabulary items chosen from the two reading passages were selected. A repeated-measure one-way ANOVA revealed that there were no group differences on the posttest attributable to treatment (p<. 105). Dictionary use with or without training had no significant effect on reading comprehension. The results of two separate repeated-measures one-way ANOVAs showed group differences attributable to treatment on the vocabulary learning in terms of both vocabulary production and vocabulary selection. Follow-up post hoc tests were conducted. For vocabulary production, the DT group performed significantly better than the D group (p<. 001) and the G group (p<. 001). There was no significant difference between the D and the G groups. For vocabulary selection, the DT group performed better than the D group (p<. 05) and the G group (p=. 001). There was no significant difference between the D and G groups. The findings of this study indicate that access to a monolingual dictionary, with or without training, had no significant effect on students' reading comprehension. However, dictionary training had a positive effect on both production (recall) and selection (recognition) of vocabulary. Dictionary access without training was not superior to guessing. Altun, ArifM. S...|$|R
40|$|Although subject {{indexing}} {{is part of}} {{the conceptual}} core of library and information sciences, it is characterized by theoretical and practical ambiguities. These are a result of the fact that subject indexing is inherently a subjective and interpretative process, while at the same time, neutrality, objectivity and consistency are core principles of subject indexing. Libraries and other information institutions base their subject indexing on objective, nomological and document-centered principles. This paper, based on desk research and literature analysis, will re-examine common theoretical frameworks and practical principles in subject indexing, critically assess standards and guidelines used in libraries and analyse them in the light of user tagging, which implies a conceptual tu U radu se prikazuju osnovne značajke RDF-a, općega podatkovnog modela za opisivanje izvora na mreži i mogućnosti njegove uporabe za predstavljanje normativnih datoteka u bibliografskoj domeni. U središtu pažnje je primjena Sustava za jednostavnu organizaciju znanja (Simple Knowledge Organizaton System, SKOS) i njegovog proširenja za leksičke oznake u obliku SKOS eXtension for Labels. SKOS je podatkovni model koji se zasniva na RDF-u i izražava osnovnu semantičku strukturu zajedničku svim nadziranim strukturiranim rječnicima poput tezaurusa, klasifikacijskih sustava, taksonomija i sustava predmetnih odrednica. Njegova namjena je da omogući izražavanje i objavljivanje postojećih strukturiranih rječnika na mreži kako bi se postigla njihova općenita interoperabilnost. U radu se komentiraju i neka rješenja iz Sheme za opisivanje autoriziranih metapodataka u RDF-u (Metadata Authority Description Schema in RDF, MADS/RDF) koja je proizvod Kongresne knjižnice. MADS/RDF predstavlja prvu RDF ontologiju za predstavljanje normativnih datoteka baštinskih ustanova. rn towards a subjective, user-oriented and contextual approach to subject indexing. U drugom dijelu rada razmatra se predstavljanje UNIMARC/A formata u RDF podatkovnom modelu. Prikazan je primjer modeliranja korporativne odrednice iz CROLIST-ove normativne datoteke uporabom elemenata iz različitih rječnika. The paper presents the main features of RDF, a common data model for describing resources on the web and the possibilities of its use for representing normative and authoritative files in the library and information science domain. The paper will focus on the implementation of Simple Knowledge Organization System (SKOS) and its extended vocabulary for representation of relationships between lexical labels, SKOS eXtension for Labels (SKOSXL). SKOS is a data model based on RDF and expresses the basic semantic structure common to all <b>controlled</b> structured <b>dictionaries</b> such as thesauri, classification systems, taxonomies and subject headings lists. Its purpose is to enable the expression and publication of existing structured vocabularies on the web in order to achieve their global interoperability. Furthermore, some solutions from Metadata Authority Description Schema in DF(MADS/RDF) are presented and commented. MADS/RDF is the product of the Library of Congress and the first RDF ontology for representation of normative files in heritage institutions. The second part of the paper discusses the representation of UNIMARC/A format in RDF data model. A corporate heading from the CROLIST authority file is modeled as an example of using various elements from different vocabularies...|$|R

