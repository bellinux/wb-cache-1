6|27|Public
40|$|In the 2010 case of R. v. Sinclair, the Supreme Court of Canada {{explained}} how the Charter right to counsel applies {{in the context}} of police questioning a detainee. Sinclair constitutes the final instalment in the so-called “interrogation trilogy,” which also includes R. v. Oickle and R. v. Singh. This trilogy of cases, which lays out the legal framework for police interrogation in Canada, has been criticized as providing insufficient safeguards for detainees. Yet, despite the strong reaction from critics, this paper argues that the interrogation trilogy has done little to change Canadian law. The author identifies two main conclusions to be drawn from these cases. First, the Supreme Court has clearly rejected the idea that detainees can cut off interrogation by invoking Charter rights. Second, the voluntary confessions rule has been established as the principal protection for interrogated suspects. The author argues that while the Court’s rejection of questioning <b>cut-off</b> <b>rules</b> was unfortunate, the current system of safeguards centred on the confessions rule may well protect detainees against abuse in the interrogation room better than a system centered on warnings, waivers and questioning <b>cut-off</b> <b>rules...</b>|$|E
40|$|In this thesis, we {{consider}} a gametheoretical model of income taxation. We {{take a closer}} look at the strategy of the central authority with respect to the audit probability of tax returns, given the fact that taxpayers may underdeclare their income. Different <b>cut-off</b> <b>rules</b> are compared to investigate which one optimizes the net tax revenue of the central authority. We conclude by giving the optimal strategy, depending on specific conditions, and apply this result to the Ukrainian and Dutch taxation systems...|$|E
40|$|Theory of {{tax evasion}} is reviewed. The Marrelli {{approach}} is studied {{in a more}} general environment. Indirect tax evasion by firms is extended to the general case of oligopolic markets with a conjectural variations model and either a price discrimination model, considering both ad valorem and specific taxation. From the normative point of view, <b>cut-off</b> <b>rules</b> of tax enforcement are studied and modified inverse elasticity rules for optimal taxation are derived and shown {{to depend on the}} level of distortion on the supply side, i. e. on firms` collusion, shifting and tax evasion decisions...|$|E
30|$|Terminate by {{a chosen}} {{iteration}} <b>cut-off</b> <b>rule.</b>|$|R
40|$|In this study, we {{employ a}} game-theoretic {{framework}} to formulate and analyze {{a number of}} tax audit schemes. We then test the theoretical predictions in a laboratory experiment. We compare audit schemes based on three audit rules: the random <b>rule,</b> <b>cut-off</b> <b>rule,</b> and lowest income reporter audited <b>rule.</b> While the <b>cut-off</b> <b>rule</b> {{is known to be}} optimal in theory, it has not thus far been examined in a controlled laboratory experimental setting. Contrary to the theory, the lowest income reporter audited rule yielded higher compliance behavior than the optimal <b>cut-off</b> <b>rule</b> in the experiment, even after controlling for social norms regarding tax payment perceived by the subjects. This empirical finding is practically important because the tax authorities in most countries assign higher priority to enhancing tax compliance...|$|R
3000|$|... {{that do not}} {{correspond}} to consolidation periods are re-set to zero. A simple <b>cut-off</b> <b>rule</b> inspired by prudence criterion is chosen: whenever {{the change in the}} instrumented primary structural balance is less than 0.5 % of GDP, the top-down fiscal policy variable FC [...]...|$|R
40|$|The present article {{discusses}} {{and compares}} multiple testing procedures (MTP) for controlling Type I error rates defined as tail probabilities {{for the number}} (gFWER) and proportion (TPPFP) of false positives among the rejected hypotheses. Specifically, we consider the gFWER- and TPPFP-controlling MTPs proposed recently by Lehmann 2 ̆ 6 Romano (2004) and {{in a series of}} four articles by Dudoit et al. (2004), van der Laan et al. (2004 b,a), and Pollard 2 ̆ 6 van der Laan (2004). The former Lehmann 2 ̆ 6 Romano (2004) procedures are marginal, {{in the sense that they}} are based solely on the marginal distributions of the test statistics, i. e., on <b>cut-off</b> <b>rules</b> for the corresponding unadjusted p-values. In contrast, the procedures discussed in our previous articles take into account the joint distribution of the test statistics and apply to general data generating distributions, i. e., dependence structures among test statistics. The gFWER-controlling common-cut-off and common-quantile procedures of Dudoit et al. (2004) and Pollard 2 ̆ 6 van der Laan (2004) are based on the distributions of maxima of test statistics and minima of unadjusted p-values, respectively. For a suitably chosen initial FWER-controlling procedure, the gFWER- and TPPFP-controlling augmentation multiple testing procedures (AMTP) of van der Laan et al. (2004 a) can also take into account the joint distribution of the test statistics. Given a gFWER-controlling procedure, we also propose AMTPs for controlling tail probability error rates, Pr(g(V_n,R_n) 3 ̆e q), for arbitrary functions g(V_n,R_n) of the numbers of false positives V_n and rejected hypotheses R_n. The different gFWER- and TPPFP-controlling procedures are compared in a simulation study, where the tests concern the components of the mean vector of a multivariate Gaussian data generating distribution. Among notable findings are the substantial power gains achieved by joint procedures compared to marginal procedures...|$|E
40|$|Sustainable {{development}} {{is an issue}} that is gaining more and more relevance in all areas of society, though specifically in industry. In order to move towards the goal of sustainability, life cycle thinking is an essential element. For the implementation of life cycle thinking in industry life cycle management (LCM) has been proposed as the general concept. However, the assessment of environmental impacts with the method of life cycle assessment, which is essentially the only tool available for this purpose, has been limited in industrial practice due to involved complexity and the resulting necessary effort and know-how. Therefore, this thesis proposed improved methods that enhance the application efficiency of LCA for industrial uses. After a short introduction in Chapter 1, new developments for the more efficient application of LCA for environmental assessments are presented in Chapter 2 – both for situations where pre-existing data are available and for studies, where no or very limited information of the involved unit processes and elementary flows exist. This concerns the modeling based on reusable elements as well as limiting system boundaries by recommended cut-offs. Specific and easy to apply recommendations for using <b>cut-off</b> <b>rules</b> are proposed. Chapter 3 explores another path, namely the usage of LCA models for life cycle costing. A consistent framework of the economic life cycle based assessment in sustainable {{development is}} proposed and tested. Essentially, one can conclude that the deployment of the systems approach and underlying model of LCA is extremely useful also for conducting economic analyses, while causing very little additional efforts. Chapter 4 elaborates case studies, one for an automotive component, and one for the service of waste water treatment. Detailed LCA results are presented and discussed, and the aforementioned methods in regards to a more efficient LCA "from scratch" and relating to life cycle costing are tested and demonstrated. The thesis concludes with some concise recommendations for future research and development activities in relation to a better usage of LCA and related life cycle approaches...|$|E
40|$|Goal, Scope and Background. The aim of {{the present}} study is to evaluate, through LCA, the {{potential}} environmental impact associated to urban waste dumping in a sanitary landfill for four case studies and to compare different technologies for waste treatment and leachate or biogas management in the framework of the EPD® system. Specific data were collected on the four Italian landfills during a five-year campaign from 2000 to 2004. This work also analyses the comparability of EPD results for different products in the same product category. For this purpose, a critical review of PSR 2003 : 3, for preparing an EPD on 'Collection, transfer and disposal service for urban waste in sanitary landfills', is performed. Methods. PSR 2003 : 3 defines the requirements, based on environmental parameters, that should be considered in an LCA study for collecting and disposal service of Municipal Solid Waste (MSW) in a sanitary landfill. It defines functional unit, system boundaries towards nature, other technical systems and boundaries in time, <b>cut-off</b> <b>rules,</b> allocation rules and parameters to be declared in the EPD. This PSR is tested on four case studies representing the major landfills located from the farthest west to the farthest east side of the Ligurian Region. Those landfills are managed with different technologies as concerns waste pretreatment and leachate or biogas treatment. For each landfill, a life cycle assessment study is performed. Results and Discussion. The comparison of the LCA results is performed separately for the following phases: transport, landfill, leachate and biogas. The following parameters are considered: Resource use (Use of non-renewable resources with and without energy content, Use of renewable resources with and without energy content, Water consumption); Pollutant emissions expressed as potential environmental impact (Global Warming Potential from biological and fossil sources, Acidification, Ozone depletion, Photochemical oxidant formation, Eutrophication, Land use, Hazardous and other Waste production). The comparison of the LCA results obtained for alternative landfill and biogas management techniques in the case studies investigated shows that the best practicable option that benefits the environment as a whole must be identified and chosen in the LCA context. For example, a strong waste pre-treatment causes a high biological GWP in the landfill phase, but a low GWP contribution in the biogas phase, due to the consequent low biogas production, evaluated for 30 years since landfill closure. Conclusion. The analysis of four case studies showed that, through the EPD tool, it is possible to make a comparison among different declarations for the same product category only with some modification and integration to existent PSR 2003 : 3. Results showed that different products have different performances for phases and impact categories. It is not possible to identify the 'best product' from an environmental point of view, but it is possible to identify the product (or service) with the lowest impact on the environment for each impact category and resource use. Recommendation and Perspective. In consequences of the verification of the comprehensiveness of existent PSR 2003 : 3 for the comparability of EPD, some modifications and integration to existent rules are suggested...|$|E
40|$|The paper {{examines}} a delegated monitoring problem between {{investors and}} a bank holding {{a portfolio of}} correlated balloon loans displaying “contagion. ” Moral hazard prevents the bank from monitoring continuously unless it is compensated with the right incentive-compatible contract. Under the Monotone-Likelihood-Ratio Property (MLRP), the asset pool is liquidated when losses exceed a <b>cut-off</b> <b>rule.</b> The bank bears a relatively high share of the risk initially, {{as it should have}} high-powered incentives to monitor, but its long term financial stake tapers off as losses unfold. Securitization can approximate the optimal contract. The sponsor provides the trust with credit enhancement {{in the form of a}} weighted portfolio of collateralized debt obligations which is funded by the proceeds of the sale and extends into the <b>cut-off</b> <b>rule.</b> In compensation the trust pays servicing fees as well as rent-preserving fees if the sponsor has a high discount rate. Rather than being detrimental, well-designed securitization seems an effective and convenient means of implementing the second best...|$|R
50|$|Relative Age Competition - {{competition}} {{designed to}} avoid <b>cut-off</b> date <b>rules,</b> such as ‘average team age’ or ‘bio-banded’ competition.|$|R
40|$|Using {{a lender}} <b>cut-off</b> <b>rule</b> that generates plausibly {{exogenous}} variation in credit supply, I analyze real effects of loan rejections {{in a sample}} of small and medium-sized enterprises. I find that loan rejections reduce asset growth, investments, and employment, and these effects are concentrated among low liquidity firms. Precautionary savings motives aggravate real effects: firms whose loan applications got rejected increase cash holdings and cut non-cash assets in excess of the requested loan amount. These results point to the amplifying effect of precautionary savings motives in the transmission of credit supply shocks...|$|R
40|$|Localization of damages becomes rather {{challenging}} {{when the}} associated stiffness reduction is small in presence of structural uncertainties. This work presents a sensitivity analysis and an improvement {{of a novel}} pseudo-modal approach, recently proposed by the authors. Starting from free vibrations of the undamaged and damaged states, the method aims to maximize the damage signature embedded in the data exploiting the peculiar features of the Orthogonal Empirical Mode Decomposition technique. The role {{of the length of}} the signals and the boundary effects are here investigated; a <b>cut-off</b> <b>rule</b> useful for reducing the latter issue is also proposed...|$|R
40|$|This paper studies a {{practical}} multi-factory job allocation and scheduling problem involving inland and maritime transport limits. A new heuristic called Due-date Based <b>Cut-off</b> <b>rule</b> (DBC) is developed {{to improve the}} computational efficiency of both exact and genetic algorithms (GA). Except the application of DBC, this proposed GA is guided by a novel fuzzy controller aimed at eliminating the drawbacks other GAs have when dealing with multi-factory models. The tests of the solution quality and computational efficiency for this GA are carried out. The numerical experiments demonstrate {{the value of the}} proposed approach in this practical global supply chain. Department of Industrial and Systems Engineerin...|$|R
50|$|Relative Age Environment - {{the system}} of rules {{governing}} inclusion or exclusion from cohorts based on <b>cut-off</b> date eligibility <b>rules.</b>|$|R
40|$|Using {{census data}} on three cohorts of 5 th grade Italian {{students}} we investigate how the ordinal {{rank in the}} within-school age distribution affects the probability of being bullied. Identification is achieved by exploiting within-school between-cohort variation in the age composition of different school cohorts, and through an IV strategy based on the discontinuity in the probability of enrolling in a given school year generated by an end-of-year <b>cut-off</b> <b>rule.</b> We find that being in {{the upper part of}} the school age distribution reduces the probability of being bullied: a one-decile increase in the within-school rank decreases the probability of being victimized by about one percentage point. The effects are stronger for females, children from disadvantaged backgrounds, and children spending the entire day at school; they do not depend on the choice of the reference group, as defined according to socio-demographic characteristics...|$|R
40|$|Copyright © 2015 by SAGE PublicationsThe {{tools of}} {{predictive}} analytics {{are widely used}} {{in the analysis of}} large data sets to predict future patterns in the system. In particular, predictive analytics is used to estimate risk of engaging in certain behaviour. Risk-based audits are used by revenue services to target potentially non-compliant taxpayers, but the results of predictive analytics serve predominantly only as a guide rather than a rule. “Auditor judgment” retains an important role in selecting audit targets. The paper assesses the effectiveness of using predictive analytics in a model of the compliance decision that incorporates several components from behavioral economics: subjective beliefs about audit probabilities, a social custom reward from honest tax payment, and a degree of risk aversion that increases with age. Simulation analysis shows that predictive analytics is successful in raising compliance and that the resulting pattern of audits is very close to being a <b>cut-off</b> <b>rule...</b>|$|R
40|$|We {{characterize}} the optimal job design in a multitasking environment when the firms rely on implicit incentive contracts (i. e., bonus payments). Two natural forms of job design are compared: (i) individual accountability, where each agent {{is assigned to}} a particular job and assumes full responsibility for its outcome; and (ii) team accountability, {{where a group of}} agents share responsibility for a job and are jointly accountable for its outcome. The key trade-off is that team accountability mitigates the multitasking problem but may weaken the implicit contracts. The optimal job design follows a cut-off rule: firms with high reputation concerns opt for team accountability, whereas firms with low reputation concerns opt for individual accountability. Team accountability is more likely the more acute the multitasking problem is. However, the <b>cut-off</b> <b>rule</b> need not hold if the firm combines implicit incentives with explicit pay-per-performance contracts. JEL codes...|$|R
40|$|We {{provide a}} new model of consumption-saving {{decisions}} which explicitly allows for internal commitment mechanisms and self-control. Agents have the ability to invoke either automatic processes that are susceptible to the temptation of ‘overconsuming,’ or alternative control processes which require internal commitment but are immune to such temptations. Standard models in behavioral economics ignore such internal commitment mechanisms. We justify our model by showing that much of its construction is consistent with dynamic choice and cognitive control as they are understood in cognitive neuroscience. The dynamic consumption-saving behavior of an agent in the model is characterized by a simple consumption-saving goal, a propensity to consume out of wealth which is independent of any realized temptation, and a <b>cut-off</b> <b>rule</b> for invoking control processes to inhibit automatic processes and implement the consumptionsaving goal. We compare the behavior induced by our model with that induced by standard behavioral models where agents have no internal commitment ability. We discuss empirical tests of our model with available individual consumption data and we suggest critical tests with brain-imaging and experimental data...|$|R
40|$|The Studi di Settore {{are used}} by the Italian tax {{administration}} to calculate reference revenue levels for small businesses and provide a kind of cut-off level for tax audits. Recently new rules have been introduced in order to render the Studi di Settore more efficient in producing realistic estimates, {{with the aim of}} reducing the "legalized evasion"Â that might arise in case of a systematic downward bias. Voices of the involved categories, however, convinced the Government to partially step back. Building upon the standard firm's tax evasion model of Cowell [Cowell, F. A., 2004. Carrots and sticks in enforcement. In: Aaron, H. J., Slemrod, J. (Eds.), The Crisis in Tax Administration. The Brookings Institution, Washington DC, pp. 230 - 275] and the approach of Santoro [Santoro, A. C., 2006. Evasione delle società di capitali: evidenze empiriche e proposte di policy. In: Brosio, G., Muraro, M. (Eds.), Il Finanziamento del Settore Pubblico. SIEP, Angeli, Milano, pp. 163 - 186] we show that, under given conditions, a stringency increase might backfire implying a larger overall tax evasion and a smaller tax revenue. Tax evasion by firms <b>Cut-off</b> <b>rule...</b>|$|R
25|$|As per the {{derivation}} {{study of}} the LRINEC score, a score of ≥6 is a reasonable <b>cut-off</b> to <b>rule</b> in necrotizing fasciitis, but a LRINEC <6 does not completely rule out the diagnosis. Diagnoses of severe cellulitis or abscess should also be considered due to similar presentations. 10% of patients with necrotizing fasciitis in the original study still had a LRINEC score <6. But a validation study showed that patients with a LRINEC score ≥6 have {{a higher rate of}} both mortality and amputation.|$|R
40|$|A {{prospective}} observational {{cohort study}} was undertaken with two endpoints: (1) {{to compare the}} time cut-off of 48 h and the carrier state criterion for classifying lower airway infections in adult and paediatric long-term ventilated patients, and (2) to evaluate the potential of optimized time cut-offs for characterizing imported and ICU-acquired lower airway infections. All patients admitted to the general and paediatric intensive care units and expected to require mechanical ventilation for a period > or = 3 days were enrolled. Surveillance cultures of throat and rectum were obtained on admission and thereafter twice weekly to distinguish micro-organisms that were imported into the unit from those acquired during the stay on the unit. A total of 130 adults and 400 children were studied. In the adult population, 70 % of lower airway infections were classified as ICU-acquired by the 48 h cut-off and 48 % by the criterion of carriage; on the paediatric ICU the percentages were 65 % and 20 %, respectively. To separate imported from ICU-acquired infections, eight days was optimal in the adult population and 10 days in the paediatric population. Sensitivity, specificity, positive predictive value and negative predictive value for a time cut-off of eight days for adults were 86, 77, 80, 83 %, respectively, and using 10 days for children were 87, 62, 90, 56 %, respectively. The use of the 48 h <b>cut-off</b> <b>rule</b> classifies patients as having nosocomial pneumonia, when in fact the infections are commonly caused by microorganisms carried in by the patients. In contrast, using the carriage method, the proportion of lung infections due to nosocomial bacteria was relatively small and was a late phenomenon. Although in prolonging the time cut-off {{the difference between the}} two types of classification was shorter, time cut-offs were still found to be unreliable for distinguishing imported from unit-acquired lower airway infections...|$|R
40|$|International audienceThe aim of {{this study}} was to {{evaluate}} procalcitonin as an adjunct to diagnose bacterial infections in older patients. One hundred seventy-two patients admitted to an acute-care geriatric unit during a 6 -month period were prospectively included, 39 of them with an invasive bacterial infection. The best <b>cut-off</b> value to <b>rule</b> in a bacterial infection was 0. 51 [*]µg/l with sensitivity 64 % and specificity 94 %. The best <b>cut-off</b> value to <b>rule</b> out a bacterial infection was 0. 08 [*]µg/l with sensitivity 97 % and specificity 20 %. Procalcitonin was inconclusive (between 0. 08 and 0. 51 [*]µg/l) for 112 admissions. Procalcitonin over 0. 51 [*]µg/l was useless 22 times out of 33 (infection already ruled in on clinical grounds) and misleading in eight of the 11 remaining cases (no infection). Procalcitonin below 0. 08 [*]µg/l was useless 23 times out of 27 (infection already ruled out on clinical grounds) and misleading in one of the four remaining cases (infection). Despite a good overall diagnostic accuracy, the clinical usefulness of PCT to diagnose invasive bacterial infections in elderly patients hospitalized in an acute geriatric ward appears to be very limited...|$|R
40|$|Classifiers {{are seen}} here as systems in which input feature values are used with fitted or learned {{functions}} that produce output values which are interpreted as probabilities or fuzzy degrees of class membership, or in which output values are used with <b>cut-off</b> decision <b>rules</b> to choose bivalent class membership. Two complementary measurements for evaluating training, validation, testing, and deployment phase performances in human, mechanical, and computerized classifiers are proposed here. These measurements {{are derived from}} samples of classifier output values paired with their corresponding known probabilistic, fuzzy, or bivalent classification values. The first measurement is the area under the ROC plot. The second is the separation index newly introduced here. Both of these measurements are easy to understand and to compute. It is proposed that they be considered standard metrics for evaluating and comparing classifier intelligence. Keywords: 1...|$|R
40|$|While many {{researchers}} are still unsure {{as to why}} CEOs in the United States on average are compensated the highest on the global scale, this paper attempts {{to bridge the gap}} to this confusion by conducting a thorough analysis through the application of primary and secondary sources. The findings provided in this research are intended to lay the foundations to an underdeveloped total CEO compensation comparison between Canadian CEOs and American CEOs. This dissertation also investigates why there is a premium associated with US CEOs. An econometric model has been constructed to demonstrate some of the determinants influencing total CEO compensation, from which it can be understood that firm size is statistically significant. The longitudinal dataset included a final sample of 50 Canadian firms and 44 US firms, over the years 2009 to 2013 (See Appendix 9 and 10). An OLS baseline estimation was applied to test the model. Although, some of the initial results displayed significant relationships between firm performance, tenure, and board size with regards to total CEO compensation, the model was deemed to be relatively unfit at explaining the overall variation. This experiment was further extended using a <b>cut-off</b> <b>rule</b> of 2 % from the upper and lower limits within each model to omit some of the outliers found in the dataset. Subsequently, OLS was conducted with robust standard errors to eliminate the presence of heteroskedasticity. Firm size still showed a positive and highly significant relationship under both samples. In addition, under both samples, firm performance was expressed by the return on assets and it illustrated a negative association with total CEO compensation. Although the financial crisis of 2007 and the adoption of a new Canadian accounting system were considered to be sound evidence for receiving results, which happened to be counter-intuitive to previous findings, this evidence may have be driven by the presence of an idiosyncratic sample. This work remains to be the entire responsibility of the client. I am pleased for my dissertation to be shared and made available as an example of good practice...|$|R
40|$|Determining {{the optimal}} {{age at which}} a child should enter school is a {{controversial}} topic in education policy. In particular, German policy makers, pedagogues, parents, and teachers have since long discussed whether the traditional, established age of school entry at 6 years remains appropriate. Policies of encouraging early school entry or increased consideration of a particular child 2 ̆ 7 s competency for school (2 ̆ 7 Schulfähigkeit 2 ̆ 7) have been suggested. Using a dataset capturing children who entered school in the late 1960 s through the late 1970 s, a time when delaying enrolment was common, we investigate the effect of age at school entry on educational attainment for West and East Germany. Empirical results from linear probability models and matching suggest a qualitatively negative relation between the age at school entry and educational outcomes {{both in terms of}} schooling degree and probability of having to repeat a grade. These findings are likely driven by unobserved ability differences between early and late entrants. We therefore use a <b>cut-off</b> date <b>rule</b> and the corresponding age at school entry according to the regulation to instrument the actual age at school entry. The IV estimates suggest there is no effect of age at school entry on educational performance...|$|R
40|$|BACKGROUND: Anxiety scales {{may help}} primary care {{physicians}} to detect specific anxiety disorders among the many emotionally distressed patients presenting in primary care. The anxiety scale of the Four-Dimensional Symptom Questionnaire (4 DSQ) consists of an admixture of symptoms of specific anxiety disorders. The research questions were: (1) Is the anxiety scale unidimensional or multidimensional? (2) To what extent does the anxiety scale detect specific DSM-IV anxiety disorders? (3) Which cut-off points are suitable to rule out or to rule in (which) anxiety disorders? METHODS: We analyzed 5 primary care datasets with standardized psychiatric diagnoses and 4 DSQ scores. Unidimensionality was assessed through confirmatory factor analysis (CFA). We examined mean scores and anxiety score distributions per disorder. Receiver operating characteristic (ROC) {{analysis was used to}} determine optimal cut-off points. RESULTS: Total n was 969. CFA supported unidimensionality. The anxiety scale performed slightly better in detecting patients with panic disorder, agoraphobia, social phobia, obsessive compulsive disorder (OCD) and post traumatic stress disorder (PTSD) than patients with generalized anxiety disorder (GAD) and specific phobia. ROC-analysis suggested that >/= 4 was the optimal <b>cut-off</b> point to <b>rule</b> out and >/= 10 the <b>cut-off</b> point to <b>rule</b> in anxiety disorders. CONCLUSIONS: The 4 DSQ anxiety scale measures a common trait of pathological anxiety that is characteristic of anxiety disorders, in particular panic disorder, agoraphobia, social phobia, OCD and PTSD. The anxiety score detects the latter anxiety disorders to a slightly greater extent than GAD and specific phobia, without being able to distinguish between the different anxiety disorder types. The cut-off points >/= 4 and >/= 10 can be used to separate distressed patients in three groups with a relatively low, moderate and high probability of having one or more anxiety disorders...|$|R
40|$|In common thrombofilia is {{the state}} of the vessel system in arterial, venous or microcirculation part when the {{probability}} of thrombs-creation is increased, but thrombs are not present yet. It is the pre-state of thrombotization. Thrombofilia is denoted as hypercoagulation status. Thrombofilia also is a common term for hereditary and acquired prothrombotic states in arterial, venous and microcirculation system. Progress in thrombotic and anticoagulation treatment and high-powered clinical research in pathogenesis together enable the radical progress in modern conception of pathophysiology in clinical diagnostic and introduce usage of very efficient treatments. Results of big multi-central studies show clearly how the precise fibrinolytic and coagulation tests and treatments can help in long-term surviving of patients with thrombosis. The search part of this study shows a summary of last information in pathophysiology and last potentialities in laboratory tests and interpretations focused on clinical exposure and diagnostic of PE and DVT. Inspiration for this study are results published in JOURNAL OF THROMBOSIS AND HAEMOSTASIS 10 (7) : Penaloza, A., Roy, P. -M., Kline, J., Verschuren, F., Le Gal, G., Quentin-Georget, S., Delvau, N., Thys, F. Performance of age-adjusted D-dimer <b>cut-off</b> to <b>rule</b> out pulmonary embolism. 1291 - 1296, 2012. doi: 10. 1111 /j. 1538 - 7836. 2012. 04769. x and HAEMATOLOGICA-THE HEMATOLOGY JOURNAL 97 (10) Douma, Renee A.; Tan, Melanie; Schutgens, Roger E. G.; et al. Using an age-dependent D-dimer cut-off value increases the number of older patients in whom deep vein thrombosis can be safely excluded. 1507 - 1513, 201, doi: 10. 3324 /haematol. 2011. 060657. In this part also the need of Good Laboratory Praxis for pre-analytics extraction and transportation of whole blood samples for coagulation and fibrinolytic tests is highlighted. The practical part of this study covers either the laboratory results aimed to confirm the diagnosis or the results of D-dimer concentration done for preventive reasons and selected in specific files. D dimer is the specific fissile product of fibrin and the presence of D-dimer in plasma testifies the activation of coagulation and fibrinolysis. D-dimer tests were done on the automatic coagulation analyzer ACT Elite Pro. D-dimer test has been done by immunological method based on reaction between antigen and antibody. Analyzer ACL Elite PRO uses nephelometry for detection of blood clogs and reads the intensity of scattered 90 ° angle light in the sample. Results of sonography are from data stored in hospital information system FONS Akord STAPRO Hospital Jindřichův Hradec a. s. All data were collected in several data-files. This study verificates the hypothesis of predicted difference of D-dimers in patients from preventive file and patients in file with thrombofilia diagnosis. The hypothesis of D-dimer test positivity in venous thrombosis was confirmed as well, D-dimer as a marker of thrombofilia shows an actual activation in-vivo system and increased D-dimer concentration confirmed venous thrombosis DVT together with a positive sonography. The most interesting and the most important result of this study is the confirmation of the hypothesis of using an age-dependent D-dimer cut-off in patients over 50 of age with suspect DVT or/and PE diagnosis. The importance of using an age-dependent D-dimer cut-off was published in articles in JOURNAL OF THROMBOSIS AND HAEMOSTASIS 10 (7) : Penaloza, A., Roy, P. -M., Kline, J., Verschuren, F., Le Gal, G., Quentin-Georget, S., Delvau, N., Thys, F. Performance of age-adjusted D-dimer <b>cut-off</b> to <b>rule</b> out pulmonary embolism. 1291 - 1296, 2012. doi: 10. 1111 /j. 1538 - 7836. 2012. 04769. x and HAEMATOLOGICA-THE HEMATOLOGY JOURNAL 97 (10) Douma, Renee A.; Tan, Melanie; Schutgens, Roger E. G.; et al. Using an age-dependent D-dimer cut-off value increases the number of older patients in whom deep vein thrombosis can be safely excluded...|$|R
40|$|Background Antibiotic {{resistance}} poses {{a significant}} threat to patients suffering from infectious diseases. Early readings of antibiotic susceptibility test (AST) {{results could be}} of critical importance to ensure adequate treatment. Disc diffusion is a well-standardized, established and cost-efficient AST procedure; however, its use in the clinical laboratory is hampered by the many manual steps involved, and an incubation time of 16 - 18 [*]h, which is required to achieve reliable test results. Methods We have evaluated a fully automated system for its potential for early reading of disc diffusion diameters after 6 - 12 [*]h of incubation. We assessed availability of results, methodological precision, categorical agreement and interpretation errors as compared with an 18 [*]h standard. In total, 1028 clinical strains (291 Escherichia coli, 272 Klebsiella pneumoniae, 176 Staphylococcus aureus and 289 Staphylococcus epidermidis) were included in this study. Disc diffusion plates were streaked, incubated and imaged using the WASPLab TM automation system. Results and conclusions Our results demonstrate that: (i) early AST reading is possible for important pathogens; (ii) methodological precision is not hampered at early timepoints; and (iii) species-specific reading times must be selected. As inhibition zone diameters change over time and are phenotype/drug combination dependent, specific <b>cut-offs</b> and expert <b>rules</b> will be essential to ensure reliable interpretation and reporting of early susceptibility testing results...|$|R
40|$|The {{recently}} introduced simplified Wells rule for {{the exclusion of}} pulmonary embolism (PE) assigns only one point to the seven variables of the original Wells rule. This study was performed to independently validate the simplified Wells rule for the exclusion of PE. We retrospectively calculated the prevalence of PE in the "unlikely" probability categories of the original Wells (cut-off <= 4) and the simplified Wells <b>rule</b> (<b>cut-off</b> <= 1) in 922 consecutive patients with clinically suspected PE from a multicenter cohort study. We compared the three-month incidence of venous thromboembolism (VTE) in patients with an unlikely probability and a normal D-dimer test using both scores, {{and the proportion of}} patients with this combination (clinical utility). The proportion of patients categorized as PE "unlikely" was similar using the original (78 %) and the simplified (70 %) Wells rule. The prevalence of PE was 13 % (95 % confidence interval [CI], 11 - 16 %) and 12 % (95 %CI, 9. 7 - 15 %) for the original Wells and simplified Wells "unlikely" categories, respectively. None of the patients with PE "unlikely" and a normal D-dimer test experienced VTE during three-month follow-up. The proportions of patients in whom further tests could safely be withheld based on PE "unlikely" and a normal D-dimer test was 28 % (95 %CI, 25 - 31 %) using the original and 26 % (95 %CI, 24 - 29 %) using the simplified Wells rule. In this external retrospective validation study, the simplified Wells rule appeared to be safe and clinically useful, although prospective validation remains necessary. Simplification of the Wells rule may enhance the applicabilit...|$|R
40|$|International audienceThe {{recently}} introduced simplified Wells rule for {{the exclusion of}} pulmonary embolism (PE) assigns only one point to the seven variables of the original Wells rule. This study was performed to independently validate the simplified Wells rule for the exclusion of PE. We retrospectively calculated the prevalence of PE in the "unlikely" probability categories of the original Wells (cut-off < or = 4) and the simplified Wells <b>rule</b> (<b>cut-off</b> < or = 1) in 922 consecutive patients with clinically suspected PE from a multicenter cohort study. We compared the three-month incidence of venous thromboembolism (VTE) in patients with an unlikely probability and a normal D-dimer test using both scores, {{and the proportion of}} patients with this combination (clinical utility). The proportion of patients categorized as PE "unlikely" was similar using the original (78 %) and the simplified (70 %) Wells rule. The prevalence of PE was 13 % (95 % confidence interval [CI], 11 - 16 %) and 12 % (95 %CI, 9. 7 - 15 %) for the original Wells and simplified Wells "unlikely" categories, respectively. None of the patients with PE "unlikely" and a normal D-dimer test experienced VTE during three-month follow-up. The proportions of patients in whom further tests could safely be withheld based on PE "unlikely" and a normal D-dimer test was 28 % (95 %CI, 25 - 31 %) using the original and 26 % (95 %CI, 24 - 29 %) using the simplified Wells rule. In this external retrospective validation study, the simplified Wells rule appeared to be safe and clinically useful, although prospective validation remains necessary. Simplification of the Wells rule may enhance the applicability...|$|R
40|$|Background: A {{substantial}} {{proportion of}} patients with suspected pulmonary embolism (PE) have active malignancy. Although a clinical decision rule (CDR) combined with D-dimer testing is safe to rule out PE in cancer patients, this combination is less applicable in cancer patients due to a lower specificity. Therefore, we analysed whether elevating the D-dimer cut-off increases the clinical utility in cancer patients. Methods: Consecutive cancer patients with suspected PE from a large management study were included. The proportion {{of patients with}} an unlikely clinical probability according to the Wells (cut-off ≤ 4) or Simplified Wells <b>rule</b> (<b>cut-off</b> < 1) were assessed and combined with different D-dimer cut-off levels. Safety was determined as a PE failure rate below 2. 5 % {{after three months of}} follow up. Results: Of a total of 3306 with suspected PE, 474 (14 %) were cancer patients. Combined with the traditional Wells rule, the D-dimer cutoff level could safely be increased to 700 μg/L. At this level, the proportion of patients in whom PE could be ruled out increased from 48 (10 %,) to 67 (14 %), whereas the failure rate was 2. 1 % (95 % confidence interval [CI], 0. 0 - 11 %) with the new and 1. 4 % (95 %CI, 08 %) with the traditional 500 μg/L cut-off, respectively. Combined with the Simplified Wells <b>rule,</b> the D-dimer <b>cut-off</b> could be raised to 1100 μg/L, increasing the proportion of cancer patients in whom PE was ruled out from 25 (5 %) to 77 (16 %), with a failure rates of 0. 0 % (95 %CI 0 - 13 %) and 0. 0 % (95 %CI 0 - 6. 2 %), respectively. Conclusion: Increasing the D-dimer cut-off to exclude PE in cancer patients with an unlikely clinical probability for PE results in only a modest increase in clinical utility. This implies that additional diagnostic methods will remain necessary in the large majority cancer patients with suspected PE, irrespective of the D-dimer cut-off value...|$|R

