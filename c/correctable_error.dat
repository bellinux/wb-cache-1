28|69|Public
25|$|Phthalocyanine dye CD-Rs {{are usually}} silver, gold or light green. The patents on {{phthalocyanine}} CD-Rs {{are held by}} Mitsui and Ciba Specialty Chemicals. Phthalocyanine is a natively stable dye (has no need for stabilizers) and CD-Rs based on this are often given a rated lifetime of hundreds of years. Unlike cyanine, phthalocyanine is less resistant to UV rays and CD-Rs based on this dye show signs of degradation only {{after two weeks of}} direct sunlight exposure. However, phthalocyanine is more sensitive than cyanine to writing laser power calibration, meaning that the power level used by the writing laser has to be more accurately adjusted for the disc {{in order to get a}} good recording; this may erode the benefits of dye stability, as marginally written discs (with higher <b>correctable</b> <b>error</b> rates) will lose data (i.e. have uncorrectable errors) after less dye degradation than well written discs (with lower <b>correctable</b> <b>error</b> rates).|$|E
2500|$|If we {{can show}} that all bursts of length [...] or less occur in {{different}} cosets, we can use them as coset leaders that form <b>correctable</b> <b>error</b> patterns. The reason is simple: we know that each coset has a unique syndrome decoding associated with it, and if all bursts of different lengths occur in different cosets, then all have unique syndromes, facilitating error correction.|$|E
5000|$|The species epithet was {{originally}} published erroneously as [...] "philliraeoides", but this a <b>correctable</b> <b>error,</b> because {{it refers to}} the genus Phillyrea.|$|E
50|$|A {{non-degenerate}} code is one {{for which}} different {{elements of the}} set of <b>correctable</b> <b>errors</b> produce linearly independent results when applied to elements of the code. If distinct of the set of <b>correctable</b> <b>errors</b> produce orthogonal results, the code is considered pure.|$|R
5000|$|Many early {{implementations}} of ECC memory mask <b>correctable</b> <b>errors,</b> acting [...] "as if" [...] {{the error}} never occurred, and only report uncorrectable errors. Modern implementations log both <b>correctable</b> <b>errors</b> (CE) and uncorrectable errors (UE). Some people proactively replace memory modules that exhibit high error rates, {{in order to}} reduce the likelihood of uncorrectable error events.|$|R
5000|$|Predictive failure {{analysis}} to predict which intermittent <b>correctable</b> <b>errors</b> will lead eventually to hard non-correctable errors.|$|R
5000|$|A 2009 paper {{using data}} from Google's datacentres {{provided}} evidence demonstrating that in observed Google systems, DRAM errors were recurrent at the same location, and that 8% of DIMMs were affected each year. Specifically, [...] "In more than 85% of the cases a <b>correctable</b> <b>error</b> is followed {{by at least one}} more <b>correctable</b> <b>error</b> in the same month". DIMMs with chipkill error correction showed a lower fraction of DIMMs reporting uncorrectable errors compared to DIMMs with error correcting codes that can only correct single-bit errors. A 2010 paper from University of Rochester also showed that Chipkill memory gave substantially lower memory errors, using both real world memory traces and simulations.|$|E
50|$|It {{is a small}} fern, with erect fertile fronds, and sterile fronds {{shaped like}} small oak leaves. The name is often erroneously spelled 'zeylanica', but it was {{originally}} published as 'zeilanica', {{and this is not}} a <b>correctable</b> <b>error</b> under the International Code of Nomenclature for algae, fungi, and plants.|$|E
5000|$|Phthalocyanine dye CD-Rs {{are usually}} silver, gold or light green. The patents on {{phthalocyanine}} CD-Rs {{are held by}} Mitsui and Ciba Specialty Chemicals. Phthalocyanine is a natively stable dye (has no need for stabilizers) and CD-Rs based on this are often given a rated lifetime of hundreds of years. Unlike cyanine, phthalocyanine is less resistant to UV rays and CD-Rs based on this dye show signs of degradation only {{after two weeks of}} direct sunlight exposure. However, phthalocyanine is more sensitive than cyanine to writing laser power calibration, meaning that the power level used by the writing laser has to be more accurately adjusted for the disc {{in order to get a}} good recording; this may erode the benefits of dye stability, as marginally written discs (with higher <b>correctable</b> <b>error</b> rates) will lose data (i.e. have uncorrectable errors) after less dye degradation than well written discs (with lower <b>correctable</b> <b>error</b> rates).|$|E
5000|$|Define {{hypothetical}} multidimensional Reed-Solomon error correcting architecture {{to act as}} {{a filter}} and view all other analyses after <b>correctable</b> <b>errors</b> are removed from the error location stream.|$|R
50|$|The {{channel and}} storage control under certain {{conditions}} can inter-operate to cause a CCW to be retried without an I/O interruption.This procedure is initiated by the storage control and used to recover from <b>correctable</b> <b>errors.</b>|$|R
50|$|The {{family name}} Glomeraceae upon which this order level name is based, was {{incorrectly}} spelled 'Glomaceae', hence the order name was incorrectly spelled 'Glomales'. Both are <b>correctable</b> <b>errors,</b> to Glomeraceae and Glomerales, as {{governed by the}} International Code of Botanical Nomenclature. The incorrect spellings are commonplace in the literature, unfortunately.|$|R
5000|$|If we {{can show}} that all bursts of length [...] or less occur in {{different}} cosets, we can use them as coset leaders that form <b>correctable</b> <b>error</b> patterns. The reason is simple: we know that each coset has a unique syndrome decoding associated with it, and if all bursts of different lengths occur in different cosets, then all have unique syndromes, facilitating error correction.|$|E
50|$|Due to {{the high}} {{integration}} density of modern computer memory chips, the individual memory cell structures became small enough to be vulnerable to cosmic rays and/or alpha particle emission. The errors caused by these phenomena are called soft errors. Over 8% of DIMM modules experience at least one <b>correctable</b> <b>error</b> per year. This {{can be a problem}} for DRAM and SRAM based memories. The probability of a soft error at any individual memory bit is very small. However, together with the large amount of memory modern computersespecially serversare equipped with, and together with extended periods of uptime, the probability of soft errors in the total memory installed is significant.|$|E
50|$|Recent studies give widely varying {{error rates}} with over seven orders of {{magnitude}} difference, ranging from 10&minus;10−10−17 error/bit·h, roughly one bit error, per hour, per gigabyte of memory to one bit error, per century, per gigabyte of memory. The Schroeder et al. 2009 study reported a 32% chance that a given computer in their study would suffer from at least one <b>correctable</b> <b>error</b> per year, and provided evidence that most such errors are intermittent hard rather than soft errors. A 2010 study at the University of Rochester also gave evidence that a substantial fraction of memory errors are intermittent hard errors. Large scale studies on non-ECC main memory in PCs and laptops suggest that undetected memory errors account for {{a substantial number of}} system failures: the study reported a 1-in-1700 chance per 1.5% of memory tested (extrapolating to an approximately 26% chance for total memory) that a computer would have a memory error every eight months.|$|E
50|$|Data {{scrubbing}} is {{an error}} correction technique {{that uses a}} background task to periodically inspect main memory or storage for errors, then correct detected errors using redundant data {{in the form of}} different checksums or copies of data. Data scrubbing reduces the likelihood that single <b>correctable</b> <b>errors</b> will accumulate, leading to reduced risks of uncorrectable errors.|$|R
40|$|It is {{important}} to study {{the behavior of a}} t-error correcting quantum code when the number of errors is greater than t, because it is likely that there are also small errors besides t large <b>correctable</b> <b>errors.</b> We estimate the fidelity of a t-error correcting stabilizer code over a general memoryless channel, allowing more than t errors...|$|R
40|$|We {{propose a}} {{decoding}} method for the generalized algebraic geometry codes proposed by Xing et al. To show its practical usefulness, we {{give an example}} of generalized algebraic geometry codes of length 567 over F_ 8 whose numbers of <b>correctable</b> <b>errors</b> by the proposed method are larger than the shortened codes of the primitive BCH codes of length 4095 in the most range of dimension. Comment: LaTeX 2 e, 8 pages, 1 figur...|$|R
40|$|Abstract—Today’s HPC systems use two {{mechanisms}} to ad-dress main-memory errors. Error-correcting codes make cor-rectable errors transparent to software, while checkpoint/restart (CR) enables recovery from uncorrectable errors. Unfortunately, CR overhead will be enormous at exascale {{due to the}} high failure rate of memory. We propose a new OS-based approach that proactively avoids memory errors using prediction. This scheme exposes <b>correctable</b> <b>error</b> information to the OS, which migrates pages and offlines unhealthy memory to avoid application crashes. We analyze memory error patterns in extensive logs from a BG/P system and show how <b>correctable</b> <b>error</b> patterns {{can be used to}} identify memory likely to fail. We implement a proactive memory management system on BG/Q by extending the firmware and Linux. We evaluate our approach with a realistic workload and compare our overhead against CR. We show improved resilience with negligible performance overhead for applications...|$|E
40|$|AbstractThe Newton {{radius of}} a code {{is the largest}} weight of a uniquely <b>correctable</b> <b>error.</b> We {{establish}} a lower bound for the Newton radius {{in terms of the}} rate. In particular we show that in any family of linear codes of rate below one half, the Newton radius increases linearly with the codeword length...|$|E
40|$|In quantum coding theory, {{stabilizer}} {{codes are}} probably the most important class of quantum codes. They are regarded as the quantum analogue of the classical linear codes and the properties of stabilizer codes have been carefully studied in the literature. In this paper, a new but simple construction of stabilizer codes is proposed based on syndrome assignment by classical parity-check matrices. This method reduces the construction of quantum stabilizer codes to the construction of classical parity-check matrices that satisfy a specific commutative condition. The quantum stabilizer codes from this construction have a larger set of <b>correctable</b> <b>error</b> operators than expected. Its (asymptotic) coding efficiency is comparable to that of CSS codes. A class of quantum Reed-Muller codes is constructed, which have a larger set of <b>correctable</b> <b>error</b> operators than that of the quantum Reed-Muller codes developed previously in the literature. Quantum stabilizer codes inspired by classical quadratic residue codes are also constructed and some of which are optimal in terms of their coding parameters. Comment: 34 pages, 3 figures, 5 tables, index terms add, abstract and conclusion slightly modifie...|$|E
40|$|Two {{observations}} are given on the fidelity of schemes for quantum information processing. In the first one, {{we show that}} the fidelity of a symplectic (stabilizer) code, if properly defined, exactly equals the `probability' of the <b>correctable</b> <b>errors</b> for general quantum channels. The second observation states that for any coding rate below the quantum capacity, exponential convergence of the fidelity of some codes to unity is possible. Comment: 20 pages. Ver. 2 : typographycal errors correcte...|$|R
40|$|Abstract. Hamming code-based LDPC (H-LDPC) block {{codes are}} {{obtained}} by replacing the single parity-check constituent codes in Gallager’s LDPC codes with Hamming codes. This paper investigates the asymptotic performance of ensembles of random H-LDPC codes, used over the binary symmetric channel and decoded with a low-complexity hard-decision iterative decoding algorithm. It is shown that there exist H-LDPC codes for which such iterative decoding corrects any error pattern {{with a number}} of errors that grows linearly with the code length. The number of required decoding iterations is a logarithmic function of the code length. The fraction of <b>correctable</b> <b>errors</b> is computed numerically for different code parameters. ...|$|R
5000|$|In 2010 Rosenberg founded MediaBugs.org, a [...] "service for {{reporting}} specific, <b>correctable</b> <b>errors</b> and problems in media coverage." [...] In an interview, he explains, [...] "We'll try {{to alert the}} journalists or news organization involved about your report and bring them into a conversation," [...] which may get the error corrected. It is funded by the John S. and James L. Knight Foundation {{as part of their}} News Challenge. In September 2012, {{at the end of the}} funding period, he explained in a blog post that 'Much of the public sees media-outlet accuracy failures as [...] "not our problem." [...] The journalists are messing up, they believe, and it's the journalists' job to fix things.' ...|$|R
40|$|AbstractThe Newton {{radius of}} a code {{is the largest}} weight of a uniquely <b>correctable</b> <b>error.</b> The {{covering}} radius is the largest distance between a vector and the code. In this paper, we use the modular representation of a linear code to give an efficient algorithm for computing coset leaders of relatively high Hamming weight. The weights of these coset leaders serve as lower bounds on the Newton radius and the covering radius for linear codes...|$|E
40|$|We {{study the}} problem of list {{decoding}} with focus on the case {{when we have a}} list size limited to two. Under this restriction we derive general lower bounds on the maximum possible size of a list-of- 2 -decodable code. We study the set of <b>correctable</b> <b>error</b> patterns in an attempt to obtain a characterization. For a special family of Reed-Solomon codes - which we identify and name 'class-I codes' - we give a weight-based characterization of the <b>correctable</b> <b>error</b> patterns under list-of- 2 decoding. As a tool in this analysis we use the theoretical framework of Sudan's algorithm. The characterization is used in an exact calculation of the probability of transmission error in the symmetric channel when list-of- 2 decoding is used. The results from the analysis and complementary simulations for QAM-systems show that a list-of- 2 decoding gain of nearly 1 dB can be achieved. Further we study Sudan's algorithm for list decoding of Reed-Solomon codes for the special case of the class-I codes. For these codes algorithms are suggested for both the first and second step of Sudan's algorithm. Hardware solutions for both steps based on the derived algorithms are presented...|$|E
40|$|Residue (QR) {{codes are}} presented. The key ideas behind this {{decoding}} technique {{are based on}} one-to-one mapping between the syndromes “S 1 ” and <b>correctable</b> <b>error</b> patterns. Such algorithms determine the error locations directly by lookup tables without the operations of multiplication over a finite field. Moreover, the method of utilizing shiftsearch algorithm, to dramatically reduce the memory requirement is given for decoding QR codes. The algorithm has been verified through a software simulation that program in C-language. The new approach is modular, regular and naturally suitable for DSP software implementation...|$|E
40|$|In this paper, we {{consider}} robust system identification under sparse outliers and random noises. In our problem, system parameters are observed through a Toeplitz matrix. All observations {{are subject to}} random noises and a few are corrupted with outliers. We reduce this problem of system identification to a sparse error correcting problem using a Toeplitz structured real-numbered coding matrix. We prove the performance guarantee of Toeplitz structured matrix in sparse error correction. Thresholds on the percentage of <b>correctable</b> <b>errors</b> for Toeplitz structured matrices are also established. When both outliers and observation noise are present, we {{have shown that the}} estimation error goes to 0 asymptotically as long as the probability density function for observation noise is not "vanishing" around 0. Comment: conferenc...|$|R
40|$|This paper proposes an {{associative}} memory neural network whose limiting {{state is the}} nearest point in a polyhedron from a given input. Two implementations of the proposed {{associative memory}} network are presented based on Dykstra's algorithm and a fixed point theorem for nonexpansive mappings. By these implementations, the set of all <b>correctable</b> <b>errors</b> by the network is characterized as a dual cone of the polyhedron at each pattern to be memorized, {{which leads to a}} simple amplifying technique to improve the error correction capability. It is shown by numerical examples that the proposed associative memory realizes much better error correction performance than the conventional one based on POCS {{at the expense of the}} increase of necessary number of iterations in the recalling stage...|$|R
40|$|Abstract—We {{consider}} a concatenated coding scheme using a single inner code, a single outer code, and a fixed single-trial decoding strategy that maximizes {{the number of}} errors guaranteed to be corrected in a con-catenated codeword. For this scheme, we investigate whether maximizing the guaranteed error correction rate, i. e., the number of <b>correctable</b> <b>errors</b> per transmitted symbol, necessitates pushing the code rate to zero. We show {{that this is not}} always the case for a given inner or outer code. Furthermore, to maximize the guaranteed error correction rate over all inner and outer codes of fixed dimensions and alphabets, the code rate of one (but not both) of these two codes should be pushed to zero. Index Terms—Concatenated codes, decoding strategy, erasure correc-tion, error correction, error detection. I...|$|R
40|$|Magnetic {{order in}} graphene-related {{structures}} can arise from size effects or from topological frustration. We introduce a rigorous classification scheme for {{the types of}} finite graphene structures (nano-flakes) which lead to large net spin or to antiferromagnetic coupling between groups of electron spins. Based on this scheme, we propose specific examples of structures that can serve as the fundamental (NOR and NAND) logic gates {{for the design of}} high-density ultra-fast spintronic devices. We demonstrate, using ab initio electronic structure calculations, that these gates can in principle operate at room temperature with very low and <b>correctable</b> <b>error</b> rates. Comment: Typo in title fixe...|$|E
40|$|A {{simplified}} algorithm for decoding binary quadratic residue (QR) codes is devel-oped in this paper. The {{key idea}} {{is to use the}} efficient Euclidean algorithm to determine the greatest common divisor of two specific polynomials which can be shown to be the error-locator polynomial. This proposed technique differs from the previous schemes de-veloped for QR codes. It is especially simple due to the well-developed Euclidean algo-rithm. In this paper, an example using the proposed algorithm to decode the (41, 21, 9) quadratic residue code is given and a C++ program of the proposed algorithm has been executed successfully to run all <b>correctable</b> <b>error</b> patterns. The simulations of this new algorithm compared with the Berlekamp-Massey (BM) algorithm for the (71, 36, 11) and (79, 40, 15) quadratic residue codes are shown...|$|E
40|$|AbstractIt was conjectured by Kronk and Mitchem in 1973 {{that simple}} plane graphs of maximum degree Δ are entirely (Δ+ 4) -colourable, i. e., the vertices, edges, and faces {{of a simple}} plane graph may be {{simultaneously}} coloured with Δ+ 4 colours {{in such a way}} that adjacent or incident elements are coloured by distinct colours. Before this paper, the conjecture has been confirmed for Δ⩽ 3 and Δ⩾ 6 (the proof for the Δ= 6 case has a <b>correctable</b> <b>error).</b> In this paper, we settle the whole conjecture in the positive. We prove that if G is a plane graph with maximum degree 4 (parallel edges allowed), then G is entirely 8 -colourable. If G is a plane graph with maximum degree 5 (parallel edges allowed), then G is entirely 9 -colourable...|$|E
40|$|It is {{important}} to study {{the behavior of a}} t-error correcting quantum code when the number of errors is greater than t, because it is likely that there are also small errors besides t large <b>correctable</b> <b>errors.</b> We give a lower bound for the fidelity of a t-error correcting stabilizer code over a general memoryless channel, allowing more than t errors. We also show that the fidelity can be made arbitrary close to 1 by increasing the code length. Comment: 9 pages, ReVTeX 4 beta 5. To be published in Phys. Rev. A. The lower bound is made tighter in version 4 and 5. All approximations in the first version are removed, and a lower bound for the average of the fidelity is given in the second version. A critical error is correcte...|$|R
40|$|Abstract—A media {{access control}} {{protocol}} for optical code-division multiple-access packet networks with variable length data traffic is proposed. This protocol exhibits a sliding window with variable size. A model for interference-level fluctuation and an accurate analysis for channel usage are presented. Both multiple-access interference (MAI) and photodetector’s shot noise are con-sidered. Both chip-level and correlation receivers are adopted. The system performance is evaluated using a traditional average sys-tem throughput and average delay. Finally, {{in order to enhance}} the overall performance, error control codes (ECCs) are applied. The results indicate that the performance can be enhanced to reach its peak using the ECC with an optimum number of <b>correctable</b> <b>errors.</b> Furthermore, chip-level receivers are shown to give much higher performance than that of correlation receivers. Also, {{it has been shown that}} MAI is the main source of signal degradation. Index Terms—Chip-level receiver, correlation receiver, MAC protocols, multiple-access interference (MAI), optical CDMA...|$|R
40|$|If one {{accepts the}} {{standard}} view that error correction in second language classes should be selective rather than comprehensive, the question naturally arises {{of how the}} selection process should be carried out. The paper explores this question by asking which types of errors {{are most likely to}} be eliminated or reduced by means of correction; in other words, which are most correctable. The focus is on (a) practical problems affecting the success of correction and (b) existing empirical work on the effectiveness of correction. The major conclusion is that the most <b>correctable</b> <b>errors</b> are those that involve simple problems in relatively discrete items. Least correctable are those stemming from problems in a complex system, particularly the syntactic system. Grammar errors in general are not good targets, though certain types can be identified that are more promising than others. These conclusions are developed in detail in the paper, with numerous examples...|$|R
