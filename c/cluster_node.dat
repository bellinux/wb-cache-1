135|2829|Public
50|$|Three {{configuration}} {{options were}} available: a single-processor model at US$2999, a dual-processor model at $3999, and a dual-processor <b>cluster</b> <b>node</b> model (with an unchanged appearance from the G4 <b>cluster</b> <b>node)</b> at US$2999.|$|E
50|$|The terms logical host or cluster logical host {{is used to}} {{describe}} the network address that is used to access services provided by the cluster. This logical host identity is not tied to a single <b>cluster</b> <b>node.</b> It is actually a network address/hostname that is linked with the service(s) provided by the cluster. If a <b>cluster</b> <b>node</b> with a running database goes down, the database will be restarted on another <b>cluster</b> <b>node.</b>|$|E
50|$|HAST {{provides}} a block device to be synchronized between two servers {{for use as}} a filesystem. The two machines comprise a cluster, where each machine is a <b>cluster</b> <b>node.</b> HAST uses a Primary-Secondary (or Master-Slave) configuration, so only one <b>cluster</b> <b>node</b> is active at a time.|$|E
50|$|In {{computer}} clusters, heartbeat {{network is}} a private network which is shared only by the <b>cluster</b> <b>nodes,</b> and is not accessible from outside the cluster. It is used by <b>cluster</b> <b>nodes</b> in order to monitor each node's status and communicate with each other.|$|R
5000|$|Cluster {{monitors}} (...) {{that keep}} track of active and failed <b>cluster</b> <b>nodes</b> ...|$|R
5000|$|RIP (Real IP address): the IP address used {{to connect}} to the <b>cluster</b> <b>nodes</b> ...|$|R
50|$|GPU {{driver for}} the {{each type of}} GPU present in each <b>cluster</b> <b>node.</b>|$|E
5000|$|The system {{identifies}} the <b>cluster</b> <b>node</b> {{with the lowest}} number of virtual machines.|$|E
5000|$|Cluster - Represents {{the cluster}} itself {{and it is}} the parent object of the <b>cluster</b> <b>node</b> objects.|$|E
30|$|A cache-enabled {{hybrid network}} is formed by {{cellular}} network and autonomous nodes in military network [51]. In the hybrid network, the back-end server cache all the needed content, the in-MANET <b>cluster</b> <b>nodes</b> {{are equipped with}} caches and decide which content to cache. On the one hand, back-end server can provide the requested contents via the cellular network; on the other hand, <b>cluster</b> <b>nodes</b> may offer the contents.|$|R
50|$|A {{clustered}} NAS is a NAS that {{is using}} a distributed file system running simultaneously on multiple servers. The key difference between a clustered and traditional NAS {{is the ability to}} distribute (e.g. stripe) data and metadata across the <b>cluster</b> <b>nodes</b> or storage devices. Clustered NAS, like a traditional one, still provides unified access to the files from any of the <b>cluster</b> <b>nodes,</b> unrelated to the actual location of the data.|$|R
50|$|Clusterpoint Console is used {{to manage}} {{underlying}} hardware (<b>cluster</b> <b>nodes)</b> to share computing resources among different databases in parallel.|$|R
5000|$|Multi-master cluster {{software}} architecture: {{no single}} point of failure, any <b>cluster</b> <b>node</b> {{can serve as}} a master and run the management application ...|$|E
5000|$|... eXtremeDB Cluster is the {{clustering}} sub-system for McObject's eXtremeDB {{embedded database}} product family. It maintains database consistency across multiple hardware nodes by replicating transactions in a synchronous manner (two-phase commit). An important characteristic of eXtremeDB Cluster is transaction replication, {{in contrast to}} log file-based, SQL statement-based, or other replication schemes {{that may or may}} not guarantee the success or failure of entire transactions. Accordingly, eXtremeDB Cluster is an ACID compliant system (not BASE or eventual consistency); a query executed on any <b>cluster</b> <b>node</b> will return the same result as if executed on any other <b>cluster</b> <b>node.</b>|$|E
50|$|Gmond is a multi-threaded daemon {{which runs}} on each <b>cluster</b> <b>node</b> {{you want to}} monitor. Installation does not require having a common NFS {{filesystem}} or a database back-end, installing special accounts or maintaining configuration files.|$|E
5000|$|Shared-data databases-an {{architecture}} that assumes all database <b>cluster</b> <b>nodes</b> share a single partition. Inter-node communications {{is used to}} synchronize update activities performed by different <b>nodes</b> on the <b>cluster.</b> Shared-data data management systems are limited to single-digit <b>node</b> <b>clusters.</b>|$|R
50|$|Two {{widely used}} {{approaches}} for communication between <b>cluster</b> <b>nodes</b> are MPI, the Message Passing Interface and PVM, the Parallel Virtual Machine.|$|R
30|$|Generally, a {{clustered}} ad hoc network {{consists of}} three kinds of nodes: <b>cluster</b> head <b>nodes,</b> gateway nodes, and ordinary <b>nodes.</b> <b>Cluster</b> head <b>nodes</b> are vested with the responsibility of managing the cluster. For example, {{in the case of}} a cluster-based control channel design for a CRAHN, the <b>cluster</b> head <b>node</b> is responsible for allocating a set of control channels out of the set of channels available to every member of that cluster. The coordination between two adjacent clusters is conducted through the gateway nodes. A <b>node</b> of a <b>cluster</b> can act as a gateway only if it has a neighboring node as well as a channel in common with the designated control channel of its neighboring <b>cluster.</b> All <b>nodes</b> other than the <b>cluster</b> head <b>node</b> and gateway node are ordinary nodes. Both gateway nodes and ordinary nodes are also known as cluster members and are managed by the <b>cluster</b> head <b>node.</b>|$|R
50|$|HAST-provided devices appear like disk {{devices in}} the /dev/hast/ {{directory}} in FreeBSD, {{and can be}} used like standard block devices. HAST is similar to a RAID1 (mirror) where each RAID component is provided across the network by one <b>cluster</b> <b>node.</b>|$|E
5000|$|Management node (ndb_mgmd process): Used for {{configuration}} {{and monitoring}} of the cluster. They are required only to start or restart a <b>cluster</b> <b>node.</b> They can also be configured as arbitrators, {{but this is not}} mandatory (MySQL Servers can be configured as arbitrators instead).|$|E
50|$|Kerrighed {{provides}} several {{features such}} as a distributed shared memory with a sequential consistency model, processes migration from one <b>cluster</b> <b>node</b> to another, and to a limited extent checkpointing. Kerrighed introduces a container concept: this entity is an abstraction of both files and memory.|$|E
40|$|AbstractIn this article, {{we present}} a routing {{protocol}} through intra-clustering. Most hierarchical protocols use direct intra-cluster routing, {{so that all the}} <b>cluster</b> member <b>nodes</b> forward their data to the <b>cluster</b> head <b>node</b> directly. Considered parameters are cluster lifetime and end to end delay between <b>cluster</b> member <b>nodes</b> and <b>cluster</b> head <b>node.</b> Also, rules related to queue theory have been used to determine end to end delay. At last, simulation results show the efficiency of the proposed protocol...|$|R
40|$|Abstract: As {{wireless}} sensor networks {{continue to}} grow, so does the need for effective security mechanisms. But {{one of the main}} challenges for efficient key distribution in {{wireless sensor networks}} is the resource limitation. Some nodes {{play a central role in}} key distribution. For example, cluster heads in the LEAP security protocol have the main responsibility for key distribution to their <b>cluster</b> <b>nodes.</b> Therefore, they use more resources than other <b>nodes</b> in their <b>cluster.</b> In case of a cluster head failure, e. g. battery wear out, all <b>nodes</b> within the <b>cluster</b> lose their connection to the base station and considered dead by the base station although the <b>cluster</b> <b>nodes</b> are alive. The unavailability of the base station has a similar implication too, but in a wider extent covering all live cluster heads and <b>cluster</b> <b>nodes</b> in the network leading to the failure of the whole network. This paper presents a new energy aware key distribution solution to enhance the availability of a secure wireless sensor network environment by increasing the lifetime of all constituents of the network including the base station, cluster heads and <b>cluster</b> <b>nodes...</b>|$|R
50|$|Cluster virtual IP address:NSC {{provided}} a single IP address {{for access to}} the cluster from other systems. Incoming connections were load-balanced between the available <b>cluster</b> <b>nodes.</b>|$|R
50|$|The above {{described}} mechanisms only {{resolve the}} issues with LVM's {{access to the}} storage. The file system selected {{to be on top}} of such LVs must either support clustering by itself (such as GFS2 or VxFS) or it must only be mounted by a single <b>cluster</b> <b>node</b> at any time (such as in an active-passive configuration).|$|E
50|$|The {{architecture}} of a diskless computer cluster {{makes it possible}} to separate servers and storage array. The operating system as well as the actual reference data (userfiles, databases or websites) are stored competitively on the attached storage system in a centralized manner. Any server that acts as a <b>cluster</b> <b>node</b> can be easily exchanged by demand.|$|E
5000|$|A {{distributed}} lock {{manager is}} used to broker concurrent LVM metadata accesses. Whenever a <b>cluster</b> <b>node</b> needs to modify the LVM metadata, it must secure permission from its local , which is in constant contact with other [...] daemons in the cluster and can communicate a desire to get a lock on {{a particular set of}} objects.|$|E
5000|$|PxFS (Proxy file system) is a distributed, high availability, POSIX {{compliant}} filesystem internal to Solaris <b>Cluster</b> <b>nodes.</b> Global {{devices in}} Sun Cluster are {{made possible by}} PxFS.|$|R
5000|$|... when {{a job is}} {{submitted}} for execution by a submitter, it's moved to preinput queue: while there, JCL is validated (JCL validation is done by a <b>cluster's</b> <b>node)</b> ...|$|R
5000|$|A {{geographically}} distributed, highly available clustered storage setup leveraging {{the virtual}} disk mirroring feature across datacenters within 300 km distance. Stretched Clusters can span 2, 3 or 4 datacenters (chain or ring topology, a 4-site cluster requiring 8 <b>cluster</b> <b>nodes).</b> <b>Cluster</b> consistency is ensured {{by a majority}} voting set.|$|R
5000|$|As {{each new}} node is booted it will locate one <b>cluster</b> <b>node,</b> then {{negotiate}} its {{entry into the}} cluster. If the IP address of a node is not supplied to the booting node, it will multicast for one. The first responding node {{will be used as}} the point of negotiation. The local CHAOS node initiates an IPSEC tunnel to the elected negotiation node using a pre-shared key. If the tunnel fails to establish, the new node is unable to join the cluster. With the tunnel established the new node requests a copy of the openMosix cluster map from the negotiating <b>cluster</b> <b>node.</b> The new node then repeats this process with every node in the cluster map; establishing an IPSEC tunnel, validating the cluster map, then moving on. In this way, every node is interconnected with every other node by [...] "n-1" [...] IPSEC tunnel connections. All openMosix cluster communications are then said to be authenticated and encrypted via the CHAOS platform.|$|E
50|$|A cluster-aware {{application}} is a software application designed to call cluster APIs {{in order to}} determine its running state, in case a manual failover is triggered between cluster nodes for planned technical maintenance, or an automatic failover is required, if a computing <b>cluster</b> <b>node</b> encounters hardware or software failure, to maintain business continuity. A cluster-aware application may be capable of failing over LAN or WAN.|$|E
5000|$|Writes from hosts are {{acknowledged}} {{once they}} have been committed into the SVC mirrored cache, but prior to being destaged to the underlying storage controllers. Data is protected by replication to the peer node in an I/O group (<b>cluster</b> <b>node</b> pair). Cache size {{is dependent on the}} SVC hardware model and installed options. Fast-write cache is especially useful to increase performance in midrange storage configurations.|$|E
40|$|Abstract â€” Graphical {{processing}} Units (GPUs) {{are finding}} widespread use as accelerators in computer clusters. It {{is not yet}} trivial to program applications that use multiple GPU-enabled <b>cluster</b> <b>nodes</b> efficiently. A key aspect of this is managing effective communication between GPU memory on separate devices on separate nodes. We develop a algorithmic framework for Finite-Difference numerical simulations that would normally require highly synchronous data-parallelism so they can effectively use loosely coupled GPU-enabled <b>cluster</b> <b>nodes.</b> We employ asynchronous communications and appropriate memory overlay of computations and communications to hide latency...|$|R
30|$|For the {{benchmarks}} at {{the detection}} phase, we varied {{the number of}} Hadoop <b>cluster</b> <b>nodes</b> for different file sizes and observed the CPU and memory usage at the NameNode.|$|R
40|$|In recent years, {{wireless}} {{sensor network}} (WSN) applications have tended to transmit data hop by hop, from sensor <b>nodes</b> through <b>cluster</b> <b>nodes</b> to the base station. As a result, users must collect data from the base station. This study considers two different applications: hop by hop transmission of data from <b>cluster</b> <b>nodes</b> to the base station and the direct access to <b>cluster</b> <b>nodes</b> data by mobile users via mobile devices. Due to the hardware limitations of WSNs, some low-cost operations such as symmetric cryptographic algorithms and hash functions are used to implement a dynamic key management. The session key can be updated to prevent threats of attack from each communication. With these methods, the data gathered in {{wireless sensor network}}s can be more securely communicated. Moreover, the proposed scheme is analyzed and compared with related schemes. In addition, an NS 2 simulation is developed in which the experimental {{results show that the}} designed communication protocol is workable...|$|R
