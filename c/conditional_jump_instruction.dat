5|347|Public
50|$|A local branch {{predictor}} has {{a separate}} history buffer for each <b>conditional</b> <b>jump</b> <b>instruction.</b> It may use a two-level adaptive predictor. The history buffer is separate for each <b>conditional</b> <b>jump</b> <b>instruction,</b> while the pattern history table may be separate {{as well or}} it may be shared between all conditional jumps.|$|E
50|$|The Cambridge Programmable (sold in the U.S as the Radio Shack EC-4001) was {{released}} in 1975. It lacked accuracy in many of its scientific functions, some yielding only four significant digits. It featured a single memory register and a limit of 36 program steps, along with a <b>conditional</b> <b>jump</b> <b>instruction.</b> The Programmable came with a program library consisting of 4 books, covering general functions, finance & statistics, mathematics, physics & engineering and electronics.|$|E
50|$|The {{first time}} a <b>conditional</b> <b>jump</b> <b>instruction</b> is encountered, {{there is not much}} {{information}} to base a prediction on. But the branch predictor keeps records of whether branches are taken or not taken. When it encounters a conditional jump that has been seen several times before then it can base the prediction on the history. The branch predictor may, for example, recognize that the conditional jump is taken more often than not, or that it is taken every second time.|$|E
50|$|Flow {{control is}} {{facilitated}} through {{a group of}} one unconditional and twelve <b>conditional</b> <b>Jump</b> <b>instructions.</b> <b>Jump</b> targets are relative to PC with an offset of -128 to +127 word addresses.|$|R
5000|$|The Programma 101 {{was able}} to {{calculate}} the basic four arithmetic functions (addition, subtraction, multiplication, and division), plus square root, absolute value, and fractional part. It was equipped with memory registers with features such as clear, transfer, and exchange, plus printing and halt for input. There were 16 <b>jump</b> <b>instructions</b> and 16 <b>conditional</b> <b>jump</b> <b>instructions.</b> Its features of <b>conditional</b> <b>jump</b> <b>instructions,</b> an alphanumeric programming language, an internal memory, and a data storage system define it as a [...] "computer". Thirty-two label statements were available as destinations for the <b>jump</b> <b>instructions</b> and/or the four start keys (V, W, Y, Z). Routines on magnetic cards could be used without knowledge of programming.|$|R
2500|$|In {{interpreted}} programming languages, for-loops can {{be implemented}} in many ways. Oftentimes, the for-loops are directly translated to assembly-like compare <b>instructions</b> and <b>conditional</b> <b>jump</b> <b>instructions.</b> However, {{this is not always}} so. In some interpreted programming languages, for-loops are simply translated to while-loops. For instance, take the following Mint/Horchata code: ...|$|R
5000|$|Two-way {{branching}} {{is usually}} implemented with a <b>conditional</b> <b>jump</b> <b>instruction.</b> A conditional jump {{can either be}} [...] "not taken" [...] and continue execution with the first branch of code which follows immediately after the conditional jump, {{or it can be}} [...] "taken" [...] and jump to a different place in program memory where the second branch of code is stored. It is not known for certain whether a conditional jump will be taken or not taken until the condition has been calculated and the conditional jump has passed the execution stage in the instruction pipeline (see fig. 1).|$|E
50|$|Without branch prediction, the {{processor}} {{would have to}} wait until the <b>conditional</b> <b>jump</b> <b>instruction</b> has passed the execute stage before the next instruction can enter the fetch stage in the pipeline. The branch predictor attempts to avoid this waste of time by trying to guess whether the conditional jump is most likely to be taken or not taken. The branch that is guessed to be the most likely is then fetched and speculatively executed. If it is later detected that the guess was wrong then the speculatively executed or partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.|$|E
5000|$|In {{practical}} software, the [...] and [...] {{instructions are}} used to clear and set the direction flag, respectively. Some instructions in assembly language use the FLAGS register. The <b>conditional</b> <b>jump</b> <b>instructions</b> use certain flags to compute. For example, [...] uses the zero flag, [...] uses the carry flag and [...] uses the overflow flag. Other conditional instructions look at combinations of several flags.|$|R
50|$|The <b>conditional</b> <b>jump</b> <b>instructions</b> {{are listed}} in pairs, the former opcode is for a forward jump, and the latter one for a {{backward}} jump. Again, the offset of the jump is obtained from the second byte of the instruction. Thus, all instructions in rows 0 to 7 and row 9 consist of two bytes (the opcode and a data byte) while all the other instructions consist of just a single opcode byte.|$|R
5000|$|Flags {{are heavily}} used for {{comparisons}} in the x86 architecture. When a comparison is made between two data, the CPU sets the relevant flag or flags. Following this, <b>conditional</b> <b>jump</b> <b>instructions</b> {{can be used}} to check the flags and branch to code that should run, e.g.: cmp eax, ebx jne do_something ...do_something: do something hereFlags are also used in the x86 architecture to turn on and off certain features or execution modes. For example, to disable all maskable interrupts, you can use the instruction: cli ...|$|R
5000|$|Within {{microprocessors}} {{and other}} logic devices, collections of bit fields called [...] "flags" [...] {{are commonly used}} to control or to indicate the intermediate state or outcome of particular operations. Microprocessors typically have a status register that is composed of such flags, used to indicate various post-operation conditions, for example an arithmetic overflow. The flags can be read and used to decide subsequent operations, such as in processing <b>conditional</b> <b>jump</b> <b>instructions.</b> For example, a je (<b>Jump</b> if Equal) <b>instruction</b> in the x86 assembly language {{will result in a}} jump if the Z (zero) flag was set by some previous operation.|$|R
40|$|Abstract- The path {{constraints}} are leaked by binary <b>conditional</b> <b>jump</b> <b>instructions</b> {{which are}} the binary representation of software’s internal logic. Based {{on the problem of}} software’s path constraints leaking, reverse engineering using path-sensitive techniques such as symbolic execution and theorem proving poses a new threat to software intellectual property protection. In order to mitigate path information leaking problem, this paper proposed a novel branch obfuscation scheme that uses binary code side effects to hide path constraints and takes advantage of remote trusted entity to protect software’s control flow graph, without changing software’s functionality. The experimental results show that this branch obfuscation technique could effectively protect software’s path constraints against state-of-the-art reverse engineering, yet practical in terms of performance. Index Terms- code obfuscation, binary code side effects, exception handling, symbolic execution, trusted entity 1...|$|R
40|$|The master’s thesis {{incorporates}} {{the control system}} Sinumerik 840 D to {{the most widely used}} control systems of CNC machine tools. In the first thesis part are analyzed the NC programming methodics, which are relative only to the control system Sinumerik 840 D and an essentials necessary to understand the issue in the area of CNC machine tools. Next part of the thesis includes design of the technology and processing of the NC program for machined component (gearbox cover). The NC program is made by ShopMill that is focused on a workshop area. During the program creation is combined a ShopMill and parametric programming, with using of a <b>conditional</b> <b>jump</b> <b>instructions</b> and repetitions. The final part of the thesis includes verification of the selected component production process by simulation and technical-economic evaluation of the designed technology...|$|R
50|$|The {{idea of a}} {{subroutine}} was {{worked out}} after computing machines had already existed for some time.The arithmetic and <b>conditional</b> <b>jump</b> <b>instructions</b> were planned {{ahead of time and}} have changed relatively little; but the special instructions used for procedure calls have changed greatly over the years.The earliest computers and microprocessors, such as the Small-Scale Experimental Machine and the RCA 1802, did not have a single subroutine call instruction.Subroutines could be implemented, but they required programmers to use the call sequence—a series of instructions—at each call site.Some very early computers and microprocessors, such as the IBM 1620, the Intel 8008, and the PIC microcontrollers, have a single-instruction subroutine call that uses dedicated hardware stack to store return addresses—such hardware supports only a few levels of subroutine nesting, but can support recursive subroutines.Machines before the mid 1960s—such as the UNIVAC I, the PDP-1, and the IBM 1130—typically use a calling convention which saved the instruction counter in the first memory location of the called subroutine. This allows arbitrarily deep levels of subroutine nesting, but does not support recursive subroutines.The PDP-11 (1970) {{is one of the first}} computers with a stack-pushing subroutine call instruction; this feature supports both arbitrarily deep subroutine nesting and also supports recursive subroutines.|$|R
40|$|Introduction The {{final phase}} of the Verdi {{compiler}} translates a structured intermediate language into a sequence of <b>instructions,</b> <b>jumps,</b> <b>conditional</b> <b>jumps,</b> and (pseudo-) <b>instructions</b> that define labels. This transformation can be defined and proved correct in a quite general setting. To that end, we define that transformation {{in terms of a}} syntax that includes some unspecified set of "primitive" commands, and a semantics that is based on some unspecified semantic functions. 2 Parameterization We will assume a set Prim of primitive actions and a set Test of primitive tests. We further assume a semantic domain A of semantic actions. We suppose that v is a partial order on A that makes (A; v) a chain-complete parial order [1]. In particular, then, any monotonic function from A to A has a least fixed point. We assume the existence...|$|R
5000|$|Previously {{executed}} {{instructions were}} saved in an eight-word cache, called the [...] "stack". In-stack jumps were quicker than out-of-stack jumps because no memory fetch was required. The stack was flushed by an unconditional <b>jump</b> <b>instruction,</b> so unconditional <b>jumps</b> {{at the ends}} of loops were conventionally written as <b>conditional</b> <b>jumps</b> that would always succeed.|$|R
5000|$|<b>Conditional</b> <b>jump</b> if r is positive; i.e. IF r > 0 THEN <b>jump</b> to <b>instruction</b> z else {{continue}} in sequence (Cook and Reckhow call this: [...] "TRAnsfer control to line m if Xj > 0") ...|$|R
5000|$|Xilinx {{documents}} the PicoBlaze as requiring just 96 FPGA slices. The small implementation size is achieved {{in part through}} a fairly rigid separation of the instruction sequencing side (program counter, call-return stack, implied stack pointer, and interrupt enable bit) from the execution side (ALU, register file, scratchpad RAM, Z/C status bits). The only information which flows from the compute side to the sequencing side are the zero and carry ALU status bits, when tested by the <b>conditional</b> <b>JUMP</b> and CALL <b>instructions.</b> It {{is not possible to}} implement computed jumps or function pointers. The only information which flows from the sequencing side to the execution side are operand fields: destination register (4 bits), ALU opcode (six bits), optional source register (4 bits), optional 8-bit immediate value/port-address, optional 6-bit scratchpad address. There is no mechanism to inspect the value of the stack pointer, the contents of the 31-entry stack, the interrupt enable bit, or the contents of program memory.|$|R
50|$|The {{parity flag}} is usually used in <b>conditional</b> <b>jumps,</b> where e.g. the JP <b>instruction</b> <b>jumps</b> to the given target when the parity flag is {{set and the}} JNP <b>instruction</b> <b>jumps</b> {{if it is not}} set. The flag may be also read {{directly}} with instructions such as PUSHF, which pushes the flags register on the stack.|$|R
50|$|A <b>conditional</b> <b>jump</b> that {{controls}} a loop is best predicted {{with a special}} loop predictor. A <b>conditional</b> <b>jump</b> {{in the bottom of}} a loop that repeats N times will be taken N-1 times and then not taken once. If the <b>conditional</b> <b>jump</b> is placed at the top of the loop, it will be not taken N-1 times and then taken once. A <b>conditional</b> <b>jump</b> that goes many times one way and then the other way once is detected as having loop behavior. Such a <b>conditional</b> <b>jump</b> can be predicted easily with a simple counter. A loop predictor is part of a hybrid predictor where a meta-predictor detects whether the <b>conditional</b> <b>jump</b> has loop behavior.|$|R
5000|$|Some {{instructions}} {{manipulate the}} program counter rather than producing result data directly; such instructions are generally called [...] "jumps" [...] and facilitate program behavior like loops, conditional program execution (through {{the use of}} a <b>conditional</b> <b>jump),</b> and existence of functions. In some processors, some other instructions change the state of bits in a [...] "flags" [...] register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a [...] "compare" [...] instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later <b>jump</b> <b>instruction</b> to determine program flow.|$|R
40|$|Abstract. We study {{sequential}} {{programs that}} are instruction sequences with direct and indirect <b>jump</b> <b>instructions.</b> The intuition is that indirect <b>jump</b> <b>instructions</b> are <b>jump</b> <b>instructions</b> where {{the position of the}} <b>instruction</b> to <b>jump</b> to is the content of some memory cell. We consider several kinds of indirect <b>jump</b> <b>instructions.</b> For each kind, we define the meaning of programs with indirect <b>jump</b> <b>instructions</b> of that kind by means of a translation into programs without indirect <b>jump</b> <b>instructions.</b> For each kind, the intended behaviour of a program with indirect <b>jump</b> <b>instructions</b> of that kind under execution is the behaviour of the translated program under execution on interaction with some memory device...|$|R
5000|$|... { Increment (r), Decrement (r), Clear (r); Copy (rj,rk), <b>conditional</b> <b>Jump</b> if {{contents}} of r=0, <b>conditional</b> <b>Jump</b> if rj=rk, unconditional Jump, HALT } ...|$|R
40|$|We study {{sequential}} {{programs that}} are instruction sequences with direct and indirect <b>jump</b> <b>instructions.</b> The intuition is that indirect <b>jump</b> <b>instructions</b> are <b>jump</b> <b>instructions</b> where {{the position of the}} <b>instruction</b> to <b>jump</b> to is the content of some memory cell. We consider several kinds of indirect <b>jump</b> <b>instructions.</b> For each kind, we define the meaning of programs with indirect <b>jump</b> <b>instructions</b> of that kind by means of a translation into programs without indirect <b>jump</b> <b>instructions.</b> For each kind, the intended behaviour of a program with indirect <b>jump</b> <b>instructions</b> of that kind under execution is the behaviour of the translated program under execution on interaction with some memory device. Comment: 23 pages; typos corrected, phrasing improved, reference replace...|$|R
50|$|A global branch {{predictor}} {{does not}} keep a separate history record for each <b>conditional</b> <b>jump.</b> Instead it keeps a shared history of all <b>conditional</b> <b>jumps.</b> The {{advantage of a}} shared history is that any correlation between different <b>conditional</b> <b>jumps</b> is part of making the predictions. The disadvantage is that the history is diluted by irrelevant information if the different <b>conditional</b> <b>jumps</b> are uncorrelated, and that the history buffer may not include any bits from the same branch if {{there are many other}} branches in between. It may use a two-level adaptive predictor.|$|R
5000|$|<b>Jump</b> <b>instructions</b> - 2 {{kinds of}} <b>Jump</b> <b>instructions</b> which reload ANTIC's program counter (3-byte instructions) ...|$|R
5000|$|Machine level branch {{instructions}} are sometimes called <b>jump</b> <b>instructions.</b> Machine level <b>jump</b> <b>instructions</b> typically have unconditional and conditional forms where the latter {{may be taken}} or not taken depending on some condition.|$|R
40|$|Instruction {{sequences}} {{with direct}} and indirect <b>jump</b> <b>instructions</b> are as expressive as instruction sequences with direct <b>jump</b> <b>instructions</b> only. We show that, in the case where the number of instructions is not bounded, there exist instruction sequences of the former kind from which elimination of indirect <b>jump</b> <b>instructions</b> is possible without a super-linear increase of their maximal internal delay on execution only {{at the cost of}} a super-linear increase of their length...|$|R
40|$|The overall prrformance of {{supercomputers}} is slow {{compared to}} the speed of their underly-ing logic technology. This discrepancy is due to several bottlenecks: memories are slower than the CPU, <b>conditional</b> <b>jumps</b> limit the usefulness of pipelining and pre-fetching mechanisms, and functional-unit parallelism {{is limited by the}} speed of hardware scheduling. This paper describes a supercomputer architecture called Ring of Pre-fetch Elements (ROPE) that attempts to solve the problems of memory latency and <b>conditional</b> <b>jumps,</b> without hardware scheduling. ROPE consists of a very pipelined CPU data path with a new instruction pre-fetching mechanism that supports general multi-way <b>conditional</b> <b>jumps.</b> An optimizing compiler based on a global code transformation technique (Percolation Scheduling or PS) gives high performance without scheduling hardware...|$|R
40|$|Supercomputer {{architectures}} {{are not as}} fast as logic technology allows because {{memories are}} slow than the CPU, <b>conditional</b> <b>jumps</b> limit the usefulness of pipelining and prefetching mechanisms, and functional-unit parallelism {{is limited by the}} speed of hardware scheduling. We propose a supercomputer architecture called Ring Of Prefetch Elements (ROPE) that attempts to solve the problems of memory latency and <b>conditional</b> <b>jumps</b> without hardware scheduling. ROPE consists of a pipelined CPU or very-large-instruction-word data path with a new instruction prefetching mechanism that supports general multi-way <b>conditional</b> <b>jumps.</b> To get high-performance without scheduling hardware, ROPE relies on an optimizing compiler based on a global code transformation technique (Percolation Scheduling). This paper describes both the promise and the limitations of ROPE...|$|R
40|$|Instruction {{sequences}} {{with direct}} and indirect <b>jump</b> <b>instructions</b> are as expressive as instruction sequences with direct <b>jump</b> <b>instructions</b> only. We show that, in the case where the number of instructions is not bounded, {{we are faced with}} increases of the maximal internal delays of instruction sequences on execution that are not bounded by a linear function if we strive for acceptable increases of the lengths of instruction sequences on elimination of indirect <b>jump</b> <b>instructions.</b> Comment: 10 pages, definition of maximal internal delay and theorem 1 are stated more precise; presentation improve...|$|R
5000|$|Jump threading: In this pass, {{consecutive}} <b>conditional</b> <b>jumps</b> predicated entirely or partially on {{the same}} condition are merged.|$|R
50|$|Finally, because calls, jumps, and {{interrupts}} {{reset the}} prefetch queue, and because loading the IP register requires {{communication between the}} EU and the BIU (since the IP register is in the BIU, not in the EU, where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any <b>conditional</b> <b>jump</b> requires 4 clock cycles if not taken, but if taken, it requires 16 cycles in addition to resetting the prefetch queue; therefore, <b>conditional</b> <b>jumps</b> should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a <b>conditional</b> <b>jump</b> that skips over one or two instructions {{to achieve the same}} result.|$|R
50|$|Among the x86 instructions, {{some use}} {{implicit}} registers {{for one of}} the operands or results (multiplication, division, counting <b>conditional</b> <b>jump).</b>|$|R
5000|$|During the {{execution}} of certain programs there are places where the program execution flow can continue in several ways. These are called branches, or <b>conditional</b> <b>jumps.</b> The CPU also uses a pipeline which allows several instructions to be processed at the same time. When the code for a <b>conditional</b> <b>jump</b> is read {{we do not yet}} know the next instruction to execute and insert into {{the execution}} pipeline. This is where branch prediction comes in.|$|R
