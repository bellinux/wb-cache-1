21|307|Public
5000|$|Often an {{interval}} containing [...] with a specified probability is required. Such {{an interval}}, a <b>coverage</b> <b>interval,</b> can be deduced from the probability distribution for [...] The specified probability {{is known as}} the coverage probability. For a given coverage probability, {{there is more than one}} <b>coverage</b> <b>interval.</b> The probabilistically symmetric <b>coverage</b> <b>interval</b> is an interval for which the probabilities (summing to one minus the coverage probability) of a value to the left and the right of the interval are equal. The shortest <b>coverage</b> <b>interval</b> is an interval for which the length is least over all coverage intervals having the same coverage probability.|$|E
5000|$|... a <b>coverage</b> <b>interval</b> {{containing}} [...] with {{a specified}} coverage probability.|$|E
50|$|When the {{measurement}} model is multivariate, that is, it has {{any number of}} output quantities, the above concepts can be extended. The output quantities are now described by a joint probability distribution, the <b>coverage</b> <b>interval</b> becomes a coverage region, the law of propagation of uncertainty has a natural generalization, and a calculation procedure that implements a multivariate Monte Carlo method is available.|$|E
40|$|Abstract: The {{main purpose}} of this paper is to present a {{possibility}} theory-based generalization of conventional <b>interval</b> propagation to <b>coverage</b> <b>intervals.</b> Indeed, the whole set of <b>coverage</b> <b>intervals</b> for all the probability levels stacked on top of one another constitutes a possibility distribution. Thus by slight modifications of Zadeh’s extension principle, we will prove that it is possible to compute the <b>coverage</b> <b>intervals</b> of an indirect measurement from many other measurements (possibly dependent) by a known non decreasing relationship...|$|R
5000|$|Kott, P., & Liu, Y. (2009). One-sided <b>coverage</b> <b>intervals</b> for a {{proportion}} estimated from a stratified simple random sample. International Statistical Review, 77, 251-265.|$|R
40|$|International audienceThis paper {{deals with}} the {{foundations}} of a possibility/fuzzy expression of measurement uncertainty. Indeed the notion of possibility distribution is clearly identified to a family of probability distributions whose <b>coverage</b> <b>intervals</b> {{are included in the}} level cuts of the possibility distribution Thus the fuzzy inclusion ordering, dubbed specificity ordering, constitutes the basis of a maximal specificity principle. The latter is sounder than the maximal entropy principle to deal with cases of partial or incomplete information in a measurement context. The two approaches can be compared on some common practical measurement cases thanks to the respective <b>coverage</b> <b>intervals</b> they provide...|$|R
40|$|The {{standard}} two-sided Wald <b>coverage</b> <b>interval</b> {{for a small}} proportion, P, may perversely include negative values. One way {{to correct}} this anomaly when analyzing data from a simple random sample is to compute an asymmetric Wilson (or score) <b>coverage</b> <b>interval.</b> This approach has proven not only theoretically satisfying but empirically effective. Some have suggested computing an ad hoc Wilson-like <b>coverage</b> <b>interval</b> for P when it is weighted or is estimated with complex sample data. We propose an alternative, theoretically motivated, approach to two-sided coverage-interval construction. In the case where the population P is unweighted and the data from a simple random sample, the <b>coverage</b> <b>interval</b> generated by the new pivotal is asymptotically identical to the Wilson <b>coverage</b> <b>interval.</b> A modest empirical evaluation shows that our coverage intervals are slightly better than those derived from the ad hoc Wilson approach and much better than standard Wald intervals. Better yet is a model-based Wilson approach, but in our study the model was correct. Key Words: Wald <b>coverage</b> <b>interval,</b> Wilson <b>coverage</b> <b>interval,</b> Asymptotic, Asymmetric. Background Statisticians, especially those dealing with biological and environmental data, are increasingly being asked to estimate very small (or very large) proportions. Although it is often not difficult to develop a point estimate for such proportions, statistically defensible coverage-interval determination can be more problematic...|$|E
40|$|Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Reference interval of all haematological and biological analytes should be measured for every population {{because of the}} huge diversity in genetic make-up, dietary habits. The <b>coverage</b> <b>interval</b> of antioxidant vitamins (vitamin C an...|$|E
40|$|A proper {{evaluation}} of the uncertainty associated to the quantification of micropollutants in the environment, like Polycyclic Aromatic Hydrocarbons (PAHs), is crucial for {{the reliability of the}} measurement results. The present work describes a comparison between the uncertainty evaluation carried out according to the GUM uncertainty framework and the Monte Carlo (MC) method. This comparison was carried out starting from real data sets obtained from the quantification of benzo[a]pyrene (BaP), spiked on filters commonly used for airborne particulate matter sampling. BaP was chosen as target analyte as it is listed in the current European legislation as marker of the carcinogenic risk for the whole class of PAHs. MC method, being useful for nonlinear models and when the resulting output distribution for the measurand is non-symmetric, can particularly fit the cases in which the results of intrinsically positive quantities are very small and the lower limit of a desired <b>coverage</b> <b>interval,</b> obtained according to the GUM uncertainty framework, can be dramatically close to zero, if not even negative. In the case under study, it was observed that the two approaches for the uncertainty evaluation provide different results for BaP masses in samples containing different masses of the analyte, MC method giving larger coverage intervals. In addition, in cases of analyte masses close to zero, the GUM uncertainty framework would give even negative lower limit of uncertainty <b>coverage</b> <b>interval</b> for the measurand, an unphysical result which is avoided when using MC method. MC simulations, indeed, can be configured in a way that only positive values are generated thus obtaining a <b>coverage</b> <b>interval</b> for the measurand that is always positive...|$|E
40|$|These two {{papers are}} the result of {{research}} conducted by the authors on methods for constructed one-sided <b>coverage</b> <b>intervals</b> based on relatively small simple random or stratified simple random samples. The first paper lays out the theory for a new method. The second paper evaluates that method and a number of alternatives...|$|R
40|$|AbstractIn {{this paper}} a new {{criterion}} for comparing measurement results and determining conformity with specifications is proposed, which essentially {{is a strategy}} of estimating the empirical relationships of objects. Comparing with traditional methods given in GUM: 2008 and ISO 14253 - 1, this criterion improves the resolution of comparison by reducing the sizes of the <b>coverage</b> <b>intervals</b> to be compared. Interval order (a binary relation) is used for comparing the <b>coverage</b> <b>intervals</b> of the measurand and represents the empirical relations. The systematic effects of measurement are classified into two types: monotonic and non-monotonic effects, so that, without correcting the monotonic effects, a biased measurand can be specified to represent the empirical relations. Thereby the uncertainty components arising from the monotonic effects can {{be removed from the}} combined uncertainty. A strategy is given for determining the relationships among measurement results and specification limits. An example is given to demonstrate the application of the criterion...|$|R
40|$|In {{discussing}} the two papers in this session, “Sample size considerations for multilevel surveys” by Michael P. Cohen and “Two sided <b>coverage</b> <b>intervals</b> for small proportions based on survey data ” by Philip S. Kott, Per Gosta Andersson, and Olle Nerman, I will follow a format. I will first make some general comments. Then {{for each of}} these papers (which I will consider i...|$|R
40|$|Reference {{interval}} of all haematological and biological analytes should be measured for every population {{because of the}} huge diversity in genetic make-up, dietary habits. The <b>coverage</b> <b>interval</b> of antioxidant vitamins (vitamin C and α-tocopherol) in the plasma and serum of reference Bengali population was determined and compared with the reference intervals of antioxidant vitamins in the established literature. Adult healthy volunteers from 18 to 68 years of age underwent extensive clinical and investigational procedure and {{were included in the}} study. Vitamin C and α-tocopherol were estimated using simple Spectrophotometric method. Of the 71 healthy Bengali volunteers participated, 31 were males and 40 were females. The mean concentration of plasma vitamin C was found to be 0. 65 [*]mg/dL. The mean α-tocopherol was found 6. 35 [*]mg/L (14. 74 [*]μmol/L) in the study population higher than the normal threshold value for α-tocopherol but lower than other populations. The study data enabled us to determine the gender nonspecific <b>coverage</b> <b>interval</b> of antioxidant vitamins, and the intervals were lower than the established reference interval in other populations...|$|E
40|$|There is {{potential}} for confusion between the term ‘coverage interval’, which {{is developing a}} common usage in metrology, and the term ‘statistical coverage interval’, as defined in the Guide to the Expression of Uncertainty in Measurement. The meaning of a ‘statistical <b>coverage</b> <b>interval</b> ’ is explained further, and {{possible courses of action}} are suggested. The term ‘coverage interval ’ is developing a common usage in metrology without yet having a formal definition in th...|$|E
40|$|Abstract- The {{main purpose}} of the paper is to present a new fully deterministic, {{numerical}} approach to calculation of <b>coverage</b> <b>interval</b> at certain coverage confidence. The main crucial aspects for this method are revealed and the accuracy of this method is presented as well. A brief comparison to others methods, analytical and numerical based on Monte Carlo methods and also so widely applied and recommended in GUM law of propagation of uncertainties are described. Several examples are presented. I...|$|E
40|$|In {{this paper}} we {{consider}} the estimation of the weights of tangent portfolios from the Bayesian point of view assuming normal conditional distributions of the logarithmic returns. For diffuse and conjugate priors for the mean vector and the covariance matrix, we derive stochastic representations for the posterior distributions of the weights of tangent portfolio and their linear combinations. Separately we provide the mean and variance of the posterior distributions, which are of key importance for portfolio selection. The analytic results are evaluated within a simulation study, where the precision of <b>coverage</b> <b>intervals</b> is assessed. ...|$|R
40|$|We {{formulate}} Bayesian {{approaches to}} the problems of determining the required sample size for Bayesian interval estimators of a predetermined length for a single Poisson rate, for the difference between two Poisson rates, and for the ratio of two Poisson rates. We demonstrate the efficacy of our Bayesian-based sample-size determination method with two real-data quality-control examples and compare the results to frequentist sample-size determination methods. Average <b>coverage</b> criterion, <b>interval</b> estimators, HPD <b>intervals,</b> <b>coverage</b> probability,...|$|R
40|$|One of {{the main}} {{challenges}} {{in the analysis of}} dynamic measurements is the estimation of the timedependent value of the measurand and the corresponding propagation of uncertainties. In this paper we outline the design of appropriate digital compensation filters as a means of estimating the quantity of interest. For the propagation of uncertainty in the application of such digital filters we present online formulae for finite impulse response and infinite impulse response filters. We also demonstrate a recently developed efficient Monte Carlo method for uncertainty propagation in dynamic measurements which allows calculating point-wise <b>coverage</b> <b>intervals</b> in real-time...|$|R
40|$|Confidence nets, that is, {{collections}} of confidence intervals that {{fill out the}} parameter space and whose exact parameter coverage can be computed, are familiar in nonparametric statistics. Here, the distributional assumptions are based on invariance under the action of a finite reflection group. Exact confidence nets are exhibited for a single parameter, based on the root system of the group. The main result is a formula for the generating function of the <b>coverage</b> <b>interval</b> probabilities. The proof makes use {{of the theory of}} "buildings" and the Chevalley factorization theorem for the length distribution on Cayley graphs of finite reflection groups. Comment: 20 pages. To appear in Bernoull...|$|E
40|$|Abstract: This paper {{considers}} {{the determination of}} measurement results and associated uncertainties when prior physical knowledge of the quantities concerned is available. The scientific concepts that {{provide a basis for}} determining realistic solutions to such problems are discussed, and implementations of these concepts are considered. To illustrate the concepts, an example concerning the determination of an analyte concentration in chemical metrology is used. This will be the basis for a discussion concerning the various approaches available to deal with such constraints. Results stating a <b>coverage</b> <b>interval</b> containing infeasible values (values the quantity cannot physically take) should be avoided, and this fact will assist in the comparison of the relative merits of each method...|$|E
40|$|It {{is shown}} how the {{decision}} threshold, the detection limit {{and the limits}} of a <b>coverage</b> <b>interval</b> - summarily called the characteristic limits - and, in addition, the best estimate and the associated standard uncertainty of a non-negative radiation measurand are to be calculated by using the Monte Carlo (MC) method in ionising-radiation measurements. The limits are mathematically defined by means of quantiles of the Bayesian distributions of the possible measurand values. The MC-induced uncertainties of the limits and typical problems connected with MC application are also treated. The paper can serve as a bridge between the ISO Guide to the Expression of Uncertainty in Measurement (GUM), Supplement 1 applying the MC method and ISO/FDIS 11929 (at present in preparation) dealing with the characteristic limits. As an illustration, a net count rate measurand, being the difference of a gross and a background count rate, is treated theoretically and numerically. More complex examples deal with the wipe test for surface contamination and with a linear multi-channel spectrum unfolding...|$|E
40|$|<b>Coverage</b> <b>intervals</b> for trace {{elements}} in human scalp hair commonly {{provide the basis}} forinterpreting laboratory results and also in comparative decision-making processes regardingexposure risk assessment. This short communication documents, by some examples, thatthose computed for human hair are to be considered site specific, as they reflect local envi-ronmental conditions; also each geographic area has a typical profile of hair elementalcomposition of its inhabitants. Therefore, the levels of {{trace elements}} in hair are not strictlycomparable between {{different areas of the}} world. This issue is particularly relevant whenidentification of anomalous environmental exposures are requested or even in detectingphysiological disorders...|$|R
40|$|This paper {{presents}} a straightforward set of Bayesian techniques for analyzing models involving limited dependent variables; the techniques are demonstrated {{in an analysis}} of Kennan's (1985) data on contract strikes in U. S. manufacturing. The data are analyzed by deriving posterior distributions [...] including probability distributions [...] of hazard functions for strike durations using numerical Monte Carlo methods. The distributions are employed to derive <b>coverage</b> <b>intervals</b> for hazard functions, to assess the relative plausibility of nonnested hypotheses concerning the shape of the functions, and {{to assess the impact of}} industrial production on duration. Copyright 1993 by John Wiley & Sons, Ltd. ...|$|R
40|$|The {{generalized}} lambda distribution family {{fits the}} probability distributions {{of a wide}} variety of data sets, including the most important distributions encountered in the measurement applications (normal, uniform, Student's t, U-shaped, exponential). This paper illustrates how the four parameters needed for such distribution can be exploited in the expression of measurement uncertainty and to extend the information related to a measurement. The obtained representation allows an immediate calculation of <b>coverage</b> <b>intervals</b> and is particularly useful to support the techniques commonly applied in the estimation of the combined uncertainty. Moreover, in order to include the classical measurement information, a novel parameterization of the distribution is proposed...|$|R
40|$|One of {{the most}} popular methodologies for {{estimating}} the average treatment effect at the threshold in a regression discontinuity design is local linear regression (LLR), which places larger weight on units closer to the threshold. We propose a Gaussian process regression method that acts as a Bayesian analog to LLR for sharp regression discontinuity designs. Our Gaussian process regression method provides a flexible fit for treatment and control responses by placing a general prior on the mean response functions. We prove our method is consistent in estimating the average treatment effect at the threshold. Furthermore, we find via simulation that our method exhibits promising <b>coverage,</b> <b>interval</b> length, and mean squared error properties compared to standard LLR and state-of-the-art LLR methodologies. Finally, we explore the performance of our method on a real-world example by studying the impact of being a first-round draft pick on the performance and playing time of basketball players in the National Basketball Association. Comment: 39 pages, 5 figures, 4 table...|$|E
40|$|International audienceAt the {{application}} level, {{it is important}} to be able to define the measurement result as an interval that will contain an important part of the distribution of the measured values, that is, a <b>coverage</b> <b>interval.</b> This practice acknowledged by the International Organization for Standardization (ISO) Guide is a major shift from the probabilistic representation. It can be viewed as a probability/possibility transformation by viewing possibility distributions as encoding coverage intervals. In this paper, we extend previous works on unimodal distributions by proposing a possibility representation of bimodal probability distributions. Indeed, U-shaped distributions or Gaussian mixture distribution are not very rare in the context of physical measurements. Some elements to further propagate such bimodal possibility distributions are also exposed. The proposed method is applied to the case of three independent or positively correlated C-grade resistors in series and compared with the Guide to the Expression of Uncertainty in Measurement (GUM) and Monte Carlo methods...|$|E
40|$|In modern {{communication}} networks, Voice over IP (VoIP) {{users are}} even more interested to quantify the quality experienced in a VoIP call. This practice very often addresses the common need of rating {{the service of a}} telecommunication provider. To this aim, international standard committees have defined suitable metrics at application layer of the ISO/OSI protocol stack. Unfortunately, the evaluation of such metrics requires procedures and measurement setups very difficult to develop and, consequently, deliver to the final user. On the other hand, VoIP services performance is strongly influenced by the values assumed by the Internet Protocol Packet Delay Variation (¡PDV), also known as packet jitter. IPDV is defined at network layer and can be more easily evaluated than the cited metrics at application layer. The paper deals with the study of the correlation among measured IPDV values at network layer and those characterizing performance metrics at application layer, directly related to the quality perceived by the user in VoIP applications. The final aim is twofold: i) to design a reliable estimator able to forecast the objective quality perceived by the user on the basis of IPDV measurement results, and ii) to implement a procedure compliant with ISO-GUM for <b>coverage</b> <b>interval</b> assessment...|$|E
40|$|The {{determination}} of stoichiometries and stability constants of complexes {{by means of}} UV-visible spectrophotometry applying traditional methods does not evaluate {{the quality of the}} values obtained, since the classic application of these methods does not provide <b>coverage</b> <b>intervals.</b> However, the use of chemometric techniques in different steps of the application of these methods makes it possible to obtain not only a real value of the characteristics of the complexes but a validation of such a value. In this paper a methodology is proposed that combines some traditional methods, three different regression models (LMSR, LSR and LSPR) and a small number of mathematical algorithms. This methodology is able to estimate, in a simple and rigorous way, the stoichiometry and stability constant of a complex and its corresponding uncertainties...|$|R
40|$|Hair {{analysis}} {{is a powerful}} tool for assessing human exposure to metals and metalloids (MM). The basis for interpreting laboratory results lie on the use of <b>coverage</b> <b>intervals</b> (CI), computed between the 0. 025 and 0. 975 fractiles, from a well-defined group of reference individuals reflecting normal and healthy people. A critical point in efficient use of CI, when used for comparative decision-making processes, forensic and clinic considerations, is constituted by confounding factors as the specific living site of study population and gender of participants. Our study aims to demonstrate that hair levels of trace elements are site specific and also gender specific. We have taken into account the levels of 20 trace elements in hair samples from children of the same age (11 - 14 years old) residing in Sicilian sites characterized by different environmental conditions. The dataset consisted of hair samples collected within the urban area of Palermo, in several small towns located around the volcanic area of Mt. Etna and close to the industrial area of Gela. The study was organized as follow: (a) the first part was addressed to establish whether <b>coverage</b> <b>intervals</b> for MM in human scalp hair are to be considered site specific reflecting local environmental conditions; (b) the second part was intended to provide more information about the gender effect on MM distribution in human scalp hair. The obtained main results can be summarized as follows: a) for many elements, the computed CIs are clearly not equivalent for the different sites, but rather the interval of an element for a site extends far beyond that one calculated for another site, suggesting that what is normal for one site may not be normal for another site. Therefore CIs are valid only for a well defined area and can hardly be extended to other areas with different characteristics. b) CIs of several elements computed for hair samples from female subjects are statistically different from those computed for male subjects. For example, Cd, Li and Rb male <b>coverage</b> <b>intervals</b> extend far beyond those calculated for females, instead, those of Cu, Mn, Ni, Sr, V and Zn are wider for females than males. The Mann-Whitney test (p < 0. 01) showed statistically significative differences, between males and females, for Al, Ba, Cd, Co, Cr, Cu, Mn, Ni, Pb, Rb, Sb, Se, Sr, V and Zn. Linear Discriminant Analysis indicated a clear-cut separation in terms of Sr, Ni, Zn, V, Cd, Cu, Mn, Mo and Pb levels for females and Se, Cr, As, Li, Rb and Sb levels for males. We have concluded that CI computed for human hair are to be considered gender specific and they reflect the different basal metabolism between boys and girls. Therefore, the levels of trace elements in hair cannot strictly comparable between different areas and between subjects of different sex. This issue is particularly relevant when identification of anomalous (peculiar) environmental exposures are requested or even in detecting physiological disorders...|$|R
40|$|In {{a common}} ROC study design, several readers {{are asked to}} rate {{diagnostics}} of the same cases processed under dierent modalities. We describe a Bayesian hierarchical model that facilitates the analysis of this study design by explicitly modelling the three sources of variation inherent to it. In so doing, we achieve substantial reductions in the posterior uncertainty associated with estimates of the dierences in areas under the estimated ROC curves and corresponding reductions in the mean squared error (MSE) of these estimates. Based on simulation studies, both the widths of <b>coverage</b> <b>intervals</b> and MSE of estimates of dierences in the area under the curves appear to be reduced by a factor that often exceeds ve. Thus, our methodology {{has important implications for}} increasing the power of analyses based on ROC data collected from an available study population. Copyright? 2005 John Wiley & Sons, Ltd...|$|R
40|$|The Guide to Uncertainty of Measurement (GUM) {{approach}} and the adaptive Monte Carlo method (MCM) provide two alternative approaches for the propagation {{stage of the}} uncertainty estimation. These two approaches are implemented and compared concerning the 95 % <b>coverage</b> <b>interval</b> estimation of the measurement of Gross Heat of Combustion (GHC) of an automotive diesel fuel by bomb calorimetry. The GUM approach, which assumes either a Gaussian or a t-distribution for the output quantity (GHC) gives half width intervals of 0. 28 MJ kg- 1 (Gaussian distribution) or 0. 29 MJ kg- 1 (t-distribution). On the other hand, MCM, which provides a reliable probability density function of GHC through numerical approximation, gives a half width interval of 0. 32 MJ kg- 1. Thus, the GUM approach underestimates the calculated uncertainties and coverage intervals by up to 7 - 12 %. The main reasons of these differences are the approximations and the assumptions introduced by GUM approach, i. e. assumption for the GHC probability distribution and overestimation of effective degrees of freedom by the Welch-Satterwaite formula. Moreover, the estimation {{and the use of}} sensitivity coefficients and uncertainty budget within GUM and MCM approaches are examined. © 2011 Elsevier B. V. All rights reserved...|$|E
40|$|Abstract—In this paper, {{we present}} a novel general {{methodology}} which ensure a minimum uncertainty in the measurement of the real part of the permittivity of a material measured using cylindrical shielded dielectric resonators. The method {{is based on the}} fact that for any given value of the dielectric permittivity there is an optimal radius of the cylindrical dielectric rod sample. When the dielectric rod sample has the optimum radius, the width of the <b>coverage</b> <b>interval</b> associated to the real part of the dielectric permittivity measurement result — for a given confidence level — is reduced due to a lower sensitivity of the dielectric permittivity to be measured versus the variations in the resonant frequency. The appropriated radius of a given sample under test is calculated using Monte Carlo simulations for a specific mode and a specific resonant frequency. The results show that the confidence interval could be reduced by one order of magnitude with respect to its maximum width predicted by the uncertainty estimation performed using the Monte Carlo method (MCM) as established by the supplement 1 of the Guide to the Expression of Uncertainty in Measurement (GUM). The optimum radius of the sample under examination is fundamentally determined by the electromagnetic equations that describe the measurement and does not depend specifically of the sources of uncertainty considered...|$|E
40|$|In {{this paper}} {{we present a}} novel general {{methodology}} which ensure a minimum uncertainty in the measurement of the real part of the permittivity of a material measured using cylindrical shielded dielectric resonators. The method {{is based on the}} fact that for any given value of the dielectric permittivity there is an optimal radius of the cylindrical dielectric rod sample. When the dielectric rod sample has the optimum radius, the width of the <b>coverage</b> <b>interval</b> associated to the real part of the dielectric permittivity measurement result [...] - for a given confidence level [...] - is reduced due to a lower sensitivity of the dielectric permittivity to be measured versus the variations in the resonant frequency. The appropriated radius of a given sample under test is calculated using Monte Carlo simulations for a specific mode and a specific resonant frequency. The results show that the confidence interval could be reduced by one order of magnitude with respect to its maximum width predicted by the uncertainty estimation performed using the Monte Carlo method (MCM) as established by the supplement 1 of the Guide to the Expression of Uncertainty in Measurement (GUM). The optimum radius of the sample under examination is fundamentally determined by the electromagnetic equations that describe the measurement and does not depend specifically of the sources of uncertainty considered. Postprint (published version...|$|E
40|$|Using an Edgeworth {{expansion}} {{to speed}} up the asymptotics, we develop one-sided <b>coverage</b> <b>intervals</b> for a proportion based on a stratified simple random sample. To this end, we assume the values of the population units are generated from independent random variables with a common mean within each stratum. These stratum means, in turn, may either be free to vary or are assumed to be equal. The more general assumption is equivalent to a model-free randomization-based framework when finite population correction is ignored. Unlike when an Edgeworth expansion is used to construct one-sided intervals under simple random sampling, it is necessary to estimate the variance of the estimator for the population proportion when the stratum means are allowed to differ. As a result, there may be accuracy gains from replacing the normal "z"-score in the Edgeworth expansion with a "t"-score. Copyright (c) 2009 The Authors. Journal compilation (c) 2009 International Statistical Institute. ...|$|R
5000|$|If all {{assumptions}} used in deriving {{a confidence}} interval are met, the nominal coverage probability will equal the coverage probability (termed [...] "true" [...] or [...] "actual" [...] coverage probability for emphasis). If any assumptions are not met, the actual coverage probability could either {{be less than}} or greater than the nominal coverage probability. When the actual coverage probability {{is greater than the}} nominal <b>coverage</b> probability, the <b>interval</b> is termed [...] "conservative", if it is less than the nominal <b>coverage</b> probability, the <b>interval</b> is termed [...] "anti-conservative", or [...] "permissive." ...|$|R
40|$|This paper {{investigates the}} effect of {{relevant}} physical parameters on transient temperature elevation induced in human tissues by electromagnetic waves in the terahertz (THz) band. The problem is defined by assuming a plane wave, which, during a limited time interval, normally impinges {{on the surface of}} a 3 -layer model of the human body, causing a thermal transient. The electromagnetic equations are solved analytically, while the thermal ones are handled according to the finite element method. A parametric analysis is performed with the aim of identifying the contribution of each parameter, showing that the properties of the first skin layer (except blood flow) {{play a major role in}} the computation of the maximum temperature rise for the considered exposure situation. Final results, obtained by combining all relevant parameters together, show that the deviation from the reference solution of the maximum temperature elevation in skin is included in the <b>coverage</b> <b>intervals</b> from 30...|$|R
