17|161|Public
5000|$|<b>Cached</b> <b>page</b> from Google of {{nominations}} {{from the}} Vloggies 2006 ...|$|E
50|$|To give a {{concrete}} example, assume the page in consideration takes 3 seconds to render {{and we have}} a traffic of 10 requests per second. Then, when the <b>cached</b> <b>page</b> expires, we have 30 processes simultaneously recomputing the rendering of the page and updating the cache with the rendered page.|$|E
50|$|Between {{visits by}} the spider, the cached version of page (some {{or all the}} content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case the page may differ from the search terms indexed. The <b>cached</b> <b>page</b> holds the {{appearance}} of the version whose words were indexed, so a cached version of a page can be useful to the web site when the actual page has been lost, but this problem is also considered a mild form of linkrot.|$|E
5000|$|Some {{search engines}} keep <b>cached</b> <b>pages,</b> copies of {{previously}} indexed Web pages, and these pages {{are not always}} blocked. <b>Cached</b> <b>pages</b> may be identified with a small link labeled [...] "cached" [...] in a list of search results. Google allows the retrieval of <b>cached</b> <b>pages</b> by entering [...] "cache:some-blocked-url" [...] as a search request.|$|R
5000|$|Layered caching scheme, which {{supports}} data <b>caching,</b> <b>page</b> <b>caching,</b> fragment caching and dynamic content. The storage medium of caching can be changed.|$|R
5000|$|He {{programmed}} wp-cache, a WordPress plugin for {{the purpose}} of <b>caching</b> <b>pages</b> to make one's blog [...] "faster and more responsive".|$|R
5000|$|The page cache also aids {{in writing}} to a disk. Pages {{in the main}} memory that have been {{modified}} during writing data to disk are marked as [...] "dirty" [...] {{and have to be}} flushed to disk before they can be freed. When a file write occurs, the <b>cached</b> <b>page</b> for the particular block is looked up. If it is already found in the page cache, the write is done to that page in the main memory. If it is not found in the page cache, then, when the write perfectly falls on page size boundaries, the page is not even read from disk, but allocated and immediately marked dirty. Otherwise, the page(s) are fetched from disk and requested modifications are done. A file that is created or opened in the page cache, but not written to, might result in a zero byte file at a later read.|$|E
50|$|If {{the page}} working set {{does not fit}} into the TLB, then TLB {{thrashing}} occurs, where frequent TLB misses occur, with each newly <b>cached</b> <b>page</b> displacing one that will soon be used again, degrading performance {{in exactly the same}} way as thrashing of the instruction or data cache does. TLB thrashing can occur even if instruction cache or data cache thrashing are not occurring, because these are cached in different size units. Instructions and data are cached in small blocks (cache lines), not entire pages, but address lookup is done at the page level. Thus even if the code and data working sets fit into cache, if the working sets are fragmented across many pages, the virtual address working set may not fit into TLB, causing TLB thrashing. Appropriate sizing of the TLB thus requires considering not only the size of the corresponding instruction and data caches, but also how these are fragmented across multiple pages.|$|E
30|$|If the PS {{receives}} a webpage, it stores {{a copy of}} this page as is, and delivers it to the browser. Else, it modifies the links on a copy of its <b>cached</b> <b>page</b> to include the SID received from Server A, and sends it to the browser along with the Referrer field value it received from A.|$|E
5000|$|Counting is {{activated}} by opening the page (given that the web client runs the tag scripts), not requesting {{it from the}} server. If a <b>page</b> is <b>cached,</b> {{it will not be}} counted by server-based log analysis. <b>Cached</b> <b>pages</b> can account for up to one-third of all page views. Not counting <b>cached</b> <b>pages</b> seriously skews many site metrics. It is for this reason server-based log analysis is not considered suitable for analysis of human activity on websites.|$|R
5000|$|LIRS organizes {{metadata}} of <b>cached</b> <b>pages</b> {{and some}} uncached pages and conducts its replacement operations described as below, {{which are also}} illustrated with an example [...] in the graph.|$|R
50|$|However, not all <b>cached</b> <b>pages</b> can {{be written}} to as program code is often mapped as {{read-only}} or copy-on-write; in the latter case, modifications to code will only be visible to the process itself {{and will not be}} written to disk.|$|R
40|$|G o o g l e's cache is the {{snapshot}} {{that we took}} of {{the page}} as we crawled the web. The page may have changed since that time. Click here for the current page without highlighting. This <b>cached</b> <b>page</b> may reference images which are no longer available. Click here for the cached text only. Google is neither affiliated with {{the authors of this}} page nor responsible for its content...|$|E
30|$|Finally, {{when the}} new object {{is added to the}} cache, SACS parses it in the {{background}} to extract its link information and updates the link information in the page graph. As for the pages that are evicted, the cache does not remove their information from the graph immediately, as it may still be useful to compute another page’s distance. Instead, this information is garbage collected either when no other <b>cached</b> <b>page</b> links to the removed page, or when memory becomes scarce.|$|E
30|$|Our {{algorithm}} comprises {{two main}} phases: monitoring and eviction. In the monitoring phase (see Algorithm 1), the cache collects information about access to pages and user requests. The algorithm keeps, for each <b>cached</b> <b>page,</b> the page’s hit count while cached (frequency) and the timestamp {{of the latest}} access to the page (recency). On a cache hit, the frequency and recency information of the requested object is updated and its cached version is returned to the user. If, on the other hand, the requested object is not cached, the system gets it from the origin server before returning it to the requesting user. In addition, a cache miss triggers the eviction phase of the algorithm if the requested object does not fit in the available free space.|$|E
5000|$|Mozilla Firefox {{implements}} a registered, strictly internal [...] {{uniform resource identifier}} (URI) {{scheme to}} sort and later reference locally <b>cached</b> <b>pages</b> that were generated or modified by a script on the client side (a common practice for Web 2.0 sites).|$|R
5000|$|WebCite checks robots.txt only at {{the time}} of archiving, Internet Archive checks robots.txt occasionally, so changes in robots.txt (which can be caused by a change in the {{ownership}} of the domain name) can result in removing the <b>cached</b> <b>pages</b> from the Internet Archive ...|$|R
5000|$|Since <b>cached</b> <b>pages</b> can {{be easily}} evicted and re-used, some {{operating}} systems, notably Windows NT, even report the page cache usage as [...] "available" [...] memory, while the memory is actually allocated to disk pages. This has led to some confusion about the utilization of page cache in Windows.|$|R
40|$|A popular {{technique}} {{to improve the}} scalabil-ity of a web based system is caching at proxy servers. Caching has the drawback that a <b>cached</b> <b>page</b> becomes stale when the page is updated at the web server. In some cases, stal-eness may not be completely avoided because the server may not wish to expend the pro-cessing and communication resources required to transmit all the updates immediately. In general, if updates are transmitted less fre-quently, the staleness will tend to increase, but the amount of resources consumed will be reduced. The tradeoff between resource con-sumption and staleness is investigated. A mea-sure of staleness is defined and optimization problems are formulated. The solutions to these problems allow one {{to come up with}} an optimal page transmission strategy. Numeri-cal examples showing the resource consump-tion/staleness tradeoff are presented. ...|$|E
30|$|Due to the {{importance}} of the replacement algorithm for the caching system, a large body of work in the area of cache replacement can be found in literature. According to a survey by Podlipnig and Böszörmenyi [9], these algorithms can be grouped into five different categories. The first category consists of recency based algorithms [16, 17, 29], which use the time elapsed since the last request to a <b>cached</b> <b>page</b> as the main factor in their replacement decisions. LRU is the most representative of such algorithms and other algorithms are usually extensions of standard LRU. For example, LRU-min [16] tries to minimize the number of removed pages by applying LRU to a candidate list composed solely of pages that are larger than the most recently requested page. The main drawback of these algorithms is, as mentioned throughout the paper, {{the fact that they are}} oblivious to page popularity.|$|E
40|$|This paper {{presents}} a simple approach to utilize past test collections as a material for user experiments. We {{have built a}} Web-based user interface for NTCIR- 5 WEB run results, and conducted a user experiment with 29 subjects to investigate whether performance evaluation metrics of information retrieval systems used in test collections such as TREC and NT-CIR comparable to user performance. In this experiment, we selected three types of systems from among systems that participated in NTCIR- 5 WEB, and then selected three topics with roughly the same values from among several search topics. The results of the experiment showed no significant differences among these systems and topics in the time for search. While, in general, the user experiment itself have been successfully conducted and shown similar trends with prior study, the approach seems to have some limitations mainly on interactivity and <b>cached</b> <b>page</b> display...|$|E
40|$|The RAMpage {{hierarchy}} moves {{main memory}} up a level {{to replace the}} lowest-level cache by an equivalent-sized SRAM main memory, with a TLB <b>caching</b> <b>page</b> translations for that main memory. This paper illustrates how more aggressive components higher in the hierarchy increase the fraction of total execution time spent waiting for DRAM...|$|R
40|$|We {{present a}} {{consistent}} and transparent caching system for dynamic web pages {{produced by a}} serverside application using a back-end database. <b>Cached</b> <b>pages</b> always reflect current database values. No intervention from the programmer is necessary to implement caching. The system is an improvement on earlier methods that either did not guarantee consistency and/or relied on substantial programmer intervention...|$|R
50|$|Prior to January 2011 the Ben Ali regime had blocked {{thousands}} of websites (such as pornography, mail, search engine <b>cached</b> <b>pages,</b> online documents conversion and translation services) and peer-to-peer and FTP transfer using a transparent proxy and port blocking. Cyber dissidents including pro-democracy lawyer Mohammed Abbou were jailed by the Tunisian government for their online activities.|$|R
40|$|Information {{delivery}} via the web {{has become}} very popular. Along with a growing user population, systems increasingly are supporting content that changes frequently, personalised information, and differentiation and choice. This thesis {{is concerned with the}} design and evaluation of resource management strategies for such systems. An architecture that provides scalability through caching proxies is considered. When a <b>cached</b> <b>page</b> is updated at the server, the cached copy may become stale if the server is not able to transmit the update to the proxies immediately. From the perspective of the server, resources are required to transmit updates for cached pages and to process requests for pages that are not cached. Analytic results on how the available resources should be managed in order to minimise staleness-related cost are presented. An efficient algorithm that the server can use to determine the set of pages that should be cached and a policy for transmitting updates for these pages are also presented. We then apply these results to page fragments, a technique that can provide increased efficiency for delivery of personalised pages...|$|E
40|$|Abstract A major {{overhead}} {{in software}} DSM (Distributed Shared Memory) {{is the cost}} of remote memory accesses necessitated by the protocol as well as induced by false sharing. This paper introduces a dynamic prefetching method implemented in the JIAJIA software DSM to reduce system overhead caused by remote accesses. The prefetching method records the interleaving string of INV (invalidation) and GETP (getting a remote page) operations for each <b>cached</b> <b>page</b> and analyzes the periodicity of the string when a page is invalidated on a lock or barrier. A prefetching request is issued after the lock or barrier if the periodicity analysis indicates that GETP will be the next operation in the string. Multiple prefetching requests are merged into the same message if they are to the same host. Performance evaluation with eight well-accepted benchmarks in a cluster of sixteen PowerPC workstations shows that the prefetching scheme can signicantly reduce the page fault overhead and as a result achieves a performance increase of 15 %{ 20 % in three benchmarks and around 8 %{ 10 % in another three. The average extra traÆc caused by useless prefetches is only 7 %{ 13 % in the evaluation...|$|E
40|$|This project adds new cache-related {{features}} to Yioop, an Open Source, PHP-based search engine. Search engines often maintain caches of pages downloaded by their crawler. Commercial search engines like Google display {{a link to}} the cached version of a web page along with the search results for that particular web page. The first feature enables users to navigate through Yioop 2 ̆ 7 s entire cache. When a <b>cached</b> <b>page</b> is displayed along with its contents, links to cached pages saved in the past are also displayed. The feature also enables users to navigate cache history based on year and month. This feature is similar in function to the Internet Archive as it maintains snapshots of the web taken at different times. The contents of a web page can change over time. Thus, a search engine caching web pages has {{to make sure that the}} cached pages are fresh. The second feature of this project implements cache validation using information obtained from web page headers. The cache validation mechanism is implemented using Entity Tags and Expires header. The cache validation feature is then tested for effect on crawl speed and savings in bandwidth...|$|E
40|$|Write {{detection}} {{is essential}} in multiple writer protocols to identify writes to shared pages so that these writes can be correctly propagated. This paper studies different write detection schemes in a home-based software DSM system called JIAJIA. It compares the performance of three write detection schemes: the traditional virtual memory page fault write detection scheme which write-protects both home and <b>cached</b> <b>pages</b> {{at the beginning of}} an interval, a cache only write detection scheme which does not detect writes to home pages but invalidates all <b>cached</b> <b>pages</b> at the beginning of an interval, and an API write detection scheme which requires the programmer or pre-complier to explicitly records writes in program. Evaluation with some well-known DSM benchmarks reveals that tradeoffs of different write detection schemes vary with data (home) distribution and memory reference patterns of applications. 1 Introduction Most recent software DSM systems employ the multiple write [...] ...|$|R
5000|$|... in write-around mode FlashCache has no {{information}} {{to compare the}} age of <b>cached</b> <b>pages</b> over the on-disk ones. (1) Because the device could have been mounted outside of FlashCache (2) Because no writes are tracked in this mode. This results in an empty cache after each volume activation (i.e.: reboot). Performance will be degraded until all hot areas have been cached.|$|R
40|$|This paper {{describes}} {{the system and}} key techniques used for achieving performance and high availability at the official Web site for the 1998 Olympic Winter Games {{which was one of}} the most popular Web sites for the duration of the Olympic Games. The Web site utilized thirteen SP 2 systems scattered around the globe containing a total of 143 processors. A key feature of the Web site was that the data being presented to clients was constantly changing. Whenever new results were entered into the system, updated Web pages reflecting the changes were made available to the rest of the world within seconds. One technique we used to serve dynamic data efficiently to clients was to cache dynamic pages so that they only had to be generated once. We developed and implemented a new algorithm we call Data Update Propagation (DUP) which identifies the <b>cached</b> <b>pages</b> that have become stale as a result of changes to underlying data on which the <b>cached</b> <b>pages</b> depend, such as databases. For the Olympic G [...] ...|$|R
40|$|AbstractControl flow {{is often}} key problem in current web applications. For example, using the back button gives a POSTDATA error, using {{multiple}} windows books the wrong hotel, and sending {{a link to}} a friend does not work. Previous solutions used continuations {{as a model for}} user interaction. However continuations are insufficient as a model of all web interactions. We believe the protocol and browsers themselves are insufficiently powerful to represent the control flow desired in a web application. Our solution is to extend the protocol and browser sufficiently that these problems can be avoided. We seek to be agnostic about how web applications are written and instead recognise that many of the problems stem from underlying weaknesses in the protocol. As an example, the application {{ought to be able to}} inform the browser that pressing back on a payment confirmation page is not allowed. Instead, the <b>cached</b> <b>page</b> can be displayed in a read-only, archive fashion to the user, or a new page can be shown instead which is consistent with the global state. We discuss how some of these ideas may be implemented within the existing HTTP/ 1. 1 protocol; and what modest extensions to the protocol would enable full implementation. We also discuss the interaction with Web 2. 0 and the security and privacy implications of our extensions...|$|E
5000|$|... multi-layered <b>caching</b> of <b>page</b> output (for XML-{{output of}} page modules, output of boxes, full pages {{and support for}} memcached servers and xslcache) ...|$|R
50|$|In computing, a page cache, {{sometimes}} {{also called}} disk cache, is a transparent cache for the pages originating from a secondary storage device {{such as a}} hard disk drive (HDD). The operating system keeps a page cache in otherwise unused portions of the main memory (RAM), resulting in quicker access to the contents of <b>cached</b> <b>pages</b> and overall performance improvements. A page cache is implemented in kernels with the paging memory management, and is mostly transparent to applications.|$|R
3000|$|The {{distance}} metric {{is a novel}} element {{introduced by}} our approach. It measures the distance between two objects {{in terms of the}} minimum number of links that need to be followed in order to navigate from one object to the other. In other words, given the web objects graph G=(V,E), where V is the set of <b>cached</b> <b>pages</b> and E is the set of links between them, distance broadly corresponds to the length of the shortest navigation path between pages x [...]...|$|R
50|$|Since 1996, the Wayback Machine {{has been}} {{archiving}} <b>cached</b> <b>pages</b> of websites onto its large cluster of Linux nodes. It revisits sites {{every few weeks}} or months and archives a new version. Sites can also be captured on the fly by visitors who enter the site's URL into a search box. The intent is to capture and archive content that otherwise would be lost whenever a site is changed or closed down. The overall vision of the machine's creators is to archive the entire Internet.|$|R
40|$|Abstract—As dynamic content becomes {{increasingly}} dominant, {{it becomes an}} important research topic as how the edge resources such as client-side proxies, which are otherwise underutilized for such content, can be put into use. However, {{it is unclear what}} will be the best strategy, and the design/deployment trade offs lie therein. In this paper, using one representative e-commerce benchmark, we report our experience of an extensive investigation of different offloading and caching options. Our results point out that, while great benefits can be reached in general, advanced offloading strategies can be overly complex and even counterproductive. In contrast, simple augmentation at proxies to enable fragment <b>caching</b> and <b>page</b> composition achieves most of the benefit without compromising important considerations such as security. We also present Proxy+ architecture which supports such capabilities for existing Web applications with minimal reengineering effort. Index Terms—Edge caching, offloading, dynamic content, fragment <b>caching,</b> <b>page</b> composition. ...|$|R
