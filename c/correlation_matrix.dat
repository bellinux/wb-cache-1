3624|1507|Public
25|$|The <b>correlation</b> <b>matrix</b> is {{symmetric}} {{because the}} correlation between X'i and X'j {{is the same as}} {{the correlation between}} X'j andX'i.|$|E
25|$|A <b>correlation</b> <b>matrix</b> appears, for example, in one {{formula for}} the {{coefficient}} of multiple determination, a measure of goodness of fit in multiple regression.|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the covariance matrix or the <b>correlation</b> <b>matrix</b> (in which each variable is scaled to have its sample variance equal to one). For the covariance or <b>correlation</b> <b>matrix,</b> the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the <b>correlation</b> <b>matrix</b> provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
50|$|These {{empirical}} sample <b>correlation</b> <b>matrices</b> are {{the most}} straightforward and most often used estimators for the <b>correlation</b> <b>matrices,</b> but other estimators also exist, including regularised or shrinkage estimators, which may have better properties.|$|R
40|$|We {{study the}} time {{dependence}} of maximal spanning trees and asset graphs based on <b>correlation</b> <b>matrices</b> of stock returns. In these networks the nodes represent companies and links {{are related to}} the correlation coefficients between them. Special emphasis is given to the comparison between ordinary and denoised <b>correlation</b> <b>matrices.</b> The analysis of single- and multi-step survival ratios of the corresponding networks reveals that the ordinary <b>correlation</b> <b>matrices</b> are more stable in time than the denoised ones. Our study also shows that some information about the cluster structure of the companies is lost in the denoising procedure. Cluster structure that makes sense from an economic point of view exists, and can easily be observed in networks based on denoised <b>correlation</b> <b>matrices.</b> However, this structure is somewhat clearer in the networks based on ordinary <b>correlation</b> <b>matrices.</b> Some technical aspects, such as the random matrix denoising procedure, are also presented. Comment: 23 pages, 12 figures, 1 table. v 2 : 3 references added, minor revision...|$|R
40|$|In {{a recent}} paper, {{the concept of}} {{synchronous}} quantum <b>correlation</b> <b>matrices</b> was introduced and these were shown to correspond to traces on certain C*-algebras. In particular, synchronous <b>correlation</b> <b>matrices</b> arose {{in their study of}} various versions of quantum chromatic numbers of graphs and other quantum versions of graph theoretic parameters. In this paper we develop these ideas further, focusing on the relations between synchronous <b>correlation</b> <b>matrices</b> and microstates. We prove that Connes' embedding conjecture is equivalent to the equality of two families of synchronous quantum <b>correlation</b> <b>matrices.</b> We prove that if Connes' embedding conjecture has a positive answer, then the tracial rank and projective rank are equal for every graph. We then apply these results to more general non-local games. Comment: 10 page...|$|R
25|$|A {{special case}} of {{generalized}} least squares called weighted least squares occurs {{when all the}} off-diagonal entries of Ω (the <b>correlation</b> <b>matrix</b> of the residuals) are null; the variances of the observations (along the covariance matrix diagonal) may still be unequal (heteroscedasticity).|$|E
25|$|Principal {{component}} analysis {{is used to}} study large data sets, such as those encountered in bioinformatics, data mining, chemical research, psychology, and in marketing. PCA is popular especially in psychology, {{in the field of}} psychometrics. In Q methodology, the eigenvalues of the <b>correlation</b> <b>matrix</b> determine the Q-methodologist's judgment of practical significance (which differs from the statistical significance of hypothesis testing; cf. criteria for determining the number of factors). More generally, principal {{component analysis}} {{can be used as a}} method of factor analysis in structural equation modeling.|$|E
2500|$|For a given <b>correlation</b> <b>matrix</b> , the Gaussian copula with {{parameter}} matrix [...] can {{be written}} as ...|$|E
40|$|Correlation {{coefficients}} among multiple {{variables are}} commonly {{described in the}} form of matrices. Applications of such <b>correlation</b> <b>matrices</b> can be found in many fields, such as finance, engineering, statistics, and medicine. This article proposes an efficient way to sequentially obtain the theoretical bounds of correlation coefficients together with an algorithm to generate n | n <b>correlation</b> <b>matrices</b> using any bounded random variables. Interestingly, the <b>correlation</b> <b>matrices</b> generated by this method using uniform random variables as an example produce more extreme relationships among the variables than other methods, which might be useful for modeling complex biological systems where rare cases are very important...|$|R
40|$|We {{calculate}} the `one-point function', meaning the marginal probability density function for any single eigenvalue, of real and complex Wishart <b>correlation</b> <b>matrices.</b> No explicit expression had been {{obtained for the}} real case so far. We succeed in doing so by using supersymmetry techniques to express the one-point function of real Wishart <b>correlation</b> <b>matrices</b> as a twofold integral. The result {{can be viewed as}} a resummation of a series of Jack polynomials in a non-trivial case. We illustrate our formula by numerical simulations. We also rederive a known expression for the one-point function of complex Wishart <b>correlation</b> <b>matrices.</b> Comment: 21 pages, 2 figure...|$|R
30|$|The only missing {{parameters}} {{of this system}} of equations are the <b>correlation</b> <b>matrices.</b>|$|R
2500|$|The <b>correlation</b> <b>matrix</b> of n random {{variables}} X1, ..., X'n is the n×n matrix whose i,j entry is corr(X'i,X'j). [...] If {{the measures of}} correlation used are product-moment coefficients, the <b>correlation</b> <b>matrix</b> {{is the same as}} the covariance matrix of the standardized {{random variables}} [...] for [...] [...] This applies to both the matrix of population correlations (in which case ' is the population standard deviation), and to the matrix of sample correlations (in which case ' denotes the sample standard deviation). Consequently, each is necessarily a positive-semidefinite matrix. Moreover, the <b>correlation</b> <b>matrix</b> is strictly positive definite if no variable can have all its values exactly generated as a linear function of the values of the others.|$|E
2500|$|Since , the {{weighted}} network adjacency is linearly {{related to the}} co-expression similarity on a logarithmic scale. Note that a high power [...] transforms high similarities into high adjacencies, while pushing low similarities towards 0. Since this soft-thresholding procedure applied to a pairwise <b>correlation</b> <b>matrix</b> leads to weighted adjacency matrix, the ensuing analysis {{is referred to as}} weighted gene co-expression network analysis.|$|E
2500|$|... where [...] is {{the inverse}} {{cumulative}} distribution function of a standard normal and [...] is the joint {{cumulative distribution function}} of a multivariate normal distribution with mean vector zero and covariance matrix equal to the <b>correlation</b> <b>matrix</b> [...] While there is no simple analytical formula for the copula function, , it can be upper or lower bounded, and [...] approximated using numerical integration. The density can be written as ...|$|E
40|$|AbstractWe extend {{and improve}} two {{existing}} methods of generating random <b>correlation</b> <b>matrices,</b> the onion method of Ghosh and Henderson [S. Ghosh, S. G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276 – 294] and the recently proposed method of Joe [H. Joe, Generating random <b>correlation</b> <b>matrices</b> based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177 – 2189] based on partial correlations. The latter {{is based on}} the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random <b>correlation</b> <b>matrices</b> from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random <b>correlation</b> <b>matrices</b> of given dimensions...|$|R
5000|$|...Regular vines {{have proven}} useful in other {{problems}} such as (constrained) sampling of <b>correlation</b> <b>matrices</b> ...|$|R
40|$|AbstractMultivariate {{asymptotic}} (normal) distributions for eigenvalues and unit-length eigenvectors {{of sample}} variance and <b>correlation</b> <b>matrices</b> are derived. Beside the general case, when {{existence of the}} (finite) fourth-order moments of the population distribution is assumed, formulae for the asymptotic variance matrices {{in the cases of}} normal and elliptical populations are also derived. It is assumed throughout that population variance and <b>correlation</b> <b>matrices</b> are nonsingular and without multiple eigenvalues...|$|R
5000|$|The goal {{of factor}} {{analysis}} {{is to choose}} the fitting hyperplane such that the reduced <b>correlation</b> <b>matrix</b> reproduces the <b>correlation</b> <b>matrix</b> as nearly as possible, except for the diagonal elements of the <b>correlation</b> <b>matrix</b> which {{are known to have}} unit value. In other words, the goal is to reproduce as accurately as possible the cross-correlations in the data. Specifically, for the fitting hyperplane, the mean square error in the off-diagonal components ...|$|E
5000|$|Possible {{consequences}} of QLC {{have been investigated}} in the literature and in particular a simple correspondence between the PMNS and CKM matrices have been proposed and analyzed {{in terms of a}} <b>correlation</b> <b>matrix.</b> The <b>correlation</b> <b>matrix</b> VM is simply defined as the product of the CKM and PMNS matrices: ...|$|E
5000|$|... is {{the sample}} <b>correlation</b> <b>matrix</b> because the [...] 's are normalized.|$|E
40|$|Abstract. High-dimensional {{statistical}} tests often ignore correlations to gain {{simplicity and}} stability leading to null distributions {{that depend on}} functionals of <b>correlation</b> <b>matrices</b> such as their Frobenius norm and other `r norms. Motivated by the computation of critical values of such tests, we investigate the difficulty of estimation the functionals of sparse <b>correlation</b> <b>matrices.</b> Specifically, we show that simple plug-in procedures based on thresholded estimators of <b>correlation</b> <b>matrices</b> are sparsity-adaptive and minimax optimal over a large class of corre-lation matrices. Akin to previous results on functional estimation, the minimax rates exhibit an elbow phenomenon. Our results are further il-lustrated in simulated data {{as well as an}} empirical study of data arising in financial econometrics...|$|R
40|$|It {{has been}} empirically {{observed}} that <b>correlation</b> <b>matrices</b> of forward {{interest rates have}} the first three eigenvalues which are simple and their corresponding eigenvectors, termed as shift, slope and curvature respectively, with elements presenting changes of sign in a regular way. These spectral properties {{are very similar to}} those exhibited by Strictly Totally Positive and Oscillatory matrices. In the present paper we investigate how these spectral properties are related with those characterizing the <b>correlation</b> <b>matrices</b> considered, i. e. the positivity and the monotonicity of their elements. On the basis of these relations we prove the simplicity of the first two eigenvalues and provide an estimate of the second one. Forward rates, <b>Correlation</b> <b>matrices,</b> Principal Component Analysis, Total Positivity...|$|R
40|$|This paper {{concerns}} the facial {{geometry of the}} set of n × n <b>correlation</b> <b>matrices.</b> The main result states that almost every set of r vertices generates a simplicial face, provided that r ≤√(c n), where c is an absolute constant. This bound is qualitatively sharp because the set of <b>correlation</b> <b>matrices</b> has no simplicial face generated by more than √(2 n) vertices. Comment: 12 pages, 2 figure...|$|R
5000|$|In some {{applications}} (e.g., building {{data models}} from only partially observed data) {{one wants to}} find the [...] "nearest" [...] covariance matrix or <b>correlation</b> <b>matrix</b> to a given symmetric matrix (e.g., of observed covariances). In 2002, Higham formalized the notion of nearness using a weighted Frobenius norm and provided a method for computing the nearest <b>correlation</b> <b>matrix.</b>|$|E
5000|$|With Rayleigh fading, the Kronecker model {{means that}} the channel matrix can be factorized aswhere the {{elements}} of [...] are independent and identically distributed as circular symmetric complex Gaussian with zero-mean and unit variance. The {{important part of the}} model is that [...] is pre-multiplied by the receive-side spatial <b>correlation</b> <b>matrix</b> [...] and post-multiplied by transmit-side spatial <b>correlation</b> <b>matrix</b> [...]|$|E
5000|$|... {{the robust}} {{technology}} of improving <b>correlation</b> <b>matrix</b> stipulation with {{the availability of}} errors in their elements; ...|$|E
40|$|Asymptotic {{chi-squared test}} {{statistics}} for testing {{the equality of}} moment vectors are developed. The test statistics proposed are generalized Wald test statistics that specialize for different settings by inserting an appropriate asymptotic variance matrix of sample moments. Scaled test statistics are also considered for dealing with nonstandard conditions. The specialization {{will be carried out}} for testing the equality of multinomial populations, and the equality of variance and <b>correlation</b> <b>matrices</b> for both normal and nonnormal data. When testing the equality of <b>correlation</b> <b>matrices,</b> a scaled version of the normal theory chi-squared statistic is proven to be an asymptotically exact chi-squared statistic in the case of elliptical data. chi-squared test distribution-free ellipticity moment vectors multinomial distribution normal-theory scaled test statistic variance and <b>correlation</b> <b>matrices</b> Wald test...|$|R
40|$|For a large {{class of}} quantum systems the {{statistical}} properties of their spectrum show remarkable agreement with random matrix predictions. Recent advances {{show that the}} scope of random matrix theory is much wider. In this work, we show that the random matrix approach can be beneficially applied to a completely different classical domain, namely, to the empirical <b>correlation</b> <b>matrices</b> obtained from {{the analysis of the}} basic atmospheric parameters that characterise the state of atmosphere. We show that the spectrum of atmospheric <b>correlation</b> <b>matrices</b> satisfy the random matrix prescription. In particular, the eigenmodes of the atmospheric empirical <b>correlation</b> <b>matrices</b> that have physical significance are marked by deviations from the eigenvector distribution. Comment: 8 pages, 9 figs, revtex; To appear in Phys. Rev. ...|$|R
40|$|The Pearsonian {{coefficient}} of correlation {{as a measure of}} association between two variates is highly prone to the deleterious effects of outlier observations (in data). Statisticians have proposed a number of formulas to obtain robust measures of correlation that are considered to be less affected by errors of observation, perturbation or presence of outliers. Spearman’s rho, Blomqvist’s signum, Bradley’s absolute r and Shevlyakov’s median correlation are some of such robust measures of correlation. However, in many applications, <b>correlation</b> <b>matrices</b> that satisfy the criterion of positive semi-definiteness are required. Our investigation finds that while Spearman’s rho, Blomqvist’s signum and Bradley’s absolute r make positive semi-definite <b>correlation</b> <b>matrices,</b> Shevlyakov’s median <b>correlation</b> very often fails to do that. The use of <b>correlation</b> <b>matrices</b> based on Shevlyakov’s formula, therefore, is problematic. ...|$|R
50|$|Despite {{the naive}} {{relations}} between the PMNS and CKM angles,a detailed analysis shows that the <b>correlation</b> <b>matrix</b> is phenomenologically compatible with a tribimaximal pattern, and only marginally with a bimaximal pattern. It is possible to include bimaximal forms of the <b>correlation</b> <b>matrix</b> VM in models with renormalization effects that are relevant, however, only in particular cases with tanβ > 40 and with quasi-degenerate neutrino masses.|$|E
5000|$|The <b>correlation</b> <b>matrix</b> of n random {{variables}} X1, ..., Xn is the n × n matrix whose i,j entry is corr(Xi, Xj). If {{the measures of}} correlation used are product-moment coefficients, the <b>correlation</b> <b>matrix</b> {{is the same as}} the covariance matrix of the standardized {{random variables}} [...] for [...] This applies to both the matrix of population correlations (in which case &sigma; is the population standard deviation), and to the matrix of sample correlations (in which case &sigma; denotes the sample standard deviation). Consequently, each is necessarily a positive-semidefinite matrix. Moreover, the <b>correlation</b> <b>matrix</b> is strictly positive definite if no variable can have all its values exactly generated as a linear function of the values of the others.|$|E
5000|$|... where N is the <b>correlation</b> <b>matrix</b> of {{the noise}} signal. The problem can thus be {{formulated}} as ...|$|E
30|$|Spatial {{correlation}} of MIMO channels is implemented via a Kronecker correlation model with <b>correlation</b> <b>matrices</b> {{as described in}} [68].|$|R
3000|$|See[29]for proof. Similar {{results are}} also {{demonstrated}} in[26], [28]with some {{differences in the}} assumptions on <b>correlation</b> <b>matrices.</b> □ [...]...|$|R
5000|$|Technology of calculating robust norma­lized <b>correlation</b> <b>matrices</b> // Cybernetics and Systems Analysis, New-York, Springer, Vol. 46, No. 1, 2011, pp.152-165 ...|$|R
