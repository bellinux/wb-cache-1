36|425|Public
5000|$|JavaCC, Java <b>Compiler</b> <b>Compiler</b> tm (JavaCC tm) - The Java Parser Generator.|$|E
50|$|OGNL {{began as}} a way to map {{associations}} between front-end components and back-end objects using property names. As these associations gathered more features, Drew Davidson created Key-Value Coding language (KVCL). Luke Blanshard then reimplemented KVCL using ANTLR and started using the name OGNL. The technology was again reimplemented using the Java <b>Compiler</b> <b>Compiler</b> (JavaCC).|$|E
5000|$|Atlas Autocode's second-greatest {{claim to}} fame (after being the {{progenitor}} of Imp and EMAS) was that it had many {{of the features of}} the original [...] "Compiler Compiler". A variant of the AA compiler included run-time support for a top-down recursive descent parser. The style of parser used in the <b>Compiler</b> <b>Compiler</b> was in use continuously at Edinburgh from the 60's until almost the turn of the millennium.|$|E
5000|$|Language translators, an Assembler and an RPG <b>compiler.</b> <b>Compilers</b> for FORTRAN IV and COBOL {{were added}} later.|$|R
50|$|Like most parsers, the LR(1) parser is {{automatically}} generated by <b>compiler</b> <b>compilers</b> like GNU Bison,MSTA, Menhir, HYACC, and LRSTAR.|$|R
5000|$|The Hungarian {{notation}} is redundant when type-checking {{is done by}} the <b>compiler.</b> <b>Compilers</b> for languages providing type-checking {{ensure the}} usage of a variable is consistent with its type automatically; checks by eye are redundant and subject to human error.|$|R
5000|$|An early LALR parser {{generator}} {{and probably the}} most popular one for many years was [...] "yacc" [...] (Yet Another <b>Compiler</b> <b>Compiler),</b> created by Stephen Johnson in 1975 at AT&T Labs. [...] Another, [...] "TWS", was created by Frank DeRemer and Tom Pennello. Today, there are many LALR {{parser generator}}s available, many inspired by and largely compatible with the original Yacc, for example GNU bison, a pun on the original Yacc/Yak. See Comparison of deterministic context-free language parser generators {{for a more detailed}} list.|$|E
50|$|The {{fundamental}} task {{of taking}} input in one language and producing output in a non-trivially different language {{can be understood}} in terms of the core transformational operations of formal language theory. Consequently, some techniques that were originally developed for use in compilers have come to be employed in other ways as well. For example, YACC (Yet Another <b>Compiler</b> <b>Compiler)</b> takes input in Backus-Naur form and converts it to a parser in C. Though it was originally created for automatic generation of a parser for a compiler, yacc is also often used to automate writing code that needs to be modified each time specifications are changed.|$|E
5000|$|JavaCC (Java <b>Compiler</b> <b>Compiler)</b> is an {{open source}} parser {{generator}} and lexical analyzer generator written in the Java programming language. JavaCC is similar to yacc in that it generates a parser from a formal grammar written in EBNF notation. Unlike yacc, however, JavaCC generates top-down parsers. JavaCC can resolve choices based on the next k input tokens, and so can handle LL(k) grammars automatically; by use of [...] "lookahead specifications", it can also resolve choices requiring unbounded look ahead. JavaCC also generates lexical analyzers in a fashion similar to lex. The tree builder that accompanies it, JJTree, constructs its trees from the bottom up.|$|E
50|$|Early in {{the history}} of <b>compilers,</b> <b>compiler</b> {{optimizations}} were not as good as hand-written ones. As <b>compiler</b> technologies have improved, good <b>compilers</b> typically generate code good enough to generally no longer warrant the much higher effort to program hand-optimized code in assembly language (except for in a few special cases).|$|R
40|$|<b>Compiler</b> <b>compilers</b> are in {{widespread}} use, but decompiler <b>compilers</b> are a more novel concept. This paper {{presents an}} approach for the decompilation of object code back to source code using a decompiler generator. An example decompilation is presented. Potential applications include reverse engineering, quality assessment, debugging and safety-critical code validation or verification...|$|R
5000|$|Haxe <b>compiler,</b> a <b>compiler</b> for the {{language}} Haxe, free open source.|$|R
50|$|The XPL {{language}} is a simple, small, efficient dialect of PL/I intended mainly for the task of writing compilers. The XPL language was also used for other purposes once it was available. XPL can be compiled easily to most modern machines by a simple <b>compiler.</b> <b>Compiler</b> internals can be written easily in XPL, and the code is easy to read. The PL/I language was designed by an IBM committee in 1964 as a comprehensive language replacing Fortran, COBOL, and ALGOL, and meeting all customer and internal needs. These ambitious goals made PL/I complex, hard to implement efficiently, and sometimes surprising when used. XPL is a small dialect of the full language. XPL has one added feature not found in PL/I: a STRING datatype with dynamic lengths. String values live in a separate text-only heap memory space with automatic garbage collection of stale values. Much of what a simple compiler does is manipulating input text and output byte streams, so this feature helps simplify XPL-based compilers.|$|E
40|$|Introduction to yacc and bison Handout {{written by}} Maggie Johnson and revised by Julie Zelenski. yacc is a parser generator. It is to parsers what lex is to scanners. You provide {{the input of}} a grammar {{specification}} and it generates an LALR(1) parser to recognize sentences in that grammar. yacc stands for &quot;yet another <b>compiler</b> <b>compiler</b> &quot; and it i...|$|E
40|$|This paper details what's {{new in the}} IBM ® XL C/C++ {{compiler}} family. IBM XL C/C++ for AIX ® and XL C/C++ for Linux compiler {{products are}} the successor to IBM's VisualAge ® C++ <b>compiler.</b> <b>Compiler</b> features vary slightly by operating system platform, and platform-specific features are described in the appropriate sections. The IBM XL C/C++ compilers share the common feature...|$|E
50|$|Chart parsers {{can also}} be used for parsing {{computer}} languages. Earley parsers in particular have been used in <b>compiler</b> <b>compilers</b> where their ability to parse using arbitrary Context-free grammars eases the task of writing the grammar for a particular language. However their lower efficiency has led to people avoiding them for most <b>compiler</b> work.|$|R
5000|$|PP <b>Compiler,</b> a <b>compiler</b> for Palm OS {{that runs}} {{directly}} on the handheld computer.|$|R
50|$|TI {{consulted}} with RedHat to provide official {{support for the}} MSP430 architecture to the GNU <b>Compiler</b> Collection C/C++ <b>compiler.</b> This msp430-elf-gcc <b>compiler</b> is supported by TI's Code Composer Studio version 6.0 and higher.|$|R
40|$|Abstract. This paper {{presents}} {{an extension of}} the Tatoo <b>compiler</b> <b>compiler</b> that supports separate compilation and dynamic linking of formal grammars. It allows the developer to define reusable libraries of grammars such as those of arithmetic expressions or of classical control operators. The aim of this feature is to simplify the development of domain specific languages especially for non specialists in grammar writing. 1...|$|E
40|$|Abstract: We present STARLET, a new <b>compiler</b> <b>compiler</b> which compiles Extended Affix Grammars {{defining}} a translation into an executable program: the translator. We {{look at its}} operational semantics and {{we focus on the}} points which are close to or different from Prolog procedural semantics. We discuss the two interwoven issues which are Program Reliability (due to many static checks) and Program Efficiency (optimizations at compile time). Both are reached through a systematic use of grammatical properties. ...|$|E
40|$|Many {{universities}} and other research organizations have proceeded construction of academic resource repositories {{under the leadership}} of National Institute of Informatics (NII). Because it requires much labor to create metadata contents of academic resources, development of tools or services enabling to easily create metadata contents is expected. This paper proposes a system which converts BIBTEXmetadata of academic resources { widely used by researchers in science and engineering { into NII metadata format in order to support constructions of academic resource repositories. The proposed system enables to reduce almost four-¯fth of workloads of creating metadata contents. In addition, utilizing <b>compiler</b> <b>compiler</b> to develop the proposed system reduced the develop cost...|$|E
50|$|In the 1990s, Hewlett-Packard {{researched}} {{this problem}} {{as a side}} effect of ongoing work on their PA-RISC processor family. They found that the CPU could be greatly simplified by removing the complex dispatch logic from the CPU and placing it in the <b>compiler.</b> <b>Compilers</b> of the day were far more complex than those of the 1980s, so the added complexity in the <b>compiler</b> was considered to be a small cost.|$|R
5000|$|FFC (FEniCS Form <b>Compiler),</b> a <b>compiler</b> for {{finite element}} variational forms taking UFL code as input and {{generating}} UFC output; ...|$|R
5000|$|CAPS <b>Compilers,</b> CAPS Entreprise <b>compilers</b> for hybrid {{computing}} ...|$|R
40|$|Matlab is a proprietary, interactive, dynamically-typed {{language}} for technical computing. It {{is widely used}} for prototyping algorithms and applications of scientific computations. Since it is a dynamically typed language, the execution of programs has to be analyzed and interpreted which results in lower computational performance. In order to increase the performance and integrate with Modelica applications {{it is useful to}} be able to translate Matlab programs to statically typed Modelica programs. This project presents the design and implementation of Matlab to Modelica translator. The Lexical and Syntax analysis is done {{with the help of the}} OMCCp (OpenModelica <b>Compiler</b> <b>Compiler</b> parser generator) tool which generates the Matlab AST, which is later used by the translator for generating readable and reusable Modelica code...|$|E
40|$|It {{is common}} {{practice}} to bootstrap compilers of programming languages. By using the compiled language {{to implement the}} <b>compiler,</b> <b>compiler</b> developers can code in their own high-level language and gain a large-scale test case. In this paper, we investigate bootstrapping of compiler-compilers as they occur in language workbenches. Language workbenches support the development of compilers {{through the application of}} multiple collaborating domain-specific meta-languages for defining a language’s syntax, analysis, code generation, and editor support. We analyze the bootstrapping problem of language workbenches in detail, propose a method for sound bootstrapping based on fixpoint compilation, and show how to conduct breaking meta-language changes in a bootstrapped language workbench. We have applied sound bootstrapping to the Spoofax language workbench and report on our experience. Programming Language...|$|E
40|$|Domain-specific {{languages}} {{improve the}} productivity of application engineers by raising {{the level at which}} they define domain instances. However, more support for designing and implementing these languages in practice is needed. The Software Design Automation (SDA) method is a specific approach for doing domain-specific language design and implementation based on a principled, semantics-based approach to language definition and implementation. This paper articulates SDA as a software development method {{to be used in the}} context of existing domain engineering methods. 1. Introduction Throughout the history of computing, domain-specific languages (DSLs) have codified knowledge and increased {{the productivity of}} software developers. Significant DSLs include the Formula-Translator (FORTRAN), yet another <b>compiler</b> <b>compiler</b> (yacc), spread sheet languages, and hyper-text markup languages. Traditionally, DSLs have been created opportunistically by visionary experts. Also, techniques for designi [...] ...|$|E
5000|$|The ISA manual {{recommends}} that software be optimized to avoid branch stalls {{by using the}} default branch predictions. This reuses the most significant bit of the signed relative address as a [...] "hint bit" [...] to tell whether the conditional branch will be taken or not. So, no other hint bits are needed in the operation codes of RISC-V branches. This makes more bits available in the branch operation codes. Simple, inexpensive CPUs can merely follow the default predictions and still perform well with optimizing <b>compilers.</b> <b>Compilers</b> can still perform statistical path optimization, if desired.|$|R
50|$|Alliant {{offered a}} number of {{software}} packages for its machines, including a solver for linear equations (FX/Skyline Solver), a C <b>compiler</b> (FX/C <b>compiler),</b> and scientific libraries (FX/Linpack and FX/Eispack).|$|R
5000|$|<b>Compiler</b> - Reflexive <b>compiler</b> {{and dynamic}} plugin loader module.|$|R
40|$|Attribute grammars are a {{formalism}} for specifying computations on syntax trees. SSCC (a Sufficiently Smart <b>Compiler</b> <b>Compiler)</b> is {{a practical}} attribute-grammar system based on a polynomial-time extension to Kastens's ordered attribute grammars. The new class of attribute grammars is strictly larger than the class of ordered attribute grammars and it retains the property {{that there is a}} polynomial-time procedure for finding an evaluation order. The SSCC system comprises of two subsystems. The generation subsystem computes the evaluation order of attribute occurrences in production rules and translates attribute equations into low-level code for a virtual stack machine. The evaluation subsystem invokes tools to perform lexical and syntactic analyses and evaluates the attribute instances during a traversal of the syntax tree. Three features make SSCC capable of performing any desired computation (within the constraints of ordered attribute grammars) : user-defined data types, user-defined [...] ...|$|E
40|$|Enhancing the ISE Eiffel Parse library {{enabled us}} to develop YOOCC (Yes! An OO <b>Compiler</b> <b>Compiler)</b> and TROOPER (Truly Reusable OO Parser for Eiffel Re-engineering) written entirely in Eiffel. It is {{concluded}} that not only do these tools allow typical users to develop processors 1 without being concerned with the intricacies behind processor construction, but they significantly contribute towards bringing the ISE Eiffel Parse libraries to their full realisation. We demonstrate this by applying YooCC and T roopeR to a diverse range of realistic domains. 1 Research Motivation Grammar-based processor generation {{is one of the}} most widely studied areas in language processor construction, with current research climaxing at OO attribute grammar systems. Although the ISE Eiffel Parse libraries and OO attribute grammar approaches resolve the deficiencies inherent in using traditional techniques, it has been concluded that existing OO at- 1 We use the same terminology as Meyer (1994). The te [...] ...|$|E
40|$|Abstract- Instead of {{the source}} code level, {{improving}} and maintaining the program code is more efficient when performed on model level. Source code is used to create the program model {{if it is not}} available. The new source code is created after performing the necessary changes in the model. To a certain extent, CASE (Computer Aided Software Engineering) tools are supporting this kind of round-trip engineering process, but in those cases both model elements and accepted source codes are fixed, and modelers can not modify the modeling process. Authors were aiming to implement a development infrastructure which supports user initiated component generation. According to their concept, meta-compilers and metagenerators are employed to create components based on metalevel task descriptions. In the present article authors introduce the process of creating the model {{of the source}} code, and the automatic generation of the program component implementing the model creation. Keywords: Round-trip engineering, Metaprogramming, MOF, <b>Compiler</b> <b>compiler...</b>|$|E
50|$|This sort of test {{also depends}} {{heavily on the}} {{selection}} of a particular programming language, <b>compiler,</b> and <b>compiler</b> options, so algorithms being compared must all be implemented under the same conditions.|$|R
5000|$|... is not {{respected}} by the <b>compiler</b> (ignored by <b>compiler</b> cost/benefit analyzer), and ...|$|R
50|$|A {{metacompiler}} is a metaprogram usually {{written in}} its own metalanguage or an existing computer programming language. The process of a metacompiler, written {{in its own}} metalanguage, compiling itself is equivalent to self-hosting <b>compiler.</b> Most common <b>compilers</b> written today are Self-hosting <b>compilers.</b> Self-hosting is a powerful tool, of many metacompilers, allowing the easy extension of their own metaprogramming metalanguage. The feature that separates a metacompiler apart from other <b>compiler</b> <b>compilers</b> is that it takes as input a specialized metaprogramming language that describes {{all aspects of the}} <b>compilers</b> operation. A metaprogram produced by a metacompiler is as complete a program as a program written in C++, BASIC or any other general programming language. The metaprogramming metalanguage is a powerful attribute allowing the ease of development of computer programming languages and other computer tools. Command line processors, text string transforming and analysis are easily coded using metaprogramming metalanguages of metacompilers.|$|R
