11|14|Public
5000|$|The SAE International G-48 System Safety Committee held The Safety Case Workshop at APT Research in Huntsville, AL on 15 January 2014 {{with several}} DoD {{agencies}} and leading contractors present to further study and capture the Safety Case process and methods for refinement and possible future promulgation in several Safety Standards, as several already use {{as part of}} internal best practices. [...] The G-48, composed of a NASA safety Office, DoD Agencies and several leading defense contractor representatives, cite several evidence based safety advantages of Safety Cases over ANSI/GEA-STD-010 and MIL-STD-882, including 1. Upfront articulation of Arguments (rationale and claims) to be used and (2) independent review to verify and validate. Since Safety Cases are structured, evidence based approaches to satisfy the safety argument established {{at the start of}} programs, they may find a good fit in augmenting existing and proven hazard analyses methods and techniques. It is envisioned as Safety Cases gain popularity and are included in current best practices they will not replace any current effective safety methods, such as Functional Hazard Assessments (FHA), but may be included in those up front and in more comprehensive and blended safety methodologies to argument and improve capturing and documenting objective safety evidence through the program. A final Safety Case should have all of the necessary and required specific artifacts such as test evidence supporting safety claims. A well balanced Safety Case must also allow for special safety directed verification, such as testing of <b>credible</b> <b>failure</b> conditions, testing of malfunctions to observe predicted safe states and planned behavior, fault insertion for expected functionality under worse case conditions, failure immunity to ensure system ignores corruption and rouge threats, and off nominal or modified conditions, out of bounds, and other type test results to prove safety requirements are met outside normal operation.|$|E
40|$|Assessment of the NSTAR {{ion engine}} service life is being {{accomplished}} {{through a combination}} of long duration testing and probabilistic analyses of the <b>credible</b> <b>failure</b> modes. A literature review that examined 65 ion engine endurance tests perfromed over the past 35 years was conducted to compile a list of possible ion engine failure modes...|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy at Loughborough University. Alarms are used on all process plants to alert the operators to abnormal conditions. The current literature {{on the use of}} alarms has been reviewed. It is concluded from this that alarm systems suffer from lack of careful deign. A design procedure is presented which selects the alarms based on the <b>credible</b> <b>failure</b> events to be identified. The effect of the plant operating modes is also taken into account. [Continues. ...|$|E
40|$|To {{provide a}} {{suitable}} electromagnetic torque summing approach to {{flight control system}} redundancy, a four channel motor capable of sustaining full performance after any two <b>credible</b> <b>failures</b> was designed, fabricated, and tested. The design consists of a single samarium cobalt permanent magnet rotor with four separate three phase windings arrayed in individual stator quadrants around the periphery. Trade studies established the sensitivities of weight and performance to such parameters as design speed, winding pattern, number of poles, magnet configuration, and strength. The motor electromagnetically sums the torque of the individual channels on a single rotor and eliminate complex mechanical gearing arrangements...|$|R
50|$|Despite <b>Credible</b> Sport <b>failure,</b> the Honey Badger {{exercises}} {{continued until}} after the 1980 U.S. presidential election, when they became superfluous. Even so, numerous special operations applications and techniques were developed which {{became part of the}} emerging USSOCOM repertoire.|$|R
40|$|This paper {{describes}} {{the application of}} a recently developed CFD program, HYDR- 3 D, to the analysis of separation of the Centaur G-Prime vehicle from the Shuttle Orbiter. The typical application presented illustrates a particularly difficult design task - deployment of a large, liquid-filled, densely packaged vehicle from a manned vehicle. Since it represents a potential catastrophic hazard, a vast number of conditions and parameters must be analyzed to ensure tolerance of at least two <b>credible</b> <b>failures.</b> Validation of the HYDR- 3 D program against zero- and low-gravity experimental data is also presented. Using the fluid dynamics program, this approach can be used confidently to analyze and determine design requirements for a variety of OTV/space-station deployment and docking problems...|$|R
40|$|The LNG {{industry}} in the US is currently facing challenges obtaining approvals for new receiving terminals. A factor of concern at public meetings relates to the potential hazards associated with marine transport accidents or terrorist events. The {{purpose of this study}} is to develop a range of well conceived maximum <b>credible</b> <b>failure</b> cases from accidental or terrorism causes and to predict hazard zones using a well validated model. Hazard zones that are presented in this paper tend to fall below many of the values previously quoted. While additional experimental trials may be warranted, current results are of sufficient confidence to draw valid conclusions...|$|E
40|$|Description of the Proposed Activity/REPORTABLE OCCURRENCE or PIAB: This ECN {{changes the}} {{computer}} systems design description support document describing the computers system used to control, monitor and archive the processes and outputs {{associated with the}} Hydrogen Mitigation Test Pump installed in SY- 101. There is no new activity or procedure associated with the updating of this reference document. The updating of this computer system design description maintains an agreed upon documentation program initiated within the test program and carried into operations at time of turnover to maintain configuration control as outlined by design authority practicing guidelines. There are no new <b>credible</b> <b>failure</b> modes associated with the updating of information in a support description document. The failure analysis of each change was reviewed {{at the time of}} implementation of the Systems Change Request for all the processes changed. This document simply provides a history of implementation and current system status...|$|E
30|$|Next, {{the set of}} secure {{scenarios}} is {{expanded in}} an iterative fashion. The initial set Q_ 0 is combined sequentially with each single credible SPS failure scenario to generate trial sets Q̃_ 1 ^i, where i runs over all included failure scenarios. Partial security candidates κ̃_ 1 ^i are generated for each trial set and the best candidate is selected through enumeration and explicit simulation, according to (3). The winning candidate solution and its corresponding secure scenario set are labeled κ _ 1 and Q_ 1, respectively. In case of multiple best candidates, the method decides on a ‘first come first served’ basis: selecting the first candidate that attains the local optimum. The process proceeds analogously in subsequent stages: single <b>credible</b> <b>failure</b> scenarios are added to Q_ 1 to generate Q̃_ 2 ^i and associated candidate solutions κ̃_ 2 ^i, and the winning candidate solution is denoted by κ _ 2. This algorithm continues until the objective function of κ _j+ 1 at iteration j+ 1 ceases to improve on the previous iteration κ _j.|$|E
40|$|The {{introduction}} of sustainable {{and renewable energy}} sources into traditional networks will be limited {{if we continue to}} use inappropriate methods for security analysis. The probabilistic nature of variable and non-schedulable renewable generation is not well represented in current on-line security assessment schemes. This paper presents a novel method of analyzing and comparing system security schemes and provides initial results of one such scheme. It does so by dynamic simulation of Monte Carlo samples on the IEEE Reliability Test System (IEEE-RTS). It aims to provide information on both how often and how badly the system security scheme fails. After testing on the IEEE-RTS it can be shown that there are <b>credible</b> <b>failures</b> that N- 1 does not consider. It highlights the need for a new security assessment scheme that goes beyond a small deterministic set of test cases...|$|R
5000|$|The largest <b>credible</b> single {{generation}} <b>failure</b> event, {{which is}} currently either Sizewell B nuclear power station (1,260 MW) or one cable of the HVDC Cross-Channel interconnector (1,000 MW) ...|$|R
40|$|Abstract. This {{paper for}} product obeys {{exponential}} distribution, {{in the case}} of data with only one failure, one-sided Bayesian upper credible limit, two-sided Bayesian lower and upper <b>credible</b> limits of <b>failure</b> rate are proposed, moreover, the one-sided Bayesian lower credible limit, two-sided Bayesian lower and upper credible limits of the reliability are also provided. Finally, calculation performed on practical problems, show that the provided method is feasible and easy to perform in engineering...|$|R
40|$|The 9 / 11 {{terrorist}} attacks on the twin towers of the World Trade Center were one of the saddest events in history which resulted in a tragic loss of life. An understanding of the structural response of the North Tower (WTC 1) to the fire following the aircraft impact and the collapse sequence is necessary, so that similar attacks on high-rise buildings can be resisted with limited casualties and minimal structural damage. To this end, the effect of elevated temperature on the behavior of structural steel members was studied. The reduction of material properties, such as the yield stress and Young 2 ̆ 7 s modulus of elasticity with temperature was discussed. Experimental data from fire tests on structural steel members were presented and conclusions about the deflection characteristics, the critical temperature, the failure mode and the effect of important parameters, such as the loading level, the axial and rotational restraint, were drawn. Once the interaction between fire and steel members was studied, the thermal environment of North Tower was discussed. The structural system of the building was then analyzed in the discussed environment while taking into account the materials, the unique building structural characteristics, and the evidence (observed and inferred) of the initial damage of the impacted zone. <b>Credible</b> <b>failure</b> mechanisms for the North Tower structure were then identified and the dominant factors affecting them were established. The failure mechanism that most likely triggered the collapse initiation was identified. ...|$|E
40|$|Current V-shaped {{stainless}} steel pyrovalve initiators have rectified {{many of the}} deficiencies of the heritage Y-shaped aluminum design. However, a <b>credible</b> <b>failure</b> mode still exists for dual simultaneous initiator (NSI) firings in which low temperatures were detected at the booster cap and less consistent ignition was observed than when a single initiator was fired. In order to asses this issue, a numerical framework has been developed for predicting the flow through pyrotechnically actuated valves. This framework includes a fully coupled solution of the gas-phase equation with a non-equilibrium dispersed phase for solid particles {{as well as the}} capability to model conjugate gradient heat transfer to the booster cap. Through a hierarchy of increasingly complex simulations, a hypothesis for the failure mode of the nearly simultaneous dual NSI firings has been proven. The simulations indicate that the failure mode for simultaneous dual NSI firings may be caused by flow interactions between the flame channels. The shock waves from each initiator interact in the booster cavity resulting in a high pressure that prevents the gas and particulate velocity from rising in the booster cap region. This impedes the bulk of the particulate phase from impacting the booster cap and reduces the heat transfer to the booster cap since the particles do not impact it. Heat transfer calculations to the solid metal indicate that gas-phase convective heat transfer may not be adequate by itself and that energy transfer from the particulate phase may be crucial for the booster cap burn through...|$|E
40|$|During {{the shuttle}} era NASA {{utilized}} a failure reporting system called the Problem Reporting and Corrective Action (PRACA) it {{purpose was to}} identify and track system non-conformance. The PRACA system over the years evolved from a relatively nominal way to identify system problems to a very complex tracking and report generating data base. The PRACA system became the primary method to categorize any and all anomalies from corrosion to catastrophic failure. The systems documented in the PRACA system range from flight hardware to ground or facility support equipment. While the PRACA system is complex, it does possess all the failure modes, times of occurrence, length of system delay, parts repaired or replaced, and corrective action performed. The difficulty is mining the data then to utilize that data in order to estimate component, Line Replaceable Unit (LRU), and system reliability analysis metrics. In this paper, we identify a methodology to categorize qualitative data from the ground system PRACA data base for common ground or facility support equipment. Then utilizing a heuristic developed for review of the PRACA data determine what reports identify a <b>credible</b> <b>failure.</b> These data are the used to determine inter-arrival times to perform an estimation of a metric for repairable component-or LRU reliability. This analysis is used to determine failure modes of the equipment, determine the probability of the component failure mode, and support various quantitative differing techniques for performing repairable system analysis. The result is that an effective and concise estimate of components used in manned space flight operations. The advantage is the components or LRU's are evaluated in the same environment and condition that occurs during the launch process...|$|E
40|$|Systematic {{software}} testing provides {{an important source}} of software failure analysis. The field suffers, however, from insufficiently reproducible results, lack of standard credible data, and insufficiently explicit assumptions. The present article attempts to provide an objective basis for failure analysis through an automatic testing framework (AutoTest) for contractequipped software. We present five principles for scientific failure analysis, a set of reproducible test results, and a first analysis of their consequences for software development. 1 Testing strategies and their purpose The area of {{software testing}} provides one of the best possible illustrations of the lack of <b>credible</b> large-scale <b>failure</b> analysis highlighted by the call for proposals for this workshop. While research in software testing has made considerable advances in recent years, and succeeded in rehabilitating an approach that used to arise considerable suspicions caused by the proponents of formal software development, it still suffers from insufficient credibilit...|$|R
50|$|Typically, {{weapons systems}} {{pertaining}} to ships, land vehicles, guided missiles and aircraft differ in hazards and effects; some are inherent, such as explosives, {{and some are}} created due to the specific operating environments (as in, for example, aircraft sustaining flight). In the military aircraft industry safety-critical functions are identified and the overall design architecture of hardware, software and human systems integration are thoroughly analyzed and explicit safety requirements are derived and specified during proven hazard analysis process to establish safeguards to ensure essential functions are not lost or function correctly in a predictable manner. Conducting comprehensive hazard analyses and determining <b>credible</b> faults, <b>failure</b> conditions, contributing influences and causal factors, that can contribute to or cause hazards, are an essentially part of the systems engineering process. Explicit safety requirements must be derived, developed, implemented, and verified with objective safety evidence and ample safety documentation showing due diligence. Highly complex software intensive systems with many complex interactions affecting safety-critical functions requires extensive planning, special know-how, use of analytical tools, accurate models, modern methods and proven techniques. Prevention of mishaps is the objective.|$|R
40|$|This paper {{examines}} why {{market and}} government institutions failed to prevent over {{fishing in the}} Southern Gulf snow crab fishery, whereas non-market institutions succeeded. A general conclusion is that the institutional environment in which economic behaviour must be coordinated for successful fisheries management is complex. More specifically, collective action dilemmas arise from the interdependency of human and fish species interactions. However, successful institutions are capable of resolving these dilemmas when they achieve credible commitment. Coordination mechanisms such as co-management contracts, horizontal patterns of communication and win-win negotiations all contribute to building an institutional arrangement in which participants are motivated to comply with conservation objectives. Fisheries management institutions Economic incentives Co-management Institutional <b>failure</b> <b>Credible</b> commitment...|$|R
40|$|This thesis has {{identified}} the failure and damage processes in a particle filled epoxy which {{is typical of}} adhesives used industrially. Micromechanical analyses {{have been carried out}} to predict the material properties of damaged adhesive, and to investigate the applicability of different failure criteria. The general body of evidence suggests that there is no direct method of predicting the failure load of adhesive joints from the strength and toughness of the adhesive used. Therefore, a favoured approach has been to postulate a failure criterion, and to implement it in the constitutive equation for the adhesive. In contrast, this work has begun from the microstructural modelling of damage, and derived <b>credible</b> <b>failure</b> criteria from this model. The experimental program quantified the adhesive morphology and identified the damage processes that occur in the adhesive prior to failure. Bulk and joint specimens were tested both in-situ in a scanning electron microscope, and on a conventional tensile testing machine. The tests showed that the mechanisms for damage and failure in both joint and bulk form are particle debonding followed by cracking in the matrix. The concept of a representative unit cell of material was used {{to determine the effects of}} particle cracking and debonding. In a regular' array of cracked particles, the stiffness remained relatively unchanged in the plane of the cracks, but perpendicular to it, a significant reduction was found. Modelling debonded particles is more complex, because partial contact must be considered in addition to the fully bonded and fully debonded conditions. The unit cell was used to define the elasticity matrix for adhesive containing debonded particles as a function of strain state. The unit cell concept was extended further by including material that obeyed a modified (i. e. hydrostatically sensitive) Von Mises yield criterion. Particle debonding was found to contribute significantly to the hydrostatic sensitivity and to the softening of the adhesive. The unit cell concept was used to implement a strain at a distance failure criteria, using both elastic and plastic material properties. New types of failure criteria also based on the unit cell have been proposed. The criteria relate the strain state in an adhesive joint to the likelihood of shear banding or tensile plastic flow. The regions in a joint that experience one or the other of the mechanisms were identified. Hence the nature and extent of the adhesive failure in joints with varying joint geometry and loading may be predicted...|$|E
40|$|Lawrence Livermore and Los Alamos National Laboratories have {{developed}} a common framework and key elements of a national certification methodology called Quantification of Margins and Uncertainties (QMU). A spectrum from senior managers to weapons designers has been engaged in this activity at the two laboratories for {{on the order of}} a year to codify this methodology in an overarching and integrated paper. Following is the certification paper that has evolved. In the process of writing this paper, an important outcome has been the realization that a joint Livermore/Los Alamos workshop on QMU, focusing on clearly identifying and quantifying differences between approaches between the two labs plus developing an even stronger technical foundation on methodology, will be valuable. Later in FY 03, such a joint laboratory workshop will be held. One of the outcomes of this workshop will be a new version of this certification paper. A comprehensive approach to certification must include specification of problem scope, development of system baseline models, formulation of standards of performance assessment, and effective procedures for peer review and documentation. This document concentrates on the assessment and peer review aspects of the problem. In addressing these points, a central role is played by a 'watch list' for weapons derived from <b>credible</b> <b>failure</b> modes and performance gate analyses. The watch list must reflect our best assessment of factors that are critical to weapons performance. High fidelity experiments and calculations as well as full exploitation of archival test data are essential to this process. Peer review, advisory groups and red teams {{play an important role in}} confirming the validity of the watch list. The framework for certification developed by the Laboratories has many basic features in common, but some significant differences in the detailed technical implementation of the overall methodology remain. Joint certification workshops held in June and December of 2001 and continued in 2002 have proven useful in developing the methodology, and future workshops should prove useful in further refining this framework. Each laboratory developed an approach to certification with some differences in detailed implementation. The general methodology introduces specific quantitative indicators for assessing confidence in our nuclear weapon stockpile. The quantitative indicators are based upon performance margins for key operating characteristics and components of the system, and these are compared to uncertainties in these factors. These criteria can be summarized in a quantitative metric (for each such characteristic) expressed as: (i. e., confidence in warhead performance depends upon CR significantly exceeding unity for all these characteristics). These Confidence Ratios are proposed as a basis for guiding technical and programmatic decisions on stockpile actions. This methodology already has been deployed in certifying weapons undergoing current life extension programs or component remanufacture. The overall approach is an adaptation of standard engineering practice and lends itself to rigorous, quantitative, and explicit criteria for judging the robustness of weapon system and component performance at a detailed level. There are, of course, a number of approaches for assessing these Confidence Ratios. The general certification methodology was publicly presented for the first time to a meeting of Strategic Command SAG in January 2002 and met with general approval. At that meeting, the Laboratories committed to further refine and develop the methodology through the implementation process. This paper reflects the refinement and additional development to date. There will be even further refinement at a joint laboratory workshop later in FY 03. A common certification methodology enables us to engage in peer reviews and evaluate nuclear weapon systems on the basis of explicit and objective metrics. The clarity provided by such metrics enables each laboratory and our common customers to understand the meaning and logic of technical and management decisions affecting stockpile performance and safety...|$|E
40|$|The {{adequacy}} of the EGCR steam and feedwater systems for continued heat removal from the core was assessed for the following accidents: steam line failures, steam generator internal failures, and feedwater system failures. The reaction of the reactor coolant blowers and vessel cooling compressors to steam- helium mixtures was evaluated. The steam generator isolation systems are described. The containment system {{was found to be}} adequate to prevent activity release to the atmosphere in the event of all credible steam line or feedwater system failures. The design of the drum water level automatic isolation system does not prevent activity release during certain <b>credible</b> steam generator <b>failures,</b> however, this activity is released {{at the top of the}} plant stack and does not result in a dose which exceeds the acceptable yearly exposure. (M. C. G. ...|$|R
40|$|The {{braking system}} of the landing gear wheels of a {{mainline}} aircraft has to meet mandatory requirements {{laid out in the}} Aviation Regulations AP- 25 (Para 25. 735. «Brakes and brake systems"). These requirements are essential when creating the landing gear wheel brake control system (WBCS) and are used as main initial data in its mathematical modeling. The WBCS {{is one of the most}} important systems to ensure the safe completion of the flight. It is a complex of devices, i. e. units (hydraulic, electrical, and mechanical), connected through piping, wiring, mechanical constraints. This complex should allow optimizing the braking process when a large number of parameters change. The most important of them are the following: runway friction coefficient (RFC), lifting force, weight and of the aircraft, etc. The main structural elements involved in braking the aircraft are: aircraft wheels with pneumatics (air tires) and brake discs, WBCS, and cooling system of gear wheels when braking. To consider the aircraft deceleration on the landing run is of essence at the stage of design, development, and improvement of brakes and braking systems. Based on analysis of equation of the aircraft motion and energy balance can be determined energy loading and its basic design parameters, braking distances and braking time. As practice and analysis of energy loading show, they (brake + wheel) absorb the aircraftpossessed kinetic energy at the start of braking as much as 60 - 70 %, 70 - 80 %, and 80 - 90 %, respectively, under normal increased, and emergency operating conditions. The paper presents a procedure for the rapid calculation of energy loading of the brake wheel. Currently, the mainline aircrafts use mainly electrohydraulic brake systems in which there are the main, backup, and emergency-parking brake systems. All channels are equipped with automatic anti-skid systems. Their presence in the emergency (the third reserve) channel significantly improves the reliability and safety of the aircraft braking mode with a slight increase in weight and complexity of the system. Mathematical modeling of the WBCS is intended to provide the possibility for studying the effect of various parameters on the braking process, choice of a rational law of the anti-skid automatics and minimization of the brake way on the runway in designing the WBCS, and its certification for compliance with AP 25 under normal operation and in appearing of <b>credible</b> <b>failures.</b> The article presents differential equations of motion of the braking {{system of the}} aircraft landing gear wheel, which is an electro-hydraulic actuator to form the braking torque Мт, depending on the control signal Uу. The actuator comprises a remote control system of pressure and multi-disc friction brake. This mathematical model of the braking system of aircraft landing gear wheel allows us to study the braking process in a wide variation range of different parameters both of the braking system itself and its components, and of the aircraft parameters, runway conditions, and anti-skid system parameters, i. e. it provides an optimized braking process in conditions of changing a large number of different parameters the most important of which are: RFC, lifting force and aircraft weight; speed of the aircraft; parameters of the WBCS hydraulic units, etc. </p...|$|R
40|$|This thesis {{work was}} a study into how the {{effectiveness}} of ignition source isolation can be estimated. This safety system works by isolating electrical equipment from power when flammable gas is detected on oil and gas installations. Improving the understanding of how effective this system actually is at reducing explosion risk in hazardous areas was the main motivation, as this could help operators and authorities form a more accurate risk picture. The {{main part of the}} work is the development and discussion of a model for ignition that was made so that it could be used to estimate this effectiveness. A detailed model is presented first, based on evaluating failure modes of equipment, then suggestions are made for how it could be simplified to be of practical use in risk analysis. The second part of the project was to gather enough data from industry sources to be able to estimate key parameters in the model relating to the failure probability of Ex barriers and the ignition probability resulting from common types of process equipment. Not enough data was found to support a quantification of these parameters, but results from a major maintenance project on an oil and gas installation in the Norwegian sector was reviewed and discussed. The method of systematically evaluating failure modes in order to determine risk could be useful in other applications, and the suggested way to proceed with the work in this report is to continue gathering data and analyzing it to build up a <b>credible</b> set of <b>failure</b> probabilities for Ex barriers and common equipment types. </p...|$|R
40|$|Industrialized {{society is}} linked to the {{transport}} of hazardous materials by road and rail, among other. During transportation, accidents may occur and propagate among the tankers leading to severe fires, explosion or toxic dispersions. This may increase the level of individual and social risk associated to those activities, since the transport network often crosses densely populated area. The escalation of a primary event, in this case the fire, is typically denoted as domino effect, and the triggered secondary events typically are amplified. In the framework of liquefied petroleum gas (LPG) transportation, severe fire and explosion hazards are associated to the possible catastrophic rupture of tankers, which may be induced by domino effect of accidental fires. Heat resistant coatings may protected tankers against the fire, reducing the heat load that reaches the tank shell wall and the lading. Indeed, the rupture {{is the result of the}} double effect of thermal weakening of the tank material and the increasing pressure due to LPG evaporation. However, this protection systems are not ideal and undergo defects due to both material degradation and accidental damage. Therefore, protection may be ineffective. The present work is aimed at characterizing the performance of defective coatings. The first part of the work is devoted to the characterization of past accidents occurred in the framework of road and rail transportation of hazardous materials. The ARIA and MHIDAS databases are adopted as data sources, identifying 245 road and 220 rail accidents involving hazardous materials. The analysis highlighted the importance of protecting tank from heat load to avoid the rupture and related severe scenario. For these reasons, in North America the installation of a heat resistant coating is used to protect dangerous good tankers from accidental fire exposure. In Europe, ADR and RID regulations govern transnational transport of hazardous materials by road and by rail, respectively, and still not include any section about thermal protection systems of tankers. Possible concerns related to the installation of these systems is due presence of defects that may be formed accidentally in the fireproofing layer. It is therefore important to establish what level of defect is acceptable in order to avoid the failure of tankers, in the prospective of a wider implementation of tankers fire protections in the European framework. Since large scale bonfire tests are expensive and difficult to be carried out in order to verify the thermal protections adopted, modelling the behaviour of pressurized insulated tankers when exposed to the fire is a possible solution to test the adequateness of defective protections. In order to describe the thermal behaviour of real scale LPG tanks exposed to fire, a lumped model (namely, ‘RADMOD’) and a Finite Elements Model (FEM) are developed. The models are validated against available experimental data and allow predicting the thermal behaviour of tankers with defective coating when exposed to fire, with the aim to assess the thermal protection performance. The phenomena taking place through the vessel in presence of defects are investigated and characterized, in order to reproduce the experimental data on thermal behaviour of defective thermal protection systems exposed to fire. The FEM model allows to determine the wall temperature profile and the stress distribution over the vessel, determining, in the end, a critical defect size that lead to the tank failure, with respect different fire conditions. A sensitivity analysis is performed on the FEM model in order to identify the parameters that mostly affect the heat exchanges of the system. This analysis highlights the main relevance of the flame temperature against other parameters, such as convective heat transfer coefficients and emissivity of flame and steel. The complex analysis performed by FEM model, requires high computational times, which may be prohibitive when a wide number of runs is required. The RADMOD code is a simplified lumped model, which allows to assess the behaviour, among other, of the pressure and the fluid temperature with lower, and thus acceptable, computational time. Another plus of the RADMOD model is that it can be run for a wide range of materials, substances, geometries and fire scenario, estimating a conservative but <b>credible</b> time to <b>failure</b> of the tank. The novel mathematical code for defective thermal protection system is added to the previous version of the RADMOD model, which was implemented for unprotected or completely coated tanks, thus all the phenomena related to the defect enclosure are characterised. In addition, other phenomena, already present in the RADMOD model, are revised to enhance the potentiality of the code. The comparison of results with available experimental data on medium-scale shows that the model proposed in this thesis work can reasonably predict the thermal response. The application of the modelling tool to different geometries is performed considering real-scale defects. Thus, several case-studies were defined in order to reproduce medium- and large-scale tanks varying a few parameters, such as defect size and liquid filling level, for testing the reproducibility of the new model. The results from the case studies highlight the potentiality and the flexibility of the RADMOD code in modelling the thermal response. The ultimate goal would be to apply the data collected from RADMOD code about temperature and pressure of lading, as boundary condition in the FEM model for an improved modelling of thermal behaviour of real-scale LPG tanks in fire scenarios even if there is a defective thermal protection system...|$|R
40|$|This {{document}} {{reports the}} preliminary {{findings of a}} review of integrated farming standards (IFS), food eco-labelling and their role in improving the overall environmental performance of UK agriculture. This review has been undertaken {{as part of a}} research project, funded by Defra, entitled 'Assessment of Reduction in Environmental Burdens through Targeted Measures compared with Whole Farm Approaches in Cropping and Livestock Systems' (CSA 7471 /IF 0131). The review is broad and shallow in its approach and aims to draw together the activities currently taking place within the United Kingdom (UK), Europe and more globally with respect to the application of IFS standards, eco-labelling and associated activities. The purpose of this review is to inform and direct the wider research project but also to provide policy support for emerging related issues. This includes the possibility of developing a standard for IFS. The agricultural industry is no stranger to change, {{largely as a result of}} changing circumstances. Retailer and consumer demands, public requirements, legislation and regulation, and even the response of the environment itself have presented the industry with a very dynamic situation in which to operate. Two of the key responses in the past 20 years have been the shift towards more integrated approaches to farming, including organic farming, and the development of assurance schemes and marketing labels and brands. There is no exact definition of integrated farming (IF). It is generally recognised that it is not based on a set of fixed parameters but on informed management processes. There are a generally accepted set of principles covering a broad set of topics such as pesticide and fertiliser use, animal husbandry and welfare, health and safety and environment, but there is much that is open to interpretation. This is the essence of the approach - it is about tailoring best practice to the individual circumstances of the farm to deliver the best for both the farm and the environment. As such it is difficult to develop a detailed 'standard' for IF. Management plans are a key tool often used to implement IF. However, just having a management plan is not adequate to achieve the standard for IF. The plan needs to be properly considered, implemented and monitored in order to be effective. As such, what a management plan should cover and how it is implemented needs to be defined in as much detail as any other standard. In the UK there are many initiatives and schemes that can be considered to have some connection with integrated farming to greater or lesser extents. These include all the schemes under the umbrella of Assured Food Standards (AFS), LEAF Marque, Conservation Grade, White and Wild, Freedom Food, Lion Quality Mark, Fairtrade, all the Organic schemes, geographically based schemes, and other initiatives such as the Voluntary Initiative. There are also initiatives operated by some of the retailers, such as Tesco's Nature's Choice and Marks and Spencer's "field-to-fork". All of these encourage more integrated approaches to varying degrees. In Europe the level of recognition and implementation of IFS standards varies considerably. There are some over-arching initiatives and organisations that deal with integrated approaches. Most notably there is the European Initiative for Sustainable Development in Agriculture (EISA) who have tried to define a framework for integrated farming in a similar fashion to an assurance scheme, and who are connected with organisations such as LEAF and equivalent organisations in other EU member states. There are also other organisations and initiatives including the IOBC Integrated Production Guidelines, the Global Food Safety Initiative (GFSI), International Social and Environmental Accreditation and Labelling (ISEAL) Alliance and EUREPGAP. The latter of these being connected with AFS and other assurance schemes by providing an umbrella standard, which national standards can benchmark themselves against. There are also many schemes in other European countries that are equivalent to LEAF, AFS or other schemes in the UK. These include ??PUL in Austria, Flandria in Belgium, FARRE in France, FNL in Germany, SMK in the Netherlands, and Odling i Balans in Sweden. There are also many other labels, including those for organic production. The level of IFS in other countries is also growing, especially to increase national export markets where EUREPGAP compliance or membership of an IFS-related scheme is becoming essential. Amongst all these labels and schemes there are number of common issues to understand and acknowledge. These include taking a life cycle perspective for a 'true' eco-label, assessing performance (what is achieved) against outcomes and not just what is considered to be best practice, embracing the concept of continuous improvement, and understanding the advantages and disadvantages of labels. There is also the structure of advisory services for supplying help and guidance to support the adoption of best practice (knowledge transfer) to consider and the IT and information infrastructure required to manage the schemes and labels. In the UK, many of the labels and schemes that might be perceived as being an eco-label, e. g. LEAF, Nature's Choice, or Organic, are only based on production practices meeting a set standard, and typically this standard only applies to production at the farm level. However, a 'true' eco-label must be based on a full life cycle assessment (LCA) of a product from its production through to consumption, where its performance is based on environmental impacts. At the moment we judge performance by achieving levels of best practice, but this does not ensure that desired environmental (or other) outcomes are being achieved. There is only the relationship that best practice should lead to the outcomes, but this relationship is not always strong or guaranteed. The reason we use 'practice-based' measures is because 'outcome-based' ones are difficult to measure. Data, particularly at the farm level, is often difficult obtain, as recently highlighted with current initiatives to develop carbon labelling. However, it is possible to combine LCA, outcome and practice based approaches. This is demonstrated with the SMK label in the Netherlands, where LCAs of production processes are used to set the standard for practices that must be achieved to obtain the label. Other techniques such as the balanced scorecard approach used by businesses also acknowledge that it is important to assess performance using a combination of both outcome and practice based measures. These can then be used to help identify practices that might be the cause of failure to achieve outcomes. Central to all of these approaches is the concept of continuous improvement. It is important that businesses do not remain static and know where to make improvements. Both outcome and practice based measures can help identify these areas and benchmarking tools can also provide information on the performance of a business in relation to others. A key part of continuous improvement is education and learning. Thus it is important to recognise that any approaches that aim to improve standards need to be accompanied with an effective and targeted programme of knowledge transfer. This can be as simple as providing advice/guidance or can include detailed training, education and awareness programmes. Some assurance schemes have recognised this, for example in New Zealand the Farmsure scheme has made a software package available to participating farmers to help them formulate plans for land and animal management and social responsibility. It is clear that whatever approach is utilised, the amount of data and information that needs to be handled is vast and no system will be viable without an effective IT infrastructure. There is already a substantial amount of data that is recorded and communicated for assurance schemes, if we are to add the data for eco-labelling the 'dossier' of data for each product will be large. So it is important to acknowledge that the need for an effective infrastructure should not be underestimated. It should also be recognised that duplication of data should be avoided. If it has already been collected by one scheme, it should be available to another. To conclude, this report provides a broad and shallow review of the various schemes, initiatives and eco-labels that are related to integrated farming, and has highlighted some of the key issues and challenges associated with them. At the heart of the matter is the need to understand the outcomes we want to achieve, for example: safe, affordable, healthy food, a productive, economically viable agriculture and a sustainable environmental footprint. These are ultimate basis by which we will judge the performance of our production systems and the 'quality' of the produce we consume. How integrated farming, assurance schemes and labels contribute towards achieving these outcomes needs to be understood. The relationships between practices being promoted and desired outcomes needs to be fully incorporated into the system in order ensure transparency and a clear direction for improvement. The labels we place on our food can help this process. Although there is much criticism of food labelling, if based on sound science, it can provide a positive force for change. Labels can influence consumers purchasing behaviour and can drive industry practices to meet the required standards. However, care needs to be taken to ensure that any labels are valid and <b>credible,</b> as <b>failure</b> to do so could result in confusion and a negative effect. This review has established that there are many aspects to consider when examining integrated farming, assurance schemes and eco-labels. There are many interesting approaches in the UK and across Europe and these are outlined in detail in the report. Although there are some findings, this report has not identified a 'blueprint' for an integrated farming standard or an eco-label for food. This report is just the preliminary step of the project (CSA 7471 /IF 0131) which seeks to improve the understanding of different approaches and how they influence outcomes. This project will continue over the next 2 years and will be completed in April 2009...|$|R

