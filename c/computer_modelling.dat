1681|10000|Public
5|$|As {{the planet}} is not known to transit from Earth and {{atmospheric}} conditions are not observable with current technology, no atmosphere for the planet has been confirmed to date. As such, all climate predictions for the planet are based on predicted orbits and <b>computer</b> <b>modelling</b> of theoretical atmospheric conditions.|$|E
5|$|Plasmid {{stabilising}} toxin-antitoxin {{systems have}} been used as examples of selfish DNA as part of the gene centered view of evolution. It has been theorised that toxin-antitoxin loci serve only to maintain their own DNA, {{at the expense of the}} host organism. Other theories propose the systems have evolved to increase the fitness of plasmids in competition with other plasmids. Thus, the toxin-antitoxin system confers an advantage to the host DNA by eliminating competing plasmids in cell progeny. This theory was corroborated through <b>computer</b> <b>modelling.</b> This does not, however, explain the presence of toxin-antitoxin systems on chromosomes.|$|E
5|$|Similar to its predecessor, {{the game}} utilises the same Silicon Graphics (SGI) and Advanced <b>Computer</b> <b>Modelling</b> (ACM) {{rendering}} technology. Pre-rendered images are modelled as 3D objects and then transformed into 2D sprites and background layers. Rare founder Tim Stamper served {{as director of}} the game, whereas his colleague Brendan Gunn, who had worked on the original, returned to design the game. Development of Diddy's Kong Quest began shortly after the release of its predecessor. Rare took significant financial risks in purchasing the expensive SGI equipment used to render the graphics. David Wise, Rare's composer from 1985 to 1994, admitted that the workstations Rare purchased were worth £80,000 each. A new compression technique they developed allowed them to incorporate more detail and animation for each sprite for a given memory footprint than previously achieved on the SNES, which better captured the pre-rendered graphics.|$|E
40|$|<b>Computer</b> <b>models</b> are {{commonly}} used for studying complex physical systems. Under certain conditions, hidden constraints may cause unexpected <b>computer</b> <b>model</b> failure. This research presents a novel approach involving Gaussian stochastic process <b>models</b> to accomplish <b>computer</b> <b>model</b> optimization {{in the presence of}} hidden constraints...|$|R
3000|$|Lithology {{templates}} class (lithology {{class in}} <b>computer</b> <b>model)</b> and lithological set class (lithologies class in <b>computer</b> <b>model)</b> [...]...|$|R
40|$|There {{are many}} areas of science and {{engineering}} where research and decision making are performed using <b>computer</b> <b>models.</b> These <b>computer</b> <b>models</b> are usually deterministic and may take minutes, hours or days to produce an output for a single value of the model inputs. Fitting mixtures of experts of <b>computer</b> <b>models</b> where the expert components use different values of the <b>computer</b> <b>model</b> parameters is considered. The efficient calibration of such models using emulators, which are fast statistical surrogates for the <b>computer</b> <b>model,</b> is discussed. It is argued that mixtures of experts are often insightful for describing model discrepancy and {{ways in which the}} <b>computer</b> <b>model</b> can be improved. This is not a strength of standard approaches to the statistical analysis of <b>computer</b> <b>models</b> where a certain 2 ̆ 2 best input 2 ̆ 2 assumption is usually made and model discrepancy is often described through a stationary Gaussian process prior on the discrepancy function. Application of the framework is presented for a dynamic hydrological rainfall-runoff model in which the mixture approach is helpful for highlighting model deficiencies. 2013 Elsevier B. V. All rights reserved...|$|R
5|$|Similar to its predecessors, Dixie Kong's Double Trouble utilises {{the same}} Silicon Graphics (SGI) and Advanced <b>Computer</b> <b>Modelling</b> (ACM) {{rendering}} technology, in which pre-rendered 3D animations are turned into 2D sprites. Rare founder Tim Stamper re-took the {{role as the}} game's director, whereas Rare staffers Andrew Collard and Paul Weaver designed the game. Development of Dixie Kong's Double Trouble! began shortly {{after the release of}} Diddy's Kong Quest. Rare took significant financial risks in purchasing the expensive SGI equipment used to render the graphics. David Wise, Rare's composer from 1985 to 1994, admitted that the workstations Rare purchased were worth £80,000 each. A new compression technique they developed allowed them to incorporate more detail and animation for each sprite for a given memory footprint than previously achieved on the SNES, which better captured the pre-rendered graphics. Dixie Kong's Double Trouble!s soundtrack was composed by Eveline Fischer and David Wise, with Fischer producing most of the game's music.|$|E
5|$|While {{extremely}} severe {{commercial and}} recreational overfishing in the 1800s {{and the early}} 1900s caused the first strong declines of Murray cod, overfishing by recreational fishermen, aided by inadequate fishing regulations, continues today and remains an extremely serious threat to Murray cod. The current size limit of 60centimetres in most states is inadequate now that scientific studies have documented average size at sexual maturity in Murray cod. This and catch data and <b>computer</b> <b>modelling</b> exercises on wild Murray cod stocks indicate measures such as raising the size limit to 70centimetres and reducing the bag and possession limits from 2 and 4 fish respectively to 1 fish are urgently needed to maintain the long-term viability of wild Murray cod populations. As of November 2014, the NSW Department of Fisheries has introduced a maximum size limit of 75cm for Murray Cod to provide protection for large breeding fish, {{as well as a}} new minimum size limit of 55cm.|$|E
25|$|Using <b>computer</b> <b>modelling</b> {{and solar}} data, Scottish {{scientists}} {{determine that the}} last living species on Earth in the distant future will be extremophile microbes able to survive harsh conditions.|$|E
40|$|A key {{question}} in evaluation of <b>computer</b> <b>models</b> is Does the <b>computer</b> <b>model</b> adequately represent reality? A complete Bayesian approach to answering {{this question is}} developed for the challenging practical {{context in which the}} <b>computer</b> <b>model</b> (and reality) produce functional data. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different – but related – scenarios through hierarchical modeling. It is also shown how one can formally test if the <b>computer</b> <b>model</b> reproduces reality. The approach is illustrated through study of a <b>computer</b> <b>model</b> developed to model vehicle crashworthiness...|$|R
40|$|A major {{question}} {{for the application of}} <b>computer</b> <b>models</b> is Does the <b>computer</b> <b>model</b> adequately represent reality? Viewing the <b>computer</b> <b>models</b> as a potentially bi-ased representation of reality, Bayarri et al. (2005 b) develop the Simulator Assessment and Validation Engine (SAVE) method as a general framework for answering this ques-tion. In this paper, we apply the SAVE method to the challenge problem: a thermal <b>computer</b> <b>model</b> designed for certain devices, and develop a statement of confidence that the devices can be applied in intended situations...|$|R
40|$|Risk {{assessment}} of rare natural hazards — such as large volcanic block and ash or pyroclastic flows — is addressed. Assessment is approached {{through a combination}} of <b>computer</b> <b>modeling,</b> statistical modeling, and extreme-event probability computation. A <b>computer</b> <b>model</b> of the natural hazard is used to provide the needed extrapolation to unseen parts of the hazard space. Statistical modeling of the available data is needed to determine the initializing distribution for exercising the <b>computer</b> <b>model.</b> In dealing with rare events, direct simulations involving the <b>computer</b> <b>model</b> are prohibitively expensive. Solution instead requires a combination of adaptive design of <b>computer</b> <b>model</b> approximations (emulators) and rare event simulation. The techniques that are developed for risk assessment are illustrated on a test-bed example involving volcanic flow...|$|R
25|$|Determination of IAQ {{involves}} {{the collection of}} air samples, monitoring human exposure to pollutants, collection of samples on building surfaces, and <b>computer</b> <b>modelling</b> of air flow inside buildings.|$|E
25|$|Wind {{turbines}} are designed, using a {{range of}} <b>computer</b> <b>modelling</b> techniques, to exploit the wind energy that exists at a location. For example, Aerodynamic modeling is {{used to determine the}} optimum tower height, control systems, number of blades and blade shape.|$|E
25|$|In 2013, {{an impact}} between minor planets was {{detected}} around the star NGC 2547 by Spitzer and confirmed by ground observations. <b>Computer</b> <b>modelling</b> {{suggests that the}} impact involved large asteroids or protoplanets similar to the events believed to {{have led to the}} formation of terrestrial planets like the Earth.|$|E
40|$|<b>Computer</b> <b>models</b> to {{simulate}} physical phenomena are now {{widely available in}} engineering and science. Before relying on a <b>computer</b> <b>model,</b> a natural first step is often to compare its output with physical or field data, to assess whether the <b>computer</b> <b>model</b> reliably represents the real world. Field data, when available, {{can also be used}} to calibrate or tune unknown parameters in the <b>computer</b> <b>model.</b> Calibration is particularly problematic in the presence of systematic discrepan-cies between the <b>computer</b> <b>model</b> and field observations. We introduce a likelihood alternative to previous Bayesian methodology for estimation of calibration or tun-ing parameters. In an important special case, we show that maximum likelihood estimation will asymptotically find values of the calibration parameter that give an unbiased <b>computer</b> <b>model,</b> if such a model exists. However, the calibration parame-ters are not necessarily estimable. We also show in some settings that calibration or tuning need to take into account the end-use prediction strategy. Depending o...|$|R
40|$|<b>Computer</b> <b>modeling</b> {{of social}} and {{economic}} systems is only about three decades old. Yet in that time, <b>computer</b> <b>models</b> have been used to analyze everything from inventory management in corporations to the performance of national economies, from the optimal distribution of fire stations in New York City to the interplay of global population, resources, food, and pollution. Certain <b>computer</b> <b>models,</b> such as The Limit...|$|R
40|$|The {{application}} of <b>computer</b> <b>modeling</b> gives additional incentives of development to {{the branches of}} science, helps to limit intuitive speculative modeling and expands the appendix of rational methods of information technologies. The review of methodology of <b>computer</b> <b>modeling</b> has been offered, the basic concepts of <b>computer</b> <b>modeling</b> have been given, the methodology of algorithms of stage-by-stage and step-by-step modeling has been also given.  </p...|$|R
25|$|Experimental {{research}} and <b>computer</b> <b>modelling</b> {{suggest that the}} surfaces of mineral particles inside hydrothermal vents have catalytic properties {{similar to those of}} enzymes and are able to create simple organic molecules, such as methanol (CH3OH) and formic, acetic and pyruvic acid out of the dissolved CO2 in the water.|$|E
25|$|The School of Civil Engineering and Geosciences at Newcastle University is {{conducting}} a study to gather as much information about the flash-flooding that hit Newcastle and the North East during summer 2012, launching the ToonFlood Project for the submission of photos and comments to help calibrate <b>computer</b> <b>modelling</b> of the event.|$|E
25|$|Modelling {{biological}} systems {{is a significant}} task of systems biology and mathematical biology. Computational systems biology aims to develop and use efficient algorithms, data structures, visualization and communication tools {{with the goal of}} <b>computer</b> <b>modelling</b> of {{biological systems}}. It involves the use of computer simulations of biological systems, including cellular subsystems (such as the networks of metabolites and enzymes which comprise metabolism, signal transduction pathways and gene regulatory networks), to both analyze and visualize the complex connections of these cellular processes.|$|E
50|$|The Utah Teapot {{has come}} full circle from being a <b>computer</b> <b>model</b> based on an actual teapot to being an actual teapot based on the <b>computer</b> <b>model.</b> It is widely {{available}} in many renderings in different materials from small plastic knick-knacks to a fully functional ceramic teapot. It is sometimes intentionally rendered as a blocky, low poly object to celebrate its origin as a <b>computer</b> <b>model.</b>|$|R
40|$|<b>Computer</b> <b>models</b> {{are used}} to model complex {{processes}} in various disciplines. Often, a key source of uncertainty {{in the behavior of}} complex <b>computer</b> <b>models</b> is uncertainty due to unknown model input parameters. Statistical <b>computer</b> <b>model</b> calibration is the process of inferring model parameter values, along with associated uncertainties, from observations of the physical process and from model outputs at various parameter settings. Observations and model outputs are often in the form of high-dimensional spatial fields, especially in the environmental sciences. Sound statistical inference may be computationally challenging in such situations. Here we introduce a composite likelihood-based approach to perform <b>computer</b> <b>model</b> calibration with high-dimensional spatial data. While composite likelihood has been studied extensively in the context of spatial statistics, <b>computer</b> <b>model</b> calibration using composite likelihood poses several new challenges. We propose a computationally efficient approach for Bayesian <b>computer</b> <b>model</b> calibration using composite likelihood. We also develop a methodology based on asymptotic theory for adjusting the composite likelihood posterior distribution so that it accurately represents posterior uncertainties. We study the application of our new approach in the context of calibration for a climate model...|$|R
40|$|Mathematical and <b>computer</b> <b>model</b> of carioles? {{vibratory}} gyroscope with temperature {{influence is}} developed using well-known dependencies of stiffness and damping coefficients on temperature. <b>Computer</b> <b>modeling</b> of carioles? vibratory gyroscope with temperature influence is carried out. ?????????? ?????????????? ? ????????????? ???????????? ?????? ????????????? ????????????? ????????? ? ?????? ?????????????? ??????????? ?? ?????? ????????? ???????????? ???????????? ????????? ? ???????????? ????????????? ?? ???????????. ????????? ????????????? ???????????? ?????? ????????????? ????????????? ????????? ? ?????? ?????????????? ???????????...|$|R
25|$|In the United States, an {{integrated}} approach to real-time hydrologic <b>computer</b> <b>modelling</b> utilizes observed {{data from the}} U.S. Geological Survey (USGS), various cooperative observing networks, various automated weather sensors, the NOAA National Operational Hydrologic Remote Sensing Center (NOHRSC), various hydroelectric companies, etc. combined with quantitative precipitation forecasts (QPF) of expected rainfall and/or snow melt to generate daily or as-needed hydrologic forecasts. The NWS also cooperates with Environment Canada on hydrologic forecasts that affect both the USA and Canada, like {{in the area of}} the Saint Lawrence Seaway.|$|E
25|$|The Innovation Tower in Hong Kong (2007–2014) {{is part of}} Hong Kong Polytechnic University. The {{building}} of 15 floors has 15,000 square metres of space, with laboratories, classrooms, studios and other facilities for 1,800 students and their faculty. It {{was built on the}} site of the university's former football pitch. The extremely complex forms of the building required <b>computer</b> <b>modelling.</b> Early designs experimented with a facade made of reinforced plastic, textiles or aluminium, but Hadid finally settled upon metal panels with multiple layers. The building seems to lean towards the city. The floors inside are visible from the exterior like geological strata.|$|E
25|$|NASA {{scientists}} Hartman and McKay {{argue that}} plate tectonics {{may in fact}} slow the rise of oxygenation (and thus stymie complex life rather than promote it). <b>Computer</b> <b>modelling</b> by Tilman Spohn in 2014 found that plate tectonics on Earth may have arisen {{from the effects of}} complex life's emergence, {{rather than the other way}} around as the Rare Earth might suggest. The action of lichens on rock may have contributed to the formation of subduction zones in the presence of water. Kasting argues that if oxygenation caused the Cambrian explosion then any planet with oxygen producing photosynthesis should have complex life.|$|E
40|$|<b>Computer</b> <b>models</b> enable {{scientists}} to investigate real-world phenomena virtually using com- puter experiments. Recently, statistical calibration enabled {{scientists to}} incorporate field data and estimate unknown physical constants. In this thesis, we outline three new developments for the statistical calibration of <b>computer</b> <b>models.</b> The first {{development is a}} practical approach for calibrating large and non-stationary <b>computer</b> <b>model</b> output. We present a new computationally efficient approach using a criterion that measures {{the discrepancy between the}} <b>computer</b> <b>model</b> and field data. One can then construct empirical distributions for the parameters and sequentially add design points to improve these estimates. The strength of this approach is its simple computation using existing algorithms. Our method also provides good parameter estimates for large and non-stationary data. The second development deals with incorporating derivative information from the <b>computer</b> <b>model</b> into a calibration experiment. Many <b>computer</b> <b>models</b> are governed by differential equations, and including this derivative information can be helpful. Although incorporating such information has garnered a lot of attention in some areas of statistics, such as penalized regression, it has been largely ignored in computer experiments. We develop a new statistical methodology for the calibration of a <b>computer</b> <b>model</b> when derivative information is additionally available. The final development deals with extending the methodology incorporating derivatives to allow for the inclusion of possible bias in the <b>computer</b> <b>model.</b> A statistical model accounting for such bias was previously proposed, but heavily criticised as not being identifiable. We develop a model that accounts for this possible bias while simultaneously including the derivative information from the <b>computer</b> <b>model</b> in the hopes that such identifiability issues can be reduced or eliminated. Our results indicate some modest improvements over the previous approach in some experimental conditions. Proving exact conditions where such models can be identified remains interesting and challenging research to explore in future work...|$|R
40|$|I. What {{are some}} life {{insurance}} company applications of <b>computer</b> <b>modeling</b> that have proved useful for agency problems? Investment problems? Ad-ministrative problems? Pricing problems? Other? II. Why was <b>computer</b> <b>modeling</b> used rather than another method (such as analytical solutions) ? III. What applications have proved {{not to be}} useful? Why not? IV. What methods {{proved to be more}} successful than <b>computer</b> <b>modeling?</b> V. What dollar and personnel commitment is needed to make effective use of <b>computer</b> <b>modeling</b> in a life insurance company? New York Regional Meeting MR. RUSSELL M. COLLINS, JR. : The first use of <b>computer</b> <b>modeling</b> that I would like to mention applies when it is either impossible or ex-tremely costly to observe a real process in order to obtain the desired in-formation. The classic example of this perhaps is the way wind tunnels are used in the aircraft industry. Conceivably, the aircraft industry coul...|$|R
40|$|Although <b>computer</b> <b>models</b> {{are often}} used for {{forecasting}} future outcomes of complex systems, the uncertainties in such forecasts are not usually treated formally. We describe a general Bayesian approach for using a <b>computer</b> <b>model</b> or simulator of a complex system to forecast system outcomes. The approach is based on constructing beliefs derived {{from a combination of}} expert judgments and experiments on the <b>computer</b> <b>model.</b> These beliefs, which are systematically updated as we make runs of the <b>computer</b> <b>model,</b> are used for either Bayesian or Bayes linear forecasting for the system. Issues of design and diagnostics are described in the context of forecasting. The methodology is applied to forecasting for an active hydrocarbon reservoir...|$|R
25|$|Most of what {{is known}} about the {{suggested}} techniques is based on laboratory experiments, observations of natural phenomena, and on <b>computer</b> <b>modelling</b> techniques. Some proposed climate engineering methods employ methods that have analogues in natural phenomena such as stratospheric sulfur aerosols and cloud condensation nuclei. As such, studies about the efficacy of these methods can draw on information already available from other research, such as that following the 1991 eruption of Mount Pinatubo. However, comparative evaluation of the relative merits of each technology is complicated, especially given modelling uncertainties and the early stage of engineering development of many proposed climate engineering methods.|$|E
25|$|Neurolinguistics is {{the study}} of the {{structures}} in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and <b>computer</b> <b>modelling.</b> Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.|$|E
25|$|Caribou are {{monitored}} {{through a}} capture and collaring with VHF or Global Positioning System (GPS) collars tracking collars. Satellite tracking Argos system, a satellite-based system collects, processes and disseminates {{the data from}} caribou tracking collars, clearly locating exact geographically coordinates. Satellite networks have tracked the migration and territorial movements of caribou. Electronic tags are giving scientists a complete, accurate picture of migration patterns. Using radio transmitters to track one herd of caribou, scientists learned that the herd moves much more than previously thought and they learned that each year the herd returns to about the same place to give birth. Environment Canada uses Landsat satellite imagery, for example, to identify anthropogenic disturbance (human-caused disturbance) to the natural landscape. This includes roads, seismic lines, pipe lines, well sites, and cutblocks that accompany industrial activities such as {{oil and gas exploration}} and development and forestry. British Columbia uses telemetry and <b>computer</b> <b>modelling.</b>|$|E
40|$|The {{increasing}} {{amount of}} data, available {{not only from}} your financial department but from personnel, marketing and operations, facilitates monitoring and tracking various small business firm indicators. However, the quantity of data can be overwhelming at times. This is especially true when {{trying to make a}} decision with conflicting information. One {{of the best ways to}} make complicated decisions, where a lot of data is available, is to use <b>computer</b> <b>modeling</b> techniques. A <b>computer</b> <b>model,</b> which is a specific set of variables and their interrelationships designed to represent a situation, helps assess data and simplify the decision-making process. <b>Computer</b> <b>modeling</b> is fast becoming one of the preferred methods in making decisions. <b>Computer</b> <b>modeling</b> is no longer relegate...|$|R
40|$|Abstract—Investigating {{language}} acquisition {{is one of}} the most challenging problems in the area of studying language. Syllable learning as a level of {{language acquisition}} has a considerable significance since it plays an important role in language acquisition. Because of impossibility of studying language acquisition directly with children, especially in its developmental phases, <b>computer</b> <b>models</b> will be useful in examining language acquisition. In this paper a <b>computer</b> <b>model</b> of early language learning for syllable learning is proposed. It is guided by a conceptual model of syllable learning which is named Directions Into Velocities of Articulators <b>model</b> (DIVA). The <b>computer</b> <b>model</b> uses simple associational and reinforcement learning rules within neural network architecture which are inspired by neuroscience. Our simulation results verify the ability of the proposed <b>computer</b> <b>model</b> in producing phonemes during babbling and early speech. Also, it provides a framework for examining the neural basis of language learning and communication disorders. Keywords—Brain <b>modeling,</b> <b>computer</b> <b>models,</b> language acquisition, reinforcement learning...|$|R
40|$|<b>Computer</b> <b>modeling</b> {{technologies}} have increasingly {{become part of}} the conduct of science, public policy making, and the practice of politics. Their contribution to the world can be understood as intellectual, scientific, forecasting, governmental, truth production, and political technologies. This article focuses on the ways in which <b>computer</b> <b>modeling</b> has reconfigured the world of politics as illustrated in a case study of the use of economic modeling in the politics of Australia 2 ̆ 7 s greenhouse gas emissions policy. From 1997, the Australian government pursued a policy of increased greenhouse gas emissions supported by <b>computer</b> <b>modeling</b> that forecast significant negative economic impacts on the Australian economy if emissions were reduced. These modeling results were publicly contested. The case study illustrates how <b>computer</b> <b>models</b> can be used in politics to construct a partisan point of view. It is argued that despite differences in the political use of <b>computer</b> <b>models,</b> their growing complexity constrains the capacity for the conduct of democratic politics...|$|R
