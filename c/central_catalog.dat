15|24|Public
2500|$|The OhioLINK consortium, {{providing}} {{access to}} 12,000+ commercially licensed online journals, 130 databases, 18,000+ ebooks and is a digital media collection. [...] The OhioLINK <b>Central</b> <b>Catalog</b> represents the library holdings of 87 libraries in the state, including the State Library of Ohio, plus the Center for Research Libraries. The collection is nearing 10 million unique records representing 27.5 million holdings in the system, and undergraduates account for the larger percentage of OhioLINK online borrowing – {{the process by which}} any enrolled student can readily request the loan of books and other items from any other library in the system.|$|E
50|$|In July 2010, it was {{announced}} that the entire Open-Apple and Resource <b>Central</b> <b>catalog</b> was reclassified to the Creative Commons 3.0 Attribution License.|$|E
50|$|In 1990, OhioLINK {{selected}} Innovative Interfaces, Inc {{to develop}} the unique software system to create the OhioLINK <b>central</b> <b>catalog</b> and selected Digital Equipment Corporation for the computer hardware base. OhioLINK licensed four databases from University Microfilms International, UMI, for citations to millions of business, newspaper and periodical articles and to academic dissertations. These elements formed {{the foundation of the}} growing OhioLINK system of services. In 1992, six universities installed OhioLINK systems and began the ongoing process of building the <b>central</b> <b>catalog.</b> In February 1996, OhioLINK began offering services through the World Wide Web.|$|E
2500|$|Landscape Paintings of Hunan (<b>Central</b> China) (<b>Catalog),</b> Taiwan, 1989 ...|$|R
40|$|The {{search in}} P 2 P systems {{is still a}} problem because of the dynamic nature of these systems {{and the lack of}} <b>central</b> <b>catalogs.</b> In order to improve the search in P 2 P systems, a sorted {{structure}} is created from the content of all nodes in the system. Different from other approaches, a tree like structure is built by tokens which are constantly moving between the nodes and carrying all structural information...|$|R
5000|$|By 1923, the library's {{numerous}} deficiencies {{were all}} too clear; one dean later recalled the library [...] "consisted of a half-dozen half-filled stacks. Nearly everything was hopelessly out of date." [...] Following a report {{produced by a}} federal expert, the new college president, Eugene Brooks, hired James R. Gulledge (1891-1941) as the first professionally trained head librarian. That fall, all materials in departmental libraries were brought under a <b>central</b> <b>cataloging</b> system {{as a part of}} the main library.|$|R
5000|$|The Pica Foundation {{was created}} in 1969 as an {{initiative}} of the Koninklijke Bibliotheek, the national library of the Netherlands, and several university libraries, in order to advance the Dutch <b>Central</b> <b>Catalog</b> (Nederlandse Centrale Catalogus). The acronym PICA stands for Project for Integrated Catalogue Automation.|$|E
50|$|The OhioLINK consortium, {{providing}} {{access to}} 12,000+ commercially licensed online journals, 130 databases, 18,000+ ebooks and is a digital media collection. The OhioLINK <b>Central</b> <b>Catalog</b> represents the library holdings of 87 libraries in the state, including the State Library of Ohio, plus the Center for Research Libraries. The collection is nearing 10 million unique records representing 27.5 million holdings in the system, and undergraduates account for the larger percentage of OhioLINK online borrowing - {{the process by which}} any enrolled student can readily request the loan of books and other items from any other library in the system.|$|E
50|$|Some tools {{supporting}} direct-to-fan include: storefronts to sell direct-to-fan on band websites and {{on social}} networking sites such as Facebook or MySpace, Widget tools to embed sales, gig or event calendar, profile info anywhere including blogs, Marketing tools such as email marketing and messaging, Event tools, Central Content Management tools, <b>Central</b> <b>Catalog</b> Management tools for both digital and physical products, and digital delivery platforms that {{make it easy for}} you to sell direct from your website or social media by integrating with payment gateways, providing a checkout and automatically delivering music to fans via a download link.|$|E
50|$|Orga Systems {{provided}} {{real time}} charging and billing, integrated policy control and charging, {{as well as}} order management, all driven by a <b>central</b> product <b>catalog.</b>|$|R
40|$|Türkiye Bibliyografyası {{is being}} {{published}} since 1934. Because it lists bibliographical information about ali Turkish publications it is accepted as the bibliographic control tool of Turkish Literatüre. On {{the other hand}} as it gives the bibliographic descriptions according to the AACR 2 {{it can be used}} as the main cenralized cataloging tool of Turkey. in this work-, Türkiye Bibliyografyası is ânalyzed according to its aboye mentioned functions the hyphotesis is stated as below: From {{the point of view of}} contents, arrangement and updatedness the Bibliography does not carry the characteristics of uniformity in cataloging. it does not have 'cataloging consistency' in the use of cataloging rules, and thıs will kinder it from providing f ör sound <b>central</b> <b>cataloging</b> functions. To prove the hypothesis the bibliographic records in Türkiye Bibliyografyası which are picked according to sampling methods are examined starting from 1985 up till 1990. The records that vere picked are evaluated according to their decription area, choise of heading, classification and indexes. As aresult it is found that the ratio of errors is high, there are more arrors at the first years of the use of AACR 2 and the most of errors are in punctuation, there is very low cataloging consistency in Türkiye Bibliyografyası and its indexes are inadequate. For conclusion it can be-said that"Türkiye Bibliyografyası is inadequate as a <b>central</b> <b>cataloging</b> and national bibliographic control tool and with its low 'cataloging consistency' it will effect the other organizations that use it as a. catalogingguide negativety...|$|R
50|$|Participating {{agencies}} also submit metadata {{records to}} a <b>central</b> web-accessible <b>catalog,</b> describing {{the data that}} has been contributed to NGDS. Both NGDS data and the NGDS catalog can be accessed by common web browsers and web applications. NGDS data can also be accessed by geographic information system software applications including ArcGIS, UDig, QGIS, and GvSIG.|$|R
40|$|Consideration of the World Wide Web as a {{tool for}} {{architectural}} education, especially through the production, presentation, and cataloging of critical case studies of buildings. Focuses on development of a collaborative paradigm for distributed development of such information through a case study template and <b>central</b> <b>catalog</b> web site. Includes an sample case study...|$|E
40|$|The {{exponential}} growthoftheInternet {{has resulted}} in whatisknown as the resource discovery problem. Although thenetworkmakes it possible for users toretrieve enormous amounts of information, it provides insufficientsupport for locating the specific information that is needed. Various tools have emerged thateither simplify the task of browsing the network or attempt toindex all the available information in a <b>central</b> <b>catalog.</b> Alib...|$|E
40|$|A Virtual {{union catalog}} is a {{possible}} alternative to the centralized database of distributed resources found in many library systems. Such a catalog would not be maintained in a single location but would be created in real time by searching each local campus or affiliate library’s catalog through the Z 39. 50 protocol. This would eliminate the redundancy of record storage {{as well as the}} expense of loading and maintaining access to the <b>central</b> <b>catalog.</b> This article describes a test implementation of a virtual union catalog for the University of California system. It describes some {{of the differences between the}} virtual catalog and the existing, centralized union catalog (MELVYL). The research described in the paper suggests enhancements that must be made if the virtual union catalog is to become a reasonable service alternative to the MELVYL® catalog...|$|E
50|$|Nimbit artists use {{an online}} {{dashboard}} {{to see and}} manage their music business. This includes sales information, fan information, marketing promotions and results, storefront results, <b>central</b> product <b>catalog</b> (e.g., digital and physical music, merchandise, eTickets), central content management (e.g., band profile and gigs). This allows products and content to be entered once then viewed and changed in one place.|$|R
40|$|The MEDIDEV service {{combines}} a <b>central</b> product <b>catalog</b> featuring basic information with Internet-connected websites and online catalogs hosting the detailed information. This approach {{is compatible with}} the manufacturers' requirement to maintain control over the detailed information of their products. At the same time, it allows the end user to efficiently choose from a potentially long list of products-those which mostl...|$|R
40|$|All major {{experiments}} at Large Hadron Collider (LHC) need {{to measure}} real storage usage at the Grid sites. This information {{is equally important}} for the resource management, planning, and operations. To verify consistency of the <b>central</b> <b>catalogs,</b> experiments are asking sites to provide full list of files they have on storage, including size, checksum, and other file attributes. Such storage dumps provided at regular intervals give a realistic view of the storage resource usage by the experiments. Regular monitoring of the space usage and data verification serve as additional internal checks of the system integrity and performance. Both the importance and the complexity of these tasks increase with the constant growth of the total data volumes during the active data taking period at the LHC. Developed common solutions help to reduce the maintenance costs both at the large Tier- 1 facilities supporting multiple virtual organizations, and at the small sites that often lack manpower. We discuss requirements and solutions to the common tasks of data storage accounting and verification, and present experiment-specific strategies and implementations used within the LHC experiments according to their computing models...|$|R
40|$|This paper {{presents}} {{a relatively simple}} and cheap method for shortening the subject indexes in library catalogs. The method involves taking a set of several dozen general concepts, characterized by a low semantic awareness barrier. Built around these words are subindexes {{made up of the}} words which appear in descriptions containing a particular general concept. The effectiveness of the method was studied by analyzing the content of fragments of subject indexes of the NUKAT <b>central</b> <b>catalog</b> of Polish libraries, the University Library in Poznań and the Library of Congress. Compared with the subject headings language method, this method reduces the length of an index by an average of two-thirds, and makes it significantly easier for readers to navigate the vocabulary used by the cataloger. This method has been developed for the needs of Digital Library of Wielkopolska, and will probably be used in all regional digital libraries in Poland...|$|E
40|$|Clearinghouse (ECHO). The OWS {{provides}} {{specifications for}} accessing digital maps and images, {{geographic information system}} (GIS) data and services. The IVOA provides specifications for accessing astrophysics registries, catalogs, data and services. The PDS-OODT system provides servers to access distributed planetary science registries, catalogs and data. The ECHO system provides a <b>central</b> <b>catalog</b> and order system for accessing distributed collections of earth science data and processing services. These are all &quot;bolt-on &quot; architectures that are added to existing data repositories as middleware to provide standard access protocols. There are many other access architectures in use or under development but it is felt that these four represent a good range of approaches to contemporary distributed data access. The goals of this paper are: • to describe how each scheme works • to compare the different approaches to common problems • to identify any exemplary or noteworthy features of the various schemes Terminology The descriptions {{will focus on the}} services that each architecture provides. The &quot;Service Oriente...|$|E
40|$|Resource sharing through {{interlibrary loan}} has allowed {{libraries}} to offer {{access to information}} far beyond the walls of any one institution. Since the first interlibrary loan transaction, however, librarians have worked to reduce borrowing and lending costs and to improve delivery speed. The Ohio Library and Information Network (OhioLINK), in contract with Innovative Interfaces, Inc., is pushing the idea of resource sharing a step further. OhioLINK 2 ̆ 7 s membership includes fifteen public universities, two private universities, 23 community and technical colleges and the State Library of Ohio. Its goal is to position its member libraries for the future {{so that it is}} established as the information gateway of choice for users of OhioLINK libraries. To accomplish this, the network will cooperatively develop collections, offer a growing number of electronic resources to all member libraries, and improve the interlibrary loan of printed material across all institutions to the point that access is virtually equivalent to ownership. Tom Sanville, Director of OhioLINK, describes the project as a work in progress which may never be finished. He believes that as technology changes and improves, so will the design of the project. The work on this project began in 1987, when the Ohio Board of Regents began investigating the possibility of creating a state-wide automated network so that resources could easily and efficiently be shared at the state level between academic libraries. The network that the Board of Regents envisioned would use current future technology as a tool to improve library service. Successful implementation of this program would increase the research effectiveness and productivity of faculty and students throughout the state of Ohio. In the Fall of 1989 the Request for Proposal for OhioLINK was issued, and eight vendors responded. Innovative Interfaces, Inc. (III was chosen in the Summer of 1990 as the vendor that could most effectively create the OhioLINK system. The committees that worked on the original proposal began working with III on the exact system configurations required to make the project work. One of the first steps of this project was the installation of the Innovative Interfaces, Inc. integrated library system at each of the member institutions. Each local database was merged into a <b>central</b> <b>catalog</b> which was then made available for patron-initiated borrowing. The service, called online borrowing, offers patrons the opportunity to select materials from the catalogs {{of a growing number of}} institutions. The building of the <b>central</b> <b>catalog</b> struted with three libraries in August 1992, and new libraries have been added at the rate of approximately one per month. The patron online borrowing function was activated in January 1994...|$|E
2500|$|The game is {{referred}} to as a [...] "massively single-player online game" [...] and [...] "asynchronous sharing." [...] Simultaneous multiplayer gaming is not a feature of Spore. The content that the player can create is uploaded automatically to a <b>central</b> database, <b>cataloged</b> and rated for quality (based on how many users have downloaded the object or creature in question), and then re-distributed to populate other players' games. The data transmitted is very small— only a couple of kilobytes per item transmitted– due to procedural generation of material.|$|R
40|$|Since 1966, the State Library {{has also}} {{published}} the book <b>catalog</b> for North <b>Central</b> Regional Libraries (NCRL) and now provides catalog data from LC proofslips or original cataloging for all new NCRL titles, but neither orders nor physically processes their materials. Beginning in January 1970, the Timberland and North <b>Central</b> book <b>catalogs</b> have been combined {{into a single}} catalog which shows the libraries holding each title. The experimental MARC book catalog produced for these two library systems and the King County library system is described later in this paper. published or submitted for publicatio...|$|R
5000|$|The game is {{referred}} to as a [...] "massively single-player online game" [...] and [...] "asynchronous sharing." [...] Simultaneous multiplayer gaming is not a feature of Spore. The content that the player can create is uploaded automatically to a <b>central</b> database, <b>cataloged</b> and rated for quality (based on how many users have downloaded the object or creature in question), and then re-distributed to populate other players' games. The data transmitted is very small — only a couple of kilobytes per item transmitted - due to procedural generation of material.|$|R
40|$|A peer {{mediator}} system (PMS) is a decentralized mediator {{system based}} on the P 2 P paradigm, where mediators integrate data sources and other mediators through views defined in a multi-mediator query language. In a PMS mediator peers compose views in terms of views in other peers - mediators and sources, or directly pose queries in the multi-mediator query language to some peer. All peers are fully autonomous {{and there is no}} <b>central</b> <b>catalog</b> or controller. Each peer in a PMS must provide an interface to its data and meta-data sufficient to allow the cooperative processing of queries by the PMS. We analyze the computational capabilities and meta-data that a software system has to export in order to participate as a peer in a PMS. For the analysis we identify and compare six classes of peer interfaces with increasing complexity. For each class we investigate the performance and scalability implications that result from the available capabilities and required meta-data. Our results are two-fold: i) we provide guidelines for the design of mediator peers that can make best use of the interfaces provided by the data sources, and ii) we analyze the tradeoffs in the design of inter-mediator interfaces so that mediator peers can efficiently cooperate to process queries against other composed mediators. Finally we describe the choices made in a concrete implementation of a PMS...|$|E
40|$|We {{investigate}} {{the impact of}} a variety of analysis assumptions that influence cluster identification and location on the kSZ pairwise momentum signal and covariance estimation. Photometric and spectroscopic galaxy tracers from SDSS, WISE, and DECaLs, spanning redshifts 0. 05 <z< 0. 7, are considered in combination with CMB data from Planck and WMAP. With two complementary techniques, analytic offset modeling and direct comparisons of redMaPPer brightest and <b>central</b> <b>catalog</b> samples, we find that miscentering uncertainties average to 0. 4 - 0. 7 σ for the Planck kSZ statistical error budget obtained with a jackknife estimator. We also find that jackknife covariance estimates are significantly more conservative than those obtained by CMB rotation methods. Using redMaPPer data, we concurrently compare the impact of photometric redshift errors and miscentering. At separations <∼ 50 Mpc, where the kSZ signal is largest, miscentering uncertainties can be comparable to JK errors, while photometric redshifts are lower but still significant. For the next generation of CMB and LSS surveys the statistical and photometric errors will shrink markedly. Our results demonstrate that uncertainties introduced through using galaxy proxies for cluster locations will need to be fully incorporated, and actively mitigated, for the kSZ to reach its full potential as a cosmological constraining tool for dark energy and neutrino physics. Comment: Version accepted for publication in PR...|$|E
40|$|We are {{currently}} witnessing data explosion and exponential data growth. I {{will talk about}} real world experience with very large data sets storage and services. We are storing Peta and {{in the near future}} Exa bytes and hundreds or thousands millions of data sets. The one problem is very large number of data objects. File systems were not created to effectively manage thousands of million data items. Inode space is often limited. Storing such large data sets is costly when using rotating storage. Electricity bills for cooling and spinning disks can be prohibitive. Therefore we prefer using magnetic tape technology for the lowest tier of our HSM solution. At OPESOL we employ the LTFS to overcome the old tape technology based limitations and we can provide full POSIX I/O capabilities even for data stored on magnetic tapes The management of masses of data is a key issue: should both ensure the availability and continuity of access for ever. Scientific collaborations are usually geographically dispersed, which requires the ability to share, distribute and manage efficiently and securely. The media, hardware and software storage systems used can differ greatly from one treatment center to another. At this heterogeneity, are added the continuing evolution of storage media (which induces physical migration) and technological developments in software (which may involve changes in the naming or data access protocols the latter). Such environments can take advantage of middleware for the management and distribution of data in a heterogeneous environment, including virtualizing storage, that is to say, hiding the complexity and diversity of systems underlying storage while federating data access. Virtual distributed hierarchical storage system, data grids or data clouds require using and re-using existing underlying storage systems. Creating completely new vertically integrated system is out of the question. iRODS based solutions can take advantage of existing HSM like IBM’s HPSS and TSM, SGI DMF, ORACLE (SUN) SAM QFS or emerging cloud storage system like Amazon S 3, Google, Microsoft Azure and other Good middleware distributed cloud service for very large data sets should work with all main existing such system and be extensible enough to support main future systems. iRODS (integrated Rule based Data System) is being developed for over 20 years mainly by the DICE group bi-located at the University of California at San Diego and the University of North Carolina at Chapel Hill. iRODS provides a rich palette if management tools (metadata extraction, data integrity and more). iRODS can interface with virtually unlimited existing and even future storage technologies (mass storage systems, distributed file systems, relational databases, Amazon S 3, Hadoop and more). iRODS is company agnostic and the users have all the source code. Migration from one storage resource to another (new) one is just one iRODS command regardless of the data size or number of data objects. But what makes iRODS particularly attractive is its rules engine that has no equivalent among its competitors. The rules engine allows complex tasks at data management. These policies are remote management of the server side: for example, when data is stored in iRODS, background tasks can be triggered automatically on the server side such as replication across multiple sites, data integrity checks, post-treatment on them (metadata extraction [...] ) without specific action on the client side. So, the management policy data is virtualized. This virtualization ensures strict rules set by users, regardless of location data or application that accesses iRODS. iRODS like systems can deliver full vertical data storage stack including complex tape system management using the existing standard LFTS technology. OPESOL (Open Solutions Inc.). delivers such a system for free. This solution is using LTFS ([URL] which exists on all modern tape drives and tape libraries I will talk today about sever sites which have chosen a data grid system based on the iRODS (Rule-Oriented Data management) system. IRODS provides a rule-based system management approach which makes data replication much easier and provides extra data protection. Unlike the metadata provided by traditional file systems, the metadata system of iRODS is comprehensive and extensible by user and allows users to customize their own application level metadata. Users can then query the metadata to find and track data. The iRODS is used in production at L'Institut national de physique nucléaire et de physique des particules (IN 2 P 3). The Computing Center of IN 2 P 3 (CCIN 2 P 3) offers iRODS service IN 2 P 3 since 2008. This service is open to all who wish to use it. Currently, 34 groups in the fields of particle physics (BaBar, dChooz [...] .), nuclear physics (Indra, Fazia [...] .), astroparticle physics and astrophysics (AMS, Antares, Auger, Virgo, OHSA [...] .), Science Human and Social (Huma-Num) and biology, using the iRODS service CC-IN 2 P 3 for the management and dissemination of data. The CC-IN 2 P 3 also provides hosting the <b>central</b> <b>catalog</b> of iRODS newly created service of France Grille, as well as support administrators to France on the use of Grid technology. The iRODS service has its own disk servers and is interfaced with our HPSS mass storage (storage on magnetic tape) currently managing over 8 petabytes of data, making it the largest volume service identified internationally. The service is federated with other services such as iRODS SLAC example. In this perspective, it is also quite possible to federate storage servers available in laboratories with iRODS service CC-IN 2 P 3. The BNF (Bibliothèque nationale de France) The Bnf is using iRODS together with open (closed) SAM QFS to store hundreds of million books in its Long-term data preservation. BnF is using iRODS to provide a distributed private data cloud where multiple replicas of data sets are kept at primary BnF site in Paris and secondary site about 40 km From Paris. BnF created a tool to implement its policies for digital preservation SPAR System (Distributed Archiving and Preservation), launched in May 2010, and is continually updated with new collections and feature. BnF employs SPAR and Gallicca for the WEB interface to the distributed private data cloud in iRODS. The NKP and NDK site and project (Czech National Library, Czech National Digital Library). I have helped to implement iRODS together with Fedora Commons and other tools at NKP in Prague Czech republic as a base for the EU funded digital library project. The system is now in a full production. It is using IBMS GPFS and TSM as a base layer for its HSM. The system stores over 300 million data objects. Its data comes nonstop from scanning paper books, electronic data input from Born Digital documents, constant WEB archiving of the “. cz” domain and from all Czech TV and radio broadcasts among others...|$|E
5000|$|Automation Department - this {{department}} is tasked with expanding {{and maintaining the}} local library network and library equipment, providing technical support to library employees, and acquiring licensed software. Department employees also maintain the library facility; administer LIS (Library Information System) [...] "Irbis" [...] and the library’s online electronic catalog; administer CUCC (<b>Central</b> Ukrainian Cooperative <b>Catalog)</b> and KRCC (Kropyvnytskyi Regional Cooperative Catalog); partners {{with the department of}} rare and valuable documents to create the electronic book museum. They introduce new electronic services on the Library Web-site, including its socialization with the help of Web 2.0 technologies.|$|R
40|$|MicroRNAs (miRNAs) are non-coding RNAs (ncRNAs) {{involved}} in regulation of gene expression. Intragenic miRNAs, especially those exhibiting {{a high degree}} of evolutionary conservation, {{have been shown to be}} coordinately regulated and/ or expressed with their host genes, either with synergistic or antagonistic correlation patterns. However, the degree of cross-species conservation of miRNA/host gene co-location is not known and co-expression information is incomplete and fragmented among several studies. Using the genomic resources (miRBase and Ensembl) we performed a genome-wide in silico screening (GWISS) for miRNA/host gene pairs in three well-annotated vertebrate species: human, mouse, and chicken. Approximately half of currently annotated miRNA genes resided within host genes: 53. 0 % (849 / 1, 600) in human, 48. 8 % (418 / 855) in mouse, and 42. 0 % (210 / 499) in chicken, which we present in a <b>central</b> publicly available <b>Catalog</b> of intragenic miRNA...|$|R
40|$|The {{purpose of}} this {{document}} is to present a final description of the original MEDLARS system as it evolved through four years of operation. The system is described as it was functioning on January 1, 1968. Among the various system elements discussed are: (1) the input subsystem, including journal selection and coverage, Medical Subject Headings (MeSH), and indexing; (2) the retrieval subsystem, including request analysis, search formulation, file search, and printout of retrieved citations; and (3) the publication subsystem, including the MEDLARS photocomposer and computer programs for producing MEDLARS publications. A summary of operating experience for the. period includes discussion of system problems, changes, and evaluation. Also discussed is the extension of MEDLARS technology to the cataloging of books and serial titles. This system produces two major products: (1) catalog cards for the <b>central</b> NLM card <b>catalog</b> and (2) the NLM current catalog, a computer-produced book-form catalog available to other medical libraries on a current and frequent basis for use as an acquisitions and cataloging tool. Appended {{is a list of}} approximately 200 selected references. (JW) IM...|$|R
40|$|The {{institutional}} {{context in}} which CIS operate has failed to evolve with either the rate of software and hardware developments, or the numbers of systems installed. This realization has been dawning on the CIS community {{over a number of}} years, and has led to studies such as the Wisconsin Land Records Committee and the Minnesota Inventory of Mapping Systems. The failure of institutions to evolve in the context of GIS is rapidly resulting in costly and repetitive efforts in database development and expertise at all levels of government, and in other areas. In this paper we will present a generalized model for the idealized institutional setting of GIS. The model is independent of implementation and could be realized in a continuous spectrum of contexts from fully manual to fully automated, and in size from Federal Government to small city or private company. The main elements are a <b>central</b> database with <b>catalog</b> and coordinating organization, access by as many users as require, secure databases for users if needed, and the implementation of standards...|$|R
40|$|Many natural {{phenomena}} exhibit size distributions that are power laws or power law type distributions Power laws are specific {{in the sense}} that they can exhibit extremely long or heavy tails. The largest event in a sample from such distribution usually dominates the underlying physical or generating process (floods, earthquakes, diamond sizes and values, incomes, insurance). Often, the practitioner is faced with the difficult problem of predicting values far beyond the highest sample value and designing his "system" either to profit from them, or to protect against extreme quantiles. In this paper we present a novel approach to estimating such heavy tails. The estimation of tail characteristics such as the extreme value index, extreme quantiles, and percentiles (rare events) is shown to depend primarily on the number of extreme data that are used to model the tail. Because only the most extreme data are useful for studying tails, thresholds must be selected above which the data are modeled as power laws. The mean square error (MSE) is used to select such thresholds. A semiparametric bootstrap method is developed to study estimation bias and variance and to derive confidence limits. A simulation study is performed to assess the accuracy of these confidence limits. The overall methodology is applied to the Harvard <b>Central</b> Moment Tensor <b>catalog</b> of global earthquakes. status: publishe...|$|R
40|$|An {{electronic}} {{online access}} showroom, {{which will be}} used to record and to announce the newly acquired books to the users of the Library of the University of Macedonia, is already running in pilot phase. The online showroom includes a specially designed digital "exhibition area", which is accessible via the World Wide Web and which is periodically updated with the new book acqui¬sitions of the Library. The users of the Library can browse the book catalog of this exhibition area, directly from the Wed, with an ordinary Internet Browser. The catalog can be sorted by Date, Author, or Title, according to the user's preference. Also, instead of browsing the book catalog, the users are able to execute a search on the stored records, by Keyword, Title; Author, Subject, ISBN, or Publisher, in order to limit their search to the subset of records that mostly interest them. Furthermore, from the <b>central</b> book <b>catalog,</b> or from their "search-results" list, the users can retrieve more information concerning a specific book, simply by doing a mouse-click on the book title. The returned information includes the table of contents, the cover and the backside of the book, the classification number, the ISBN, the title and subtitle, up to three subject headings rel¬evant to its content, the author(s), the publisher and the year of publication. Additionally, if they wish, they can make a reservation on one or more books, by filling in a simple form, with their full name, their e-mail address and their user code number (a number which is provided to them upon their registration with the Library), in order to be notified as soon as the book(s) become available. This electronic browsing system is available in two languages, Greek and English. The user is able to change the language preference from any screen at anytime. In addition, the online showroom comes with a separate on-line database management envi¬ronment, which is also accessible via the Web but only by authorized users. This environment allows the management of the book records and of the authorized user accounts, including the insertion, deletion and editing of the records and users...|$|R
40|$|Mantle {{processes}} often involve large-scale mass transport, {{ranging from}} mantle convection, tectonic motions, glacial isostatic adjustment, to tides, atmospheric and oceanic loadings, volcanism and seismicity. On {{very short time}} scale of less than an hour, co-seismic event, apart from the "shaking" that is the earthquake, leaves behind permanent (step-function-like) dislocations in the crust and mantle. This redistribution of mass changes the Earth's inertia tensor (and hence Earth's rotation in both length-of-day and polar motion), and the gravity field (in terms of spherical harmonic Stokes coefficients). The question is whether these effects are {{large enough to be}} of any significance. In this paper we report updated calculation results. The calculation uses the normal mode summation scheme, applied to 15, 814 major earthquakes that occurred during 1976 - 1998, according to source mechanism solutions given by the Harvard <b>Central</b> Moment Tensor <b>catalog.</b> Compared to the truly large ones earlier in the century, the earthquakes we study are individually all too small to have left any discernible signature in geodetic records of Earth rotation or global gravity field. However, their collective effects continue to exhibit an extremely strong statistical tendencies. For example, earthquakes conspire to decrease J(sub 2) and J(sub 22) while shortening LOD, resulting in a rounder and more compact Earth. Strong tendency is also seen in the earthquakes trying to "nudge" the Earth rotation pole towards about 140 degree E, roughly opposite to the observed polar drift direction. The geophysical significance and implications will be further studied...|$|R
40|$|The {{detection}} of radio pulsars within the central few parsecs of the Galaxy {{would provide a}} unique probe of the gravitational and magneto-ionic environments in the Galactic Center (GC) and, if close enough to Sgr A*, precise tests of general relativity in the strong-field regime. While {{it is difficult to}} find pulsars at radio wavelengths because of interstellar scattering, the payoff from detailed timing of pulsars in the GC warrants a concerted effort. To motivate pulsar surveys and help define search parameters for them, we constrain the pulsar number and spatial distribution using a wide range of multiwavelength measurements. These include the five known radio pulsars within 15 arcmin of Sgr A*, radio and gamma-ray measurements of diffuse emission, non-detections in high frequency pulsar surveys of the <b>central</b> parsec, a <b>catalog</b> of radio point sources from an imaging survey, infrared observations of massive star populations in the central few parsecs, candidate pulsar wind nebulae in the inner 20 pc and estimates of the core-collapse supernova rate based on X-ray measurements. We find that under current observational constraints, the inner parsec of the Galaxy could harbor as many as ~ 10 ^ 3 active radio pulsars that are beamed towards Earth. Such a large population would distort the low-frequency measurements of both the intrinsic spectrum of Sgr A* and the free-free absorption along the line of sight of Sgr A*. Comment: 17 pages, 4 figures, 4 tables, Published in ApJ, Updated to match published versio...|$|R
40|$|A {{fast growing}} south Florida school {{district}} struggled with providing needed <b>central</b> <b>cataloging</b> and processing services to its 103 school centers for library books and non-print media materials. Previous methods employed involved the manual typing of spine labels, book/material check out cards and pockets, and either the original production of catalog cards, the duplication of cards {{held in the}} master file or the ordering of available cards from the Library of Congress by U. S. Mail. Prior analysis by the researcher indicated that a computer-based bibliographic retrieval system, properly configured to meet district and school specifications, might be implemented to eliminate the mail ordering of card sets from the Library of Congress and serve to simplify and expedite the 2 ̆ 2 in-house 2 ̆ 2 production of cards and processing of materials not cataloged by the Library of Congress. It was assumed by the researcher that the providing of district-wide cataloging services and full 2 ̆ 2 shelf-ready 2 ̆ 2 processing of media materials to 103 school centers was a significant study worthy of review and relevant to existing problems in the information science field. A comprehensive search of professional literature was conducted to obtain more information about currently used bibliographic retrieval systems - their merits and disadvantages. Media supervisors in selected colleges and other Florida school districts were queried for their input about research conducted and solutions they employed relative to the selection phase of the study. Based on the information gathering process, possible retrieval systems and/or ancillary products capable of solving the institutional problem were identified. Selected vendors were contacted for specific information about their individual products that was further analyzed for possible acquisition. Based on information received from all sources, the Biblio-File system {{was found to be}} the most cost-effective solution, and the one most capable of enhancing cataloging and processing operations. Its purchase was recommended to, and approved by, higher level district administrative personnel. Once the system was received, it had to configure to insure that produced materials were consistent with both existing institutional guidelines and the MARC, AACR II and ISBD formats. During this phase, existing personnel were trained to use the system and queried for input relative to its implementation. Care was taken during this phase to insure that existing cataloging and processing standards, etc. we’re not sacrificed by an inadvertent enthusiasm to effect positive implementation of the system. By the same token, safeguards were taken to insure that dislike of change, particularly, automated change, on the part of existing personnel, and did not adversely affect the implementation of the system. During the configuration and limited implementation stages, which lasted two months, many procedural changes were identified that would enhance the full implementation of the system. Configuration adjustments were made throughout the configuration and limited implementation stages until system produced materials were of the desired quality and format. Once the system was up and running and producing materials at a high level of staff satisfaction, system utilization moved into the full implementation stage. During this six month phase the system was used to produce processing materials for all books and audio visual materials cataloged by the Library of Congress. Additionally, the system was used for the in-house production of processing materials for books and audio visual materials for which there was no cataloging data either in the system database or in the district master file. During this phase, many procedural changes were identified and implemented, resulting in the writing of revised procedures for the Processing Section. Significant hardware changes were effected during this phase to enhance the production capabilities. Following the full implementation phase, it became necessary to evaluate the system for effect. In the researcher 2 ̆ 7 s opinion, system evaluation had to be based on both a survey of school media specialists relative to their needs and expectations and an in-house time-cost study effected at the institutional level to determine relative costs or savings of the new system as opposed to the preexisting procedures. In that regard, an evaluative instrument was constructed and distributed to district media personnel that facilitated the gathering of data about the effectiveness of the newly operational system from their point of view. Also, a time-cost study comparing the production of processing materials, under the old set of procedures and with the new system, was conducted by gathering direct time measurement data of the cataloging and processing functions. Results from both analyses strongly indicated that system production was viewed favorably from both the standpoint of district school media specialists and administratively from a cost-effectiveness point of view. Several recommendations from both staff and media specialists were analyzed and incorporated into the system production capability. Additionally, the researcher has considered several future measures that would facilitate the storage of cataloging data into a proposed district union catalog. The researcher was able to supervise the selection, installation, configuration, implementation and evaluation of the Biblio-File system...|$|R
