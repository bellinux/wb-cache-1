162|196|Public
50|$|The actual main {{conclusions}} {{are based in}} the identification of different types of retinal neurons, each one with a different <b>coverage</b> <b>factor</b> value revealing graded degrees of homotypic dendritic repulsion. Developmental sequence accepted is 1) define number and spacing of cells, 2) controlled growth of branches and 3) fine-tune of dendritic tiling for maximal coverage of the structure. Experiments with mutant mice for Math5 and Brn3b (responsible of degeneration of 95% and 80% of retinal ganglion cells, respectively) shows that removal of ganglion cells doesn't decrease the retinal ganglion cell types and that position of these cells isnot defined by dendritic homotypic interactions only, but for some kind of intrinsic genetic program.|$|E
30|$|Kumar and Kumar (2011) {{stated that}} the {{probability}} of successful reconfiguration operation of a fault-tolerant system {{is defined as a}} <b>coverage</b> <b>factor.</b> It is denoted by ‘c’ and if its value is less than 1, then it is known as imperfect coverage. Ram et al. (2013) defined the <b>coverage</b> <b>factor</b> as the conditional probability of recovery, given that a fault has occurred. The <b>coverage</b> <b>factor</b> {{is one of the most}} important aspects to take into account in design, management and evaluation of fault-tolerant systems.|$|E
30|$|Effect {{of repair}} rate of {{repairman}} and <b>coverage</b> <b>factor</b> (b, c): From Figs.  4 and 7, it is {{noticed that the}} availability A(t) of the system increases as repair rate (b) increases. The average number of failed machines EN(t) reveals the increasing trend as the <b>coverage</b> <b>factor</b> c increases.|$|E
30|$|Motivated by {{this fact}} {{and to improve}} the {{efficacy}} of the generated test sets, this paper now extends the previous work by instantiating and evaluating the approach with a new objective: the pairwise coverage. The idea is to obtain a set of products to the feature testing with a minimal number of test cases, factor related to the cost, and high mutation score and pairwise <b>coverage,</b> <b>factors</b> related to the quality and efficacious to reveal faults.|$|R
50|$|The {{next day}} the Italians responded by attacking the {{population}} of the neighboring Mechiya oasis, killing thousands of people including women and children over the course of three days. Though the Italians allegedly took measures to prevent news of this action from reaching the outside world, foreign press correspondents covered the event in detail. This negative <b>coverage</b> <b>factored</b> into the British Parliament's decision later that month to take a more pro-Turkish course, rejecting a proposed Anglo-Italian Mediterranean agreement.|$|R
40|$|Abstract—We {{present an}} {{iterative}} algorithm for calibrating vector network analyzers based on orthogonal distance regression. The algorithm features a robust, yet efficient, search algorithm, an error analysis that includes both random and systematic errors, a full covariance matrix relating calibration and measurement errors, 95 % <b>coverage</b> <b>factors,</b> and an easy-to-use user interface that supports {{a wide variety}} of calibration standards. We also discuss evidence that the algorithm outperforms the MultiCal software package in the presence of measurement errors and accurately estimates the uncertainty of its results. Index Terms—Calibration, measurement, scattering parameter, uncertainty, vector network analyzer (VNA). I...|$|R
40|$|DAccording to the Guide to the Expression of Uncertainty in Measurement, a <b>coverage</b> <b>factor</b> that {{produces}} an expanded uncertainty having an approximate {{level of confidence}} is recommended in the final expression of the measurement result. The numerical value for the <b>coverage</b> <b>factor</b> depends on the probability density function. When a Type A evaluation following a Gaussian (normal) distribution is combined with a Type B one following a rectangular distribution, the resulting probability density function is neither Gaussian nor rectangular, but similar to both, which we term a Flatten–Gaussian probability density function. The variance of such a function is the combined variance in the usual way, but the <b>coverage</b> <b>factor</b> must be calculated...|$|E
40|$|In {{this paper}} we propose a new {{coverage}} model which shows improvement in the availability of the system and the reduction in the cost which are important measures for reliability. The present study discusses the effect of <b>coverage</b> <b>factor</b> on the reliability characteristics of a parallel redundant complex system. The system is analysed under Preemptive resume-repair policy. Using the concept of <b>coverage</b> <b>factor,</b> the availability and cost analysis of the system have been computed and analysed by graphical illustrations...|$|E
40|$|One of the {{key issues}} in the {{quantitative}} evaluation of programmable electronic systems is the diagnostic capability of the equipment. This is measured by a parameter called the <b>Coverage</b> <b>Factor,</b> C. This factor can vary widely. The range of possible values is often the subject of great debate. Within limits, the diagnostic <b>coverage</b> <b>factor</b> can be calculated by knowing which component failure modes are detected by diagnostics. An extension of the Failure Modes and Effects Analysis (FMEA) {{can be used to}} show this information. This extension, called a Failure Modes, Effects and Diagnostic Analysis can serve as a useful design verification tool as well as a means to provide more precise input to reliability and safety modeling...|$|E
40|$|Abstract. The {{effects of}} {{variations}} in the workload input when estimating error detection coverage using fault injection are investigated. Results from scanchain implemented fault injection experiments using the FIMBUL tool on the Thor microprocessor show that the estimated error non-coverage may vary by more than five percentage units for different workload input sequences. A methodology for predicting error coverage for a particular input sequence based on results from fault injection experiments with another input sequence is presented. The methodology {{is based on the}} fact that workload input variations alter the usage of sensitive data and cause different parts of the workload code to be executed different number of times. By using the results from fault injection experiments with a chosen input sequence, the error <b>coverage</b> <b>factors</b> for the different parts of the code and the data are calculated. The error coverage for a particular input sequence is then predicted by means of a weighted sum of these <b>coverage</b> <b>factors.</b> The weight factors are obtained by analysing the execution profile and data usage of the input sequence. Experimental results show that the methodology can identify input sequences with high, medium or low coverage although the accuracy of the predicted values is limited. The results show that the coverage of errors in the data cache is preferably predicted using data usage based prediction while the error coverage for the rest of the CPU is predicted more favourably using execution profile based prediction. ...|$|R
40|$|International audienceDependability {{estimation}} of a fault tolerant computer system (FTCS) perturbed by single event upsets (SEUs) requires obtaining first the probability distribution functions {{for the time}} to recovery (TTR) and the time to failure (TTF) random variables. The application cross section (AP) approach does not give directly all the required information. This problem can be solved {{by means of the}} construction of suitable Markoff models. In this paper, a new method for constructing such models based on the system's failure and <b>coverage</b> <b>factors</b> is presented. Analytical dependability estimation is consistent with fault injection experiments performed in a fault tolerant operating system developed for a complex, real time data processing system...|$|R
40|$|Non-peer-reviewedThe use {{of several}} {{distinct}} recovery procedures {{is one of}} the techniques {{that can be used to}} ensure high availability and fault-tolerance of computer systems. This method has been applied to telecommunications systems and usually uses redundant hardware and special recovery software to restore the system after hardware and software failures. We propose a simple practical analytical approach to availability evaluation of systems with several recovery procedures based on a new ‘segregated failures’ model. To illustrate this method, it is applied to availability evaluation of a Lucent Technologies Reliable Clustered Computing application. Detailed numerical results are provided and the impact of various types of failures and <b>coverage</b> <b>factors</b> on down time is analysed...|$|R
40|$|Abstract − Measurements are {{nowadays}} permanent attendants {{of scientific}} research, health, {{medical care and}} treatment, industrial development, safety and even global economy. All of them depend on accurate measurements and tests, {{and many of these}} fields are under the legal metrology because of their severity. How the measurement is accurate, is expressed by uncertainty, which is obtained by multiplication of standard deviations by coverage factors to increase trustfulness in the measured results. These coverage factors depend on degree of freedom, which is the function of the number of implied repetitions of measurements, and therefore the reliability of the results is increased. The standard <b>coverage</b> <b>factor</b> is 1. 96 for normal (Gaussian) distributions or near-Gaussian distributions, and the obtained expanded uncertainty has the 95 % statistical probability. In general, {{it is not possible to}} achieve the 95 % confidence interval by using the standard <b>coverage</b> <b>factor</b> 1. 96, nevertheless of the degree of freedom. The present paper describes the method of estimating the expanded uncertainty by an algorithm of this model based on the 95 % confidence interval of any probability distribution of any shape, dealing with the A-type or the B-type uncertainty. Furthermore the <b>coverage</b> <b>factor</b> is determined due to the 95 % confidence interval of the actual probability distribution. The algorithm successfully copes with adding two or more uncertainties with mathematical properties of sums, and is established in accordance with the standards and guides. The model is introduced in procedures carried out in the calibration laboratory...|$|E
30|$|An {{uncertainty}} source may be ‘Type A’, {{which is}} evaluated by {{statistical analysis of}} a series of observations, or ‘Type B’, which is evaluated by using means other than the statistical analysis {{of a series of}} observations. To calculate standard uncertainty from the parameters of two most important distribution functions, if the limits x[*]±[*]a are given without a confidence limit, it is appropriate to assume a rectangular distribution {{with a standard deviation of}} a/√ 3, but if values are given with confidence level, it is triangular distribution with a standard deviation of a/√ 6 (Ellison et al. 2000). Normal distribution is assumed when an estimate is made from repeated observation of a randomly varying process. In analytical chemistry, an expanded uncertainty (U) is used, which is obtained by multiplying the combined standard uncertainty, by a <b>coverage</b> <b>factor</b> k. The choice of <b>coverage</b> <b>factor</b> is based on the level of confidence. For an approximate level of confidence of 95 %, the value of k is 2.|$|E
40|$|Problem statement: The {{purpose of}} this study was to compute fuzzy {{reliability}} and fuzzy availability of the serial process in butter-oil processing plant for various choices of failure and repair rates of sub-system. This plant consists of eight sub-systems out of which two are supported by stand-by units with perfect switch over devices and considered that these two sub-systems never fail. The effect of <b>coverage</b> <b>factor</b> on the fuzzy availability also studied. Approach: In this study the chapman-Kolmogorov differential equations were formed using mnemonic rule from the transition diagram of the butter-oil processing plant. These equations were solved for steady state recursively and results were obtained by computer program. Results: Result in the study analyzed fuzzy availability for various values of system <b>coverage</b> <b>factor,</b> failure and repair rates. Industrial implications of the results also briefly discussed. Conclusion: The findings in the study suggested that the management of butter-oil processing plants sensitive sub-system is important to improve its performance...|$|E
40|$|We {{study the}} {{statistical}} inferences of an availability system with imperfect coverage. The system {{consists of two}} active components and one warm standby. The time-to-failure and time-to-repair of the components are assumed to follow an exponential and a general distribution respectively. The <b>coverage</b> <b>factors</b> for an active-component failure and for a standby-component failure {{are assumed to be}} the same. We construct a consistent and asymptotically normal estimator of availability for such repairable system. Based on this interval estimation and testing hypothesis are performed. To implement the estimator, simulation inference for the system availability, we adopt two repair-time distributions, namely, lognormal and Weibull and three types of Weibull distributions characterized by their shape parameters are considered. Finally, all simulation results are displayed in appropriate tables and curves for highlighting the performance of the statistical inference procedures. (C) 2009 Elsevier B. V. All rights reserved...|$|R
40|$|This paper {{includes}} {{an examination of}} the decline in foreign news <b>coverage,</b> the <b>factors</b> contributing to the decline, the implications of decreased coverage in a democratic society, the market for international news, and an explanation of various models of foreign correspondence in relation to the quality of work produced contrasted against the cost of production...|$|R
30|$|Consequently, {{we sought}} to {{estimate}} the extent of off-label use of any chemotherapies, targeted agents, hormone therapies, and immunotherapies approved by the FDA as a cancer therapy {{at the time of}} this study and investigate whether drug <b>coverage</b> or <b>factors</b> such as patient demographics, drug, treatment centre and physician characteristics influence off-label prescribing among breast cancer patients.|$|R
40|$|The {{calibration}} of dial calliper is {{the form}} of activity to determine {{the truth of the}} conventional value by using measuring ruler with the measuring clock as the substitute of a nonius scale. The degree of accuracy from a dial inching ruler {{is the same as the}} nonius inching bar, 0. 10 mm, 0. 05 mm or 0. 02 mm. On the dial inching bar with the accuracy of 0, 10 mm by using the standard JIS B 7507 - 1993 method. As a result of analysis, the uncertainty value of the measuring instrument from dial caliper I at a rate of trust 95 % with a <b>coverage</b> <b>factor</b> of k = 2 is U 95 = ± 9. 50. The uncertainty value of the dial caliper II at a rate of trust 95 % with the <b>coverage</b> <b>factor</b> k = 2 is U 95 = ± 9, 50. The uncertainty value of the dial caliper III at a rate of trust 95 % with the <b>coverage</b> <b>factor</b> k = 2 is U 95 = ± 9, 50. Measurement error / deviation measuring clock caliper (dial calliper) I and II with the measuring block measuring 150 mm is 0. 00007 mm. While the clock measuring caliper (dial calliper) III with the measuring block measuring 150 mm is - 0. 04993 mm. Based on the table errors that allowed JIS B 7507 - 1993 is ± 0. 07 mm so caliper measuring clock (dial calliper) I, II and III are still within the allowed tolerance limit mistakes and still feasible to be used...|$|E
40|$|Gamma ray burst {{outflows}} may entrain small blobs or filaments of dense, highly ionized metal rich material. Such inhomogeneities, {{accelerated by}} the flow to Lorentz {{factors in the}} range 10 - 100, could have a significant <b>coverage</b> <b>factor,</b> and give rise to broad features, especially due to Fe K-edges, which influence the burst spectrum below the MeV range, leading to a progressively decreasing hardness ratio...|$|E
40|$|Abstract: Problem statement: The {{purpose of}} this study was to compute fuzzy {{reliability}} and fuzzy availability of the serial process in butter-oil processing plant for various choices of failure and repair rates of sub-system. This plant consists of eight sub-systems out of which two are supported by standby units with perfect switch over devices and considered that these two sub-systems never fail. The effect of <b>coverage</b> <b>factor</b> on the fuzzy availability also studied. Approach: In this study the chapman-Kolmogorov differential equations were formed using mnemonic rule from the transition diagram of the butter-oil processing plant. These equations were solved for steady state recursively and results were obtained by computer program. Results: Result in the study analyzed fuzzy availability for various values of system <b>coverage</b> <b>factor,</b> failure and repair rates. Industrial implications of the results also briefly discussed. Conclusion: The findings in the study suggested that the management of butteroil processing plant’s sensitive sub-system is important to improve its performance. Key words: Manufacturing system, modeling, markov processes, fuzzy reliabilit...|$|E
50|$|Disappointingly {{moved to}} a Friday night to {{accommodate}} a meaningless England v Saudi Arabia friendly match, the Play-Off Final with Torquay attracted just 19,486 with live TV <b>coverage</b> another <b>factor.</b> David Gregory's 22nd minute penalty was enough to fire U's back to the third tier after 17 years away. Once again the streets of Colchester thronged to an open-top bus parade.|$|R
40|$|Abstract. Search {{engines are}} the primary means by which people locate {{information}} on the Web. Unfortunately most Web users are not information retrieval experts {{and there is a}} tendency for Web queries to be ambiguous and under-specified. Query expansion and recom-mendation techniques offer one way to solve the ambiguous query problem in Web search, by automatically identifying and adding new terms to a vague query in order to focus the search. In this paper we describe and evaluate a novel query recommendation technique based on reusing previous search histories. This is achieved by select-ing, ranking, and then recommending previously successful queries to users. Its novelty stems from the way in which queries are scored and ranked using relevance and <b>coverage</b> <b>factors</b> in order to priori-tise those queries that {{are most likely to be}} successful in the current search context. We demonstrate that these recommendations can lead to improved search performance based on live-user data. ...|$|R
40|$|The aim of {{this paper}} is to gain insight into the {{potential}} use of random effects choice models as analytical instruments. These instruments then may be used to identify the latent predisposition and benefit segments of a market in the context of store selection, specifically, in the context of the selection of store format. First, the theoretical bases, formulations, and practical aspects that make up the modelling technique in question are reviewed and revised. Second, an adaptation to detect latent segments with predispositions to shop at a store format, isolated from spatial <b>coverage</b> <b>factors,</b> is proposed. Third, the proposal is empirically tested in the case of the hypermarket. The results confirm the explanatory possibilities of this modelling approach and show that various segments with different levels of response to the hypermarket exist. They can be characterised ad hoc by means of demographic and sociological variables to facilitate their identification, access, and evaluation, with a view to possible marketing strategies. Random utility choice models Market heterogeneity Random coefficient choice models Latent segments Store format choice...|$|R
40|$|A {{diffraction}} geometry utilizing convergent X-rays from a polycapillary optic {{incident on}} a stationary crystal is described. A mathematical simulation {{of the resulting}} diffraction pattern (in terms of spot shape, position and intensity) is presented along with preliminary experimental results recorded from a lysozyme crystal. The effective source <b>coverage</b> <b>factor</b> is introduced to bring the reflection intensities onto the same scale. The feasibility of its application to macromolecular crystal data collection is discussed...|$|E
30|$|A {{system is}} known to be fault tolerant, if it can {{tolerate}} some faults and function successfully even in the presence of these faults. It is generally achieved by using redundancy concepts. Automatic recovery and reconfiguration mechanism (detection, location and isolation) plays a crucial role in implementing fault tolerance, because an uncovered fault may lead to a system or subsystem failure even when adequate redundancy exists. Hence, a system subjected to imperfect fault coverage (also known as <b>coverage</b> <b>factor)</b> may fail prior to the exhaustion of redundancy due to uncovered component failures.|$|E
40|$|The isotope {{abundance}} {{ratio of}} lithium in the Li isotopic reference material IRMM- 016 was redetermined by thermal ionisation mass spectrometry {{by means of}} synthetic isotope mixtures. This resulted in an absolute ratio n(6 -Li) /(7 -Li) of 0. 08212 +/- 0. 00028, and corresponding isotope abundances of 7. 589 +/- 0. 024 amount% 6 -Li, 92. 411 +/- 0. 024 amount% 7 -Li and an atomic weight of lithium of 6. 94005 +/- 0. 00024. Uncertainties are expanded uncertainties using a <b>coverage</b> <b>factor</b> of two. JRC. D-Institute for Reference Materials and Measurements (Geel...|$|E
5000|$|... "SSDI and SSI ... {{the rapid}} {{escalation}} of {{costs and the}} narrowing of employer insurance <b>coverage</b> ... and other <b>factors</b> ... keep the American Dream {{out of reach for}} many Americans with disabilities." [...] ("Disability in America," [...] 2006) ...|$|R
30|$|Evidence {{from around}} the world shows that firms factor in costs {{associated}} with benefits they provide women (e.g. maternity and family leave, maternity insurance <b>coverage).</b> They also <b>factor</b> costs for replacing women during longer work absences (Ruhm, 1998).|$|R
50|$|A {{claim of}} {{defamation}} is defeated if the defendant proves {{on the balance}} of probabilities that the statement was true. If the defence fails, a court may treat any material produced by the defence to substantiate it, and any ensuing media <b>coverage,</b> as <b>factors</b> aggravating the libel and increasing the damages. A statement quoting another person cannot be justified merely by proving that the other person had also made the statement: {{the substance of the}} allegation must be proved.|$|R
40|$|NIST-built 10 T{Omega} and {{commercial}} 1 T{Omega} standard resistors were hand carried between NIST and Sandia {{for a high}} resistance comparison. The comparison tested the ruggedness of the new NIST-built standard resistors, provided {{a check of the}} scaling between the two laboratories, supported measurements to reestablish NIST calibration services at 10 T{Omega} and 100 T{Omega}, and demonstrated the possibility of establishing a NIST high resistance measurement assurance program (MAP). The comparison has demonstrated agreement on the order of 0. 07 % which is within the expanded uncertainties (<b>coverage</b> <b>factor</b> = 2) of NIST and Sandia at 1 T{Omega} and 10 T{Omega}...|$|E
40|$|This paper deals {{first of}} all with an {{improvement}} of the Kelvin probe (KP) theory taking into consideration the series resistance of the input circuit. Then it illustrates a number of work function measurements performed on self-assembled monolayers interacting with varieties of analytes, and on Langmuir-Blodgett (LB) films of porphyrins of different thickness. The output intensities of the work function have been investigated and comments are given of the results obtained. The link between the work function and thickness of material under test has been analyzed and discussed as a method for the <b>coverage</b> <b>factor</b> estimation of absorbing surfaces...|$|E
40|$|International audienceThe paper {{presents}} a new reference correlation for the viscosity of squalane at 0. 1 MPa. The correlation should be valuable {{as it is}} the first to cover a moderately high viscosity range, from 3 to 118 mPa[*]s. It is based on new viscosity measurements carried out for this work, as well as other critically evaluated experimental viscosity data from the literature. The correlation is valid from 273 to 373 K at 0. 1 MPa. The average absolute percentage deviation of the fit is 0. 67, and the expanded uncertainty, with a <b>coverage</b> <b>factor</b> k = 2, is 1. 5 %...|$|E
40|$|In this study, {{the effects}} of {{activation}} time and temperature, as {{two of the most}} prominent parameters affecting the porous structure of the carbonaceous materials, have been evaluated. It has been demonstrated that increasing the activation degree enhances the porous structure of the prepared activated carbons. The Methylene Blue (MB) adsorption capacity of the activated carbon with the highest surface area has been determined to be higher than that of a commercial activated carbon, F 400, although the surface areas of these two adsorbents are very close. It has been attributed to the higher "effective" surface area of the former adsorbent and thus accessibility of more pores for dye adsorption. Also, the mechanism of Methylene Blue adsorption by tyre char activated carbon has been elucidated by isotherm modeling. It has been demonstrated that since the exponent of the best-fit isotherm model, Redlich-Peterson, approaches unity, a monolayer dye adsorption on a surface with homogeneous active sites can be assumed. Considering the MB adsorption capacities of the produced activated carbons, the MB molecule size and the effective surface areas of the adsorbents, the MB <b>coverage</b> <b>factors</b> have been calculated and the possible MB adsorption orientation has been proposed and modeled. © 2015 Elsevier Ltd. All rights reserved...|$|R
40|$|The {{most popular}} forms of fault {{tolerance}} against design faults use "asymmetric" architectures {{in which a}} "primary" part performs the computation and a "secondary" part {{is in charge of}} detecting errors and performing some kind of error processing and recovery. In contrast, the most studied forms of software fault tolerance are "symmetric" ones, e. g. N-version programming. The latter are often controversial, the former are not. We discuss how to assess the dependability gains achieved by these methods. Substantial difficulties have been shown to exist for symmetric schemes, but we show that the same difficulties affect asymmetric schemes. Indeed, the latter present somewhat subtler problems. In both cases, to predict the dependability of the fault-tolerant system {{it is not enough to}} know the dependability of the individual components. We extend to asymmetric architectures the style of probabilistic modeling that has been useful for describing the dependability of "symmetric" architectures, to highlight factors that complicate the assessment. In the light of these models, we finally discuss fault injection approaches to estimating <b>coverage</b> <b>factors.</b> We highlight the limits of what can be predicted and some useful research directions towards clarifying and extending the range of situations in which estimates of coverage of fault tolerance mechanisms can be trusted...|$|R
40|$|Session - Public Transit in Urban SocietiesThe idea {{of using}} transit-oriented {{development}} (TOD) in reducing automobile dependency and improving the sustainability of transportation activities is gaining popularity in recent years. The existing literature has shown that residents living in TOD neighborhood use transit more frequently than other people having similar socio-economic characteristics but living elsewhere. Most of the existing studies on TOD and transit ridership used recently developed sites or suburban neighborhoods as case studies. However, limited research {{studies have been conducted}} on TOD using comprehensive station-level data for all major transit stations at the city level. Due to the more comprehensive <b>coverage,</b> <b>factors</b> affecting transit ridership may be different from the existing studies on specific stations only. By using the heavy rail systems in New York City and Hong Kong as case studies, factors which are expected to contribute to higher rail transit ridership will be analyzed by using multiple regressions. The results show that a combination of variables in different dimensions, including land use, station characteristics and human factors, is important in accounting for higher rail transit ridership and successful rail-based TOD. The findings are of value to municipal governments hoping to promote rail-based TOD in their cities...|$|R
