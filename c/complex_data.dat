5210|5055|Public
25|$|Third-party {{software}} {{available for}} use on BlackBerry devices includes full-featured database management systems, {{which can be used}} to support customer relationship management clients and other applications that must manage large volumes of potentially <b>complex</b> <b>data.</b>|$|E
25|$|BiFC {{does not}} require {{specialised}} equipment, as visualisation is possible with an inverted fluorescence microscope that can detect fluorescence in cells. In addition, analysis {{does not require}} <b>complex</b> <b>data</b> processing or correction for other sources of fluorescence.|$|E
25|$|Virtual Situation Room (VSRoom) {{provides}} unified, real-time {{views to}} the information provided by your monitoring systems. With VSRoom {{you will be able}} to collect, visualize and share monitoring data collected from your critical infrastructure. It provides beautiful situation overviews of <b>complex</b> <b>data</b> for decision makers and first line operation centers.|$|E
40|$|<b>Complex</b> survey <b>data,</b> as {{highlighted}} {{in this issue}} of Evaluation Review, provide a wealth of opportunities for answering methodological and/or applied research questions. However, the analytic issues of nonindependence and unequal selection probability must be addressed when analyzing this type of data. Thus, to ensure that research questions are accurately answered when using <b>complex</b> survey <b>data,</b> researchers must take extra precautions to ensure <b>complex</b> survey <b>data</b> are correctly analyzed. The {{purpose of this article is}} to provide software recommendations for analysis of and tips on troubleshooting when analyzing <b>complex</b> sample <b>data.</b> <b>complex</b> samples; survey weights; sampling weights; survey research; statistical software...|$|R
50|$|Combat {{reality is}} {{simulated}} and represented in <b>complex</b> layered <b>data</b> through HMD.|$|R
40|$|Experimental data on {{the atomic}} {{cracking}} of propyl radicals and on the deuterization of methyl radicals are compared with some theoretical calculations. With the aid of some assumptions concerning intramolecular energy transfer in the dissociating molecules involved in these and other reactions and concerning the corresponding activated <b>complexes,</b> <b>data</b> {{on a number of}} free radical reactions are correlated...|$|R
25|$|BIND {{is based}} on a data {{specification}} written using Abstract Syntax Notation 1 (ASN.1) language. ASN.1 is used also by NCBI when storing data for their Entrez system and because of this BIND uses the same standards as NCBI for data representation. The ASN.1 language is preferred because it can be easily translated into other data specification languages (e.g. XML), can easily handle <b>complex</b> <b>data</b> and can be applied to all biological interactions – not just proteins. Bader and Hogue (2000) have prepared a detailed manuscript on the ASN.1 data specification used by BIND.|$|E
25|$|In cult leader-like fashion, Lore had {{manipulated}} {{them into}} following him {{by appealing to}} their restored emotions and exploiting their new-found senses of individuality and fear, hoping to turn them on the Federation. Lore also corrupts Data {{through the use of}} the emotion chip he had stolen from Noonien Soong (Data and Lore's creator). In the end, Data's ethical subroutines are restored (having been suppressed by Lore through use of the emotion chip) and he manages to deactivate Lore after a battle in which a renegade Borg faction led by Hugh attacks the main <b>complex.</b> <b>Data</b> reclaims the emotion chip, Lore is mentioned as needing to be dismantled (for safety) and the surviving Borg fall under the leadership of Hugh. The fate of these deassimilated Borg is not revealed, though non-canon material suggests that they remained on the planet and established a permanent colony.|$|E
25|$|Computational {{advances}} in pharmacogenomics {{has proven to}} be a blessing in research. As a simple example, for nearly a decade the ability to store more information on a hard drive has enabled us to investigate a human genome sequence cheaper and in more detail with regards to the effects/risks/safety concerns of drugs and other such substances. Such computational advances are expected to continue in the future. The aim is to use the genome sequence data to effectively make decisions in order to minimise the negative impacts on, say, a patient or the health industry in general. A large amount of research in the biomedical sciences regarding Pharmacogenomics as of late stems from combinatorial chemistry, genomic mining, omic technologies and high throughput screening. In order for the field to grow, rich knowledge enterprises and business must work more closely together and adopt simulation strategies. Consequently, more importance must be placed on the role of computational biology with regards to safety and risk assessments. Here, we can find the growing need and importance of being able to manage large, <b>complex</b> <b>data</b> sets, being able to extract information by integrating disparate data so that developments can be made in improving human health.|$|E
5000|$|Genedata Screener - For {{analysis}} {{and management of}} plate-based and more <b>complex</b> High-throughput <b>data.</b>|$|R
5000|$|Petroleum Geo-Services has an XC40 {{supercomputer}} {{used for}} the processing of <b>complex</b> seismic <b>data</b> sets.|$|R
5000|$|A {{proactive}} communications {{platform that}} translates <b>complex</b> scientific <b>data</b> into material easily understood by many stakeholders ...|$|R
500|$|Perl 5 added {{features}} that support <b>complex</b> <b>data</b> structures, first-class functions (that is, closures as values), and an object-oriented programming model. These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma). A major additional feature introduced with Perl 5 was {{the ability to}} package code as reusable modules. Wall later stated that [...] "The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core." ...|$|E
500|$|An {{alternative}} educational approach {{informed by}} cognitive flexibility is hypertext, which is frequently computer-supported instruction. Computers allow for <b>complex</b> <b>data</b> {{to be presented}} in a multidimensional and coherent format, allowing users to access that data as needed. The most widely used example of hypertext is the Internet, which dynamically presents information in terms of interconnection (e.g. hyperlinks). Hypertext documents, therefore, include nodes – bits of information – and links, the pathways between these nodes. Applications for teacher education have involved teacher-training sessions based on video instruction, whereby novice teachers viewed footage of master teachers conducting a literacy workshop. [...] In this example, the novice teachers received a laserdisc of the course content, a hypertext document that allowed the learners to access content in a self-directed manner. These [...] (CFH) provide a “three-dimensional” and “open-ended” representation of material for learners, enabling them to incorporate new information and form connections with preexisting knowledge. While {{further research is needed}} to determine the efficacy of CFH as an instructional tool, classrooms where cognitive flexibility theory is applied in this manner are hypothesized to result in students more capable of transferring knowledge across domains.|$|E
2500|$|... A {{combination}} of SVD and higher-order SVD {{also has been}} applied for real time event detection from <b>complex</b> <b>data</b> streams (multivariate data with space and time dimensions) in Disease surveillance.|$|E
50|$|High-order pattern {{discovery}} {{facilitate the}} capture of high-order (polythetic) patterns or event associations that are intrinsic to <b>complex</b> real-world <b>data.</b>|$|R
40|$|Under the {{direction}} of C. M. SUCHINDRAN) ·e This research is concerned with developing a methodology for performing a life table analysis for data obtained in a complex sample survey. Four methods for estimating the variance of a function estimated using <b>complex</b> survey <b>data</b> are reviewed. These four methods are then applied to estimates of the conditional probabilities of an event and the associated survivorship probabilities, and suggestions are made as to the Ibest l variance estimation method to use. A test statistic analogous to the Mantel-Haenszel test is developed for comparing survivorship probabilities between two or more domains of interest for <b>complex</b> survey <b>data.</b> Use of the test statistic is illustrated for two data sets. Values of the test statistic based on the complex survey design are compared to values obtained using the Mantel-Haenszel test statistic assuming a simple random sample. Life table regression models are reviewed and a model is developed {{which can be used}} with <b>complex</b> survey <b>data.</b> This brings to discussion the issue of likelihood based inference and estimation in complex surveys. Arguments based upon superpopulation models are given which support the use of likelihood methods for <b>complex</b> survey <b>data.</b> Variance estimates for the maximum likelihood estimators obtained from <b>complex</b> survey <b>data</b> are developed, and examples illustrating the given approach are given. i i ACKNOWLEDGMENTS It has been a great privilege to have been a student in th...|$|R
5|$|<b>Complex</b> Air <b>data</b> {{computer}} (ADC) for {{the automated}} monitoring and transmission of aerodynamic measurements (total pressure, static pressure, angle of attack, side-slip).|$|R
2500|$|This {{complexity}} has no equivalent {{either in}} common {{use of natural}} language or in other programming languages, and it causes high cognitive load when writing code to manipulate <b>complex</b> <b>data</b> structures. Compare this with Perl 6: ...|$|E
2500|$|The {{scope of}} AI is disputed: as {{machines}} become increasingly capable, tasks considered as requiring [...] "intelligence" [...] are often {{removed from the}} definition, a phenomenon known as the AI effect, leading to the quip [...] "AI is whatever hasn't been done yet." [...] For instance, optical character recognition is frequently excluded from [...] "artificial intelligence", having become a routine technology. Capabilities generally classified as AI [...] include successfully understanding human speech, competing {{at a high level}} in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery networks, military simulations, and interpreting <b>complex</b> <b>data,</b> including images and videos.|$|E
2500|$|With version 2007 onwards, Access {{includes}} an Office-specific version of Jet, initially called the Office Access Connectivity Engine (ACE), {{but which is}} now called the Access Database Engine (However MS-Access consultants and VBA developers who specialize in MS-Access {{are more likely to}} refer to it as [...] "the ACE Database Engine"). This engine was backward-compatible with previous versions of the Jet engine, so it could read and write (.mdb) files from earlier Access versions. It introduced a new default file format, (.accdb), that brought several improvements to Access, including <b>complex</b> <b>data</b> types such as multivalue fields, the attachment data type and history tracking in memo fields. It also brought security changes and encryption improvements and enabled integration with Microsoft Windows SharePoint Services 3.0 and Microsoft Office Outlook 2007.|$|E
50|$|Care {{coordination}} and real-time monitoring {{of safety and}} outcomes with integration of <b>complex</b> molecular <b>data,</b> phenotypic data obtained from disparate electronic records.|$|R
5000|$|<b>Complex</b> Air <b>data</b> {{computer}} (ADC) for {{the automated}} monitoring and transmission of aerodynamic measurements (total pressure, static pressure, angle of attack, side-slip).|$|R
5000|$|Instances of <b>complex</b> entity <b>data</b> {{types are}} {{represented}} in the STEP file by using either the internal mapping or the external mapping.|$|R
5000|$|OPC <b>Complex</b> <b>Data</b> : Standards for {{specifying the}} {{communication}} of <b>complex</b> <b>data</b> types such as binary data and XML documents ...|$|E
5000|$|Supports large, <b>complex</b> <b>data</b> {{modeling}} and data warehousing projects ...|$|E
5000|$|ASReml-R {{statistical}} R {{package to}} allow user model <b>complex</b> <b>data.</b>|$|E
5000|$|Structures {{and classes}} are a <b>complex</b> (abstract) <b>data</b> type {{that can be}} {{operated}} in MQL4. Classes differ from structures in the following characteristics: ...|$|R
50|$|Physical level: The {{lowest level}} of {{abstraction}} describes how a system actually stores data. The physical level describes <b>complex</b> low-level <b>data</b> structures in detail.|$|R
40|$|In this article, I {{discuss the}} main {{approaches}} to resampling variance es- timation in complex survey data: balanced repeated replication, the jackknife, and the bootstrap. Balanced repeated replication and the jackknife are {{implemented in the}} Stata svy suite. The bootstrap for <b>complex</b> survey <b>data</b> is implemented by the bsweights command. I describe this command and provide working examples. Copyright 2010 by StataCorp LP. bsweights, balanced repeated replication, balanced bootstrap, bootstrap, <b>complex</b> survey <b>data,</b> Hadamard matrix, half-samples, jackknife, re- sampling, weighted bootstrap, mean bootstrap...|$|R
5000|$|JSON, {{with support}} for <b>complex</b> <b>data</b> types and data {{structures}} ...|$|E
5000|$|... system folders (to handle <b>complex</b> <b>data</b> such as {{registered}} users) ...|$|E
5000|$|Item 64. Use map and grep to {{manipulate}} <b>complex</b> <b>data</b> structures.|$|E
40|$|We {{compared}} three statistical packages (SAS, SPSS and STATA) {{in analyzing}} <b>complex</b> survey <b>data</b> {{in the context}} of multiple regression analysis using concrete examples from two national healthcare database (MEPS and NDHS). The three packages are found to be efficient and flexible in analyzing <b>complex</b> survey <b>data,</b> but SAS in some cases seems to over estimate the variances of the sample statistics. Adjustment for stratification (incorporating stratification) is very important in complex survey analysis, especially if the stratification variable is endogenous...|$|R
50|$|Pimcore’s MDM/PIM {{is built}} for {{managing}} master {{data in a}} central location. Pimcore enables users to configure <b>complex</b> multi-domain <b>data</b> models and to consolidate data from multiple sources and formats. Pimcore's data models are powered by more than 40 data entry components {{that are able to}} solve <b>complex,</b> multi-domain <b>data</b> modeling scenarios including text, media, relations and classifications (ecl@ss, GS1, BMEcat,...). Manage product data, including hierarchy, structure, validations, versioning and enriching master data with attributes, descriptions, translations, documentation and other related data.|$|R
40|$|Future {{applications}} {{will require}} integrity of <b>complex,</b> persistent <b>data</b> {{in the face}} of hardware and program failures. Thor [6], a new object-oriented database, offers a computational model that ensures data integrity in a distributed system without sacrificing expressiveness or performance. Perhaps surprisingly, compiler technology is important to making this work well. 1 Traditional Approaches Currently, writing applications that share <b>complex,</b> persistent <b>data</b> across a network is too hard, because the underlying software substrate is not available. No existing system combines high performance, complete data integrity, and a sufficiently powerful data model. Allowing multiple applications to share <b>complex</b> persistent <b>data</b> creates problems because of the desire for protection. Increasingly, persistent data contains semantic information. It may only be accessed in particular ways that ensure its integrity is not violated. Data may be accessed by multiple users, or shared across a network [...] ...|$|R
