1049|1338|Public
5|$|Due to {{its status}} an {{international}} language, English is expeditious {{when it comes}} adopting foreign words, and borrows vocabulary from {{a large number of}} other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora, collections of actual written texts and spoken passages. Many statements published {{before the end of the}} 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic <b>corpus</b> <b>data</b> becomes available.|$|E
25|$|The {{study of}} parole (which manifests through {{cultural}} discourses and dialects) is {{the domain of}} sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by {{its own set of}} grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language. This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or <b>corpus</b> <b>data).</b>|$|E
5000|$|Beryl T. (Sue) Atkins is a lexicographer, specialising in {{computational}} lexicography, who {{pioneered the}} creation of bilingual dictionaries from <b>corpus</b> <b>data.</b>|$|E
40|$|<b>corpus</b> of <b>data</b> {{is already}} quite rich in {{information}} pertinent to modern|$|R
5000|$|Data scraping, extracting {{parts of}} a <b>corpus</b> of <b>data</b> with {{automated}} tools.|$|R
40|$| 1) Add to the <b>corpus</b> of <b>data</b> {{on human}} {{proteins}} {{that is already}} in Swiss-|$|R
5000|$|... a 50-page section {{providing}} {{guidance on}} writing academic English, {{based on a}} collaboration with the Centre for English Corpus Linguistics in Louvain, Belgium and using the Centre’s learner <b>corpus</b> <b>data</b> ...|$|E
5000|$|Molesworth, C. J., Bowler, D. M., & Hamptom, J. A. (2005). Extracting prototypes from exemplars {{what can}} <b>corpus</b> <b>data</b> {{tell us about}} concept representation?. Journal of Child Psychology and Psychiatry, 46(6), 661-672. doi: 10.1111/j.1469-7610.2004.00383.x ...|$|E
50|$|Besides pure {{linguistic}} inquiry, corpus linguistics {{had begun}} {{to be applied to}} other academic and professional fields, such as the emerging sub-discipline of law and corpus linguistics, which seeks to understand the meaning of legal texts using <b>corpus</b> <b>data</b> and tools.|$|E
40|$|The AVOZES <b>data</b> <b>corpus</b> has {{recently}} been made publicly available for other interested researchers. It is the first publicly available audio-video speech <b>data</b> <b>corpus</b> for Australian English. It contains recordings from 20 speakers and the sequences provide both a systematic coverage of the phonemes and visemes of Australian English {{as well as some}} application-driven utterances. AVOZES is also the first audio-video speech <b>data</b> <b>corpus</b> with stereo-video recordings, which enable a more accurate measurement of geometric facial features. 1...|$|R
40|$|<b>Data</b> <b>corpora</b> {{are very}} {{important}} for digital forensics education and research. Several corpora are available to academia; these range from small manually-created data sets of a few megabytes to many terabytes of real-world <b>data.</b> However, different <b>corpora</b> are suited to different forensic tasks. For example, real <b>data</b> <b>corpora</b> are often desirable for testing forensic tool properties such as effectiveness and efficiency, but these corpora typically lack the ground truth that is vital to performing proper evaluations. Synthetic <b>data</b> <b>corpora</b> can support tool development and testing, {{but only if the}} methodologies for generating the <b>corpora</b> guarantee <b>data</b> with realistic properties. This paper presents an overview of the available digital forensic corpora and discusses the problems that may arise when working with specific corpora. The paper also describes a framework for generating synthetic corpora for education and research when suitable real-world data is not available...|$|R
40|$|Abstract: Data driven {{language}} learning promotes learner autonomy and discovery learning by providing learners with authentic foreign language data for self-directed or guided exploration. However, the {{effective use of}} <b>corpora</b> <b>data</b> requires {{a certain level of}} linguistic knowledge. We propose an information retrieval augmentation to concordance for adapting to self-directed context of independent learners. The approach involves an expression element model and a retrieving mechanism so as to reduce linguistic threshold and enhance learner empowerment. Simulation results with English proficiency tests and students’ writing samples support the effectiveness of the approach...|$|R
5000|$|The Cambridge Guide to English Usage by Pam Peters is a usage dictionary, {{giving an}} {{up-to-date}} {{account of the}} debatable issues of English usage and written style. It is based on extensive, up-to-date <b>corpus</b> <b>data</b> {{rather than on the}} author’s personal intuition or prejudice, and differentiates among US, UK, Canadian and Australian usages. British lexicographer Sidney Landau remarked: ...|$|E
50|$|Her {{principal}} interests {{include the}} lexical analysis of <b>corpus</b> <b>data,</b> {{and in particular}} the use of linguistic theory as a basis for a systematic description of the language; designing databases to store lexicographic data for use by human lexicographers and computer lexicons; using such databases in the creation of monolingual and bilingual dictionaries; the training of lexicographers; and the study of how people actually use dictionaries.|$|E
5000|$|In the 1990s, Fillmore taught {{classes in}} {{computational}} lexicography at the University of Pisa, {{where he met}} Sue Atkins, who was conducting frame-semantic analyses from a lexicographic perspective. In their subsequent discussions and collaborations, Fillmore came to acknowledge the importance of considering <b>corpus</b> <b>data.</b> They discussed the [...] "dictionary of the future", in which every word would be linked to example sentences from corpora.|$|E
30|$|The {{collected}} <b>data</b> <b>corpus</b> {{was captured}} by a speech acquisition board using a 16 -bit linear coding A/D converter and sampled at a sampling rate of 16 [*]kHz. The <b>data</b> <b>corpus</b> was a 16 -bit per sample linear data. The speech signals were applied every 5 [*]ms to a 30 [*]ms Hamming window.|$|R
40|$|<b>Data</b> <b>corpora</b> are an {{important}} part of any audio-visual research. However, the time and effort needed to build a good dataset are very large. Therefore, we argue that the researchers should follow some general guidelines when building a corpus that guarantees that the resulted datasets have common properties. This will give the opportunity to compare the results of different approaches of different research groups even without sharing the same <b>data</b> <b>corpus.</b> In this paper we will formulate the set of guidelines that should always be taken into account when developing an audio-visual <b>data</b> <b>corpus</b> for bi-modal speech recognition. During the process we compare samples from different existing datasets, and give solutions for solving the drawbacks that these datasets suffer. In the end we give a complete list with all the properties of some of the most known <b>data</b> <b>corpora...</b>|$|R
40|$|This paper {{develops}} a methodology {{for the design}} of audiovideo <b>data</b> <b>corpora</b> of the speaking face. Existing corpora are surveyed and the principles of data specification, data description and statistical representation are analysed both from an application-driven and from a scientifically motivated perspective. Furthermore, the possibility of "opportunistic" design of speaking-face <b>data</b> <b>corpora</b> is considered...|$|R
50|$|Due to {{copyright}} and licence restrictions, the DeReKo archive {{may not be}} copied nor {{offered for}} download. It can be queried and analyzed free of charge via the system COSMAS II - end-users are required to register by name and to agree to use the <b>corpus</b> <b>data</b> exclusively for non-commercial, academic purposes. COSMAS II enables users to compile from DeReKo a virtual corpus suitable for their specific research questions.|$|E
50|$|The Cambridge English Corpus (formerly the Cambridge International Corpus) is a multi-billion word {{corpus of}} English {{language}} (containing both text corpus and spoken <b>corpus</b> <b>data).</b> The Cambridge English Corpus (CEC) contains {{data from a}} number of sources including written and spoken, British and American English. The CEC also contains the Cambridge Learner Corpus, a 40m word corpus made up from English exam responses written by English language learners.|$|E
50|$|At {{the time}} of its completion, BABEL was the largest {{high-quality}} speech database available for research purposes in languages such as Hungarian and Estonian. It has been used for research into topics such as pronunciation modeling and automatic speech recognition. The project was also part of what has been called the most significant recent development in corpus linguistics - the increasing range of languages covered by <b>corpus</b> <b>data,</b> which promises to bring to a wider range of languages the benefits that corpus linguistics has brought to the study of Western European languages.|$|E
5000|$|A large <b>corpus</b> of <b>data</b> {{indicates}} that it maize was dispersed into lower Central America by 7600 BP BC and had moved into the inter-Andean valleys of Colombia between 7000 and 6000 BP BC. Dolores Piperno ...|$|R
40|$|If one {{considers}} the recordings from textbooks of French as a foreign language, some typical spoken French phenomena appear not to reflect results provided by spoken French corpora. In {{order to have a}} better insight into this issue, we analyzed the recordings of the first three lessons of ten beginners’ textbooks (1982 – 2012). In this article, we examine /ə/ deletion (/ȝəvøləvwaʁ/ vs. /ȝvølvwaʁ/ ‘je veux le voir’), reduction from /ty/ to /t/ before vowel (/tyabitu/ vs /tabitu/, ‘tu habites où?’), and /l/ deletion in il(s) /elle(s) before consonant (/imãȝ/ vs. /ilmãȝ/, ‘il mange’). We then compare our <b>corpus’s</b> <b>data</b> with spoken French corpora. Finally, we discuss the possible origins of the discrepancies between both corpora...|$|R
5000|$|... model training, {{learning}} the conditional distributions between the [...] and feature functions from some <b>corpus</b> of training <b>data.</b>|$|R
50|$|Svenja Adolphs is a British {{linguist}} whose research involves {{analysis of}} <b>corpus</b> <b>data</b> including sources of multimodal material {{such as the}} Nottingham Multimodal Corpus (NMMC) to examine communication in new forms of digital records. Using visual mark-up systems, her work allows {{a better understanding of}} the nature of natural language use. She is a co-founder (along with Paul Crawford and Ronald Carter) of the Health Language Research Group at the University of Nottingham, bringing together academics and clinicians to advance the work of applied linguistics in health care settings.|$|E
50|$|The {{study of}} parole (which manifests through {{cultural}} discourses and dialects) is {{the domain of}} sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by {{its own set of}} grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language. This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or <b>corpus</b> <b>data).</b>|$|E
50|$|Due to {{its status}} an {{international}} language, English is expeditious {{when it comes}} adopting foreign words, and borrows vocabulary from {{a large number of}} other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora, collections of actual written texts and spoken passages. Many statements published {{before the end of the}} 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic <b>corpus</b> <b>data</b> becomes available.|$|E
40|$|Part 4 : Forensic Tools and TrainingInternational audienceData corpora {{are very}} {{important}} for digital forensics education and research. Several corpora are available to academia; these range from small manually-created data sets of a few megabytes to many terabytes of real-world <b>data.</b> However, different <b>corpora</b> are suited to different forensic tasks. For example, real <b>data</b> <b>corpora</b> are often desirable for testing forensic tool properties such as effectiveness and efficiency, but these corpora typically lack the ground truth that is vital to performing proper evaluations. Synthetic <b>data</b> <b>corpora</b> can support tool development and testing, {{but only if the}} methodologies for generating the <b>corpora</b> guarantee <b>data</b> with realistic properties. This paper presents an overview of the available digital forensic corpora and discusses the problems that may arise when working with specific corpora. The paper also describes a framework for generating synthetic corpora for education and research when suitable real-world data is not available...|$|R
40|$|In lexicography, it is {{currently}} evident that <b>corpora</b> <b>data</b> still provide lexical informaiton as objective criteria of language descriptions in dictionary-making, especially in assigning meanings to lexical items and describing actual use. At this point, {{it means that the}} quantitative approach can add up our understanding of linguistic behavior and give a basic representation of language, together with qualitative approach systemically. The aim {{of this paper is to}} analyze the principles of compiling bilingual English-Thai dictionary in two main issues: (1) defining lexical items in bilingual dictionary-making by employing translation process and corpus-based information, and also proposing bottom-up definition model for bilingual dictionary-making. (2) considering correlation between qualitative and quantitative approaches in assigning meanings to lexical items of bilingual corpus-based dictionary. ...|$|R
50|$|A corpus {{may contain}} texts {{in a single}} {{language}} (monolingual <b>corpus)</b> or text <b>data</b> in multiple languages (multilingual corpus).|$|R
50|$|The Europarl corpus may {{not only}} be used for {{developing}} SMT systems but also for their assessment. By measuring {{the output of the}} systems against the original <b>corpus</b> <b>data</b> for the target language the adequacy of the translation can be assessed. Koehn uses the BLEU metric by Papineni et al. (2002) for this, which counts the coincidences of the two compared versions—SMT output and corpus data—and calculates a score on this basis. The more similar the two versions are, the higher the score, and therefore the quality of the translation. Results reflect that some SMT systems perform better than others, e.g., Spanish-French (40.2) in comparison to Dutch-Finnish (10.3). Koehn states that {{the reason for this is}} that related languages are easier to translate into each other than those that are not.|$|E
50|$|The Europarl Corpus is a corpus (set of documents) that {{consists}} of {{the proceedings of the}} European Parliament from 1996 to the present. In its first release in 2001, it covered eleven official languages of the European Union (Danish, Dutch, English, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish). With the political expansion of the EU the official languages of the ten new member states {{have been added to the}} <b>corpus</b> <b>data.</b> The latest release (2012) comprised up to 60 million words per language with the newly added languages being slightly underrepresented as data for them is only available from 2007 onwards. This latest version includes 21 European languages: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek.|$|E
50|$|Both {{belonging}} to CBMT, {{sometimes referred to}} as data-driven MT, EBMT and SMT have something in common which distinguish them from RBMT. First, they both use a bitext as the fundamental data source. Second, they are both empirical with the principle of machine learning instead of rational with the principle of linguists writing rules. Third, they both can be improved by getting more data. Fourth, new language pairs can be developed just by finding suitable parallel <b>corpus</b> <b>data,</b> if possible. Apart from these similarities, there are also some dissimilarities. SMT essentially uses statistical data such as parameters and probabilities derived from the bitext, in which preprocessing the data is essential and even if the input is in the training data, the same translation is not guaranteed to occur. By contrast, EBMT uses the bitext as its primary data source, in which preprocessing the data is optional and if the input is in the example set, the same translation is to occur.|$|E
30|$|In this section, the {{performances}} of the proposed method are evaluated. The MSRA mandarin <b>corpus</b> test <b>data</b> that has 500 utterances with 0.74 h length is used as the test set, and the training set from MSRA has 19688 utterances with 31.5 h length, referring to [21] for details.|$|R
40|$|Parallel {{sentences}} are {{a relatively}} scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our new methodologies for mining such data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are {{a much more}} useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from e. g. Wikipedia dumps and Euronews web page. The improvements in machine translation are shown on Polish-English language pair for various text domains. We also tested another method of building parallel corpora based on comparable <b>corpora</b> <b>data.</b> It lets automatically broad existing corpus of sentences from subject of corpora based on analogies between them. Comment: Springer p. 433 - 441, 201...|$|R
40|$|Ever {{increasing}} {{volumes of}} data in electronic format are being produced and stored, due {{mainly to the}} relatively inexpensive cost of storage and emerging techniques for extracting useful information from such <b>data.</b> A large <b>corpus</b> of <b>data,</b> in itself does not offer a lot of value {{but if it were}} categorised into relevant...|$|R
