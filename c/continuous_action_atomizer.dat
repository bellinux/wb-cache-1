0|1184|Public
40|$|Action {{systems have}} been used {{successfully}} to describe discrete systems, i. e. systems with discrete control acting upon a discrete state space. In this paper we define <b>continuous</b> <b>action</b> systems, which extend the action system approach to hybrid systems, i. e. systems with discrete control over continuously evolving processes. The meaning of <b>continuous</b> <b>action</b> systems is {{defined in terms of}} traditional (discrete) action systems. Properties of <b>continuous</b> <b>action</b> systems are proved using standard action system proof techniques. <b>Continuous</b> <b>action</b> systems are very general, and can be used to describe a diverse range of hybrid systems. We describe the essential notions of <b>continuous</b> <b>action</b> systems and illustrate the framework by a collection of examples...|$|R
40|$|This work {{concerns}} {{the solution of}} delayed Reinforcement Learning problems having <b>continuous</b> <b>action</b> spaces. The problems associated with <b>continuous</b> <b>action</b> spaces are discussed and various existing algorithms for solving the problem are presented. A extension of Q-learning for solving delayed RL problems having <b>continuous</b> <b>action</b> spaces is proposed which overcomes drawbacks associated with existing methods. Simulation results are presented to demonstrate the working of the proposed algorithm...|$|R
40|$|Action {{systems are}} a {{framework}} for reasoning about discrete reactive systems. Back, Petre and Porres have extended these <b>action</b> systems to <b>continuous</b> <b>action</b> systems, {{which can be used}} to model hybrid systems. In this paper we define a refinement relation, and develop practical data refinement rules for <b>continuous</b> <b>action</b> systems. The meaning of <b>continuous</b> <b>action</b> systems is expressed in terms of a mapping from <b>continuous</b> <b>action</b> systems to action systems. First, we present a new mapping from <b>continuous</b> <b>action</b> systems to action systems, such that Back’s definition of trace refinement is correct with respect to it. Second, we present a stream semantics that is compatible with the trace semantics, but is preferable to it because it is more general. Although action system trace refinement rules are applicable to <b>continuous</b> <b>action</b> systems with a stream semantics, they are not complete. Finally, we introduce a new data refinement rule that is valid with respect to the stream semantics and can be used to prove refinements that are not possible in the trace semantics, and we analyse the completeness of our new rule in conjunction with the existing trace refinement rules. 22 page(s...|$|R
40|$|AbstractWe {{show that}} every countably {{infinite}} group admits a free, <b>continuous</b> <b>action</b> on the Cantor set having an invariant probability measure. We {{also show that}} every countably infinite group admits a free, <b>continuous</b> <b>action</b> on a non-homogeneous compact metric space and the action is minimal (that is to say, every orbit is dense). In {{answer to a question}} posed by Giordano, Putnam and Skau, we establish that there is a <b>continuous,</b> minimal <b>action</b> of a countably infinite group on the Cantor set such that no free <b>continuous</b> <b>action</b> of any group gives rise to the same equivalence relation...|$|R
40|$|Abstract. We {{introduce}} mean dimensions for <b>continuous</b> <b>actions</b> of countable sofic {{groups on}} compact metrizable spaces. These generalize the Gromov-Lindenstrauss-Weiss mean dimensions for actions of countable amenable groups, and {{are useful for}} distinguishing <b>continuous</b> <b>actions</b> of countable sofic groups with infinite entropy. 1...|$|R
40|$|Properties of <b>continuous</b> <b>actions</b> on {{convergence}} {{spaces are}} investigated. The primary focus is the characterization {{as to when}} a <b>continuous</b> <b>action</b> on a convergence space can be continuously extended to an action on a compactification of the convergence space. The largest and smallest such compactifications are studied...|$|R
40|$|We {{introduce}} mean dimensions for <b>continuous</b> <b>actions</b> of countable sofic {{groups on}} compact metrizable spaces. These generalize the Gromov-Lindenstrauss-Weiss mean dimensions for actions of countable amenable groups, and {{are useful for}} distinguishing <b>continuous</b> <b>actions</b> of countable sofic groups with infinite entropy. Comment: 40 pages. To appear in Adv. Mat...|$|R
5000|$|... {{algorithms}} {{that work}} well with large (or <b>continuous)</b> <b>action</b> spaces ...|$|R
40|$|Abstract. We {{show that}} a <b>continuous</b> <b>action</b> of a quantum semigroup S on a finite quantum space (finite {{dimensional}} C ∗-algebra) preserving a faithful state comes from a <b>continuous</b> <b>action</b> of the quantum Bohr compactification bS of S. Using the classification of continuous compact quantum group actions on M 2 we give a complete description of all <b>continuous</b> quantum semigroup <b>actions</b> on this quantum space preserving a faithful state...|$|R
5000|$|OR [...] "He {{is putting}} it there (right now)." [...] Present <b>continuous</b> <b>action</b> ...|$|R
5000|$|... いく iku: Can express <b>continuous</b> <b>action</b> or {{a change}} of state in the future.|$|R
5000|$|... くる kuru: Can express <b>continuous</b> <b>action</b> or {{a change}} of state in the past.|$|R
40|$|Let F and G be finitely {{generated}} groups of polynomial growth with the degrees of polynomial growth d(F) and d(G) respectively. Let S = {Sf}f∈F be a <b>continuous</b> <b>action</b> of F on a compact metric space X {{with a positive}} topological entropy h(S). Then (i) there are no expansive <b>continuous</b> <b>actions</b> of G onX commuting with S if d(G) 0 (called an expansiv...|$|R
40|$|In this paper, we {{introduce}} topological {{pressure for}} <b>continuous</b> <b>actions</b> of countable sofic groups on compact metrizable spaces. This generalizes the classical topological pressure for <b>continuous</b> <b>actions</b> of countable amenable groups on such spaces. We also establish the variational principle for topological pressure in this sofic context. Comment: 28 pages. Minor changes. Title changed. To appear in Ergod. Th. Dynam. Sy...|$|R
40|$|We {{show that}} a <b>continuous</b> <b>action</b> of a quantum semigroup S on a finite quantum space (finite {{dimensional}} C^*-algebra) preserving a faithful state comes from a <b>continuous</b> <b>action</b> of the quantum Bohr compactification bS of S. Using the classification of continuous compact quantum group actions on M_ 2 we give a complete description of all <b>continuous</b> quantum semigroup <b>actions</b> on this quantum space preserving a faithful state. Comment: A characterization of actions preserving a faithful state adde...|$|R
40|$|AbstractLet U be a C∗-algebra, and G be {{a locally}} compact abelian group. Suppose α is a <b>continuous</b> <b>action</b> of G on U. Then {{there exists a}} <b>continuous</b> <b>action</b> ga of the dual group Ĝ of G on the C∗-crossed product by α such that the C∗-crossed product is {{isomorphic}} to the tensor product and the C∗-algebra of all compact operators on L 2 (G) ...|$|R
40|$|Standard CLA-EC {{which is}} {{introduced}} recently is an evolutionary computing model obtained by com evolutionary computing (EC) model. Some drawbacks {{of this model}} are low convergence speed an this paper {{a new version of}} CLA-EC called <b>Continuous</b> <b>Action</b> Set CLA-EC or in short <b>Continuous</b> CL <b>action</b> set learning automaton in each cell is replaced by a <b>continuous</b> <b>action</b> set learning automat model it is tested on some function optimization problems. The results of experimentations have s comparing to standard CLA-EC has both higher accuracy and speed of convergence. IEEE term...|$|R
40|$|Online {{model-free}} {{reinforcement learning}} (RL) methods with <b>continuous</b> <b>actions</b> {{are playing a}} prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted {{to changes in the}} environment. Here we propose an active exploration algorithm for RL in structured (parameterized) <b>continuous</b> <b>action</b> space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature β parameter. In parallel, a Gaussian exploration is applied to the <b>continuous</b> <b>action</b> parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune β and the width of the Gaussian distribution from which <b>continuous</b> <b>action</b> parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm. Comment: Submitted to EWRL 201...|$|R
40|$|<b>Continuous</b> <b>action</b> space {{games are}} {{ubiquitous}} in economics. However, whilst learning dynamics in normal form games with finite action sets are now well studied, {{it is not}} until recently that their <b>continuous</b> <b>action</b> space counterparts have been examined. We extend stochastic fictitious play to the <b>continuous</b> <b>action</b> space framework. In normal form games with finite action sets the limiting behaviour of a discrete time learning process is often studied using its continuous time counterpart via stochastic approximation. In this paper we study stochastic fictitious play in games with <b>continuous</b> <b>action</b> spaces using the same method. This requires the asymptotic pseudo-trajectory approach to stochastic approximation to be extended to Banach spaces. In particular the limiting behaviour of stochastic fictitious play is studied using the associated smooth best response dynamics on the space of finite signed measures. Using this approach, stochastic fictitious play is shown to converge to an equilibrium point in two-player zero-sum games and a stochastic fictitious play-like process is shown to converge to an equilibrium in negative definite single population games...|$|R
40|$|The {{convergence}} {{properties for}} reinforcement learning approaches such as temporal dif-ferences and Q-learning {{have been established}} under moderate assumptions for discrete state and action spaces. In practice, however, many systems have either <b>continuous</b> <b>action</b> spaces or {{a large number of}} discrete elements. This paper presents an approximate dynamic pro-gramming approach to reinforcement learning for <b>continuous</b> <b>action</b> set-point regulator prob-lems which learns near-optimal control policies based on scalar performance measures. The <b>Continuous</b> <b>Action</b> Space (CAS) algorithm uses derivative-free line search methods to obtain the optimal <b>action</b> in the <b>continuous</b> space. The theoretical convergence properties of the algorithm are presented. Several heuristic stopping criteria are investigated and practical ap-plication is illustrated on two example problems{the inverted pendulum balancing problem and the power system stabilization problem. ...|$|R
2500|$|... not {{an integer}} but the <b>continuous</b> <b>action</b> {{variable}} J, but Heisenberg performed analogous manipulations with matrices, where the intermediate ...|$|R
50|$|AT-SD-2 - {{wind tunnel}} with <b>continuous</b> <b>action,</b> with {{installed}} power of vacuum pumps’ engine 6 x 35 = 210 kW, 1956.|$|R
40|$|AbstractIn {{this paper}} we study <b>continuous</b> <b>actions</b> of topological groups. We {{introduce}} a parametrized notion of periodicity – {{relative to a}} fixed class of compactifications of the acting group. This yields a natural generalization of Devaney's well-recognized concept of chaos. As our main result, we establish a geometric characterization of those classes of compactifications of a locally compact Hausdorff topological group for which the group admits a faithful chaotic <b>continuous</b> <b>action</b> on some (compact) Hausdorff space...|$|R
50|$|A <b>Continuous</b> <b>Action</b> Tamping Machine (CAT) can pack {{between one}} and four {{sleepers}} at a time, with outputs between 320m/h and 2600m/h generally anticipated.|$|R
40|$|Reinforcement {{learning}} (RL) is {{a powerful}} machine-learning methodology that has an established theoretical foundation and has proven effective {{in a variety of}} small, simulated domains. There has been considerable work on applying RL, a method originally conceived for discrete state-action spaces, to problems with continuous states. The extension of RL to allow <b>continuous</b> <b>actions,</b> on the other hand, has seen relatively little research. One proposed approach to allowing <b>continuous</b> <b>actions</b> is to represent the value function using a tile-coding function approximator. We introduc...|$|R
5000|$|Standard Korean distinguishes hae itda ( [...] , {{referring}} to a continuous state) and hago itda ( [...] , {{referring to}} a <b>continuous</b> <b>action).</b> For instance, [...] "to be sitting" [...] is anja itda (...) , not ango itda (...) , as the latter would mean [...] "being {{in the middle of}} the action of sitting, but has not completed the action yet". Zainichi Korean, however, does not distinguish these two, as Japanese does not either; it uses hago itda form for both continuous state and <b>continuous</b> <b>action.</b>|$|R
5000|$|On {{the other}} hand, {{if the entire}} action is expressed, not as a <b>continuous</b> <b>action,</b> but as a single {{undivided}} event, the aorist is used: ...|$|R
40|$|When {{controlling}} {{an autonomous}} system, it is inefficient or sometimes {{impossible for the}} human operator to specify detailed commands. Instead, the field of AI autonomy has developed goal-directed systems, in which human operators specify a series of goals to be accomplished. Increasingly, the control of autonomous systems involves performing a mix of discrete and <b>continuous</b> <b>actions.</b> For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and set sonar, and <b>continuous</b> <b>actions,</b> like descend and ascend, which involve continuous dynamics of the vehicle. Accordingly, we develop a hybrid planner that determines a series of discrete and <b>continuous</b> <b>actions</b> that achieve the mission goals. In this paper, we describe a novel approach to solving the generative planning problem for hybrid systems, involving both <b>continuous</b> and discrete <b>actions.</b> The planner, Kongming 1, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for <b>continuous</b> <b>actions.</b> Second, it encodes the Hybrid Flow Graph as a mixed logic linear/nonlinear program, which it solves using an off-theshelf solver. We empirically demonstrate that Kongming can efficiently plan for real-world scenarios {{that are based on}} science missions performed at the Monterey Bay Aquarium Research Institute (MBARI) ...|$|R
50|$|The present participle is -ысь. It is a participle which expresses <b>continuous</b> <b>action</b> and {{is always}} active. It is {{affixed to the}} stems of the verb.|$|R
40|$|Although several {{researchers}} have integrated methods for reinforcement learning (RL) with case-based reasoning (CBR) to model <b>continuous</b> <b>action</b> spaces, existing integrations typically employ discrete approximations of these models. This limits {{the set of}} actions that can be modeled, and may lead to non-optimal solutions. We introduce the <b>Continuous</b> <b>Action</b> and State Space Learner (CASSL), an integrated RL/CBR algorithm that uses continuous models directly. Our empirical study shows that CASSL significantly outperforms two baseline approaches for selecting actions on a task from a real-time strategy gaming environment. 1...|$|R
40|$|Abstract. We {{target the}} problem of {{closed-loop}} learning of control policies that map visual percepts to <b>continuous</b> <b>actions.</b> Our algorithm, called Reinforcement Learning of Joint Classes (RLJC), adaptively discretizes the joint space of visual percepts and <b>continuous</b> <b>actions.</b> In a sequence of attempts to remove perceptual aliasing, it incrementally builds a decision tree that applies tests either in the input perceptual space or in the output action space. The leaves of such a decision tree induce a piecewise constant, optimal state-action value function, which is computed through a reinforcement learning algorithm that uses the tree as a function approximator. The optimal policy is then derived by selecting the action that, given a percept, leads to the leaf that maximizes the value function. Our approach is quite general and applies also to learning mappings from continuous percepts to <b>continuous</b> <b>actions.</b> A simulated visual navigation problem illustrates the applicability of RLJC. ...|$|R
50|$|The play {{takes place}} over two acts, both with <b>continuous</b> <b>action.</b> The first act takes place one evening, {{and the second}} act takes place the {{following}} morning.|$|R
50|$|Although glycols are {{effective}} air disinfectants in controlled laboratory environments, {{it is more}} difficult to use them effectively in real-world environments because the disinfection of air is sensitive to <b>continuous</b> <b>action.</b> <b>Continuous</b> <b>action</b> in real-world environments with outside air exchanges at door, HVAC, and window interfaces, and in the presence of materials that adsorb and remove glycols from the air, poses engineering challenges that are not critical for surface disinfection. The engineering challenge associated with creating a sufficient concentration of the glycol vapours in the air have not to date been sufficiently addressed.|$|R
5000|$|... #Caption: [...] Plasser and Theurer DGS 62 N dynamic track stabilizer, {{followed}} by SSP 300 Regulator, then a Plasser 09-16 CAT <b>Continuous</b> <b>Action</b> Tamper, in Blueskin Bay, New Zealand.|$|R
5000|$|The present participle is -(ӥ)сь/-(и)сь. It is a participle which expresses <b>continuous</b> <b>action.</b> It is {{affixed to}} short stems in {{conjugation}} I verbs. The present participle caritive is -(ӥ)сьтэм/-(и)сьтэм ...|$|R
40|$|International audienceReinforcement {{learning}} {{methods are}} often con- sidered {{as a potential}} solution to enable a robot to adapt to changes in real time to an unpredictable environment. However, with <b>continuous</b> <b>action,</b> only a few existing algorithms are practical for real-time learning. In such a setting, most effective methods have used a parameterized policy structure, often with a separate parameterized value function. The goal {{of this paper is}} to assess such actor-critic methods to form a fully specified practical algorithm. Our specific contributions include 1) developing the extension of existing incremental policy-gradient algorithms to use eligibility traces, 2) an empir- ical comparison of the resulting algorithms using <b>continuous</b> <b>actions,</b> 3) the evaluation of a gradient-scaling technique that can significantly improve performance. Finally, we apply our actor-critic algorithm to learn on a robotic platform with a fast sensorimotor cycle (10 ms). Overall, these results constitute an important step towards practical real-time learning control with <b>continuous</b> <b>action...</b>|$|R
