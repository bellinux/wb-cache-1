0|15|Public
5000|$|S. Blake-Wilson and A. Menezes, [...] "Authenticated Diffie-Hellman key {{agreement}} protocols", Proceedings of the 5th Annual Workshop on Selected Areas in <b>Cryptography,</b> Lecture <b>Notes</b> in Computer Science, 1556, 339-361, 1999.|$|R
5000|$|S. Blake-Wilson and A. Menezes, [...] "Unknown key-share {{attacks on}} the Station-to-Station (STS) protocol", Proceedings of Public Key <b>Cryptography</b> '99, Lecture <b>Notes</b> in Computer Science, 1560, 154-170, March 1999.|$|R
40|$|There {{has been}} {{spectacular}} {{progress in the}} field of quantum information in recent decades. The development of this field highlights the importance of the role of entanglement in quantum computing, quantum teleportation and quantum <b>cryptography.</b> These <b>notes</b> serve to provide a gentle introduction to the entanglement of bipartite states. In these notes, we introduce Bell's theorem in the form derived by Clauser, Horne, Shimony and Holt. We discuss the Schmidt decomposition and the Peres-Horodecki criterion in the entanglement of pure and mixed bipartite states. Finally, we describe a teleportation protocol as an illustration of the use of entangled states...|$|R
40|$|AbstractThe {{concept of}} pseudorandomness plays an {{important}} role in <b>cryptography.</b> In this <b>note,</b> we contrast the notions of complexity-theoretic pseudorandom strings (from algorithmic information theory) and pseudorandom strings (from cryptography). For example, we show that we can easily distinguish a complexity-theoretic pseudorandom ensemble from the uniform ensemble. Both notions of pseudorandom strings are uniformly unpredictable; in contrast with pseudorandom strings, complexity-theoretic pseudorandom strings are not polynomial-time unpredictable...|$|R
40|$|Abstract-Lagged Fibonacci Generators (LFG) {{are used}} as a {{building}} block of key-streamgenerator in stream cipher <b>cryptography.</b> In this <b>note</b> we have used the self-shrinkingconcept in LFG and gives an upper bound 2 n+m 8 for the self-shrinking lagged fibonaccigenerator, where n {{is the number of}} stage and m is the word size of the LFG. Wehave also shown that the bound is attained by all the LFGs of degree n < 28, resultsupported by experiments. Keywords- Cryptography,Stream cipher,LFSR,LFG...|$|R
40|$|Lagged Fibonacci Generators (LFG) {{are used}} as a {{building}} block of key-stream generator in stream cipher <b>cryptography.</b> In this <b>note,</b> we have used the self-shrinking concept in LFG and given an upper bound $frac{ 2 ^{n+m}} 8 $ for the self-shrinking LFG, where $n$ is the umber of stage and $m$ is the word size of the LFG. We have also shown that the bound is attained by all the LFGs of degree $n< 28 $, result supported by experiments...|$|R
40|$|Abstract In 2001, Gottesman and Chuang {{proposed}} a quantum signature scheme. Unlike classical signature schemes, the public {{keys in the}} scheme can only be used once. The authors claim that the scheme is somewhat cumbersome but {{it serves as a}} good model and suggests novel research directions for the field of quantum <b>cryptography.</b> In this <b>note,</b> we remark that the Gottesman-Chuang quantum signature scheme is so commonplace and cumbersome that it can not suggest the potential for quantum public key cryptography. The authors ignore an ultimate fact, namely, the cost to guarantee the authenticity of a user’s public key is expensive in the scenario of Public Key Infrastructure. It entails that a user’s public key should be repeatedly usable in the life duration...|$|R
40|$|In {{the last}} few years the world in which Quantum Cryptography evolves has deeply changed. On the one side the revelations of Snowden, though he said nothing really new, made the world more aware of the {{importance}} to protect sensitive data from all kinds of adversaries, including sometimes "friends. " On the other side, breakthroughs in quantum computation, in particular in superconducting qubits and surface-codes, made it possible that in 15 to 25 years there might be a quantum machine able to break today's codes. This implies that in order to protect today's data over a few decades, one has to act now and use some quantum-safe <b>cryptography.</b> Comment: Key <b>note</b> talk for Qcrypt 2015, Tokyo, September 28 - October 2. 3 page...|$|R
40|$|Abstract — In {{this report}} we study some {{watermarking}} methods and the comparison {{result of their}} combination, {{the first one is}} based on the CDMA (Code Division Multiple Access) in frequency domain DWT(Discrete Wavelet Transform) noted CDMA-DWT,CDMA in DCT(Discrete Cosine Transform) noted CDMA-DCT and CDMA in spatial domain noted CDMA-SD and its aim is to verify the image authenticity whereas the second one is the reversible watermarking (the least significant bits LSB and <b>cryptography</b> tools) <b>noted</b> RW and the reversible contrast mapping RCM its objective is to check the integrity of the image and to keep the confidentiality of the patient data. A new scheme of watermarking is the combination of the reversible watermarking and the CDMA method on the field of spatial domain noted RW/CDMA-SD and second combination in frequency domain (DWT and DCT) domain noted respectively RW/CDMA-DWT and RW/CDMA-DCT to verify the three security properties Integrity, Authenticity and Confidentiality of medical data and patient information. In the end,we made a comparison between these methods within the parameters of quality of medical images. Initially, an in-depth study on the characteristics of medical images would contribute to improve these methods to measurements have been done on the watermarked image to verify that this technique does not lead to a wrong diagnostic. The robustness of the watermarked images against attacks has been verified on the parameters of PSN...|$|R
40|$|Klapper [1] {{showed that}} there are binary {{sequences}} of period q 1 (q is a prime power p, p is an odd prime) with the maximal possible linear complexity q when considered as sequences over GF (2), while the sequences have very low linear complexities when considered as sequences over GF (p). This suggests that the binary sequences with high GF (2) linear complexities and low GF (p) linear complexities are <b>note</b> secure in <b>cryptography.</b> In this <b>note</b> we give some simple constructions of the binary sequences with high GF (2) linear complexities and low GF (p) linear complexities. We also prove some lower bounds on the GF (p) linear complexities of binary sequences and a lower bound {{on the number of}} the binary sequences with high GF (2) linear complexities and low GF (p) linear complexities...|$|R
40|$|Abstract. We {{discuss the}} {{complexity}} of MQ, or solving multivariate systems of m equations in n variables over the finite field Fq of q elements. MQ is an important hard problem in cryptography. In particular, the complexity to solve overdetermined MQ systems with randomly chosen coefficients when m = cn {{is related to the}} provable security of a number of cryptosystems. In this context there are two basic approaches. One is to use XL (“eXtended Linearization”) with the solving step tailored to sparse linear algebra; the other is of the many variations of Jean-Charles Faugère’s F 4 /F 5 algorithms. Although F 4 /F 5 has been the de facto standard in the cryptographic com-munity, it was proposed (Yang-Chen, 2004) that XL with Sparse Solver may be superior in some cases, particularly the generic overdetermined case with m/n = c+ o(1). At the Steering Committee Meeting of the Post-Quantum Cryptography work-shop in 2008, Johannes Buchmann listed several key research questions to all post-quantum cryptographers present. One problem in MQ-based <b>cryptography,</b> he <b>noted,</b> is “if the difference between the operating degrees of XL(-with-Sparse-Solver) and F 4 /F 5 approaches can be accurately bounded for random systems. ” We answer in the affirmative when m/n = c + o(1), using Saddle Point analysis: 1. For instances with randomly drawn coefficients, the degrees of operation of XL and F 4 /F 5 has the most pronounced differential in the large-field, “barely overdetermined ” (m−n = c) cases, where the discrepancy is ∝ √n. 2. In most other types of random systems with m/n = c+ o(1), the expected difference in the operating degrees of XL and F 4 /F 5 is constant which can be evaluated mathematically via asymptotic analysis. Our conclusions are partially backed up using tests with Maple, MAGMA, and an XL implementation featuring Block Wiedemann as the sparse-matrix solver...|$|R
40|$|Lattice {{cryptography}} {{is one of}} {{the hottest}} and fastest moving areas in mathematical cryptography today. Interest in lattice cryptographyis due toseveral concurring factors. On thetheoretical side, lattice cryptography is supported by strong worst-case/average-case security guarantees. On the practical side, lattice cryptography {{has been shown to be}} very versatile, leading to an unprecedented variety of applications, from simple (and efficient) hash functions, to complex and powerful public key cryptographic primitives, culminating with the celebrated recent development of fully homomorphic encryption. Still, one important feature of lattice cryptography is simplicity: most cryptographic operations can be implemented using basic arithmetic on small numbers, and many cryptographic constructions hide an intuitive and appealing geometric interpretation in terms of point lattices. So, unlike other areas of mathematical cryptology even a novice can acquire, with modest effort, a good understanding of not only the potential applications, but also the underlying mathematics of lattice <b>cryptography.</b> In these <b>notes,</b> we give an introduction to the mathematical theory of lattices, describe the main tools and techniques used in lattice cryptography, and present an overview of the wide range of cryptographic applications. This material should be accessible to anybody with a minimal background in linear algebra and some familiarity with the computational framework of modern cryptography, but no prior knowledge about point lattices. ...|$|R
40|$|This {{research}} note {{suggests a}} new way to realize a high speed direct encryption based on quantum detection theory. The conventional cipher is designed by a mathematical algorithm and its security is evaluated by the complexity of the algorithm for cryptanalysis and ability of computers. This kind of cipher cannot exceed the Shannon limit of cryptography,and it can be decrypted with probability one in principle by trying all the possible keys against the data length equal to the secret key length. A cipher with quantum effect in physical layer may exceed the Shannon limit of cryptography. The quantum stream cipher by α/η or Yuen- 2000 protocol (Y- 00) which operates at Gbit/sec is a typical example of such a cipher. That is, ciphertext of mathematical cipher with a secret key is masked by quantum noise of laser light when an eavesdropper observes optical signals as a ciphertext of the mathematical cipher, while the legitimate receiver does not suffer the quantum noise effect. As a result, the inherent difference of accuracy of ciphertext between eavesdropper and legitimate receiver arises. This is a necessary condition to exceed the Shannon limit of <b>cryptography.</b> In this <b>note,</b> we present a new method to generate an inherent difference of accuracy of the ciphertext, taking into account a fundamental properties of quantum detection schemes. Comment: Typos were correcte...|$|R
40|$|In the "correlated sampling" problem, two players, say Alice and Bob, {{are given}} two distributions, say P and Q respectively, {{over the same}} {{universe}} and access to shared randomness. The two players are required to output two elements, without any interaction, sampled according to their respective distributions, while trying to minimize the probability that their outputs disagree. A well-known protocol due to Holenstein, with close variants (for similar problems) due to Broder, and to Kleinberg and Tardos, solves this task with disagreement probability at most 2 δ/(1 +δ), where δ is the total variation distance between P and Q. This protocol {{has been used in}} several different contexts including sketching algorithms, approximation algorithms based on rounding linear programming relaxations, the study of parallel repetition and <b>cryptography.</b> In this <b>note,</b> we give a surprisingly simple proof that this protocol is in fact tight. Specifically, for every δ∈ (0, 1), we show that any correlated sampling scheme should have disagreement probability at least 2 δ/(1 +δ). This partially answers a recent question of Rivest. Our proof is based on studying a new problem we call "constrained agreement". Here, Alice is given a subset A ⊆ [n] and is required to output an element i ∈ A, Bob is given a subset B ⊆ [n] and is required to output an element j ∈ B, and the goal is to minimize the probability that i ≠ j. We prove tight bounds on this question, which turn out to imply tight bounds for correlated sampling. Though we settle basic questions about the two problems, our formulation also leads to several questions that remain open...|$|R

