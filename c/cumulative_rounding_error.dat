0|1057|Public
50|$|When {{performing}} a calculation, {{do not follow}} these guidelines for intermediate results; keep as many digits as is practical (at least 1 more than implied by the precision of the final result) {{until the end of}} calculation to avoid <b>cumulative</b> <b>rounding</b> <b>errors.</b>|$|R
40|$|Methods of {{controlling}} round-off error in one-step methods in the numerical solution of ordinary differential equations are compared. A new Algorithm called theoretical <b>cumulative</b> <b>rounding</b> is formulated. Round-off error bounds are obtained for single precision, and theoretical <b>cumulative</b> <b>rounding.</b> Limits of these bounds are obtained as the step length approaches zero. It is {{shown that the}} limit of the bound on the round-off error is unbounded for single precision and double precision, is constant for theoretical partial double precision, and is zero for theoretical <b>cumulative</b> <b>rounding.</b> The limits of round-off bounds are not obtainable in actual practice. The round-off error increases for single precision, remains about constant for partial double precision and decreases for <b>cumulative</b> <b>rounding</b> as the step length decreases. Several examples are included. (34 pages...|$|R
2500|$|The {{discussion}} {{up to now}} {{has ignored}} the consequences of <b>rounding</b> <b>error.</b> In step n of the Euler method, the <b>rounding</b> <b>error</b> is roughly of the magnitude εy'n where ε is the machine epsilon. Assuming that the <b>rounding</b> <b>errors</b> are all of approximately the same size, the combined <b>rounding</b> <b>error</b> in N steps is roughly Nεy0 if all errors points in the same direction. Since the number of steps is inversely proportional to the step size h, the total <b>rounding</b> <b>error</b> is proportional to ε / h. In reality, however, it is extremely unlikely that all <b>rounding</b> <b>errors</b> point in the same direction. If instead {{it is assumed that}} the <b>rounding</b> <b>errors</b> are independent <b>rounding</b> variables, then the total <b>rounding</b> <b>error</b> is proportional to [...]|$|R
25|$|As of 2016, the United Nations has enacted five <b>cumulative</b> <b>rounds</b> of {{sanctions}} against North Korea for {{its nuclear program}} and missile tests.|$|R
25|$|Thus, for {{extremely}} small {{values of the}} step size, the truncation error will be small but the effect of <b>rounding</b> <b>error</b> may be big. Most {{of the effect of}} <b>rounding</b> <b>error</b> can be easily avoided if compensated summation is used in the formula for the Euler method.|$|R
5000|$|<b>Rounding</b> <b>Error,</b> International group {{exhibition}} of emerging artists ...|$|R
5000|$|A {{choice for}} h which is small without {{producing}} a large <b>rounding</b> <b>error</b> is [...] (though not when x = 0!) where the machine epsilon &epsilon; is typically {{of the order}} 2.2&times;10−16. A formula for h that balances the <b>rounding</b> <b>error</b> against the secant error for optimum accuracy is ...|$|R
5000|$|Interval {{arithmetic}} is {{used with}} error analysis, to control <b>rounding</b> <b>errors</b> arising from each calculation.The advantage of interval arithmetic {{is that after}} each operation there is an interval that reliably includes the true result. The distance between the interval boundaries gives the current calculation of <b>rounding</b> <b>errors</b> directly: ...|$|R
40|$|<b>Rounding</b> <b>errors</b> {{present an}} {{inherent}} problem to all computer programs involving floating-point numbers. They appear nearly in all elementary operations. By accumu-lation, these errors {{can affect the}} accuracy of a computed result possibly leading to some or total inaccuracy. First in this article, computer arithmetic that uses binary system and the IEEE standard for floating-point numbers is described together with some problems introduced by <b>rounding</b> <b>errors.</b> The effects of these <b>rounding</b> <b>errors</b> can be analyzed and studied by some methods like forward/backward analysis, interval or stochastic arithmetic that are presented. Finally, it is shown how {{the accuracy of the}} results can be improved by using compensated methods and multiprecision arithmetic. Key words: <b>rounding</b> <b>error,</b> floating-point arithmetic, IEEE 754 standard, accurate computation, interval arithmetic, stochastic arithmetic, multiprecision. ∗This paper is an extended version of [9]. ...|$|R
5000|$|<b>Rounding</b> <b>errors</b> {{will slow}} the convergence. It is {{recommended}} {{to keep at}} least one extra digit beyond the desired accuracy of the [...] being calculated to minimize <b>round</b> off <b>error.</b>|$|R
2500|$|... {{might be}} {{repeated}} 9 or 10 times, depending on <b>rounding</b> <b>errors</b> and/or the hardware and/or the compiler version. Furthermore, if the increment of X occurs by repeated addition, accumulated <b>rounding</b> <b>errors</b> {{may mean that}} the value of X in each iteration can differ quite significantly from the expected sequence 0.1, 0.2, 0.3, ..., 1.0.|$|R
40|$|The paper {{emphasizes}} the specific manner {{to perform the}} arithmetic operations in the computer as always {{it is possible to}} have "rounding errors". Neglecting the influence of the computer <b>rounding</b> <b>errors</b> could affect a right conclusion, for example, {{in the case of an}} account balance verification. <b>rounding</b> <b>errors,</b> Monte Carlo technique, specific distributions...|$|R
40|$|SUMMARY. It {{is shown}} {{practically}} and theoretically that a smoothed {{version of the}} bootstrap perfonns well when used to construct confidence intervals for binomial proportions. Smoothing enables <b>rounding</b> <b>error</b> to be minimized in a uniform manner. Without smoothing, the size of <b>rounding</b> <b>error</b> can almost double. Generalizations to other lattice distributions are indicated...|$|R
50|$|Most {{processing}} operations on digital audio involve requantization of samples, and thus introduce additional <b>rounding</b> <b>error</b> {{analogous to the}} original quantization error introduced during analog to digital conversion. To prevent <b>rounding</b> <b>error</b> larger than the implicit error during ADC, calculations during processing must be performed at higher precisions than the input samples.|$|R
5000|$|... {{might be}} {{repeated}} 9 or 10 times, depending on <b>rounding</b> <b>errors</b> and/or the hardware and/or the compiler version. Furthermore, if the increment of X occurs by repeated addition, accumulated <b>rounding</b> <b>errors</b> {{may mean that}} the value of X in each iteration can differ quite significantly from the expected sequence 0.1, 0.2, 0.3, ..., 1.0.|$|R
40|$|Run-time {{analysis}} {{provides an}} effective method {{for measuring the}} sensitivity of programs to <b>rounding</b> <b>errors.</b> To date, implementations have required significant changes to source code, detracting from their widespread application. In this work we present an open source system that automates the quantitative analysis of floating point <b>rounding</b> <b>errors,</b> {{through the use of}} C-based source-to-source compilation and a Monte Carlo Arithmetic library. We demonstrate its application to the comparison of algorithms, detection of catastrophic cancellation, and determination of whether single-precision floating point provides sufficient accuracy for a given application. Methods for obtaining quantifiable measurements of sensitivity to <b>rounding</b> <b>error</b> are also detailed...|$|R
5000|$|... 1Free Voters Community2All for one3Citizens Initiative for Future4regardless of <b>rounding</b> <b>errors</b> ...|$|R
500|$|Atticus Ross (12 <b>Rounds,</b> <b>Error,</b> How to Destroy Angels, Nine Inch Nails) ...|$|R
5000|$|In {{recent years}} the {{equation}} has changed due to <b>rounding</b> <b>errors</b> to: ...|$|R
40|$|In this paper, {{lossless}} audio coding {{using the}} integer modified discrete cosine transform (IntMDCT) is discussed. The IntMDCT is constructed as an integer approximation of the MDCT using the lifting scheme and is reversible. The <b>rounding</b> <b>error</b> {{shape of the}} IntMDCT is derived. When the spectral energy of the input audio signal is concentrated at the low frequencies, the <b>rounding</b> <b>error</b> spectrum limits the lossless coding performance. A method for shaping the <b>rounding</b> <b>error</b> in the transform domain is presented. This <b>rounding</b> <b>error</b> shaping scheme manipulates the error {{so that it is}} below the spectral envelope of the signal at the high frequencies in order to improve the lossless coding performance for the signal. Examples of an error shaping filter design are presented and verified by simulations. An IntMDCT-based lossless coding implementation is carried out to illustrate the use of the error shaping filters...|$|R
5000|$|Floating point numbers {{may contain}} <b>rounding</b> <b>errors</b> {{since they are}} stored as text ...|$|R
40|$|The {{numerical}} {{construction of}} polynomials {{in the product}} representation (as used for instance in variants of the multiboson technique) can become problematic if <b>rounding</b> <b>errors</b> induce an imprecise or even unstable evaluation of the polynomial. We give criteria to quantify {{the effects of these}} <b>rounding</b> <b>errors</b> on the computation of polynomials approximating the function $ 1 /s$. We consider polynomials both in a real variable $s$ and in a Hermitian matrix. By investigating several ordering schemes for the monomials of these polynomials, we finally demonstrate that there exist orderings of the monomials that keep <b>rounding</b> <b>errors</b> at a tolerable level. Comment: Latex 2 e file, 7 figures, 32 page...|$|R
40|$|Monte-Carlo {{arithmetic}} {{is a form}} of self-validating arithmetic {{that accounts}} for the effect of <b>rounding</b> <b>errors.</b> We have implemented a floating point unit that can perform either IEEE 754 or Monte-Carlo floating point computation, allowing hardware accelerated validation of results during execution. Experiments show that our approach has a modest hardware overhead and allows the propagation of <b>rounding</b> <b>error</b> to be accurately estimated...|$|R
25|$|The {{incremental}} method with reduced <b>rounding</b> <b>errors</b> {{can also be}} applied, with some additional complexity.|$|R
25|$|Interval {{arithmetic}} is {{used with}} error analysis, to control <b>rounding</b> <b>errors</b> arising from each calculation.|$|R
50|$|The {{incremental}} method with reduced <b>rounding</b> <b>errors</b> {{can also be}} applied, with some additional complexity.|$|R
5000|$|... #Caption: Example {{showing the}} {{difficulty}} of choosing [...] due to both <b>rounding</b> <b>error</b> and formula error ...|$|R
40|$|A-posteriori forward <b>rounding</b> <b>error</b> {{analyses}} tend to give sharper error estimates than a-priori ones, as {{they use}} actual data quantities. One of such a-posteriori analysis – running error analysis – uses expressions {{consisting of two}} parts; one generates the error and the other propagates input errors to the output. This paper suggests replacing the error generating term with an FPU-extracted <b>rounding</b> <b>error</b> estimate, which produces a sharper error bound...|$|R
40|$|Using {{automatic}} differentiation (AD) {{to estimate}} the propagation of <b>rounding</b> <b>errors</b> in numerical algorithms is classic. We propose a new application of AD to roundoff analysis providing an automatic correction of the first order effect of the elementary <b>rounding</b> <b>errors.</b> We present the main characteri- stics of this method and significant examples of its application to improve the accuracy of computed results and/or {{the stability of the}} algorithm...|$|R
40|$|Pipelined Krylov solvers {{typically}} offer better scalability in {{the strong}} scaling limit compared to standard Krylov methods. The synchronization bottleneck is mitigated by overlapping time-consuming global communications with useful computations in the algorithm. However, to achieve this communication hiding strategy, pipelined methods feature multiple recurrence relations on additional auxiliary variables to update the guess for the solution. This paper aims at studying the influence of <b>rounding</b> <b>errors</b> on the convergence of the pipelined Conjugate Gradient method. It is analyzed why rounding effects have a significantly larger impact on the maximal attainable accuracy of the pipelined CG algorithm compared to the traditional CG method. Furthermore, an algebraic model for the accumulation of <b>rounding</b> <b>errors</b> throughout the (pipelined) CG algorithm is derived. Based on this <b>rounding</b> <b>error</b> model, we then propose an automated residual replacement strategy to reduce the effect of <b>rounding</b> <b>errors</b> on the final iterative solution. The resulting pipelined CG method with automated residual replacement improves the maximal attainable accuracy of pipelined CG to a precision {{comparable to that of}} standard CG, while maintaining the efficient parallel performance of the pipelined method...|$|R
40|$|The Chebyshev {{points are}} {{commonly}} used for spectral differentiation in non-periodic domains. The <b>rounding</b> <b>error</b> in the Chebyshev approximation to the n-the derivative increases at a rate greater than n^ 2 m for the m-th derivative. The mapping technique of Kosloff and Tal-Ezer (J. Comp. Physics, vol. 104 (1993), p. 457 - 469) ameliorates this increase in <b>rounding</b> <b>error.</b> We show that the argument used to justify {{the choice of the}} mapping parameter is substantially incomplete. We analyze <b>rounding</b> <b>error</b> as well as discretization error and give a more complete argument for the choice of the mapping parameter. If the discrete cosine transform is used to compute derivatives, we show that a different choice of the mapping parameter yields greater accuracy...|$|R
5000|$|According to The Economist, however, IKEA's {{charitable}} giving is meager, [...] "barely a <b>rounding</b> <b>error</b> in the foundation's assets." ...|$|R
50|$|In the {{rightmost}} column, {{summed up}} area {{adds up to}} 100858.55 rather than the correct 100858.53 due to <b>rounding</b> <b>error.</b>|$|R
5000|$|A {{variant of}} the last formula which avoids <b>rounding</b> <b>errors</b> being blown up is {{sometimes}} used in high precision computation: ...|$|R
50|$|Some languages, such as REXX and Java, provide decimal {{floating}} points operations, {{which provide}} <b>rounding</b> <b>errors</b> {{of a different}} form.|$|R
40|$|Arevised {{algorithm}} is given for unconstrained optimization using quasi-Newton methods. The method {{is based on}} recurring the factorization of an approximation to the Hessian matrix. Knowledge of this factorization allows greater flexibility when choosing the direction of search while minimizing the adverse effects of <b>rounding</b> <b>error.</b> The control of <b>rounding</b> <b>error</b> is particularly important when analytical derivatives are unavailable, and a modification of the algorithm to accept finite-difference approximations to the derivatives is given. 1...|$|R
