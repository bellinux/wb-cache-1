796|1939|Public
2500|$|The science {{mission was}} {{conducted}} by the three instruments detailed previously: DIRBE, FIRAS and the DMR. [...] The instruments overlapped in wavelength coverage, providing <b>consistency</b> <b>check</b> on measurements in the regions of spectral overlap and assistance in discriminating signals from our galaxy, Solar System and CMB.|$|E
50|$|When opening {{existing}} archives, {{a strict}} <b>consistency</b> <b>check</b> can be requested.|$|E
5000|$|... it is a {{comprehensive}} <b>consistency</b> <b>check</b> {{as it should be}} able to reproduce its own object code.|$|E
40|$|This paper {{describes}} {{a class of}} formal analysis called <b>consistency</b> <b>checking</b> that mechanically checks requirements speci cations, expressed in the SCR tabular notation, for application-independent properties. Properties include domain coverage, type correctness, and determinism. As background, the SCR notation for specifying requirements is reviewed. A formal requirements model describing {{the meaning of the}} SCR notation is summarized, and <b>consistency</b> <b>checks</b> derived from the formal model are described. The results of experiments to evaluate the utility of automated <b>consistency</b> <b>checking</b> are presented. Where <b>consistency</b> <b>checking</b> of requirements ts in the software development process is discussed. ...|$|R
40|$|<b>Consistency</b> <b>checking</b> of {{cardinal}} directions {{is one of}} {{the important}} problems in qualitative spatial reasoning. This paper presents a graph model to visually represent direction specifications. In the model, nodes represent regions occupied by objects, and directed edges indicate direction relationships between objects. This graph model can be applied not only to <b>consistency</b> <b>checking,</b> but also to general spatial reasoning. Based on this model, we present an efficient algorithm that performs <b>consistency</b> <b>checking</b> on a set of definitive direction specifications by analyzing the connectivity of the participating nodes. The <b>consistency</b> <b>checking</b> algorithm is performed in O(n 4) time. 1...|$|R
30|$|Local-based <b>consistency</b> <b>checking.</b>|$|R
50|$|The <b>consistency</b> <b>check</b> of step 1 {{is usually}} a very {{interactive}} and iterative process that might take weeks to complete. During this time discrepancies need to be detected, investigated and resolved.|$|E
50|$|As a <b>consistency</b> <b>check,</b> if the {{empirical}} estimator happens to equal the incidence rate, i.e. μi = xi / N, the smoothed estimator {{is independent of}} α and also equals the incidence rate.|$|E
50|$|When {{an update}} message is received, a node updates its {{distance}} table and reassesses the best route paths. It also carries out a <b>consistency</b> <b>check</b> with its neighbors, to help eliminate loops and speed up convergence.|$|E
30|$|<b>Consistency</b> <b>checking</b> {{for each}} MIC set.|$|R
5000|$|Semantic diagram editor, {{real-time}} model <b>consistency</b> <b>checking</b> ...|$|R
30|$|Step 5 Balancing the {{database}} and <b>consistency</b> <b>checks.</b>|$|R
50|$|The system utility fsck (file system <b>consistency</b> <b>check)</b> is a {{tool for}} {{checking}} the consistency of a file system in Unix and Unix-like operating systems, such as Linux and macOS. A similar command, CHKDSK exists in Microsoft Windows.|$|E
50|$|The science {{mission was}} {{conducted}} by the three instruments detailed previously: DIRBE, FIRAS and the DMR. The instruments overlapped in wavelength coverage, providing <b>consistency</b> <b>check</b> on measurements in the regions of spectral overlap and assistance in discriminating signals from our galaxy, Solar System and CMB.|$|E
50|$|Attention {{should also}} {{be paid to the}} {{implementation}} of the <b>consistency</b> <b>check,</b> as it is usually carried out after each read operation or {{at the end of each}} variable's life period. Carefully implementing this check can minimize the CPU time and code size for this application.|$|E
40|$|International audienceWe {{study the}} problem of <b>consistency</b> <b>checking</b> for {{constraint}} networks over combined qualitative formalisms. We propose a framework which encompasses loose integrations and a form of spatio-temporal reasoning. In particular, we identify sufficient conditions ensuring the polynomiality of <b>consistency</b> <b>checking,</b> and we use them to find tractable subclasses...|$|R
40|$|Multiperspectives {{naturally}} arise out of co-operative work {{in applying}} appropriate technologies to construct {{different parts of}} an application. The representation styles of various perspectives can be highly heterogeneous and open-ended since those perspectives should be presented in a form appropriate to each participant in the software development process. This {{makes it difficult to}} provide <b>consistency</b> <b>checking</b> and integration mechanisms for the perspectives. This paper presents an approach of dealing with automated <b>consistency</b> <b>checking</b> for multiperspective software specifications regardless of their heterogeneity of representation. We apply expressive graph notations called Conceptual Graphs (CGs) as visual <b>consistency</b> <b>checking</b> notations. The combination of underlying logical reasoning and graph-based reasoning of CGs provide a powerful <b>consistency</b> <b>checking</b> mechanism. Our framework is illustrated with excerpts of a case study using Unified Modeling Language (UML) ...|$|R
40|$|This paper {{describes}} {{a class of}} formal analysis called <b>consistency</b> <b>checking</b> that mechanically checks requirements specifications, expressed in the SCR tabular notation, for application-independent properties. Properties include domain coverage, type correctness, and determinism. As background, the SCR notation for specifying requirements is reviewed. A formal requirements model describing {{the meaning of the}} SCR notation is summarized, and <b>consistency</b> <b>checks</b> derived from the formal model are described. The results of experiments to evaluate the utility of automated <b>consistency</b> <b>checking</b> are presented. Where <b>consistency</b> <b>checking</b> of requirements fits in the software development process is discussed. 1 Introduction A recent study of industrial application of formal methods concludes that formal methods, including those for specifying and analyzing requirements, are "beginning to be used seriously and successfully by industry: : : to develop systems of significant scale and importance" [5] [...] ...|$|R
50|$|All data columns that {{refer to}} Master Data may be {{validated}} for its <b>consistency</b> <b>check.</b> A DQ check administered {{on the data}} {{at the point of}} entry discovers new data for the MDM process, but a DQ check administered after the point of entry discovers the failure (not exceptions) of consistency.|$|E
50|$|The native {{file system}} is non-CBM style {{at the low}} level to allow {{partitions}} greater than 16 MiB. High-level features like the 16-character filenames or filetypes are retained. Due to complexity and memory requirements, the filesystem creation and <b>consistency</b> <b>check</b> {{is not part of}} the operating system, unlike CBM DOS or CMD DOS.|$|E
5000|$|T=W /* current theory */ A=0 /* set of {{defaults}} applied so far */ [...] /* apply {{a sequence}} of defaults */ {{while there is a}} default d that is not in A and is applicable to T add the consequence of d to T add d to A [...] /* final <b>consistency</b> <b>check</b> */ if [...] for every default d in A T is consistent with all justifications of d then output T ...|$|E
40|$|This paper {{describes}} a formal analysis technique, called <b>consistency</b> <b>checking,</b> for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique {{is designed to}} analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the SCR notation and a foundation for <b>consistency</b> <b>checking,</b> a formal requirements model is introduced; the model represents a software system as a finite state automaton, which produces externally visible outputs in response to changes in monitored environmental quantities. Results are presented of two experiments which evaluated the utility and sealability of our technique for <b>consistency</b> <b>checking</b> in a real-world avionics application. The role of <b>consistency</b> <b>checking</b> during the requirements phase of software development is discussed...|$|R
40|$|This paper {{considers}} {{definitions of}} consistency {{arising from the}} RM-ODP and relates these in a mathematical framework for <b>consistency</b> <b>checking.</b> We place existing FDTs, in particular LOTOS, into this framework. Then we consider the prospects for viewpoint translation. Our conclusions centre {{on the relationship between}} the different definitions of consistency and on the requirements for realistic <b>consistency</b> <b>checking...</b>|$|R
5000|$|Elicitation {{from various}} sources (users, {{interfaces}} to other systems), specification, and <b>consistency</b> <b>checking</b> ...|$|R
50|$|The AGM postulates (named {{after the}} names of their proponents, Alchourrón, Gärdenfors, and Makinson) are {{properties}} that an operator that performs revision should satisfy in order for that operator to be considered rational. The considered setting is that of revision, that is, different pieces of information referring to the same situation. Three operations are considered: expansion (addition of a belief without a <b>consistency</b> <b>check),</b> revision (addition of a belief while maintaining consistency), and contraction (removal of a belief).|$|E
50|$|In most cases, safety-critical {{applications}} have strict constraints {{in terms}} of memory occupation and system performance. The duplication of the whole set of variables {{and the introduction of}} a <b>consistency</b> <b>check</b> before every read operation represent the optimum choice from the fault coverage point of view. Duplication of the whole set of variables enables an extremely high percentage of faults to be covered by this software redundancy technique. On the other side, by duplicating a lower percentage of variables one can trade off the obtained fault coverage with the CPU time overhead.The experimental result shows that duplicating only 50% of the variables is enough to cover 85% of faults with a CPU time overhead of just 28%.|$|E
50|$|Whenever possible, {{redundant}} characters {{should be}} used at each step. For example, if a group is to be split into two subgroups, one characterized by six black spots and the other by four brown stripes, the user should be queried about all three characters (number, shape, and color of the markings) — even though any {{single one of them}} would be sufficient in theory. This redundancy improves the reliability of identification, provides a <b>consistency</b> <b>check</b> against user errors, and allows the user to proceed even if some of the characters could not be observed. In this case, the characters should be ordered according to their reliability and convenience. Further error tolerance can be achieved by using reticulation.|$|E
40|$|Software {{models are}} key in {{separating}} and solving independent development concerns. However, {{there is still}} a gap on how to transition design information among these separate, but related models during development and maintenance. This paper addresses the problem on how to maintain the consistency of UML class diagrams during various levels of refinement. We present a new approach to automated <b>consistency</b> <b>checking</b> called ViewIntegra. Our approach separates <b>consistency</b> <b>checking</b> into transformation and comparison. It uses transformation to translate model elements to simplify their subsequent comparison. Transformation-based <b>consistency</b> <b>checking,</b> in the manner we use it, is new since we use transformation {{to bridge the gap between}} software models...|$|R
40|$|We present view transactions, a {{model for}} relaxed <b>consistency</b> <b>checks</b> in {{software}} transactional memory (STM). View transactions always operate on a consistent snapshot of memory but may commit in a different snapshot. They are therefore simpler to reason about, provide opacity and maintain composability. In addition, view transactions avoid many of the overheads associated with previous approaches for relaxing <b>consistency</b> <b>checks.</b> ...|$|R
40|$|In this paper, {{we address}} the problem of <b>consistency</b> <b>checking</b> for Euclidean spatial constraints. A {{dimension}} graph representation is pmposed to maintain the Euclidean spatial constraints among objects. The basic idea is to project the spatial constraints on both X and Y dimensions, and to construct a dimension graph on each dimension. Using a dimension graph representation transfonns the problem of <b>consistency</b> <b>checking</b> into the problem of graph cycle detection. <b>Consistency</b> <b>checking</b> can be achieved with O(N+E) time as well as space complexity, where N is the number of spatial objects, and E is the number of spatial predicates in the constraint. The proposed approach is faster than O(N 2) when the number of predicates is much smaller than N 2 and there are few disjunctions in the spatial constraint. The dimension graph and <b>consistency</b> <b>checking</b> algorithm can be used for points, intervals and polygons in two-dimensional space. The algorithm can also guarantee global consistency...|$|R
5000|$|If {{you have}} disk corruption, {{if you have}} DRAM corruption, {{if you have any}} kind of {{problems}} at all, Git will notice them. It's not a question of if, it's a guarantee. You can have people who try to be malicious. They won't succeed. ... Nobody has been able to break SHA-1, but the point is the SHA-1, as far as Git is concerned, isn't even a security feature. It's purely a <b>consistency</b> <b>check.</b> The security parts are elsewhere, {{so a lot of people}} assume that since Git uses SHA-1 and SHA-1 is used for cryptographically secure stuff, they think that, Okay, it's a huge security feature. It has nothing at all to do with security, it's just the best hash you can get. ...|$|E
5000|$|... ‘System’, ‘Encapsulated Class’, ‘Program’, and ‘Process’, {{are other}} base classes for complex structures, {{which can be}} {{combined}} iteratively to produce more complex systems. The component class ‘System’ is to store in the knowledge base a set of propositions composed into ontologies, axiomatic systems, complex systems like say a human body, an artifact like a vehicle etc., with or without <b>consistency</b> <b>check.</b> An ‘Encapsulated Class’ is to com-pose declarative and behavioural objects in a flexible way to build classes. A ‘Program’ {{is not only to}} store the logic of any complete program or a component class, composed from the already available behavioural instances in the knowledge base with built-in connectives (conditions, and loops), but also execute them as web services. A ‘Process’ is to structure temporal objects with sequence, concurrency, synchronous or asynchronous specifications.|$|E
50|$|The {{inventory}} {{consists of}} 225 pairs of statements in which items {{from each of}} the 15 scales are paired with items from the other 14 plus the other fifteen pairs of items for the optional <b>consistency</b> <b>check.</b> This leaves the total number of items (14x15) at 210. Edwards has used the last 15 items to offer the candidate the same item twice, using the results to calculate a consistency score. The result will be considered valid if the consistency checks for more than 9 out of 15 paired items. Within each pair, the subjects choose one statement as more characteristic of themselves, reducing the social desirability factor of the test. Due to the forced choice, the EPPS is an ipsative test, the statements are made in relation to the strength of an individual's other needs. Hence, like personality, it is not absolute. Results of the test are reliable, although there are doubts about the consistency scale.|$|E
40|$|Every Argo {{data file}} {{submitted}} by a DAC for distribution on the GDAC has its format and data <b>consistency</b> <b>checked</b> by the Argo FileChecker. Two {{types of checks}} are applied: 1. Format checks. Ensures the file formats match the Argo standards precisely. 2. Data <b>consistency</b> <b>checks.</b> Additional data <b>consistency</b> <b>checks</b> are performed on a file after it passes the format checks. These checks do not duplicate any of the quality control checks performed elsewhere. These checks {{can be thought of}} as “sanity checks” to ensure that the data are consistent with each other. The data <b>consistency</b> <b>checks</b> enforce data standards and ensure that certain data values are reasonable and/or consistent with other information in the files. Examples of the “data standard” checks are the “mandatory parameters” defined for meta-data files and the technical parameter names in technical data files. Files with format or consistency errors are rejected by the GDAC and are not distributed. Less serious problems will generate warnings and the file will still be distributed on the GDAC. Reference Tables and Data Standards: Many of the <b>consistency</b> <b>checks</b> involve comparing the data to the published reference tables and data standards. These tables are documented in the User’s Manual. (The FileChecker implements “text versions” of these tables. ...|$|R
40|$|This paper {{implements}} {{the reconstruction}} of 3 D object from real scene input images using voxel coloring. We proposed new color <b>consistency</b> <b>checking</b> formula in voxel coloring by using mean charts. Using this color <b>consistency</b> <b>checking,</b> the system do not require to be given parameters as constant threshold input and the system can reconstruct more detail than using original color <b>consistency</b> <b>checking</b> formula using threshold on standard deviation of image pixels value. Photorealistic 3 D object reconstruction is an active area in computer graphics. Input of this system is multiple photographs which are taken from multiple view points of scene. And then the object in the photographs will be projected back to the 3 D coordinate and applied voxel coloring method to reconstruct and view the object in 3 D. Voxel coloring works by discretizing scene space into voxels, the smallest unit in 3 D, then traversed and colored in specific order. The output of voxel coloring {{is a collection of}} voxels that represent the object. Those voxels can be rendered to get a new view of that object. To determine whether voxel has to be colored or be emptied, some color <b>consistency</b> <b>checking</b> formula must be used. The original color <b>consistency</b> <b>checking</b> formula is standard deviation calculation of all image pixels which voxel has been projected and compare it with constant threshold which is defined by user. This constant threshold must be chosen carefully because wrong value can cause defect reconstruction. We propose new color <b>consistency</b> <b>checking</b> using mean charts which do not require user to choose constant threshold input...|$|R
40|$|Correctness is the {{essential}} requirement criteria for military web ontology based information systems. Reasoning and <b>consistency</b> <b>checking</b> can be useful at many stages during the design, maintenance and deployment of Semantic Web (SW) ontology. Formal methods can provide automatic reasoning and <b>consistency</b> <b>checking</b> services for SW. In this paper, we use military plan ontology {{as a case study}} to demonstrate how Alloy can be applied to check SW ontology...|$|R
