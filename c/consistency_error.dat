118|198|Public
500|$|The subplot of Kenny's {{diarrhea}} problems {{came from}} a real-life high school experience from Stone, who said students used to offer each other $20 if they would pass a note to the teacher explaining they had [...] "explosive diarrhea", like Kenny did in the episode. The image of Kenny sitting on a toilet in [...] "Death" [...] became a popular South Park poster. [...] "Death" [...] included a <b>consistency</b> <b>error</b> in that Mr. Garrison's classroom had its own separate bathroom, which was never again seen in future South Park episodes. During one scene, Cartman moons Kyle while making fun of Kyle's mother. Comedy Central censors forced Parker and Stone to remove the image of Cartman's bare bottom, although such images would be allowed in future episodes. A man named Mr. McCormick was killed in [...] "Death" [...] after he was flung via slingshot into the Cartoon Central building. The character's name led many to mistakenly believe it was Kenny's father, Stuart McCormick, but Parker denied this and said the similar character names were just a coincidence. The character appears again very briefly in a later episode, Starvin' Marvin.|$|E
500|$|The [...] "Volcano" [...] {{episode was}} in {{production}} when South Park debuted on August 13, 1997. Comedy Central executives did {{not object to}} most of the content of the episode, but said the scene in which Kyle farted while talking to Stan should have been removed because nothing happened after the fart, and they said it was not funny. Parker and Stone, however, insisted it stay in the episode, and they said the lack of any reaction whatsoever to the fart was what made it funny. During close-ups of Cartman's face while telling the story of Scuzzlebutt around the campfire, the flames from the fire stop moving. Parker and Stone noticed the <b>consistency</b> <b>error</b> after the episode was filmed, but they did not have enough time to go back and fix it before the broadcast date, so it was left in. A cat featured in the background of one of the outdoor scenes was designed to look exactly like Parker's cat, Jake. The scene in which Ned catches fire was based on an experience Parker had during a Colorado camping trip where he tried to do an [...] "Indian Fire Trick", in which one pours gasoline onto a fire to create large flames. Although nobody caught fire, Parker said the trick misfired and he nearly burned down the forest. After finishing the episode, Parker and Stone realized [...] "Volcano" [...] was about two minutes shorter than the time length required for the episode. In order to add time to the episode, Parker and Stone added the scene with Ned singing the song [...] "Kumbaya" [...] around the fire, as well as the long freeze-frame on a shocked Chef and Mayor McDaniels reacting to the news of the volcano's imminent eruption.|$|E
5000|$|The subplot of Kenny's {{diarrhea}} problems {{came from}} a real-life high school experience from Stone, who said students used to offer each other $20 if they would pass a note to the teacher explaining they had [...] "explosive diarrhea", like Kenny did in the episode. The image of Kenny sitting on a toilet in [...] "Death" [...] became a popular South Park poster. [...] "Death" [...] included a <b>consistency</b> <b>error</b> in that Mr. Garrison's classroom had its own separate bathroom, which was never again seen in future South Park episodes. During one scene, Cartman moons Kyle while making fun of Kyle's mother. Comedy Central censors forced Parker and Stone to remove the image of Cartman's bare bottom, although such images would be allowed in future episodes. A man named Mr. McCormick was killed in [...] "Death" [...] after he was flung via slingshot into the Cartoon Central building. The character's name led many to mistakenly believe it was Kenny's father, Stuart McCormick, but Parker denied this and said the similar character names were just a coincidence. The character appears again very briefly in a later episode, Starvin' Marvin.|$|E
50|$|Threads {{communicate}} {{primarily by}} sharing access to {{fields and the}} objects that reference fields refer to. This form of communication is extremely efficient, but makes two kinds of errors possible: thread interference and memory <b>consistency</b> <b>errors.</b> The tool needed to prevent these errors is synchronization.|$|R
40|$|Abstract—One-sided {{communication}} decouples data move-ment and synchronization {{by providing}} support for asynchronous reads and updates of distributed shared data. While such inter-faces {{can be extremely}} efficient, they also impose challenges in properly performing asynchronous accesses to shared data. This paper presents MC-Checker, a new tool that detects memory <b>consistency</b> <b>errors</b> in MPI one-sided applications. MC-Checker first performs online instrumentation and captures relevant dynamic events, such as one-sided communications and load/store operations. MC-Checker then performs analysis to detect memory <b>consistency</b> <b>errors.</b> When found, errors are reported along with useful diagnostic information. Experiments indicate that MC-Checker is effective at detecting and diagnosing memory consistency bugs in MPI one-sided applications, with low overhead, ranging from 24. 6 % to 71. 1 %, with an average o...|$|R
40|$|Historically, error {{inconsistency}} {{has been}} considered {{a defining feature of}} AOS. While recent studies have demonstrated <b>consistency</b> of certain <b>errors</b> over time, changes following intervention have not been reported. Therefore, we examined <b>error</b> <b>consistency</b> across successive repetitions of the same utterance following 120 hours of Multimodal intensive Treatment (MMiT). Three males with AOS and aphasia produced three repetitions of 10 target words before and after treatment. SLP evaluations of transcribed responses revealed increased <b>consistency</b> of <b>error</b> location across all participants and increased <b>consistency</b> in <b>error</b> number across successive responses. Further investigation of MMiT in relation to these findings is warranted...|$|R
5000|$|The [...] "Volcano" [...] {{episode was}} in {{production}} when South Park debuted on August 13, 1997. Comedy Central executives did {{not object to}} most of the content of the episode, but said the scene in which Kyle farted while talking to Stan should have been removed because nothing happened after the fart, and they said it was not funny. Parker and Stone, however, insisted it stay in the episode, and they said the lack of any reaction whatsoever to the fart was what made it funny. During close-ups of Cartman's face while telling the story of Scuzzlebutt around the campfire, the flames from the fire stop moving. Parker and Stone noticed the <b>consistency</b> <b>error</b> after the episode was filmed, but they did not have enough time to go back and fix it before the broadcast date, so it was left in. A cat featured in the background of one of the outdoor scenes was designed to look exactly like Parker's cat, Jake. The scene in which Ned catches fire was based on an experience Parker had during a Colorado camping trip where he tried to do an [...] "Indian Fire Trick", in which one pours gasoline onto a fire to create large flames. Although nobody caught fire, Parker said the trick misfired and he nearly burned down the forest. After finishing the episode, Parker and Stone realized [...] "Volcano" [...] was about two minutes shorter than the time length required for the episode. In order to add time to the episode, Parker and Stone added the scene with Ned singing the song [...] "Kumbaya" [...] around the fire, as well as the long freeze-frame on a shocked Chef and Mayor McDaniels reacting to the news of the volcano's imminent eruption.|$|E
50|$|In November and December 1941, the United States National Defense Research Committee {{conducted}} extensive tests {{between the}} Bausch and Lomb M1 stereoscopic rangefinder and the Barr and Stroud FQ 25 and UB 7 coincidence rangefinders:COINCIDENCE AND STEREOSCOPIC RANGE FINDERSThe {{first of these}} reports {{is concerned with the}} comparative test of coincidence and stereoscopic range finders. (353) In these tests the American stereoscopic Height Finder Ml was operated against the British coincidence type Range Finders FQ 25 and UB 7, in ranging on fixed ground targets, moving naval targets and moving aerial targets. The coincidence and stereoscopic methods utilize the same basic principles of geometrical optics for the determination of the distance to a target. The two methods differ radically, however, {{in the nature of the}} criterion presented for human judgement. These British instruments were of the split field coincident type. American crews were being trained at Fort Monroe to operate the coincidence instruments but this plan was dropped when six British seamen, who were experienced range takers, were made available for the tests. Until recently the British Services had tended strongly to the coincidence type of instrument while the American Services had adopted the stereoscopic principle for long-base instruments at least. The decisions of both the British and American Services apparently grow out of different interpretations of the experience of the Battle of Jutland in World War I and are of no concern in this place. Tests were run in November and December 1941 using the British seamen on the British instrument and experienced American observers on the Standard M1. Bad weather conditions and various experimental difficulties and mishaps made it impossible to obtain a really satisfactory quantity of data before the tests hall to be terminated. Fixed target reading were made on targets from 2,700 to 14,500 yards. Only five aerial courses could be recorded and these were all level flight courses, at altitudes of 3,000 to 4000 yards and slant range between 4,000 and 12,000 yards Continuous contact was used. Nine courses were obtained on slow moving naval targets at ranges from 4,000 to 12,000 yards. In these latter courses continuous and broken contact were used at different times.It was found, throughout the tests, that the performances of the various instruments were more nearly alike when measured in external units (Reciprocal range) than when measured in terms of error at the observer's eye, in spite of marked differences in physical dimensions of the instruments. The American MI has a base length of 4.5 yards and used 12 power; FQ 25 with a 6 yard base used 28 power and UB 7, a portable instrument, has 25 power and 3-yard base. The coincidence instruments did not use internal adjusters but were calibrated on targets of known range. In other words. the net performance of the different instruments were essentially comparable although the instruments exhibited varying degrees of efficiency in performance relative to the size. On aerial courses precision errors of the four instruments were about alike when measured in reciprocal-range units. In UOE, the FQ 25 had comparatively poor precision, while the UB 7, for three of the five aerial courses, had very small precision errors. The number of aerial courses was too small to yield much information about consistency of observations from one course to the next.For the naval target courses, one American instrument was not operating. Precision errors of the other three instruments were similar to those on aerial height courses. In reciprocal range units the three instruments had comparable precision. In UOE the FQ 25 was worse and the UB 7 was better than the American M1. <b>Consistency</b> <b>error</b> of the UB 7 was smaller than that of the M1, even when measured in reciprocal range units, while the FQ 25 was similar in consistency to the Ml, again in reciprocals units. On ground targets the same general situation holds. Consistency errors of the four instruments over the 9-day period were the same when measured in reciprocal-range units. Again the UB 7 was better than the stereoscopic instruments in UOE and the FQ 25 was worse. Consistency over the 9 days was not perceptibly worse than daily consistency for any of the instruments. In other words, the readings over the 9 days did not scatter in total more than did readings for a typical day.An analysis or these results leads to the following conclusions. (1) Performance of the coincidence and stereoscopic instruments was about the same when range errors were measured in yards (2) The UB 7 however, with a virtual base length smaller than that of the American stereoscopic instruments was more efficient than the stereoscopic height finders in terms of performance for its size, while the large coincidence instrument, the FQ 25. was less efficient in this sense. This situation held for all types of targets— fixed ground, naval, and aerial. (3) The UB 7 is somewhat better than the American instrument in consistency on naval targets, even when measured in external units. This report is attached, as supporting data, to a Report to the Services issued by the Fire Control Division of NDRC (20). This points out that the tests indicate no important difference in the precision obtainable from the two types of instrument— coincidence and stereoscopic They do indicate, however, that the difference in performance between large and small instruments is by no means as great as would be anticipated from simple geometrical optics. The report concludes with the belief that stereoscopic and coincidence acuities are about equal. Under favourable conditions existing instruments of the two types perform about equally well, and the choice between them for any given purpose must be based on matters of convenience related to the particular conditions under which they are to be used.|$|E
40|$|We {{prove that}} the time {{averaged}} <b>consistency</b> <b>error</b> of the Nth approximate deconvolution LES model converges to zero uniformly in the kinematic viscosity and in the Reynolds number as the cube root of the averaging radius. We also give a higher order but non-uniform <b>consistency</b> <b>error</b> bound for the zeroth order model directly from the Navier-Stokes equations...|$|E
50|$|The Job layer {{provides}} a common interface for submitting Trove, BMI, and flow jobs and reporting their completion. It also implements the request scheduler as a non-blocking job that records {{what kind of}} requests are in progress on which objects and prevents <b>consistency</b> <b>errors</b> due to simultaneously operating on the same file data.|$|R
40|$|This paper {{discusses}} {{the problem of}} risk in optimistic simulation protocols, using as example simulation of a distributed mutual exclusion protocol with strong consistency properties. The simulation model is augmented to detect model inconsistency errors resulting from risky optimistic simulation. While the model runs sequentially without <b>consistency</b> <b>errors,</b> errors occur when the model is executed in parallel optimistically. Some of the errors entirely violate the fundamentalmutual exclusion properties of the model itself. To address this problem we extend the optimistic simulation library to eliminate these inconsistencies. We discuss the details of these extensions and the performance tradeoff for adding them...|$|R
40|$|International audienceAtmospheric {{dispersion}} {{models are}} usually off-line coupled to mesoscale meteorological models. This may generate {{the loss of}} mass consistency, defined as the conservation of uniform mixing ratio. We investigate in this short paper {{the impact of the}} resulting mass <b>consistency</b> <b>errors.</b> Three methods based on the renormalization of density, on fluxes computed with mass mixing ratio and on the adjustment of the vertical velocity, respectively, are aimed at reducing the mass <b>consistency</b> <b>errors.</b> They are benchmarked and applied to two test cases: air quality modeling over Europe for summer 2001 (typical of reactive dispersion) and simulation of the Chernobyl accident (typical of passive dispersion). Our tests indicate the differences between the passive and the reactive cases. The investigation of the spatial patterns (especially of the vertical distribution) discriminates the method based on the adjustment of the vertical velocity. Indeed, this method suffers from the enhancement of the numerical diffusion (illustrated in the passive case) and from the modification of the escape flux (for ozone) ...|$|R
40|$|Abstract. We {{prove that}} {{that the time}} {{averaged}} <b>consistency</b> <b>error</b> of the Nth approximate deconvolution LES model converges to zero uniformly in the kinematic viscosity and in the Reynolds number as the cube root of the averaging radius. We also give a higher order but non-uniform <b>consistency</b> <b>error</b> bound for the zeroth order model directly from the Navier-Stokes equations. Key words. large eddy simulation, approximate deconvolution model, turbulence 1. Introduction. Direc...|$|E
3000|$|The {{evaluation}} {{of these three}} segmentation algorithms is based on two metrics defined by Martin et al.: Global <b>Consistency</b> <b>Error</b> (GCE), and Local <b>Consistency</b> <b>Error</b> (LCE) [4]. These measures operate by computing the degree of overlap between clusters or the cluster associated with each pixel in one segmentation and its [...] "closest" [...] approximation in the other segmentation. GCE and LCE metrics allow labeling refinement in either one or both directions, respectively.|$|E
40|$|International audienceWe {{consider}} the case of a homogeneous, isotropic, fully developed, turbulent flow. We show analytically by using the - 5 / 3 Kolmogorov's law that the time-averaged <b>consistency</b> <b>error</b> of the Nth approximate deconvolution LES (large eddy simulation) model converges to zero following a law as the cube root of the averaging radius, independently of the Reynolds number. The <b>consistency</b> <b>error</b> is measured by the residual stress. The filter under consideration is a second-order differential filter, but the 1 / 3 law is still valid {{in the case of the}} Gaussian filter and large class of filters used in LES. We also show how the 1 / 3 error law can be derived by a dimensional analysis...|$|E
50|$|In Java, {{to prevent}} thread {{interference}} and memory <b>consistency</b> <b>errors,</b> blocks of code are wrapped into synchronized (lock_object) sections. This forces any thread {{to acquire the}} said lock object before it can execute the block. The lock is automatically released when thread leaves the block or enter the waiting state within the block. Any variable updates, made by the thread in synchronized block, become visible to other threads whenever those other threads similarly acquires the lock.|$|R
5000|$|For example, DC's Crisis on Infinite Earths {{addressed}} {{continuity and}} <b>consistency</b> <b>errors</b> over almost 50 years of comics publication, and retrofitted events and characters {{into the history}} of the DCU as if they had always been there. (For example, the JSA went from being JLA-contemporaries from a parallel world to being their earlier, historical counterparts some years previously.) The Post-Crisis DC Universe removed many stories from [...] "official canon", explaining them as Imaginary Tales or ignoring them completely.|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 2015 Effective {{treatment}} programs for communication disorders {{are based on}} the underlying nature of the impairment; therefore, accurate diagnosis is critical. In some cases, however, reliable and valid methods of differentially diagnosing disorders with similar behavioral profiles are lacking. This is particularly true of acquired apraxia of speech (AOS) and aphasia characterized by frequent occurrences of phonemic paraphasia (PP). The differential diagnosis of AOS and aphasia with PP is challenging because both disorders result from left hemisphere stroke and share clinical characteristics. Therefore, the identification of characteristics that pattern uniquely to each disorder is important. One way in which to strengthen the current diagnostic process is to examine the validity of diagnostic criteria used to inform differential diagnosis. The current criteria proposed to differentiate AOS from aphasia with PP include: 1) slow speech rate characterized by prolonged segment and intersegment durations, 2) sound distortions, 3) distorted sound substitutions, 4) prosodic abnormalities, and 5) relatively consistent errors in regard to error location and error type. Of these characteristics, <b>error</b> <b>consistency</b> is the most controversial. <b>Error</b> <b>consistency</b> refers to whether or not errors are relatively consistent from trial to trial in regard to the location of errors within a word (e. g., word initial) and the type of errors produced (e. g., distortions vs. substitutions). Investigations comparing the nature of <b>error</b> <b>consistency</b> in AOS and aphasia with PP have revealed conflicting results. These studies, however, differ in important methodological areas, making it difficult to draw conclusions about the nature of <b>error</b> <b>consistency</b> in these two populations. Furthermore, previous studies suggest that <b>error</b> <b>consistency</b> may be influenced by a number of variables, such as error rate, severity of impairment, and stimulus presentation condition. This study sought to further examine the nature of <b>error</b> <b>consistency</b> in a group of 10 individuals with AOS and concomitant aphasia and a group of 11 individuals with aphasia with PP. Specifically, this study examined group differences in the <b>consistency</b> of <b>error</b> location and error type during the repetition of two-, three-, and five-syllable words. The influence of error rate, severity of impairment, and stimulus presentation condition on measures of <b>error</b> <b>consistency</b> was also examined, as well as group differences in the types of errors produced. Results suggest that <b>consistency</b> of <b>error</b> location does not differentiate group performance, whereas the variability of error type does. In particular, individuals with AOS and aphasia demonstrate more variable errors compared individuals with aphasia with PP. Results also indicate that the <b>consistency</b> of <b>error</b> location is influenced by error rate and severity of impairment. Stimulus presentation condition, however, did not appear to influence group performance on either measure of <b>error</b> <b>consistency.</b> Lastly, results of an error type analysis show that individuals with AOS and aphasia demonstrate significantly more phonetic errors compared to individuals with aphasia with PP. In conclusion, results do not support the use of <b>error</b> <b>consistency</b> as a valid measure in which to differentiate individuals with AOS and aphasia from individuals with aphasia with PP...|$|R
40|$|Robust {{registration}} between prone and supine data acquisitions for CT colonography is pivotal {{for medical}} interpretation but a challenging problem. One measure when evaluating non-rigid registration algorithms {{over the whole}} of the deformation field is the inverse <b>consistency</b> <b>error,</b> which suggests improved registration quality when the inverse deformation is consistent with the forward deformation. We show that using computed landmark displacements to initialise an intensity based registration reduces the inverse <b>consistency</b> <b>error</b> when using a state-of-the-art non-rigid b-spline registration method. This method aligns prone and supine 2 D images derived from CT colonography acquisitions in a cylindrical domain. Furthermore, we demonstrate that using the same initialisation also improves registration accuracy for a set of manually identified reference points in cases exhibiting local luminal collapse...|$|E
40|$|Abstract. We {{consider}} a bilinear reduced-strain finite {{element of the}} MITC family for a shallow Reissner-Naghdi type shell. We estimate the <b>consistency</b> <b>error</b> of the element in both membrane- and bending-dominated states of deformation. We prove that in the membrane-dominated case, under severe assumptions on the domain, the finite element mesh and the regularity of the solution, an error bound O(h + t − 1 h 1 +s) can be obtained if the contribution of transverse shear is neglected. Here t is {{the thickness of the}} shell, h the mesh spacing, and s a smoothness parameter. In the bending-dominated case, the uniformly optimal bound O(h) is achievable but requires that membrane and transverse shear strains are of order O(t 2) ast → 0. In this case we also show that under sufficient regularity assumptions the asymptotic <b>consistency</b> <b>error</b> has the bound O(h). 1...|$|E
40|$|Abstract: We {{consider}} a bilinear reduced-strain nite {{element of the}} MITC family for a shallow Reissner-Naghdi type shell. We estimate the <b>consistency</b> <b>error</b> of the element in both membrane- and bending-dominated states of de-formation. We prove that in the membrane-dominated case, under severe assumptions on the domain, the nite element mesh and on the regularity of the solution, an error bound O(h+ t 1 h 1 +s) can be obtained if the contribu-tion of transverse shear is neglected. Here t is {{the thickness of the}} shell, h the mesh spacing, and s a smoothness parameter. In the bending-dominated case, the uniformly optimal bound O(h) is achievable but requires that membrane and transverse shear strains are of order O(t 2) as t! 0. In this case we also show that under sucient regularity assumptions the asymptotic <b>consistency</b> <b>error</b> has the bound O(h). AMS subject classications: 65 N 30, 73 K 1...|$|E
40|$|Much {{work has}} gone into the {{construction}} of quasicontinuum energies that reduce the coupling error along the interface between atomistic and continuum regions. The largest <b>consistency</b> <b>errors</b> are typically pointwise O(1 /) errors, {{and in some cases}} this has been reduced to pointwise O(1) errors. In this paper we show that one cannot create a coupling method using a finite-range coupling interface that has o(1) -consistency in the interface, and we use this to give an upper bound on the order of convergence in discrete w^ 1,p-norms in 1 D. Comment: 6 page...|$|R
40|$|Abstract. Much {{work has}} gone into the {{construction}} of quasicontinuum energies that reduce the coupling error along the interface between atomistic and continuum regions. The largest <b>consistency</b> <b>errors</b> are typically pointwise O (1) errors, {{and in some cases}} this has been reduced ε to pointwise O(1) errors. In this paper we show that one cannot create a coupling method using a finite-range coupling interface that has o(1) -consistency in the interface, and we use this to give an upper bound on the order of convergence in discrete w 1,p-norms in 1 D. hal- 00620875, version 1 - 8 Sep 2011 1...|$|R
40|$|Stability and {{convergence}} of full discretizations of various surface evolution equations are studied in this paper. The proposed discretization combines a higher-order evolving-surface {{finite element method}} (ESFEM) for space discretization with higher-order linearly implicit backward difference formulae (BDF) for time discretization. The stability of the full discretization is studied in the matrix [...] vector formulation of the numerical method. The geometry of the problem enters into the bounds of the <b>consistency</b> <b>errors,</b> but does not enter into the proof of stability. Numerical examples illustrate the convergence behaviour of the full discretization. Comment: arXiv admin note: text overlap with arXiv: 1607. 0717...|$|R
40|$|Abstract. This paper {{presents}} {{a new image}} registration algorithm that accommodates locally large nonlinear deformations. The algorithm concurrently estimates the forward and reverse transformations between a pair of images while minimizing the inverse <b>consistency</b> <b>error</b> between the transformations. It assumes that the two images to be registered contain topologically similar objects and were collected using the same imaging modality. The large deformation transformation from one image to the other is accommodated by concatenating a sequence of small deformation transformations. Each incremental transformation is regularized using a linear elastic continuum mechanical model. Results of ten 2 D and twelve 3 D MR image registration experiments are presented that tested the algorithm’s performance on real brain shapes. For these experiments, the inverse <b>consistency</b> <b>error</b> was reduced on average by 50 times in 2 D and 30 times in 3 D compared to the viscous fluid registration algorithm. ...|$|E
40|$|Abstract. The {{purpose of}} this report is to give a {{self-contained}} and detailed mathematical introduction to the analysis in the report [LL 04 c] of the accuracy of some predictive model’s of turbulence. The models are based on approximate de-convolution methods and were introduced into LES by Stolz and Adams. We recall {{the development of the}} models, review the known theory of the models and expand the proof from [LL 04 c] that the time averaged <b>consistency</b> <b>error</b> of the Nth approximate deconvolution LES model converges to zero uniformly in the kinematic viscosity and in the Reynolds number as the cube root of the averaging radius. We also give a higher order but non-uniform <b>consistency</b> <b>error</b> bound for the zeroth order model directly from the Navier-Stokes equations and study the distribution of consistency errors among length-scales. Key words. large eddy simulation, approximate deconvolution model, turbulence 1. Introduction. Direc...|$|E
40|$|The paper {{deals with}} a {{non-conforming}} finite element method on a class of anisotropic meshes. The Crouzeix-Raviart element is used on triangles and tetrahedra. For rectangles and prismatic (pentahedral) elements a novel set of trial functions is proposed. Anisotropic local interpolation error estimates are derived for all these types of element and for functions from classical and weighted Sobolev spaces. The <b>consistency</b> <b>error</b> is estimated for a general differential equation under weak regularity assumptions. As a particular application, an example is investigated where anisotropic finite element meshes are appropriate, namely the Poisson problem in domains with edges. A numerical test is described. Key Words Anisotropic mesh, Crouzeix-Raviart element, non-conforming finite element method, anisotropic interpolation error estimate, <b>consistency</b> <b>error,</b> edge singularity. AMS(MOS) subject classification 65 N 30; 65 N 15, 65 N 50, 65 D 05. Preprint-Reihe des Chemnitzer SFB 393 SFB 393 / 99 - 10 Mai C [...] ...|$|E
40|$|A {{non-conforming}} nite element method with anisotropic mesh grading for the Stokes {{problem in}} domains with edges Preprint SFB 393 / 00 - 11 Abstract The {{solution of the}} Stokes problem in three-dimensional domains with edges has anisotropic singular behaviour which is treated numerically by using anisotropic nite element meshes. The velocity is approximated by Crouzeix-Raviart (non-conforming P 1 elements and the pressure by piecewise constants. This method is stable for general meshes (without minimal or maximal angle condition). The interpolation and <b>consistency</b> <b>errors</b> are of the optimal order h N 1 = 3 which is proved for tensor product meshes. As a by-product, we analyse also non-conforming prismatic elements with P 1 span f...|$|R
40|$|Purpose: Deformable image {{registration}} (DIR) {{is necessary for}} accurate dose accumulation between multiple radiotherapy image sets. DIR algorithms can suffer from inverse and transitivity inconsistencies. When using deformation vector fields (DVFs) that exhibit inverse-inconsistency and are nontransitive, dose accumulation on a given image set via different image pathways will lead to different accumulated doses. The {{purpose of this study}} was to investigate the dosimetric effect of and propose a postprocessing solution to reduce inverse <b>consistency</b> and transitivity <b>errors.</b> Methods: Four MVCT images and four phases of a lung 4 DCT, each with an associated calculated dose, were selected for analysis. DVFs between all four images in each data set were created using the Fast Symmetric Demons algorithm. Dose was accumulated on the fourth image in each set using DIR via two different image pathways. The two accumulated doses on the fourth image were compared. The inverse <b>consistency</b> and transitivity <b>errors</b> in the DVFs were then reduced. The dose accumulation was repeated using the processed DVFs, the results of which were compared with the accumulated dose from the original DVFs. To evaluate the influence of the postprocessing technique on DVF accuracy, the original and processed DVF accuracy was evaluated on the lung 4 DCT data on which anatomical landmarks had been identified by an expert. Results: Dose accumulation to the same image via different image pathways resulted in two different accumulated dose results. After the inverse <b>consistency</b> <b>errors</b> were reduced, the difference between the accumulated doses diminished. The difference was further reduced after reducing the transitivity errors. The postprocessing technique had minimal effect on the accuracy of the DVF for the lung 4 DCT images. Conclusions: This study shows that inverse <b>consistency</b> and transitivity <b>errors</b> in DIR have a significant dosimetric effect in dose accumulation; Depending on the image pathway taken to accumulate the dose, different results may be obtained. A postprocessing technique that reduces inverse <b>consistency</b> and transitivity <b>error</b> is presented, which allows for consistent dose accumulation regardless of the image pathway followed...|$|R
40|$|In {{the recent}} past the {{adaptive}} finite element method {{has proved to be}} successfully applicable for the efficient numerical solution {{of a large number of}} problems. The main focus of this thesis lies in the development of an adaptive finite element scheme based on a standard residual-type a posteriori error estimate for the numerical solution of distributed optimal control problems for second order elliptic variational inequalities of obstacle type. As one of the main results, a convergence result is proven for a sequence of discrete C-stationary points. Furthermore, the residual-type a posteriori error estimator is shown to be reliable and efficient. Particular emphasis is put on the approximation of the reliability and efficiency related <b>consistency</b> <b>errors.</b> A detailed documentation of numerical results for a selection of test examples illustrates the performance of the adaptive approach...|$|R
40|$|Five image {{segmentation}} algorithms are evaluated: mean shift, normalised cuts, efficient graph-based segmentation, hierarchical watershed, and waterfall. The evaluation is done using three evaluation metrics: probabilistic Rand index, global <b>consistency</b> <b>error,</b> and boundary precision-recall. We examine region-based metrics {{as a function}} of the number of regions produced by an algorithm. This allows new insights into algorithms and evaluation metrics to be gained. 1...|$|E
40|$|Abstract. For a next-nearest {{neighbour}} pair {{interaction model}} in a periodic domain, a priori and a posteriori {{analyses of the}} quasinonlocal quasicontin-uum method (QNL-QC) are presented. The results are valid for large deforma-tions and essentially guarantee a one-to-one correspondence between atomistic solutions and QNL-QC solutions. The analysis is based on <b>consistency</b> <b>error</b> estimates in negative norms, novel a priori and a posteriori stability estimates, and a quantitative inverse function theorem...|$|E
40|$|A major {{challenge}} in single particle reconstruction from cryo-electron microscopy {{is to establish}} a reliable ab-initio three-dimensional model using two-dimensional projection images with unknown orientations. Common-lines based methods estimate the orientations without additional geometric information. However, such methods fail when the detection rate of common-lines is too low due to the high level of noise in the images. An approximation to the least squares global self <b>consistency</b> <b>error</b> was obtained using convex relaxation by semidefinite programming. In this paper we introduce a more robust global self <b>consistency</b> <b>error</b> and show that the corresponding optimization problem can be solved via semidefinite relaxation. In order to prevent artificial clustering of the estimated viewing directions, we further introduce a spectral norm term that is added as a constraint or as a regularization term to the relaxed minimization problem. The resulted problems are solved by using either the alternating direction method of multipliers or an iteratively reweighted least squares procedure. Numerical experiments with both simulated and real images demonstrate that the proposed methods significantly reduce the orientation estimation error when the detection rate of common-lines is low...|$|E
3000|$|... are unknown, yet {{we need to}} {{differentiate}} between the two cases of CRLB {{based on the assumption}} that either the target lies on-grid or off-grid. For CRLB, the error has to be consistent. In order to keep the <b>consistency</b> of <b>error</b> for CRLB, we will use the CRLB for known θ [...]...|$|R
40|$|International audienceSafety {{is now a}} {{major concern}} in many complex systems such as medical robots. A way to control the {{complexity}} of such systems is to manage risk. The first and important step of this activity is risk analysis. During risk analysis, two main studies concerning human factors must be integrated: task analysis and human error analysis. This multidisciplinary analysis often leads to a work sharing between several stakeholders who use their own languages and techniques. This often produces <b>consistency</b> <b>errors</b> and understanding difficulties between them. Hence, this paper proposes to treat the risk analysis on the common expression language UML (Unified Modeling Language) and to handle human factors concepts for task analysis and human error analysis based on the features of this language. The approach {{is applied to the}} development of a medical robot for teleechography...|$|R
40|$|In {{this paper}} we {{consider}} {{the problem of the}} nonparametric regression estimation, when measurement errors are involved in the explanatory variable. Using Pollard empirical process the uniform consistency with sharp rates is established for the nonparametric estimator. Applications in the Engel curve analysis are discussed. Uniform <b>consistency</b> Measurement <b>errors</b> Regression estimation Simulations Engel curves...|$|R
