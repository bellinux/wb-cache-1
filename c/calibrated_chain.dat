1|20|Public
50|$|Substantial guard rails are {{constructed}} from 316 marine grade stainless steel and are through-bolted for security. Stainless steel self stowing stemhead fitting, housing a 16 kg Delta anchor with 30m of 8mm <b>calibrated</b> <b>chain.</b> Manual anchor winch. 6 substantial cleats. Stainless steel grab rails {{on the side}} of the flybridge. Stainless steel/teak bathing platform access ladder. A full set of IMCO navigation lights. Twin electric horns. Fuel and water filters. Hatch for gas bottle stowage (2 bottles). Hatch access to chain locker with stowage space. Hatch to forecabin. Stainless steel guard wires to access openings in side deck guard rails. Bathing ladder.|$|E
40|$|This paper {{describes}} {{the development of}} a novel and highly integrated, digitally-controlled active SAR system calibrator (DARC). It consists of both an active transponder path for absolute radiometric calibration and a <b>calibrated</b> receiver <b>chain</b> for antenna pattern evaluation of the satellite antenna. A total of 16 active transponder and receiver systems and 17 receiver-only systems will be fabricated for a calibration campaign in 2006...|$|R
40|$|Farm {{management}} models often produce average crop shares over {{a number}} of years, whereas models from the natural sciences often require inputs of sequences of crops grown on a specific field over several years. In interdisciplinary modelling, this difference can be a relevant obstacle. To bridge this gap, an approach is presented that allows disaggregating results from farm management models to the level required by many natural science models. The approach presented includes two methodological innovations: first, minimum cross entropy is used to ensure a unique solution when modelling a linear programming model at the field level, even when objective and constraint coefficients are identical for different fields. Second, the use of a <b>calibrated</b> Markov <b>chain</b> approach allows the creation of land-use sequences that are closer to the linear programming model's results than an unconditional stochastic simulation would be. The <b>calibrated</b> Markov <b>chain</b> makes use of a prior matrix of transition probabilities that can be empirically derived. Both simulations and analytical calculations with case study data show that the variances of the Markov chain approach are systematically lower than those yielded by a simple stochastic simulation approach. The approach introduced in this paper can improve the coupling of farm-level economic models with natural science models at the field level. Maximum entropy Markov chains Farm management Linear programming Crop rotation Land use modelling...|$|R
40|$|This paper {{emphasizes}} on establishment of traceability for the strain measuring {{data acquisition system}} in terms of voltage. If this amplifier's output voltage is not <b>calibrated</b> then traceability <b>chain</b> breaks. To complete the traceability chain, the amplifier's output voltage has been calibrated for corresponding strain. The sensitivity is calculated using calibration results and further used to feed in data acquisition system to display the result in terms of force/strain...|$|R
40|$|Through {{a series}} of {{standard}} tests conducted on homogeneous metallic materials (carbon steel, stainless steel and non ferrous metal alloys) we have obtained the corresponding values of the elastic module. Non-destructive local penetration tests in the elastic field are carried out on materials identical to each of those indicated above. For each of the aforementioned materials the results obtained by the two tests described above are correlated. Lastly, we have <b>calibrated</b> the measuring <b>chain</b> created especially for the micropenetration tests, and then validated the measurement procedure...|$|R
3000|$|At {{the end of}} a listened sequence, VQmon {{extracts}} {{packet loss}} characterization metrics, e.g., interval durations and their corresponding Good/Bad status and features, from a 4 -state <b>chain</b> <b>calibrated</b> at run-time (see 'Appendix' section for further details). These control data are used to calculate the overall rating factor as follows, the built perceptual instantaneous rating function RP over a given Good and the next adjacent Bad segment is integrated over time. Then, the obtained value is divided by the interval duration. The resulting rating factor is referred to as average rating factor, R [...]...|$|R
40|$|This paper {{proposes a}} {{conceptual}} {{model for a}} firm's capability to <b>calibrate</b> supply <b>chain</b> knowledge (CCK). Knowledge calibration is achieved {{when there is a}} match between managers' ex ante confidence in the accuracy of held knowledge and the ex post accuracy of that knowledge. Knowledge calibration is closely related to knowledge utility or willingness to use the available ex ante knowledge: a manager uses the ex ante knowledge if he/she is confident in the accuracy of that knowledge, and does not use it or uses it with reservation, when the confidence is low. Thus, knowledge calibration attained through the firm's CCK enables managers to deal with incomplete and uncertain information and enhances quality of decisions. In the supply chain context, although demand- and supply-related knowledge is available, supply chain inefficiencies, such as the bullwhip effect, remain. These issues may be caused not by a lack of knowledge but by a firm's lack of capability to sense potential disagreement between knowledge accuracy and confidence. Therefore, this paper contributes to the understanding of supply chain knowledge utilization by defining CCK and identifying a set of antecedents and consequences of CCK in the supply chain context...|$|R
40|$|Abstract — With the {{increasing}} miniaturization of robotic devices, some actuators are not provided with absolute position sensing, thus making {{the state of}} the system unknown at startup. In this paper we present a vision based method for the automatic calibration of serial pan-tilt kinematic structures with a perspective camera on the end-effector. Examples of such systems are surveillance cameras and humanoid robot heads. The method is based on prospective motions of one joint in the kinematic chain to induce image motion in the camera. The analysis of the induced homography allows the computation of the angle of the other joint. The method can be iterated on more axes to <b>calibrate</b> longer serial <b>chains</b> composed of rotational joints. The method requires calibrated cameras, but runs completely automatic. We have implemented and validated the method in a small humanoid robot head. I...|$|R
40|$|A 180 ° hybrid and a {{directional}} coupler {{to be employed}} in the P-band cryogenic receiver of the Sardinia Radio Telescope are proposed in this work. An in-depth study of the {{issues related to the}} use of microwave components for cryogenic radio astronomy receivers is carried out to select the best suited technology and configuration. As a result, a planar fractal 180 ° hybrid configuration available in the literature has been optimized aiming to increase the operating bandwidth in order to comply with the design specifications of the application at hand. A coupled line {{directional coupler}} with weak coupling and high isolation, used to <b>calibrate</b> the receiver <b>chain,</b> is cascaded to the 180 ° hybrid and realized in the same layout. The final device, consisting of the 180 ° hybrid and the directional coupler, has been manufactured and tested at the cryogenic temperature of 20 K, showing a good agreement between experimental results and predicted performance...|$|R
40|$|With the {{increasing}} miniaturization of robotic devices, many actuators lack absolute position sensing. In these cases the initial {{position of the}} joints is unknown at startup. In this paper we present a vision based method for the automatic homing of serial kinematic structures composed of rotational joints, and having a perspective camera on the end-effector. Examples of such systems are pan-tilt surveillance cameras and most kinds of humanoid robot heads. The method is based on producing motions with known amplitude {{in one of the}} joints of the kinematic chain to induce image motion in the camera. The analysis of the induced homography allows the computation of the unknown angle of the other joint. The method can be iterated on more axes to <b>calibrate</b> longer serial <b>chains.</b> It requires <b>calibrated</b> cameras, static objects while homing and short links in the kinematics structure (or, equivalently, far away objects). We have implemented and validated the method in a small humanoid robot head. 1...|$|R
40|$|International audienceThis paper {{describes}} novel parametric {{speech quality}} models which subsume {{the effect of}} packet loss distribution and voicing feature of missing signal waves. Speech quality estimate models for voiced and unvoiced loss location patterns are developed following multiple statistical regression analysis of measurements gathered from a built speech quality assessment framework. The overall speech quality is estimated by combining voiced and unvoiced speech quality estimate scores using an expression calibrated using {{a large number of}} speech samples. The input parameters namely, mean loss durations and ratios for voiced and unvoiced packets, of speech quality estimate models are extracted at run-time using a new voicing-aware packet loss Markov model. This <b>chain,</b> <b>calibrated</b> at run-time, finely models bursty packet loss behavior over voiced and unvoiced missing speech waves. Performance evaluation study shows that our voicing-aware speech quality estimate models clearly outperform voicing-agnostic speech quality models in terms of accuracy over a wide range of conditions...|$|R
40|$|An {{accurate}} {{estimation of}} carbon fluxes {{is very important}} in carbon cycle studies. A remote sensing based gross primary production (GPP) and net ecosystem production (NEP) algorithm, RS-CFLUX, was presented in this work. The algorithm was <b>calibrated</b> with Markov <b>Chain</b> Monte Carlo (MCMC) method at Daman superstation and Zhangye wetland station in the midstream of the Heihe River Basin. Results indicated that both of the stations present high GPP (1442. 04 g C/m 2 /year at Daman superstation and 928. 89 g C/m 2 /year at Zhangye wetland station) and NEP (409. 38 g C/m 2 /year at Daman superstation and 422. 60 g C/m 2 /year at Zhangye wetland station). The RS-CFLUX model can correctly simulate the seasonal dynamics and quantities of carbon fluxes at both stations, using photosynthetically active radiation (PAR), land surface temperature (LST), normalized difference water index (NDWI) and enhanced vegetation index (EVI) as input. RS-CFLUX model were sensitive to maximum light use efficiency, respiration at reference temperature, activation energy parameter of respiration...|$|R
40|$|The aim of {{this paper}} is to compare the {{kinematic}} features of motorcycles with those of passenger cars in urban traffic. The hypothesis that motorcycles' capability to swerve in urban traffic contributes to their seemingly assertive behaviour is examined. Data for this study were collected in afternoon peak hours at Central London using video recorders. Detailed information on the trajectories of 2109 vehicles (including 477 motorcycles and 1293 passenger cars) was extracted from the video images and the observable kinematic features were analysed. In addition, a model describing the longitudinal following behaviour of motorcycles was employed to analyse the impacts of motorcycles' swerving behaviour. The model was <b>calibrated</b> using Markov <b>Chain</b> Monte Carlo (MCMC) numerical methods. The observable kinematic features show that in comparison to passenger cars, motorcycles have shorter safety gaps, higher speeds and severer acceleration and deceleration rates reflecting their generally much higher power to weight ratios and usage of available braking power. However, the data also support the hypothesis that motorcyclists maintain a considerable safety margin as they have the ability to avoid a collision by swerving away...|$|R
40|$|Summary. We propose {{an online}} binary {{classification}} procedure for cases {{when there is}} uncertainty about the model to use and parameters within a model change over time. We account for model uncertainty through dynamic model averaging, a dynamic extension of Bayesian model averaging in which posterior model probabilities may also change with time. We apply a state-space model to the parameters of each model and we allow the data-generating model to change over time according to a Markov <b>chain.</b> <b>Calibrating</b> a “forgetting ” factor accommodates different levels {{of change in the}} data-generating mechanism. We propose an algorithm that adjusts the level of forgetting in an online fashion using the posterior predictive distribution, and so accommodates various levels of change at different times. We apply our method to data from children with appendicitis who receive either a traditional (open) appendectomy or a laparoscopic procedure. Factors associated with which children receive a particular type of procedure changed substantially over the 7 years of data collection, a feature that is not captured using standard regression modeling. Because our procedure can be implemented completely online, future data collection for similar studies would require storing sensitive patient information only temporarily, reducing the risk of a breach of confidentiality...|$|R
40|$|A Monte Carlo {{algorithm}} is defined for generating replicas of textile composite specimens that possess the same statistical characteristics as specimens imaged using high resolution computed tomography. The textile reinforcement {{is represented by}} one-dimensional tow loci in three-dimensional space, which are easily incorporated into the Binary Model of textile composites. A tow locus is expressed as the sum of non-stochastic, periodic variations in the coordinates of the tow centroid and stochastic, non-periodic deviations. The non-stochastic variations have period commensurate with {{the dimensions of the}} unit cell of the textile, while the stochastic deviations, which describe geometrical defects, exhibit correlation lengths that may be incommensurate with the unit cell. The model is calibrated with data deduced in prior work from computed tomography images. The calibration obviates the need for assuming any ideal shape functions for the tow loci, which can take very general form. The approach is therefore valid {{for a wide range of}} textile architectures. Once <b>calibrated,</b> a Markov <b>Chain</b> algorithm can generate numerous stochastic replicas of a textile architecture very rapidly. These virtual specimens can be much larger than the real specimens from which the data were originally gathered, a necessary feature when real specimen size is limited by the nature of high resolution computed tomography. The virtual specimen generator is illustrated using data for an angle interlock weave...|$|R
40|$|A common {{approach}} to solving multi-label learning problems {{is to use}} problem transformation methods and dichotomizing classifiers as in the pair-wise decomposition strategy. One {{of the problems with}} this strategy is the need for querying a quadratic number of binary classifiers for making a prediction that can be quite time consuming, especially in learning problems with a large number of labels. To tackle this problem, we propose a Two Stage Architecture (TSA) for efficient multi-label learning. We analyze three implementations of this architecture the Two Stage Voting Method (TSVM), the Two Stage Classifier Chain Method (TSCCM) and the Two Stage Pruned Classifier Chain Method (TSPCCM). Eight different real-world datasets are used to evaluate the performance of the proposed methods. The performance of our approaches is compared with the performance of two algorithm adaptation methods (Multi-Label k-NN and Multi-Label C 4. 5) and five problem transformation methods (Binary Relevance, Classifier <b>Chain,</b> <b>Calibrated</b> Label Ranking with majority voting, the Quick Weighted method for pair-wise multi-label learning and the Label Powerset method). The results suggest that TSCCM and TSPCCM outperform the competing algorithms in terms of predictive accuracy, while TSVM has comparable predictive performance. In terms of testing speed, all three methods show better performance as compared to the pair-wise methods for multi-label learning. Keywords: multi-label learning, multi-label ranking, multi-label classification, two stage architecture, classifier chai...|$|R
40|$|International audienceA {{pyrolysis}} source {{coupled to}} a supersonic expansion {{has been used}} to produce the CH 3 radical from two precursors, iodomethane CH 3 I and nitromethane CH 3 NO 2. The relative ionization yield of CH 3 has been recorded at the SOLEIL Synchrotron Radiation source in the range 9. 0 − 11. 6 eV, and its ionization threshold has been modeled by taking into account the vibrational and rotational temperature of the radical in the molecular beam. The relative photoionization yield has been normalized to an absolute cross section scale at a fixed wavelength (118. 2 nm, σ i CH 3 = 6. 7 − 1. 8 + 2. 4 Mb, 95 % confidence interval) in an independent laboratory experiment using the same pyrolysis source, a vacuum ultraviolet (VUV) laser, and a carefully <b>calibrated</b> detection <b>chain.</b> The resulting absolute cross section curve is in good agreement with the recently published measurements by Taatjes et al.,(1) although with an improved signal-to-noise ratio. The absolute photoionization cross section of CH 3 I at 118. 2 nm has also been measured to be σ i CH 3 I = (48. 2 ± 7. 9) Mb, in good agreement with previous electron impact measurements. Finally, the photoionization yield of the iodine atom in its ground state 2 P 3 / 2 has been recorded using the synchrotron source and calibrated {{for the first time on}} an absolute cross section scale from our fixed 118. 2 nm laser measurement, σ i I 2 P 3 / 2 = 74 − 23 + 33 Mb (95 % confidence interval). The ionization curve of atomic iodine is in good agreement, although with slight variations, with the earlier relative ionization yield measured by Berkowitz et al. (2) and is also compared to an earlier calculation of the iodine cross section by Robicheaux and Greene. (3) It is demonstrated that, in the range of pyrolysis temperature used in this work, all the ionization cross sections are temperature-independent. Systematic care has been taken to include all uncertainty sources contributing to the final confidence intervals for the reported results...|$|R
40|$|State-of-the-art {{models in}} {{preliminary}} wing design apply physics-based methods for primary structures while using empirical correlations for secondary structures. Using those methods, a detailed optimization such as e. g. rear spar positions or flap size is only possible within a limited design space. Novel structural {{concepts such as}} multi-spar flap layouts or the introduction of composite materials cannot be analyzed using statistical methods and require extended higher level structural modeling. Therefore, a flexible wing modeling and physical mass estimation system for early aircraft design stages is developed – the WINGmass system. The core of the interdisciplinary tool chain is a central model generator that automatically generates all analysis models from the DLR aircraft data format CPACS (Common Parametric Aircraft Configuration Scheme). For the automatic model generation, {{a large amount of}} engineering rules are implemented in the model generator, {{to reduce the amount of}} required input parameters and therefore to relieve the aircraft designer. Besides the multi-model generator, the tool chain consist of a structural finite element model (incl. wing primary structures, flaps, flap tracks, ailerons, engine pylon and landing gear), a structural sizing algorithm and loads models for aerodynamic, fuel, landing gear and engine loads. The wing mass estimation system is calibrated against real mass values of the wing primary structures and the trailing edge devices of the Airbus A 320 and A 340 - 200. The results of the <b>calibrated</b> tool <b>chain</b> are compared to the masses of the primary structures of the B 747 - 100 and the aluminum baseline version of the MD- 90 - 40 X. The calibration factors for composite primary structures are derived from the composite version of the MD- 90 - 40 X. Finally, the benefits of the extended physics-based modeling and the application of the WINGmass system in an interdisciplinary aircraft design environment are shown in an aircraft design study. The objective of this study is to compute the optimal wing shape in terms of mission fuel as a function of the take-off field length. Therefore, a parameter variation of the wing and flap geometry is performed, the engine scaled correspondingly and the mission fuel evaluated...|$|R
40|$|AbstractWe aim to {{investigate}} the impact of marketing science articles and tools on the practice of marketing. This impact may be direct (e. g., an academic article may be adapted to solve a practical problem) or indirect (e. g., its contents may be incorporated into practitioners' tools, which then influence marketing decision making). We use the term “marketing science value chain” to describe these diffusion steps, and survey marketing managers, marketing science intermediaries (practicing marketing analysts), and marketing academics to <b>calibrate</b> the value <b>chain.</b> In our sample, we find that (1) the impact of marketing science {{is perceived to be}} largest on decisions such as the management of brands, pricing, new products, product portfolios, and customer/market selection, and (2) tools such as segmentation, survey-based choice models, marketing mix models, and pre-test market models have the largest impact on marketing decisions. Exemplary papers from 1982 to 2003 that achieved dual – academic and practice – impact are Guadagni and Little (1983) and Green and Srinivasan (1990). Overall, our results are encouraging. First, we find that the impact of marketing science has been largest on marketing decision areas that are important to practice. Second, we find moderate alignment between academic impact and practice impact. Third, we identify antecedents of practice impact among dual impact marketing science papers. Fourth, we discover more recent trends and initiatives in the period 2004 – 2012, such as the increased importance of big data and the rise of digital and mobile communication, using the marketing science value chain as an organizing framework...|$|R
40|$|The Andean Amazon is an {{endangered}} biodiversity hot spot but its forest dynamics are less studied {{than those of}} the Amazon lowland and forests from middle or high latitudes. This is because its landscape variability, complex topography and cloudy conditions constitute a challenging environment for any remote-sensing assessment. Breakpoint detection with Landsat time-series data is an established robust approach for monitoring forest dynamics around the globe but has not been properly evaluated for implementation in the Andean Amazon. We analyzed breakpoint detection-generated forest dynamics in order to determine its limitations when applied to three different study areas located along an altitude gradient in the Andean Amazon in Ecuador. Using all available Landsat imagery for the period 1997 – 2016, we evaluated different pre-processing approaches, noise reduction techniques, and breakpoint detection algorithms. These procedures were integrated into a complex function called the processing chain generator. Calibration was not straightforward since it required us to define values for 24 parameters. To solve this problem, we implemented a novel approach using genetic algorithms. We <b>calibrated</b> the processing <b>chain</b> generator by applying a stratified training sampling and a reference dataset based on high resolution imagery. After the best calibration solution was found and the processing chain generator executed, we assessed accuracy and found that data gaps, inaccurate co-registration, radiometric variability in sensor calibration, unmasked cloud, and shadows can drastically affect the results, compromising the application of breakpoint detection in mountainous areas of the Andean Amazon. Moreover, since breakpoint detection analysis of landscape variability in the Andean Amazon requires a unique calibration of algorithms, the time required to optimize analysis could complicate its proper implementation and undermine its application for large-scale projects. In exceptional cases when data quality and quantity were adequate, we recommend the pre-processing approaches, noise reduction algorithms and breakpoint detection algorithms procedures that can enhance results. Finally, we include recommendations for achieving a faster and more accurate calibration of complex functions applied to remote sensing using genetic algorithms...|$|R
40|$|Breast {{cancer is}} an {{important}} disease, accounting worldwide for 25. 2 % of all female cancers and for 16 % of cancer deaths in adult women. Early detection of suspicious lesions via breast screening increases survival chances. Mammography is the corner- stone of population-based breast cancer screening. The quality of screening tools is strictly controlled andcontinuously improved. A possible improvement of the current screening programs is an even more speciﬁc identiﬁcation of the population at risk. Today, most screening programs invite women only based on and sometimes on patient or family history. Another improvement could be a better estimation of the associated radiation risk. For both improvements, breast density, a quantitative evaluation of ﬁbroglandular tissue, diﬀering in amount and distribution within the population, is a key parameter. Breast density has been identiﬁedas {{a risk factor for}} breast cancer by many studies. In addition an abundant dense tissue tends to mask suspicious lesions and reduces their detection. In this thesis we developed a volumetric breast density computation method in mammographic images, based on <b>calibrating</b> the image <b>chain</b> with breast-equivalent phantoms, and the acquisition data stored in the image-header. We applied and published a new validation method for breast density computation methods based on regular thorax CT images. In 1979 Hammerstein et al. stated that mammary gland is tissue at high risk for radiation damage, as opposed to skin, fat and connective tissue, which are not at high risk. They concluded with the proposition of total energy absorbed in glandular tissue as the most relevant indicator of riskin mammography. The average glandular dose (AGD), the currently accepted measure of mammographic dose, is not a function of the amount of tissue at risk, and as a consequence is not a good indicator for the radiation risk from a speciﬁc mammographic examination. In the individualized evaluation of the radiation related risk in mammography, the diﬀerent radiation sensitivities of glandular and adipose tissues should be taken into account. This also requires the knowledge of the amount and localization of each of these components of the breast at an individual level. Thanks to methods as the volumetric breast density computation it is possible to assess to the amount of glandular tissue. Thanks to recent developments in breast imaging such as breast tomosynthesis it ispossible to partly overcome the problem of the glandular tissue localization. Hence, breast tomosynthesis reconstructs a 3 D volume of the breast from a limited number of projections over a small angular range. In this thesis we apply a method to estimate the tissue-diﬀerentiated absorbed energy for a Senographe Essential conﬁguration with the SenoClaire Digital Tomosynthesis attachment (GE Healthcare, Chalfont,UK). From the 0 ◦ projection image we computed the volumetric breast density. Contrary to CT scanning that produces Hounsﬁeld units, the pixel values in reconstructed DBT volumes have no physical meaning andtherefore segmentation of the glandular versus adipose tissue cannot bereadily performed using simple thresholding techniques. We have proposed and worked out a procedure in which the volumetric breast density computation is used to label the local tissue, based on the conservation of glandular tissue percentage between the projection and the 3 D volume. Wethen computed the locally imparted energy by Monte Carlo simulations applied to the reconstructed volume. Combining the labeled volume and the local imparted energy leads to the total imparted energy to the glandular and adipose tissues separately for the given breast tomosynthesis exam. For daily use more research is necessary to facilitate the calibrationof the system, the computation of locally imparted energy avoiding Monte Carlo simulations and an improvement of the reconstructed volume with for goal glandular segmentation. List of abbreviations vii Abstract ix Résumé xi Présentation du projet xi Densité volumique du sein xii Etat de l’art xii Nos contributions xv Energie déposée dans la glande xviii Etat de l’art xviii Nos contributions xix Conclusion xxii Samenvatting xxv Probleemstelling xxv Volumetrische borstdensiteit xxvi State of the art xxvi Onze bijdragen xxix Energie geabsorbeerd door het klierweefsel xxxii State of the art xxxii Onze bijdragen xxxiii Conclusie xxxvii 1 Introduction 5 1. 1 General problem statement 5 1. 2 State of the art 6 1. 2. 1 Breast anatomy and histology 6 1. 2. 2 Breast density 7 1. 2. 3 Breast dose 12 1. 2. 4 Conclusion 16 1. 3 Research objectives 17 I BREAST DENSITY 19 2 Validation of breast equivalent phantom material 23 2. 1 Breast equivalent material for VBD computation 23 2. 2 CT measurements of the breast equivalent phantom 24 2. 2. 1 CIRS 24 2. 2. 2 Conﬁgurations 25 2. 2. 3 Measuring the CIRS characteristics 26 2. 3 Breast equivalent phantom characteristics 27 2. 4 Breast equivalent phantom in mammography 30 2. 5 Conclusion 32 3 Breast density computation 33 3. 1 Volumetric breast density: introduction 33 3. 2 VBD: from theory to practice 33 3. 2. 1 Theoretical model for the computation of V BDMX from digital mammographic images 33 3. 2. 2 Implementation of the V BD computation 36 3. 2. 3 Application to mammographic images 40 3. 3 Calibration of the VBD computation 43 3. 3. 1 VBD for phantoms 43 3. 3. 2 VBDMX for the database of mammographic images 49 3. 4 Conclusion: VBD computation is possible 52 4 New validation of the breast density computation 57 4. 1 CT versus mammography 57 4. 2 VBD computation in CT 57 4. 2. 1 Theoretical derivation 57 4. 2. 2 Database of mammographic and CT images 59 4. 3 VBDCT versus VBDMX 59 4. 3. 1 Calibration of the CT method 59 4. 3. 2 VBDCT for the databases 60 4. 3. 3 Correlation between V BDCT and V BDMX 61 4. 4 Same breast, same VBD 63 Discussion Part I 65 II BREAST DOSIMETRY 67 5 Validation of the Monte-Carlo simulation tool CatDose 71 5. 1 Monte-Carlo simulations 71 5. 2 TG 195 AAPM manual 72 5. 3 Computation of dose conversion factors 73 5. 3. 1 Dance conversion factor 73 5. 3. 2 Boone conversion factors 75 5. 4 Limits and strengths 76 6 Evaluation of irradiation in mammography 79 6. 1 Dosimetry for individuals 79 6. 2 Use the right density 82 6. 3 New quantity used for individualized risk 83 6. 4 Computation of the local GIE 86 6. 5 Discussion and conclusion on individual risk assessment 89 7 Segmentation of glandular tissue from tomosynthesis 93 7. 1 Brief introduction to tomosynthesis 93 7. 2 Tomosynthesis limitations 94 7. 3 Segmentation method 96 7. 4 Reconstructions 99 7. 5 Textured phantom 105 7. 6 Real patient cases 107 7. 7 Conclusion on tomosynthesis segmentation 109 Discussion Part II 111 8 General conclusions and perspectives 113 APPENDIX 114 A Breast statistics 115 B Level estimators 119 List of publications 121 Bibliography 122 nrpages: 180 status: publishe...|$|R

