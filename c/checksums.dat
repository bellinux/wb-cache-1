425|997|Public
25|$|Hierarchical checksumming of {{all data}} and metadata, {{ensuring}} that the entire storage system can be verified on use, and confirmed to be correctly stored, or remedied if corrupt. <b>Checksums</b> are stored with a block's parent block, rather than with the block itself. This contrasts with many file systems where <b>checksums</b> (if held) are stored with the data so that if the data is lost or corrupt, the checksum {{is also likely to}} be lost or incorrect.|$|E
25|$|Automated and (usually) silent {{self-healing}} of data inconsistencies {{and write}} failure when detected, for all errors where {{the data is}} capable of reconstruction. Data can be reconstructed using all of the following: error detection and correction <b>checksums</b> stored in each block's parent block; multiple copies of data (including <b>checksums)</b> held on the disk; write intentions logged on the SLOG (ZIL) for writes that should have occurred but did not occur (after a power failure); parity data from RAID/RAIDZ disks and volumes; copies of data from mirrored disks and volumes.|$|E
25|$|Entropy {{effectively}} bounds {{the performance}} of the strongest lossless compression possible, which can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. See also Kolmogorov complexity. In practice, compression algorithms deliberately include some judicious redundancy in the form of <b>checksums</b> to protect against errors.|$|E
50|$|Alternately, {{you can use}} {{the same}} <b>checksum</b> {{creation}} algorithm, ignoring the <b>checksum</b> already in place as if it had not yet been calculated. Then calculate the <b>checksum</b> and compare this calculated <b>checksum</b> to the original <b>checksum</b> included with the credit card number. If the included <b>checksum</b> matches the calculated <b>checksum,</b> then the number is valid.|$|R
50|$|<b>Checksum</b> Coverage (CsCov) (4 bits): <b>Checksum</b> Coverage {{determines the}} parts of the packet that are covered by the <b>Checksum</b> field.|$|R
50|$|The rolling <b>checksum</b> used in rsync {{is based}} on Mark Adler's adler-32 <b>checksum,</b> which is used in zlib, and is itself based on Fletcher's <b>checksum.</b>|$|R
25|$|The Ethernet {{physical}} layer is the {{physical layer}} {{component of the}} Ethernet family of computer network standards which defines the electrical or optical properties of the physical connection between a device and the network or between network devices. It is complemented by the MAC layer and the logical link layer which provide features like framing, addressing, <b>checksums,</b> virtual LANs, and link aggregation.|$|E
25|$|If other {{copies of}} the damaged data exist or can be reconstructed from <b>checksums</b> and parity data, ZFS will use {{a copy of the}} data (or {{recreate}} it via a RAID recovery mechanism), and recalculate the checksum—ideally resulting in the reproduction of the originally expected value. If the data passes this integrity check, the system can then update all faulty copies with known-good data and redundancy will be restored.|$|E
25|$|Scrub / {{scrubbing}} – ZFS can periodically or {{on demand}} check all data and all copies of that data, {{held in the}} entire of any pool, dataset or volume (including nested datasets and volumes they contain), to confirm that all copies match the expected integrity <b>checksums,</b> and correct them if not. This is an intensive process and can run in the background, adjusting its activity to match how busy the system is.|$|E
50|$|When TCP {{runs over}} IPv4, {{the method used}} to compute the <b>checksum</b> is defined in RFC 793:The <b>checksum</b> field is the 16 bit one's {{complement}} of the one's complement sum of all 16-bit words in the header and text. If a segment contains an odd number of header and text octets to be <b>checksummed,</b> the last octet is padded on the right with zeros to form a 16-bit word for <b>checksum</b> purposes. The pad is not transmitted {{as part of the}} segment. While computing the <b>checksum,</b> the <b>checksum</b> field itself is replaced with zeros.|$|R
5000|$|<b>Checksum</b> (16 bits): The Internet <b>checksum</b> of the packet's DCCP header (including options), a network-layer pseudoheader, and, {{depending}} on <b>Checksum</b> Coverage, all, some, or {{none of the}} application data ...|$|R
50|$|To assure {{correctness}} a <b>checksum</b> {{field is}} included; see <b>checksum</b> computation section {{for details on}} <b>checksumming.</b> The TCP <b>checksum</b> is a weak check by modern standards. Data Link Layers with high bit error rates may require additional link error correction/detection capabilities. The weak <b>checksum</b> is partially compensated for by the common use of a CRC or better integrity check at layer 2, below both TCP and IP, such as is used in PPP or the Ethernet frame. However, {{this does not mean}} that the 16-bit TCP <b>checksum</b> is redundant: remarkably, introduction of errors in packets between CRC-protected hops is common, but the end-to-end 16-bit TCP <b>checksum</b> catches most of these simple errors. This is the end-to-end principle at work.|$|R
25|$|Modular {{arithmetic}} {{is often}} used to calculate <b>checksums</b> that are used within identifiers. International Standard Book Number (ISBN) uses modulo 11 (if issued before 1 January, 2007) or modulo 10 (if issued on or after 1 January, 2007) arithmetic for error detection. Likewise, International Bank Account Numbers (IBANs), for example, make use of modulo 97 arithmetic to trap user input errors in bank account numbers. In chemistry, the last digit of the CAS registry number (a number which is unique for each chemical compound) is a check digit, which is calculated by taking the last digit of the first two parts of the CAS registry number times 1, the previous digit times 2, the previous digit times 3 etc., adding all these up and computing the sum modulo 10.|$|E
25|$|For ZFS, data {{integrity}} {{is achieved by}} using a Fletcher-based checksum or a SHA-256 hash throughout the file system tree. Each block of data is checksummed and the checksum value is then saved in the pointer to that block—rather than at the actual block itself. Next, the block pointer is checksummed, with the value being saved at its pointer. This checksumming continues {{all the way up}} the file system's data hierarchy to the root node, which is also checksummed, thus creating a Merkle tree. In-flight data corruption or phantom reads/writes (the data written/read <b>checksums</b> correctly but is actually wrong) are undetectable by most filesystems as they store the checksum with the data. ZFS stores the checksum of each block in its parent block pointer so the entire pool self-validates.|$|E
25|$|Use of {{hardware}} RAID cards, {{perhaps in the}} mistaken belief that these will 'help' ZFS. While routine for other filing systems, ZFS handles RAID natively, and is designed {{to work with a}} raw and unmodified low level view of storage devices, so it can fully use its functionality. A separate RAID card may leave ZFS less efficient and reliable. For example ZFS <b>checksums</b> all data, but most RAID cards will not do this as effectively, or for cached data. Separate cards can also mislead ZFS about the state of data, for example after a crash, or by mis-signalling exactly when data has safely been written, and in some cases this can lead to issues and data loss. Separate cards can also slow down the system, sometimes greatly, by adding latency to every data read/write operation, or by undertaking full rebuilds of damaged arrays where ZFS would have only needed to do minor repairs of a few seconds.|$|E
5000|$|The <b>checksum</b> is {{calculated}} by forming the ones' complement of the ones' complement {{sum of the}} header's 16-bit words. The result of summing the entire IP header, including <b>checksum,</b> should be zero {{if there is no}} corruption. At each hop, the <b>checksum</b> is recalculated and the packet will be discarded upon <b>checksum</b> mismatch. The router must adjust the <b>checksum</b> if it changes part of the IP header (such as when decrementing the TTL.) ...|$|R
5000|$|To {{validate}} a header's <b>checksum</b> {{the same}} algorithm {{may be used}} - the <b>checksum</b> of a header which contains a correct <b>checksum</b> field is a word containing all zeros (value 0): ...|$|R
50|$|The actual {{procedure}} which {{yields the}} <b>checksum</b> from a data input {{is called a}} <b>checksum</b> function or <b>checksum</b> algorithm. Depending on its design goals, a good <b>checksum</b> algorithm will usually output a significantly different value, even for small changes made to the input. This {{is especially true of}} cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed <b>checksum</b> for the current data input matches the stored value of a previously computed <b>checksum,</b> there is a very high probability the data has not been accidentally altered or corrupted.|$|R
500|$|Built-in resilience: ReFS employs an allocation-on-write update {{strategy}} for metadata, [...] which allocates new chunks for every update transaction and uses large IO batches. All ReFS metadata has built-in 64-bit <b>checksums</b> which are stored independently. The file data {{can have an}} optional checksum in a separate [...] "integrity stream", {{in which case the}} file update strategy also implements allocation-on-write; this is controlled by a new [...] "integrity" [...] attribute applicable to both files and directories. If nevertheless file data or metadata becomes corrupt, the file can be deleted without taking the whole volume offline. As a result of built-in resiliency, administrators do not need to periodically run error-checking tools such as CHKDSK when using ReFS.|$|E
500|$|Compatibility with {{existing}} APIs and technologies: ReFS {{does not require}} new system APIs and most file system filters {{continue to work with}} ReFS volumes. [...] ReFS supports many existing Windows and NTFS features such as BitLocker encryption, Access Control Lists, USN Journal, change notifications, symbolic links, junction points, mount points, reparse points, volume snapshots, file IDs, and oplock. ReFS seamlessly integrates with Storage Spaces, a storage virtualization layer that allows data mirroring and striping, as well as sharing storage pools between machines. ReFS resiliency features enhance the mirroring feature provided by Storage Spaces and can detect whether any mirrored copies of files become corrupt using background data scrubbing process, which periodically reads all mirror copies and verifies their <b>checksums</b> then replaces bad copies with good ones.|$|E
2500|$|... fsck cannot always {{validate}} {{and repair}} data when <b>checksums</b> are stored with data (often {{the case in}} many file systems), because the <b>checksums</b> may also be corrupted or unreadable. ZFS always stores <b>checksums</b> separately from the data they verify, improving reliability {{and the ability of}} scrub to repair the volume. ZFS also stores multiple copies of data – metadata in particular may have upwards of 4 or 6 copies (multiple copies per disk and multiple disk mirrors per volume), greatly improving the ability of scrub to detect and repair extensive damage to the volume, compared to fsck.|$|E
5000|$|... <b>checksum</b> length: {{length in}} bytes of the {{cryptographic}} <b>checksum</b> {{of the message}} ...|$|R
50|$|The <b>checksum</b> {{field is}} the 16-bit one's {{complement}} of the one's complement {{sum of all}} 16-bit words in the header. For purposes of computing the <b>checksum,</b> {{the value of the}} <b>checksum</b> field is zero.|$|R
25|$|This field {{provides}} a CRC32 <b>checksum</b> {{of the data}} in the entire page (including the page header, calculated with the <b>checksum</b> field set to 0). This allows verification that the data has not been corrupted since it was created. Pages that fail the <b>checksum</b> should be discarded. The <b>checksum</b> is generated using a polynomial value of 0x04C11DB7.|$|R
2500|$|When a {{block is}} accessed, {{regardless}} of whether it is data or meta-data, its checksum is calculated and compared with the stored checksum value of what it [...] "should" [...] be. If the <b>checksums</b> match, the data are passed up the programming stack to the process that asked for it; if the values do not match, then ZFS can heal the data if the storage pool provides data redundancy (such as with internal mirroring), assuming that the copy of data is undamaged and with matching <b>checksums.</b> It is optionally possible to provide additional in-pool redundancy by specifying [...] (or [...] or more), which means that data will be stored twice (or three times) on the disk, effectively halving (or, for , reducing to one third) the storage capacity of the disk. Additionally some kinds of data used by ZFS to manage the pool are stored multiple times by default for safety, even with the default copies=1 setting.|$|E
2500|$|ZFS uses a copy-on-write {{transactional}} object model. All block pointers {{within the}} filesystem contain a 256-bit checksum or 256-bit hash (currently {{a choice between}} [...] Fletcher-2, Fletcher-4, or SHA-256) of the target block, which is verified when the block is read. Blocks containing active data are never overwritten in place; instead, a new block is allocated, modified data is written to it, then any metadata blocks referencing it are similarly read, reallocated, and written. To reduce the overhead of this process, multiple updates are grouped into transaction groups, and ZIL (intent log) write cache is used when synchronous write semantics are required. The blocks are arranged in a tree, as are their <b>checksums</b> (see Merkle signature scheme).|$|E
2500|$|A 2010 paper {{examining}} {{the ability of}} file systems to detect and prevent data corruption observed that ZFS itself is effective in detecting and correcting data errors on storage devices, but that it assumes data in RAM are [...] "safe", and not prone to error. Thus when ZFS caches pages, or stores copies of metadata, in RAM, or holds data in its [...] "dirty" [...] cache for writing to disk, no test is made whether the <b>checksums</b> still match the data {{at the point of}} use. Much of this risk can be mitigated by use of ECC RAM but the authors considered that error detection related to the page cache and heap would allow ZFS to handle certain classes of error more robustly.|$|E
50|$|The IPv4 header <b>checksum</b> is {{a simple}} <b>checksum</b> used in version 4 of the Internet Protocol (IPv4) to protect the header of IPv4 data packets against data corruption. This <b>checksum</b> is {{calculated}} only for the header bytes (with the <b>checksum</b> bytes set to 0), is 16 bits long and {{is a part of}} the IP packet header.|$|R
5000|$|... checksum: Kerberos keyed <b>checksum</b> {{over the}} entire message {{excluding}} the <b>checksum</b> field itself ...|$|R
50|$|The {{universe}} of possible <b>checksum</b> values {{is now the}} square of the value for the simple <b>checksum.</b> In our example, the two sums, each with 255 possible values, result in 65025 possible values for the combined <b>checksum.</b>|$|R
2500|$|For RAID subsystems, data {{integrity}} and fault-tolerance requirements {{also reduce the}} realized capacity. For example, a RAID1 array has about half the total capacity {{as a result of}} data mirroring, while a RAID5 array with [...] drives loses [...] of capacity (which equals to the capacity of a single drive) due to storing parity information. RAID subsystems are multiple drives that appear to be one drive or more drives to the user, but provide fault tolerance. Most RAID vendors use <b>checksums</b> to improve {{data integrity}} at the block level. Some vendors design systems using HDDs with sectors of 520 bytes to contain 512 bytes of user data and eight checksum bytes, or by using separate 512-byte sectors for the checksum data.|$|E
50|$|CRC-32C <b>checksums</b> are {{computed}} for both data and metadata and stored as checksum items in a checksum tree. There is room of 256 bits for metadata <b>checksums</b> {{and up to}} a full leaf block (roughly 4 KB or more) for data <b>checksums.</b> More checksum algorithm options are planned for the future.|$|E
50|$|The {{recipient}} splits its copy of {{the file}} into chunks and computes two <b>checksums</b> for each chunk: the MD5 hash, and a weaker but easier to compute 'rolling checksum'. It sends these <b>checksums</b> to the sender.|$|E
5000|$|According to {{the manual}} page, [...] uses two {{different}} algorithms for calculating the <b>checksum</b> and blocks, the SYSV <b>checksum</b> algorithm and the BSD <b>checksum</b> (default) algorithm. Switching {{between the two}} algorithms is done via command line options.|$|R
50|$|Stores a <b>checksum</b> {{with each}} {{encrypted}} block, causing corruption or {{modification of the}} encrypted files to be detected by EncFS. The <b>checksum</b> (blockMACBytes) is 8 bytes, and optionally up to 8 additional bytes of random data (blockMACRandBytes) {{can be added to}} each block to prevent two blocks with the same unencrypted data from having the same <b>checksum.</b> This option creates a large amount of CPU overhead, as each block's <b>checksum</b> must be calculated when data is read (to verify integrity) or written (to update the <b>checksum).</b>|$|R
50|$|The first {{weakness}} of the simple <b>checksum</b> {{is that it is}} insensitive to the order of the blocks (bytes) in the data word (message). If the order is changed, the <b>checksum</b> value will be the same and the change will not be detected. The second weakness is that the universe of <b>checksum</b> values is small, being equal to the chosen modulus. In our example, there are only 255 possible <b>checksum</b> values, so {{it is easy to see}} that even random data has about a 0.4% probability of having the same <b>checksum</b> as our message.|$|R
