176|2576|Public
2500|$|A {{major step}} in the module centric {{analysis}} is to cluster genes into network modules using a network proximity measure. Roughly speaking, a pair of genes has a high proximity if it is closely interconnected. By convention, the maximal proximity between two genes is 1 and the minimum proximity is 0. Typically, WGCNA uses the topological overlap measure (TOM) as proximity. which can also be defined for weighted networks. The TOM combines the adjacency of two genes and the connection strengths these two genes share with other [...] "third party" [...] genes. The TOM is a highly robust measure of network interconnectedness (proximity). This proximity is used as input of average linkage hierarchical clustering. Modules are defined as branches of the resulting <b>cluster</b> <b>tree</b> using the dynamic branch cutting approach ...|$|E
50|$|Peer-to-peer (or point-to-point) {{networks}} {{can form}} arbitrary patterns of connections, and their extension is only {{limited by the}} distance between each pair of nodes. They are meant {{to serve as the}} basis for ad hoc networks capable of performing self-management and organization. Since the standard does not define a network layer, routing is not directly supported, but such an additional layer can add support for multihop communications. Further topological restrictions may be added; the standard mentions the <b>cluster</b> <b>tree</b> as a structure which exploits the fact that an RFD may only be associated with one FFD at a time to form a network where RFDs are exclusively leaves of a tree, and most of the nodes are FFDs. The structure can be extended as a generic mesh network whose nodes are <b>cluster</b> <b>tree</b> networks with a local coordinator for each cluster, in addition to the global coordinator.|$|E
50|$|Topology {{selection}} {{plays an}} important role in routing because the network topology decides the transmission path of the data packets to reach the proper destination. Here, all the topologies (Flat / Unstructured, <b>cluster,</b> <b>tree,</b> chain and hybrid topology) are not feasible for reliable data transmission on sensor nodes mobility. Instead of single topology, hybrid topology plays a vital role in data collection, and the performance is good. Hybrid topology management schemes include the Cluster Independent Data Collection Tree (CIDT). and the Velocity Energy-efficient and Link-aware Cluster-Tree (VELCT); both have been proposed for mobile wireless sensor networks (MWSNs).|$|E
40|$|Abstract. Constrained {{clustering}} investigates how {{to incorporate}} domain {{knowledge in the}} clustering process. The domain knowledge {{takes the form of}} constraints that must hold on the set of clusters. We consider instance level constraints, such as must-link and cannot-link. This type of constraints has been successfully used in popular clustering algorithms, such as k-means and hierarchical agglomerative clustering. This paper shows how <b>clustering</b> <b>trees</b> can support instance level constraints. <b>Clustering</b> <b>trees</b> are decision trees that partition the instances into homogeneous <b>clusters.</b> <b>Clustering</b> <b>trees</b> provide a symbolic description for each cluster. To handle non-trivial constraint sets, we extend <b>clustering</b> <b>trees</b> to support disjunctive descriptions. The paper’s main contribution is ClusILC, an efficient algorithm for building such trees. We present experiments comparing ClusILC to COP-k-means. ...|$|R
40|$|AbstractThe {{visualization}} of clustered graphs is a classical algorithmic topic that has several practical applications and is attracting increasing research interest. In this paper {{we deal with}} the {{visualization of}} <b>clustered</b> <b>trees,</b> a problem that is somehow foundational with respect to the one of visualizing a general clustered graph. We show many, in our opinion, surprising results that put in evidence how drawing <b>clustered</b> <b>trees</b> has many sharp differences with respect to drawing “plain” trees. We study a wide class of drawing standards, giving both negative and positive results. Namely, we show that there are <b>clustered</b> <b>trees</b> that do not have any drawing in certain standards and others that require exponential area. On the contrary, for many drawing conventions there are efficient algorithms that allow to draw <b>clustered</b> <b>trees</b> with polynomial asymptotically-optimal area...|$|R
30|$|The {{records from}} the data stream are {{inserted}} into the hierarchical <b>clustering</b> <b>tree</b> sequentially.|$|R
5000|$|A {{major step}} in the module centric {{analysis}} is to cluster genes into network modules using a network proximity measure. Roughly speaking, a pair of genes has a high proximity if it is closely interconnected. By convention, the maximal proximity between two genes is 1 and the minimum proximity is 0. Typically, WGCNA uses the define the topological overlap measure (TOM) as proximity. which can also be defined for weighted networks. The TOM combines the adjacency of two genes and the connection strengths these two genes share with other [...] "third party" [...] genes. The TOM is a highly robust measure of network interconnectedness (proximity). This proximity is used as input of average linkage hierarchical clustering. Modules are defined as branches of the resulting <b>cluster</b> <b>tree</b> using the dynamic branch cutting approach Next the genes inside a given module are summarize with the module eigengene, which {{can be considered as}} the best summary of the standardized module expression data. The module eigengene of a given module is defined as the first principal component of the standardized expression profiles. To find modules that relate to a clinical trait of interest, module eigengenes are correlated with the clinical trait of interest, which gives rise to an eigengene significance measure. One can also construct co-expression networks between module eigengenes (eigengene networks), i.e. networks whose nodes are modules To identify intramodular hub genes inside a given module, one can use two types of connectivity measures. The first, referred to as , is defined based on correlating each gene with the respective module eigengene. The second, referred to as kIN, is defined as a sum of adjacencies with respect to the module genes. In practice, these two measures are equivalent.To test whether a module is preserved in another data set, one can use various network statistics, e.g[...]|$|E
40|$|The goal of {{clustering}} is {{to detect}} the presence of distinct groups in a data set and assign group labels to the observations. Nonparametric clustering {{is based on the}} premise that the observations may be regarded as a sample from some underlying density in feature space and that groups correspond to modes of this density. The goal then is to find the modes and assign each observation to the domain of attraction of a mode. The modal structure of a density is summarized by its cluster tree; modes of the density correspond to leaves of the <b>cluster</b> <b>tree.</b> Estimating the <b>cluster</b> <b>tree</b> is the primary goal of nonparametric cluster analysis. We adopt a plug-in approach to <b>cluster</b> <b>tree</b> estimation: estimate the <b>cluster</b> <b>tree</b> of the feature density by the <b>cluster</b> <b>tree</b> of a density estimate. For some density estimates the <b>cluster</b> <b>tree</b> can be computed exactly, for others we have to be content with an approximation. We present a graph-based method that can approximate the <b>cluster</b> <b>tree</b> of any density estimate. Density estimates tend to have spurious modes caused by sampling variability, leading to spurious branches in the <b>cluster</b> <b>tree.</b> We propose excess mass as a measure for the size of a branch, reflecting the height of the corresponding peak of the density above the surrounding valley floor as well as its spatial extent. Excess mass can be used as a guide for pruning the graph <b>cluster</b> <b>tree.</b> We point out mathematical and algorithmic connections to single linkage clustering and illustrate our approach on several examples...|$|E
40|$|ABSTRACT-Wireless sensor {{networks}} {{consist of}} hundreds {{to thousands of}} low power multi functioning sensor nodes. It operates in an unattended environment with limited computational and sensing capabilities. A wireless sensor network is constructed based on a <b>cluster</b> <b>tree.</b> Isolated node with {{the maximum number of}} neighbor isolated nodes that launch the cluster generation process. Therefore, the total amount of cluster heads is minimizing. In the <b>cluster</b> <b>tree</b> creation algorithm, the <b>cluster</b> <b>tree</b> architecture is projected and it minimizes the entire number of nodes included in a <b>cluster</b> <b>tree</b> hence, the routing cost is reduced. In the <b>cluster</b> <b>tree</b> repair algorithm, while a cluster head or a cluster associate node fails or move out of communication range of router, a new cluster head or cluster associate node is elected to maintain the <b>cluster</b> <b>tree</b> topology. The <b>cluster</b> <b>tree</b> repair algorithm doesn’t consider the non uniform node distribution. In order to elect cluster heads with higher energy, the parameter of cluster head competition in EADC and EADUC is based on the ratio between the average residual energy of neighbor nodes and the residual energy of the node itself. The aim is to improve load balance among cluster heads, energy consumption and improve the network life time significantly among uniform and non uniform node distribution...|$|E
40|$|Abstract- We {{present the}} TreeSOM method {{and a set}} of tools to perform {{unsupervised}} SOM cluster analysis, determine cluster confidence and visualize the result as a tree facilitating comparison with existing hierarchical classifiers. We also introduce a distance measure for <b>cluster</b> <b>trees</b> that allows to select a SOM with the most confident clusters. Key words- self-organizing map, hierarchical <b>clustering,</b> <b>tree,</b> reliability, visualization, tool...|$|R
40|$|A novel {{class of}} {{applications}} of predictive <b>clustering</b> <b>trees</b> is addressed, namely ranking. Predictive <b>clustering</b> <b>trees,</b> as implemented in Clus, allow for predicting multiple target variables. This approach makes sense {{especially if the}} target variables are not independent of each other. This is typically the case in ranking, where the (relative) performance of several approaches on the same task has to be predicted from a given description of the task. We propose to use predictive <b>clustering</b> <b>trees</b> for ranking. As compared to existing ranking approaches which are instance-based, our approach also allows for {{an explanation of the}} predicted rankings. We illustrate our approach on the task of ranking machine learning algorithms, where the (relative) performance of the learning algorithms on a dataset has to be predicted from a given dataset description. status: publishe...|$|R
40|$|Struyf, J., Dzeroski, S. Blockeel, H. and Clare, A. (2005) Hierarchical Multi-classification with Predictive <b>Clustering</b> <b>Trees</b> in Functional Genomics. In {{proceedings}} of the EPIA 2005 CMB WorkshopThis paper investigates how predictive <b>clustering</b> <b>trees</b> {{can be used to}} predict gene function in the genome of the yeast Saccharomyces cerevisiae. We consider the MIPS FunCat classification scheme, in which each gene is annotated with one or more classes selected from a given functional class hierarchy. This setting presents two important challenges to machine learning: (1) each instance is labeled with a set of classes instead of just one class, and (2) the classes are structured in a hierarchy; ideally the learning algorithm should also take this hierarchical information into account. Predictive <b>clustering</b> <b>trees</b> generalize decision trees and can be applied {{to a wide range of}} prediction tasks by plugging in a suitable distance metric. We define an appropriate distance metric for hierarchical multi-classification and present experiments evaluating this approach on a number of data sets that are available for yeast. Non peer reviewe...|$|R
30|$|Figure 7 : <b>cluster</b> <b>tree</b> {{of cases}} with broadly even bifurcations.|$|E
30|$|The authors {{proposed}} a Heterogeneous tracking model (HTM) for object tracking in [3]. They used Variable memory Markov (VMM) {{to predict the}} patterns of moving objects and used these patterns to construct the <b>cluster</b> <b>tree.</b> The drawback is the higher computation complicity of VMM. Moreover, when the prediction patterns are wrong, {{the performance of the}} <b>cluster</b> <b>tree</b> will become worse.|$|E
30|$|After {{creating}} <b>cluster</b> <b>tree</b> {{by definition}} of a special level called “cutoff” we can introduce arbitrary large or small clusters. It {{is so important to}} select the foremost and proper number of plotted clusters; as the number of plotted clusters which entirely establish a <b>cluster</b> <b>tree</b> should reflect the most proper types of rock for carbonated rocks (Intera ECL Petroleum Technologies Ltd 1992).|$|E
40|$|This paper applies {{conformal}} prediction {{techniques to}} compute simultane-ous prediction bands and <b>clustering</b> <b>trees</b> for functional data. These tools {{can be used}} to detect outliers and clusters. Both our prediction bands and <b>clustering</b> <b>trees</b> provide prediction sets for the underlying stochastic process with a guaranteed finite sample behavior, under no distributional assumptions. The prediction sets are also informative in that they cor-respond to the high density region of the underlying process. While or-dinary conformal prediction has high computational cost for functional data, we use the inductive conformal predictor, together with several novel choices of conformity scores, to simplify the computation. Our methods are illustrated on some real data examples. ...|$|R
5000|$|... hinge {{decomposition}} {{enhanced with}} <b>tree</b> <b>clustering</b> generalizes and beats both hinge decomposition and <b>tree</b> <b>clustering</b> ...|$|R
5000|$|Any {{partitioning}} of <b>clusters</b> of a <b>tree</b> [...] can {{be represented}} by a <b>Cluster</b> Partition <b>Tree</b> CPT by replacing each <b>cluster</b> in the <b>tree</b> [...] by an edge. If we use a strategy P for partitioning [...] then the CPT would be CPTP This is done recursively till only one edge remains.|$|R
40|$|AbstractThe {{decision}} tree is a flexible and useful classification tool. But {{on the data}} with high dimensionality, it meets problems. For most of current {{decision tree}} algorithms, when splitting a node of a tree, only the “best” one feature is selected and used. Since more features are ignored, the classification accuracy is not high. To solve the problem, this paper uses a <b>cluster</b> <b>tree</b> for text categorization. Unlike familiar decision trees (e. g. CART, C 4. 5), clustering results are used as the splitting rule and more features are considered. Obviously, the used clustering algorithm is an {{very important to the}} <b>cluster</b> <b>tree.</b> For better performance, a text clustering algorithm is proposed to enhance the <b>cluster</b> <b>tree.</b> Experiments show that the <b>cluster</b> <b>tree</b> solves the high-dimensionality problem and outperforms C 4. 5 and CART on text data. Sometimes, it may do better than LibSVM, which may be the most powerful tool for text categorization...|$|E
40|$|The {{difficulty}} of clustering and {{the variety of}} clustering methods {{suggest the need for}} a theoretical study of clustering. Using the idea of a standard statistical framework, we propose a new framework for clustering. For a well-defined clustering goal we assume that the data to be clustered come from an underlying distribution and we aim to find a high-density <b>cluster</b> <b>tree.</b> We regard this tree as a parameter of interest for the underlying distribution. However, it is not obvious how to determine a connected subset in a discrete distribution whose support is located in a Euclidean space. Building a <b>cluster</b> <b>tree</b> for such a distribution is an open problem and presents interesting conceptual and computational challenges. We solve this problem using graph-based approaches and further parameterize clustering using the high-density <b>cluster</b> <b>tree</b> and its extension. Motivated by the connection between clustering outcomes and graphs, we propose a graph family framework. This framework {{plays an important role in}} our clustering framework. A direct application of the graph family framework is a new cluster-tree distance measure. This distance measure can be written as an inner product or kernel. It makes our clustering framework able to perform statistical assessment of clustering via simulation. Other applications such as a method for integrating partitions into a <b>cluster</b> <b>tree</b> and methods for <b>cluster</b> <b>tree</b> averaging and bagging are also derived from the graph family framework...|$|E
40|$|For a density f on R d, a {{high-density}} {{cluster is}} any connected component of {x: f(x) ≥ λ}, for some λ> 0. The set of all high-density clusters form a hierarchy called the <b>cluster</b> <b>tree</b> of f. We present a procedure for estimating the <b>cluster</b> <b>tree</b> given samples fromf. We give finite-sample convergence rates for our algorithm, {{as well as}} lower bounds on the sample complexity of this estimation problem. ...|$|E
40|$|The minimum {{spanning}} <b>tree</b> <b>clustering</b> {{algorithm is}} capable of detecting clusters with irregular boundaries. In this paper we propose minimum spanning <b>tree</b> based <b>clustering</b> algorithm. The algorithm produces k clusters with Minimum Spanning <b>Clustering</b> <b>Tree</b> (MSCT), a new data structure {{which can be used}} as search tree. Our algorithm works in two phases. The first phase produces subtree (cluster). The second phase converts the subtree into binary tree called MSCT...|$|R
40|$|Abstract This paper applies {{conformal}} prediction {{techniques to}} compute simultaneous prediction bands and <b>clustering</b> <b>trees</b> for functional data. These tools {{can be used}} to detect outliers and clusters. Both our prediction bands and <b>clustering</b> <b>trees</b> provide prediction sets for the underlying stochastic process with a guaranteed finite sample behavior, under no distributional assumptions. The prediction sets are also informative in that they correspond to the high density region of the underlying process. While ordinary conformal prediction has high computational cost for functional data, we use the inductive conformal predictor, together with several novel choices of conformity scores, to simplify the computation. Our methods are illustrated on some real data examples. Keywords prediction sets · conformal prediction · functional data · simultaneous bands · Gaussian mixture...|$|R
40|$|Evolutionary {{relationships}} between organisms are represented as phylogenetic trees inferred from multiple sequence alignments (MSAs). The proposed approach <b>clusters</b> <b>trees</b> created with many MSA parameterizations. It displays the chosen parameters' {{impact on the}} phylogenetic trees. This view offers interactive parameter exploration and automatic identication of relevant parameters...|$|R
40|$|A <b>cluster</b> <b>tree</b> {{provides}} a highly-interpretable summary of a density function by representing {{the hierarchy of}} its high-density clusters. It is estimated using the empirical tree, which is the <b>cluster</b> <b>tree</b> constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical <b>cluster</b> <b>tree.</b> We first study a variety of metrics {{that can be used}} to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true <b>cluster</b> <b>tree.</b> We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set. Comment: 20 pages, 6 figures, accepted in Neural Information Processing Systems (NIPS) 201...|$|E
40|$|As {{a typical}} {{resource}} constrained system,Wireless Sensor Network (WSN) can be significantly improved {{the performance of}} guarantee quality of service (QoS) by effective resource allocation,including channel and time slot. But the structure is not fixed in smart medical system,which {{can lead to the}} uncertainty service quality. A non-balanced <b>cluster</b> <b>tree</b> structure is present for overcoming the collision and uncertain delay,taking into account the allocation of traffic. The resource parameter is implemented in TinyOS system,which be verified by CC 2530 platform. And Graphical User Interface is designed for modifying data flow parameters and inspecting scheduling results. The experimental results show that this <b>cluster</b> <b>tree</b> scheduling algorithm can ensure the non-balanced structure of network quality,and provide the effective service guarantee for large-scale <b>cluster</b> <b>tree</b> network...|$|E
40|$|For a density f on R^d, a {{high-density}} {{cluster is}} any connected component of {x: f(x) ≥λ}, for some λ > 0. The set of all high-density clusters forms a hierarchy called the <b>cluster</b> <b>tree</b> of f. We present two procedures for estimating the <b>cluster</b> <b>tree</b> given samples from f. The {{first is a}} robust variant of the single linkage algorithm for hierarchical clustering. The second {{is based on the}} k-nearest neighbor graph of the samples. We give finite-sample convergence rates for these algorithms which also imply consistency, and we derive lower bounds on the sample complexity of <b>cluster</b> <b>tree</b> estimation. Finally, we study a tree pruning procedure that guarantees, under milder conditions than usual, to remove clusters that are spurious while recovering those that are salient...|$|E
40|$|Abstract—A {{new data}} {{structure}} called “clustering tree ” {{is presented in}} this paper. With this new data structure, a new clustering algorithm producing a set of clusters with guaranteed similarity is described. We also show that the same <b>clustering</b> <b>tree</b> can be used efficiently as a d-dimensional search tree...|$|R
30|$|The {{authors of}} [15] divide the network {{into two groups}} of roughly equal size using {{modularity}} and hierarchical <b>clustering</b> <b>tree.</b> They show that this split corresponds almost perfectly with the actual division of the club members following the break-up. Only one node, node 3, is classified incorrectly by the method of [15].|$|R
40|$|Description Decompose given {{hierarchical}} <b>clustering</b> <b>tree</b> into non-overlapping <b>clusters</b> in a semi-supervised way {{by using}} available patients follow-up information as guidance. Contains functions for snipping HC <b>tree,</b> various <b>cluster</b> quality evaluation criteria, assigning new patients {{to one of}} the two given HC trees, testing the significance of clusters with permutation argument and clusters visualization using sample's molecular entropy...|$|R
40|$|CrocoCosmos {{analyzes}} and visualizes abstractions of graphs, because large irregular graphs {{are generally}} incomprehensible without abstraction. It enables {{the user to}} interactively control abstraction by filtering vertices and edges, and by aggregating vertices and edges along a hierarchical clustering. The main application of CrocoCosmos is the comprehension, evaluation, and reduction of dependencies in large software systems [1, 3, 4]. However, it is not restricted to this domain, and has also provided new insights e. g. into hyperlink structures, bibliographical networks, and social networks [2, 3]. Graph Model. The input of CrocoCosmos is a hierarchically clustered, attributed graph. A hierarchically clustered graph consists of a (plain) graph called underlying graph, and a tree called <b>cluster</b> <b>tree,</b> such that {{the leaves of the}} <b>cluster</b> <b>tree</b> are exactly the vertices of the underlying graph. For example, the underlying graph may model the classes of an object-oriented program and their inheritance relationships, and the <b>cluster</b> <b>tree</b> may correspond to the containment hierarchy of packages and classes of the program. Numerical attributes store additiona...|$|E
40|$|Thesis (Master's) [...] University of Washington, 2012 We {{propose a}} method for {{estimating}} the number of groups in a data set. Our method {{is an extension of}} Generalized Single Linkage clustering (GSL) (Stuetzle and Nugent 2010), a nonparametric clustering method {{based on the premise that}} groups in the data correspond to modes of the underlying data density. GSL starts with a nonparametric density estimate. It recursively splits the data into high density regions separated by valleys. The leaves of the resulting <b>cluster</b> <b>tree</b> correspond to modes of the density estimate. The problem is that nonparametric density estimates tend to have spurious modes due to sampling variability, giving rise to spurious splits in the <b>cluster</b> <b>tree.</b> We propose a resampling method aimed at assessing the significance of splits and a way of constructing a <b>cluster</b> <b>tree</b> making only significant splits. The only parameter is the significance level. Our method can identify highly non-linear groups. Simulation experiments suggest that the method is very conservative, which may explain its low power...|$|E
40|$|Abstract. Wireless sensor {{networks}} {{with multiple}} users collecting data {{directly from the}} sensors have many potential applications. An important problem is to allocate for each user a query range to achieve certain global optimality while avoid congesting the sensors in the meanwhile. We study this problem for a ZigBee <b>cluster</b> <b>tree</b> by formulating it into a multi-dimensional multi-choice knapsack problem. Maximum overall query range and max-min fair query range objectives are investigated. Distributed algorithms are proposed which exploit the ZigBee <b>cluster</b> <b>tree</b> structure to keep the computation local. Extensive simulations show that the proposed methods achieve good approximation to the optimal solution with little overhead and improve the network performance. ...|$|E
40|$|Map {{matching}} is {{an important}} algorithm for any location-based service, especially in navigation and tracking systems and services. Identifying the relevant road segments accurately and efficiently, given positioning data, is {{the first and most}} important step in any map matching algorithm. This paper proposes a new approach to searching for road candidates by clustering and then searching road segments through a constructed hierarchical <b>clustering</b> <b>tree,</b> rather than using indexing techniques to query segments within a fixed search window. A binary tree is created based on the hierarchical <b>clustering</b> <b>tree</b> and adaptive searches are conducted to identify candidate road segments given GPS positions. The approach was validated using road maps with different scales and various scenarios in which moving vehicles were located. Both theoretical analysis and experimental results confirm that the proposed approach can efficiently find candidate road segments for map matching. © 2013 The Royal Institute of Navigation...|$|R
40|$|Approximated {{algorithms}} for clustering large-scale document collection {{are proposed}} and evaluated under {{the context of}} cluster-based document retrieval (i. e., associative document search). These algorithms use a precise clustering algorithm as a subroutine to construct a stratified structure of <b>cluster</b> <b>trees.</b> An experiment showed that more than 100 times speedup in cpu time was gained at best. Through experiments of self retrieval and topic assignment, we confirmed sufficient search performance on <b>cluster</b> <b>trees</b> that are constructed by approximated algorithms. In particular, top down construction offered over 99 % accuracy of self retrieval which is comparable performance to exhaustive search. Top down construction also offered promising performance in topic assignment, that is, better recall/precision than that obtained by exhaustive search. All of the results for cluster-based retrieval were obtained by simple and efficient binary tree search. 1 Introduction As the size of document re [...] ...|$|R
40|$|In {{this paper}} we {{consider}} problems {{from the industry}} that have characteristic of clustering. We show how to model these problems as <b>clustering</b> <b>trees</b> problems and use our solutions in order to solve the problems. The problems we consider are problems of manufacturing, robotics {{and all of them}} can be applied also for communication networks. For the application from manufacturing we want to construct a minimum <b>clustering</b> <b>tree</b> where each <b>cluster</b> induces a complete star in the solution tree. For this case we have a structure theorem and based on it a polynomial algorithm. We prove that our algorithm provides the same construction for the setting up of a system and for the efficient running of a system. The application from robotics is where we want to find a minimum clustering TSP-path. We prove that the general case is NP-complete and we have polynomial algorithms for some restricted cases. 1...|$|R
