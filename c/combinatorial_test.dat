57|320|Public
25|$|The {{second step}} of test design then follows the {{principles}} of <b>combinatorial</b> <b>test</b> design.|$|E
25|$|One way of {{modelling}} constraints {{is using}} the refinement mechanism in the classification tree method. This, however, {{does not allow for}} modelling constraints between classes of different classifications. Lehmann and Wegener introduced Dependency Rules based on Boolean expressions with their incarnation of the CTE. Further features include the automated generation of test suites using <b>combinatorial</b> <b>test</b> design (e.g. all-pairs testing).|$|E
2500|$|In 2000, Lehmann and Wegener {{introduced}} Dependency Rules {{with their}} {{incarnation of the}} CTE, the CTE XL (eXtended Logics). [...] Further features include the automated generation of test suites using <b>combinatorial</b> <b>test</b> design (e.g. all-pairs testing).|$|E
40|$|Abstract- <b>Combinatorial</b> <b>testing</b> has {{attracted}} {{a lot of attention}} from both industry and academia. A number of reports suggest that <b>combinatorial</b> <b>testing</b> can be effective for practical applications. However, there are few systematic, controlled studies on the effectiveness of <b>combinatorial</b> <b>testing.</b> In particular, input parameter modeling is a key step in the <b>combinatorial</b> <b>testing</b> process. But most studies do not report the details of the modeling process. In this paper, we report an experiment that applies <b>combinatorial</b> <b>testing</b> to the Siemens suite. The Siemens suite {{has been used as a}} benchmark to evaluate the effectiveness of many testing techniques. Each program in the suite has a number of faulty versions. The effectiveness of <b>combinatorial</b> <b>testing</b> is measured in terms of the number of faulty versions that are detected. The experimental results show that <b>combinatorial</b> <b>testing</b> is effective in terms of detecting most of the faulty versions with a small number of tests. In addition, we report the details of our modeling process, which we hope to shed some lights on this critical, yet often ignored step, in the <b>combinatorial</b> <b>testing</b> process...|$|R
50|$|One {{application}} of ADS is software <b>testing,</b> particularly <b>combinatorial</b> <b>testing.</b> A framework {{has been proposed}} based on ADS for concurrent <b>combinatorial</b> <b>testing</b> using AR and TA.|$|R
40|$|Software {{product lines}} are the common trend in {{software}} development which helps {{in reducing the}} development cost. Mostly the interaction faults {{are very difficult to}} identify during the process of debugging. By the use of <b>combinatorial</b> <b>testing</b> a set of features can be identified and all small combinations can be verified to a certain level only. By introducing random testing can improve the accuracy and ratio of t-wise fault detection. Through random testing can acquire a higher level of improvements over the <b>combinatorial</b> <b>testing</b> which will be under the budgetary limit of the product. Random testing can provide minimum guarantees on the probability of fault detection at any interaction level using the set of theories. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with <b>combinatorial</b> <b>testing.</b> Given that <b>combinatorial</b> <b>testing</b> entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random <b>testing</b> may outperform <b>combinatorial</b> <b>testing</b> in large systems. Furthermore, in common situations where test budgets are constrained and unlike <b>combinatorial</b> <b>testing,</b> random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than <b>combinatorial</b> <b>testing.</b> Index Terms: <b>Combinatorial</b> <b>testing,</b> random testing, t-wise faul...|$|R
50|$|Software {{developers}} can't test everything, {{but they}} can use <b>combinatorial</b> <b>test</b> design to identify the minimum number of tests {{needed to get the}} coverage they want. <b>Combinatorial</b> <b>test</b> design enables users to get greater test coverage with fewer tests. Whether they are looking for speed or test depth, they can use <b>combinatorial</b> <b>test</b> design methods to build structured variation into their test cases.|$|E
50|$|Re-assert scan mode, {{and see if}} the <b>combinatorial</b> <b>test</b> passed.|$|E
50|$|The {{second step}} of test design then follows the {{principles}} of <b>combinatorial</b> <b>test</b> design.|$|E
40|$|Modern {{systems are}} {{becoming}} highly configurable {{to satisfy the}} varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. <b>Combinatorial</b> <b>testing</b> is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to <b>combinatorial</b> <b>testing</b> {{when there are no}} constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with <b>combinatorial</b> <b>testing.</b> Given that <b>combinatorial</b> <b>testing</b> entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random <b>testing</b> may outperform <b>combinatorial</b> <b>testing</b> in large systems. Furthermore, in common situations where test budgets are constrained and unlike <b>combinatorial</b> <b>testing,</b> random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than <b>combinatorial</b> <b>testing.</b> As a result, {{in order to have a}} practical impact, future research should focus on better understanding the decision process to choose between random <b>testing</b> and <b>combinatorial</b> <b>testing,</b> and improve <b>combinatorial</b> <b>testing</b> in the presence of feature constraints...|$|R
40|$|Abstract—Modern {{systems are}} {{becoming}} highly configurable {{to satisfy the}} varying needs of customers and users. Software product lines are hence becoming a common trend in software development to reduce cost by enabling systematic, large-scale reuse. However, high levels of configurability entail new challenges. Some faults might be revealed only if a particular combination of features is selected in the delivered products. But testing all combinations is usually not feasible in practice, due to their extremely large numbers. <b>Combinatorial</b> <b>testing</b> is a technique to generate smaller test suites for which all combinations of t features are guaranteed to be tested. In this paper, we present several theorems describing the probability of random testing to detect interaction faults and compare the results to <b>combinatorial</b> <b>testing</b> {{when there are no}} constraints among the features that can be part of a product. For example, random testing becomes even more effective as the number of features increases and converges toward equal effectiveness with <b>combinatorial</b> <b>testing.</b> Given that <b>combinatorial</b> <b>testing</b> entails significant computational overhead in the presence of hundreds or thousands of features, the results suggest that there are realistic scenarios in which random <b>testing</b> may outperform <b>combinatorial</b> <b>testing</b> in large systems. Furthermore, in common situations where test budgets are constrained and unlike <b>combinatorial</b> <b>testing,</b> random testing can still provide minimum guarantees on the probability of fault detection at any interaction level. However, when constraints are present among features, then random testing can fare arbitrarily worse than <b>combinatorial</b> <b>testing.</b> As a result, {{in order to have a}} practical impact, future research should focus on better understanding the decision process to choose between random <b>testing</b> and <b>combinatorial</b> <b>testing,</b> and improve <b>combinatorial</b> <b>testing</b> in the presence of feature constraints. Index Terms—Combinatorial testing, random testing, interaction testing, theory, constraint, feature diagram, lower bound Ç...|$|R
40|$|T-way <b>combinatorial</b> <b>testing</b> aims to {{generate}} a smaller test suite size. The purpose of t-way <b>combinatorial</b> <b>testing</b> is to overcome exhaustive testing. Although many existing strategies {{have been developed for}} t-way <b>combinatorial</b> <b>testing,</b> study in this area is encouraging as it falls under NP-hard optimization problem. This paper focuses on the analysis of existing algorithms or tools for the past seven years. Taxonomy of <b>combinatorial</b> <b>testing</b> is proposed to ease the analysis. 20 algorithms or tools were analysed based on strategy approach, search technique, supported interaction and year published. 2015 was the most active year in which researchers developed t-way algorithms or tools. OTAT strategy and metaheuristic search technique are the most encouraging research areas for t-way <b>combinatorial</b> <b>testing.</b> There is a slight difference in the type of interaction support. However, uniform strength is the most utilized form of interaction from 2010 to the first quarter of 2017...|$|R
5000|$|In 2000, Lehmann and Wegener {{introduced}} Dependency Rules {{with their}} {{incarnation of the}} CTE, the CTE XL (eXtended Logics). [...] Further features include the automated generation of test suites using <b>combinatorial</b> <b>test</b> design (e.g. all-pairs testing).|$|E
50|$|In a {{full scan}} design, {{automatic}} {{test pattern generation}} (ATPG) is particularly simple. No sequential pattern generation is required - combinatorial tests, which are much easier to generate, will suffice. If you have a <b>combinatorial</b> <b>test,</b> it can be easily applied.|$|E
50|$|One way of {{modelling}} constraints {{is using}} the refinement mechanism in the classification tree method. This, however, {{does not allow for}} modelling constraints between classes of different classifications. Lehmann and Wegener introduced Dependency Rules based on Boolean expressions with their incarnation of the CTE. Further features include the automated generation of test suites using <b>combinatorial</b> <b>test</b> design (e.g. all-pairs testing).|$|E
40|$|With {{their many}} {{interacting}} parameters, modern software systems are highly configurable. <b>Combinatorial</b> <b>testing</b> {{is a widely}} used and practical technique that can detect the failures triggered by the parameters and their interactions. One of the key challenges in <b>combinatorial</b> <b>testing</b> is covering array generation...|$|R
40|$|<b>Combinatorial</b> <b>testing</b> aims at {{reducing}} {{the cost of}} software and system testing by {{reducing the number of}} test cases to be executed. We propose an approach for <b>combinatorial</b> <b>testing</b> that generates a set of test cases that is as small as possible, using incremental SAT solving. We present several search-space pruning techniques that further improve our approach. Experiments show a significant improvement of our approach over other SAT-based approaches, and considerable reduction of the number of test cases over other <b>combinatorial</b> <b>testing</b> tools. QC 20170117 </p...|$|R
40|$|Abstract: This paper {{compares the}} {{effectiveness}} of random and t-way <b>combinatorial</b> <b>testing,</b> where t = 2, 3, 4, for a grid computer network simulator. Previous investigations of random vs. <b>combinatorial</b> <b>testing</b> have reached conflicting results, with some showing more effective fault detection for <b>combinatorial</b> <b>testing</b> and others finding {{no significant difference between}} the two methods. In this paper, these two methods are compared for deadlock detection on a simulator with tests covering 2 -way to 4 -way combinations of configuration values, paired with an equal number of randomly generated tests. Random testing provided better results than pairwise (2 way) testing and there was no statistically significant difference between the methods for 3 -way testing, but 4 -way <b>combinatorial</b> <b>tests</b> detected more deadlocks than the same number of random tests. The paper reviews explanations for these results and implications for testing. ...|$|R
40|$|This paper {{proposes a}} novel {{approach}} to <b>combinatorial</b> <b>test</b> generation, which {{achieves an increase of}} not only the number of new combinations but also the distance between test cases. We applied our distance-integrated approach to a state-of-the-art greedy algorithm for traditional <b>combinatorial</b> <b>test</b> generation by using two distance metrics, Hamming distance, and a modified chi-square distance. Experimental results using numerous benchmark models show that <b>combinatorial</b> <b>test</b> suites generated by our approach using both distance metrics can improve interaction coverage for higher interaction strengths with low computational overhead. QC 20170109 </p...|$|E
40|$|The {{biggest problem}} for <b>combinatorial</b> <b>test</b> is a {{numerous}} number of {{combinations of input}} parameters by combinatorial explosion. Pair-wise combinatorial coverage testing is an effective method which can reduce the test cases in a suite {{and is able to}} detect about 70 % program errors. But, under many circumstances, the parameters in programs under test (PUT s) have relations with each other. So there are some ineffective test cases in pair-wise <b>combinatorial</b> <b>test</b> suites. In this paper, we propose a method of reducing ineffective <b>combinatorial</b> <b>test</b> cases from pair-wise test suite. The main ideas of the method is that we firstly analyzes the dependent relationships among input parameters, then use the relationships to reduce ineffective pair-wise combinations of input parameters, and lastly generate the pair-wise combinatorial coverage test suite. The experiments show that the method is feasible and effective, and considerably reduce the number of pair-wise <b>combinatorial</b> <b>test</b> cases for some programs under test. </p...|$|E
40|$|Combinatorial {{interaction}} {{testing is}} a well-recognized testing method, {{and has been}} widely applied in practice, often {{with the assumption that}} all test cases in a <b>combinatorial</b> <b>test</b> suite have the same fault detection capability. However, when testing resources are limited, an alternative assumption may be that some test cases are more likely to reveal failure, thus making the order of executing the test cases critical. To improve testing cost-effectiveness, prioritization of <b>combinatorial</b> <b>test</b> cases is employed. The most popular approach is based on interaction coverage, which prioritizes <b>combinatorial</b> <b>test</b> cases by repeatedly choosing an unexecuted test case that covers the largest number of uncovered parameter value combinations of a given strength (level of interaction among parameters). However, this approach suffers from some drawbacks. Based on previous observations that the majority of faults in practical systems can usually be triggered with parameter interactions of small strengths, we propose a new strategy of prioritizing <b>combinatorial</b> <b>test</b> cases by incrementally adjusting the strength values. Experimental results show that our method performs better than the random prioritization technique and the technique of prioritizing <b>combinatorial</b> <b>test</b> suites according to test case generation order, and has better performance than the interaction-coverage-based test prioritization technique in most cases...|$|E
30|$|To corroborate this claim, in (Segall et al. 2011) a BDD-based approach, {{implemented}} in the Focus tool, was better in terms of cost than the greedy solutions Advanced <b>Combinatorial</b> <b>Testing</b> System (ACTS) (Yu et al. 2013), Pairwise Indepedent <b>Combinatorial</b> <b>Testing</b> (PICT) (Czerwonka 2006), and jenny (Jenkins 2016) in the constrained domain. However, their method was worse than such greedy solutions for unconstrained problems.|$|R
40|$|Computer {{software}} is {{in high demand}} everywhere in the world. The high dependence on software makes software requirements more complicated. As a result, software testing tasks get costlier and challenging due to {{a large number of}} test cases, coupled with the vast number of the system requirements. This challenge presents the need for reduction of the system redundant <b>test</b> cases. A <b>combinatorial</b> <b>testing</b> approach gives an intended result from the optimization of the system test cases. Hence, this study implements a <b>combinatorial</b> <b>testing</b> strategy called Artificial Bee Colony Test Generation (ABC-TG) that helps to get rid of some of the current <b>combinatorial</b> <b>testing</b> strategies. Results obtained from the ABC-TG were benchmarked with the results obtained from existing strategies {{in order to determine the}} efficiency of the ABC-TG. Finally, ABC-TG shows the efficiency and effectiveness in terms of generating optimum test cases size of some of the case studies and a comparable result with the existing <b>combinatorial</b> <b>testing</b> strategies...|$|R
40|$|Abstract—In this paper, {{we develop}} a <b>combinatorial</b> <b>testing</b> {{technique}} for tree-structured test models. First, we generalize our previous <b>test</b> models for <b>combinatorial</b> <b>testing</b> based on and-xor trees with constraints {{limited to a}} syntactic subset of propositional logic, to allow for constraints in full propositional logic. We prove that the generalized test models are strictly more expressive than the limited ones. Then we develop an algorithm for <b>combinatorial</b> <b>testing</b> for the generalized models, and show its correctness and computational complexity. We apply a tool based on our algorithm to an actual ticket gate system {{that is used by}} several large transportation companies in Japan. Experimental results show that our technique outperforms existing techniques. I...|$|R
40|$|Abstract—Generating {{a set of}} <b>combinatorial</b> <b>test</b> {{cases for}} web {{application}} and development is very important. Linear programming (LP) is one of approaches to generating small <b>combinatorial</b> <b>test</b> suite. But, for software under test (SUT), the number of <b>combinatorial</b> <b>test</b> cases is often extremely large. If using LP to optimize test suite, the optimizing efficiency is low for the enormous size of LP variables and constraints (we call macro-LP). In view of the issue, we propose {{a new approach to}} optimization of test suite by using LP. The main ideas is to divide macro-LP into micro-LP which has relatively small sizes of LP variables and constraints. Experimental results show that the approach can improve the efficiency of solving test suite via LP greatly. Index Terms—web test, cover-matrix, linear programmin...|$|E
40|$|Combinatorial {{testing is}} a widely-used {{technique}} to detect system interaction failures. To improve test effectiveness with given priority weights of parameter values {{in a system}} under test, prioritized combinatorial testing constructs test suites where highly weighted parameter values appear earlier or more frequently. Such order-focused and frequency-focused <b>combinatorial</b> <b>test</b> generation algorithms have been evaluated using metrics called weight coverage and KL divergence but not sufficiently with fault detection effectiveness so far. We evaluate the fault detection effectiveness on a collection of open source utilities, applying prioritized <b>combinatorial</b> <b>test</b> generation and investigating its correlation with weight coverage and KL divergence. QC 20170109 </p...|$|E
40|$|In {{this paper}} we {{describe}} {{an approach to}} use formal analysis tools in conjunction with traditional testing to improve {{the efficiency of the}} test generation process. We have developed a technique for the construction of <b>combinatorial</b> <b>test</b> suites, featuring expressive constraints over the models under test and cross coverage evaluation between multiple coverage criteria: combinatorial, structural and fault based. Our approach is tightly integrated with formal logic, since it uses formal logic to specify the system inputs (including the constraints), test predicates to formalize testing as a logic problem, and applies the SAL model checker tool to solve it, and hence to generate <b>combinatorial</b> <b>test</b> suites. Early results of experimental assessment are presented, supported by a prototype tool implementation. 1...|$|E
40|$|Abstract. We study {{practically}} efficient {{methods for}} performing combinatorialgroup testing. We present efficient non-adaptive and two-stage <b>combinatorial</b> group <b>testing</b> algorithms, which identify the at most d items {{out of a}} given setof n items that are defective, using fewer tests for all practical set sizes. For ex-ample, our two-stage algorithm matches the information theoretic lower bound {{for the number of}} <b>tests</b> in a <b>combinatorial</b> group <b>testing</b> regimen. Keywords: <b>combinatorial</b> group <b>testing,</b> Chinese remaindering, Bloom filters 1 Introduction The problem of <b>combinatorial</b> group <b>testing</b> dates back to World War II, for the prob-lem of determining which in a group of n blood samples contain the syphilis antigen(hence, are contaminated). Formally, in <b>combinatorial</b> group <b>testing,</b> we are given a se...|$|R
40|$|Abstract—Combinatorial testing aims at {{reducing}} {{the cost of}} software and system testing by {{reducing the number of}} test cases to be executed. We propose an approach for <b>combinatorial</b> <b>testing</b> that generates a set of test cases that is as small as possible, using incremental SAT solving. We present several search-space pruning techniques that further improve our approach. Experiments show a significant improvement of our approach over other SAT-based approaches, and considerable reduction of the number of test cases over other <b>combinatorial</b> <b>testing</b> tools. I...|$|R
40|$|Numerous <b>combinatorial</b> <b>testing</b> {{tools are}} {{available}} for generating test cases. However, {{many of them are}} never used in practice. One of the reasons is the lack of empirical studies that involve human subjects applying testing techniques. This paper aims to investigate the applicability of a <b>combinatorial</b> <b>testing</b> tool in the company SOFTEAM. A case study is designed and conducted within the development team responsible for a new product. The participants consist of 3 practitioners from the company. The applicability of the tool has been examined in terms of efficiency, effectiveness and learning effort...|$|R
40|$|Part 4 : Short ContributionsInternational audienceTesting is an {{important}} and expensive part of software and hardware development. Over the recent years, the construction of combinatorial interaction tests rose to play {{an important}} role towards making the cost of testing more efficient. Covering arrays are the key element of combinatorial interaction testing and a means to provide abstract test sets. In this paper, we present a family of set-based algorithms for generating covering arrays and thus <b>combinatorial</b> <b>test</b> sets. Our algorithms build upon an existing mathematical method for constructing independent families of sets, which we extend sufficiently in terms of algorithmic design in this paper. We compare our algorithms against commonly used greedy methods for producing 3 -way <b>combinatorial</b> <b>test</b> sets, and these initial evaluation results favor our approach in terms of generating smaller test sets...|$|E
40|$|Random testing (RT), a {{fundamental}} software testing technique, {{has been widely}} used in practice. Adaptive random testing (ART), an enhancement of RT, performs better than original RT in terms of fault detection capability. However, not much {{work has been done}} on effectiveness analysis of ART in the <b>combinatorial</b> <b>test</b> spaces. In this paper, we propose a novel family of ART-based algorithms for generating <b>combinatorial</b> <b>test</b> suites, mainly based on fixed-size-candidateset ART and restricted random testing (that is, ART by exclusion). We use an empirical approach to compare the effectiveness of test sets obtained by our proposed methods and random selection strategy. Experimental data demonstrate that the ART-based tests cover all possible combinations at a given strength more quickly than randomly chosen tests, and often detect more failures earlier and with fewer test cases in simulations...|$|E
40|$|This {{position}} paper addresses some {{weaknesses of the}} standard logical languages used for specification of system models in <b>combinatorial</b> <b>test</b> design. To overcome these weaknesses, we propose a new logical language which uses visual elements with the aim to lower the cognitive load of the modeller and thereby {{reduce the risk of}} modelling errors...|$|E
40|$|The gaming {{industry}} has been on constant rise {{over the last few}} years. Companies invest huge amounts of money for the release of their games. A part of this money is invested in testing the games. Current game testing methods include manual execution of pre-written test cases in the game. Each test case may or may not result in a bug. In a game, a bug is said to occur when the game does not behave according to its intended design. The process of writing the test cases to test games requires standardization. We believe that this standardization can be achieved by implementing experimental design to video game testing. In this thesis, we discuss the implementation of <b>combinatorial</b> <b>testing</b> to <b>test</b> games. <b>Combinatorial</b> <b>testing</b> is a method of experimental design that is used to generate test cases and is primarily used for commercial software testing. In addition to the discussion of the implementation of <b>combinatorial</b> <b>testing</b> techniques in video game testing, we present a method for finding combinations resulting in video game bugs...|$|R
40|$|Abstract:- We propose in {{this paper}} a general {{framework}} for an integrated End-to-End Testing of IT Architecture and Applications using the simultaneous application of <b>combinatorial</b> <b>testing</b> and virtualization. <b>Combinatorial</b> <b>testing</b> methods are often applied in cases of the configuration <b>testing.</b> The <b>combinatorial</b> approach to software testing uses models, particularly an Orthogonal Array Testing Strategy (OATS) is proposed as a systematic, statistical way of testing pair-wise interactions to generate a minimal number of test inputs so that selected combinations of input values are covered. Virtualization, {{in the process of}} testing, is based on setting the necessary environment to multiple virtual machines, which run on one or in smaller groups of physical computers, which are: reduce the cost of equipment and related resources, reduce the time required to manage the testing process, and favors raising removal of <b>test</b> infrastructure. Together, <b>combinatorial</b> <b>testing</b> and virtualization presents practical approach to improving process of testing, through the balancing quality, cost and time...|$|R
40|$|Abstract. We study {{practically}} efficient {{methods for}} performing <b>combinatorial</b> group <b>testing.</b> We present efficient non-adaptive and two-stage <b>combinatorial</b> group <b>testing</b> algorithms, which identify the at most d items {{out of a}} given set of n items that are defective, using fewer tests for all practical set sizes. For ex-ample, our two-stage algorithm matches the information theoretic lower bound {{for the number of}} <b>tests</b> in a <b>combinatorial</b> group <b>testing</b> regimen...|$|R
