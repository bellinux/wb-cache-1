17|58|Public
40|$|This {{dissertation}} {{describes a}} number of algorithms developed to increase the robustness of automatic speech recognition systems with respect {{to changes in the}} environment. These algorithms attempt to improve the recognition accuracy of speech recognition systems when they are trained and tested in different acoustical environments, and when a desk-top microphone (rather than a <b>close-talking</b> <b>microphone)</b> is used for speech input. Without such processing, mismatches between training and testing conditions produce an unacceptable degradation in recognition accuracy. Two kinds o...|$|E
30|$|In case of noisy speech {{recognition}} for in-car situations in[5, 6], a fundamentally different approach for {{the exploitation of}} spatially distributed distant microphones is introduced. It is proposed to estimate the log speech spectrum at a hypothetical <b>close-talking</b> <b>microphone</b> that should have good quality by multiple regression of the log spectra of several distant microphone signals. In other environments, where the speaker’s location is not known before, the microphones are mounted arbitrarily in a living room or in an office {{to take advantage of}} the space diversity for distant-talking {{speech recognition}}[7] or to process a real-time speaker localization as in[8].|$|E
40|$|In {{this paper}} we {{consider}} the problem of detecting speaker changes in audio signals recorded by distant microphones. It is shown that the possibility to exploit the spatial separation of speakers more than makes up the degradation in detection accuracy due to the increased source-to-sensor distance compared to close-talking microphones. Speaker direction information {{is derived from the}} filter coefficients of an adaptive Filter-and-Sum Beamformer and is combined with BIC analysis. The experimental results reveal significant improvements compared to BIC-only change detection, be it with the distant or <b>close-talking</b> <b>microphone.</b> Index Terms: speaker diarization, position estimation, beamforming, BIC...|$|E
3000|$|... words each), {{according}} to syntactic boundaries, and shown {{on a computer}} screen. The speech samples were recorded with a <b>close-talk</b> <b>microphone</b> (Call 4 U Comfort Headset, DNT GmbH, Dietzenbach, Germany; sampling frequency 16 [*]kHz, amplitude resolution 16 bit).|$|R
40|$|Abstract. We {{describe}} {{the latest version}} of the SRI-ICSI meeting and lecture recognition system, as was used in the NIST RT- 07 evaluations, highlighting improvements made over the last year. Changes in the acoustic preprocessing include updated beamforming software for processing of multiple distant microphones, and various adjustments to the speech segmenter for <b>close-talking</b> <b>microphones.</b> Acoustic models were improved by the combined use of neuralnet-estimated phone posterior features, discriminative feature transforms trained with fMPE-MAP, and discriminative Gaussian estimation using MPE-MAP, as well as model adaptation specifically to nonnative and non-American speakers. The net effect of these enhancements was a 14 - 16 % relative error reduction on distant microphones, and a 16 - 17 % error reduction on <b>close-talking</b> <b>microphones.</b> Also, for the first time, we report results on a new “coffee break ” meeting genre, and on a new NIST metric designed to evaluate combined speech diarization and recognition. ...|$|R
40|$|<b>Close-talk</b> headset <b>microphones</b> {{have been}} {{traditionally}} used for speech acquisition {{in a number}} of applications, as they naturally provide a higher signal-to-noise ratio-needed for recognition tasksthan single distant microphones. However, in multi-party conversational settings like meetings, microphone arrays represent an important alternative to <b>close-talking</b> <b>microphones,</b> as they allow for localisation and tracking of speakers and signal-independent enhancement, while providing a non-intrusive, hands-free operation mode. In this article, we investigate the use of an audio-visual sensor array, composed of a small table-top microphone array and a set of cameras, for speaker tracking and speech enhancement in meetings. Our methodology first fuses audio and video for person tracking, and then integrates the output of the tracker with a beamformer for speech enhancement. We compare and discuss the features of the resulting speech signal with respect to that obtained from single <b>close-talking</b> and table-top <b>microphones.</b> 1...|$|R
40|$|Abstract. Meeting {{transcription}} {{is one of}} {{the main}} tasks for large vo-cabulary automatic speech recognition (ASR) and is supported by several large international projects in the area. The conversational nature, the difficult acoustics, and the necessity of high quality speech transcripts for higher level processing make ASR of meeting recordings an interesting challenge. This paper describes the development and system architecture of the 2007 AMIDA meeting transcription system, the third of such sys-tems developed in a collaboration of six research sites. Different variants of the system participated in all speech to text transcription tasks of the 2007 NIST RT evaluations and showed very competitive performance. The best result was obtained on <b>close-talking</b> <b>microphone</b> data where a final word error rate of 24. 9 % was obtained. ...|$|E
40|$|We {{present results}} on the {{recognition}} accuracy of a continuous speech, speaker independent HMM recognition system that incorporates a novel noise reduction algorithm. The algorithm is a {{minimum mean square error}} estimation tailored for a filter-bank front-end. It introduces a significant improvement over similar published algorithms by incorporating a better statistical model for the filter-bank log-energies, and by attempting to jointly estimate the log-energies vector rather than individual components. The algonthm was tested with SRrs recognizer trained on the official speaker-independent "Resource management task " clean speech database. When tested with additive white gaussian noise, the noise reduction achieved by the algorithm is equivalent to a 13 dB SNR improvement. When tested with desktop microphone recordings, the error rate at 13 dB SNR is only 40 % higher than that with <b>close-talking</b> <b>microphone</b> at 31 dB SNR. I...|$|E
40|$|A {{major problem}} for speech {{recognition}} systems is relieving the talker {{of the need to}} use a close-talking, head-mounted or a desk-stand microphone. A likely solution is the use of an array of mi-crophones that can steer itself to the talker and can use a beam-forming algorithm to overcome the reduced signal-to-noise ratio due to room acoustics. This paper reports results for a tracking, real-time microphone-array as an input to an HMM-based con-nected alpha-digits speech recognizer. For a talker in the very near field of the array (within a meter), performance approaches that of a <b>close-talking</b> <b>microphone</b> input device. The effects of both the noise reducing steered array and the use of a Maximum a poste-riori (MAP) training step are shown to be significant. Here, the array system and the recognizer are described, experiments are presented, and the implications of combining these two systems discussed. 1...|$|E
40|$|Vocal {{activity}} detection in <b>close-talk</b> <b>microphone</b> {{recordings of}} multiparty conversation continues to pose problems for meeting recognition systems, {{as evidenced by}} a 2 - 3 % absolute gap in word error rates achieved with automatic and manual segmentations. State-of-the-art segmentation systems in this domain have adopted one of three different acoustic model approaches: • independent decoding of each of K close-talk channels, in a space of two microphone states (speech / silence) per channe...|$|R
30|$|In our evaluation, {{the audio}} {{contains}} microphone recordings of real talks in real workshops, in large conference rooms with public. Microphones, conference rooms, and even recording conditions change from one recording to another. <b>Microphones</b> are not <b>close-talking</b> <b>microphones</b> but mainly tabletop and ground standing microphones. This {{difference in the}} evaluation conditions makes our evaluation pose different challenges and {{makes it difficult to}} compare the results obtained in our evaluation to those of the previous NIST STD evaluations.|$|R
40|$|The {{recognition}} of speech in meetings poses {{a number of}} challenges to current Automatic Speech Recognition (ASR) techniques. Meetings typically take place in rooms with non-ideal acoustic conditions and significant background noise, and may contain large sections of overlapping speech. In such circumstances, headset microphones have to date provided the best recognition performance, however participants are often reluctant to wear them. Microphone arrays provide an alternative to <b>close-talking</b> <b>microphones</b> by providing speech enhancement through directional discrimination. Unfortunately, however, development of array front-end systems for state-of-the-art large vocabulary continuous speech recognition suffers {{from a lack of}} necessary resources, as most available speech corpora consist only of single-channel recordings. This paper describes the collection of an audio-visual corpus of read speech from a number of instrumented meeting rooms. The corpus, based on the WSJCAM 0 database, is suitable for use in continuous speech recognition experiments and is captured using a variety of microphones, including arrays, as well as close-up and wider angle cameras. The paper also describes some initial ASR experiments on the corpus comparing the use of <b>close-talking</b> <b>microphones</b> with both a fixed and a blind array beamforming technique. 1...|$|R
40|$|The {{need for}} {{hands-free}} communication {{has led to}} an increased popularity in the use of headsets with mobile phones. Comfort and portability concerns have led to the desire for headsets with a small form factor. Unfortunately, this size constraint typically requires that the microphone be placed farther from the user’s mouth, making it highly susceptible to environmental noise. One long term goal of our work is to develop a headset that can achieve the sound capture performance of a <b>close-talking</b> <b>microphone</b> located at the user’s mouth, while maintaining the desired compact size. Toward this end, we have designed a headset consisting of three air microphones and a bone-conductive sensor. The speech enhancement is performed in two stages, a fixed beamformer followed by a single-channel adaptive post-filter. Unlike other techniques, the beamformer is calibrated in a purely data-drive manner. We present preliminary experimental results using real data collected in multiple environments. The proposed approach results in significant improvements in both speech recognition accuracy and SNR. 1...|$|E
30|$|The {{experiments}} {{presented in}} this paper have been implemented using CARVUI database recorded inside a moving car. The data was collected in Bell Labs area, under various driving conditions (highway/city roads) and noise environments (with and without radio/music in the background). About two thirds of the recordings contain music or bubble noise in the background. A total of 56 speakers participated in the data collection. The speech material from 50 speakers is used for training, and the data from the 6 remaining speakers is used for test. Simultaneous recordings were made using a close-taking microphone and a 16 -channel array of first-order hypercardiod microphones mounted on the visor. Data from two channels only are used. The first one is the <b>close-talking</b> <b>microphone</b> (CT) channel. The second one is a single channel from the microphone array, referred to as hands-free data (HF) henceforward. The average SNR is about 21 db for the CT channel and 8 db for the HF channel. The experiments were implemented using the part of the database that contains only the digit utterances.|$|E
40|$|Infovox is mqltetlng a speaker-dependent, pattem-matching word {{recognition}} system, {{developed at}} KTII. The algorithms {{in the system}} have been-modified foi noise im-munity, and performance has been evaluated in moving cars. The mainproblems were word detection and noise compensation. After simulaoons we decided to use a <b>close-talking</b> <b>microphone</b> and a "noise additionu method, where we added the measured nois-e in tle moving car to the reference pattems recorded in a parked car. Using this method, the recognition rate was improved fron 697 o to 97 Vo on a ten-word voiabu-lary using the best microphone. A more extensive test {{was performed on the}} modified recognition system using two cars and twelve speakers, seven male and five female. Most of them were naive speakers. The twenty-word vocabulary contained some con-fusable words and was trained in a parted car. During 98 sessions, 1, 960 words were read under different conditions with an average recognition rute of. 867 o. With closed windows at 90 kn/h the mean was 9 l%o. An open window at the same speed decreased theresult to 82 Vo...|$|E
40|$|This paper {{investigates the}} use of {{microphone}} arrays to acquire and recognise speech in meetings. Meetings pose several interesting problems for speech processing, as they consist of multiple competing speakers within a small space, typically around a table. Due {{to their ability to}} provide hands-free acquisition and directional discrimination, microphone arrays present a potential alternative to <b>close-talking</b> <b>microphones</b> in such an application. We first propose an appropriate microphone array geometry and improved processing technique for this scenario, paying particular attention to speaker separation during possible overlap segments. Data collection of a small vocabulary speech recognition corpus (Numbers) was performed in a real meeting room for a single speaker, and several overlapping speech scenarios. In speech recognition experiments on the acquired database, the performance of the microphone array system is compared to that of a <b>close-talking</b> lapel <b>microphone,</b> and a single table-top microphone...|$|R
30|$|Automatic speech {{recognition}} (ASR) systems and hands-free speech acquisition systems have achieved satisfactory performance for <b>close-talk</b> <b>microphones.</b> However, {{the performance of}} these systems is still poor for far-talk microphones where {{the distance between the}} speaker and the microphone is large. This is because the speech signals recorded by the far-talk microphones are significantly corrupted by background noise and room reverberation. Hence, improving the robustness of the ASR and other speech-processing systems for far-talk speech is an important task for the deployment of such systems in more realistic environments.|$|R
30|$|The most {{important}} {{difference is the}} nature of the audio content used for the evaluations. In the MediaEval evaluations, the speech was typically telephone speech, either conversational or read and elicited speech, or speech recorded with in-room microphones. In the ALBAYZIN evaluations, the audio consisted of microphone recordings of real talks in workshops that took place in large conference rooms in the presence of an audience. The microphones, the conference rooms, and the recording conditions changed from one recording to another. The <b>microphones</b> were not <b>close-talking</b> <b>microphones</b> but were mainly tabletop or floor standing microphones.|$|R
40|$|In this work, {{we address}} an {{acoustic}} beamforming application where two speakers are simultaneously active. We construct one subband domain beamformer in generalized sidelobe canceller (GSC) configuration for each source. In contrast to normal practice, we then jointly adjust the active weight vectors of both GSCs to obtain two output signals with minimum mutual information (MMI). In order {{to calculate the}} mutual information of the complex subband snapshots, we consider four probability density functions (pdfs), namely the Gaussian, Laplace, K 0 and Γ pdfs. The latter three belong to the class of super-Gaussian density functions that are typically used in independent component analysis as opposed to conventional beamforming. We demonstrate the effectiveness of our proposed technique {{through a series of}} far-field automatic speech recognition experiments on data from the PASCAL Speech Separation Challenge. In the experiments, the delay-and-sum beamformer achieved a word error rate (WER) of 70. 4 %. The MMI beamformer under a Gaussian assumption achieved 55. 2 % WER which was further reduced to 52. 0 % with a K 0 pdf, whereas the WER for data recorded with <b>close-talking</b> <b>microphone</b> was 21. 6 %. Index Terms — microphone array, beamforming, independent component analysis, far-field speech recognition 1...|$|E
40|$|In {{this paper}} we {{describe}} our initial {{efforts to make}} SPHINX, the CMU continuous speech recognition system, environmentally robust. Our work has two major goals: to enable SPHINX to adapt to changes in microphone and acoustical environment, and to improve the performance of SPHINX when it is trained and tested using a desk-top microphone. This talk will describe some of our work in acoustical pre-processing techniques, specifically spectral normalization and spectral subtraction performed using an efficient pair of algorithms that operate primarily in the cepstral domain. The effects of these signal processing algorithms on the recognition accuracy of the Sphinx speech recognition system was compared using speech simultaneously recorded from two types of microphones: the standard close-talking Sennheiser HMD 224 microphone and the desk-top Crown PZM 6 fs microphone. A naturallyelicited alphanumeric speech database was used. In initial results using the stereo alphanumeric database, we found that both the spectral subtraction and spectral normalization algorithms were able to provide very substantial improvements in recognition accuracy when the system was trained on the <b>close-talking</b> <b>microphone</b> and tested on the desk-top microphone, or vice versa. Improving the recognition accuracy of the system when trained and tested on the desk-top microphone remains a difficult problem requinng more sophisticated noise suppression techniques...|$|E
40|$|Abstract — In this work, we {{consider}} an acoustic beamforming application where two speakers are simultaneously active. We construct one subband-domain beamformer in generalized sidelobe canceller (GSC) configuration for each source. In contrast to normal practice, we then jointly optimize the active weight vectors of both GSCs to obtain two output signals with minimum mutual information (MMI). Assuming that the subband snapshots are Gaussian-distributed, this MMI criterion reduces to {{the requirement that}} the cross-correlation coefficient of the subband outputs of the two GSCs vanishes. We also compare separation performance under the Gaussian assumption with that obtained from several super-Gaussian probability density functions (pdfs), namely, the Laplace, K 0, and Γ pdfs. Our proposed technique provides effective nulling of the undesired source, but without the signal cancellation problems seen in conventional beamforming. Moreover, our technique does not suffer from the source permutation and scaling ambiguities encountered in conventional blind source separation algorithms. We demonstrate the effectiveness of our proposed technique {{through a series of}} far-field automatic speech recognition experiments on data from the PASCAL Speech Separation Challenge (SSC). On the SSC development data, the simple delay-and-sum beamformer achieves a word error rate (WER) of 70. 4 %. The MMI beamformer under a Gaussian assumption achieves a 55. 2 % WER, which is further reduced to 52. 0 % with a K 0 pdf, whereas the WER for data recorded with a <b>close-talking</b> <b>microphone</b> is 21. 6 %. Index Terms — microphone arrays, beamforming, speech recognition, source separation I...|$|E
40|$|Abstract. Automatic speech {{recognition}} (ASR) systems, trained on speech signals from <b>close-talking</b> <b>microphones,</b> generally fail in recognizing far-field speech. In this paper, {{we present a}} Hilbert Envelope based feature extraction technique to alleviate the artifacts introduced by room reverberations. The proposed technique is based on modeling temporal envelopes of the speech signal in narrow sub-bands using Frequency Domain Linear Prediction (FDLP). ASR experiments on far-field speech using the proposed FDLP features show significant performance improvements {{when compared to other}} robust feature extraction techniques (average relative improvement of 43 % in word error rate) ...|$|R
40|$|In {{this paper}} {{we present a}} dereverberation {{algorithm}} for improving automatic speech recognition (ASR) results with minimal CPU overhead. As the reverberation tail hurts ASR the most, late reverberation is reduced via gain-based spectral subtraction. We use a multi-band decay model with an efficient method to update it in realtime. In reverberant environments the multi-channel version of the proposed algorithm reduces word error rates (WER) up to {{one half of the}} way between those of a microphone array only and a <b>close-talk</b> <b>microphone.</b> The four channel implementation requires less than 2 % of the CPU power of a modern computer...|$|R
30|$|The most {{important}} {{difference is the}} nature of the audio content used for the evaluation. In MediaEval evaluations all speech is telephone speech, either conversational or read and elicited speech. In our evaluation, the audio contains microphone recordings of real talks in real workshops, on large conference rooms with public. Microphones, conference rooms, and even recording conditions change from one recording to another. <b>Microphones</b> are not <b>close-talking</b> <b>microphones</b> but mainly tabletop and ground standing microphones. This difference in the evaluation conditions makes our evaluation to pose different challenges, and makes it difficult to compare the results obtained in our evaluation to previous MediaEval evaluations.|$|R
40|$|We {{investigate}} whether probabilistic modeling of prosody can aid various automatic labeling tasks essential for processing of multi-party meetings. Task 1, automatic punctuation, seeks to classify sentence boundaries and disfluencies. Task 2, jumpin points, predicts locations within foreground speech at which background speakers start talking; Task 3, jump-in words, examines {{characteristics of the}} speech they use to do so. Data are from the ICSI Meeting Recorder corpus. To infer inherent cues, analyses are based on <b>close-talking</b> <b>microphone</b> signals and recognizer forced alignments. As a generous baseline for word-level cues, we compare prosodic models to those of a language model given the true words. Results for Task 1 show prosody reduces classification error by 10 % relative over the cheating language model; furthermore when this task is run in "online" mode the prosodic model degrades less than does the language model. For Task 2, the language model provides no information, while the prosodic model reduces entropy by 13 % over chance. For Task 3, a prosodic model reduces entropy by 25 % over chance. Analyses also show interesting prosodic patterns, which differ over tasks. Task 1 uses cues similar to those for Switchboard (but not Broadcast News) data. Task 2 predicts jump-in points that look prosodically like sentence boundaries but that are not actually such boundaries. And Task 3 shows that speakers "raise" their voice when starting during another's talk, compared to starting during silence. These results provide evidence that prosodic modeling can be of use for the automatic processing of meetings. Further results and implications for future automatic meeting processing systems are discussed...|$|E
40|$|Hidden Markov Models (HMMs) {{have been}} used with {{considerable}} success in continuous speech recognition. It {{is well known that}} high accuracy can be obtained when the HMM system is trained and tested in a quiet environment and the speech signal is acquired from a <b>close-talking</b> <b>microphone.</b> However, mismatches between training and testing environment severely degrade erformance. Two major sources of mismatches are speaker and environment variability. Speaker variation is typically caused by di erent speaking styles and other physiological di erences between speakers, such asvocal tract lengths, etc. Environment variability includes channel distortion, such as that which a ects telephone speech, additive noise, and reverberation which results when the microphone is far away from the speaker. The goal of this report is to explore di erent adaptation algorithms that mitigate the e ects of speaker and environmental variability for speech recognition. The adaptation algorithm closely examined in this report is a Linguistic Tree based Maximum Likelihood Linear Regression (LT-MLLR). Speech Recognition experiments using the LT-MLLR for speaker and environment adaptation are given. It is shown that the LT-MLLR algorithm is superior to other adaptation algorithms discussed. For speaker adaptation, a 30 % reduction is achieved over the baseline word error rate (WER) using this algorithm. In addition it is shown that the use of Matched Filter Array Processing (MFA) with LT-MLLR reduces the WER of distant-talking speech with high reverberation. In the case when the reverberation time is as high as 0. 9 s, the WER is reduced from 57. 89 % to 19. 41 %, a reduction of 66. 47 %. [This work was supported by DARPA Contract DABT 63 - 93 -C- 0037. ] i...|$|E
30|$|In many {{practical}} applications, {{the observations}} collected by an array may be either mixed far-field and near-field signals or multiple far-field signals or multiple near-field sources. Most {{of the above}} techniques localize sources in far-field or near-field. In recent years, source localization in mixed near-field and far-field has been developed using MUSIC algorithm [5 – 7], ESPRIT-like technique [8], or sparse signal reconstruction method [4] based on a linear array. Jiang et al. proposed a 3 D source localization algorithm with a cross array [9]. First, the elevation angles are obtained based on the generalized ESPRIT method. Similar to the root MUSIC method, the range parameters are estimated with the elevation estimates. Finally, a MUSIC pseudo-spectrum function is used to get the azimuth angles with the elevation and range estimates. Due to the 3 D symmetrical structure, spherical arrays {{have been widely used}} in far-field source localization [23, 24]. A spherical microphone array was used in the near-field, and a new <b>close-talking</b> <b>microphone</b> array was proposed in [34]. It can adaptively compensate for the distance and orientation of a near-field source. Fisher and Rafaely presented a near-field spherical microphone array and defined the near-field criterion in terms of the array order and radius [35]. They analyzed spherical microphone array capabilities in the near-field and designed a radial filter discriminating the distances between the sources incident from the same direction [36]. Although the aforementioned work considers the near-field processing of the spherical microphone array, near-field or mixed field source localization via a spherical microphone array has not yet been studied. Based on the recurrent relation of the spherical harmonics, only DOAs were estimated for mixed field sources simultaneously [37]. However, how to distinguish near-field and far-field sources and how to estimate the ranges of near-field sources were not considered.|$|E
30|$|The basic {{training}} sets for our recognizer are dialogues from the Verbmobil project [18]. The {{topic of the}} recordings is appointment scheduling of normal speakers. The data were recorded with a <b>close-talk</b> <b>microphone</b> with 16 [*]kHz sampling frequency and 16 bit resolution. The speakers were from all over Germany and thus covered most dialect regions. However, {{they were asked to}} speak standard German. About 80 % of the 578 training speakers (304 males, 274 females) were between 20 and 29 years old; less than 10 % were over 40. This is important in view of the test data, because the fact that the average age of our test speakers was more than 60 years may influence the recognition results.|$|R
40|$|A {{scenario}} concerning hands-free connected digit {{recognition in}} a noisy office environment is investigated. An array of six omnidirectional microphones and a corresponding time delay compensation module {{are used to}} provide a beamformed signal as input to a Hidden Markov Model (HMM) based recognizer. Two different techniques of phone HMM adaptation have been considered, to reduce the mismatch between training and test conditions. Adaptation material and test material were collected in two different sessions. Results show that a digit accuracy close to 98 % can be achieved when the talker is at 1. 5 m distance from the array. This result has to be compared with 99. 5 % accuracy obtained by using a <b>close-talk</b> <b>microphone...</b>|$|R
40|$|Abstract—Microphone array {{systems have}} been an area of active {{research}} for several years. The potential for high quality hands-free speech acquisition in noisy and reflecting environments makes microphone arrays an attractive alternative to conventional <b>close-talking</b> <b>microphones.</b> The signal-enhancement and sourcelocation capabilities of microphone arrays make them applicable {{to a variety of}} tasks including teleconferencing, speaker tracking, speaker recognition and speech recognition. In this paper we evaluate techniques for setting up microphone arrays for speaker identification. We propose the use of an active noise canceling beamformer based on the generalized sidelobe canceller (GSC) beamformer. Significant improvements in identification rate are achieved using this method compared to other beamforming techniques investigated in this paper...|$|R
40|$|With the {{prevalence}} of the information age, privacy and personalization are forefront in today 2 ̆ 7 s society. As such, biometrics are viewed as essential components of current and evolving technological systems. Consumers demand unobtrusive and noninvasive approaches. In our previous work, we have demonstrated a speaker verification system that meets these criteria. However, there are additional constraints for fielded systems. The required recognition transactions are often performed in adverse environments and across diverse populations, necessitating robust solutions. There are two significant problem areas in current generation speaker verification systems. The first is the difficulty in acquiring clean audio signals (in all environments) without encumbering the user with a head-mounted <b>close-talking</b> <b>microphone.</b> Second, unimodal biometric systems do not work with {{a significant percentage of}} the population. To combat these issues, multimodal techniques are being investigated to improve system robustness to environmental conditions, as well as improve overall accuracy across the population. We propose a multimodal approach that builds on our current state-of-the-art speaker verification technology. In order to maintain the transparent nature of the speech interface, we focus on optical sensing technology to provide the additional modality–giving us an audio-visual person recognition system. For the audio domain, we use our existing speaker verification system. For the visual domain, we focus on lip motion. This is chosen, rather than static face or iris recognition, because it provides dynamic information about the individual. In addition, the lip dynamics can aid speech recognition to provide liveness testing. The visual processing method makes use of both color and edge information, combined within a Markov random field (MRF) framework, to localize the lips. Geometric features are extracted and input to a polynomial classifier for the person recognition process. A late integration approach, based on a probabilistic model, is employed to combine the two modalities. The system is tested on the XM 2 VTS database combined with AWGN (in the audio domain) over a range of signal-to-noise ratios...|$|E
40|$|A {{study of}} Matbat tone {{based on the}} data I am sending to you can be found in: B. Remijsen (2001) Word-prosodic systems of Raja Ampat languages. PhD diss. Leiden University - LOT Dissertation Series vol. 49. It is {{available}} online from www. papuaweb. org/dlib/s 123 /remijsen/_phd. pdf. I misinterpreted one phonological distinction, erroneously analysing it as allophonic. This mistake is corrected in: B. Remijsen (2007) Lexical tone in Magey Matbat. In V. J. van Heuven & E. van Zanten (eds.) Prosody in Indonesian languages. Utrecht: LOT Occasional Series vol. 9. Further information on the grammar of Matbat is available in: B. Remijsen (2010) Nouns and verbs in Magey Matbat. In M. C. Ewing & M. Klamer (eds.) Typological and Areal Analyses: Contributions from East Nusantara. Pacific Linguistics 618, 281 - 311. I failed to record permission / informed consent to publicise the speech materials themselves. So please do not publicise the speech materials themselves; {{but you can do}} quantitative analysis on this material and use it for your research, in the same way Roy Becker-Kristal did. In acknowledging the materials please refer to the above mentioned paper on Matbat tone (Remijsen 2001, 2007). RA 2 MB ===== ra 2 = Data collected on my 2 nd trip to Raja Ampat islands (January to March 2000); mb = Data on Matbat, collected in the village Magey, on the island Misol Recording and processing ========================== - Recordings were made with Sony Professional walkman and Shure SM 10 A headset-mounted <b>close-talking</b> <b>microphone,</b> in the village of Magey. - recorded and digitised using dolby C setting on recorder, tape NORM I, - digitised using line out on recorder, line in on pc - soundblaster: pci 128, with mixer rec. properties Aux slider at 'Aux' button under 'Record' - in aifc format - sampling freq 22050 - continued with left channel - chopped into sentence-sized files Filenames ========= naming convention of sound files: ra 2 mb_ _ _ _ elicitation series 1 : itemnumber 1 -> elicitation series 2 : itemnumber 21 -> elicitation series 3 : itemnumber 41 -> itemnumber - see ra 2 mb_data_forKristineYu. xls context = 1 : sentence-final position context = 2 : sentence-medial position speaker = 1 Ripka Jemput (WOMAN, AGE 29) speaker = 2 Lukas Botot (MAN, AGE 40) speaker = 3 Pelipus Hamui (MAN, AGE 35) speaker = 4 Antomina Jemput (WOMAN, AGE 30) speaker = 5 Kristina Hai (WOMAN, AGE 30) speaker = 6 Orpa Kapownon (WOMAN, AGE 32) speaker = 7 Mika Absalom (MAN, AGE 20) speaker = 8 Sem Jemput (MAN, AGE 40) As I processed the materials, I worked with a list of the items for each speaker - ls_ra 2 mb_sp 4. txt, ls_ra 2 mb_sp 5. txt, etc. These are the filenames without extension. These speaker-specific lists are collected - across speakers - in ls_ra 2 mb_col. txt. I took out of the speaker-specific lists and the collection list tokens that were problematic, e. g. because of a hestiation, or because a speaker used a different tone pattern from the intended one. I have a text file in which my comments on taking out an item are stated, but these notes are in Dutch. I attach it FYI; it is maybe up to 10 tokens per speaker that have been taken out. So I recommend that you use ls_ra 2 mb_col. txt as a listing of materials to be used. Segmentation ============ Codes: 1 : carrier before target word 4 : onset 5 : nucleus (6) : coda - optional (7) : weak vowel /(o) / - only found after Low fall toneme (9) : carrier after target word - optional: only present in medial context NB 1. In sentence-medial context, if the coda is a stop, and it is not realeased, then the silence before speech continues is segmented with coda 6. 2. Geminates are segmented halfway Pitch objects ============= I have checked the F 0 traces - individually by means of a script that I can make available...|$|E
30|$|The airbone {{database}} {{consists of}} 120 utterances read by six speakers – three male and three female – of the Austrian variety of German (Domes 2009). The utterances are {{recorded by the}} <b>close-talk</b> <b>microphone</b> of a headset with a sampling frequency of 16 kHz. The headset is further supplied with a bone conduction microphone, hence the name airbone database. The signal of the bone microphone, however, is not used in this work. The data is corrupted by AWGN and by car noise from the NOISEX- 92 database (Varga and Steeneken 1993) with consideration of the ASL. A subset of two utterances per speaker and SNR condition is used for development, i.e., for setting the kernel variance or for deriving the mapping function for estimating the kernel variance.|$|R
40|$|Far-field {{microphone}} speech signals cause high {{error rates}} for {{automatic speech recognition}} systems, due to room reverberation and lower signal-to-noise ratios. We have observed large increases in speech recognition word error rates when using a far-field (3 - 6 feet) microphone in a conference room, in comparison with recordings from <b>close-talking</b> <b>microphones.</b> In an earlier paper, we showed improvements in far-field speech recognition performance using a longterm log spectral subtraction method to combat reverberation. This method {{is based on a}} principle similar to cepstral mean subtraction but uses a much longer analysis window (e. g., 1 s) {{in order to deal with}} reverberation. Here we show that a combination of short-term noise filtering and longterm log spectral subtraction can further reduce recognition word error rates. 1...|$|R
40|$|In {{this paper}} the FAU IISAH corpus and its {{recording}} conditions are described: a new speech database consisting of human-machine and human-human interaction recordings. Beside <b>close-talking</b> <b>microphones</b> {{for the best}} possible audio quality of the recorded speech, far-distance microphones were used to acquire the interaction and communication. The recordings took place during a Wizard-of-Oz experiment in the intelligent, senior-adapted house (ISA-House). That is a living room with a speech controlled home assistance system for elderly people, based on a dialogue system, which is able to process spontaneous speech. During the studies in the ISA-House more than eight hours of interaction data were recorded including 3 hours and 27 minutes of spontaneous speech. The data were annotated under the aspect of human-human (off-talk) and human-machine (on-talk) interaction. 1...|$|R
30|$|The test {{database}} {{is composed}} of 330 read sentences (5, 353 words) from 8 different speakers. Fourteen different versions of this set are created. The first dataset is clean and is recorded with the same <b>close-talk</b> <b>microphone</b> as used while recording the training data. It is artificially corrupted by adding six types of noise to establish datasets 2 - 7 : car (set 2), babble (set 3), restaurant (set 4), street (set 5), airport (set 6), and train (set 7). Set 8 is recorded with far-talk microphones. Test sets 9 - 14 are created by artificially adding the same six types of noise as used for generating sets 2 - 7. Each test set contains 330 utterances and has an SNR that ranges from 5 to 15 dB.|$|R
