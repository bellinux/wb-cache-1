3048|615|Public
25|$|A <b>computational</b> <b>framework</b> was presented, {{based on}} {{statistical}} shape modelling, {{for construction of}} race-specific organ models for internal radionuclide dosimetry and other nuclear-medicine applications. The proposed technique used to create the race-specific statistical phantom maintains anatomic realism and provides the statistical parameters for application to radionuclide dosimetry.|$|E
25|$|A {{series of}} steps are {{involved}} in analysis of CAPP-Seq data from mutation detection to validation and open source software can {{do most of the}} analysis. After the first step of variant calling, germline and loss of heterozygosity (LOH) mutations are removed in CAPP-seq to reduce the background biases. Several statistical significance tests can be performed against background to all type of variant calling. For example, statistical significance of tumor-derived SNVs can be estimated by random sampling of background alleles using Monte Carlo method. For the indel calls, statistical significance is calculated applying a separate method that used a strand specific analysis by Z-test shown in previous work. Finally, a computational validation steps reduces the false positive calls. However, a robust <b>computational</b> <b>framework</b> specific for CAPP-seq data analysis is a high demand in this field.|$|E
2500|$|Generally, simple {{programs}} {{tend to have}} a {{very simple}} abstract framework. Simple cellular automata, Turing machines, and combinators are examples of such frameworks, while more complex cellular automata do not necessarily qualify as simple programs. It is also possible to invent new frameworks, particularly to capture the operation of natural systems. The remarkable feature of simple programs is that a significant percentage of them are capable of producing great complexity. Simply enumerating all possible variations of almost any class of programs quickly leads one to examples that do unexpected and interesting things. [...] This leads to the question: if the program is so simple, where does the complexity come from? In a sense, there is not enough room in the program's definition to directly encode all the things the program can do. Therefore, simple programs {{can be seen as a}} minimal example of emergence. A logical deduction from this phenomenon is that if the details of the program's rules have little direct relationship to its behavior, then it is very difficult to directly engineer a simple program to perform a specific behavior. An alternative approach is to try to engineer a simple overall <b>computational</b> <b>framework,</b> and then do a brute-force search through all of the possible components for the best match.|$|E
40|$|Abstract. The {{nature of}} scienti c {{programming}} is evolving to larger, composite applications that {{are composed of}} smaller element applications. These composite applications are more frequently being targeted for distributed, heterogeneous networks of computers. They are most likely programmed {{by a group of}} developers. Software component technology and <b>computational</b> <b>frameworks</b> are being proposed and developed to meet the programming requirements of these new applications. Historically, programming systems have had a hard time being accepted by the scienti c programming community. In this paper, a programming model is outlined that attempts to organize the software component concepts and fundamental programming entities into programming abstractions that will be better understood by the application developers. The programming model is designed to support <b>computational</b> <b>frameworks</b> that manage many of the tedious programming details, but also that allow su cient programmer control to design an accurate, high-performance application. Key words. software components, <b>computational</b> <b>frameworks,</b> scienti c applications, computational grids, distributed computing Subject classi cation. Computer Science 1. Focus. Programming e ciency has been a problem in the scienti c community for many years...|$|R
5000|$|Mission: CCB's {{mission is}} to develop {{modeling}} tools and theory for understanding biological processes and to create <b>computational</b> <b>frameworks</b> that will enable {{the analysis of the}} large, complex data sets being generated by new experimental technologies.|$|R
40|$|This paper {{examines}} {{the relationships between}} natural language and our attempts to codify and manage business knowledge in particular the normative statements that spawn all business decisions. I argue that such knowledge is inextricably bound to language and experience and that our conceptualizations, which ultimately constitute knowledge, are inseparable from language. Furthermore if we wish to develop a model of knowledge that can be represented computationally {{we need to understand}} the extent to which language can be mapped to its empirical content and or to sentences, which correlate with equivalent sentences. I argue that under current <b>computational</b> <b>frameworks</b> such a project is doomed due to inherent indeterminacies in terms and the concepts those terms seek to denote. I conclude thus that knowledge cannot be logically and systematically represented under our current and traditional <b>computational</b> <b>frameworks</b> 1...|$|R
50|$|The <b>computational</b> <b>framework</b> of label free {{approach}} includes detecting peptides, {{matching the}} corresponding peptides across multiple LC-MS data, selecting discriminatory peptides.|$|E
50|$|Apache Samza is an {{open-source}} near-realtime, asynchronous <b>computational</b> <b>framework</b> for {{stream processing}} {{developed by the}} Apache Software Foundation in Scala and Java.|$|E
5000|$|A <b>Computational</b> <b>Framework</b> to Infer Human Disease-Associated Long Noncoding RNAs. Liu MX, Chen X, Chen G, Cui QH, Yan GY. PLOS ONE 9.1 (2014) ...|$|E
40|$|The {{nature of}} {{scientific}} programming is evolving to larger, composite applications that {{are composed of}} smaller element applications. These composite applications are more frequently being targeted for distributed, heterogeneous networks of computers. They are most likely programmed {{by a group of}} developers. Software component technology and <b>computational</b> <b>frameworks</b> are being proposed and developed to meet the programming requirements of these new applications. Historically, programming systems have had a hard time being accepted by the scientific programming community. In this paper, a programming model is outlined that attempts to organize the software component concepts and fundamental programming entities into programming abstractions that will be better understood by the application developers. The programming model is designed to support <b>computational</b> <b>frameworks</b> that manage many of the tedious programming details, but also that allow sufficient programmer control to design an accurate, high-performance application...|$|R
30|$|The {{processing}} and analysis of big data often requires specialized <b>computational</b> <b>frameworks</b> and environments that utilize computing clusters and parallel algorithms. The more popular <b>computational</b> <b>frameworks</b> for working with big data include Apache Spark [16], MapReduce (original), and Apache Hadoop [46]. MapReduce generally divides the original dataset into subsets which are relatively easier to process, and then combines the multiple partial solutions that are obtained in determining the final outcome. Apache Hadoop is an open source implementation and variant of MapReduce. Apache Spark performs faster distributed computing of big data by using in-memory operations instead of the divide-and-conquer approach of MapReduce [16]. Although Spark can run on Hadoop, this is generally not a requirement. Apache Mahout, which contains various implementations of classification models, is an open-source machine-learning library that can run on Apache Hadoop or Apache Spark [46, 47]. It is a distributed linear algebra framework and mathematically expressive Scala programming language designed for quick implementation of algorithms.|$|R
40|$|Abstract. Argumentation {{theory is}} an {{interdisciplinary}} field studying how {{conclusions can be}} reached through logical reasoning. The notion of argument is completely general, including for example legal arguments, scientific arguments, and political arguments. Computational argumentation theory is studied {{in the context of}} artificial intelligence, and a number of <b>computational</b> argumentation <b>frameworks</b> have been put forward to date. However, {{there is a lack of}} concrete realisations of these frameworks, which hampersresearch andapplications at anumberoflevels. We hypothesise that the lack of suitable domain-specific languages in which to formalise argumentation frameworks is a contributing factor. In this paper, we present a formalisation of a particular <b>computational</b> argumentation <b>framework,</b> Carneades, as a case study with a view to investigate the extent to which functional languages are useful as a means to realising <b>computational</b> argumentation <b>frameworks</b> and reason about them...|$|R
5000|$|Electronic scale: Schroedinger {{equations}} {{are used}} in <b>computational</b> <b>framework</b> as density functional theory (DFT) models of electron orbitals and bonding on angstrom to nanometer scales.|$|E
5000|$|... 2009 Joseph Felsenstein (evolution), for revolutionizing {{population}} genetics, phylogenetic biology, and systematics {{by developing}} a sophisticated <b>computational</b> <b>framework</b> to deduce evolutionary relationships of genes and species from molecular data.|$|E
5000|$|In {{software}} engineering, {{the blackboard}} pattern is a behavioral design pattern {{that provides a}} <b>computational</b> <b>framework</b> for the design and implementation of systems that integrate large and diverse specialized modules, and implement complex, non-deterministic control strategies.|$|E
40|$|AtomSim, a {{collection}} of interfaces for computational crystallography simulations, has been developed. It uses forcefield-based dynamics through physics engines such as the General Utility Lattice Program, and can be integrated into larger <b>computational</b> <b>frameworks</b> such as the Virtual Neutron Facility for processing its dynamics into scattering functions, dynamical functions etc. It is also available as a Google App Engine-hosted web-deployed interface. Examples of a quartz molecular dynamics run and a hafnium dioxide phonon calculation are presented. ...|$|R
40|$|The {{computer}} simulation of complex phenomena is a challenging issue for studying their properties. Several models and techniques {{have been developed}} {{in order to provide}} useful conceptual and <b>computational</b> <b>frameworks.</b> The aim {{of this paper is to}} present a simulation model based on the Reaction-Diffusion Machine (RDM) and to discuss its implementation by a system based on multiagent and Object Oriented paradigms. The chemical extraction of substances in washing phenomena occurring in percolation processes has been considered as reference application. 1...|$|R
40|$|Carry Select Adder (CSLA) {{is one of}} the speedest adder {{utilized}} as a part {{of numerous}} <b>computational</b> <b>frameworks</b> to perform quick number-crunching operations. The Carry select adder utilizes an effective plan by imparting the Common Boolean logic (CLB) term. The modified CSLA architecture building design has created utilizing Binary to Excess- 1 converter (BEC). This paper introduces an unique method that replaces the BEC using common Boolean logic. Experimental analysis illustrates that the proposed architecture achieves advantages in terms of speed, area consumption and power. General term...|$|R
5000|$|Spatial Analysis: 3D city models {{provide the}} <b>computational</b> <b>framework</b> for 3D spatial {{analysis}} and simulation. For example, {{they can be}} used to compute solar potential for 3D roof surfaces of cities, visibility analysis within the urban space, noise simulation, thermographic inspections of buildings ...|$|E
50|$|A <b>computational</b> <b>framework</b> was presented, {{based on}} {{statistical}} shape modelling, {{for construction of}} race-specific organ models for internal radionuclide dosimetry and other nuclear-medicine applications. The proposed technique used to create the race-specific statistical phantom maintains anatomic realism and provides the statistical parameters for application to radionuclide dosimetry.|$|E
5000|$|Abductive logic {{programming}} is a <b>computational</b> <b>framework</b> that extends normal {{logic programming}} with abduction. It separates the theory [...] into two components, {{one of which}} is a normal logic program, used to generate [...] by means of backward reasoning, the other of which is a set of integrity constraints, used to filter the set of candidate explanations.|$|E
40|$|AbstractHow cells utilize {{intracellular}} spatial {{features to}} optimize their signaling characteristics {{is still not}} clearly understood. The physical distance between the cell-surface receptor and the gene expression machinery, fast reactions, and slow protein diffusion coefficients {{are some of the}} properties that contribute to their intricacy. This article reviews <b>computational</b> <b>frameworks</b> that can help biologists to elucidate the implications of space in signaling pathways. We argue that intracellular macromolecular crowding is an important modeling issue, and describe how recent simulation methods can reproduce this phenomenon in either implicit, semi-explicit or fully explicit representation...|$|R
40|$|AbstractIn {{this note}} we obtain, phrased in present day {{geometric}} and <b>computational</b> <b>frameworks,</b> the characteristic {{numbers of the}} family Unod of non-degenerate nodal plane cubics in P 3, first obtained by Schubert in his Kalkül der abzählenden Geometrie. The main geometric contribution is a detailed study of a variety Xnod, which is a compactification of the family Unod, including the boundary components (degenerations) and a generalization to P 3 of a formula of Zeuthen for nodal cubics in P 2. The computations {{have been carried out}} with the Wiris boost WIT...|$|R
40|$|Abstract How cells utilize {{intracellular}} spatial {{features to}} optimize their signaling characteristics {{is still not}} clearly understood. The physical distance between the cell-surface receptor and the gene expression machinery, fast reactions, and slow protein diffusion coefficients {{are some of the}} properties that contribute to their intricacy. This article reviews <b>computational</b> <b>frameworks</b> that can help biologists to elucidate the implications of space in signaling pathways. We argue that intracellular macromolecular crowding is an important modeling issue, and describe how recent simulation methods can reproduce this phenomenon in either implicit, semi-explicit or fully explicit representation...|$|R
5000|$|The unscented transform, {{especially}} {{as part of}} the UKF, has largely replaced the EKF in many nonlinear filtering and control applications, including for underwater, ground and air navigation, and spacecraft. [...] The unscented transform has also been used as a <b>computational</b> <b>framework</b> for Riemann-Stieltjes optimal control. [...] This computational approach is known as unscented optimal control.|$|E
50|$|Although pedometric mapping {{is mainly}} data-driven, {{it can also}} largely be based on use of expert knowledge. The expert knowledge, however, needs to be {{plugged-in}} into a pedometric <b>computational</b> <b>framework</b> {{so that it can}} be used to produce more accurate prediction models. For example, data assimilation techniques, such as the space-time Kalman filter, can be used to integrate pedogenetic knowledge and field observations.|$|E
50|$|QuakeSim is a NASA {{project for}} {{modeling}} earthquake fault systems. It {{was started in}} 2001 with NASA funding as a follow up to the General Earthquake Models (GEM) initiative. The multi-scale nature of earthquakes requires integrating data types and models to fully simulate and understand the earthquake process. QuakeSim is a <b>computational</b> <b>framework</b> for modeling and understanding earthquake and tectonic processes.|$|E
40|$|In {{this note}} we obtain, phrased in present day {{geometric}} and <b>computational</b> <b>frameworks,</b> the characteristic {{numbers of the}} family Unod of non–degenerate nodal plane cubics in P 3, first obtained by Schubert in his Kalk¨ul der abz¨ahlenden Geometrie. The main geometric contribution is a detailed study of a variety Xnod, which is a compactification of the family Unod, including the boundary components (degenerations) and a generalization to P 3 of a formula of Zeuthen for nodal cubics in P 2. The computations {{have been carried out}} with the OmegaMath intersection theory module WIT...|$|R
40|$|This book {{is unique}} in that its stress {{is not on the}} mastery of a {{programming}} language, but on the importance and value of interactive problem solving. The authors focus on several specific interest worlds: mathematics, computer science, artificial intelligence, linguistics, and games; however, their approach can serve as a model that may be applied easily to other fields as well. Those who are interested in symbolic computing will find that Interactive Problem Solving Using LOGO provides a gentle introduction from which one may move on to other, more advanced <b>computational</b> <b>frameworks</b> or mor...|$|R
5000|$|Van Loan's {{best known}} book is Matrix Computations, 3/e (Johns Hopkins University Press, 1996, [...] ), written with Gene H. Golub. He {{is also the}} author of Handbook for Matrix Computations (SIAM, 1988, [...] ), <b>Computational</b> <b>Frameworks</b> for the Fast Fourier Transform (SIAM, 1992, [...] ), Introduction to Computational Science and Mathematics (Jones and Bartlett, 1996, [...] ), and Introduction to Scientific Computation: A Matrix-Vector Approach Using MATLAB (2nd ed., Prentice-Hall, 1999, [...] ). His latest book is Insight Through Computing: A MATLAB Introduction to Computational Science and Engineering (SIAM, 2009, [...] ) written with K-Y Daisy Fan.|$|R
50|$|CellCognition {{is a free}} {{open-source}} <b>computational</b> <b>framework</b> for {{quantitative analysis}} of high-throughput fluorescence microscopy (time-lapse) images {{in the field of}} bioimage informatics and systems microscopy. The CellCognition framework uses image processing, computer vision and machine learning techniques for single-cell tracking and classification of cell morphologies. This enables measurements of temporal progression of cell phases, modeling of cellular dynamics and generation of phenotype map.|$|E
5000|$|Emergency Management: For emergency, risk, and {{disaster}} management systems, 3D city models provide the <b>computational</b> <b>framework.</b> In particular, they serve to simulate fire, floodings, and explosions For example, the DETORBA project aims at simulating and analyzing effects of explosion {{in urban areas}} at high precision to support prediction of effects for the structural integrity and soundness of the urban infrastructure and safety preparations of rescue forces.|$|E
50|$|When the {{rotation}} or stretch of a sub volume is large, errors can be {{introduced into the}} calculation of cell surface tractions since most TFM techniques employ a <b>computational</b> <b>framework</b> based on linear elasticity. Recent advances in TFM have shown that cells are capable of exerting deformations with strain magnitudes up to 40%, which requires usage of a finite deformation theory approach to account for large strain magnitudes.|$|E
40|$|We {{report about}} the {{inclusion}} of many-body electron interactions in the simulation of transport properties. We derive a general Landauer-like expression for the current, valid also {{in the case of}} conductors in which the charge carriers undergo generic scattering processes. An important focus is put on the derivation of the theoretical framework, both for the general formalism and for the actual implementation of the method, including the treatment of electronic correlation. We then show an example of application and compare the results on the electronic and conduction properties obtained with our new scheme to those given by alternative <b>computational</b> <b>frameworks...</b>|$|R
40|$|Automatic metaphor {{identification}} and interpretation in text have been traditionally considered as two separate tasks in {{natural language processing}} (NLP) and addressed individually within <b>computational</b> <b>frameworks.</b> However, cognitive evidence suggests that humans are likely to perform these two tasks simultaneously, {{as part of a}} holistic metaphor comprehension process. We present a novel method that performs metaphor identification through its interpretation, being the first one in NLP to combine the two tasks in one step. It outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage, as well as providing an interpretation for each identified expression. ...|$|R
40|$|Face of {{a person}} conveys a wealth of {{information}} about his /her attentive state. Particularly, head and eyes have the potential to derive where and at what the person is looking. Since humans primarily attend to objects of interest, knowledge of salient objects in the surrounding region can help to accurately infer the focus of visual attention of the person. We present novel <b>computational</b> <b>frameworks</b> and systems to infer visual attention by analyzing dynamics of head, eyes and salient objects. We evaluate proposed systems in intelligent automobile spaces with an emphasis on accurate, robust and continuous performance in the naturalistic driving condition...|$|R
