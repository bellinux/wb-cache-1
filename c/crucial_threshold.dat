21|22|Public
50|$|By early September 2015, 34 Senators had {{publicly}} confirmed {{support for}} the deal, a <b>crucial</b> <b>threshold</b> because it ensured that the Senate could sustain (i.e., uphold) any veto of a resolution of disapproval. Senator Barbara Mikulski of Maryland announced support on 2 September, a day after Chris Coons of Delaware and Bob Casey, Jr. of Pennsylvania also announced support, reaching 34 votes and assuring that an eventual disapproval resolution passed in the Senate could not override an Obama veto. By the following day, 38 Democratic senators supported the deal, 3 were opposed, and 5 were still undecided.|$|E
40|$|We {{propose a}} {{generalized}} extreme shock {{model with a}} possibly increasing failure threshold. Although standard models assume that the <b>crucial</b> <b>threshold</b> for the system might only decrease over time, because of weakening shocks and obsolescence, we assume that, especially {{at the beginning of}} the system's life, some strengthening shocks might increase the system tolerance to large shock. This is, for example, the case of turbines' running-in in the field of engineering. On the basis of parametric assumptions, we provide theoretical results and derive some exact and asymptotic univariate and multivariate distributions for the model. In the last part of the article we show how to link this new model to some nonparametric approaches proposed in the literatur...|$|E
30|$|Two {{arguments}} {{have been}} put forward that attempt to explain this paradox. The first view holds that as big banks become bigger, more efficient, and unchallenged, they tend to abuse their power to exploit customers by creating monopolistic practices (Mitchell and Onvural 1996). The second argument is that increased bank size beyond certain thresholds introduces diseconomies of scale that in turn lead to inefficiency. According to this view, larger banks find themselves with higher and higher average costs as they grow beyond a <b>crucial</b> <b>threshold,</b> and these costs lead to wider interest margins and inefficiency (Berger et al. 1987; Noulas et al. 1990; Mester 1992; Clark 1996; Karray and Chichti 2013). 3 In other words, contrary to expectations, increased size beyond the identified threshold would widen interest margins {{to the detriment of}} customers.|$|E
3000|$|... {{the precise}} {{information}} on parameter {{space for the}} finite state-dependent feedback control actions, <b>crucial</b> for designing <b>threshold</b> control strategies; [...]...|$|R
40|$|Staphylococcus aureus (S. aureus) is an {{important}} cause of infection both as community acquired and hospital acquired infection. Osteomyelitis is an invasive infectious disease {{that is hard to}} treat. It usually needs long periods of treatment. The most prevalent cause of osteomyelitis is S. aureus. The present study has been carried out with an aim to know the prevalence and antibiotic sensitivity pattern of S. aureus isolates of patients with osteomyelitis with E-test method. Our results showed high resistance of oxacillin, vancomycin, clindamycin, trimethoprim-sulfamethoxazole, tetracycline and ciprofloxacin. We can see that S. aureus {{is becoming more and more}} resistant to different antibiotics and when this resistance to the essential, last line of treatment antibiotics reaches to its <b>crucial</b> <b>thresholds,</b> it is too late to plan and make policies against irrational antibiotic use, especially in Iran. Â© IDOSI Publications, 2013...|$|R
40|$|Recurrence plots display binary {{texture of}} time series from {{dynamical}} systems with single dots and line structures. Using fuzzy recurrence plots, recurrences of the phase-space states can be visualized as grayscale texture, {{which is more}} informative for pattern analysis. The proposed method replaces the <b>crucial</b> similarity <b>threshold</b> required by symmetrical recurrence plots {{with the number of}} cluster centers, where the estimate of the latter parameter is less critical than the estimate of the former...|$|R
40|$|This paper {{analyzes}} {{a simple}} game with n players. Fix a mean in interval [0, 1] and let each player choose any random variable distributed on that interval with the given mean. The {{winner of the}} zero-sum game is the player whose random variable has the highest realization. We show that {{the position of the}} mean within the interval is crucial. Remarkably, if the given mean is above a <b>crucial</b> <b>threshold</b> then the equilibrium must contain a point mass on 1. The cutoff is strictly decreasing in the number of players, n; and for fixed μ, as the number of players is increased, each player places more weight on 1 at equilibrium. We also characterize the unique symmetric equilibrium of the game when the mean is sufficiently low...|$|E
40|$|Parkinson's disease (PD) {{patients}} display motor symptoms, e. g. tremor, rigidity and bradykinesia, {{largely due}} to a dramatic loss of dopaminergic neurons in the substantia nigra. Grafts of human embryonic dopamine neurons can survive in the striatum and reduce several of the motor symptoms. Several lines of evidence suggest that a <b>crucial</b> <b>threshold</b> of surviving dopaminergic neurons must be exceeded for the grafts to become functional and relieve symptoms. A {{relatively small number of}} operations have been performed so far. The major obstacle to large clinical trials has been that tissue from large numbers of donor embryos is needed for each patient. Thus, there is definitely a need for alternative sources of donor tissue for grafting in PD. Clearly various forms of stem cells are interesting options. This presentation will focus the possible future use of embryonic stem cells and bone marrow stem cells as donor tissue for transplantation in PD...|$|E
40|$|Hundreds of {{thousands}} of people have lost or been denied Social Security Disability Insurance and Supplemental Security Income disability benefits (SSD) in the last few years, as the Reagan Administration has endeavored to clear the disability rolls of people who are not disabled. In response, tens {{of thousands}} of actions have been filed by individual SSD claimants, as well as hundreds of class actions, challenging the legality of the denial and termination procedures utilized by the Secretary of Health and Human Services These actions raise the <b>crucial</b> <b>threshold</b> question of whether federal courts have jurisdiction in the absence of timely exhaustion of administrative remedies. To resolve this issue, courts balance the claimant 2 ̆ 7 s need for judicial review against the Secretary 2 ̆ 7 s interest in completing administrative review. In balancing these interests, courts address the separation of powers between the judiciary and the Social Security Administration...|$|E
50|$|Despite {{all their}} achievements, Kanzi and Panbanisha also have not {{demonstrated}} {{the ability to}} ask questions so far. Joseph Jordania suggested {{that the ability to}} ask questions could be the <b>crucial</b> cognitive <b>threshold</b> between human and ape mental abilities. Jordania suggested that asking questions {{is not a matter of}} the ability of using syntactic structures, that it is primarily a matter of cognitive ability. Questions can be asked without the use of syntactic structures, with the help of the question's intonation only, as is the case in children's early pre-linguistic development.|$|R
40|$|Wavelet {{transforms}} {{enable us}} to represent signals {{with a high degree}} of scarcity. Wavelet thresholding is a signal estimation technique that exploits the capabilities of wavelet transform for signal denoising. The aim of this paper is to study various thresholding techniques such as Sure Shrink, Visu Shrink and Bayes Shrink and determine the best one for image denoising. This paper presents an overview of various threshold methods for image denoising. Wavelet transform based denoising techniques are of greater interest because of their performance over Fourier and other spatial domain techniques. Selection of optimal <b>threshold</b> is <b>crucial</b> since <b>threshold</b> value governs the performance of denoising algorithms. Hence it is required to tune the threshold parameter for better PSNR values. In this paper, we present various wavelet based shrinkage methods for optimal threshold selection for noise removal...|$|R
40|$|FANOVA graphs have a {{broad range}} of {{application}} in model interpretation, sensitivity analysis and analysis of computer experiments. They give a clear and easy to understand visualization of function structures, reveal blockadditive decompositions and also effectively improve Kriging model predictions by kernel adaption. These several fields are combined in a new released R package fanovaGraph. This tutorial presents the implemented methods step by step together with theoretical background and illustrations. Additionally it includes methods in simulation and the <b>crucial</b> question of <b>threshold</b> decision, where graphical solutions are suggested...|$|R
40|$|Civic {{engagement}} {{and civil society}} are important issues in EU member {{states as well as}} on the European level. The chapter proposes to use the tools of government approach in order to identify currently emerging en-gagement policies. By comparing the application of policy instruments in these contexts, the chapter identifies central challenges for engagement policies. A <b>crucial</b> <b>threshold</b> is to gain an institutional foothold in govern-ment. However, the cross-departmental character of civil society-related policies emerges as a paradox: it has the potential to either facilitate or impede the promotion of engagement. When talking about European governance, one almost automatically thinks of gov-ernance in the European Union. That is an oversimplification, particularly in terms of policies related to civil society, a sphere that is traditionally seen as independent of the state, not to mention a supranational polity like the EU. Usually the state is thought of as the agent of policies. However, the governance paradigm has shown that other actors such as regions, municipalities, corporations, associations, NGOs...|$|E
40|$|The poliheuristic (PH) {{theory of}} {{decision}} making has made important contributions {{to our understanding of}} political decision making but remains silent about certain key aspects of the decision process. Specifically, PH theory contends that leaders screen out politically unacceptable options, but it provides no guidance on (1) the <b>crucial</b> <b>threshold</b> at which leaders reject options as politically unacceptable, (2) whether this threshold varies across leaders and situations, and, (3) if so, which factors shape variation in this threshold. We integrate PH theory with research on political leadership and decision context and derive hypotheses from this modified PH framework. An experimental test reveals that situational context and leadership style affect both (1) the ‘‘noncompensa-tory threshold’ ’ at which decision makers reject options as politically unacceptable and (2) how much decision makers rely on their constituents ’ views in making policy choices. We conclude that a modified PH theory incorporating these insights will have enhanced explanatory and predictive power...|$|E
40|$|Abstract. In {{a credit}} market with {{enforcement}} constraints, we study {{the effects of}} a change in the outside options of a potential defaulter on the terms of the credit contract, as well as on borrower payoffs. The results crucially depend on the allocation of “bargaining power ” between the borrower and the lender. We prove that there is a <b>crucial</b> <b>threshold</b> of relative weights such that if the borrower has power that exceeds this threshold, her expected utility must go up whenever her outside options come down. But if the borrower has less power than this threshold, her expected payoff must come down with her outside options. In the former case a deterioration in outside options brought about, say, by better enforcement, must create a Lorenz improvement in state-contingent consumption. In particular, borrower consumption rises in all “bad ” states in which loans are taken. In the latter case, in contrast, the borrower’s consumption must decline, at least for all the bad states. These disparate findings within a single model permit us to interpret existing literature on credit markets in a unified way...|$|E
40|$|Virginia Woolf dates the {{beginning}} of modernity “In or about December, 1910,” when “human character changed. ” This change appears first not in the writer’s study, nor the cosmopolitan metropole. It begins in the servants’ hall, when a cook leaves the kitchen and unexpectedly crosses the threshold to chat with her mistress in the drawing-room. This dissertation examines novels by four modernist women writers: Woolf, Gertrude Stein, Nella Larsen, and Jean Rhys. Their texts demonstrate that the influence of domesticity and domestic servants on modernist fiction both appears {{in the content of}} the novels and pervades their forms. Analyzing the depictions and deployments of domestic servants in modernist fiction reveals how the structure of modernist formal experimentation can be read as a reaction to, and as an often-uncomfortable negotiation with, those servants’ still-necessary presences in the house of fiction. ^ A new way of engaging with modernist fiction, and particularly with modernist fiction written by women, is at stake in this study. These writers’ encounters with the intersections of modernism, domesticity, and the labor of domestic servants lead to two types of structural innovations. One adopts some of the characteristics of servant labor into the shape of narrative, as seen in Woolf’s Mrs. Dalloway and To the Lighthouse and Stein’s Three Lives. The other, which surfaces in Larsen’s Passing and Rhys’s Wide Sargasso Sea, mirrors the central characters’ myopia and paranoia about the meaning and controllability of that labor. Both of these narrative types center on representations of and control over the space of the threshold, and the concept of the threshold centers my argument. The threshold as physical and psychological space takes on a new resonance in modernism, as seemingly stable divisions within personal and national spaces begin to shift under the pressure of modernity. Attention to liminality also refocuses attention on those servant characters who open doors, who stand at and cross these <b>crucial</b> <b>thresholds.</b> All four novelists recognize and dramatize {{the degree to which the}} employer class is dependent upon the labor and the loyalty of their servants. Their formal experiments reveal how each grapples with this dependence. ...|$|R
40|$|AbstractAs {{one of the}} top energy {{consumption}} country, China is under huge pressure of energy security, GHG emission control and environmental protection. Accounting for half of China energy-related emission, Chinese power sector especially need a quick development of clean generation technologies. However, the current developing speed is not enough and the <b>crucial</b> development <b>thresholds</b> of clean generation technologies are poorly understood. For these reasons, this study uses MESEIC (Multi-regional model for Energy Supply system and their Environmental ImpaCts) model to offer a suggested roadmap for Chinese power sector. Through different scenarios, this study sheds light on the penetration thresholds of various clean generation technologies and put forward policy suggestions in different time-scale for breaking out the thresholds. Setting solar power case as an example, the short-term threshold is its generating cost. The suggested solution is enacting promotional policies, such as feed-in tariffs (0. 45 RMB/kWh), renewable portfolio standards, renewable energy certificate trading, tax based incentives etc. The long-term threshold is geography limitation, which can be solved by improving the power grid transmission capacity and developing distributed solar energy...|$|R
40|$|Spectral {{bound and}} {{reproduction}} number for infinite-dimensional population structure and time heterogeneity. (English summary) SIAM J. Appl. Math. 70 (2009), no. 1, 188 – 211. Summary: “Spectral bounds of quasi-positive matrices are <b>crucial</b> mathematical <b>threshold</b> parameters in population models that are formulated as systems of ordinary differential equations: {{the sign of}} the spectral bound of the variational matrix at 0 decides whether, at low density, the population becomes extinct or grows. Another important threshold parameter is the reproduction number R, which is the spectral radius of a related positive matrix. As is well known, the spectral bound and R − 1 have the same sign provided that the matrices have a particular form. The relation between spectral bound and reproduction number extends to models with infinite-dimensional state space and then holds between the spectral bound of a resolvent-positive closed linear operator and the spectral radius of a positive bounded linear operator. We also extend an analogous relation between the spectral radii of two positive linear operators which is relevant for discrete-time models. We illustrate the general theory by applying it to an epidemic model with distributed susceptibility, population models with age structure, and, using evolution semigroups, to time-heterogeneous population models. ...|$|R
40|$|Cascading {{failures}} and epidemic dynamics, as two successful application realms of network science, are usually investigated separately. How do they affect {{each other is}} still one open, interesting problem. In this letter, we couple both processes {{and put them into}} the framework of interdependent networks, where each network only supports one dynamical process. Of particular interest, they spontaneously form a feedback loop: virus propagation triggers cascading failures of systems while cascading failures suppress virus propagation. Especially, there exists <b>crucial</b> <b>threshold</b> of virus transmissibility, above which the interdependent networks collapse completely. In addition, the interdependent networks will be more vulnerable if the network supporting virus propagation has denser connections; otherwise the interdependent systems are robust against the change of connections in other layer(s). This discovery differs from previous framework of cascading failure in interdependent networks, where better robustness usually needs denser connections. Finally, to protect interdependent networks we also propose the control measures based on the identification capability. The larger this capability, more robustness the interdependent networks will be...|$|E
40|$|The {{notion of}} fuzzy soft sets is a hybrid soft {{computing}} model that integrates both gradualness and parameterization methods in harmony {{to deal with}} uncertainty. The decomposition of fuzzy soft sets is of great importance in both theory and practical applications with regard to decision making under uncertainty. This study aims to explore decomposition of fuzzy soft sets with finite value spaces. Scalar uni-product and int-product operations of fuzzy soft sets are introduced and some related properties are investigated. Using t-level soft sets, we define level equivalent relations and show that the quotient structure of the unit interval induced by level equivalent relations is isomorphic to the lattice consisting of all t-level soft sets of a given fuzzy soft set. We also introduce the concepts of <b>crucial</b> <b>threshold</b> values and complete threshold sets. Finally, some decomposition theorems for fuzzy soft sets with finite value spaces are established, illustrated by an example concerning the classification and rating of multimedia cell phones. The obtained results extend some classical decomposition theorems of fuzzy sets, since every fuzzy set {{can be viewed as}} a fuzzy soft set with a single parameter...|$|E
40|$|In {{a credit}} market with {{enforcement}} constraints, we study {{the effects of}} a change in the outside options of a potential defaulter on the terms of the credit contract, as well as on borrower payoffs. The results crucially depend on the allocation of >bargaining power> between the borrower and the lender. We prove that there is a <b>crucial</b> <b>threshold</b> of relative weights such that if the borrower has power that exceeds this threshold, her expected utility must go up whenever her outside options come down. But if the borrower has less power than this threshold, her expected payoff must come down with her outside options. In the former case a deterioration in outside options brought about, say, by better enforcement, must create a Lorenz improvement in state-contingent consumption. In particular, borrower consumption rises in all >bad> states in which loans are taken. In the latter case, in contrast, the borrower's consumption must decline, at least for all the bad states. These disparate findings within a single model permit us to interpret existing literature on credit markets in a unified way. © 2006 Elsevier B. V. All rights reserved. Ray acknowledges support from the National Science Foundation under grant no. 0241070. Peer Reviewe...|$|E
40|$|This paper tackles {{the problem}} of non-equispaced data {{smoothing}} using the non-linear, non-parametric estimation technique of wavelet thresholding. Wavelets on irregular grids can be constructed, using the so-called lifting scheme. The lifting scheme is grid-adaptive in that it provides smooth basis functions on the given data set. We demonstrate however that this scheme pays {{little attention to the}} numerical condition of the transform. This bad condition originates from the fact that a straightforward application of a lifting scheme on irregular point sets mixes up scales within one level of transformation. Experiments illustrate that stability is a <b>crucial</b> issue in <b>threshold</b> or other shrinking algorithms. Unstable transforms blow up the bias in the smoothing curve. We propose a scale-adaptive scheme that avoids scale-mixing, leading to a smooth and close fit...|$|R
40|$|Saltation threshold, {{the minimum}} wind speed for {{sediment}} transport, {{is a fundamental}} parameter in aeolian processes. The presence of liquid, such as water on Earth or methane on Titan, may affect the threshold values to a great extent. Sediment density is also <b>crucial</b> for determining <b>threshold</b> values. Here we provide quantitative data on density and water content of common wind tunnel materials {{that have been used}} to study conditions on Earth, Titan, Mars, and Venus. The measured density values for low density materials are higher compared to literature values, whereas for the high density materials, there is no such discrepancy. We also find that low density materials have much higher water content and longer atmospheric equilibration timescales compared to high density sediments. In the Titan Wind Tunnel, we performed threshold experiments with the standard walnut shells (125 - 150 μ m, 7. 2...|$|R
40|$|Early-warning models most {{commonly}} optimize signaling thresholds on crisis probabilities. The expost threshold optimization {{is based upon}} a loss function accounting for preferences between forecast errors, but comes with two <b>crucial</b> drawbacks: unstable <b>thresholds</b> in recursive estimations and an in-sample overfit {{at the expense of}} out-of-sample performance. We propose two alternatives for threshold setting: (i) including preferences in the estimation itself and (ii) setting thresholds ex-ante according to preferences only. Given probabilistic model output, it is intuitive that a decision rule is independent of the data or model specification, as thresholds on probabilities represent a willingness to issue a false alarm vis- 0 -vis missing a crisis. We provide simulated and real-world evidence that this simplification results in stable thresholds and improves out-of-sample performance. Our solution is not restricted to binary-choice models, but directly transferable to the signaling approach and all probabilistic early-warning models...|$|R
40|$|We {{argue that}} {{in the context of}} biology-inspired {{problems}} in computer science, in addition to studying the time com-plexity of solutions {{it is also important to}} study the selec-tion complexity, a measure of how likely a given algorithmic strategy is to arise in nature. In this spirit, we propose a selection complexity metric χ for the ANTS problem [Fein-erman et al. ]. For algorithm A, we define χ(A) = b + log `, where b is the number of memory bits used by each agent and ` bounds the fineness of available probabilities (agents use probabilities of at least 1 / 2 `). We consider n agents searching for a target in the plane, within an (unknown) distance D from the origin. We iden-tify log logD as a <b>crucial</b> <b>threshold</b> for our selection com-plexity metric. We prove a new upper bound that achieves near-optimal speed-up of (D 2 /n + D) · 2 O(`) for χ(A) ≤ 3 log logD + O(1), which is asymptotically optimal if ` ∈ O(1). By comparison, previous algorithms achieving simi-lar speed-up require χ(A) = Ω(logD). We show that this threshold is tight by proving that if χ(A) < log logD−ω(1), then with high probability the target is not found if each agent performs D 2 −o(1) moves. This constitutes a sizable gap to the straightforward Ω(D 2 /n+D) lower bound...|$|E
40|$|Is {{research}} into whistleblowing the same {{all over the}} world? Can we even say that ‘whistleblowing’ occurs all over the world, in every country or every society, {{in a way that}} is so similar that it can be researched everywhere the same way? The answer to these broad questions is almost certainly ‘no’. As discussed in Chapter 1, the term ‘whistleblowing’ itself has its own origins in particular societies and particular cultures, where there are many ways of understanding it, even before trying to cross into other languages and cultures. How the concept is perceived in different cultures is a <b>crucial</b> <b>threshold</b> question. Yet, at the same time there is good reason to believe that everywhere, in all societies, humanity functions using conceptions of right and wrong conduct. As social animals, humanity everywhere organizes itself in groups, and increasingly, in organizations and institutions, albeit in a myriad of ways and for different social, economic and political purposes. Whatever the mode of human organization, we know that the processes by which perceived wrongdoing comes to light and is dealt with are inherently important (for a definition of wrongdoing see Skivenes and Trygstad, Chapter 4). And so, we search for research approaches which acknowledge and explain the different ways that this occurs, in order to know whether or how the reporting or disclosure of wrongdoing is manifested, how it compares, what its implications are, and how it is best understood, encouraged or managed. No Full Tex...|$|E
40|$|The Supersymmetric SO(10) theory ("NMSO(10) GUT") {{based on}} the 210 + 126 + Higgs system {{proposed}} in 1982 {{has evolved into a}} realistic theory capable of fitting the known low energy Particle Physics data besides providing a Dark matter candidate and embedding Inflationary Cosmology. It dynamically resolves longstanding issues such as fast dimension five operator mediated proton decay in Susy GUTs by allowing explicit and complete calculation of <b>crucial</b> <b>threshold</b> effects at M_Susy and M_GUT in terms of fundamental parameters. This shows that SO(10) Yukawas responsible for observed fermion masses as well as operator dimension 5 mediated proton decay can be highly suppressed on a "Higgs dissolution edge" in the parameter space of GUTs with rich superheavy spectra. This novel and generically relevant result highlights the need for every realistic UV completion model with a large/infinite number of heavy fields coupled to the light Higgs doublets to explicitly account for the large wave function renormalization effects on emergent light Higgs fields in order to be considered a quantitatively well defined candidate UV completion. The NMSGUT predicts large soft Susy breaking trilinear couplings and distinctive sparticle spectra. Measurable or near measurable level of tensor perturbations- and thus large Inflaton mass scale- may be accommodated by Supersymetric Seesaw inflation within the NMSGUT based on an LHN flat direction Inflaton if the Higgs component contains contributions from heavy Higgs components. Successful NMSGUT fits suggest a renormalizable Yukawon Ultra minimal gauged theory of flavor based upon the NMSGUT Higgs structure. Comment: 20 Pages, 1 Fig., PdfLatex. Proceedings of UNICOS 2014. References and historical comments adde...|$|E
2500|$|In mid-1986 the SPÖ-FPÖ {{coalition}} under Franz Vranitzky collapsed {{when the}} junior coalition partner in government, the Freedom Party of Austria, elected the controversial Jörg Haider as their new [...] party leader. Vranitzky determined that his {{government would not}} stand with Haider as Vice-Chancellor and early elections were called for November 23, 1986. The pressure for the green movement to present a united front for the election increased, with Günther Nenning attempting to bring together the BIP, VGÖ and GRAS parties onto a single candidates list. Meissner-Blau emerged as {{the most popular and}} uniformly acceptable candidate to lead the list, and she once again assumed leadership of the campaign under the banner The Green Alternative - List Freda Meissner-Blau. Meissner-Blau proved to be a popular and unifying figure in the movement, however she and Nenning were ultimately unsuccessful at uniting all the dissident factions. On October 4, GRAS - the largest green movement not yet committed to running under the Meissner-Blau list - voted on whether to join the main list or run separately under another leader. The left-wing dominated GRAS was heavily divided on the prospect of the moderate Meissner-Blau's leadership, and the final ballot saw left-wing historian Andrea Komlosy receive 222 votes to Meissner-Blau's 150. The loss came as a shock and a considerable disappointment to Nenning and Meissner-Blau. Yet in spite of this the majority of the green movement continued to rally behind Meissner-Blau's leadership, and a strong campaign was made which presented {{for the first time a}} professional and viable political alternative to the Austrian public. The 1986 election would prove to be a major upset in traditional Austrian politics with both major parties (the SPÖ and ÖVP) losing seats to the smaller Greens and FPÖ. The Greens polled 4.8% (234,028 votes), taking them over the <b>crucial</b> 4% <b>threshold,</b> and 8 candidates (including Meissner-Blau) were elected to the Austrian National Council. The alternative green lists run by GRAS and disaffected segments of the VGÖ captured only around 7000 votes, or 0.1%.|$|R
40|$|A {{combined}} {{analysis of}} the reactions π^-p→ K^ 0 Λ and η n is carried out with a chiral quark model. The data in the center-of-mass (c. m.) energy range from threshold up to W≃ 1. 8 GeV are reasonably described. For π^-p→ K^ 0 Λ, {{it is found that}} N(1535) S_ 11 and N(1650) S_ 11 paly <b>crucial</b> roles near <b>threshold.</b> The N(1650) S_ 11 resonance contributes to the reaction through configuration mixing with N(1535) S_ 11. The constructive interference between N(1535) S_ 11 and N(1650) S_ 11 is responsible for the peak structure around threshold in the total cross section. The n-pole, u- and t-channel backgrounds provide significant contributions to the reaction as well. While, for the π^-p→η n process, the "first peak" in the total cross section is dominant by N(1535) S_ 11, which has a sizeable destructive interference with N(1650) S_ 11. Around P_π≃ 1. 0 GeV/c (W≃ 1. 7 GeV), {{there seems to be a}} small bump structure in the total cross section, which might be explained by the interference between the u-channel and N(1650) S_ 11. The N(1520) D_ 13 resonance notably affects the angle distributions of the cross sections, although less effects are seen in the total cross section. The role of P-wave state N(1720) P_ 13 should be further confirmed by future experiments. If N(1720) P_ 13 has a narrow width of Γ≃ 120 MeV as found in our previous work by a study of the π^ 0 photoproduction processes, obvious evidence should be seen in the π^-p→ K^ 0 Λ and η n processes as well. Comment: 12 pages, 8 figure...|$|R
40|$|A novel wavelet seismic {{denoising}} method using type II fuzzyDOI: 10. 1016 /j. asoc. 2016. 06. 024 Link: [URL] Filiació URV: SIWavelet based denoising of {{the observed}} non stationary time series earthquake loading has become an important process in seismic analysis. The process of denoising ensures a noise free seismic data, which is essential to extract features accurately (max acceleration, max velocity, max displacement, etc.). However, the efficiency of wavelet denoising is decided by the identification of a <b>crucial</b> factor called <b>threshold.</b> But, identification of optimal threshold is not a straight forward process as the signal involved is non-stationary. i. e. The information which separates the wavelet coefficients that correspond to the region of interest from the noisy wavelet coefficients is vague and fuzzy. Existing works discount this fact. In this article, we have presented an effective denoising procedure that uses fuzzy tool. The proposal uses type II fuzzy concept in setting the threshold. The need for type II fuzzy instead of fuzzy is discussed in this article. The proposed algorithm is compared with four current popular wavelet based procedures adopted in seismic denoising (normal shrink, Shannon entropy shrink, Tsallis entropy shrink and visu shrink). It was first applied on the synthetic accelerogram signal (gaussian waves with noise) to determine the efficiency in denoising. For a gaussian noise of sigma = 0. 075, the proposed type II fuzzy based denoising algorithm generated 0. 0537 {{root mean square error}} (RMSE) and 16. 465 signal to noise ratio (SNR), visu shrink and normal shrink could be able to give 0. 0682 RMSE with 14. 38 SNR and 0. 068 RMSE with 14. 2 SNR, respectively. Also, Shannon and Tsallis generated 0. 0602 RMSE with 15. 47 SNR and 0. 0610 RMSE with 15. 35 SNR, respectively. The proposed method is then applied to real recorded time series accelerograms. It is found that th...|$|R
40|$|Montreal, QC H 3 A 1 B 1, Canada {{recognition}} {{occurs when}} maternal S-alleles match those {{expressed in the}} haploid or diploid life stages of pollen parents, respectively. In heteromorphic SI systems, incom-Review population density; for example, as can occur when individual reproductiveplants that share recognition alleles at this locus are incapable of successfully producing offspring [5]. In homo-SI systems have long been studied by plant evolutionary biologists because they strongly influence the distribution of genetic variation in populations [2, 3]. SI probably evolved early during the diversification of angiosperms, and is found in {{nearly half of all}} flowering plant species [4]. Despite the variation among SI systems, most share one fundamental property: SI is controlled by a linked cluster of genes collectively known as the ‘S-locus’, and individual success declines at low population densities owing to difficulty in encounter-ing mates. An Allee effect can lead to extinction below a <b>crucial</b> <b>threshold</b> density. Genetic drift: random fluctuations in allele frequency across generations in finite populations. The expected change in the frequency of an allele caused by random sampling is inversely proportional to the population size. Inbreeding depression: reduced offspring fitness as a result of breeding between related parents. Inbreeding depression is a major force maintaining SI systems in plants. Negative frequency dependent selection: mode of selection whereby the relative fitness of an allele or genotype decreases (increases) as it becomes more (less) prevalent in the population. Because this mode of selection favors variants when they are uncommon, it is synonymous with ‘rare advantage’. Mate limitation in SI taxa Allee effect: positive correlation between population growth rate andThe evolution of se when mates are li...|$|E
40|$|The aim of {{the study}} was to {{investigate}} the hypothesis that women using depogestagens and low-dose oral contraceptives (Ethinylýstradiol ? 20 ýg) carry a higher risk of postmenopausal osteoporosis. The use of these hormonal contraceptives results in marked decrease of estradiol serum level, which suggest a negative impact on bone metabolism with long-term effects on peak bone mass and premenopausale bone density. The present study contributed to clarifying this hypothesis since the influence of progestins of different partial effects on bone density in adult female rats were investigated. The first experiment detected for the first time the relationship between 17 -? -estradiol substitution doses, 17 -? -estradiol-serum level and the decrease of bone density in adult ovariectomised rats. 52 three month old female rats were randomised into six groups. Five groups were ovariectomised and one group SHAM-operated. Four groups received 17 -? -estradiol-substitution over 28 days as follows: 0, 5 ýg/ rat * day 0, 3 ýg/ rat * day 0, 1 ýg/ rat * day 0, 05 ýg/ rat * day. Along with rapid decrease of 17 -? -estradiol serum level, the OVX-control rats showed a 30 % reduction of bone density. Osteocalcin was significantly increased. The oestrogen treatment of the ovariectomised groups influenced the bone density and 17 -? -estradiol-serum levels in a dose-dependent manner. In the substitution groups of 0, 5 ýg and 0, 3 ýg/ rat*day the bone density was not changed. The 17 -? -estradiol -serum concentration reached nearly constant levels for the first group at 140 pmol/l and for the last at 60 pmol/l. Contrary to these findings, bone resorption increased in the 0, 1 ýg and 0, 05 ýg/ rat*day treated groups at about 10 % and 14 %. The average 17 -? -estradiol -serum levels ranged between 50 and 32 pmol/l. From this experiment it is suggested that the serum estradiol level of 50 pmol/l is a <b>crucial</b> <b>threshold</b> value for bone density in female rats. As the 17 -? -estradiol -serum levels fell below this level, negative effects on bone density were observed. On the other hand, serum estradiol levels above 50 pmol/l suppress bone resorption and work bone protective. In a second experiment, the minimal ovulation inhibition dose for the applied progestins (Levonorgestrel, Medroxyprogesteronacetat und Promegeston) were determined. Three doses for each progestin were tested. According to this experiment and internally performed trials at the Schering AG the following doses for application in the third experiment were scheduled: Levonorgestrel ý 50 ýg/ rat and day Medroxyyprogesteronactat ý 500 ýg/ rat and day Promegeston ý 50 ýg/ rat and day In the third experiment, the influence of progestin treatment in an ovulation inhibiting dose over a period of seven weeks on bone density was investigated. 47 three-month-old female rats were randomised into five groups. Three groups, each with eight rats, received the three mentioned progestins in ovulation inhibiting dose. One group with 16 rats received vehicle and served as an intact control group. One group with seven rats was ovariectomised. At the end of the experiment none of the progestin treated groups showed a significant change of bone density compared to the intact control group. However, the bone density decreased by 30 % in the ovariectomised group, as had been expected. The 17 -? -estradiol -serum level of the progestin treated groups reached 80 pmol/l. In the ovariectomised group 30 pmol/l was determined. These results confirm the findings of the first experiment where a <b>crucial</b> <b>threshold</b> value for bone density preservation at 50 pmol/l was found. 17 -? -estradiol serum levels higher than 50 pmol/l are bone protective. None of the investigated progestins leads to a decrease of the 17 -? -estradiol serum levels below this limit. These findings suggest that negative effects on the bone density of the investigated progestins given in an ovulation inhibiting dose to adult rats can be excluded. In this context, no influence of an androgen partial effect on bone density and bone structure could be observed...|$|E
40|$|Data {{collection}} for {{digital elevation model}} (DEM) generation {{can be carried out}} by two main methods in space-borne remote sensing such as stereoscopy using optical or radar satellite imagery (stereophotogrammetry, respectively radargrammetry) and interferometry based on interferometric synthetic aperture radar (InSAR) data. These techniques have advantages and disadvantages in comparison against each other. Especially filling the gaps which arise from the problem of cloud coverage in DEM generation by optical imagery, InSAR became operational in recent years and DEMs became the most demanded interferometric products. Essentially, in comparison, DEM generation from synthetic aperture radar (SAR) images is not a simple manner like generation from optical satellite imagery. Interferometric processing has several complicated steps for the production of a DEM. The quality of the data set and used software package come into prominence for the stability of the generated DEM. In the paper, the interferometric processing steps for DEM generation from InSAR data and the <b>crucial</b> <b>threshold</b> values are tried to be explained. For DEM generation, a part of Istanbul (historical peninsula and near surroundings) was selected as the test field because of data availability. The data sets of two different imaging modes (StripMap 3 m resolution and High Resolution Spotlight 1 m resolution) of TerraSAR-X have been used. At the implementation, besides the determination of crucial points at interferometric processing steps, to define the effect of computer software, DEM production have been performed using two different software packages in parallel and the products have been compared In the result section of the paper, besides the colorful visualizations of final products along with the height scales, accuracy evaluations have been performed for both DEMs {{with the help of a}} more accurate reference digital terrain model (DTM). This reference model has been achieved by large scale aerial photos. Normally, it has a 5 m original grid spacing, however it has been resampled at a spacing of 1 m towards the needs of the research...|$|E
30|$|In recent years, OPV devices {{enhanced}} by incorporated metallic NP have {{become one of}} the most intensively studied topic in OPV research [1, 2]. The basic principle behind NP-incorporated OPV devices is the application of the LSPR phenomenon in metallic NP to improve the light trapping and absorption of small molecule and polymer OPV devices which typically have very thin (~ 100  nm) absorber layers [3]. The LSPR is a collective oscillation of surface conduction electrons driven by an incident electromagnetic field at the resonance frequency [4, 5]. As a result of the LSPR, the PCE of the plasmonic OPV device can be (but not necessarily always) higher than a conventional BHJ OPV device made from the same donor and acceptor materials. The technique has attracted such interest because the discovery of new hole conducting (p-type) and especially electron conducting (n-type) organic semiconductors has slowed somewhat in recent years. The highest certified PCE for OPV devices at present is 11.5 % [6, 7]. In the research literature, a current highest PCE of 10.5 % was reported for a low bandgap polymer OPV device [8]. Based on the existing set of organic semiconductors and BHJ device architecture, the PCE is unlikely to significantly exceed the <b>crucial</b> 10 % <b>threshold</b> required for renewable energy application of OPV [9]. It is important to point out at the outset that the introduction of metallic NPs does not alter basic operating principle of OPV devices. The photovoltaic energy conversion still follows the well-established mechanism comprising: (1) sun light absorption, (2) exciton diffusion/dissociation, (3) carrier transport and (4) carrier collection [10, 11]. The metallic NPs primarily enhance the efficiency of the first step so that more excitons are generated within the BHJ layer.|$|R
40|$|Lowered pH {{conditions}} in aquatic environments {{can have a}} negative impact on many aquatic organisms. In fish it has been shown that acidic water conditions may result in altered gill morphology, reduced reproductive success, changes in locomotor and feeding behavior and even death. Juveniles and adults have been the primary focus of research. However, the larval stage is potentially the most sensitive to toxicity. It is also important ontogenetically as several <b>crucial</b> development <b>thresholds,</b> such as first feeding, occur during this period. The impact of acute low pH on feeding performance was investigated in larval fathead minnows one to fifteen days post-hatching. Each day, replicate sets of five larvae were exposed to each of three pH concentrations (4. 5, 5 - 5, and 7. 7) under various environmental conditions: (1) light conditions with live prey (LL), (2) light conditions with dead prey (LD), (3) dark conditions with live prey (DL) and (4) dark conditions with dead prey (DD). Each treatment regime was designed to isolate different sensory modalities available for prey capture. Temporal patterns in locomotor activity were examined qualitativeely using LOWESS and number of larvae feeding across days were analyzed using ANCOVA. Growth rates per day were statistically the same and within normal ranges for each batch of fish. Levels of locomotor activity were uncorrelated with feeding rates and were qualitatively different early vs. late in the larval period when food was present. There was no clear pattern in locomotor activity in the absence of food. pH had a significant negative impact on feeding success, although the nature of this effect varied with respect to test conditions. LD and DD treatments displayed heterogeneity of slopes among pH levels; at pH 4. 5, the rate of increase in feeding activity across days was significantly depressed relative to other pH levels. LL and DL treatments were characterized by significant intercept differences among pH conditions; larvae exposed to pH 4. 5 showed decreased feeding efficiencies relative to those at higher pH. Mechanoreception appeared to play a significant role in prey capture at pH 7. 7, while photoreception contributed little. The removal of any sensory modality seemed to have an impact at pH 5 - 5, but the degree of impact depended on the sense that was eliminated. pH 4. 5 appeared to strongly inhibit chemoreception; the data also suggest close coupling of chemoreception and mechanoreception in feeding. While it is evident that prey capture and feeding success in larval fish is complex, these data show clear impact of pH on feeding performance. These results point to the necessity for studies of acidification effects on larvae and suggest an approach by which aspects of sensory integration may be dissected...|$|R
30|$|Epistasis is a {{major factor}} {{underlying}} quantitative traits (Caicedo et al. 2004). In our study, potential epistatic interactions between marker loci identified on 10 different chromosomes revealed 49 significant digenic interactions (2 interactions for E 1, 31 for E 2, and 16 for E 3) through single environment analysis for SES score, shoot length, root length, shoot fresh weight, and shoot dry weight, with quite a wide range of PVE. But, we are not elaborating on this as our focus is on QTL interactions with environments; hence, we worked with multi-environment analysis. The ME analysis illustrated 17 marker intervals resulting in 30 two-way interactions (Table 3; Fig. 3); these interactions significantly influenced the traits, suggesting that epistasis is {{an important component of the}} genetic basis for complex traits, including tolerance of salt stress. Individually, all the complementary/background markers reported in Table 3 had no significant effect on the trait alone (otherwise, they would have mapped as reliable large-effect QTLs), but resulted in an enhanced effect when combined and interacting with QTLs and other markers. However, only one significant digenic interaction each was identified for SL, two interactions for SES score, and three interactions for FWsht, as several large-effect QTLs were detected for these traits. Some researchers have suggested the presence of significant epistatic interactions among QTL-linked or -unlinked markers (Cocherham and Zeng 1996; Eshed and Zamir 1996; Li et al. 1997; Hossain et al. 2015). So, there is a need to assess the importance of epistatic gene interactions as this complicates the genotype-phenotype relationships; in addition, different computing models used in analyzing epistasis vary and give different results (Malmberg et al. 2005). The interaction effect may enhance or reduce the expected manifestation of overall QTL effect depending upon the degree and direction of the interaction. Type II interactions (between complementary loci) had relatively higher PVE than type I interactions, probably because the trait manifests itself only through interaction between two complementary loci as no main-effect QTL is involved. The influence of epistatic QTL interactions alone explained the trait variation, ranging from 2.1 % to 14.1 %, which could be <b>crucial</b> when the <b>threshold</b> limits for salinity tolerance of a variety are to be enhanced to withstand environmental stress.|$|R
