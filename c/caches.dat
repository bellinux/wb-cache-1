8487|10000|Public
5|$|All nuthatches {{appear to}} store food, {{especially}} seeds, in tree crevices, in the ground, under small stones, or behind bark flakes, and these <b>caches</b> are remembered {{for as long}} as 30 days. Similarly, the rock nuthatches wedge snails into suitable crevices for consumption in times of need. European nuthatches have been found to avoid using their <b>caches</b> during benign conditions in order to save them for harsher times.|$|E
5|$|Although Half-Life 2 has {{the largest}} {{proportion}} of Achievements, there are 99 spread across all five games, exceeding the 50-Achievement limit that Microsoft maintains to feature the most Achievements of any Xbox 360 product. These Achievements include killing {{a certain number of}} monsters, finding hidden weapon <b>caches,</b> or other tasks specific to each game.|$|E
5|$|For the {{remainder}} of 1929, the party prepared for the revolt. They located and manufactured weapons, storing them in hidden depots. The preparation was hindered by French police, particularly the seizure of arms <b>caches.</b>|$|E
5000|$|Though {{initially}} {{proposed by}} Joupii to improve <b>cache</b> {{performance of a}} direct-mapped <b>cache</b> Level 1, modern day microprocessors with multi-level <b>cache</b> hierarchy employ Level 3/ Level 4 <b>cache</b> to act as victim <b>cache</b> for the <b>cache</b> lying above it in the memory hierarchy. Intel's Crystal Well of its Haswell processors introduced an on-package Level 4 <b>cache</b> {{which serves as a}} victim <b>cache</b> to processor's Level 3 <b>cache.</b> A 4 - 12 MB Level 3 <b>cache</b> is used as a victim <b>cache</b> in POWER5(IBM) microprocessors.|$|R
40|$|<b>Cache</b> {{becomes very}} {{important}} in high-load computer application. In a web application, <b>cache</b> can improve the performance of application by several orders of magnitude generally. The article analyzes the role of <b>cache</b> in software application level optimization in detail and divides <b>cache</b> into local <b>cache,</b> local shared-memory <b>cache,</b> distributed memory <b>cache,</b> disk <b>cache.</b> Several <b>cache</b> control policies including alive time, invalid in writing, invalid in reading et. al are studied and the problems when using <b>cache</b> in applications are pointed out. Key word...|$|R
40|$|Our broader {{goal is to}} {{incorporate}} <b>cache</b> shar-•Concurrent access to <b>cache</b> is limited, causing •Occurs when frequently missing in the <b>cache.</b> currently in pairs on a shared <b>cache</b> multiproc-ing effects into software performance models. timing penalties without extra <b>cache</b> misses. •Flushing dirty <b>cache</b> lines increases pressure. essor. •Existing <b>cache</b> sharing models such as [2, 3] deal •Experiment: artificial interfering workload hit-•Experiment: comparing slowdown due to inter-•What performance changes can we expect com-with <b>cache</b> misses, not timing penalties. ting or missing in the shared <b>cache</b> with multi-fering workload missing either in shared <b>cache,</b> pared to isolated execution? ple outstanding requests and negligible compe-or in another <b>cache</b> sharing only memory bus. •Competing for <b>cache</b> capacity causes extra <b>cache</b> Our Goals tition for <b>cache</b> storage capacity. misses. •Is the increase of execution time proportional to the increase of <b>cache</b> misses? Execution time added by interference [s...|$|R
5|$|Other seeds, such as apple pips and plum stones, have fleshy {{receptacles}} {{and smaller}} fruits like hawthorns have seeds enclosed in edible tissue; animals including mammals and birds eat the fruits and either discard the seeds, or swallow {{them so they}} pass through the gut to be deposited in the animal's droppings {{well away from the}} parent tree. The germination of some seeds is improved when they are processed in this way. Nuts may be gathered by animals such as squirrels that cache any not immediately consumed. Many of these <b>caches</b> are never revisited, the nut-casing softens with rain and frost, and the seed germinates in the spring. Pine cones may similarly be hoarded by red squirrels, and grizzly bears may help to disperse the seed by raiding squirrel <b>caches.</b>|$|E
5|$|Rodents have {{advanced}} cognitive abilities. They can quickly learn to avoid poisoned baits, {{which makes them}} difficult pests to deal with. Guinea pigs can learn and remember complex pathways to food. Squirrels and kangaroo rats are able to locate <b>caches</b> of food by spatial memory, rather than just by smell.|$|E
5|$|A {{symmetric}} multiprocessor (SMP) is {{a computer}} system with multiple identical processors that share memory and connect via a bus. Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32processors. Because of {{the small size of}} the processors and the significant reduction in the requirements for bus bandwidth achieved by large <b>caches,</b> such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.|$|E
5000|$|CPU <b>cache</b> design: An {{exclusive}} <b>cache</b> {{design means}} that {{contents of the}} L1 <b>cache</b> is not duplicated in the L2 <b>cache,</b> providing a larger total <b>cache.</b>|$|R
50|$|In a Fully {{associative}} <b>cache,</b> the <b>cache</b> {{is organized}} {{into a single}} <b>cache</b> set with multiple <b>cache</b> lines. A memory block can occupy any of the <b>cache</b> lines. The <b>cache</b> organization can be framed as (1*m) row matrix.|$|R
50|$|Data is {{transferred}} between memory and <b>cache</b> in blocks of fixed size, called <b>cache</b> lines or <b>cache</b> blocks. When a <b>cache</b> line is copied from memory into the <b>cache,</b> a <b>cache</b> entry is created. The <b>cache</b> entry {{will include the}} copied data {{as well as the}} requested memory location (called a tag).|$|R
5|$|After the {{initiation}} of U.S. naval interdiction efforts in coastal waters, known as Operation Market Time, the trail had to do double duty. Materiel sent from the north was stored in <b>caches</b> in the border regions that were soon retitled Base Areas, which, in turn, became sanctuaries for NLF and PAVN forces seeking respite and resupply after conducting operations within South Vietnam.|$|E
5|$|At Copán in Honduras, another {{important}} Maya site, archaeologists found a potbelly sculpture {{on top of}} the Northwest Platform, {{to the west of the}} Great Plaza. Another was found in a cache under Stela 4. Further potbelly monuments have been found in <b>caches</b> under Late Classic stelae in the Great Plaza itself and throughout the Copán valley.|$|E
5|$|Betances also {{promoted}} {{direct intervention}} of Puerto Ricans in the Cuban independence struggle, which eventually {{happened in the}} Cuban War of Independence (1895–98). Spain had promoted political reform in Puerto Rico, and the local political climate was not conducive to a second revolution at the time. Therefore, Betances and the Puerto Rican revolutionaries ceded their <b>caches</b> of firearms hidden in Saint Thomas, Curaçao and Haiti to the Cuban rebels in October 1871, since their struggle was deemed as a priority.|$|E
50|$|The precise {{effect of}} {{inlining}} on <b>cache</b> performance is complicated. For small <b>cache</b> sizes (much {{smaller than the}} working set prior to expansion), the increased sequentiality dominates, and inlining improves <b>cache</b> performance. For <b>cache</b> sizes close to the working set, where inlining expands the working set so it no longer fits in <b>cache,</b> this dominates and <b>cache</b> performance decreases. For <b>cache</b> sizes larger than the working set, inlining has negligible impact on <b>cache</b> performance. Further, changes in <b>cache</b> design, such as load forwarding, can offset the increase in <b>cache</b> misses.|$|R
5000|$|<b>Cache</b> enhancements: Montecito added a split L2 <b>cache,</b> which {{included}} a dedicated 1 MB L2 <b>cache</b> for instructions. The original 256 KB L2 <b>cache</b> was converted to a dedicated data <b>cache.</b> Montecito also included up to 12 MB of on-die L3 <b>cache.</b>|$|R
40|$|Current <b>cache</b> designs support only {{fixed line}} size. Fixed <b>cache</b> line size limits <b>cache's</b> ability in spatial/temporal {{locality}} utilization. In this report, {{we present a}} <b>cache</b> design with adaptive line size, where an individual <b>cache</b> line can change size dynamically based on the spatial locality of the miss fetched data. For this <b>cache,</b> the spatial locality {{of a line of}} data in the <b>cache</b> can be detected by its access pattern during its stay in the <b>cache.</b> Then on replacement from the <b>cache,</b> the optimal line size for a line of data is determined by the detected spatial locality and is recorded in the memory. Overall, a better performance is achieved by this novel <b>cache</b> design. Simulations of SPEC 95 benchmarks show that this <b>cache</b> design can significantly reduce <b>cache</b> miss rates. A 2 -level adaptive line size (ALS) <b>cache</b> can achieve an average speedup of 1. 7 over a 2 -level fixed line size (FLS) <b>cache.</b> A 128 k direct-mapped L 2 ALS <b>cache</b> outperforms a 512 K 2 -way set-associative L 2 FLS <b>cache...</b>|$|R
5|$|Despite its name, the Valley of the Kings also {{contains}} the tombs of favorite nobles {{as well as}} the wives and children of both nobles and pharaohs. Therefore, only about 20 of the tombs actually contain the remains of kings. The remains of nobles and of the royal family, together with unmarked pits and embalming <b>caches,</b> make up the rest. Around the time of Ramesses I (ca. 1301 BC) construction commenced in the separate Valley of the Queens.|$|E
5|$|From arms <b>caches</b> {{hidden in}} 1939, the Home Army obtained: 614 heavy machine guns, 1,193 light machine guns, 33,052 rifles, 6,732 pistols, 28 antitank light field guns, 25 antitank rifles, and 43,154 hand grenades. However, {{due to their}} {{inadequate}} preservation, which had had to be improvised in {{the chaos of the}} September Campaign, most of the guns were in poor condition. Of those that had been buried in the ground and had been dug up in 1944 during preparations for Operation Tempest, only 30% were usable.|$|E
5|$|Structure 9 {{is a large}} mound on the {{northwest}} side of Plaza 1. The last phase of construction shows evidence of the interruption of construction by the Spanish Conquest. Terraces were absent on three sides of the structure with only the lowest level having been built on the fourth. Early Classic ceramic <b>caches</b> were discovered under Structure 9.|$|E
40|$|The <b>cache</b> {{replacement}} algorithm plays a {{very important}} role in the overall effectiveness of a cooperative proxy <b>cache</b> system. In this paper, we study new <b>cache</b> replacement strategies in a self-configured, self-managed P 2 P proxy <b>cache</b> system. In this proposed P 2 P proxy <b>cache</b> system, cooperative proxies possess only local knowledge of the entire <b>cache</b> system and some limited global statistical information. Designing an efficient <b>cache</b> replacement algorithm (a global predicate) using only local knowledge is more challenging. We propose a Criteria Weighted <b>cache</b> The <b>cache</b> replacement algorithm to manage the <b>cache</b> space in each individual proxy. The aggregate effect of <b>cache</b> replacement in individual proxies effectively optimizes the overall <b>cache</b> space in the entire <b>cache</b> system. Furthermore, we refine the proposed algorithm by pushing the replacement victim (the last replica known to current proxy) to one of its neighbors. We compare our protocols with some popular <b>cache</b> replacement algorithms using extensive simulations. The results show that the proposed CW and CW-VP <b>cache</b> replacement algorithms improve the <b>cache</b> hit ratio and average request latencies compared to the other algorithms...|$|R
50|$|When the {{processor}} needs {{to read or}} write a location in main memory, it first checks for a corresponding entry in the <b>cache.</b> The <b>cache</b> checks for {{the contents of the}} requested memory location in any <b>cache</b> lines that might contain that address. If {{the processor}} finds that the memory location is in the <b>cache,</b> a <b>cache</b> hit has occurred. However, if the processor does not find the memory location in the <b>cache,</b> a <b>cache</b> miss has occurred. In the case of a <b>cache</b> hit, the processor immediately reads or writes the data in the <b>cache</b> line. For a <b>cache</b> miss, the <b>cache</b> allocates a new entry and copies data from main memory, then the request is fulfilled from the contents of the <b>cache.</b>|$|R
40|$|Problem statement: Multi-core {{trends are}} {{becoming}} dominant, creating sophisticated and complicated <b>cache</b> structures. One {{of the easiest}} ways to design <b>cache</b> memory for increasing performance is to double the <b>cache</b> size. The big <b>cache</b> size {{is directly related to}} the area and power consumption. Especially in mobile processors, simple increase of the <b>cache</b> size may significantly affect its chip area and power. Without increasing the size of the <b>cache,</b> we propose a novel method to improve the overall performance. Approach: We proposed a composite <b>cache</b> mechanism for 1 and L 2 <b>cache</b> to maximize <b>cache</b> performance within a given <b>cache</b> size. This technique could be used without increasing <b>cache</b> size and set associatively by emphasizing primary way utilization and pseudo-associatively. We also added victim <b>cache</b> to composite pseudo associative <b>cache</b> for further improvement. Results: Based on our experiments with the sampled SPEC CPU 2006 workload, the proposed <b>cache</b> mechanism showed the remarkable reduction in <b>cache</b> misses without affecting the size. Conclusion/Recommendation: The variation of performance improvement depends on benchmark, <b>cache</b> size and set associatively, but the proposed scheme shows more sensitivity to <b>cache</b> size increase than set associatively increase...|$|R
5|$|Technical {{difficulties}} {{included the}} very high transistor counts needed to support the wide instruction words and the large <b>caches.</b> There were also structural problems within the project, as the {{two parts of the}} joint team used different methodologies and had slightly different priorities. Since Merced was the first EPIC processor, the development effort encountered more unanticipated problems than the team was accustomed to. In addition, the EPIC concept depends on compiler capabilities that had never been implemented before, so more research was needed.|$|E
5|$|The ARVN {{operation}} soon {{settled down}} to become a search and destroy mission, with South Vietnamese troops combing the countryside in small patrols looking for PAVN supply <b>caches.</b> Phase II of the operation began {{with the arrival of}} elements of the 9th Infantry Division. Four tank-infantry task forces attacked into the Parrot's Beak from the south. After three days of operations, ARVN claimed 1,010 PAVN troops had been killed and 204 prisoners taken for the loss of 66 ARVN dead and 330 wounded.|$|E
5|$|The {{game was}} {{developed}} by Ocean Software, {{in which they were}} renowned for creating video games related to their respective films. Navy SEALS focuses on Hawkins recovering <b>caches</b> of Stinger missiles from Arab soldiers in the Middle East. The game received positive reviews upon release, with critics mainly praising the graphics, presentation and challenging gameplay. However, criticism was directed at the ZX Spectrum port, which received disapproval over its monochrome graphics.|$|E
30|$|HCC: It is a caching {{insertion}} policy {{based on}} the social relations with the mobile user. The <b>cache</b> space {{is divided into three}} levels: namely self <b>cache</b> space, friends’ <b>cache</b> space and strangers’ <b>cache</b> space. The self <b>cache</b> space is used for the mobile user to store the data items according to his/her preference. The friends’ <b>cache</b> space is remained to help his/her friends to <b>cache</b> some data items, and the strangers’ <b>cache</b> space is for the mobile user to store <b>cache</b> data items randomly.|$|R
40|$|Abstract. <b>Cache</b> of a {{persistent}} data structure represents an important part, which significantly influence its efficiency. <b>Cache</b> consists from {{an array of}} main mem-ory blocks (caled <b>cache</b> nodes) with a constant size. <b>Cache</b> nodes buffer the data structure nodes hence they can be accessed quickly. However, data struc-ture nodes do not usually fully utilize the whole main memory block. Therefore, the constant <b>cache</b> node size the waste of the main memory. In this article, we propose the solution where the <b>cache</b> consists from the <b>cache</b> nodes with a dif-ferent size. <b>Cache</b> is split into several <b>cache</b> rows. The data structure nodes with the similar size are stored in one <b>cache</b> row. Such a solution has two problems: (1) that the some <b>cache</b> row has to replace the nodes too often during the query-ing (<b>cache</b> row contention), and (2) that the data structure nodes has to be moved between the <b>cache</b> rows during the insert and the delete operation. In our experi-mental section we show {{that the effect of}} the <b>cache</b> row contention is minimized if we set the <b>cache</b> row size according to the data structure node distribution. We also discuss a solution of the node <b>cache</b> moves problem. Key words: <b>Cache</b> utilization, flexible <b>cache...</b>|$|R
30|$|The dual-access {{prediction}} mechanism {{used in the}} DAWP <b>cache</b> yields {{high accuracy}} since the scaled-up index table and the fully associative <b>cache</b> are used. Unlike the WP <b>cache,</b> the performance gain of the DAWP <b>cache</b> (against the conventional <b>cache)</b> increases with increasing set associativity of the <b>cache.</b>|$|R
5|$|When {{exploiting}} distant {{food sources}} found in clearings, grey jays were observed temporarily concentrating their <b>caches</b> in an arboreal site {{along the edge}} of a black spruce forest in interior Alaska. This allowed a high rate of caching in the short term and reduced the jay's risk of predation. A subsequent recaching stage occurred, and food items were transferred to widely scattered sites to reduce theft.|$|E
5|$|With the French forces {{withdrawn}} from the operation by the late summer of 1953, Viet Minh Regiment 95 re-infiltrated Route One and resumed ambushes of French convoys, retrieving weapons <b>caches</b> missed by the French forces. Regiment 95 occupied the area {{for the remainder of}} the First Indochina War and were still operating there as late as 1962 against the South Vietnamese Army during the Second Indochina, or Vietnam War.|$|E
5|$|Agouti {{species are}} one of the few animal groups that can break open the large {{capsules}} of the Brazil nut fruit. Too many seeds are inside to be consumed in one meal, so the agouti carries some off and <b>caches</b> them. This helps dispersal of the seeds as any that the agouti fails to retrieve are distant from the parent tree when they germinate. Other nut-bearing trees tend to bear a glut of fruits in the autumn. These are too numerous to be eaten in one meal and squirrels gather and store the surplus in crevices and hollow trees. In desert regions, seeds are often available only for short periods. The kangaroo rat collects all it can find and stores them in larder chambers in its burrow.|$|E
30|$|When the FFT size {{is greater}} than the {{allocated}} <b>cache</b> size, the considered radix- 22 scheme may present some inefficiency toward the L 1 D <b>cache.</b> The L 1 D <b>cache</b> is composed of a number of <b>cache</b> lines (usually of 64  bytes), used to store external data prior to their use by the CPU. A <b>cache</b> miss occurs when the requested data is not yet available into the cache; in this case, the CPU is stalled waiting for a <b>cache</b> line to be updated. Many <b>cache</b> structures were used in CPU architectures: direct-mapped <b>cache</b> associates each address of the external memory with a unique position into the <b>cache</b> (therefore with one <b>cache</b> line); as a result, two addresses that are separated by a multiple of the <b>cache</b> size could not survive into the <b>cache</b> at the same time. An advanced mechanism is called the set-associative <b>cache,</b> where the <b>cache</b> memory is divided into a number of p ways, such that when a <b>cache</b> miss occurs, data is transferred to the way whose <b>cache</b> line is the least recently used (LRU); consequently, there are p unique locations into the <b>cache</b> for every address. The main advantage for increasing the associativity is to let non-contiguous data survive into <b>cache</b> lines without overwriting each other (without <b>cache</b> thrashes).|$|R
50|$|In a banked <b>cache,</b> the <b>cache</b> {{is divided}} into {{instruction}} <b>cache</b> and data <b>cache.</b> In contrast, a unified <b>cache</b> contains both the instructions and data combined in the same <b>cache.</b> During a process, the upper-level <b>cache</b> is accessed to get the instructions to the processor in each cycle. The <b>cache</b> will also be accessed to get data. Requiring both actions to be implemented {{at the same time}} requires multiple ports in the <b>cache</b> and as well as takes more access time. Having multiple ports requires additional hardware and wiring leading to huge structure. Therefore, the L1 <b>cache</b> is organized as a banked <b>cache</b> which results in fewer ports, less hardware and low access time.|$|R
40|$|The speed {{gap between}} the {{processor}} and disks is becoming a serious problem in computer systems today. To rectify this problem, various caching techniques have been traditionally used. For example, the UNIX operating system has a built-in <b>cache</b> called the buffer <b>cache</b> to keep recently used disk blocks in main memory. Another example is the so-called disk <b>cache</b> in I/O processors and intelligent disk controllers. These two types of <b>cache</b> form a two-level disk <b>cache</b> hierarchy where the buffer <b>cache</b> is the primary <b>cache</b> and the disk <b>cache</b> the secondary <b>cache.</b> In such a two-level <b>cache</b> hierarchy, unless special care is taken, {{it is likely that}} the disk blocks kept in one <b>cache</b> are also kept in the other <b>cache</b> resulting in reduced total caching capacity. In this paper, we describe the design and implementation of a unified <b>cache</b> management algorithm called the push-pull algorithm that tries to avoid such double caching. In the proposed algorithm, a new disk block is stored in only one <b>cache</b> [...] ...|$|R
