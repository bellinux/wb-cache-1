1|5837|Public
40|$|Abstract Several {{forms of}} {{synaptic}} plasticity in the neocortex and hippocampus {{depend on the}} temporal coincidence of presynaptic activity and postsynaptic trains of action potentials (APs). This requirement {{is consistent with the}} Hebbian, or correlational, type of <b>cellular</b> <b>learning</b> <b>rule</b> used in many studies of associative synaptic plasticity. Recent experimental evidence suggests that APs initiated in the axosomatic area are actively back-propagated to the dendritic arborization of neocortical and pyramidal cells. High-frequency trains of postsynaptic APs that are used as conditioning stimuli for the induction of Hebbian-like plasticity in both neocortical and hippocampal pyramidal cells display attenuation of the dendritic AP amplitude during the train. This attenuation {{has been shown to be}} modulated by neurotransmitters and by electrical activity. We suggest here that both spike train attenuation in the dendrite and its modulation b...|$|E
40|$|How {{learning}} and memory is {{achieved in the}} brain is a central question in neuroscience research. Key to today’s research into information storage in the brain {{is the concept of}} synaptic plasticity, a notion that has been heavily influenced by Donald Hebb’s 1949 postulate. Hebb conjectured that repeatedly and persistently coactive cells should increase connective strength among populations of interconnected neurons as a means of storing a memory trace, also known as an engram. Hebb certainly was not the first to make such a conjecture, as we show in this history. Nevertheless, literally thousands of studies into the classical frequency-dependent paradigm of <b>cellular</b> <b>learning</b> <b>rules</b> were directly inspired by the Hebbian postulate. But in more recent years, a novel concept in <b>cellular</b> <b>learning</b> has emerged, where temporal order instead of frequency is emphasized. This new learning paradigm — known as Spike-Timing-Dependent Plasticity, or STDP — has rapidly gained tremendous interest, perhaps because of its combination of elegant simplicity, biological plausibility, and computational power. But what are the roots of today’s STDP concept? Here, we discuss several centuries of diverse thinking, beginning with philosophers such as Aristotle, Locke and Ribot, traversing e. g. Lugaro’s plasticit&# 224; and Rosenblatt’s Perceptron, and culminating with the discovery of STDP. We highlight interactions between theoretical and experimental fields, showing how discoveries sometimes occurred in parallel, seemingly without much knowledge of the other field, and sometimes via concrete back-and-forth communication. We point out where the future directions may lie, which includes interneuron STDP, the functional impact of STDP, its mechanisms and its neuromodulatory regulation, and the linking of STDP to the developmental formation and continuous plasticity of neuronal networks...|$|R
40|$|Abstract — The minimum vertex {{cover of}} a given graph G {{is a set of}} {{vertices}} such that every vertex in G belongs either to the set or adjacent to vertices of the covering set. Finding the minimum vertex cover in an arbitrary graph is NP-Complete and several approximation algorithms have been proposed for solving this problem in graphs. In this paper, <b>cellular</b> <b>learning</b> automata based algorithm is proposed for solving the minimum vertex cover problem. In this algorithm each vertex as a cell is equipped with a learning automaton that has been influenced by adjacent cells and <b>learning</b> <b>rule.</b> This approach gradually reaches near optimal solution for minimum vertex cover and reduces the number of cells as covering set. Taking advantage of parallel implementation, proposed algorithm reduces running time in most of instances. The proposed algorithm is tested on DIMACS benchmark graphs and is compared with other well-known algorithms. Experimental results show that proposed algorithm uniformly performs efficiently and gives better results over other methods. Keywords- <b>cellular</b> <b>learning</b> automata; minimum vertex cover problem; NP-Complete problem; irregular <b>cellular</b> <b>learning</b> automata I...|$|R
40|$|<b>Cellular</b> <b>learning</b> automata is a {{combination}} of <b>cellular</b> automata and <b>learning</b> automata. The synchronous version of <b>cellular</b> <b>learning</b> automata in which all learning automata in different cells are activated synchronously, has found many applications. In some applications a type of <b>cellular</b> <b>learning</b> automata in which learning automata in different cells are activated asynchronously (asynchronous <b>cellular</b> <b>learning</b> automata) is needed. In this paper, we introduce asynchronous <b>cellular</b> <b>learning</b> automata and study its steady state behavior. Then an application of this new model to cellular networks has been presented. c ○ 2008 Published by Elsevier Lt...|$|R
40|$|<b>Cellular</b> <b>learning</b> automata is a {{combination}} of <b>learning</b> automata and <b>cellular</b> automata. This model is superior to <b>cellular</b> <b>learning</b> automata because of its ability to learn and also is superior to single learning automaton because it is a collection of learning automata which can interact together. In some applications such as image processing, a type of <b>cellular</b> <b>learning</b> automata in which the action of each cell in the next stage of its evolution not only depends on the local environment (actions of its neighbors) but it also depends on the external environments. We call such a <b>cellular</b> <b>learning</b> automata as open <b>cellular</b> <b>learning</b> automata. In this paper, we introduce open <b>cellular</b> <b>learning</b> automata and then study its steady state behavior. It is shown that for a class of rules called commutative <b>rules,</b> the open <b>cellular</b> <b>learning</b> automata in stationary external environments converges to a stable and compatible configuration. Then the application of this new model to image segmentation has been presented. <b>Cellular</b> automata, <b>learning</b> automata, <b>cellular</b> <b>learning</b> automata, dynamical systems...|$|R
40|$|The <b>cellular</b> <b>learning</b> automata, {{which is}} a {{combination}} of <b>cellular</b> automata, and <b>learning</b> automata, is a new recently introduced model. This model is superior to cellular automata because of its ability to learn and is also superior to a single learning automaton because it is a collection of learning automata which can interact with each other. The basic idea of <b>cellular</b> <b>learning</b> automata, {{which is a}} subclass of stochastic <b>cellular</b> <b>learning</b> automata, is to use the learning automata to adjust the state transition probability of stochastic cellular automata. In this paper, we first provide a mathematical framework for <b>cellular</b> <b>learning</b> automata and then study its convergence behavior. It is shown that for a class of rules, called commutative <b>rules,</b> the <b>cellular</b> <b>learning</b> automata converges to a stable and compatible configuration. The numerical results also confirm the theoretical investigations. <b>Cellular</b> <b>learning</b> automata, <b>cellular</b> automata, <b>learning</b> automata, interconnected automata...|$|R
40|$|The <b>cellular</b> <b>learning</b> automata, {{which is}} a {{combination}} of <b>cellular</b> automata and <b>learning</b> automata, is introduced recently. This model is superior to cellular automata because of its ability to learn and also is superior to single learning automaton because it is a collection of learning automata which can interact with each other. The basic idea of <b>cellular</b> <b>learning</b> automata is to use learning automata to adjust the state transition probability of stochastic cellular automata. Recently, various types of <b>cellular</b> <b>learning</b> automata such as synchronous, asynchronous, and open <b>cellular</b> <b>learning</b> automata have been introduced. In some applications such as cellular networks {{we need to have a}} model of <b>cellular</b> <b>learning</b> automata for which multiple learning automata resides in each cell. In this paper, we study a <b>cellular</b> <b>learning</b> automata model for which each cell has several learning automata. It is shown that for a class of rules, called commutative <b>rules,</b> the <b>cellular</b> <b>learning</b> automata converges to a stable and compatible configuration. Two applications of this new model such as channel assignment in cellular mobile networks and function optimization are also given. For both applications, it has been shown through computer simulations that the <b>cellular</b> <b>learning</b> automata based solutions produce better results...|$|R
40|$|Abstract. <b>Cellular</b> <b>learning</b> automata is a {{combination}} of <b>learning</b> automata and <b>cellular</b> automata. This model is superior to <b>cellular</b> <b>learning</b> automata because of its ability to learn and also is superior to single learning automaton because it is a collection of learning automata which can interact together. In some applications such as image processing, a type of <b>cellular</b> <b>learning</b> automata in which the action of each cell in next stage of its evolution not only depends on the local environment (actions of its neighbors) but it also depends on the external environments. We call such a CLA as open <b>cellular</b> <b>learning</b> automata. In this paper, we introduce open <b>cellular</b> <b>learning</b> automata and then study its steady state behavior. It is shown that for class of rules called commutative <b>rules,</b> the open <b>cellular</b> <b>learning</b> automata in stationary external environments converges to a stable and compatible configuration. Then the application of this new model to image segmentation has been presented...|$|R
40|$|Abstract. The <b>cellular</b> <b>learning</b> automata, {{which is}} a {{combination}} of <b>cellular</b> automata and <b>learning</b> automata, is introduced recently. This model is superior to cellular automata because of its ability to learn and also is superior to single learning automaton because it is a collection of learning automata which can interact with each other. The basic idea of <b>cellular</b> <b>learning</b> automata is to use the learning automata to adjust the state transition probability of stochastic cellular automata. Recently, various types of <b>cellular</b> <b>learning</b> automata such as synchronous, asynchronous, and open <b>cellular</b> <b>learning</b> automata have been introduced. In some applications such as cellular networks {{we need to have a}} model of <b>cellular</b> <b>learning</b> automata for which multiple learning automata resides in each cell. In this paper, we study a cellular automata model for which each cell has several learning automata. It is shown that for a class of rules, called commutative <b>rules,</b> the <b>cellular</b> <b>learning</b> automata converges to a stable and compatible configuration. Two applications of this new model such as channel assignment in cellular mobile networks and function optimization are also given. For both applications, it has been shown through computer simulations that the CLA based solutions produce better results. ...|$|R
40|$|The paper {{describes}} {{the application of}} neural networks as <b>learning</b> <b>rules</b> for the training of neural networks. The <b>learning</b> <b>rule</b> {{is part of the}} neural network architecture. As a result the <b>learning</b> <b>rule</b> is non-local and globally distributed within the network. The <b>learning</b> <b>rules</b> are evolved using an evolution strategy. The survival of a <b>learning</b> <b>rule</b> is based on its performance in training neural networks on a set of tasks. Training algorithms will be evolved for single layer artificial neural networks. Experimental results show that a <b>learning</b> <b>rule</b> of this type is very capable of generating an efficient training algorithm...|$|R
40|$|We {{summarize}} the Storkey <b>Learning</b> <b>Rules</b> for the Hopfield Model, and evaluate performance {{relative to other}} <b>learning</b> <b>rules.</b> Hopfield Models are normally used for auto-association, and Storkey <b>Learning</b> <b>Rules</b> {{have been found to}} have good balance between local learning and capacity. In this paper we outline different <b>learning</b> <b>rules</b> and summarise capacity re-sults. Hopfield networks are related to Boltzmann Machines: they are the same as fully visible Boltzmann Machines in the zero temperature limit. Perhaps renewed interest in Boltzmann machines will produce renewed interest in Hopfield <b>learning</b> <b>rules?...</b>|$|R
40|$|This paper {{considers}} <b>learning</b> <b>rules</b> for {{environments in}} which little prior and feedback information is available to the decision-maker. Two properties of such <b>learning</b> <b>rules,</b> absolute expediency and monotonicity, are studied. The paper provides some necessary and some sufficient conditions for these properties. A number of examples {{show that there is}} quite a large variety of <b>learning</b> <b>rules</b> which have these properties. It is also shown that all <b>learning</b> <b>rules</b> that have these properties are, in some sense, related to replicator dynamics of evolutionary game theory. Absolute expediency, monotonicity, <b>learning</b> <b>rule,</b> decision making...|$|R
40|$|We study a {{model of}} local {{evolution}} in which agents located on a network interact strate-gically with their neighbours. Strategies are chosen {{with the help of}} <b>learning</b> <b>rules</b> that are themselves based on the success of strategies observed in the neighbourhood. Previous work on local evolution assumes fixed <b>learning</b> <b>rules</b> while we study <b>learning</b> <b>rules</b> that are determined endogenously. Simulations indicate that endogenous <b>learning</b> <b>rules</b> put more weight on a learning player’s own experience than on the experience of an observed neighbour. Nevertheless stage game behaviour is similar to behaviour with symmetric <b>learning</b> <b>rules...</b>|$|R
40|$|A <b>learning</b> <b>rule</b> is risk averse if, for all distributions, it is {{expected}} to add more probability mass to an action that gives the expected value of a distribution with certainty than to an action that gives the distribution itself. We provide several characterizations of risk averse <b>learning</b> <b>rules.</b> Our analysis reveals that the theory of risk averse <b>learning</b> <b>rules</b> is isomorphic to the theory of risk averse expected utility maximizers. We consider two additional properties of the risk attitudes of a <b>learning</b> <b>rule.</b> We show that both are sufficient for a <b>learning</b> <b>rule</b> to be risk averse and characterize the set of all <b>learning</b> <b>rules</b> that satisfy the more stringent of these sufficient conditions. ...|$|R
40|$|We study a {{model of}} local evolution. Players are located on a network and play games agains their neighbors. Players are {{characterized}} by three properties: (1) The stage game strategies they use agains their neighbors. (2) The repeated game strategy that determines the former. (3) A <b>learning</b> <b>rule</b> that selects the repeated game strategy, {{on the basis of}} the player's own and the neighbors' payoff and repeated game strategy. The dynamics that specifies <b>learning</b> <b>rules</b> is given exogenously. Players sample their neighbors' <b>learning</b> <b>rules</b> and their respective payoff. Then they construct a model that related parameters of the <b>learning</b> <b>rules</b> to payoffs. Given this model they choose an optimal <b>learning</b> <b>rule.</b> We find that under this dynamics <b>learning</b> <b>rules</b> emerge in the long run which behave deterministically but which are asymmetric in the sense that while learning they put more weight on the learning players experience then on the observed players one. Nevertheless stage game behavior under these <b>learning</b> <b>rules</b> is similar to behavior using symmetric <b>learning</b> <b>rules.</b> Evolutionary Game Theory, Networks. ...|$|R
40|$|Observational {{learning}} andractice {{learning are}} two important learning styles andlay {{important roles in}} our information acquisition. In thisaper, we study a spacial evolutionaryrisoner's dilemma game, wherelayers can choose the observational <b>learning</b> <b>rule</b> or theractice <b>learning</b> <b>rule</b> when updating their strategies. In theroposed model, we use aarameter controlling thereference oflayers choosing the observational <b>learning</b> <b>rule,</b> and found that there exists an optimal value of leading to the highest cooperation level, which indicates that the cooperation can beromoted by these two <b>learning</b> <b>rules</b> collaboratively and one single <b>learning</b> <b>rule</b> is not favor theromotion of cooperation. By analysing the dynamical behavior of the system, {{we find that the}} observational <b>learning</b> <b>rule</b> can make thelayers residing on cooperative clusters more easily realize the bad sequence of mutual defection. However, a too high observational learningrobability suppresses thelayers to form compact cooperative clusters. Our results highlight the importance of a strategy-updating rule, more importantly, the observational <b>learning</b> <b>rule</b> in the evolutionary cooperation. Department of Electronic and Information Engineerin...|$|R
40|$|In {{ensemble}} teacher learning, ensemble {{teachers have}} only uncertain {{information about the}} true teacher, and this information is given by an ensemble consisting of {{an infinite number of}} ensemble teachers whose variety is sufficiently rich. In this learning, a student learns from an ensemble teacher that is iteratively selected randomly from a pool of many ensemble teachers. An interesting point of ensemble teacher learning is the asymptotic behavior of the student to approach the true teacher by learning from ensemble teachers. The student performance is improved by using the Hebbian <b>learning</b> <b>rule</b> in the <b>learning.</b> However, the perceptron <b>learning</b> <b>rule</b> cannot improve the student performance. On the other hand, we proposed a perceptron <b>learning</b> <b>rule</b> with a margin. This <b>learning</b> <b>rule</b> is identical to the perceptron <b>learning</b> <b>rule</b> when the margin is zero and identical to the Hebbian <b>learning</b> <b>rule</b> when the margin is infinity. Thus, this rule connects the perceptron <b>learning</b> <b>rule</b> and the Hebbian <b>learning</b> <b>rule</b> continuously through the size of the margin. Using this rule, we study changes in the learning behavior from the perceptron <b>learning</b> <b>rule</b> to the Hebbian <b>learning</b> <b>rule</b> by considering several margin sizes. From the results, we show that by setting a margin of kappa > 0, the effect of an ensemble appears and becomes significant when a larger margin kappa is used. Comment: 12 pages, 5 figures. Journal of Physical Society of Japan, vol. 81, (2012) 06400...|$|R
40|$|In this paper, we {{introduce}} open <b>cellular</b> <b>learning</b> automata {{and then}} study its convergence behavior. It is shown {{that for a}} class of rules called commutative <b>rules,</b> the open <b>cellular</b> <b>learning</b> automata in stationary external environments converges to a stable and compatible configuration. The numerical results also confirm the theory. 1...|$|R
40|$|<b>Learning</b> <b>rules</b> are {{increasingly}} being used in macroeconomic models. However one criticism that has been levelled at this assumption is that the choice of variables {{for inclusion in the}} <b>learning</b> <b>rule,</b> and the actual specification of the <b>learning</b> <b>rule</b> itself, is arbitrary. In this paper we test how important the particular <b>learning</b> <b>rule</b> specification is by incorporating a battery of <b>learning</b> <b>rules</b> into a large-scale macro model. The model’s dynamics are then compared to those from a version of the model simulated under rational expectations (RE). The results indicate that although there are large differences between the RE solution and each of the solutions under learning, differences amongst the <b>learning</b> <b>rule</b> solutions are minor. 1 The first and third authors are at the Centre for International Macroeconomics (CIM), University o...|$|R
40|$|A new {{unsupervised}} competitive <b>learning</b> <b>rule</b> is introduced, {{called the}} kernel-based Maximum Entropy <b>learning</b> <b>Rule</b> (kMER), for equiprobabilistic topographic map formation. The application envisaged is density-based clustering. An empirical study is conducted {{to compare the}} clustering performance of kMER {{with that of a}} number of other unsupervised competitive <b>learning</b> <b>rules.</b> status: publishe...|$|R
40|$|The BCM <b>learning</b> <b>rule</b> {{originally}} {{arose from}} experiments intended {{for measuring the}} selectivity of neurons in the primary visual cortex, and it dependence on input stimuli. This <b>learning</b> <b>rule</b> incorporates a dynamic LTP threshold, which depends on the time averaged postsynaptic activity. Although the BCM <b>learning</b> <b>rule</b> has been well studied and some experimental evidence of neuronal adherence {{has been found in}} the other areas of the brain, including the hippocampus, there is still much to be known about the dynamic behavior of this <b>learning</b> <b>rule...</b>|$|R
40|$|The paper reviews single-neuron <b>learning</b> <b>rules</b> {{for minor}} {{component}} analysis and suggests a novel minor component <b>learning</b> <b>rule.</b> In this rule, the weight vector length is self-stabilizing, i. e. moving towards unit length in each learning step. In simulations with low- and medium-dimensional data, {{the performance of}} the novel <b>learning</b> <b>rule</b> is compared with previously suggested rules...|$|R
40|$|In {{previous}} work ([4, 2, 1]) {{we discussed the}} subject of parametric <b>learning</b> <b>rules</b> for neural networks. In this article, we present a theoretical basis permitting to study the generalization property of a <b>learning</b> <b>rule</b> whose parameters are estimated from a set of learning tasks. By generalization, we mean {{the possibility of using}} the <b>learning</b> <b>rule</b> to <b>learn</b> solve new tasks. Finally, we describe simple experiments on two-dimensional categorization tasks and show how they corroborate the theoretical results. 1 Introduction Learning mechanisms in neural networks are usually associated with changes in synaptic efficiency. In such models, synaptic <b>learning</b> <b>rules</b> control the variations of the parameters (synaptic weights) of the network. Researchers in neural networks have proposed <b>learning</b> <b>rules</b> based on mathematical principles (such as backpropagation) or biological analogy (such as Hebbian <b>rules),</b> but better <b>learning</b> <b>rules</b> may be needed to achieve human-like performance in many lear [...] ...|$|R
40|$|This paper {{considers}} a <b>learning</b> <b>rules</b> for {{environments in which}} little prior in-formation is available to the decision maker. Two properties of such <b>learning</b> <b>rules,</b> absolute expediency and monotonicity, are studied. The paper pro-vides some necessary, and some su¢cient conditions for these properties. A number of examples show that the there is a quite a large variety of <b>learning</b> <b>rules</b> which have the properties. It is also shown that all <b>learning</b> <b>rules</b> which have these properties are, in some sense, related to the replicator dynamics of evolutionary game theory. ...|$|R
40|$|Abstract- We {{demonstrate}} in {{this article}} that a Hebb-like <b>learning</b> <b>rule</b> with memory paves the way for active learning {{in the context of}} recurrent neural networks. We compare active with passive learning and a Hebb-like <b>learning</b> <b>rule</b> with and without memory for the problem of timing to be learned by the neural network. Moreover, we study the influence of the topology of the recurrent neural network. Our results from numerical simulations reveal that active learning decreases the learning time significantly only for the Hebb-like <b>learning</b> <b>rule</b> with memory whereas the <b>learning</b> <b>rule</b> without memory remains unaffected. This result can be observed in all investigated network topologies, indicating the robustness of this effect. Keywords- Active <b>learning,</b> Hebb-like <b>learning</b> <b>rule,</b> Recurrent neural network 1...|$|R
40|$|This paper {{considers}} <b>learning</b> <b>rules</b> for {{environments in}} which little prior and feedback information is available to the decision maker. Two properties of such <b>learning</b> <b>rules</b> are studied: absolute expediency and monotonicity. Both require that {{some aspect of the}} decision maker’s performance improves from the current period to the next. The paper provides some necessary, and some sufficient conditions for these properties. It turns out that there is a large variety of <b>learning</b> <b>rules</b> that have the properties. However, all <b>learning</b> <b>rules</b> that have these properties are related to the replicator dynamics of evolutionary game theory. For the case in which there are only two actions, it is shown that one of the absolutely expedient <b>learning</b> <b>rules</b> dominates all others...|$|R
40|$|In this paper, {{we present}} a {{framework}} where a <b>learning</b> <b>rule</b> can be optimized within a parametric <b>learning</b> <b>rule</b> space. We define what we call parametric <b>learning</b> <b>rules</b> and present a theoretical study of their generalization properties when estimated from a set of learning tasks and tested over another set of tasks. We corroborate {{the results of this}} study with practical experiments...|$|R
40|$|Several <b>learning</b> <b>rules</b> for {{synaptic}} plasticity, {{that depend}} on either spike timing or internal state variables, have been proposed in the past imparting varying computational capabilities to Spiking Neural Networks. Due to design complications these <b>learning</b> <b>rules</b> are typically not implemented on neuromorphic devices leaving the devices to be only capable of inference. In this work we propose a unidirectional post-synaptic potential dependent <b>learning</b> <b>rule</b> that is only triggered by pre-synaptic spikes, and easy to implement on hardware. We demonstrate that such a <b>learning</b> <b>rule</b> is functionally capable of replicating computational capabilities of pairwise STDP. Further more, we demonstrate that this <b>learning</b> <b>rule</b> {{can be used to}} learn and classify spatio-temporal spike patterns in an unsupervised manner using individual neurons. We argue that this <b>learning</b> <b>rule</b> is computationally powerful and also ideal for hardware implementations due to its unidirectional memory access. Comment: Published in IEEE BioCAS 2016 Proceedings, BioMedical Circuits and Systems Conference, 201...|$|R
40|$|Gerstner {{and colleagues}} have {{proposed}} a <b>learning</b> <b>rule</b> in which the incrementation of synaptic weight is adjusted according to the time difference between neuron firing and spike arrival. In this study, a continuous-time associative memory model is constructed by using a <b>learning</b> <b>rule</b> based on that idea, and {{the functions of the}} <b>learning</b> <b>rule</b> are investigated. First, a continuous-time associative memory model is constructed {{on the basis of the}} <b>learning</b> <b>rule</b> in continuous time, in which the neuron can store memory as the synchronous firing dynamics of the neuron. A result is presented in which multiple memory patterns can be recalled simultaneously under the proposed model. Then, using the proposed <b>learning</b> <b>rule,</b> an attempt is made to compose a nesting structure formed by arbitrary memory patterns. Based on the above series of results, it is shown that the <b>learning</b> <b>rule</b> has the function of modifying the memory storage structure according to changes in the environment...|$|R
40|$|This {{dissertation}} contains three {{essays on}} learning and risk aversion. In the first essay we consider how learning {{may lead to}} risk averse behavior. A <b>learning</b> <b>rule</b> {{is said to be}} risk averse if it is expected to add more probability to an action which provides, with certainty, the expected value of a distribution rather than when it provides a randomly drawn payoff from this distribution, for every distribution. We characterize risk averse <b>learning</b> <b>rules.</b> The result reveals that the analysis of risk averse learning is isomorphic to that of risk averse expected utility maximizers. A <b>learning</b> <b>rule</b> is said to be monotonically risk averse if it is expected to increase the probability of choosing the actions whose distribution second-order stochastically dominates all others in every environment. We characterize monotonically risk averse <b>learning</b> <b>rules.</b> In the second essay we analyze risk attitudes for learning within the mean-variance paradigm. A <b>learning</b> <b>rule</b> is variance-averse if the expected reduced distribution of payoffs in the next period has a smaller variance than that of the current reduced distribution, in every set where all the actions provide the same expected payoff. A <b>learning</b> <b>rule</b> is monotonically variance-averse if it is expected to add probability to the set of actions that have the smallest variance in the set, when all the actions have the same expected payoff. A <b>learning</b> <b>rule</b> is monotonically mean-variance-averse if it is expected to add probability to the set of actions that have the highest expected payoff and smallest variance whenever this set is not empty. We characterize monotonically variance-averse and monotonically mean-variance-averse <b>learning</b> <b>rules.</b> In the last essay we analyze the social learning process of a group of individuals. We say that a <b>learning</b> <b>rule</b> is first-order monotone if the number of individuals that play actions with first-order stochastic dominant payoff distributions is expected to increase. We characterize these <b>learning</b> <b>rules...</b>|$|R
40|$|In a {{physical}} neural system, where {{storage and processing}} are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as {{the activity of the}} pre- and post-synaptic neurons, resulting in local <b>learning</b> <b>rules.</b> A systematic framework for studying the space of local <b>learning</b> <b>rules</b> is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each <b>learning</b> <b>rule.</b> Such a framework enables also the systematic discovery of new <b>learning</b> <b>rules</b> and exploration of relationships between <b>learning</b> <b>rules</b> and group symmetries. We study polynomial local <b>learning</b> <b>rules</b> stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local <b>learning</b> <b>rules</b> in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. We estimate the learning channel capacity associated with several algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost, even in recurrent networks. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local <b>learning</b> <b>rules,</b> introduces the <b>learning</b> channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of <b>learning</b> <b>rules</b> discovered so far...|$|R
40|$|DoctorIn this thesis, we {{consider}} a general nonlinear output tracking problem with P-type <b>learning</b> <b>rule.</b> In particular, the P-type high order <b>learning</b> <b>rule</b> is studied. In general, the high order <b>learning</b> <b>rule</b> shows a better performance for convergence speed and robustness than conventional <b>learning</b> <b>rule</b> in {{presence of the}} periodic disturbances. The robustness and convergence is analyzed and verified by computer simulations. However, when we use the iterative learning control (ILC) in real world, an unexpected huge overshoot can be observed easily. Therefore we study these phenomenon with various {{point of view and}} propose a new <b>learning</b> <b>rule</b> to avoid this unexpected situation. The PD-type and P-type normalized <b>learning</b> <b>rules</b> are proposed and their convergence is proved. Simulation results show that the proposed <b>learning</b> <b>rule</b> is effective for the unexpected huge overshoot. Moreover, the robust optimal design is presented when there is an uncertainty interval. The new robust performance indices are introduced and studied. This design method is very useful when we apply the ILC to a real world application. The optimal design has better performance for convergence speed and global uniform bound than conventional design method. Finally, the ILC is applied to a continuous casting process. To reduce a periodic bulging disturbances, a proposed <b>learning</b> <b>rule</b> with forgetting factor and switching mechanism is applied to the continuous casting system. A 1 : 4 hardware simulator is used to verify performance of the proposed method...|$|R
40|$|The {{nature of}} the basins of {{attraction}} of a Hopfield network {{is as important as}} the capacity. Here a new <b>learning</b> <b>rule</b> is re-introduced. This <b>learning</b> <b>rule</b> has a higher capacity than that of the Hebb rule, and still keeps important functionality, such as incrementality and locality, which the pseudo-inverse lacks. However the basins of attraction of the fixed points of this <b>learning</b> <b>rule</b> have not yet been studied...|$|R
5000|$|There {{are various}} {{different}} <b>learning</b> <b>rules</b> {{that can be}} used to store information in the memory of the Hopfield Network. It is desirable for a <b>learning</b> <b>rule</b> to have both of the following two properties: ...|$|R
40|$|Learning {{describes}} how behavior changes {{in response to}} experience. We consider how learning may lead to risk averse behavior. A <b>learning</b> <b>rule</b> {{is said to be}} risk averse if it is expected to add more probability to an action which provides, with certainty, the expected value of a distribution rather than when it provides a randomly drawn payo¤ from this distribution, for every distribution. We characterize risk averse <b>learning</b> <b>rules.</b> The result reveals that the analyses of risk averse learning is isomorphic to that of risk averse expected utility maximizers. A <b>learning</b> <b>rule</b> is said to be monotonically risk averse if it is expected to increase the probability of choosing the actions whose distribution second order stochastically dominates all others in every environment. We characterize monotonically risk averse <b>learning</b> <b>rules</b> and show that such <b>learning</b> <b>rules</b> are risk averse. ...|$|R
40|$|A biologically realizable, {{unsupervised}} <b>learning</b> <b>rule</b> {{is described}} for the online extraction of object features, suitable for solving {{a range of}} object recognition tasks. Alterations to the basic <b>learning</b> <b>rule</b> are proposed which allow the rule to better suit the parameters of a given input space. One negative consequence of such modifications {{is the potential for}} learning instability. The criteria for such instability are modeled using digital filtering techniques and predicted regions of stability and instability tested. The result is a family of <b>learning</b> <b>rules</b> which can be tailored to the specific environment, improving both convergence times and accuracy over the standard <b>learning</b> <b>rule,</b> while simultaneously insuring learning stability...|$|R
