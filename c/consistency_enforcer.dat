0|294|Public
40|$|This paper {{discusses}} {{the complexity of}} bound consistency on n-ary linear constraint system and investigates the relationship between equivalent binary equation systems {{from the perspective of}} bound consistency techniques. We propose an efficient bound <b>consistency</b> <b>enforcing</b> algorithm whose complexity is ### ###. In addition, by transforming a binary equation system into solved form, an efficient <b>consistency</b> <b>enforcing</b> algorithm can be achieve...|$|R
5000|$|... #Caption: Arc <b>consistency</b> <b>enforced</b> by {{removing}} 1 as a value for x2. As a result, x3 {{is no longer}} arc-consistent with x2 because x3=2 does not correspond to a value for x2.|$|R
40|$|Several recent {{approaches}} for processing graphical models (constraint and Bayesian networks) simultaneously exploit graph decomposition and local <b>consistency</b> <b>enforcing.</b> Graph decomposition exploits the problem structure and offers {{space and time}} complexity bounds while hard information propagation provides practical improvements {{of space and time}} behavior inside these theoretical bounds. Concurrently, the extension of local consistency to weighted constraint networks has led to important improvements in branch and bound based solvers. Indeed, soft local consistencies give incrementally computed strong lower bounds providing inexpensive yet powerful pruning and better informed heuristics. In this paper, we consider combinations of tree decomposition based approaches and soft local <b>consistency</b> <b>enforcing</b> for solving weighted constraint problems. The intricacy of weighted information processing leads to different approaches, with different theoretical properties. It appears that the most promising combination sacrifices a bit of theory for improved practical efficiency...|$|R
40|$|We {{show that}} several {{constraint}} propagation algorithms (also called (local) <b>consistency,</b> <b>consistency</b> <b>enforcing,</b> Waltz, ltering or narrowing algorithms) are instances of algorithms {{that deal with}} chaotic iteration. To this end we propose a simple abstract framework {{that allows us to}} classify and compare these algorithms and to establish in a uniform way their basic properties...|$|R
40|$|Many {{problems}} and applications can be naturally modelled and solved using constraints {{with more than}} two variables. Such n-ary constraints, in particular, arithmetic constraints are provided by many finite domain constraint programming systems. The best known worst case time complexity of existing algorithms(3 l C-schema) for <b>enforcing</b> arc <b>consistency</b> on general CSPs isO() where d {{is the size of}} domain, e is the number of constraints and n is the maximum number of variables in a single constraint. We address the question of efficient <b>consistency</b> <b>enforcing</b> for n-ary constraints. An observation here is that even with a restriction of n-ary constraints to linear constraints, arc <b>consistency</b> <b>enforcing</b> is NP-complete. We identify a general class of monotonic n-ary constraints(3 h includes linear inequalities as a special case). Such monotonic constraints can be made arc consistent in time d). The special case of linear inequalities can be made arc consistent in time d) using bounds-consistency which exploits special properties of the projection function...|$|R
40|$|We {{show that}} several {{constraint}} propagation algorithms (also called (local) <b>consistency,</b> <b>consistency</b> <b>enforcing,</b> Waltz, filtering or narrowing algorithms) are instances of algorithms {{that deal with}} chaotic iteration. To this end we propose a simple abstract framework {{that allows us to}} classify and compare these algorithms and to establish in a uniform way their basic properties. Comment: To appear in Theoretical Computer Science in the special issue devoted to the 24 th ICALP conference (Bologna 1997...|$|R
40|$|The Weighted Constraint Satisfaction Problem (WCSP) {{framework}} allows representing {{and solving}} problems involving both hard constraints and cost functions. It has been ap- plied to various problems, including resource allocation, bioinformatics, scheduling, etc. To solve such problems, solvers usually rely on branch-and-bound algorithms equipped with local consistency filtering, mostly soft arc consistency. However, these techniques {{are not well}} suited to solve problems with very large domains. Motivated by the resolution of an RNA gene localization problem inside large genomic sequences, and {{in the spirit of}} bounds consistency for large domains in crisp CSPs, we introduce soft bounds arc consistency, a new weighted local consistency specifically designed for WCSP with very large domains. Compared to soft arc consistency, BAC provides significantly improved time and space asymptotic complexity. In this paper, we show how the semantics of cost functions can be exploited to further improve the time complexity of BAC. We also compare both in theory and in practice the efficiency of BAC on a WCSP with bounds <b>consistency</b> <b>enforced</b> on a crisp CSP using cost variables. On two different real problems modeled as WCSP, including our RNA gene localization problem, we observe that maintaining bounds arc con- sistency outperforms arc consistency and also improves over bounds <b>consistency</b> <b>enforced</b> on a constraint model with cost variables...|$|R
40|$|We propose ALLDIFFPREC, a {{new global}} {{constraint}} that combines together an ALLDIFFERENT constraint with precedence constraints that strictly order given pairs of variables. We identify a number of applications for this global constraint including instruction scheduling and symmetry breaking. We give an efficient propagation algorithm that <b>enforces</b> bounds <b>consistency</b> on this global constraint. We show how to implement this propagator using a decomposition that extends the bounds <b>consistency</b> <b>enforcing</b> decomposition proposed for the ALLDIFFERENT constraint. Finally, we prove that <b>enforcing</b> domain <b>consistency</b> on this global constraint is NP-hard in general...|$|R
40|$|This paper {{deals with}} {{constraint}} handling in manufacturing. It {{starts with a}} basic de#nition of the constraint satisfaction problem, followed by a description of <b>consistency</b> <b>enforcing</b> and constraint guided search. Constraint handling in scheduling problems is the main topic of the paper also presenting di#erent types of schedule construction methods and explaining the use of temporal constraints. Special attention {{is given to the}} task of mid-term production planning. First, production planning in ordinary MRP II-based systems is introduced, followed by the presentation of the master production scheduling leitstand #MPSL#, a research and development activity of a #Forschungsschwerpunkt " at the University GH Essen...|$|R
40|$|In {{this paper}} {{we present a}} {{scalable}} fully-automated PDDL planner capable of reasoning with PDDL+ events and linear processes. Processes and events model (respectively) contin-uous and discrete exogenous activity in the environment, oc-curring when certain conditions hold. We discuss the signifi-cant research challenges posed in creating a forward-chaining planner that can reason with these, and present novel state-progression and <b>consistency</b> <b>enforcing</b> techniques {{that allow us to}} meet these challenges. Finally we present results show-ing that our new planner, using PDDL+ domain models, is able to solve realistic expressive problems more efficiently than the current state-of-the-art alternative: a compiled PDDL 2. 1 representation with continuous numeric effects. ...|$|R
40|$|As logic, {{constraint}} satisfaction faces {{the problem of}} inconsistency which itself naturally leads to the need of expressing preferences. Starting from (Rosenfeld, Hummel, & Zucker 1976), which defined fuzzy constraint networks, a variety of extended constraint frameworks have been proposed. Since 1995, several general axiomatic frameworks that try to cover all these proposals have been introduced. In this paper, we show how algorithms and axioms interacts on a specific class of algorithms: arc <b>consistency</b> <b>enforcing</b> algorithms. The generalization of arc consistency to the Valued CSP framework was recently made possible thanks to the addition of an additional axiom that captures the existence of difference between preferences. We show that many usual (and less usual) instances satisfy this axiom. This new axio...|$|R
40|$|This paper {{addresses}} {{three main}} issues. Firstly, {{the combination of}} formal specification languages to model proposed systems. For this paper we introduce the dual specification of a case study system using the formal languages LOTOS and the Z notation to capture the behaviour of the complete system, including the modelling of data abstraction, information hiding and modularisation. Secondly, the production of an industrial-strength specification using a mechanical, automated CASE tool to verify the syntax of the formal specification. It is hoped that specifications which are verified mechanically will be more widely acceptable to industry because of the <b>consistency</b> <b>enforced</b> by the CASE tools used to check them. Finally, the transition from formal specification to implementation using the dual formal specification approach introduced in this paper...|$|R
40|$|International audienceSimilarly to {{what has}} been done with Global Constraints in Constraint Programming, {{different}} results have been recently published on Global Cost Functions in weighted CSPs, defining the premises of a Cost Function Programming paradigm. In this paper, in the spirit of Berge-acyclic decompositions of global constraints such as Regular, we explore the possibility of decomposing Global Cost Functions {{in such a way that}} <b>enforcing</b> soft local <b>consistencies</b> on the decomposed cost function offers guarantees on the level of <b>consistency</b> <b>enforced</b> on the original global cost function. We show that an extension of Directional Arc Consistency to arbitrary arities and Virtual Arc Consistency offer specific guarantees. We conclude by preliminary experiments on Weighted Regular decompositions that show that decompositions may be very useful to easily integrate global cost functions in existing solvers with good efficiency...|$|R
40|$|Abstract. The {{concept of}} local {{consistency}} plays {{a central role}} in constraint satisfaction and has been extended to handle general constraint-based inference (CBI) problems. We propose a family of novel generalized local consistency concepts for the junction graph representation of CBI problems. These concepts are based on a general condition that depends only on the existence and property of the multiplicative absorbing element and does not depend on the other semiring properties of CBI problems. We present several local <b>consistency</b> <b>enforcing</b> algorithms and their approximation variants. Theoretical complexity analyses and empirical experimental results for the application of these algorithms to both MaxCSP and probability inference are given. We also discuss the relationship between these local consistency concepts and message passing schemes such as junction tree algorithms and loopy message propagation. ...|$|R
50|$|Due {{to trace}} fossils' history of being {{difficult}} to classify, {{there have been}} several attempts to <b>enforce</b> <b>consistency</b> in the naming of ichnotaxa.|$|R
40|$|Abstract. We {{are dealing}} with the problem of {{enforcing}} certain level of consistency in Boolean satisfaction problems (SAT problems). As a starting point we took path-consistency which we made stronger by requiring a more restrictive condition on paths connecting consistent pairs of values. Unfortunately, we found that enforcing such a modified path-consistency is an NP-complete problem. However, the definition of the modified path-consistency provides many degrees of freedom for relaxing it. This allowed us to propose an approximation algorithm for enforcing the modified path-consistency. The algorithm enforces a relaxation of the originally proposed modified path-consistency. This relaxed consistency represents a trade-off between the inference strength and the complexity of propagation algorithm. Our experimental evaluation on several SAT benchmark problems showed that the <b>consistency</b> <b>enforced</b> by our approximation algorithm has still significantly better inference strength than the standard path-consistency...|$|R
40|$|Arc {{consistency}} and generalized arc consistency {{are two of}} the most important local consistency techniques for binary and non-binary classic constraint satisfaction problems (CSPs). Based on the Semiring CSP and Valued CSP frameworks, arc consistency has also been extended to handle soft constraint satisfaction problems such as fuzzy CSP, probabilistic CSP, max CSP, and weighted CSP. This extension is based on an idempotent or strictly monotonic constraint combination operator. In this paper, we present a weaker condition for applying the generalized arc consistency approach to constraint-based inference problems other than classic and soft CSPs. These problems, including probability inference and maximal likelihood decoding, can be processed using generalized arc <b>consistency</b> <b>enforcing</b> approaches. We also show that, given an additional monotonic condition on the corresponding semiring structure, some of constraintbased inference problems can be approximately preprocessed using generalized arc consistency algorithms. ...|$|R
40|$|Similarly to {{what has}} been done with Global Constraints in Constraint Programming, di erent results have been {{recently}} published on Global Cost Functions in weighted CSPs, de ning the premises of a Cost Function Programming paradigm. In this paper, in the spirit of Berge-acyclic decompositions of global constraints such as Regular, we explore the possibility of decomposing Global Cost Functions {{in such a way that}} <b>enforcing</b> soft local <b>consistencies</b> on the decomposed cost function o ers guarantees on the level of <b>consistency</b> <b>enforced</b> on the original global cost function. We show that an extension of Directional Arc Consistency to arbitrary arities and Virtual Arc Consistency o er speci c guarantees. We conclude by preliminary experiments on WeightedRegular decompositions that show that decompositions may be very useful to easily integrate global cost functions in existing solvers with good e fficiency...|$|R
40|$|We provide here a simple, yet {{very general}} {{framework}} {{that allows us}} to explain several constraint propagation algorithms in a systematic way. In particular, using the notions commutativity and semi-commutativity, we show how the well-known AC- 3, PC- 2, DAC and DPC algorithms are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt [1]. 1 Introduction Constraint programming in a nutshell consists of formulating and solving so-called constraint satisfaction problems. One of the most important techniques developed in this area is constraint propagation that aims at reducing the search space while maintaining equivalence. We call the corresponding algorithms constraint propagation algorithms but several other names have also been used in the literature: consistency, local <b>consistency,</b> <b>consistency</b> <b>enforcing,</b> Waltz, filtering or narrowing algorithms. These algorithms usually aim at reaching some form of "local consistency", a notion that in a l [...] ...|$|R
40|$|Local <b>{{consistency}}</b> <b>enforcing</b> is at {{the core}} of CSP (Constraint Satisfaction Problem) solving. Although arc consistency is still the most widely used level of local consistency, researchers are going on investigating more powerful levels, such as path consistency, k-consistency, (i,j) -consistency. Recently, more attention has been turned to inverse local consistency levels, such as path inverse consistency, k-inverse consistency, neighborhood inverse consistency, which do not suffer from the drawbacks of the other local consistency levels (changes in the constraint definitions and in the constraint graph, prohibitive memory requirements). In this paper, we propose a generic framework for inverse local consistency, which includes most of the previously defined levels and allows a rich set of new levels to be defined. The first benefit of such a generic framework is to allow a user to define and test many different inverse local consistency levels, in accordance with the [...] ...|$|R
50|$|Forth {{does not}} <b>enforce</b> <b>consistency</b> of data type usage; {{it is the}} programmer's {{responsibility}} to use appropriate operators to fetch and store values or perform other operations on data.|$|R
50|$|For example, a {{consistency}} {{model can}} define that a {{process is not}} allowed to issue an operation until all previously issued operations are completed. Different <b>consistency</b> models <b>enforce</b> different conditions. One consistency model can be considered stronger than another if it requires all conditions of that model and more. In other words, a model with fewer constraints is considered a weaker consistency model.|$|R
40|$|Abstract: Helmholtz {{stereopsis}} is {{an advanced}} 3 D reconstruction technique for objects with arbitrary reflectance proper-ties that uniquely characterises surface points by both depth and normal. Traditionally, in Helmholtz stereopsis consistency of depth and normal estimates is assumed rather than explicitly enforced. Furthermore, conven-tional Helmholtz stereopsis performs maximum likelihood depth estimation without neighbourhood consid-eration. In this paper, we demonstrate that reconstruction accuracy of Helmholtz stereopsis can be greatly enhanced by formulating depth estimation as a Bayesian maximum {{a posteriori probability}} problem. In re-formulating the problem we introduce neighbourhood support by formulating and comparing three priors: a depth-based, a normal-based and a novel depth-normal <b>consistency</b> <b>enforcing</b> one. Relative performance eval-uation of the three priors against standard maximum likelihood Helmholtz stereopsis is performed on both real and synthetic data to facilitate both qualitative and quantitative assessment of reconstruction accuracy. Observed superior performance of our depth-normal consistency prior indicates a previously unexplored ad-vantage in joint optimisation of depth and normal estimates. ...|$|R
40|$|Constraint-Based Inference (CBI) [1] is an {{umbrella}} term for various superficially different problems including probabilistic inference, decision-making under uncertainty, constraint satisfaction, propositional satisfiability, decoding problems, and possibility inference. In this project we explicitly use the semiring concept to generalize various CBI problems {{into a single}} formal representation framework with a broader coverage of the problem space, based on the synthesis of existing generalized frameworks from both constraint processing and probability inference communities. Based on our generalized CBI framework, extensive comparative studies of exact and approximate inference approaches are commenced. First, we extend generalized arc consistency to probability inference based on a weaker condition [2]. All the existing arc <b>consistency</b> <b>enforcing</b> algorithms can be generalized and migrated to handle other concrete CBI problems that satisfy this condition. Second, based on our CBI framework we apply junction tree algorithms in probability inferences to solve soft CSPs [1]. We show that the message-passing schemes of junction tree algorithms can b...|$|R
40|$|Helmholtz {{stereopsis}} is {{an advanced}} 3 D reconstruction technique for objects with arbitrary reflectance properties that uniquely characterises surface points by both depth and normal. Traditionally, in Helmholtz stereopsis consistency of depth and normal estimates is assumed rather than explicitly enforced. Furthermore, conventional Helmholtz stereopsis performs maximum likelihood depth estimation without neighbourhood consideration. In this paper, we demonstrate that reconstruction accuracy of Helmholtz stereopsis can be greatly enhanced by formulating depth estimation as a Bayesian maximum {{a posteriori probability}} problem. In reformulating the problem we introduce neighbourhood support by formulating and comparing three priors: a depth-based, a normal-based and a novel depth-normal <b>consistency</b> <b>enforcing</b> one. Relative performance evaluation of the three priors against standard maximum likelihood Helmholtz stereopsis is performed on both real and synthetic data to facilitate both qualitative and quantitative assessment of reconstruction accuracy. Observed superior performance of our depth-normal consistency prior indicates a previously unexplored advantage in joint optimisation of depth and normal estimates...|$|R
40|$|The AC- 3 {{algorithm}} {{is a basic}} and widely used arc <b>consistency</b> <b>enforcing</b> algorithm in Constraint Satisfaction Problems (CSP). Its strength lies {{in that it is}} simple, empirically efficient and extensible. However its worst case time complexity was not considered optimal since the first complexity result for AC- 3 [Mackworth and Freuder, 1985] with the bound O(ed 3), where e is the number of constraints and d the size of the largest domain. In this paper, we show suprisingly that AC- 3 achieves the optimal worst case time complexity with O(ed 2). The result is applied to obtain a path consistency algorithm which has the same time and space complexity as the best known theoretical results. Our experimental results show that the new approach to AC- 3 is comparable to the traditional AC- 3 implementation for simpler problems where AC- 3 is more efficient than other algorithms and significantly faster on hard instances. ...|$|R
40|$|This paper studies {{a version}} of the job shop {{scheduling}} problem in which some operations have to be scheduled within non-relaxable time windows (i. e. earliest/latest possible start time windows). This problem is a wellknown NP-complete Constraint Satisfaction Problem (CSP). A popular method for solving these types of problems consists in using depth- rst backtrack search. Our earlier work focused on developing e cient <b>consistency</b> <b>enforcing</b> techniques and e cient variable/value ordering heuristics to improve the e ciency of this procedure. In this paper, we combine these techniques with new lookback schemes that help the search procedure recover from so-called deadend search states (i. e. partial solutions that cannot be completed without violating some constraints). More speci cally, we successively describe three intelligent backtracking schemes: Dynamic <b>Consistency</b> Enforcement dynamically <b>enforces</b> higher levels of consistency in selected critical subproblems, Learning From Failure dynamically modi es the order in which variables are instantiated based on earlier con icts, and Heuristic Backjumping gives up searching areas of the search space that appear too di cult. These schemes are shown to (1) further reduce the average complexity of the search procedure, (2) enable our system to e ciently solve problems that could not be solved otherwise due to excessive computational cost, and (3) be more e ective at solving job shop scheduling problems than other look-back schemes advocated in the literature...|$|R
40|$|We {{perform an}} {{extensive}} study of several different models of permutation problems proposed by Smith in [Smith 2000]. We first define {{a measure of}} constraint tightness parameterized by the level of local <b>consistency</b> being <b>enforced.</b> We then compare the constraint tightness in these different models {{with respect to a}} large number of local consistency properties including arc-consistency, (restricted) pathconsistency, path inverse consistency, singleton arc-consistency and bounds consistency. We als...|$|R
40|$|Abstract. Connecting {{soft arc}} {{consistency}} with distributed search in DCOP solv-ing {{has been very}} beneficial for performance. However, including higher levels of soft arc consistency breaks usual privacy requirements. To avoid this issue, we propose to keep different representations of the same problem on each agent, on which soft arc <b>consistencies</b> are <b>enforced</b> respecting privacy. Deletions caused in one representation can be legally propagated to others. Experimentally, this causes significant benefits. ...|$|R
5000|$|Relative Model Strength: Some {{consistency}} {{models are}} more restrictive than others. In other words, strict <b>consistency</b> models <b>enforce</b> more constraints as consistency requirements. The {{strength of a}} model can be defined by the program order or atomicity relaxations {{and the strength of}} models can also be compared. Some models are directly related if they apply same relaxations or more. On the other hand, the models that relax different requirements are not directly related.|$|R
40|$|Gutierrez and Meseguer {{show how}} to <b>enforce</b> <b>consistency</b> in BnB-ADOPT + for {{distributed}} constraint optimization, but they consider unconditional deletions only. However, during search, more values can be pruned conditionally according to variable instantiations that define subproblems. <b>Enforcing</b> <b>consistency</b> in these subproblems can cause further search space reduction. We introduce efficient methods to maintain soft arc consistencies in every subproblem during search, a non trivial task due to asynchronicity and induced overheads. Experimental results show substantial benefits on three different benchmarks. © 2013 Springer-Verlag. The work of Gutierrez and Meseguer was partially {{supported by the}} Spanish project TIN 2009 - 13591 -C 02 - 02 and Generalitat de Catalunya 2009 -SGR- 1434. Peer Reviewe...|$|R
40|$|Subset {{problems}} (set partitioning, packing, and covering) are formal {{models for}} many practical optimization problems. A set partitioning problem determines how {{the items in}} one set (S) can be partitioned into smaller subsets. All items in S must be contained in one and only one partition. Related problems are set packing (all items must be contained in zero or one partitions) and set covering (all items must be contained {{in at least one}} partition). Here, we present a hybrid solver based on ant colony optimization (ACO) combined with arc consistency for solving this kind of problems. ACO is a swarm intelligence metaheuristic inspired on ants behavior when they search for food. It allows to solve complex combinatorial problems for which traditional mathematical techniques may fail. By other side, in constraint programming, the solving process of Constraint Satisfaction Problems can dramatically reduce the search space by means of arc <b>consistency</b> <b>enforcing</b> constraint <b>consistencies</b> either prior to or during search. Our hybrid approach was tested with set covering and set partitioning dataset benchmarks. It was observed that the performance of ACO had been improved embedding this filtering technique in its constructive phase...|$|R
40|$|Shared Memory Systems with a {{globally}} consistent memory abstraction {{are currently}} very successful. The {{main reason for}} {{this can be seen}} in their ease [...] of [...] use and the convenient programming model which is close to the sequential one. Especially the memory coherency mechanisms contribute to this, as the programmer does not {{have to take care of}} any data conflicts or memory update operations. This convenience, however, comes with the price of a restricted scalability of such systems since <b>consistency</b> <b>enforcing</b> mechanisms are of global nature and therefore infer machine global data traffic. This is true for both UMA architectures with their snooping protocols and NUMA systems, which mostly deploy some kind of directory scheme. One way to solve this scalability problem, while maintaining the abstraction of a global memory, is to omit any hardware coherency mechanism and to replace it with appropriate software mechanisms. This can be done in a way that the impact on the programming model is minimized by relying on a formalized relaxed consistency model, a formalism well established in SW [...] DSM systems. The result are architectures with similar programmability properties compared to their coherent counterparts, but with significantly higher scalability properties...|$|R
50|$|This {{algorithm}} {{is equivalent to}} <b>enforcing</b> adaptive <b>consistency.</b> Since they both <b>enforce</b> <b>consistency</b> of a variable with all its parents, and since no new constraint is added after a variable is considered, what results is an instance that can be solved without backtracking.|$|R
5000|$|... #Caption: <b>Enforcing</b> <b>consistency</b> on x5 {{removes the}} red line, thus {{creating}} a new non-trivial constraint between x3 and x4. As a result, x4 has x3 as a new parent, in addition to x1 and x2. This change increases the width to 3.|$|R
40|$|We {{describe}} {{the use of}} array expressions as constraints, which represents a consequent generalisation of the "element" constraint. Constraint propagation for array constraints is studied theoretically, and {{for a set of}} domain reduction rules the local <b>consistency</b> they <b>enforce,</b> arc-consistency, is proved. An efficient algorithm is described that encapsulates the rule set and so inherits the capability to enforce arc-consistency from the rules. Comment: 10 pages. Accepted at the 6 th Annual Workshop of the ERCIM Working Group on Constraints, 200...|$|R
