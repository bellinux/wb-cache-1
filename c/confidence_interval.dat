10000|10000|Public
5|$|Radiocarbon dating tests {{conducted}} on 1QpHab and 4QpPsa at the Arizona Accelerator Mass Spectrometry Facility gave a {{one standard deviation}} <b>confidence</b> <b>interval</b> of 104-43 BCE and a two sigma <b>confidence</b> <b>interval</b> of 120-5 BCE (97%); for 4QpPsa (4Q171) the one standard deviation <b>confidence</b> <b>interval</b> was 22-78 CE and the two sigma <b>confidence</b> <b>interval</b> was 5-111 CE. Earlier paleographic dating of 1QpHab indicated a date range of 30-1 BCE.|$|E
5|$|A {{study of}} a group with a mean dosage of aspirin of 270mg per day {{estimated}} an average absolute risk increase in intracerebral hemorrhage (ICH) of 12 events per 10,000 persons. In comparison, the estimated absolute risk reduction in myocardial infarction was 137 events per 10,000 persons, and a reduction of 39 events per 10,000 persons in ischemic stroke. In cases where ICH already has occurred, aspirin use results in higher mortality, with a dose of about 250mg per day resulting in a relative risk of death within {{three months after the}} ICH around 2.5 (95% <b>confidence</b> <b>interval</b> 1.3 to 4.6).|$|E
5|$|The dodos on this islet may not {{necessarily}} {{have been the last}} members of the species. The last claimed sighting of a dodo was reported in the hunting records of Isaac Johannes Lamotius in 1688. Statistical analysis of these records by Roberts and Solow gives a new estimated extinction date of 1693, with a 95% <b>confidence</b> <b>interval</b> of 1688–1715. The authors also pointed out that because the last sighting before 1662 was in 1638, the dodo was probably already quite rare by the 1660s, and thus a disputed report from 1674 by an escaped slave cannot be dismissed out of hand.|$|E
40|$|Presenting <b>confidence</b> <b>intervals</b> around {{means is}} a common method of expressing {{uncertainty}} in data. Loftus and Masson (1994) describe <b>confidence</b> <b>intervals</b> for means in within-subjects designs. These <b>confidence</b> <b>intervals</b> {{are based on the}} ANOVA mean squared error. Cousineau (2005) presents an alternative to the Loftus and Masson method, but his method produces <b>confidence</b> <b>intervals</b> that are smaller than those of Loftus and Masson. I show why this is the case and offer a simple correction that makes the expected size of Cousineau <b>confidence</b> <b>intervals</b> {{the same as that of}} Loftus and Masson <b>confidence</b> <b>intervals...</b>|$|R
40|$|More {{and more}} {{population}} forecasts are being produced with associated 95 percent <b>confidence</b> <b>intervals.</b> How confident are we of those <b>confidence</b> <b>intervals?</b> In this paper, we produce a simulated dataset {{in which we}} know both past and future population sizes, and the true 95 percent <b>confidence</b> <b>intervals</b> at various future dates. We use the past data to produce population forecasts and estimated 95 percent <b>confidence</b> <b>intervals</b> using various functional forms. We, then, compare the true 95 percent <b>confidence</b> <b>intervals</b> with the estimated ones. This comparison shows {{that we are not}} at all confident of the estimated 95 percent <b>confidence</b> <b>intervals...</b>|$|R
30|$|In {{contrast}} to OpenModel and DegKin Manager, CAKE, gmkin and KinGUII also provide <b>confidence</b> <b>intervals</b> for the parameter estimates. The <b>confidence</b> <b>intervals</b> obtained with CAKE and KinGUII {{are based on}} untransformed parameters, which makes it possible that they include physically unrealistic values like negative values for rate constants or values exceeding unity for formation fractions. In contrast, the <b>confidence</b> <b>intervals</b> obtained with gmkin, when using default settings, are based on estimates for transformed parameters {{in order to obtain}} more realistic <b>confidence</b> <b>intervals</b> [19]. The <b>confidence</b> <b>intervals</b> obtained for the test datasets illustrate these differences, but no systematic comparison of the <b>confidence</b> <b>intervals</b> obtained with the different tools was made.|$|R
5|$|The latest {{definite}} {{sighting of}} dodos, on Amber Island in 1662, {{may not necessarily}} {{have been the last}} members of the species. The last claimed sighting of a dodo was reported in the hunting records of Isaac Johannes Lamotius in 1688. Statistical analysis of these records by Roberts and Solow gives a new estimated extinction date of 1693, with a 95% <b>confidence</b> <b>interval</b> of 1688–1715. The authors also pointed out that because the last sighting before 1662 was in 1638, the dodo was probably already quite rare by the 1660s, and thus a disputed report from 1674 by an escaped slave cannot be dismissed out of hand.|$|E
5|$|On June 7, 2010, {{long after}} the microlensing event had subsided, the science teams {{studying}} the star used the NACO adaptive optics facility at the Very Large Telescope in Chile to determine the actual apparent magnitude of the star that microlensed its background star, hoping {{to compare it to}} the magnitude of the star measured during the microlensing event. A discrepancy was found, a discrepancy that may have been a result of either error or of a planetary body. Interpretation of follow-up observations led to the planet's confirmation. The ratio between the planet's mass and its host star's mass is well-constrained, but a large interval of uncertainty exists because the host star's mass is known within a large <b>confidence</b> <b>interval</b> that spans the mass of all red dwarf stars.|$|E
5|$|The Minoan {{eruption}} {{is a key}} {{marker for}} the Bronze Age chronology of the Eastern Mediterranean world. It provides a fixed point for aligning the entire chronology of the second millennium BCE in the Aegean, {{as evidence of the}} eruption is found throughout the region. Despite the evidence, the exact date of the eruption has been difficult to determine. Archaeologists have traditionally placed it at approximately 1500 BCE. Radiocarbon dates, including analysis of an olive branch buried beneath a lava flow from the volcano that gave a date between 1627 BCE and 1600 BCE (95% <b>confidence</b> <b>interval),</b> suggest an eruption date more than a century earlier than suggested by archaeologists. Thus, the radiocarbon dates and the archaeological dates are in substantial disagreement.|$|E
50|$|<b>Confidence</b> <b>intervals</b> {{behave in}} a {{predictable}} way. By definition, 95% <b>confidence</b> <b>intervals</b> have a 95% chance of capturing the underlying population mean (μ). This feature remains constant with increasing sample size; what changes is that the interval becomes smaller (more precise). In addition, 95% <b>confidence</b> <b>intervals</b> are also 83% prediction intervals: one experiment's <b>confidence</b> <b>intervals</b> have an 83% chance of capturing any other future experiment's mean. As such, knowing a single experiment's 95% <b>confidence</b> <b>intervals</b> give the analyst a plausible range for the population mean, and plausible outcomes of any subsequent replication experiments.|$|R
40|$|AbstractWe {{study the}} large sample {{behavior}} of the standard bootstrap, the m-out-of-n bootstrap, and the oracle bootstrap (Giurcanu and Presnell, 2009) [14] percentile <b>confidence</b> <b>intervals</b> in non-regular smooth function models. We show that the oracle bootstrap percentile <b>confidence</b> <b>intervals</b> are consistent while the standard bootstrap and the m-out-of-n bootstrap <b>confidence</b> <b>intervals</b> are inconsistent. Further analysis of coverage probabilities reveals that, for large samples, the iterated oracle bootstrap percentile <b>confidence</b> <b>intervals</b> are more accurate than their non-iterated versions. We also describe the large sample local {{behavior of the}} bootstrap <b>confidence</b> <b>intervals</b> for parameter values near the points of inconsistency of the standard bootstrap. In a simulation study, we describe the finite sample local behavior of various bootstrap <b>confidence</b> <b>intervals...</b>|$|R
40|$|In this paper, we {{consider}} simultaneous <b>confidence</b> <b>intervals</b> for all contrasts in the means when the observations are missing at random in the intraclass correlation model. An exact test statistic for {{the equality of}} the means and Scheffe, Bonferroni and Tukey types of simultaneous <b>confidence</b> <b>intervals</b> are given by an extension of Bhargava and Srivastava [On Tukey's <b>confidence</b> <b>intervals</b> for the contrasts in {{the means of the}} intraclass correlation model, J. Royal Statist. Soc. B 35 (1973) 147 - 152] when the missing observations are of the monotone type. Finally, numerical results of simultaneous <b>confidence</b> <b>intervals</b> are presented. Intraclass correlation model Contrast Scheffe's simultaneous <b>confidence</b> <b>intervals</b> Bonferroni's inequality Tukey's simultaneous <b>confidence</b> <b>intervals</b> Monte Carlo simulation...|$|R
25|$|The {{estimated}} percentage plus {{or minus}} its margin of error is a <b>confidence</b> <b>interval</b> for the percentage. In other words, {{the margin of error}} is half the width of the <b>confidence</b> <b>interval.</b> It can be calculated as a multiple of the standard error, with the factor depending of the level of confidence desired; a margin of one standard error gives a 68% <b>confidence</b> <b>interval,</b> while the estimate {{plus or minus}} 1.96 standard errors is a 95% <b>confidence</b> <b>interval,</b> and a 99% <b>confidence</b> <b>interval</b> runs 2.58 standard errors {{on either side of the}} estimate.|$|E
25|$|Intake of artificially {{sweetened}} {{soft drinks}} {{were associated with}} an increased risk of ischemic stroke, all-cause dementia, and Alzheimer’s disease dementia. When comparing daily cumulative intake to 0 per week (reference), the hazard ratios were 2.96 (95% <b>confidence</b> <b>interval,</b> 1.26–6.97) for ischemic stroke and 2.89 (95% <b>confidence</b> <b>interval,</b> 1.18–7.07) for Alzheimer’s disease.|$|E
25|$|This {{approach}} is usable, but the resulting interval {{will not have}} the repeated sampling interpretation – it is not a predictive <b>confidence</b> <b>interval.</b>|$|E
40|$|Presenting <b>confidence</b> <b>intervals</b> around {{means is}} a common method of expressing {{uncertainty}} in data. Loftus and Masson (1994) describe <b>confidence</b> <b>intervals</b> for means in within-subjects designs. These <b>confidence</b> <b>intervals</b> {{are based on the}} ANOVA mean squared error. Cousineau (2005) presents an alternative to the Loftus and Masson method, but his method produces <b>confidence</b> <b>intervals</b> that are smaller than those of Loftus and Masson. I show why this is the case and offer a simple correction that makes the expected size of Cousineau <b>confidence</b> <b>intervals</b> {{the same as that of}} Loftus and Masson <b>confidence</b> <b>intervals.</b> <b>Confidence</b> <b>intervals</b> (CIs) are a staple in the presentation of psychological data because they allow researchers to quickly gage the amount of uncertainty in data (Rouder & Morey, 2005). For within-subjects designs, there are several approaches to creating <b>confidence</b> <b>intervals.</b> For a given design it may not be clear which to choose. Consider a simple within-subjects design with two conditions, a pre-test and post-test. For this design, there are multiple methods of generating <b>confidence</b> <b>intervals.</b> I will discuss each in turn. Approaches to <b>confidence</b> <b>intervals</b> The standard way to build <b>confidence</b> <b>intervals</b> is to compute the standard error of the mean for each condition, and multiply it by the appropriate t-distribution quantile. In order to make this concrete, Table 1 lists hypothetical data for N = 10 participants. A paired t-test reveals a significant effect of condition (MSE = 5 : 19; t(9) = 3 : 65; p =: 005...|$|R
40|$|A {{generalization}} of order statistics, is presented. Using this generalization, nonparametric <b>confidence</b> <b>intervals</b> are constructed for the quantiles of an absolutely continuous distribution. Finally, an application, concerning <b>confidence</b> <b>intervals</b> for the unique median, is given. Intermediate order statistics Nonparametric <b>confidence</b> <b>intervals</b> Quantiles Median...|$|R
40|$|Abstract—This paper {{considers}} {{the construction of}} <b>confidence</b> <b>intervals</b> for a cumulative distribution function (), and its inverse quantile function 1 (), at some fixed points, and {{on the basis of}} an i. i. d. sample = = 1, where is relatively small. The sample is modeled as having a flexible, generalized gamma distribution with all three parameters being unknown. Hence, the technique can be considered as an alternative to nonparametric <b>confidence</b> <b>intervals,</b> when is a continuous random variable. The <b>confidence</b> <b>intervals</b> are constructed on the basis of Jeffreys noninformative prior. Performance of the resulting <b>confidence</b> <b>intervals</b> is studied via Monte Carlo simulations, and compared to the performance of nonparametric <b>confidence</b> <b>intervals</b> based on binomial proportion. It is demonstrated that the <b>confidence</b> <b>intervals</b> are robust; when data comes from Poisson or geometric distributions, <b>confidence</b> <b>intervals</b> based on a generalized gamma distribution outperform nonparametric <b>confidence</b> <b>intervals.</b> The theory is applied to the assessment of the reliability of the Pad Hypergol Servicing System of the Shuttle Orbiter. Index Terms—Confidence intervals, generalized gamma distribution, Jeffreys non-informative prior. KS...|$|R
25|$|Between 392,979 and 942,636 {{estimated}} Iraqi (655,000 with a <b>confidence</b> <b>interval</b> of 95%), {{civilian and}} combatant, {{according to the}} second Lancet survey of mortality.|$|E
25|$|In vitro fertilisation, {{including}} ICSI, {{is associated}} with an increased risk of imprinting disorders, with an odds ratio of 3.7 (95% <b>confidence</b> <b>interval</b> 1.4 to 9.7).|$|E
25|$|The {{presence}} of anti-thyroid antibodies {{is associated with}} an increased risk of unexplained subfertility with an odds ratio of 1.5 and 95% <b>confidence</b> <b>interval</b> of 1.1–2.0.|$|E
40|$|The article argues {{to replace}} null {{hypothesis}} significance testing by <b>confidence</b> <b>intervals.</b> Correctly interpreted, <b>confidence</b> <b>intervals</b> avoid {{the problems associated}} with null hypothesis statistical testing. <b>Confidence</b> <b>intervals</b> are formally valid, do not depend on a-priori hypotheses and do not result in trivial knowledge. The first part presents critique of null hypothesis significance testing; the second part replies to critique against <b>confidence</b> <b>intervals</b> and tries to demonstrate their superiority to null hypothesis significance testing...|$|R
40|$|Consider {{the problem}} of {{comparing}} variances of k populations with the variance of a control population. When the experimenter has a prior expectation that the variances of k populations are not less than the variance of a control population, one-sided simultaneous <b>confidence</b> <b>intervals</b> provide more inferential sensitivity than two-sided simultaneous <b>confidence</b> <b>intervals.</b> But the two-sided simultaneous <b>confidence</b> <b>intervals</b> have advantages over the one-sided simultaneous <b>confidence</b> <b>intervals</b> as they provide both lower and upper bounds for the parameters of interest. In this article, a new multiple comparison procedure is developed for comparing several variances with a control variance, which provides two-sided simultaneous <b>confidence</b> <b>intervals</b> for the ratios of variances with the control variance and maintains the inferential sensitivity of one-sided simultaneous <b>confidence</b> <b>intervals.</b> Computation of the critical constants {{for the implementation of}} the proposed procedure is discussed and the selected critical constants are tabulated. Acceptance sets Comparisons with a control Critical points Directional decisions Simultaneous <b>confidence</b> <b>intervals...</b>|$|R
40|$|In some cases, {{such as in}} the {{estimation}} of impulse responses, it has been found that for plausible sample sizes the coverage accuracy of single bootstrap <b>confidence</b> <b>intervals</b> can be poor. The error in the coverage probability of single bootstrap <b>confidence</b> <b>intervals</b> may be reduced by the use of double bootstrap <b>confidence</b> <b>intervals.</b> The computer resources required for double bootstrap <b>confidence</b> <b>intervals</b> are often prohibitive, especially in the context of Monte Carlo studies. Double bootstrap <b>confidence</b> <b>intervals</b> can be estimated using computational algorithms incorporating simple deterministic stopping rules that avoid unnecessary computations. These algorithms may make the use and Monte Carlo evaluation of double bootstrap <b>confidence</b> <b>intervals</b> feasible in cases where otherwise they would not be feasible. The efficiency gains due to the use of these algorithms are examined by means of a Monte Carlo study for examples of <b>confidence</b> <b>intervals</b> for a mean and for the cumulative impulse response in a second order autoregressive model...|$|R
25|$|If {{the exact}} {{confidence}} intervals are used, then {{the margin of}} error takes into account both sampling error and non-sampling error. If an approximate <b>confidence</b> <b>interval</b> is used (for example, by assuming the distribution is normal and then modeling the <b>confidence</b> <b>interval</b> accordingly), then {{the margin of error}} may only take random sampling error into account. It does not represent other potential sources of error or bias such as a non-representative sample-design, poorly phrased questions, people lying or refusing to respond, the exclusion of people who could not be contacted, or miscounts and miscalculations.|$|E
25|$|IVF, {{including}} ICSI, {{is associated}} with an increased risk of imprinting disorders (including Prader-Willi syndrome and Angelman syndrome), with an odds ratio of 3.7 (95% <b>confidence</b> <b>interval</b> 1.4 to 9.7).|$|E
25|$|When {{the sample}} {{is not a simple}} random sample from a large population, the {{standard}} error and the <b>confidence</b> <b>interval</b> must be estimated through more advanced calculations. Linearization and resampling are widely used techniques for data from complex sample designs.|$|E
5000|$|One {{can also}} compute <b>confidence</b> <b>intervals</b> {{matching}} the test decision using the Šidák correction by using 100(1 − α)1/m% <b>confidence</b> <b>intervals.</b>|$|R
40|$|Classical <b>confidence</b> <b>intervals</b> {{are often}} {{misunderstood}} by particle physicists {{and the general}} public alike. The confusion arises from the two different definitions of probability in common use. As a result, there is general dissatisfaction when <b>confidence</b> <b>intervals</b> are empty or they exclude parameter values for which the experiment is insensitive. In order to clarify these issues, the relation between <b>confidence</b> <b>intervals</b> and credible intervals is explored. A method to identify problematic <b>confidence</b> <b>intervals</b> is demonstrated for two cases. ...|$|R
40|$|This note {{presents}} {{three ways}} of constructing simultaneous <b>confidence</b> <b>intervals</b> for linear estimates of linear functionals in inverse problems, including "Backus-Gilbert" estimates. Simultaneous <b>confidence</b> <b>intervals</b> {{are needed to}} compare estimates, for example, to find spatial variations in a distributed parameter. The notion of simultaneous <b>confidence</b> <b>intervals</b> is introduced using coin tossing as an example before moving to linear inverse problems. The first method for constructing simultaneous con dence intervals is based on the Bonferroni inequality, and applies generally to <b>confidence</b> <b>intervals</b> for any set of parameters, from dependent or independent observations. The second method for constructing simultaneous <b>confidence</b> <b>intervals</b> in inverse problems is based on a "global" measure of fit to the data, which allows one to compute simultaneous <b>confidence</b> <b>intervals</b> for any number of linear functionals of the model that are linear combinations of the data mappings. This leads to <b>confidence</b> <b>intervals</b> whose widths depend on percentage points of the chi-square distribution with n degrees of freedom, where n is the number of data. The third method uses the joint normality of the estimates to nd shorter <b>confidence</b> <b>intervals</b> than the other methods give, at the cost of evaluating some integrals numerically...|$|R
25|$|The second survey {{published}} on 11 October 2006, estimated 654,965 excess deaths {{related to the}} war, or 2.5% of the population, {{through the end of}} June 2006. The new study applied similar methods and involved surveys between May 20 and July 10, 2006. More households were surveyed, allowing for a 95% <b>confidence</b> <b>interval</b> of 392,979 to 942,636 excess Iraqi deaths. 601,027 deaths (range of 426,369 to 793,663 using a 95% <b>confidence</b> <b>interval)</b> were due to violence. 31% (186,318) of those were attributed to the US-led Coalition, 24% (144,246) to others, and 46% (276,472) unknown. The causes of violent deaths were gunshot (56% or 336,575), car bomb (13% or 78,133), other explosion/ordnance (14%), air strike (13% or 78,133), accident (2% or 12,020), and unknown (2%).|$|E
25|$|This {{section will}} briefly discuss the {{standard}} error of a percentage, the corresponding <b>confidence</b> <b>interval,</b> and connect these two concepts to {{the margin of}} error. For simplicity, the calculations here assume the poll {{was based on a}} simple random sample from a large population.|$|E
25|$|While {{the margin}} of error {{typically}} reported in the media is a poll-wide figure that reflects the maximum sampling variation of any percentage based on all respondents from that poll, the term margin of error also refers to the radius of the <b>confidence</b> <b>interval</b> for a particular statistic.|$|E
40|$|Quantiles, {{which are}} known as values-at-risk in finance, are often used to measure risk. <b>Confidence</b> <b>intervals</b> provide a way of {{assessing}} the error of quantile estimators. When estimating extreme quantiles using crude Monte Carlo, the <b>confidence</b> <b>intervals</b> may have large half-widths, thus motivating the use of variance-reduction techniques (VRTs). This paper develops methods for constructing <b>confidence</b> <b>intervals</b> for quantiles when applying the VRT importance sampling. The <b>confidence</b> <b>intervals,</b> which are asymptotically valid {{as the number of}} samples grows large, are based on a technique known as sectioning. Empirical results seem to indicate that sectioning can lead to <b>confidence</b> <b>intervals</b> having better coverage than other existing methods. ...|$|R
40|$|The {{problem of}} {{constructing}} <b>confidence</b> <b>intervals</b> for the long-memory parameter of stationary Gaussian processes with long-range dependence is studied. The {{focus is on}} <b>confidence</b> <b>intervals</b> for the wavelet estimator introduced by Abry and Veitch (1998). An approximation to {{the distribution of the}} estimator based on subsampling is proposed. Such an approximation is used to construct <b>confidence</b> <b>intervals</b> for the long-memory parameter. The performance of these <b>confidence</b> <b>intervals,</b> in terms of both coverage probability and length, is studied by using a Monte Carlo simulation. The proposed <b>confidence</b> <b>intervals</b> have more accurate coverage probability than the method of Veitch and Abry (1999), and are easy to compute in practice...|$|R
40|$|Bayesian <b>confidence</b> <b>intervals</b> of a {{smoothing}} spline {{are often}} used to distinguish two curves. In this paper, we provide an asymptotic formula for sample-size calculations based on Bayesian <b>confidence</b> <b>intervals.</b> Approximations and simulations on special functions indicate that this asymptotic formula is reasonably accurate. Bayesian <b>confidence</b> <b>intervals</b> Sample size Smoothing spline...|$|R
