1|10000|Public
40|$|Electric Power {{is today}} playing an {{increasingly}} {{important role in the}} life of the community. In the electric power system the production and transmission of power are two predominant factors. For the purpose of transmission of electricity towers are the main medium with some wires at required distances and altitudes. The remotehydroelectric power plants have given rise to the need for extra high voltage. Prior to 1950, 150 kV electric transmission lines were considered and still higher voltages are being considered these days. Hence it has given rise to the need for relative tall structures such as towers. Thus the study of designing and erection of steel towers has become a challenging task. Transmission line tower normally omprise of several hundred - angle members eccentrically connected. Structural analysis of this type of structure requires extensive data generation. Conventional process of data generation in describing the topology, geometry, load and support conditions are very tedious, time consuming and susceptible to error. In general, most towers may be idealized as statically determinate and analyzed for forces. However, the computations are very lengthy and there are many <b>chances</b> <b>of</b> <b>making</b> <b>errors</b> in such cases. Further if a member is to be redesigned then the entire wind load computations have to be repeated. The computational problem in tower is more acute and tedious as the load depends on the member sizes. A rational and economical design can be made with the help of software’s like STADD Pro 2003, SAP- 2000. In this dissertation an attempt is made to develop a software for load calculations on transmission line towers of capacity 220 kV, only as per new code i. e., IS – 802 (Part- I / Sec- 1) : 1995 by considering reliability, security, safety and anti-cascading conditions. In MS-office the EXCEL is used to develop program for wind load calculations on the tower and other loads like wind load on conductor, conductor weights are calculated manually. Then these loads are applied on the model done in the STADD Pro for analysis. Different types of failure conditions are modeled and their effects on the performance of the tower are studied...|$|E
40|$|Synchronous {{hardware}} can be modelled as {{a mapping}} from input and state to output {{and a new}} state, such mappings {{are referred to as}} transition functions. It is natural to use a functional language to implement transition functions. The CaSH compiler is capable of translating transition functions to VHDL. Modelling hardware using multiple components is convenient. Components in CaSH can be considered as instantiations of functions. To avoid packing and unpacking state when composing components, functions are lifted to arrows. By using arrows the <b>chance</b> <b>of</b> <b>making</b> <b>errors</b> will decrease as it is not required to manually (un) pack the state. Furthermore, the Haskell do-syntax for arrows increases the readability of hardware designs. This is demonstrated using a realistic example of a circuit which consists of multiple components...|$|R
40|$|It {{has been}} demonstrated, using {{abstract}} psychophysical stimuli, that speeds appear slower when contrast is reduced under certain conditions. Does this effect {{have any real}} life consequences? One previous study has found, using a low fidelity driving simulator, that participants perceived vehicle speeds to be slower in foggy conditions. We replicated this finding with a more realistic video-based simulator using the Method of Constant Stimuli. We also found that lowering contrast reduced participants’ ability to discriminate speeds. We argue that these reduced contrast effects could partly explain the higher crash rate of drivers with cataracts (this is a substantial societal problem and the crash relationship variance can {{be accounted for by}} reduced contrast). Note that even if people with cataracts can calibrate for the shift in their perception of speed using their speedometers (given that cataracts are experienced over long periods), they may still have an increased <b>chance</b> <b>of</b> <b>making</b> <b>errors</b> in speed estimation due to poor speed discrimination. This could result in individuals misjudging vehicle trajectories and thereby inflating their crash risk. We propose interventions that may help address this problem...|$|R
40|$|The repetitious {{nature of}} air {{conditioning}} calculation leads to arithmetic errors {{and a high}} cost of designing air conditioning sys-tems. Short cut calculations are employed to limit the calculation and the <b>chance</b> <b>of</b> <b>making</b> <b>errors.</b> Computers with their high speed of calculation {{and the ability to}} make limited "logic's decisions are ideally suited for air condition-ing calculations. Previously programs were developed to calculate the heating and cooling loads, design the duct system, design the water and steam pipes and to evaluate designed systems. No work had been done to develop a program for the air analysis for the building. Most programs developed thus far are proprietory and thus not any engineers can use them. Also they use old data and methods of calculation. This thesis discusses the development of two new programs. The first is a new program to calculate the cooling and heating loads for buildings. This program Incorporates the latest data and methods in calculating the solar loads. The second is a new program to perform the air analysis for different types of air distribution systems. The two programs were tested on a small school structure. The results compared favorably with those obtained using the conventional method...|$|R
40|$|This study {{investigated}} self-monitoring {{in children with}} autism spectrum disorder (ASD) with event-related potentials looking at both the error-related negativity (ERN) and error-related positivity (Pe). The ERN is related to early error/conflict detection, and the Pe {{has been associated with}} conscious error evaluation or attention allocation. In addition, post-error slowing in reaction times (RTs) was measured. Children with ASD and age- and IQ- matched controls were administered an easy and a hard version of an auditory decision task. Results showed that the ERN was smaller in children with ASD but localized in the anterior cingulate cortex (ACC) in both groups. In addition we found a negativity on correct trials (CRN) that did not differ between the groups. Furthermore, a reduced Pe and a lack of post-error slowing in RTs were found in children with ASD. The reduced ERN in children with ASD, in the presence of an intact CRN, might suggest a specific insensitivity to detect situations in which the <b>chance</b> <b>of</b> <b>making</b> <b>errors</b> is enhanced. This might in turn lead to reduced error awareness/attention allocation to the erroneous event (reduced Pe) and eventually in a failure in change of strategy to deal with a situation, as becomes evident from the lack of post-error slowing in the ASD group. This relates well to the perseverative behaviour that is seen in children with ASD. We discuss these results in terms of a general deficit in self-monitoring, underlying social disturbance in ASD and the involvement of the ACC...|$|R
40|$|Abstract Background On average, health worsens with age, {{but many}} people have periods of improvement. A {{stochastic}} model provides an excellent description of how such changes occur. Given that cognition also changes with age, we wondered whether the same model might also describe the accumulation of errors in cognitive test scores in community-dwelling older adults. Methods In this prospective cohort study, 8954 older people (aged 65 + at baseline) from the Canadian Study of Health and Aging were followed for 10 years. Cognitive status was defined {{by the number of}} errors on the 100 -point Modified Min-Mental State Examination. The error count was chosen to parallel the deficit count in the general model of aging, which is based on deficit accumulation. As with the deficit count, a Markov chain transition model was employed, with 4 parameters. Results On average, the <b>chance</b> <b>of</b> <b>making</b> <b>errors</b> increased linearly with the number of errors present at each time interval. Changes in cognitive states were described with high accuracy (R 2 = 0. 96) by a modified Poisson distribution, using four parameters: the background <b>chance</b> <b>of</b> accumulating additional errors, the <b>chance</b> <b>of</b> incurring more or fewer errors, given the existing number, and the corresponding background and incremental <b>chances</b> <b>of</b> dying. Conclusion The change in the number of errors in a cognitive test corresponded to a general model that also summarizes age-related changes in deficits. The model accounts for both improvement and deterioration and appears to represent a clinically relevant means of quantifying how various aspects of health status change with age. </p...|$|R
40|$|An {{important}} {{human factors}} research interest area is error reduction. Although pilots placed in highly stressful situations {{have an increased}} <b>chance</b> <b>of</b> <b>making</b> <b>errors,</b> they use coping skills to lower their stress level and {{reduce the likelihood of}} errors. Typically, coping skills are conceptually separated into three different types: active coping skills which attack and change the situation to make it inherently less stressful, emotionfocused coping skills which use discussion or thinking about the situation in a different way to diminish the negative emotional reaction associated with the stressful situation, and avoidant coping skills which allow one to mentally and/or physically disengage through the use of daydreams, sleep, drugs, and/or alcohol. In this research project, a sample of 49 inexperienced private pilots and 30 experienced multi-engine commercial pilots were surveyed to determine if significant differences existed between their levels of perceived stress and the frequency with which they used different types of coping skills using a one-time, written survey. Variables measured included demographic information, factors of personality, frequency of binge drinking, perceived level of stress, and coping skills usage. The results showed that there was an association between experience level and stress (F = 5. 46, p =. 022), emotional coping, (r =. 200, p =. 078) and instrumental coping (r =. 201, p =. 075). There was also an association between stress and self-blame (r =. 273, p =. 015), humor (r = -. 214, p =. 059), positive reframing (r = -. 204, p =. 071), and the frequency of binge drinking (-. 200, p =. 078) ...|$|R
40|$|Abstract Background Readers may {{question}} {{the interpretation of}} findings in clinical trials when multiple outcome measures are used without adjustment of the p-value. This question arises because of the increased risk of Type I errors (findings of false "significance") when multiple simultaneous hypotheses are tested at set p-values. The primary {{aim of this study}} was to estimate the need to make appropriate p-value adjustments in clinical trials to compensate for a possible increased risk in committing Type I errors when multiple outcome measures are used. Discussion The classicists believe that the <b>chance</b> <b>of</b> finding at least one test statistically significant due to chance and incorrectly declaring a difference increases as the number of comparisons increases. The rationalists have the following objections to that theory: 1) P-value adjustments are calculated based on how many tests are to be considered, and that number has been defined arbitrarily and variably; 2) P-value adjustments reduce the <b>chance</b> <b>of</b> <b>making</b> type I <b>errors,</b> but they increase the <b>chance</b> <b>of</b> <b>making</b> type II <b>errors</b> or needing to increase the sample size. Summary Readers should balance a study's statistical significance with the magnitude of effect, the quality of the study and with findings from other studies. Researchers facing multiple outcome measures might want to either select a primary outcome measure or use a global assessment measure, rather than adjusting the p-value. </p...|$|R
40|$|Due to grave {{potential}} human, {{environmental and}} economical consequences of collisions at sea, collision avoidance {{has become an}} important safety concern in navigation. To {{reduce the risk of}} collisions at sea, appropriate collision avoidance actions need to be taken in accordance with the regulations, i. e., International Regulations for Preventing Collisions at Sea. However, the regulations only provide qualitative rules and guidelines, and therefore it requires navigators to decide on collision avoidance actions quantitatively by using their judgments which often leads to <b>making</b> <b>errors</b> in navigation. To better help navigators in collision avoidance, this paper develops a comprehensive collision avoidance decision making model for providing whether a collision avoidance action is required, when to take action and what action to be taken. The model is developed based on three types of collision avoidance actions, such as course change only, speed change only, and a combination of both. The model has potential to reduce the <b>chance</b> <b>of</b> <b>making</b> human <b>error</b> in navigation by assisting navigators in decision making on collision avoidance actions...|$|R
40|$|Background: So far 11 therapy {{studies have}} been {{reported}} which aimed to re-teach semantic knowledge in brain-damaged patients presenting with a semantic deficit consecutive to stroke, herpes encephalitis, or semantic dementia. All these semantic therapy studies but one recorded a significant improvement in the patients' performance on tasks requiring semantic processing. The exception to this pattern was the semantic therapy study by Sartori, Miozzo, and Job (1994), which yielded negative results. Because the study concerned two patients with anterograde amnesia associated with the semantic deficit, Sartori et at. concluded that reacquiring semantic knowledge was not possible when such association of deficits was present. Aims: Sartori et al. 's study, {{like all the other}} semantic therapy studies, applied an errorful learning procedure during the therapy. However, the question can be raised of whether such procedure is appropriate when amnesia is associated with the semantic deficit. Because error elimination is likely a function of explicit memory, which is impaired in amnesic patients, wrong stimulus-response associations would be repeatedly retrieved and strengthened in (spared) implicit memory, thus preventing the patient from learning novel semantic knowledge. In the present single-case study we addressed this issue by using an errorless learning procedure during semantic therapy in a post-encephalitis patient (DL) who suffered both a semantic deficit and anterograde amnesia. Methods & Procedures: The therapy aimed at re-teaching semantic attributes of 16 items. The design included, further to these 16 target items, 16 contrast and 16 control items, which were semantic coordinates of the target items. Both shared (category) and distinctive (non-category) attributes were included in the learning set. Learning was based on an attribute classification task in which the properties of the target items had to be contrasted with those of coordinate items, within a paradigm that greatly reduced the <b>chance</b> <b>of</b> <b>making</b> <b>errors.</b> A pre- and post-therapy picture naming and an attribute verification task allowed us to assess the therapy effects at the end of therapy and 1 year later. Outcomes & Results: Significant therapy effects were observed in the attribute verification task and were still present 1 year afterwards. Thus, the patient's performance significantly improved for the category (i. e., shared) attributes of the target, contrast, and control items, and for the non-category (i. e., distinctive) attributes of the target items. Conclusions: This finding showed that, contrary to Sartori et al. 's claim, re-acquiring semantic knowledge was possible in a patient with anterograde amnesia associated with her semantic deficit...|$|R
40|$|Uncertainty is {{a crucial}} issue for {{producers}} who must make input decisions without knowing prices and without perfect knowledge of realized output. In this context, price expectations strongly determine the production choices and market prices that result from market-clearing conditions. This study analyzed the role that price expectations play in price dynamics, developing a theoretical model of trade in varieties following Armington (1969) and augmented with yield and price uncertainty to highlight several main determinants of domestic producer prices, including exchange rates, proximity to world markets, input prices, natural disasters, and producers' expectations. An econometric estimation of the rice sector, using a panel of 13 developing Asian countries during 1965 - 2003, confirmed that expectations count, with a 1 % increase in the expected price resulting in a 1. 18 % decrease in the market price. A simulation exercise based on these empirical results demonstrated that forecasting errors are large. Specifically, Asian rice farmers have a 50 % <b>chance</b> <b>of</b> <b>making</b> prediction <b>errors</b> <b>of</b> 10 % or more on the final market price. This high error rate suggests the need for developing ways of sharing information, such as radio programs dedicated to agricultural producers or the introduction of futures markets, to stabilize agricultural incomes. Rice; Asia; price dynamics; price expectations;...|$|R
40|$|AbstractClassification is a {{fundamental}} image processing task. Recent empirical evidence suggests that classification algorithms which make use of redundant linear transforms will regularly outperform their nonredundant counterparts. We provide a rigorous explanation of this phenomenon in the single-class case. We begin by developing a measure-theoretic analysis of the set of points at which a given decision rule will have an intolerable <b>chance</b> <b>of</b> <b>making</b> a classification <b>error.</b> We then apply this general theory to the special case where the class is compact and convex, showing that such a class may be arbitrarily well approximated by frame sets, namely, preimages of hyperrectangles under frame analysis operators. This leads to a frame-based classification scheme in which frame coefficients are regarded as features. We show that, indeed, the accuracy of such a classification scheme approaches perfect accuracy as the redundancy of the frame grows large...|$|R
40|$|Classification is a {{fundamental}} image processing task. Recent empirical evidence suggests that classification algorithms which make use of redundant linear transforms will regularly outperform their nonredundant counterparts. We provide a rigorous explanation of this phenomenon in the single-class case. We begin by developing a measure-theoretic analysis of the set of points at which a given decision rule will have an intolerable <b>chance</b> <b>of</b> <b>making</b> a classification <b>error.</b> We then apply this general theory to the special case where the class is compact and convex, showing that such a class may be arbitrarily well-approximated by frame sets, namely, preimages of hyperrectangles under frame analysis operators. This leads to a framebased classification scheme in which frame coefficients are regarded as features. We show that, indeed, the accuracy of such a classification scheme approaches perfect accuracy as the redundancy of the frame grows large. Key words: classification, frames, convex sets, decision rule...|$|R
40|$|Popular {{procedures}} {{to control the}} <b>chance</b> <b>of</b> <b>making</b> type I <b>errors</b> when multiple statistical tests are performed come at a high cost: a reduction in power. As the number of tests increases, power for an individual test may become unacceptably low. This is a consequence <b>of</b> minimizing the <b>chance</b> <b>of</b> <b>making</b> even a single type I error, which is the aim of, for instance, the Bonferroni and sequential Bonferroni procedures. An alternative approach, control of the false discovery rate (FDR), has recently been advocated for ecological studies. This approach aims at controlling the proportion of significant results that are in fact type I errors. Keeping the proportion of type I errors low among all significant results is a sensible, powerful, and easy-to-interpret way of addressing the multiple testing issue. To encourage practical use of the approach, in this note we illustrate how the proposed procedure works, we compare it to more traditional methods that control the familywise error rate, and we discuss some recent useful developments in FDR control...|$|R
40|$|Professionals {{working in}} risky or {{emergency}} situations {{have to make}} very accurate decisions, while {{the quality of the}} decisions might be affected by the stress that these situations bring about. Integrating task feedback and biofeedback into computer-based training environments could improve trainees’ stress-coping behaviour. This paper presents and assesses a refined version of the cognitive performance and error (COPE) model that describes the effects of stressful events on decisions as a foundation for such a support tool. Within a high-fidelity simulator of a ship’s bridge at the Royal Netherlands Naval College, students of the naval college (n = 10) were observed while completing a 2 -h-long shadowing and boarding operation combined with a search-and-rescue operation. For every action, variables were measured: objective and subjective task demand, challenge and threat appraisal, and arousal based on heart rate and heart rate variability. The data supported the COPE model and were used to create predictive models. The variables could provide minute-by-minute predictions of performance that can be divided into performance rated by experts and errors. The predictions for performance rated by experts correlated with the observed data (r = 0. 77), and 68. 3 % of the predicted errors were correct. The error predictions concern the <b>chances</b> <b>of</b> <b>making</b> specific <b>errors</b> <b>of</b> communication, planning, speed, and task allocation. These models will be implemented into a real-time feedback system for trainees performing in stressful simulated training tasks. © 2015, The Author(s) ...|$|R
40|$|A Doctoral Thesis. Submitted in partial {{fulfillment}} of the requirements for the award of Doctor of Philosophy of Loughborough University. This thesis deals with selection strategies in gaze interaction, specifically for a context where gaze is the sole input modality for users with severe motor impairments. The goal has been {{to contribute to the}} subfield of assistive technology where gaze interaction is necessary for the user to achieve autonomous communication and environmental control. From a theoretical point of view research has been done on the physiology of the gaze, eye tracking technology, and a taxonomy of existing selection strategies has been developed. Empirically two overall approaches have been taken. Firstly, end-user research has been conducted through interviews and observation. The capabilities, requirements, and wants of the end-user have been explored. Secondly, several applications have been developed to explore the selection strategy of single stroke gaze gestures (SSGG) and aspects of complex gaze gestures. The main finding is that single stroke gaze gestures can successfully be used as a selection strategy. Some of the features of SSGG are: That horizontal single stroke gaze gestures are faster than vertical single stroke gaze gestures; That there is a significant difference in completion time depending on gesture length; That single stroke gaze gestures can be completed without visual feedback; That gaze tracking equipment has a significant effect on the completion times and error rates of single stroke gaze gestures; That there is not a significantly greater <b>chance</b> <b>of</b> <b>making</b> selection <b>errors</b> with single stroke gaze gestures compared with dwell selection. The overall conclusion is that the future of gaze interaction should focus on developing multi-modal interactions for mono-modal input...|$|R
40|$|When {{conducting}} a statistical test, we typically set α =. 05 or α =. 01 {{so that the}} probability <b>of</b> <b>making</b> a Type I error is. 05 or. 01. Suppose, however, we conduct 100 such tests. Further suppose that the null hypothesis for each test is, in fact, true. Although α for each individual test may be. 05, the probability <b>of</b> <b>making</b> at least one Type I error across the entire set of 100 tests is much greater than. 05. Why? Because the more tests we do, the greater the <b>chances</b> <b>of</b> <b>making</b> an <b>error.</b> More precisely, the probability <b>of</b> <b>making</b> at least one Type I error is P (at least one Type I error) = αF W = 1 − (1 − αP C) C (1) where C {{is the number of}} tests performed 1. αP C is the per comparison Type I error rate; it represents the probability <b>of</b> <b>making</b> a Type I error for each test. αF W, on the other hand, is the familywise Type I error rate, and it represents the probability <b>of</b> <b>making</b> a Type I error across the entire family, or set, of tests. For the one-way designs we are considering, αF W also equals the α level for the entire experiment, or the experimentwise error rate (αEW). For the current example, α =. 05, C = 100, and so αF W = 0. 994079, which means that it is very likely that we would make at least one Type I error in our set of 100 tests. Here, in a nutshell, is the problem of conducting multiple tests of group means: the probability <b>of</b> <b>making</b> a Type I error increases with the number of tests. If the number of tests, C, is large, then it becomes very likely that we will make a Type I error. When we are conducting multiple tests on group means, we generally want to minimize Type I errors across the entire experiment, and so we need some way of maintaining αEW at some reasonably low level (e. g.,. 05). One obvious way of controlling αEW is to rearrange Equation 1 to calculate the αP C that is required for a given αF W and C: αP C = 1 − (1 − αF W) 1 /C (2) According to Equation 2, when C = 100 and we want αF W =. 05, we must set αP C to. 0005128...|$|R
40|$|In this paper, we {{construct}} {{a family of}} WZW models with N = 2 superconformal symmetry, including a deformation of the Kazama-Suzuki model. Recall that if V is a complex vector space with a non-degenerate bilinear form, a polarization of V is a splitting of V, V = V+ ⊕ V−, into complementary isotropic subspaces. If the vector space is a Lie algebra g and the inner product is invariant, it is natural to restrict attention to polarizations g = g+ ⊕ g − such that g ± are isotropic Lie subalgebras of g; such a structure is called a Manin triple. Manin triples arise naturally in Drinfeld’s approach to completely integrable systems and quantum groups [3]. As we explain in Section 1, Spindel, Sevrin, Troust and van Proyen ([10], [9]) have shown how to associate to a Manin triple an N = 2 superconformal field theory (the work of Kazama-Suzuki [7] is a special case of their results). We will {{construct a}} deformation of this theory, parametrized by an element α ∈ g 0; here, g 0 is the subspace of g orthogonal to [g+, g+] ⊕ [g−, g−] ⊂ g. The N = 2 central charge of the deformed model is d = 1 dim g − (ρ, ρ) − (α, α), 2 where ρ is a certain element of g, determined by {{the structure of a}} Manin triple on g. In Section 2, we give an exposition of some formulas from the theory of Manin triples which we need. We show how to associate a Manin triple to a pair (k,p), where k is a simple Lie algebra and p is a parabolic subalgebra of k. If p = b is a Borel subalgebra, we obtain the Kazama-Suzuki model, while if p = k, we obtain the G/G model. We will see that in the first case, any value of the central charge may be realized, while in the second case, ρ and α lie in g+, and thus d = dimk is independent of the choice of α. In Section 3, we give the proof of N = 2 superconformal symmetry in our model. In an appendix, we explain the formulas which are used in the manipulation of operator products: while straightforward, their application leads to very complicated formulas, and the <b>chances</b> <b>of</b> <b>making</b> an <b>error</b> are greatly reduced by the availability of an excellent Mathematica package (Thielemans [11]) ...|$|R
40|$|Humans {{regularly}} help strangers, {{even when}} interactions are apparently unobserved and {{unlikely to be}} repeated. Such situations have been simulated in the laboratory using anonymous one-shot games (e. g. prisoner's dilemma) where the payoff matrices used make helping biologically altruistic. As in real-life, participants often cooperate in the lab in these one-shot games with non-relatives, despite that fact that helping is under negative selection under these circumstances. Two broad explanations for such behavior prevail. The 'big mistake' or 'mismatch' theorists argue that behavior is constrained by psychological mechanisms that evolved predominantly {{in the context of}} repeated interactions with known individuals. In contrast, the cultural group selection theorists posit that humans have been selected to cooperate in anonymous one-shot interactions due to strong between-group competition, which creates interdependence among in-group members. We present these two hypotheses before discussing alternative routes by which humans could increase their direct fitness by cooperating with strangers under natural conditions. In doing so, we explain why the standard lab games do not capture real-life in various important aspects. First, asymmetries in the cost of perceptual errors regarding the context of the interaction (one-shot versus repeated; anonymous versus public) might have selected for strategies that minimize the <b>chance</b> <b>of</b> <b>making</b> costly behavioral <b>errors.</b> Second, helping strangers might be a successful strategy for identifying other cooperative individuals in the population, where partner choice can turn strangers into interaction partners. Third, in many real-world situations individuals are able to parcel investments such that a one-shot interaction is turned into a repeated game of many decisions. Finally, in contrast to the assumptions of the prisoner's dilemma model, it is possible that benefits of cooperation follow a non-linear function of investment. Non-l...|$|R
5000|$|The {{technique}} sensitivity or {{the risk}} <b>of</b> <b>making</b> <b>errors</b> during application is low ...|$|R
60|$|DAISY. Of course not. It'll {{give me a}} <b>chance</b> <b>of</b> <b>making</b> Mr. Conway's acquaintance.|$|R
5000|$|Stan the Man from BBC Bargain Hunt {{who always}} give contestants a decent deal and a <b>chance</b> <b>of</b> <b>making</b> a profit ...|$|R
6000|$|... 'You {{threw away}} a {{wonderful}} <b>chance</b> <b>of</b> <b>making</b> all sorts <b>of</b> money. Why, a single tip from Mr Breitstein {{would have made}} your fortune.' ...|$|R
50|$|At the half-way {{point for}} {{collecting}} signatures, {{it appeared that}} only the five major candidates and possibly Lugner and Awadalla had a <b>chance</b> <b>of</b> <b>making</b> the ballot.|$|R
60|$|This was self-evident. An enemy on {{the rock}} above {{would be able to}} fire down through the roof, without their having a <b>chance</b> <b>of</b> <b>making</b> an effectual reply.|$|R
6000|$|... "It {{is not so}} {{well paid}} as it should be," [...] said the wily Tredgold, [...] "but I suppose one gets <b>chances</b> <b>of</b> <b>making</b> money in outside ways sometimes." ...|$|R
60|$|There was but one <b>chance</b> <b>of</b> <b>making</b> {{her return}} to the Adagio--the <b>chance</b> <b>of</b> hitting on a {{suggestion}} which would satisfy and quiet her. Julius laid his violin on the piano, and considered the question before him carefully.|$|R
5000|$|... 2. Consumer’s Expenses - Companies {{are usually}} {{interested}} at the consumer’s expense. The multinational companies commonly {{have the power}} of monopoly that gives them the <b>chance</b> <b>of</b> <b>making</b> excess profit.|$|R
6000|$|... "Well, well, {{we shall}} {{see if there is}} a <b>chance</b> <b>of</b> <b>making</b> a {{successful}} fight," [...] Captain Martin said, unable to resist a smile at the sailor's way of putting it.|$|R
6000|$|... "No. Or he wouldn't have lusitaniaed. This war was {{his first}} <b>chance</b> <b>of</b> <b>making</b> his name, and he chucked it all away {{for the sake of}} showin' off as a foul Gottstrafer." ...|$|R
6000|$|... "Of course. I take it {{that that}} was why you came over here, because you {{realised}} how you were wasting your life and wanted a <b>chance</b> <b>of</b> <b>making</b> good in my office." ...|$|R
50|$|Although born in Italy, Valcareggi's {{mother is}} Greek, {{allowing}} him to compete for Greece. He chose to compete for Greece because {{he felt he had}} a better <b>chance</b> <b>of</b> <b>making</b> their national team.|$|R
50|$|Kinney was {{selected}} by the Chicago Fire with the 45th overall pick of the 2010 MLS SuperDraft. He then headed to Fire training camp with a strong <b>chance</b> <b>of</b> <b>making</b> the final squad.|$|R
50|$|Penrith's {{victory over}} Cronulla in the dying moments saw the clear eight cut and dried, {{although}} the Panthers {{did have a}} small <b>chance</b> <b>of</b> <b>making</b> the final eight had results gone their way.|$|R
6000|$|... "A fine master. Is it {{owing to}} his lessons that you lost the <b>chance</b> <b>of</b> <b>making</b> a fortune and earn your living now by serving behind a counter in a ten cent store?" ...|$|R
6000|$|... "They {{would have}} no <b>chance</b> <b>of</b> <b>making</b> {{communication}} with him were there a dozen of them, wife. Long Tom and his comrades will take good care that none come near enough for speech." ...|$|R
