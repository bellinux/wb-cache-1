10000|2922|Public
5|$|Given {{the number}} of {{navigation}} algorithms calculating simultaneously to provide all the information in real time, {{research has been conducted}} to improve the <b>computation</b> <b>time</b> of some numerical methods using field-programmable gate arrays. The research focused on visual perception. The first part was focused on the simultaneous localization and mapping with an extended Kalman filter that estimates the state of a dynamic system from a series of noisy or incomplete measures. The second focused on the location and the detection of obstacles.|$|E
5|$|Instead of {{generating}} {{a set of}} woven source code, some AspectJ weavers instead weave the aspects and classes together directly into bytecode, acting both as the aspect weaver and compiler. While {{it is expected that}} the performance of aspect weavers which also perform the compilation process will require more <b>computation</b> <b>time</b> due to the weaving process involved. However, the bytecode weaving process produces more efficient runtime code than would usually be achieved through compiled woven source.|$|E
25|$|Uspensky's transformations are not {{the ones}} {{described}} in Vincent's theorem, and consequently, his transformations take twice as much <b>computation</b> <b>time</b> as the ones needed for Vincent's method.|$|E
30|$|<b>Computation</b> <b>times</b> are short.|$|R
30|$|As ‘the {{number of}} blocks’ (hereafter {{abbreviated}} by NB) increased, <b>computation</b> <b>times</b> for the block partition (BP) and the combining of block images (CB) increased. Increments in <b>computation</b> <b>times</b> were, however, very small. The {{time for the}} single block iteration, however, linearly decreased as NB increased. There were also some slight increments in RSE and IN as NB increased.|$|R
40|$|We {{examine the}} use of {{adiabatic}} quantum algorithms to solve structured, or nested, search problems. We construct suitable time dependent Hamiltonians and derive the <b>computation</b> <b>times</b> for a general class of nested searches involving n qubits. As expected, we find that as additional structure is included, the Hamiltonians become more local and the <b>computation</b> <b>times</b> decrease. ...|$|R
25|$|In 2014, Padma et al. used {{combined}} wavelet statistical texture {{features to}} segment and classify AD benign and malignant tumor slices. Zhang et al. found kernel {{support vector machine}} decision tree had 80% classification accuracy, with an average <b>computation</b> <b>time</b> of 0.022s for each image classification.|$|E
25|$|The {{classical}} {{method of}} multiplying two n-digit numbers requires n2 simple multiplications. Multiplication algorithms {{have been designed}} that reduce the <b>computation</b> <b>time</b> considerably when multiplying large numbers. In particular for very large numbers, methods based on the Discrete Fourier Transform can {{reduce the number of}} simple multiplications to the order of n log2(n) log2 log2(n).|$|E
25|$|The depth-first minimax {{strategy}} will use <b>computation</b> <b>time</b> proportional to game's tree-complexity, since it must explore the whole tree, and {{an amount of}} memory polynomial in the logarithm of the tree-complexity, since the algorithm must always store one node of the tree at each possible move-depth, {{and the number of}} nodes at the highest move-depth is precisely the tree-complexity.|$|E
5000|$|Use {{new generations}} of GPUs through the CUDA API to greatly improve <b>computation</b> <b>times</b> ...|$|R
30|$|Our {{approach}} enables addressing complex WLO techniques, {{since the}} <b>computation</b> <b>times</b> are drastically reduced while providing {{high levels of}} accuracy.|$|R
40|$|We {{propose a}} new {{geometric}} multigrid solver for anisotropic image diffusion. Anisotropic diffusion in image processing {{has been widely}} accepted as a denoising method; however, the large <b>computation</b> <b>times</b> for large volumes are prohibitive for interactive exploration of the parameter space. Our approach is able to reduce <b>computation</b> <b>times</b> via a new method for restricting the anisotropic diffusion operator to coarser grids for the multigrid solver. This operator restriction is based on computing equivalent conductance between two nodes in an electrical circuit. ...|$|R
25|$|Using the Gelfond–Schneider theorem and Lindemann–Weierstrass theorem {{many of the}} {{standard}} elementary functions can be proved to return transcendental results when given rational non-zero arguments; therefore it is always possible to correctly round such functions. However, determining a limit for a given precision on how accurate results need to be computed, before a correctly rounded result can be guaranteed, may demand a lot of <b>computation</b> <b>time.</b>|$|E
25|$|The {{development}} of fast algorithms for DFT {{can be traced}} to Gauss's unpublished work in 1805 when he needed it to interpolate the orbit of asteroids Pallas and Juno from sample observations. His method was very similar to the one published in 1965 by Cooley and Tukey, who are generally credited for the invention of the modern generic FFT algorithm. While Gauss's work predated even Fourier's results in 1822, he did not analyze the <b>computation</b> <b>time</b> and eventually used other methods to achieve his goal.|$|E
25|$|In 2011 {{researchers}} at Hanyang University, Korea, reported a polygon-surface reference Korean male phantom (PSRK-Man). This phantom was constructed by converting the Visible Korean Human-Man (VKH-man) into a polygonal mesh-based phantom. The height, weight, geometry of organs and tissues were adjusted {{to match the}} Reference Korean data. Without voxelization the PSRK-man could be directly implemented in Geant4 Monte Carlo simulation using a built-in function, but the <b>computation</b> <b>time</b> was 70~150 times longer than that required by High Definition Reference Korean-Man (HDRK-Man), a voxelized phantom derived also from VKH-man.|$|E
30|$|Fundamental {{components}} of the power are calculated by using both Goertzel and FFT algorithms respectively. <b>Computation</b> <b>times</b> were calculated for each algorithm within the sampling period.|$|R
30|$|HS is {{a simple}} {{algorithm}} that typically requires {{a high number of}} iterations to converge toward the global optima, but does so in low <b>computation</b> <b>times.</b>|$|R
30|$|J—when all {{the data}} is stored off-chip. The <b>computation</b> <b>times</b> (column 12) are very similar for each data assignment, so only the {{ballpark}} values are given.|$|R
25|$|Meeting this {{scalability}} {{condition is}} possible {{for a wide range}} of systems. However, the use of error correction brings with it the cost of a greatly increased number of required bits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between L and L2, where L is the number of bits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of L. For a 1000-bit number, this implies a need for about 104 bits without error correction. With error correction, the figure would rise to about 107 bits. <b>Computation</b> <b>time</b> is about L2 or about 107 steps and at 1nbsp&MHz, about 10 seconds.|$|E
25|$|In Monte Carlo, {{an initial}} {{configuration}} is refined by taking random steps which are accepted or rejected {{based on their}} induced improvement in score (see the Metropolis criterion), until {{a certain number of}} steps have been tried. The assumption is that convergence to the best structure should occur from a large class of initial configurations, only one of which needs to be considered. Initial configurations may be sampled coarsely, and much <b>computation</b> <b>time</b> can be saved. Because of the difficulty of finding a scoring function which is both highly discriminating for the correct configuration and also converges to the correct configuration from a distance, the use of two levels of refinement, with different scoring functions, has been proposed. Torsion can be introduced naturally to Monte Carlo as an additional property of each random move.|$|E
25|$|For the {{complexity}} classes defined in this way, {{it is desirable}} to prove that relaxing the requirements on (say) <b>computation</b> <b>time</b> indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), {{it would be interesting}} to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.|$|E
40|$|According to VLSI theory, [log n, √n]is {{the range}} of <b>computation</b> <b>times</b> for which there may exist an AT 2 -optimal {{multiplier}} of n-bit integers. Such networks were previously known for the time range [Ω(log 2 n), O(√n) ]; this theoretical question is settled, by exhibition of a class of AT 2 -optimal multipliers with <b>computation</b> <b>times</b> [Ω(log n), O(√n) ]. The designs {{are based on the}} DFT on a Fermat ring, whose elements are represented in a redundant radix- 4 form to ensure O(1) addition time...|$|R
40|$|Optimal {{deadline}} {{assignment for}} periodic real-time tasks in dynamic priority systems Real-time systems are often designed using {{a set of}} periodic tasks. Task periods are usually set by the system requirements, but deadlines and <b>computation</b> <b>times</b> can be modified {{in order to improve}} system performance. Sensitivity analysis in real-time systems has focused on changes in task <b>computation</b> <b>times,</b> using fixed priority analysis. Only a few studies deal with the modification of deadlines in dynamic-priority scheduling. The aim of this work is to provide a sensitivity analysis for task deadlines in the context of dynamic-priority, pre-emptive, uniprocessor scheduling. In this paper, we present a deadline minimisation method that achieves the maximum reduction. As undertaken in other studies concerning <b>computation</b> <b>times,</b> we also define and calculate the critical scaling factor for task deadlines. Our proposal is evaluated and compared with other works in terms of jitter. The deadline minimisation can be used to strongly reduce jitter of control tasks, in a real-time control application...|$|R
30|$|The {{experiments}} are performed using our non-optimized implementation in Python and OpenCV. The average SAT <b>computation</b> <b>times</b> for processing one frame of scenes A and B were about 4 and 6 min.|$|R
25|$|The {{asymptotic}} complexity {{is defined}} by the most efficient (in terms of whatever computational resource one is considering) algorithm for solving the game; the most common complexity measure (<b>computation</b> <b>time)</b> is always lower-bounded by the logarithm of the asymptotic state-space complexity, since a solution algorithm must work for every possible state of the game. It will be upper-bounded by the complexities of each individual algorithm for the family of games. Similar remarks apply to the second-most commonly used complexity measure, the amount of space or computer memory used by the computation. It is not obvious that there is any lower bound on the space complexity for a typical game, because the algorithm need not store game states; however many games of interest are known to be PSPACE-hard, and it follows that their space complexity will be lower-bounded by the logarithm of the asymptotic state-space complexity as well (technically the bound is only a polynomial in this quantity; but it is usually known to be linear).|$|E
25|$|The DFT is {{obtained}} by decomposing {{a sequence of}} values into components of different frequencies. This operation is useful in many fields (see discrete Fourier transform for properties and applications of the transform) but computing it directly from the definition is often too slow to be practical. An FFT {{is a way to}} compute the same result more quickly: computing the DFT of N points in the naive way, using the definition, takes O(N2) arithmetical operations, while an FFT can compute the same DFT in only O(N log N) operations. The difference in speed can be enormous, especially for long data sets where N may be in the thousands or millions. In practice, the <b>computation</b> <b>time</b> can be reduced by several orders of magnitude in such cases, and the improvement is roughly proportional to NlogN. This huge improvement made the calculation of the DFT practical; FFTs are of great importance {{to a wide variety of}} applications, from digital signal processing and solving partial differential equations to algorithms for quick multiplication of large integers.|$|E
500|$|The {{history of}} {{numerical}} weather prediction considers how current weather conditions as input into mathematical models of the atmosphere and oceans to predict the weather and future sea state (the process of {{numerical weather prediction}}) {{has changed over the}} years. [...] Though first attempted manually in the 1920s, {{it was not until the}} advent of the computer and computer simulation that <b>computation</b> <b>time</b> was reduced to less than the forecast period itself. [...] ENIAC was used to create the first forecasts via computer in 1950, and over the years more powerful computers have been used to increase the size of initial datasets as well as include more complicated versions of the equations of motion. [...] The development of global forecasting models led to the first climate models. [...] The development of limited area (regional) models facilitated advances in forecasting the tracks of tropical cyclone as well as air quality in the 1970s and 1980s.|$|E
3000|$|In {{the same}} paper the <b>computation</b> <b>times</b> {{for some of}} the other {{denoising}} methods presented in Table 1 are appreciated. For example for the redundant BLS-GSM with estimation window of size [...]...|$|R
40|$|This paper {{presents}} {{a new approach}} for video completion of high-resolution video sequences. Current state-ofthe- art exemplar-based methods that use non-parametric patch sampling work well and provide good results for low-resolution video sequences. Unfortunately, because of memory consumption problems and long <b>computation</b> <b>times,</b> these methods handle only relatively low-resolution video sequences. This paper {{presents a}} video completion method that can handle much higher resolutions than previous ones. First, {{to address the problem}} of long <b>computation</b> <b>times,</b> a dual inpainting-sampling filling-order completion method is proposed. The quality of our results is then significantly improved by a second innovation introducing a coherence-based matches refinement that conducts intelligent and localized searches without relying on approximate searches or compressed data. Finally, with respect to the <b>computation</b> <b>times</b> and memory problems that prevent high-resolution video completion, the third innovation is a new localized search completion approach, which also uses uncompressed data and an exact search. Combined together, these three innovations make it possible to complete high-resolution video sequences, thus leading to a significant increase in resolution as compared to previous works...|$|R
30|$|As we can observe, in {{the real}} datasets, DFRT is in general more time consuming, {{specially}} when using {{a small number of}} CUs. However, as the number of CUs increases, both algorithms need similar <b>computation</b> <b>times.</b>|$|R
500|$|The {{history of}} {{numerical}} weather prediction began in the 1920s {{through the efforts of}} Lewis Fry Richardson, who used procedures originally developed by Vilhelm Bjerknes to produce by hand a six-hour forecast for the state of the atmosphere over two points in central Europe, taking at least six weeks to do so. [...] It was not until the advent of the computer and computer simulations that <b>computation</b> <b>time</b> was reduced to less than the forecast period itself. The ENIAC was used to create the first weather forecasts via computer in 1950, based on a highly simplified approximation to the atmospheric governing equations. In 1954, Carl-Gustav Rossby's group at the Swedish Meteorological and Hydrological Institute used the same model to produce the first operational forecast (i.e., a routine prediction for practical use). Operational {{numerical weather prediction}} in the United States began in 1955 under the Joint Numerical Weather Prediction Unit (JNWPU), a joint project by the U.S. Air Force, Navy and Weather Bureau. [...] In 1956, Norman Phillips developed a mathematical model which could realistically depict monthly and seasonal patterns in the troposphere; this became the first successful climate model. Following Phillips' work, several groups began working to create general circulation models. [...] The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory.|$|E
2500|$|The direct {{stiffness}} {{method was}} developed specifically to effectively and easily implement into computer software to evaluate complicated structures that contain {{a large number}} of elements. [...] Today, nearly every finite element solver available is based on the direct stiffness method. [...] While each program utilizes the same process, many have been streamlined to reduce <b>computation</b> <b>time</b> and reduce the required memory. [...] In order to achieve this, shortcuts have been developed.|$|E
2500|$|Monte Carlo methods {{provide a}} way out of this {{exponential}} increase in <b>computation</b> <b>time.</b> As long as the function in question is reasonably well-behaved, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the central limit theorem, this method displays [...] convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.|$|E
40|$|Grid {{applications}} that use {{a considerable number}} of processors for their computations need effective predictions of the expected <b>computation</b> <b>times</b> on the different nodes. Currently, there are no effective prediction methods available that satisfactorily cope with those ever-changing dynamics of <b>computation</b> <b>times</b> in a grid environment. Motivated by this, in this paper we develop the Dynamic Exponential Smoothing (DES) method to predict job processing times in a grid environment. To compare predictions of DES to those of the existing prediction methods, we have performed extensive experiments in a real large-scale grid environment. The results illustrate a strong and consistent improvement of DES in comparison with the existing prediction methods. 1...|$|R
40|$|This paper proposes an {{optimisation}} model and a meta-heuristic algorithm for solving the urban network design problem. The problem consists in optimising {{the layout of}} an urban road network by designing directions of existing roads and signal settings at intersections. A non-linear constrained {{optimisation model}} for solving this problem is formulated, adopting a bi-level approach {{in order to reduce}} the complexity of solution methods and the <b>computation</b> <b>times.</b> A Scatter Search algorithm based on a random descent method is proposed and tested on a real dimension network. Initial results show that the proposed approach allows local optimal solutions to be obtained in reasonable <b>computation</b> <b>times.</b> Network design Transportation Scatter Search...|$|R
50|$|A {{drawback}} of the well-founded variational approaches {{is their}} large <b>computation</b> <b>times.</b> Therefore, many different approximate approaches were devised as well. Several well-known approximate theories for the rolling contact problem are Kalker’s FASTSIM approach, the Shen-Hedrick-Elkins formula, and Polach’s approach.|$|R
