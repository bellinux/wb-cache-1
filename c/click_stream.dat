103|78|Public
50|$|Prior to Oracle around June 2011, Smith {{was leading}} {{security}} for the User Data Analytics(UDA) business unit at Yahoo and developed innovative security controls to secure Yahoo's <b>click</b> <b>stream</b> revenues. Smith also lead Yahoo's Enterprise Security Triage Program for monitoring enterprise vulnerabilities and tracking remediation activities.|$|E
50|$|Attitudinal {{analytics}} is {{a marketing}} technology application {{that involves the}} integration of online surveys that capture visitor intent and critical demographic attributes with the tracking of explicit behavior through <b>click</b> <b>stream</b> monitoring on websites. This quantitative user experience collects data from thousands of user sessions rather than hundreds. This data is typically compared against key performance indicators for performance, customer satisfaction and overall customer experience success or failure. Reports of finding and recommendations are used to improve a website or customer experience program that is heavily dependent upon {{the use of an}} online campaigns driving traffic to a website or collection of sites. Offline measurement can also be incorporated to extend the customer experience understanding to illuminate what elements of the online experience impacted offline behavior such as purchasing in a store or visiting a branch office.|$|E
40|$|<b>Click</b> <b>stream</b> data {{represents}} {{a rich source}} of information for understanding web site activity, browsing patterns to purchasing decisions. Standard tools produce hundreds of reports that are not particularly useful. The problem with fixed web reports, and report-based analysis in general, is that reports only answer specific, predefined questions that are insufficient for today's highly competitive and rapidly changing businesses. To overcome this problem, we have developed a <b>click</b> <b>stream</b> analysis tool called eBizInsights. eBizInsights consists of a web log parser, a <b>click</b> <b>stream</b> warehouse, a reporting engine, and a rich visual interactive workspace. Using the workspace, analysts perform ad hoc analysis, discover patterns, and identify correlations that are impossible to find using fixed reports. ...|$|E
40|$|Enterprise-class {{web sites}} receive {{a large amount}} of tra#c, from both {{registered}} and anonymous users. Data warehouses are built to store and help analyze the <b>click</b> <b>streams</b> within this tra#c to provide companies with valuable insights into the behavior of their customers. This article proposes a parallel sequence mining algorithm, webSPADE, to analyze the <b>click</b> <b>streams</b> found in site web logs. In this process, raw web logs are first cleaned and inserted into a data warehouse. The <b>click</b> <b>streams</b> are then mined by webSPADE. An innovative web-based front-end is used to visualize and query the sequence mining results. The webSPADE algorithm is currently used by Verizon to analyze the daily tra#c of the Verizon. com web site...|$|R
40|$|Enterprise-class {{web sites}} receive {{a large amount}} of traffic, both from {{registered}} and anonymous users. This traffic consists of many <b>click</b> <b>streams,</b> which are defined as collections of hits (requests) from a specific user during a specific session. Data warehouses are built to store and help analyze these <b>click</b> <b>streams</b> to provide companies with valuable insights into their customers 9 ̆ 2 behaviors. This article proposes a parallel sequence mining algorithm, webSPADE, to analyze the <b>click</b> <b>streams</b> found in site web logs. In this process, raw web logs are first cleaned and inserted into a data warehouse. The <b>click</b> <b>streams</b> are then mined by webSPADE, the proposed algorithm that uses one full scan and several partial scans of the data. An innovative web-based front-end is used to visualize and query the sequence mining results. Based on relational database technology, this analysis technique enables the analysis of very large amounts of data in a short amount of time. Thus, an analysis of monthly data can be performed quickly and easily, as shown by the algorithm run-times reported in this paper. The webSPADE algorithm is currently in use at Verizon to analyze the daily traffic of the Verizon. com web site. Keywords: Web Mining, Sequence Mining, Web Log, e-commerce, Parallel Mining, Association Minin...|$|R
50|$|Its diet {{may also}} include the eggs of seafowl. At Robben Island, mole snakes were {{observed}} eating the eggs of Spheniscus demersus (African penguin), Larus hartlaubii (Harlaub's gull), and Numida meleagris (helmeted guinea-fowl). Juveniles were observed eating Strongylopus grayii (<b>clicking</b> <b>stream</b> frogs).|$|R
3000|$|... {{times per}} second. The {{number of threads}} {{is equal to the}} number of servers having {{accumulated}} <b>click</b> <b>stream</b> data, where each thread performs a first stage lookup to associate the <b>click</b> <b>stream</b> data with relevant data in the database, and then runs the analysis on all related click data in the database in order to update the statistics. Each analysis event maps to a thread that consumes memory to maintain its stack, utilizes the processor to run the lookup and the analysis processes, and incurs additional overhead resulting from thread context switches. For the computations, we suppose that the lookup process takes T [...]...|$|E
30|$|This {{distribution}} {{was applied}} to model four extremely skewed count data sets namely Text data from the English Bible, Sales data from a large retailer chain, Telecommunications data customer data from an AT&T service of monthly usage volumes, and <b>Click</b> <b>stream</b> data and browsing behavior of internet users.|$|E
40|$|Various topic {{extraction}} {{techniques for}} digital libraries {{have been proposed}} over the past decade. Generally the topic extraction system requires {{a large number of}} features and complicated lexical analysis. While these features and analysis are effective to represent the statistical characteristics of the document, they didn't capture the high level semantics. In this paper, we present a new approach for topic extraction. Our approach combines user's <b>click</b> <b>stream</b> data with traditional lexical analysis. From our point of view, the user's <b>click</b> <b>stream</b> directly reflects human understanding of the high-level semantics in the document. Furthermore, a simple, yet effective, piece-wise linear model for topic evolution is proposed. We apply genetic algorithm to estimate the model and extract topics. Experiments on the set of US congress digital library documents demonstrate that our approach achieves better accuracy for the topic extraction than traditional methods...|$|E
40|$|The ongoing {{explosion}} of web information {{calls for more}} intelligent and personalized methods towards better search result quality for advanced queries. Query logs and <b>click</b> <b>streams</b> obtained from web browsers or search engines can contribute to better quality by exploiting the collaborative recommendations that are implicitly embedded in this information. The method presented in this work incorporates the notion of query nodes into the PageRank model and integrates the implicit relevance feedback given by <b>click</b> <b>streams</b> into the automated process of authority analysis. The enhanced PageRank scores, coined QRank scores, can be computed oine; at query-time they are combined with query-specific relevance measures with virtually no overhead. In our experiments significant improvements in the precision of search results were observed, which demonstrate the eectiveness of our model...|$|R
25|$|Big data {{analysis}} technologies {{make it possible}} to perform controlled experiments to test hypotheses and analyze results to discover complex correlations. For example, retailers can track the behavior of individual consumers from their Internet <b>click</b> <b>streams,</b> update their preferences and predict their behavior in real time.|$|R
40|$|Abstract. Data streams have {{recently}} attracted attention for their applicability to numerous domains including credit fraud detection, network intrusion detection, and <b>click</b> <b>streams.</b> Stream clustering {{is a technique}} that performs cluster analysis of data streams that is able to monitor the results in real time. A data stream is continuously generated sequences of data for which {{the characteristics of the}} data evolve over time. A good stream clustering algorithm should recognize such evolution and yield a cluster model that conforms to the current data. In this paper, we propose a new technique for stream clustering which supports five evolutions that are appearance, disappearance, self-evolution, merge and split. ...|$|R
3000|$|The {{statistical}} analysis process is also multithreaded. It {{is responsible for}} keeping the traffic statistics up to date, and may be run according to a time schedule, or triggered by an event (e.g., {{when the number of}} received <b>click</b> <b>stream</b> records concerning a given server surpasses a given threshold). This process is assumed to run λ [...]...|$|E
30|$|The process {{continues}} until Server A receives {{no further}} messages from other web servers {{for the same}} session. To do this, it uses a timeout scheme {{to decide on the}} end of the <b>click</b> <b>stream</b> session. It is at this point that Server A inserts a record in the Ship_Data table, including the information mentioned at the top of this section.|$|E
40|$|Abstract. E-commerce {{website design}} {{is a highly}} complex and {{subjective}} task which is often {{very difficult to get}} right from the start. Once a design has been implemented, customer behavior usually needs to be monitored in order to correct bugs and inefficiencies. While there are tools available that record and visualize customer <b>click</b> <b>stream</b> data, the information provided by these tools cannot be directly translated into changes in the website design. Rather, an expert needs to go through a lengthy process of manual data analysis in order to identify and overcome weaknesses of websites. In this paper, we describe a <b>click</b> <b>stream</b> visualization technique using footstep graphs. We describe the experiment that produced these graphs and show how specific patterns can be identified that both point to problems with the website design and contain information about how to remedy them. We contend that the results we present in this paper represent the first step towards a technique for producing automatic recommendations for website improvements. ...|$|E
40|$|The ongoing {{explosion}} of web information {{calls for more}} intelligent and personalied methods towards better search result quality for advanced queries. Query log and <b>click</b> <b>streams</b> obtained from web browsers or search engines can contribute to better quality by exploiting the collaborative recommendations that are implicitly embedded in this information. This paper presents a new method that incorporates the notion of query nodes into PageRank model and integrates the implicite relevance feedback given by <b>click</b> <b>streams</b> into the automated process of authority analysis. This approach generalizes the well-known random-surfer model into a random-expert model that mimics the behavior of an expert user in an extended session consisting of queries, query refinements, and result-navigation steps. The enhanced PageRank scores, coined QRank scores, can be computed offline; at query-time they are combined with query-specific relevance measures with virtually no overhead. Our preliminary experiments, based on real-life query-log and click-stream traces from eight different trial users indicate significant improvements in the precision of search results...|$|R
30|$|The {{vertebrates}} {{included three}} species of amphibians recorded {{on the summit}}; Amietophrynus rangeri (Raucous Toad), Cacosternum boettgeri (Dainty Frog) and Strongylopus grayii (<b>Clicking</b> <b>Stream</b> Frog). Both Strongylopus grayii and Cacosternum boettgeri were recorded from a streamlet on the summit plateau, also indicating the watershed value of the Kamdebooberge. Though not recorded from the summit (hence {{not included in the}} list) Tomopterna tandyi (Tandy’s Sand Frog) was found from the foothills of the Kamdebooberge, on Farm Oaklands 104.|$|R
40|$|There exist {{emerging}} {{applications of}} data streams that require association rule mining, such as network traffic monitoring and web <b>click</b> <b>streams</b> analysis. Different from data in traditional static databases, data streams typically arrive continuously in high speed with huge amount and changing data distribution. This raises new issues {{that need to}} be considered when developing association rule mining techniques for stream data. This paper discusses those issues and how they are addressed in the existing literature. 1...|$|R
40|$|Abstract. One of the {{important}} Internet challenges in coming years will be the introduction of intelligent services {{and the creation of}} a more personalized environment for users. A key prerequisite for such services is the modeling of user behavior and a natural starting place for this are Web logs. In this paper we propose a model for predicting sequences of user accesses which is distinguished by two elements: it is customizable and it reflects sequentiality. Customizable, in this context, means that the proposed model can be adapted to the characteristics of the server to more accurately capture its behavior. The concept of sequentiality in our model consists of three elements: (1) preservation of the sequence of the <b>click</b> <b>stream</b> in the antecedent, (2) preservation of the sequence of the <b>click</b> <b>stream</b> in the consequent and (3) a measure of the gap between the antecedent and the consequent in terms of the number of user clicks. ...|$|E
40|$|The Web mining {{extracts}} {{useful information}} from the web pages. Web mining techniques seek to extract knowledge from Web data, including web documents, hyperlinks between documents, and usage logs of web sites. Web usage mining mines knowledge from diverse websites. Extracting appropriate data from deep web pages is an exigent dilemma due to the overflow of data in to the web. Web servers generates {{a huge amount of}} information on web users browsing activities. These are called <b>click</b> <b>stream</b> or web access log data. The <b>click</b> <b>stream</b> data can be enriched with information about the content of visited pages. The aim of this research paper is to develop a practically implemented search engine in which it extracts only the relevant links by analyzing user behaviour. With the continuous growth of Web services the user data collected by Web based organizations has reached enormous capacity. The paper will undertake a review of the existing literature available on this arena and develop an empirical model of search engine showing real time data flow after retrieval of significant information from data warehouse...|$|E
30|$|Several modern data-intensive {{applications}} need {{to process}} {{large volumes of}} data on the fly as they are produced. Examples range from credit card fraud detection systems, which analyze massive streams of credit card transactions to identify suspicious patterns, to environmental monitoring applications that continuously analyze sensor data, to <b>click</b> <b>stream</b> analysis of Web sites that identify frequent patterns of interactions. More in general, stream processing is a central requirement in today’s information systems.|$|E
40|$|We {{study how}} a website {{adaptation}} based on segment predictions from <b>click</b> <b>streams</b> affects visitor behavior and user experience. Through statistical analysis we investigate how the adaptation changed actual behavior. Through {{structural equation modeling}} of subjective experience we answer why the change in behavior occurred. The study shows the value of using survey data for constructing and evaluating predictive models. It additionally shows how a website adaptation influences user experience and how this in turn influences visitor behavior...|$|R
40|$|E-Commerce domain {{can provide}} all the right {{ingredients}} for successful data mining. It is a killer domain for data mining[1] Heavy usage of eCommerce websites by consumers {{has led to the}} generation of huge amounts of data like <b>click</b> <b>streams,</b> web logs, routing and server requests, web links and cookie information etc. Innovative analysis of all this information is helping businesses in customer segmentation, profiling, targeted advertising etc. Utilizing this kind of information about customers to boost sales is often classified a...|$|R
50|$|Several {{bird species}} {{can be seen}} here {{and this is a}} popular site for fishing and school excursions. The mammal life, {{including}} otter, mongoose and porcupine, is mostly nocturnal and rarely seen. Small reptiles such as the Angulate tortoise, the Marsh Terrapin and a range of snakes and lizards are also found here. Amphibian life includes a wide range of species such as the endangered Western Leopard Toad, the Arum Lily Frog, the Cape River Frog and the <b>Clicking</b> <b>Stream</b> Frog.|$|R
40|$|Abstract:-WWW {{stands for}} World Wide Web, {{and it is}} an {{advanced}} information retrieval system. As years passed World Wide Web became weighed down with information and it became hard to retrieve data according to the need. The Web mining extracts useful information from the web pages. Web mining techniques seek to extract knowledge from Web data, including web documents, hyperlinks between documents, and usage logs of web sites. Web usage mining mines knowledge from diverse websites. Extracting appropriate data from deep web pages is an exigent dilemma due to the overflow of data in to the web. Web servers generates a huge amount of information on web users browsing activities. These are called <b>click</b> <b>stream</b> or web access log data. The <b>click</b> <b>stream</b> data can be enriched with information about the content of visited pages. The aim {{of this paper is to}} obtain all the data behind a form by multiple submissions of the form filled out in all possible ways by using agent, but efficiency concerns lead us to consider alternatives. We can estimate the amount of remaining data after a small number of submissions maximize the coverage of the data. I...|$|E
30|$|The {{targeted}} works {{presented in}} this section provide empirical support to further explore and develop novel Deep Learning algorithms and architectures for analyzing large-scale, fast moving streaming data, as is encountered in some Big Data application domains such as social media feeds, marketing and financial data feeds, web <b>click</b> <b>stream</b> data, operational logs, and metering data. For example, Amazon Kinesis is a managed service designed to handle real-time streaming of Big Data – {{though it is not}} based on the Deep Learning approach.|$|E
40|$|Frequent pattern mining {{plays an}} {{essential}} role in many data analysis tasks including association-, correlation-, and causality analysis and has broad applications. Examples are market basket analysis and web <b>click</b> <b>stream</b> analysis. Al-though a number of efficient methods for mining frequent patterns where proposed in the past, there exist {{only a small number of}} visual exploration tools for discovering frequent patterns. In this paper we present a novel visualization tech-nique for exploring frequent itemsets interactivly, based on a radial visual layout approach...|$|E
40|$|Data mining plays a {{vital role}} in last few years there are broad area for the researchers. There exist {{emerging}} applications of data streams that require association rule mining, such as network traffic monitoring and web <b>click</b> <b>streams</b> -analysis. Different from data in traditional static databases, data streams typically arrive continuously in high speed with huge amount and changing data distribution. This raises new issues that need to be considered when developing association rule mining techniques for data stream this paper discuss those issues and challenges...|$|R
40|$|We {{consider}} {{the problem of}} finding duplicates in data streams. Duplicate detection in data streams is utilized in various applications including fraud detection. We develop a solution based on Bloom Filters [9], and discuss the space and time requirements for running the proposed algorithm in both the contexts of sliding, and landmark stream windows. We run a comprehensive set of experiments, using both real and synthetic <b>click</b> <b>streams,</b> to evaluate {{the performance of the}} proposed solution. The results demonstrate that the proposed solution yields extremely low error rates. ...|$|R
50|$|The {{technology}} for Audience Screening is {{beneficial to the}} publishers because {{it allows them to}} further segment their audience without the weight of customer surveys and deeper analytics packages being overload in their existing inventory. The technology works simply with a pixel tag being placed on the publisher’s site and a simple database feed being established whereby the technology owner can amass a wealth of information concerning past habits and <b>click</b> <b>streams</b> as well as the syndicated data sources being used to categorize the audience members, or potential customers.|$|R
40|$|AbstractThe {{major problem}} of many on-line web sites is the {{presentation}} of many choices to the client at a time; this usually results to strenuous and time consuming task in finding the right product or information on the site. In this work, we present a study of automatic web usage data mining and recommendation system based on current user behavior through his/her <b>click</b> <b>stream</b> data on the newly developed Really Simple Syndication (RSS) reader website, {{in order to provide}} relevant information to the individual without explicitly asking for it. The K-Nearest-Neighbor (KNN) classification method has been trained to be used on-line and in Real-Time to identify clients/visitors <b>click</b> <b>stream</b> data, matching it to a particular user group and recommend a tailored browsing option that meet the need of the specific user at a particular time. To achieve this, web users RSS address file was extracted, cleansed, formatted and grouped into meaningful session and data mart was developed. Our result shows that the K-Nearest Neighbor classifier is transparent, consistent, straightforward, simple to understand, high tendency to possess desirable qualities and easy to implement than most other machine learning techniques specifically when {{there is little or no}} prior knowledge about data distribution...|$|E
40|$|Data stream {{applications}} like {{sensor network}} data, <b>click</b> <b>stream</b> data, have data arriving continuously {{at high speed}} rates and require online mining process capable of delivering current and near accurate results on demand without full access to all historical stored data. Frequent sequential mining {{is the process of}} discovering frequent sequential patterns in data sequences as found in applications like Web log access sequences. Mining frequent sequential patterns on data stream applications contend with many challenges such as limited memory for unlimited data, inability of algorithms to scan infinitely flowing original dataset more than once and to deliver current and accurate result on demand. Existing work on mining frequent patterns on data streams are mostly for non-sequential patterns. This paper proposes SSM-algorithm (sequential stream mining-algorithm), that uses three types of data structures (D-List, PLWAP tree and FSP-tree) to handle the complexities of mining frequent sequential patterns in data streams. It summarizes frequency counts of items with the D-list, continuously builds PLWAP tree and mines frequent sequential patterns of batches of stream records, maintains mined frequent sequential patterns incrementally with FSP tree. The proposed algorithm can be deployed to analyze e-commerce data where the primary source of data is <b>click</b> <b>stream</b> data...|$|E
40|$|The {{major problem}} of many on-line web sites is the {{presentation}} of many choices to the client at a time; this usually results to strenuous and time consuming task in finding the right product or information on the site. In this work, we present a study of automatic web usage data mining and recommendation system based on current user behavior through his/her <b>click</b> <b>stream</b> data on the newly developed Really Simple Syndication (RSS) reader website, {{in order to provide}} relevant information to the individual without explicitly asking for it. The K-Nearest-Neighbor (KNN) classification method has been trained to be used on-line and in Real-Time to identify clients/visitors <b>click</b> <b>stream</b> data, matching it to a particular user group and recommend a tailored browsing option that meet the need of the specific user at a particular time. To achieve this, web users RSS address file was extracted, cleansed, formatted and grouped into meaningful session and data mart was developed. Our result shows that the K-Nearest Neighbor classifier is transparent, consistent, straightforward, simple to understand, high tendency to possess desirable qualities and easy to implement than most other machine learning techniques specifically when {{there is little or no}} prior knowledge about data distribution...|$|E
3000|$|Big data {{associated}} with time stamp is called big data stream. Sensor data, call centre records, <b>click</b> <b>streams,</b> and health- care data {{are examples of}} big data streams. Quality of service (QoS) parameters such as end-to-end delay, accuracy, and real-time processing are some constraints of big data stream processing. The most pre-requirement of big data stream mining in applications such as health-care is privacy preserving [54]. One of the common approaches to anonymize static data is k-anonymity. This approach is not directly applicable for the big data streams anonymization. The reasons are as follows [55]: [...]...|$|R
40|$|Abstract — Web Usage Mining is an {{important}} type of Web Mining, which deals with extraction of interesting knowledge from the web log files. The lots of research has done in this field but basically this paper emphasize on user future next request prediction using web log record, <b>click</b> <b>streams</b> record and user information. The aim {{of this paper is}} to provide past, current evaluation and update in web usage mining- future request prediction. This paper also reports the comparisons and summary of various methods of future request prediction with application, which gives the overview of development in research...|$|R
40|$|Recommender Systems {{is one of}} the {{effective}} tools to deal with information overload issue. Similar with the explicit rating and other implicit rating behaviours such as purchase behaviour, <b>click</b> <b>streams,</b> and browsing history etc., the tagging information implies user’s important personal interests and preferences information, which can be used to recommend personalized items to users. This paper is to explore how to utilize tagging information to do personalized recommendations. Based on the distinctive three dimensional relationships among users, tags and items, a new user profiling and similarity measure method is proposed. The experiments suggest that the proposed approach is better than the traditional collaborative filtering recommender systems using only rating data...|$|R
