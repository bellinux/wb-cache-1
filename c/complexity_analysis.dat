2119|1307|Public
25|$|An early {{example of}} {{algorithm}} <b>complexity</b> <b>analysis</b> is the running time {{analysis of the}} Euclidean algorithm done by Gabriel Lamé in 1844.|$|E
25|$|Treewidth is {{commonly}} used as a parameter in the parameterized <b>complexity</b> <b>analysis</b> of graph algorithms. The graphs with treewidth at most k are also called partial k-trees; many other well-studied graph families also have bounded treewidth.|$|E
2500|$|... {{formulate}} {{the edge}} coloring problem as an integer program and describe their experience using an integer programming solver to edge color graphs. However, {{they did not}} perform any <b>complexity</b> <b>analysis</b> of their algorithm.|$|E
50|$|Skordev's {{field of}} {{scientific}} interests include computability and <b>complexity</b> in <b>analysis,</b> mathematical logic, generalized recursion theory, and theory {{of programs and}} computation.|$|R
30|$|Regarding the {{fit between}} self- and {{supervisor}} ratings of meta abilities such as task learning {{and dealing with}} task <b>complexity</b> our <b>analysis</b> also revealed an impact on performance outcomes.|$|R
5000|$|... #Caption: The {{bacterial}} flagellum {{has been}} invoked in creation science and in intelligent design {{to illustrate the}} concept of irreducible <b>complexity.</b> Careful <b>analysis</b> shows {{that there are no}} major obstacles to a gradual evolution of flagella.|$|R
2500|$|An {{example of}} such a case is , for which no type can be derived using HM. [...] Practically, types are only small terms and do not build up {{expanding}} structures. [...] Thus, in <b>complexity</b> <b>analysis,</b> one can treat comparing them as a constant, retaining O(1) costs.|$|E
2500|$|The {{question}} of microbial life on Mars remains unresolved. Nonetheless, on April 12, 2012, an international {{team of scientists}} reported studies, based on mathematical speculation through <b>complexity</b> <b>analysis</b> of the Labeled Release experiments of the 1976 Viking Mission, that may suggest the detection of [...] "extant microbial life on Mars." ...|$|E
5000|$|<b>Complexity</b> <b>analysis</b> of {{interior}} point methods for linear programming, ...|$|E
30|$|This {{paper will}} {{elaborate}} on our proposed RIePDMA system and BP-IDD-IC algorithm. The {{rest of the}} paper is organized as follows. In Section 2, we introduce the fundamental principle of PDMA and introduce the system model of the RIePDMA uplink system. Different detection algorithms, including maximum likelihood with IC (ML-IC), {{minimum mean square error}} with IC (MMSE-IC), iterative detection and decoding based on belief propagation (BP-IDD), and our proposed BP-IDD-IC are illustrated in Section 3. Section 4 shows simulation results, together with the <b>complexities</b> <b>analysis</b> of those above-mentioned algorithms. In Section 5, we discuss the possible future research directions in PDMA briefly. Finally, Section 6 concludes this paper.|$|R
3000|$|If the {{polarized}} {{states are}} not exactly estimated, that is, some estimation errors are introduced, we also {{consider the case of}} two-radar-member in the RSN for simplifying the <b>complexity</b> of <b>analysis.</b> For the 1 st user, if [...]...|$|R
30|$|Tables 1 and 3 {{indicate}} the <b>complexity</b> of the <b>analysis</b> of JNCC for large networks.|$|R
5000|$|<b>Complexity</b> <b>analysis</b> [...] - [...] {{the problem}} of {{estimating}} {{the time needed to}} terminate ...|$|E
50|$|It {{has been}} {{suggested}} that the <b>complexity</b> <b>analysis</b> of human songs can be a useful pedagogic device for teaching students complexity theory.|$|E
50|$|An early {{example of}} {{algorithm}} <b>complexity</b> <b>analysis</b> is the running time {{analysis of the}} Euclidean algorithm done by Gabriel Lamé in 1844.|$|E
30|$|The {{concept of}} FPT is {{generally}} {{used to analyze}} problems and, in particular, to find which parameters significantly influence the computational <b>complexity.</b> This <b>analysis</b> can then be used to design efficient algorithms for solving NP-hard problems such as the TARP.|$|R
40|$|Study of the {{implementation}} of data structures and control structures in professional computer programs. Introduction to the fundamentals of <b>complexity</b> and <b>analysis.</b> Study of common standard problems and solutions (e. g., transitive closure and critical path). Emphasis on high-level language software design...|$|R
50|$|His main {{research}} {{areas are}} complexity theory and proof complexity, with excursions into programming language semantics, parallel computation, and artificial intelligence. Other areas {{which he has}} contributed to include bounded arithmetic, bounded reverse mathematics, complexity of higher type functions, <b>complexity</b> of <b>analysis,</b> and lower bounds in propositional proof systems.|$|R
5000|$|Their {{purpose was}} to compare execution-times of the various models: RAM, RASP and multi-tape Turing machine {{for use in the}} theory of <b>complexity</b> <b>analysis.</b>|$|E
5000|$|Similarly, the <b>complexity</b> <b>analysis</b> of the k-interchangeability {{algorithm}} for a {{worst case}} , with -tuples of variables and , for -tuples of values, then the bound is : [...]|$|E
50|$|N. Z. Shor is {{well known}} for his method of {{generalized}} gradient descent with space dilation {{in the direction of the}} difference of two successive subgradients (the so-called r-algorithm), that was created in collaboration with Nikolay G. Zhurbenko. The ellipsoid method was re-invigorated by A.S. Nemirovsky and D.B. Yudin, who developed a careful <b>complexity</b> <b>analysis</b> of its approximation properties for problems of convex minimization with real data. However, it was Leonid Khachiyan who provided the rational-arithmetic <b>complexity</b> <b>analysis,</b> using an ellipsoid algorithm, that established that linear programming problems can be solved in polynomial time.|$|E
40|$|While {{aggregate}} level pay equity comparisons between Australia and the UK confirm expectations {{based on their}} different wage distributions and regulatory systems, observation of trends and occupational level <b>analysis</b> reveal additional <b>complexity.</b> Our <b>analysis</b> suggests {{the need for a}} multi-faceted approach to closing the average gender pay gap...|$|R
30|$|Image {{segmentation}} plays a {{vital role}} in MRI abnormality detection. This paper presents a robust MRI segmentation method to outline potential abnormality blobs. Thresholding and boundary tracing strategies are employed to remove background noises, and hence, the ROIs in the whole process are set. Subsequently, a polyfit surface evolution is proposed to approximately estimate bias field, which makes segmentation robust to image noises. Simultaneously, customized initial level set functions are devised so as to detect subtle bright and dark blobs which are highly potential abnormality regions. The proposed method improves bias field estimation and level set method to acquire fine segmentation with low computational <b>complexities.</b> <b>Analysis</b> of experimental results and comparisons with existing algorithms demonstrates that the proposed method can segment weak-edged, low-resolution MR brain images, and its performance prevails in accuracy and effectiveness.|$|R
5000|$|ChIA-PET {{involves}} ChIP {{to reduce}} the <b>complexity</b> for genome-wide <b>analysis</b> and adds specificity to chromatin interactions bound by specific factors of interest.|$|R
5000|$|G. M. Provan (1988). A <b>complexity</b> <b>analysis</b> of assumption-based truth {{maintenance}} systems. In B. Smith and G. Kelleher, editors, Reason Maintenance Systems {{and their}} Applications, pages 98-113. Ellis Horwood, New York.|$|E
50|$|Shengwei Mei {{from the}} Tsinghua University, Beijing, China was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2015 for {{contributions}} to power systems robust control and <b>complexity</b> <b>analysis.</b>|$|E
5000|$|... {{formulate}} {{the edge}} coloring problem as an integer program and describe their experience using an integer programming solver to edge color graphs. However, {{they did not}} perform any <b>complexity</b> <b>analysis</b> of their algorithm.|$|E
40|$|Abstract — The {{complexity}} of round robin iterative data flow analysis has been traditionally defined as 1 + d where d is {{the depth of}} a control flow graph. However, this bound is restricted to bit vector frameworks, which by definition, are separable. For non-separable frameworks, the <b>complexity</b> of <b>analysis</b> {{is influenced by the}} interdependences of program entities, hence the bound of 1 + d is not applicable. This motivates the need for capturing the interdependences of entities to define a general complexity measure. We propose Degree of dependence δ which quantifies the effect of non-separability on the <b>complexity</b> of <b>analysis</b> for a particular problem instance. We define the complexity bound of 1 + δ + d which explains the {{complexity of}} round robin analysis of general nonseparable data flow problems. Like d, δ is a theoretical concept useful for understanding the complexity rather than estimating it. In bit vector frameworks the bound 1 + δ + d reduces to 1 + d due to δ = 0. Apart from being general, our bound is also precise, as corroborated by empirical results. Index Terms — Data flow <b>analysis,</b> <b>Complexity,</b> Constant propagatio...|$|R
40|$|These are {{lecture notes}} for a {{tutorial}} seminar which I gave at a satellite seminar of “Computability and <b>Complexity</b> in <b>Analysis</b> 2004 ” in Kyoto. The main {{message of the}} notes is that computable mathematics is the realizability interpretation of constructive mathematics. The presentation is targeted at an audience which is familiar with computable mathematics bu...|$|R
40|$|This paper {{studies the}} {{stability}} of complex-valued nonlinear differential system. The stability criteria of complex-valued nonlinear autonomous system are established. For the general complex-valued nonlinear non-autonomous system, the comparison principle {{in the context of}} complex fields is given. Those derived stability criteria not only provide a new method to analyze complex-valued differential system, but also greatly reduce the <b>complexity</b> of <b>analysis</b> and computation...|$|R
50|$|Treewidth is {{commonly}} used as a parameter in the parameterized <b>complexity</b> <b>analysis</b> of graph algorithms. The graphs with treewidth at most k are also called partial k-trees; many other well-studied graph families also have bounded treewidth.|$|E
50|$|With no {{structural}} constraints, it {{may seem}} that a skew heap would be horribly inefficient. However, amortized <b>complexity</b> <b>analysis</b> {{can be used to}} demonstrate that all operations on a skew heap can be done in O(log n).|$|E
5000|$|Together {{with the}} Turing machine and counter-machine models, the RAM and RASP models {{are used for}} {{computational}} <b>complexity</b> <b>analysis.</b> Van Emde Boas (1990) calls these three plus the pointer machine [...] "sequential machine" [...] models, to distinguish them from [...] "parallel random-access machine" [...] models.|$|E
40|$|We give a {{correspondence}} between two notions of complexity for real numbers and functions: poly-time computability according to Ko and {{a notion that}} arises naturally when one considers the application of Melhorn’s class of the Basic Feasible Functionals to computable analysis. We show that both notions define {{the same set of}} real numbers and functions. Key words: Basic feasible functionals. <b>Complexity</b> in <b>analysis.</b> ...|$|R
40|$|In {{this paper}} we cryptanalyze the {{proposed}} (almost accepted) ANSI X 9. 52 CBCM mode. The CBCM mode is a triple-DES CBC variant {{which was designed}} against powerful attacks which control intermediate feedbacks {{for the benefit of}} the attacker. For this purpose, it uses intermediate feedbacks that the attacker cannot control, choosing them as a keyed OFB stream, independent of the plaintexts and ciphertexts. The attack we describe finds a way to use even this kind of feedback {{for the benefit of the}} attacker. It requires a single chosen ciphertext of 2 65 blocks and 2 58 <b>complexity</b> of <b>analysis.</b> We also describe an adaptive known-IV related-key attack which find one of three 56 -bit keys requiring one known plaintext encrypted under 2 33 different but related keys with 2 57 <b>complexity</b> of <b>analysis.</b> Key words. Cryptanalysis. ANSI X 9. 52. Modes of operation. CBCM mode. Triple-DES. Multiple Encryption. 1 Introduction The Data Encryption Standard (DES) [14] has been the subject of intense [...] ...|$|R
30|$|To {{answer the}} second {{research}} question, we analyzed the identified literature sources according to general approaches for complexity driver’s identification, operationalization and visualization. Parry, Purchase and Mills [194] argue that recognition and {{identification of the}} complexity drivers “enable managers to realize value” and “reducing complexity where possible". Ehrenmann [67] argues further that <b>complexity</b> driver’s <b>analysis</b> enables first indications about the success of process’s changing.|$|R
