11|79|Public
3000|$|... c, above {{which all}} {{earthquakes}} {{are considered to}} be detected by a seismic network, is vital for seismicity-related studies. We paid attention to catalog completeness and performed a <b>completeness</b> <b>check</b> as a preprocessing step of individual analyses while referring to a comprehensive analysis of M [...]...|$|E
40|$|The {{simulation}} method of digital circuits applied {{to solving the}} problem of finding fault is not detected by the reaction scheme on the same input sequence signals (<b>completeness</b> <b>check</b> test). The method allows to build models of digital circuits that have better performance simulation speed compared with known methods. High speed simulation is achieved by bringing the process modeling operations successive logical multiplication and addition business fields, which contain all the necessary information about signals and malfunctions scheme. ??????????????? ????? ????????????? ???????? ???? ????????????? ? ??????? ?????? ?????? ?????????????, ?? ?????????????? ?? ??????? ????? ?? ???????? ??????? ?????????????????? ???????? (???????? ??????? ?????). ????? ????????? ??????? ?????? ???????? ????, ?????????? ??????? ???????????? ???????? ????????????? ?? ????????? ? ?????????? ????????. ??????? ???????? ????????????? ??????????? ?? ???? ???????? ???????? ????????????? ? ????????? ???????????? ??????????? ????????? ? ???????? ??????? ?????, ? ??????? ?????????? ??? ??????????? ?????????? ? ???????? ? ?????????????? ?????...|$|E
40|$|Abstract. We {{describe}} the new software package Aligator for automatically inferring polynomial loop invariants. The package combines algorithms from symbolic summation and polynomial algebra with computational logic, and is {{applicable to the}} rich class of P-solvable loops. Aligator contains routines for checking the P-solvability of loops, transforming them into a system of recurrence equations, solving recurrences and deriving closed forms of loop variables, computing the ideal of polynomial invariants by variable elimination, invariant filtering and <b>completeness</b> <b>check</b> of the resulting set of invariants. ...|$|E
50|$|Undertake <b>completeness</b> <b>checks.</b>|$|R
50|$|<b>Completeness</b> <b>checks</b> - {{controls}} that ensure all records were processed from initiation to completion.|$|R
30|$|Ongoing {{and future}} work covers several topics {{related to the}} ODDs. Beside generic ODD editors, which have been {{introduced}} in [22], also automatic mechanisms for validating the created ODDs are required in practice to ensure valid specifications. Research is being spent on reasoning-based approaches for consistency and <b>completeness</b> <b>checks</b> of ODDs. Reasoning is applicable since OWL as the underlying specification language is a formal logical language that directly supports reasoning, which is a major advantage of OWL.|$|R
40|$|Abstract. This {{article focuses}} on how to assess {{projects}} implementing business processes and IT systems on compliance with an Enterprise Architecture that provides constraints and high-level solutions. First, the core elements of Enterprise Architecture compliance testing are presented. Second, we discuss the testing process and four types of compliance checks (correctness check, justification check, consistency check and <b>completeness</b> <b>check).</b> Finally, an empirical case is reported in which a real-life project has been tested on conformance. The results show that our approach works. Furthermore, to increase the objectivity of compliance testing, operationalization of EA prescriptions is recommended...|$|E
40|$|The ISO-standard for LCA distinguishes four phases, {{of which}} the last one, the interpretation, is the least elaborated. It can be {{regarded}} as containing procedural steps (like a <b>completeness</b> <b>check)</b> as well as numerical steps (like a sensitivity check). This paper provides five examples of techniques {{that can be used for}} the numerical steps. These are the contribution analysis, the perturbation analysis, the uncertainty analysis, the comparative analysis, and the discernibility analysis. All five techniques are described at a non technical level with respect to basic concept, possibilities, tabular and graphical representation, restriction and warnings, and all are illustrated with a simple example...|$|E
30|$|To {{strengthen}} the related conclusion, {{it would be}} better to conduct a reliability analysis of the revealed anomalies in detail, e.g., following the approach taken by Huang (2006). However, the main purpose of this paper was to provide the first results on how to recognize possible precursory episodes to the 2016 Kumamoto earthquakes, as described in “Introduction.” Thus, we presented essential parts of the reliability analysis such as <b>completeness</b> <b>check,</b> quality test of a declustered catalog, and some levels of a parameter survey. For a full justification of the present conclusion, further detailed investigations involving the robustness of the temporal and spatial pattern should be conducted.|$|E
40|$|To date most {{validation}} {{techniques are}} highly biased towards calculations involving symbolic representations of problems. These calculations are either formal (in {{the case of}} consistency and <b>completeness</b> <b>checks),</b> or informal {{in the case of}} code inspections. The authors believe that an essential type of evidence of the correctness of the formalization process must be provided by (i. e., must originate from) human-based calculation. They further believe that human calculation can by significantly amplified by shifting from symbolic representations to graphical representations. This paper describes their preliminary efforts in realizing such a representational shift...|$|R
30|$|All {{census data}} were entered in Microsoft Excel®. Data were {{examined}} for inconsistencies or anomalies and cleaned to remove outliers and entry errors and checked for missing fields, and common answers entered with different spellings were <b>checked.</b> <b>Completeness</b> and precision <b>checks</b> were undertaken by cross-checking handwritten form data with Excel spread sheet data.|$|R
40|$|A {{software}} development methodology is defined that integrates software specification and validation efforts. The integration helps in achieving the twin goals of correct software with well-defined specifications that document it. The {{major focus of}} the paper is on dynamic lifecycle models. We indicate how language analysis of the problem description can be extended to derive not only a static class model for the system but also an initial dynamic lifecycle model. Consistency and <b>completeness</b> <b>checks</b> are provided which further drive the requirements elicitation. A case study is presented that clearly demonstrates the methodology. Lakos, Charles and Malhotra, Vish...|$|R
40|$|This article {{examines}} how to assess projects, which implement business processes and IT systems, {{on compliance with}} an Enterprise Architecture (EA) that provides them with constraints and high-level solutions. The authors begin by presenting the core elements of EA compliance testing. Next, the authors discuss the testing process and four types of compliance checks (i. e., correctness check, justification check, consistency check, and <b>completeness</b> <b>check).</b> Finally, an empirical case is reported in which a real-life project has been tested on conformance, demonstrating and evaluating the authors ’ approach. The results indicate that objective compliance testing cannot be taken for granted. Therefore, several suggestions are presented to decrease the subjectivity of assessments, such as operationalization of EA prescriptions. Keywords...|$|E
40|$|A practitioner’s {{approach}} to integrate databases and evolve them {{so as to}} support new database applications is presented. The approach consists of a joint bottom-up and top-down methodology; the bottom-up approach is taken to integrate existing database using standard schema integration techniques(B-Schema), the top-down approach is used to develop a database schema for the new applications(T-Schema). The T-Schema uses a joint functional-data analysis. The B-schema is evolved by comparing it with the generated T-schema. This facilitates an evolutionary {{approach to}} integrate existing databases to support new applications as and when needed. The mutual <b>completeness</b> <b>check</b> of the T-Schema against B-Schema derive the schema modification steps to be performed on B-Schema {{to meet the requirements}} of the new database applications. A case study is presented to illustrate the methodology...|$|E
40|$|AbstractThis paper aims {{to present}} {{the results of an}} {{experiment}} linked to positioning and the contact patch size of a double worm face gear. The worm face gear tested is provided with one-piece double worm face wheel that involves specific adjustment difficulties. Couple of used materials is the hard steel for the worm and the gray cast iron for the wheel. We mention technological conditions for obtaining the worm and the worm wheel. The contact patch <b>completeness</b> <b>check</b> gear flanks results was done on a test stand using training gear on both sides of succession worm knowing the fact that they show different pressure angles. The contact patch has different values obtained on {{the two sides of the}} flanks, the smaller pressure angle ensuring better conditions of engagement...|$|E
40|$|We {{describe}} EXPRESSION, {{a language}} supporting architectural design space exploration for embedded Systems-onChip (SOC) and automatic generation of a retargetable compiler /simulator toolkit. Key features of our language-driven design methodology include: a mixed behavioral/structural representation supporting a natural specification of the architecture; explicit specification {{of the memory}} subsystem allowing novel memory organizations and hierarchies; clean syntax and ease of modification supporting architectural exploration; a single specification supporting consistency and <b>completeness</b> <b>checking</b> of the architecture; and efficient specification of architectural resource constraints allowing extraction of detailed reservation tables for compiler scheduling. We illustrate key features of EXPRESSION through simple examples and demonstrate its efficacy in supporting exploration and automatic software toolkit generation for an embedded SOC codesign flow. 1 Introduction The advent of Syste [...] ...|$|R
40|$|The {{development}} of complex, safety-critical systems for aero-engine control {{is subject to}} the, often competing, demands for higher safety and reduced development cost. Although the commercial aerospace industry has a general good safety record, and has placed much emphasis on process improvement within a strong safety culture, there {{continues to be a}} large number of design and requirements errors found during development and after entry into service. 'The thesis assesses current system safety practice within the aero engine control system industry, including international standards, and reviews the current practice against the research at MIT by Professor Nancy Leveson. The thesis focuses in particular on software safety as this is the area that has proven most challenging and most likely to experience high costs. The particular research topics reviewed are Intent Specifications, the System Theoretic Accident Modeling and Processes (STAMP) technique, and requirements completeness criteria. Research shows that many problems arise from requirements and design errors rather than component failures. Several example incidents from an engine company are reviewed and these show a pattern of common problems which could have been caught by the use of more comprehensive requirements <b>completeness</b> <b>checks</b> and by the use of Intent Specifications. In particular, assumptions are not currently documented in the specifications but are kept separately, and the need to identify assumptions is not emphasized enough in existing processes. (cont.) It is concluded that the existing development process has significant room for improvement in the coordination between the safety assessment and system development processes. In particular, more could be done by the use of requirements <b>completeness</b> <b>checks,</b> software hazard analysis, the adoption of the Intent Specification approach and {{in the use of the}} STAMP models. by Malvern J. Atherton. Thesis (S. M.) [...] Massachusetts Institute of Technology, System Design and Management Program, 2005. Includes bibliographical references (p. 108 - 110) ...|$|R
40|$|This thesis {{concerns}} {{the issue of}} completness detection of instruction set description for the LLVM compiler, or {{the ability of a}} compiler to generate target program for every valid source program in the appropriate high-level programming language. On the basis of regular tree grammar theory and several scietific theses that also concern this issue, formal tool for inclusion checking of two grammars. Furthermore a method for automatic extraction of the two grammars from the instruction set description has been devised, as a result of which the tool can be used for <b>checking</b> <b>completeness</b> of instruction selection. In combination with <b>checking</b> <b>completeness</b> of the legalization process of the LLVM compiler, which preceeds the instruction selection, it should be feaseable to <b>check</b> <b>completeness</b> of most compiler parts dependent on the target architecture...|$|R
40|$|We {{consider}} the transport properties of multiple-particle quantum {{states in a}} class of one-dimensional systems with a single quantum impurity. In these systems, the local interaction at the quantum impurity induces strong and non-trivial correlations between the multi-particles. We outline an exact theoretical approach, based upon real-space equations of motion and the Bethe ansatz, that allows one to construct the full scattering matrix (S-matrix) for these systems. In particular, we emphasize the need for <b>completeness</b> <b>check</b> upon the eigenstates of the S-matrix, when these states obtained from Bethe Ansatz are used for describing the scattering properties. As a detailed example of our approach, we solve the transport properties of two photons incident on a single two-level atom, when the photons are restricted to a one-dimensional system such as a photonic crystal waveguide. Our approach predicts a number of novel nonlinear effects involving only two photons, including background fluorescence, spatial attraction and repulsion between the photons, as well as the emergence of a two-photon bound state. Comment: 67 pages, 11 figure...|$|E
40|$|Police is {{one form}} of defense and {{security}} organizations in Indonesia has spread all over the country. Police are social groups that become sections of society. Members of the police function as penindak and guardian of peace {{is part of the}} function Kamtibmas. Order traffic is of striking in Indonesia today. Society seen casually in driving on the highway, which resulted in many traffic accidents and fatalities take no less. Every motorist will not comply with existing signs or markers in highway. The lack of discipline and motorists would obey the traffic rules only if there is any officer is also a reflection of the cultural community [...] Viewing conditions The police then worked with Java Post issued a Letter Telegram Police Chief Java No. Pol: So St/ 599 /Ix/ 2005 /Dit Date September 9, 2005 About Ethics And Campaign Cross With Riding Safety Program. It contains: Using the left lane for motorcycles and general passenger cars. <b>Completeness</b> <b>check</b> your mirrors, lights and brakes before sein drive, ensure a 'click' to terkuncinya helmet and safety belt, "Light on" even in the daytime lights for motorcycles to be "Eye Catching" against another vehicle. A lot of work done by officers to disseminate program Riding Safety is for people to drive on the highway very well. Socialization itself is a process of dissemination of information or a new message to the community wide. From the above phenomenon, the authors take the formulation of the problem is there any effect socialization Safety Riding on driving behavior. The {{purpose of this study is}} to determine whether there is socialization influence on behavior Riding Safety drive. The theory used in this research is the theory of cognitive dissonance people behave generally steady or consistent with what he knows. But reality shows that are often inconsistent someone behave like that...|$|E
40|$|Structural Desigu Standards are {{described}} in a computer using production-rules, and advantages of such description {{in terms of the}} <b>completeness</b> <b>checking</b> of respective desigu provisions as well as their accuracy evaluation are presented. Background information for structural desigu, such as commentaries of desigu standards, previous experimental data, and other relevant documents, is also stored in a computer and linked with the computerized desigu standards. A structural desigu system, in which computerized desigu standards and structural analysis programs are linked via a user-interface, is developed. Here, object-oriented programming is employed for facilitating the identification of structural members and elements. An example of structural design using the system devised is presented, its advantages over conventional structural design systems stated, and the potential of this system as an aid to conceptual structural design demonstrated...|$|R
40|$|Tabular {{notation}} is a {{very important}} part of the functional documentation method that is used to produce computer system specifications. Many types of tables are currently used by the Software Engineering Research Group at McMaster University. Generalized decision tables are one of the types. In order to facilitate the use of tabular notation, a variety of tools were built or are being built to simplify tables, convert between table formats, check syntax, automatically generate test oracles, etc. Other types of decision tables are used by industry to write software specifications. Structured decision tables are one of the four types of tables adopted by Ontario Hydro for their safety critical software documentation. Like the Software Engineering Research Group, they also have tools to conduct syntax checking, software design verification, table <b>completeness</b> <b>checking,</b> table consistency checking, etc. The two sets of tools overlap in some areas but not others. Structured decision tables [...] ...|$|R
40|$|The {{conceptual}} {{modeling of}} the Universe of Discourse (UoD) {{is an important}} phase {{for the development of}} information systems because the conceptual model is the basis for system development. Conceptual model specifications must be formal in order to be precise and unambiguous and to support consistency and <b>completeness</b> <b>checks.</b> The object-oriented paradigm is suitable for providing an integrated formal description of all relevant static and dynamic aspects of the UoD structured in objects. In this paper we introduce a formal concept of object suitable to represent the UoD by a collection of concurrent interacting objects. The Oblog-language for object-oriented UoD-specification based on this concept supports the integrated description of data about objects, the development of objects through time and of various relationships between objects taking into account static and dynamic aspects of object interaction. 1 Introduction Information systems are data-intensive software systems which [...] ...|$|R
40|$|The Decision Model and Notation (DMN) is a {{standard}} notation to capture decision logic in business applications in general and business processes in particular. A central construct in DMN {{is that of a}} decision table. The increasing use of DMN decision tables to capture critical business knowledge raises the need to support analysis tasks on these tables such as correctness and <b>completeness</b> <b>checking.</b> This paper provides a formal semantics for DMN tables, a formal definition of key analysis tasks and scalable algorithms to tackle two such tasks, i. e., detection of overlapping rules and of missing rules. The algorithms are based on a geometric interpretation of decision tables {{that can be used to}} support other analysis tasks by tapping into geometric algorithms. The algorithms have been implemented in an open-source DMN editor and tested on large decision tables derived from a credit lending dataset. Comment: Submitted to the International Conference on Business Process Management (BPM 2016...|$|R
2500|$|For [...] is a Banach space. The {{fact that}} [...] is {{complete}} {{is often referred}} to as the Riesz-Fischer theorem. <b>Completeness</b> can be <b>checked</b> using the convergence theorems for Lebesgue integrals.|$|R
40|$|The ever {{increasing}} amount of Semantic Web data {{gives rise to}} the question: How complete is the data? Though generally data on the Semantic Web is incomplete, many parts of data are indeed complete, such as the children of Barack Obama and the crew of Apollo 11. This thesis aims to study how to manage and consume completeness information about Semantic Web data. In particular, we first discuss how completeness information can guarantee the completeness of query answering. Next, we propose optimization techniques of completeness reasoning and conduct experimental evaluations to show the feasibility of our approaches. We also provide a technique to check the soundness of queries with negation via reduction to query <b>completeness</b> <b>checking.</b> We further enrich completeness information with timestamps, enabling query answers to be checked up to when they are complete. We then introduce two demonstrators, i. e., CORNER and COOL-WD, to show how our completeness framework can be realized. Finally, we investigate an automated method to generate completeness statements from text on the Web via relation cardinality extraction...|$|R
40|$|The {{development}} of multi-agent software systems {{is considered a}} complex task due to (a) the large number and heterogeneity of documents generated during the {{development of}} these systems, (b) the lack of support for the whole development life-cycle by existing agent-oriented methodologies requiring the use of different methodologies, and (c) the possible incompleteness of the documents and models generated during the development of the systems. In order to alleviate the above problems, in this thesis, a traceability framework is described to support the development of multi-agent systems. The framework supports automatic generation of traceability relations and identification of missing elements (i. e., <b>completeness</b> <b>checking)</b> in the models created during the development life-cycle of multi-agent systems using the Belief-Desire-Intention (BDI) architecture. Traceability has been recognized as an important activity in the software development process. Traceability relations can guarantee and improve software quality and can help with several tasks such as the evolution of software systems, reuse of parts of the system, validation that a system meets its requirements, understanding of the rationale for certain design decisions, identification of common aspects of the system, and analysis of implications of changes in the system. The traceability framework presented in this thesis concentrates on multi-agent software systems developed using i* framework, Prometheus methodology, and JACK language. Here, a traceability reference model is presented for software artefacts generated when using i* framework, Prometheus methodology, and JACK language. Different types of relations between the artefacts are identified. The framework is based on a rule-based approach to support automatic identification of traceability relations and missing elements between the generated artefacts. Software models represented in XML were used to support the heterogeneity of models and tools used during the software development life-cycle. In the framework, the rules are specified in an extension of XQuery to support (i) representation of the consequence part of the rules, i. e. the actions to be taken when the conditions are satisfied, and (ii) extra functions to cover some of the traceability relations being proposed and <b>completeness</b> <b>checking</b> of the models. A prototype tool has been developed to illustrate and evaluate the work. The work has been evaluated in terms of recall and precision measurements in three different case studies. One small case study of an Automatic Teller Machine application, one medium case study of an Air Traffic Control Environment application, and one large case study of an Electronic Bookstore application. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The ATCOSIM speech corpus {{provided}} by Eurocontrol Experimental Centre has been validated against {{a list of}} specified checks. The validation covered <b>completeness,</b> formal <b>checks</b> and manual checks of randomly selected samples. The overall quality of the corpus is good {{and there should be}} no problem in using the corpus for speech applications. Minor improvements to the documentation have been proposed and may be incorporated without much effort. The ATCOSIM corpus is in a useable state. ...|$|R
40|$|Today, many {{software}} {{organizations are}} utilizing product families {{as a way}} of improving productivity, improving quality and reducing development time. When a new member is added to a product family, there must be a way to verify whether the new member 2 ̆ 7 s specific requirements are met within the reuse constraints of its product family. The contribution {{of this paper is to}} demonstrate such a verification process by describing a requirements engineering tool called DECIMAL. DECIMAL is an interactive, automated, GUI driven verification tool that automatically <b>checks</b> for <b>completeness</b> (<b>checking</b> to see if all commonalities are satisfied) and consistency (checking to see if dependencies between variabilities are satisfied) of the new member 2 ̆ 7 s requirements with the product family 2 ̆ 7 s requirements. DECIMAL also checks that variabilities are within the range and data type specified for the product family. The approach is to perform the verification using a database as the underlying analysis engine. A pilot study of a virtual reality device driver product family is also described which investigates the feasibility of this approach by evaluating the tool...|$|R
50|$|Homogenization of data through {{surrogate}} requirements - Requirements management (RM) tools allow storing, organizing, {{and managing}} all {{requirements of a}} system's specifications and typically arrange them in a specification tree that links each requirement to its parent requirement in the higher specification. Commercial RM tools with traceability support are, e.g., 3SL Cradle, CASE Spec, IRise, Gatherspace, IBM Rational RequisitePro, IBM DOORS, CaliberRM, QFDCapture and Matrix_Requirements_Medical. Freely available tools are, e.g., FreeMind and Concordion. Typical analysis functions based on recorded traceability information are, e.g., <b>completeness</b> <b>checks</b> i.e. do all system level requirements go down to equipment level (with or without modification), assessment of requirements deviations over all levels, and qualification status presentation. In order to ensure traceability to artefact types beyond requirements, RM tools often allow to import other artefacts as surrogate requirements that can then be traced with the tool's requirements tracing methods. For example, elements of a Mathworks Simulink model can be imported into an IBM Rational DOORS surrogate module and then be related to requirements. The disadvantage {{of this approach is}} that different adapters or converters for the different artefact types are necessary that need to have a consistent version and data format. In contrast to ALM tools this consistency must be carried out oneself.|$|R
40|$|Software testing {{framework}} can {{be stated}} {{as the process}} of verifying and validating that a computer program/application works as expected and meets {{the requirements of the}} user. Usually testing can be done manually or using tools. Manual testing can be time consuming and tedious, also it requires heavy investment of human resources. Testing tools in fact have many advantages and are widely used nowadays, but also has several disadvantages. One particular problem is that human intervention is no longer needed. Testing tools are of high cost and so, it cannot be used for small companies. Hence in this paper we propose Agent based testing, which is a fast growing approach in testing field. The proposed system (ABSTF) has to reduce the application testing moment, easily find out bug and solve the bug by Regression Testing. Here by we are going to use a safe efficient regression selection technique algorithm to selectively retest the modified program. We also use Traceability relation of <b>completeness</b> <b>checking</b> for agent oriented system, which identifies missing elements in entire application and classifies defect in the testing moment. With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of code can be done dynamically...|$|R
40|$|Although the {{importance}} of scenarios in modeling and simulation has long been well known, there still exists a lack of common understanding and standardized practices in simulation scenario development. This paper proposes a Domain-Specific Language (DLS) to provide a standard scenario specification {{that will lead to}} a common mechanism for verifying and executing aviation scenarios, effective sharing of scenarios among various simulation environments, improve the consistency among different simulators and simulations, and even enable the reuse of scenario specifications. Following DSL design practices, the proposed Aviation Scenario Definition Language (ASDL) will provide a well-structured definition language to formally specify complete aircraft landing scenarios. In order to capture the necessary constructs for a simulation scenario, Simulation Interoperability Standards Organization (SISO) Base Object Model (BOM) is adopted as the baseline metamodel. This baseline is extended using the fundamentals of aircraft landing that cover all the domain-related concepts and terminology as constructs. By taking a formal approach in defining aviation scenarios, ASDL aims at providing consistency and <b>completeness</b> <b>checking,</b> and model-to-text transformations capabilities for various targets in the aviation scenario definition domain. The results of this work will be used to develop a graphical modeling environment and automatic means to transform scenario models into executable scenario scripts. The work presented here is the first stepping stone in formal scenario definition in aviation domain...|$|R
40|$|The {{dissertation}} {{describes a}} practically proven, particularly efficient approach for the verification of digital circuit designs. The approach outperforms simulation based verification wrt. final circuit quality {{as well as}} wrt. required verification effort. In the dissertation, the paradigm of transaction based verification is ported from simulation to formal verification. One consequence is a particular format of formal properties, called operation properties. Circuit descriptions are verified by proof of operation properties with Interval Property Checking (IPC), a particularly strong SAT based formal verification algorithm. Furtheron, a completeness checker is presented that identifies all verification gaps in sets of operation properties. This completeness checker can handle the large operation properties that arise, if this approach is applied to realistic circuits. The methodology of operation properties, Interval Property <b>Checking,</b> and the <b>completeness</b> checker form a symbiosis that is of particular benefit to the verification of digital circuit designs. On top of this symbiosis an approach to completely verify the interaction of completely verified modules has been developed by adaptation of the modelling theories of digital systems. The approach presented in the dissertation has proven in multiple commercial application projects that it indeed completely verifies modules. After reaching a termination criterion that is well defined by <b>completeness</b> <b>checking,</b> no further bugs {{were found in the}} verified modules. The approach is marketed by OneSpin Solutions GmbH, Munich, under the names "Operation Based Verification" and "Gap Free Verification"...|$|R
40|$|When {{technical}} goods, like mainboards {{and other}} electronic components, are produced, quality assurance (QA) is very important. To achieve this goal, different optical microscopes {{can be used}} to analyze a variety of specimen to gain comprehensive information by combining the acquired sensor data. In many industrial processes, cameras are used to examine these technical goods. Those cameras can analyze complete boards at once and offer a high level of accuracy when used for <b>completeness</b> <b>checks.</b> When small defects, e. g. soldered points, need to be examined in detail, those wide area cameras are limited. Microscopes with large magnification need to be used to analyze those critical areas. But microscopes alone cannot fulfill this task within a limited time schedule, because microscopic analysis of complete motherboards of a certain size is time demanding. Microscopes are limited concerning their depth of field and depth of focus, which is why additional components like XY moving tables need to be used to examine the complete surface. Yet today’s industrial production quality standards require a 100 per cent control of the soldered components within a given time schedule. This level of quality, while keeping inspection time low, can only be achieved when combining multiple inspection devices in an optimized manner. This paper presents results and methods of combining industrial cameras with microscopy instrumenting a classificatory based approach intending to keep already deployed QA processes in place but extending them with the purpose of increasing the quality level of the produced technical goods while maintaining high throughput...|$|R
40|$|Abstract Background In the Thai Universal Coverage health {{insurance}} scheme, hospital providers {{are paid for}} their inpatient care using Diagnosis Related Group-based retrospective payment, for which quality of the diagnosis and procedure codes is crucial. However, there has been limited understandings on which health care professions are involved and how the diagnosis and procedure coding is actually done within hospital settings. The objective {{of this study is}} to detail hospital coding structure and process, and to describe the roles of key hospital staff, and other related internal dynamics in Thai hospitals that affect quality of data submitted for inpatient care reimbursement. Methods Research involved qualitative semi-structured interview with 43 participants at 10 hospitals chosen to represent a range of hospital sizes (small/medium/large), location (urban/rural), and type (public/private). Results Hospital Coding Practice has structural and process components. While the structural component includes human resources, hospital committee, and information technology infrastructure, the process component comprises all activities from patient discharge to submission of the diagnosis and procedure codes. At least eight health care professional disciplines are involved in the coding process which comprises seven major steps, each of which involves different hospital staff: 1) Discharge Summarization, 2) <b>Completeness</b> <b>Checking,</b> 3) Diagnosis and Procedure Coding, 4) Code Checking, 5) Relative Weight Challenging, 6) Coding Report, and 7) Internal Audit. The hospital coding practice can be affected by at least five main factors: 1) Internal Dynamics, 2) Management Context, 3) Financial Dependency, 4) Resource and Capacity, and 5) External Factors. Conclusions Hospital coding practice comprises both structural and process components, involves many health care professional disciplines, and is greatly varied across hospitals as a result of five main factors. </p...|$|R
40|$|In this paper, {{we present}} an active expert system (OAV-VVT) to verify and {{validate}} the knowledge during the implementation and maintenance phases of Knowledge Base system life cycle. Our approach in OAV-VVT expert {{is based on}} new representation and reasoning techniques, Ex-OAV KB, based on Object-Attribute-Value (OAV) Knowledge representation. All common issues of verification and validation (V&V) such as consistency, completeness, logical & semantic contradiction and correctness of knowledge base are considered in OAV-VVT expert. The designed system is application independent and it has several important advantages: First, OAV-VVT expert can be either used during the knowledge representation phase or refinement of knowledge base. Second, OAV-VVT expert is an expert system with reasoning and explanation mechanisms. Third, OAV-VVT expert is application and knowledge representation independent, due to use of Ex-OAV KB, {{so it can be}} applied to all knowledge representation techniques (i. e. : logic, semantic network, frame, production rule, …). Fourth, OAV-VVT expert can check the unreferenced attribute value and the illegal attribute value, which are the issues of <b>completeness</b> <b>checking,</b> during the transforming the existing KB to Ex-OAV KB. So the performance of V&V will increase. OAV-VVT expert by explanation component, which is an essential part in any expert system, can explain “how ” an invalid knowledge has been discovered and “why ” it is invalid. In this paper we also present the transforming algorithms for transforming the major knowledge representation techniques to Ex-OAV KB {{in order to be able}} to verify and validate the knowledge. In representation, we will present the same semantic of knowledge and the syntax of knowledge will change. ...|$|R
