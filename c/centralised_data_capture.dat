2|10000|Public
40|$|This paper {{presents}} {{a case study}} of the efficiency gains resulting from the introduction of electronic technologies to monitor and support adherence to highly active antiretroviral therapy (HAART) in Guguletu, South Africa. It suggests that the rollout of HAART to such resource-poor communities can be assisted significantly by the introduction of modified cellphones (to provide home based support to people on HAART and improve the management of adherence data) and simple bar-coding and scanning equipment (to manage drug supplies). The cellphones have improved the management of information, and simplified the working lives of therapeutic counsellors, thereby enabling them to spend less time on administration and to devote a constant amount of time per patient even though their case loads have risen threefold. It has helped integrate the local-level primary health service provision of HAART with the kind of <b>centralised</b> <b>data</b> <b>capture</b> and analysis that could potentially support a national HAART rollout. ...|$|E
40|$|This article {{reports about}} the {{internet}} based, second multicenter study (MCS II) of the spine study group (AG WS) of the German trauma association (DGU). It represents {{a continuation of the}} first study conducted between the years 1994 and 1996 (MCS I). For the purpose of one common, <b>centralised</b> <b>data</b> <b>capture</b> methodology, a newly developed internet-based data collection system ([URL]) of the Institute for Evaluative Research in Orthopaedic Surgery of the University of Bern was used. The aim of this first publication on the MCS II was to describe in detail the new method of data collection and the structure of the developed data base system, via internet. The goal of the study was the assessment of the current state of treatment for fresh traumatic injuries of the thoracolumbar spine in the German speaking part of Europe. For that reason, we intended to collect large number of cases and representative, valid information about the radiographic, clinical and subjective treatment outcomes. Thanks to the new study design of MCS II, not only the common surgical treatment concepts, but also the new and constantly broadening spectrum of spine surgery, i. e. vertebro-/kyphoplasty, computer assisted surgery and navigation, minimal-invasive, and endoscopic techniques, documented and evaluated. We present a first statistical overview and preliminary analysis of 18 centers from Germany and Austria that participated in MCS II. A real time data capture at source was made possible by the constant availability of the data collection system via internet access. Following the principle of an application service provider, software, questionnaires and validation routines are located on a central server, which is accessed from the periphery (hospitals) by means of standard Internet browsers. By that, costly and time consuming software installation and maintenance of local data repositories are avoided and, more importantly, cumbersome migration of data into one integrated database becomes obsolete. Finally, this set-up also replaces traditional systems wherein paper questionnaires were mailed to the central study office and entered by hand whereby incomplete or incorrect forms always represent a resource consuming problem and source of error. With the new study concept and the expanded inclusion criteria of MCS II 1, 251 case histories with admission and surgical data were collected. This remarkable number of interventions documented during 24 months represents an increase of 183 % compared to the previously conducted MCS I. The concept and technical feasibility of the MEMdoc data collection system was proven, as the participants of the MCS II succeeded in collecting data ever published on the largest series of patients with spinal injuries treated within a 2 year period...|$|E
5000|$|A {{distributed}} ledger (also called shared ledger) {{is a consensus}} of replicated, shared, and synchronized digital data geographically spread across multiple sites, countries, or institutions. [...] There is no central administrator or <b>centralised</b> <b>data</b> storage.|$|R
30|$|There are {{a diverse}} set of methods for {{collecting}} monitoring state from cloud deployments. Many tools still rely upon fully <b>centralised</b> <b>data</b> collection, {{while others have}} extended this design {{through the use of}} trees and other forms of overlay.|$|R
40|$|There is {{increasing}} realisation that edge devices, which {{are closer to}} a user, {{can play an important}} part in supporting latency and privacy sensitive applications. Such devices have also continued to increase in capability over recent years, ranging in complexity from embedded resources (e. g. Raspberry Pi, Arduino boards) placed alongside <b>data</b> <b>capture</b> devices to more complex “micro data centres”. Using such resources, a user is able to carry out task execution and data storage in proximity to their location, often making use of computing resources that can have varying ownership and access rights. Increasing performance requirements for stream processing applications (for instance), which incur delays between the client and the cloud have led to newer models of computation, which requires an application workflow to be split across data centre and edge resource capabilities. With recent emergence of edge/fog computing it has become possible to migrate services to micro-data centres and to address the performance limitations of traditional (<b>centralised</b> <b>data</b> centre) cloud based applications. Such migration can be represented as a cost function that involves incentives for micro-data centres to host services with associated quality of services and experience. Business models need to be developed for creating an open edge cloud environment where micro-data centres have the right incentives to support service hosting, and for large scale data centre operators to outsource service execution to such micro data centres. We describe potential revenue models for micro-data centers to support service migration and serve incoming requests for edge based applications. We present several cost models which involve combined use of edge devices and <b>centralised</b> <b>data</b> centres...|$|R
5000|$|To build, provide, maintain, and {{optimise}} an (electronic) databank with <b>centralised</b> <b>data</b> on HLA (human leucocyte antigen) phenotypes {{and other}} relevant data of volunteer stem cell donors and cryopreserved cord blood products and make these data {{accessible to the}} physicians, search coordinators, and other parties worldwide who search for a potential match for their patient; ...|$|R
50|$|Palo is {{a memory}} {{resident}} multidimensional (online analytical processing (OLAP) or multidimensional online analytical processing (MOLAP)) database server and typically {{used as a}} business intelligence tool for controlling and budgeting purposes with spreadsheet software acting as the user interface. Beyond the multidimensional data concept, Palo enables multiple users to share one <b>centralised</b> <b>data</b> storage (single version of the truth).|$|R
40|$|Many {{applications}} require <b>data</b> to be <b>captured</b> {{and processed}} in real time, as {{the migration of}} all the data to a central site prior to analysis can create significant overhead. Examples include the variety of sensor network-based applications where sensors interface with real world artefacts and must respond to physical phenomenon that cannot be predicted apriori. The amount of data likely to be generated by a sensor and processing requirements in such applications can not be pre-determined (as they are often dependent on {{the rate of change}} of the physical phenomenon being measured and potential occurrence of “trigger events” which are non-deterministic). We propose the use of a multilayer Cloud infrastructure that distributes processing over both sensing nodes, multiple intermediate/gateways nodes and the more complex <b>centralised</b> <b>data</b> centre. Such layers need to work in coordination to ensure more reliable and efficient use of computing and network resources, preventing the need to move data to a central location when this is not necessary and creating data processing paths from <b>data</b> <b>capture</b> to analysis. We outline the basis for a decision function that evaluates: (i) where processing should be carried out; (ii) what processing should be undertaken centrally vs at an edge node; (iii) how processing can be distributed across multiple data centre locations to achieve QoS and cost targets...|$|R
50|$|The HP 1000 {{series runs}} the RTE Real-time {{operating}} system, initially for technical applications and electronic <b>data</b> <b>capture,</b> mainly {{to store the}} <b>data</b> <b>captured.</b> It is used also to analyse the <b>data</b> <b>captured</b> to generate graphics and reports.|$|R
40|$|Correspondence {{issued by}} the General Accounting Office with an {{abstract}} that begins "Pursuant to a congressional request, GAO provided information on the Bureau of the Census' progress in: (1) performing first-pass <b>data</b> <b>capture</b> operations, including {{the performance of the}} <b>Data</b> <b>Capture</b> System 2000; (DCS) and (2) modifying DCS 2000 to perform planned second-pass <b>data</b> <b>capture</b> operations. ...|$|R
40|$|The aims of {{the present}} study were to {{incorporate}} and to validate the electronic capture of participant-related outcomes into the Oral Survey-B System, which was originally developed for the electronic <b>capture</b> of clinical <b>data.</b> The validation process compared the performances of electronic and handwritten <b>data</b> <b>captures.</b> The hypothesis of noninferiority would be established if participants performed electronic <b>data</b> <b>capture</b> of the questionnaire survey with an effectiveness of at least 95 % of that of handwritten <b>data</b> <b>capture.</b> In this multicenter, randomized, one-period crossover study design, participants (n = 261) were allocated to start with either electronic or handwritten <b>data</b> <b>capture.</b> The incorporation of the electronic self-completed questionnaire into the Oral Survey-B System was successful. The validation of the electronic questionnaire was performed by participants aged from 18 to 75 years. The interrater reliability of participants performing electronic and handwritten <b>data</b> <b>capture</b> of nonclinical assessments per questionnaire and per entry showed a kappa value of 0. 72 (95 % CI: 0. 53 - 0. 94). The noninferiority of electronic <b>data</b> <b>capture</b> in relation to that of the handwritten <b>data</b> <b>capture</b> and transfer was shown (p < 0. 0001; 95 % CI: 1. 47 - 2. 99). In conclusion, the electronic capture of participant-related outcomes with the Oral Survey-B System, originally designed for <b>capture</b> of clinical <b>data,</b> was validated. The electronic <b>data</b> <b>capture</b> was accurate and limited the number of errors. The participants were able to perform electronic <b>data</b> <b>capture</b> effectively, supporting its implementation in further National Oral Health Surveys. With the consideration of participant preference and time savings, this could lead to the implementation of electronic <b>data</b> <b>capture</b> worldwide in National Oral Health Surveys. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
5000|$|Captricity is a <b>data</b> <b>capture</b> {{software}} program (and {{the company that}} sells it) that uses a combination of machine-learning and human verification to perform OCR [...] <b>data</b> <b>capture</b> from hand-filled forms.|$|R
40|$|The Belgian National Institute of Health Insurance is {{implementing}} an {{oral health}} data registration and surveillance system. This study {{aimed to develop}} and validate a system of electronic <b>data</b> <b>capture</b> for oral health surveys at a national level - Oral Survey-B - and to identify {{the advantages and disadvantages}} of the electronic system in comparison with the traditional handwritten <b>data</b> <b>capture.</b> Six series of full-mouth recordings simulating the clinical examination of 6 patients were set up in a Powerpoint presentation. The validation was undertaken by 52 general practitioners. A randomized one-period crossover design was used with two formats of <b>data</b> <b>capture,</b> i. e. electronic followed by handwritten or handwritten followed by electronic system. Further, 6 benchmarked handwritten forms were transferred to the electronic format. For the electronic <b>data</b> <b>capture,</b> 86. 5 % of the practitioners had a correct completion rate of ≥ 95 %. The corresponding value for the handwritten <b>data</b> <b>capture</b> and transfer was 78. 8 % (p = 0. 25, McNemar test). The overall accuracy of forms without any error was 73. 4 % for the electronic and 62. 5 % for the handwritten <b>data</b> <b>capture</b> (p < 0. 001, signed-rank test). Significantly lower percentages of errors and less time were observed for the electronic <b>data</b> <b>capture</b> (p < 0. 001, signed-rank test). Practitioners considered the electronic <b>data</b> <b>capture</b> as being much more difficult to carry out (p < 0. 001). As information technology has turned into an ever more necessary working tool in epidemiology, there should be an important potential for uptake of further improvements in electronic <b>data</b> <b>capture</b> in the future. Copyright © 2011 S. Karger AG, Basel. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|The Generic <b>Data</b> <b>Capture</b> Facility, {{which can}} provide <b>data</b> <b>capture</b> {{support for a}} variety of {{different}} types of spacecraft while enabling operations costs to be carefully controlled, is discussed. The <b>data</b> <b>capture</b> functions, <b>data</b> protection, isolation of users from data acquisition problems, data reconstruction, and quality and accounting are addressed. The TDM and packet data formats utilized by the system are described, and the development of generic facilities is considered...|$|R
50|$|Remote <b>data</b> <b>capture</b> is {{the process}} of {{automatic}} collection of scientific data. It is widely used in clinic trials, where it is referred to as electronic <b>data</b> <b>capture.</b> In physical sciences, automatic observation hardware in the field can be linked to an observer in a laboratory through a cellphone or other communication link, for example in hydrology. RDC systems influenced the design of later electronic <b>data</b> <b>capture</b> (EDC) systems.|$|R
40|$|Abstract Background The {{systematic}} {{capture of}} appropriately annotated experimental data {{is a prerequisite}} for most bioinformatics analyses. <b>Data</b> <b>capture</b> is required not only for submission of data to public repositories, but also to underpin integrated analysis, archiving, and sharing – both within laboratories and in collaborative projects. The widespread requirement to <b>capture</b> <b>data</b> means that <b>data</b> <b>capture</b> and annotation are taking place at many sites, but the small scale of the literature on tools, techniques and experiences suggests that there is work to be done to identify good practice and reduce duplication of effort. Results This paper reports on experience gained in the deployment of the Pedro <b>data</b> <b>capture</b> tool in a range of representative bioinformatics applications. The paper makes explicit the requirements that have recurred when <b>capturing</b> <b>data</b> in different contexts, indicates how these requirements are addressed in Pedro, and describes case studies that illustrate where the requirements have arisen in practice. Conclusion <b>Data</b> <b>capture</b> is a fundamental activity for bioinformatics; all biological data resources build on some form of <b>data</b> <b>capture</b> activity, and many require a blend of import, analysis and annotation. Recurring requirements in <b>data</b> <b>capture</b> suggest that model-driven architectures can be used to construct <b>data</b> <b>capture</b> infrastructures that can be rapidly configured {{to meet the needs of}} individual use cases. We have described how one such model-driven infrastructure, namely Pedro, has been deployed in representative case studies, and discussed the extent to which the model-driven approach has been effective in practice. </p...|$|R
30|$|Data was {{collected}} and managed using REDCap electronic <b>data</b> <b>capture</b> tools hosted at Cleveland Clinic. REDCap (Research Electronic <b>Data</b> <b>Capture)</b> is a secure, web-based application designed to support <b>data</b> <b>capture</b> for research studies, providing: (1) an intuitive interface for validated data entry; (2) audit trails for tracking data manipulation and export procedures; (3) automated export procedures for seamless data downloads to common statistical packages; and (4) procedures for importing data from external sources.|$|R
40|$|In this paper, a day-labour Mobile Electronic <b>Data</b> <b>Capture</b> and Browsing (MEDCB) {{system is}} presented. In {{building}} and evaluating this system, the primary {{aim was to}} evaluate the possibility of applying mobile <b>data</b> <b>capture</b> and browsing to the day-labour market {{with a view to}} improving <b>data</b> <b>capture</b> and verification accuracy and efficiency. The MEDCB system consists of a mobile client application and a web interface. The system was evaluated with non-profit organizations working for day labour semiliterate job seekers. Results showed that <b>data</b> <b>capture,</b> processing and browsing is a possibility in day labour market. Improvement in accuracy and efficiency was also seen with the use of MEDCB. We describe the design process, present initial findings and discuss the results...|$|R
5000|$|One of {{the most}} useful {{application}} tasks of <b>data</b> <b>capture</b> is collecting information from paper documents and saving it into databases (CMS, ECM and other systems). There are several types of basic technologies used for <b>data</b> <b>capture</b> {{according to the data}} type: ...|$|R
30|$|The {{data were}} {{collected}} and managed using REDCap electronic <b>data</b> <b>capture</b> tools hosted at Vanderbilt University Medical Center. REDCap (Research Electronic <b>Data</b> <b>Capture)</b> is a secure, web-based application designed to support <b>data</b> <b>capture</b> for research studies, providing (1) an intuitive interface for validated data entry; (2) audit trails for tracking data manipulation and export procedures; (3) automated export procedures for seamless data downloads to common statistical packages; and (4) procedures for importing data from external sources (Harris et al. 2009).|$|R
50|$|Pork Farms {{has doubled}} the {{capacity}} of their <b>data</b> <b>capture</b> system at their Queens Drive manufacturing site and all quality paperwork has now {{been removed from the}} production process. CCP, QC and QA results are electronically captured and monitored by their <b>data</b> <b>capture</b> system.|$|R
40|$|AbstractThere {{are many}} aspects of the athlete's {{performance}} that can and need to be measured to improve performance or to fine-tune skills. This can be done visually by a coach or by using sensors attached to the athlete. This paper discusses the common sensor <b>data</b> <b>capture</b> methods and the main requirements for a <b>data</b> <b>capturing</b> system with special attention paid to the requirements for a real time <b>data</b> <b>capturing</b> system. An implementation of a real time <b>data</b> <b>capture</b> system consisting of an iPhone and a laptop is presented. The system was tested on a runner performing a slow jog. The acceleration signatures for the running was streamed from the iPhone to the laptop and displayed in real time thus validating the system...|$|R
40|$|Within {{the field}} of {{clinical}} research, there has been, for many years, a {{move away from the}} use of paper as a form of <b>data</b> <b>capture,</b> in favour of electronic <b>data</b> <b>capture,</b> particularly within industry. The advent of the use of electronic case record forms (eCRFs) is nothing new from that perspective: what remains a challenge is the use of electronic <b>data</b> <b>capture</b> (EDC) at site. This article will, I hope, with some broad generalisation, raise awareness of the issues from the sites’ perspectiv...|$|R
50|$|When <b>data</b> is <b>captured,</b> {{the user}} should {{consider}} if the <b>data</b> should be <b>captured</b> {{with either a}} relative accuracy or absolute accuracy, since this could not only influence how information will be interpreted but also the cost of <b>data</b> <b>capture.</b>|$|R
40|$|With an {{increasing}} interest in Electric Vehicles (EVs), {{it is essential}} to understand howEV charging could impact demand on the Electricity Grid. Existing approaches used to achieve this make use of a <b>centralised</b> <b>data</b> collection mechanism - which often is agnostic of demand variation in a given geographical area. We present an in-transit data processing architecture that is more efficient and can aggregate a variety of different types of data. A model using Reference nets has been developed and evaluated. Our focus in this paper is primarily to introduce requirements for such an architecture...|$|R
40|$|Abstract—This paper {{describes}} a distributed particle swarm optimisation algorithm (PSO) based on peer-to-peer computer networks. A number of modifications {{are made to}} the more traditional synchronous PSO algorithm to allow for fully de-centralised, scalable and fault-tolerent operation. The modified algorithm uses staggered propagation of objective-space knowl-edge between sub-swarms to {{eliminate the need for}} a <b>centralised</b> <b>data</b> store. Analytical test functions are used to examine the performance of the proposed algorithm and its variations in comparison with a basic synchronous PSO implementation. The results clearly show the feasibility of decentralised particle swarm optimisation. I...|$|R
50|$|Some {{financial}} institutions {{have set up}} <b>centralised</b> <b>data</b> management platforms, open to multiple sources of static and streaming data where all financial instruments traded or held can possibly be defined, documented, priced, historised and distributed across the enterprise. Such centralisation facilitates data cleansing, historising and auditing, allow organisations to define and control pricing and valuation procedures as required for compliance. For OTC instruments, the platforms also involve the definition and storage of underlying information such as yield curves and credit curves, volatility surfaces, ratings and correlation matrices and probabilities of default.|$|R
50|$|Change <b>data</b> <b>capture</b> {{can read}} the redo logs.|$|R
40|$|Ensuring {{quality of}} data during {{electronic}} <b>data</b> <b>capture</b> {{has been one of}} the most neglected components of operational research. Multicentre studies are also challenged with issues about logistics of travel, training, supervision, monitoring and troubleshooting support. Allocating resources to these issues can pose a significant bottleneck for operational research in resource-limited settings. In this article, we describe an innovative and efficient way of coordinating <b>data</b> <b>capture</b> in multicentre operational research using a combination of three open access technologies—EpiData for <b>data</b> <b>capture,</b> Dropbox for sharing files and TeamViewer for providing remote support...|$|R
40|$|There {{are many}} aspects of the athlete's {{performance}} that can and need to be measured to improve performance or to fine-tune skills. This can be done visually by a coach or by using sensors attached to the athlete. This paper discusses the common sensor <b>data</b> <b>capture</b> methods and the main requirements for a <b>data</b> <b>capturing</b> system with special attention paid to the requirements for a real time <b>data</b> <b>capturing</b> system. An implementation of a real time <b>data</b> <b>capture</b> system consisting of an iPhone and a laptop is presented. The system was tested on a runner performing a slow jog. The acceleration signatures for the running was streamed from the iPhone to the laptop and displayed in real time thus validating the system. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
5000|$|<b>Data</b> <b>Capture</b> - tcpdump, PADS, p0f, Snort (software), Wireshark ...|$|R
40|$|The {{principal}} {{aim of the}} research was to determine if a mobile <b>data</b> <b>capture</b> system could be suitably applied to the condition assessment of assets within SunWater. The study involved research into the current condition assessment process within the Irrigation & Drainage department of SunWater. Assets considered in this project involved those specific to the Barratta Section of the Burdekin Haughton Water Supply Scheme (BHWSS). This research involved the engineers and technical officers of the Asset Management, Irrigation & Drainage department within SunWater. Established during this research was the current condition assessment process, possible mobile <b>data</b> <b>capture</b> solutions to this process and a trial of a mobile <b>data</b> <b>capture</b> system within the Barratta Section of the BHWSS. The associated costs of the current process and mobile solutions were then analysed making use of the outcomes of the trial. From this analysis, conclusions were able to be drawn as to the relative merits of mobile <b>data</b> <b>capture</b> systems and their usefulness to SunWater. The physical output of the research included a trial application for the completion of asset condition assessments and a recommendation on the economic viability for implementing a mobile <b>data</b> <b>capture</b> system within SunWater. The results of this study will help to determine if SunWater can incorporate mobile <b>data</b> <b>capture</b> systems into its condition assessment processes and thereby support its productivity initiative of 'Smarter, Lighter, Faster'. ...|$|R
5000|$|ISO/IEC JTC 1/SC 31, Automatic {{identification}} and <b>data</b> <b>capture</b> techniques ...|$|R
5000|$|Method {{and system}} of {{automating}} <b>data</b> <b>capture</b> from electronic correspondence ...|$|R
5000|$|... {{techniques}} [...] like <b>data</b> <b>capture</b> tools, {{accounting records}} and measurements; ...|$|R
40|$|GIS software, introduction, the {{evolution}} of GIS software, architecture of GIS software, types of GIS software systems, lecture 2 geographic data modelling, introduction, GIS data models, example of water facility object data model, geographic data modeling in practice, lecture 3 GIS data collection, introduction, primary geographic <b>data</b> <b>capture,</b> secondary geographic <b>data</b> <b>capture,</b> obtaining <b>data</b> from external sources, <b>capturing</b> attribute <b>data,</b> managing a <b>data</b> <b>capture</b> project, lecture 4 creating and maintaining geographic, databases, introduction, database management systems, storing data in DBMS tables, geographic database types and functions, geographic database design, structuring geographic information, editing and maintenance, multi-user editing of continuous databases...|$|R
