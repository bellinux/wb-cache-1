16|0|Public
50|$|The Openbravo Technology Platform lies at {{the heart}} of all Openbravo solutions. It is a modular, mobile-enabled and <b>cloud-ready</b> {{platform}} providing the core set of technologies that allow extending Openbravo to fit companies needs or developing completely new enterprise solutions.|$|E
50|$|These days XAP is {{used for}} complex event {{processing}} and real-time business intelligence for big data. With XAP you can replicate data to a Relational database or non-relational NoSQL database, enabling data replication across remote sites over WAN, for Disaster recovery, and is also <b>cloud-ready.</b>|$|E
50|$|The Cloudscaling Group, Inc., was a {{software}} {{company based in}} San Francisco, California, USA. The company’s Open Cloud System is a cloud computing system based on the OpenStack open-source software project. It is used to deploy {{infrastructure as a service}} (IaaS) public, private and hybrid clouds that support applications typically found on public cloud infrastructures such as Amazon Web Services or Google Compute Engine. These applications are often referred to as <b>cloud-ready</b> applications, which Cloudscaling refers to as dynamic applications and are contrasted with traditional enterprise IT applications.|$|E
50|$|For {{customers}} who perceive {{that the security}} risk of cloud computing adoption is too high, IBM offers private cloud services. IDEAS International wrote in a white paper, “IBM believes that its clients are currently more comfortable with private clouds than public or hybrid clouds, and that many are ready to deploy fundamental business applications in private clouds.” For building strictly private clouds, IBM offers IBM Workload Deployer and Cloudburst as ready-to-deploy, “cloud in a box” style solutions. Cloudburst provides blade servers, middleware and virtualization for an enterprise to build its own <b>cloud-ready</b> virtual machines. Workload Deployer connects an enterprise’s existing servers to virtualization components and middleware {{in order to help}} deploy standardized virtual machines designed by IBM.For {{customers who}} prefer to perform their own integration of private clouds, IBM offers a choice of hardware and software building blocks, along with recommendations and a reference architecture, prior to deployment. Clients may choose from IBM virtualization-enabled servers, middleware and SaaS applications.|$|E
40|$|The main {{objective}} is to implement integration application for contacts and communication that will meet the <b>Cloud-ready</b> specification, therefore is ready for deployment in a cloud computing environment. The thesis summarizes the recommendations of scientific papers, case studies and documentation from cloud providers for <b>cloud-ready</b> application. It also summarizes the methodology Twelve-Factor app, which brings together a set of recommendations for cloud applications. The second {{objective is to}} define specification of application to be implemented {{on the basis of}} identified persona and its problems will be transformed to specification and requirements. It is followed by design of the architecture to meet the principles for the run in the cloud environment. The fourth partial objective is to evaluate recovered principles, procedures and actual implementation. The main contribution of this work is implemented in an application that is able to integrate contacts and communication. Another contribution of this work is to summarize and validate basic recommendations on cloud ready application and methodology Twelve-factor app...|$|E
40|$|Abstract- In cloud {{computing}} environment (cloud) typically compute, storage and software resources are offered nodemand via IaaS (Infrastructure as a Service), PaaS (Platform as a Service) or SaaS (Software as a Service) services. With proper integration of Cloud computing and virtualization with network much richer and differentiated Cloud services can be offered. A network {{is an integral}} part of a Cloud connecting various segments (multiple DCs, CSC to CSP, etc) of a Cloud. But network can be further integrated into cloud by supporting relevant Cloud features in the network itself. While there are many <b>Cloud-ready</b> features that can be supported, some of the important features are on-demand network management functions and interfaces, VM-aware networking, and layer two (L 2) and layer (L 3) scaling (such as IETF LISP and TRILL or Cisc...|$|E
40|$|Thanks to its {{flexibility}} (i. e. new computing jobs {{can be set}} up in minutes, {{without having}} to wait for hardware procurement) and elasticity (i. e. more or less resources can be allocated to instantly match the current workload), cloud computing has rapidly gained much interests from both academic and commercial users. Increasingly moving into the cloud is a clear trend in the software developments. To provide its users a fast in-memory optimised analytical database system with all the conveniences of the cloud environment, we embarked upon extending the open-source column store database MonetDB with new features to make it <b>cloud-ready.</b> In the paper, we elaborate the new distributed and replicated transaction features in MonetDB. The distributed query processing feature allows MonetDB to horizontally scale-out to multiple machines; while the transaction replication schemes increase the availability of the MonetDB database servers...|$|E
40|$|With the {{increasing}} {{role played by}} software in supporting our society, its sustainability and environmental impact have become major factors {{in the development and}} operation of software-intensive systems. Myths and beliefs hide the real truth behind Green IT: IT is energy-inefficient because software is developed to make it so-intentionally or not. But how far are we from being able to control software energy-efficiency? What makes software greener? How can we transform measuring software energy consumption in a general practice? What architectural design decisions will result in more sustainable systems? How can we ensure that new-generation software will be both <b>cloud-ready</b> and environmental-friendly? and How can we make evident the economic and social impact of developing software with 'energy in mind'? These {{are a few of the}} challenges ahead for a more sustainable digital society. This talk will discuss them, hence drawing directions for exciting challenges, promising opportunities, and ultimately inspiring research...|$|E
40|$|Increasing {{benefits}} of business process automation {{and information technology}} (IT) based governance encourage organizations to model and manage their day to day business activities using business process management systems, {{in order to achieve}} increased efficiency and productivity. Many business process languages, such as Business Process Execution Language (BPEL), use a programming oriented view in process modeling as opposed to human oriented view. Recent standardization of Business Process Model and Notation version 2. 0 (BPMN 2. 0) provides a way to support inter-operation of business processes at user level, rather than at the software engine level. Wide adoption of the BPMN 2. 0 standard is limited by the lack of runtimes natively supporting BPMN 2. 0. In this paper we discuss about Levi, a <b>cloud-ready</b> BPMN 2. 0 execution engine built using the core concurrent runtime of Apache based open source process engine ODE (Orchestration Director Engine), which executes BPMN 2. 0 processes natively...|$|E
40|$|Abstract Background Large {{comparative}} genomics studies and tools {{are becoming increasingly}} more compute-expensive {{as the number of}} available genome sequences continues to rise. The capacity and cost of local computing infrastructures are likely to become prohibitive with the increase, especially as the breadth of questions continues to rise. Alternative computing architectures, in particular cloud computing environments, may help alleviate this increasing pressure and enable fast, large-scale, and cost-effective {{comparative genomics}} strategies going forward. To test this, we redesigned a typical comparative genomics algorithm, the reciprocal smallest distance algorithm (RSD), to run within Amazon's Elastic Computing Cloud (EC 2). We then employed the RSD-cloud for ortholog calculations across a wide selection of fully sequenced genomes. Results We ran more than 300, 000 RSD-cloud processes within the EC 2. These jobs were farmed simultaneously to 100 high capacity compute nodes using the Amazon Web Service Elastic Map Reduce and included a wide mix of large and small genomes. The total computation time took just under 70 hours and cost a total of $ 6, 302 USD. Conclusions The effort to transform existing comparative genomics algorithms from local compute infrastructures is not trivial. However, the speed and flexibility of cloud computing environments provides a substantial boost with manageable cost. The procedure designed to transform the RSD algorithm into a <b>cloud-ready</b> application is readily adaptable to similar comparative genomics problems. </p...|$|E
40|$|Background: Large {{comparative}} genomics studies and tools {{are becoming increasingly}} more compute-expensive {{as the number of}} available genome sequences continues to rise. The capacity and cost of local computing infrastructures are likely to become prohibitive with the increase, especially as the breadth of questions continues to rise. Alternative computing architectures, in particular cloud computing environments, may help alleviate this increasing pressure and enable fast, large-scale, and cost-effective {{comparative genomics}} strategies going forward. To test this, we redesigned a typical comparative genomics algorithm, the reciprocal smallest distance algorithm (RSD), to run within Amazon's Elastic Computing Cloud (EC 2). We then employed the RSD-cloud for ortholog calculations across a wide selection of fully sequenced genomes. Results: We ran more than 300, 000 RSD-cloud processes within the EC 2. These jobs were farmed simultaneously to 100 high capacity compute nodes using the Amazon Web Service Elastic Map Reduce and included a wide mix of large and small genomes. The total computation time took just under 70 hours and cost a total of $ 6, 302 USD. Conclusions: The effort to transform existing comparative genomics algorithms from local compute infrastructures is not trivial. However, the speed and flexibility of cloud computing environments provides a substantial boost with manageable cost. The procedure designed to transform the RSD algorithm into a <b>cloud-ready</b> application is readil...|$|E
30|$|On {{the other}} hand, heuristics-based {{resource}} management approaches usually count on relevant analytical models to facilitate optimized resource provisions and allocations. For instance, the power models with utilization threshold algorithm [[6]] are argued to assist dynamic VM placement and migration {{so that a}} considerable amount of energy consumption and CO 2 emissions can be reduced compared with static resource allocation approaches. The cognitive trust model with dynamic levels scheduling algorithm [[20]] is proposed as a better resource scheduling technique which rests on resources trustworthiness and reliability parameters matchmaking. The resource allocation and application models with resource allocation and task scheduling algorithms [[21]] are advocated in which real-time task execution information is used to dynamically guide resource allocation actions. The workload prediction models with capacity allocation and load algorithms [[22]] is proposed to minimize overall VM allocation cost while meeting SLA requirements. The cost and time models with deferential evolution algorithms [[23]] would enable generating the optimal tasks schedules to minimize job completion cost and time. The Dual Scheduling of Cloud Services and Computing Resources models with Ranking Chaos algorithm [[24]] are designed to mitigate the inefficient service composition selection and computing resources allocation issues. Additionally, IACRS [[25]] proposes a multi-metric group <b>cloud-ready</b> heuristic algorithm that would deal with compute, network and storage metric statistics simultaneously while performing scaling decisions. InterCloud [[26]] advocates an effective resource scaling approach seen as to distribute workload appropriately across multiple independent cloud data centers without compromising service quality of service (QoS) aspects.|$|E
40|$|Good {{governance}} {{is essential}} in every society as it underlines progress. Information Communication Technologies (ICTs) support the process of good governance. In the past decade, governments {{all over the world}} have discovered the use of technologies for engaging citizens to participate in governance as well as facilitate good governance. Numerous initiatives have been undertaken by the partnership of private sector and governments, as the use of ICTs have accentuated transparency in governments and empowered citizens. The use of ICTs in governance has lead to the growth of the paradigm better known as e-governance. Lately, to extend the delivery of e-governance, the use of cloud computing has been explored by many governments around the world. Cloud based governance is regarded as a platform for sustainable inclusive growth, particularly in developing nations where the reach needs to be extend to every citizen and upfront costs need to be low. Cloud computing is a delivery mechanism for e-governance services and every nation is gearing to leverage this platform for sustainable growth. This book contains 33 papers that reflect the progress of e-governance in terms of conceptual frameworks, innovations, implementation and evaluations of existing policies and practices – as the paradigm reaches the state of cloud based delivery. We believe that this book will be beneficial to academia to further research, governments to evaluate policies and implement best practices; and industry sectors to consider innovations, for a <b>cloud-ready</b> e-governance services...|$|E
40|$|Abstract — In Today’s world cloud {{computing}} has occupied a prior {{place in the}} emerging technologies cause of its ease of access at lower costs. According Moore’s Law computer technology, through transistors and integrated circuits, along with digital electronic devices, will double every 18 months to two years. It’s a steep curve {{that began in the}} 1960 s and is expected to continue until about 2020. It is anticipated in the coming years there will be more than one trillion <b>cloud-ready</b> devices, allowing users to work more quickly, conveniently, and at lower cost [...] . this phenomenon presents us with a great risk of data theft and privacy issues. especially for those dealing with sensitive information, Questions that arise include what methods are available and how can that information remain secure to ensure client protection and confidentiality?. Among these Confidentiality privacy is the main reason that many companies and also individuals to some extent are avoiding the cloud ready devices, which also needs be addressed. For this purpose we are proposing a new model that enables convenient, on-demand network access to a shared pool of configurable computing resources (e. g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This report analyses the challenges posed by {{cloud computing}} and the standardization work being done by various standards development organizations (SDOs) to minimize privacy risks in the cloud, including the role of privacy-enhancing technologies (PETs). here the new model to provide confidentiality using fragmentation method The method supports minimal encryption to minimize the computations overhead due to encryption...|$|E
40|$|In the super-connected {{world there}} are more and more {{services}} that require user authentication and authorization. The importance of being able to use existing user credentials is growing. Being able to be always connected, from a mobile client or a web browser, is also more important than ever. Applications, especially mobile applications, should be as easy to take into use as possible. Many applications depend on a backend system for computation or data storage. When a backend system that requires user authentication is involved, users are often put through the burden of creating a new user account for the system. The number of different accounts and therefore user credentials, often usernames and passwords, becomes a burden for the user’s memory. If those credentials are written down somewhere a possible security threat is created. As the number of applications grows, so does the amount of time it requires to browse through the applications or services that might have new and interesting information to the user. Constantly opening network connections is battery consuming for the mobile devices. Push notifications can be used to help with these problems. This thesis experiments one solution for improving account creation by using OpenID Connect for initial authentication and storing user credentials to the mobile device for future use. This will enable a Single Sign-On type authentication on mobile devices. Push notification systems are available for all major mobile platforms to help the backend systems inform client software of updates. A ready push notification server was deployed to allow the backend to easily inform mobile clients on different mobile platforms of updated content. The result of this work is a prototype native iOS mobile application and a <b>cloud-ready</b> backend system that offers a REST JSON API for communication. The backend can inform the mobile application of new content through the use of push notifications. OpenID Connect originates from web world and does not yet have a standard flow for native applications. A custom flow was adapted and it was successfully implemented...|$|E

