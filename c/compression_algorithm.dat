2311|2061|Public
5|$|Innocence began {{production}} in 2006 at Alfa System {{as the next}} main entry in the Tales series. Developed parallel to the DS spin-off title Tales of the Tempest, {{the goal was to}} include all the series' main gameplay and narrative elements despite the limited medium. The character designs were done by Mutsumi Inomata, while the music was composed by series newcomer Kazuhiro Nakamura. Extensive voice acting was included using CRI Middleware's Kyuseishu Sound Streamer <b>compression</b> <b>algorithm.</b> After release, it was decided to remake Innocence for the Vita, using platform-specific gameplay functions, redone voice work and music, and new characters. Japanese singer-songwriter Kokia created the opening theme songs for both versions of Innocence. Both versions have been positively received in Japan, and Western opinions on the two versions of Innocence have been mostly positive.|$|E
25|$|The {{most common}} general-purpose, {{lossless}} <b>compression</b> <b>algorithm</b> used with TIFF is Lempel–Ziv–Welch (LZW). This compression technique, {{also used in}} GIF, was covered by patents until 2003. TIFF also supports the <b>compression</b> <b>algorithm</b> PNG uses (i.e. Compression Tag 000816 'Adobe-style') with medium usage and support by applications. TIFF also offers special-purpose lossless compression algorithms like CCITT Group IV, which can compress bilevel images (e.g., faxes or black-and-white text) better than PNG's <b>compression</b> <b>algorithm.</b>|$|E
25|$|The LZMA {{lossless}} data <b>compression</b> <b>algorithm</b> combines Markov chains with Lempel-Ziv compression {{to achieve}} very high compression ratios.|$|E
50|$|Processing power. <b>Compression</b> <b>algorithms</b> require {{different}} {{amounts of}} processing power to encode and decode. Some high <b>compression</b> <b>algorithms</b> require high processing power.|$|R
40|$|We {{show that}} the {{standard}} image <b>compression</b> <b>algorithms</b> are not suitable for compressing images in correlation pattern recognition since they aim at retaining image fidelity in terms of perceptual quality rather than preserving spectrally significant information for pattern recognition. New <b>compression</b> <b>algorithms</b> for pattern recognition are therefore developed, {{which are based on}} the modification of the standard <b>compression</b> <b>algorithms</b> to achieve higher compression ratio and simultaneously to enhance pattern recognition performance. This is done by emphasizing middle and high frequency components and discarding low frequency components according to a new developed distortion measure for compression. The operations of denoising, edge enhancement and compression can be integrated in the encoding process in the proposed <b>compression</b> <b>algorithms.</b> Simulation results show the effectiveness of the proposed <b>compression</b> <b>algorithms...</b>|$|R
40|$|Data {{compression}} is {{a common}} requirement {{for most of the}} computerized applications. There are number of data <b>compression</b> <b>algorithms,</b> which are dedicated to compress different data formats. Even for a single data type there are number of different <b>compression</b> <b>algorithms,</b> which use different approaches. This paper examines lossless data <b>compression</b> <b>algorithms</b> and compares their performance. A set of selected algorithms are examined and implemented to evaluate the performance in compressing text data. An experimental comparison of a number of different lossless data <b>compression</b> <b>algorithms</b> is presented in this paper. The article is concluded by stating which algorithm performs well for text data...|$|R
25|$|MS-DOS 6.0 and 6.20 were {{released}} in 1993, both including the Microsoft DoubleSpace disk compression utility program. Stac successfully sued Microsoft for patent infringement regarding the <b>compression</b> <b>algorithm</b> used in DoubleSpace. This {{resulted in the}} 1994 release of MS-DOS 6.21, which had disk-compression removed. Shortly afterwards came version 6.22, with {{a new version of}} the disk compression system, DriveSpace, which had a different <b>compression</b> <b>algorithm</b> to avoid the infringing code.|$|E
25|$|PNG uses DEFLATE, a non-patented {{lossless}} data <b>compression</b> <b>algorithm</b> {{that uses}} a combination of LZ77 and Huffman coding. Permissively-licensed DEFLATE implementations, such as zlib, are widely available.|$|E
25|$|Several {{important}} {{problems can}} be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image <b>compression</b> <b>algorithm</b> {{is based on the}} singular value decomposition. The corresponding tool in statistics is called principal component analysis.|$|E
50|$|Texture {{compression}} is {{a specialized}} form of image compression designed for storing texture maps in 3D computer graphics rendering systems. Unlike conventional image <b>compression</b> <b>algorithms,</b> texture <b>compression</b> <b>algorithms</b> are optimized for random access.|$|R
50|$|The {{handling}} of video data involves computation of data <b>compression</b> <b>algorithms</b> and possibly of video processing algorithms. As the template Compression methods shows, lossy video <b>compression</b> <b>algorithms</b> involve the steps: Motion estimation (ME), Discrete cosine transform (DCT), and entropy encoding (EC).|$|R
40|$|Abstract. JPEG 2000 {{algorithm}} {{has been}} developed based on the DWT techniques, which have shown how the results achieved in different areas in information technology {{can be applied to}} enhance the performance. Lossy image <b>compression</b> <b>algorithms</b> sacrifice perfect image reconstruction in favor of decreased storage requirements. Wavelets have become a popular technology for information redistribution for high-performance image <b>compression</b> <b>algorithms.</b> Lossy <b>compression</b> <b>algorithms</b> sacrifice perfect image reconstruction in favor of improved compression rates while minimizing image quality lossy...|$|R
25|$|IDAT {{contains}} the image, {{which may be}} split among multiple IDAT chunks. Such splitting increases filesize slightly, but {{makes it possible to}} generate a PNG in a streaming manner. The IDAT chunk {{contains the}} actual image data, which is the output stream of the <b>compression</b> <b>algorithm.</b>|$|E
25|$|The JPEG <b>compression</b> <b>algorithm</b> is at {{its best}} on {{photographs}} and paintings of realistic scenes with smooth variations of tone and color. For web usage, where the amount of data used for an image is important, JPEG is very popular. JPEG/Exif {{is also the most}} common format saved by digital cameras.|$|E
25|$|Shannon's theorem also {{implies that}} no {{lossless}} compression scheme can shorten all messages. If some messages come out shorter, {{at least one}} must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a <b>compression</b> <b>algorithm</b> makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a <b>compression</b> <b>algorithm</b> to already compressed data: for example, making a ZIP file of music, pictures or videos that are already in a compressed format such as FLAC, MP3, WebM, AAC, PNG or JPEG will generally result in a ZIP file that is slightly larger than the source file(s).|$|E
40|$|Normalized Compression Distance (NCD) is {{a popular}} tool that uses <b>compression</b> <b>algorithms</b> to cluster and {{classify}} data {{in a wide range}} of applications. Existing discussions of NCD's theoretical merit rely on certain theoretical properties of <b>compression</b> <b>algorithms.</b> However, we demonstrate that many popular <b>compression</b> <b>algorithms</b> don't seem to satisfy these theoretical properties. We explore the relationship between some of these properties and file size, demonstrating that this theoretical problem is actually a practical problem for classifying malware with large file sizes, and we then introduce some variants of NCD that mitigate this problem...|$|R
40|$|In {{the past}} decades, ECMA has {{published}} numerous ECMA Standards for magnetic tapes, magnetic tape cassettes and cartridges, {{as well as}} for optical disk cartridges. Those media developed recently have a very high physical recording density. In order to make optimal use of the resulting data capacity, lossless <b>compression</b> <b>algorithms</b> have been designed that allow a reduction of the number of bits required for the representation of user data. These <b>compression</b> <b>algorithms</b> are registered by ECMA, the International Registration Authority established by ISO/IEC. The registration consists in allocating to each registered algorithm a numerical identifier that will be recorded on the medium and, thus, indicate which <b>compression</b> <b>algorithm(s)</b> has been used. This ECMA Standard is the fourth ECMA Standard for <b>compression</b> <b>algorithms.</b> The three previous standards are: ECMA- 15...|$|R
5000|$|... #Subtitle level 2: Use in {{practical}} data <b>compression</b> <b>algorithms</b> ...|$|R
25|$|The entropy rate of a {{data source}} means {{the average number}} of bits per symbol needed to encode it. Shannon's {{experiments}} with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM <b>compression</b> <b>algorithm</b> can achieve a compression ratio of 1.5 bits per character in English text.|$|E
25|$|Sirius {{broadcasts}} using 12.5MHz of the S band between 2320 and 2332.5MHz. Audio {{channels are}} digitally compressed using a proprietary variant of Lucent's Perceptual audio coder <b>compression</b> <b>algorithm</b> and encrypted with a proprietary conditional access system. Sirius {{has announced that}} they intend to implement hierarchical modulation technology to economize on bandwidth up to 25%.|$|E
25|$|The {{motivation}} {{for creating the}} PNG format was in early 1995, after it became known that the Lempel–Ziv–Welch (LZW) data <b>compression</b> <b>algorithm</b> used in the Graphics Interchange Format (GIF) format was patented by Unisys. There were also other problems with the GIF format that made a replacement desirable, notably its limit of 256 colors {{at a time when}} computers able to display far more than 256 colors were becoming common.|$|E
5000|$|S3 Texture Compression, a {{group of}} lossy texture <b>compression</b> <b>algorithms</b> ...|$|R
50|$|Some of {{the most}} common {{lossless}} <b>compression</b> <b>algorithms</b> are listed below.|$|R
50|$|Lossy <b>compression</b> <b>algorithms</b> {{preserve}} {{a representation of}} the original uncompressed image that may appear to be a perfect copy, {{but it is not a}} perfect copy. Often lossy compression is able to achieve smaller file sizes than lossless compression. Most lossy <b>compression</b> <b>algorithms</b> allow for variable compression that trades image quality for file size.|$|R
25|$|A {{number of}} the university's faculty and {{students}} have also gained prominence {{in the field of}} computing sciences. Examples include QNX operating systems co-creators Gordon Bell and Dan Dodge, Rasmus Lerdorf, the creator of the PHP scripting language, Matei Zaharia, the creator of Apache Spark, Gordon Cormack, the co-creator of the Dynamic Markov <b>compression</b> <b>algorithm,</b> Ric Holt, co-creator of several programming languages, most notably Turing, and Jack Edmonds, a computer scientist, and developer of the Blossom algorithm, and the Edmonds' algorithm.|$|E
25|$|Windows XP further {{improved}} {{support for}} hibernation. Hibernation and resumption are much faster as memory pages are compressed using an improved algorithm; compression is overlapped with disk writes, unused memory pages are freed and DMA transfers are used during I/O. hiberfil.sys contains further information including processor state. This file was documented by a security researcher Matthieu Suiche during Black Hat Briefings 2008 who {{also provided a}} computer forensics framework to manage and convert this file into a readable memory dump. The <b>compression</b> <b>algorithm</b> was later documented by Microsoft as well.|$|E
25|$|The MP3 lossy {{audio data}} <b>compression</b> <b>algorithm</b> takes {{advantage}} of a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described {{a complete set of}} auditory curves regarding this phenomenon. Ernst Terhardt et al. created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths.|$|E
5000|$|Four {{different}} <b>compression</b> <b>algorithms</b> are {{set out in}} {{the original}} white paper: ...|$|R
40|$|We {{compare the}} {{performance}} of video <b>compression</b> <b>algorithms</b> in terms of compression ratio, video distortion and computation {{time in order to}} study the feasibility of a real time software videophone for 64 kbits/s channels, with flexible user defined compression methods. We can combine and adjust the <b>compression</b> <b>algorithms</b> to the required bandwidth and video quality. We introduce the double logarithmic CORT Diagram, which is a novel, visual method to combine and evaluate video <b>compression</b> <b>algorithms.</b> The implemented algorithms are subsampling, block truncation coding (BTC), adaptive discrete cosine transform (ADTC) and conditional refreshment. 1 Introduction We compare {{the performance of}} video <b>compression</b> <b>algorithms</b> in terms of compression ratio, video distortion and computation time in order to study the feasibility of a real time software videophone for 64 kbits/s channels, with flexible user defined compression methods. Our simulation bench allows us to combine interactively diffe [...] ...|$|R
50|$|Hifn {{held the}} patents for the Lempel-Ziv-Stac and Microsoft Point-to-Point <b>Compression</b> <b>compression</b> <b>algorithms.</b>|$|R
25|$|Windows Media Audio Voice (WMA Voice) is a lossy {{audio codec}} that competes with Speex (used in Microsoft's own Xbox Live online service), ACELP, and other codecs. Designed for low-bandwidth, voice {{playback}} applications, it employs low-pass and high-pass filtering of sound outside the human speech frequency range to achieve higher compression efficiency than WMA. It can automatically detect sections of an audio track containing both voice {{and music and}} use the standard WMA <b>compression</b> <b>algorithm</b> instead. WMA Voice supports up to 22.05kHz for a single channel (mono) only. Encoding is limited to constant bit rate (CBR) and up to 20kbit/s. The first and only version of the codec is WMA 9 Voice.|$|E
25|$|Windows Media Audio (WMA) is {{the most}} common codec of the four WMA codecs. Colloquial usage of the term WMA, {{especially}} in marketing materials and device specifications, usually refers to this codec only. The first version of the codec released in 1999 is regarded as WMA 1. In the same year, the bit stream syntax, or <b>compression</b> <b>algorithm,</b> was altered in minor ways and became WMA 2. Since then, newer versions of the codec have been released, but the decoding process remained the same, ensuring compatibility between codec versions. WMA is a lossy audio codec based on the study of psychoacoustics. Audio signals that are deemed to be imperceptible to the human ear are encoded with reduced resolution during the compression process.|$|E
25|$|The codec's {{bit stream}} syntax was frozen {{at the first}} version, WMA 9 Pro. Later {{versions}} of WMA Pro introduced low-bit rate encoding, low-delay audio, frequency interpolation mode, and an expanded range of sampling rate and bit-depth encoding options. A WMA 10 Pro file compressed with frequency interpolation mode comprises a WMA 9 Pro track encoded at half the original sampling rate, which is then restored using a new <b>compression</b> <b>algorithm.</b> In this situation, WMA 9 Pro players which have not been updated to the WMA 10 Pro codec can only decode the lower quality WMA 9 Pro stream. Starting with WMA 10 Pro, eight channel encoding starts at 128kbit/s, and tracks can be encoded at the native audio CD resolution (44.1kHz, 16-bit), previously the domain of WMA Standard.|$|E
50|$|Transients {{are more}} {{difficult}} to encode with many audio <b>compression</b> <b>algorithms,</b> causing pre-echo.|$|R
5000|$|... {{introduction}} of digital mobile telephony, specialized <b>compression</b> <b>algorithms</b> for high bit error rates ...|$|R
40|$|Abstract:- To {{deal with}} the large data volume of hyperspectral sensors the Canadian Space Agency (CSA) {{developed}} two near lossless <b>compression</b> <b>algorithms.</b> This paper analyzes compression errors introduced by the two <b>compression</b> <b>algorithms</b> and assesses their near lossless feature. Experimental results show that errors introduced by the two <b>compression</b> <b>algorithms</b> are smaller than the intrinsic noise of the original data caused by the instrument noise and other noise sources such as calibration and atmospheric correction errors. This level of compression errors {{is expected to have}} small to negligible impact on remote sensing applications compared to the intrinsic noise of the original data. Key-Words:- Hyperspectral imagery, data compression, near lossless, vector quantization. ...|$|R
