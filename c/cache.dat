10000|4002|Public
5|$|Cold <b>Cache,</b> , AHRS# XBD-149.|$|E
5|$|After Cersei {{fails to}} appear, Lancel Lannister {{is sent to}} {{retrieve}} her. Lancel follows one of Cersei's spies beneath the Sept, and finds a wildfire <b>cache</b> about to explode. He is stabbed in the spine before he can disarm the <b>cache.</b> Inside the Sept, Margaery Tyrell, realizing that Cersei has set a trap, warns the crowd to leave, but the High Sparrow prevents anyone from leaving. The wildfire ignites and destroys the Great Sept, killing everyone inside. King Tommen Baratheon witnesses the explosion from the Red Keep and commits suicide by jumping out a window.|$|E
5|$|New {{information}} presents {{improvements in}} multithreading, resilency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, <b>cache</b> prefetching, and data access hints).|$|E
40|$|Abstract—We {{consider}} wireless <b>caches</b> {{placed in}} the plane according to a homogeneous Poisson process. A data file is stored at the <b>caches,</b> which have limited storage capabilities. Clients can contact the <b>caches</b> to retrieve the data. The <b>caches</b> store the data {{according to one of}} the two data allocation strategies: partitioning & coding. We consider the Pareto front of the expected deployment cost of the <b>caches</b> and the expected cost of a client retrieving the data from the <b>caches.</b> We show that there is a strong trade-off between the expected retrieval and the expected deployment cost under the partitioning and the coding strategies. We also show that under coding, it is optimal to deploy a high number of <b>caches,</b> each with low storage capacity. Index Terms—Wireless communication, networks of <b>caches,</b> coding, cost optimization I...|$|R
40|$|We {{consider}} wireless <b>caches</b> {{placed in}} the plane according to a homogeneous Poisson process. A data file is stored at the <b>caches,</b> which have limited storage capabilities. Clients can contact the <b>caches</b> to retrieve the data. The <b>caches</b> store the data {{according to one of}} the two data allocation strategies: partitioning & coding. We consider the Pareto front of the expected deployment cost of the <b>caches</b> and the expected cost of a client retrieving the data from the <b>caches.</b> More precisely, we investigate their Pareto front. We show that there is a strong trade-off between the expected retrieval and the expected deployment cost under the partitioning and the coding strategies...|$|R
40|$|Abstract. Distributed {{hash tables}} are {{increasingly}} being proposed as the core substrate for content delivery applications in the Internet, such as cooperative Web <b>caches,</b> Web index and search, and content delivery systems. The performance of these applications built on DHTs fundamentally depends {{on the effectiveness of}} request routing within the DHT. In this paper, we show how to use soft state to achieve routing performance that approaches the aggressive performance of one-hop schemes, but with an order of magnitude less overhead on average. We use three kinds of hint <b>caches</b> to improve routing latency: local hint <b>caches,</b> path hint <b>caches,</b> and global hint <b>caches.</b> Local hint <b>caches</b> use large successor lists to short cut final hops. Path hint <b>caches</b> store a moderate number of effective route entries gathered while performing lookups for other nodes. And global hint <b>caches</b> store direct routes to peers distributed across the ID space. Based upon our simulation results, we find that the combination of the hint <b>caches</b> significantly improves Chord routing performance: in a network of 4, 096 peers, the hint <b>caches</b> enable Chord to route requests with average latencies only 6 % more than algorithms that use complete routing tables with significantly less overhead. ...|$|R
5|$|On 16 May 2016 a 'terrorist hide' {{was found}} by civilians in Capanagh Forest near Larne, Antrim, {{possibly}} belonging to the New IRA. It was a very substantial <b>cache.</b>|$|E
5|$|State Route 16 {{begins in}} Colusa County near Wilbur Springs at the {{junction}} with State Route 20. SR 16 goes south alongside Bear Creek, which enters a narrow canyon and joins with <b>Cache</b> Creek near the Yolo County line. SR 16 continues in the canyon, running close to the river, passing <b>Cache</b> Creek Canyon Regional Park, and emerging from the canyon north of Rumsey. This section is so prone to rock slides that there are permanent gates at each end.|$|E
5|$|Stela 3 had a <b>cache</b> of 44 {{pieces of}} {{obsidian}} buried near its base.|$|E
50|$|These {{coherency}} {{states are}} maintained through {{communication between the}} <b>caches</b> and the backing store. The <b>caches</b> have different responsibilities when blocks are read or written, or when they learn of other <b>caches</b> issuing reads or writes for a block.|$|R
40|$|This paper {{presents}} a general-purpose distributed lookup service, denoted Passive Distributed Indexing (PDI). PDI stores entries {{in form of}} (key, value) pairs in index <b>caches</b> located at mobile devices. Index <b>caches</b> are filled by epidemic dissemination of popular index entries. By exploiting node mobility, PDI can resolve most queries locally without sending messages outside the radio coverage of the inquiring node. For keeping index <b>caches</b> coherent, configurable value timeouts implementing implicit invalidation and lazy invalidation <b>caches</b> implementing explicit invalidation are introduced. Inconsistency in index <b>caches</b> due to weak connectivity or node failure is handled by value timeouts. Lazy invalidation <b>caches</b> reduce the fraction of stale index entries due to modified data at the origin node. Similar to index <b>caches,</b> invalidation <b>caches</b> are filled by epidemic distributions of invalidation messages. We evaluate the performance of PDI for a mobile P 2 P file sharing a mobile instant messaging application. Simulation results show that with the suitable integration of both invalidation mechanisms, up to 80 % of the lookup operations return correct results and more than 90 % of results delivered by PDI index <b>caches</b> are up-to-date. I...|$|R
40|$|Abstract—Peer-to-peer file-sharing {{systems are}} {{responsible}} for a significant share of the traffic between Internet service providers (ISPs) in the Internet. In order to decrease their peer-to-peer related transit traffic costs, many ISPs have deployed <b>caches</b> for peer-to-peer traffic in recent years. We consider how {{the different types of}} peer-to-peer <b>caches</b> – <b>caches</b> already available on the market and <b>caches</b> expected to become available in the future – can possibly affect the amount of inter-ISP traffic. We develop a fluid model that captures the effects of the <b>caches</b> on the system dynamics of peer-to-peer networks, and show that <b>caches</b> can have adverse effects on the system dynamics depending on the system parameters. We combine the fluid model with a simple model of inter-ISP traffic and show that the impact of <b>caches</b> cannot be accurately assessed without considering the effects of the <b>caches</b> on the system dynamics. We identify scenarios when caching actually leads to increased transit traffic. Our analytical results are supported by extensive simulations and experiments with real BitTorrent clients. I...|$|R
5|$|Structure 17 {{is located}} in the South Group, on the Santa Margarita plantation. It {{contained}} a Late Preclassic <b>cache</b> of 13 prismatic obsidian blades.|$|E
5|$|All survey grids {{having been}} {{completed}} {{by the end of}} January, the survey crew flew back to Rothera leaving the two Americans and two British to dismantle the camp. A week later the remaining team members flew back to McMurdo leaving a fuel <b>cache</b> for future expeditions.|$|E
5|$|Caching is {{inhibited}} by {{the presence}} of Steller's jays (Cyanocitta stelleri) and grey jays from adjacent territories, which follow resident grey jays to steal cached food. Grey jays carry large food items to distant <b>cache</b> sites for storage more often than small food items. To prevent theft, they also tend to carry valuable food items further from the source when caching in the company of one or more grey jays. Scatterhoarding discourages pilferage by competitors, while increased <b>cache</b> density leads to increased thievery. In southern portions of the grey jay's range, food is not cached during summer because of the chance of spoilage and the reduced need for winter stores.|$|E
25|$|Webcam <b>caches</b> are virtual <b>caches</b> whose {{coordinates}} have {{a public}} webcam. The finder is often required to capture their image from the webcam for verification of the find. New webcam <b>caches</b> are no longer allowed by Groundspeak, but they remain supported by other sites.|$|R
3000|$|... [*]pixels. The inputs of this {{software}} application are {{images in the}} BMP format, and the outputs are images in the JPEG format. The JPEG application was simulated on different architectures. The hardware components used in these architectures are MIPSR 3000 processors, 16 [*]KB SRAM memories, and micronetworks. The used <b>caches</b> contain actually two independent instruction and data <b>caches,</b> sharing the same interface. They are direct mapped <b>caches.</b> The data <b>cache's</b> writing policy is write-through.|$|R
40|$|Shared Web <b>caches,</b> also {{referred}} to as proxy Web servers, allow multiple clients to quickly access a pool of popular Web pages. An organization which provides shared caching to its Web clients will typically have a collection of shared <b>caches</b> rather than just one. For collections of shared <b>caches,</b> it is desirable to coordinate the <b>caches</b> so that all cached pages in the collection are shared among the organization's clients. In this paper we investigate two classes of protocols for coordinating a collection of shared caches: the ICP protocol, which has <b>caches</b> ping each other to locate a cached object; and the hash-routing protocols, which place objects in the shared <b>caches</b> {{as a function of the}} objects' URLs. Our contribution is twofold. First, we compare the performance of the protocols with respect to cache-server overhead and to objectretrieval latency; for a collection of shared <b>caches,</b> our analysis shows that the hash routing schemes have significant performance advantages over I [...] ...|$|R
5|$|In June, American forces {{conducted}} Operation Desert Scorpion, {{a mostly}} unsuccessful attempt {{to root out}} the burgeoning insurgency. An isolated success occurred near Rawah, where American soldiers cornered and killed more than 70 fighters on 12 June and captured a large weapons <b>cache.</b>|$|E
5|$|In {{the postwar}} period, Lawton {{underwent}} tremendous growth {{during the late}} 1940s and 1950s, leading city officials to seek additional water sources to supplement existing water from Lake Lawtonka. In the late 1950s, the city purchased large parcels of land along East <b>Cache</b> Creek in northern Comanche County {{for the construction of}} a man-made lake with a dam built in 1959 on the creek just north of U.S. 277 west of Elgin. Lake Ellsworth, named for a former Lawton mayor, soft-drink bottler C.R. Ellsworth, was dedicated in the early 1960s. It offered additional water resources, but also recreational opportunities and flood control along <b>Cache</b> Creek.|$|E
5|$|Seibal {{was first}} settled around 900 BC in the Preclassic Period. It {{reached its peak}} {{population}} in the Late Preclassic around 200 BC, {{at the end of}} the Cantuse ceramic phase (300-200 CE). Ceramics from the deep Middle Preclassic levels at Seibal belong to the little-known Real/Xe phase, found in the western Petén region. A cruciform Olmec-style <b>cache</b> consisting of a bloodletter and jade celts is similar to those found in the Olmec heartland on the Gulf Coast of Mexico, and the artifacts were probably manufactured at La Venta. This <b>cache</b> dates to approximately 900 BC.|$|E
25|$|Like {{many members}} of the family Sciuridae, the eastern gray {{squirrel}} is a scatter-hoarder; it hoards food in numerous small <b>caches</b> for later recovery. Some <b>caches</b> are quite temporary, especially those made near the site of a sudden abundance of food which can be retrieved within hours or days for reburial in a more secure site. Others are more permanent and are not retrieved until months later. Each squirrel is estimated to make several thousand <b>caches</b> each season. The squirrels have very accurate spatial memory for the locations of these <b>caches,</b> and use distant and nearby landmarks to retrieve them. Smell is used partly to uncover food <b>caches,</b> and also to find food in other squirrels' <b>caches.</b> Scent can be unreliable when the ground is too dry or covered in snow.|$|R
5000|$|... e500mc cores {{have private}} L2 <b>caches</b> but {{typically}} share other facilities like L3 <b>caches,</b> memory controllers, application specific acceleration cores, I/O and such.|$|R
5000|$|The {{benefits}} of L3 and L4 <b>caches</b> {{depend on the}} application's access patterns. Examples of products incorporating L3 and L4 <b>caches</b> include the following: ...|$|R
5|$|The NCVP is {{underlain}} by {{four large}} crustal fragments, namely Stikinia, Yukon-Tanana, Cassiar and <b>Cache</b> Creek. Stikinia comprises volcanic, plutonic and sedimentary rocks that {{were created in}} an island arc environment during the Paleozoic and Mesozoic eras. Mélange and abyssal peridotites, formed largely in an ancient oceanic basin, are the primary rocks of the <b>Cache</b> Creek Terrane. These are also Paleozoic to Mesozoic in age and are intersected by younger granitic intrusions. Yukon-Tanana and Cassiar consist of sedimentary and metamorphic rocks derived from the North American continent. The rocks of these two terranes are displaced and autochthonous in nature.|$|E
5|$|The other Amarna period tombs {{are located}} in a smaller, central area in {{the centre of the}} East Valley, with a {{possible}} mummy <b>cache</b> (KV55) that may contain the burials of several Amarna Period royals—Tiy and Smenkhkare or Akhenaten.|$|E
5|$|There are {{traces of}} early {{agriculture}} {{at the site}} dating {{as far back as}} 1000 BC, in the Middle Preclassic. A <b>cache</b> of Mamon ceramics dating from about 700-400 BC were found in a sealed chultun, a subterranean bottle-shaped chamber.|$|E
40|$|This paper {{studies the}} problem of where to place network <b>caches.</b> Emphasis is given to <b>caches</b> that are {{transparent}} to the clients since they are easier to manage and they require no cooperation from the clients. Our goal is to minimize the overall flow or the average delay by placing a given number of <b>caches</b> in the network...|$|R
50|$|The Home Guard hid <b>caches</b> {{of these}} grenades {{during the war}} {{for use in the}} event of an invasion. Not all {{locations}} were officially recorded and some <b>caches</b> were lost. Occasionally, the <b>caches</b> are discovered by builders digging foundations. In all cases, the grenades are still found to be dangerous and typically are destroyed via a controlled explosion.|$|R
50|$|Write-through {{operation}} is common when operating over unreliable networks (like an Ethernet LAN), {{because of the}} enormous complexity of the coherency protocol required between multiple write-back <b>caches</b> when communication is unreliable. For instance, web page <b>caches</b> and client-side network file system <b>caches</b> (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.|$|R
25|$|The {{relationship}} between a CPU's <b>cache</b> size {{and the number of}} <b>cache</b> misses follows the Power law of <b>cache</b> misses.|$|E
25|$|One {{limitation}} (also afflicting the Intel Pentium III) is that SRAM <b>cache</b> designs at {{the time}} were incapable of keeping up with the Athlon's clock scalability, due both to manufacturing limitations of the <b>cache</b> chips and the difficulty of routing electrical connections to the <b>cache</b> chips themselves. It became increasingly difficult to reliably run an external processor <b>cache</b> to match the processor speeds being released—and in fact it became impossible. Thus initially the Level 2 <b>cache</b> ran at half of the CPU clock speed up to 700MHz (350MHz <b>cache).</b> Faster Slot-A processors had to compromise further and run at 2/5 (up to 850MHz, 340MHz <b>cache)</b> or 1/3 (up to 1GHz, 333MHz <b>cache).</b> This later race to 1GHz (1000MHz) by AMD and Intel further exacerbated this bottleneck as ever higher speed processors demonstrated decreasing gains in overall performance—stagnant SRAM <b>cache</b> memory speeds choked further improvements in overall speed. This directly lead to the development of integrating L2 <b>cache</b> onto the processor itself and remove the dependence on external <b>cache</b> chips. AMD's integration of the <b>cache</b> onto the Athlon processor itself would later result in the Athlon Thunderbird.|$|E
25|$|The Athlon's CPU <b>cache</b> {{consisted}} of the typical two levels. Athlon was the first x86 processor with a 128kB split level 1 cache; a 2-way associative <b>cache</b> separated into 2×64kB for data and instructions (a concept from Harvard architecture). This <b>cache</b> was double the size of K6's already large 2×32kB <b>cache,</b> and quadruple the size of Pentium II and III's 2×16kB L1 <b>cache.</b> The initial Athlon (Slot A, later called Athlon Classic) used 512kB of level 2 <b>cache</b> separate from the CPU, on the processor cartridge board, running at 50% to 33% of core speed. This was done because the 250nm manufacturing process was too large to allow for on-die <b>cache</b> while maintaining cost-effective die size. Later Athlon CPUs, afforded greater transistor budgets by smaller 180nm and 130nm process nodes, moved to on-die L2 <b>cache</b> at full CPU clock speed.|$|E
40|$|Web <b>caches</b> are {{installed}} {{to reduce the}} receivers' latency and save network bandwidth. These <b>caches</b> need to cooperate {{in order to improve}} their performance. A popular technique to make <b>caches</b> cooperate is by setting up a caching hierarchy. However, there are several problems associated with a caching hierarchy: i) every hierarchy level introduces additional delay, ii) redundant document copies are stored at every hierarchy level, and iii) higher level <b>caches</b> tend to become bottlenecks. Recently, a number of researchers have proposed the setup of a totally distributed caching scheme where only <b>caches</b> at the bottom level of the network cooperate and there are no intermediate <b>caches.</b> In this paper we analyze the performance of both hierarchical and distributed caching. Our main performance measure is the expected latency to retrieve a Web document. We find that hierarchical caching gives shorter connection times than distributed caching. We also find that distributed caching gives shorter [...] ...|$|R
30|$|A {{number of}} aspects {{mentioned}} in this subsection {{can be used to}} justify the studies of exploration mechanisms for hierarchy with two-level <b>caches</b> (with separated instruction and data <b>caches</b> for both levels).|$|R
30|$|The most typical {{implementation}} of memory hierarchies {{makes use of}} <b>caches.</b> While extremely versatile and fast, <b>caches</b> are not always the best choice in embedded systems. As on-chip storage, the scratch-pad memories (SPMs)—compiler-controlled synchronous random-access memories (SRAMs), more energy-efficient than the hardware-managed caches—are widely used in embedded systems, where <b>caches</b> incur a significant penalty in aspects like area cost, energy consumption, hit latency, and real-time predictability [3].|$|R
