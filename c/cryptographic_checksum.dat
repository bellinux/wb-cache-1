15|21|Public
5000|$|... {{checksum}} length: {{length in}} bytes of the <b>cryptographic</b> <b>checksum</b> {{of the message}} ...|$|E
50|$|File {{integrity}} monitoring (FIM) is {{an internal}} control or process that performs {{the act of}} validating the integrity of operating system and application software files using a verification method between the current file state and a known, good baseline. This comparison method often involves calculating a known <b>cryptographic</b> <b>checksum</b> of the file's original baseline and comparing with the calculated checksum of {{the current state of}} the file. Other file attributes can also be used to monitor integrity.|$|E
50|$|The {{basic idea}} of MAPSEC can be {{described}} as follows. The plaintext MAP message is encrypted and the result is put into a ‘container’ in another MAP message. At the same time a <b>cryptographic</b> <b>checksum,</b> i.e. a message authentication code covering the original message, is included in the new MAP message. To be able to use encryption and message authentication codes, keys are needed. MAPSEC has borrowed the notion of a security association (SA) from IPsec.|$|E
5000|$|... uses <b>cryptographic</b> <b>checksums</b> {{of files}} to detect modifications, ...|$|R
50|$|DKIM {{requires}} <b>cryptographic</b> <b>checksums</b> to {{be generated}} for each message sent through a mail server, {{which results in}} computational overhead not otherwise required for e-mail delivery. This additional computational overhead is a hallmark of digital postmarks, making sending bulk spam more (computationally) expensive.This facet of DKIM may look similar to hashcash, except that the receiver side verification is not a negligible amount of work, and a typical hashcash algorithm would require far more work.|$|R
30|$|As mentioned, the LP-SIP {{protocol}} uses {{asymmetric encryption}} and electronic signature {{to protect this}} partially confidential information from unintended readers and symmetric encryption based on AES and <b>cryptographic</b> <b>checksums</b> based on HMAC to exchange e-coins. Thus, the information exchanged in the payment system during the client registration phase and the withdrawal e-coin phase can be seen only by the client and broker. In a similar way, in the processes of vendor registration, check validity and deposit, the information is only seen between vendor and broker.|$|R
40|$|Abstract. A {{critical}} analysis of the modified <b>cryptographic</b> <b>checksum</b> algorithm of Cohen and Huang points out some weaknesses in the scheme. We show how to exploit these weaknesses with a chosen text attack to derive the first bits of the key. This information suffices to manipulate blocks with a negligible chance of detection. 1...|$|E
3000|$|... (Master Signature Key). This is a {{symmetric}} {{master key}} {{generated by the}} vendor {{that will be used}} by the broker to provide the clients with symmetric signature keys to guarantee the integrity of the communication with the vendor. The default <b>cryptographic</b> <b>checksum</b> function to employ is hash-based message authentication code (HMAC) [34] with SHA 2 [35].|$|E
40|$|A new message {{authentication}} code (MAC) is described that exploits the tree structure present in many modern document formats, e. g. SGML and XML. The new code supports incremental updating of the <b>cryptographic</b> <b>checksum</b> {{in the process of}} making incremental changes to the document. Theoretical bounds on the probability of a successful substitution attack are derived. Through experimental results we demonstrate that for randomly chosen messages the success probability of such an attack will be smaller and is easily identified...|$|E
40|$|We {{address the}} problem of {{pollution}} attacks in coding based distributed storage systems proposed for wireless sensor networks. In a pollution attack, the adversary maliciously alters some of the stored encoded packets, which results in the incorrect decoding of {{a large part of the}} original data upon retrieval. We propose algorithms to detect and recover from such attacks. In contrast to existing approaches to solve this problem, our approach is not based on adding <b>cryptographic</b> <b>checksums</b> or signatures to the encoded packets. We believe that our proposed algorithms are suitable in practical systems. ...|$|R
40|$|We propose an e#cient and {{flexible}} {{system for a}} secure and authentic data exchange in a multiinstitutional environment, where the institutions maintain di#erent databases and provide secure and limited access services to employees of other institutions. The main motivation for building such a system was to organize e#cient cooperative use of state registers, {{in order to increase}} the e#ciency and quality of public services in Estonia. In order to meet high security requirements, several contemporary measures are integrated (using digital signatures, distributing certificate information by means of DNS protocol and linking log files with <b>cryptographic</b> <b>checksums).</b> We give rationale for the design decisions made in the implementation process and conclude with the current state of public use of the resulting infrastructure...|$|R
40|$|Abstract—We {{address the}} problem of {{pollution}} attacks in coding based distributed storage systems. In a pollution attack, the adversary maliciously alters some of the stored encoded packets, which results in the incorrect decoding of {{a large part of the}} original data upon retrieval. We propose algorithms to detect and recover from such attacks. In contrast to existing approaches to solve this problem, our approach is not based on adding <b>cryptographic</b> <b>checksums</b> or signatures to the encoded packets, and it does not introduce any additional redundancy to the system. The results of our analysis show that our proposed algorithms are suitable for practical systems, especially in wireless sensor networks. Index Terms—Network level security and protection, sensor networks, distributed data storage, network coding, pollution attack, integrity protection I...|$|R
40|$|Since {{fingerprint}} data are no secrets but of public nature, the verification data transmitted to a smartcard for oncard-matching need protection by appropriate means {{in order to}} assure data origin in the biometric sensor and to prevent bypassing the sensor. For this purpose, the verification data {{to be transferred to}} the user smartcard is protected with a <b>cryptographic</b> <b>checksum</b> that is calculated within a sepa-rate security module controlled by a tamper resistant card terminal with integrated biometric sensor. Keywords authentication, biometrics, cryptographic protocols, data in...|$|E
40|$|One of Authentication {{technique}} {{involves the}} use of a secret key to generate a small fixed size block of data known as <b>cryptographic</b> <b>checksum</b> or MAC that is appended to the message. The unauthorized thefts in our society have made the requirement for reliable information security mechanisms. Information security can be accomplished {{with the help of a}} prevailing tool like cryptography, protecting the cryptographic keys is one of the significant issues to be deal with. Here we proposed a biometric-crypto system which generates a cryptographic key from the Finger prints for calculating the MAC value of the information we considered fingerprint because it is unique and permanent through out a person’s life...|$|E
40|$|The {{benefits}} of distributed computation present complex security considerations beyond {{those associated with}} the traditional computing paradigm. This paper describes a bandwidth efficient approach to authenticate distributed Java code. Our system utilizes steganographic techniques to embed a <b>cryptographic</b> <b>checksum</b> as a tamper detection mark into Java class files. The properties of this mark make our system desirable in applications where low bandwidth utilization is a requirement (e. g., wireless networks and low power devices). We have implemented our system in Java and evaluated its performance through empirical study. Analysis indicates that our system detects any degree of alteration to a marked Java class file and can do so within {{a reasonable amount of}} time. ...|$|E
40|$|In this paper, {{we present}} a scheme for {{generating}} <b>cryptographic</b> <b>checksums</b> to perform message authentication in a one-way broadcasting system. The proposed scheme {{is based on the}} use of clockcontrolled LFSRs and is aimed at high-speed implementation for real-time applications. INTRODUCTION In a typical secure broadcasting system, source information (video, voice, images, text, [...] ., etc.) are transmitted in a scrambled structure from the broadcast center and all the receivers get the same broadcasting signal consisting of scrambled information and access control information [1]. A set of randomly generated decryption keys, master keys, are stored in a physically secure and tamperproof module inside the legitimate receivers while the center keep a copy of corresponding encryption keys at a database against receiver's unique ID, etc. Successful descrambling occurs in authorized receivers when correctly encrypted descrambling keys were delivered from the broadcast center. Although the cryp [...] ...|$|R
40|$|Database {{authentication}} via <b>cryptographic</b> <b>checksums</b> {{represents an}} important approach to achieving an affordable safeguard of the integriry of data in publicly accessible database systems against illegal manipulations. This paper revisits {{the issue of}} database integrity and offers a new method of safeguarding the authenticity of data in database systems. The method {{is based on the}} recent development of pseudo-random function families and sibling intractable function families, rather than on the traditional use of cryptosystems. The database authentication scheme can be applied to records or fields. The advantage of the scheme {{lies in the fact that}} each record can be associated with one checksum, while each data element in the record can be verified using the checksum independently of the other data elements in the record. The security of the scheme depends on the difficulty of predicting the outputs of pseudo-random functions and on inverting the sibling intractable function family. The same approach can also be applied to the generation of encipherment keys for databases...|$|R
50|$|Note that {{continuity}} {{is usually}} considered a fatal flaw for <b>checksums,</b> <b>cryptographic</b> hash functions, {{and other related}} concepts. Continuity is desirable for hash functions only in some applications, such as hash tables used in Nearest neighbor search.|$|R
40|$|Abstract — Mobile code {{provides}} a highly flexible and beneficial form of computing. However, mobile code use creates complex security considerations beyond {{those associated with}} the traditional mode of computing. This paper describes a bandwidth efficient approach to the authentication of mobile Java codes, making our tool desirable in applications where low bandwidth utilization is a requirement (e. g., wireless networks, low power devices, and distributed computation). Our tool embeds a <b>cryptographic</b> <b>checksum</b> as a tamper detection mark into Java codes so that the mark can be extracted to authenticate the source and assure the integrity of that code. We have implemented our tool in Java and evaluated its performance through empirical study. Analysis indicates that our system detects, with high probability, any degree of tampering within {{a reasonable amount of}} time, while avoiding increased bandwidth requirements. I...|$|E
40|$|Introduction Two parties {{communicating}} {{across an}} insecure channel need a {{method by which}} any attempt to modify the information sent by one to the other, or fake its origin, is detected. Most commonly such a mechanism {{is based on a}} shared key between the parties, and in this setting is usually called a MAC, or Message Authentication Code. (Other terms include Integrity Check Value or <b>Cryptographic</b> <b>Checksum).</b> The sender appends to the data D an authentication tag computed {{as a function of the}} data and the shared key. At reception, the receiver recomputes the authentication tag on the received message using the shared key, and accepts the data as valid only if this value matches the tag attached to the received message. The most common approach is to construct MACs from block ciphers like DES. Of such constructions Department of Computer Science & Engineering, Mail Code 0114, University of California at San Diego, 9500 Gilman Dri...|$|E
40|$|Wireless Sensor Networks (WSNs) are resource-constrained {{networks}} in which sensor nodes operate in an aggressive and uncontrolled environment {{and interact with}} sensitive data. Traffic aggregated by sensor nodes is susceptible to attacks and, {{due to the nature}} of WSNs, security mechanisms used in wired networks and other types of wireless networks are not suitable for WSNs. In this paper, we propose a mechanism to assure information security against security attacks and particularly node capturing attacks. We propose a cluster security management protocol, called <b>Cryptographic</b> <b>Checksum</b> Clustering Security Management (C 3 SM), to provide an efficient decentralized security management for hierarchal networks. In C 3 SM, every cluster selects dynamically and alternately a node as a cluster security manager (CSM) which distributes a periodic shared secrete key for all nodes in the cluster. The cluster head, then, authenticates identity of the nodes and derive a unique pairwise key for each node in the cluster. C 3 SM provides sufficient security regardless how many nodes are compromised, and achieves high connectivity with low memory cost and low energy consumption. Compared to existing protocols, our protocol provides stronger resilience against node capture with lower key storage overhead...|$|E
40|$|Today, {{improving}} {{the security of}} computer systems has become an important and difficult problem. Attackers can seriously damage the integrity of systems. Attack detection is complex and time-consuming for system administrators, and it is becoming more so. Current integrity checkers and IDSs operate as user-mode utilities and they primarily perform scheduled checks. Such systems are less effective in detecting attacks that happen between scheduled checks. These user tools can be easily compromised if an attacker breaks into the system with administrator privileges. Moreover, these tools result in significant performance degradation during the checks. Our system, called I 3 FS, is an on-access integrity checking file system that compares the checksums of files in real-time. It uses <b>cryptographic</b> <b>checksums</b> to detect unauthorized modifications to files and performs necessary actions as configured. I 3 FS is a stackable file system which can be mounted over any underlying file system (like Ext 3 or NFS). I 3 FS’s design improves over the open-source Tripwire system by enhancing the functionality, performance, scalability, {{and ease of use}} for administrators. We built a prototype of I 3 FS in Linux. Our performance evaluation shows an overhead of just 4 % for normal user workloads. ...|$|R
40|$|The {{original}} publication {{is available}} at www. springerlink. com???. Copyright Springer. [Full text {{of this article is}} not available in the UHRA]Because image manipulation by computer is so easy, it is hard to be sure that an image was indeed created at the date, time and place claimed, and that it has not been altered since. To gain this confidence requires a strong chain of evidence linking the collection of bits representing the image back to the camera that made the image. <b>Cryptographic</b> <b>checksums</b> used to implement parts of this chain must demonstrate that no other image was substituted for the correct one, even though a very long period of time might have been available for a malefactor to exhaustively search for an alternative that matched the checksum. It must be possible to secure the provenance of the image without taking the camera out of circulation, and subsequent tampering with the camera should not cast doubt on the provenance. Signing the image twice, but only publishing one of the public keys, allows the image to be verified whether the camera is available or not, and also undermines any claim that the signature could have been fabricated by trial and error. Watermarking, though largely irrelevant to this discussion, does provide a convenient way of ensuring that the image and the associated data can be handled in a way compatible with existing image processing software...|$|R
40|$|Abstract Today, {{improving}} {{the security of}} computer systemshas become an important and difficult problem. Attackers can seriously damage the integrity of systems. At-tack detection is complex and time-consuming for system administrators, and it is becoming more so. Currentintegrity checkers and IDSs operate as user-mode utilities and they primarily perform scheduled checks. Suchsystems are less effective in detecting attacks that happen between scheduled checks. These user tools can beeasily compromised if an attacker breaks into the system with administrator privileges. Moreover, these toolsresult in significant performance degradation during the checks. Our system, called I 3 FS, is an on-access integrity checking file system that compares the checksums offiles in real-time. It uses <b>cryptographic</b> <b>checksums</b> to detect unauthorized modifications to files and performsnecessary actions as configured. I 3 FS is a stackable file system which can be mounted over any underlying filesystem (like Ext 3 or NFS). I 3 FS's design improves over the open-source Tripwire system by enhancing the func-tionality, performance, scalability, {{and ease of use}} for administrators. We built a prototype of I 3 FS in Linux. Our performance evaluation shows an overhead of just 4 % for normal user workloads. 1 Introduction In the last few years, security advisory boards have ob-served {{an increase in the number}} of intrusion attacks on computer systems [2]. Broadly, these intrusions can becategorized as network-based or host-based intrusions. Defense against network-based attacks involves increas-ing the perimeter security of the system to monitor the network environment, and setting up firewall rules toprevent unauthorized access. Host-based defenses are deployed within each system, to detect attack signaturesor unauthorized access to resources. We developed a host-based system which performs integrity checking atthe file system level. It detects unauthorized access, malicious file system activity, or system inconsistencies,and then triggers damage control in a timely manner...|$|R
40|$|Remote Attestation (RA) {{allows a}} trusted entity (verifier) to {{securely}} measure internal state of a remote untrusted hardware platform (prover). RA {{can be used}} to establish a static or dynamic root of trust in embedded and cyber-physical systems. It can also be used as a building block for other security services and primitives, such as software updates and patches, verifiable deletion and memory resetting. There are three major classes of RA designs: hardware-based, software-based, and hybrid, each with its own set of benefits and drawbacks. This paper presents the first hybrid RA design, called HYDRA, that builds upon formally verified software components that ensure memory isolation and protection, as well as enforce access control to memory and other resources. HYDRA obtains these properties by using the formally verified seL 4 microkernel. (Until now, this was only attainable with purely hardware-based designs.) Using seL 4 requires fewer hardware modifications to the underlying microprocessor. Building upon a formally verified software component increases confidence in security of the overall design of HYDRA and its implementation. We instantiate HYDRA on two commodity hardware platforms and assess the performance and overhead of performing RA on such platforms via experimentation; we show that HYDRA can attest 10 MB of memory in less than 500 msec when using a Speck-based message authentication code (MAC) to compute a <b>cryptographic</b> <b>checksum</b> over the memory to be attested...|$|E
40|$|In this chapter, {{we discuss}} four related areas of cryptology, namely, {{authentication}}, hashing, message authentication codes (MACs), and digital signatures. These topics represent active and growing research topics in cryptology. Space limitations {{allow us to}} concentrate only on the essential aspects of each topic. The bibliography is intended to supplement our survey. We have selected those items which providean overview of {{the current state of}} knowledge in the above areas. Authentication deals with the problem of providing assurance to a receiver that a communicated message originates from a particular transmitter, and that the received message has the same content as the transmitted message. A typical authentication scenario occurs in computer networks, where the identity of two communicating entities is established by means of authentication. Hashing is concerned with the problem of providing a relatively short digest–fingerprint of a much longer message or electronic document. A hashing function must satisfy (at least) the critical requirement that the fingerprints of two distinct messages are distinct. Hashing functions have numerous applications in cryptology. They are often used as primitives to construct other cryptographic functions. MACs are symmetric key primitives that provide message integrity against active spoofing by appending a <b>cryptographic</b> <b>checksum</b> to a message that is verifiable only by the intended recipient of the message. Message authentication {{is one of the most}} important ways of ensuring the integrity of information that is transferred by electronic means. Digital signatures provide electronic equivalents of handwritten signatures. They preserve the essential features of handwritten signatures and can be used to sign electronic documents. Digital signatures can potentially be used in legal contexts...|$|E
40|$|This thesis {{presents}} {{methods to}} statically modify programs at compile-time {{to improve the}} effectiveness of power consumption based program analyses. Two related applications are considered, and algorithms are introduced for both. The first is power consumption based program tracing, {{and the second is}} software attestation with power consumption as a side-effect. We propose a framework for increasing the effectiveness of power-based program tracing techniques. These systems determine the most likely block of source code that produced an observed power trace. Our framework maximizes distinguishability between power traces for different code sections. To this end, we introduce a static transformation to reduce the probability of misclassification by reordering intermediate representation (IR) to find the ordering that produces power traces with the highest distances between them. Experimental results confirm the effectiveness of our technique. We also consider improvements to the algorithm, replacing the naïve, exhaustive permutation algorithm used in the original solution with Monte Carlo permutations. Due to the complexity of the naïve solution, its search space is constrained, making it unlikely to find a good solution when the number of instructions in a program section is too large. Variations on a basic stochastic implementation are described, and their expected results are compared. The Monte Carlo algorithms consistently found better solutions than their exhaustive counterpart while showing improved scalability. We then introduce a related technique to statically transform programs to use power consumption as the side-effect for software attestation. We show how to circumvent the undecidable nature of program execution for this purpose and present a static compiler transformation which implements the algorithm. Our approach is less intrusive than traditional software attestation because the system does not require interruption to compute a <b>cryptographic</b> <b>checksum.</b> It is particularly well suited to real-time systems where consistent timing is more important than speed...|$|E
40|$|The {{original}} publication {{is available}} at www. springerlink. com???. Copyright Springer. [Full text {{of this article is}} not available in the UHRA]In this paper we introduce a distributed object-based document architecture called DODA in order to illustrate a novel strategy for achieving both high availability and high integrity in the context of open processing distributed between mutually suspicious domains without a common management hierarchy. Our approach to availability is to structure documents into small components called folios {{in such a way as}} to allow the maximum opportunity for concurrent processing, and to allow these components to be freely replicated and distributed. Integrity conflicts are resolved using an optimistic form of control called optimistic integrity control (OIC) applied to recoverable work units. Our approach to security is to shrinkwrap the document components using <b>cryptographic</b> <b>checksums,</b> and to provide a set of building block components called functionaries which a group of users can combine in such a way as to provide each user with a means of ensuring that an agreed notion of integrity is enforced while relying upon a minimum of non-local trust. In particular, we do not rely upon a trusted computing base or a shared system infrastructure. The local availability of document versions and of the resources to process them are completely under local user control. The lack of availability of the functionaries does not have severe consequences, and the presence of mutual suspicion makes it easier to ensure that users can trust the functionaries to provide the intended service. A major benefit of using OIC is that it allows the integration of untrusted components such as filestores and directory servers into the system. In particular, an untrusted soft locking service can be used in order to reduce the number of concurrency conflicts, and untrusted security components can be used to screen out attempted access control violations...|$|R
50|$|Extended file {{attributes}} are file system features that enable users to associate computer files with metadata not {{interpreted by the}} filesystem, whereas regular attributes have a purpose strictly defined by the filesystem (such as permissions or records of creation and modification times). Unlike forks, which can usually be {{as large as the}} maximum file size, extended attributes are usually limited in size to a value significantly smaller than the maximum file size. Typical uses include storing the author of a document, the character encoding of a plain-text document, or a <b>checksum,</b> <b>cryptographic</b> hash or digital certificate, and discretionary access control information.|$|R
50|$|Linux {{distributions}} {{rely heavily}} on package management systems for distributing software. In this scheme, a package is an archive file containing a manifest file. The primary purpose is to enumerate the files which {{are included in the}} distribution, either for processing by various packaging tools or for human consumption. Manifests may contain additional information; for example, in JAR (a package format for delivering software written in Java programming language), they can specify a version number and an entry point for execution. The manifest may optionally contain a <b>cryptographic</b> hash or <b>checksum</b> of each file. By creating a cryptographic signature for such a manifest file, the entire contents of the distribution package can be validated for authenticity and integrity, as altering any of the files will invalidate the checksums in the manifest file.|$|R
40|$|M. Sc. (Computer Science) The {{aim of this}} {{dissertation}} (referred to as thesis in {{the rest}} of the document) is to present authentication techniques that can be used to provide secure Internet commerce. The thesis presents techniques that can be used to authenticate human users at logon, as well as techniques that are used to authenticate user's PC and the host system during communication. In so doing, the thesis presents cryptography as the most popular approach to provide information security. Chapter 1 introduces the authentication problem, the purpose and the structure of the thesis. The inadequate security of the Internet prevents companies and users to conduct commerce over the Internet. Authentication is one of the means of providing secure Internet commerce. - Chapter 2 provides an overview of the Internet by presenting the Internet history, Internet infrastructure and the current services that are available on the Internet. The chapter defines Internet commerce and presents some of the barriers to the Internet commerce. Chapter 3 provides an overview of network and internetwork security model. The purpose of this chapter is to put authentication into perspective, in relation to the overall security model. Security attacks, security services and security mechanisms are defined in this chapter. The IBM Security Architecture is also presented. Chapter 4 presents cryptography as the popular approach to information security. The conventional encryption and public-key encryption techniques are used to provide some of the security services described in chapter 3. Chapter 5 presents various schemes that can be used to provide computer-to-computer authentication. These schemes are grouped into the following authentication functions: message encryption, <b>cryptographic</b> <b>checksums,</b> hash functions and digital signatures. Chapter 6 differentiates between one-way authentication schemes and mutual authentication schemes. The applicability of each approach depends on the communicating parties. Chapter 7 presents some of the popular and widely used open-systems technologies Internet protocols, which employ some of the schemes discussed in chapter 5 and chapter 6. These include the SSL, PCT, SHTTP, Kerberos, SESAME and SET. Chapter 8 discusses some of the enabling technologies that are used to provide human user authentication in a computer system. The password technology, the biometric technologies and the smart card technology are discussed. The considerations of selecting a specific technology are also discussed. Chapter 9 presents some of the techniques that can be used to authentication Internet users (human users) over the Internet. The techniques discussed are passwords, knowledge-based technique, voice recognition, smart cards, cellular based technique, and the technique that integrates Internet banking. Chapter 10 defines criteria on which the Internet user authentication techniques presented in chapter 9 can be measured against. The evaluation of each of the techniques is made against the specified criteria. In fact, this chapter concludes the thesis. Chapter 11 provides case studies on two of the techniques evaluated in chapter 10. Specifically, the insurance case study and the medical aid case studies are presented...|$|R
30|$|To {{represent}} nodes’ {{functional and}} non-functional characteristics, a node has an {{association with the}} following two entities: (1) Node Functional Parameters: consists of characteristics/properties related to the functioning of a node such as operating system type, software version, {{and the type of}} the network management system. It is composed of: (a) Storage parameters which determine the available disk space, storage type, and number of storage units; (b) memory parameters which represent the size, capacity, and type of the available memory; and (c) CPU parameters which represent the information about the available processing unit(s). (2) Node Non-Functional Parameters: this class defines constraints, QoS scheme, and desired criteria that should be met when selecting a resource, namely: cost, rank, and percentage of failure. In turn, non-functional attributes are composed of the following: (a) Performance parameters representing node performance properties such as response time, uptime, capacity, and reliability level. (b) Security level parameters defining security properties that a node supports like hashing techniques (i.e. <b>Checksums,</b> <b>cryptographic</b> hash functions), encryption methods (i.e. symmetric, asymmetric) and security properties (i.e. confidentiality, integrity). (c) QoS parameters representing QoS related characteristics including the average packet loss, jitter, delay, and bit rate.|$|R
40|$|We {{consider}} {{the problem of}} booting a workstation across a network. We allow "maintenance" (that is, change without notice by untrusted parties such as adversaries and system managers) to be freely performed upon the network, the workstation, and the remote boot service itself. We assume that humans are unable to recognise long sequences of independent bits such as <b>cryptographic</b> keys or <b>checksums</b> reliably, but can remember passwords which have been sufficiently poorly chosen to succumb to guessing attacks. We also assume that {{a part of the}} workstation hardware (including a small amount of ROM) can be physically protected from modification, but that the workstation cannot protect the integrity of any mutable data, including cryptographic keys (which must change if a secret is compromised.) Nevertheless, we are able to provide strong guarantees that the code loaded by the remote boot is correct, if the boot protocol says it is. The removal of maintenance and other attacks upon system integrity then becomes desirable in order to improve performance, rather than as a pre-requisite for ensuring correct behaviour. Our approach makes essential use of a hash function which is deliberately chosen so as to be rich in collisions, in contrast with prevailing practice...|$|R
40|$|International audienceIn this paper, {{we present}} a medical image {{integrity}} verification system to detect and approximate local malevolent image alterations (e. g. removal or addition of lesions) as well as identifying {{the nature of a}} global processing an image may have undergone (e. g. lossy compression, filtering [...] .). The proposed integrity analysis process is based on non significant region watermarking with signatures extracted from different pixel blocks of interest and which are compared with the recomputed ones at the verification stage. A set of three signatures is proposed. The two firsts devoted to detection and modification location are <b>cryptographic</b> hashes and <b>checksums,</b> while the last one is issued from the image moment theory. In this paper, we first show how geometric moments can be used to approximate any local modification by its nearest generalized 2 D Gaussian. We then demonstrate how ratios between original and recomputed geometric moments can be used as image features in a classifier based strategy {{in order to determine the}} nature of a global image processing. Experimental results considering both local and global modifications in MRI and retina images illustrate the overall performances of our approach. With a pixel block signature of about 200 bit long, it is possible to detect, to roughly localize and to get an idea about the image tamper...|$|R
40|$|Memory {{attestation}} in sensor nodes is {{a recent}} but challenging issue. With the increased memory based attacks in sensor nodes owing to their use in critical applications like defense, having a robust memory attestation scheme becomes increasingly important. Due to the resource-constrained nature of the sensor networks, it becomes increasingly important to protect the memory using only software methods. The existing attestation schemes use slow <b>cryptographic</b> hashes or <b>checksum,</b> and/or are based on pseudo-random memory traversal. A brief literature review of the existing software based attestation schemes, like Software Based Attestation for embedded devices (Seshadri et al., 2004), block-based approaches (AbuHmed, T. et al., 2009), Secure Code Update By Attestation (Seshadri et al., 2006), reflects the strength of pseudo-random memory traversal methods due to the randomness in the verification routine. However, it affects the performance, as more memory traversals are required for the memory attestation, which is given by Coupon Collector problem (Mitzenmacher, M., et al., 2005). Also, since the data is verified remotely by a base station, the memory contents of a node need to be stored at the attester side, so that the response summary can be matched with the expected value. Keeping the trade-off between security and performance in mind, this thesis work, explores a new approach named 2 ̆ 7 On-the-fly memory integrity protection scheme 2 ̆ 7 which has been existing for memory attestation in general processors, and extend it to the sensor node architecture. The extended approach accommodates the necessary steps like authentication, noise-filling technique, initialization, code update and memory attestation, which are an important. The extended scheme makes use of universal hash function (NH) and the Merkle tree for multiple hashing. The use of Merkle tree to store the final hash at the base station, along with the Toeplitz approach helps in achieving an improved hash-collision probability. Also, with the modified version using Krawczyk 2 ̆ 7 s approach, the need of multiple keys in hashing is fulfilled by a single key. ...|$|R

