220|5|Public
25|$|It is also {{possible}} to compute a simpler invariant of directed graphs by ignoring the directions of the edges and computing the circuit rank of the underlying undirected graph. This principle forms {{the basis of the}} definition of <b>cyclomatic</b> <b>complexity,</b> a software metric for estimating how complicated a piece of computer code is.|$|E
25|$|The circuit rank can be {{explained}} in terms of algebraic graph theory as the dimension of the cycle space of a graph, in terms of matroid theory as the corank of a graphic matroid, and in terms of topology as one of the Betti numbers of a topological space derived from the graph. It counts the ears in an ear decomposition of the graph, forms the basis of parameterized complexity on almost-trees, and has been applied in software metrics as part of the definition of <b>cyclomatic</b> <b>complexity</b> of a piece of code. Under the name of cyclomatic number, the concept was introduced by Gustav Kirchhoff.|$|E
5000|$|... The 10 {{most complex}} methods (Source Code <b>Cyclomatic</b> <b>complexity)</b> ...|$|E
5000|$|The {{differences}} between sequential and concurrent programs {{lead to the}} differences in their testing strategies. Strategies for sequential programs can be modified to make them suitable for concurrent applications. Specialized strategies have also been developed. Conventionally, testing includes designing test cases and checking that the program produces the expected results. Thus, errors in specification, functionality, etc. are detected by running the application and subjecting it to testing methods such as Functional Testing, White Box, Black Box and Grey Box Testing. [...] Static analysis is also used for detecting errors in high performance software using methods such as Data Flow Analysis, Control Flow Analysis, <b>Cyclomatic</b> <b>Complexities,</b> Thread Escape Analysis and Static Slicing Analysis to find problems. Using static analysis before functionality testing can save time. It can detect ‘what the error is’ find the error source. Static analysis techniques can detect problems like lack of synchronization, improper synchronizations, predict occurrence of deadlocks and post-wait errors in rendezvous requests.|$|R
40|$|It is {{commonly}} assumed {{that if a}} programmer is willing to invest the potentially significant effort required to port an application program to run on a multiprocessor system using a low-level parallel language or library, {{they will be able}} to take advantage of a larger degree of parallelism to achieve higher performance than when using a higher-level language or an automatically parallelizing compiler. However, there has been little work examining the relationship between programming complexity (or ease-of-use) and performance. As a first step towards quantifying this relationship, we use the <b>cyclomatic</b> program <b>complexity</b> metric, borrowed from the software engineering field, and the number of program source statements as measures of the relative complexity of a parallel implementation of a application program compared to it's equivalent sequential implementation. We compare several different programming paradigms across a common set of application programs executed on two workstation c [...] ...|$|R
40|$|Intuitively we know, some {{software}} {{errors are}} more complex than others. If the error can be fixed by changing one faulty statement, {{it is a simple}} error. The more substantial the fix must be, the more complex we consider the error. In this work, we formally define and quantify the com-plexity of an error w. r. t. the complexity of the error’s least complex, correct fix. As a concrete measure of complexity for such fixes, we introduce <b>Cyclomatic</b> Change <b>Complexity</b> which is inspired by existing program complexity metrics. Moreover, we introduce CoREBench, a collection of 70 regression errors systematically extracted from several open-source C-projects and compare their complexity with that of the seeded errors in the two most popular error bench-marks, SIR and the Siemens Suite. We find that seeded er-rors are significantly less complex, i. e., require significantly less substantial fixes, compared to actual regression errors. For example, among the seeded errors more than 42 % are simple compared to 8 % among the actual ones. This is a concern for the external validity of studies based on seeded errors and we propose CoREBench for the controlled study of regression testing, debugging, and repair techniques...|$|R
50|$|<b>Cyclomatic</b> <b>complexity</b> is {{computed}} {{using the}} control flow graph of the program: the nodes of the graph correspond to indivisible groups of commands of a program, and a directed edge connects two nodes if the second command might be executed {{immediately after the}} first command. <b>Cyclomatic</b> <b>complexity</b> may also be applied to individual functions, modules, methods or classes within a program.|$|E
50|$|This {{corresponds}} to the intuitive notion of <b>cyclomatic</b> <b>complexity,</b> and can be calculated as above.|$|E
5000|$|It {{is useful}} because of two {{properties}} of the <b>cyclomatic</b> <b>complexity,</b> M, for a specific module: ...|$|E
40|$|High {{fidelity}} (verisimilitude to {{the system}} modelled) in simulation is particularly desirable; however, increasing fidelity generally comes {{at the cost of}} increasing computational cost and complexity. Despite the importance of this relationship of complexity and fidelity, little theoretical or empirical work has been performed to elucidate the relationship. To this end, this thesis presents a systematic, empirical study of simulation model complexity and fidelity employing multi-agent based simulations; considering three problem areas - simulation of conversation group dynamics, sheepdog herding behaviours and traffic dynamics. Multi-agent simulations facilitate modelling the complex behaviours through simple rules codified within the agents. However, the manual construction of effective rules and tuning parameters for desired behaviours are generally prohibitive or at least sub-optimal in these types of simulations. Evolutionary techniques can effectively explore the rule and parameter space for high fidelity simulations; however, pairing of these two techniques is challenging in classes of problems where the evaluation of simulation fitness is reliant on human aesthetic judgement rather than empirical data. Further, many human judgements must be made in the design of such systems. In this work, a multi-agent framework has been devised to model complex behaviours while the experiments are designed to investigate the chief research questions 1) how well can human aesthetic judgement on the behaviour fidelity of simulations be captured by a machine learning system? 2) how well can such systems automate the fitness evaluation for evolutionary algorithms in deriving high fidelity simulations? 3) how effectively can an evolutionary framework be employed to explore and understand the relationship of complexity and fidelity? 4) to what extent can a relationship between model complexity (measured through rule, parameter and <b>cyclomatic</b> computational <b>complexity)</b> and fidelity (measured through human aesthetic judgement) be generalised irrespective of a particular problem? The results show that: human aesthetic judgement on behaviour fidelity can indeed be captured by a machine learning system; that multi-objective evolutionary optimisation is an effective way to understand the relationship between model complexity and fidelity; and that there are common properties in three problem domains indicating diminishing returns in terms of simulation fidelity as the simulation complexity increases...|$|R
40|$|Graduation date: 1980 Most {{measures}} of program complexity gauge either textual or control flow attributes of a program. A recent {{addition to the}} field of complexity measures, the knot metric, is a function of both these attributes. A knot measurement reflects the degree of control-flow tangle in a program's listing. This thesis discusses and proves four functional properties of the knot measure. 1. Calculation of a program's knot content is fast with respect to the number of branches in a program. A worst-case optimal algorithm for computing knots is quadratic in time and linear in space. 2. The complexity of a program can be reduced by rearranging groups of statements in a manner that retains the program's function yet lowers its knot content. The problem of finding an arrangement with the fewest knots for any arrangement of a program is probably difficult or NP-complete, but approximation methods are fast and often find the minimum knot arrangement. 3. A direct relationship exists between the types of knots in a program text and the structuredness of that program. This leads to an easily testable, sufficient condition for unstucturedness. Thus unstuctured programs may be detected without graphically reducing the control structure to structured programming conventions. 4. An empirical investigation of a set of FORTRAN programs, testing for their knot content, rearrangement characteristics, cyclomatic number, and program length, demonstrates the practicality of the knot measure. Most programs benefited from rearrangement, and a fast, heuristic algorithm was effective in finding a program text ordering with minimal knot content. Furthermore, the knot content of a program is dissassociated from two other {{measures of}} <b>complexity,</b> <b>cyclomatic</b> number and program length. Knots must measure some aspect of complexity missed by those measures. Overall, the knot metric is an effective, and efficient means for detecting, reducing, and controlling some attributes of software complexity...|$|R
5000|$|The {{techniques}} used in white box testing are condition coverage, decision coverage, statement coverage, <b>cyclomatic</b> <b>complexity.</b>|$|E
50|$|Essential {{complexity}} is {{a numerical}} measure defined by McCabe in his highly cited, 1976 paper {{better known for}} introducing <b>cyclomatic</b> <b>complexity.</b> McCabe, defined essential complexity as the <b>cyclomatic</b> <b>complexity</b> of the reduced CFG (control flow graph) after iteratively replacing (reducing) all structured programming control structures, i.e. those having a single entry point and a single exit point (for example if-then-else and while loops) with placeholder single statements.|$|E
5000|$|This {{corresponds}} to the characterization of <b>cyclomatic</b> <b>complexity</b> as [...] "number of loops plus number of components".|$|E
5000|$|All {{three of}} the above numbers may be equal: branch {{coverage}} [...] <b>cyclomatic</b> <b>complexity</b> [...] number of paths.|$|E
5000|$|<b>Cyclomatic</b> <b>complexity</b> may be {{extended}} to a program with multiple exit points; {{in this case it}} is equal to: ...|$|E
50|$|Another {{application}} of <b>cyclomatic</b> <b>complexity</b> is {{in determining the}} number of test cases {{that are necessary to}} achieve thorough test coverage of a particular module.|$|E
50|$|The first Betti {{number is}} {{also called the}} cyclomatic number—a term {{introduced}} by Gustav Kirchhoff before Betti's paper. See <b>cyclomatic</b> <b>complexity</b> for an application to software engineering.|$|E
5000|$|A {{number of}} studies have {{investigated}} the correlation between McCabe's <b>cyclomatic</b> <b>complexity</b> number with the frequency of defects occurring in a function or method. [...] Some studies find a positive correlation between <b>cyclomatic</b> <b>complexity</b> and defects: functions and methods that have the highest complexity tend to also contain the most defects. However, the correlation between <b>cyclomatic</b> <b>complexity</b> and program size (typically measured in lines of code) has been demonstrated many times. Les Hatton has claimed that complexity has the same predictive ability as lines of code.Studies that controlled for program size (i.e., comparing modules that have different complexities but similar size) are generally less conclusive, with many finding no significant correlation, while others do find correlation. Some researchers who have studied the area question the validity of the methods used by the studies finding no correlation. Although this relation is probably true, it isn't commercially useful. Since program size is not a controllable feature of commercial software, the usefulness of McCabes's number has been called to question. The essence of this observation is that larger programs tend to be more complex and to have more defects. As a result, the metric has not been accepted by commercial software development organizations. Thus, reducing the <b>cyclomatic</b> <b>complexity</b> of code is not proven {{to reduce the number of}} errors or bugs in that code.|$|E
50|$|Flow {{complexity}} (FC) - Measures {{the complexity}} of a programs' flow control path {{in a similar way}} to the traditional <b>cyclomatic</b> <b>complexity,</b> with higher accuracy by using weights and relations calculation.|$|E
5000|$|Direct {{measures}} that are taken directly against a measurand. Examples include counts and named {{measures such as}} McCabe’s <b>cyclomatic</b> <b>complexity</b> or Gross Domestic Product. Values may be imported or queried via SMM operations.|$|E
5000|$|Neither {{of these}} cases exposes the bug. If, however, we use <b>cyclomatic</b> <b>complexity</b> to {{indicate}} the number of tests we require, the number increases to 3. We must therefore test {{one of the following}} paths: ...|$|E
50|$|One testing strategy, called basis {{path testing}} by McCabe who first {{proposed}} it, is to test each linearly independent {{path through the}} program; in this case, the number of test cases will equal the <b>cyclomatic</b> <b>complexity</b> of the program.|$|E
50|$|<b>Cyclomatic</b> <b>complexity</b> is a {{software}} metric (measurement), used {{to indicate the}} complexity of a program. It is a quantitative measure {{of the number of}} linearly independent paths through a program's source code. It was developed by Thomas J. McCabe, Sr. in 1976.|$|E
50|$|Quality metrics such as Halstead {{complexity}} measures, <b>Cyclomatic</b> <b>complexity,</b> Knots metric {{are designed}} to verify that code is clear, maintainable and testable. The Quality Report in the LDRA tool suite presents both a summary and detailed breakdown of quality metrics which are deduced during static analysis.|$|E
50|$|In this example, two {{test cases}} are {{sufficient}} {{to achieve a}} complete branch coverage, while four are necessary for complete path coverage. The <b>cyclomatic</b> <b>complexity</b> {{of the program is}} 3 (as the strongly connected graph for the program contains 9 edges, 7 nodes and 1 connected component) (9-7+1).|$|E
50|$|<b>Cyclomatic</b> <b>complexity</b> may, however, {{be applied}} to several such {{programs}} or subprograms {{at the same time}} (e.g., to all of the methods in a class), and in these cases P will be equal to the number of programs in question, as each subprogram will appear as a disconnected subset of the graph.|$|E
50|$|It is also {{possible}} to compute a simpler invariant of directed graphs by ignoring the directions of the edges and computing the circuit rank of the underlying undirected graph. This principle forms {{the basis of the}} definition of <b>cyclomatic</b> <b>complexity,</b> a software metric for estimating how complicated a piece of computer code is.|$|E
5000|$|... #Caption: The same {{function}} as above, represented using the alternative formulation, where each exit point is connected {{back to the}} entry point. This graph has 10 edges, 8 nodes, and 1 connected component, which also results in a <b>cyclomatic</b> <b>complexity</b> of 3 using the alternative formulation (10 - 8 + 1 = 3).|$|E
50|$|Synchronization {{complexity}} is a quantified attribute (see also: measurement) of {{a characteristic}} of a concurrent software product. It measures the additional complexity incurred by the synchronization constructs used in the software, and does that by analyzing the software source code. It is essentially {{an extension of the}} <b>cyclomatic</b> <b>complexity</b> for multitasking/multithreaded programs.|$|E
5000|$|One of McCabe's {{original}} applications was {{to limit}} the complexity of routines during program development; he recommended that programmers should count {{the complexity of the}} modules they are developing, and split them into smaller modules whenever the <b>cyclomatic</b> <b>complexity</b> of the module exceeded 10. This practice was adopted by the NIST Structured Testing methodology, with an observation that since McCabe's original publication, the figure of 10 had received substantial corroborating evidence, but that in some circumstances it may be appropriate to relax the restriction and permit modules with a complexity as high as 15. As the methodology acknowledged that there were occasional reasons for going beyond the agreed-upon limit, it phrased its recommendation as: [...] "For each module, either limit <b>cyclomatic</b> <b>complexity</b> to agreed-upon limit or provide a written explanation of why the limit was exceeded." ...|$|E
5000|$|An {{alternative}} formulation {{is to use}} a graph {{in which}} each exit point is connected back to the entry point. In this case, the graph is strongly connected, and the <b>cyclomatic</b> <b>complexity</b> of the program is equal to the cyclomatic number of its graph (also known as the first Betti number), which is defined as ...|$|E
5000|$|Mathematically, the <b>cyclomatic</b> <b>complexity</b> of a {{structured}} program is defined {{with reference to}} the control flow graph of the program, a directed graph containing the basic blocks of the program, with an edge between two basic blocks if control may pass from the first to the second. The complexity M is then defined as ...|$|E
50|$|The Bug Finder module {{identifies}} software bugs {{by performing}} static program analysis on source code. It finds defects such as numerical computation, programming, memory, and other errors. It also produces software metrics such as Comment density of a source file, <b>Cyclomatic</b> <b>complexity,</b> Number of lines, parameters, call levels, etc. in a function, Identified run-time {{errors in the}} software.|$|E
5000|$|... // Avoid making complex methods {{even more}} complex (source code <b>cyclomatic</b> <b>complexity)</b> warnif count > 0 [...] from m in JustMyCodeMethods where !m.IsAbstract && m.IsPresentInBothBuilds (...) && m.CodeWasChanged (...) let oldCC = m.OlderVersion (...) [...]CyclomaticComplexity where oldCC > 6 && m.CyclomaticComplexity > oldCC [...] select new { m, oldCC, [...] newCC = m.CyclomaticComplexity, [...] oldLoc = m.OlderVersion (...) [...]NbLinesOfCode, newLoc = m.NbLinesOfCode, } ...|$|E
5000|$|Complexity Count- UCC {{produces}} complexity {{counts for}} all source code files. The complexity counts may include {{the number of}} math, trig, logarithm functions, calculations, conditionals, logicals, preprocessors, assignments, pointers, and <b>cyclomatic</b> <b>complexity.</b> When counting, the complexity results are saved to the file [...] "outfile_cplx.csv", and when differencing the results are saved to the files [...] "Baseline-A-outfile_cplx.csv" [...] and [...] "Baseline-B- outfile_cplx.csv".|$|E
