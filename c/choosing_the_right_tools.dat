12|10000|Public
40|$|Abstract- Project {{managers}} have many challenges to overcome during {{all phases of}} a project. <b>Choosing</b> <b>the</b> <b>right</b> <b>tools</b> and metrics {{can make or break}} the project. This paper describes three guidelines, which when taken into consideration, have led projects to success. Metrics useful in management are also discussed as being very useful and applicable to IT development. Moreover, a new guideline was obtained by interviews with project managers of XYZ Company. 1 This new guideline helps provide good estimates of time and development {{at the beginning of a}} project. We will also be introducing the ideas of cohesion and coupling in the context of complex systems consisting of multiple programs...|$|E
40|$|PROFESSIONAL TECHNIQUES FOR MODERN LAYOUT. Smashing CSS {{takes you}} {{well beyond the}} basics, {{covering}} not only {{the finer points of}} layout and effects, but introduces you to the future with HTML 5 and CSS 3. Very few in the industry can show you {{the ins and outs of}} CSS like Eric Meyer and inside Smashing CSS Eric provides techniques that are thorough, utterly useful, and universally applicable in the real world. From <b>choosing</b> <b>the</b> <b>right</b> <b>tools,</b> to CSS effects and CSS 3 techniques with jQuery, Smashing CSS is the practical guide to building modern web layouts. With Smashing CSS you will learn how to: T...|$|E
40|$|Software {{downtime}} {{refers to}} the time when software is unavailable or its operation is prevented for some reason. New software installations, migrations and upgrades cause downtime in the system which leads to loss in revenue and reduced general level of service for the company. Difficult part is finding the right set of tools and practices that minimize this downtime at different stages of software’s deployment. <b>Choosing</b> <b>the</b> <b>right</b> <b>tools</b> that are compatible with company’s infrastructure is also challenging and typically happens through trial and error. The focus of this thesis is to study different means to reduce downtime during software deployment. Study focuses on reviewing tools and practices {{that can be used to}} deliver and deploy software to a production environment without service interruption. The research was carried out as literature review by examining the documents and materials from different sources...|$|E
50|$|Multimethodology {{and mixed}} methods {{research}} are desirable and feasible {{because they provide}} a more complete view, and because the requirement during the different phases of an intervention (or research project) make very specific demands on a general methodology. While it is demanding, it is more effective to <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job at hand.|$|R
40|$|<b>Choosing</b> <b>the</b> <b>right</b> Frame <b>tools</b> [...] . 13 FDK {{documentation}} [...] 14 Naming conventions [...] . 14 Style conventions [...] 1...|$|R
40|$|The {{first stage}} in a study on any subject is to examine and {{evaluate}} prior studies onthat subject. Modern computer technologies offer the researcher good facilities todo this easily. However, to do this, {{it is very important}} to <b>choose</b> <b>the</b> <b>right</b> <b>tools</b> andright method. <b>The</b> relational database systems are <b>the</b> <b>right</b> <b>tools</b> for this task. Thisarticle offers a database model for collecting the data from the tafsir history booksand converting them to a database. If this model is developed and published as online,it would be very useful data source for tafsir researchers...|$|R
40|$|Bachelor {{thesis is}} {{dedicated}} to the problematics of digital footprints, to what extend the user is able to control it and protect his privacy by the use of freely available tools. In the first part the term digital footprint is analysed from several different points of view and the risks linked to their creation are explained. It is clarified how one can find the extent of his digital footprint and how to manage it. Next tools for providing protection from tracking, software for ensuring anonymity in the internet environment and techniques for potential removal of an existing footprint are analysed. The practical part {{is dedicated to}} <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> from the previous chapter and their mutual comparison for the purpose of finding out to what extend the user is able to protect his privacy and avoid possible risks. Conclusions are made from the results of the comparisons...|$|E
40|$|This {{project is}} focused on {{gathering}} information about environs of the company presented on the social networks and <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> to analyze {{the perception of the}} firm and its products between the users of the social network. In the first part, the benefits of analization on social network based on the rating metrics gained from the datas of social networks are explained. In the second part the right social network is chosen to analyze and then the proccess of accessing the datas is revealed. The last part of the project is about different political situations in addition of monitoring the traditional medias and complete informational value of the analysation is shown. The result of this whole project is to compare the differences between informational value from the classic medias and the informational valuenof the social networks and eventually whole application for the company...|$|E
40|$|The goal of {{this thesis}} is to examine metrics and tools for {{analyzing}} the complexity of program source code and to compare different tools with each other. In the opening part, the issues {{in the field of}} software development and problems the company and the programmer might encounter are presented. In the next chapter, we learn about the metrics with which we can ensure that during software development individual parts of the program do not become too complex for further upgrading or for introducing new programmers to the project. This is followed by a list of tools, ranging from simple free to very complex paid tools. It is examined which tool calculates which metric and in the last chapter an automated analysis of samples of source code is conducted. The comparison of the reports about these tools will be helpful when <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> for automated analysis of the complexity of the software source code in software development in the future...|$|E
50|$|As {{companies}} grow, agile testing teams often rely {{on software}} testing tools to solve challenges that can ultimately speed-up {{the release of}} feedback making sure. Most teams look for collaboration features, automated or customized reporting and finding ways to avoid repeated efforts. <b>Choosing</b> <b>the</b> <b>right</b> <b>tool</b> {{will depend on the}} requirements of each team. Pairing up with other Agile Lifecycle Development Tools, Agile testing tools can deliver effective results by coexisting in integrated environments. Such is the case for Atlassian Marketplace and Microsoft Visual Studio.|$|R
40|$|AbstractThis paper {{presents}} an original approach {{and the corresponding}} application for programming plane contours on the CNC milling machine MORI SEIKI MSX- 504. The application comprises two modules: a processor for {{the determination of the}} characteristic points and a post-processor that takes the characteristic points from a text file and the machining parameters from the application graphical interface and outputs the final program as a. cnc file. The applications also assists the CNC programmers in dealing with very small radii for interior machining, helping them <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job...|$|R
40|$|Abstract. Workflow {{technologies}} are {{emerging as the}} dominant approach to coordinate groups of distributed services. However with a space filled with competing specifications, standards and frameworks from multiple domains, <b>choosing</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job {{is not always a}} straightforward task. Researchers are often unaware of the range of technology that already exists and focus on implementing yet another proprietary workflow system. As an antidote to this common problem, this paper presents a concise survey of existing workflow technology from the business and scientific domain and makes a number of key suggestions towards the future development of scientific workflow systems. ...|$|R
40|$|At a {{time when}} social {{networks}} are a particular phenomenon of the period include internal network still less-known tools {{to increase the efficiency}} of communication within companies. This bachelor thesis deals with the possibilities of using social network Yammer and its contribution to internal communications. The first part describes the social networks in general, it is focused on the internal network, specifically on the internal network Yammer. It describes its basic functions and discusses his contribution to the company. The practical part deals with the manner of its use in a corporate environment, and based on the survey analyzes the degree of expansion and users' attitude to its use in a specific business environment. In conclusion, I offer the possibility of its use in the context of VŠE in Prague. The aim of my bachelor thesis is to acquaint the reader with problems of internal network Yammer, with its advantages and disadvantages and thus to offer one possible solution when <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> for effective internal communication...|$|E
40|$|Cette thèse est dédier a ̀ mes parents. This thesis {{demonstrates}} {{the application of}} visual and ultrasonic SONAR sensors to a robot platform {{in such a way}} as to allow the robot to follow a person outdoors. The robot de-termines the range and bearing to the person by the person’s distinctly coloured vest and a SONAR beaconing package. Unlike similar projects, this system uses fixed latency ra-dio synchronisation between the robot and the person wearing the SONAR beacon. This system also distinguishes itself from similar projects in its application of a set of wide field-of-view SONAR transducers which allow less constrained operation than those employing acoustic reflectors. These two factors allow the robot to accurately pursue the person out-doors at speeds of up to 5 km/h, with a minimum turning radius of 3 m. Preface <b>Choosing</b> <b>the</b> <b>right</b> <b>tools</b> and learning how to use them properly will almost always result in a better product: in this case, a thesis. Often, these tools are not brand-new, with all the latest bells and whistles and commercial product support. Writing a thesis is a difficul...|$|E
40|$|Our current {{understanding}} of cancer genetics is grounded {{on the principle}} that cancer arises from a clone that has accumulated the requisite somat-ically acquired genetic aberrations, leading to the malignant transforma-tion. It also results in aberrent of gene and protein expression. Next gener-ation sequencing (NGS) or deep se-quencing platforms are being used to create large catalogues of changes in copy numbers, mutations, structural variations, gene fusions, gene expres-sion, and other types of information for cancer patients. However, inferring different types of biological changes from raw reads generated using the sequencing experiments is algorith-mically and computationally challeng-ing. In this article, we outline common steps for the quality control and pro-cessing of NGS data. We highlight the importance of accurate and applica-tion-specific alignment of these reads and the methodological steps and challenges in obtaining different types of information. We comment on the importance of integrating these data and building infrastructure to analyse it. We also provide exhaustive lists of available software to obtain informa-tion and point the readers to articles comparing software for deeper insight in specialised areas. We hope that the article will guide readers in <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> for analysing oncog-enomic datasets. Key words: next generation sequencing...|$|E
40|$|Producing 3 D {{interactive}} {{models is}} becoming a greater challenge every day. <b>Choosing</b> <b>the</b> <b>right</b> <b>tool</b> to handle <b>the</b> modelling process is essential if the final product {{is to be a}} VRML world, which can satisfy the user’s desire for both interactivity and realism. 3 ds max offers a potentially excellent development environment for creating high quality 3 D models. This paper discusses how the tools and techniques available within 3 ds max can be harnessed to produce complex interactive models, which are viewable with a VRML browser. Keywords: VRML, Virtual Reality, interactivity, 3 ds max, modelling, 3 D model...|$|R
40|$|Microsoft's Dynamic Language Runtime (DLR) is a {{platform}} for running dynamic languages such as Ruby and Python {{on an equal footing}} with compiled languages such as C#. Furthermore, the runtime is the foundation for many useful software design and architecture techniques you can apply as you develop your. NET applications. Pro DLR in. NET 4 introduces you to the DLR, showing how you can use it to write software that combines dynamic and static languages, letting you <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job. You will learn the core DLR components such as LINQ expressions, call sites, binders, and dynam...|$|R
40|$|Many {{promotion}} tools variety {{can help}} a company to promote its product. <b>The</b> company must <b>choose</b> <b>the</b> <b>right</b> promotional <b>tool</b> to optimizing the promotion cost and increasing the customer interest. <b>The</b> impact of <b>choosing</b> <b>the</b> <b>right</b> promotion <b>tool</b> was increasing the sales and giving a positive progress for the company. This research explored Carrefour and Hypermart promotion tool, as two big hypermarkets in Indonesia. These hypermarkets used some promotion tools; {{one of them was}} digital catalog. This research wanted to find which ones from the digital catalog variables will affect the customer buying interest, and measured the effectiveness of both catalog. The result showed 75. 60...|$|R
40|$|In {{this thesis}} {{we wanted to}} present the project that was made for a smaller hotel in Nova Gorica. The goal was to create an {{application}} for managing access control according to customer's wishes {{as well as to}} introduce the system into the existent infrastructure. The first step was to define what access control actually means. In broad terms it is divided into RFID – radio-frequency identification and biometric identification. Both have their strengths and their weaknesses. Next step was <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> to develop the application for managing access control. We picked Microsoft Visual Studio 2008 and Microsoft. NET framework using C# programming language. We analyzed the customer's requirements, defined the functional and non-functional requirements and set up a database. The most demanding part was of course developing the application, studying the hardware and how to communicate with it. We chose the hardware created by IDTECK, which has {{been at the forefront of}} manufacturing and developing access control hardware for more than 20 years. Their products are moderately priced and they provide software developers with the SDK development kit that allows the developers to upgrade their solutions according to customer's wishes. The thesis thus offers an overview of the development of an application for managing smart RFID card, made to suit the customer's wishes. We created an application that offers more than access control, enabling an overview of its users i. e. hotel guests as well as automatically disabling the users after a set date, which is something that access control hardware does not offer without a software solution. While researching various options we learned many details of access control and learned a lot about developing applications in Microsoft Visual Studio environment. In development we learned that there are many various other useful functions that access control allows, from calculating presence to timetables of working time and along with that an automatic calculation of the working hours...|$|E
40|$|In {{the present}} day, mobile based data {{services}} {{have become increasingly}} popular among end users and businesses and thus considered {{as one of the}} important issues in the telecommunication network, because of its high demand. The telecommunication industry is continuously striving to fulfil this demand in a cost-efficient manner. Fundamentally, the performance of a mobile communication network is constrained by the propagation environment and technical capabilities of the network equipment. The target of radio network engineers is to design and deploy a mobile network that provides effective coverage and capacity solution with a profitable implementation cost. In order to reach this target, careful examination of radio network planning and <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> are the key methods. Network densification is considered as a feasible evolutionary pathway to fulfil the exponentially increasing data capacity demand in mobile networks. The objective of this thesis work is to study and analyse the densification of classical macrocellular network, which is still the dominant form of deployment worldwide. The analysis is based on deep ray-tracing based propagation simulations in the outdoor and indoor environment, and considers two key performance metrics; cell spectral efficiency and area spectral efficiency. For analysing the impact of network densification, different cell densities, obtained from varying the inter-site distances are considered. Furthermore, the network is assumed to be operating in a full load condition; an extreme condition in which the base stations are transmitting at full power. From the simulations, it has been illustrated {{that as a result of}} densifying the network, the inter-cell interference increases, which reduce the achievable cell spectral efficiency. The system capacity, on the other hand, is shown to improve due to the increase in the area spectral efficiency, as a result of high-frequency re-use, in the outdoor settings. Nevertheless, it is observed that the densification of macrocellular network experience inefficiency in the indoor environment; mainly arising from coverage limitation due to extreme antenna tilt angles. This calls for sophisticated methods such as base station coordination or inter-cell interference cancellation technique to be employed for future cellular network. For fulfilling the indoor capacity demand in a cost-efficient manner, the operators will be required to deploy dedicated indoor small cells based solutions...|$|E
40|$|Federal {{agencies}} and their partners collect and manage {{large amounts of}} geospatial data but it is often not easily found when needed, and sometimes data is collected or purchased multiple times. In short, the best government data is not always organized and managed efficiently to support decision making in a timely and cost effective manner. National mapping agencies, various Departments responsible for collection {{of different types of}} Geospatial data and their authorities cannot, for very long, continue to operate, as they did a few years ago like people living in an island. Leaders need to look at what is now possible that was not possible before, considering capabilities such as cloud computing, crowd sourced data collection, available Open source remotely sensed data and multi source information vital in decision-making as well as new Web-accessible services that provide, sometimes at no cost. Many of these services previously could be obtained only from local GIS experts. These authorities need to consider the available solution and gather information about new capabilities, reconsider agency missions and goals, review and revise policies, make budget and human resource for decisions, and evaluate new products, cloud services, and cloud service providers. To do so, we need, <b>choosing</b> <b>the</b> <b>right</b> <b>tools</b> to rich the above-mentioned goals. As we know, Data collection is the most cost effective part of the mapping and establishment of a Geographic Information system. However, it is {{not only because of the}} cost for the data collection task but also because of the damages caused by the delay and the time that takes to provide the user with proper information necessary for making decision from the field up to the user's hand. In fact, the time consumption of a project for data collection, processing, and presentation of geospatial information has more effect on the cost of a bigger project such as disaster management, construction, city planning, environment, etc. Of course, with such a pre-assumption that we provide all the necessary information from the existing sources directed to user's computer. The best description for a good GIS project optimization or improvement is finding a methodology to reduce the time and cost, and increase data and service quality (meaning; Accuracy, updateness, completeness, consistency, suitability, information content, integrity, integration capability, and fitness for use as well as user's specific needs and conditions that must be addressed with a special attention). Every one of the above-mentioned issues must be addressed individually and at the same time, the whole solution must be provided in a global manner considering all the criteria. In this thesis at first, we will discuss about the problem we are facing and what is needed to be done as establishment of National Spatial Data Infra-Structure (NSDI), the definition and related components. Then after, we will be looking for available Open Source Software solutions to cover the whole process to manage; Data collection, Data base management system, data processing and finally data services and presentation. The first distinction among Software is whether they are, Open source and free or commercial and proprietary. It is important to note that in order to make distinction among softwares it is necessary to define a clear specification for this categorization. It is somehow very difficult to distinguish what software belongs to which class from legal point of view and therefore, makes it necessary to clarify what is meant by various terms. With reference to this concept there are 2 global distinctions then, inside each group, we distinguish another classification regarding their functionalities and applications they are made for in GIScience. According to the outcome of the second chapter, which is the technical process for selection of suitable and reliable software according to the characteristics of the users need and required components, we will come to next chapter. In chapter 3, we elaborate in to the details of the GeoNode software as our best candidate tools to take responsibilities of those issues stated before. In Chapter 4, we will discuss the existing Open Source Data globally available with the predefined data quality criteria (Such as theme, data content, scale, licensing, and coverage) according to the metadata statement inside the datasets by mean of bibliographic review, technical documentation and web search engines. We will discuss in chapter 5 further data quality concepts and consequently define sets of protocol for evaluation of all datasets according to the tasks that a mapping organization in general, needed to be responsible to the probable users in different disciplines such as; Reconnaissance, City Planning, Topographic mapping, Transportation, Environment control, disaster management and etc… In Chapter 6, all the data quality assessment and protocols will be implemented into the pre-filtered, proposed datasets. In the final scores and ranking result, each datasets will have a value corresponding to their quality according to the sets of rules that are defined in previous chapter. In last steps, there will be a vector of weight that is derived from the questions that has to be answered by user with reference to the project in hand in order to finalize the most appropriate selection of Free and Open Source Data. This Data quality preference has to be defined by identifying a set of weight vector, and then they have to be applied to the quality matrix in order to get a final quality scores and ranking. At the end of this chapter there will be a section presenting data sets utilization in various projects such as " Early Impact Analysis" as well as "Extreme Rainfall Detection System (ERDS) - version 2 " performed by ITHACA. Finally, in conclusion, the important criteria, as well as future trend in GIS software are discussed and at the end recommendations will be presente...|$|E
40|$|Graphs are a {{powerful}} data structure {{that can be}} applied to several problems in bioinformatics. Graph matching, in its diverse forms, is an important operation on graphs, involved when there is the need to compare two graphs or to find substructures into larger structures. Many graph matching algorithms exist, and their relative efficiency depends on the kinds of graphs they are applied to. In this paper we will consider some popular and freely available matching algorithms, and will experimentally compare them on graphs derived from bioinformatics applications, in order to help the researchers in this field to <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> problem at hand...|$|R
40|$|Abstract. Use of {{software}} tools to support business processes {{is both a}} possibility and necessity for {{both large and small}} enterprises of today. Given the variety of tools on the market, {{the question of how to}} <b>choose</b> <b>the</b> <b>right</b> <b>tools</b> for <b>the</b> process in question or analyze the suitability of the tools already employed arises. The paper presents an experience report of using a high-level business process model for analyzing software tools suitability at a large ICT organization that recently transitioned to scrum-based project methodology {{of software}} development. The paper gives overview of the modeling method used, describes the organizational context, presents a model built, and discusses preliminary findings based on the analysis of the model...|$|R
40|$|There is a {{high and}} growing risk in agriculture, which makes <b>choosing</b> <b>the</b> <b>right</b> <b>tool</b> to support risk {{management}} in agriculture more urgent. Traditional agricultural production insurance is very expensive and often – {{as is the case}} in Poland – does not provide adequate coverage. Income insurance, which ensures more complex coverage, may be an alternative to it and, as there is no perfect correlation between the value of individual production types, may be off ered at a comparatively lower price. Based on 2004 – 2013 data from 4, 590 Community Farm Accountancy Data Network (FADN) farms, it was proved that aggregate production insurance allows for a much lower insurance premium rate in relation to insurance of specifi c production types...|$|R
40|$|By {{improving}} the procurement process purchasing departments {{are becoming more}} and more strategic in order to increase overall companies performances. In the last years, web-based tools have effectively supported this trend by offering to buyers useful tools in order to help their activities. Purchasing departments have now to <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> <b>right</b> purchase in <b>the</b> <b>right</b> supply market; for this reason it is important for them to adopt a portfolio approach exploiting all the different solutions offered by the technology according to the different situations. In this perspective, this paper identifies four most common purchasing approaches adopted by companies, which drivers are considered to choose among these, and which level of performances companies achieve through the identified approaches. Empirical evidence is based on a survey research involving 162 US companies...|$|R
40|$|Background: <b>Choosing</b> <b>the</b> <b>right</b> {{software}} test automation tool is not trivial, {{and recent}} industrial surveys indicate lack of <b>right</b> <b>tools</b> as <b>the</b> main obstacle to test automation. Aim: In this paper, we study how practitioners tackle <b>the</b> problem of <b>choosing</b> <b>the</b> <b>right</b> test automation <b>tool.</b> Method: We synthesize the “voice” of the practitioners with a grey literature review originating from 53 different companies. The industry experts behind the sources had roles such as “Software Test Automation Architect”, and “Principal Software Engineer”. Results: Common consensus about the important criteria exists {{but those are}} not applied systematically. We summarize the scattered steps from individual sources by presenting a comprehensive process for tool evaluation with 12 steps {{and a total of}} 14 different criteria for <b>choosing</b> <b>the</b> <b>right</b> <b>tool.</b> Conclusions: <b>The</b> practitioners tend to have general interest in and be influenced by related grey literature as about 78 % of our sources had at least 20 backlinks (a reference comparable to a citation) while the variation was between 3 and 759 backlinks. There is a plethora of different software testing tools available, yet the practitioners seem to prefer and adopt the widely known and used tools. The study helps to identify the potential pitfalls of existing processes and opportunities for comprehensive tool evaluation...|$|R
40|$|AbstractThis article collates {{information}} {{about the number of}} scientific articles mentioning each of the established medulloblastoma cell lines, derived through a systematic search of Web of Science, Scopus and Google Scholar in 2016. The data for each cell line have been presented as raw number of citations, percentage share of the total citations for each search engine and as an average percentage between the three search engines. In order to correct for the time since each cell line has been in use, the raw citation data have also been divided by the number of years since the derivation of each cell line. This is a supporting article for a review of in vitro models of medulloblastoma published in “in vitro models of medulloblastoma: <b>choosing</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job” (D. P. Ivanov, D. A. Walker, B. Coyle, A. M. Grabowska, 2016) [1]...|$|R
40|$|Abstract. Sheet metal bending is a {{metal forming}} process, in which flat sheets are bent along {{straight}} bend lines {{in a specific}} bending sequence to form three-dimensional parts. A large number of tools with different characteristics {{can be used in}} this process. <b>The</b> task to <b>choose</b> <b>the</b> <b>right</b> <b>tooling</b> for a requested sheet metal part is however one of the bottle necks in process planning. An inefficient tool selection may result in failure of finding a feasible bending sequence. In previous work, methodologies for tool selection and optimization have been proposed. The presented paper describes a framework to implement these methodologies into a system that allows automatic tool selection in consistent consideration of bend sequencing. As a result, automated and optimized tool selection for sheet metal bending is achieved, as illustrated by performance test results for a robust software implementation...|$|R
40|$|For coaches, {{the most}} common and easiest way to analyse the tennis serve is to refer to their own vision. However, human vision is {{insufficient}} to observe high-speed motion with great precision. With the improvement of technology, it is now possible to study the gesture from a quantitative point of view. The quantitative evaluation of the tennis serve focuses on the kinematics and kinetics of the player but also on the stroke result, which includes the ball speed and the ball trajectory. This review aims to highlight the current tools available for players, coaches, medical staffs and biomechanical researchers, to evaluate the tennis serve. This overview will provide information to the player’s entourage in order to <b>choose</b> <b>the</b> <b>right</b> <b>tools</b> depending on their specific purposes. All of these tools can be applied in performance improvement and injury prevention. Peer reviewe...|$|R
40|$|This article collates {{information}} {{about the number of}} scientific articles mentioning each of the established medulloblastoma cell lines, derived through a systematic search of Web of Science, Scopus and Google Scholar in 2016. The data for each cell line have been presented as raw number of citations, percentage share of the total citations for each search engine and as an average percentage between the three search engines. In order to correct for the time since each cell line has been in use, the raw citation data have also been divided by the number of years since the derivation of each cell line. This is a supporting article for a review of in vitro models of medulloblastoma published in “in vitro models of medulloblastoma: <b>choosing</b> <b>the</b> <b>right</b> <b>tool</b> for <b>the</b> job” (D. P. Ivanov, D. A. Walker, B. Coyle, A. M. Grabowska, 2016) [1]...|$|R
40|$|Social {{networks}} are important means for communication, engaging millions of users around the globe. For enterprises in particular, being present and {{aware of what}} is discussed on these communication channels about their products and services has become a must. Social media monitoring tools enable enterprises {{to have access to}} real customers' opinions, complaints and questions at real time in a highly scalable way. As the number of social monitoring tools has rapidly increased in the last years, enterprises are faced with the difficult tasks of <b>choosing</b> <b>the</b> <b>right</b> <b>tool</b> for their needs. This paper proposes a structured evaluation framework comprising a set of evaluation criteria {{that can be used to}} analyze social monitoring tools from three perspectives: the concepts they implement, the technologies used and the user interface they provide. To exemplify the usefulness of our evaluation framework we analyze a set of social monitoring tools after briefly describing them...|$|R
40|$|AbstractIn present {{there are}} a lot of {{software}} tools that can be used in Power Engineering laboratories in order to teach a large variety of phenomena. Some of them are dedicated for specific applications like simulation of transient phenomena of electromagnetic and some of them are general purpose tools for modeling and simulating dynamic systems. Technical literature presents a lot of comparison between such tools but the results are presented only from the perspective of research criteria. The scope of this paper is to present how to <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> to be used in a specific laboratory, in order of a better understanding of the phenomena regarding a specific application of power system field. The paper presents a study case of a power system having two high voltage levels, modeled using two different tools: ATP-EMTP – dedicated software and MatLab Simulink – a general analyzing tool. The comparison is done for a power quality study...|$|R
40|$|Searching the World Wide Web {{can be a}} {{daunting}} task. The Web has expanded at such a rapid pace that nobody knows exactly how large it is, but {{it is safe to}} say that there are many billions of web pages residing on servers all over the world. Add to this scenario the hundreds of different search tools available to choose among – including directories, search engines, meta-searchers, and specialized search engines – and the situation begins to feel overwhelming. Fortunately, learning a few essential concepts of Web searching, along with mastering a handful of the top-rated search tools, can make the picture much brighter. Simply knowing how to <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> for your information need can make all the difference. This paper will first discuss basic concepts and terms you must know to be an effective searcher. Next, it will in turn examine each of the major categories of search tools, and recommend the best search engines and directories currently availabl...|$|R
40|$|The {{sustained}} {{efforts by}} electric motors when subjected to cutting, trimming or finishing {{are directly related}} to the material being machined and the angle of attack of <b>the</b> <b>tool.</b> <b>Choosing</b> <b>the</b> <b>right</b> <b>tool</b> for this operation depends on an expected result. So the engines behave differently to each operation. The optimization between strength, speed, power, material and type of operation, can be found to reduce operational costs of production, besides determining the exact time to make the set-up of worn tool. The reduction in operating costs is an item of sustainability that outlines the strategic positioning on companies to become competitive in the global marketplace. With the great technological development present today, this issue goes away with the very latest products on the market for professionals who productivity will be dealt with in these modern maintenance equipment such as power quality analyzer, Imager, profile projector and microscope for research. The result of this work is the optimization of the cutting operation and energy consumption thereby demonstrating an optimum point of operation in a case study presented in this work...|$|R
40|$|Although {{the use of}} {{computers}} has become widespread among architecture students, their use in design studios often lacks integration. To gain maximum advantage from computers, design students must acquire a breadth and depth of knowledge that allows them to <b>choose</b> <b>the</b> <b>right</b> <b>tools,</b> integrate multiple technologies, and apply knowledge to new situations. It is not possible for students to gain all of this knowledge in an ad hoc way as part of a design studio. Thus, an introductory CAAD course is a necessary prerequisite for participation in design studios that employ computer methods. The paper presents the experience of two faculty members currently working on the integration of their second year introductory CAAD courses and their fourth year Electronic Design Studios. The paper describes the pedagogical methods used in the introductory CAAD courses, and shows how they serve as the foundation for exercises in upper level electronic design studios. The paper also presents plans for the implementation of distance education methodologies in the delivery of computing and studio courses. The paper ends by providing conclusions that address how the use of computer technology permits the addition of instructional objectives that go beyond those of conventional design studios...|$|R
40|$|Learning {{computer}} programming has been always challenging. Since the sixties {{of the last}} century, many researchers developed Visual Programming Languages (VPLs) to help in this regard. In this thesis, ten VPLs were specifically selected, studied, experimented with, and evaluated. A total of fifteen metrics were {{used to evaluate the}} tools. Comparisons, classification, and gap analysis were then presented. A list of requirements for a general-purpose VPL and a guide to help <b>the</b> novice programmer <b>choose</b> <b>the</b> <b>right</b> <b>tool</b> were generated and finally the PWCT (Programming Without Coding Technology, a novel general-purpose visual programming language) is developed and presented. PWCT has been launched as a Sourceforge project, which currently has more than 230, 000 downloads for the language and more than 19, 500, 000 downloads for samples, tutorials and movies. Many business applications and projects are developed using PWCT, Also we developed the Supernova programming language and the Ring programming language using PWCT to prove that it can be used for advanced and large projects. Feedback from developers and results from the studies indicate that PWCT is a very appealing, competitive, and powerful language. Comment: Master of Science Thesi...|$|R
