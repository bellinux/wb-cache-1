1904|480|Public
25|$|The {{theory of}} {{hydration}} shells is well {{developed in the}} physical chemistry literature however a simple model is required that captures the essential effects with as little <b>computational</b> <b>overhead</b> as possible. For this purpose the same pairwise potential discussed by Im and Roux is implemented to include the effect of hydration shells.|$|E
25|$|Several {{security}} concerns arose in late 2014. Previously, researchers {{became aware that}} NTP servers can be susceptible to man-in-the-middle attacks unless packets are cryptographically signed for authentication. The <b>computational</b> <b>overhead</b> involved can make this impractical on busy servers, particularly during denial of service attacks. NTP message spoofing {{can be used to}} move clocks on client computers and allow a number of attacks based on bypassing of cryptographic key expiration. Some of the services impacted by fake NTP messages identified are TLS, DNSSEC, various caching schemes (such as DNS cache), BGP, Bitcoin and a number of persistent login schemes.|$|E
25|$|Solving such {{problems}} {{is actually an}} easy task and it is simply to avoid using ASCII format and use binary format instead. Not only it preserves the machine accuracy but also writing and reading to file system is a lot faster. The <b>computational</b> <b>overhead</b> to dump the trajectories becomes negligible and the trajectory files become about two orders of magnitude smaller in size. The downside might be that programming and decoding the data could become very tricky, but once it’s done correctly and with care, the advantages of using binary format are well worth the extra effort. BioMOCA is now equipped with the tools to record the trajectory information in binary format.|$|E
40|$|LBR has {{demonstrated}} outstanding classification accuracy. However, it has high <b>computational</b> <b>overheads</b> when {{large numbers of}} instances are classified from a single training set. We compare LBR and the tree-augmented Bayesian classifier, and present a new heuristic LBR classifier that combines elements of the two. It requires less computation than LBR, but demonstrates similar prediction accuracy. 1...|$|R
40|$|Domino {{logic is}} a popular con#guration for {{implementing}} high-speed circuits. An algorithm for domino logic mapping, under a parameterized library style, is presented here. Practical design methods, {{such as the use}} of multioutput domino and wide domino gates, are incorporated within the technology mapping framework. The technique can handle large circuits with small <b>computational</b> <b>overheads,</b> and shows improvements of up to about 37 # over existing methods...|$|R
40|$|We propose an {{apparently}} minor extension to Kay's (1985) notation for describing directed acyclic graphs (DAGs}. The proposed notation permits concise descriptions of phenomena which {{would otherwise be}} difficult to describe, without incurring significant extra <b>computational</b> <b>overheads</b> {{in the process of}} unification. We illustrate the notation with examples from a categorial description of a fragment of English, and discuss the computational properties of unification of DAGs specified in this way...|$|R
25|$|A {{problem in}} the field is that {{detailed}} neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. So, researchers that study large neural circuits typically represent each neuron and synapse simply, ignoring much of the biological detail. This is unfortunate as there is evidence that the richness of biophysical properties on the single neuron scale can supply mechanisms that serve as the building blocks for network dynamics. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low <b>computational</b> <b>overhead.</b> Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.|$|E
50|$|High {{performance}} with minimum <b>computational</b> <b>overhead.</b>|$|E
5000|$|... #Subtitle level 3: Computer Programming (run-time and <b>computational</b> <b>overhead)</b> ...|$|E
40|$|Abstract. DFS has {{previously}} {{been shown to be}} a simple and efficient strategy for removing cycles in graphs allowing the resulting DAGs to be scheduled using one of the many well-established DAG multiprocessor scheduling algorithms. In this paper, we investigate the inefficiencies of schedules acquired using DFS cycle removal. Further, an improved randomised DFS cycle removal algorithm is proposed that produces significantly improved results with acceptable <b>computational</b> <b>overheads.</b> ...|$|R
40|$|This paper {{presents}} a new parallelization method for an efficient implementation of unstructured array reductions on shared memory parallel machines with OpenMP. This method is {{strongly related to}} parallelization techniques for irregular reductions on distributed memory machines as employed {{in the context of}} High Performance Fortran. By exploiting data locality, synchronization is minimized without introducing severe memory or <b>computational</b> <b>overheads</b> as observed with most existing shared memory parallelization techniques...|$|R
30|$|In {{the search}} process, PKIS-II needs very slight <b>computational</b> <b>overheads,</b> within 10 ms (0.01 s). With the respect to time consumption, a search {{process is the}} most {{important}} factor. The search process of PKIS-II is similar to general plaintext search system because it can directly access the data without verifying for every row. It needs the additional time only to generate a trapdoor and to decrypt returned documents. The used cryptographic function in PKIS is also very fast.|$|R
5000|$|... {{reducing}} <b>computational</b> <b>overhead</b> {{to speed}} up parsing of compressed documents ...|$|E
50|$|Invoking a {{subroutine}} (versus using in-line code) imposes some <b>computational</b> <b>overhead</b> in {{the call}} mechanism.|$|E
5000|$|Blinn-Phong model, {{resembling}} Phong, but {{allowing for}} certain quantities to be interpolated, reducing <b>computational</b> <b>overhead.</b>|$|E
40|$|In {{the context}} of the {{nonlinear}} interaction of counterpropagating light beams, we demonstrate the unrealistic symmetries that arise in the simulation of spontaneous pattern formation when using square computational grids. We have formulated a generalization of the split-step Fourier method which allows for non-orthogonal spatial grids. Implementation is shown to be simple and to entail negligible <b>computational</b> <b>overheads.</b> Results for twodimensional Gaussian and extended cosine-bell input beam profiles are presented. © 1993 Taylor & Francis Ltd. SCOPUS: le. jinfo:eu-repo/semantics/publishe...|$|R
40|$|In this paper, an {{analytical}} model for DDoS attacks detection is proposed, in which propagation of abrupt traffic changes inside public domain is monitored {{to detect a}} wide range of DDoS attacks. Although, various statistical measures can be used to construct profile of the traffic normally seen in the network to identify anomalies whenever traffic goes out of profile, we have selected volume and flow measure. Consideration of varying tolerance factors make proposed detection system scalable to the varying network conditions and attack loads in real time. NS- 2 network simulator on Linux platform is used as simulation testbed. Simulation results show that our proposed solution gives a drastic improvement in terms of detection rate and false positive rate. However, the mammoth volume generated by DDoS attacks pose the biggest challenge in terms of memory and <b>computational</b> <b>overheads</b> as far as monitoring and analysis of traffic at single point connecting victim is concerned. To address this problem, a distributed cooperative technique is proposed that distributes memory and <b>computational</b> <b>overheads</b> to all edge routers for detecting {{a wide range of}} DDoS attacks at early stage. Comment: arXiv admin note: substantial text overlap with arXiv: 1203. 240...|$|R
40|$|Abstract — In recent years, {{encryption}} {{technology has}} been developed rapidly the chaos based cryptographic algorithms. Chaos owns certain critical properties such as sensitive dependence on initial condition, random-like behavior, and continuous broadband power spectrum, which match the confusion, diffusion, and key sensitivity requirements for cryptography. Chaos based cryptographic offer sundry features over the traditional encryption algorithms such as high security, speed, and sensible <b>computational</b> <b>overheads</b> and power. This paper presents a survey of public key encryption methods based on chaos system...|$|R
50|$|OpenSSL {{supports}} {{forward secrecy}} using elliptic curve Diffie-Hellman since version 1.0, with a <b>computational</b> <b>overhead</b> of approximately 15%.|$|E
50|$|The {{method is}} {{frequently}} used for conditionally invoking test/debugging code without requiring additional <b>computational</b> <b>overhead</b> for every input/output cycle.|$|E
50|$|DKIM {{requires}} cryptographic checksums to {{be generated}} for each message sent through a mail server, {{which results in}} <b>computational</b> <b>overhead</b> not otherwise required for e-mail delivery. This additional <b>computational</b> <b>overhead</b> is a hallmark of digital postmarks, making sending bulk spam more (computationally) expensive.This facet of DKIM may look similar to hashcash, except that the receiver side verification is not a negligible amount of work, and a typical hashcash algorithm would require far more work.|$|E
40|$|We present initial work on {{an expert}} system that will help users to debug their Pascal programs. This system asks the user {{questions}} concerning attempts to build a ‘partial model’ of the program- a model of those aspects of the program likely {{to relate to the}} error. This contrasts with previous systems in which a complete model of the user’s program is built and compared to templates of correct versions of the program. The advantages of this approach are greater flexibility, greater student involvement in the debugging process and lower <b>computational</b> <b>overheads...</b>|$|R
40|$|One of {{the most}} {{important}} aspects of Computational Cell Biology is the understanding of the complicated dynamical processes that take place on plasma membranes. These processes are often so complicated that purely temporal models cannot always adequately capture the dynamics. On the other hand, spatial models can have large <b>computational</b> <b>overheads.</b> In this article, we review some of these issues with respect to chemistry, membrane microdomains and anomalous diffusion and discuss how to select appropriate modelling and simulation paradigms based on some or all the following aspects: discrete, continuous, stochastic, delayed and complex spatial processes...|$|R
40|$|Crowds of non-player {{characters}} are increasingly common in contemporary video games. It {{is often the}} case that individual models are re-used, lowering visual variety in the crowd and potentially affecting realism and believability. This paper explores a number of approaches to increase visual diversity in large game crowds, and discusses a procedural solution for generating diverse non-player character models. This is evaluated using mixed methods, including a “clone spotting” activity and measurement of impact on <b>computational</b> <b>overheads,</b> in order to present a multi-faceted and adjustable solution to increase believability and variety in video game crowds...|$|R
50|$|It is {{possible}} to reduce the <b>computational</b> <b>overhead</b> by caching the results of interpolation. For example, a single-threaded serial version would only need to cache interpolated results for one row of the input grid.|$|E
5000|$|During each [...] and [...] operation, two random root-to-leaf {{paths of}} [...] are fully {{explored}} by [...] This takes [...] time. This {{is the same}} as the <b>computational</b> <b>overhead,</b> and is [...] since [...] is [...]|$|E
50|$|Many {{lossless}} codecs {{possess a}} low <b>computational</b> <b>overhead</b> compared to well-known lossy codecs, such as MP3 and AAC. This {{is particularly important}} for deeply embedded audio applications running on low-power mobile devices. aptX Lossless promotes low <b>computational</b> <b>overhead</b> by dynamically selecting the simplest coding functions for each short segment of audio whilst complying with other operational constraints, such as levels of compression and coding delay. Depending on the settings of other scalable parameters, aptX Lossless can encode a 48 kHz 16-bit stereo audio stream using only 10 MIPS on a modern RISC processor with signal processing extensions. The corresponding decoder represents only 6 MIPS on the same platform.|$|E
40|$|This paper {{discusses}} the logistics network design for end-of-lease computer products recovery {{by developing a}} deterministic programming model for systematically managing forward and reverse logistics flows. Due {{to the complexity of}} such network design problem, a two-stage heuristic approach is developed to decompose the integrated design of the distribution networks into a location-allocation problem and a revised network flow problem. The applicability of the proposed method is illustrated in a numerical study. Computational experiments demonstrate that high-quality solutions are obtained while modest <b>computational</b> <b>overheads</b> are incurred. Reverse logistics Product recovery Location-allocation Tabu search...|$|R
40|$|AbstractDistributed Denial of Service (DDoS) {{attacks have}} been {{increasing}} {{with the growth}} of computer and network infrastructures in Ubiquitous computing. DDoS attacks generating mass traffic deplete network bandwidth and/or system resources. It is therefore significant to detect DDoS attacks in their early stage. Our previous approach used a traffic matrix to detect DDoS attacks quickly and accurately. However, it could not find out to tune up parameters of the traffic matrix including (i) size of traffic matrix, (ii) time based window size, and (iii) a threshold value of variance from packets information with respect to various monitored environments and DDoS attacks. Moreover, the time based window size led to <b>computational</b> <b>overheads</b> when DDoS attacks did not occur. To cope with it, we propose an enhanced DDoS attacks detection approach by optimizing the parameters of the traffic matrix using a Genetic Algorithm (GA) to maximize the detection rates. Furthermore, we improve the traffic matrix building operation by (i) reforming the hash function to decrease hash collisions and (ii) replacing the time based window size with a packet based window size to reduce the <b>computational</b> <b>overheads.</b> We perform experiments with DARPA 2000 LLDOS 1. 0, LBL-PKT- 4 of Lawrence Berkeley Laboratory and generated attack datasets. The experimental results show the feasibility of our approach in terms of detection accuracy and speed...|$|R
30|$|The {{way that}} the {{high-quality}} images are processed {{by all of the}} algorithms that are present in the proposed ISP chain means that there are no iterations in the algorithm to reduce the execution time of the real-time budget [1]. While the basic idea of the algorithm is maintained, the operations in the algorithm have been simplified for easy parallelization on the SIMD architecture; in addition, heavy memory accesses and excessive <b>computational</b> <b>overheads</b> are reduced by limiting the operational ranges. Each complicated special operation is replaced by a simple operation that performs a similar function and the result was verified by experiments.|$|R
50|$|RNA {{may also}} be {{acquired}} from public databases, such as GenBank, RefSeq, 1000 Plants (1KP) and 1KITE. Public databases potentially offer curated sequences which can improve inference quality and avoid the <b>computational</b> <b>overhead</b> associated with sequence assembly.|$|E
50|$|Interpolative {{formulas}} {{attempt to}} provide a good fit to tabular values ofair mass using minimal <b>computational</b> <b>overhead.</b> The tabularvalues, however, must be determined from measurements or atmosphericmodels that derive from geometrical and physical considerations of Earth andits atmosphere.|$|E
50|$|In computing, {{software}} or data formats {{that are}} native {{to a system}} are those that the system supports with minimal <b>computational</b> <b>overhead</b> and additional components. This word is used in such terms as native mode or native code.|$|E
40|$|A {{three-dimensional}} unstructured mesh Reynolds averaged Navier-Stokes solver is described. Turbulence is simulated using {{a single}} field-equation model. <b>Computational</b> <b>overheads</b> are minimized {{through the use}} of a single edge-based data-structure, an efficient multigrid solution technique, and the use of multi-tasking on shared memory multi-processors. The accuracy and efficiency of the code are evaluated by computing two-dimensional flows in three-dimensions and comparing with results from a previously validated two-dimensional code which employs the same solution algorithm. The feasibility of computing three-dimensional turbulent flows on grids of several million points in less than two hours of wall clock time is demonstrated...|$|R
40|$|Incremental {{inference}} of even linear {{languages in}} the limit from positive data is considered. Since {{it is known}} that the family of even linear languages is not inferable from positive data, we propose a subfamily of even linear languages which is inferable from positive data in polynomial time. We consider the family of Terminal Distinguishable Even Linear Languages(TDELL) for which an inference algorithm is known. A tabular approach is developed for the inference of TDELL and shown to possess important advantages over the existing algorithm. These are minimization of <b>computational</b> <b>overheads</b> and adaptability for online inferencing...|$|R
40|$|Abstract—The suffix array and Burrows-Wheeler Transform are {{critical}} index structures in next generation sequence analysis. The construction of such index structures for mammalian-sized genomes can take thousands of seconds (i. e. tens of minutes). Its construction {{is complicated by}} <b>computational</b> <b>overheads</b> that coming from irregular or complex memory-access patterns. This paper rigorously characterizes the execution profile of the SA-IS algorithm in order to guide its optimization. The resulting optimized SA-IS, which we refer to as sais-opt, outperforms previous implementations of SA-IS as well as “best in practice” algorithms, when applied to large DNA strings. Keywords—suffix array; Burrows-Wheeler Transform; irregular memory acces...|$|R
