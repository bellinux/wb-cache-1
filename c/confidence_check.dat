6|33|Public
50|$|The {{diagnostic}} microcomputer (based {{around the}} Zilog Z80) was embedded within the CPU. Its functions included running a system <b>confidence</b> <b>check</b> when power is first applied, bootstrapping the CPU, and taking control should an unrecoverable control store parity error be detected. It {{could also be}} used to load new microcode dynamically whilst the machine was running. An RS-232C interface was provided to which a terminal could be attached. Extensive diagnostics could then be run in conjunction with special microcode to perform fault analysis {{in the event of a}} system failure. Problems could usually be isolated to one or two integrated circuits.|$|E
30|$|The {{post-processing}} step utilised in {{this evaluation}} includes <b>confidence</b> <b>check</b> and speckle removal {{for all the}} approaches except for ADAPT in which case no post-processing is performed. This is {{because it is not}} possible to adapt the <b>confidence</b> <b>check</b> to consider whole scanlines rather than separate pixels.|$|E
30|$|Several {{disparity}} refinements {{are also}} {{discussed in the}} original article [2] {{but they are not}} present in the implementation utilised in this study. Instead <b>confidence</b> <b>check</b> and speckle removal are used as required as per Section 2.5.|$|E
30|$|We {{investigate}} {{the results of}} the chosen techniques over various automotive scenes. All of the presented disparity maps (in Figures 11, 12 and 13) have been post-processed with <b>confidence</b> <b>checking</b> and speckle removal (Section 2.5) to eliminate disparity noise.|$|R
5000|$|Frank Abagnale (born 1948), former <b>confidence</b> trickster, <b>check</b> forger, skilled impostor {{and escape}} artist ...|$|R
25|$|The National Assembly is the {{legislature}} and has oversight authority. The National Assembly consists of fifty elected members, who are chosen in elections held every four years. Since the parliament can conduct inquiries into government actions and pass motions of no <b>confidence,</b> <b>checks</b> and balances are robust in Kuwait. The parliament can be dissolved under a set of conditions based on constitutional provisions. The Constitutional Court and Emir both {{have the power to}} dissolve the parliament, although the Constitutional Court can invalidate the Emir's dissolution.|$|R
30|$|The visual {{results in}} Figures 5 and 6 present two {{different}} points within the virtual test sequence. The first one illustrates a clear road between the buildings whilst the other representing a vehicle approaching a junction. The resulting disparity maps have been post-processed using <b>confidence</b> <b>check</b> and speckle removal {{in order to}} filter out noise for all five algorithms under consideration.|$|E
40|$|All {{software}} verification techniques, from theorem proving to testing, share the common goal {{of establishing a}} program’s correctness with both (1) {{a high degree of}} confidence and (2) a low cost to the user, two criteria in tension with one another. Theorem proving offers the benefit of high confidence, but requires significant expertise and effort from the user. Testing, on the other hand, can be performed for little cost, but low-cost testing does not yield high confidence in a program’s correctness. Although many static analyses can quickly and with high <b>confidence</b> <b>check</b> a program’s conformance to a specification, they achieve these goals by sacrificing the expressiveness of the specification. To date, static analyses have been largely limited to the detection of shallow properties that apply to a very large class of programs, such as absence of array-bound errors and conformance to API usage conventions. Few static analyses are capable of checking strong specifications, specifications whose satisfaction relies upon the program’s precise behavior...|$|E
40|$|All {{software}} verification techniques, from theorem proving to testing, share the common goal {{of establishing a}} program's correctness with both (1) {{a high degree of}} confidence and (2) a low cost to the user, two criteria in tension with one another. Theorem proving offers the benefit of high confidence, but requires significant expertise and effort from the user. Testing, on the other hand, can be performed for little cost, but low-cost testing does not yield high confidence in a program's correctness. Although many static analyses can quickly and with high <b>confidence</b> <b>check</b> a program's conformance to a specification, they achieve these goals by sacrificing the expressiveness of the specification. To date, static analyses have been largely limited to the detection of shallow properties that apply to a very large class of programs, such as absence of array-bound errors and conformance to API usage conventions. Few static analyses are capable of checking strong specifications, specifications whose satisfaction relies upon the program's precise behavior. This thesis presents a new program-analysis framework that allows a procedure in an object-oriented language to be automatically checked, with high confidence, against a strong specification of its behavior. The framework is based on an intermediate relational representation of code and an analysis that examines all executions of a procedure up to a bound {{on the size of the}} heap and the number of loop unrollings. If a counterexample is detected within the bound, it is reported to the user as a trace of the procedure, though defects outside the bound will be missed. (cont.) Unlike testing, many static analyses are not equipped with coverage metrics to detect which program behaviors the analysis failed to exercise. Our framework, in contrast, includes such a metric. When no counterexamples are found, the metric can report how thoroughly the code was covered. This information can, in turn, help the user change the bound on the analysis or strengthen the specification to make subsequent analyses more comprehensive. by Gregory D. Dennis. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2009. Cataloged from PDF version of thesis. Includes bibliographical references (p. 131 - 138) ...|$|E
50|$|A {{video monitor}} also called a {{broadcast}} monitor, broadcast video monitor, broadcast reference monitor or just reference monitor, is a display device {{similar to a}} television set, used to monitor the output of a video-generating device, such as playout from a video server, IRD, video camera, VCR, or DVD player. It {{may or may not}} have professional audio monitoring capability. Unlike a television set, a video monitor has no tuner (television) and, as such, is unable independently to tune into an over-the-air broadcast like a television receiver. One common use of video monitors is in television stations, television studios, production trucks and in outside broadcast vehicles, where broadcast engineers use them for <b>confidence</b> <b>checking</b> of analog signal and digital signals throughout the system.|$|R
40|$|We {{investigate}} {{the use of}} Hi data to resolve the near/far ambiguity in kinematic distances of massive young stellar object (MYSO) candidates. Kinematic distances were obtained from 13 CO 1 - 0 (and N 2 H +) spectral line observations with the Mopra Telescope towards 94 candidates selected from the Red MSX Source (RMS) survey in the fourth Galactic quadrant (282 ◦ < l < 350 ◦). Hi data from the Southern Galactic Plane Survey (SPGS) was {{used in conjunction with}} the Hi self-absorption technique to determine the near or far distance. We resolved the kinematic distance ambiguity to 70 % of the sources. We can also simultaneously solve for any multiple line-of-sight component sources. We discuss {{the advantages and disadvantages of}} this technique in comparison with other methods, and also perform <b>confidence</b> <b>checks</b> on the reliability of using the Hi self-absorption technique. We examined the projected location of these sources in both the Galactic plane and longitude-velocity diagrams to ascertain any recognisable spiral arm pattern. Although no obvious spiral pattern was found when compared to that proposed b...|$|R
40|$|Abstract. Elderly {{people are}} a great {{repository}} of knowledge, the majority of which has never been gathered by formal means. In this paper we introduce an application of multi-agent systems to support knowledge acquisition from this rich repository knowledge which is only available from elderly and experienced people. Our system provides the opportunity to complement {{different versions of the}} same knowledge produced in an extensive geographical and cultural region with the main objective of supporting Cultural Heritage. Users without much technological knowledge can search or leave information about some type of knowledge. Then, the system behaves like a swarm of bees, in this way the bee-like agents process the user contributions and the knowledge emerges from the system. Queen-like agents, honey-bee, drones and foragers have different roles inside the hive: looking for information resemblances, computing information <b>confidence,</b> <b>checking</b> the necessity of knowledge validation, and updating user’s reliability. The system’s feasibility has been tested on the specific area of ethnobotany, which concerns the ways in which specific societies name and classify plants. ...|$|R
30|$|In {{the case}} of the {{synthetic}} datasets, the fitted parameter estimates were close to the input parameters. In none of the cases where the <b>confidence</b> intervals were <b>checked</b> against these input parameters, a contradiction was found.|$|R
5000|$|A {{visual display}} unit, {{computer}} monitor or just display, {{is a piece}} of electrical equipment, usually separate from the computer case, which displays visual images without producing a permanent computer record. A display device was usually either a CRT in the 1980s, but by the 2000s, flat panel displays such as a TFT LCD had largely replaced the bulkier, heavier CRT screens. Multi-monitor setups are quite common in the 2010s, as they enable a user to display multiple programs at the same time (e.g., an email inbox and a word processing program). The display unit houses an electronic circuitry that generates its picture from signals received from the computer. Within the computer, either integral to the motherboard or plugged into it as an expansion card, there is pre-processing circuitry to convert the microprocessor's output data to a format compatible with the display unit's circuitry. The images from computer monitors originally contained only text, but as graphical user interfaces emerged and became common, they began to display more images and multimedia content. The term [...] "monitor" [...] is also used, particularly by technicians in broadcasting television, where a picture of the broadcast data is displayed to a highly standardized reference monitor for <b>confidence</b> <b>checking</b> purposes.|$|R
40|$|The paper {{concerns}} the fixed-width confidence intervals for location based on M- estimators in the location model. A robust three-stage procedure is proposed and its asymptotic properties are studied. The {{performance of the}} procedure depends on some tuning parameters. Their effect on the proposed <b>confidence</b> interval is <b>checked</b> together with the overall behaviour of the procedure in a simulation study...|$|R
40|$|Abstract — Deeply scaled CMOS {{circuits}} {{are increasingly}} sus-ceptible to transient faults and soft errors; emerging post-CMOS devices {{can be more}} vulnerable, sometimes exhibiting erratic errors of arbitrary duration. Applying timing and supply voltage margin is wasteful and becoming ineffective, and conventional checking and sparing techniques provide only a limited error coverage against widely varying errors. We propose a confidence-driven computing (CDC) model for an adaptive protection against nondeterministic errors. The CDC model employs fine-grained temporal redundancy and <b>confidence</b> <b>checking</b> for a faster adaptation and tunable reliability. The CDC model can be extended to deeply scaled CMOS circuits that are mainly affected by transient faults and soft errors, where an early checking (EC) technique {{can be used to}} perform independent error checking for more flexibility and better performance. To evaluate the CDC model, we apply a sample-based field-programmable gate array emulation along with real-time error injection. The CDC model is shown to adapt to fluctuating error rates and enhance the system reliability by effectively trading off performance. To evaluate the EC technique at a finer time scale, we create a new event-based simulation to capture path delay distribution, error model, and their interactions. The EC technique improves the system reliability by more than four orders of magnitude when errors are of short duration. Both the CDC model and the EC technique are synthesized in a 45 -nm CMOS technology for cost estimates: 1) the area overhead is as low as 12 % and 2) energy overhead can be limited to 19 %. Index Terms — Error detection, error simulation, field-programmable gate array (FPGA) emulation, reliability, resilient design. I...|$|R
40|$|In {{this paper}} {{parametric}} inference for the renewal function is considered. An algorithm is presented for computing parametric confidence intervals for the renewal function. The partial derivatives needed in this algorithm are obtained by solving {{a pair of}} coupled integral equations. The validity of asymptotic <b>confidence</b> intervals is <b>checked</b> via numerical experiments. Some extensions of this algorithm are discussed. Key Words: Integral equation, maximum likelihood estimation, parametric confidence interval, renewal functio...|$|R
2500|$|Frank William Abagnale Jr. ( [...] ; born April 27, 1948) is an American {{security}} consultant {{known for}} his history as a former <b>confidence</b> trickster, <b>check</b> forger, and impostor {{between the ages of}} 15 and 21. He {{became one of the most}} famous impostors ever, claiming to have assumed no fewer than eight identities, including an airline pilot, a physician, a U.S. Bureau of Prisons agent, and a lawyer. He escaped from police custody twice (once from a taxiing airliner and once from a U.S. federal penitentiary), before he was 21 years old. He served less than five years in prison before starting to work for the federal government. He is currently a consultant and lecturer for the FBI academy and field offices. He also runs Abagnale & Associates, a financial fraud consultancy company.|$|R
40|$|Let (X 1, Y 1),..., (Xn, Yn) be i. i. d. rvs and let l(x) be {{the unknown}} p-quantile {{regression}} curve of Y on X. A quantile-smoother ln(x) is a localised, nonlinear estimator of l(x). The strong uniform consistency rate is established under general conditions. In many applications {{it is necessary}} to know the stochastic fluctuation of the process {ln(x) - l(x) }. Using strong approximations of the empirical process and extreme value theory allows us to consider the asymptotic maximal deviation sup 06 x 61 |ln(x) -l(x) |. The derived result helps in the construction of a uniform confidence band for the quantile curve l(x). This confidence band can be applied as a model check, e. g. in econometrics. An application considers a labour market discrimination effect. Quantile Regression, Consistency Rate, <b>Confidence</b> Band, <b>Check</b> Function, Kernel Smoothing, Nonparametric Fitting...|$|R
30|$|Due to {{the random}} {{component}} {{added to the}} synthetic datasets, {{it can not be}} expected that the input parameters used for the degradation model will be exactly reproduced by the fitting procedure. However, <b>confidence</b> intervals were <b>checked</b> against these input parameters. For all evaluations without metabolites, confidence intervals obtained with KinGUII, gmkin and CAKE included the input parameters used to generate the datasets. The same check was also made for the evaluations with metabolites for the gmkin software. For all twelve synthetic datasets, these included the input parameters and were therefore consistent with them.|$|R
40|$|We {{consider}} plots {{which can}} be used to check whether a distribution is symmetric about some point and which give information about possible deviations from symmetry. Simultaneous <b>confidence</b> bands for <b>checking</b> the accuracy of the plots are derived and compared. Tests that are related to the plots and confidence bands are considered and their power functions are compared using Monte Carlo methods. As tests of normality, the powers of two of these tests compare favourably with the power of the Shapiro-Wilk statistic. Some key words: Butler statistic; Confidence band; Skewness; Symmetry; Test for normality. 1...|$|R
40|$|AbstractApriori Algorithm {{is one of}} {{the most}} {{important}} algorithm which is used to extract frequent itemsets from large database and get the association rule for discovering the knowledge. It basically requires two important things: minimum support and minimum <b>confidence.</b> First, we <b>check</b> whether the items are greater than or equal to the minimum support and we find the frequent itemsets respectively. Secondly, the minimum confidence constraint is used to form association rules. Based on this algorithm, this paper indicates the limitation of the original Apriori algorithm of wasting time and space for scanning the whole database searching on the frequent itemsets, and present an improvement on Apriori...|$|R
60|$|Mr. Wallinger was one {{of these}} men. His high {{character}} and strong purse were always in the front rank in the hour of danger. His support in the House was limited to his votes; but in other places equally important, at a meeting at a political club, or in Downing Street, he could find his tongue, take what is called a 'practical' view of a question, adopt what is called an 'independent tone,' reanimate <b>confidence</b> in ministers, <b>check</b> mutiny, and set a bright and bold example to the wavering. A man of his property, and high character, and sound views, so practical and so independent, this was evidently the block from which a Baronet should be cut, and in due time he figured Sir Joseph.|$|R
40|$|Apriori Algorithm {{is one of}} {{the most}} {{important}} algorithm which is used to extract frequent itemsets from large database and get the association rule for discovering the knowledge. It basically requires two important things: minimum support and minimum <b>confidence.</b> First, we <b>check</b> whether the items are greater than or equal to the minimum support and we find the frequent itemsets respectively. Secondly, the minimum confidence constraint is used to form association rules. Based on this algorithm, this paper indicates the limitation of the original Apriori algorithm of wasting time and space for scanning the whole database searching on the frequent itemsets, and present an improvement on Apriori. Comment: 7 pages, 4 figures, 7 tables, published in Elsevier Procedia in Computer Science and presented in ICICT 201...|$|R
6000|$|In {{those days}} he seemed much changed to such as knew him well. Instead of the delicate, detached, {{slightly}} humorous suavity {{which he had}} accustomed people to expect from him, the dry kindliness which seemed at once to <b>check</b> <b>confidence</b> and yet to say, 'If you choose to tell me anything, I should never think of passing judgment on you, whatever you have done'--instead of that rather abstracted, faintly quizzical air, his manner had become absorbed and gloomy. He seemed to jib away from his friends. His manner at the [...] "Pen and Ink" [...] was wholly unsatisfying to men who liked to talk. He {{was known to be}} writing a new book; they suspected him of having [...] "got into a hat"--this Victorian expression, found by Mr. Balladyce in some chronicle of post-Thackerayan manners, and revived by him in his incomparable way, as who should say, 'What delicious expressions those good bourgeois had!' now flourished in second childhood.|$|R
6000|$|The horses trotted {{and swung}} {{up over the}} slope, turning gradually, evidently to make a wide detour round the Ford, until Lucy's back was toward the monuments. Before her {{stretched}} the bleak, barren, dark desert, and through the opaque gloom she could see nothing. Lucy knew she was headed for the north, toward the wild canyons, unknown to the riders. Cordts and his gang hid in there. What might not happen if the Creeches fell in with Cordts? Lucy's <b>confidence</b> sustained a <b>check.</b> Still, she remembered the Creeches were like Indians. And what would Slone do? He would ride out on her trail. Lucy shivered for the Creeches if Slone ever caught up with them, and remembering his wild-horse-hunter's skill at tracking, and the fleet and tireless Wildfire, she grew convinced that Creech could not long hold her captive. For Slone would be wary. He would give no sign of his pursuit. He would steal upon the Creeches in the dark and-- Lucy shivered again. What an awful fate had been that of Dick Sears! ...|$|R
3000|$|... {{within the}} dataset. Thus, D-MIAT could {{potentially}} generate two features for every attribute based on lines 7 and 12 of the Algorithm 1. In our experiments, we defined supp equal to 10 % {{of the training}} data. Three confidence values, conf, were checked. The first value was conf=Entropy(0), meaning the cut yielded a completely decisive indication {{for one of the}} target classes and thus yielded zero entropy as per the first type of confidence mentioned above. We also considered 2 types of lift confidence: conf= Lift(1.5) and conf=Lift(2.0). As per these <b>confidence</b> levels, we <b>checked</b> if the cut yielded a stronger indication for one of the target classes as measured per their lift values. Potentially, both of these lift thresholds could be satisfied. For example, assume a cut yielded a lift of 3, then both confidence thresholds would consider this cut significant and generate a new feature accordingly. We also considered the possibility that the cuts could be added cumulatively and thus overlapping cuts could be added based on combinations of these 3 different thresholds. Conversely, if all thresholds used were not met, no cuts were created for a given attribute.|$|R
40|$|Repeated {{checking}} {{has been}} demonstrated to lead to reductions in memory confidence in several previous studies using student and clinical samples. This process of reduced confidence in memory and detail for memory, are thought to arise from the inhibition of perceptual processing that develops during repeated checking. Our research investigated whether reduced memory <b>confidence</b> from repeated <b>checking</b> could be attenuated {{through the use of}} novel stimuli during the repeated checking task. Three groups were generated through random assignment of 65 undergraduate students. As seen in previous research, individuals who repeatedly checked a stimulus (a virtual stovetop) showed reduced memory confidence, vividness, and detail, when compared with individuals who repeatedly checked a different stimulus. A third group in which the colour of the repeatedly-checked stovetop changed every five trials showed no significant decline in memory confidence between the pre-test and post-test. Results suggest that increased memory distrust can be ameliorated through the use of stimuli with characteristics that are novel and distinctive. Findings are discussed {{in the context of the}} existing model of repeated <b>checking</b> and memory <b>confidence,</b> and implications for treatment methods are presented. Griffith Health, School of Applied PsychologyFull Tex...|$|R
40|$|Objective: This {{study was}} {{conducted}} to examine the precision of the NST-NAET ® (Neuromuscular Sensitivity Testing of Nambudripad’s Allergy Elimination techniques) in repeated performances. The NST-NAET ® was also studied to determine its reliability among randomly selected examiners about the accuracy of their own performances in testing. Study Design and Methodology: This study used a sample of both experienced and inexperienced practitioners in a totally blinded and randomized testing environment. Seven NAET ® practitioners were randomly selected from a group of 50 health practitioners who responded to the invitation to participate in this study. None of them was aware {{of the nature of the}} study prior to the selection of the participants. Three out of the seven were randomly selected again as examiners and the other four were assigned to be the subjects. Four allergens (Soybean, MSG, Clorox and distilled water) were tested on these four subjects by each of the three examiners and repeated the testing on the same subjects using the same allergens two more times by the same examiner. The total number of tests performed was 48. Results: Statistical analysis was performed to assess the precision and accuracy of inter and intra-examiner reliability, percentage of agreement and degree of <b>confidence.</b> Data was <b>checked</b> for normality. The assumption...|$|R
40|$|This work {{is related}} to the {{development}} of terminal antennas for satellite communications in Ka-band (down-link 19. 70 - 20. 20 GHz and up-link- 29. 50 - 30. 0 GHz). Innovative designs are called for {{in order to meet the}} stringent requirements of ATM-Sat terminal antenna like high frequency operating band, compactness, light weight and above all low cost. A multilayer active array antenna appears to be the most suitable option for this application. Basically, it would consist of a passive patch elements array and several circuit layers with passive feeding networks and active circuit layers. To make this planar structure operate in a proper way, the indispensable transfer of power between adjacent layers must rely on robust and highly efficient 3 D transitions. Further, for ease of mounting of active elements and due to their less dispersive behaviour at high frequencies, coplanar waveguides are chosen as transmission line media. The present work is concentrated on this aspect of 3 D transition between coplanar lines. An extensive survey was carried out to collect information on different types of coplanar waveguides (CPW), modes of operation, parasitic modes and suppression techniques, CPW discontinuities, multilayer circuits, application of CPWs in antenna and MMIC circuits. Apart from these topics, thorough literature survey was carried out for 3 D CPW transitions. It includes various categories of 3 D transitions involving two coplanar lines or one coplanar line plus another type of transmission line like a microstrip or a stripline. After literature survey, work was concentrated on software simulation study. Initially a few examples from literature were simulated to build the <b>confidence</b> and <b>check</b> the applicability of the simulation tools. After gaining enough experience, the structures of interest like conductor-backed coplanar waveguides (CBCPW) -to-cover-shielded-CBCPW transitions by direct electrical contact and electromagnetic coupling were studied in detail. An exhaustive parametric study was done to maximise the transmission efficiency and bandwidth of operation of the transitions. The influence of several parameters like substrate height and its dielectric constant, via holes, air-bridge etc was investigated. The techniques for reducing the effect of parallel-plate modes were examined in these transitions...|$|R
40|$|Purpose – {{to improve}} the tactics for {{surgical}} management of bicondylar tibia fractures to gain better outcomes.   Materials and methods. The authors analyzed outcomes of surgical management of 69 patients within 36 months after the procedures. Two comparison groups were created to assess {{the effectiveness of the}} proposed techniques: the main group of 27 patients (39. 1 %) and control group of 42 patients (60. 9 %). In the main group the advanced and new techniques were applied (two-staged protocol of surgical correction, internal fixation with joint distraction, a combination of new L-shaped external and L-shaped internal approaches, bone grafting with b-TCP, carbon nanostructure implant and «Osteomatriks» xenograft). In the control group the conventional internal fixation was used. The difference in tibiofemoral (ΔFTA) and plateau-diaphyseal (ΔPDA) angles at various follow up stages were checked to evaluate reduction stability. P. S. Rasmussen score was used to assess the functional status and life quality of patients. Statistical methods of evaluation included parametric and non-parametric test to <b>check</b> <b>confidence</b> value of variances. Results. In 36 months postoperatively ∆FTA > 5 ° in the main group was observed 1. 97 times less than in control group (23. 1 % and 45. 5 % respectively); ∆PDA > 5 ° in the main group was observed 1. 66 times less than in control group (30. 8 % and 51. 5 % respectively). Total number of excellent and good results the main group 36 months postoperatively according to P. S. Rasmussen score was reported as 1. 81 times higher than in the control group (50 % and 27. 6 % respectively).   Conclusion. The paper proves the efficiency of the proposed surgical procedure for bicondylar tibia fractures management. </p...|$|R
40|$|Changes {{in memory}} and {{concerns}} regarding memory performance {{are common in}} older people, with many fearing developing dementia. Older people both with and without objective memory impairment may engage in compensatory strategies to reduce feelings of uncertainty, including checking or a reliance on memory aids. However, {{a number of studies}} have demonstrated that checking may paradoxically lead to reductions in metamemory (memory confidence, vividness and detail) as well as potential reductions in memory accuracy. The present study aimed to build upon previous research by adapting a stove paradigm developed by Radomsky, Gilchrist & Dussault (2006) to investigate the effects of repeated ‘relevant’ and ‘irrelevant’ checking on memory accuracy and metamemory in 20 community dwelling older people without memory problems, as well as a smaller sample of 14 individuals with mild cognitive impairment (MCI). The study employed 2 x 2 mixed factorial experimental designs for both samples. The independent variable was checking type (relevant checking and irrelevant checking). Participants were randomly assigned to either a ‘relevant checking’ or an ‘irrelevant checking’ condition. Participants in the ‘relevant checking’ condition completed 15 ‘checks’ of a non-functional replica stove while those in the ‘irrelevant checking’ condition completed 15 ‘checks’ of a dosette box, before completing a final checking trial of the stove. The dependent variables were measures of memory accuracy and metamemory (confidence, vividness and detail) assessed at two time points (pre-checking and post-checking). Consistent with earlier findings, repeated relevant checking led to significant decline in memory confidence, vividness and detail compared to the irrelevant checking condition for the older adult sample. The MCI sample showed significant decline in memory <b>confidence</b> following repeated <b>checking</b> although declines in vividness and detail did not reach significance. No change was observed in memory accuracy in either sample. The clinical and theoretical implications of this finding are discussed. ...|$|R
40|$|Objective: This study {{aimed to}} extend current {{research}} into cognitive models of obsessive–compulsive disorder (OCD) in a pediatric sample {{by examining the}} impact of perceived responsibility on memory confidence, intolerance of uncertainty (IU) and checking urge using an experimental design to manipulate perceived responsibility. It was hypothesised that the high responsibility condition would result in higher ratings of responsibility, lower memory confidence and higher IU, which would also result in higher ratings on urge to check. Moreover, it was hypothesised that adolescents would report significantly higher ratings of responsibility than children. Finally, it was hypothesised {{that the effect of}} perceived inflated responsibility on the urge to check in a high responsibility condition would be mediated by IU. Method: Twenty-seven children and adolescents diagnosed with OCD completed an experimental cognitive appraisal task (CAT) in which they heard two standardised vignettes presented in counterbalanced order; one in which participants were responsible and one in which they were not responsible for preventing harm to a friend's pet cat. Memory <b>confidence,</b> IU and <b>checking</b> urge were assessed after each scenario using Likert scales. Results: The manipulation of perceived responsibility was successful with children and adolescents rating increased responsibility in the high compared with the low responsibility scenario. There were no differences across high and low responsibility conditions, however, in ratings of memory confidence, IU or the urge to check. There were no significant age-related differences; however, there was a trend for adolescents to report higher ratings across all variables. Finally, the relationship between perceived inflated responsibility and the urge to check was not mediated by IU. Conclusions: Responsibility is not related to ratings of memory confidence, IU or the urge to check in a pediatric sample, suggesting that biases of responsibility may not be central to the formulation of childhood OCD. Results are discussed in terms of implications for cognitive formulations and cognitive approaches to treatment in pediatric OCD. 15 page(s...|$|R
40|$|This study {{aimed to}} extend current {{research}} into cognitive models of obsessive- compulsive disorder (OCD) in a pediatric sample {{by examining the}} impact of perceived responsibility on memory confidence, intolerance of uncertainty (IU) and checking urge using an experimental design to manipulate perceived responsibility. It was hypothesised that the high responsibility condition would result in higher ratings of responsibility, lower memory confidence and higher IU, which would also result in higher ratings on urge to check. Moreover, it was hypothesised that adolescents would report significantly higher ratings of responsibility than children. Finally, it was hypothesised {{that the effect of}} perceived inflated responsibility on the urge to check in a high responsibility condition would be mediated by IU. Method: Twenty-seven children and adolescents diagnosed with OCD completed an experimental cognitive appraisal task (CAT) in which they heard two standardised vignettes presented in counterbalanced order; one in which participants were responsible and one in which they were not responsible for preventing harm to a friend's pet cat. Memory <b>confidence,</b> IU and <b>checking</b> urge were assessed after each scenario using Likert scales. Results: The manipulation of perceived responsibility was successful with children and adolescents rating increased responsibility in the high compared with the low responsibility scenario. There were no differences across high and low responsibility conditions, however, in ratings of memory confidence, IU or the urge to check. There were no significant age-related differences; however, there was a trend for adolescents to report higher ratings across all variables. Finally, the relationship between perceived inflated responsibility and the urge to check was not mediated by IU. Conclusions: Responsibility is not related to ratings of memory confidence, IU or the urge to check in a pediatric sample, suggesting that biases of responsibility may not be central to the formulation of childhood OCD. Results are discussed in terms of implications for cognitive formulations and cognitive approaches to treatment in pediatric OCD. Griffith Health, School of Applied PsychologyFull Tex...|$|R
40|$|The {{objective}} {{of this research is}} to identify and analyze sexual characters in mammalian skeletons in order to develop new methods for sex determination of archaeological animal remains. The study begins with an examination of the evolutionary and developmental framework of sexual characters, and a review of the current methods used for sex classification of animal remains. The materials and methods used in this research have been designed to locate the tertiary sexual characters in the fox, dog, pig, deer, and sheep skeletons. Morphometric and osteometric analyses of 11 elements of the post-cranial skeleton (atlas, axis, glenoid, proximal humerus, distal humerus, proximal metacarpus, innominate, proximal femur, distal tibia, astragalus, proximal metatarsus) have been conducted. Shape and size differences of bones have been analyzed using the F- test of variance and canonical variates analysis for shape variables, and discriminant analysis and the two-sample t- test for metrical data, to determine significance. Eigenshape analysis, an outline-based form of morphometrics, has been implemented for comparing bone shapes. Score plots have been produced by comparing eigenshape scores to indicate shape trends formed by the male and female bone groups. Mean shapes, calculated by the eigenshape program, have been superimposed so that differences in bone morphology between the sexes can be identified. Two alternative methods are introduced in this study, the Mean Shape Method for identifying sexual dimorphic or tnmorphic (with castrates) bones, and the Table Test for sexing canid humeri. These methods have been tested in a blind test to <b>check</b> <b>confidence</b> of sex classification. The new methods have been applied to bone samples from archaeological sites: Silchester for dog remains, Star Carr for red deer remains, and Canterbury for sheep remains. The results suggest that dogs buried at Silchester were female individuals, that predominantly male deer were hunted at Star Can, and that castration of sheep was practiced at Canterbury. Overall, the alternative methods developed here can aid in identifying the sex of archaeological bones more effectively...|$|R
40|$|Khartoum's State has two public {{wastewater}} treatment plants, of which only one is purely waste stabilization ponds system, i. e. Soba treatment plant. As most researches have proved, the removal {{rates of the}} plant is no satisfactory, especially coliforms'. However, most of these studies {{are concerned with the}} redesign of the plant or replacing it with a new one, while only a few discuss the factors affecting the treatment process or how to remedy its imperfections. As proper analysis and environmental control can improve almost all wastewater biological treatments; this study basically aims to decide the cause /effect relationship of the removal rates of organic and suspended solid loads {{with some of the most}} important environmental conditions, in addition to major constituents' concentrations in the wastewater. This relation can be used to enhance the removal rates of the ponds' system. As the removal efficiency for suspended solids had dropped to as low as 60 %, while BOD removal stays above 75 % most of the time, compared to the 80 % TSS and 85 % BOD design removal; the treatment is in desperate need for modification. Based on the effluent/ influent concentrations for a period of 73 months' averages, the relation is expressed as statistically derived equations (deterministic model) using multiple regression analysis, while the strength of it is measured using correlation coefficients and ANOVA tables technique. After eliminating outlier points, the model was drawn with ± 95 % <b>confidence</b> interval to <b>check</b> its reliability. For further confirmation of the soundness of the stochastic model, it was converted into a mechanistic /phenomenological model using Thirumurthi graphical solution of the well known Wehner & Wilhem equation. The resulting model was compared with the old model and the results were satisfactory in accordance to approach and precision. After the compatibility of the two models is proved, it can be used to improve the performance of the plant by controlling the predominating factors on the treatment to improve the effluent's quality, in addition to improving design of similar plants making use of current experience. As sun shine, dissolved oxygen, pH level, and temperature are the factors with the higher correlation, and consequently having more influence on the quality than the rest of smaller correlation; they can be controlled to v achieve the maximum possible efficiency of the plant using simple inexpensive methods i. e. scum removal, equalization tanks, algal harvesting, etc. As the simplicity and inexpensiveness of stabilization ponds technique is well known, the remediation and quality enhancement has been proven to be no exception. However, researches and experimental work is of major importance in order to find the extent of improvement achieved from the recommended alterations...|$|R
