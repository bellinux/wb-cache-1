10000|10000|Public
5|$|In practice, {{interpolation}} {{search is}} slower than binary search for small arrays, as interpolation search requires extra <b>computation.</b> Although its time complexity grows {{more slowly than}} binary search, this only compensates for the extra <b>computation</b> for large arrays.|$|E
5|$|Each {{background}} processor {{consisted of}} a <b>computation</b> section, a control section and local memory. The <b>computation</b> section performed 64-bit scalar, floating point and vector arithmetic. The control section provided instruction buffers, memory management functions, and a real-time clock. 16 kwords (128 kbytes) of high-speed local memory was incorporated into each background processor for use as temporary scratch memory.|$|E
5|$|In {{the early}} days, GPGPU {{programs}} used the normal graphics APIs for executing programs. However, several new programming languages and platforms {{have been built}} to do general purpose <b>computation</b> on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for <b>computation</b> in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.|$|E
30|$|According to Table  3, Chen et al.’s {{protocol}} needs eight {{hash function}} <b>computations,</b> Yoon el at.’s needs thirteen hash function <b>computations,</b> Yuan et al.’s also need thirteen hash function <b>computations,</b> Das’s protocol needs six hash function <b>computations.</b> And Benenson et al.’s protocol needs 2 n hash function <b>computations</b> and 3 n[*]+[*] 1 modular exponential <b>computations</b> [22]. Ohood et al.’s biometric authentication protocol needs four RC 5 <b>computations</b> and ten hash function <b>computations.</b> Yeh et al.’s protocol needs fifteen hash function <b>computations,</b> four elliptic curve point addition <b>computations,</b> ten elliptic curve point multiply <b>computations</b> and two elliptic curve polynomial <b>computations.</b> Wenbo et al.’s protocol needs eighteen hash function <b>computations</b> and seven elliptic curve point multiply <b>computations.</b> Our proposed protocol needs eighteen hash function <b>computations,</b> four elliptic curve point multiply <b>computations,</b> eleven AES <b>computations</b> and six pairing <b>computations.</b> Although our protocol needs more <b>computations</b> than their protocols, their protocols suffer from security issues or need complicated biometric equipments. Our protocol addressed {{these issues and}} provides better security and more security services than the other related protocols.|$|R
5000|$|Often {{researchers}} use {{results from}} both probabilistic symbolic <b>computations</b> and numerical <b>computations</b> to prove theorems. The notation Theorem Star {{is often used}} to indicate the reliance on such <b>computations.</b> [...] Our methods include probabilistic symbolic <b>computations</b> and numerical <b>computations.</b> Though they have been carefully tested and produce completely reproducible results, they are technically only true with high probability, or up to the numerical precision of the computers we use. To indicate reliance on such <b>computations,</b> we designate those theorems, corollaries, and propositions with a star.|$|R
40|$|Numerical <b>computations</b> form an {{essential}} part of almost any real-world program. Traditional approaches are restricted domains isomorphic to N, more recent works study termination of integer <b>computations.</b> Termination of <b>computations</b> involving real numbers is cumbersome and counter-intuitive due to rounding errors and implementation conventions. We present a novel technique that allows us to prove termination of such <b>computations.</b> Our approach extends the previous work on termination of integer <b>computations...</b>|$|R
5|$|In {{the uniform}} cost model (suitable for {{analyzing}} {{the complexity of}} gcd calculation on numbers that fit into a single machine word), {{each step of the}} algorithm takes constant time, and Lamé's analysis implies that the total running time is also O(h). However, in a model of <b>computation</b> suitable for <b>computation</b> with larger numbers, the computational expense of a single remainder <b>computation</b> in the algorithm can be as large as O(h2). In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also O(h2). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.|$|E
5|$|The <b>computation</b> {{of complex}} powers is {{facilitated}} by converting the base w to polar form, {{as described in}} detail below.|$|E
5|$|The {{earliest}} {{research into}} thinking machines {{was inspired by}} a confluence of ideas that became prevalent in the late 30s, 40s and early 50s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of <b>computation</b> showed that any form of <b>computation</b> could be described digitally. The close relationship between these ideas suggested that {{it might be possible to}} construct an electronic brain.|$|E
40|$|The need of {{studying}} infinite <b>computations</b> has been emphasized in recent years, e. g., see (Vardi and Wolper, 1994). By infinite <b>computations,</b> one means the <b>computations</b> done by some programs that create non-terminating processes or {{very long time}} running processes. For such programs, the <b>computations</b> done by them usually go through infinite sequences of running states (or configurations), unlike finite <b>computations</b> in which only finite sequences of running states are involved...|$|R
40|$|Static analysis, proceedingsNumerical <b>computations</b> form an {{essential}} part of almost any real-world program. Traditional approaches to termination of logic programs are restricted to domains isomorphic to N, more recent works study termination of integer <b>computations.</b> Termination of <b>computations</b> involving real numbers is cumbersome and counter-intuitive due to rounding errors and implementation conventions. We present a novel technique that allows us to prove termination of such <b>computations.</b> Our approach extends the previous work on termination of integer <b>computations.</b> status: publishe...|$|R
25|$|The {{remaining}} {{item for}} discussion {{is the final}} protonation of the cyclohexadienyl anion. In 1961 {{it was found that}} simple Hückel <b>computations</b> were unable to distinguish between the different protonation sites. However, when the <b>computations</b> were modified with somewhat more realistic assumptions, the Hückel <b>computations</b> revealed the center carbon to the preferred. The more modern 1990 and 1993 <b>computations</b> were in agreement.|$|R
5|$|Xgrid is a {{proprietary}} program and distributed computing protocol {{developed by the}} Advanced <b>Computation</b> Group subdivision of Apple Inc that allows networked computers {{to contribute to a}} single task.|$|E
25|$|Computer algebra, {{also called}} {{symbolic}} <b>computation</b> or algebraic <b>computation</b> is a scientific area {{that refers to}} the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical <b>computation</b> with approximate floating point numbers, while symbolic <b>computation</b> emphasizes exact <b>computation</b> with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic <b>computation).</b>|$|E
25|$|We {{would like}} to be able to use this type as a simple sort of checked exception: at any point in a <b>computation,</b> the <b>computation</b> may fail, which causes the rest of the <b>computation</b> to be skipped and the final result to be Nothing. If all steps of the {{calculation}} succeed, the final result is Just x for some value x.|$|E
5000|$|... #Subtitle level 3: Sequential <b>computations</b> form an ω-complete {{subdomain}} of {{the domain}} of Actor <b>computations</b> ...|$|R
30|$|A tuning {{model for}} {{controlling}} the resource allocation between business functions and adaptive <b>computations</b> by upgrading and degrading the autonomic levels of adaptive <b>computations</b> dynamically. The features and gains of adaptive <b>computations</b> {{have been taken}} into considered in the model.|$|R
40|$|Abstract. Numerical <b>computations</b> form an {{essential}} part of almost any real-world program. Traditional approaches to termination of logic programs are restricted to domains isomorphic to (N,>), more recent works study termination of integer <b>computations</b> where the lack of well-foundness of the integers has to be taken into account. Termination of <b>computations</b> involving floating point numbers can be counter-intuitive due to rounding errors and implementation conventions. We present a novel technique that allows us to prove termination of such <b>computations.</b> Our approach extends the previous work on termination of integer <b>computations...</b>|$|R
25|$|The <b>computation</b> of the {{factored}} form, called factorization is, in general, {{too difficult}} {{to be done by}} hand-written <b>computation.</b> However, efficient polynomial factorization algorithms are available in most computer algebra systems.|$|E
25|$|In {{computer}} science, combinatory {{logic is}} used as a simplified model of <b>computation,</b> used in computability theory and proof theory. Despite its simplicity, combinatory logic captures many essential features of <b>computation.</b>|$|E
25|$|His Media <b>Computation</b> {{course has}} been taught at Georgia Tech since 2003 and has shown {{increases}} in computing across underrepresented groups, including women and minorities. Guzdial’s Media <b>Computation</b> curriculum {{is being used}} at universities across the country. He received {{a grant from the}} National Science Foundation in 2006 to pursue his “Using Media <b>Computation</b> to Attract and Retain Students in Computing” curriculum.|$|E
40|$|We {{show that}} real-number <b>computations</b> in the interval-domain {{environment}} are "inherently parallel" in a precise mathematical sense. We {{do this by}} reducing <b>computations</b> of the weak parallel-or operation on the Sierpinski domain to <b>computations</b> of the addition operation on the interval domain...|$|R
30|$|Similar {{results were}} {{obtained}} for the S 500 E 64 D (see Figure 11). The RE+ policy was again the one that presented the best trade-off between efficiency and effectiveness. Except the Random and the MST policies, {{the effectiveness of the}} clusters obtained from the PAM-SLIM algorithm, considering the other node-split policies was equivalent (see Figure 11 a). In general, the smallest values for the number of distance <b>computations</b> were obtained by the RE+ policy (see Figure 11 b). Considering the results obtained when k was 15, this policy showed 93.4 % less distance <b>computations</b> than the MST policy, 91.9 % less distance <b>computations</b> than the RE policy, 90.2 % less distance <b>computations</b> than the MD policy, 90.1 % less distance <b>computations</b> than the PDS policy, 85.2 % less distance <b>computations</b> than the MinMax policy and 18.8 % less distance <b>computations</b> than the Random policy.|$|R
40|$|Sparse matrix <b>computations</b> and {{irregular}} type <b>computations</b> show poor data locality behavior. Recently compiler optimizations techniques {{have been proposed}} to improve the data locality for regular type loop structures. Sparse matrix <b>computations</b> do not fall into this categorie of <b>computations</b> {{and the issue of}} compiler optimizations for sparse <b>computations</b> is merely understood. In this paper we describe how compiler optimizations based on pattern matching techniques can be used to improve the data locality behavior for sparse <b>computations.</b> 1 Introduction The effective use of caches {{is becoming more and more}} crucial for obtaining efficient implementations of large scale <b>computations</b> on parallel computing platforms. Fortunately, many applications show good cache behavior on sequential architectures [HP 90]. However, in the context of parallel computing cache coherency can deteriorate this behavior. This is due to the fact that data residing in caches is invalidated by write actions of other [...] ...|$|R
25|$|Computable {{functions}} are a fundamental concept within computer science and mathematics. The λ-calculus provides a simple semantics for <b>computation,</b> enabling properties of <b>computation</b> {{to be studied}} formally. The λ-calculus incorporates two simplifications that make this semantics simple.|$|E
25|$|They {{run on a}} {{realistic}} model of quantum <b>computation.</b> The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum <b>computation</b> such as quantum superposition or quantum entanglement.|$|E
25|$|An {{everyday}} {{example of}} the use of different forms of linear equations is <b>computation</b> of tax with tax brackets. This is commonly done with a progressive tax <b>computation,</b> using either point–slope form or slope–intercept form.|$|E
30|$|All <b>computations</b> {{which have}} been {{presented}} in this research are composed of two main parts: (a) the optimization of electron structure and (b) the <b>computations</b> concerned with electron transport properties. All of these <b>computations</b> have been performed using SIESTA and TranSIESTA softwares [29, 30].|$|R
40|$|Abstract. The aim of {{this paper}} is to present a step toward {{building}} computational models for interactive systems. Such <b>computations</b> are performed in an integrated distributed environments on objects of different kinds of complexity, called here as information granules. The <b>computations</b> are progressing by interactions among information granules and physical objects. We distinguish global and local <b>computations.</b> The former ones are performed by the environment (the nature) while the local <b>computations</b> are, in a sense, projections of the global <b>computations</b> on local systems and they represent information on global <b>computations</b> perceived by local systems. We assume that, the laws of the nature are only partially known by the local systems. This approach seems to be of some importance for developing computing models in different areas such as natural computing (e. g., computing models for meta-heuristics or <b>computations</b> models for complex processes in molecular biology), computing in distributed environments under uncertainty realized by multi-agent systems, modeling of <b>computations</b> for feature extraction (constructive induction) for approximation of complex vague concepts, hierarchical learning, discovery of planning strategies or strategies for coalition formation by intelligent systems as well as for approximate reasoning about interactive <b>computations</b> based on such computing models. In the presented computing models, a mixture of reasoning based on deduction and induction is used...|$|R
40|$|We {{show how}} to view <b>computations</b> {{involving}} very general liveness properties as limits of finite approximations. This computational model {{does not require}} introduction of infinite nondeterminism as with most traditional approaches. Our results allow us directly to relate finite <b>computations</b> in order to infer properties about infinite <b>computations...</b>|$|R
25|$|<b>Computation</b> of isotopic distributions.|$|E
25|$|This table {{summarizes}} the <b>computation.</b>|$|E
25|$|Expectations from {{mathematics}} may not {{be realized}} {{in the field of}} floating-point <b>computation.</b> For example, it is known that , and that , however these facts cannot be relied on when the quantities involved are the result of floating-point <b>computation.</b>|$|E
5000|$|... {{inductive}} Turing machines, which perform <b>computations</b> {{similar to}} <b>computations</b> of Turing machines and produce their results after {{a finite number}} of steps (Mark Burgin) ...|$|R
5000|$|... limit Turing {{machines}}, which perform <b>computations</b> {{similar to}} <b>computations</b> of Turing machines but their final results are {{limits of their}} intermediate results (Mark Burgin) ...|$|R
3000|$|Note {{that in this}} scenario, {{there are}} m cores and n <b>computations.</b> To simplify the notation, we number the cores and {{corresponding}} resources by the numbers of the <b>computations</b> that are utilizing them. As a result, when there are n <b>computations,</b> the n cores serving them are named ξ 1 through ξ [...]...|$|R
