5|30|Public
50|$|Black Hat {{search engine}} {{optimization}} (SEO) {{is a technique}} used to trick search engines into displaying malicious URLs in search results. The malicious webpages are filled with popular keywords {{in order to achieve}} a higher ranking in the search results. When the end user searches the web, one of these infected webpages is returned. Usually the most popular keywords from services such as Google Trends are used to generate webpages via PHP scripts placed on the <b>compromised</b> <b>website.</b> These PHP scripts will then monitor for search engine crawlers and feed them with specially crafted webpages that are then listed in the search results. Then, when the user searches for their keyword or images and clicks on the malicious link, they will be redirected to the Rogue security software payload.|$|E
40|$|Significant recent {{research}} advances {{have made it}} possi-ble to design systems that can automatically determine with high accuracy the maliciousness of a target website. While highly useful, such systems are reactive by nature. In this paper, we take a complementary approach, and at-tempt to design, implement, and evaluate a novel classi-fication system which predicts, whether a given, not yet <b>compromised</b> <b>website</b> will become malicious in the fu-ture. We adapt several techniques from data mining and machine learning which are particularly well-suited for this problem. A key aspect of our system is that the set of features it relies on is automatically extracted from the data it acquires; this allows {{us to be able}} to detect new attack trends relatively quickly. We evaluate our imple-mentation on a corpus of 444, 519 websites, containing a total of 4, 916, 203 webpages, and show that we man-age to achieve good detection accuracy over a one-year horizon; that is, we generally manage to correctly predict that currently benign websites will become compromised within a year. ...|$|E
30|$|We further {{studied the}} {{relationships}} between different Bars, fetched by the same websites. From our dataset, 11, 442 (3.5 %) websites are found to access at least two Bars. Among them, 8283 were served as front-end websites, and 3159 other sites on redirection chains. Also, 60.9 % of these sites link to the repositories on the same cloud platforms and 39.1 % use those on different platforms. In some cases, two buckets are often used together. For example, we found that a click-hijacking program was separated into the code part and the configuration part: the former is kept on CloudFront while the latter is on Akamaihd; the two buckets always show up together on redirection chains. Such a separation seems to be done deliberately, {{in an attempt to}} evade detection. Also we saw that Bars carrying the same attack vectors are often used together, which are apparently deliberately put there to serve parties of the same interests: as another example, a <b>compromised</b> <b>website</b> was observed to access four different Bars on different cloud platforms, redirecting its visitors to different places for downloading Adware to the visitor’s system. Our findings show that Bars are widely deployed in attacks and serve in a complex infrastructure.|$|E
50|$|As malware also harms the <b>compromised</b> <b>websites</b> (by {{breaking}} reputation, blacklisting {{in search}} engines, etc.), some websites offer vulnerability scanning.Such scans check the website, detect malware, may note outdated software, and may report known security issues.|$|R
40|$|In July 2009, several Irish {{websites}} {{were attacked}} and had malware code injected into them. These (<b>compromised)</b> <b>websites</b> redirected end-users to malicious websites, which subsequently served malware {{to anyone who}} browsed to the original legitimate sites. The notification of this compromise resulted in me beginning the Incident Handling Process. The subsequent investigation into the complex infrastructure behind, what initially {{appeared to be a}} simple <b>website</b> <b>compromise</b> prompted this paper. The paper will walk through [...] . Copyright SANS Institut...|$|R
5000|$|In November 2009, {{in another}} {{joint venture with}} Andrew Martin and Scott Logan, Jart Armin and HostExploit {{released}} a report called [...] "MALfi, A Cybercrime International Report - A Silent Threat". The report describes how hackers and cybercriminals use blended attacks - a combination of RFI (remote file inclusion), LFI (local file inclusion), XSA (cross-server attack), and RCE (remote code execution) - to <b>compromise</b> <b>websites</b> and servers.|$|R
40|$|The {{large-scale}} {{deployment of}} modern phishing attacks {{relies on the}} automatic exploitation of vulnerable websites in the wild, to maximize profit while hindering attack traceability, detection and blacklisting. To {{the best of our}} knowledge, this is the first work that specifically leverages this adversarial behavior for detection purposes. We show that phishing webpages can be accurately detected by highlighting HTML code and visual differences with respect to other (legitimate) pages hosted within a <b>compromised</b> <b>website.</b> Our system, named DeltaPhish, can be installed as part of a web application firewall, to detect the presence of anomalous content on a website after compromise, and eventually prevent access to it. DeltaPhish is also robust against adversarial attempts in which the HTML code of the phishing page is carefully manipulated to evade detection. We empirically evaluate it on more than 5, 500 webpages collected in the wild from compromised websites, showing that it is capable of detecting more than 99 % of phishing webpages, while only misclassifying less than 1 % of legitimate pages. We further show that the detection rate remains higher than 70 % even under very sophisticated attacks carefully designed to evade our system. Comment: Preprint version of the work accepted at ESORICS 201...|$|E
40|$|SQLIA {{is adopted}} to attack {{websites}} {{with and without}} confidential information. Hackers utilize the <b>compromised</b> <b>website</b> as intermediate proxy to attack others for avoiding being committed of cyber-criminal and also enlarging the scale of Distributed Denial of Service Attack (DDoS). The DDoS is that hackers maliciously turn down a website and make network resources unavailable to web users. It {{is extremely difficult to}} effectively detect and prevent SQLIA because hackers adopt various evading SQLIA Intrusion Detection System techniques. Victims always are not aware of that their confidential information has been compromised for a long time. The contributions of this thesis are: (1) systematically explore SQLIA, SQLIA prevention in theory; (2) demonstrate, evaluate imitative SQLIA with open source SQLIA tools and SQLIA prevention tools in practice; (3) new filters for eliminating SQLIA evading IDS/IPS detection techniques to improve SQLIA prevention. The achievements of this thesis are to successfully obtain 637 copies replied questionaire of surveying open source SQLIA tools and open source SQLIA prevention tools in quantitative research. Up to 76 virtual websites which have not been installed any SQLIA prevention tools have been successfully compromised in 500 penetration tests by SQLIA experiments in virtual environment of qualitative research. Furthermore, 27 compromised virtual websites that are installed with SQLIA prevention tools have experiences 600 times penetration tests. The open source SQLIA prevention tools successfully prevent total 573 times out of 600 times SQLIA penetration tests. To conduct 100 times penetration tests for each new filters of eliminating SQL injection evading IDS/IPS detection and testing result shows that all new filters can successfully prevent evading techniques with a high percentage, but with some side effect...|$|E
50|$|Outsourced {{security}} {{licensing and}} delivery is boasting a multibillion-dollar market. SECaaS provides users with Internet security services providing protection from online threats and attacks such as DDoS that are constantly searching for access points to <b>compromise</b> <b>websites.</b> As the demand {{and use of}} cloud computing skyrockets, users are more vulnerable to attacks due to accessing the Internet from new access points. SECaaS serves as a buffer against the most persistent online threats.|$|R
50|$|In May 2008, <b>websites</b> {{worldwide}} were <b>compromised</b> with a malicious JavaScript. Initially a {{half million}} websites worldwide were infected with a SQL injection which leveraged a ZLOB variant which then downloaded additional Trojan onto users’ PCs. Then websites in China, Taiwan and Singapore were compromised followed shortly thereafter by humanitarian, government and news sites in the UK, Israel and Asia. In this attack the <b>compromised</b> <b>websites</b> led, {{through a variety of}} redirects, to the download of a Trojan.|$|R
40|$|It {{is common}} for {{websites}} to instruct browsers to make requests to third-party sites for content, advertisements, {{as well as for}} purely usertracking purposes. Additionally, malicious or <b>compromised</b> <b>websites</b> can perform attacks on users and websites by leveraging their ability to initiate cross-site requests in a user’s browser. There are currently only limited protections against these threats to user privacy and security. We design and implement RequestPolicy, an extension for Mozilla browsers that provides a client-side whitelist for controlling cross-site requests...|$|R
5000|$|... 22 June 2014: The Reuters website was hacked {{a second}} time and showed a SEA message condemning Reuters for {{publishing}} [...] "false" [...] articles about Syria. Hackers <b>compromised</b> the <b>website</b> corrupting ads served by Taboola.|$|R
30|$|Cybercriminals {{have adopted}} two {{well-known}} strategies for defrauding consumers online: large-scale and targeted attacks. Many successful scams {{are designed for}} massive scale. Phishing scams impersonate banks and online service providers by the thousand, blasting out millions of spam emails to lure a very small fraction of users to fake websites under criminal control [1],[2]. Miscreants peddle counterfeit goods and pharmaceuticals, succeeding despite very low conversion rates [3]. The criminals profit because they can easily replicate content across domains, despite efforts to quickly take down content hosted on <b>compromised</b> <b>websites</b> [1]. Defenders have responded by using machine learning techniques to automatically classify malicious websites [4] and to cluster website copies together [5]-[8].|$|R
40|$|<b>Compromised</b> <b>websites</b> {{are often}} used by attackers to deliver {{malicious}} content or to host phishing pages designed to steal private information from their victims. Unfortunately, most of the targeted websites are managed by users with little security background-often unable to detect this kind of threats or to afford an external professional security service. In this paper we test the ability of web hosting providers to detect <b>compromised</b> <b>websites</b> and react to user complaints. We also test six specialized services that provide security monitoring of web pages for a small fee. During a period of 30 days, we hosted our own vulnerable websites on 22 shared hosting providers, including 12 {{of the most popular}} ones. We repeatedly ran five different attacks against each of them. Our tests included a bot-like infection, a drive-by download, the upload of malicious files, an SQL injection stealing credit card numbers, and a phishing kit for a famous American bank. In addition, we also generated traffic from seemingly valid victims of phishing and drive-by download sites. We show that most of these attacks could have been detected by free network or file analysis tools. After 25 days, if no malicious activity was detected, we started to file abuse complaints to the providers. This allowed us to study the reaction of the web hosting providers to both real and bogus complaints. The general picture we drew from our study is quite alarming. The vast majority of the providers, or “add-on ” security monitoring services, are unable to detect the most simple signs of malicious activity on hosted websites...|$|R
50|$|Virut is a malware botnet that {{is known}} to be used for {{cybercrime}} activities such as DDoS attacks, spam (in collaboration with the Waledac botnet), fraud, data theft, and pay-per-install activities. It spreads through executable file infection (through infected USB sticks and other media), and more recently, through compromised HTML files (thus infecting vulnerable browsers visiting <b>compromised</b> <b>websites).</b> It has infected computers associated with at least 890,000 IP addresses in Poland. In 2012, Symantec estimated that the botnet had control of over 300,000 computers worldwide, primarily in Egypt, Pakistan and Southeast Asia (including India). A Kaspersky report listed Virut as the fifth-most widespread threat {{in the third quarter of}} 2012, responsible for 5.5% of computer infections.|$|R
40|$|Abstract—Compromised {{websites}} that redirect web {{traffic to}} malicious hosts {{play a critical}} role in organized web crimes, serving as doorways to all kinds of malicious web activities (e. g., drive-by downloads, phishing etc.). They are also among the most elusive components of a malicious web infrastructure and extremely difficult to hunt down, due to the simplicity of redirect operations, which also happen on legitimate sites, and extensive use of cloaking techniques. Making the detection even more challenging is the recent trend of injecting redirect scripts into JavaScript (JS) files, as those files are not indexed by search engines and their infections are therefore more difficult to catch. In our research, we look at the problem from a unique angle: the adversary’s strategy and constraints for deploying redirect scripts quickly and stealthily. Specifically, we found that such scripts are often blindly injected into both JS and HTML files for a rapid deployment, changes to the infected JS files are often made minimum to evade detection and also many JS files are actually JS libraries (JS-libs) whose uninfected versions are publicly available. Based upon those observations, we developed JsRED, a new technique for the automatic detection of unknown redirect-script injections. Our approach analyzes the difference between a suspicious JS-lib file and its clean counterpart to identify malicious redirect scripts and further searches for similar scripts in other JS and HTML files. This simple, lightweight approach is found to work effectively against redirect injection campaigns: our evaluation shows that JsRED captured most of <b>compromised</b> <b>websites</b> with almost no false positives, significantly outperforming a commercial detection service in terms of finding unknown JS infections. Based upon the <b>compromised</b> <b>websites</b> reported by JsRED, we further conducted a measurement study that reveals interesting features of redirect payloads and a new Peer-to-Peer network the adversary constructed to evade detection. I...|$|R
30|$|Through {{analyzing}} the redirection {{traces of the}} campaign, we found that two Aka- mai Bars, akamaid.net_cdncache 3 -a and akamaihd_asrv-a, frequently inject scripts into <b>compromised</b> <b>websites,</b> which serve as first-hop redirectors to move a visitor down the redirection chain before hitting malicious landing pages (that serve malicious content). Interestingly, all the follow-up redirectors are <b>compromised</b> or malicious <b>websites</b> that are not hosted on the cloud. The scripts in the Bars were found to change over time, redirecting the visitor to different next-hop sites (also redirectors). On average, {{the life span of}} such sites is only 120  h, but the Bar was still alive when we submitted this paper. Such redirections end at at least 216 mali- cious landing sites, which all retrieve deceptive images from an Amazon S 3 bucket s 3.amazonaws.com_cicloudfront (a Bar never reported before and is still alive). An example is a system update warning, as shown in Fig.  1. From the reposi- tory, we collected 134 images, including those for free software installation, updates on all mainstream OSes, browsers and some popular applications. If she clicks and downloads the program promoted on the site, the code will be fetched from multiple Bars, such as s 3.amazonaws.com_wbt_media where the PUP puts a Bitcoin miner on the victim’s system, and cloudfront.net_d 12 mrm 7 igk 59 vq, whose program modifies Chrome’s security setting.|$|R
40|$|The {{inevitable}} vulnerabilities {{and criminal}} targeting of cloud environments demand {{an understanding of}} how digital forensic investigations of the cloud can be accomplished. We present two hypothetical case studies of cloud crimes; child pornography being hosted in the cloud, and a <b>compromised</b> cloud-based <b>website.</b> Our cases highlight shortcomings of current forensic practices and laws. We describe significant challenges with cloud forensics, including forensic acquisition, evidence preservation and chain of custody, and open problems for continued research...|$|R
40|$|Abstract — Forensic Investigation into {{security}} incidents often includes {{the examination of}} huge lists of internet activity gathered from a suspect computer. In today’s age of increased internet usage, the internet activity log on any given system could produce a huge list of websites. This, couples {{with the fact that}} a huge percentage of malware is now distributed via the internet, often through <b>compromised</b> <b>websites,</b> means that valuable clues regarding the source and identity of malware infections are often hidden within the internet activity logs on a computer. While a multitude of tools exist to extract internet activity data from a host computer, most do not filter this activity data. As a result, an investigator could be faced with thousands of website URL’s to sift through for clues regarding malware infection. In this paper, we discuss some of the ways that computers are infected, and why internet activity data is an important resource that must be analyzed in a forensic investigation. We then present a tool that utilizes the Google Safe Browsing Lookup API, which is an extension of the broader Google Safe Browsing API, to do quick lookups on long lists of URL’s and significantly narrow the list to enable the investigator to conduct a more efficient investigation...|$|R
5000|$|The {{next step}} is to host the {{malicious}} content that the attacker wishes to distribute. One option is for the attacker to host the malicious content on their own server. However, because of the difficulty in directing users to a new page, it may also be hosted on a <b>compromised</b> legitimate <b>website,</b> or a legitimate website unknowingly distributing the attackers content through a third party service (e.g. an advertisement). When the content is loaded by the client, the attacker will analyze the fingerprint of the client in order to tailor the code to exploit vulnerabilities specific to that client.|$|R
30|$|Malicious web {{activities}} have been extensively studied (Invernizzi et al. 2012; Invernizzi et al. 2014; Moore et al. 2011; Nelms et al. 2015). Most related to our work {{here is the}} use of HTML content and redirection paths to detect malicious or <b>compromised</b> <b>websites.</b> Examples for the content-based detection include a DOM-based clustering systems for monitoring Scam websites (Der et al. 2014), clas- sification of websites for predicting whether some of them will turn bad based on the features extracted from HTML sources, and a monitoring mechanism (Borgolte et al. 2013) (called Delta) {{to keep track of the}} changes made to the content of a website for detecting script-injection campaigns. For those using malicious redirection paths, prominent prior approaches use short redirection sequences to capture bad sites (Li et al. 2014), unique prop- erties of malicious infrastructure (its density) for detecting drive-by downloads (Invernizzi et al. 2014) or malware distribution (Stringhini et al. 2013) and a trace-back mechanism that goes through the redi- rection paths (Nelms et al. 2015) for labeling malware download sites. Compared with those prior studies, which all rely on the properties of the targets they try to capture, BarFinder utilizes the features found from the front-end websites using cloud buckets, as those repositories may not be directly accessible. Also, our approach leverages a set of unique collective features, based on the connected components of a graph, which, to our knowledge, has never been used before.|$|R
40|$|While passwords, by definition, {{are meant}} to be secret, recent trends in the Internet usage have {{witnessed}} an increasing number of people sharing their email passwords for both personal and professional purposes. As sharing passwords increases the chances of your passwords being <b>compromised,</b> leading <b>websites</b> like Google strongly advise their users not to share their passwords with anyone. To cater to this conflict of usability versus security and privacy, we introduce ChaMAILeon, an experimental service, which allows users to share their email passwords while maintaining their privacy and not compromising their security. In this report, we discuss the technical details of the implementation of ChaMAILeon. Comment: 6 pages, 4 image...|$|R
40|$|Automated online {{password}} guessing {{attacks are}} {{facilitated by the}} fact that most user authentication techniques provide a yes/no answer as the result of an authentication attempt. These attacks are somewhat restricted by Automated Turing Tests (ATTs, e. g., captcha challenges) that attempt to mandate human assistance. ATTs are not very difficult for legitimate users, but always pose an inconvenience. Several current ATT implementations are also found to be vulnerable to improved image processing algorithms. ATTs can be made more complex for automated software, but that is limited by the trade-off between user-friendliness and effectiveness of ATTs. As attackers gain control of large-scale botnets, relay the challenge to legitimate users at <b>compromised</b> <b>websites,</b> or even have ready access to cheap, sweat-shop human solvers for defeating ATTs, online guessing attacks are becoming a greater security risk. Using deception techniques (as in honeypots), we propose the user-verifiable authentication scheme (Uvauth) that tolerates, instead of detecting or counteracting, guessing attacks. Uvauth provides access to all authentication attempts; the correct password enables access to a legitimate session with valid user data, and all incorrect passwords lead to fake sessions. Legitimate users are expected to learn the authentication outcome implicitly from the presented user data, and are relieved from answering ATTs; the authentication result never leaves the server and thus remains (directly) inaccessible to attackers. In addition, we suggest using adapted distorted images and pre-registered images/text as a complement to convey an authentication response, especially for accounts that do not host much personal data...|$|R
25|$|On February 5–6, 2011, Anonymous <b>compromised</b> the HBGary <b>website,</b> copied tens of {{thousands}} of documents from both HBGary Federal and HBGary, Inc., posted tens {{of thousands}} of both companies' emails online, and usurped Barr's Twitter account in apparent revenge. Anonymous also claimed to have wiped Barr's iPad remotely, though this act remains unconfirmed. The Anonymous group responsible for these attacks would go on to become LulzSec.|$|R
40|$|International audienceCompromised {{websites}} {{are often}} used by attackers to deliver ma- licious content or to host phishing pages designed to steal private information from their victims. Unfortunately, most of the targeted websites are managed by users with little security background - often unable to detect this kind of threats or to afford an external professional security service. In this paper we test the ability of web hosting providers to detect <b>compromised</b> <b>websites</b> and react to user complaints. We also test six specialized services that provide security monitoring of web pages for a small fee. During a period of 30 days, we hosted our own vulnerable web- sites on 22 shared hosting providers, including 12 of the most pop- ular ones. We repeatedly ran five different attacks against each of them. Our tests included a bot-like infection, a drive-by download, the upload of malicious files, an SQL injection stealing credit card numbers, and a phishing kit for a famous American bank. In ad- dition, we also generated traffic from seemingly valid victims of phishing and drive-by download sites. We show {{that most of these}} attacks could have been detected by free network or file analysis tools. After 25 days, if no malicious activity was detected, we started to file abuse complaints to the providers. This allowed us to study the reaction of the web hosting providers to both real and bogus complaints. The general picture we drew from our study is quite alarming. The vast majority of the providers, or "add-on" security monitoring services, are unable to detect the most simple signs of malicious activity on hosted websites...|$|R
5000|$|Just {{six weeks}} before the general day of 9 May 2016, the [...] website was hacked by a group called [...] "Anonymous Philippines" [...] {{on the night of}} 27 March 2016. Anonymous Philippines asked the poll body to {{implement}} security on Precinct Count Optical Scanners (PCOS) — automated voting machines. Another group calling itself LulzSec Pilipinas, claimed to have hacked 's website, and posted its database on their Facebook account shortly after Anonymous Philippines <b>compromised</b> 's <b>website.</b> These exploits exposed voter data and the vulnerability of both voter registration data and the functionality of their website. LulzSec posts 3 mirror links on their Facebook account that can be downloaded. The incident was considered the biggest private leak data in the Philippine history and leaving millions of registered voters at risk.|$|R
50|$|As {{a digital}} rights {{management}} measure, software developers may hardcode a unique serial number directly into a program. A program that has a unique serial number may regularly check its maker's website to verify that it hasn't been blacklisted as <b>compromised.</b> If that <b>website</b> moves or the company goes out of business, {{this may cause the}} program to fail, even for perfectly legal users, if that check is programmed to fail when no response is received.|$|R
50|$|CORS {{can be used}} as {{a modern}} {{alternative}} to the JSONP pattern. While JSONP supports only the GET request method, CORS also supports other types of HTTP requests. Using CORS enables a web programmer to use regular XMLHttpRequest, which supports better error handling than JSONP. On the other hand, JSONP works on legacy browsers which predate CORS support. CORS is supported by most modern web browsers. Also, while JSONP can cause cross-site scripting (XSS) issues when the external site is <b>compromised,</b> CORS allows <b>websites</b> to manually parse responses to ensure security.|$|R
5000|$|In March 2010, Google and McAfee {{announced}} on their security blogs {{that they believe}} that hackers <b>compromised</b> the VPS <b>website</b> and replaced the program with a trojan. The trojan, which McAfee has code-named W32/VulcanBot, creates a botnet {{that could be used to}} launch distributed denial of service attacks on websites critical of the Vietnamese government's plan to mine bauxite in the country's Central Highlands. McAfee suspects that the authors of the trojan have ties to the Vietnamese government. However, Nguyễn Tử Quảng of Bách Khoa Internet Security (Bkis) called McAfee's accusation [...] "somewhat premature". The Vietnamese Ministry of Foreign Affairs issued a statement calling Google's and McAfee's comments [...] "groundless".|$|R
40|$|We {{investigate}} {{the manipulation of}} web search results to promote the unauthorized sale of prescription drugs. We focus on search-redirection attacks, where miscreants <b>compromise</b> high-ranking <b>websites</b> and dynamically redirect traffic to different pharmacies based upon the particular search terms issued by the consumer. We constructed a representative list of 218 drug-related queries and automatically gathered the search results {{on a daily basis}} over nine months in 2010 - 2011. We find that about one third of all search results are one of over 7 000 infected hosts triggered to redirect to a few hundred pharmacy websites. Legitimate pharmacies and health resources have been largely crowded out by search-redirection attacks and blog spam. Infections persist longest on websites with high PageRank and from. edu domains. 96 % of infected domains are connected through traffic redirection chains, and network analysis reveals that a few concentrated communities link many otherwise disparate pharmacies together. We calculate that the conversion rate of web searches into sales lies between 0. 3 % and 3 %, and that more illegal drugs sales are facilitated by search-redirection attacks than by email spam. Finally, we observe that concentration in both the source infections and redirectors presents an opportunity for defenders to disrupt online pharmacy sales. 1 Introduction an...|$|R
40|$|We {{investigate}} {{the evolution of}} search-engine poisoning using data on over 5 million search results collected over nearly 4 years. We build on prior work investigating search-redirection attacks, where criminals <b>compromise</b> high-ranking <b>websites</b> and direct search traf-fic to the websites of paying customers, such as unlicensed phar-macies who lack access to traditional search-based advertisements. We overcome several obstacles to longitudinal studies by amalga-mating different resources and adapting our measurement infras-tructure to changes brought by adaptations by both legitimate op-erators and attackers. Our goal is to empirically characterize how strategies for carrying out and combating search poisoning have evolved over a relatively long time period. We investigate how the composition of search results themselves has changed. For in-stance, we find that search-redirection attacks have steadily grown {{to take over a}} larger share of results (rising from around 30 % in late 2010 to a peak of nearly 60 % in late 2012), despite efforts by search engines and browsers to combat their effectiveness. We also study the efforts of hosts to remedy search-redirection attacks. We find that the median time to clean up source infections has fallen from around 30 days in 2010 to around 15 days by late 2013, yet the number of distinct infections has increased considerably over the same period. Finally, we show that the concentration of traffic to the most successful brokers has persisted over time. Further, these brokers have been mostly hosted on a few autonomous systems, which indicates a possible intervention strategy. Categories and Subject Descriptors K. 4. 1 [Public Policy Issues]: Abuse and crime involving comput-er...|$|R
40|$|Abstract. We {{describe}} a case-control study to identify risk {{factors that are}} asso-ciated with higher rates of webserver compromise. We inspect {{a random sample of}} around 200 000 webservers and automatically identify attributes hypothesized to affect the susceptibility to compromise, notably content management system (CMS) and webserver type. We then cross-list this information with data on web-servers hacked to serve phishing pages or redirect to unlicensed online pharma-cies. We find that webservers running WordPress and Joomla {{are more likely to be}} hacked than those not running any CMS, and that servers running Apache and Nginx are more likely to be hacked than those running Microsoft IIS. Further-more, using a series of logistic regressions, we find that a CMS’s market share is positively correlated with <b>website</b> <b>compromise.</b> Finally, we examine the link be-tween webservers running outdated software and being compromised. Contrary to conventional wisdom, we find that servers running outdated versions of Word-Press (the most popular CMS platform) are less likely to be hacked than those running more recent versions. We present evidence that this may be explained by the low install base of outdated software. ...|$|R
40|$|In today's world, {{more and}} more people are {{managing}} every aspect of their lives over the Internet. As a result, the study of Internet traffic, which is undergoing constant evolution as new technologies emerge, has attracted much attention from the research community. In this dissertation, we present a three-pronged approach to help ISPs and network administrators: a) gain insight about the applications that generate traffic in their networks, b) understand the Web browsing behaviors of their users, and c) detect in a timely fashion when external malicious entities seek to <b>compromise</b> their <b>websites.</b> The first component of our approach is SubFlow, a Machine Learning-based tool that classifies traffic flows into classes of applications that generate them, for example P 2 P or Web. The key novelty of SubFlow is its ability to learn the characteristics of the traffic from each application class in isolation while traditional approaches simply try to assign flows to predefined categories. This allows SubFlow to exhibit very high classification accuracy even when new applications emerge. The second component is ReSurf, a tool to reconstruct users' web-surfing activities from Web traffic. ReSurf enables the separation of users' intentional web-browsing (such as the click user makes) from the traffic automatically generated when the website is rendered. ReSurf, then, can be an effective method to study the browsing behaviors of users and gain insights into the evolution of modern Web traffic, which accounts for about 80 % of Internet traffic. The last component of our approach is Scanner Hunter, an algorithm to detect HTTP Scanners, external entities that selectively probe websites for vulnerabilities that may be exploited in subsequent intrusion attempts. Our algorithm is developed in response to the fact that HTTP scanners have not received much attention despite the high risk and danger they pose. Scanner Hunter utilizes a novel combination of graph-mining approaches to expose the community structure of scanners. Using Scanner Hunter, we conduct the first extensive study of scanners in the wild during a half-year period, which we also provide novel insight on this little-studied emerging phenomenon...|$|R
40|$|AbstractDistributed Denial of Service (DDoS) attacks {{continue}} to instigate intense wars against popular ecommerce and content websites. One in five companies worldwide become a DDoS attack victim. Such attacks remain active causing prolonged damage {{from a few}} hours to several weeks. Deccan Chronicle 34, 35, dated April 29, 2015, reported above statement as conclusion of Kaspersky Lab's and B 2 B's international survey with categorizing two types of DDoS attacks: “a powerful short term attack or persistent long running campaign”. Both the above types of popular DDoS attacks can be detected, prevented and mitigated using the proposed novel Qualified Vector Match and Merge Algorithm (QVMMA) in real time. 14 feature components are used to generate an attack signature in real time and stored in dynamically updated DDoS Captured Attack Pattern (DCAP) 30 database. It's effective in detecting new and old attacks. Persistent DDoS attacks cause financial damage or reputation loss by loss of the company's valuable clients. The server's availability is heavily <b>compromised.</b> Popular <b>websites</b> Github and BBC UK faced DDoS attacks in 2015. Long term DDoS attack directed on Github continued for over 118 hours 34, 35. Short term DDoS attack experienced by BBC 36 website caused its patchy response. The main crux of the problem is the absence of a way to differentiate between attack records and legitimate records while the attack is occurring in real time. Several methods 1 - 31, 37 - 42 are listed in the paper. Post mortem solutions are not applicable in real time. Available real time solutions are slow. QVMMA is an ideal faster real time solution to prevent DDoS attacks using Statistical Feature Vector Generation. Matlab is used for DDoS real time simulation where the topologies (bus, star, abilene network) are created using OMNET++ 33. QVMMA generates and uses Statistical Feature Vector for Attack Signature Generation, Matching and Identification only for qualifier satisfied records. The web server's log files used as input to QVMMA are according to W 3 C log format standard 34. Experimentation is completed with exhaustive 336 cases. Four networks are tested with 5, 8, 10, 13 nodes. Performance evaluation of QVMMA concludes EER is 11. 8 % when threshold is 1. 6. Abilene network achieves best result. As the number of attackers, nodes and intermediate routers increase, detection time increases. If threshold is increased, the accuracy reduces. If the number of nodes increases, accuracy increases. Thus it is concluded that QVMMA can be used for effective layer 3 DDoS Prevention and Mitigation in real time based on results generated in Matlab simulation...|$|R

