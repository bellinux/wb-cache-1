4|1932|Public
50|$|Concepts {{can be used}} to <b>choose</b> <b>function</b> {{template}} overloads {{and class}} template specializations based on properties of their template arguments, as an alternative to SFINAE and tag dispatching. If an argument satisfies more than one concept, the overload associated with the more constrained concept is chosen.|$|E
50|$|A Jain text, the Bhagabati Sutra, had {{the first}} mention of a {{combinatorics}} problem; it asked how many ways one could take six tastes one, two, or three tastes at a time. The Bhagabati Sutra was written around 300 BC, {{and was the first}} book to mention the <b>choose</b> <b>function.</b> The next ideas of Combinatorics came from Pingala, who was interested in prosody. Specifically, he wanted to know how many ways a six-syllable meter could be made from short and long notes. He wrote this problem in the Chanda sutra (also Chandahsutra) in the second century BC. In addition, he also found the number of meters that had n long notes and k short notes, which is equivalent to finding the binomial coefficients.|$|E
40|$|Suppose that u and v are vectors that {{represent}} possible populations in an evolutionary algorithm. u and v each have n elements, where n is the cardinality {{of the search}} space, i. e., {{the total number of}} possible unique individuals. N is the population size, so that the sum of the elements of u is equal to N, and the sum of the elements of v is also equal to N. The Markov transition matrix Q for an evolutionary algorithm gives the probability that the algorithm will transition from one population to another population after one genera-tion. Q is therefore a T × T matrix, where T is the total number of possible population distributions. That is, T is the number of possible n × 1 integer vectors whose elements sum to N and each of whose elements in in [0, N]. This number can be calculated several different ways. In [1] it is shown that T can be calculated with the <b>choose</b> <b>function...</b>|$|E
5000|$|... {{minimization}} of angular {{difference is}} emphasized when <b>choosing</b> <b>functions</b> (mentioned in section above), and ...|$|R
50|$|In {{computer}} science, a randomization function or randomizing {{function is}} an algorithm or procedure that implements a randomly <b>chosen</b> <b>function</b> between two specific sets, {{suitable for use}} in a randomized algorithm.|$|R
40|$|Abstract—This paper {{presents}} {{a method of}} model selection and identification of Hammerstein systems by hybridization of the genetic algorithm (GA) and particle swarm optimization (PSO). An unknown nonlinear static part to be estimated is approximately represented by an automatic <b>choosing</b> <b>function</b> (ACF) model. The weighting parameters of the ACF and the system parameters of the linear dynamic part are estimated by the linear least-squares method. On the other hand, the adjusting parameters of the ACF model structure are properly selected by the hybrid algorithm of the GA and PSO, where the Akaike information criterion is utilized as the evaluation value function. Simulation results are shown to demonstrate {{the effectiveness of the}} proposed hybrid algorithm. Keywords—Hammerstein system, identification, automatic <b>choosing</b> <b>function</b> model, genetic algorithm, particle swarm optimization. I...|$|R
40|$|A {{rule-based}} programming environment consists of • A data base (which we’ll leave undefined for now); • A set of rules: condition-action pairs. The condition evaluates {{some aspect of}} the data base. The action(s) typically modify {{some aspect of the}} data base. • A monitor that determines which rules can be applied (i. e. which rules have conditions that evaluate to true), and that chooses an applicable rule and applies it. What happens in a rule-based environment The following sequence of events is repeated by the monitor: 1. Decide what rules are applicable. 2. Choose a rule to apply. When there are more than one applicable rule, there are several possible strategies (“meta-rules”) : a. choose the highest priority rule; b. choose the most specific rule; c. choose the rule that refers to the element most recently added to the data base; d. choose the most recently applied rule; e. choose the rule that has been applicable for the longest time; f. choose a rule that has not previously been applied; g. choose the first rule in the list; h. make a random choice. 3. The monitor can also choose not to choose, i. e. to explore all the applicable rules in parallel. Apply the rule. How this is done in Scheme Here is a function that implements the monitor of a rule-based system. It takes a list of rules and a data base as arguments. (define (monitor rules data-base) (let ((rule-set (applicable rules database))) (if (not (empty? rule-set)) (monitor rules (apply-rule (choose rule-set) data-base))))) The applicable function returns a list of applicable rules; it often involves patternmatching. The <b>choose</b> <b>function</b> applies one of the criteria mentioned above. (Note the difference between this and the way a Scheme cond is executed.) The apply-rule function “executes ” the actions in the action-part of the rule...|$|E
5000|$|... note: if ifThenElse {{is passed}} two {{functions}} as {{the left and}} right conditionals; it is necessary to also pass an empty tuple (...) to the result of ifThenElse in order to actually call the <b>chosen</b> <b>function,</b> otherwise ifThenElse will just return the function object without getting called.|$|R
50|$|Next,which can be {{interpreted}} to predict that a solution to a second order linear p.d.e. in three variables is expressible by two freely <b>chosen</b> <b>functions</b> of two variables, {{one of which is}} used immediately, and the second, only after taking a first derivative, in order to express the solution.|$|R
5000|$|... #Subtitle level 5: <b>Choosing</b> {{anonymous}} <b>functions</b> {{only for}} lifting ...|$|R
40|$|We {{construct}} {{an example of}} a real plane analytic singular metric, degenerating only at the origin, such that any gradient trajectory (respectively to this singular metric) of some well <b>chosen</b> <b>function</b> spirals around the origin. The inversion mapping carries this example into {{an example of a}} gradient spiraling dynamics at infinity...|$|R
5000|$|In the {{statistical}} learning theory framework, an algorithm {{is a strategy}} for <b>choosing</b> a <b>function</b> [...] given a training set [...] of inputs, , and their labels, [...] (the labels are usually [...] ). Regularization strategies avoid overfitting by <b>choosing</b> a <b>function</b> that fits the data, but is not too complex. Specifically: ...|$|R
50|$|There exist {{a unique}} {{solution}} for a <b>chosen</b> kernel <b>function.</b>|$|R
40|$|We {{show how}} {{appropriately}} <b>chosen</b> <b>functions</b> {{which we call}} distinguishing {{can be used to}} make deterministic finite automata backward deterministic. These ideas can be exploited to design regular language classes identifiable in the limit from positive samples. Special cases of this approach are the k-reversible and terminal distinguishable languages as discussed in [1, 7, 8, 14, 15]...|$|R
40|$|Run Transferable Libraries (RTL) is an {{extension}} for GP where individuals in a population <b>choose</b> <b>functions</b> from an external library of ADF-like functions rather than from a set of standard GP functions. All previous work done with RTL provided a predefined function set. This work investigates mechanisms by which the library can be seeded with domain relevant functionality...|$|R
5000|$|Let [...] have fixed {{load factor}} [...]Bradford and Katehakisshowed the {{expected}} number of probes for an unsuccessful search in , still using these initially <b>chosen</b> hash <b>functions,</b> is [...] {{regardless of the}} distribution of the inputs.More precisely, these two uniformly, randomly and independently <b>chosen</b> hash <b>functions</b> are <b>chosen</b> from a set of universal hash functions where pair-wise independence suffices.|$|R
5000|$|... {{function}} FullRehash(x) is Put all unmarked {{elements of}} T in list L; if (x is in U) [...] append x to L; end if count = length of list L; M = (1 + c) * max{count, 4}; repeat [...] h = randomly <b>chosen</b> <b>function</b> in Hs(M); for all j < s(M) [...] form a list Lj for h(x) = j; bj = length of Lj; [...] mj = 2 * bj; [...] sj = 2 * mj * (mj - 1); end for until {{the sum total}} of all sj ≤ 32 * M2 / s(M) + 4 * M for all j < s(M) [...] Allocate space sj for subtable Tj; repeat [...] hj = randomly <b>chosen</b> <b>function</b> in Hsj; until hj is injective on the elements of list Lj; end for for all x on list Lj [...] store x in position hj(x) of Tj; end for end ...|$|R
5000|$|The dual {{problem with}} respect to the <b>chosen</b> {{perturbation}} <b>function</b> is given by ...|$|R
40|$|AbstractWe {{consider}} {{the problem of}} approximating the global minimum of an r-times continuously differentiable function on the unit interval, based on sequentially <b>chosen</b> <b>function</b> and derivative evaluations. Using a probability model based on the r-fold integrated Wiener measure, we establish a lower bound on the expected number of function evaluations required to approximate the minimum to within ϵ on average...|$|R
5000|$|Every class may in {{addition}} have one function aliased to [...] "", the [...] "bracket" [...] operator, allowing the notation [...] as {{a synonym for}} [...] where [...] is the <b>chosen</b> <b>function.</b> This is particularly useful for container structures such as arrays, hash tables, lists etc. For example access to an element of a hash table with string keys can be written ...|$|R
5000|$|In {{the first}} step, we define {{a new family}} [...] of hash {{functions}} , where each function [...] is obtained by concatenating [...] functions [...] from , i.e., [...] In other words, a random hash function [...] is obtained by concatenating [...] randomly <b>chosen</b> hash <b>functions</b> from [...] The algorithm then constructs [...] hash tables, each corresponding to a different randomly <b>chosen</b> hash <b>function</b> [...]|$|R
30|$|<b>Choosing</b> the <b>function</b> p in Theorems 1 - 6, we {{can obtain}} several new results.|$|R
40|$|Copyright @ 2010 Walter de Gruyter GmbHSome {{modified}} direct localized boundary-domain integral equations (LBDIEs) systems {{associated with}} the mixed boundary value problem (BVP) for a scalar “Laplace” PDE with variable coefficient are formulated and analyzed. The main results established in the paper are the LBDIEs equivalence to the original variable-coefficient BVPs and the invertibility of the corresponding localized boundary-domain integral operators in appropriately <b>chosen</b> <b>function</b> spaces...|$|R
40|$|We {{construct}} a sequence {ϕi(·-j) ∣j∈ℤ,[*][*]i= 1,…,r} which constitutes a p-frame for the weighted shift-invariant space Vμp(Φ) ={∑i= 1 r∑j∈ℤci(j) ϕi(·-j) ∣{ci(j) }j∈ℤ∈ℓμp,[*][*]i= 1,…,r}, p∈[1,∞], and generates a closed shift-invariant subspace of Lμp(ℝ). The first construction is obtained by <b>choosing</b> <b>functions</b> ϕi, i= 1,…,r, with compactly supported Fourier transforms ϕ^i, i= 1,…,r. The second construction, with compactly supported ϕi, i= 1,…,r, gives the Riesz basis...|$|R
5000|$|Ojibwe: Ikwekaazo, [...] "Men who <b>chose</b> to <b>function</b> as women...One who {{endeavors to}} be like a woman." ...|$|R
5000|$|Partition: Using a {{carefully}} <b>chosen</b> mapKey <b>function,</b> divide the A2 into subarrays using {{the keys in}} A ...|$|R
3000|$|Without loss of {{generality}} we can <b>choose</b> the <b>function</b> g to be differentiable on each quantization interval [...]...|$|R
40|$|We {{present and}} analyze an {{improved}} parallel algorithm {{to compute the}} structure of a graph induced by a randomly <b>chosen</b> <b>function</b> on a finite set. The improvements center around how to use the vast memory resources of a parallel computer to store intermediate results, i. e. knowledge about graph nodes that have already been visited. Further improvements and extensions are pointed out. Experiments are presented that confirm the theoretical analysis. ...|$|R
40|$|International audienceThis paper aims to {{describe}} an identification method for Hammerstein systems. The concept of Automatic <b>Choosing</b> <b>Function</b> {{is used to}} approximate the nonlinear static component. The specific coefficients of the ACF and also {{the parameters of the}} linear dynamic component are estimated using linear least squares. Particle Swarm Optimization is used for selecting the widths of the subdomains and the shape of ACF. The method is validated by numerical experiments...|$|R
40|$|AbstractFor an {{irreducible}} admissible {{representation of}} a connected reductive p-adic group, we consider standard intertwining operators holomorphic at zero. Using algebraic methods connected with the structure of linear algebraic groups, we control supports of particularly <b>chosen</b> <b>functions</b> from the induced space. We prove linear independence of standard intertwining operators. This is used to extend {{the definition of the}} R-group from a square integrable representation to its Aubert involutio...|$|R
40|$|In {{this paper}} we present and analyze on-line routing schemes with contant buffer size and hot-potato routing schemes for vertex-symmetric networks. In particular, we prove that for any vertex-symmetric network with n vertices, degree d, and {{diameter}} D =Ω (log n), a randomly <b>chosen</b> <b>function</b> and any permutation can be routed in time [...] O(log n Δ D), with high probability (w. h. p.), if constant size buffers {{are available for}} each edge, [...] O(log n Δ D log 1 +ffl D) for any ffl ? 0, w. h. p., if for each vertex buffers of size 3, independent {{of the degree of}} the network, are available. The schedule for the second result can be converted into a hot-potato routing schedule, if a self-loop is added to each vertex. E. g., for any bounded degree vertex-symmetric network with self-loops and diameter O(log n) (among them expanders) we obtain a hot-potato routing protocol that needs time O(log 2 n(log log n) 1 +ffl) for any ffl ? 0 to route a randomly <b>chosen</b> <b>function</b> and any pe [...] ...|$|R
3000|$|There {{is another}} example for a {{sufficient}} condition: <b>Choose</b> the <b>function</b> f in Theorem  5.4 with the additional property [...]...|$|R
3000|$|... (l) is the bias. Here, we <b>choose</b> sigmoid <b>function</b> as the {{activation}} {{function of the}} layers for spectral feature mining.|$|R
500|$|In other words, every {{value of}} x <b>chooses</b> a <b>{{function}},</b> denoted fx, {{which is a}} function of one real number. That is, ...|$|R
3000|$|... or left {{untouched}} {{depending upon}} {{the context of the}} differential equation. There is no appropriate general method for <b>choosing</b> the <b>function</b> [...]...|$|R
40|$|We {{construct}} a sequence ϕ_i(·-j) | j∈, i= 1, [...] .,r which constitutes a p-frame for the weighted shift-invariant space [V^p_μ(Φ) =∑_i= 1 ^r∑_j∈Zc_i(j) ϕ_i(·-j) | c_i(j) _j∈Z∈ℓ^p_μ, i= 1, [...] .,r, p∈[1,∞],] and generates a closed shift-invariant subspace of L^p_μ(R). The first construction is obtained by <b>choosing</b> <b>functions</b> ϕ_i, i= 1, [...] .,r, with compactly supported Fourier transforms ϕ̂_i, i= 1, [...] .,r. The second construction, with compactly supported ϕ_i,i= 1, [...] .,r, gives the Riesz basis...|$|R
