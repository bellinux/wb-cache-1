0|59|Public
50|$|One of {{the main}} failings of {{endogenous}} growth theories is the collective failure to explain <b>conditional</b> convergence <b>reported</b> in empirical literature.|$|R
40|$|An {{attractive}} {{feature of}} variance-components methods (including the Haseman-Elston tests) {{for the detection}} of quantitative-trait loci (QTL) is that these methods provide estimates of the QTL effect. However, estimates that are obtained by commonly used methods can be biased for several reasons. Perhaps the largest source of bias is the selection process. Generally, QTL effects are reported only at locations where statistically significant results are obtained. This <b>conditional</b> <b>reporting</b> can lead to a marked upward bias. In this article, we demonstrate this bias and show that its magnitude can be large. We then present a simple method-of-moments (MOM) –based procedure to obtain more-accurate estimates, and we demonstrate its validity via Monte Carlo simulation. Finally, limitations of the MOM approach are noted, and we discuss some alternative procedures that may also reduce bias...|$|R
40|$|Synchronization of {{chaotic system}} may occur {{only when the}} largest {{conditional}} Lyapunov exponent of the driven system is negative. The synchronization with positive <b>conditional</b> Lyapunov <b>reported</b> in a recent paper (Phys. Rev. E, 56, 2272 (1997)) is a combined result of the contracting region {{of the system and}} the finite precision in computer simulations. Comment: 6 pages, 7 figure...|$|R
40|$|As part of {{existing}} tasking, the Military Traffic Management Command (MTMC) requested that Oak Ridge National Laboratory (ORNL) assist with writing test scenarios for the formal {{testing of the}} Worldwide Port System (WPS) Regional Integrated Cargo Database (ICDB). In collaboration with MTMC, ORNL wrote almost 600 Test <b>Conditional</b> <b>Reports</b> (TCRs), which were used to test specific functional processes. In addition, ORNL prepared the overall test order, managed tracking of problem reports and code uploads, and interacted with the testers throughout the entire testing period. Because ORNL provided analysis and design for ICDB and because ORNL was intimately involved in development, it was unusual to be so deeply involved in system testing. This document reports on the testing process and on lessons learned. ORNL also assisted MTMC during the initial implementation period and during transition from a developmental to a production system. A maintenance contractor was hired for ICDB, and ORNL assisted this contractor in preparing for system maintenance responsibilities. This document reports on this transition period also...|$|R
40|$|I {{revisit the}} {{question}} whether financial reporting has changed {{after the passage of}} the Sarbanes Oxley (SOX) Act in 2002, with a particular focus on opportunistic earnings management. I employ a sample of more than 400 listed firms in US stock exchanges and for each firm I calculate values for a collection of different measures of unconditional reporting conservatism, earnings management and earnings quality for the years before and after the enforcement of the SOX Act. I employ a war chest of data mining tools and identify interesting attributes in the data. I document that the measures of unconditional reporting conservatism, earnings management and quality are useful predictors of firm level <b>conditional</b> <b>reporting</b> conservatism for both the pre and post SOX years. Comparing the pre and post SOX periods, I provide evidence that in the post SOX year even though firms tend to be reporting more conservative in their Balance Sheets, they also tend to report more opportunistically in their Income Statements. The result is surprising given that the SOX act has been enforced in response to a series of corporate scandals entailing extensive earnings management...|$|R
3000|$|..., {{which is}} {{statistically}} significant at any significance level. Therefore, {{there is evidence}} that the data are highly over-dispersed even after conditioning on the regressors. As far as the reporting process is concerned, this model predicts that the average <b>conditional</b> probability of <b>reporting</b> a committed crime, calculated as [...]...|$|R
30|$|Workers {{should be}} {{protected}} more through unemployment insurance rather than high employment protection (the flexicurity model). Unemployment insurance can be generous but {{only if it is}} coordinated with effective active labor market policies. Unemployment benefits should be <b>conditional</b> on <b>reported</b> job search, on training, and on job acceptance if acceptable jobs are available. A good vocational training system for adults coordinated with unemployment insurance is also essential. In practice, however, implementing effective active labor market policies is difficult and costly. The empirical evidence on the success of retraining programs is mixed; they have to be done right, and often they are not 17.|$|R
30|$|Prior to 1980, the Census {{did not ask}} {{the usual}} hours worked so we used hours last week as a proxy. In 1980, <b>conditional</b> on both <b>reports</b> being positive, the {{correlation}} is only 0.61. While quite low, this correlation is not materially different than those found in validation studies; see Barron et al. (1997) for a discussion.|$|R
40|$|Recent results {{regarding}} {{the performance of}} mutual fund managers is mixed {{in terms of their}} ability to select winning stocks. Unlike previous studies we isolate the selectivity performance of 85 Canadian equity funds by examining their excess monthly returns <b>conditional</b> on their <b>reported</b> sector and asset allocations. In general we find little evidence of superior selectivity performance, particularly after management expenses...|$|R
5000|$|Goldstone, however, {{explained}} {{that what he}} had headed wasn't an investigation, but a fact-finding mission. [...] "If this was a court of law, {{there would have been}} nothing proven", Goldstone said, emphasizing that his conclusion that war crimes had been committed was always intended as <b>conditional.</b> Nevertheless, the <b>report</b> itself is replete with bold and declarative legal conclusions seemingly at odds with the cautious and conditional explanations of its author.|$|R
30|$|Conventionally, {{standard}} errors of propensity score matching estimates are obtained using bootstrap methods, but with large samples {{such as those}} available to this study, it is not feasible to calculate bootstrap {{standard errors}} for all estimates. We have chosen to <b>report</b> <b>conditional</b> standard errors using methods recommended by Imbens and Wooldridge (2008). We undertook limited comparisons with bootstrap standard errors and found similar results. Additional details {{are included in the}} Additional file 1.|$|R
40|$|Harassment bribes - {{payments}} {{people give}} {{in order not}} to be denied what they are legally entitled to – are common in for example India. Kaushik Basu recently made a '’radical'’ proposal to reduce its occurrence: Legalize the act of giving the bribe and double the fine for accepting the bribe! We develop a formal model and delineate circumstances under which Basu’s proposal works well or poorly. We discuss a modified scheme where immunity is <b>conditional</b> on <b>reporting</b> that we argue addresses the main issues raised against the proposal. We highlight complementarities between these schemes and other policies aimed a improving the accountability and performance of the public sector, and of law enforcement agencies in particular. We conclude discussing the implications for the fight of more harmful forms of corruption. Bribes; Corruption; Governance; Immunity; Law enforcement; Leniency; Whistleblowers. ...|$|R
40|$|Abstract. In SAC 2010, Sepehrdad, Vaudenay and Vuagnoux have re-ported some {{empirical}} biases {{between the}} secret key, the internal state {{variables and the}} keystream bytes of RC 4, by searching over a space of all linear correlations between the quantities involved. In this paper, for the first time, we give theoretical proofs for all such significant em-pirical biases. Our analysis not only builds a framework to justify the origin of these biases, it also brings out several new conditional biases of high order. We establish that certain <b>conditional</b> biases <b>reported</b> earlier are potentially non-causal in nature as they are correlated with a third event with much higher probability. This {{gives rise to the}} discovery of new keylength-dependent biases of RC 4, some as high as 50 /N. The new biases in turn result in successful keylength prediction from the initial keystream bytes of the cipher...|$|R
40|$|Synchronization of {{chaotic system}} may occur {{only when the}} largest {{conditional}} Lyapunov exponent of the driven system is negative. The synchronization with positive <b>conditional</b> Lyapunov <b>reported</b> in a recent paper (Phys. Rev. E, 56, 2272 (1997)) is a combined result of the contracting region {{of the system and}} the finite precision in computer simulations. PACS number(s) : 05. 45. +b; 1 Sensitivity to initial conditions is a generic feature of chaotic dynamical systems. Two chaotic orbits, starting from slightly different initial points in the state space, separate exponentially with time, and become totally uncorrelated. As a result, independent identical chaotic systems cannot synchronize with each other. The sensitivity is quantitatively described by positive Lyapunov exponent(s) in the Lyapunov exponent spectrum of the chaotic system. However, chaotic systems linked by common signal can synchronize with each other. Several cases could be distinguished. In the first case, a replica subsystem driven by chaoti...|$|R
40|$|Synthetic {{biological}} {{tools that}} enable precise regulation of gene function within in vivo systems have enormous potential to discern gene function in diverse physiological settings. Here we report {{the development and}} characterization of a synthetic gene switch that, when targeted in the mouse germline, enables <b>conditional</b> inactivation, <b>reports</b> gene expression and allows inducible restoration of the targeted gene. Gene inactivation and reporter expression is achieved through Cre-mediated stable inversion of an integrated gene-trap reporter, whereas inducible gene restoration is afforded by Flp-dependent deletion of the inverted gene trap. We validate our approach by targeting the p 53 and Rb genes and establishing cell line and in vivo cancer model systems, to study the impact of p 53 or Rb inactivation and restoration. We term this allele system XTR, to denote each of the allelic states and the associated expression patterns of the targeted gene: eXpressed (XTR), Trapped (TR) and Restored (R) ...|$|R
30|$|Techniques base on {{the signal}} {{distortion}} reduce high peaks by non-linearly distorting the OFDM signal. Due to the simple implementation together with the high PAPR gains, an approach based on signal distortion has become a favored PAPR reduction technology. According to literature, the major drawback of these techniques lies in the BER performance degradation [11]. However, {{it is possible to}} improve the BER performance via upsampling and <b>conditional</b> filtering as <b>reported</b> in [6]. In the following, we provide a brief overview of PAPR reduction algorithms based on the signal distortion.|$|R
40|$|There is some {{confusion}} in political science, {{and the social}} sciences in general, about the meaning and interpretation of interaction effects in models with non-interval, non-normal outcome variables. Often these terms are casually thrown into a model specification without observing that their presence fundamentally changes the interpretation of the resulting coefficients. This article explains the <b>conditional</b> nature of <b>reported</b> coefficients in models with interactions, defining the necessarily different interpretation required by generalized linear models. Methodological issues are illustrated with an application to voter information structured by electoral systems and resulting legislative behavior and democratic representation in comparative politics...|$|R
40|$|Using a {{previously}} <b>reported</b> <b>conditional</b> expression system {{for use in}} Bacillus subtilis (A. P. Bhavsar, X. Zhao, and E. D. Brown, Appl. Environ. Microbiol. 67 : 403 – 410, 2001), we report the first precise deletion of a teichoic acid biosynthesis (tag) gene, tagD, in B. subtilis. This teichoic acid mutant showed a lethal phenotype when characterized at a physiological temperature and in a defined genetic background. This tagD mutant was subject to full phenotypic rescue upon expression of the complementing copy of tagD. Depletion of the tagD gene product (glycerol 3 -phosphate cytidylyltransferase) via modulated expression of tagD from the amyE locus revealed structural defects centered on shape, septation, and division. Thickening of the wall and ultimately lysis followed these events...|$|R
40|$|Summary: Pre-experimental Frequentist error probabilities do not {{summarize}} adequately {{the strength}} of evidence from data. The Conditional Frequentist paradigm overcomes this problem by selecting a “neutral ” statistic S to reflect {{the strength of}} the evidence and <b>reporting</b> a <b>conditional</b> error probability, given the observed value of S. We introduce a neutral statistic S that makes the <b>Conditional</b> Frequentist error <b>reports</b> identical to Bayesian posterior probabilities of the hypotheses. In symmetrical cases we can show this strategy to be optimal from the Frequentist perspective. A Conditional Frequentist who uses such a strategy can exploit the consistency of the method with the Likelihood Principle—for example, the validity of sequential hypothesis tests even if the stopping rule is informative or is incompletely specified. 1...|$|R
40|$|Poly-ubiquitination {{of target}} {{proteins}} typically marks them for destruction via the proteasome and provides an essential {{mechanism for the}} dynamic control of protein levels. The E 1 ubiquitin-activating enzyme lies {{at the apex of}} the ubiquitination cascade, and its activity is necessary for all subsequent steps in the reaction. We have isolated a temperature-sensitive mutation in the Caenorhabditis elegans uba- 1 gene, which encodes the sole E 1 enzyme in this organism. Manipulation of UBA- 1 activity at different developmental stages reveals a variety of functions for ubiquitination, including novel roles in sperm fertility, control of body size, and sex-specific development. Levels of ubiquitin conjugates are substantially reduced in the mutant, consistent with reduced E 1 activity. The uba- 1 mutation causes delays in meiotic progression in the early embryo, a process that is known to be regulated by ubiquitin-mediated proteolysis. The uba- 1 mutation also demonstrates synthetic lethal interactions with alleles of the anaphase-promoting complex, an E 3 ubiquitin ligase. The uba- 1 mutation provides a sensitized genetic background for identifying new in vivo functions for downstream components of the ubiquitin enzyme cascade, and {{it is one of the}} first <b>conditional</b> mutations <b>reported</b> for the essential E 1 enzyme in a metazoan animal model...|$|R
40|$|This paper {{addresses}} {{the issue of}} how negative components affect people's ability to draw-conditional inferences. The study was motivated by an attempt to resolve a difficulty for the mental models theory of Johnson-Laird and Byrne, whose account of matching bias in the selection task is apparently inconsistent with Johnson-Laird's explanation of the double negation effects in <b>conditional</b> inference <b>reported</b> by Evans, Clibbens, and Rood (1995). Two experiments are reported, which investigate frequencies of conditional inferences with task presentation {{similar to that of the}} selection task in two respects: the presence of a picture of four cards and the use of implicit negations in the premises. The latter variable was shown to be critical and demonstrated a new phenomenon: Conditional inferences of all kinds are substantially suppressed when based on implicitly negative premises. This phenomenon was shown to operate independently of and in addition to the double negation effect. A third experiment showed that the implicit negation effect could be extended to the paradigm in which people are asked to produce their own conclusions. It is argued that these two effects can be explained within either the mental models theory or the inference rule theory, of propositional reasoning, but that each will require some revision in order to offer a convincing account. 31 page(s...|$|R
40|$|A bivariate probit {{model with}} sample {{selection}} {{is used to}} estimate the <b>conditional</b> probability of <b>reporting</b> a need for personal assistance (NPA) {{with at least one}} activity of daily living among French community-dwelling elderly. 71. 8 % of men and 77. 3 % of women reported impairments and among those who reported impairments, 7. 5 % of men and 10. 8 % of women reported NPA. NPA is associated not only with age (i. e., the oldest individuals, for women only) and health status (such as a specific type of impairment), but also with socioeconomic (living with intermediate income; living with someone, partner or other) and environmental factors (having and using assistive technologies). Elderly Impairments Need for personal assistance Andersen and Newman' s behavioral model Bivariate probit model with sample selection...|$|R
40|$|Markov {{models have}} been widely used for {{modelling}} users’ web navigation behaviour. In previous work we have presented a dynamic clustering-based Markov model that accurately represents second-order transition probabilities given by a collection of navigation sessions. Herein, we propose a generalisation of the method that takes into account higher-order conditional probabilities. The method makes use of the state cloning concept together with a clustering technique to separate the navigation paths that reveal differences in the <b>conditional</b> probabilities. We <b>report</b> on experiments conducted with three real world data sets. The results show that some pages require a long history to understand the users choice of link, while others require only a short history. We also show that the number of additional states induced by the method can be controlled through a probability threshold parameter...|$|R
40|$|This paper {{develops}} a second-order Newton algorithm for finding local solutions of rank-constrained LMI problems in robust synthesis. The algorithm {{is based on}} a quadratic approximation of a suitably defined merit function and generates sequences of LMI feasible iterates. The main trust of the algorithm is that it inherits the good local convergence properties of Newton methods and thus overcome the difficulties encountered with earlier methods such as the Frank & Wolfe or conditional gradient methods which tend to be very slow in the neighborhood of a local solution. Moreover, it is easily implemented using available Semi-Definite Programming (SDP) codes. Proposed algorithms have proven global and local convergence properties and thus represent improvements over classically used D-K iteration schemes but also outperform earlier <b>conditional</b> gradient algorithms. <b>Reported</b> computational results demonstrate these facts...|$|R
40|$|Abstract. We {{study the}} problem of multiclass {{classification}} {{within the framework of}} error correcting output codes (ECOC) using margin-based binary classifiers. An important open problem in this context is how to measure the distance between class codewords and the outputs of the classifiers. In this paper we propose a new decoding function that combines the margins through an estimate of their class <b>conditional</b> probabilities. We <b>report</b> experiments using support vector machines as the base binary classifiers, showing the advantage of the proposed decoding function over other functions of the margin commonly used in practice. We also present new theoretical results bounding the leave-one-out error of ECOC of kernel machines, which can be used to tune kernel parameters. An empirical validation indicates that the bound leads to good estimates of kernel parameters and the corresponding classifiers attain high accuracy...|$|R
40|$|Keywords: Purpose: To {{investigate}} the online communications and self-disclosure practices of youth reporting depressive symptomatology. Method: The Youth Internet Safety Survey was a nationally representative telephone survey of 1501 Internet-using youth {{between the ages}} of 10 and 17 years, and one caregiver in their household. Fifty-three percent of youth participants were male and 73 % were white race. The purpose of the survey was to obtain prevalence rates for unwanted sexual solicitation, harassment, and unwanted exposure to sexual material among young people online. Questions about current depressive symptomatology were also queried; this variable was defined based upon the DSM-IV definition of a major depressive episode: major depressive-like symptomatology (5 � symptoms of depression and functional impairment in at least one area); minor depressive-like symptomatology (3 � symptoms of depression); mild or no depressive symptomatology (� 3 symptoms of depression). Data were cross-sectional and collected between the fall of 1999 and spring 2000. Multinomial logistic regression was used to estimate the <b>conditional</b> odds of <b>reporting</b> DSM-IV-like major o...|$|R
40|$|We {{investigate}} {{the evolution of}} axis ratios of triaxial haloes using the phase space description of triaxial collapse. In this formulation, {{the evolution of the}} triaxial ellipsoid is described in terms of the dynamics of eigenvalues of three important tensors: the Hessian of the gravitational potential, the tensor of velocity derivatives and the deformation tensor. The eigenvalues of the deformation tensor are directly related to the parameters that describe triaxiality, namely, the minor to major and intermediate to major axes ratios (s and q) and the triaxiality parameter T. Using the phase space equations, we evolve the eigenvalues and examine the evolution of the PDF (probability distribution function) of the axes ratios as a function of mass scale and redshift for Gaussian initial conditions. We find that the ellipticity and prolateness increase with decreasing mass scale and decreasing redshift. These trends agree with previous analytic studies but differ from numerical simulations. However, the PDF of the scaled parameter q̃ = (q-s) /(1 -s) follows a universal distribution over two decades in mass range and redshifts which is in qualitative agreement with the universality for <b>conditional</b> PDF <b>reported</b> in simulations. We further show using the phase space dynamics that, in fact, q̃ is a phase space invariant and is conserved individually for each halo. These results, demonstrate that the phase space analysis is a useful tool that provides a different perspective on the evolution of perturbations and can be applied to more sophisticated models in the future. Comment: 10 pages, 5 figures submitted to MNRA...|$|R
40|$|Preexperimental frequentist error probabilities are arguably inadequate, as {{summaries}} of evidence from data, in many hypothesis-testing settings. The conditional frequentist may {{respond to this}} by identifying certain subsets of the outcome space and <b>reporting</b> a <b>conditional</b> error probability, given the subset of the outcome space in which the observed data lie. Statistical methods consistent with the likelihood principle, including Bayesian methods, avoid the problem by a more extreme form of conditioning. In this paper we prove that the conditional frequentist 2 ̆ 7 s method can be made exactly equivalent to the Bayesian 2 ̆ 7 s in simple versus simple hypothesis testing: specifically, we find a conditioning strategy for which the conditional frequentist 2 ̆ 7 s <b>reported</b> <b>conditional</b> error probabilities {{are the same as}} the Bayesian 2 ̆ 7 s posterior probabilities of error. A conditional frequentist who uses such a strategy can exploit other features of the Bayesian approach [...] for example, the validity of sequential hypothesis tests (including versions of the sequential probability ratio test, or SPRT) even if the stopping rule is incompletely specified...|$|R
40|$|SummaryEpigenetic {{regulation}} of hematopoietic stem cells (HSCs) ensures lifelong production {{of blood and}} bone marrow. Recently, we reported that loss of de novo DNA methyltransferase Dnmt 3 a results in HSC expansion and impaired differentiation. Here, we <b>report</b> <b>conditional</b> inactivation of Dnmt 3 b in HSCs either alone or combined with Dnmt 3 a deletion. Combined loss of Dnmt 3 a and Dnmt 3 b was synergistic, resulting in enhanced HSC self-renewal and a more severe block in differentiation than in Dnmt 3 a-null cells, whereas loss of Dnmt 3 b resulted in a mild phenotype. Although the predominant Dnmt 3 b isoform in adult HSCs is catalytically inactive, its residual activity in Dnmt 3 a-null HSCs can drive some differentiation and generates paradoxical hypermethylation of CpG islands. Dnmt 3 a/Dnmt 3 b-null HSCs displayed activated β-catenin signaling, partly accounting for the differentiation block. These data demonstrate distinct roles for Dnmt 3 b in HSC differentiation and provide insights into complementary de novo methylation patterns governing {{regulation of}} HSC fate decisions...|$|R
40|$|We {{investigated}} {{the association of}} the intensity of newspaper reporting of charcoal burning suicide with the incidence of such deaths in Taiwan during 1998 - 2002. A counting process approach was used to estimate the incidence of suicides and intensity of news <b>reporting.</b> <b>Conditional</b> Poisson generalized linear autoregressive models were performed to assess the association of the intensity of newspaper reporting of charcoal burning and non-charcoal burning suicides with {{the actual number of}} charcoal burning and non-charcoal burning suicides the following day. We found that increases in the reporting of charcoal burning suicide were associated with increases in the incidence of charcoal burning suicide on the following day, with each reported charcoal burning news item being associated with a 16 % increase in next day charcoal burning suicide (p<. 0001). However, the reporting of other methods of suicide was not related to their incidence. We conclude that extensive media reporting of charcoal burning suicides appears to have contributed to the rapid rise in the incidence of the novel method in Taiwan during the initial stage of the suicide epidemic. Regulating media reporting of novel suicide methods may prevent an epidemic spread of such new methods...|$|R
40|$|Policymakers {{involved}} in climate change negotiations are key users of climate science. It is therefore vital {{to understand how}} to communicate scientific information most effectively to this group. We tested how a unique sample of policymakers and negotiators at the Paris COP 21 conference update their beliefs on year 2100 global mean temperature increases in response to a statistical summary of climate models' forecasts. We randomized the way information was provided across participants using three different formats similar to those used in Intergovernmental Panel on Climate Change reports. In spite of having received all available relevant scientific information, policymakers adopted such information very conservatively, assigning it less weight than their own prior beliefs. However, providing individual model estimates in addition to the statistical range was more effective in mitigating such inertia. The experiment was repeated with a population of European MBA students who, despite starting from similar priors, <b>reported</b> <b>conditional</b> probabilities closer to the provided models' forecasts than policymakers. There was also no effect of presentation format in the MBA sample. These results highlight the importance of testing visualization tools directly on the population of interest...|$|R
40|$|Keywords: B-ISDN, congestion, ATM, {{statistical}} multiplexer, cell-loss. Cell loss {{is often}} assumed to occur at random. However, when an ATM cell {{is lost because}} of buffer overflow at a statistical multiplexer, there is an altered probability that the cells following it will be lost. In this <b>report,</b> <b>conditional</b> probabilities of cell-loss due to congestion are modelled mathematically, and then simulations used to verify the models are described. Conditional probabilities are modelled and simulated using M/M/ 1 /K, MID/ 1 /K, M/D/ 1 /K with minimum interarrival times and SPP/D/ 1 /K queues. Results from the models and simulations show that cell loss exhibits 'bursty' behaviour. If one cell is lost, the probability of following cells being lost is substantially higher than the overall cell loss probability. Consequently, cell loss has to be characterised by {{more than just an}} average rate. The average and variance of the number of cells lost are proposed as additional statistics to characterise cell loss caused by buffer overflow. In this paper these statistics are calculated and tabulated for M/M/ 1 /K and M/D/ 1 /K multiplexer models. Some of the repercussions of the bursty nature of cell loss for Broadband services are discussed...|$|R
40|$|Using a {{conditional}} life {{or death}} screen in yeast, we have isolated a tomato (Lycopersicon esculentum) gene encoding a phospholipid hydroperoxide glutathione peroxidase (LePHGPx). The protein displayed reduced glutathione-dependent phospholipid hydroperoxide peroxidase activity, but differs from counterpart mammalian enzymes that instead contain an active seleno-Cys. LePHGPx functioned as a cytoprotector in yeast (Saccharomyces cerevisiae), preventing Bax, hydrogen peroxide, and heat stress induced cell death, while also delaying yeast senescence. When tobacco (Nicotiana tabacum) leaves were exposed to lethal levels of salt and heat stress, features associated with mammalian apoptosis were observed. Importantly, transient expression of LePHGPx protected tobacco leaves from salt and heat stress and suppressed the apoptotic-like features. As has been <b>reported,</b> <b>conditional</b> expression of Bax was lethal in tobacco, resulting in tissue collapse and membrane permeability to Evans blue. When LePHGPx was coexpressed with Bax, little cell death and no vital staining were observed. Moreover, stable expression of LePHGPx in tobacco conferred protection against the fungal phytopathogen Botrytis cinerea. Taken together, our data indicated that LePHGPx can protect plant tissue {{from a variety of}} stresses. Moreover, functional screens in yeast are a viable tool for the identification of plant genes that regulate cell death...|$|R
40|$|There is {{substantial}} confusion {{in political science}} and related literatures about the meaning and interpreta-tion of interaction effects in models with non-interval, non-normal outcome variables. Often these terms are casually thrown into the model specification without observing that their presence fundamentally changes {{the interpretation of the}} resulting coefficients. This article addresses this lack of clarity and rigor by explaining the <b>conditional</b> nature of <b>reported</b> coefficients and their standard errors in models with interactions, defining the necessarily different interpretation of interactions in generalized linear models, and introducing useful hi-erarchies of interaction effects in these specifications. Unfortunately, there is little written about specifying the uncertainty of these interaction effects through reported standard errors, and much of which is currently reported is actually misleading. This article fills a gap in the methodological literature by providing a general analytical method for correctly calculating coefficient standard errors in models with second-order or higher interactions and complex interaction specifications. The methodology is demonstrated with applications to current work in political science. These examples demonstrate the utility of interaction hierarchy specifications in generalized linear models by providing analyses of data from comparative politics, judicial decision-making, and education public policy...|$|R
40|$|Estimates from {{economic}} panel surveys are generally {{required to be}} published soon after the survey reference period, resulting in missing data due to late reporting as well as nonresponse. Estimators currently in use make some attempt to correct {{for the impact of}} missing data. However, these approaches tend to simplify the assumed nature of the missing data and often ignore a portion of the reported data for the reference period. Discrepancies between preliminary and revised estimates highlight the inability of the estimation methodology to correct for all error due to late reporting. The current model for one economic panel survey, the Current Employment Statistics survey, is examined to identify factors related to potential model misspecification error, leading to identification of an extended model. An approach is developed to utilize all reported data from the current and prior reference periods, through missing data imputation. Two alternatives to the current models that assume growth rates are related to recent reported data and reporting patterns are developed, one a simple proportional model, the other a hierarchical fixed effects model. Estimation under the models is carried out and performance compared to that of the current estimator through use of historical data from the survey. Results, although not statistically significant, suggest the potential associated with use of reported data from recent time periods in the working model, especially for smaller establishments. A logistic model for predicting likelihood of late reporting for sample units that did not report for preliminary estimates is also developed. The model uses a combination of operational, respondent, and environmental factors identified from a reporting pattern profile. Predicted <b>conditional</b> late <b>reporting</b> rates obtained under the model are compared to actual rates through use of historical information for the survey. Results indicate the appropriateness of the parameters chosen and general ability of the model to predict final reporting status. Such a model has the potential to provide information to survey managers for addressing late reporting and nonresponse...|$|R
30|$|Bound et al. (2001) {{summarise}} {{a number}} of the approaches that have been suggested for dealing with measurement error. One popular approach relies on the availability of auxiliary data. While auxiliary data allow researchers to examine the nature of measurement error, these data typically do not contain information on the dependent variable of interest. As a result the information gained from the auxiliary data must be “transported” into the main survey data. One approach is to use the auxiliary data to estimate the expected value of the true variable <b>conditional</b> on the <b>reported</b> value and then to use this model to estimate expected BMI in the original survey data. This approach is known as the “conditional expectation (CE)” approach (Lyles and Kupper 1997) or the “regression calibration” approach (Guo and Little 2011) and has been used to correct for measurement error in previous studies looking at the impact of obesity on labour market outcomes (Cawley 20002004;Lindeboom et al. 2010). A second approach uses the instrumental variable (IV) estimator to obtain consistent estimates in the presence of measurement error. 2 With classical measurement error, both of these approaches provide consistent estimates but it is easy to show that these standard approaches do not work if measurement error is non-classical.|$|R
