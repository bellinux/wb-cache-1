21|14|Public
50|$|Compiler {{correctness}} is {{the branch}} of software engineering that deals with trying {{to show that a}} compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called <b>compiler</b> <b>validation)</b> on an existing compiler.|$|E
50|$|MALPAS {{has been}} used to confirm the {{correctness}} of safety critical applications in the nuclear, aerospace and defence industries. It has also been used to provide <b>compiler</b> <b>validation</b> in the nuclear industry on Sizewell B. Languages that have been analysed include: Ada, C, PLM and Intel Assembler.|$|E
50|$|DDC-I was in {{the same}} market as several other Ada {{compiler}} firms, including Alsys, TeleSoft, Verdix, Tartan Laboratories, and TLD Systems. (DDC-I would go on to stay in business longer than any of these others.)As with other Ada compiler vendors, much of the time of DDC-I engineers was spent in conforming to the large, difficult Ada <b>Compiler</b> <b>Validation</b> Capability (ACVC) standardized language and runtime test suite.|$|E
40|$|Custom {{computing}} involves customising computations {{for one or}} more {{applications in}} a given implementation technology. We describe a framework for customising designs using appropriate libraries, <b>compilers,</b> <b>validation</b> facilities, application programming interfaces and front-end tools. The development of custom architectures, data formats and operations is presented. We show how circuits can be customised at run time to adapt {{to changes in the}} operating conditions. Graphics examples are used throughout the paper to illustrate our approach. ...|$|R
50|$|Critics {{argue the}} need for extra ballots in any {{language}} can be mitigated by providing a process to print ballots at voting locations. They argue further, the cost of software <b>validation,</b> <b>compiler</b> trust <b>validation,</b> installation validation, delivery validation and validation of other steps related to electronic voting is complex and expensive, thus electronic ballots are not guaranteed to be less costly than printed ballots.|$|R
40|$|International audienceTranslation {{validation}} {{consists of}} transforming a {{program and a}} posteriori validating {{it in order to}} detect a modification of its semantics. This approach {{can be used in a}} verified <b>compiler,</b> provided that <b>validation</b> is formally proved to be correct. We present two such validators and their Coq proofs of correctness. The validators are designed for two instruction scheduling optimizations: list scheduling and trace scheduling...|$|R
5000|$|<b>Compiler</b> <b>validation</b> with formal methods {{involves}} a long chain of formal, deductive logic. [...] However, since the tool {{to find the}} proof (theorem prover) is implemented in software and is complex, {{there is a high}} probability it will contain errors. One approach has been to use a tool that verifies the proof (a proof checker) which because it is much simpler than a proof-finder is less likely to contain errors.|$|E
5000|$|The cash {{infusion}} from Prudential-Bache {{was also}} used to develop a compiler system for the Ada programming language, targeted to the MIL-STD-1750A architecture. [...] This consisted of a compiler front-end licensed from DDC-I in Denmark (itself {{an offshoot of the}} Dansk Datamatik Center) married to a compiler back-end from ACT that made use of the company's existing tools for the MIL-STD-1750A. [...] ACT became the first U.S. company to successfully validate an Ada 1750A compiler past the strenuous Ada <b>Compiler</b> <b>Validation</b> Capability (ACVC) validation suite. Between JOVIAL and Ada, the company would gain a number of high-profile defense contractors as customers throughout the 1980s.|$|E
5000|$|The {{origins of}} DDC International A/S lay in Dansk Datamatik Center, a Danish {{software}} {{research and development}} organization that was formed in 1979 to demonstrate the value of using modern techniques, especially those involving formal methods, in software design and development. Among its several projects {{was the creation of}} a compiler system for the programming language Ada. Ada was a difficult language to implement and early compiler projects for it often proved disappointments. [...] But the DDC compiler design was sound and it first passed the United States Department of Defense-sponsored Ada <b>Compiler</b> <b>Validation</b> Capability (ACVC) tests on a VAX/VMS system in September 1984. [...] As such, it was the first European Ada compiler to meet this standard.|$|E
40|$|Translation {{validation}} {{consists of}} transforming a {{program and a}} posteriori validating {{it in order to}} detect a modification of its semantics. This approach {{can be used in a}} verified <b>compiler,</b> provided that <b>validation</b> is formally proved to be correct. We present two such validators and their Coq proofs of correctness. The validators are designed for two instruction scheduling optimizations: list scheduling and trace scheduling. Categories and Subject Descriptors F. 3. 1 [Logics and Meaning...|$|R
5000|$|Testing {{represents}} {{a significant portion}} of the effort in shipping a compiler, but receives comparatively little coverage in the standard literature. The 1986 edition of Aho, Sethi, & Ullman has a single-page section on compiler testing, with no named examples. [...] The 2006 edition omits the section on testing, but does emphasize its importance: “Optimizing compilers are so difficult to get right that we dare say that no optimizing compiler is completely error-free! Thus, the most important objective in writing a compiler is that it is correct.”Fraser & Hanson 1995 has a brief section on regression testing; source code is available.Bailey & Davidson 2003 cover testing of procedure callsA number of articles confirm that many released compilers have significant code-correctness bugs.Sheridan 2007 is probably the most recent journal article on general compiler testing.Commercial <b>compiler</b> compliance <b>validation</b> suites are available from Solid Sands, Perennial, and Plum-Hall.For most purposes, the largest body of information available on compiler testing are the Fortran and Cobol validation suites.|$|R
40|$|International audienceDesign of {{critical}} embedded systems demands for guarantees on {{the reliability of}} the implementation/compilation of a specification. In general, this guarantee takes either the form of a certified <b>compiler,</b> or the <b>validation</b> of each translation. Here we adopt the translation validation approach. In particular, we translate both the Signal specification and the associated C simulator into LTSs. Then, an appropriate (successful) preorder test between both LTSs can be interpreted as a refinement between the C implementation and its source Signal specification, otherwise, counter-examples are generated automatically. The feasibility of our approach is shown through examples...|$|R
40|$|This paper {{discusses}} Ada <b>Compiler</b> <b>Validation</b> {{from the}} viewpoint of both the compiler supplier and the compiler user. The objectives of the requirement for validation and the resulting benefits, as well as the limitations of validation are presented. The process of Ada <b>compiler</b> <b>validation</b> is detailed along with the various problems as well as issues relating to validation and the use of validated compilers. Solutions to these problems and issues are explained, as they are proposed in a recent AJPO draft of a revised validation policy...|$|E
40|$|This paper {{describes}} the project Quality-for-ASIS, aiming at {{the development of}} an extensive testing facility for ASIS implementations. First the specific problems and requirements are presented. After a section about the basic concepts of ASIS and after a short introduction to testing, the designs and implementations for testing important subsets of ASIS are described. Finally, adequacy coverage statistics for a test set based on the ACVC <b>compiler</b> <b>validation</b> suite are provided...|$|E
40|$|This {{paper is}} to present a security-related {{motivation}} for compiler verification, and in particular for binary compiler implementation verification. We will prove that source level verification is not sufficient in order to guarantee compiler correctness. For this, we will adopt the scenario of a well-known attack to Unix operating system programs due to intruded Trojan Horses in compiler executables. Such a compiler will pass nearly every test, {{state of the art}} <b>compiler</b> <b>validation,</b> the strong bootstrap test, any amount of source code inspection and verification, but for all that, it nevertheless might eventually cause a catastrophe...|$|E
40|$|International audienceTranslation {{validation}} {{was introduced}} in the 90 's by Pnueli et al. as a technique to formally verify the correctness of code generators. Rather than certifying the code generator or exhaustively qualifying it, translation validators attempt to verify that program transformations preserve semantics. In this work, we adopt this approach to formally verify that the clock semantics and data dependence are preserved during the compilation of the Signal <b>compiler.</b> Translation <b>validation</b> is implemented for every compilation phase from the initial phase until the latest phase where the executable code is generated, by proving that the transformation in each phase of the compiler preserves the semantics. We represent the clock semantics, the data dependence of a program and its transformed counterpart as first-order formulas which are called Clock Models and Synchronous Dependence Graphs (SDGs), respectively. Then we introduce clock refinement and dependence refinement relations which express the preservation of clock semantics and dependence, as a relation on clock models and SDGs, respectively. Our validator does not require any instrumentation or modification of the compiler, nor any rewriting of the source program...|$|R
40|$|Several {{analytical}} {{models that}} predict the memory hierarchy behavior of codes with regular access patterns have been developed. These models help understand this behavior {{and they can}} be used successfully to guide compilers in the application of locality-related optimizations requiring small computing times. Still, these models suffer from many limitations. The most important of them is their restricted scope of applicability, since real codes exhibit many access patterns they 13 cannot model. The most common source of such kind of accesses is the presence of irregular access patterns because of 14 the presence of either data-dependent conditionals or indirections in the code. This paper extends the probabilistic miss equations (PME) model to be able to cope with codes that include data-dependent conditional structures too. This approach is systematic enough to enable the automatic implementation of the extended model in a <b>compiler</b> framework. <b>Validations</b> show a good degree of accuracy in the predictions despite the irregularity of the access patterns. This opens the possibility of using our model to guide compiler optimizations for this kind of codes...|$|R
40|$|In {{this paper}} {{we present a}} binary {{instrumentation}} methodology to monitor runtime events. We demonstrate our approach on OpenMP constructs for the Intel and GNU compilers. A binary-level static analysis detects the compiler patterns and the runtime function calls corresponding to OpenMP regions. To this effect we integrate the software tool MAQAO with the scalable measurement infrastructure Score-P. We design a new interface and modify both tools to support the new events. The main advantages of using binary instrumentation are the possibility to retrieve implicit runtime events, to instrument without recompilation, to be independent from the language, and not to interact with <b>compiler</b> optimization. Our <b>validation</b> experiments and first results shows that binary instrumentation has not introduced any additional overhead...|$|R
40|$|Energy {{optimization}} of {{embedded software}} is of primary importance. Nevertheless, there {{is lack of}} accurate and usable methodologies and tools to estimate software performance (execution time, energy) and to allow a significant exploration of design alternatives. Current approaches use either instruction-level simulation (accurate but slow), or static-time source characterization (flexible but data-independent). This paper proposes a hybrid approach {{taking advantage of the}} strengths of both the above approaches. We present a fully automatic method for estimating the execution time and power consumption of a C program - run on a given architecture on given input data - based on statistically-accurate models for the architecture and for the <b>compiler.</b> <b>Validation</b> results against an ARM energy-enabled instruction-level simulator show an average absolute relative errors of 8. 5 %...|$|E
40|$|This paper {{outlines}} a {{case study}} at SGS-Thomson Microelec-tronics {{on the development of}} a firmware development environment in cooperation with Thomson Consumer Electronics Components. The enviornment is for an embedded processor used for audio decompression algorithms including: MPEG 2, Dolby AC- 3 Surround, and Dolby Pro-logic. The enabling component of the firmware environment is a retargetable compiler which maps high-level algorithms onto the embedded processor. Although compilation is the critical technology, this experience has shown that it is insufficient and that other supporting design tools are also important. For this project, that environment includes an instruction-set simulator, a source-level debugger, a custom linker, and a <b>compiler</b> <b>validation</b> strategy. The methodologies are outlined in this paper with an emphasis on the lessons learned in this hardware-software team development. ...|$|E
40|$|AbstractTranslation {{validation}} is {{a technique}} for ensuring that a translator, such as a compiler, produces correct results. Because complete verification of the translator itself is often infeasible, translation validation advocates coupling the verification task with the translation task, so that each run of the translator produces verification conditions which, if valid, prove the correctness of the translation. In previous work, the translation validation approach was used to give a framework for proving the correctness {{of a variety of}} compiler optimizations, with a recent focus on loop transformations. However, some of these ideas were preliminary and had not been implemented. Additionally, there were examples of common loop transformations which could not be handled by our previous approaches. This paper addresses these issues. We introduce a new rule Reduce for loop reduction transformations, and we generalize our previous rule Validate so that it can handle more transformations involving loops. We then describe how all of this (including some previous theoretical work) is implemented in our <b>compiler</b> <b>validation</b> tool TVOC...|$|E
40|$|AbstractTranslation {{validation}} is {{an approach}} for validating {{the output of}} optimizing compilers. Rather than verifying the <b>compiler</b> itself, translation <b>validation</b> mandates that every run of the compiler generate a formal proof that the produced target code is a correct implementation of the source code. Speculative loop optimizations are aggressive optimizations which are only correct under certain conditions which cannot be validated at compile time. We propose using an automatic theorem prover together with the translation validation framework to automatically generate run-time tests for such speculative optimizations. This run-time validation approach must not only detect {{the conditions under which}} an optimization generates incorrect code, but also provide a way to recover from the optimization without aborting the program or producing an incorrect result. In this paper, we apply the run-time validation technique to a class of speculative reordering transformations and give some initial results of run-time tests generated by the theorem prover CVC...|$|R
40|$|In {{this paper}} {{we present a}} Topic Maps Validation System – XTche {{constraint}} language and its processor. We started with our strong motivation to check a topic map for syntactic and semantic correctness – as a notation to describe an ontology that supports a sophisticated computer system where its validation is crucial! Then we assume XTM and TMCL as starting points and we used our background in <b>compilers</b> and XML <b>validation</b> {{to come up with}} our proposal. XTche complies with all requirements stated for TMCL but it is an XML Schema oriented language. This idea brings two benefits: on one hand it allows for the syntactic specification of Topic Maps (not only the constraints), eliminating the need for two separated specifications (schema and constraints); {{and on the other hand}} it enables the use of an XML Schema editor (like XMLSpy) to provide a graphical interface and the basic syntactic checker. With XTche,atopicmapdesignerdefinesasetofrestrictionsthatguaranteethataparticulartopicmapissemanticallyvalid...|$|R
40|$|Abstract Translation {{validation}} {{was introduced}} in the 90 ’s by Pnueli et al. as a technique to formally verify the correctness of code generators. Rather than certifying the code generator (by writing it entirely using a theorem prover) or exhaustively qualifying it (by obeying the 27 required documents of DO- 178 C), translation validation provides a scalable approach to assess the functional correctness of the generated code. It attempts to verify that program transformations preserve the semantics. Adopting the translation validation approach, we aim at developing a scalable and flexible approach that can be applied to an existing 500 k-lines of code of the Signal compiler. In this work, we focus on proving the preservation of clock semantic and dependence between variables in a program during the compilation of the Signal <b>compiler.</b> Translation <b>validation</b> is implemented in step-by-step style, by proving each transformation of the compiler from the initial step, until the latest step of actual C-code generation. The clock semantic and the dependence in a program and its transformed counterpart are represented as Clock Models and Synchronous Dependence Graphs (SDGs), respectively. Then we introduce a clock refinement and dependence refinement relations which express the preservation of clock semantic and dependence, respectively, as relations on clock models and SDGs. An SMT-solver (Satisfiability Modulo Theory) is used for checking the existence of these refinements. Our validator does not require any instrumentation and modification of the compiler, nor any rewriting of the Received May 28, 2013; accepted month dd, yyy...|$|R
40|$|For {{high quality}} software, an {{important}} part of the project is the choice of the programming language and compiler to be used. This paper examines some of the issues affecting software quality that should be considered when making this choice, including the extent to which the programming language encourages the writing of good quality programs; the degree to which the precise effect of the programs written in the language have been formally defined; and the degree of confidence that can be placed in the compiler implementation. As {{an important part}} of this, the paper will include reference to the role of formal definitions of programming languages, programming language standards and <b>compiler</b> <b>validation</b> techniques. Introduction For high quality software, {{an important part of}} the project is the choice of the programming language and compiler to be used. There are likely to be a large number of different factors to take into account when making this choice, such as the past experience [...] ...|$|E
40|$|This {{case study}} mainly {{focuses on the}} {{execution}} of programs. In particular we study the execution of compiler machine programs on an abstract machine that we implement in ACL 2. But this article also presents a security-related motivation for compiler verification and in particular for binary compiler implementation verification. We will prove that source level verification is not sucient to guarantee compiler correctness. For this, we will adopt the scenario of a well-known attack to Unix operating system programs due to intruded Trojan Horses in compiler executables. Such a compiler will pass nearly every test: {{state of the art}} <b>compiler</b> <b>validation,</b> the bootstrap test, any amount of source code inspection and verification. But for all that, it nevertheless might eventually cause a catastrophe. We will show such a program in detail; it is surprisingly easy to construct such a program. In that, we share a common experience with Ken Thompson, who initially documented this kind of attack in 1984 in his Turing Award Lecture [11]...|$|E
40|$|The ADA Adoption Handbook {{provides}} program managers {{with information}} {{about how best to}} tap ADA 2 ̆ 7 s strengths and manage the transition to fully using this software technology. Although the issues are complex, they are not all unique to ADA. Indeed, many of the issues addressed in this handbook must be addressed when developing any software-intensive system in any programming language. The handbook addresses the advantages and risks in adopting ADA. Significant emphasis has been placed on providing information and suggesting methods that will help program and project managers succeed in using ADA across a broad range of application domains. The handbook focuses on the following topics: ADA 2 ̆ 7 s goals and benefits; program management issues; implications for education and training; software tools with emphasis on <b>compiler</b> <b>validation</b> and quality issues; the state of ADA technology as it related to system design and implementation; and the pending update of the ADA language standard (ADA 9 X) ...|$|E
40|$|Computer {{software}} is typically written in one language and then translated {{out of that}} language into the native binary languages of the machines the software will run on. Most operating systems, for instance, are written in the low-level language C and translated by a C <b>compiler.</b> Translation <b>validation</b> is the act of checking that this translation is correct. This dissertation presents an approach and framework for validating the translation of C programs, and three experiments which test the approach. Our validation approach consists of three components, a frontend, a backend and a core, which broadly mirrors {{the design of the}} C compiler. The three experiments in this dissertation exercise these three components. Each of these components produces a formal proof of refinement, and these refinement proofs compose to produce a proof that the binary is a refinement of the source semantics. This notion of refinement can then compose with correctness proofs for a C program, resulting in a verified binary. Throughout this work, our case study of interest will be the seL 4 verified operating system kernel, compiled for the ARM instruction-set architecture, for which we will produce a verified efficient binary. The thesis of this work is that our translation validation approach offers us great flexibility. We can quickly produce verified binaries produced via many complex transformations without specifically addressing each such transformation. We can adapt our frontend to handle low-level source code which does not strictly respect the rules of the C language it is written in. We can also retarget our backend to address important timing concerns as well as correctness ones...|$|R
40|$|The Ada <b>Compiler</b> <b>Validation</b> Capability (ACVC) {{is a large}} {{collection}} of programs used to verify that compilers conform to the Ada language standard. The maintenance of the ACVC is complicated by ongoing decisions of the Ada Rapporteur Group concerning possible gaps or ambiguities in the language definition. We describe a tool that is intended to simplify the maintenance and upgrade of the ACVC, by providing a specialized data-base front-end to the test suite. This front-end consists of a patternmatching system, based on the translator of the Ada/Ed system, and index files into the ACVC, built on primary syntactic features of Ada and on previously defined patterns. The patterns used by the system are compact fragments of Ada programs that contain the features of interest. We describe in detail the pattern language, the index files, and the user interface of the system. We expect the system to be particularly useful when the modifications to Ada that will result from the Ada 9 X pr [...] ...|$|E
40|$|Translation {{validation}} is {{a technique}} that verifies the re-sults of every run of a translator, such as a compiler, in-stead of the translator itself. Previous papers by the authors and others have described translation validation for com-pilers that perform loop optimizations (such as interchange, tiling, fusion, etc), using a proof rule that treats loop opti-mizations as permutations. In this paper, we describe an improved permutation proof rule which considers the initial conditions and invariant conditions of the loop. This new proof rule not only im-proves the validation process for compile-time optimiza-tions, {{it can also be}} used to ensure the correctness of speculative loop optimizations, the aggressive optimizations which are only correct under certain conditions that can-not be known at compile time. Based on the new permu-tation rule, with the help of an automatic theorem prover, CVC Lite, an algorithm is proposed for validating loop op-timizations. The same permutation proof rule can also be used (within a compiler, for example) to generate the run-time tests necessary to support speculative optimizations. Key words: <b>Compiler</b> <b>validation,</b> speculative loop optimizations, translation validation, for-mal methods. 1...|$|E
40|$|We {{present an}} {{automated}} technique for finding defects in compilers for graphics shading languages. key challenge in compiler testing {{is the lack}} of an oracle that classifies an output as correct or incorrect; this is particularly pertinent in graphics shader compilers where the output is a rendered image that is typically under-specified. Our method builds on recent successful techniques for <b>compiler</b> <b>validation</b> based on metamorphic testing, and leverages existing high-value graphics shaders to create sets of transformed shaders that should be semantically equivalent. Rendering mismatches are then indicative of shader compilation bugs. Deviant shaders are automatically minimized to identify, in each case, a minimal change to an original high-value shader that induces a shader compiler bug. We have implemented the approach as a tool, GLFuzz, targeting the OpenGL shading language, GLSL. Our experiments over a set of 17 GPU and driver configurations, spanning the main 7 GPU designers, have led to us finding and reporting more than 60 distinct bugs, covering all tested configurations. As well as defective rendering, these issues identify security-critical vulnerabilities that affect WebGL, including a significant remote information leak security bug where a malicious web page can capture the contents of other browser tabs, and a bug whereby visiting a malicious web page can lead to a ``blue screen of death'' under Windows 10. Our findings show that shader compiler defects are prevalent, and that metamorphic testing provides an effective means for detecting them automatically...|$|E
40|$|AbstractThe paper {{presents}} {{approaches to}} the validation of optimizing compilers. The emphasis is on aggressive and architecture-targeted optimizations which try to obtain the highest performance from modern architectures, in particular EPIC-like micro-processors. Rather than verify the compiler, the approach of translation validation performs a validation check after every run of the compiler, producing a formal proof that the produced target code is a correct implementation of the source code. First we survey the standard approach to validation of optimizations which preserve the loop structure of the code (though they may move code {{in and out of}} loops and radically modify individual statements), present a simulation-based general technique for validating such optimizations, and describe a tool, VOC- 64, which implements these technique. For more aggressive optimizations which, typically, alter the loop structure of the code, such as loop distribution and fusion, loop tiling, and loop interchanges, we present a set of permutation rules which establish that the transformed code satisfies all the implied data dependencies necessary for the validity of the considered transformation. We describe the necessary extensions to the VOC- 64 in order to validate these structure-modifying optimizations. Finally, the paper discusses preliminary work on run-time validation of speculative loop optimizations, that involves using run-time tests to ensure the correctness of loop optimizations which neither the compiler nor compiler-validation techniques can guarantee the correctness of. Unlike <b>compiler</b> <b>validation,</b> run-time validation has not only the task of determining when an optimization has generated incorrect code, but also has the task of recovering from the optimization without aborting the program or producing an incorrect result. This technique has been applied to several loop optimizations, including loop interchange, loop tiling, and software pipelining and appears to be quite promising...|$|E
40|$|This study {{identified}} Ada features {{important to}} real-time performance, and investigated vari-ous real-time benchmarking methodologies. The overall effort {{consisted of three}} steps: (1) we reviewed a number of efforts concerned with Ada features and performance; (2) we examined studies that quantified Ada performance requirements; and (3) we investigated a number of real-time benchmarking efforts. Ada is a complex language, intended for the programming of large, complicated systems. It is also, somewhat paradoxically, designed for time-critical software. Thus interrupt handling, context switching, and other substantial language features must execute quickly and predictably. Both computing hardware and Ada compilers vary substantially with respect to performance. Unavoidably, there are embedded programs with constraints that cannot be met by some platforms. Thus a method is needed for deciding which platforms are suitable for running a particular embedded application. Furthermore, understanding which embedded system constraints are difficult to satisfy may lead to new insights into embedded system implementation. 1. Technical Objectives Real-time systems must reliably meet a variety of real-time constraints. Although Ada {{is intended to be}} used for such real-time applications, the Ada Language Reference Manual [LRM] deals neither with absolute or relative performance. The Ada <b>Compiler</b> <b>Validation</b> Capability Suite was established to validate the form and meaning of programs written in Ada, not to be used for measurement. Thus the Ada language definition deals with half of the real-time problem; it con-tains mechanisms to accommodate real-time applications, but leaves performance undefined and unpredictable. 1. 1. Objective 1 : Identify Critical Areas of Real-Time Performance List features of Ada that are expected to affect real-time performance. The expectations are based on experience implementing real-time systems in Ada. 1. 2. Objective 2 : Quantify Requirements of Real-Time Embedded Systems Collect the performance requirements of real-time embedded systems. These requirements quantify the needed performance of the language features identified in the first objective...|$|E

