6|19|Public
50|$|University Admissions Finland (UAF) is a <b>centralised</b> <b>application</b> {{service for}} {{international}} Master's degree student applicants for eleven Finnish universities.|$|E
5000|$|The Patent Cooperation Treaty (PCT) is {{operated}} by World Intellectual Property Organization (WIPO) {{and provides a}} <b>centralised</b> <b>application</b> process, but patents are not granted under the treaty.|$|E
40|$|Centralised fetal {{monitoring}} system belongs to signal processing system class. The main {{functions of the}} system are signal acquisition from bedside monitors, on-line trace analysis and dynamic presentation of incoming data. Collected data set is controlled by <b>centralised</b> <b>application.</b> Relational database management system can’t process sampled at high frequency biomedical signals on-line. Therefore, we decide to build our own data management system dedicated to stream processing that support continuous queries. This paper describes a method of building a query plan based on proposed algebra. The presented example of application enables implementation of algorithm determining long and short term indices for fetal heart rate variability assessment {{on the basis of}} declarative query language. Our solution enables to define query based on data streams that makes the updated answers currently available. 1...|$|E
50|$|UAC {{processes}} <b>centralised</b> <b>applications</b> {{for admission}} to undergraduate and postgraduate courses at its participating institutions. UAC also notifies current NSW HSC students of their Australian Tertiary Admission Rank (ATAR).|$|R
50|$|The Universities Admissions Centre (NSW & ACT) Pty Ltd (UAC) is the {{organisation}} that processes <b>centralised</b> <b>applications</b> for admission to tertiary education courses at participating institutions, mainly in New South Wales and the Australian Capital Territory. A not-for-profit company incorporated in July 1995, it has offices located at Sydney Olympic Park.|$|R
40|$|Abstract. Grid {{applications}} are fragile when changes to service implementations, non-functional properties or communication protocols take place. Moreover, developing Grid applications with current toolkits {{result in a}} tangling of toolkit-specific and application-specific code that makes maintenance and evolution difficult. This paper proposes solving these problems by using reflection to open up Grid toolkits, and to allow Grid applications to be developed {{as if they were}} <b>centralised</b> <b>applications.</b> This would allow changes to be handled dependably, and a clean separation between toolkit-specific and application-specific code. ...|$|R
40|$|Researchers at National Institute of Biology (NIB) use DNA {{microarray}} {{technology in}} their experimental work. Experimental data is currently saved {{in the form}} of files and just about every researcher at the Institute uses its own file format. To support searching through all experimental data by everyone at NIB, there was a need to develop a <b>centralised</b> <b>application</b> for DNA microarray data management. We have addressed this task by developing an application called Fitobase. The application allows user to describe the parameters of the experiment, upload the data, and search through the data sets stored in Fitobase. It also provides graphical visualisation of gene expression, supporting a simple means of data analysis. With analysis and comparison of gene expressions across different biological contexts, Fitobase can help biologists to understand the behaviour of the model organisms at different conditions and to hypothesise on gene function...|$|E
40|$|Today, {{planning}} {{and control of}} logistic processes on automobile terminals are generally executed by centralised logistics systems, which cannot cope with high requirements for flexible order processing due to increasing dynamics and complexity. The main business processes on automobile terminals – notification of vehicles by automobile manufacturer, transport to automobile terminal, storage and technical treatment as well as delivery to automobile dealer – are planned and controlled by <b>centralised</b> <b>application</b> software systems. In {{the context of this}} article, an innovative approach to autonomous control in automobile logistics is investigated, considering as example the logistic order processing of an idealised automobile terminal of the company E. H. Harms Automobile-Logistics. Within a simulation study, evidence of the existing application potential of autonomous control in the field of vehicle storage management is provided. Thereupon the technical feasibility of an autonomously controlled storage management system is examined. System requirements for technical implementation of an autonomously controlled storage management are deviated. Thereafter, results of an executed case study concerning the implementation and test of an autonomously controlled, radio frequency identification (RFID) based storage management system are presented and the remaining weaknesses of the implemented IT solution are identified. Finally, further research and development activities are introduced {{in the form of a}} wearable computing concept using smart clothes...|$|E
40|$|In 2015 the Swedish Migration Agency {{experienced}} {{a challenge in}} encountering the explosive increase of asylum seekers. Particular exposed {{was the first step}} of the asylum flow, the application process, since it is the part where the asylum seekers are registered into the system and their stay becomes legal. In order to improve the application process, the Agency wanted to investigate how they should structure the application units and if it should be centralised or decentralised. The decision regarding the structure needed to be made with the Agency’s objectives and constraints in mind. The objectives for the application process were to minimise cost, have flexibility in capacity and high customer service, while the constraints were that the Agency needed to encounter the fluctuating inflow of asylum seekers and follow the strategies of their business and of being a public organisation. With this in mind, the purpose of this thesis was to investigate if the Agency should have a centralised or decentralised application unit structure to minimise costs, have flexibility in capacity and high customer service under fluctuating inflow of asylum seekers and defined strategies. The purpose was achieved through a literature study, where relevant theories were presented in a theoretical framework and finally concluded in an analysis model. The analysis model mapped the theoretical constraints and objectives effects on corresponding structure of centralisation or decentralisation. The empirical findings were collected by interviewing eight employees at the Agency and by observing the application unit in Kållered, Gothenburg. Furthermore, data was collected through previous empirical findings and the Agency’s internal data system. The findings were verified and validated in a workshop. The analysis of this thesis was performed by looking into the analysis model and mapping corresponding factors for the current situation at the Agency. The factors importance to the Agency were analysed, where factors of moderate or high importance were taken into account. The result shows that the Agency should have a <b>centralised</b> <b>application</b> unit structure. But, the main conclusion highlights that this is only a direction of the decision and in order to take a definite decision regarding the structure some areas need to be further investigated. The areas worth further investigations are correlations between the factors in the analysis model, impact of the structure decision on the whole asylum flow, factor’s weighting of importance and generalizability of the analysis model for other processes or applicants within the Agency. The recommendations for the Agency is presented in an eight-step figure, where areas and questions worth further attention are presented...|$|E
40|$|Modern {{computing}} {{and networking}} hardware makethe physical interconnection of manymachines simple. However, programming an application to takeevenlimited {{advantage of the}} interconnection is notoriously difficult due {{to the complexity of}} the protocols involved. Furthermore, real world demands insist that such applications need to be programmed in an existing, preferably widely available, language. One approach aimed at easing this difficulty is based upon the concept of transparency. Bymaking the underlying distribution of the system transparent to the programmer it is hoped that the programming task becomes comparable with that of programming <b>centralised</b> <b>applications...</b>|$|R
50|$|In theory, the new <b>application</b> system <b>centralised</b> the <b>application</b> process, {{reducing}} the workload for consultants in shortlisting candidates for interview, and the workload of candidates applying multiple times for different posts.|$|R
40|$|Distributed-systems {{researchers}} at the University of Waterloo have developed tools for capturing events, such as interprocess communication, from distributed applications and displaying them using process-time diagrams. The research work has primarily concerned development of effective techniques for dealing with large collections of processes and large numbers of events, and then exploring these techniques through prototype implementation. These techniques were originally developed primarily for use in debugging applications, but are now being examined for use in monitoring and management of applications in production. This paper describes the techniques and tools which have been developed {{as well as their}} intended use in monitoring and management. 1 Introduction Designing, developing, debugging, and managing distributed applications present significant challenges in addition to those occurring with <b>centralised</b> <b>applications.</b> The interactions among application components must be under [...] ...|$|R
40|$|Event-based {{communication}} {{is appropriate for}} many application domains, ranging from small, <b>centralised</b> <b>applications</b> such as GUIs to large, distributed applications such as telecommunications, network monitoring and virtual world support systems. Consequently, many different event models have been put forward, some designed for small-scale systems and others for large-scale systems. One such model is the eco model {{which was designed to}} support virtual world applications in the Moonlight project. The eco model was designed to be scalable by including filtering capabilities that were intended to decrease network traffic in a distributed implementation. There have been two previous implementations of the eco model, and one characteristic of both was that all code was linked into the application at compile time, regardless of whether it was used at runtime or not. This resulted in executables which were larger than strictly necessary, and consequently lower scalability for applications host [...] ...|$|R
40|$|Event-based {{communication}} {{is useful in}} many application domains, ranging from small, <b>centralised</b> <b>applications</b> to large, distributed systems. Many different event models {{have been developed to}} address the requirements of different application domains. One such model is the ECO model which was designed to support distributed virtual world applications. Like many other event models, ECO has event filtering capabilities meant to improve scalability by decreasing network traffic in a distributed implementation. Our recent work in event-based systems has included building a fully distributed version of the ECO model, including event filtering capabilities. This paper describes the results of our evaluation of filters as a means of achieving increased scalability in the ECO model. The evaluation is empirical and real data gathered from an actual event-based system is used. The findings show filters to be highly valuable in making distributed implementations of the model scale, that multicast [...] ...|$|R
5000|$|The Queensland Tertiary Admissions Centre (QTAC) {{provides}} a <b>centralised</b> tertiary study <b>application</b> system and publishes entry requirements and course information for prospective applicants. Established in 1990, QTAC is a non profit, public company.|$|R
40|$|While the {{widespread}} adoption of Internet and Intranet {{technology has been}} one of the exciting developments of recent years, many hospitals are finding that their data and legacy applications do not naturally fit into the new methods of dissemination. Existing applications often rely on isolation or trusted networks for their access control or security, whereas untrusted wide area networks pay little attention to the authenticity, integrity or confidentiality of the data they transport. Many hospitals do not have the resources to develop new "network-ready " versions of existing <b>centralised</b> <b>applications.</b> In this paper, we examine the issues that must be considered when providing network access to an existing health care application, and we describe how we have implemented the proposed solution in one healthcare application – namely the diabetic register at Hope Hospital. We describe the architecture that allows remote access to the legacy application, providing it with encrypted communications and strongly authenticated access control but without requiring any modifications to the underlying application. As well as comparing alternative ways of implementing such a system, we also consider issues relating to usability and manageability, such as password management...|$|R
40|$|Translation {{has always}} {{played a major}} role within the European {{institutions}} because it provides the basis for democracy and communication among the Member States and between the EU and its citizens. The enlargements brought about changes in the internal organization of the institutions – including translation services and their workflow – to respond to the new challenge of accommodating 23 official languages. A greater need for translation support was met thanks to a growing number of shared tools and resources developed over time, such as <b>centralised</b> web-based <b>applications</b> and meta-search engines. This paper focuses on one specific tool available to translators working at the EU institutions, i. e. an internally developed multilingual concordancer. Concordancers are widely used by translators but little information is available about them in terms of tool evaluation or user behaviour. This article presents a PhD research project aimed to partly fill this gap by investigating the relationship between concordance searches (seen as manifestations of translation problems) and language combination within the EU translation services...|$|R
40|$|A {{state-of-the-art}} {{radiation monitoring}} and alarming system is being implemented at CERN for the Large Hadron Collider (LHC). The RAdiation Monitoring System for the Environment and Safety (RAMSES) comprises about 350 monitors and provides {{ambient dose equivalent}} and ambient dose equivalent rates measurements in the LHC underground areas {{as well as on}} the surface inside and outside the CERN perimeter. In addition, it monitors air and water released from the LHC installations. Although originally conceived for radiation protection only, RAMSES also integrates the monitoring of conventional environmental measurements such as physico-chemical parameters of released. All data is acquired by a distributed set of data acquisition and control units that also generate local radiation warnings, local alarms and operational interlocks. Some monitoring stations have processing units for the control of some complex measurement processes. A <b>centralised</b> SCADA <b>application</b> allows remote supervision of all measured variables and remote alarms on monitored variables. Data logging and safe long-term archiving for off-line data analysis and reporting as well as configuration management is realised with an integrated database storage, data analysis and configuration management tools. This paper describes the implementation of the software infrastructure, highlighting the different architectural choices with their benefits...|$|R
40|$|Information {{security}} assured on <b>centralised</b> systems through <b>application</b> {{of principles}} previously established for paper-based systems. The advent of personal computing and distributed computing potentially turned that model upside down. It {{seems that the}} eagerness of organisations for encouraging technology (Availability part of the CIA acronym) seemed to take precedence over the finer meaning of Confidentiality and Integrity, in spite of (in the UK, at least) changes to legislation. The huge increase in portable data storage capacities ensured that {{what may have been}} perceived as a minor irritant in the 1980 s became a potential nightmare scenario by 2007, which caused two government reports to report “systemic failure”. This paper looks at the development of end-user computing, and suggests that the problem occurred {{because of a lack of}} information risk assessment over many year...|$|R
40|$|There is {{a growing}} {{interest}} in utilising non-traditional water resources by means of water reclamation and water recycling for long term sustainability. Amongst the many treatment alternatives, membrane bioreactors (MBRs) {{have been seen as}} an effective technology capable of transforming various types of wastewater into high-quality effluent exceeding most discharge requirements and suitable for a variety of reuse applications. To date MBRs are largely restricted to <b>centralised</b> large scale <b>applications,</b> with the most common capacity of 200 ML per day or above. The aim {{of this paper is to}} review and discuss the potential and limitations of MBRs for small scale applications. Both technical and economic considerations will be delineated with respect to the future water outlook in Australia. Particular attention is also given to the impact of MBR technology on the removal of micropollutants that are of significant concern in water recycling...|$|R
40|$|Risks {{are always}} managed within a broader context of {{relationships}} between governments, citizens, civil society and business. The various elements {{of what has been}} termed ‘new governance’ include the emergence of multi-level structures and processes and the ‘hollowing out’ of the nation state; moves away from the exercise of <b>centralised</b> authority; the <b>application</b> of new forms of authority and control; and a changing distribution of responsibilities between the state and other actors. Whilst such governance characteristics can be discussed in sweeping terms, in practice there are considerable differences between countries and regions in {{the extent to which these}} trends of change have taken place. In this chapter we present a framework for profiling some of the key dimensions of natural hazard governance. The aim was to capture the variability and dynamism of governance practice through a simple structure that enables any chosen national, regional or local natural hazard governance context to be profiled against a set of eight governance characteristics...|$|R
40|$|This {{dissertation}} explores {{operating system}} mechanisms to allow resource-aware applications {{to be involved}} in the process of managing resources under the premise that these applications (1) potentially have some (implicit) notion of their future resource demands and (2) can adapt their resource demands. The general idea is to provide feedback to resource-aware applications so that they can proactively participate in the management of resources. This approach has the benefit that resource management policies can be removed from central entities and the operating system has only to provide mechanism. Furthermore, in contrast to <b>centralised</b> approaches, <b>application</b> specific features can be more easily exploited. To achieve this aim, I propose to deploy a microeconomic theory, namely congestion or shadow pricing, which has recently received attention for managing congestion in communication networks. Applications are charged based on the potential "damage" they cause to other consumers by using resources. Consumers interpret these congestion charges as feedback signals which they use to adjust their resource consumption. It can be shown theoretically that such a system with consumers merely acting in their own self-interest will converge to a social optimum. This dissertation focuses on the operating system mechanisms required to decentralise resource management this way. In particular it identifies four mechanisms: pricing & charging, credit accounting, resource usage accounting, and multiplexing. While the latter two are mechanisms generally required for the accurate management of resources, pricing & charging and credit accounting present novel mechanisms. It is argued that congestion prices are the correct economic model in this context and provide appropriate feedback to applications. The credit accounting mechanism is necessary to ensure the overall stability of the system by assigning value to credits...|$|R
40|$|Management in {{ubiquitous}} systems cannot rely {{on human}} intervention or centralised decision-making functions because systems {{are complex and}} devices are inherently mobile and cannot refer to <b>centralised</b> management <b>applications</b> for reconfiguration and adaptation directives. Management must be devolved, based on local decision-making and feedback control-loops embedded in autonomous components. Previous work has introduced a Self-Managed Cell (SMC) as an infrastructure for building ubiquitous applications. An SMC consists {{of a set of}} hardware and software components that implement a policy-driven feedback control-loop. This allows SMCs to adapt continually to changes in their environment or in their usage requirements. Typical applications include body-area networks for healthcare monitoring, and communities of unmanned autonomous vehicles (UAVs) for surveillance and reconnaissance operations. Ubiquitous applications are typically formed from multiple interacting autonomous components, which establish peer-to-peer collaborations, federate and compose into larger structures. Components must interact to distribute management tasks and to enforce communication strategies. This thesis presents an integrated framework which supports the design and the rapid establishment of policy-based SMC interactions by systematically composing simpler abstractions as building elements of a more complex collaboration. Policy-based interactions are realised – subject to an extensible set of security functions – through the exchanges of interfaces, policies and events, and our framework was designed to support the specification, instantiation and reuse of patterns of interaction that prescribe the manner in which these exchanges are achieved. We have defined a library of patterns that provide reusable abstractions for the structure, task-allocation and communication aspects of an interaction, which can be individually combined for building larger policy-based systems in a methodical manner. We have specified a formal model to ensure the rigorous verification of SMC interactions before policies are deployed in physical devices. A prototype has been implemented that demonstrates the practical feasibility of our framework in constrained resources. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|Economical and {{scalability}} factors lead to {{a growing}} number of applications using P 2 P technology. However, the absence of a server to monitor the system makes these applications susceptible to uncontrolled distribution of undesired objects (e. g. pornography, polluted content, or spam). In <b>centralised</b> <b>applications,</b> this issue is solved effectively by using content filtering or spam filtering via pattern recognition. Hence, a similar solution is appealing for P 2 P-based applications. Within P 2 P networks, pattern recognition also has the potential to provide other classification services, such as music genre classification and attack detection. Pattern recognition in the P 2 P domain involves large amounts of resource in- tensive computation and it typically requires a high-performance server which is unavailable in P 2 P networks. In order to deal with this limitation, a scalable, fully distributed classification algorithm which performs fast processing is necessary. Thus, we consider an algorithmic perspective and explore design issues that arise in pattern recognition in P 2 P networks. Some of these issues include the absence of central coordination, low computational resources, lack of synchronisation, frequent data update, large dataset scalability, large network scalability, dynamic and imbalanced data distribution. None of the current P 2 P classification algorithms deal with all of these issues, therefore, the proposed algorithm aims to adequately deal with all of these issues. Considering that P 2 P networks are large, building an exact (not approximate) global classifier which represents the combined datasets across all peers is non- trivial. Current algorithms are guaranteed to converge to exact classifiers through high communication overheads and processing times. In contrast, this thesis aims to show that exact global classifications can be obtained at any time using limited communication by building a single in-network global classifier. We refer to the resulting algorithm as the P 2 P-GN. Existing methods perform a whole state-of-the-art classifier on a single peer. This computation is intractable due to the complexity of pattern recognition (particularly in dealing with high-dimensional problems) and the reluctance of peers to share their private resources inordinately. In contrast, our method only requires each peer to perform a simple and parallel computation. This enables the algorithm to perform online learning to handle frequent local data updates and frequent prediction requests leading to processing large datasets efficiently. We also incorporate a fault-tolerance scheme to preserve the system performance within dynamic networks. In order to assess the performance of the algorithm, we compare it to the existing centralised and distributed classification methods. The experimental results show that our method has comparable accuracy to the centralised classifiers. The efficiency of the algorithm for high-dimensional datasets is shown through a series of experiments. The results also show that our method is invariant to imbalanced data distributions (imbalanced class distribution and small in size dataset per peer). The results from our experiments and analyses show that our algorithm incurs low communication overheads and the communication per peer is independent of the network size. This shows that the algorithm scales well, performing effectively irrespective of network size. As a final example of the efficiency of the proposed algorithm we consider spam problems in P 2 P applications. We extend our algorithm to be an economical, scalable and fullydistributed spam detection system called the DASMET. This algorithm is specifically designed for datasets that consist of similar occurring patterns. Our experimental results show that the DASMET performs best with a relatively low amount of resources for the spam detection compared to other distributed methods. In conclusion, this thesis proposes the P 2 P-GN for efficient pattern recognition within P 2 P networks. The efficacy of the algorithm for datasets with large number of attributes and training samples are validated through a series of experiments. Furthermore, the scalability of the algorithm is demonstrated through experiments with varying network sizes. The experimental results show that the P 2 P-GN provides a practical alternative for scalable pattern recognition within P 2 P networks...|$|R
40|$|Current Internet based auction {{services}} rely, in general, on a <b>centralised</b> auction server; <b>applications</b> with large and geographically dispersed bidder client bases are thus supported in a centralised manner. Such an approach is fundamentally restrictive as too many users can overload the server, making the whole auction process unresponsive. Further, such an architecture can {{be vulnerable to}} server's failures, if not equipped with sufficient redundancy. In addition, bidders who are closer to the server {{are likely to have}} relatively faster access to the server than remote bidders, thereby gaining an unfair advantage. To overcome these shortcomings, this thesis investigates ways of enabling widely distributed, arbitrarily large number of auction servers to cooperate in conducting an auction. Allowing a bidder to register with anyone of the auction servers and place bids there, coupled with periodic exchange of auction information between servers forms the basis of the solution investigated to achieve scalability, responsiveness and fairness. Scalability and responsiveness are achieved since the total load is shared amongst many bidder servers; fairness is achieved since bidders are able to register with their local servers. The thesis presents the design and implementation of an hierarchically structured distributed Internet auction system. Protocols for inter-server cooperation are presented. Each server may be replicated locally to mask node failures. Performance evaluations of centralised and distributed configurations are performed to show the advantages of the distributed configuration over the centralised one. EThOS - Electronic Theses Online ServiceIranian Ministry of Science, Research and Technology : Isfahan UniversityGBUnited Kingdo...|$|R

