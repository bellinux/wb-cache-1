7|41|Public
40|$|We {{show how}} {{difference}} lists {{can be used}} for systematically compiled linear clauses for Lambek categorial grammar and its generalisations, in analogy with standard Horn clauses for <b>CF</b> <b>grammar.</b> We also consider use of difference bags for partitioning of linear sequents, and methods for ambiguity and polymorphism...|$|E
40|$|AbstractWe {{investigate}} {{the complexity of}} basic decidable cases of the commutation problem for languages: testing the equality XY=YX for two languages X and Y. We show that it varies from co-NEXPTIME complete through PSPACE complete and co-NP complete to deterministic polynomial time, when Y is an explicitly given finite language and X is given by a <b>CF</b> <b>grammar</b> generating a finite language, a nondeterministic finite automaton (or a regular expression), an acyclic nondeterministic finite automaton or an explicitly given finite language, respectively. Interestingly {{in most cases the}} complexity status does not change if instead of explicitly given finite Y we consider general Y of the same type as X. For deterministic finite automata the problem remains open, due to the asymmetry of the catenation...|$|E
40|$|The Context-Free Backbone of {{some natural}} {{language}} analyzers produces all possible CF parses {{as some kind}} of shared forest, from which a singie tree is to be chosen by a disam- blguation process that may be Based on the finer features of the language. We study the structure of these forests with respect to optimality of sharing, and in relation with the parsing schema used to produce them. In addition to a theo- retical and experimental framework for studying these issues, the main results presented are: - sophist/cat/on in chart parsing schemata (e. g. use of look-ahead) may reduce time and space efficiency instead of improving it, - there is a shared forest structure w/th at most cubic size for any <b>CF</b> <b>grammar,</b> - when O(n z) complexity is reqnired, the shape of a shared forest is dependent on the parsing schema used...|$|E
40|$|A purely {{functional}} {{implementation of}} LR-parsers is given, {{together with a}} simple correctness proof. It {{is presented as a}} generalization of the recursire descent parser. For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions, i. e. functions that memorize the results of previous invocations. Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest. For LR(0) grammars, our algorithm is closely related to the recursire ascent parsers recently discovered by Kruse- man Aretz [1] and Roberts [2]. Extended <b>CF</b> <b>grammars</b> (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal <b>CF</b> <b>grammars...</b>|$|R
40|$|In {{contrast}} to the usual depth-first derivations of context-free (<b>CF)</b> <b>grammars,</b> breadth-first derivations (also in combination with depth-first ones) yield a class of augmented context-free grammars (ACF) (also termed multi-breadth-depth grammars) endowed with greater generative capacity, yet manageable. The inadequacy of <b>CF</b> <b>grammars</b> to treat distant dependencies is overcome by the new model. ACF grammars can be classified {{with respect to their}} disposition, a concept related to the data structure needed to parse their strings. For such augmented <b>CF</b> <b>grammars</b> we consider the LL(k) condition, that ensures top-down deterministic parsing. We restate the condition as an adjacency problem and we prove that it is decidable for any disposition. The deterministic linear-time parser differs from a recursive descent parser by using instead of a LIFO stack a more general data structure, involving FIFO queues and LIFO stacks in accordance with the disposition. ACF grammars can be also viewed as a formalized version of ATN (Augmented Transition Networks) ...|$|R
50|$|Different <b>CF</b> <b>grammars</b> can {{generate}} the same CF language. Intrinsic {{properties of the}} language can be distinguished from extrinsic properties of a particular grammar by comparing multiple grammars that describe the language.|$|R
40|$|Bernard LANG INRIA B. P. 105, 78153 Le Chesnay, France lang@inria. inria. fr The {{intent of}} this {{presentation}} is to exhibit the commonalities between the following syntactic problems: 1. parsing ambiguous or incomplete input, often known as word lattice parsing; 2. parsing ill-formed input, i. e. input {{that does not}} belong to the language formally defined by a grammar; 3. syntactic disambiguation of ambiguous sentences; 4. accounting for deviant syntactic structures in grammatical language descriptions. The key idea behind this work is that of a weighted grammar. For simplicity we consider here only a special case of the more general definition given in [Teitelbaum 73], which could lead to other interesting variations (e. g. probabilities as weights, using multiplication instead of addition as below). We define a weighted grammar as a Context-Free (<b>CF)</b> <b>grammar</b> with a numeric weight attached to each of its rules. We attach to any derivation tree a weight that is the sum of the weights of a [...] ...|$|E
40|$|This {{document}} {{describes the}} `English Phrases for IR' (EP 4 IR) grammar and {{the rationale behind}} it. This grammar was conceived for the extraction of phrases from English text and their transduction to binary frames. The reader {{is supposed to be}} familiar with [Koster et al., 1999], which describes {{the context in which the}} grammar is to be used. The grammar is written in the AGFL formalism [Koster, 1991] which was developed for the syntactic description of natural languages. It has a number of properties making it particularly useful for this purpose. An AGFL can be seen as a <b>CF</b> <b>grammar</b> extended with set-valued features, where the features express number, person, time, etc. Penalties indicated in the grammar can be used to distinguish preferred from nonpreferred analyses. The AGFL system includes a lexicon system capable of storing many wordforms with their part of speech (POS) and features. More information about AGFL can be found a...|$|E
40|$|The {{intent of}} this {{presentation}} is to exhibit the commonalities between the following syntactic problems: 1. parsing ambiguous or incomplete input, often known as word lattice parsing; 2. parsing ill-formed input, i. e. input {{that does not}} belong to the language formally defined by a grammar; 3. syntactic disambiguation of ambiguous sentences; 4. accounting for deviant syntactic structures in grammatical language descriptions. The key idea behind this work is that of a weighted grammar. For simplicity we consider here only a special case of the more general definition given in [Teitelbaum 73], which could lead to other interesting variations (e. g. probabilities as weights, using multiplication instead of addition as below). We define a weighted grammar as a Context-Free (<b>CF)</b> <b>grammar</b> with a numeric weight attached to each of its rules. We attach to any derivation tree a weight that is the sum of the weights of all instances of rules used in that derivation. Syntactic disambiguatio...|$|E
40|$|We {{investigate}} {{the role of}} pseudoterminals of E 0 L forms. This leads us {{to the definition of}} m-interpretation which avoids the introduction of additional pseudoterminals via interpretation. We solve the problem of m-completeness for simple and short EP 0 L forms, thus establishing a normal form result for EP 0 L forms which can be viewed as an analogy to the Chomsky Normal Form for <b>CF</b> <b>grammars.</b> Finally, we consider the validity of some basic results on E 0 L forms under m-interpretation...|$|R
40|$|Recently, a novel model, called Tile Rewriting Grammar (TRG), {{has been}} {{introduced}} to apply the generative grammar approach to picture languages, or 2 D languages. In many respects, the TRGs can be considered the equivalent of context-free (<b>CF)</b> <b>grammars</b> for 2 D languages. However, the possibility to investigate applications was precluded so far {{by the lack of}} a good parsing algorithm. We propose a parsing algorithm for TRGs, which can be described as an extension to 2 D of Cocke, Kasami and Younger’s classical parsing technique for 1 D context-free grammars. ...|$|R
40|$|The Associative Language Description model (ALD) is a {{combination}} of locally testable and constituent structure ideas. It is consistent with current views on brain organization and can rather conveniently describe typical technical languages such as Pascal or HTML. ALD languages are strictly enclosed in context-free languages but in practice the ALD model equals <b>CF</b> <b>grammars</b> in explanatory adequacy. Various properties of ALD have been investigated, but many theoretical questions are still open. For instance, it is unknown, at the present, whether the ALD family includes the regular languages. Here it is proved that several different classes of regular languages are ALD. ...|$|R
40|$|The {{development}} of translator writing systems and extensible languages {{has led to}} a simultaneous {{development of}} more efficient and general syntax analyzers, usually for context-free (CF) syntax. Our paper describes a type of parser that can be used with reasonable efficiency for any <b>CF</b> <b>grammar,</b> even one which is ambiguous. Our parser, like most others, is based on the pushdown automaton model, and thus its generality requires it to be non-deterministic (in the automata theoretic sense). An actual imple-mentation of a non-deterministic automaton requires that we explore every computational path that could be followed by the theoretical automaton. This can be done serially [5], following successively every computational path whenever a choice occurs. It leads to the algorithms described in [I], whose time bounds can be exponential functions of the length of the string to be parsed. We can also use a parallel implementation which consists in following "simultaneously " all the possible computational paths whenever a non-deterministic choice occurs. Then we can merge paths that have ceased to be different after a certain point. Those mergings reduce drastically the amount of computation. The best known example is the top-down algorithm described in [2]. The algorithm we describe {{in the first part of}} our paper is a basic bottom-up parser, similar to [2] in its organization. Both parsers can be shown to work within time bounds which are at most the cube of the length of the input string, and are often a linear function of it. The space bounds are at most the square of that length. The second part of our paper deals with the optimization of the basic parallel bottom-up algorithm, using the properties of weak precedence relations [3]. The various optimization techniques further reduce the amount of computation required and cause frequent occurrence of a "sparse determinism " phenomenon which allows determi-nation of part of the parse-tree before the non-deterministic analysis is ended. These optimizations also considerably lessen the space requirements. Full details can be found in [6]. The parser is easy to generate for any <b>CF</b> <b>grammar,</b> requiring only the computation of precedence tables. It is slower than most deterministic parsers (on the order of ten times); but all example...|$|E
40|$|We {{extend the}} notion of regular sets of strings to those of trees and of forests in a unified {{mathematical}} approach, and investigate their properties. Then by taking certain one-dimensional expressions of these objects, we come to an interesting subclass of CF languages defined over paired alphabets. They are shown to form a Boolean algebra with the Dyck set as the universe, and {{to play an important}} role in the whole class of CF languages. In particular, using the subclass we prove a refinement of the well-known Chomsky—Schützenberger Theorem, and also prove that the decision procedure for parenthesis grammars can be extended to a broader class of <b>CF</b> <b>grammars...</b>|$|R
40|$|In {{this paper}} we propose a fixed size {{one-dimensional}} VLSI architecture for the parallel parsing of arbitrary context free (<b>CF)</b> <b>grammars,</b> based on Earley's algorithm. The algorithm is transformed into an equivalent double nested loop with loop-carried dependencies. We first map the algorithm into a 1 -D array with unbounded number of cells. The time complexity of this architecture is O(n), which is optimal. We next propose the partitioning into fixed number of off-the-shelf processing elements. Two alternative partitioning strategies are presented, considering restrictions, not only in the number of the cells, but also in the inner structure of each cell. In the most restricted case, the proposed architecture has time complexity O(n 3 /p*k), where p is the number of available cells and the elements inside each cell are at most k...|$|R
40|$|International audienceThe Associative Language Description model (ALD) is a {{combination}} of locally testable and constituent structure ideas. It is consistent with current views on brain organization and can rather conveniently describe typical technical languages such as Pascal or HTML. ALD languages are strictly enclosed in context-free languages but in practice the ALD model equals <b>CF</b> <b>grammars</b> in explanatory adequacy. Various properties of ALD have been investigated, but many theoretical questions are still open. For instance, it is unknown, at the present, whether the ALD family includes the regular languages. Here it is proved that several known classes of regular languages are ALD: threshold locally testable languages, group languages, positive commutative languages and commutative languages on 2 -letter alphabets. Moreover, we show that there is an ALD language in each level of (restricted) star height hierarchy. These results seem to show that ALD languages are well-distributed over the class of regular languages...|$|R
40|$|This paper {{describes}} some developments {{with respect}} to the agfl formalism and its implementation. agfl is an extremely "lean" syntactic formalism, suitable for describing the surface structure of natural languages, supported by a grammar development system, available on unix systems and large pcs. Our intention is to increase its expressive power by ffl introducing a transduction capability ffl introducing a free word-order operator while further improving the efficiency and usefullness of the parsers generated. Keywords: transducers, Affix Grammars, free word order, optimization. Affix Grammars over a Finite Lattice (agfl) [6] were developed as a formalism for the description of the surface structure of natural languages. Formally, they are not more powerful than <b>CF</b> <b>grammars,</b> but they are much more compact, which makes it easy to develop and maintain large grammars. The agfl formalism is accompanied by a grammar development environment. This agfl system consists at the moment of [...] ...|$|R
50|$|In Latin, most second declension {{masculine}} nouns in -us {{form their}} plural in -i. However, some Latin nouns ending in -us are not second declension (<b>cf.</b> Latin <b>grammar).</b> For example, third declension neuter nouns such as opus and corpus have plurals opera and corpora, and fourth declension {{masculine and feminine}} nouns such as sinus and tribus have plurals sinūs and tribūs.|$|R
40|$|AbstractThe new Associative Language Description (ALD) model, a {{combination}} of locally testable and constituent structure ideas, is proposed, arguing that in practice it equals context-free (<b>CF)</b> <b>grammars</b> in explanatory adequacy, yet it provides a simple description and it excludes mathematical sets based on counting properties, which are rarely (if ever) used in compiler construction or in computational linguistics. The ALD model has been recently proposed as an approach consistent with current views on brain organization. ALD is a “pure”, i. e., nonterminal-free definition. The strict inclusion of ALD languages in CF languages is proved, based on a lemma which strengthens the Pumping Lemma for CF languages. Basic nonclosure and undecidability properties are considered and {{compared with those of}} CF languages. It is shown that the hardest context-free language is in ALD, that there exists a hierarchy of ALD languages and that each ALD tree language enjoys the noncounting property of parenthesized CF languages. Typical technical languages (Pascal, HTML) can be rather conveniently described by ALD rules...|$|R
40|$|In {{the quest}} for a useful syntax for {{language}} definition formalisms [...] - the SDF part of ASD+SDF of Klint and his co-workers [...] - Visser has proposed to extend context free rules with reject productions. We propose to modify the definition to ensure modularity of reject grammars. Next, attention is drawn to the close link between reject grammars (under the modified definition), indexed least fixpoint grammars (Rounds) and simple literal movement grammars (Groenink). This link indicates that reject grammars are closely related to the `mildly context-sensitive' grammar formalisms that have been developed for use in natural language analysis. 1 Reject Grammars To increase the versatility of the ASD+SDF meta-environment for specification and syntax definition created by Paul Klint and his co-workers [6], Visser [9] proposes to add reject rules A ! r A 1 ΔΔΔ An to <b>CF</b> <b>grammars.</b> The recipe for handling these reject rules is, roughly, the following: if A ! r ff is a rule of the [...] ...|$|R
40|$|AbstractIn this paper, we {{deal with}} editing tabular forms for program {{specifications}} based on a particular graph grammar HNGG [2]. First, we formalize syntax-directed editing methods by extending {{of the notion of}} the Cornell Program Synthesizer [8] to attribute NCE graph <b>grammars</b> (<b>cf.</b> [1]). Next, we discuss the algorithms of the editing methods...|$|R
5000|$|Does the {{relation}} [...] correlate {{the class of}} all numbers with one of its subclasses? No. It correlates any arbitrary number with another, {{and in that way}} we arrive at infinitely many pairs of classes, of which one is correlated with the other, but which are never related as class and subclass. Neither is this infinite process itself in some sense or other such a pair of classes... In the superstition that [...] correlates a class with its subclass, we merely have yet another case of ambiguous grammar. Philosophical Remarks § 141, <b>cf</b> Philosophical <b>Grammar</b> p. 465 ...|$|R
40|$|Ever {{since the}} {{traditional}} <b>grammar</b> (<b>cf.</b> Curme 1931), {{it has been}} observed that certain types of English noun phrases can behave like an adverb or an adverbial prepositional phrase on their own, i. e. without any support by a preposition. Such noun phrases have been dubbed adverbial accusatives. They can play virtually the same range of adverbial functions as the "real " adverbials...|$|R
40|$|This paper {{presents}} a model-theoretic semantics for directional modifiers in English. The semantic theory presupposed {{for the analysis}} is that of Montague <b>Grammar</b> (<b>cf.</b> Montague 1970, 1973) which {{makes it possible to}} develop a strongly compositional treatment of directional modifiers, Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the AI literature...|$|R
40|$|AbstractTransformations of graphlike {{expressions}} {{are called}} correct if they preserve a given functional semantics of the expressions. Combining the algebraic theories of graph <b>grammars</b> (<b>cf.</b> [10]) and programming language semantics (cf. [1]) {{it will be}} proved that the correctness of transformation rules carries over to the correctness of derivations via such rules. Applying this result to LISP we show that a LISP interpreter represented by a graph grammar is correct {{with respect to the}} functional semantics of graphlike LISP expressions...|$|R
40|$|We give {{a hybrid}} {{algorithm}} for parsing ffl-grammars based on Tomita's non-ffl-grammar parsing algorithm ([Tom 86]) and Nozohoor-Farshi's ffl- grammar recognition algorithm ([NF 91]). The hybrid parser handles {{the same set}} of grammars handled by Nozohoor-Farshi's recognizer. The algorithm's details and an example of its use are given. We also discuss the deployment of the hybrid algorithm within a GB parser, and the reason an ffl-grammar parser is needed in our GB parser. Contents 1 Introduction 3 2 Tomita's Method for Parsing with ffl-grammars 3 3 Fong's Method for Parsing with ffl-grammars 4 4 Nozohoor-Farshi's Method for Recognition with ffl-grammars 5 5 A Family of Algorithms 5 6 The Hybrid Algorithm 8 7 A Crucial Modification 13 8 Handling Cyclic Grammars 21 1 Introduction Many GB parsers utilize a core context-free parser to recover the x-bar phrase structure of an input sentence. A <b>CF</b> s-structure <b>grammar</b> used in this phrase structure recovery stage will contain several erasi [...] ...|$|R
40|$|The need {{to improve}} the spoken English of {{kindergarten}} students in an international preschool in Surabaya prompted this Classroom Action Research (CAR). It involved the implementation of Form-Focused Instruction (FFI) strategy coupled with Corrective Feedback (<b>CF)</b> in <b>Grammar</b> lessons. Four grammar topics were selected, namely Regular Plural form, Subject Pronoun, Auxiliary Verbs Do/Does, and Irregular Past Tense Verbs as they were deemed to be the morpho-syntax which children acquire early in life based {{on the order of}} acquisition in Second Language Acquisition. The results showed that FFI and CF contributed to the improvement of the spoken grammar in varying degrees, depending on the academic performance, personality, and specific linguistic traits of the students. Students with high academic achievement could generally apply the grammar points taught after the FFI lessons in their daily speech. Students who were rather talkative were sensitive to the CF and could provide self-repair when prompted. Those with lower academic performance generally did not benefit much from the FFI lessons nor the CF...|$|R
40|$|Studies {{with adult}} and infants {{have shown that}} {{subjects}} can learn fairly complex probabilistic relationships. Researchers have used statistical learning as a laboratory to explore issues like word segmentation (Saffran, Aslin & Newport, 1996) and the acquisition of grammar (Morgan, Meier & Newport, 1987). Statistical learning has become a scenario for an argument between the two competing views about language acquisition: the view that assumes that humans have some innate ability to acquire <b>grammar</b> (<b>cf.,</b> Chomsky, 1965); and the view that claims that statistical learning {{is based on the}} same learning mechanisms (e. g., distributed supervised learning) as other domains (see Seidenberg...|$|R
40|$|Abstract: The need {{to improve}} the spoken English of {{kindergarten}} students in an international preschool in Surabaya prompted this Classroom Action Research (CAR). It involved the implementation of Form-Focused Instruction (FFI) strategy coupled with Corrective Feedback (<b>CF)</b> in <b>Grammar</b> lessons. Four grammar topics were selected, namely Regular Plural form, Subject Pronoun, Auxiliary Verbs Do/Does, and Irregular Past Tense Verbs as they were deemed to be the morpho-syntax which children acquire early in life based {{on the order of}} acquisition in Second Language Acquisition. The results showed that FFI and CF contributed to the improvement of the spoken grammar in varying degrees, depending on the academic performance, personality, and specific linguistic traits of the students. Students with high academic achievement could generally apply the grammar points taught after the FFI lessons in their daily speech. Students who were rather talkative were sensitive to the CF and could provide self-repair when prompted. Those with lower academic performance generally did not benefit much from the FFI lessons nor the CF. Keywords: form-focused instructio...|$|R
40|$|Associative Language Descriptions are {{a recent}} grammar model, {{theoretically}} less powerful than Context Free grammars, but adequate for describing the syntax of programming languages. ALD {{do not use}} nonterminal symbols, but rely on permissible contexts for specifying valid syntax trees. In order to assess ALD adequacy, we analyze the descriptional complexity of structurally equivalent <b>CF</b> and ALD <b>grammars,</b> finding comparable measures. The compression obtained using CF copy rules is matched by context inheritance in ALD. The family of hierarchical parentheses languages, an abstract paradigm of HTML, and of expressions with operator precedences are studied in detail. A complete ALD grammar of Pascal is presented to testify of the practicality of the ALD approach. Keywords: Context Free Grammars, Associative Grammars, Grammar Size, Context Inheritance, Descriptional Complexity. 1...|$|R
5000|$|Dependency grammar (DG) can {{accommodate}} the V2 phenomenon simply by stipulating that {{one and only}} one constituent can be a predependent of the finite verb (i.e. a dependent which precedes its head) in declarative (matrix) clauses (in this, Dependency Grammar assumes only one clausal level and one position of the verb, instead of a distinction between a VP-internal and a higher clausal position of the verb as in Generative <b>Grammar,</b> <b>cf.</b> the next section). On this account, the V2 principle is violated if the finite verb {{has more than one}} predependent or no predependent at all. The following DG structures of the first four German sentences above illustrate the analysis (the sentence means 'The kids play soccer in the park before school'): ...|$|R
40|$|International audienceMotivation. Context-free (CF) and {{context-sensitive}} (CS) formal grammars {{are often}} regarded as {{more appropriate to}} model proteins than regular level models such as finite state automata and Hidden Markov Models (HMM). In theory, the claim is well founded {{in the fact that}} many biologically relevant interactions between residues of protein sequences have a character of nested or crossed dependencies. In practice, there is hardly any evidence that grammars of higher expressiveness have an edge over old good HMMs in typical applications including recognition and classification of protein sequences. This is in contrast to RNA modeling, where CFG power some of the most successful tools. There have been proposed several explanations of this phenomenon. On the biology side, one difficulty is that interactions in proteins are often less specific and more " collective " in comparison to RNA. On the modeling side, a difficulty is the larger alphabet which combined with high complexity of <b>CF</b> and CS <b>grammars</b> imposes considerable trade-offs consisting on information reduction or learning sub-optimal solutions. Indeed, some studies hinted that CF level of expressiveness brought an added value in protein modeling when <b>CF</b> and regular <b>grammars</b> where implemented in the same framework (Dyrka, 2007; Dyrka et al., 2013). However, there have been no systematic study of explanatory power provided by various grammatical models. The first step to this goal is define objective criteria of such evaluation. Intuitively, a decent explanatory grammar should generate topology, or the parse tree, consistent with topology of the protein, or its secondary and/or tertiary structure. In this piece of research we build on this intuition and propose a set of measures to compare topology of the parse tree of a grammar with topology of the protein structure...|$|R
40|$|Context-free and {{context-sensitive}} formal grammars {{are often}} regarded as {{more appropriate to}} model proteins than regular level models such as finite state automata and Hidden Markov Models. In theory, the claim is well founded {{in the fact that}} many biologically relevant interactions between residues of protein sequences have a character of nested or crossed dependencies. In practice, there is hardly any evidence that grammars of higher expressiveness have an edge over old good HMMs in typical applications including recognition and classification of protein sequences. This is in contrast to RNA modeling, where CFG power some of the most successful tools. There have been proposed several explanations of this phenomenon. On the biology side, one difficulty is that interactions in proteins are often less specific and more "collective" in comparison to RNA. On the modeling side, a difficulty is the larger alphabet which combined with high complexity of <b>CF</b> and CS <b>grammars</b> imposes considerable trade-offs consisting on information reduction or learning sub-optimal solutions. Indeed, some studies hinted that CF level of expressiveness brought an added value in protein modeling when <b>CF</b> and regular <b>grammars</b> where implemented in the same framework. However, there have been no systematic study of explanatory power provided by various grammatical models. The first step to this goal is define objective criteria of such evaluation. Intuitively, a decent explanatory grammar should generate topology, or the parse tree, consistent with topology of the protein, or its secondary and/or tertiary structure. In this piece of research we build on this intuition and propose a set of measures to compare topology of the parse tree of a grammar with topology of the protein structure. Comment: The 13 th International Conference on Grammatical Inference (ICGI 2016), Delft, The Netherlands, 5 - 7 Oct 201...|$|R
40|$|This article aims to {{demonstrate}} the frequency of constructional subschemas, rather than lexical items, can affect acceptability judgment. Cognitive <b>Grammar</b> (<b>cf.</b> Langacker 1987, 2008) theoretically predicts that the frequency of constructional subschemas has a greater effect on recognition of well-formedness of linguistic expressions than the frequencies of lexical items that compose the expressions. To verify this, the experiment was conducted in which {{participants were asked to}} rate the acceptability of coined Japanese N-N compounds on a scale from 0 to 5, and its result was subsequently analyzed using a multiple regression analysis. 524, 307 patterns of the statistically significant models show the statistic significance of the frequency of constructional subschemas to predict the distribution of acceptability, whereas the frequencies of the component nouns are not significant in this respect...|$|R
40|$|Grammaticalization has {{provoked}} a recent movement for new descriptive insights into grammar writing. Two recent books testify to this movement, particularly for German. The first volume {{to be discussed}} here is devoted to a typical grammaticalization subject, namely prepositions. Di Meola, {{on the basis of}} wide underlying materials, has explored a number of cases that can be analyzed to a certain extent as prepositional phrases. The approach adopted by Di Meola is canonically fitted into grammaticalization theory. Therefore, after a survey of the ingredients necessary for the definition of the category preposition, the latter is looked at specifically under the perspective of grammaticalization. In this respect, it seems more adequate to speak of prepositional phrase or locution, since from the viewpoint of grammaticalization, intended in a strong way as emergent <b>grammar</b> (<b>cf.</b> Hopper 1987), everything may potentially become a preposition: “Ich gehe davo...|$|R
40|$|A special type {{of linear}} bounded automata, called {{deleting}} automata with a restart operation, are introduced, and their language recognition power is studied. 1 Introduction Our motivation partly {{comes from the}} area of natural language analysis and resembles the motivation for Marcus contextual <b>grammars</b> (<b>cf.</b> [8]). Another part of motivation comes from the area of grammar checking of natural as well as programming languages (cf. [4]). We try to capture the feature of (some) languages which can be expressed as follows: in any long sentence, several parts can be left out (i. e. deleted) so that the (in) correctness of the sentence is not affected. The mentioned feature can be naturally modelled by special types of linear bounded automata. One of the obvious possibilities is to consider deleting automata, i. e. linear bounded automata which cannot rewrite the input cells, they can only delete (i. e. completely remove) them. (More natural {{in this context is}} to imagine a [doubly linked] list of [...] ...|$|R
