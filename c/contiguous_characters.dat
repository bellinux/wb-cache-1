7|9|Public
50|$|While {{individual}} strings are {{arrays of}} <b>contiguous</b> <b>characters,</b> {{there is no}} guarantee that the strings are stored as a contiguous group.|$|E
40|$|Let X and Y be any two {{strings of}} finite length. The problem of {{transforming}} X to Y using the edit operations of substitution, deletion, and insertion {{has been extensively}} studied in the literature. The problem can be solved in quadratic time if the edit operations are extended to include the operation of transposition of adjacent characters, and is NP-complete if the characters can be edited repeat-edly. In this paper we consider the problem of transforming X to Y when the set of edit operations is extended to include the squashing and expansion operations. Whereas in the squashing operation two (or more) <b>contiguous</b> <b>characters</b> of X can {{be transformed into a}} single character of Y, in the expansion operation a single character in X may be expanded into two or more <b>contiguous</b> <b>characters</b> of Y. These operations are typically found in the recognition of cursive script. A quadratic time solution to the problem has been presented. This solution is optimal for the infinite-alphabet case. The strategy to compute the sequence of edit operations is also presented. 1...|$|E
40|$|Approved {{for public}} release; {{distribution}} unlimited. Multi-Frequency Modulation (MFM) {{has been developed}} at NPS using both differential quadrature-amplitude-modulation (DQAM) and differential quadrature- phase-shift-keying (DQPSK) encoding formats. This report discusses the use {{of each of these}} formats in transmitting a facsimile encoded message over a voice frequency channel. The satisfactory transmission and receipt of facsimile messages was achieved using both DQPSK and D 16 -QAM encoding formats. Research and testing for this report included the use of variable facsimile transmission rates in an attempt to optimize MFM operating parameters. Experimental results revealed a higher error rate when decoding messages contained similar <b>contiguous</b> <b>characters.</b> Lieutenant Commander, United States Nav...|$|E
5000|$|Script tags {{identify}} the scripts (writing systems) represented in an OpenType typeface. Each tag corresponds to <b>contiguous</b> <b>character</b> code ranges in Unicode. A script tag can consist of 4 or fewer lowercase letters, such as [...] for the Arabic alphabet, [...] for the Cyrillic script and [...] for the Latin alphabet. The math script tag, added by Microsoft for Cambria Math, {{has been added}} to the specification.|$|R
5000|$|In a data stream, both {{text and}} control (or {{formatting}} functions) are interspersed allowing an entire screen to be [...] "painted" [...] {{as a single}} output operation. The concept of formatting in these devices allows the screen to be divided into fields (clusters of <b>contiguous</b> <b>character</b> cells) for which numerous field attributes (colour, highlighting, character set, protection from modification) can be set. A field attribute occupies a physical location on the screen that also determines {{the beginning and end}} of a field.|$|R
40|$|Abstract: Regular {{expression}} signatures {{are most}} widely used in network traffic classification for trusted network management. These signatures are generated by the sequence alignment of the traffic payload. The most commonly used sequence alignment algorithm is Longest Common Subsequence (LCS) algorithm which computes the global similarity between two strings but it fails in consecutive character matches. This paper presents a new divide and conquer alignment algorithm for generating regular expression signature by rewarding <b>contiguous</b> <b>character</b> matches. The {{results indicate that the}} sequence alignment algorithm that used is the space efficient way and the algorithm outperforms LCS in terms of efficiency and accuracy...|$|R
40|$|In this thesis, {{we study}} {{the problem of}} {{representing}} and manipulating a document to facilitate browsing, editing, string/content searches and document assembly. Two major data models in which documents are represented and stored are : 1. a relational data model, where all text contents in a document are represented in relations, each with several attributes, or 2. a text data model, where documents are represented as <b>contiguous</b> <b>characters,</b> typically interspersed with tags to capture their various logical, semantic, and presentational features and relationships Each approach has its own strengths and limitations. In our work, we study how a hybrid system based on a combined text/relational model can support document management. We describe database design trade-offs involving the appropriate placement of information in the text and relational database components. With an appropriate design, the advantages of both models can be exploited, while the shortcomings of using them individua [...] ...|$|E
40|$|In {{real-time}} collaborative editing systems, users {{create a}} shared document by issuing insert, delete, and undo operations on their local replica anytime and anywhere. Data consistency issues arise due to concurrent editing conflicts. Traditional consistency models put restrictions on editing operations updating different portions {{of a shared}} document, which is unnecessary for many editing scenarios, and cause their view synchronization strategies to become less efficient. To address these problems, we propose a new data consistency model that preserves convergence and synchronizes editing operations only when they access overlapped or <b>contiguous</b> <b>characters.</b> Our view synchronization strategy is implemented by a novel data structureâ€“partial persistent sequence. A partial persistent sequence is an ordered set of items indexed by persistent and unique position identifiers. It captures data dependencies of editing operations and encodes {{them in a way}} that they can be correctly executed on any document replica. As a result, a simple and efficient view synchronization strategy can be implemented. Keywords collaborative editing system, data consistency model, persistent data structure 1...|$|E
40|$|Relational {{database}} management systems are mostly used for effective representation and retrieval of data. For the user, {{it is hard}} to learn the database interface language to deal with various operations on databases. Hence {{there is a need to}} construct a bridge between natural language query and database understandable query which is a major challenge. In this paper, we have proposed a Natural Language Parser for Natural Language Interface to customer database. The parser converts the Natural Language query into First order Logic and then the First order logic query is converted into structured query. This paper also addresses the word sense disambiguation problem using ontologies and n-grams. The lexical meaning of the natural language query can be captured with n <b>contiguous</b> <b>characters</b> or words of the query. The proposed system is able to handle extraction, insertion, deletion and updation queries. It is also able to process join, conditional, single and multiple column retrieval queries. The performance of the system is measured using precision, recall and F-measure. The results are progressive...|$|E
50|$|Conventional text {{comparison}} tools {{based on}} the longest common subsequence problem algorithm can potentially miss a lot of similarities between original and modified files, if blocks of text are moved around. Diff-Text is systematic and allows the user to specify the minimum number of <b>contiguous</b> words or <b>characters</b> {{to be considered a}} valid move.|$|R
50|$|Unlike {{many other}} base 32 {{notation}} systems, triacontakaidecimal is <b>contiguous</b> and includes <b>characters</b> that may visually conflict. With the right font {{it is possible}} to visually distinguish between 0, O and 1, I. Other fonts are unsuitable because the context that English usually provides is not provided by a notation system that is expressing numbers. However, the choice of font is not controlled by notation or encoding which is why it's risky to assume a distinguishable font will be used.|$|R
40|$|Three {{representation}} {{methods are}} empirically investigated for Chinese information retrieval: 1 -gram (single <b>character),</b> bigram (two <b>contiguous</b> overlapping <b>characters),</b> and short-word indexing {{based on a}} simple segmentation of the text. The retrieval collection is the approximately 170 MB TREC- 5 Chinese corpus of news articles, and 28 queries that are long and rich in wordings. Evaluation shows that 1 -gram indexing is good but not sufficiently competitive, while bigram indexing works surprisingly well. Bigram indexing leads to a large index term space, three times that of short-word indexing, but {{is as good as}} short-word indexing in precision, and about 5 % better in relevants retrieved. The best average non-interpolated precision is about 0. 45, 17 % better than 1 -gram indexing and quite high for a mainly statistical approach. 1. Introduction While information retrieval (IR) in English has over thirty years of history, IR in Chinese is relatively recent. It is well-known that written Chi [...] ...|$|R
40|$|Abstract Background The {{quality of}} {{multiple}} sequence alignments {{plays an important}} role in the accuracy of phylogenetic inference. It has been shown that removing ambiguously aligned regions, but also other sources of bias such as highly variable (saturated) characters, can improve the overall performance of many phylogenetic reconstruction methods. A current scientific trend is to build phylogenetic trees from a large number of sequence datasets (semi-) automatically extracted from numerous complete genomes. Because these approaches do not allow a precise manual curation of each dataset, there exists a real need for efficient bioinformatic tools dedicated to this alignment character trimming step. Results Here is presented a new software, named BMGE (Block Mapping and Gathering with Entropy), that is designed to select regions in a multiple sequence alignment that are suited for phylogenetic inference. For each character, BMGE computes a score closely related to an entropy value. Calculation of these entropy-like scores is weighted with BLOSUM or PAM similarity matrices in order to distinguish among biologically expected and unexpected variability for each aligned character. Sets of <b>contiguous</b> <b>characters</b> with a score above a given threshold are considered as not suited for phylogenetic inference and then removed. Simulation analyses show that the character trimming performed by BMGE produces datasets leading to accurate trees, especially with alignments including distantly-related sequences. BMGE also implements trimming and recoding methods aimed at minimizing phylogeny reconstruction artefacts due to compositional heterogeneity. Conclusions BMGE is able to perform biologically relevant trimming on a multiple alignment of DNA, codon or amino acid sequences. Java source code and executable are freely available at ftp://ftp. pasteur. fr/pub/GenSoft/projects/BMGE/. </p...|$|E
50|$|Primarily for {{languages}} {{which do}} not support pointers explicitly but do support arrays, the array {{can be thought of}} and processed {{as if it were the}} entire memory range (within the scope of the particular array) and any index to it can be thought of as equivalent to a general purpose register in assembly language (that points to the individual bytes but whose actual value is relative to the start of the array, not its absolute address in memory).Assuming the array is, say, a <b>contiguous</b> 16 megabyte <b>character</b> data structure, individual bytes (or a string of contiguous bytes within the array) can be directly addressed and manipulated using the name of the array with a 31 bit unsigned integer as the simulated pointer (this is quite similar to the C arrays example shown above). Pointer arithmetic can be simulated by adding or subtracting from the index, with minimal additional overhead compared to genuine pointer arithmetic.|$|R
40|$|A key {{decision}} when developing in-memory computing applications is {{choice of}} a mechanism to store and retrieve strings. The most efficient current data structures for this task are the hash table with move-to-front chains and the burst trie, both of which use linked lists as a substructure, and variants of binary search tree. These data structures are computationally efficient, but typical implementations use large numbers of nodes and pointers to manage strings, which is not efficient in use of cache. In this article, we explore two alternatives to the standard representation: the simple expedient of including the string in its node, and, for linked lists, the more drastic step of replacing each list of nodes by a <b>contiguous</b> array of <b>characters.</b> Our experiments show that, for large sets of strings, the improvement is dramatic. For hashing, in the best case the total space overhead is reduced to less than 1 bit per string. For the burst trie, over 300 MB of strings {{can be stored in}} a total of under 200 MB of memory with significantly improved search time. These results, on a variety of data sets, show that cache-friendly variants of fundamental data structures can yield remarkable gains in performance...|$|R
40|$|Measuring {{the amount}} of {{information}} and of shared information in biological strings, as well as relating information to structure, function and evolution, are fundamental computational problems in the post-genomic era. Classical analyses of the information content of biosequences are grounded in Shannon's statistical telecommunication theory, while the recent focus is on suitable specializations of the notions introduced by Kolmogorov, Chaitin and Solomonoff, based on data compression and compositional redundancy. Symmetrically, classical estimates of mutual information based on string editing are currently being supplanted by compositional methods hinged on the distribution of controlled substructures. Current compositional analyses and comparisons of biological strings are almost exclusively limited to short sequences of <b>contiguous</b> solid <b>characters.</b> Comparatively little is known about longer and sparser components, both {{from the point of view}} of their effectiveness in measuring information and in separating biological strings from random strings, and from the point of view of their ability to classify and to reconstruct phylogenies. Yet, sparse structures are suspected to grasp long-range correlations and, at short range, they are known to encode signatures and motifs that characterize molecular families. In this thesis, we introduce and study compositional measures based on the repertoire of distinct subsequences of any length, but constrained to occur with a predefined maximum gap between consecutive symbols. Such measures highlight previously unknown laws that relate subsequence abundance to string length and to the allowed gap, across a range of structurally and functionally diverse polypeptides. Measures on subsequences are capable of separating only few amino acid strings from their random permutations, but they reveal that random permutations themselves amass along previously undetected, linear loci. This is perhaps the first time in which the vocabulary of all distinct subsequences of a set of structurally and functionally diverse polypeptides is systematically counted and analyzed. Another objective of this thesis is measuring the quality of phylogenies based on the composition of sparse structures. Specifically, we use a set of repetitive gapped patterns, called motifs, whose length and sparsity have never been considered before. We find that extremely sparse motifs in mitochondrial proteomes support phylogenies of comparable quality to state-of-the-art string-based algorithms. Moving from maximal motifs [...] motifs that cannot be made more specific without losing support [...] to a set of generators with decreasing size and redundancy, generally degrades classification, suggesting that redundancy itself is a key factor for the efficient reconstruction of phylogenies. This is perhaps the first time in which the composition of all motifs of a proteome is systematically used in phylogeny reconstruction on a large scale. Extracting all maximal motifs, or even their compact generators, is infeasible for entire genomes. In the last part of this thesis, we study the robustness of measures of similarity built around the dictionary of LZW [...] the variant of the LZ 78 compression algorithm proposed by Welch [...] and of some of its recently introduced gapped variants. These algorithms use a very small vocabulary, they perform linearly in the input strings, and they can be made even faster than LZ 77 in practice. We find that dissimilarity measures based on maximal strings in the dictionary of LZW support phylogenies that are comparable to state-of-the-art methods on test proteomes. Introducing a controlled proportion of gaps does not degrade classification, and allows to discard up to 20 % of each input proteome during comparison. PhDCommittee Chair: Apostolico, Alberto; Committee Member: Gray, Alexander; Committee Member: Harvey, Steve; Committee Member: Heitsch, Christine; Committee Member: Istrail, Sori...|$|R

