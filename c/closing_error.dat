8|328|Public
50|$|However recent {{research}} has shown that Eupalinos used three straight lines for his navigation. First he constructed a “mountain” line, over the mountain at the easiest part of the summit which gave a non-optimal position both for feeding water into the tunnel and for water delivery to the city. He connected a “south line” to the mountain line at the south side going straight into the mountain around which Eupalinos undulated the south tunnel. At the north side a “north line” is connected to the mountain line. This line guided the cut into the mountain from the north side. After 273 meters Eupalinos directed the tunnel to the west obviously because of a combination of water, weak rock and mud. When leaving the north line Eupalinos used for navigating an equal legged triangle with angles 22.5, 45 and 22.5 degrees in theory. Measuring errors occurred and were handled by Eupalinos within the accuracy he could obtain. The cutting of the south tunnel was stopped after 390 metres for the north tunnel to catch up. For the rendezvous of the two tunnels a tentacle was applied {{at the head of the}} south tunnel giving 17 metres wider catching width. When the two tunnels reach within earshot which can be estimated for this rock to approximate 12 metres, the tunnels are directed towards each other and met at a nearly right angle. This took place almost under the summit but this was coincidental. Eupalinos levelled around the mountain probably following a contour line but not necessary at the same level as the tunnel. He underestimated his measuring accuracy because before the rendezvous Eupalinos raised the ceiling of the north tunnel by 2.5 m and lowered the floor of the south tunnel by 0.6 m, giving him a catching height of almost 5 metres. At the rendezvous the <b>closing</b> <b>error</b> in altitude for the two tunnels was a few decimetres. Eupalinos used a unit of 20.59 metres for distance measurements and 7.5 degrees (1/12 of a right angle) for setting out directions.|$|E
40|$|International audienceAn honest {{declaration}} of the error in a mass, momentum or energy balance, ?, simply {{raises the question of}} its acceptability: "At what value of ? is the attempted balance to be rejected?" Answering this question requires a reference quantity against which to compare ?. This quantity must be a mathematical function of all the data used in making the balance. To deliver this function, a theory grounded in a workable definition of acceptability is essential. A distinction must be drawn between a retrospective balance and a prospective budget in relation to any natural space-filling body. Balances look to the past; budgets look to the future. The theory is built on the application of classical sampling theory to the measurement and closure of a prospective budget. It satisfies R. A. Fisher's "vital requirement that the actual and physical conduct of experiments should govern the statistical procedure of their interpretation". It provides a test, which rejects, or fails to reject, the hypothesis that the <b>closing</b> <b>error</b> on the budget, when realised, was due to sampling error only. By increasing the number of measurements, the discrimination of the test can be improved, controlling both the precision and accuracy of the budget and its components. The cost-effective design of such measurement campaigns is discussed briefly. This analysis may also show when campaigns to close a budget on a particular space-filling body are not worth the effort for either scientific or economic reasons. Other approaches, such as those based on stochastic processes, lack this finality, because they fail to distinguish between different types of error in the mismatch between a set of realisations of the process and the measured data. Keywords: balance, budget, sampling, hypothesis test, <b>closing</b> <b>error,</b> Earth Syste...|$|E
40|$|In recent years, {{many firms}} have favoured {{residual}} income for value based management. One main argument for this measure is its identity with the {{net present value}} rule and that this compatibility with the net present value rule holds true for all possible depreciation schedules selected. In this article, we analyse whether there are other, undiscussed, accrual accounting numbers that enable net present value-consistent investment decisions for all possible depreciation schedules. Our analysis provides an if-and-only-if characterisation of the entire class of net present value-consistent investment criteria, based on accounting information. This provides {{new insights into the}} residual income concept, hurdle rates, opening and <b>closing</b> <b>error</b> conditions achieved by applying more common performance measure structures, and allocation rules. Moreover, our analysis shows the limits of constructing such investment criteria. Copyright Blackwell Publishing Ltd, 2004. ...|$|E
50|$|Calculations used {{to design}} {{expansion}} chambers {{take into account}} only the primary wave actions. This is usually fairly <b>close</b> but <b>errors</b> can occur due to these complicating factors.|$|R
40|$|Abstract—In {{this paper}} we {{introduce}} a novel kernel classifier {{based on a}} iterative shrinkage algorithm developed for compressive sensing. We have adopted Bregman iteration with soft and hard shrinkage functions and generalized hinge loss for solving l 1 norm minimization problem for classification. Our experimental results with face recognition and digit classification using SVM as the benchmark have shown that our method has a <b>close</b> <b>error</b> rate compared to SVM but do not perform better than SVM. We {{have found that the}} soft shrinkage method give more accuracy and in some situations more sparseness than hard shrinkage methods...|$|R
50|$|FireDaemon Pro is {{sometime}} {{unable to}} <b>close</b> all <b>error</b> popup windows for {{applications such as}} Source Dedicated Server. This is because FireDaemon only intercepts popups of type WS_POPUP and the window class is #32770. Workarounds include leaving the computer logged in rather than logged out or writing custom GUI automation scripts with tools such as AutoIT to automatically <b>close</b> popups. Windows <b>Error</b> Reporting can also interfere with the correct function FireDaemon Pro and should generally be disabled.|$|R
40|$|Abstract. A railway is a {{strip-shaped}} corridor usually {{divided into}} multiple sections that are constructed separately. Associated construction contracts must consider scheduling. Because a railway is a continual linear structure, high-precision level surveys {{are needed for}} rail sections to connect smoothly. However, the different phases and sections of construction require multiple-level surveys, which often lead to bias in the benchmarks. Since the railway is a continuous alignment, and an inconsistent elevation system affects the civil work and subsequent track laying. This study therefore used a statistical method to eliminate inconsistencies in benchmarks. Statistics are widely used in engineering and in daily life, to solve decision making problems involving uncertainties. This study used the expected value of <b>closing</b> <b>error</b> between benchmarks as an index for recalculating level, and used the standard error of expected value as the accuracy index. Application of the proposed method using measurement data for a Taipei underground project of the Taiwan Railway Administration showed that it eliminates bias in benchmarks and provides the required accuracy for closure between adjacent points...|$|E
40|$|Risk {{assessment}} and economic evaluation of mining projects are mainly {{affected by the}} determination of grades and tonnages. In the case of iron ore, multiple variables must be determined for ore characterization which estimation must satisfy the original mass balances and stoichiometry among granulometric fractions and chemical species. Models of these deposits are generally built from estimates obtained using ordinary kriging or cokriging, most time using solely the global grades and determining the ones present at different granulometric partitions by regression. Alternative approaches include determining {{the totality of the}} chemical species and distributing the <b>closing</b> <b>error</b> or leaving one variable aside and determining it by difference afterwards, adding up the error of previous determinations. Furthermore, the estimates obtained are outside the interval of the original variables or even exhibiting negative values. These inconsistencies are generally overridden by post-processing the estimates to satisfy the closed sum condition and positiveness. In this paper, cokriging of additive log-ratios (alr) is implemented to determine global grades of iron, silica, alumina, phosphorous, manganese and loss by ignition and masses of three different granulometric partitions, providing better results than the ones obtained through cokriging of the original variables, with all the estimates within the original data values interval and satisfying the considered mass balance...|$|E
40|$|An honest {{declaration}} of the error in a mass, momentum or energy balance, ε, simply {{raises the question of}} its acceptability: “At what value of ε is the attempted balance to be rejected? ” Answering this question requires a reference quantity against which to compare ε. This quantity must be a mathematical function of all the data used in making the balance. To deliver this function, a theory grounded in a workable definition of acceptability is essential. A distinction must be drawn between a retrospective balance and a prospective budget in relation to any natural space-filling body. Balances look to the past; budgets look to the future. The theory is built on the application of classical sampling theory to the measurement and closure of a prospective budget. It satisfies R. A. Fisher’s “vital requirement that the actual and physical conduct of experiments should govern the statistical procedure of their interpretation”. It provides a test, which rejects, or fails to reject, the hypothesis that the <b>closing</b> <b>error</b> on the budget, when realised, was due to sampling error only. By increasing the number of measurements, the discrimination of the test can be improved, controlling both the precision and accuracy of the budget and its components. The cost-effective design of such measurement campaigns is discussed briefly. This analysis may also show when campaigns to close a budget on a particular space-filling body are not worth the effort for either scientific or economic reasons. Other approaches, such as those based on stochastic processes, lack this finality, because they fail to distinguish between different types of error in the mismatch between a set of realisations of the process and the measured data...|$|E
40|$|Laminar steady forced {{convection}} through {{an array of}} disconnected conducting cylindrical fins is simulated by implementing a pore-scale numerical simulation. Effects of fin array porosity, fin arrangements, pore-scale Reynolds number, and the solid to fluid thermal conductivity ratio, on the heat flux bifurcation at the uniformly heated wall are investigated. The results show that decreasing the Reynolds number or increasing the solid to fluid thermal conductivity ratio monotonically increases the solid to fluid wall heat flux ratio. It is also observed that solid to fluid heat flux ratio increases with increasing the porosity. The obtained results have been compared to available models in the literature. While {{in some cases the}} results are <b>close,</b> <b>errors</b> associated with available models are analyzed and accurate correlations for heat flux bifurcation are proposed...|$|R
40|$|Various {{examples}} of systems and methods are provided for Lyapunov control for uncertain systems. In one example, a system includes a process plant and a robust Lyapunov controller configured to control an input {{of the process}} plant. The robust Lyapunov controller includes an inner closed loop Lyapunov controller and an outer <b>closed</b> loop <b>error</b> stabilizer. In another example, a method includes monitoring a system output of a process plant; generating an estimated system control input based upon a defined output reference; generating a system control input using the estimated system control input and a compensation term; and adjusting the process plant based upon the system control input to force the system output to track the defined output reference. An inner closed loop Lyapunov controller can generate the estimated system control input and an outer <b>closed</b> loop <b>error</b> stabilizer can generate the system control input...|$|R
3000|$|... 0, {{the error}} floor {{provides}} an excellent approximation {{of the performance of}} ML decoding. In this sense, it represents a first benchmark for any sub-optimum decoding algorithm: the <b>closer</b> the <b>error</b> rate offered by the decoding algorithm to the error floor, the smaller the gap between the sub-optimum decoder and the optimum ML decoder. As from (1), the computation of CER [...]...|$|R
40|$|I n this paper, a {{microcomputer}} aided {{system for}} the calibration of the flatness of engineering surfaces is proposed. Rectangular grid-type measurement procedures with <b>closing</b> <b>error</b> technique are offered for angular assessment on the surfaces using electronic precision level. A new technique called ‘enclose tilt technique ’ (E T T) is developed for the analysis offlatness in terms of minimum zone, and defines theflatness in terms of British and I S 0 standards. These measurement analysis processes are performed on-line with a microcomputer using an analogueldigital interface, amplijier with low-pass filter, triggering technique, etc., and appropriate high-level and low-level programming language. A practical flatness calibration of a granite table has been performed for application, {{and the results are}} shown with up to 0. 1 pm uncertainty. The effect of the weight of the precision level, and the rigidity test of the granite table have also been assessed. NOTATION coefficients of plane equation 2 = aX + bY + c (or cu, cL) three possible enclosing points in the tilt of data sum of least-squares error vertical distances between the enclosing plane and A, B, C, PI, respectively values of Ha, Hb,Hc, H, after 84 tilt distances between tilt axis and A, B, C, PI, respectively values of La, Lb, Lc, L, after 84 tilt enclosing points which can enclose the whole data surface spatial coordinates of the ith planar deviations data effective length (step size times number of step) of table in X, Y direction, respectively (m) variables in plane equation flatness data at ith point (pm) change in maximum flatness due to the level weight (pm) angles between enclosing plan...|$|E
40|$|It is {{well known}} that the {{eruption}} of a volcano is often attended by the swelling and subsidence of the volcano itself {{as well as of the}} region surrounding it. In order to study both the spatial and time distribution of such crustal deformations related to eruptions, precise levelling was clone this summer at volcano Asama, one of the most active in Japan. The new line of levels, which is shown in Fig. 1, which begins from bench mark No. 551 previously established at Miyota, goes up the western slope of the volcano, and after passing arround the southern brink of the crater at the summit, descends the eastern slope down to the Asama Volcano Observatory, ending at bench mark No. 547 previously established at Kutukake, the whole forming a loop 48 km long. The observations were made by Mr. Tiyozo Urano of the Japanese Land Survey. A Zeiss geodetic level was used. The observation conditions were extremely severe owing to the abnormal bad weather this summer, to the long and steep slopes of the mountain, and also to the frequent violent eruptions that occurred during the observations. Thanks to the ability of the observer, the <b>closing</b> <b>error</b> was surprisingly small. The results are given in Table I, This being the first observation, naturally crustal deformations cannot yet be known, but upon comparing these results with the heights given in the topographic map of 1912, we may conclude that there has been no crustal deformations exceeding 5 m, except for the central cone, the eastern brink of the crater of which seems to have risen about 18 m. Undoubtedly, this elevation is partly due to the volcanic bombs and ashes that have accumulated since 1912, although there are evidences that true upheavals of the central cone take place during active periods of the volcano...|$|E
40|$|In this study, an {{adaptive}} Neural Network (NN) based backstepping controller is proposed to realize chaos synchronization of two gap junction coupled FitzHugh-Nagumo (FHN) neurons with uncertain time delays. In the designed backstepping controller, a simple Radial Basis Function (RBF) NN {{is used to}} approximate the uncertain nonlinear part of the error dynamical system. The weights of the NN are tuned on-line. A Lyapunov-Krasovskii function is designed to overcome the difficulties from the unknown time delays. Moreover, to relax the requirement for boundness of disturbance, {{an adaptive}} law to adapt the disturbance in real time is given. According to the Lyapunov stability theory, {{the stability of the}} <b>closed</b> <b>error</b> system is guaranteed. The control scheme is robust to the uncertainties such as approximate error, ionic channel noise and external disturbances. Chaos synchronization is obtained by proper choice of the control parameters. The simulation results demonstrate the effectiveness of the proposed control method...|$|R
40|$|The {{differential}} delays of four two-way satellite {{time and}} frequency transfer (TWSTFT) earth stations {{were determined by}} using a portable TWSTFT station. This station was assembled by TUG and visited the time laboratories of PTB, OP, NPL, and VSL from 5 th to 16 th July 2004. A number of calibration measurements was performed during a four hours slot at each location. These measurements were supplemented by differential measurements between the portable and the co-located local stations. Differential delays between the portable and co-located earth stations show a statistical uncertainty below 0. 6 ns for a standard TWSTFT measurement. The final closure measurement at PTB allows a stability analysis of the differential delay between the portable and the local station. The deviation between the two co-locations is only 0. 4 ns. We achieved total estimated uncertainties down to 0. 9 ns. As a further test the results were checked for <b>closing</b> <b>errors</b> and also against previous calibration results. 1...|$|R
40|$|The Constraint Hierarchy (CH) {{framework}} {{is used to}} tackle multiple criteria selection (MCS), consisting {{of a set of}} candidates and a set of, possibly competing, criteria for selecting the "best" candidate(s). In this paper, we identify aspects of the CH framework for further enhancement so as to model and solve MCS problems more accurately. We propose the Fuzzy Constraint Hierarchies framework, which allows constraints to belong to, possibly, more than one level in a constraint hierarchy to a varying degree. We also propose to replace the standard equality relation = used in valuation comparators of the CH framework by the ff-approximate equality relation = a(ff) for providing more flexible control over the handling of valuations with <b>close</b> <b>error</b> values. These proposals result in three new classes of valuation comparators. Formal properties of the new comparators are given, wherever possible. 1 Introduction An over-constrained system [3] is a set of constraints with no solution, ca [...] ...|$|R
40|$|A {{controller}} {{for position}} synchronization at two (or more) robot systems, under a cooperative scheme, {{in the case}} when only position measurement are available, is presented. The synchronization controller consists of a feedback control law {{and a set of}} nonlinear observers. It is shown that the controller yields semi-global exponential convergence of the synchronization <b>closed</b> loop <b>errors.</b> Experimental results show a good agreement with the predicted convergence...|$|R
40|$|In the {{presence}} of parameter uncertainty tracking control can result in significant tracking errors. To overcome this problem adaptive control is applied, which estimates and compensates for the errors of the uncertain parameters. A new adaptive tracking control scheme is presented for standard fully actuated port-Hamiltonian mechanical systems. The adaptive control is such that the <b>closed</b> loop <b>error</b> system is still port-Hamiltonian and asymptotically stable. ...|$|R
40|$|Abstract — The central {{challenge}} in robotic mapping is obtaining reliable data associations (or “loop closures”) : state-ofthe-art inference algorithms can fail catastrophically if even one erroneous loop closure is {{incorporated into the}} map. Consequently, much {{work has been done}} to push <b>error</b> rates <b>closer</b> to zero. However, a long-lived or multi-robot system will still encounter errors, leading to system failure. We propose a fundamentally different approach: allow richer error models that allow the probability of a failure to be explicitly modeled. In other words, we optimize the map while simultaneously determining which loop closures are correct from within a single integrated Bayesian framework. Unlike earlier multiple-hypothesis approaches, our approach avoids exponential memory complexity and is fast enough for realtime performance. We show that the proposed method not only allows loop <b>closing</b> <b>errors</b> to be automatically identified, but also that in extreme cases, the “front-end ” loop-validation systems can be unnecessary. We demonstrate our system both on standard benchmarks and on the real-world datasets that motivated this work. I...|$|R
3000|$|... [*]=[*] 50, the SFFTv 1 and SFFTv 2 {{algorithms}} manifest very <b>close</b> absolute <b>error</b> values; in particular, the attained average {{absolute error}} values were 5.6162 [*]×[*] 10 − 5 and 5.0689 [*]×[*] 10 − 5, respectively. However, the SFFTv 3 attains a lower absolute average error values than other SFFT versions. It is noteworthy {{to mention that}} the lowest absolute average error was attained with the DFTCOMM algorithm at a value of 2.7642 [*]×[*] 10 − 10.|$|R
40|$|Phone {{tokenization}} {{followed by}} n-gram language modeling has consistently provided good {{results for the}} task of language identification. In this paper, this technique is generalized by using Gaussian mixture models {{as the basis for}} tokenizing. Performance results are presented for a system employing a GMM tokenizer in conjunction with multiple language processing and score combination techniques. On the 1996 CallFriend LID evaluation set, a 12 -way <b>closed</b> set <b>error</b> rate of 17 % was obtained. 1...|$|R
40|$|This paper {{discusses}} the {{factors contributing to}} the choice of beam stay-clear (BSC) in DIAMOND. The lifetime which results from this definition is then calculated {{to ensure that it}} is adequate for the operation of the machine. The BSC is defined using a semi-empirical approach by calculating the requirement for a momentum acceptance of at least 4 % at all locations around the lattice. Allowance for injection and <b>closed</b> orbit <b>errors</b> also contribute to the overall BSC...|$|R
40|$|Abstract. Focusing on {{the problem}} of NURBS curve {{interpolation}} in high speed manufacture, a new trajectory planning algorithm, which is suitable for chord <b>error</b> <b>closed</b> loop controlled interpolator is proposed. This tragjectory can determine accelerating, decelerating or maintenance last velocity in the next period via judging the braking distance. By the way of testing different calculating time under different CPU core, the real-time characteristic is validated. The simulation shows that chord <b>error</b> <b>closed</b> loop interpolator can automatically adjust the velocity to satisfying the precision demand, through calculating the curvature. In addition, it can assure that the maximal velocity and the acceleration were equal to the referenced parameters and machine runs with the dynamic characteristic of operator set completely...|$|R
40|$|Changes Improvements Add {{support for}} resolving DataCite and mEDRA DOIs. Add Journal of Applied Clinical Medical Physics style Bugfixes Fix {{other half of}} "cannot read {{property}} 'properties' of undefined" <b>error.</b> (<b>Closes</b> # 175) Fix issue where exception is thrown when an invalid or unavailable ISBN is queried. (Closes # 194) Fix "this. registry. citationreg. citationById[r[0]] is undefined" <b>error.</b> (<b>Closes</b> # 197) Fix "SyntaxError: Unexpected token u in JSON at position 0 " error - caused by invalid PMIDs making {{their way into the}} CSL object. (Closes # 198) Fix CSS class for fixed reference list to conform to BEM. Fix Remove Apple Safari recommendation from options page. Dev Improve test coverage Refactor throughout Fix wallaby configs Improve ISSUE_TEMPLATE. m...|$|R
40|$|The central {{challenge}} in robotic mapping is obtaining reliable data associations (or “loop closures”) : state-of-the-art inference algorithms can fail catastrophically if even one erroneous loop closure is {{incorporated into the}} map. Consequently, much {{work has been done}} to push <b>error</b> rates <b>closer</b> to zero. However, a long-lived or multi-robot system will still encounter errors, leading to system failure. We propose a fundamentally different approach: allow richer error models that allow the probability of a failure to be explicitly modeled. In other words, rather than characterizing loop closures as being “right ” or “wrong”, we propose characterizing the error of those loop closures in a more expressive manner that can account for their non-Gaussian behavior. Our approach leads to an fully-integrated Bayesian framework for dealing with error-prone data. Unlike earlier multiple-hypothesis approaches, our approach avoids exponential memory complexity and is fast enough for real-time performance. We show that the proposed method not only allows loop <b>closing</b> <b>errors</b> to be automatically identified, but also that in extreme cases, the “front-end ” loop-validation systems can be unnecessary. We demonstrate our system both on standard benchmarks and on the real-world datasets that motivated this work. ...|$|R
40|$|We derive a {{computationally}} feasible <b>closed</b> form <b>error</b> {{expression for}} {{probability of error}} for a small multiple access DS-BPSK system operating in a Rayleigh fading environment. Two properties of the crosscorrelation functions of the DS codes help {{reduce the number of}} error probability evaluations required when one considers all possible phase shifts of the users’ codes. Examination of the error rates for different phases of three maximal-length codes reveal that the probability of error is only weakly dependent on the phases...|$|R
40|$|In {{this note}} we propose a {{controller}} that solves {{the problem of}} coordination of two (or more) robots, under a master-slave scheme, in the case when only position measurements are available. The controller consists of a feedback control law, and two non-linear observers. It is shown that the controller yields ultimate uniformly boundedness of the <b>closed</b> loop <b>errors,</b> a relation between this bound and the gains on the controller is established. Simulation results on two twolink robot systems show the predicted convergence performance...|$|R
40|$|AbstractBack in the 1960 s Goldschmidt {{presented}} {{a variation of}} Newton–Raphson iterations for division that is well suited for pipelining. The problem in using Goldschmidt's division algorithm is to present an error analysis that enables one to save hardware by using {{just the right amount}} of precision for intermediate calculations while still providing correct rounding. Previous implementations relied on combining formal proof methods (that span thousands of lines) with millions of test vectors. These techniques yield correct designs but the analysis is hard to follow and is not quite tight. We present a simple parametric error analysis of Goldschmidt's division algorithm. This analysis sheds more light on the effect of the different parameters on the error. In addition, we derive <b>closed</b> <b>error</b> formulae that allow to determine optimal parameter choices in four practical settings. We apply our analysis to show that a few bits of precision can be saved in the floating-point division (FP-DIV) micro-architecture of the AMD-K 7 TMmicroprocessor. These reductions in precision apply to the initial approximation and to the lengths of the multiplicands in the multiplier. When translated to cost, the reductions reflect a savings of 10. 6 % in the overall cost of the FP-DIV micro-architecture...|$|R
40|$|The EX 6 - 0, EX 7 - 0, and EX 7 - 1 {{representative}} benchmark {{sets are}} {{developed for the}} fast evaluation {{of the performance of}} a density functional, or more generally of a computational protocol, in modeling low-lying valence singlet–singlet excitation energies of organic dyes within the range of 1. 5 to 4. 5 eV. All sets share the advantage of being small (a maximum of 7 molecules) but providing statistical errors representative of larger and extended databases. To that extent, the EX 7 - 1 benchmark set goes a step further and is composed of systems as small as possible in order to alleviate the associated computational cost. The reliability of all the sets is assessed through the benchmarking of 15 modern double-hybrid density functionals. The investigation shows not only that the 3 benchmark sets provide <b>close</b> <b>error</b> metrics for each density functional but also that when taking advantage of the Resolution-of-the-Identity and a balanced triple-ζ basis set (e. g., def 2 -TZVP), double hybrids overperform the “popular” hybrids in modeling vertical absorption, emission, and adiabatic energies. A. J. P. J. and J. C. S. G. thank the “Ministerio de Economía y Competitividad” of Spain and the “European Regional Development Fund” through the project CTQ 2014 - 55073 -P for financial support...|$|R
50|$|RNA viruses which {{replicate}} <b>close</b> to the <b>error</b> threshold have a genome size {{of order}} 104 base pairs. Human DNA is about 3.3 billion (109) base units long. This {{means that the}} replication mechanism for DNA must be orders of magnitude more accurate than for RNA.|$|R
40|$|A Fractional Order (FO) Proportional- Integral- Derivative (PID) {{controller}} {{has been}} proposed in this paper which works on the <b>closed</b> loop <b>error</b> and its fractional derivative and fractional integrator. FOPID is a PID controller whose derivative and integral orders are of fractional rather than integer. The extension of derivative and integral order from integer to fractional order provides more flexibility in design of the controller, thereby controlling wide range of dynamics of a system. Frequency domain specifications are used as the performance criteria to be optimizing the FOPID controlle...|$|R
30|$|Each of {{the three}} {{scanners}} produced accurate representations, with no consistent pattern of systematic errors. One of the Ortho Insight 3 D measures showed statistically significant differences (0.161  mm) between replicate measurements. Importantly, the systematic errors {{in the present study}} were <b>close</b> to <b>errors</b> previously reported (ranging from − 0.10 to 0.25  mm) for similar measurements [18, 19]. Measurement differences less than 0.20  mm have been suggested to be clinically acceptable [22]. If the individual has been adequately calibrated and maintains the same landmark definitions, systematic intraobserver differences should not be expected to occur.|$|R
40|$|In this paper, {{we propose}} a {{synchronization}} controller for flexible joint robots, which are interconnected in a master-slave scheme. The synchronization controller {{is based on}} feedback linearization and only requires measurements of the master and slave link positions, since the velocities and accelerations are estimated by means of model-based nonlinear observers. It is shown, using Lyapunov function based stability analysis, that the proposed synchronization controller yields local uniformly ultimately boundedness of the <b>closed</b> loop <b>errors.</b> A tuning gain procedure is presented. The results are supported by simulations in a one degree of freedom master-slave system...|$|R
40|$|The two {{cleaning}} insertions in the LHC, for betatron {{and momentum}} collimation, are optimized for an ideal lattice and collimator jaw setup. We have studied a collimation beam line with randomly generated jaw misalignments and quadrupole field and alignment errors, the resultant {{distortion of the}} reference orbit being corrected {{with the help of}} monitors placed near critical collimators. Different <b>closed</b> orbit <b>errors</b> and beam shapes are considered at the entrance. We report the level of errors for which no corrections are needed and the level for which corrections are not possible. ...|$|R
40|$|This paper {{describes}} a 3 D SLAM system using information from an actuated laser scanner and camera installed on a mobile robot. The laser samples the local {{geometry of the}} environment and is used to incrementally build a 3 D point-cloud map of the workspace. Sequences of images from the camera are used to detect loop closure events (without reference to the internal estimates of vehicle location) using a novel appearance-based retrieval system. The loop closure detection is robust to repetitive visual structure and provides a probabilistic measure of confidence. The images suggesting loop closure are then further processed with their corresponding local laser scans to yield putative Euclidean image-image transformations. We show how naive application of this transformation to effect the loop closure can lead to catastrophic linearization errors and go on to describe a way in which gross, pre-loop <b>closing</b> <b>errors</b> can he successfully annulled. We demonstrate our system working in a challenging, outdoor setting containing substantial loops and beguiling, gently curving traversais. The results are overlaid on an aerial image to provide a ground truth comparison with the estimated map. The paper concludes with an extension into the multi-robot domain in which 3 D maps resulting from distinct SLAM sessions (no common reference frame) are combined without recourse to mutual observation. © 2006 IEEE...|$|R
