39|59|Public
40|$|Abstract: Learning {{performance}} of natural gradient actor-critic algorithms is outstanding especially in high-dimensional spaces than conventional actor-critic algorithms. However, representation issues of stochastic policies or value functions are remaining because the actor-critic approaches need to design it carefully. The author has proposed random rectangular <b>coarse</b> <b>coding,</b> {{that is very}} simple and suited for approximating Q-values in high-dimensional state-action space. This paper shows a quantitative analysis of the random <b>coarse</b> <b>coding</b> comparing with regular-grid approaches, and presents a new approach that combines the natural gradient actor-critic with the random rectangular <b>coarse</b> <b>coding...</b>|$|E
40|$|Abstract — This paper {{presents}} a <b>coarse</b> <b>coding</b> technique and an action selection scheme for reinforcement learning (RL) in multi-dimensional and continuous state-action spaces following conventional and sound RL manners. RL in high-dimensional continuous domains includes two issues: One is a generalization problem for value-function approximation, {{and the other}} is a sampling problem for action selection over multi-dimensional continuous action spaces. The proposed method combines random rectangular <b>coarse</b> <b>coding</b> with an action selection scheme using Gibbs-sampling. The random rectangular <b>coarse</b> <b>coding</b> is very simple and quite suited both to approximate Q-functions in high-dimensional spaces and to execute Gibbs sampling. Gibbs sampling enables us to execute action selection following Boltsmann distribution over high-dimensional action space. The algorithm is demonstrated through Rod in maze problem and a redundant-arm reaching task comparing with conventional regular grid approaches. I...|$|E
40|$|The {{language}} comprehension {{deficits in}} adults with relatively focal right hemisphere brain damage (RHD) can cause considerable social handicap. To date, however, treatment for language deficits {{in this population}} remains almost entirely untested and is often based on theoretically- and empirically-tenuous positions. This abstract presents preliminary, Phase I data from a novel, implicit language processing treatment for adults with RHD. The focus of treatment is motivated by two major accounts of common language comprehension problems in adults with RHD: <b>coarse</b> <b>coding</b> and suppression deficits. <b>Coarse</b> <b>coding</b> processes activate wide-ranging aspects of word meaning independent of the surrounding context, and <b>coarse</b> <b>coding</b> deficits in adults with RHD impair the processing of distant meanings or features of words (e. g., “rotten ” as a feature of “apple”) 1. A normal suppression process reduces mental activation of concepts that become less relevant to a current context, and its impairment in RHD is indexed by prolonged interference from contextually-inappropriate interpretations (e. g., the “ink ” {{meaning of the word}} “pen, ” in the sentence “He built a pen”) 2, 3. <b>Coarse</b> <b>coding</b> and suppression are partially domain-general language comprehension processes. For example, both predict aspects of discours...|$|E
40|$|An amplitude-discriminating type twin-channel {{brushless}} resolver decoder {{based on}} AD 2 S 80 A chip and its decoding strategy {{have been proposed}} in this paper. The decoder uses two AD 2 S 80 A chips as resolver-to-digital converter(RDC). One of them is used to generate 16 -bit <b>coarse</b> <b>code</b> {{and the other to}} 16 -bit fine code. Then software uses a novel strategy to combinate the <b>coarse</b> <b>code</b> and fine code into a 19 -bit high precise angle-measuring code. Finally, an experiment has verified the strategy which is practical for high precise angle position sensing. © (2013) Trans Tech Publications, Switzerland. Korea Maritime University; Hong Kong Industrial Technology Research Centre; Inha Universit...|$|R
40|$|<b>Coarse</b> <b>codes</b> {{are widely}} used {{throughout}} the brain to encode sensory and motor variables. Methods designed to interpret these codes, such as population vector analysis, are either inefficient (the variance of the estimate is {{much larger than the}} smallest possible variance) or biologically implausible, like maximum likelihood. Moreover, these methods attempt to compute a scalar or vector estimate of the encoded variable. Neurons are faced with a similar estimation problem. They must read out the responses of the presynaptic neurons, but, by contrast, they typically encode the variable with a further population code rather than as a scalar. We show how a nonlinear recurrent network can be used to perform estimation in a near-optimal way while keeping the estimate in a <b>coarse</b> <b>code</b> format. This work suggests that lateral connections in the cortex may be involved in cleaning up uncorrelated noise among neurons representing similar variables...|$|R
40|$|Abstract: 2 ̆ 2 <b>Coarse</b> <b>coded</b> {{memories}} {{have appeared}} in several neural network symbol processing models, such as Touretzky and Hinton 2 ̆ 7 s distributed connectionist production system DCPS, Touretzky 2 ̆ 7 s distributed implementation of Lisp S-expressions on a Boltzmann machine, and St. John and McClelland 2 ̆ 7 s PDP model of case role defaults. In order to determine how these models would scale, one must first have some understanding of the mathematics of <b>coarse</b> <b>coded</b> representations. For example, the working memory of DCPS, which stores triples of symbols and consists of 2, 000 units, can hold roughly 20 items at a time out of a 15, 625 -symbol alphabet. How would DCPS scale if the alphabet size were raised to 50, 000 ? With the current alphabet size, how many units {{would have to be}} added simply to double the working memory capacity to 40 triples? We present some analytical results related to these questions. 2 ̆...|$|R
40|$|This study {{presents}} preliminary, Phase I {{data from}} a novel, implicit language processing treatment for adults with right hemisphere damage (RHD). The focus of treatment is motivated by two major accounts of common language comprehension problems in adults with RHD: <b>Coarse</b> <b>coding</b> and suppression deficits. The treatment approach is novel in that it aims to facilitate <b>coarse</b> <b>coding</b> and suppression processes implicitly, through contextual prestimulation. The treatment also targets partially domain-general operations with broad implications for language comprehension, rather than specific language structures or forms. Discussion will address the promise and limitations of treatment results for three adults with RHD...|$|E
40|$|A higher-order {{neural network}} (HONN) can be {{designed}} to be invariant to changes in scale, translation, and inplane rotation. Invariances are built directly into the architecture of a HONN and {{do not need to}} be learned. Consequently, fewer training passes and a smaller training set are required to learn to distinguish between objects. The size of the input field is limited, however, because of the memory required for the large number of interconnections in a fully connected HONN. By <b>coarse</b> <b>coding</b> the input image, the input field size can be increased to allow the larger input scenes required for practical object recognition problems. We describe a <b>coarse</b> <b>coding</b> technique and present simulation results illustrating its usefulness and its limitations. Our simulations show that a third-order neural network can be trained to distinguish between two objects in a 4096 x 4096 pixel input field independent of transformations in translation, in-plane rotation, and scale in less than ten passes through the training set. Furthermore, we empirically determine the limits of the <b>coarse</b> <b>coding</b> technique in the object recognition domain...|$|E
40|$|In many {{application}} areas, {{particularly in}} the biological sciences, there {{is the need to}} store several values of variables. Given a finite precison, one can store these values in Nk explicit cells, refered to as value cells, in a k-dimensional space of grain N. Typically, the number of values that must be stored is a very small fraction of the total number specified by the grain of the multidimensional space. This leads to data structuring that reduces the number of explicit cells required for a given level of accuracy. One idea is <b>coarse</b> <b>coding,</b> intersection of larger, coarser grained cells. <b>Coarse</b> <b>coding</b> has been shown {{to reduce the number of}} cells required by a factor of lilY-I where D is the diameter of the coarse cell in units of fine grained cells. This intuitively appealing idea in fact involves many subtle tradeoffs that are the focus of this paper. <b>Coarse</b> <b>coding</b> is shown to be independant of the isptrophy of the cells and superior to simply reducing the grain of the representation space. Loss of information due to the possibility of some fine grained cells sharing some of the same coarse grained cells and due to uncertainty in the inpu...|$|E
50|$|Another {{application}} of filter banks is signal compression when some frequencies {{are more important}} than others. After decomposition, the important frequencies can be coded with a fine resolution. Small differences at these frequencies are significant and a coding scheme that preserves these differences must be used. On the other hand, less important frequencies {{do not have to be}} exact. A <b>coarser</b> <b>coding</b> scheme can be used, even though some of the finer (but less important) details will be lost in the coding.|$|R
40|$|Applications {{involving}} {{transmission of}} color images through low bandwidth telecommunication media involve a trade-off between image quality and decoding complexity. In this paper we investigate techniques that locally optimize the output quality of foreground {{objects in the}} decoded image, while obtaining significant gain by <b>coarser</b> <b>coding</b> of information of low interest areas of the image. High compression/quality ratios are achieved by introducing high-quality windows, {{as well as by}} using progressive coding principles in image filtering, and enhanced run length encoding techniques. The system described is currently fully operational and serving telemedicine image communication applications for remote diagnosis...|$|R
3000|$|... by {{applying}} a <b>coarse</b> quantization <b>coding</b> stage with quantization parameters fixed {{so as to}} assure the desired average initial quality set to PSNR 0 = 20 dB.e The optimal frame bit budgets r [...]...|$|R
40|$|In {{discussions of}} <b>coarse</b> <b>coding,</b> {{previous}} results {{have suggested that}} the larger the overlap between receptive fields is made, the sharper the achievable hyperacuity becomes. This note moderates this view by giving for an array of noisy analog receptors the scaling laws for the best achievable hyperacuity as a function of increasing receptive field size...|$|E
40|$|<b>Coarse</b> <b>coding</b> {{can also}} support the {{assessment}} of item familiarity, {{one of the main}} memory functions ascribed to PRh [8]. In a recent connectionist model of recognition memory [9], the extrahippocampal component that includes PRh supplies a familiarity signal based on extensively overlapping distributions, and yet supports fine discriminations between similar objects in forced-choice recognition memory tasks successfully. A model of PRh that incorporates <b>coarse</b> <b>coding</b> thus would not only offer a promising account of its perceptual functions, but would also be in line with computational accounts of its memory functions. Acknowledgments We thank Dr Melvyn Goodale for his comments on an earlier draft of this manuscript. We are also grateful to Edward O’Neil and Benjamin Bowles for their contributions to many helpful discussions on this topic. A. C. is supported by a fellowship from the CIHR Group for Action and Perception. S. K. is funded by a CIHR operating grant...|$|E
40|$|For {{the task}} of position-, scale-, and rotation-invariant pattern {{recognition}} higher order neural networks have shown good separation results for different object classes. However, their use {{is limited by the}} large number of monomials that have to be calculated to get the invariant features. Even with the application of methods like <b>coarse</b> <b>coding</b> {{to reduce the number of}} monomials the computational requirements are still large...|$|E
40|$|Abstract—The {{first stage}} of the signal {{processing}} chain in a Global Positioning System (GPS) receiver is the acquisition, which provides for a desired satellite <b>coarse</b> <b>code</b> phase and Doppler frequency estimates to subsequent stages. Thus, acquisition is a two-dimensional search, implemented as demodulation and non-coherent correlation. For a certain Doppler estimate, software-defined GPS receivers typically compute the correlation for all time lags in parallel. One way to detect {{the presence of a}} signal is by comparing the ratio between the largest and the second largest correlation peak against a threshold. For this type of receivers, the detection and false alarm probabilities are derived. Interestingly, the false alarm probability is independent of the noise power spectral density, which allows a fixed threshold setting. The analytic results are verified by a series of simulations. I...|$|R
40|$|Abstract- Human {{episodic memory}} pro-vides a {{seemingly}} unlimited storage for ev-eryday experiences, and a retrieval {{system that allows}} us to access the experiences with partial activation of their components. This paper presents a computational model of epi-sodic memory inspired by Damasio's idea of Convergence Zones. The model consists of a layer of perceptual feature maps and a bind-ing layer. A perceptual feature pattern is <b>coarse</b> <b>coded</b> in the binding layer, and stored on the weights between layers. A partial ac-tivation of the stored features activates the binding pattern which in turn reactivates the entire stored pattern. A worst-case analy-sis shows that with realistic-size layers, the memory capacity of the model is several times larger than the number of units in the model, and could account for the large capacity of human episodic memory. I...|$|R
40|$|The {{paper is}} {{concerned}} with communication within a team of players trying to coordinate in response to information dispersed among them. The problem is non-trivial because they cannot communicate all information instantaneously, but have to send longer or shorter sequences of messages, using <b>coarse</b> <b>codes.</b> We focus {{on the design of}} these codes and show that members may gain comaptibility advantages by using identical codes, and that this can support the existence of several, more or less efficient, symmetric equilibria. Asymmetric eqilibria exist if coordination across different sets of members is of differing importance, and fewer symmetric equilibria exist if the members' local environments are sufficiently heterogeneous. The results are consistent with the stylized fact that firms differ even within industries, and that coordination between divisions is harder than coordination inside divisions. ...|$|R
40|$|Electrophysiological {{studies in}} various sensory systems of {{different}} species show that many neurons involved in object localization have large receptive fields. This seems {{to contradict the}} high sensory resolution and the behavioral precision observed in localization experiments. Assuming a <b>coarse</b> <b>coding</b> mechanism, the resolution obtained by an ensemble of neurons is analytically calculated {{as a function of}} receptive field size. It is shown that particularly large receptive fields yield a high resolution...|$|E
40|$|Language {{comprehension}} {{deficits in}} adults with focal right hemisphere brain damage (RHD) can cause considerable social handicap. To date, however, treatment for these deficits remains almost entirely untested. This abstract reports {{an investigation of}} whether Contextual Constraint Treatment (CCT) [...] a novel, implicit, stimulation-facilitation treatment for language comprehension processes 1, 2 [...] can yield generalized gains to measures of discourse comprehension in adults with RHD. The focus of CCT is motivated by two major accounts of typical RHD language comprehension problems: that they are due to <b>coarse</b> <b>coding</b> or suppression deficits. <b>Coarse</b> <b>coding</b> (CC) activates wide-ranging aspects of word meaning independent of surrounding context. In RHD, CC deficits impair processing of distant meanings/features of words (e. g., “rotten ” as a feature of “apple”) 3. A normal suppression (SUPP) process reduces mental activation of concepts that become less relevant to a current context. RHD SUPP impairment is indexed by prolonged processing interference from contextually-inappropriate interpretations (e. g., the “ink ” {{meaning of the word}} “pen, ” in the sentence “He built a pen”) 4, 5. CC and SUPP are partially domain-general language comprehension processes. For example, both predic...|$|E
40|$|In a {{previous}} study, we calculated the resolution obtained by {{a population of}} overlapping receptive fields, assuming a <b>coarse</b> <b>coding</b> mechanism. The results, which favor large receptive fields, are applied to the visual system of tongue-projecting salamanders. An analytical calculation gives the number of neurons necessary to determine the direction of their prey. Direction localization and distance determination are studied in neural network simulations of the orienting movement and the tongue projection, respectively. In all cases, large receptive fields {{are found to be}} essential to yield a high sensory resolution. The results are in good agreement with anatomical, electrophysiological and behavioral data...|$|E
40|$|A system, TRIDEC, that {{is capable}} of {{distinguishing}} between a set of objects despite changes in the objects' positions in the input field, their size, or their rotational orientation in 3 D space is described. TRIDEC combines very simple yet effective features with the classification capabilities of inductive decision tree methods. The feature vector is a list of all similar triangles defined by connecting all combinations of three pixels in a <b>coarse</b> <b>coded</b> 127 x 127 pixel input field. The classification is accomplished by building a decision tree using the information provided from a limited number of translated, scaled, and rotated samples. Simulation results are presented which show that TRIDEC achieves 94 percent recognition accuracy in the 2 D invariant object recognition domain and 98 percent recognition accuracy in the 3 D invariant object recognition domain after training on only a small sample of transformed views of the objects...|$|R
40|$|The feature {{learning}} algorithm for information-extreme classifier by clustering of Fast Retina Keypoint binary descriptor, calculated for local features, and usage of spatial pyramid kernel for increasing noise immunity and informativeness of feature representation are considered. Proposed {{a method of}} parameters optimization for feature extractor and decision rules based on multi-level <b>coarse</b> features <b>coding</b> using information criterion and population-based search algorithm...|$|R
40|$|Human {{episodic memory}} {{provides}} a seemingly unlimited storage for everyday experiences, and a retrieval {{system that allows}} us to access the experiences with partial activation of their components. This paper presents a computational model of episodic memory inspired by Damasio's idea of Convergence Zones. The model consists of a layer of perceptual feature maps and a binding layer. A perceptual feature pattern is <b>coarse</b> <b>coded</b> in the binding layer, and stored on the weights between layers. A partial activation of the stored features activates the binding pattern which in turn reactivates the entire stored pattern. A worst-case analysis shows that with realistic-size layers, the memory capacity of the model is several times larger than the number of units in the model, and could account for the large capacity of human episodic memory. I. Introduction Human episodic memory is characterized by an extremely high capacity. New memories are formed every few seconds, and many of those persist [...] ...|$|R
40|$|The {{subject of}} neural coding has {{generated}} much debate. A key {{issue is whether}} the nervous system uses coarse or fine coding. Each has different strengths and weaknesses and, therefore, different implications for how the brain computes. For example, the strength of <b>coarse</b> <b>coding</b> {{is that it is}} robust to fluctuations in spike arrival times; downstream neurons do not have {{to keep track of the}} details of the spike train. The weakness, though, is that individual cells cannot carry much information, so downstream neurons have to pool signals across cells and/or time to obtain enough information to represent the sensory world and guide behavior. In contrast, with fine coding, individual cells can carry much more information, but downstream neurons have to resolve spike train structure to obtain it. Here, we set up a strategy to determine which codes are viable, and we apply it to the retina as a model system. We recorded from all the retinal output cells an animal uses to solve a task, evaluated the cells' spike trains for as long as the animal evaluates them, and used optimal, i. e., Bayesian, decoding. This approach makes it possible to obtain an upper bound on the performance of codes and thus eliminate those that are insufficient, that is, those that cannot account for behavioral performance. Our results show that standard <b>coarse</b> <b>coding</b> (spike count coding) is insufficient; finer, more information-rich codes are necessary...|$|E
40|$|This paper {{presents}} {{results of}} experiments in subsymbolic processing of visual data, to achieve identification and tracking of arbitrary objects, which {{are intended to}} be used in autonomous robots for novelty detection and navigation purposes. Artificial neural networks with unsupervised training are used as the classification stage for the vision system, in order to provide the robot the ability to develop its own representations from perceptual data without the need of any external human-provided information. We present an evaluation of the behaviour of the system when using very simple feature extraction techniques, such as horizontal and vertical average histograms, as well as average <b>coarse</b> <b>coding.</b> ...|$|E
40|$|This study {{examines}} two language processing functions {{that have the}} potential to create socially handicapping language comprehension difficulties in adults with right hemisphere brain damage (RHD). The first, coarse semantic coding, allows normal comprehenders to bring to mind distant meanings or features of words that are appropriate in highly specific contexts (e. g., the "rotten" feature of the word "apple" in the context of spoiled produce). The second, suppression, is a process that inhibits contextually-irrelevant meanings (e. g., the "card-playing" meaning of the word "spade" in "He dug with the spade. "). In prior work, some adults with RHD were found to have impaired suppression 1 - 4 or <b>coarse</b> <b>coding</b> processes 5 - 6. These language processing impairments can make it difficult for individuals with RHD to participate in everyday social communication. For example, they can have trouble thinking beyond the most typical instance of an entity (e. g., an apple that's red, round, and crunchy) when another instance is being referred to (e. g., an apple that's rotten). Another possibility is they can be misled by ambiguities which are commonplace in conversation, and have difficulty getting back on track (e. g., keeping in mind the "card-playing" meaning of the word "spade" in a sentence like "He dug with the spade"). These problems predict comprehension performance on measures of narrative comprehension, as well 7, 8. To date, there is no information about how prevalent these deficits are, or how often they may co-occur in the same individual. This project identifies the proportions of a sizeable group of adults with RHD that have either a <b>coarse</b> <b>coding</b> deficit, a suppression deficit, co-occurring deficits, or neither deficit in reference to criteria developed from prior studies of healthy control subjects 1 - 3, 5, 6...|$|E
40|$|We have {{implemented}} a reinforcement learning architecture as the reactive {{component of a}} two layer control system for a simulated race car. We have found that separating the layers has expedited gradually improving competition and multagent interaction. We ran experiments to test the tuning, decomposition and coordination of the low level behaviors. We then extended our control system to allow passing of other cars and tested its ability to avoid collisions. The best design used reinforcement learning with separate networks for each behavior, <b>coarse</b> <b>coded</b> input and a simple rule based coordination mechanism. Introduction Autonomous agents require a mix of behaviors, i. e., responses to different stimuli. This {{is especially true in}} situations where there are other agents present or where the environment is otherwise nondeterministic. For an agent to be effective in its environment, it must have a large repertoire of behaviors and must be able to coordinate the use of those [...] ...|$|R
40|$|This chapter {{describes}} a functional architecture for word recognition {{that focuses on}} how orthographic and phonological information cooperate in initial form-based processing of printed word stimuli. Computational mechanisms for orthographic processing and orthography-tophonology translation are described. Two types of orthographic code are defined: a <b>coarse</b> <b>code</b> used to rapidly access semantic information during silent reading, and a finer-grained code used to access phonology from orthography. Furthermore, two types of phonological code are distinguished: one that is required for silent reading, {{and one that is}} required for reading aloud (articulation). The proposed architecture was initially constrained by behavioral data and purely computational considerations. The present chapter summarizes this work and describes recent research using electrophysiological recording of brain activity to further constrain the theory. 2 1. Visual word recognition. Understanding how literate adults can read single words {{has been one of the}} major objectives of cognitive psychology since the very beginning of this science. For many adults...|$|R
40|$|The {{paper is}} {{concerned}} with communication within a team of players trying to react to information dispersed among them. The problem is nontrivial because they cannot communicate all information, but have to use <b>coarse</b> <b>codes.</b> The paper argues that members gain compatibility advantages by using identical codes, but that this may limit the ability of large teams to adapt to heterogeneous environments. This argument is made in four steps. (1) It is easier to integrate the messages of two members if they use the same code, even if decoding is not a problem. (2) In games where members decide on their codes individually, then for any code, there exists a symmetric equilibrium in which all members use that code. (3) When local environments are heterogeneous, it may be efficient if different members of larger teams use different codes. (4) If the probabilities of alternative equilibria are influenced by which communication channels are open, it may be better {{to reduce the size}} of the team by cu [...] ...|$|R
40|$|The {{language}} comprehension {{deficits in}} adults with focal right hemisphere brain damage (RHD) can cause considerable social handicap. To date, however, treatment for language deficits {{in this population}} remains almost entirely untested. This abstract reports a single-subject experimental design study, performed to investigate whether Contextual Constraint Treatment [...] a novel, implicit, stimulation-facilitation treatment for language comprehension processes [...] can yield generalized gains to broader measures of language comprehension in adults with RHD. The focus of Contextual Constraint Treatment (CCT) is motivated by two major accounts of common language comprehension problems in adults with RHD: <b>coarse</b> <b>coding</b> and suppression deficits. The patient in this study had a <b>coarse</b> <b>coding</b> (CC) deficit, so we describe here only the CC version of the treatment. CC processes activate wide-ranging aspects of word meaning independent of the surrounding context, and CC deficits in adults with RHD impair the processing of distant meanings or features of words (e. g., “rotten” as a feature of “apple”) 1. CC is a partially domain-general language comprehension process. That is, CC ability predicts aspects of discourse comprehension, is hypothesized to underpin figurative language comprehension, and is involved in processing phrasal metaphors 2. Thus, treatment that improves CC processes {{has the potential to}} generalize to a range of communicative outcomes. CCT is novel in aiming to facilitate comprehension processes implicitly, through contextual prestimulation. This approach contrasts with the majority of treatments for neurologically-based cognitive-linguistic disorders, which are direct, explicit, and/or metalinguistic. We implemented this approach to avoid confounding treatment of impaired processes with irrelevant, and potentially difficult, task demands, as adults with RHD who can perform well on implicit assessments of language processing often have difficulty with metalinguistic assessments of the same processing operations 2...|$|E
40|$|We present {{modifications}} to Recursive Auto-Associative Memory which increase its robustness and storage capacity. This {{is done by}} introducing an extra layer to the compressor and reconstructor networks, employing integer rather than realvalued representations, pre-conditioning the weights and presetting the representations to be compatible with them, and using a quick-prop modification. Initial studies have shown this method to be reliable for data sets with up to three hundred subtrees. Introduction In the late 1980 's {{a number of new}} connectionist models were developed in response to criticisms (e. g. Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were <b>coarse</b> <b>coding</b> (Touretzky, 1986), tensor based representation (Smolensky, 1990), reduced representations (Hinton, McClelland & Rumelhart, 1986), and RAAM (Pollack, 1990). Compared to earlier systems, they had the advantage of compo [...] ...|$|E
40|$|Modifications to Recursive Auto-Associative Memory are presented, {{which allow}} it to store deeper and more complex data {{structures}} than previously reported. These modifications include adding extra layers to the compressor and reconstructor networks, employing integer rather than real-valued representations, pre-conditioning the weights and pre-setting the representations to be compatible with them. The resulting system is tested on a data set of syntactic trees extracted from the Penn Treebank. 1 Introduction In the late 1980 's {{a number of new}} connectionist models were developed in response to criticisms (e. g. Fodor & Pylyshyn, 1988) that connectionism lacked the flexibility and representational adequacy needed for higher level cognitive tasks. Chief among these were <b>coarse</b> <b>coding</b> (Touretzky, 1986), tensor based representations (Smolensky, 1990), reduced representations (Hinton et al., 1986), and RAAM (Pollack, 1990). Compared to earlier systems, they had the advantage of compositiona [...] ...|$|E
40|$|International audienceThe aim of {{partially}} and dynamically reconfigurable hardware is {{to provide}} an increased flexibility through the load of multiple applications on the same reconfigurable fabric at the same time. However, a configuration bit-stream loaded at runtime should be created offline for each task of the application. Moreover, modern applications {{use a lot of}} specialized hardware blocks to perform complex operations, which tends to cancel the "single bit-stream for a single application" paradigm, as the logic content for different locations of the reconfigurable fabric may be different. In this paper we propose a design flow for generating compressed configuration bit-streams abstracted from their final position on the logic fabric. Those configurations will then be decoded and finalized in real-time and at run-time by a dedicated reconfiguration controller to be placed at a given physical location. Our experiments show that densely routed applications gain the most with a compression factor of more than 2 × using the finest cluster size, but <b>coarser</b> <b>coding</b> can be implemented to achieve a compression factor up to 10 ×...|$|R
5000|$|On 22 April, {{the network}} issued a {{statement}} that read, [...] "within an M classification <b>code,</b> <b>coarse</b> language is permitted provided {{it is appropriate to}} the storyline or program context." [...] An apology was not made.|$|R
50|$|One {{idea has}} been that such cells form {{ensembles}} for the <b>coarse</b> or distributed <b>coding</b> of faces rather than detectors for specific faces. Thus, a specific grandmother may be represented by a specialized ensemble of grandmother or near grandmother cells.|$|R
