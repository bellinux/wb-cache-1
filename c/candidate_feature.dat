112|541|Public
50|$|Two popular filter metrics for {{classification}} {{problems are}} correlation and mutual information, although neither are true metrics or 'distance measures' in the mathematical sense, since {{they fail to}} obey the triangle inequality and thus do not compute any actual 'distance' - they should rather be regarded as 'scores'. These scores are computed between a <b>candidate</b> <b>feature</b> (or set of features) and the desired output category. There are, however, true metrics that are a simple function of the mutual information; see here.|$|E
3000|$|... [...]) 2 = αβ is the {{determinant}} of H. If the ratio R {{for a certain}} <b>candidate</b> <b>feature</b> point is larger than (r [...]...|$|E
40|$|This paper {{presents}} {{a method to}} automatically locate facial feature points under large variations in pose, illumination and facial expressions. First we propose a method to calculate probabilistic-like output for each pixel of image. This probabilistic-like output describes {{the possibility of the}} pixel {{to be the center of}} specified object. A Gaussian Mixture Model is used to approximate the distribution of probabilistic-like output. The centers of these Gaussians are assigned with a probabilistic-like measure and they are considered as <b>candidate</b> <b>feature</b> points. There might be one or more <b>candidate</b> <b>feature</b> points in each facial region. A 3 D model of facial feature points is built to enforce constraints on the localization results of feature points. Compared with Active Shap...|$|E
40|$|Mobile mapping systems {{typically}} {{employ a}} combination of satellite (GPS), inertial (INS) and other sensors to provide the positional and attitude information required to georeference digital imagery captured by the vehicle. This paper outlines the current development status of an alternative, image-based bridging strategy. The sequence of stereo imagery is processed to enable the direct estimation of the three-dimensional motion parameters of the mapping platform. A feature-based approach to the motion estimation problem is adopted, based on four primary steps: (1) extraction of <b>candidate</b> <b>features</b> from each image independently; (2) establishment of stereo (spatial) correspondence of <b>candidate</b> <b>features</b> within each stereo pair; (3) establishment of temporal (motion) correspondence of <b>candidate</b> <b>features</b> between adjacent stereo pairs; and (4) determination of the orientation parameters of the subsequent stereo pairs relative to the first from the corresponding (conjugate) features. A high [...] ...|$|R
40|$|The Relevance Vector Machine (RVM) is a {{recently}} developed machine learning framework capable of building simple models from large sets of <b>candidate</b> <b>features.</b> Here, we describe a protocol {{for using the}} RVM to explore very large numbers of <b>candidate</b> <b>features,</b> and a family of models which apply {{the power of the}} RVM to classifying and detecting interesting points and regions in biological sequence data. The models described here have been used successfully for predicting transcription start sites and other features in genome sequences. 2...|$|R
40|$|The maximum entropy {{framework}} {{has proved}} to be expressive and powerful for statistical language modelling, but it suffers from the computational expensiveness of model building. The iterative scaling algorithm that is used for parameter estimation is rather slow while the feature selection process might require parameters for many <b>candidate</b> <b>features</b> to be estimated many times. In this paper we present a novel approach for building maximum entropy models. Our approach uses a feature collocation lattice as a feature generation engine and selects <b>candidate</b> <b>features</b> without resorting to iterative scaling but instead through our own frequency redistribution algorithm. After the <b>candidate</b> <b>features</b> have been selected we use iterative scaling to estimate a fully saturated model for the maximal (factorial) feature space and then start to relax (eliminate) the most specific features. During constraint relaxation we always have a fully fit maximum entropy model, so we rank the constraints on th [...] ...|$|R
3000|$|... {{is larger}} than 0.5 in any dimension, then the {{extremum}} should be closer to another <b>candidate</b> <b>feature</b> point. If so, the interpolation is again performed around a different point. Otherwise the offset {{is added to the}} candidate point to produce the interpolated estimate of the extremum.|$|E
3000|$|... where ρ {{represents}} generalized {{correlation coefficient}} between different feature types, G̅^(i) the i-th selected feature type, and G(j) denotes a certain feature type from the <b>candidate</b> <b>feature</b> set, F^G-F^G^'_v- 1. Generalized correlation coefficient is degraded to Pearson’s correlation coefficient when the dimensionality of G̅^̅(̅i̅)̅ and G(j) is 1.|$|E
30|$|Filter {{approach}} [9] is used {{to select}} a subset of features from high dimensional datasets without using a learning algorithm. Filter-based feature selection methods are typically faster but the classifier accuracy is not ensured. Whereas, wrapper approach [4] uses a learning algorithm to evaluate the accuracy of a selected subset of features during classification. Wrapper methods can give high classification accuracy than filter method for particular classifiers but they are less cost effective. Embedded approach [9] performs feature selection {{during the process of}} training and is specific to the applied learning algorithms. Hybrid approach [14] is basically a combination of both filter and wrapper-based methods. Here, a filter approach selects a <b>candidate</b> <b>feature</b> set from the original feature set and the <b>candidate</b> <b>feature</b> set is refined by the wrapper approach. It exploits the advantages of both these approaches.|$|E
40|$|Maximum entropy {{framework}} {{proved to}} be expressive and powerful for the statistical lan- guage modelling, but it suffers from the computational expensiveness of the model building. The iterative scaling algorithm that {{is used for the}} paxameter estimation is computationally expensive while the feature selection process might require to estimate paxameters for many <b>candidate</b> <b>features</b> many times. In this paper we present a novel approach for building maximum entropy models. Our approach uses the feature collocation lattice and builds com- plex <b>candidate</b> <b>features</b> without resorting to it- erative scaling...|$|R
40|$|The scalp {{electroencephalography}} (EEG) {{signal is}} an important clinical tool for the diagnosis of several brain disorders. The objective of the presented work is to analyze the feasibility of the spectral features extracted from the scalp EEG signals in detecting brain tumors. A set of 16 <b>candidate</b> <b>features</b> from frequency domain is considered. The significance on the mean values of these features between 100 brain tumor patients and 102 normal subjects is statistically evaluated. Nine of the <b>candidate</b> <b>features</b> significantly discriminate the brain tumor case from the normal one. The results {{encourage the use of}} (quantitative) scalp EEG for the diagnosis of brain tumor...|$|R
3000|$|... where Δref is the {{disparity}} map {{from the original}} stereoscopic image, and Δdis is {{the disparity}} map from the distorted stereoscopic image. Here, QA denotes a QA function that uses one of the <b>candidate</b> <b>features,</b> {{as described in the}} Section 3.4.|$|R
40|$|Feature {{selection}} {{is a key}} problem to pattern recognition. So far, most methods of feature selection focus on sample data where class information is available. For sample data without class labels, however, the related methods for fea-ture selection are few. This paper proposes {{a new way of}} unsupervised feature selection. Our method is a hybrid ap-proach based on ranking the features according to their rel-evance to clustering using a new ranking index which be-longs to exponential entropy. Firstly a <b>candidate</b> <b>feature</b> subset is selected using a modified Fuzzy Feature Evalua-tion Index (FFEI) with a new method to calculate the fea-ture weight, which makes the algorithm to be robust and independent of domain knowledge. Then a wrapper method is used to select compact feature subset from the <b>candidate</b> <b>feature</b> set based on the clustering performance. Experi-mental results on benchmark data sets indicate the effec-tiveness of the proposed method. 1...|$|E
40|$|To {{solve the}} poor {{real-time}} performance {{problem of the}} visual odometry based on embedded system with limited computing resources, an image matching method based on Harris and SIFT is proposed, namely the Harris-SIFT algorithm. On {{the basis of the}} review of SIFT algorithm, the principle of Harris-SIFT algorithm is provided. First, Harris algorithm is used to extract the corners of the image as <b>candidate</b> <b>feature</b> points, and scale invariant feature transform (SIFT) features are extracted from those <b>candidate</b> <b>feature</b> points. At last, through an example, the algorithm is simulated by Matlab, then the complexity and other performance of the algorithm are analyzed. The experimental results show that the proposed method reduces the computational complexity and improves the speed of feature extraction. Harris-SIFT algorithm can be used in the real-time vision odometer system, and will bring about a wide application of visual odometry in embedded navigation system...|$|E
40|$|Abstract: Feature points {{can be used}} {{to match}} images. <b>Candidate</b> <b>feature</b> points are {{extracted}} through SIFT firstly. Then feature points are selected from candidate points through singular value decomposing. Distance between feature points sets is computed According to theory of invariability of feature points set, images are matched if the distance is less than a threshold. Experiment showed that this algorithm is available...|$|E
40|$|Abstract. To learn {{a visual}} code in an {{unsupervised}} manner, one may attempt to capture those {{features of the}} stimulus set that would contribute significantly to a statistically efficient representation (as dictated, e. g., by the Minimum Description Length principle). Paradoxically, all the <b>candidate</b> <b>features</b> in this approach need to be known before statistics over them can be computed. This paradox may be circumvented by confining the repertoire of <b>candidate</b> <b>features</b> to actual scene fragments, which resemble the “what+where ” receptive fields found in the ventral visual stream in primates. We describe a single-layer network that learns such fragments from unsegmented raw images of structured objects. The learning method combines fast imprinting in the feedforward stream with lateral interactions to achieve single-epoch unsupervised acquisition of spatially localized features that can support systematic treatment of structured objects [1]. 1 A paradox and some ways of resolving i...|$|R
30|$|In the {{learning}} state, users should first calculate <b>candidate</b> <b>features</b> {{of the number}} of links and weights obtained via vertical visibility by using possible statistical mechanics and measurements and then consider selecting only the effective features based on the selected set of training signals. The selected statistical mechanics and measurements are then used to find final features in the testing state.|$|R
40|$|Abstract. In many {{knowledge}} discovery applications {{the data}} mining step {{is followed by}} further data acquisition. New data may consist of new instances and/or new features for the old instances. When new features are to be added an acquisition policy can help decide what features have to be acquired based on their predictive capability {{and the cost of}} acquisition. This can be posed as a feature selection problem where the feature values are not known in advance. We propose a technique to actively sample the feature values with the ultimate goal of choosing between alternative <b>candidate</b> <b>features</b> with minimum sampling cost. Our algorithm is based on extracting <b>candidate</b> <b>features</b> in a “region ” of the instance space where the feature value is likely to alter our knowledge the most. An experimental evaluation on a standard database shows that it is possible outperform a random subsampling policy in terms of the accuracy in feature selection. ...|$|R
40|$|Software cost {{estimation}} {{is one of}} the most crucial processes in software development management because it involves many management activities such as project planning, resource  allocation and risk assessment. Accurate software {{cost estimation}} not only does help to make investment and bid plan but also enable the project to be completed in the limited cost and time. The research interest of this master thesis will focus on feature selection method and case selection method and the goal is to improve the accuracy of software cost estimation model. Case based reasoning in software cost estimation is an immediate area of research focus. It can predict the cost of new software project via constructing estimation model using historical software projects. In order to construct estimation model, case based reasoning in software cost estimation needs to pick out relatively independent candidate features which are relevant to the estimated feature. However, many sequential search feature selection methods used currently are not able to obtain the redundancy value of candidate features precisely. Besides, when using local distance of candidate features to calculate the global distance of two software projects in case selection, the different impact of each <b>candidate</b> <b>feature</b> is unproven. To solve these two problems, this thesis explores the solutions with the help from NSFC. In this thesis, a feature selection algorithm based on hierarchical clustering is proposed. It gathers similar candidate features into the same clustering and selects one feature that is most similar to the estimated feature as the representative feature. These representative features form the <b>candidate</b> <b>feature</b> subsets. Evaluation metrics are applied to these <b>candidate</b> <b>feature</b> subsets and the one that can produce best performance will be marked as the final result of feature selection. The experiment result shows that the proposed algorithm improves 12. 6 % and 3. 75 % in PRED (0. 25) over other sequential search feature selection methods on ISBSG dataset and Desharnais dataset, respectively. Meanwhile, this thesis defines <b>candidate</b> <b>feature</b> weight using symmetric uncertainty which origins from information theory. The feature weight is capable of reflecting the impact of each feature with the estimated feature. The experiment result demonstrates  that by applying feature weight, the performance of estimation model improves 8. 9 % than that without feature weight in PRED (0. 25) value. This thesis discusses and analyzes the drawback of proposed ideas as well as mentions some improvement directions...|$|E
30|$|Therefore, {{deriving}} {{the optimal}} chroma-like channel (i.e., optimal [α∗,β∗,−α∗−β∗]) {{is equivalent to}} finding the largest hyperplane margin among <b>candidate</b> <b>feature</b> spaces (features extracted from chroma-like channels) which are mapped into higher spaces using gaussian kernel. Since {{there is no direct}} parameter optimization scheme solving (16) and (17), a numerical optimization method is employed to obtain the optimal θ defined as θ=[α,β] for marking convenience, which consists of two steps: initial parameter selection and local optimum searching.|$|E
40|$|Freehand {{sketching}} is a {{very efficient}} means for us to communicate each other. As Table PC is widely popularized, the research about sketch recognition became one of important research issue. To recognize sketch, the feature point should be extracted and then each feature point is analyzed as line or curve. However, most of feature extraction algorithms suffers from noise which is occurred from the bad drawing sketch. In this paper, we propose the feature extraction algorithm robust to noise. The proposed algorithm consists of three cascade steps: <b>candidate</b> <b>feature</b> point extraction, noise reduction, and hook elimination. At the <b>candidate</b> <b>feature</b> point extraction step, the feature points is selected among input points. Then, in second step, we reduce the noise which is occurred from the previous step by using noise reduction rule based on inner product between two neighbor vectors. Finally, the hook, {{which can not be}} eliminated from two previous steps, is eliminated by the proposed hook elimination method. The experimental result shows that the average approximation error is less than 1 about 1004 line-curve hybrid shapes, and the proposed algorithm is the good feature methods. 1...|$|E
3000|$|... [15]. A weight {{value is}} {{computed}} for each feature, such that those features with better weight values are selected {{to represent the}} original data set. On the other hand, wrapper approaches generate a set of <b>candidate</b> <b>features</b> by adding and removing features to compose a subset of features. Then, they employ accuracy to evaluate the resulting feature set. Wrapper methods usually achieve superior results than filter methods.|$|R
25|$|However, {{all three}} <b>candidates</b> were <b>featured</b> in two {{additional}} radio-only privately sponsored debates {{shortly before the}} primary.|$|R
40|$|We {{present a}} reranking {{approach}} to sentence-like unit (SU) boundary detection, {{one of the}} EARS metadata extraction tasks. Techniques for generating relatively small n-best lists with high oracle accuracy are presented. For each <b>candidate,</b> <b>features</b> are derived {{from a range of}} information sources, including the output of a number of parsers. Our approach yields significant improvements over the best performing system from the NIST RT- 04 F community evaluation 1. 1...|$|R
3000|$|... where F′ {{represents}} m-[*] 1 -dimensional feature subset {{that has}} been already selected from F. Equation (4) aims to selecting the m-th from the <b>candidate</b> <b>feature</b> subset F−F′ and implements trade-off between high class relevance and low feature redundancy. As shown in Fig.  1 b, the features selected from the same feature type are scattering in terms of ranking, which affects the quantitative evaluation of multiple features, resulting in the lack of interpretability. In addition, the selected results may greatly change due to data fluctuation.|$|E
40|$|Abstract: A feature {{selection}} algorithm {{based on the}} optimal hyperplane of SVM is raised. Using the algorithm, the contribution to the classification of each feature in the <b>candidate</b> <b>feature</b> set is test, and then the feature subset with best classification ability will be selected. The algorithm {{is used in the}} recognition process of storm monomers in weather forecast, and experimental data show that the classification ability of the features can be effectively evaluated; the optimal feature subset is selected to enhance the working performance of the classifier...|$|E
40|$|Best Paper AwardInternational audienceHyperspectral {{images have}} a strong {{potential}} for landcover/landuse classification, since the spectra of the pixels can highlight subtle differences between materials and provide information beyond the visible spectrum. Yet, a limitation of most current approaches is the hypothesis of spatial independence between samples: images are spatially correlated and the classification map should exhibit spatial regularity. One way of integrating spatial smoothness is to augment the input spectral space with filtered versions of the bands. However, open questions remain, such as {{the selection of the}} bands to be filtered, or the filterbank to be used. In this paper, we consider the entirety of the possible spatial filters by using an incremental feature learning strategy that assesses whether a <b>candidate</b> <b>feature</b> would improve the model if added to the current input space. Our approach is based on a multiclass logistic classifier with group-lasso regularization. The optimization of this classifier yields an optimality condition, that can easily be used to assess the interest of a <b>candidate</b> <b>feature</b> without retraining the model, thus allowing drastic savings in computational time. We apply the proposed method to three challenging hyperspectral classification scenarios, including agricultural and urban data, and study both the ability of the incremental setting to learn features that always improve the model {{and the nature of the}} features selected...|$|E
40|$|Feature {{selection}} researchers often {{encounter a}} peaking phenomenon: a feature subset {{can be found}} that is smaller but still enables building a more accurate classifier than the full set of all the <b>candidate</b> <b>features.</b> However, the present study shows that this peak may often be just an artifact due to the still too common mistake in pattern recognition [...] - that of not using an independent test set...|$|R
40|$|This paper {{describes}} {{a new approach}} for the selection of discriminant time-frequency features for classification. Unlike previous approaches that use the individual discrimination power of expansion coefficients, the proposed approach selects a subset of features by implementing a classifier directed pruning of an initial redundant set of <b>candidate</b> <b>features.</b> The <b>candidate</b> <b>features</b> are calculated from a structured redundant time-frequency analysis of the signal, such as an undecimated wavelet transform. We show that the proposed approach has a performance that {{is as good as}} or better than traditional classification approaches while using a much smaller number of features. In particular, we provide experimental results to demonstrate the superior performance of the algorithm in the area of impact acoustic classification for food kernel inspection. The proposed algorithm achieved 91. 8 % and 98. 5 % classification accuracies in separating open shell from closed shell pistachio nuts and discriminating between empty and full hazelnuts respectively. Traditional methods used in this area resulted in 82 % and 97 % classification accuracies respectively. © 2007 EURASIP...|$|R
40|$|Structure {{learning}} of Conditional Random Fields (CRFs) can be cast into an L 1 -regularized optimization problem. To avoid optimizing over a fully linked model, gain-based or gradient-based feature selection methods start from an empty model and incrementally add top ranked features to it. However, for high-dimensional problems like statistical relational learning, training time of these incremental methods can {{be dominated by}} the cost of evaluating the gain or gradient of a large collection of <b>candidate</b> <b>features.</b> In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which only evaluates a subset of features that involve both variables with high signals (deviation from mean) and variables with high errors (residue). We prove that the gradient of <b>candidate</b> <b>features</b> can be represented solely {{as a function of}} signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real data sets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting...|$|R
40|$|A new {{unsupervised}} forward orthogonal search (FOS) {{algorithm is}} introduced for feature selection and ranking. In the new algorithm, features are selected in a stepwise way, {{one at a}} time, by estimating the capability of each specified <b>candidate</b> <b>feature</b> subset to represent the overall features in the measurement space. A squared correlation function is employed as the criterion to measure the dependency between features and this makes the new algorithm easy to implement. The forward orthogonalization strategy, which combines good effectiveness with high efficiency, enables the new algorithm to produce efficient feature subsets with a clear physical interpretation...|$|E
40|$|Identifying {{mammalian}} vigilance {{states has}} recently {{become an important}} topic in biological science research. The biological researchers concern not only to improve the accuracy rate for classifying the vigilance states, but also to extract the meaningful frequency bands. In this study, we propose a novel feature selection to extract the critical frequency bands of rat’s EEG signals. The proposed algorithm adopts the concept of neighborhood relation during adding and eliminating a <b>candidate</b> <b>feature.</b> In the experiments, the proposed method shows better accuracy rate, and find out the feature subset which locate on the critical frequency bands for recognizing rat’s vigilance states...|$|E
40|$|The {{detection}} of camouflaged objects {{is important for}} industrial inspection, medical diagnoses, and military applications. Conventional supervised learning methods for hyperspectral images can be a feasible solution. Such approaches, however, require a priori information of a camouflaged object and background. This letter proposes a fully autonomous feature selection and camouflaged object detection method based on the online analysis of spectral and spatial features. The statistical distance metric can generate <b>candidate</b> <b>feature</b> bands and further analysis of the entropy-based spatial grouping property can trim the useless feature bands. Camouflaged objects can be detected better with less computational complexity by optical spectral-spatial feature analysis...|$|E
40|$|A novel {{approach}} to oil-spill classification, {{based on the}} paradigm of one-class classification, is proposed. Basically, a classifier is trained using only examples of oil-spills, instead of using oil-spills and look-alikes, as in two-class approaches. In addition, as {{a large number of}} <b>candidate</b> <b>features</b> have been considered in the literature, a feature selection algorithm, to objectively select the most effective subset, is proposed. Results on two case study datasets are reported to validate the proposed approach...|$|R
40|$|Nowadays, {{semantic}} wikis {{are used}} in software development. In requirements en-gineering process, semantic wikis are used as lightweight and semantic/social-web based collaboration platforms. This paper first makes a survey on existing seman-tic wikis and their <b>candidate</b> <b>features,</b> which can be interesting in requirements engineering. Secondly, specific semantic wikis for requirements engineering are analyzed/compared based on the features identified in the first step. We conclude this paper with promising features which are provided by semantic wikis, and ca...|$|R
5000|$|Despite the show's {{primary focus}} and title, Crime Watch Daily is {{designated}} as a news program by the Federal Communications Commission (FCC) {{by way of a}} ruling made by the agency on August 11, 2015, through a declaratory ruling sought by GHN Productions; because of the ruling, the program therefore is exempt from FCC requirements that would have obligated it to provide airtime to political <b>candidates</b> <b>featured</b> in coverage of trials, criminal cases and other [...] "crime-related" [...] matters.|$|R
