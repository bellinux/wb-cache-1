52|336|Public
5000|$|Kott, P. (2006). Using {{calibration}} weighting {{to adjust}} for nonresponse and <b>coverage</b> <b>errors.</b> Survey Methodology, 133-142 ...|$|E
5000|$|<b>Coverage</b> <b>errors,</b> such as {{failure to}} {{accurately}} represent all population {{units in the}} sample, or the inability to obtain information about all sample cases; ...|$|E
50|$|Information on {{citizenship}} {{should be}} used with caution as it is subject to content and <b>coverage</b> <b>errors</b> especially for non-citizens as in censuses in most countries.|$|E
5000|$|Reimbursement {{for certain}} out-of-pocket {{expenses}} paid {{as a result}} of a <b>coverage</b> <b>error,</b> ...|$|R
40|$|Conventional bootstrap-"t" {{intervals}} for density functions {{based on}} kernel density estimators exhibit poor coverages due to {{failure of the}} bootstrap to estimate the bias correctly. The problem can be resolved by either estimating the bias explicitly or undersmoothing the kernel density estimate to undermine its bias asymptotically. The resulting bias-corrected intervals have an optimal <b>coverage</b> <b>error</b> of order arbitrarily close to second order for a sufficiently smooth density function. We investigated the effects on <b>coverage</b> <b>error</b> of both bias-corrected intervals when the nominal coverage level is calibrated by the iterated bootstrap. In either case, an asymptotic reduction of <b>coverage</b> <b>error</b> is possible provided that the bias terms are handled using an extra round of smoothed bootstrapping. Under appropriate smoothness conditions, the optimal <b>coverage</b> <b>error</b> of the iterated bootstrap-"t" intervals has order arbitrarily close to third order. Examples of both simulated and real data are reported to illustrate the iterated bootstrap procedures. Copyright (c) Board of the Foundation of the Scandinavian Journal of Statistics 2007. ...|$|R
50|$|<b>Coverage</b> <b>error</b> {{is a kind}} of nonsampling error. Further {{discussion}} and elaboration can be found in Salant and Dillman (1995).|$|R
5000|$|The {{data in the}} decennial {{publication}} represent final Malaysian Census {{population figures}} for respective decades. As in censuses in most other countries, the information obtained during enumeration is subject to coverage and content errors. In term of <b>coverage</b> <b>errors,</b> part of living quarters, households or population maybe left out, erronerously included or duplicated. Content errors in particular were based on erronerous responses on gender, age, ethnic group, citizenship, religion and marital status. As a result, the figures were [...] "adjusted" [...] based on the estimates of under-enumeration derived from the Census Coverage Evaluation Survey.|$|E
40|$|Communicated by Wei-Yin Loh) Abstract. Confidence {{interval}} construction for {{parameters of}} lattice distributions is considered. By using saddlepoint formulas and bootstrap calibration, we obtain relatively short intervals and bounds with O(n − 3 / 2) <b>coverage</b> <b>errors,</b> {{in contrast with}} O(n − 1) andO(n − 1 / 2) <b>coverage</b> <b>errors</b> for normal theory intervals and bounds when the population distribution is absolutely continuous. Closed form solutions are also provided for the cases of binomial and Poisson distributions. The method is illustrated by some simulation results. 1...|$|E
40|$|Nonparametric {{versions}} of Wilks' theorem are proved for empirical likelihood estimators of slope and mean parameters {{for a simple}} linear regression model. They enable us to construct empirical likelihood confidence intervals for these parameters. The <b>coverage</b> <b>errors</b> of these confidence intervals are of order n- 1 and {{can be reduced to}} order n- 2 by Bartlett correction. ...|$|E
40|$|Nonparametric methods play {{a central}} role in modern {{empirical}} work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing <b>coverage</b> <b>error</b> and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive <b>coverage</b> <b>error</b> optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the MSE-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest <b>coverage</b> <b>error</b> decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too "large". Finally, for odd-degree local polynomial regression, we show that, as with point estimation, <b>coverage</b> <b>error</b> adapts to boundary points automatically when appropriate Studentization is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard <b>errors,</b> have smaller <b>coverage</b> <b>error</b> and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available...|$|R
40|$|A new {{algorithm}} for optimization {{problems with}} three objective functions is presented which computes a representation for {{the set of}} nondominated points. This representation is guaranteed to have a desired <b>coverage</b> <b>error</b> and a bound {{on the number of}} iterations needed by the algorithm to meet this <b>coverage</b> <b>error</b> is derived. Since the representation does not necessarily contain nondominated points only, ideas to calculate bounds for the representation error are given. Moreover, the incorporation of domination during the algorithm and other quality measures are discussed...|$|R
40|$|A {{letter report}} {{issued by the}} Government Accountability Office with an {{abstract}} that begins "Evaluations of past censuses show that certain groups were undercounted compared to other groups, a problem known as "coverage error. " To address this, the Census Bureau included in its 2000 Census design the Accuracy and Coverage Evaluation Program (A. C. E.) to (1) measure <b>coverage</b> <b>error</b> and (2) use the results to adjust the census, if warranted. However, the Bureau found the A. C. E. results inaccurate and decided not to adjust or plan for adjustment in 2010. Congress asked GAO to determine (1) factors contributing to A. C. E. 's reported failure to accurately estimate census <b>coverage</b> <b>error,</b> and (2) {{the reliability of the}} revised <b>coverage</b> <b>error</b> estimates the Bureau subsequently produced. To do this, GAO examined three sets of Bureau research published in March 2001, October 2001, and March 2003 and interviewed Bureau officials. ...|$|R
40|$|Basically, the PFS is an {{independent}} survey that repeats frame update. The survey results are compared with house units frame results, permitting estimates {{to be made of}} : coverage and content errors. <b>Coverage</b> <b>errors</b> refer to house units missed in the census or incorrectly included, while content errors evaluate response quality of selected questions. The PFS allows SESRI to uncover deficiencies in the methodology of the frame update and make adjustments for future censuses...|$|E
40|$|Weighting {{procedures}} are commonly applied in surveys {{to compensate for}} nonsampling errors such as nonresponse errors and <b>coverage</b> <b>errors.</b> Two types of weight-adjustment {{procedures are}} commonly used {{in the context of}} unit nonresponse: (i) nonresponse propensity weighting followed by calibration, also known as the two-step approach and (ii) nonresponse calibration weighting, also known as the one-step approach. In this article, we discuss both approaches and warn against the potential pitfalls of the one-step procedure. Results from a simulation study, evaluating the properties of several point estimators, are presented...|$|E
40|$|At present we {{typically}} assess quality {{by relying}} heavily on summary process measures or macro paradata {{that is a}} by-product of sample selection and survey administration (e. g., item and unit nonresponse rates, response variances, sampling and <b>coverage</b> <b>errors).</b> These measures are {{an outgrowth of the}} randomization-based approach to survey sampling that triumphed in government agencies around the world in the two decades after the seminal 1934 paper of Neyman. When most of our present measures were developed, therefore, nonsampling error problems, like nonresponse, were less serious or less well understood...|$|E
40|$|Surveys on voting {{behavior}} typically overestimate turnout rates substantially. To disentangle {{different sources}} of bias - <b>coverage</b> <b>error,</b> nonresponse bias, and overreporting - {{we conducted a}} validation study in which respondents' self-reported voting behavior was compared to administrative voting records (N = 2000). Our results show that all three sources of error inflate the survey estimate of the turnout rate and also bias estimates from political participation models, although <b>coverage</b> <b>error</b> is only moderate compared to the more pronounced biases due to nonresponse and overreporting. Furthermore, results from a wording experiment do not provide evidence that revised wording reduces measurement bias...|$|R
40|$|This article {{develops}} {{confidence interval}} procedures for functions of simple, partial, and squared multiple correlation coefficients. It {{is assumed that}} the observed multivariate data represent a random sample from a distribution that possesses infinite moments, {{but there is no}} requirement that the distribution be normal. The <b>coverage</b> <b>error</b> of conventional one-sided large sample intervals decreases at rate 1 √n as n increases, where n is an index of sample size. The <b>coverage</b> <b>error</b> of the proposed intervals decreases at rate 1 /n as n increases. The results of a simulation study that evaluates the performance of the proposed intervals is reported and the intervals are illustrated on a real data set...|$|R
50|$|<b>Coverage</b> <b>error</b> is {{an error}} {{that occurs in}} {{statistical}} estimates of a survey. It results from gaps between the sampling frame and the total population. This can lead to biased results and can affect the variance of results.|$|R
40|$|List-based {{samples are}} often biased because of <b>coverage</b> <b>errors.</b> The problem is {{especially}} acute in societies where {{the level of}} internal migration is high andwhere record keeping on the population is not reliable. Weproposea solutionbasedon spatial sampling that overcomes the inability to reach migrants in traditional area samples based on household lists. A comparison between a traditional study and our sample of Beijing demonstrates that coverage bias is greatly reduced. The successful incorporation of mobile urban residents has important substantive effects, in both univariate and multivariate analyses of public opinion data. ...|$|E
40|$|AbstractThis paper {{proposes a}} {{constrained}} empirical likelihood confidence region for a parameter β 0 in the linear errors-in-variables model: Yi=xiτβ 0 +εi,Xi=xi+ui,(1 ⩽i⩽n), which is constructed {{by combining the}} score function corresponding to the squared orthogonal distance with a constrained region of β 0. It is shown that the coverage error of the confidence region is of order n− 1, and Bartlett corrections can reduce the <b>coverage</b> <b>errors</b> to n− 2. An empirical Bartlett correction is given for practical implementation. Simulations show that the proposed confidence region has satisfactory coverage not only for large samples, but also for small to medium samples...|$|E
40|$|Among {{the civil}} service {{retirement}} issues addressed in bills introduced thus far in the 106 th Congress are the correction of retirement <b>coverage</b> <b>errors</b> for federal employees assigned to the wrong retirement system; immediate eligibility for federal employees {{to participate in the}} Thrift Savings Plan (TSP); improved portability of pension benefits; and repeal of the temporary increase in employee retirement contributions that was mandated by the Balanced Budget Act of 1997. Other bills would expand TSP eligibility to include members of the armed services; improve pension coverage for temporary and part-time federal employees; and designate several categories of federal employees as law enforcement officers for purposes of determining their retirement benefits...|$|E
40|$|It {{is shown}} that the {{bootstrap}} percentile method of constructing confidence intervals for quantiles {{is equivalent to the}} sign test method. As a result, and in contrast to many other problems, percentile-method intervals have <b>coverage</b> <b>error</b> of precise order n- 1 / 2 in both one- and two-sided cases. We show that bootstrap iteration of percentile-method intervals has no role to play in quantile problems, and cannot be used to improve coverage accuracy. This is in marked distinction to more classical problems, where each iteration reduces the order of <b>coverage</b> <b>error</b> by the factor n- 1 / 2 for one-sided intervals and n- 1 for two-sided intervals. It thus emerges that standard bootstrap techniques perform poorly in constructing confidence intervals for quantiles: the percentile method is inaccurate for given levels, and produces nothing new; iteration of the percentile method fails completely; bias correction and accelerated bias correction are no more than adjustments to sign test intervals, and therefore fail; and percentile-t is hardly an efficacious alternative because of the non-availability of a suitable variance estimate. bootstrap bootstrap iteration <b>coverage</b> <b>error</b> Edgeworth expansion percentile method quantile sign test...|$|R
40|$|In {{traditional}} bootstrap applications {{the size}} of a bootstrap sample equals the parent sample size, n say. Recent studies have shown that using a bootstrap sample size different from n may sometimes provide a more satisfactory solution. In this paper we apply the latter approach to correct for <b>coverage</b> <b>error</b> in construction of bootstrap confidence bounds. We show that the <b>coverage</b> <b>error</b> of a bootstrap percentile method confidence bound, which is of order O(n- 1 / 2) typically, can be reduced to O(n- 1) by use of an optimal bootstrap sample size. A simulation study is conducted to illustrate our findings, which also suggest that the new method yields intervals of shorter length and greater stability compared to competitors of similar coverage accuracy. link_to_subscribed_fulltex...|$|R
40|$|This paper {{proposes a}} {{bootstrap-based}} procedure for selecting the best moment conditions among {{a set of}} correctly specified moment conditions. The proposed method can be motivated by Edgeworth expansions and chooses moment conditions that minimize the approximate <b>coverage</b> <b>error</b> of confidence intervals and hypothesis tests for parameters. Because of the analytical intractability of Edgeworth expansions, we estimate the <b>coverage</b> <b>error</b> by the bootstrap. The proposed method {{can be applied to}} a wide class of estimators for possibly non-linear and possibly dynamic models. We investigate the small sample performance of the proposed method in Monte Carlo experiments. Copyright Royal Economic Society 2006 Published by Blackwell Publishing Ltd, 9600 Garsington Road, Oxford OX 4 2 DQ, UK and 350 Main Street, Malden, MA, 02148, USA. ...|$|R
40|$|The methods {{underpinning}} the UK's annual structural earnings survey-the New Earnings Survey-have {{remained largely}} unchanged since the survey's inception in 1970. Gradual {{changes in the}} labour market over recent years have led to <b>coverage</b> <b>errors</b> in the survey; non-response may also introduce error if it is non-random. The paper describes some of the New Earnings Survey's main non-sampling errors, the effect on survey results and the research that was undertaken by the Office for National Statistics {{as part of the}} development of the new Annual Survey of Hours and Earnings to be able to remove or otherwise to account for these errors. Copyright 2007 Royal Statistical Society. ...|$|E
40|$|This paper {{proposes a}} {{constrained}} empirical likelihood confidence region for a parameter [beta] 0 in the linear errors-in-variables model: Yi=xi[tau][beta] 0 +[var epsilon]i,Xi=xi+ui,(1 [less-than-or-equals, slant]i[less-than-or-equals, slant]n), which is constructed {{by combining the}} score function corresponding to the squared orthogonal distance with a constrained region of [beta] 0. It is shown that the coverage error of the confidence region is of order n- 1, and Bartlett corrections can reduce the <b>coverage</b> <b>errors</b> to n- 2. An empirical Bartlett correction is given for practical implementation. Simulations show that the proposed confidence region has satisfactory coverage not only for large samples, but also for small to medium samples. Bartlett correction Confidence region Coverage error Empirical likelihood Errors-in-variables Linear regression...|$|E
40|$|This study {{illustrates}} how the Total Survey Error (TSE) paradigm can identify and help reduce multiple sources of error inherent in survey {{work in the}} developing world. Of particular concern are mode errors and <b>coverage</b> <b>errors</b> caused by the 2 ̆ 7 theoretical teledensity threshold 2 ̆ 7 of doing phone surveys in developing countries. The study outlines ways to improve response rate and to avoid interviewer and measurement error. It narrates the sampling design and its limitations {{as well as some}} of the qualitative aspects of total survey quality such as, translation, ethics and budgeting. The final section discusses implications for further research in statistical auto-correlation and data gathering using PDAs. <br /...|$|E
40|$|We {{consider}} {{construction of}} two-sided nonparametric confidence intervals in a smooth function model setting. A nonparametric likelihood approach based on Stein's least favourable family is proposed {{as an alternative}} to empirical likelihood. The approach enjoys the same asymptotic properties as empirical likelihood, but is analytically and computationally less cumbersome. The simplicity of the method allows us to propose and analyse asymptotic and bootstrapping techniques as a means of reducing <b>coverage</b> <b>error</b> to levels comparable with those obtained by more computationally-intensive techniques such as the iterated bootstrap. A simulation study confirms that <b>coverage</b> <b>error</b> may be substantially reduced by simple analytic adjustment of the nonparametric likelihood interval and that bootstrapping the distribution of the nonparametric likelihood ratio results in very desirable coverage accuracy. © 1999 Biometrika Trust. link_to_subscribed_fulltex...|$|R
40|$|This paper {{identifies}} {{and describes}} the primary sources of survey error in mail surveys and some methods typically {{used to assess}} and quantify some of these sources of error. Four sources of survey error- sampling <b>error,</b> non <b>coverage</b> <b>error,</b> non-response error and measurement error were discussed. Descriptions of how each of these sources of error occurs in mail surveys are provided. The paper summarizes ways of reducing these errors, if they cannot be totally avoided...|$|R
40|$|ISBN: 4930813670 This paper {{addresses}} {{the detection of}} permanent and transient faults in complex VLSI circuits, with a particular focus on faults leading to sequencing errors. Several Finite State Machine implementations using signature monitoring for control-flow checking are compared in terms of error detection latency, theoretical <b>error</b> <b>coverage,</b> experimental <b>error</b> <b>coverage</b> and area overheads. Advantages and drawbacks of each approach are presented...|$|R
3000|$|The {{aircraft}} {{track is}} located mostly between 7 and 9  km in height, {{ranging from the}} 29 th to 32 nd model levels. Since {{the space between the}} model levels is sparse (~ 1  km), to avoid the uncertainty introduced by the vertical interpolation, we compare the UND-Citation data with the model values at the nearest model vertical level. Meanwhile, compared with NEXRAD observations, the WRF simulations of the convection system have both position and <b>coverage</b> <b>errors</b> (Figs.  1, 2). Therefore, it is difficult to directly compare the UND-Citation observations with the simulations. Thus, a method of comparison is designed based on the classification of the convective system using 3 D distribution of radar reflectivity. The details of the comparison procedure are as follows: [...]...|$|E
40|$|U. S. Census Bureau’s {{previous}} coverage measurement {{surveys were}} designed primarily to estimate net Census <b>Coverage</b> <b>Errors</b> (CCE) using Dual System Estimation Model (DSEM). National research council (2009) gave a recommendation on developing plans {{for measuring the}} components of CCE in U. S. 2010 census coverage measurement using a framework provided by Mulry and Kostanich (2006). Firstly, this paper analysed the lacks between DSEM in theory and application. One assumptions {{was added to the}} model. Secondly, expanded DSEM, to measure a general census coverage error in China, was constructed based on ideas of the framework and the added assumptions. At last, matching properties were proved and estimation approaches to measure the components of CCE were also discussed in this presentation...|$|E
40|$|Mascon (mass concentration) {{solutions}} {{computed for}} entire land area of Earth with several variants from Jul. 2003 through Dec. 2005 Automated scripts developed, "pipeline" now in place. Solutions generally consistent with harmonics for large features but appear {{able to resolve}} and localize smaller features more cleanly. Greenland solutions generally consistent with areas of max ice mass loss in South, but mascons seem to clearly identify sub-regions of ice mass growth. May be amplified by mascon sensitivity and ground tracks. Irregular <b>coverage,</b> <b>errors</b> due to tides in Arctic or other leakage from nearby sources? Although mascons are technically 30 + years old, gravity/geodesy community has vastly more experience with harmonics and thus we are still learning the full advantages, limitations, and idiosyncrasies of mascons...|$|E
40|$|Beran & Hall's (1993) simple linear {{interpolation}} provides a very convenient approach for constructing nonparametric confidence intervals for population quantiles {{based on a}} random sample of size n. We show that the <b>coverage</b> <b>error</b> of the interpolated interval, which is of order O(n - 1), can be improved upon by calibrating the nominal coverage level. Three distinct methods of calibration are considered. The analytical and Monte Carlo methods succeed in reducing the order of <b>coverage</b> <b>error</b> to O(n - 3 / 2), while the smoothed bootstrap method reduces it further to O(n- 25 / 14). We provide guidelines for practical implementation of the calibration methods. Their performance is compared with the simple linear interpolated interval in a simulation study which confirms superiority of the calibrated intervals. © 2005 Biometrika Trust. link_to_subscribed_fulltex...|$|R
40|$|For {{estimation}} of a d-variate mean vector 6 {{based on a}} random sample of size n drawn from a distribution of a location family, a generalized Stein estimator T n,S may be defined which shrinks the sample mean towards a proper linear subspace double-struck L sign of ℝ d. In general, the conventional parametric bootstrap consistently estimates the limit distribution of n 1 / 2 (T n,S - θ) when θ ∉ double-struck L sign but fails to be consistent otherwise. We establish consistency of two modified forms of the parametric bootstrap for any θ ∈ ℝ d, which are therefore useful for statistical inference about 9. In the context of constructing confidence sets for θ, we show that the first approach, which is based on the m out of n bootstrap, yields <b>coverage</b> <b>error</b> of order O(n - 1 / 4) for all θ, provided that the bootstrap resample size m has an order determined by a minimax criterion. The second approach bootstraps from a distribution with an adaptively estimated mean vector, and is shown to yield <b>coverage</b> <b>error</b> of exponentially small order for θ ∈ double-struck L sign and of order O(n - 1) for θ ∉ double-struck L sign. Iterated versions of the two approaches are also developed to give improved orders of <b>coverage</b> <b>error.</b> A simulation study is reported to illustrate our asymptotic findings. published_or_final_versio...|$|R
40|$|This report {{addresses}} {{the problem of}} ascertaining the quantitative properties of dependable distributed systems. The approach is based on fault injection experimentation with two distinct real-time workloads using a fault-injection-based automated testing environment. Experimental results are presented which indicate dependability properties (e. g., <b>error</b> detection <b>coverage,</b> <b>error</b> latency) based on {{a limited number of}} fault injections...|$|R
