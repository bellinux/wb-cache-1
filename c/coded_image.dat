108|3331|Public
5000|$|... #Caption: Colour <b>coded</b> <b>image</b> of actin {{filaments}} in {{a cancer}} cell.|$|E
50|$|The line 23 {{data format}} allows {{signaling}} {{of the source}} (<b>coded</b> <b>image)</b> aspect ratio and the Active Format Descriptor.|$|E
5000|$|In most cases, the {{transmitter}} can't start sending a <b>coded</b> <b>image</b> until within a half-second of when it's to be decoded (vbv_delay {{less than or}} equal to 45000 90-kHz clock increments).|$|E
40|$|The {{objective}} {{of this paper is}} to perform the innovation design for improving the recognition of a captured QR <b>code</b> <b>image</b> with blur through the Pillbox filter analysis. QR <b>code</b> <b>images</b> can be captured by digital video cameras. Many factors contribute to QR code decoding failure, such as the low quality of the image. Focus is an important factor that affects the quality of the image. This study discusses the out-of-focus QR <b>code</b> <b>image</b> and aims to improve the recognition of the contents in the QR <b>code</b> <b>image.</b> Many studies have used the pillbox filter (circular averaging filter) method to simulate an out-of-focus image. This method is also used in this investigation to improve the recognition of a captured QR <b>code</b> <b>image.</b> A blurred QR <b>code</b> <b>image</b> is separated into nine levels. In the experiment, four different quantitative approaches are used to reconstruct and decode an out-of-focus QR <b>code</b> <b>image.</b> These nine reconstructed QR <b>code</b> <b>images</b> using methods are then compared. The final experimental results indicate improvements in identification...|$|R
40|$|This paper {{presents}} a cryptographic technique that encrypts secret information using a <b>coding</b> <b>image</b> by transforming the pixels {{of this image}} from the intensity domain to the characters domain using a hash function. In the proposed technique, the <b>coding</b> <b>image</b> {{will be used to}} encrypt the secret information at the sender and decrypt it at the receiver using the pixels whose intensity values are transformed to characters. A matrix of characters corresponding to the <b>coding</b> <b>image</b> is generated where each character in this matrix corresponds to a pixel in the <b>coding</b> <b>image</b> and each character in the secret information is mapped to a character in the matrix of characters. The locations of characters in the matrix of characters that correspond to pixels in the <b>coding</b> <b>image</b> and correspond to characters in the secret information forms the pixels map. The pixels map is encrypted using a secret key before being sent to the receiver on a secure communication channel different from that used to send the <b>coding</b> <b>image</b> and at different times. Upon receiving the <b>coding</b> <b>image</b> and the encrypted pixels map the receiver uses the secret key to decrypt the pixels map and uses the <b>coding</b> <b>image</b> and the hash function to generate the matrix of characters. Each location in the pixels map is used to retrieve a character from the matrix of characters in order to decrypt the secret information. Experimental results showed the effectiveness and the efficiency of the proposed algorithm where a message was encrypted using a <b>coding</b> <b>image</b> without modifying its pixels and it was decrypted without errors...|$|R
40|$|This paper {{introduces}} {{an extension}} of conditional entropy-constrained residual vector quantization (CEC-RVQ) to include quantization cell shape gain. The method {{is referred to as}} conditional entropy-constrained trellis-coded RVQ (CEC-TCRVQ). The new design is based on <b>coding</b> <b>image</b> vectors by taking into account their 2 D correlation and employing a higher order entropy model with a trellis structure. We employed CEC-TCRVQ to <b>code</b> <b>image</b> subbands at low bit rate. The CEC-TCRVQ <b>coded</b> <b>images</b> do well in term of preserving low-magnitude textures present in some image...|$|R
5000|$|A JPEG XR <b>coded</b> <b>image</b> can be {{segmented}} into tile regions. The {{data for}} each region can be decoded separately. This enables rapid access {{to parts of}} an image without needing to decode the entire image. When a type of tiling referred to as [...] "soft tiling" [...] is used, the tile region structuring can be changed without fully decoding the image and without introducing additional distortion.|$|E
40|$|This paper {{presents}} a no-reference image quality assessment model for JPEG/JPEG 2000 coding. This assessment model {{are based on}} the blockiness around the block boundary, the average absolute difference between adjacent pixels within block, and the zero-crossing rate within block. The descrim-ination of JPEG <b>coded</b> <b>image</b> and JPEG 2000 <b>coded</b> <b>image</b> is performed using the information of the blockiness and the average absolute difference between adjacent pixels. For im-age quality assessment of JPEG 2000 coding, the blur mea-sure is introduced instead of the blockiness of JPEG coding. 1...|$|E
40|$|We {{consider}} the recently proposed double phase-encoding system [Opt. Lett. 20, 767 (1995) ]. We study the robustness of the decoding process, that is, {{the way in}} which a perturbation of the <b>coded</b> <b>image</b> modifies the decoded image. We demonstrate that the amplitude signal-to-noise ratio (SNR) in the decoded image is strictly (and not only statistically) equal to the SNR in the <b>coded</b> <b>image</b> for different kinds of coded-image perturbations. In optical implementations the intensity of the decoded image is measured at the output of the decoding system. We show that there exists a simple relation between the intensity SNR of the decoded image and the amplitude SNR of the <b>coded</b> <b>image</b> and that this relation is quasi-independent {{of the nature of the}} coded-image perturbation. The results presented could provide a simple and efficient way of determining the precision level of the components of the optical decoding system necessary to reach a predefined quality level of the decoded image...|$|E
5000|$|The {{following}} QR <b>Code</b> <b>image</b> is {{an example}} containing the text: '''''' ...|$|R
40|$|Abstract. Image {{communication}} {{is a significant}} research area which involves improvement in <b>image</b> <b>coding</b> and communication techniques. In this paper, Principal Component Analysis (PCA) is used for face <b>image</b> <b>coding</b> and the <b>coded</b> <b>images</b> are protected with convolutional codes for transmission over Ad-ditive White Gaussian Noise (AWGN) channel. Binary Phase Shift Keying (BPSK) {{is used for the}} modulation of digital (binarized) <b>coded</b> <b>images.</b> Re-ceived binarized <b>coded</b> <b>images</b> are first decoded by the convolutional decoder using the Viterbi algorithm and then PCA decoded for recognition of the face. Unequal error protection (UEP) with two convolutional encoders with different rates is used to increase the overall performance of the system. The recognition rate of the transmitted <b>coded</b> face <b>images</b> without any protection is 35 %, while equal protection with convolutional codes gives rates up to 85 % accuracy. On the other hand, the proposed UEP scheme provides recognition rates up to 95 % with reduced redundancy. ...|$|R
3000|$|... [...]. The image {{decomposition}} {{is widely}} used {{in the literature of}} <b>image</b> <b>coding,</b> <b>image</b> denoising, image registration, and texture discrimination. A general way to obtain this decomposition using the variational approach is to solve the problem min {T [...]...|$|R
30|$|MSE is {{the mean}} square error, d is the maximal <b>coded</b> <b>image</b> intensity, n and m are the image sizes, and f and r are the {{original}} and the filtered image.|$|E
40|$|In this paper, {{we propose}} a new {{postprocessing}} method for low bit-rate wavelet-based image coding which uses {{the technique of}} wavelet modulus maximum representation (WMMR). The edge degradation from wavelet-based coding is discussed under the overcomplete wavelet expansion, and interpreted as the distortion of wavelet modulus maxima, i. e. magnitude decays. Based on the empirical analysis and experimental results, we develop a set of compensation functions to restore the distorted wavelet modulus maxima of a <b>coded</b> <b>image.</b> Therefore, we can reconstruct the <b>coded</b> <b>image</b> using the WMMR with improved image quality {{in terms of both}} subjective perception and image fidelity (PSNR). ...|$|E
3000|$|Finally, the <b>coded</b> <b>image</b> is {{transmitted}} over an AWGN channel {{by means of}} the BPSK modulation. In our simulation, we used an open source implementation of JPEG 2000 called OpenJPEG (J 2000 library). More details about the implementation are available in [URL] [...]...|$|E
40|$|Region of {{interest}} (ROI) image and video compression techniques {{have been widely}} used in visual communication applications in an effort to deliver good quality images and videos at limited bandwidths. Most image quality metrics have been developed for uniform resolution images. These metrics are not appropriate for the assessment of ROI <b>coded</b> <b>images,</b> where space-variant resolution is necessary. The spatial resolution of the human visual system (HVS) is highest around the point of fixation and decreases rapidly with increasing eccentricity. Since the ROIs are usually the regions "fixated" by human eyes, the foveation property of the HVS supplies a natural approach for guiding the design of ROI image quality measurement algorithms. We have developed an objective quality metric for ROI <b>coded</b> <b>images</b> in the wavelet transform domain. This metric can serve to mediate the compression and enhancement of ROI <b>coded</b> <b>images</b> and videos. We show its effectiveness by applying it to an embedded foveated <b>image</b> <b>coding</b> system...|$|R
40|$|Abstract: In this paper, a {{technique}} employing artifkial neural networks for post-processing block <b>coded</b> <b>images</b> is presented. Visually important image features are {{extracted from the}} de-compressed image and used as input to a feedforward neural network. The neural network learns to reconstruct the difference image between the original (uncompressed) and the de-compressed <b>image.</b> <b>Coding</b> artifact reduction is achieved by adding the neural networks output to the de-compressed image. Experimental results using the new technique for post-processing quadtree <b>coded</b> <b>images</b> are presented. It is shown the new technique can sigmficantly improve the compressed image {{both in terms of}} peak signal to noise ratio (PSNR) and visual quality of the image. 1...|$|R
40|$|In {{real time}} systems {{controlled}} by icroprocessors, programs are often stored in ROM. The nature of ROM means that certain restrictions are made. For example self-modifying code {{is not possible}} and compile time initialisations of data variables are difficult since the data segment of the software {{is not included in}} the ROM <b>code</b> <b>image.</b> These reAtrictions manifest themselves in the developed software so that the programmer must at all times be conscious of the memory configuration of the destination machine. This restricts the natural and intended uGage of many High Level Languages. This paper highlights these and other problems associated with <b>code</b> <b>images</b> suitable for ROM and offers a solution to these problems which is both more efficient than the current method of dealing with executable <b>code</b> <b>images</b> in RAM as well as ROM and is easily portable to any UNIX* development environment and associated destination architecture...|$|R
40|$|Abstract—In this paper, {{we propose}} a new {{postprocessing}} method for low bit-rate wavelet-based image coding which uses {{the technique of}} wavelet modulus maximum representation (WMMR). The edge degradation from wavelet-based coding is dis-cussed under the overcomplete wavelet expansion, and interpreted as the distortion of wavelet modulus maxima, i. e., magnitude decays. Based on the empirical analysis and experimental results, we develop a set of compensation functions to restore the distorted wavelet modulus maxima of a <b>coded</b> <b>image.</b> Therefore, we can enhance the image quality for low bit-rate wavelet coding by reconstructing the <b>coded</b> <b>image</b> using the WMMR with improved visual quality and image fidelity (peak signal-to-noise ratio). Index Terms—Low bit-rate image coding, multiscale edge char-acterization, postprocessing, wavelet modulus maximum represen-tation, wavelet transforms. I...|$|E
3000|$|... {{denotes the}} central pixel of SLBP <b>coded</b> <b>image,</b> which {{contains}} the information {{of the local}} regions like edges, flat areas and spots over the entire image. These regional histograms are combined to make one global histogram of whole image. SLBP features histogram is also shown in Figure [...]...|$|E
40|$|It is {{well known}} that low bit rate BDCT <b>coded</b> <b>image</b> {{exhibits}} visually annoying coding artifacts. In this paper, we proposed a POCS (projection onto convex sets) -based deblocking algorithm using a novel region smoothness constraint for the class of graphic images. For this type of image, there usually exists large smooth varying regions. Current POCS based deblocking algorithms usually do not enforce smoothness across several blocks. In our method, the BDCT (block-based discrete cosine transform) <b>coded</b> <b>image</b> is modeled using a smooth, spatially adaptive, thin-plate spline surface, and the region smoothness constraint set is constructed from this spline surface. The spline modeling allows us to model the long range smoothness across the smooth region. Simulation experiments indicated that blockiness in smooth regions is effectively suppressed while genuine edges are preserved. Full Tex...|$|E
3000|$|... [...]). A page when {{successfully}} received {{is stored}} in flash memory by each target sensor node. The lack of security is a well-known shortcoming in Deluge, and prior schemes such as SecDeluge [6], Sluice [7], and Seluge [9] mentioned earlier have extended Deluge to incorporate <b>code</b> <b>image</b> authentication. None of these proposals however ensure privacy of the <b>code</b> <b>image</b> broadcast. Our scheme, which we call PrivCIB (Priv ate C ode I mage B roadcast), implements privacy and authentication of Deluge packet transmissions. We emphasise to the reader that at present our scheme is limited to single-hop systems where the base station broadcasts new <b>code</b> <b>images</b> directly to all sensor nodes; extension to the true multihop [...] "epidemic" [...] dissemination mechanism of Deluge is deferred to future work. We also note that the key loss recovery mechanism outlined earlier in Section 4 above is unnecessary in this application since Deluge has in-built mechanisms for reliable packet delivery.|$|R
40|$|This paper {{presents}} an effective signal boundary solution in lossy-to-lossless <b>image</b> <b>coding</b> {{which is the}} unification of lossy and loss-less <b>image</b> <b>coding.</b> AlthoughM-channel filter banks (FBs) for lossy <b>image</b> <b>coding</b> have several effective signal boundary solutions, M-channel lifting based FBs (L-FBs) for lossless <b>image</b> <b>coding</b> do not have such an effective signal boundary solutions due to rounding er-ror in each lifting step. This paper proposes reversible non-expansive symmetric convolution forM-channel lifting based linear-phase FBs (L-LPFBs) to apply lossy-to-lossless <b>image</b> <b>coding.</b> Our proposal is validated by comparing with the periodic extension in lossy-to-lossless <b>image</b> <b>coding.</b> Index Terms — M-channel lifting based linear-phase filter bank (L-LPFB), lossy-to-lossless <b>image</b> <b>coding,</b> reversible non-expansive symmetric convolution 1...|$|R
40|$|The article {{researches}} {{the problem}} of interaction, mutual influence of Russian and Kazakh pictures {{of the world in}} the works of Kazakh bilingual poet Olzhas Suleimenov, who creats works in Russian using elements of Kazakh culture, images, similes, symbols, stereotypes. The article analyzes the features of use of poetic comparisons in Suleimenov's works, which are being a way of understanding the world, consolidate the results of this knowledge in the culture, inseparable from the culture. It describes comparisons, which are based on Russian cultural <b>codes,</b> <b>images</b> and comparisons and based on the Kazakh cultural <b>codes,</b> <b>images...</b>|$|R
40|$|The {{matching}} {{process is}} performed by nine similarity computation circuits (see Fig. 2). The relationship between similarity (input voltage difference) and output current is a Gaussian (square) func-tion if the circuit operates in the weak (strong) inversion region. The nine output currents are then summed {{to determine if}} a match has occurred. Nonlinear adaptive processing of block <b>coded</b> <b>image...</b>|$|E
40|$|In {{the context}} of low-delay {{reliable}} image transmission over CDMA slowly Rayleigh fading channels, this paper addresses the proposal of a coding control technique that dynamically adapts the source coder rate to the channel condition, in order to transmit <b>coded</b> <b>image</b> data with low delay. In previous work, we proposed tools for error-resilient coding {{in order to improve}} the quality of transmitted images over the aforementioned channel. However, in case of degrading channel conditions, an increase in the transmission delay may result due to the use of an equal ARQ error control for all <b>coded</b> <b>image</b> data. Therefore, we propose to decrease the volume of coded data when the channel is in poor condition to decrease the amount of data to be transmitted with ARQ, and hence reduce the image transmission delay. The approach proposed to dynamically control the coder rate to the channel condition is based on an estimate of the channel status. The scheme proposed is shown to offer acceptable delay a [...] ...|$|E
40|$|In {{this paper}} packetization and {{reconstruction}} algorithms in transmission of JPEG coded images over lossy channels are proposed. An interleaving method is devised that maximizes the minimum {{distance between the}} 8 # 8 blocks, used in JPEG, {{that are in the}} same packet. Then a method for estimating the missing DC coe#cients of a JPEG <b>coded</b> <b>image</b> which is required for decoding the compressed image, is suggested and evaluated. As e#ects of errors in estimating the missing DC value will appear as stripes across the image, a postprocessing technique for removing such stripes is also developed. Finally the missing data is reconstructed by exploiting the correlation between adjacent blocks. A novel reconstruction technique which has a good performance in reconstruction of edges is proposed. Simulation results indicate that our algorithms perform very well, even when more than one packet is lost during transmission of the JPEG <b>coded</b> <b>image.</b> Keywords: JPEG, image reconstruction, packetiza [...] ...|$|E
30|$|The use of wavelets in {{the broad}} areas of source coding, communications, and {{networks}} is surveyed. Specifically, the impact of wavelets and wavelet theory in <b>image</b> <b>coding,</b> video <b>coding,</b> <b>image</b> interpolation, image-adaptive lifting transforms, multiple-description coding, and joint source-channel coding is overviewed. Recent contributions in these areas arising in subsequent papers of the present special issue are described.|$|R
40|$|This paper {{proposes a}} new {{approach}} toreduce coding artifacts in transform <b>image</b> <b>coding.</b> We approach the problem in an estimation of each transform coefficient from the quantized data by using its local mean and variance. The proposed method can reduce much coding artifacts of low bit-rate <b>coded</b> <b>images,</b> and atthe same time guarantee that the resulting images satisfies the quantization error constraint...|$|R
40|$|This paper {{proposes a}} new {{approach}} to reduce coding artifacts in transform <b>image</b> <b>coding.</b> We approach the problem in an estimation of each transform coefficient from the quantized data by using its local mean and variance. The proposed method can reduce much coding artifacts of low bit-rate <b>coded</b> <b>images,</b> {{and at the same time}} guarantee that the resulting images satisfies the quantization error constraint. 1. INTRODUCTION Block transform-based <b>image</b> <b>coding</b> offers a good tradeoff between bit rate and subjective image quality, and hence is the most widely used technique in image compression. Unfortunately, noise caused by the coarse quantization of transform coefficients is noticeable in a form of visible block boundaries when the compression ratio is sufficiently high. Various techniques had been proposed to remove blocking artifacts of low bitrate <b>coded</b> <b>images.</b> In order to make encoder efficient, most of them involves post-processing at the decoding side, rather than approaching the pro [...] ...|$|R
40|$|This paper {{presents}} a combined approach for image restoration with edge-preserving regularization, subband coding, and artificial neural network. The edge information is detected {{from the source}} image as a priori knowledge to recover the details and reduce the ringing artifact of the subband <b>coded</b> <b>image.</b> The multilayer perceptron model is employed to implement the restoration of images. The main merit of the presented approach is that the neural network model is massively parallel with stronger robustness for transmission noise and parameter or structure perturbation, {{and it can be}} realized by very large scale integrated technologies for real-time applications. To evaluate the performance of the proposed approach, a comparative study with the set partitioning in hierarchical tree (SPIHT) has been made by using a set of gray-scale digital images. The experiment has shown that the proposed approach could result in considerably better performances compared with SPIHT on both objective and subjective quality for lower compression ratio subband <b>coded</b> <b>image.</b> Department of Computin...|$|E
40|$|Coded {{aperture}} {{imaging spectrometer}} {{based on the}} concept of compressed sensing can acquire the spectral diagram of object. Coded aperture spectral data restoration reconstructs three-dimensional data cube from two-dimensional <b>coded</b> <b>image.</b> The two-step iterative shrinkage/thresholding algorithms were derived from the iterative shrinkage threshold algorithm and weighted iterative shrinkage algorithm. Recovering coded aperture spectral data using the two-step iterative shrinkage/thresholding algorithms obtained the three-dimensional spectral data cube successfully...|$|E
40|$|Landsat MSS data off Cochin Coast was {{analysed}} ond on attempt {{made to look}} in to {{the relationship}} between MSS gray values and concentration of pigment/particulate matter. MSS band 4 and 5 showed the maximum gray value range as compared to band 6 and 7. A density sliced image of band 4 was generated in the form of Q color <b>coded</b> <b>image</b> showing the gray levels corresponding to various pigment levels...|$|E
40|$|Abstract—Wireless sensor {{networks}} are considered ideal candidates {{for a wide}} range of applications. It is desirable and sometimes necessary to reprogram sensor nodes through wireless links after they are deployed to remove bugs or add new functionalities. Several approaches (e. g., Seluge, Sluice) have been proposed recently for secure code dissemination in wireless sensor networks, all as security extensions to the state-of-theart code dissemination system named Deluge. However, existing approaches all focused on securing the propagation of <b>code</b> <b>images,</b> but overlooked the security vulnerabilities in other image management aspects such as rebooting and erasing <b>code</b> <b>images.</b> In this paper, we identify the security vulnerabilities in epidemic image management in all existing solutions to secure code dissemination in wireless sensor networks. Such vulnerabilities allow an attacker to reboot a sensor network to undesirable images or erase critical images, exposing the network to security risks. We then develop a sequence of lightweight techniques to address these vulnerabilities. Our approach takes into consideration the limited resources on current sensor platforms, and removes the security vulnerabilities without introducing significant overhead. To evaluate the feasibility of our approach, we implement the proposed approach as a remote image management system named Seluge-ImageMan, which is intended to work with Seluge, a security extension to Deluge for injecting new <b>code</b> <b>images.</b> We perform a substantial set of experiments in the WiSeNeT sensor testbed, which consists of 72 MicaZ motes, to assess the performance overhead of Seluge-ImageMan. The experimental results indicate that our approach introduces very light overhead while completing the secure remote <b>code</b> <b>image</b> management solution for wireless sensor networks. I...|$|R
40|$|Abstract—This letter {{presents}} {{a new approach}} to reduce coding artifacts in transform <b>image</b> <b>coding.</b> We approach the problem in an estimation of each transform coefficient from its quantized version with its local mean and variance. The proposed method can reduce much coding artifacts of low bit-rate <b>coded</b> <b>images,</b> {{and at the same time}} guarantee that the resulting images satisfies the quantization error constraint. I...|$|R
40|$|Interaction {{prediction}} has a {{wide range}} of applications such as robot controlling and prevention of dangerous events. In this paper, we introduce a new method to capture deep temporal information in videos for human interaction prediction. We propose to use flow <b>coding</b> <b>images</b> to represent the low-level motion information in videos and extract deep temporal features using a deep convolutional neural network architecture. We tested our method on the UT-Interaction dataset and the challenging TV human interaction dataset, and demonstrated the advantages of the proposed deep temporal features based on flow <b>coding</b> <b>images.</b> The proposed method, though using only the temporal information, outperforms {{the state of the art}} methods for human interaction prediction...|$|R
