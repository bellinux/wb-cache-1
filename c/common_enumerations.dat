5|36|Public
40|$|We present {{compiler}} {{technology for}} generating sparse matrix code from (i) dense matrix code and (ii) {{a description of}} the indexing structure of the sparse matrices. This technology embeds statement instances into a Cartesian product of statement iteration and data spaces, and produces efficient sparse code by identifying <b>common</b> <b>enumerations</b> for multiple references to sparse matrices. This approach works for imperfectly-nested codes with dependences, and produces sparse code competitive with hand-written library code...|$|E
40|$|We present {{compiler}} {{technology for}} synthesizing sparse matrix code from (i) dense matrix code, and (ii) {{a description of}} the index structure of a sparse matrix. Our approach is to embed statement instances into a Cartesian product of statement iteration and data spaces, and to produce efficient sparse code by identifying <b>common</b> <b>enumerations</b> for multiple references to sparse matrices. The approach works for imperfectly-nested codes with dependences, and produces sparse code competitive with hand-written library code for the Basic Linear Algebra Subroutines (BLAS) ...|$|E
40|$|We present {{compiler}} {{technology for}} generating sparse matrix code from (i) dense matrix code and (ii) {{a description of}} the indexing structure of the sparse matrices. This technology embeds statement instances into a Cartesian product of statement iteration and data spaces, and produces efficient sparse code by identifying <b>common</b> <b>enumerations</b> for multiple references to sparse matrices. This approach works for imperfectly-nested codes with dependences, and produces sparse code competitive with hand-written library code. 1 Introduction Sparse matrices are usually stored in compressed formats in which zeros are not stored explicitly [9]. This reduces storage requirements, and in many codes, also eliminates the need to compute with zeros. Figure 1 shows a sparse matrix and a number of commonly used compressed formats that we will use as running examples in this paper. The simplest format is Co-ordinate storage (COO) in which three arrays are used to store non-zero elements and the [...] ...|$|E
5000|$|<b>Common</b> Configuration <b>Enumeration</b> (CCE) (prior web-site at MITRE) ...|$|R
50|$|In March 2012, Klocwork Insight {{received}} the Certificate of CWE Compatibility from The MITRE Corporation's <b>Common</b> Weakness <b>Enumeration</b> Program.|$|R
40|$|Abstract: To develop secure {{software}}, software engineers need to {{have the}} mindset of attackers. Developing abuse cases can help software engineers to think more like attackers. This paper describes a method for developing abuse cases based on threat modeling, attack patterns, and <b>Common</b> Weakness <b>Enumeration.</b> The method also includes ranking the abuse cases according to their risks. This method intends to help non-experts create abuse cases following a specific process, and leveraging the knowledge bases of threat modeling, attack patterns, and <b>Common</b> Weakness <b>Enumeration.</b> The proposed method was evaluated through two evaluation studies conducted in two secure software engineering courses at two different universities. Evaluation studies show that the proposed method was easier to follow by non-experts in generating abuse cases than brainstorming, and could reduce the time needed for creating abuse cases. Other findings from the evaluation studies are also discussed in the paper. Key words: Abuse cases, threat modeling, attack patterns, <b>common</b> weakness <b>enumeration,</b> secure software development...|$|R
40|$|We present {{compiler}} {{technology for}} synthesizing sparse matrix code from (i) dense matrix code, and (ii) {{a description of}} the index structure of a sparse matrix. Our approach is to embed statement instances into a Cartesian product of statement iteration and data spaces, and to produce efficient sparse code by identifying <b>common</b> <b>enumerations</b> for multiple references to sparse matrices. The approach works for imperfectly-nested codes with dependences, and produces sparse code competitive with hand-written library code for the Basic Linear Algebra Subroutines (BLAS). 1 Introduction Many applications that require high-performance computing perform computations on sparse matrices. For example, the finite-element method for solving partial differential equations approximately requires the solution of large linear systems of the form Ax = b where A is a large sparse matrix. Some web-search engines and data-mining codes compute eigenvectors of large sparse matrices that represent how often cer [...] ...|$|E
5000|$|<b>Common</b> Weakness <b>Enumeration</b> (CWE) Compatibility program {{allows a}} service or a product to be {{reviewed}} and registered as officially [...] "CWE-Compatible" [...] and [...] "CWE-Effective". The program assists organizations in selecting the right software tools and learning about possible weaknesses and their possible impact.|$|R
40|$|This report {{describes}} the Software Engineering Institute 2 ̆ 7 s work in calendar year 2010 for the National Security Agency Computer Network Defense Research and Technology Program Management Office to develop standards for remediation of vulnerabilities and compliance issues on Department of Defense (DoD) networked systems. The overall goals are {{to assist in}} the development of remediation standards, demonstrate the functionality DoD would like in a remediation manager, and increase efficiency and effectiveness of remediation by automating the remediation process. The 2010 Remediation Manager reference implementation demonstrates the following potential applications of remediation and other security automation standards: (1) Ingest scan findings in Security Content Automation Protocol (SCAP) format, extracting host compliance issues (in <b>Common</b> Configuration <b>Enumeration</b> [CCE] format) and vulnerabilities (in <b>Common</b> Vulnerability <b>Enumerations</b> [CVE] format). (2) Map CCE and CVE to remediation actions (in <b>Common</b> Remediation <b>Enumeration</b> [CRE] format). (3) Build remediation tasks in Remediation Tasking Language (RTL), based on CRE. (4) Transmit remediation tasks to a Remediation Tool on a host system. (5) Receive remediation task execution status, in RTL Results Format, from the Remediation Tool. This report identifies capabilities considered for future versions of the reference implementation and the operational system as well as challenges for future work...|$|R
5000|$|In a {{sense of}} his own, Brown also, while {{accepting}} the <b>common</b> Aristotelian <b>enumeration</b> of principles, inclined to the opinion that [...] "all suggestion may be found to depend on prior coexistence, or at least on such proximity as is itself very probably a modification of coexistence", provided account be taken of ...|$|R
40|$|This {{paper is}} a status {{update on the}} <b>Common</b> Weaknesses <b>Enumeration</b> (CWE) initiative, one of the efforts focused on {{improving}} the utility and effectiveness of code-based security assessment technology. It is hoped that the CWE initiative will help to dramatically accelerate the use of tool-based assurance arguments in reviewing software systems for security issues. 1...|$|R
40|$|Abstract. The {{rapid growth}} in {{magnitude}} and complexity of cyber-security information and event management (CSIEM) has ignited a trend toward security automation and information exchange standards. Making Security Measurable (MSM) references a collection of open community standards for the <b>common</b> <b>enumeration,</b> expression and reporting of cyber-security-related information. While MSM-related standards are valuable for enabling security automation; insufficient vocabulary management and data interoperability methods as well as domain complexity that exceeds current representation capabilities impedes the adoption of these important standards. This paper describes an Agile, ontology architecture-based approach for improving the ability to represent, manage, and implement MSM-related standards. Initial cross-standard analysis revealed enough common concepts to warrant four ontologies that are reusable across standards. This reuse will simplify standards-based data interoperability. Further, early prototyping enabled us to streamline vocabulary management processes and demonstrate the ability to represent complex domain semantics in OWL ontologies...|$|R
40|$|Abstract. Contour-based object {{detection}} can be formulated as {{a matching}} problem between model contour parts and image edge fragments. We propose a novel solution by treating this problem {{as the problem}} of finding dominant sets in weighted graphs. The nodes of the graph are pairs composed of model contour parts and image edge fragments, and the weights between nodes are based on shape similarity. Because of high consistency between correct correspondences, the correct matching corresponds to a dominant set of the graph. Consequently, when a dominant set is determined, it provides a selection of correct correspondences. As the proposed method is able {{to get all the}} dominant sets, we can detect multiple objects in a image in one pass. Moreover, since our approach is purely based on shape, we also determine an optimal scale of target object without a <b>common</b> <b>enumeration</b> of all possible scales. Both theoretic analysis and extensive experimental evaluation illustrate the benefits of our approach. ...|$|R
40|$|AbstractAs the {{computer}} network has evolved {{to provide the}} user many services, the attacks on these networks to disrupt the services and {{to gain access to}} resources has also evolved. New entities in form of services, hardware, network protocols etc. are being added to the network, which is leading to new ways to attack the network. The complexity of the system is increasing so fast that it is becoming increasingly difficult for network administrator to comprehend the situation and react in an appropriate manner. Situation becomes more complex as there is no uniform terminology. Though serious efforts in form of <b>Common</b> Vulnerability <b>Enumeration</b> (CVE), <b>Common</b> Weakness <b>Enumeration</b> (CWE), <b>Common</b> Attack Pattern <b>Enumeration</b> and Classification (CAPEC) etc. has been made, still a long way is to go. In this paper we have proposed a formal specification of the framework for network security situational awareness (NSSA) using ontological engineering approach. We have modeled a computer network by modeling its components i. e. hardware, software, services using ontology. Also vulnerabilities and attacks on these computers are modeled. We populate our ontology with various instances of vulnerabilities, CVSS scores, attacks and possible services in the network. Knowledge representation methods are used in order to provide Description Logic reasoning and inference over network security status concept. Secondly we propose an ontology based system which predicts probable attacks using inference and information provided by the environment...|$|R
50|$|Among other efforts, Mitre {{maintains}} the Common Vulnerabilities and Exposures (CVE) {{system and the}} <b>Common</b> Weakness <b>Enumeration</b> (CWE) project. Since 1999, the Mitre Corporation functions as editor and primary CNA of the Common Vulnerabilities and Exposures). CVE is now the industry standard for vulnerability and exposure names, providing reference points for data exchange so that information security products and services can interoperate with each other.|$|R
5000|$|<b>Common</b> Weakness <b>Enumeration</b> is a {{software}} community project that aims at creating {{a catalog of}} software weaknesses and vulnerabilities. The goal of the project is to better understand flaws in software and to create automated tools {{that can be used}} to identify, fix, and prevent those flaws. [...] The project is sponsored by the National Cybersecurity FFRDC, which is owned by The MITRE Corporation.|$|R
50|$|One way {{to improve}} {{software}} security is {{to gain a better}} understanding of the most common weaknesses that can affect software security. With that in mind, there is a current community-based program called the <b>Common</b> Weaknesses <b>Enumeration</b> project, which is sponsored by The Mitre Corporation to identify and describe such weaknesses. The list, which is currently in a very preliminary form, contains descriptions of common software weaknesses, faults, and flaws.|$|R
5000|$|J is the zero-based century (actually [...] ) For example, the zero-based centuries for 1995 and 2000 are 19 and 20 {{respectively}} (to not {{be confused}} with the <b>common</b> ordinal century <b>enumeration</b> which indicates 20th for both cases).|$|R
40|$|The goal of {{this project}} {{is to create a}} {{reliable}} and accurate data set that is made by collecting information from several famous data sources such National Vulnerabilities Database (NVD) and <b>Common</b> Weakness <b>Enumeration</b> (CVE). The nest step is to prepare a multi-dimensional environment for users to apply OLAP techniques in order to having accurate analysis and exploiting their requirements. In this project data warehouse and statistical methods are used to enable the user to analyze data with in multi-dimensional environment...|$|R
25|$|This view of {{software}} quality on a linear continuum {{has to be}} supplemented by the identification of discrete Critical Programming Errors. These vulnerabilities may not fail a test case, {{but they are the}} result of bad practices that under specific circumstances can lead to catastrophic outages, performance degradations, security breaches, corrupted data, and myriad other problems (Nygard, 2007) that make a given system de facto unsuitable for use regardless of its rating based on aggregated measurements. A well-known example of vulnerability is the <b>Common</b> Weakness <b>Enumeration,</b> a repository of vulnerabilities in the source code that make applications exposed to security breaches.|$|R
5000|$|The most <b>common</b> use of <b>enumeration</b> in {{set theory}} {{occurs in the}} context where {{infinite}} sets are separated into those that are countable {{and those that are}} not. In this case, an enumeration is merely an enumeration with domain &omega;, the ordinal of the natural numbers. This definition can also be stated as follows: ...|$|R
40|$|Abstract—Many {{vulnerabilities}} in today’s software {{products are}} rehashes of past vulnerabilities. Such rehashes {{could be a}} result of software complexity that masks inadvertent loopholes in design and implementation, developer ignorance/disregard for security issues, or use of software in contexts not anticipated for the original specification. While weaknesses and exposures in code are vendor, language, or environment specific, to understand them we need better descriptions that identify their precise characteristics in an unambiguous representation. In this paper, we present a methodology to develop precise and accurate descriptions of common software weaknesses through lightweight formal modeling using Alloy. Natural language descriptions of software weaknesses used for formalization are based on the community developed <b>Common</b> Weakness <b>Enumerations</b> (CWE). Index Terms—Software weakness, Alloy modeling, CWE. I...|$|R
40|$|Abstract-Security metrics for {{software}} systems provide quantitative measurement for {{the degree of}} trustworthiness {{for software}} systems. This paper proposes {{a new approach to}} define software security metrics based on vulnerabilities included in the software systems and their impacts on software quality. We use the Common Vulnerabilities and Exposures (CVE), an industry standard for vulnerability and exposure names, the <b>Common</b> Weakness <b>Enumeration</b> (CWE), a list of software weaknesses, and the Common Vulnerability Scoring System (CVSS), a vulnerability scoring system designed to provide an open and standardized method for rating software vulnerabilities, in our metric definition and calculation. Examples are provided {{at the end of the}} paper, which show that our definition is consistent with the common practice and real-world experience about software quality...|$|R
40|$|<b>Common</b> Attack Pattern <b>Enumeration</b> and Classification (CAPEC) {{effort is}} a {{publicly}} available, community-developed list of common attack patterns {{along with a}} comprehensive schema and classification taxonomy. Each attack pattern captures knowledge about how specific parts of an attack are designed and executed, providing the attacker’s perspective on the problem and the solution, and gives guidance on ways to mitigate the attack’s effectiveness. The author looked at 453 documented attack patterns and enumerated the associated severity, likelihood of exploit and attacker skill required as documented by CAPEC, eventually identifying 27 attack patterns. More information at [URL]...|$|R
40|$|With {{water we}} have trust that qualities harmful to its {{intended}} use are not present. In {{order to avoid}} a regulatory “solution ” to problems with “contaminants ” that endanger software’s intended use, the industry needs {{to put in place}} processes and technical methods for examining software for the contaminants that are most dangerous given the intended use of specific software. The <b>Common</b> Weakness <b>Enumeration</b> (CWE™) [1] offers the industry a list of potentially dangerous contaminants to software. Common Weakness Scoring System (CWSS™) [2] and Common Weakness Risk Analysis Framework (CWRAF™) [3] provide a standard method for identifying which of these dangerous contaminants would be most harmful to a particular organization, given the intended use of a specific piece of software within that organization. By finding systematic and verifiable ways of identifying, removing, and gaining assurance that contaminated software has been addressed, software providers can improve customers’ confidence in systems and possibly avoid regulatory solutions...|$|R
40|$|In this paper, we {{analyze the}} <b>Common</b> Platform <b>Enumeration</b> (CPE) {{dictionary}} and the Common Vulnerabilities and Exposures (CVE) feeds. These repositories {{are widely used}} in Vulnerability Management Systems (VMSs) to check for known vulnerabilities in software products. The analysis shows, among other issues, a lack of synchronization between both datasets {{that can lead to}} incorrect results output by VMSs relying on those datasets. To deal with these problems, we developed a method that recommends to a user a prioritized list of CPE identifiers for a given software product. The user can then assign (and, if necessary, adapt) the most suitable CPE identifier to the software so that regular (e. g., daily) checks can find known vulnerabilities for this software in the CVE feeds. Our evaluation of this method shows that this interaction is indeed necessary because a fully automated CPE assignment is prone to errors due to the CPE and CVE shortcomings. We implemented an open-source VMS that employs the proposed method and published it on GitHub...|$|R
50|$|The company shares {{its name}} with its {{flagship}} product, Code Dx Enterprise. Enterprise is a vulnerability management system that combines and correlates the results {{generated by a}} wide variety of static and dynamic testing tools. For static analysis, the product installs and configures several bundled open source static analysis tools, and also connects automatically to a variety of commercial tools. The software selects the most appropriate analysis tool or tools for the language(s) in which the tested application is written, and maps the results of those tools (which vary according to the tool) to the <b>Common</b> Weakness <b>Enumeration</b> (CWE). For dynamic testing, Enterprise gathers the results of dynamic tool tests and integrates them into its vulnerability reports. In situations during which several tools are run simultaneously, results are consolidated and redundancies are removed. Identified vulnerabilities are mapped to various industry standards (like OWASP Top 10 and Web Application Security Consortium). Additionally, it identifies sections of code that are not compliant with applicable regulatory standards, such as HIPAA software regulations. The product supplies a visual interface that makes it simpler to identify vulnerability trends within the source code of the tested application.|$|R
40|$|Classification of {{software}} security vulnerability no doubt facilitates {{the understanding of}} security-related information and accelerates vulnerability analysis. The lack of proper classification not only hinders its understanding but also renders the strategy of developing mitigation mechanism for clustered vulnerabilities. Now software developers and researchers are agreed {{on the fact that}} requirement and design phase of the software are the phases where security incorporation yields maximum benefits. In this paper we have attempted to design a classifier that can identify and classify design level vulnerabilities. In this classifier, first vulnerability classes are identified on the basis of well established security properties like authentication and authorization. Vulnerability training data is collected from various authentic sources like <b>Common</b> Weakness <b>Enumeration</b> (CWE), <b>Common</b> Vulnerabilities and Exposures (CVE) etc. From these databases only those vulnerabilities were included whose mitigation is possible at the design phase. Then this vulnerability data is pre-processed using various processes like text stemming, stop word removal, cases transformation. After pre-processing, SVM (Support Vector Machine) is used to classify vulnerabilities. Bootstrap validation is used to test and validate the classification process performed by the classifier. After training the classifier, a case study is conducted on NVD (National Vulnerability Database) design level vulnerabilities. Vulnerability analysis is done on the basis of classification result...|$|R
40|$|The <b>Common</b> Attack Pattern <b>Enumeration</b> and Classification (CAPEC™) is a publicly-available {{enumeration}} and {{classification of}} cyber-attack patterns. CAPEC provides {{a catalog of}} known cyber attacks, along with standard language for describing attack classes and their hierarchical relationships. The CAPEC catalog is a complex hierarchy of hundreds of attack classes, representing textual, categorical, and referential data. We describe a number of analytic and visual techniques for interacting with this comprehensive knowledge base. We visualize the CAPEC attack pattern hierarchy, which shows taxonomic relationships of attack classes and sub-classes. We also visualize hierarchical clusters based on the textual content of CAPEC entries, providing an automated and mathematically sound alternative to the current ad hoc manual process. For interacting with referential data, we employ bipartite graph visualizations of attack-pattern references to standardized software weakness...|$|R
40|$|Injection attacks, {{including}} SQL injection, cross-site scripting, {{and operating}} system command injection, rank {{the top two}} entries in the MITRE <b>Common</b> Vulnerability <b>Enumeration</b> (CVE) [1]. Under this attack model, an application (e. g., a web application) uses some untrusted input to produce an output program (e. g., a SQL query). Applications may be vulnerable to injection attacks because the untrusted input may alter the output program in malicious ways. Recent work has established a rigorous definition of injection attacks. Injections are benign iff they obey the NIE property, which states that injected symbols strictly insert or expand noncode tokens in the output program. Noncode symbols are strictly those that are either removed by the tokenizer (e. g., insignificant whitespace) or span closed values in the output program language, and code symbols are all other symbols. This thesis demonstrates that such attacks are possible on applications for Android—a mobile device operating system—and Bash—a common Linux shell—and shows by construction that these attacks can be detected precisely. Specifically, this thesis examines the recent Shellshock attacks on Bash and shows how it widely differs from ordinary attacks, but can still be precisely detected by instrumenting the output program’s runtime. The paper closes {{with a discussion of}} the lessons learned from this study and how best to overcome the practical challenges to precisely preventing these attacks in practice...|$|R
40|$|Abstract: Researchers {{have devised}} {{multiple}} solutions to cross-site scripting, but vulnerabilities persists in many Web applications due to developer‟s lack of {{expertise in the}} problem identification and their unfamiliarity with the current mechanisms. As proclaimed by the experts, cross-site scripting is among the serious and widespread threats in Web applications these days more than buffer overflows. Recent study shows XSS has ranked first in the MITRE <b>Common</b> Weakness <b>Enumeration</b> (CWE) /SANS Institute list of Top 25 Most Dangerous Software Errors and second in the Open Web Application Security Project (OWASP). However, vulnerabilities continue to exist in many Web applications due to developers ‟ {{lack of understanding of}} the problem and their unfamiliarity with current guarding strengths and limitations. Existing techniques for defending against XSS exploits suffer from various weaknesses: inherent limitations, incomplete implementations, complex frameworks, runtime overhead, and intensive manual-work requirements. Security researchers can address these weaknesses from two different perspectives. They need to look beyond current techniques by incorporating more effective input validation and sanitization features. In time, development tools will incorporate security frameworks such as ESAPI that implement state-of-the-art technology. This paper focus on program verification perspective, how researchers must integrate program analysis, pattern recognition, concolic testing, data mining, and AI algorithms to solve different software engineering problems and to enhance the effectiveness of vulnerability detection. Focus on such issues would improve th...|$|R
40|$|Researchers {{have devised}} {{multiple}} solutions to cross-site scripting, but vulnerabilities persists in many Web applications due to developer’s lack of {{expertise in the}} problem identification and their unfamiliarity with the current mechanisms. As proclaimed by the experts, cross-site scripting is among the serious and widespread threats in Web applications these days more than buffer overflows. Recent study shows XSS has ranked first in the MITRE <b>Common</b> Weakness <b>Enumeration</b> (CWE) /SANS Institute list of Top 25 Most Dangerous Software Errors and second in the Open Web Application Security Project (OWASP). However, vulnerabilities continue to exist in many Web applications due to developers’ {{lack of understanding of}} the problem and their unfamiliarity with current guarding strengths and limitations. Existing techniques for defending against XSS exploits suffer from various weaknesses: inherent limitations, incomplete implementations, complex frameworks, runtime overhead, and intensive manual-work requirements. Security researchers can address these weaknesses from two different perspectives. They need to look beyond current techniques by incorporating more effective input validation and sanitization features. In time, development tools will incorporate security frameworks such as ESAPI that implement state-of-the-art technology. This paper focus on program verification perspective, how researchers must integrate program analysis, pattern recognition, concolic testing, data mining, and AI algorithms to solve different software engineering problems and to enhance the effectiveness of vulnerability detection. Focus on such issues would improve the precision of current methods by acquiring attack code patterns from outside experts as soon as they become available...|$|R
40|$|Vulnerabilities {{are one of}} {{the main}} {{concerns}} faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker’s mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i. e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2, 200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 <b>Common</b> Weakness <b>Enumeration</b> (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i. e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities...|$|R
40|$|Tools and {{techniques}} are emerging {{that allow us}} to directly evaluate software artifacts to gain assurance that they are free of exploitable vulnerabilities. This is complementary to the current capability to assess the process used to build the software and the ability to assess the specification and design of security-relevant features (encryption, authentication, etc.). The current ad-hoc use of these direct evaluation tools {{and techniques}} results in critical gaps in coverage and inefficiencies. In this paper, we explore an approach to using these tools in a more structured way that aligns with the desire to document an "assurance case" for a particular piece of software. The approach starts with having the user specify a list of narrowly defined attributes to assure. This list will generally be a subset of standard dictionaries like Mitre's <b>Common</b> Weakness <b>Enumeration</b> (CWE) (Mitre) or CERT's Secure Coding Standards (CERT) but may also include attributes for a particular purpose/product/organization. The user can decide upon the initial list based upon risk and then evolve the list {{over the life of the}} product. The user will then choose a regime of tools and techniques to "positively assure " each of the attributes enumerated in the chosen set. The output of using those tools and techniques is all gathered into a measurement information system so it is available as evidence for the assurance case. This paper (1) defines "positive assurance", (2) gives more detail on this proposed approach, (3) discusses some of the issues that we expect to arise, and (4) explores the use of this approach with an example from the CWE and CERT'...|$|R
40|$|We {{study the}} problem of {{generating}} monomials of a polynomial {{in the context of}} enumeration complexity. In this setting, the complexity measure is the delay between two solutions and the total time. We present two new algorithms for restricted classes of polynomials, which have a good delay and the same global running time as the classical ones. Moreover they are simple to describe, use little evaluation points {{and one of them is}} parallelizable. We introduce three new complexity classes, TotalPP, IncPP and DelayPP, which are probabilistic counterparts of the most <b>common</b> classes for <b>enumeration</b> problems, hoping that randomization will be a tool as strong for enumeration as it is for decision. Our interpolation algorithms proves that a lot of interesting problems are in these classes like the enumeration of the spanning hypertrees of a 3 -uniform hypergraph. Finally we give a method to interpolate a degree 2 polynomials with an acceptable (incremental) delay. We also prove that finding a specified monomial in a degree 2 polynomial is hard unless RP = NP. It suggests that there is no algorithm with a delay as good (polynomial) as the one we achieve for multilinear polynomials...|$|R
40|$|The term “supply chain ” has a {{long history}} in the {{business}} community and includes recent trends such as such justin-time inventory. In the past, the business community considered supply chains as relevant only to the delivery of physical products. Now the business community uses the technology supply chain to develop most IT systems (hardware, software, public and classified networks, and connected devices), which together enable the uninterrupted operations of key government and industrial base actors, such as the Department of Defense, the Department of Homeland Security, and their major suppliers. While we have decades of physical supply chain data that have led to effective management practices, we have limited experience with software supply chains. While no perfect solution exists, much can be done to enable organizations to reduce risk effectively and efficiently while leveraging the significant opportunities afforded by supply chains. On-time delivery and costs often get the most commercial attention, but some of the most serious risks are associated with system assurance, the confidence that the system behaves as expected. Software defects, such as design and implementation errors, can lead to unexpected behaviors or to system failure. Defects that enable an attacker to purposely change system behavior are often referred to as vulnerabilities. The source of such vulnerabilities is the supply chain, which includes commercial product vendors, custom development and integration contractors, and suppliers and subcontractors to those organizations. This research considers how to better manage the acquisition of software developed through a supply chain to reduce the likelihood of operational vulnerabilities. Unfortunately, exploitable software defects are widespread. MITRE has analyzed successful attacks and identified more than 600 common software weaknesses, described in its <b>Common</b> Weakness <b>Enumeration</b> (CWE). Many of the CWE defects are widely known, as are the techniques that eliminate them. But those techniques are frequently not applied. For example, countermeasures for SQL injections are well established, yet SQL injections still rank second on the MITRE/SANS list of the top 25 most dangerous softwar...|$|R
