691|270|Public
2500|$|The <b>{{conditional}}</b> <b>entropy</b> or conditional {{uncertainty of}} [...] given random variable [...] (also called the equivocation of [...] about [...] ) {{is the average}} <b>conditional</b> <b>entropy</b> over : ...|$|E
2500|$|Because entropy can be {{conditioned}} on {{a random}} variable or on that random variable being a certain value, {{care should be}} taken not to confuse these two definitions of <b>conditional</b> <b>entropy,</b> the former of which is in more common use. [...] A basic property of this form of <b>conditional</b> <b>entropy</b> is that: ...|$|E
2500|$|One {{may also}} define the <b>conditional</b> <b>entropy</b> of two events [...] and [...] taking values [...] and [...] respectively, as ...|$|E
3000|$|To {{estimate}} the bit-rate of the quantized control signals, we use (47), which require the computations of discrete <b>conditional</b> <b>entropies.</b> To {{estimate the}}se <b>conditional</b> <b>entropies,</b> {{we use a}} histogram-based entropy estimation on the sequence of discrete (quantized) control signals {ũ_t}_t= 0 ^T. Specifically, we first estimate H(ũ_t(1)) directly from {ũ_t(1)}_t= 0 ^T. Then, we estimate H(ũ_t(1), ũ_t(2)) from {ũ_t(1),ũ_t(2)}_t= 0 ^T and use that H(ũ_t(2)|ũ_t(1)) = H(ũ_t(1), ũ_t(2)) - H(ũ_t(1)). We obtain H(ũ_t(3)|ũ_t(1),ũ_t(2)) = H(ũ_t) - H(ũ_t(1),ũ_t(2)) in a similar way. Finally, these estimates of the <b>conditional</b> <b>entropies</b> are inserted into (47) in order to approximate the resulting operational bit-rate R. The resulting total discrete entropy R [...]...|$|R
5000|$|Analogous to the above, {{conditional}} {{total correlation}} reduces to {{a difference of}} <b>conditional</b> <b>entropies,</b> ...|$|R
2500|$|Note {{that the}} <b>conditional</b> <b>entropies</b> [...] and [...] {{do not have}} to be both non-negative.|$|R
2500|$|... that is, the <b>conditional</b> <b>entropy</b> of {{a symbol}} {{given all the}} {{previous}} symbols generated. [...] For the more general case of {{a process that is}} not necessarily stationary, the average rate is ...|$|E
2500|$|A 2009 paper {{published}} by Rajesh P N Rao, Iravatham Mahadevan, {{and others in}} the journal Science also challenged the argument that the Indus script might have been a nonlinguistic symbol system. The paper concluded that the <b>conditional</b> <b>entropy</b> of Indus inscriptions closely matched those of linguistic systems like the Sumerian logo-syllabic system, Rig Vedic Sanskrit etc., though they are careful to stress that this does not by itself imply that the script is linguistic. A follow-up study presented further evidence in terms of entropies of longer sequences of symbols beyond pairs.|$|E
50|$|Unlike the {{classical}} <b>conditional</b> <b>entropy,</b> the conditional quantum entropy can be negative. This is true {{even though the}} (quantum) von Neumann entropy of single variable is never negative. The negative <b>conditional</b> <b>entropy</b> {{is also known as}} the coherent information, and gives the additional number of bits above {{the classical}} limit that can be transmitted in a quantum dense coding protocol. Positive <b>conditional</b> <b>entropy</b> of a state thus means the state cannot reach even the classical limit, while the negative <b>conditional</b> <b>entropy</b> provides for additional information.|$|E
2500|$|Equivalently, the {{statement}} can be recast {{in terms of}} <b>conditional</b> <b>entropies</b> to show that for tripartite state , ...|$|R
2500|$|... theorem {{that the}} <b>conditional</b> <b>entropies</b> [...] and [...] are both non-negative. In the quantum case, however, both can be negative, ...|$|R
40|$|We derive entropic Bell inequalities from {{considering}} entropy Venn diagrams. These entropic inequalities, {{akin to the}} Braunstein-Caves inequalities, are violated for {{a quantum}} mechanical Einstein-Podolsky-Rosen pair, which implies that the <b>conditional</b> <b>entropies</b> of Bell variables must be negative in this case. This suggests that the satisfaction of entropic Bell inequalities {{is equivalent to the}} non-negativity of <b>conditional</b> <b>entropies</b> as a necessary condition for separability. Comment: 4 pages RevTeX, 2 figures. Minor revisions. To appear in Phys. Rev. ...|$|R
5000|$|The <b>{{conditional}}</b> <b>entropy</b> or conditional {{uncertainty of}} [...] given random variable [...] (also called the equivocation of [...] about [...] ) {{is the average}} <b>conditional</b> <b>entropy</b> over : ...|$|E
5000|$|The {{conditional}} quantum entropy is {{an entropy}} measure used in quantum information theory. It is a generalization of the <b>conditional</b> <b>entropy</b> of classical information theory. For a bipartite state , the <b>conditional</b> <b>entropy</b> is written , or , {{depending on the}} notation being used for the von Neumann entropy. The quantum <b>conditional</b> <b>entropy</b> was {{defined in terms of}} a conditional density operator [...] by Nicolas Cerf and Chris Adami, who showed that quantum conditional entropies can be negative, something that is forbidden in classical physics. The negativity of quantum <b>conditional</b> <b>entropy</b> is a sufficient criterion for quantum non-separability.|$|E
5000|$|... #Subtitle level 3: <b>Conditional</b> <b>entropy</b> of {{independent}} random variables ...|$|E
5000|$|... #Caption: Individual (H(X), H(Y)), joint (H(X, Y)), and <b>conditional</b> <b>entropies</b> {{for a pair}} of {{correlated}} subsystems X, Y with {{mutual information}} I(X; Y).|$|R
5000|$|However Abdallah and Plumbley (2010) showed its {{equivalence}} to the easier-to-understand form of {{the joint}} entropy minus the sum of <b>conditional</b> <b>entropies</b> via the following: ...|$|R
40|$|Recently a new quantum {{generalization}} of the Renyi divergence {{and the corresponding}} <b>conditional</b> Renyi <b>entropies</b> was proposed. Here we report on a surprising relation between <b>conditional</b> Renyi <b>entropies</b> based on this new generalization and <b>conditional</b> Renyi <b>entropies</b> based on the quantum relative Renyi entropy {{that was used in}} previous literature. Our result generalizes the well-known duality relation H(A|B) + H(A|C) = 0 of the <b>conditional</b> von Neumann <b>entropy</b> for tripartite pure states to Renyi entropies of two different kinds. As a direct application, we prove a collection of inequalities that relate different <b>conditional</b> Renyi <b>entropies</b> and derive a new entropic uncertainty relation. Comment: 10 pages, 1 figur...|$|R
5000|$|Because entropy can be {{conditioned}} on {{a random}} variable or on that random variable being a certain value, {{care should be}} taken not to confuse these two definitions of <b>conditional</b> <b>entropy,</b> the former of which is in more common use. A basic property of this form of <b>conditional</b> <b>entropy</b> is that: ...|$|E
5000|$|Joint entropy {{is used in}} the {{definition}} of <b>conditional</b> <b>entropy</b> ...|$|E
5000|$|... <b>{{conditional}}</b> <b>entropy,</b> mean conditional {{information content}}, average {{conditional information content}} H(X|Y) ...|$|E
25|$|These {{statements}} run {{parallel to}} classical intuition, except that quantum <b>conditional</b> <b>entropies</b> can be negative, and quantum mutual informations can exceed the classical bound of the marginal entropy.|$|R
40|$|We develop information-theoretic {{measures}} of spatial structure and pattern {{in more than}} one dimension. As is well known, the entropy density of a two-dimensional configuration can be efficiently and accurately estimated via a converging sequence of <b>conditional</b> <b>entropies.</b> We show that the manner in which these <b>conditional</b> <b>entropies</b> converge to their asymptotic value serves as a measure of global correlation and structure for spatial systems in any dimension. We compare and contrast entropy-convergence with mutual-information and structure-factor techniques for quantifying and detecting spatial structure. Comment: 11 pages, 5 figures, [URL]...|$|R
40|$|Abstract. In this paper, {{information}} theoretic cryptography {{is discussed}} based on <b>conditional</b> Rényi <b>entropies.</b> Our discussion focuses {{not only on}} cryptography {{but also on the}} definitions of <b>conditional</b> Rényi <b>entropies</b> and the related information theoretic inequalities. First, we revisit <b>conditional</b> Rényi <b>entropies,</b> and clarify what kind of properties are required and actually satisfied. Then, we propose security criteria based on Rényi entropies, which suggests us deep relations between (<b>conditional)</b> Rényi <b>entropies</b> and error probabilities by using several guessing strategies. Based on these results, unified proof of impossibility, namely, the lower bounds of key sizes is derived based on <b>conditional</b> Rényi <b>entropies.</b> Our model and lower bounds include the Shannon’s perfect secrecy, and the minentropy based encryption presented by Dodis, and Alimomeni and Safavi-Naini. Finally, new optimal symmetric key cryptography and almost optimal secret sharing schemes are proposed which achieve our lower bounds...|$|R
5000|$|The above {{definition}} is for discrete random variables {{and no more}} valid {{in the case of}} continuous random variables. The continuous version of discrete <b>conditional</b> <b>entropy</b> is called conditional differential (or continuous) entropy. Let [...] and [...] be a continuous random variables with a joint probability density function [...] The differential <b>conditional</b> <b>entropy</b> [...] is defined as ...|$|E
5000|$|... where I(A:B) is {{the quantum}} mutual {{information}} and S(B|A) is the quantum <b>conditional</b> <b>entropy.</b>|$|E
5000|$|By {{analogy with}} the {{classical}} <b>conditional</b> <b>entropy,</b> one defines the conditional quantum entropy as [...]|$|E
40|$|As a {{causality}} criterion {{we propose}} the <b>conditional</b> relative <b>entropy.</b> The relationship with information theoretic functionals mutual information and entropy is established. The <b>conditional</b> relative <b>entropy</b> criterion is compared with 3 well-established techniques for causality detection: ‘Sims’, ‘Geweke-Meese-Dent’ and ‘Granger’. It is {{shown that the}} <b>conditional</b> relative <b>entropy,</b> as opposed to these 3 criteria, is sensitive to non-linear causal relationships. All results are illustrated on real-world time series of human gait. status: publishe...|$|R
40|$|The aim of {{this paper}} is to show that the Tsallis-type (q-additive) entropic chain rule allows for a wider class of entropic functionals than {{previously}} thought. In particular, we point out that the ensuing entropy solutions (e. g., Tsallis entropy) can be determined uniquely only when one fixes the prescription for handling <b>conditional</b> <b>entropies.</b> By using the concept of Kolmogorov–Nagumo quasi-linear means, we prove this with the help of Darótzy’s mapping theorem. Our point is further illustrated with a number of explicit examples. Other salient issues, such as connections of <b>conditional</b> <b>entropies</b> with the de Finetti–Kolmogorov theorem for escort distributions and with Landsberg’s classification of non-extensive thermodynamic systems are also briefly discussed...|$|R
40|$|We {{propose a}} {{generalization}} of the quantum entropy power inequality involving <b>conditional</b> <b>entropies.</b> For the special case of Gaussian states, we give a proof based on perturbation theory for symplectic spectra. We discuss some implications for entanglement-assisted classical communication over additive bosonic noise channels. Comment: 27 pages, 4 figure...|$|R
5000|$|The {{differential}} analogies of entropy, joint entropy, <b>conditional</b> <b>entropy,</b> {{and mutual}} information {{are defined as}} follows: ...|$|E
50|$|In {{contrast}} to the <b>conditional</b> <b>entropy</b> for discrete random variables, the conditional differential entropy may be negative.|$|E
50|$|These {{definitions}} {{parallel the}} use of the classical joint entropy to define the <b>conditional</b> <b>entropy</b> and mutual information.|$|E
50|$|As {{with the}} {{classical}} Shannon entropy and its quantum generalization, the von Neumann entropy, one can define a conditional versions of min <b>entropy.</b> The <b>conditional</b> quantum min <b>entropy</b> is a one-shot, or conservative, analog of <b>conditional</b> quantum <b>entropy.</b>|$|R
2500|$|If [...] and , as {{is always}} {{the case for the}} {{classical}} Shannon entropy, this inequality has nothing to say. For the quantum entropy, on the other hand, {{it is quite possible that}} the <b>conditional</b> <b>entropies</b> satisfy [...] or [...] (but never both!). Then, in this [...] "highly quantum" [...] regime, this inequality provides additional information.|$|R
40|$|Information- Self {{information}}, Shannon’s <b>Entropy,</b> {{joint and}} <b>conditional</b> <b>entropies,</b> mutual information and their properties. Discrete Memory less channels: Classification of channels, calculation of channel capacity. Source Coding, and Channels Coding. Unique decipherable Codes, condition of Instantaneous codes, Average code word length, Kraft Inequality. Shannon’s Noiseless Coding Theorem. Construction of codes: Shannon Fano, Shannon Binary and Huffma...|$|R
