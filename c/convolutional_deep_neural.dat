26|5043|Public
50|$|DNNs are {{typically}} feedforward networks in which data {{flows from the}} input layer to the output layer without looping back. However, recurrent neural networks, in which data can flow in any direction, are also used, especially long short-term memory, for applications such as language modeling. <b>Convolutional</b> <b>deep</b> <b>neural</b> networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).|$|E
30|$|Various deep {{learning}} architectures such as deep neural networks, <b>convolutional</b> <b>deep</b> <b>neural</b> networks, deep belief networks, and recurrent neural networks {{have been applied}} to fields like computer vision, automatic speech recognition, natural language processing, audio recognition, and bioinformatics {{where they have been}} shown to produce state-of-the-art results on various tasks [5, 10].|$|E
30|$|In this paper, a <b>convolutional</b> <b>deep</b> <b>neural</b> {{network is}} {{executed}} {{by using a}} library called Caffe [20]. Table 1 lists the details for the deep learning implemented in this paper. In addition, Table 1 shows the recognition precision of a main object used in the experiment. Figure 3 shows {{the structure of the}} deep learning.|$|E
30|$|We {{propose a}} <b>deep</b> <b>convolutional</b> <b>neural</b> network that can {{identify}} Alzheimer’s disease and classify the current disease stage.|$|R
40|$|In this paper, {{we propose}} a <b>deep</b> <b>neural</b> network {{architecture}} for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling {{layer of the}} <b>deep</b> <b>convolutional</b> <b>neural</b> network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR- 10 and SVHN. The result suggests that ReNet is {{a viable alternative to}} the <b>deep</b> <b>convolutional</b> <b>neural</b> network, and that further investigation is needed...|$|R
5000|$|Waifu2x {{is officially}} {{described}} as [...] "Image Super-Resolution for Anime-styled art using <b>Deep</b> <b>Convolutional</b> <b>Neural</b> Networks. It also supports photos."The demo application {{can be found}} at http://waifu2x.udp.jp/ ...|$|R
40|$|In <b>convolutional</b> <b>deep</b> <b>neural</b> networks, {{receptive}} field (RF) size increases with hierarchical depth. When RF size approaches full {{coverage of the}} input image, different RF positions result in RFs with different specificity, as portions of the RF {{fall out of the}} input space. This leads to a departure from the convolutional concept of positional invariance and opens the possibility for complex forms of context specificity...|$|E
40|$|We train a deep {{convolutional}} {{neural network}} to accurately predict the energies and magnetizations of Ising model configurations, using both the traditional nearest-neighbour Hamiltonian, as well as a long-range screened Coulomb Hamiltonian. We demonstrate the capability of a <b>convolutional</b> <b>deep</b> <b>neural</b> network in predicting the nearest-neighbour energy of the 4 x 4 Ising model. Using its success at this task, we motivate the study of the larger 8 x 8 Ising model, showing that the deep neural network can learn the nearest-neighbour Ising Hamiltonian after only seeing a vanishingly small fraction of configuration space. Additionally, we show that the neural network has learned both the energy and magnetization operators with sufficient accuracy to replicate the low-temperature Ising phase transition. Finally, we teach the <b>convolutional</b> <b>deep</b> <b>neural</b> network to accurately predict a long-range interaction through a screened Coulomb Hamiltonian. In this case, the benefits of the neural network become apparent; it is able to make predictions {{with a high degree of}} accuracy, 1600 times faster than a CUDA-optimized "exact" calculation...|$|E
40|$|Fully <b>convolutional</b> <b>deep</b> <b>neural</b> {{networks}} {{carry out}} excellent potential for fast and accurate image segmentation. One {{of the main}} challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often {{much lower than the}} number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index {{to address the issue of}} data imbalance and achieve much better trade-off between precision and recall in training 3 D fully <b>convolutional</b> <b>deep</b> <b>neural</b> networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F 2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks...|$|E
40|$|Knowledge {{transfer}} is widely {{held to be}} a primary mechanism that enables humans to quickly learn new complex concepts when given only small training sets. In this paper, we apply knowledge transfer to <b>deep</b> <b>convolutional</b> <b>neural</b> nets, which we argue are particularly well suited for knowledge transfer. Our initial results demonstrate that components of a trained <b>deep</b> <b>convolutional</b> <b>neural</b> net can constructively transfer information to another such net. Furthermore, this {{transfer is}} completed {{in such a way}} that one can envision creating a net that could learn new concepts throughout its lifetime...|$|R
30|$|For {{the first}} variant, we {{implemented}} an ensemble of four <b>deep</b> <b>convolutional</b> <b>neural</b> networks: M_ 1, M_ 2, M_ 3 and M_ 4. We will {{refer to this}} model as E_ 1.|$|R
40|$|We {{present a}} {{framework}} for vision-based model predictive control (MPC) for the task of aggressive, high-speed autonomous driving. Our approach uses <b>deep</b> <b>convolutional</b> <b>neural</b> networks to predict cost functions from input video which are directly suitable for online trajectory optimization with MPC. We demonstrate the method in a high speed autonomous driving scenario, where we use a single monocular camera and a <b>deep</b> <b>convolutional</b> <b>neural</b> network to predict a cost map of the track {{in front of the}} vehicle. Results are demonstrated on a 1 : 5 scale autonomous vehicle given the task of high speed, aggressive driving. Comment: 11 pages, 7 figure...|$|R
40|$|We {{present the}} IBM speech {{activity}} detection {{system that was}} elded in the phase 2 evaluation of the DARPA RATS (robust automatic transcription of speech) program. Key ingredients of the system are: multi-pass HMM Viterbi segmentation, fusion of multiple feature streams, le-based and speech-based nor-malization schemes, the use of regular and <b>convolutional</b> <b>deep</b> <b>neural</b> networks, and model fusion through frame-level score combination of channel-dependent models. These techniques were instrumental in achieving a 1. 4 % equal error rate on the RATS phase 2 evaluation data. Index Terms: speech activity detection, robust speech recogni-tio...|$|E
40|$|Separation of {{competing}} speech {{is a key}} challenge in signal processing and a feat routinely performed by the human auditory brain. A long standing benchmark of the spectrogram approach to source separation {{is known as the}} ideal binary mask. Here, we train a <b>convolutional</b> <b>deep</b> <b>neural</b> network, on a two-speaker cocktail party problem, to make probabilistic predictions about binary masks. Our results approach ideal binary mask performance, illustrating that relatively simple deep neural networks are capable of robust binary mask prediction. We also illustrate the trade-off between prediction statistics and separation quality...|$|E
40|$|In {{the process}} of recording, storage and {{transmission}} of time-domain audio signals, errors may be introduced {{that are difficult to}} correct in an unsupervised way. Here, we train a <b>convolutional</b> <b>deep</b> <b>neural</b> network to re-synthesize input time-domain speech signals at its output layer. We then use this abstract transformation, which we call a deep transform (DT), to perform probabilistic re-synthesis on further speech (of the same speaker) which has been degraded. Using the convolutive DT, we demonstrate the recovery of speech audio that has been subject to extreme degradation. This approach may be useful for correction of errors in communications devices...|$|E
40|$|Dissertation supervisor: Dr. Tony X. Han. Includes vita. Significant {{advancement}} {{of research on}} image classification and object detection has been achieved in the past decade. <b>Deep</b> <b>convolutional</b> <b>neural</b> networks have exhibited superior performance in many visual recognition tasks including image classification, object detection, and scene labeling, due to their large learning capacity and resistance to overfit. However, learning a robust deep CNN model for object recognition is still quite challenging because image classification and object detection is a severely unbalanced large-scale problem. In this dissertation, we aim at improving the performance of image classification and object detection algorithms {{by taking advantage of}} <b>deep</b> <b>convolutional</b> <b>neural</b> networks by utilizing the following strategies: We introduce <b>Deep</b> <b>Neural</b> Pattern, a local feature densely extracted from an image with arbitrary resolution using a well trained <b>deep</b> <b>convolutional</b> <b>neural</b> network. We propose a latent CNN framework, which will automatically select the most discriminate region in the image to reduce the effect of irrelevant regions. We also develop a new combination scheme for multiple CNNs via Latent Model Ensemble to overcome the local minima problem of CNNs. In addition, a weakly supervised CNN framework, referred to as Multiple Instance Learning Convolutional Neural Networks is developed to alleviate strict label requirements. Finally, a novel residual-network architecture, Residual networks of Residual networks, is constructed to improve the optimization ability of very <b>deep</b> <b>convolutional</b> <b>neural</b> networks. All the proposed algorithms are validated by thorough experiments and have shown solid accuracy on large scale object detection and recognition benchmarks. Includes bibliographical references (pages 105 - 119) ...|$|R
3000|$|... where z and a are {{the input}} and output of {{activation}} function, respectively. Experiments in [41] show that <b>deep</b> <b>convolutional</b> <b>neural</b> networks with ReLUs train several times faster than their equivalents with tanh units.|$|R
40|$|We {{introduce}} a new <b>deep</b> <b>convolutional</b> <b>neural</b> network, CrescendoNet, by stacking simple building blocks without residual connections. Each Crescendo block contains independent convolution paths with increased depths. The numbers of convolution layers and parameters are only increased linearly in Crescendo blocks. In experiments, CrescendoNet with only 15 layers outperforms almost all networks without residual connections on benchmark datasets, CIFAR 10, CIFAR 100, and SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with 15 layers and 4. 1 M parameters can match the performance of DenseNet-BC with 250 layers and 15. 3 M parameters. CrescendoNet provides {{a new way to}} construct high performance <b>deep</b> <b>convolutional</b> <b>neural</b> networks without residual connections. Moreover, through investigating the behavior and performance of subnetworks in CrescendoNet, we note that the high performance of CrescendoNet may come from its implicit ensemble behavior, which differs from the FractalNet that is also a <b>deep</b> <b>convolutional</b> <b>neural</b> network without residual connections. Furthermore, the independence between paths in CrescendoNet allows us to {{introduce a}} new path-wise training procedure, which can reduce the memory needed for training...|$|R
40|$|<b>Convolutional</b> <b>deep</b> <b>neural</b> {{networks}} (DNN) are {{state of}} the art in many engineering problems but have not yet addressed the issue of how to deal with complex spectrograms. Here, we use circular statistics to provide a convenient probabilistic estimate of spectrogram phase in a complex convolutional DNN. In a typical cocktail party source separation scenario, we trained a convolutional DNN to re-synthesize the complex spectrograms of two source speech signals given a complex spectrogram of the monaural mixture - a discriminative deep transform (DT). We then used this complex convolutional DT to obtain probabilistic estimates of the magnitude and phase components of the source spectrograms. Our separation results are on a par with equivalent binary-mask based non-complex separation approaches...|$|E
40|$|Audio source {{separation}} {{is a difficult}} machine learning problem and performance is measured by comparing extracted signals with the component source signals. However, if {{separation is}} motivated by {{the ultimate goal of}} re-mixing then complete separation is not necessary and hence separation difficulty and separation quality are dependent {{on the nature of the}} re-mix. Here, we use a <b>convolutional</b> <b>deep</b> <b>neural</b> network (DNN), trained to estimate 'ideal' binary masks for separating voice from music, to perform re-mixing of the vocal balance by operating directly on the individual magnitude components of the musical mixture spectrogram. Our results demonstrate that small changes in vocal gain may be applied with very little distortion to the ultimate re-mix. Our method may be useful for re-mixing existing mixes...|$|E
40|$|Experiments in {{particle}} physics produce enormous quantities {{of data that}} must be analyzed and interpreted by teams of physicists. This analysis is often exploratory, where scientists are unable to enumerate the possible types of signal prior to performing the experiment. Thus, tools for summarizing, clustering, visualizing and classifying high-dimensional data are essential. In this work, we show that meaningful physical content can be revealed by transforming the raw data into a learned high-level representation using deep neural networks, with measurements taken at the Daya Bay Neutrino Experiment as a case study. We further show how <b>convolutional</b> <b>deep</b> <b>neural</b> networks can provide an effective classification filter with greater than 97 % accuracy across different classes of physics events, significantly better than other machine learning approaches...|$|E
40|$|By {{the reason}} of the {{variability}} of light and pedestrians’ appearance, {{it is hard for}} a camera to obtain a clear human figure. Person re-identification with different cameras is a difficult visual recognition task. In this paper, a novel approach called attribute learning based on distributed <b>deep</b> <b>convolutional</b> <b>neural</b> network model is proposed to address person re-identification task. It shows how attributes, namely the mid-level medium between classes and features, are obtained automatically and how they are employed to re-identify person with semantics when an author-topic model is used to mapping category. Besides, considering the ability to operate on raw pixel input without the need to design special features, <b>deep</b> <b>convolutional</b> <b>neural</b> network is employed to generate features without supervision for attributes learning model. To overcome the model’s weakness in computing speed, parallelized implementations such as distributed parameter manipulation and attributes learning are employed in attribute learning based on distributed <b>deep</b> <b>convolutional</b> <b>neural</b> network model. Experiments show that the proposed approach achieves state-of-the-art recognition performance in the VIPeR data set and is with a good semantic explanation...|$|R
30|$|For {{the second}} variant, we {{implemented}} an ensemble system of five <b>deep</b> <b>convolutional</b> <b>neural</b> networks: M_ 1, M_ 2, M_ 3, M_ 4 and M_ 5. We will {{refer to this}} model as E_ 2.|$|R
3000|$|<b>Deep</b> <b>convolutional</b> <b>neural</b> {{networks}} <b>Deep</b> CNNs are architectures {{that try}} to exploit the spatial structure of input information [12]. They have been used with great success in various applications, including image analysis, vision, object and emotion recognition. The most successful CNN was used for classifying millions of images in 1000 classes [21].|$|R
40|$|This study {{presents}} a novel end-to-end architecture that learns hierarchical representations from raw EEG data using fully <b>convolutional</b> <b>deep</b> <b>neural</b> networks {{for the task}} of neonatal seizure detection. The deep neural network acts as both feature extractor and classifier, allowing for end-to-end optimization of the seizure detector. The designed system is evaluated on a large dataset of continuous unedited multi-channel neonatal EEG totaling 835 hours and comprising of 1389 seizures. The proposed deep architecture, with sample-level filters, achieves an accuracy that {{is comparable to the}} state-of-the-art SVM-based neonatal seizure detector, which operates on a set of carefully designed hand-crafted features. The fully convolutional architecture allows for the localization of EEG waveforms and patterns that result in high seizure probabilities for further clinical examination. Comment: IEEE International Workshop on Machine Learning for Signal Processin...|$|E
40|$|We {{present a}} {{practical}} framework to automatically de-tect shadows {{in real world}} scenes from a single photograph. Previous works on shadow detection {{put a lot of}} effort in designing shadow variant and invariant hand-crafted fea-tures. In contrast, our framework automatically learns the most relevant features in a supervised manner using mul-tiple <b>convolutional</b> <b>deep</b> <b>neural</b> networks (ConvNets). The 7 -layer network architecture of each ConvNet consists of alternating convolution and sub-sampling layers. The pro-posed framework learns features at the super-pixel level and along the object boundaries. In both cases, features are ex-tracted using a context aware window centered at interest points. The predicted posteriors based on the learned fea-tures are fed to a conditional random field model to gen-erate smooth shadow contours. Our proposed framework consistently performed better than the state-of-the-art on all major shadow databases collected under a variety of con-ditions. 1...|$|E
40|$|We {{present a}} {{framework}} to automatically detect and remove shadows {{in real world}} scenes from a single image. Previous works on shadow detection {{put a lot of}} effort in designing shadow variant and invariant hand-crafted features. In contrast, our framework automatically learns the most relevant features in a supervised manner using multiple <b>convolutional</b> <b>deep</b> <b>neural</b> networks (ConvNets). The features are learned at the super-pixel level and along the dominant boundaries in the image. The predicted posteriors based on the learned features are fed to a conditional random field model to generate smooth shadow masks. Using the detected shadow masks, we propose a Bayesian formulation to accurately extract shadow matte and subsequently remove shadows. The Bayesian formulation is based on a novel model which accurately models the shadow generation process in the umbra and penumbra regions. The model parameters are efficiently estimated using an iterative optimization procedure. Our proposed framework consistently performed better than the state-of-the-art on all major shadow databases collected under a variety of conditions...|$|E
40|$|This paper {{addresses}} {{the challenge of}} establishing a bridge between <b>deep</b> <b>convolutional</b> <b>neural</b> networks and conventional object detection frameworks for accurate and efficient generic object detection. We introduce Dense Neural Patterns, short for DNPs, which are dense local features derived from discriminatively trained <b>deep</b> <b>convolutional</b> <b>neural</b> networks. DNPs can be easily plugged into conventional detection frameworks {{in the same way}} as other dense local features(like HOG or LBP). The effectiveness of the proposed approach is demonstrated with the Regionlets object detection framework. It achieved 46. 1 % mean average precision on the PASCAL VOC 2007 dataset, and 44. 1 % on the PASCAL VOC 2010 dataset, which dramatically improves the original Regionlets approach without DNPs...|$|R
40|$|A {{multiscale}} input {{strategy for}} multiview deep learning is proposed for supervised multispectral land-use classification {{and it is}} validated on a well-known dataset. The hypothesis that simultaneous multiscale views can improve compositionbased inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation {{with the aid of}} convolutional layers to shift the burden of feature determination from hand-engineering to a <b>deep</b> <b>convolutional</b> <b>neural</b> network. This allows the classifier to obtain problemspecific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trades optimality for generality. A heuristic approach to the optimization of the <b>deep</b> <b>convolutional</b> <b>neural</b> network hyperparameters is used, based on empirical performance evidence. It is shown that a single <b>deep</b> <b>convolutional</b> <b>neural</b> network can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced dataset where the 93. 48 % accuracy of multiview deep learning outperforms the 85. 37 % accuracy of SIFT-based methods and the 90. 26 % accuracy of unsupervised feature learning. National Research Foundation (NRF) of South Africa[URL]...|$|R
40|$|In this paper, {{we propose}} a <b>deep</b> <b>convolutional</b> <b>neural</b> network {{solution}} {{to the analysis of}} image data for the detection of rail surface defects. The images are obtained from many hours of automated video recordings. This huge amount of data makes it impossible to manually inspect the images and detect rail surface defects. Therefore, automated detection of rail defects can help to save time and costs, and to ensure rail transportation safety. However, one major challenge is that the extraction of suitable features for detection of rail surface defects is a non-trivial and difficult task. Therefore, we propose to use convolutional neural networks as a viable technique for feature learning. <b>Deep</b> <b>convolutional</b> <b>neural</b> networks have recently been applied to a number of similar domains with success. We compare the results of different network architectures characterized by different sizes and activation functions. In this way, we explore the efficiency of the proposed <b>deep</b> <b>convolutional</b> <b>neural</b> network for detection and classification. The experimental results are promising and demonstrate the capability of the proposed approach. Accepted Author ManuscriptHybrid and Distributed Systems and ControlRailway EngineeringIntelligent Control & Robotic...|$|R
40|$|The {{increasing}} demand for high image quality in mobile devices brings forth {{the need for}} better computational enhancement techniques, and image denoising in particular. At the same time, the images captured by these devices can be categorized into a small set of semantic classes. However simple, this observation has not been exploited in image denoising until now. In this paper, we demonstrate how the reconstruction quality improves when a denoiser {{is aware of the}} type of content in the image. To this end, we first propose a new fully <b>convolutional</b> <b>deep</b> <b>neural</b> network architecture which is simple yet powerful as it achieves state-of-the-art performance even without being class-aware. We further show that a significant boost in performance of up to $ 0. 4 $ dB PSNR can be achieved by making our network class-aware, namely, by fine-tuning it for images belonging to a specific semantic class. Relying on the hugely successful existing image classifiers, this research advocates for using a class-aware approach in all image enhancement tasks...|$|E
30|$|There {{are other}} {{variants}} of recently proposed SSIM called multi-scale structural similarity (MS-SSIM) [19]. Also, {{there are other}} image quality measurement metrics such as image quality index (IQI) [18], Normalized Correlation [11], Sum of Absolute Differences, and many others [15]. A possible future work will test our proposed method by using all of these measures. Recently, BM 3 D has been extended for video denoising. Also there is BM 4 D [14]. Since our idea {{is to change the}} core of BMxD in general, a study of assessing our method in all BMxD versions will be considered. In addition, there are recent works such as finding visually similar images using a <b>convolutional</b> <b>deep</b> <b>neural</b> network [24], {{it would be interesting to}} study if finding a similarity between images with deep neural networks helps in forming a quality metric that can eventually be used in a Wiener filter. Also, applying our improved denoising method could help in reducing noise for applications such as text extraction from complex background images [25].|$|E
40|$|The {{state-of-the-art}} {{methods used}} for relation classification are primarily based on statistical ma-chine learning, and their performance strongly {{depends on the}} quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language process-ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a <b>convolutional</b> <b>deep</b> <b>neural</b> network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings 1. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the fea-tures are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods. ...|$|E
40|$|The fully {{connected}} layers of a <b>deep</b> <b>convolutional</b> <b>neural</b> network typically contain over 90 % {{of the network}} parameters, and consume {{the majority of the}} memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating <b>deep</b> <b>neural</b> networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all {{fully connected}} layers in a <b>deep</b> <b>convolutional</b> <b>neural</b> network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named <b>deep</b> fried <b>convolutional</b> networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance. Comment: svd experiments include...|$|R
30|$|Facial image {{recognition}} {{is one of}} the most challenging tasks in surveillance systems due to problems such as low quality of images and significant variance in pose, expression, illumination, and resolution. Although a number of face recognition algorithms have been proposed in the literature, face recognition in an unconstrained environment still presents low accuracy. Recently, <b>deep</b> <b>convolutional</b> <b>neural</b> network (DCNN)-based techniques have shown excellent results in face recognition by discovering intricate features in large data-sets. However, DCNN-based models struggle to suggest uncertainty in the prediction of the output class which can be useful to reduce false positives. In this study, Bayesian <b>deep</b> <b>convolutional</b> <b>neural</b> network (B-DCNN) is employed to represent model uncertainty to improve the accuracy of facial {{image recognition}}.|$|R
40|$|This thesis tackles {{fine-grained}} image recognition, {{the task}} of sub-category or species classification. It explores general methods to improve fine-grained image classification {{including the use of}} generative models and <b>deep</b> <b>convolutional</b> <b>neural</b> networks leading to novel models such as a Mixture of <b>deep</b> convolution <b>neural</b> networks. This work led to 9 peer reviewed publications and a Best Paper Award...|$|R
