286|2324|Public
5000|$|... 3 Compute Nodes: Dell R175 {{each with}} 32 CPU cores/ <b>compute</b> <b>node</b> (96 in total), 128GB RAM/ <b>compute</b> <b>node</b> (384GB in total), 600GB Secondary Memory/ <b>compute</b> <b>node</b> (1.8TB in total) ...|$|E
5000|$|... the compute {{elements}} run <b>Compute</b> <b>Node</b> Linux (CNL) (which is {{a customized}} Linux kernel) ...|$|E
5000|$|Given {{that modern}} massively {{parallel}} supercomputers typically separate computations {{from other services}} by using multiple types of nodes, they usually run different operating systems on different nodes, e.g., using a small and efficient lightweight kernel such as <b>Compute</b> <b>Node</b> Kernel (CNK) or <b>Compute</b> <b>Node</b> Linux (CNL) on compute nodes, but a larger system such as a Linux-derivative on server and input/output (I/O) nodes.|$|E
40|$|Abstract—Cloud c o m p u t i n g is {{the set of}} {{distributed}} <b>computing</b> <b>nodes.</b> The {{distribution of}} virtual machine (VM) images {{to a set of}} distributed <b>compute</b> <b>nodes</b> based on priority value in a Cross- Cloud computing environment is main issue considered in this paper. This paper will be dealing with the problem o f scheduling virtual machine (VM) images to a set of distributed <b>compute</b> <b>nodes</b> in a Cross-Cloud computing environment...|$|R
50|$|Sequoia is a Blue Gene/Q design, {{based on}} {{previous}} Blue Gene designs. It consists of 96 racks containing 98,304 <b>compute</b> <b>nodes,</b> i.e., 1024 per rack. The <b>compute</b> <b>nodes</b> are 16-core A2 processor chips with 16 GB of DDR3 memory each. Thus, the system contains a total of 96·1024·16 = 1,572,864 processor cores with 1.5 PiB memory. It covers an area of about 3000 sq ft. The <b>compute</b> <b>nodes</b> are interconnected in a 5-dimensional torus topology.|$|R
30|$|In-memory {{querying}} and analytics {{needed to}} reduce query response times {{and execution of}} analytics operations by caching large volumes of data in the <b>computing</b> <b>node</b> RAMs and issuing queries and other operation in parallel on the main memory of <b>computing</b> <b>nodes.</b>|$|R
50|$|<b>Compute</b> <b>Node</b> Kernel (CNK) is the node level {{operating}} {{system for the}} IBM Blue Gene series of supercomputers.|$|E
50|$|The XK6 {{runs the}} Cray Linux Environment. This {{incorporates}} SUSE Linux Enterprise Server and Cray's <b>Compute</b> <b>Node</b> Linux.|$|E
5000|$|OpenMP, i.e. using OpenMP {{to manage}} {{multiple}} cores or processors {{in a single}} <b>compute</b> <b>node,</b> but not MPI.|$|E
40|$|Performance {{analysis}} of the ABySS genome sequence assembler (ABYSS-P) executing on the K computer with up to 8192 <b>compute</b> <b>nodes</b> is described which identified issues that limited scalability to less than 1024 <b>compute</b> <b>nodes</b> and required prohibitive message buffer memory with 16384 or more <b>compute</b> <b>nodes.</b> The open-source Scalasca toolset was employed to analyse executions, revealing the impact of massive amounts of MPI point-to-point communication used particularly for master/worker process coordination, and inefficient parallel file operations that manifest as waiting time at later MPI collective synchronisations and communications. Initial remediation via use of collective communication operations and alternate strategies for parallel file handling show large performance and scalability improvements, with partial executions validated on the full 82, 944 <b>compute</b> <b>nodes</b> of the K computer...|$|R
40|$|Abstract This paper {{describes}} an {{implementation of a}} 5 -term GFSR (Generalized Feedback Shift Register) random number generator that generates mutually uncorrelated random number sequences on <b>computing</b> <b>nodes</b> of networked personal computers (PCs). As GFSR generators have extremely long periods and their autocorrelation functions are known, {{it is possible for}} the generator on each <b>computing</b> <b>node</b> to generate a subsequence uncorrelated with each other if initial terms of the generator are properly set up on each <b>computing</b> <b>node.</b> Some preliminary results of simple Monte Carlo simulations are also shown...|$|R
50|$|The {{previous}} version, MareNostrum 3, {{consisted of}} 3,056 IBM DataPlex DX360M4 <b>compute</b> <b>nodes,</b> {{for a total}} of 48,896 physical Intel Sandy Bridge cores running at 2.6 Ghz, and 84 Xeon Phi 5110P in 42 nodes. MareNostrum 3 had 36 racks dedicated to calculations. In total, each rack had 1,344 cores and 2,688 GB of memory. Each IBM iDataPlex Compute rack was composed of 84 IBM iDataPlex dx360 M4 <b>compute</b> <b>nodes</b> and 4 Mellanox 36-port Managed FDR10 IB Switches. dx360 M4 <b>compute</b> <b>nodes</b> were grouped into a 2U Chassis, having two columns of 42 2U Chassis.|$|R
50|$|Jaguar's XT5 {{partition}} contains 18,688 compute nodes {{in addition}} to dedicated login/service nodes. Each XT5 <b>compute</b> <b>node</b> contains dual hex-core AMD Opteron 2435 (Istanbul) processors and 16 GB of memory. Jaguar's XT4 partition contains 7,832 compute nodes {{in addition to}} dedicated login/service nodes. Each XT4 <b>compute</b> <b>node</b> contains a quad-core AMD Opteron 1354 (Budapest) processor and 8 GB of memory. Total combined memory amounts to over 360 terabytes (TB).|$|E
50|$|The XE6 {{runs the}} Cray Linux Environment version 3. This {{incorporates}} SUSE Linux Enterprise Server and Cray's <b>Compute</b> <b>Node</b> Linux.|$|E
5000|$|The XT5h's {{operating}} system is UNICOS/lc, {{a combination of}} SUSE Linux Enterprise Server and Sandia's Catamount or Cray's <b>Compute</b> <b>Node</b> Linux.|$|E
40|$|International audienceNowadays, high {{performance}} applications exploit multiple level architectures, {{due to the}} presence of hardware accelerators like GPUs inside each <b>computing</b> <b>node.</b> Data transfers occur at two different levels: inside the <b>computing</b> <b>node</b> between the CPU and the accelerators and between <b>computing</b> <b>nodes.</b> We consider the case where intra-node parallelism is handled with HMPP compiler directives and message-passing programming with MPI is used to program inter-node communications. This way of programming on such an heterogeneous architecture is costly and error-prone. In this paper, we specifically demonstrate the transformation of HMPP programs designed to exploit a single <b>computing</b> <b>node</b> equipped with a GPU into an heterogeneous HMPP+MPI exploiting multiple GPUs located on different <b>computing</b> <b>nodes.</b> The STEP tool focuses on generating communications combining both powerful static analyses and runtime execution to reduce the volume of communications. Our source-to-source transformation is implemented inside the PIPS workbench. We detail the generated source program of the jacobi kernel and show that the execution times and speedups are encouraging. At last we give some directions for improvement of the too...|$|R
40|$|Blocking coordinated {{checkpointing}} is {{a well-known}} method for achieving fault tolerance in cluster computing systems. In this work, we introduce a new approach for blocking coordinated checkpointing using two-level checkpointing. The first level of checkpointing is local checkpointing, and <b>computing</b> <b>nodes</b> save the checkpoints in local disk. If a transient failure occurs in the <b>computing</b> <b>node,</b> the process can recover from local disk. Second level of checkpointing is global checkpointing and <b>computing</b> <b>nodes</b> send their checkpoints to highly reliable global stable storage. If a permanent failure occurs in the <b>computing</b> <b>node,</b> {{it can not be}} used and the process can recover from global storage in a new <b>computing</b> <b>node.</b> Local checkpoints are taken more frequently than global checkpoints. Also, in the end of each local checkpointing interval, the system determines the expected recovery time in the case of permanent failure and adaptively takes a global checkpoint, or skips. Experimental results show that average execution time of NAS-BT application is significantly reduced by using the proposed method. Maximum reduction of execution time of this application is 38 %...|$|R
40|$|The Active Nodal Task Seeking (ANTS) {{approach}} {{to the design of}} multicomputer systems is named for its basic component: an Active Nodal Task-Seeker (ANT). In this system, there is no load balancing or load sharing, instead, each ANT <b>computing</b> <b>node</b> is actively finding out how it can contribute to the execution of the needed tasks. A run-time partition is established such that some of the ANT <b>computing</b> <b>nodes</b> are under exhaustive diagnosis at any given time. An ANTS multicomputer system can achieve a mean time to failure of more than 20 years with just 8 <b>computing</b> <b>nodes</b> and 3 buses, while the minimum requirements are 3 <b>computing</b> <b>nodes</b> and 1 bus, and with a worst case <b>computing</b> <b>node</b> failure rate of 5 Θ 10 Γ 4 per hour. In this final report, we present the many findings during the project duration. First, the first version of ANTS kernel has been implemented on a network of UNIX workstations. Employing TCP/IP protocol for communication via Ethernet LAN. The second version of ANTS [...] ...|$|R
50|$|The XT5 family run the Cray Linux Environment, {{formerly}} known as UNICOS/lc. This incorporates SUSE Linux Enterprise Server and Cray's <b>Compute</b> <b>Node</b> Linux.|$|E
5000|$|Server <b>compute</b> <b>node</b> designs {{included}} one for Intel processors {{and one for}} AMD processors. In 2013, Calxeda contributed a design with ARM architecture processors.|$|E
5000|$|... {{a shared}} memory <b>compute</b> <b>node</b> (14 {{internal}} NUMA nodes) with 112 Intel Sandy Bridge processor cores, 2 Intel Xeon Phi 5110P coprocessors and 1.7 TB of memory.|$|E
30|$|We {{evaluated}} scalability of read mapping phase {{by increasing}} number of <b>computing</b> <b>nodes</b> for Spark from 1 to 5. Figure 10 clearly shows that mapping time decreases as number of <b>computing</b> <b>nodes</b> increases. Therefore, {{we can say that}} StreamAligner is highly scalable and can align reads more efficiently as cluster size increases.|$|R
5000|$|<b>Compute</b> <b>nodes</b> are disk-less. I/O {{functionalities}} {{are provided}} by I/O nodes.|$|R
5000|$|... #Caption: The Cyclops64 {{architecture}} {{will contain}} {{many hundreds of}} <b>computing</b> <b>nodes</b> ...|$|R
50|$|One of {{the first}} nCUBE {{machines}} to be released was the nCUBE 10 of late 1985. It was originally called NCUBE/ten but the name morphed over time. These {{were based on a}} set of custom chips, where each <b>compute</b> <b>node</b> had a processor chip with 32-bit ALU, a 64-bit IEEE 754 FPU, special communication instructions, and 128 KB of RAM. A node delivered 2 MIPS, 500 kiloFLOPS (32-bit single precision), or 300 kiloFLOPS (64-bit double precision). There were 64 nodes per board. The host board, based on an Intel 80286, ran Axis, a custom Unix-like operating system, and each <b>compute</b> <b>node</b> ran a 4KB kernel, Vertex.|$|E
50|$|A modern microserver {{typically}} offers medium-high {{performance at}} high packaging densities, allowing very small <b>compute</b> <b>node</b> form factors. This {{can result in}} high energy efficiency (operations per Watt), typically better than that of highest single-thread performance processors.|$|E
5000|$|Cascade Supercomputer - Theoretical peak {{processing}} speed of 3.4 petaflops. 1440 compute nodes with conventional Xeon processors plus Intel Xeon Phi [...] "MIC" [...] accelerators. 128 GB memory per <b>compute</b> <b>node.</b> 2.7 petabyte shared parallel filesystem (60 gigabytes per second read/write) ...|$|E
5000|$|... #Caption: The Cray XT3 {{supercomputer}} {{which uses}} Catamount on its <b>compute</b> <b>nodes</b> ...|$|R
40|$|This book {{discusses}} {{questions of}} numerical solutions of applied problems on parallel computing systems. Nowadays, engineering and scientific computations {{are carried out}} on parallel computing systems, which provide parallel data processing on a few <b>computing</b> <b>nodes.</b> In constructing computational algorithms, mathematical problems are separated in relatively independent subproblems in order to solve them on a single <b>computing</b> <b>node...</b>|$|R
30|$|For the MMR algorithm, {{since the}} base {{learners}} which process {{part of the}} original datasets work in one single machine sequentially, in the next step, we plan to parallelize this step and distribute the computation to more <b>computing</b> <b>nodes</b> for the sake of increasing the computational efficiency. Moreover, we used the same algorithm: AdaBoost.M 1 for all the <b>computing</b> <b>nodes</b> in this work. We intend to use different algorithms on different <b>computing</b> <b>nodes</b> to increase the accuracy further. The reason is that Meta-learning belongs to the ensemble learning paradigm of machine learning and the more diverse the base learners are, the higher accuracy could be expected.|$|R
50|$|The compute nodes of the Blue Gene {{family of}} {{supercomputers}} run <b>Compute</b> <b>Node</b> Kernel (CNK), a lightweight kernel {{that runs on}} each node and supports one application running for one user on that node. To maximize operating efficiency, the design of CNK was kept simple and minimal. It was implemented in about 5,000 lines of C++ code. Physical memory is statically mapped and the CNK neither needs nor provides scheduling or context switching, given that at each point it runs one application for one user. By not allowing virtual memory or multi-tasking, the design of CNK aimed to devote as many cycles as possible to application processing. CNK does not even implement file input/output (I/O) on the <b>compute</b> <b>node,</b> but delegates that to dedicated I/O nodes.|$|E
50|$|The CX1000-SM and CX1000-SC nodes can be {{used for}} cluster computing, but they are {{designed}} for scale-up Symmetric Multi-Processing (SMP). When used for cluster computing, the CX1000-SM node is intended to be the master (service) node, although it can instead be a <b>compute</b> <b>node.</b> Similarly, the CX1000-SC node, when used for cluster computing, is intended to be a <b>compute</b> <b>node,</b> but can instead act as the master (service) node. Either or both the CX1000-SC and/or CX1000-SM nodes can be deployed in a HPC cluster. The CX1000-SM and CX1000-SC nodes, when used for SMP, are connected by a cache-coherency interconnect which is a built-in subassembly of the CX1000-SM and CX1000-SC nodes, rather than a standalone device, and is called the Drawer Interconnect Switch in Cray literature. The Drawer Interconnect Switch uses the Intel QuickPath Interconnect technology.|$|E
5000|$|The compute nodes of the Blue Gene {{family of}} {{supercomputers}} run CNK (for <b>Compute</b> <b>Node</b> Kernel), a lightweight kernel {{that runs on}} each node and supports a single application running for a single user on that node. For the sake of efficient operation, the design of CNK was kept simple and minimal, and it was implemented in about 5,000 lines of C++ code. Physical memory is statically mapped and the CNK neither needs nor provides scheduling or context switching, given that at each point it runs a single application for a single user. By not allowing virtual memory or multi-tasking, the design of CNK aimed to devote as many cycles as possible to application processing. CNK does not even implement file I/O on the <b>compute</b> <b>node,</b> but delegates that to dedicated I/O nodes.|$|E
50|$|Thinking Machines Corporation {{built the}} Connection Machine {{employing}} hypercube topology for its <b>compute</b> <b>nodes.</b>|$|R
25|$|The University of Tokyo's Information Technology Center in Kashiwa, Chiba began {{operating}} Oakleaf-FX in April 2012. This supercomputer is a Fujitsu PRIMEHPC FX10 (a commercial {{version of}} the K computer) configured with 4,800 <b>compute</b> <b>nodes</b> for a peak performance of 1.13PFLOPS. Each of the <b>compute</b> <b>nodes</b> is a SPARC64 IXfx processor connected to other nodes via a six-dimensional mesh/torus interconnect.|$|R
30|$|We {{evaluated}} {{our system}} on Cooley, a visualization cluster located at Argonne National Laboratory [45]. Cooley has 126 <b>compute</b> <b>nodes,</b> where each node consists of 12 cores (two 2.4 GHz Intel Haswell CPUs, each with 6 cores). Moreover, each node has 384 GB of memory for large-scale data visualization and analysis. The <b>compute</b> <b>nodes</b> are connected with FDR InfiniBand for high-performance communication.|$|R
