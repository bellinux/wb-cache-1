1399|1462|Public
5|$|The {{input to}} a <b>clustering</b> <b>problem</b> {{consists}} {{of a set of}} points. A cluster is any proper subset of the points, and a hierarchical clustering is a maximal family of clusters with the property that any two clusters in the family are either nested or disjoint.|$|E
5|$|Known {{methods for}} {{repeatedly}} finding the closest pair of clusters in a dynamic set of clusters either require superlinear space {{to maintain a}} data structure that can find closest pairs quickly, or they take greater than linear time to find each closest pair. The nearest-neighbor chain algorithm uses a smaller {{amount of time and}} space than the greedy algorithm by merging pairs of clusters in a different order. In this way, it avoids the problem of repeatedly finding closest pairs. Nevertheless, for many types of <b>clustering</b> <b>problem,</b> it can be guaranteed {{to come up with the}} same hierarchical clustering as the greedy algorithm despite the different merge order.|$|E
500|$|The same method {{also can}} be used as a {{subroutine}} when the individual cluster diameters are unknown. To test whether a given sum of diameters can be achieved without knowing the individual cluster diameters, one may try all maximal pairs of target diameters that add up to at most the given sum, representing each pair of diameters as a 2-satisfiability instance and using a 2-satisfiability algorithm to determine whether that pair can be realized by a clustering. To find the optimal sum of diameters one may perform a binary search in which each step is a feasibility test of this type. The same approach also works to find clusterings that optimize other combinations than sums of the cluster diameters, and that use arbitrary dissimilarity numbers (rather than distances in a metric space) to measure the size of a cluster. The time bound for this algorithm is dominated by the time to solve a sequence of 2-satisfiability instances that are closely related to each other, and [...] shows how to solve these related instances more quickly than if they were solved independently from each other, leading to a total time bound of [...] for the sum-of-diameters <b>clustering</b> <b>problem.</b>|$|E
40|$|We {{discuss a}} variety of <b>clustering</b> <b>problems</b> arising in {{combinatorial}} applications and in classifying objects into homogenous groups. For each problem we discuss solution strategies that work well in practice. We also discuss the importance of careful modelling in <b>clustering</b> <b>problems.</b> Â© 1996 Kluwer Academic Publishers...|$|R
40|$|In this paper, we {{investigate}} application of various options of algorithms with greedy agglomerative heuristic procedure for object <b>clustering</b> <b>problems</b> in continuous space {{in combination with}} various local search methods. We propose new modifications of the greedy agglomerative heuristic algorithms with local search in SWAP neighborhood for the p-medoid problems and j-means procedure for continuous <b>clustering</b> <b>problems</b> (p-median and k-means). New modifications of algorithms were applied to <b>clustering</b> <b>problems</b> in both continuous and discrete settings. Computational results with classical data sets and real data show the comparative efficiency of new algorithms for middle-size problems only...|$|R
40|$|The article {{contains}} a preliminary glance at balanced <b>clustering</b> <b>problems.</b> Basic balanced structures and combinatorial balanced problems are briefly described. A special attention is targeted to various balance/unbalance indices (including some new versions of the indices) : by cluster cardinality, by cluster weights, by inter-cluster edge/arc weights, by cluster element structure (for element multi-type clustering). Further, versions of optimization <b>clustering</b> <b>problems</b> are suggested (including multicriteria problem formulations). Illustrative numerical examples describe calculation of balance indices and element multi-type balance <b>clustering</b> <b>problems</b> (including example for design of student teams). Comment: 21 pages, 17 figures, 14 table...|$|R
50|$|Other {{works have}} {{examined}} trigrams in their {{approaches to the}} Brown <b>clustering</b> <b>problem.</b>|$|E
5000|$|MATLAB {{implements}} PAM, CLARA, and {{two other}} algorithms to solve the k-medoid <b>clustering</b> <b>problem.</b>|$|E
5000|$|Regarding {{computational}} complexity, {{finding the}} optimal {{solution to the}} k-means <b>clustering</b> <b>problem</b> for observations in d dimensions is: ...|$|E
50|$|<b>Clustering</b> <b>problems</b> have {{applications}} in biology, medicine, psychology, economics, {{and many other}} disciplines.|$|R
40|$|We give {{deterministic}} {{versions of}} randomized approximation algorithms for several ranking and <b>clustering</b> <b>problems</b> that were proposed by Ailon, Charikar and Newman. We show that under a reasonable {{extension of the}} triangle inequality in <b>clustering</b> <b>problems,</b> we can resolve Ailon et al. 's open question wehter there is an approximation algorithm for weighted correlation clustering with weights satisfying the triangle inequality...|$|R
40|$|We {{present a}} novel {{analysis}} of a random sampling approach for four <b>clustering</b> <b>problems</b> in metric spaces: k-median, k-means, min-sum k-clustering, and balanced k-median. For all these problems we consider the following simple sampling scheme: select a small sample set of input points uniformly at random and then run some approximation algorithm on this sample set to compute an approximation of the best possible clustering of this set. Our main technical contribution is a significantly strengthened analysis of the approximation guarantee by this scheme for the <b>clustering</b> <b>problems.</b> The main motivation behind our analyses was to design sublinear-time algorithms for <b>clustering</b> <b>problems.</b> Our second contribution {{is the development of}} new approximation algorithms for the aforementioned <b>clustering</b> <b>problems.</b> Using our random sampling approach we obtain for these problems the first time approximation algorithms that have running time independent of the input size, and depending on k and the diameter of the metric space only...|$|R
5000|$|The k-medoid <b>clustering</b> <b>problem</b> {{and other}} related {{facility}} location problems for which local search offers the best known approximation ratios from a worst-case perspective ...|$|E
50|$|Brown {{clustering}} {{is a hard}} hierarchical agglomerative <b>clustering</b> <b>problem</b> {{based on}} distributional information. It is typically applied to text, grouping words into clusters that {{are assumed to be}} semantically related by virtue of their having been embedded in similar contexts.|$|E
50|$|Determining {{the number}} of {{clusters}} in a data set, a quantity often labelled k as in the k-means algorithm, is a frequent problem in data clustering, and is a distinct issue from the process of actually solving the <b>clustering</b> <b>problem.</b>|$|E
40|$|The article {{presents}} auxiliary functions of clusterSim package (see Walesiak & Dudek (2006)) and selected functions of packages stats, cluster, and ade 4, which are applied lo solving <b>clustering</b> <b>problems.</b> In addition {{the examples of}} the procedures for solving different <b>clustering</b> <b>problems</b> are presented. These procedures, which are not available in statistical packages (SPSS, Statistica, SAS), can help solving {{a broad range of}} classification problems. Marek Walesia...|$|R
40|$|We survey some {{practical}} techniques for designing fixedparameter algorithms for NP-hard graph-modeled data <b>clustering</b> <b>problems.</b> Such <b>clustering</b> <b>problems</b> ask to modify a given graph into {{a union of}} dense subgraphs. In particular, we discuss (polynomial-time) kernelizations and depth-bounded search trees and provide concrete applications of these techniques. After that, we shortly review the use of two further algorithmic techniques, iterative compression and average parameterization, applied to graph-modeled data clustering. Finally, we address some challenges for future research...|$|R
40|$|Clustering is a {{somewhat}} confusing topic theoretically. In large part {{this is because}} there are many different kinds of <b>clustering</b> <b>problems.</b> In addition, the true goals in clustering are often difficult to measure, making the task seem not well-defined and under-specified. In this note we discuss one approach from [6] to theoretically formulating a certain broad class of <b>clustering</b> <b>problems,</b> and discuss relations between this and other approaches. We also make a few suggestions and state a few open directions...|$|R
5000|$|The k-Center <b>Clustering</b> <b>problem</b> {{can also}} be defined on a {{complete}} undirected graph G = (V, E) as follows:Given a complete undirected graph G = (V, E) with distances d(vi, vj) &isin; N satisfying the triangle inequality, find a subset C &sube; V with |C| = k while minimizing: ...|$|E
50|$|Quadratic probing {{can be a}} more {{efficient}} algorithm in a closed hash table, since it better avoids the <b>clustering</b> <b>problem</b> that can occur with linear probing, {{although it is not}} immune. It also provides good memory caching because it preserves some locality of reference; however, linear probing has greater locality and, thus, better cache performance.|$|E
50|$|Mounts's {{main area}} of {{research}} is Computational Geometry, which is the branch of algorithms devoted to solving problems of a geometric nature. This field includes problems from classic geometry, like the closest pair of points problem, {{as well as more}} recent applied problems, such as computer representation and modeling of curves and surfaces. In particular, Mount has worked on the k-means <b>clustering</b> <b>problem,</b> nearest neighbor search, and point location.|$|E
40|$|The {{study of}} human {{performance}} on discrete optimization problems has a considerable history that spans various disciplines. The {{purpose of this paper}} is to outline a program of study for the measurement of human performance on discrete optimization <b>problems</b> related to <b>clustering</b> of points in the two-dimensional plane. I describe possible objective criteria for <b>clustering</b> <b>problems,</b> the measurement of agreement of solutions produced by subjects, and categories of experiments for investigating human performance on <b>clustering</b> <b>problems.</b> To facilitate future experimental testing of human subjects on clus-tering problems, optimal partitions were obtained for 233 two-dimensional <b>clustering</b> <b>problems</b> ranging in size from 10 to 70 points. For each test problem, an optimal solu-tion was obtained for each of three objective criteria: (a) maximizing partition split, (b) minimizing partition diameter, and (c) minimizing within-cluster sums of squares, and similarity of the solutions among these criteria has been computed...|$|R
40|$|Bayesian {{model for}} <b>clustering</b> <b>problems</b> {{involving}} multiple groups of data. Each group of data is modeled with a mixture, {{with the number}} of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped <b>clustering</b> <b>problems</b> occur often in practice, e. g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models. ...|$|R
40|$|In {{this paper}} {{we present a}} novel {{analysis}} of a random sampling approach for three <b>clustering</b> <b>problems</b> in metric spaces: k-median, min-sum k- clustering, and balanced k-median. For all these problems we consider the following simple sampling scheme: select a small sample set of points uniformly at random from V and then run some approximation algorithm on this sample set to compute an approximation of the best possible clustering of this set. Our main technical contribution is a significantly strengthened analysis of the approximation guarantee by this scheme for the <b>clustering</b> <b>problems.</b> The m ai...|$|R
50|$|The {{currently}} available algorithms are sub-optimal {{as they can}} only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely {{in the near future}} as the problem has been shown to generalize the k-means <b>clustering</b> <b>problem</b> which is known to be NP-complete. However, as in many other data mining applications, a local minimum may still prove to be useful.|$|E
5000|$|... 1. Clustering {{ensemble}} (Strehl and Ghosh): They considered various formulations for the problem, most {{of which}} reduce the problem to a hyper-graph partitioning problem. In one of their formulations they considered the same graph as in the correlation <b>clustering</b> <b>problem.</b> The solution they proposed is to compute the best k-partition of the graph, which {{does not take into}} account the penalty for merging two nodes that are far apart.|$|E
50|$|As the Bi-CoPaM {{specially}} meets many {{requirements of}} gene discovery studies, its current main applications are within this field of bioinformatics; though, it was defined {{in a completely}} independent manner such that it is applicable for any other <b>clustering</b> <b>problem.</b> For example, a recent experiment in which the Bi-CoPaM was applied over multiple yeast cell-cycle datasets revealed important information about a poorly characterised gene, CMR1/YDL156W, and about its relation with many other genes.|$|E
40|$|In this paper, {{well-known}} PSO algorithms {{reported in}} the literature for solving continuous function optimization problems were comparatively evaluated by considering real world data <b>clustering</b> <b>problems.</b> Data <b>clustering</b> <b>problems</b> are solved, by considering three performance clustering metrics such as TRace Within criteria (TRW), Variance Ratio Criteria (VRC) and Marriott Criteria (MC). The results obtained by the PSO variants were compared with the basic PSO algorithm, Genetic algorithm and Differential evolution algorithms. A detailed performance analysis has been carried out to study the convergence behavior of the PSO algorithms using run length distribution. Key words...|$|R
40|$|The {{study of}} human {{performance}} on discrete optimization problems has a considerable history that spans various disciplines. The two most widely studied problems are the Euclidean traveling salesperson problem and the quadratic assignment problem. The {{purpose of this paper}} is to outline a program of study for the measurement of human performance on discrete optimization <b>problems</b> related to <b>clustering</b> of points in the two-dimensional plane. I describe possible objective criteria for <b>clustering</b> <b>problems,</b> the measurement of agreement of solutions produced by subjects, and categories of experiments for investigating human performance on <b>clustering</b> <b>problems...</b>|$|R
40|$|Abstract â The Self-Organizing Map (SOM) {{is popular}} algo-rithm for {{unsupervised}} learning and visualization introduced by Teuvo Kohonen. One {{of the most}} attractive applications of SOM is clustering and several algorithms for various kinds of <b>clustering</b> <b>problems</b> have been reported and investigated. In this study, we propose {{a new type of}} SOM algorithm, which is called Fatigable SOM (FSOM) algorithm. The important feature of FSOM is that the neurons are fatigable, namely, the neurons which have become a winner can not become a winner during a certain period of time. Because of this feature, FSOM tends to self-organize only in the area where input data are concentrated. We investigate the behavior of FSOM and apply FSOM to <b>clustering</b> <b>problems.</b> Further, we introduce the fatigue level to FSOM to increase its flexibility for various kinds of <b>clustering</b> <b>problems.</b> The efficiencies of FSOM and the fatigue level are confirmed by several simulation results. I...|$|R
50|$|Finally, one of {{the most}} {{promising}} methods is spatial color quantization, conceived by Puzicha, Held, Ketterer, Buhmann, and Fellner of the University of Bonn, which combines dithering with palette generation and a simplified model of human perception to produce visually impressive results even for very small numbers of colors. It does not treat palette selection strictly as a <b>clustering</b> <b>problem,</b> in that the colors of nearby pixels in the original image also affect the color of a pixel. See sample images.|$|E
50|$|The {{input to}} a <b>clustering</b> <b>problem</b> {{consists}} {{of a set of}} points. A cluster is any proper subset of the points, and a hierarchical clustering is a maximal family of clusters with the property that any two clusters in the family are either nested or disjoint.Alternatively, a hierarchical clustering may be represented as a binary tree with the points at its leaves; the clusters of the clustering are the sets of points in subtrees descending from each node of the tree.|$|E
5000|$|The kernel k-means {{problem is}} an {{extension}} of the k-means problem where the input data points are mapped non-linearly into a higher-dimensional feature space via a kernel function [...] The weighted kernel k-means problem further extends this problem by defining a weight [...] for each cluster as the reciprocal of the number of elements in the cluster,Suppose [...] is a matrix of the normalizing coefficients for each point for each cluster [...] if [...] and zero otherwise. Suppose [...] is the kernel matrix for all points. The weighted kernel k-means problem with n points and k clusters is given as,such thatsuch that [...] In addition, there are identity constrains on [...] given by,where [...] represents a vector of ones.This problem can be recast as,This problem is equivalent to the spectral <b>clustering</b> <b>problem</b> when the identity constraints on [...] are relaxed. In particular, the weighted kernel k-means problem can be reformulated as a spectral clustering (graph partitioning) problem and vice versa. The output of the algorithms are eigenvectors which do not satisfy the identity requirements for indicator variables defined by [...] Hence, post-processing of the eigenvectors is required for the equivalence between the problems.Transforming the spectral <b>clustering</b> <b>problem</b> into a weighted kernel k-means problem greatly reduces the computational burden.|$|E
40|$|Abstract â High {{dimensional}} database {{is having}} large datasets while solving the <b>cluster</b> identification <b>problem</b> and for identifying dense clusters in a noisy data. Our analysis works to identifies clusters through {{the identification of}} densely intra connected sub graphs, we have employed a pattern recognition algorithms representation of the graph and solve the <b>cluster</b> identification <b>problem</b> using K-means, K-modes, Single Linkage Clustering. The computational analysis indicate that when running on 150 CPUs, one of our algorithm can solve a <b>cluster</b> identification <b>problem</b> on a data set with 1, 000, 000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data <b>clustering</b> <b>problems</b> in an efficient manner The implementation of the clustering algorithm as the software CLUMP. Index Terms â Data Mining, Clustering, K-means and K-mode...|$|R
40|$|We {{present a}} general {{approach}} for designing approximation algorithms for a fundamental class of geometric <b>clustering</b> <b>problems</b> in arbitrary dimensions. More specifically, our approach leads to simple randomized algorithms for the k-means, k-median and discrete k-means problems that yield (1 + Îµ) approximations with probability â¥ 1 / 2 and running times of O(2 (k/Îµ) O(1) dn). These {{are the first}} algorithms for these problems whose running times are linear {{in the size of}} the input (nd for n points in d dimensions) assuming k and Îµ are fixed. Our method is general enough to be applicable to <b>clustering</b> <b>problems</b> satisfying certain simple properties and is likely to have further applications...|$|R
40|$|We {{propose a}} new {{algorithm}} using tabu search {{to deal with}} biobjective <b>clustering</b> <b>problems.</b> A <b>cluster</b> {{is a collection of}} records that are similar to one other and dissimilar to records in other clusters. Clustering has applications in VLSI design, protein-protein interaction networks, data mining and many others areas. <b>Clustering</b> <b>problems</b> have been subject of numerous studies; however, most of the work has focused on single-objective problems. In the context of multiobjective optimization our aim is to ï¬nd a good approximation to the Pareto front and provide a method to make decisions. As an application problem we present the zoning problem by allowing the optimization of two objectives...|$|R
