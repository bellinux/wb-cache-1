11|1265|Public
40|$|We present {{algorithms}} that {{compute the}} linear and quadratic factors of supersparse (lacunary) bivariate polynomials over the rational numbers in polynomial-time in the input size. In supersparse polynomials, the term degrees can {{have hundreds of}} digits as binary numbers. Our algorithms are Monte Carlo randomized for quadratic factors and deterministic for linear factors. Our approach relies on the results by H. W. Lenstra, Jr., on <b>computing</b> <b>factors</b> of univariate supersparse polynomials over the rational numbers. Furthermore, we show {{that the problem of}} determining the irreducibility of a supersparse bivariate polynomial over a large finite field of any characteristic is co-NP-hard via randomized reductions. Categories and Subject Descriptor...|$|E
40|$|We present new {{results on}} Boolean matrix {{factorization}} {{and a new}} algorithm based on these results. The results emphasize the significance of factorizations that provide from-below approximations of the input matrix. While the previously proposed algorithms do not consider the possibly different significance of different matrix entries, our results help measure such significance and suggest where to focus when <b>computing</b> <b>factors.</b> An experimental evaluation of the new algorithm on both synthetic and real data demonstrates its good performance in terms of good coverage by the first k factors {{as well as a}} small number of factors needed for exact decomposition and indicates that the algorithm outperforms the available ones in these terms. We also propose future research topics...|$|E
40|$|Security and {{computer}} expertise of end users can be {{significant predictors of}} user behaviour and interactions in the security and privacy context. Standardized, externally valid instruments for measuring end-user security expertise are non-existent. To address this need, we developed a questionnaire to identify critical factors that constitute expertise in end-users. It combines skills and knowledge based questions. Using exploratory factor analysis on the results from 898 participants {{from a range of}} populations, we identified 12 questions within 4 factors that correspond to computing and security expertise. Ordered logistic regression models were applied to measure efficacy of proposed security and <b>computing</b> <b>factors</b> in predicting user comprehension of security concepts (phishing and certificates). We conclude with a framework for informing future user-centered security expertise research. Novice; Factor Analysis; End-user; Computer expertise; Security expertis...|$|E
3000|$|... {{respectively}} {{correspond to}} the unnormalized probability and the softmax-normalizing <b>factor.</b> <b>Computing</b> this <b>factor</b> z [...]...|$|R
3000|$|... {{value is}} set to 0 for the first cycle of {{bandwidth}} granting process. FPBWforecast indicates the <b>computed</b> <b>factor</b> point of BW forecast submodule while maxFP is the maximum factor point with 0.25 as its value.|$|R
30|$|If set I is empty, CQI {{prioritization}} is not required. Then, F_k^QCI_m= 1 ∀ k, and {{the algorithm}} is finished. If set I is not empty, CQI prioritization is triggered. Then, the algorithm starts to <b>compute</b> <b>factor</b> F_k^QCI_m for k = 1.|$|R
40|$|In mobile <b>computing,</b> <b>factors</b> such as add-on {{hardware}} {{components and}} heterogeneous networks {{result in an}} environment made up of changing resource constraints. An application in such a constrained environment must react to these changes so that available resources are properly utilized. In this paper, we propose an architecture to report changes in the environment to interested applications. The architecture {{is based on an}} event delivery mechanism that decouples event detection from delivery, giving the flexibility and extensibility that is necessary in a mobile computing environment. Information associated with the event is delivered as part of the event notification, while delivery latency is reduced by clever thread scheduling. We demonstrate the utility of our architecture by structuring an environment aware networking subsystem around a prototype implementation. The performance of this implementation is competitive with current event delivery mechanisms such as the Unix signal. 1 In [...] ...|$|E
40|$|We present {{algorithms}} that compute all irreducible {{factors of}} degree â 8 ̆ 9 ¤ d of supersparse (lacunary) multivariate polynomials in n variables over an algebraic number field in deterministic polynomial-time in (l+d) n, where l {{is the size}} of the input polynomial. In supersparse polynomials, the term degrees enter logarithmically as their numbers of binary digits into the size measure l. The factors are again represented as supersparse polynomials. If the factors are represented as straight-line programs or black box polynomials, we can achieve randomized polynomial-time in (l + d) O(1). Our approach follows that by H. W. Lenstra, Jr., on <b>computing</b> <b>factors</b> of univariate supersparse polynomials over algebraic number fields. We generalize our ISSAC 2005 results for computing linear factors of supersparse bivariate polynomials over the rational numbers by appealing to recent lower bounds on the height of algebraic numbers and to a special case of the former Lang conjecture...|$|E
40|$|This paper {{presents}} {{the application of}} a statistical model for predicting yellowfin tuna fish abundance in the Gulf of Mexico. Data for twelve different independent variables was collected through sensors installed in various space satellites, capable of photographing, and <b>computing</b> <b>factors</b> such as ocean temperatures, sea levels, and concentrations of photosynthetic pigments, among others. In addition, specialized measuring devices were installed on a fishing ship to collect further information on yellowfin tuna feeding behavior, and barometric pressure readings during twenty fishing trips (randomly distributed) across the Mexican waters of the Gulf of Mexico. The results from the study show that barometric pressure is a more significant factor for predicting the distribution of yellowfin tuna than water temperature, a fact not fully addressed by documented research efforts in the field. Moreover, it was concluded that a multiple-degree polynomial model, probably a third-degree model, best re{{presents the}} relationship between the identified independent variables and the dependent one (i. e., the expected number of yellowfin tuna caught per fishing trip) ...|$|E
50|$|Factor scores (also called {{component}} {{scores in}} PCA): are {{the scores of}} each case (row) on each <b>factor</b> (column). To <b>compute</b> the <b>factor</b> score for a given case for a given factor, one takes the case's standardized score on each variable, multiplies by the corresponding loadings of the variable for the given factor, and sums these products. <b>Computing</b> <b>factor</b> scores allows one to look for factor outliers. Also, factor scores {{may be used as}} variables in subsequent modeling. (Explained from PCA not from Factor Analysis perspective).|$|R
30|$|Using IBM SPSS Statistics 19.0, twenty-six {{observations}} with twelve {{factors were}} analyzed for data dimensionality reduction. Average values of 15 replicates for each observation were reserved for further analysis. In factor analysis, principal component method was used for data extraction, varimax method for rotation and regression method for <b>computing</b> <b>factor</b> scores. Factor scores for 26 observations were then utilized in the cluster analysis. Hierarchical cluster analysis was performed using between-group linkage method, and data were standardized by Z scores method to compute the squared Euclid distances.|$|R
50|$|The Real gas article {{features}} more theoretical methods to <b>compute</b> compressibility <b>factors.</b>|$|R
40|$|As the {{availability}} of open-source information online increases, there are growing concerns regarding its reliability. This has led to renewed emphasis in quality- and trust-metrics research within the social computing space, to assist individuals in determining how reliable pieces of information actually might be. In this article, we {{take a step back}} to rigorously investigate the utility of trustworthiness information support provided via computer and information technologies. Our research aim is to assess whether people can cognitively combine trustworthiness advice and evaluative content to make decisions, particularly in a risk-related context. Moreover, we analyse individuals’ ability to sensitise their decisions given that information and the criticality of a set task. The results suggest that individuals can perform well at both these tasks even when there are only subtle variations in information and advice. This empirically validated contribution provides a basis for a commonly-made assumption, and reinforces humans as efficient and effective information processors. The study also highlighted several social <b>computing</b> <b>factors</b> that may affect such decisions including quantity of content, existing trust relationships and reasoning behind trustworthiness advice...|$|E
40|$|The {{libraries}} {{have been}} changing their {{activities in the}} housekeeping operations with {{the invention of the}} computer and communication technologies. Like the other organizations the college libraries are trying to adopt the applications of cloud computing, the new technology model for IT services. This new technology concept will minimize the cost of the hardware and software applicable to the libraries. At the same time cloud computing can simplify the different functions and activities within shorter period to time. The cloud computing is composed of three service models (Software as a Service, Platform as a Service and Infrastructure as a Service) and four deployment Models (Private Cloud, Community Cloud, Public Cloud and Hybrid Cloud). College libraries need to take data backups through a storage media on regular basis. The College Library data backup would be more easer with the could computing technologies. This paper defines cloud <b>computing,</b> <b>factors</b> to be considered for library data backup and finally propose a framework for cloud based data backup services for college libraries...|$|E
40|$|Abstract. Mobile {{computing}} is {{an innovative}} field gaining increasing attention as many new systems are designed towards that direction. Among these systems, many are desired to be context-aware, {{with the aim}} of optimizing and automating their offered services. Such systems provide components whose main feature is to manage the context information which is communicated between sensors, actuators and applications. In these systems the use of middleware is a solution to the need for detecting and adapting to the changing context. In mobile <b>computing,</b> <b>factors</b> such as scalability, support for distribution, self-adaptivity, support for mobility, modularity/plug-ability, etc are of particular interest. Many attempts have been documented in the literature concerning systems aiming to address some or all of these requirements and which are used for the implementation of context-aware systems. The scope {{of this paper is to}} study and present the current state of the art in context-aware system architectures. These are evaluated and compared, based on a set of characteristics such as support for distribution, privacy, mobility or fault tolerance. Finally, we document our current results and initial decisions concerning the design of a context management middleware system enabling the design and deployment of adaptive applications in mobile and ubiquitous computing environments. ...|$|E
40|$|In {{this work}} we {{introduce}} a new method for <b>computing</b> Form <b>Factors</b> in Radiosity. We demostrate how our method improves on existing projective techniques such as the hemicube. We use the Nusselt analog to directly <b>compute</b> form <b>factors</b> by projecting the scene onto the unit circle. We compare our method with other form factor computation methods. The results show an improvement in the quality/speed ratio using our technique...|$|R
40|$|The {{clinical}} {{management of}} bacterial infection <b>computes</b> <b>factors</b> related to bacteria (e. g., resistance profile [1]), antibiotics (e. g., activity spectrum, distribution volume, etc.), host characteristics (e. g., vascularization of the infected tissue, effectiveness of host defenses, etc.), {{as well as}} pharmacokinetic (PK) parameters [2]. The serum inhibitory (SIT) and bactericidal titers (SBT) are labora-tory tests that simulate the interactions between antibio-tics and bacteria {{in the human body}} milieu...|$|R
3000|$|... where x {{is number}} of cycle, and f(x) denotes the {{changes of the}} {{bandwidth}} request for x cycle. BWRequestχ- 1 and BWAllocatedχ- 1 are the bandwidth request and bandwidth allocation for x- 1 cycle, respectively. However, the f(x) {{is the amount of}} bandwidth request for the first cycle of bandwidth granting process. FPBRstatus indicates the <b>computed</b> <b>factor</b> point of BR status submodule while maxFP is the maximum factor point with 0.25 as its value.|$|R
40|$|The {{purpose of}} this study is to {{identify}} factors that determine computer and security expertise in end users. They can be significant determinants of human behaviour and interactions in the security and privacy context. Standardized, externally valid instruments for measuring end-user security expertise are non-existent. A questionnaire encompassing skills and knowledge-based questions was developed to identify critical factors that constitute expertise in end users. Exploratory factor analysis was applied on the results from 898 participants from a wide range of populations. Cluster analysis was applied to characterize the relationship between computer and security expertise. Ordered logistic regression models were applied to measure efficacy of the proposed security and <b>computing</b> <b>factors</b> in predicting user comprehension of security concepts: phishing and certificates. There are levels to peoples’ computer and security expertise that could be reasonably measured and operationalized. Four factors that constitute computer security-related skills and knowledge are, namely, basic computer skills, advanced computer skills, security knowledge and advanced security skills, and these are identified as determinants of computer expertise. Findings from this work can be used to guide the design of security interfaces such that it caters to people with different expertise levels and does not force users to exercise more cognitive processes than required. Privacy; Psychometrics; Security; Expertise; Security comprehensio...|$|E
40|$|Strengths, Weaknesses, Opportunities and Threats (SWOT) {{analysis}} do {{not provide}} an analytical means to determine {{the importance of the}} identified factors of green computing strategy and implementation. Although the SWOT analysis successfully explores the factors, individual factors are usually described very generally. For this reason, SWOT analysis possesses deficiencies in the measurement and evaluation of green computing steps. Even though the analytic hierarchy process (AHP) technique eliminates these deficiencies, it does not allow for measuring the possible dependencies among the individual factors. The AHP method assumes that the green <b>computing</b> <b>factors</b> presented in the hierarchical structure are independent; however, this assumption may be inappropriate in light of certain situation. Therefore, it is important to utilize a form of SWOT analysis that calculates and takes into account the possible dependency among the factors. This paper demonstrates a process for quantitative SWOT analysis of green computing implementation that can be performed even when there is dependence among strategic factors. The proposed algorithm uses the analytic network process (ANP), which allows measurement of the dependency among the green computing implementation factors, as well as AHP, which is based on the independence between the factors. There are four alternatives: campus awareness program, computer procurement, increase in heat removal requirement, and increase in equipment power density for improving the implementation of green computing in campus. Dependency among the SWOT factors is observed to effect the strategic and sub-factor weights, as well as to change the strategy priorities. Based on ANC method, the best alternative for this implementation is computer procurement...|$|E
40|$|We {{propose a}} {{corrected}} plug-in method for constructing confidence intervals of the conditional quantiles of an original response variable through a transformed regression with heteroscedastic errors. The interval {{is easy to}} <b>compute.</b> <b>Factors</b> affecting {{the magnitude of the}} correction are examined analytically through the special case of Box-Cox regression. Monte Carlo simulations show that the new method works well in general and is superior over the commonly used delta method and the quantile regression method. An empirical application is presented. [PUBLICATION ABSTRACT...|$|R
5000|$|Researchers gain extra {{information}} from a PCA approach, such as an individual’s score on a certain component - such information is not yielded from factor analysis. However, as Fabrigar et al. contend, the typical aim of factor analysis - i.e. to determine the factors accounting for {{the structure of the}} correlations between measured variables - does not require knowledge of factor scores and thus this advantage is negated. It is also possible to <b>compute</b> <b>factor</b> scores from a factor analysis.|$|R
5000|$|The {{security}} of an RSA {{system would be}} compromised if the number [...] could be factored or if [...] could be <b>computed</b> without <b>factoring</b> [...]|$|R
40|$|A {{very simple}} {{structure}} is sought when using factor analysis to develop measurement scales. The present article {{is about the}} SIMLOAD program; it <b>computes</b> measures of <b>factor</b> simplicity for rows and columns of loading matrices (usually the factor pattern) {{as well as some}} overall measures. These include Kaiser’s (1974) index of factor simplicity for variables (rows), Fleming’s scale fit index for factors (columns), Bentler’s (1977) scale-free matrix measure, and hyperplane counts. Routine use of these measures is recommended for multifactor scale development. The measures may also be useful in more general factor applications, and in confirmatory as well as  exploratory analysis. SIMLOAD additionally <b>computes</b> <b>factor</b> scale intercorrelations, scale alpha coefficients, including alpha when item removed, and sorted loadings for ease of interpretation...|$|R
30|$|When {{criterion}} 1 is {{used with}} tension cutoff, the <b>computed</b> <b>factor</b> of safety by ABAQUS obviously decreases but its variation with the dilation angle is unremarkable; when criterion 2 is {{used with the}} tension cutoff, ABAQUS is unable to show the plastic penetration zone so that no values are listed in those columns in Table  1, and in this case, criterion 3 was adopted to determine the factors of safety, which actually are around 0.94 for both tensile strengths σ t =  7.11 and 0.01  kPa and for all three dilation angles.|$|R
30|$|When {{criterion}} 1 is used without tension cutoff, the <b>computed</b> <b>factors</b> {{of safety}} {{with respect to}} the three dilation angles by ABAQUS are respectively 15 %, 14 %, and 6 % greater than the exact factor of safety of 1.00; when criterion 2 is used without tension cutoff, they are respectively 8 %, 3 % greater or 2 % less than 1.00. Similar results are obtained by the UMAT. Moreover, for both ABAQUS and the UMAT, the factors of safety by criterion 1 often deviate from 1.00 farther than those of by criterion 2.|$|R
30|$|Step 2 : <b>Compute</b> the perturbed <b>factor</b> as (25).|$|R
5000|$|Manin Classical computing, quantum <b>computing</b> and Shor´s <b>factoring</b> algorithm, ...|$|R
3000|$|Next we <b>compute</b> the <b>factor</b> {{by which}} the Y current has been enhanced. The bottom panel of Fig. 4 shows {{the ratio of the}} {{secondary}} current to the primary current, j [...]...|$|R
40|$|AbstractWe derive componentwise error {{bound for}} the {{factorization}} H = GJGT, where H is a real symmetric matrix, G has full column rank, and J is diagonal with ± 1 's on the diagonal. We also derive a componentwise forward error bound, that is, we bound {{the difference between the}} exact and the <b>computed</b> <b>factor</b> G, in the cases where such a bound is possible. We extend these results to the Hermitian case, and to the well-known Bunch-Parlett factorization. Finally, we prove bounds for the scaled condition of the matrix G, and show that the factorization can have the rank-revealing property...|$|R
40|$|The band {{spectrum}} of BaO has been obtained by spraying BaCl 2 soln. into a flame. The integrated intensities {{of the bands}} have been detd. by photographic photometry. The exptl. results along with the theoretically <b>computed</b> Franck-​Condon <b>factors</b> {{have been used to}} evaluate a relation between Re, the electronic transition moment, and r, the internuclear sepn., in the form, Re(r v'v'') = const. (1 - 0. 536 r) ​. This relation has been used to obtain improved Franck-​Condon <b>factors.</b> The theoretically <b>computed</b> Franck-​Condon <b>factors,</b> with and without the inclusion of Re variation, have been compared with the exptl. band strengths...|$|R
40|$|This paper proposes Bayesian {{methods for}} {{estimating}} the cointegration rank using Bayes factors. We consider natural conjugate priors for <b>computing</b> Bayes <b>factors.</b> First, we estimate the cointegrating vectors for each possible rank. Then, we <b>compute</b> the Bayes <b>factors</b> for each rank against 0 rank. Monte Carlo simulations show that using Bayes factor with conjugate priors produces fairly good results. The methods proposed here are also applied for selecting the appropriate lags and testing for over-identifying restrictions on cointegrating vectors...|$|R
3000|$|... {{between the}} {{upstream}} and downstream locations. If both can be determined (which is quite straightforward to obtain), one could <b>compute</b> platoon dispersion <b>factors</b> α and β. These parameters subsequently {{can be used to}} <b>compute</b> the smoothing <b>factor</b> R, from which the degree of how an upstream platoon will disperse at the downstream location can be computed.|$|R
40|$|We {{present a}} {{deterministic}} polynomial-time algorithm which <b>computes</b> the multilinear <b>factors</b> of multivariate lacunary polynomials over number fields. It {{is based on}} a new Gap theorem which allows to test whether P(X) = ∑kj= 1 ajX αj(vX+ t) β j(uX+ w) γj is identically zero in polynomial time. Previous algorithms for this task were based on Gap Theorems expressed in terms of the height of the coefficients. Our Gap Theorem is based on the valuation of the polynomial and is valid for any field of characteristic zero. As a consequence we obtain a faster and more elementary algorithm. Furthermore, we can partially extend the algorithm to other situations, such as absolute and approximate factorizations. We also give a version of our Gap Theorem valid for fields of large characteristic, and deduce a randomized polynomial-time algorithm to <b>compute</b> multilinear <b>factors</b> with at least three monomials of mul-tivariate lacunary polynomials of finite fields of large characteristic. We provide NP-hardness results to explain our inability to <b>compute</b> binomial <b>factors...</b>|$|R
40|$|Solving sparse linear {{systems of}} {{equations}} {{is a common}} and important application of a multitude of scientific and engineering applications. The SPOOLES software package 1 provides this functionality {{with a collection of}} software objects. The first step to solving a sparse linear system is to find a good low-fill ordering of the rows and columns. The library contains several ways to perform this operation: minimum degree, generalized nested dissection, and multisection. The second step is to factor the matrix as a product of triangular and diagonal matrices. The library supports pivoting for numerical stability (when required), approximation techniques to reduce the storage for and work to <b>compute</b> the matrix <b>factors,</b> and the computations are based on BLAS 3 numerical kernels to take advantage of high performance computing architectures. The third step is to solve the linear system using the <b>computed</b> <b>factors.</b> The library is written in ANSI C using object oriented design. Good design and [...] ...|$|R
50|$|In <b>computing,</b> {{the form}} <b>factor</b> is the {{specification}} of a motherboard - the dimensions, power supply type, location of mounting holes, number of ports {{on the back}} panel, etc. Specifically, in the IBM PC compatible industry, standard form factors ensure that parts are interchangeable across competing vendors and generations of technology, while in enterprise <b>computing,</b> form <b>factors</b> ensure that server modules fit into existing rackmount systems. Traditionally, the most significant specification is for that of the motherboard, which generally dictates the overall size of the case. Small form factors have been developed and implemented.|$|R
