26|10000|Public
25|$|Working {{with the}} Flight Test Center's Technical Director, Paul Bikle, he defined the basic flight test {{techniques}} {{that are still}} used by the Air Force Flight Test Center. Aiming to reduce the increasing length of time and costs required to determine {{the results of the}} Center's flight tests, they standardized all of its data acquisition methods and set up a <b>centralized</b> <b>Data</b> <b>Processing</b> System. This made it possible for test teams to analyze their test data more rapidly, and to publish their Technical Reports more quickly. He also established training and indoctrination procedures for new military and civilian flight test engineers. Impressing his own long-thought-out ideals upon these changes and goals, Jack Ridley is still credited for creating the Flight Test Center's basic philosophy in use today.|$|E
5000|$|The Air Force Space Surveillance System (AFSSS), {{also known}} as the [...] "space fence", was a very high {{frequency}} radar network located at sites across the southern United States (from California to Georgia) with a <b>centralized</b> <b>data</b> <b>processing</b> site at the Naval Network and Space Operations Command in Dahlgren, Virginia. AFSSS began as the Navy's Space Surveillance (SPASUR) system in 1961 (later renamed NAVSPASUR). It was transferred to the Air Force in 2004 and renamed AFSSS. The [...] "fence" [...] was operated by the U.S. Air Force (20th Space Control Squadron Detachment 1).|$|E
50|$|Working {{with the}} Flight Test Center's Technical Director, Paul Bikle, he defined the basic flight test {{techniques}} {{that are still}} used by the Air Force Flight Test Center. Aiming to reduce the increasing length of time and costs required to determine {{the results of the}} Center's flight tests, they standardized all of its data acquisition methods and set up a <b>centralized</b> <b>Data</b> <b>Processing</b> System. This made it possible for test teams to analyze their test data more rapidly, and to publish their Technical Reports more quickly. He also established training and indoctrination procedures for new military and civilian flight test engineers. Impressing his own long-thought-out ideals upon these changes and goals, Jack Ridley is still credited for creating the Flight Test Center's basic philosophy in use today.|$|E
5|$|Shoup {{favored a}} more frugal {{approach}} to the military budget, feeling the military was too susceptible to influence from large corporations arguing for expensive and unnecessary programs. As the Kennedy administration brought more emphasis on conventional warfare, Shoup sought to use increased funds to improve military logistics. He is credited with formulating an entirely new system of financial management, supply, and inventory management. He also created a new <b>Data</b> <b>Processing</b> Division to <b>centralize</b> the <b>data</b> <b>processing</b> functions of several combat service support branches.|$|R
30|$|The SPINE (signal {{processing}} in node environment) framework {{goes beyond}} {{by allowing the}} rapid prototyping of activity-aware application on the mobile phone using the data from motion sensors distributed on the body [29]. SPINE <b>centralizes</b> the <b>data</b> <b>processing</b> {{on the phone and}} is well suited to environments where a design-time-defined set of sensors are available. It does not, however, allow the run-time instantiation of Pervasive Apps according to the run time discovered resources, as we envision here.|$|R
40|$|Asset {{is one of}} {{valuable}} item in the industry or institution, missing or lose of asset may have problem in asset management system. The advantages of Radio Frequency Identification (RFID) technology have made this technology useful for asset management and tracking system. The use of active RFID technology for asset tracking is by attaching the tag at the asset or item with assigned a unique ID for identification. A few of active RFID readers install at strategic points or location to track asset movement and collect information when anyone of item pass by in reader coverage area, reader collect information with in reading range and send to backend system. Integration every single system by using wires or wireless methods to keep <b>centralize</b> <b>data</b> <b>processing</b> system. Alert message will be send to representative department to give warning. This asset tracking and management system that use active type of RFID technology is working at ISM band frequency of 433 MHz. The backend systems consist of application software, middleware and database. All the information have been sent from every single system recorded in one central database...|$|R
50|$|At {{the time}} of the New York's 1920s {{constitutional}} reforms, the Executive Department—headed by the Governor—housed only a few core functions such as budgeting, procurement, the state police and military and naval affairs. Since that time, numerous agencies have been created within the Executive Department to accommodate governmental functions not anticipated in the 1920s, while conforming with the limits established by the Constitution. These additions include divisions and offices that do not logically fit into the framework of the other departments, such as the Division of Veterans' Affairs (which advises veterans on services, benefits and entitlements, and administers payments of bonuses and annuities to blind veterans) and the Office of General Services (which provides <b>centralized</b> <b>data</b> <b>processing,</b> construction, maintenance and design services as well as printing, transportation and communication systems).|$|E
40|$|This paper {{deals with}} an {{original}} diagnosis system for LV switchboards. It {{is based on}} a local and global diagnosis approach with local temperature and current measurements. The thermal measurements including ambient and temperatures near electrical joint, are done by wireless thermal sensors. The system is composed of three complementary stages: An internet data collection stage, and two <b>centralized</b> <b>data</b> <b>processing</b> stages for the local detection of failures and the global diagnosis respectively. The analyses done with the diagnosis stage lead to predictive maintenance recommendations in order to avoid LV switchboard breakdown, which although rare could be catastrophic. Each stage of the <b>centralized</b> <b>data</b> <b>processing</b> is presented and discussed. Some results based on experimental data and expertise's information are presented to validate the feasibility of these methods...|$|E
30|$|An {{analysis}} of context recognition methods based on body-worn and environmental sensors {{was carried out}} in [14] and favors a streaming processing approach realized by an interconnection of tasks. This has {{led to the development of}} the Context Recognition Network [15]. This toolbox allows the realization of activity recognition algorithms by interconnecting signal processing elements using a simple scripting language. This system, however, assumes a static availability of sensors and only allows <b>centralized</b> <b>data</b> <b>processing.</b>|$|E
40|$|Context {{processing}} {{refers to}} the operation of processing different types of context data and/or information using different kinds of operators. These operators are applied according to some conditions or constraints given in context queries. Existing context aware systems process context <b>data</b> in a <b>centralized</b> fashion to answer context queries and generate context information. However, this method can cause scalability issues and give poor system throughput. In this paper, we aim {{to address this issue}} by proposing a distributed context <b>data</b> <b>processing</b> mechanism in which the context <b>data</b> <b>processing</b> computations of different context queries will be distributed to different computing devices. Relying on the developed prototype, a performance evaluation was conducted with <b>centralized</b> context <b>data</b> <b>processing</b> method as benchmark...|$|R
40|$|International audienceInternet of Things (IoT) {{will be one}} of {{the driving}} {{application}} for digital data generation in the next years as more than 50 billions of objects will be connected by 2020. IoT data can be processed and used by different devices spread all over the network. The traditional way of <b>centralizing</b> <b>data</b> <b>processing</b> in the Cloud can hardly scale because it cannot satisfy many of the latency critical IoT applications. In addition, it generates a too high network traffic when the number of objects and services increase. Fog infrastructure provides a beginning of an answer to such an issue. In this paper, we present a data placement strategy for Fog infrastructures called iFogStor. The objective of iFogStor is to take profit of the heterogeneity and location of Fog nodes to reduce the overall latency of storing and retrieving data in a Fog. We formulated the data placement problem as a Generalized Assignment Problem (GAP) and proposed two ways to solve it: 1) an exact solution using integer programming and 2) a heuristic one based on geographical zoning to reduce the solving time. Both solutions proved very good performance as they reduced the latency by more than 86 % as compared to a Cloud basedsolution and by 60 % as compared to a naive Fog solution. Using geographical zoning heuristic can allow solving problems with large number of Fog nodes efficiently and in a couple of seconds making iFogStor feasible in runtime and scalable...|$|R
40|$|The {{coordination}} of base stations in mobile access networks {{is an important}} approach to reduce harmful interference and to deliver high data rates to the users. Such coordination mechanisms, like Coordinated Multi-Point (CoMP) where multiple BSs transmit data to a user equipment, can be easily implemented when <b>centralizing</b> the <b>data</b> <b>processing</b> of the base stations, known as Cloud RAN. This centralization also imposes significant requirements on the backhaul network for high capacities and low latencies for the connections to the base stations. These requirements can be mitigated by (a) a flexible placement of the base station <b>data</b> <b>processing</b> functionality and by (b) dynamically assigning backhaul network resources. We show how these two techniques increase the feasibility of base station coordination in dense mobile access networks by using a heuristic algorithm. We furthermore present a prototype implementation of our approach based on software defined networking (SDN) with OpenDaylight and Maxinet...|$|R
40|$|Abstract. 172 basic Cessna {{plane in}} the process of operation, the {{production}} of equipment failure is random, so the evaluation of equipment performance and to predict its failure time to improve the safe operation of the 172 basic plane has important application value. On the plane this complex system, the grey theory combined with 172 basic Cessna plane, the collection of 172 basic aircraft fault information <b>centralized</b> <b>data</b> <b>processing,</b> analysis, prediction model GM (1, 1), through the calculation of the GM model data, and the error precision fitting test, better realize the basic 172 aircraft equipment failure time prediction...|$|E
40|$|International audienceThe BES-III {{experiment}} at the Institute of High Energy Physics (Beijing, China) {{is aimed}} at the precision measurements in e+e– annihilation in the energy range from 2. 0 till 4. 6 GeV. The world’s largest samples of J/psi and psi’ events and unique samples of XYZ data have been already collected. The expected increase of the data volume {{in the coming years}} required a significant evolution of the computing model, namely shift from a <b>centralized</b> <b>data</b> <b>processing</b> to a distributed one. This report summarizes a current design of the BES-III distributed computing system, some of key decisions and experience gained during 2 years of operations...|$|E
40|$|Object {{oriented}} development {{languages and}} event driven programming, distributed or <b>centralized</b> <b>data</b> <b>processing</b> with thick or rich clients are used at present in information systems (IS) development. Using of existing methodologies and methods In IS designing {{does not always}} mean reaching of needed project solution quality. In some cases it is more effective to use combination of structured and object oriented tools or new methodology, which is built according the newest information technologies. One of such methodologies is Advanced Customer Oriented Development of Software (ACES), which is oriented on IS of economic organization development and application of the newest information technologies. ACES was established by the authors at Faculty of Economic Informatics of University of Economics in Bratislava...|$|E
40|$|The {{project was}} divided into four subprojects. In three of them prototypes have been developed. The test results of these prototypes {{guarantee}} a high quality allowing preparatory activities to start production. The subprojects are: control unit for thread cutting {{by means of a}} universal cost effective electronic realization, special purpose sensor to measure flow rates of coolants in a small reaction time, especially for use in grinding machines and other machine tools, electronic measurement of water consumption with <b>centralized</b> <b>data</b> storing and <b>processing</b> (substitution of conventional mechanical principles), demands on a high-integrated ASIC used for a multichannel <b>data</b> <b>processing</b> system (Design study). (orig.) SIGLEAvailable from TIB Hannover: F 93 B 864 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Forschung und Technologie (BMFT), Bonn (Germany) DEGerman...|$|R
40|$|The {{concept of}} smart {{microgrid}} (SMG) is {{widely recognized as}} one of the most promising and enabling technologies for smart grids. In this context, the development of integrated and decentralized frameworks for executing complex control and monitoring applications is still in its infancy and needs to be researched. To address this issue, this paper conceptualizes a self-organizing computing framework, based on self-organizing agents, for solving the fundamental control and monitoring problems of an SMG without the need for <b>centralized</b> <b>data</b> acquisition and <b>processing.</b> Simulation results obtained for an 18 -bus test network are presented and discussed in order to demonstrate the significance and validity of the proposed new framework...|$|R
40|$|The {{computational}} technological efforts {{towards a}} <b>centralized</b> <b>data</b> storage and <b>processing</b> {{have contributed to}} provide more sustainable solutions under the environmental, administrative and business perspectives. However, {{it is not yet}} worldwide adopted by public institutions, specially, in the Brazilian educational systems, where these technological models are still under constant discussions and development. In this sense, this work presents a brief survey about cloud and mobile integrated technologies and their possible contributions to support a <b>centralized</b> <b>data</b> management in educational systems, relating improvements in governance, data security, mobility, economic viability and environmental impact. Therefore, this work also present a list of already free and private technologies and their advantages and disadvantages in the Brazilian scenario. In this sense, the herein technological aspects considers the integration between cloud and mobile technologies as essential alternative to suppress the online requirements, which a limitation for a large number of public institutions that have problems to be effectively connected on the Internet...|$|R
40|$|International audienceThe paper {{deals with}} an entire system of {{monitoring}} and diagnosis of LV switchboards based on the measurements of currents, ambient temperatures and local temperatures of electrical joints. This system meets the needs to prevent the breakdowns of LV switchboards, which, although rare, can involve huge financial and human loss. The thermal measurements are done by wireless thermal sensor. The measured data are transmitted via internet and collected in a server, to be centrally processed. This <b>centralized</b> <b>data</b> <b>processing</b> includes a local detection of failures and a global diagnosis which leads to some maintenance recommendations. This paper will focus on, the local detection by comparison with an healthy model, and the global diagnosis using Bayesian network technique. The feasibility of these methods is tested with experimental data and expert's information...|$|E
40|$|Mega City Apartment Bekasi is an {{integrated}} residence with shopping and office center developed by PT. Mega Utama Development. The sales data {{encountered in the}} Mega City apartment project is still manualy by using speadsheet application. There has been no <b>centralized</b> <b>data</b> <b>processing</b> with an application support, especially marketing division. The {{purpose of this study}} to build a performance-based sales information system at PT. Mega Utama Development. The method used in developing this information system is SDLC. The SDLC model used in the development of this information system is the "Classic Life Cycle" or waterfall model. With the support of performance-based sales information system, the sales process of apartment units in PT. Mega Utama Development becomes more effective and marketing managers can directly control the performance of sales eksekutifs based on performance reports on this information system...|$|E
40|$|In this paper, {{we propose}} a fuzzy system to control vehicle traffic flows {{on a street}} network. At a given point of the street network, data are {{collected}} by a peripheral unit equipped with infrared sensors. Row data are sent by the GSM/GPRS network to a <b>centralized</b> <b>data</b> <b>processing</b> server, where a simple set of fuzzy rules is employed to classify the row data samples into three flow states corresponding to flowing, intense and congested conditions. The core {{of the system is}} constituted by a neuro-fuzzy system, which is used to predict the time series constituted by the fuzzy membership of traffic measures to the three predefined flow states. We report the results concerning the comparison tests we carried out using an ANFIS network synthesized by a hyperplane clustering procedure and some well-known prediction techniques used as benchmarks...|$|E
40|$|Abstract. In most {{database}} {{systems with}} production rule facilities, rules respond to operations on <b>centralized</b> <b>data</b> and rule <b>processing</b> is {{performed in a}} centralized, sequential fashion. In parallel and distributed database environments, for maximum autonomy it is desirable for rule processing to occur separately at each site (or node), responding to operations on data at that site. However, since rules at one site may read rules respond to operations on <b>centralized</b> <b>data,</b> and rule <b>processing</b> usually is performed in a centralized, sequential fashion. (There has been some work on parallelizing rule firings {{in the context of}} the OPS 5 production rule language, but this work still considers <b>centralized</b> <b>data</b> and does not directly apply to database production rule languages; see Section 1. 1.) Given the high interest or modify data and interact with rules at other sites, inde- in parallel and distributed database systems [DGS+SO, pendent rule processing at each site may be impossible or incorrect. We describe mechanisms that allow rule processing to occur separately at each site and guarantee correctness: parallel or distributed rule processing is provably equivalent to rule processing in the corresponding centralized environment. Our mechanisms include locking schemes, communication protocols, and rule restrictions. Based on a given parallel or distributed environment and desired level of transparency, the mechanisms may be combined or may be used indepen-OV 91], it clearly is important for database production rule facilities to be adapted to these environments. For maximum autonomy in parallel and distributed environments, it is desirable for rule processing to occur separately at each site (or node), responding to operations on data at that site. Rule processing can, in fact, be completely independent at each site: as long as each rule is restricted to reference (i. e. be triggered by, read, and modify) data at one site only. However, this seridently. ously limits the expressible rules, provides no notion of transparency (the illusion to the user that the database...|$|R
40|$|Wireless sensor {{networks}} (WSNs) {{facilitate a}} new paradigm to structural identification and monitoring for civil infrastructure. Conventional structural monitoring systems based on wired sensors and <b>centralized</b> <b>data</b> acquisition systems are costly for installation as well as maintenance. WSNs have emerged as a technology that can overcome such difficulties, making deployment of a dense array of sensors on large civil structures both feasible and economical. However, as opposed to wired sensor networks in which <b>centralized</b> <b>data</b> acquisition and <b>processing</b> is common practice, WSNs require decentralized computing algorithms to reduce data transmission due to the limitation associated with wireless communication. In this paper, the stochastic subspace identification (SSI) technique is selected for system identification, and SSI-based decentralized system identification (SDSI) is proposed to be implemented in a WSN composed of Imote 2 wireless sensors that measure acceleration. The SDSI is tightly scheduled in the hierarchical WSN, and its performance is experimentally verified in a laboratory test using a 5 -story shear building model. ⓒ 2015 by the authors; licensee MDPI, Basel, Switzerlandopen 0...|$|R
40|$|Atmospheric {{electric}} parameters such as {{electric field}} strength and air polar electric conductivity have been continuously monitored {{on the network}} of Roshydromet stations since 1950 s. Evaluation of the data suggests that the changes of atmospheric electric parameters are caused both industrial emissions of aerosol gases and by emissions of radioactive substances during nuclear tests and the Chernobyl accident. Now the network works by the uniform technique; the <b>centralized</b> gathering, <b>data</b> <b>processing</b> and their storage in the databank “Atmospheric electricity, ver. 1. 0 ” is provided. The device for checking used atmospheric electric sensors is developed and authorized by the Rosstandart, and also methods of carrying out of their control in situ are developed. In connection with predicted climate changes, a change of atmospheric electric parameters near the earth's surface is probable, therefore the further support of these observations are important. Thus, our intentions consist in the following: development of the atmospheric electricity monitoring network, improvement of measuring instruments for satisfaction of the most actual requirements, maintenance of joint <b>processing</b> and the <b>data</b> analysis in a real mode of time, restoration of the international atmospheric electricity data exchange. ...|$|R
40|$|Associating sensor {{measurements}} with target tracks is {{a fundamental}} and challenging problem in multi-target tracking. The problem is even more challenging {{in the context of}} sensor networks, since association is coupled across the network, yet <b>centralized</b> <b>data</b> <b>processing</b> is in general infeasible due to power and bandwidth limitations. Hence efficient, distributed solutions are needed. We propose techniques based on graphical models to efficiently solve such data association problems in sensor networks. Our approach scales well with the number of sensor nodes in the network, and it is well [...] suited for distributed implementation. Distributed inference is realized by a message [...] passing algorithm which requires iterative, parallel exchange of information among neighboring nodes on the graph. So as to address trade [...] offs between inference performance and communication costs, we also propose a communication [...] sensitive form of message [...] passing that is capable of achieving near [...] optimal performance using far less communication. We demonstrate the effectiveness of our approach with experiments on simulated data...|$|E
40|$|Land-observing {{satellites}} {{with multiple}} thematic mappers will produce data at rates of 100 to 300 Mbps. When {{coupled with a}} high daily scene production rate, these rates will require new approaches to ground processing. Consideration is given here to future downlink rates and data volumes, and requirements peculiar to the future user community are discussed. The advanced technologies required to attain an operational system in the years 1985 - 1990 are considered, together with advances foreseen in communications, mass storage, bulk memories, and data processing. Using advanced devices, a <b>centralized</b> <b>data</b> <b>processing</b> system capable of handling the 100 Mbps data rate is described. New approaches, among them a parallel pipelined calibration front-end, real-time browse image production, a high bandwidth optical disk archive, regional image broadcast and massively parallel product production, are considered. A distributed system capable of handling the 300 Mbps data rate is then described. Designs for a hub system and a regional processing center are presented...|$|E
40|$|The Texas A & M University Library embraced {{automation}} {{as a way}} of {{life when}} it became the first library in the Southwest to employ a Data Processing Supervisor as a full-time Library staff member in September, 1964. The creation of such a position as part of the Library staff was only one of several favorable circumstances which combined to provide the necessary foundation for the achievements outlined in this paper. In addition to an enthusiastic University administration which provided requested supplemental funds for a special conversion project, the Library has access to the University's <b>centralized</b> <b>data</b> <b>processing</b> facility, which is one of the largest such University installations in the Southwest. The Data Processing Center houses an IBM 7094 - 1401 computer system with 14 magnetic tape drives, two separate off-line 1401 tape systems (one with a 1404 printer), and a battery of high speed sorters, collators, and card punches. This tremendous hardware capability has proved to be a great asset to our automation program. published or submitted for publicatio...|$|E
40|$|In most {{database}} {{systems with}} production rule facilities, rules respond to operations on <b>centralized</b> <b>data</b> and rule <b>processing</b> is {{performed in a}} centralized, sequential fashion. In parallel and distributed database environments, for maximum autonomy it is desirable for rule processing to occur separately at each site (or node), responding to operations on data at that site. However, since rules at one site may read or modify data and interact with rules at other sites, independent rule processing at each site may be impossible or incorrect. We describe mechanisms that allow rule processing to occur separately at each site and guarantee correctness: parallel or distributed rule processing is provably equivalent to rule processing in the corresponding centralized environment. Our mechanisms include locking schemes, communication protocols, and rule restrictions. Based on a given parallel or distributed environment and desired level of transparency, the mechanisms may be combined or may [...] ...|$|R
40|$|Driven by {{the needs}} to address {{problems}} with our rapidly aging civil infrastructure, structural health monitoring (SHM) has arisen {{as an important}} tool to improve maintenance and operation. Introduced as a promising alternative to the traditional wired sensors, wireless smart sensors offer unique features (low cost, wireless communication, onboard computation, and small size) that enable deployment of dense array of sensors essential for assessing structural damage. The <b>centralized</b> <b>data</b> collection approach, which the wired sensor system commonly employs, is not suitable to wireless smart sensor networks (WSSNs) due to limitations in the wireless communication; decentralized <b>data</b> aggregation and <b>processing</b> is required in the WSSNs. Rather than collecting uncondensed raw sensor <b>data</b> at a <b>centralized</b> location, in-network <b>data</b> <b>processing,</b> {{made possible by the}} onboard computational capability of smart sensors, is utilized to condense the raw data and extract meaningful information. By transferring only the condensed <b>data</b> to the <b>centralized</b> location, <b>data</b> communication over the wireless links can be greatly reduced. Decentralized data aggregation approaches can be placed in two broad categories: (a) independent processing (each node processes sensor data independently), and (b) coordinated processing (sensor nodes collaborate to process sensor data by sharing information). This report outlines the implementation of both decentralized data aggregation approaches for the WSSNs employing Crossbow???s Imote 2 smart sensor platform. Design considerations for developing WSSN applications are described herein, including network-wide flow and timing, fault-tolerant feature, and network topology to account for the decentralized data aggregation. WSSN applications introduced in this report can be downloaded at the Illinois SHM Project website ([URL] or submitted for publicatio...|$|R
40|$|Networks of {{hundreds}} or thousands of sensor nodes equipped with sensing, computing and communication ability are conceivable with recent technological advancement. Methods are presented in this report to recover and visualize data from wireless sensor networks, as well as to estimate node positions. A communication system is assumed wherein information from sensor nodes can be transferred to a <b>centralized</b> computer for <b>data</b> <b>processing,</b> though suggestions are made for extensions to distributed computation. Specifically, this report presents four topics. First, the notion of using network connectivity to reconstruct node positions via linear or semidefinite programming is explored. Random feasible node placement and bounding methods are both found to increase in precision with the indiviual geographical constraints. Second, the potential effectiveness of two correlation-based sensor data encoding schemes is reported. Blind correlation methods are found to provide meager compression while [...] ...|$|R
40|$|The paper {{develops}} DILOC, a distributive, iterative algorithm that locates M sensors in R m, m ≥ 1, {{with respect}} to a minimal number of m + 1 anchors with known locations. The sensors exchange data with their neighbors only; no <b>centralized</b> <b>data</b> <b>processing</b> or communication occurs, nor is there centralized knowledge about the sensors’ locations. DILOC uses the barycentric coordinates of a sensor {{with respect to}} its neighbors that are computed using the Cayley-Menger determinants. These are the determinants of matrices of inter-sensor distances. We show convergence of DILOC by associating with it an absorbing Markov chain whose absorbing states are the anchors. We introduce a stochastic approximation version extending DILOC to random environments when the knowledge about the intercommunications among sensors and the inter-sensor distances are noisy, and the communication links among neighbors fail at random times. We show a. s. convergence of the modified DILOC and characterize the error between the final estimates and the true values of the sensors ’ locations. Numerical studies illustrate DILOC under a variety of deterministic and random operating conditions. Keywords: Distributed iterative sensor localization; sensor networks; Cayley-Menger determinant; barycentric coordinates; absorbing Markov chain; stochastic approximation...|$|E
40|$|International audienceThe in-situ {{evaluation}} of the tensile load in cables and tie-rods plays a primary role in the continuous monitoring and health assessment of strategic as well as historic structures. The main advantage of the indirect determination of cable forces from dynamic tests in operational conditions {{is the opportunity to}} achieve accurate, cheap and fast quality checks in the construction phase (after pre-stressing), and safety checks and structural maintenance over the structure lifespan. Based on the most recent developments in the field of operational modal analysis, an automated system for continuous monitoring of axial loads based on dynamic measurements has been developed. The system consists of a distributed measurement system based on programmable hardware and wireless modules, and of a <b>centralized</b> <b>data</b> <b>processing</b> server for the estimation of the tensile loads in the monitored cable from records of its dynamic response to ambient vibrations. The experimental identification of the flexural modes in terms of natural frequencies and mode shapes is used to solve an inverse problem for the identification of the axial loads. A prototype of the SHM system has been installed to monitor the tensile load in one of the cables of a sample steel arch. Preliminary experimental results from continuous monitoring of the cable are discussed pointing out the effect of environmental factors...|$|E
40|$|The {{verification}} of aerospace structures, including full-scale fatigue and static test programs, {{is essential for}} structure strength design and evaluation. However, the current overall ground strength testing systems employ {{a large number of}} wires for communication among sensors and data acquisition facilities. The <b>centralized</b> <b>data</b> <b>processing</b> makes test programs lack efficiency and intelligence. Wireless sensor network (WSN) technology might be expected to address the limitations of cable-based aeronautical ground testing systems. This paper presents a wireless sensor network based aircraft strength testing (AST) system design and its evaluation on a real aircraft specimen. In this paper, a miniature, high-precision, and shock-proof wireless sensor node is designed for multi-channel strain gauge signal conditioning and monitoring. A cluster-star network topology protocol and application layer interface are designed in detail. To verify the functionality of the designed wireless sensor network for strength testing capability, a multi-point WSN based AST system is developed for static testing of a real aircraft undercarriage. Based on the designed wireless sensor nodes, the wireless sensor network is deployed to gather, process, and transmit strain gauge signals and monitor results under different static test loads. This paper shows the efficiency of the wireless sensor network based AST system, compared to a conventional AST system...|$|E
40|$|Cloud {{computing}} has {{showed up}} {{as a popular}} design in managing world to back up managing large volumetric details using cluster of commodity computer systems. It is the newest effort in offering and managing computing as a service. Cloud Computing has become a boon for an IT industry nowadays. It is like a next stage platform {{in the evolution of}} Internet. It provides a platform with an enhanced and efficient way to store data in the cloud i. e. server with different range of capabilities and application. It provides an easy way of accessing one’s personal file or data and use application without installing it on machines by just having Internet access. We can have efficient computing by <b>centralized</b> <b>data</b> storage, <b>processing</b> and bandwidth. Example: Yahoo, Gmail, Amazon etc. are good cloud service providers. So all we need is to have Internet access then we can send mail and can access our account from any part of the world. The server and the email management software is installed on the cloud and managed by service providers. Providing an easy access to work and business still it has a major problem and threat i. e. “DATA SECURITY”. In this Research Paper, we have tried to assess Cloud Storage Methodology and Data Security in cloud by the Implementation of digital signature with RSA algorithm...|$|R
40|$|ABSTRACT: Cloud Computing is a {{technology}} that uses the internet and central remote servers to maintain data and applications. Cloud computing allows consumers and businesses to use applications without installation and access their personal files at any computer with internet access. This technology allows for much more efficient computing by <b>centralizing</b> <b>data</b> storage, <b>processing</b> and bandwidth. The use of cloud computing has increased rapidly in many organizations. Cloud computing provides many benefits in terms of low cost and accessibility of data. Ensuring the security of cloud computing {{is a major factor}} in the cloud computing environment, as users often store sensitive information with cloud storage providers but these providers may be untrusted. Dealing with “single cloud ” providers is predicted to become less popular with customers due to risks of service availability failure and the possibility of malicious insiders in the single cloud. A movement towards “multi-clouds”, or in other words, “interclouds ” or “cloud-of clouds ” has emerged recently. This paper surveys recent research related to single and multi-cloud security and addresses possible solutions. It is found that the research into the use of multicloud providers to maintain security has received less attention from the research community than has the use of single clouds. This work aims to promote the use of multi-clouds due to its ability to reduce security risks that affect the cloud computing user...|$|R
40|$|Fog-aided network {{architectures}} for 5 G systems encompass {{wireless edge}} nodes, {{referred to as}} remote radio systems (RRSs), as well as remote cloud center (RCC) processors, which {{are connected to the}} RRSs via a fronthaul access network. RRSs and RCC are operated via Network Functions Virtualization (NFV), enabling a flexible split of network functionalities that adapts to network parameters such as fronthaul latency and capacity. This work focuses on uplink communications and investigates the cloud-edge allocation of two important network functions, namely the control functionality of rate selection and the data-plane function of decoding. Three functional splits are considered: (i) Distributed Radio Access Network (D-RAN), in which both functions are implemented in a decentralized way at the RRSs, (ii) Cloud RAN (C-RAN), in which instead both functions are carried out centrally at the RCC, and (iii) a new functional split, referred to as Fog RAN (F-RAN), with separate decentralized edge control and <b>centralized</b> cloud <b>data</b> <b>processing.</b> The model under study consists of a time-varying uplink channel in which the RCC has global but delayed channel state information (CSI) due to fronthaul latency, while the RRSs have local but more timely CSI. Using the adaptive sum-rate as the performance criterion, it is concluded that the F-RAN architecture can provide significant gains in the presence of user mobility. Comment: 28 pages, 11 figures. This manuscript was presented in part at arXiv: 1606. 0913...|$|R
