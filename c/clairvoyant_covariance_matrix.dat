1|10000|Public
40|$|Closed form {{expressions}} {{are derived}} for {{the calculation of}} the <b>clairvoyant</b> <b>covariance</b> <b>matrix</b> for radar sea clutter arising from individual clutter components with Gaussian spectra. Two forms are presented corresponding to a power series expansion of a sinc 2 antenna gain pattern and a Gaussian approximation to the mainlobe of the antenna pattern. The variation of covariance matrix eigenvalues due to factors such as coherent processing interval, spectra bandwidth, centre frequency and relative power is examined...|$|E
40|$|Abstract—Studies {{of massive}} MIMO in {{wireless}} communi-cations have recently attracted significant attention. Here we study {{the benefits of}} very large arrays in space-time adaptive processing (STAP) for radar by analyzing {{the performance of a}} reduced-dimension separable STAP algorithm that exploits the large-array assumption. In particular, we begin by studying the behavior of the algorithm for <b>clairvoyant</b> interference <b>covariance</b> <b>matrices</b> with orthogonality assumptions on the steering vectors, and show that in the asymptotic sense this simplified scheme performs as well as the fully adaptive STAP method. We then appeal to random matrix theory to analyze performance when the <b>covariance</b> <b>matrix</b> is estimated using secondary data. I...|$|R
3000|$|... } are {{the target}} <b>covariance</b> <b>matrix,</b> {{interference}} <b>covariance</b> <b>matrix,</b> deceptive jamming <b>covariance</b> <b>matrix,</b> and noise <b>covariance</b> <b>matrix,</b> respectively.|$|R
5000|$|... where [...] is the <b>covariance</b> <b>matrix</b> of X, [...] is the <b>covariance</b> <b>matrix</b> of Y and [...] is the <b>covariance</b> <b>matrix</b> between X and Y.|$|R
30|$|The {{technique}} applied here is {{to optimize}} the filter based on the <b>covariance</b> <b>matrix</b> of clutter and noise. The target statistics and waveform (orthogonal waveform initially) are also considered. The <b>covariance</b> <b>matrix</b> is a trained matrix of clutter statistics for K secondary data. Here, the clutter information is estimated by the received signals before the target appears. The <b>covariance</b> <b>matrix</b> of filter and clutter statistics are estimated. Using this <b>covariance</b> <b>matrix,</b> the signal <b>covariance</b> <b>matrix</b> is estimated from target, noise, clutter and filter <b>covariance</b> <b>matrix.</b> Then this waveform <b>covariance</b> <b>matrix</b> is normalized and transmitted by NxR MIMO radar system. Thus, obtained waveform is orthogonal optimal waveform.|$|R
50|$|In statistics, {{sometimes}} the <b>covariance</b> <b>matrix</b> of a multivariate random variable {{is not known}} but has to be estimated. Estimation of <b>covariance</b> <b>matrices</b> then deals {{with the question of}} how to approximate the actual <b>covariance</b> <b>matrix</b> on the basis of a sample from the multivariate distribution. Simple cases, where observations are complete, can be dealt with by using the sample <b>covariance</b> <b>matrix.</b> The sample <b>covariance</b> <b>matrix</b> (SCM) is an unbiased and efficient estimator of the <b>covariance</b> <b>matrix</b> if the space of <b>covariance</b> <b>matrices</b> is viewed as an extrinsic convex cone in Rp×p; however, measured using the intrinsic geometry of positive-definite matrices, the SCM is a biased and inefficient estimator. In addition, if the random variable has normal distribution, the sample <b>covariance</b> <b>matrix</b> has Wishart distribution and a slightly differently scaled version of it is the maximum likelihood estimate. Cases involving missing data require deeper considerations. Another issue is the robustness to outliers, to which sample <b>covariance</b> <b>matrices</b> are highly sensitive.|$|R
3000|$|As of {{the other}} covariance-based {{detection}} techniques, complexity of the PC algorithm also comprises of two major steps, one is the computation of the <b>covariance</b> <b>matrix</b> as in (6) {{and the other is}} the decomposition of the <b>covariance</b> <b>matrix</b> to calculate eigenvectors in our case. As the <b>covariance</b> <b>matrix</b> is a block Toeplitz and Hermitian, due to these properties of <b>covariance</b> <b>matrix,</b> we only need to evaluate its first block. Calculation of the <b>covariance</b> <b>matrix</b> requires (M [...]...|$|R
40|$|The Gaussian two-category {{classification}} {{problem with}} known category mean value vectors and identical but unknown category <b>covariance</b> <b>matrices</b> is considered. The weight vector {{depends on the}} unknown common <b>covariance</b> <b>matrix,</b> so the procedure is to estimate the <b>covariance</b> <b>matrix</b> {{in order to obtain}} an estimate of the optimum weight vector. The measure of performance for the adapted classifier is the output signal-to-interference noise ratio (SIR). A simple approximation for the expected SIR is gained by using the general sample <b>covariance</b> <b>matrix</b> estimator; this performance is both signal and true <b>covariance</b> <b>matrix</b> independent. An approximation is also found for the expected SIR obtained by using a Toeplitz form <b>covariance</b> <b>matrix</b> estimator; this performance is found to be dependent on both the signal and the true <b>covariance</b> <b>matrix...</b>|$|R
40|$|This thesis {{deals with}} {{univariate}} and multivariate rank statistics in making statistical inference. The affine equivariant ranks and rank <b>covariance</b> <b>matrix</b> (RCM) were defined and studied in Visuri, Ollila et al (2003). In this work, additional information {{is supposed to}} be known about the <b>covariance</b> <b>matrix</b> — it is assumed that the <b>covariance</b> <b>matrix</b> has a certain structure. Since in the location-scale families a rank <b>covariance</b> <b>matrix</b> is proportional to the inverse of the corresponding <b>covariance</b> <b>matrix,</b> a structure in the <b>covariance</b> <b>matrix</b> results in a certain structure in the RCM. The main problem of this thesis is how to estimate an RCM corresponding to a structured <b>covariance</b> <b>matrix</b> in families of elliptical distributions. As the first step, the RCM is transformed to a block-diagonal matrix using the affine equivariance property. Thereafter, the blocks on the diagonal of the transformed RCM are expressed via the marginal rank <b>covariance</b> <b>matrices.</b> To estimate one of the marginal rank <b>covariance</b> <b>matrices</b> of the transformed observations, a multivariate regression problem needs to be solved. An idea of how to reduce...|$|R
40|$|Maximum {{likelihood}} estimation under constraints for estimation in the Wishart {{class of}} distributions, is considered. It provides a unified approach to estimation {{in a variety}} of problems concerning <b>covariance</b> <b>matrices.</b> Virtually all <b>covariance</b> structures can be translated to constraints on the covariances. This includes <b>covariance</b> <b>matrices</b> with given structure such as linearly patterned <b>covariance</b> <b>matrices,</b> <b>covariance</b> <b>matrices</b> with zeros, independent <b>covariance</b> <b>matrices</b> and structurally dependent <b>covariance</b> <b>matrices.</b> The methodology followed in this paper provides a useful and simple approach to directly obtain the exact maximum likelihood estimates. These maximum likelihood estimates are obtained via an estimation procedure for the exponential class using constraints. [URL]...|$|R
3000|$|... are the {{estimation}} error <b>covariance</b> <b>matrix,</b> prediction error <b>covariance</b> <b>matrix,</b> and Kalman Gain, respectively; e [...]...|$|R
40|$|In this paper, {{the multivariate}} normal {{distribution}} with a Kronecker product structured <b>covariance</b> <b>matrix</b> is studied. Particularly focused is {{the estimation of}} a Kronecker structured <b>covariance</b> <b>matrix</b> of order three, the so called double separable <b>covariance</b> <b>matrix.</b> The suggested estimation generalizes the procedure proposed by Srivastava et al. (2008) for a separable <b>covariance</b> <b>matrix.</b> The restrictions imposed by separability and double separability are also discussed...|$|R
40|$|International audienceWe {{propose a}} novel {{variant of the}} <b>covariance</b> <b>matrix</b> {{adaptation}} evolution strategy (CMA-ES) using a <b>covariance</b> <b>matrix</b> parameterized with {{a smaller number of}} parameters. The motivation of a restricted <b>covariance</b> <b>matrix</b> is twofold. First, it requires less internal time and space complexity that is desired when optimizing a function on a high dimensional search space. Second, it requires less function evaluations to adapt the <b>covariance</b> <b>matrix</b> if the restricted <b>covariance</b> <b>matrix</b> is rich enough to express the variable dependencies of the problem. In this paper we derive a computationally efficient way to update the restricted <b>covariance</b> <b>matrix</b> where the model richness of the <b>covariance</b> <b>matrix</b> is controlled by an integer and the internal complexity per function evaluation is linear in this integer times the dimension, compared to quadratic in the dimension in the CMA-ES. We prove that the proposed algorithm is equivalent to the sep-CMA-ES if the <b>covariance</b> <b>matrix</b> is restricted to the diagonal matrix, it is equivalent to the original CMA-ES if the matrix is not restricted. Experimental results reveal the class of efficiently solvable functions depending on the model richness of the <b>covariance</b> <b>matrix</b> and the speedup over the CMA-ES...|$|R
3000|$|... ([3] or see {{for example}} our {{demonstration}} in [4]), {{because even though}} {{the elements of the}} sample <b>covariance</b> <b>matrix</b> are unbiased estimates of the elements of the population <b>covariance</b> <b>matrix,</b> the eigenvalues of the sample <b>covariance</b> <b>matrix,</b> the sample eigenvalues L[*]=[*]{l [...]...|$|R
40|$|In {{this paper}} the multivariate normal {{distribution}} with a Kronecker product structured <b>covariance</b> <b>matrix</b> is studied. Particularly, estimation of a Kronecker structured <b>covariance</b> <b>matrix</b> of order three, {{the so called}} double separable <b>covariance</b> <b>matrix.</b> The estimation procedure, suggested in this paper, is a generalization of the procedure derived by Srivastava et al. (2008), for a separable <b>covariance</b> <b>matrix.</b> Furthermore, the restrictions imposed by separability and double separability are discussed...|$|R
40|$|The {{problem of}} {{estimating}} a positive semi-definite Toeplitz <b>covariance</b> <b>matrix</b> {{consisting of a}} low rank matrix plus a scaled identity from noisy data arises in many applications. We propose a computationally attractive (noniterative) <b>covariance</b> <b>matrix</b> estimator with certain optimality properties. For example, under suitable assumptions the proposed estimator achieves the Cramer-Rao lower bound on the <b>covariance</b> <b>matrix</b> parameters. The resulting <b>covariance</b> <b>matrix</b> estimate is also guaranteed to possess all of the structural properties of the true <b>covariance</b> <b>matrix.</b> Previous approaches to this problem have either resulted in computationally unattractive iterative solutions or have provided estimates that only satisfy some of the structural relations...|$|R
40|$|National audienceWe aim {{to improve}} the {{robustness}} of sound localization to diffuse noise coming from various directions. Using a subspace model of the noise <b>covariance</b> <b>matrix,</b> we estimate the signal <b>covariance</b> <b>matrix</b> from the observed <b>covariance</b> <b>matrix</b> and subsequently apply MUSIC (MUltiple SIgnal Classification). In [2], we proposed a maximum likelihood method given the rank of the signal <b>covariance</b> <b>matrix.</b> In this paper, we propose an alternative method based on minimizing the trace norm...|$|R
40|$|Abstract—Adaptive {{beamforming}} methods degrade in {{the presence}} of model mismatch. In this paper, we develop a modified interference <b>covariance</b> <b>matrix</b> reconstruction based beamformer that is robust against large array calibration errors. The calibration errors can come from the element position errors, and/or amplitude and phase errors, etc [...] The proposed method is {{based on the fact that}} the sample <b>covariance</b> <b>matrix</b> can approximate the interference <b>covariance</b> <b>matrix</b> properly when the desired signal is small, and a reconstructed <b>covariance</b> <b>matrix</b> based on the Capon spectral will be better than the sample <b>covariance</b> <b>matrix</b> when the desired signal is large. A weighted summation of two <b>covariance</b> <b>matrices</b> in references is used to reconstruct the interference <b>covariance</b> <b>matrix.</b> Moreover, a computationally efficient convex optimization-based algorithm is used to estimate the mismatch of the steering vector associated with the desired signal. Several simulation cases are applied to show the superiority of the proposed method over other robust adaptive beamformers. 1...|$|R
40|$|There is {{normally}} a simple choice {{made in the}} form of the <b>covariance</b> <b>matrix</b> to be used with HMMs. Either a diagonal <b>covariance</b> <b>matrix</b> is used, with the underlying assumption that elements of the feature vector are independent, or a full or block-diagonal matrix is used, where all or some of the correlations are explicitly modelled. Unfortunately when using full or block-diagonal <b>covariance</b> <b>matrices</b> there tends to be a dramatic increase in the number of parameters per Gaussian component, limiting the number of components which may be robustly estimated. This paper introduces a new form of <b>covariance</b> <b>matrix</b> which allows a few "full" <b>covariance</b> <b>matrices</b> to be shared over many distributions, whilst each distribution maintains its own "diagonal" <b>covariance</b> <b>matrix.</b> In contrast to other schemes which have hypothesised a similar form, this technique fits within the standard maximum-likelihood criterion used for training HMMs. The new form of <b>covariance</b> <b>matrix</b> is evaluated on a large-vocabulary [...] ...|$|R
40|$|The {{determinant}} of the <b>covariance</b> <b>matrix</b> for high-dimensional data {{plays an important}} role in statistical inference and decision. It has many real applications including statistical tests and information theory. Due to the statistical and computational challenges with high dimensionality, little work has been proposed in the literature for estimating the {{determinant of}} high-dimensional <b>covariance</b> <b>matrix.</b> In this paper, we estimate the determinant of the <b>covariance</b> <b>matrix</b> using some recent proposals for estimating high-dimensional <b>covariance</b> <b>matrix.</b> Specifically, we consider a total of eight <b>covariance</b> <b>matrix</b> estimation methods for comparison. Through extensive simulation studies, we explore and summarize some interesting comparison results among all compared methods. We also provide practical guidelines based on the sample size, the dimension, and the correlation of the data set for estimating the determinant of high-dimensional <b>covariance</b> <b>matrix.</b> Finally, from a perspective of the loss function, the comparison study in this paper may also serve as a proxy to assess the performance of the <b>covariance</b> <b>matrix</b> estimation...|$|R
3000|$|... [...]. In (9), the <b>covariance</b> <b>matrix</b> C is {{calculated}} using (30) {{in terms of}} Q and the noise <b>covariance</b> <b>matrix</b> [...]...|$|R
3000|$|... [...]. It {{should be}} noted that, in this {{formulation}} the first term in the inverse operator is the noise <b>covariance</b> <b>matrix,</b> the second term is the desired signal <b>covariance</b> <b>matrix</b> and the last term is the summation of CCI <b>covariance</b> <b>matrices</b> originating from interfering BSs not serving user [...]...|$|R
40|$|We {{consider}} {{the problem of}} constructing a fixed-size confidence region of the difference of two multinormal means when the <b>covariance</b> <b>matrices</b> have intraclass correlation structure. When the <b>covariance</b> <b>matrices</b> are known, we derive an optimal allocation. A two-stage procedure is given for the problem with unknown <b>covariance</b> <b>matrices...</b>|$|R
30|$|Interestingly, {{it can be}} {{seen that}} the inverse <b>covariance</b> <b>matrix</b> {{asymptotically}} exhibits a similar structure to that of the <b>covariance</b> <b>matrix</b> model.|$|R
30|$|So we can {{estimate}} the <b>covariance</b> <b>matrix</b> {{of the system}} {{as long as we}} calculate the <b>covariance</b> <b>matrix</b> of the residual vectors.|$|R
40|$|Accurate <b>covariance</b> <b>matrix</b> {{estimation}} for high-dimensional {{data can}} be a difficult problem. A good approximation of the <b>covariance</b> <b>matrix</b> needs in most cases a prohibitively large number of pixels, that is, pixels from a stationary section of the image whose number is greater than several {{times the number of}} bands. Estimating the <b>covariance</b> <b>matrix</b> with a number of pixels that is on the order of the number of bands or less will cause not only a bad estimation of the <b>covariance</b> <b>matrix</b> but also a singular <b>covariance</b> <b>matrix</b> which cannot be inverted. In this paper we will investigate two methods to give a sufficient approximation for the <b>covariance</b> <b>matrix</b> while only using a small number of neighboring pixels. The first is the quasilocal <b>covariance</b> <b>matrix</b> (QLRX) that uses the variance of the global covariance instead of the variances that are too small and cause a singular covariance. The second method is sparse matrix transform (SMT) that performs a set of K-givens rotations to estimate the <b>covariance</b> <b>matrix.</b> We will compare results from target acquisition that are based on both of these methods. An improvement for the SMT algorithm is suggested...|$|R
40|$|Covariances play a {{fundamental}} {{role in the}} theory of time series and they are critical quantities that are needed in both spectral and time domain analysis. Estimation of <b>covariance</b> <b>matrices</b> is needed in the construction of confidence regions for unknown parameters, hypothesis testing, principal component analysis, prediction, discriminant analysis among others. In this paper we consider both low- and high-dimensional <b>covariance</b> <b>matrix</b> estimation problems and present a review for asymptotic properties of sample <b>covariances</b> and <b>covariance</b> <b>matrix</b> estimates. In particular, we shall provide an asymptotic theory for estimates of high dimensional <b>covariance</b> <b>matrices</b> in time series, and a consistency result for <b>covariance</b> <b>matrix</b> estimates for estimated parameters. ...|$|R
40|$|This {{paper is}} the third chapter {{of three of the}} author's {{undergraduate}} thesis. In this paper, we study the convergence of local bulk statistics for linearized <b>covariance</b> <b>matrices</b> under Dyson's Brownian motion. We consider deterministic initial data $V$ approximate the Dyson Brownian motion for linearized <b>covariance</b> <b>matrices</b> by the Wigner flow. Using universality results for the Wigner flow, we deduce universality for the linearized <b>covariance</b> <b>matrices.</b> We deduce bulk universality of averaged bulk correlation functions for both biregular bipartite graphs and honest <b>covariance</b> <b>matrices.</b> We also deduce a weak level repulsion estimate for the Dyson Brownian motion of linearized <b>covariance</b> <b>matrices.</b> Comment: 32 page...|$|R
3000|$|... {{is testing}} the null {{hypothesis}} that the <b>covariance</b> <b>matrix</b> implied by the model {{is equal to the}} population <b>covariance</b> <b>matrix.</b> Since X [...]...|$|R
30|$|Note {{that the}} {{detection}} scheme in (9) assumes no {{structure for the}} <b>covariance</b> <b>matrix,</b> except that the <b>covariance</b> <b>matrix</b> is Hermitian and non-singular.|$|R
40|$|This thesis {{considers}} {{two problems}} related to high-dimensional <b>covariance</b> <b>matrices,</b> namely, <b>covariance</b> <b>matrix</b> estimation and multivariate volatility modeling. <b>Covariance</b> <b>matrix</b> estimation is important in many statistical methods and applications. For example, it is applied in asset allocation, classification of human tumors based on gene expression arrays, and many others. Sample <b>covariance</b> <b>matrix</b> is frequently used as an estimator of the population <b>covariance</b> <b>matrix.</b> However, sample <b>covariance</b> <b>matrix</b> becomes poor and unstable {{with the increase in}} the dimensions of the data vectors. A typical problem is that the eigenvalues of the population <b>covariance</b> <b>matrix</b> become distorted. This thesis proposes a method for resolving this problem, namely, an efficient estimation method that imposes a new likelihood penalty on the <b>covariance</b> <b>matrix.</b> The proposed estimator is guaranteed to be non-negative definite and its estimation algorithm is fast and efficient. The proposed method is compared with several existing methods via simulation and empirical studies. Another interesting topic in analyzing high-dimensional <b>covariance</b> <b>matrices</b> is multivariate volatility modeling, which is crucial in option pricing, portfolio risk forecasting, and risk measurement and management. The increasing availability of intra-day trading data has drawn widespread attention to the study of daily realized <b>covariance</b> <b>matrices</b> constructed from high-frequency data. The existing methods for reducing the dimensions of a sample of <b>covariance</b> <b>matrices</b> include matrix-based methods such as matrix factor (MFA) model and common component analysis (CCA), and vector-based method such as principal component analysis (PCA). The relationship and differences in performance between these two classes of methods have yet to be explored in the literature. In this thesis, I fill this research gap by comparing the methods theoretically and empirically. Note that the factor loading matrix in the MFA model is constant over time, and hence it may not work well in forecasting high-dimensional <b>covariance</b> <b>matrices.</b> To resolve this problem, a dynamic matrix factor (DMF) model is proposed to allow the loading matrix to vary over time. The DMF model assumes that the realized <b>covariance</b> <b>matrix</b> follows a central Wishart distribution with the conditional expectation of the realized <b>covariance</b> <b>matrix</b> being decomposed into a loading matrix and a diagonal matrix via spectral decomposition. In the DMF model, a loading-driven process with the scalar BEKK model is used to capture the dynamics of the loading matrix, and each diagonal term is modeled by a separate GARCH(1, 1) model. The forecasts of the realized <b>covariance</b> <b>matrices</b> are guaranteed to be positive definite. To maintain a parsimonious model structure, the DMF model is extended to the case of high-dimensional <b>covariance</b> <b>matrix.</b> Finally, the proposed DMF models are applied to several real-world data sets. published_or_final_versionStatistics and Actuarial ScienceDoctoralDoctor of Philosoph...|$|R
3000|$|... is {{the signal}} <b>covariance</b> <b>matrix,</b> σ 2 is {{the energy of}} Gaussian white noise, I is the {{normalized}} noise <b>covariance</b> <b>matrix,</b> and ([*]⋅[*]) [...]...|$|R
40|$|The {{combination}} of tracking and regularization in recursive identification is studied. It is shown that regularization {{of the information}} matrix corresponds to a normalization of the <b>covariance</b> <b>matrix,</b> and {{that several of the}} proposed methods for dealing with <b>covariance</b> <b>matrix</b> blow up can be interpreted as approximate implementations of <b>covariance</b> <b>matrix</b> normalization...|$|R
40|$|<b>Covariance</b> <b>matrix</b> {{data has}} gained {{significant}} importance in many applications, e. g. diffusion tensor imaging, principal component analysis, structure tensor analysis, etc. Our work mainly focuses on developing statistical methodologies for analysing <b>covariance</b> <b>matrix</b> data, {{taking into account}} the non-Euclidean nature of the <b>covariance</b> <b>matrix.</b> Non-Euclidean metrics including the log-Euclidean, Riemannian, power Euclidean and, in particular, Procrustes size-and-shape distance have been used for defining the mean of <b>covariance</b> <b>matrices.</b> The proposed methods have been applied to process diffusion tensor images from human brain...|$|R
40|$|The {{eigenvalue}} {{spectrum of}} <b>covariance</b> <b>matrices</b> is of central importance {{to a number}} of data analysis techniques. Usually the sample <b>covariance</b> <b>matrix</b> is constructed from a limited number of noisy samples. We describe a method of inferring the true eigenvalue spectrum from the sample spectrum. Results of Silverstein which characterise the eigenvalue spectrum of the noise <b>covariance</b> <b>matrix</b> and inequalities between the eigenvalues of Hermitian matrices are used to infer probability densities for the eigenvalues of the noise-free <b>covariance</b> <b>matrix,</b> using Bayesian inference. Posterior densities for each eigenvalue are obtained, which yield error estimates. The evidence framework gives estimates of the noise variance and permits model order selection by estimating the rank of the <b>covariance</b> <b>matrix.</b> The method is illustrated with numerical examples. Keywords: sample covariance, eigenvalue spectrum, Bayesian evidence, model order selection. 1 Introduction The <b>covariance</b> <b>matrix</b> and its spectrum [...] ...|$|R
30|$|H_ 0 : the {{retained}} points {{come from}} a gaussian distribution with <b>covariance</b> <b>matrix</b> given by the pooled <b>covariance</b> <b>matrix</b> (13) of all the clusters.|$|R
