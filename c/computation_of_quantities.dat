16|10000|Public
30|$|Such {{operators}} are particularly {{useful in the}} distributed implementation of signal processing tasks related to learning or regularization on graphs, such as denoising [4], semi-supervised learning [18], signal reconstruction [3], interpolation and reconstruction of bandlimited graph signals [19]. These types of applications typically require the <b>computation</b> <b>of</b> <b>quantities</b> such as the forward application of the dictionary and its adjoint to be performed only by local exchange of information.|$|E
40|$|Concerning Numerical Stochastic Perturbation Theory, {{we discuss}} the {{convergence}} of the stochastic process (idea of the proof, features of the limit distribution, rate of convergence to equilibrium). Then we also discuss the expected fluctuations in the observables and give some idea to reduce them. In the end we show that also <b>computation</b> <b>of</b> <b>quantities</b> at fixed (Landau) Gauge is now possible. Comment: 3 pages. Contributed to 17 th International Symposium on Lattice Field Theory (LATTICE 99), Pisa, Italy, 29 Jun - 3 Jul 199...|$|E
40|$|AbstractWe {{consider}} the approximation of Reissner–Mindlin plates with curved boundaries, using a p-version MITC finite element method. We describe {{in detail the}} formulation {{and implementation of the}} method, and emphasize the need for a Piola-type map in order to handle the curved geometry of the elements. The results of our numerical computations demonstrate the robustness of the method and suggest that it gives near exponential convergence when the error is measured in the energy norm. For the robust <b>computation</b> <b>of</b> <b>quantities</b> of engineering interest, such as the shear force, the proposed method yields very satisfactory results without the need for any additional post-processing. Comparisons are made with the standard finite element formulation, with and without post-processing...|$|E
50|$|Witness tank {{soundings}} {{of ships}} and barges, including the <b>computation</b> <b>of</b> the <b>quantity</b> <b>of</b> liquids in tanks.|$|R
30|$|<b>Computation</b> <b>of</b> Lyapunov <b>quantities</b> is a {{hot topic}} with large number of {{articles}} per year, {{but it is very}} difficult to obtain general results. The methods <b>of</b> <b>computation</b> <b>of</b> Lyapunov <b>quantities</b> when the critical points are non-degenerate have been greatly developed by many mathematicians. The method in [1, 2] is based on the sequential construction of Lyapunov functions. Furthermore, computing Lyapunov quantities using the reduction of system to normal form could be seen in [3 – 5]. Another approach to numerical <b>computation</b> <b>of</b> Lyapunov <b>quantities</b> which uses the passage to the polar coordinates and the procedure of sequential construction of solution approximations is related with the obtaining of approximations of system solution, see [2], they also could be seen in [6, 7]. But <b>computations</b> <b>of</b> Lyapunov <b>quantities</b> become difficult when the critical points are degenerate because the method of the Poincaré formal series cannot be used in order to compute Lyapunov constants in a neighborhood of the critical point.|$|R
50|$|Some analemmas {{are marked}} {{to show the}} {{position}} of the Sun on the graph on various dates, a few days apart, throughout the year. This enables the analemma to be used to make simple analog <b>computations</b> <b>of</b> <b>quantities</b> such as the times and azimuths of sunrise and sunset. Analemmas without date markings are used as decorations on such things as sundials. They have little practical usefulness.|$|R
40|$|In a {{previous}} paper [1], we derived formulae for estimating the storage {{requirements of the}} Rectangular and L-shaped Corner Stitching data structures [2, 3] for a given layout. These formulae require the <b>computation</b> <b>of</b> <b>quantities</b> called violations, which are geometric properties of the layout. In this paper, we present optimal Θ(n log n) algorithms for computing violations, where n {{is the number of}} rectangles in the layout. These algorithms are incorporated into a software tool called CLOTH MEASURE. Experiments conducted with CLOTH MEASURE show that it is a viable tool for estimating the memory requirements of a layout without having to implement the corner stitching data structures, which is a tedious and time-consuming task...|$|E
40|$|We {{discuss the}} {{formulation}} of "thermal renormalization group-equations" and their application to the finite temperature phase-transition of scalar O(N) -theories. Thermal renormalization group-equations allow for a computation of both the universal and the non-universal aspects of the critical behavior directly {{in terms of the}} zero-temperature physical couplings. They provide a nonperturbative method for a <b>computation</b> <b>of</b> <b>quantities</b> like real-time correlation functions in a thermal environment, where in many situations straightforward perturbation theory fails due to the bad infrared-behavior of the thermal fluctuations. We present results for the critical temperature, critical exponents and amplitudes as well as the scaling equation of state for self-interacting scalar theories. Comment: 32 pages with 10 figures and 4 tables included, latex 2...|$|E
40|$|Abstract. We {{present a}} method for the semi-automatic, model-driven {{segmentation}} of the Left Ventricle (LV) in cardiac SPECT data. Accurate segmentation of the LV allows <b>computation</b> <b>of</b> <b>quantities</b> that describe the degree of a perfusion defect. A 3 D surface is initialised in the dataset and deforms to fit {{the shape of the}} LV. The model is chosen from a small number of prototypes. The behaviour of the dynamic model is controlled by an Active Surface which is simulated by using the Finite Element Method. Segmentation results were obtained for nine datasets for which a manual segmentation by an expert existed. Results show a good agreement with the manual segmentation with an average of the mean contour distance of 0. 24 voxel. ...|$|E
3000|$|... [...]. Fortunately careful {{curation}} of {{the number}} of previously encountered connections and gaps facilitates the <b>computation</b> <b>of</b> these <b>quantities.</b> Line 40 computes the number of structures where v [...]...|$|R
5000|$|The <b>computation</b> <b>of</b> {{an average}} <b>quantity</b> [...] over the {{phase-space}} requires {{the evaluation of}} an integral: ...|$|R
5000|$|<b>Computation</b> <b>of</b> many {{measuring}} <b>quantities,</b> e.g. diffusion coefficients, stress-strain diagrams, elastic constants, distribution functions, correlation {{functions and}} shortest-path-ring statistics ...|$|R
40|$|By {{making use}} of Numerical Stochastic Perturbation Theory (NSPT) we can compute {{renormalization}} constants for Lattice QCD to high orders, e. g. three or four loops for quark bilinears. We report {{on the status of}} our computations, which provide several results for Wilson quarks and in particular (values and/or ratios of) Z_V, Z_A, Z_S, Z_P. Results are given for various number of flavors (n_f = 0, 2, 3, 4). While we recall the care which is due for the <b>computation</b> <b>of</b> <b>quantities</b> for which an anomalous dimension is in place, we point out that our computational framework is well suited to a variety of other calculations and we briefly discuss the application of NSPT to other regularizations (in particular the Clover action) ...|$|E
40|$|I {{review a}} number of {{theoretical}} issues in the <b>computation</b> <b>of</b> <b>quantities</b> in heavy-quark physics on the lattice. Since, particularly for the b-quark, mba> 1, {{it is necessary to}} use effective theories, such as the Heavy Quark Effective Theory (HQET). In order to be useful for flavour physics, power corrections in 1 /mb must be calculated, leading to ultra-violet divergences which behave like inverse powers of the lattice spacing. I argue that the mixing coefficients between operators of different dimensions must be determined non-perturbatively if generic non-perturbative QCD effects are to be included. I briefly outline the Zeuthen approach for achieving such a non-perturbative renormalization. I also discuss the Fermilab formulation of heavy quark physics on the lattice and its generalizations, noting that non-perturbative determinations of the parameters of the theory are beginning...|$|E
40|$|In {{the past}} fifteen years {{computational}} statistics has been enriched by a powerful, somewhat abstract method of generating variates from a target probability distribution that is based on Markov chains whose stationary distribution is the probability distribution of interest. This class of methods, popularly referred to as Markov chain Monte Carlo methods, or simply MCMC methods, have been influential in the modern practice of Bayesian statistics where these methods are used to summarize the posterior distributions that arise {{in the context of the}} Bayesian prior-posterior analysis (Tanner and Wong, 1987; Gelfand and Smith, 1990; Smith and Roberts, 1993; Tierney, 1994; Besaget al., 1995; Chib and Greenberg, 1995, 1996; Gilks et al., 1996; Tanner, 1996; Gammerman, 1997; Robert and Casella, 1999; Carlin and Louis, 2000; Chen et al., 2000; Chib, 2001; Congdon, 2001; Liu, 2001; Robert, 2001; Gelman at al, 2003). MCMC methods have proved useful in practically all aspects of Bayesian inference, for example, in the context of prediction problems and in the <b>computation</b> <b>of</b> <b>quantities,</b> such as the marginal likelihood, that are used for comparing competing Bayesian models. [...] ...|$|E
40|$|Recent {{progress}} in Lattice QCD is highlighted. After a brief {{introduction to the}} methodology <b>of</b> lattice <b>computations</b> the presentation focuses on three main topics: Hadron Spectroscopy, Hadron Structure and Lattice Flavor Physics. In each case a summary <b>of</b> recent <b>computations</b> <b>of</b> selected <b>quantities</b> is provided. ...|$|R
40|$|Multiple {{harmonic}} sums {{appear in}} the perturbative <b>computation</b> <b>of</b> various <b>quantities</b> <b>of</b> interest in quantum field theory. In this article we introduce a class of Hopf algebras that describe the structure of such sums, and develop some of their properties that can be exploited in calculations. Comment: Talk for Loops & Legs in QFT, Zinnowitz, 200...|$|R
40|$|A global {{quantity}} called "theoretical dimension" {{is roughly}} {{proportional to the}} number of coherent structures that expert observers count in simulated two-dimensional turbulent viscous flows. This paper reviews some previously published <b>computations</b> <b>of</b> this <b>quantity</b> for a few academic examples and for a small number of flows computed from random initial vorticity fields...|$|R
40|$|This paper {{focuses on}} a study related to the rate of removal of {{pollutants}} (suspended solids or particulates) from granular media deep-bed (depth) filters designed and constructed {{for the production of}} potable water. An approach was developed for <b>computation</b> <b>of</b> <b>quantities</b> of solids deposited in the pores of granular media filters. The continuity equation was applied in assessing the deposition (accumulation) of pollutants within the interstices of mono and dual-media filtration beds in space and time. Normalized turbidity removal-ratio curves were developed for the dual-media and mono-media filters using data from pilot filters that exhibited high efficiency. The results of the study indicate that deposit morphology decreases with media depth and increases with time. The total quantities of material obtained from the summation of deposits in individual layers are substantially equal for the dual and mono-media filters up to the end of filter run time (14 hours) for the mono-media sand filter. The results and findings from this study will be useful to designers and managers of water treatment works in developing and developed countries...|$|E
40|$|The {{study of}} {{differential}} equations, or dynamical systems in general, has two fundamentally different approaches. We are most {{familiar with the}} construction of solutions to differential equations. Another approach is to study the statistical behavior of the solutions. Ergodic Theory {{is one of the most}} developed methods to study the statistical behavior of the solutions of differential equations. In the theory of satellite orbits, the statistical behavior of the orbits is used to produce 'Coverage Analysis' or how often a spacecraft is in view of a site on the ground. In this paper, we consider the use of Ergodic Theory for Coverage Analysis. This allows us to greatly simplify the <b>computation</b> <b>of</b> <b>quantities</b> such as the total time for which a ground station can see a satellite without ever integrating the trajectory, see Lo 1, 2. More over, for any quantity which is an integrable function of the ground track, its average may be computed similarly without the integration of the trajectory. For example, the data rate for a simple telecom system is a function of the distance between the satellite and the ground station. We show that such a function may be averaged using the Ergodic Theorem...|$|E
40|$|Conference ABSTRACT: We have {{developed}} {{a new type of}} model for the recurrence time between earthquakes greater than some (large) reference magnitude e. g. M 7. The model is the sum of two parts, representing 'aftershocks ' and 'background ' earthquakes. The model allows ready <b>computation</b> <b>of</b> <b>quantities</b> of interest in seismic hazard. In particular it allows a calculation of the distribution of time of occurrence to the next (large) earthquake in a region given any elapsed time since the last. The model is characterised by four parameters, of which two are critical: the relative weights of the two parts and a time constant for the 'background'. It appears that 30 - 45 % of M ≥ 7 earthquakes can be regarded as aftershocks, the balance being 'new ' earthquakes. The model fits quite well the time between earthquakes of M ≥ 7 in New Zealand since 1840. However, comparison with similar global models suggests that some M ≥ 7 aftershocks are missing from the NZ catalogue. Conditional probability curves show that, in New Zealand, the probability of an M 7 earthquake following an M 7 earthquake is several times greater than predicted by current Poisson models for intervals up to about three years following the first earthquake...|$|E
40|$|We give an {{introduction}} to several regularization schemes that deal with ultraviolet and infrared singularities appearing in higher-order computations in quantum field theories. Comparing the <b>computation</b> <b>of</b> simple <b>quantities</b> in the various schemes, we point out {{similarities and differences between}} them. Comment: 61 pages, 12 figures; version sent to EPJC, references update...|$|R
40|$|The work at hand {{treats the}} {{extension}} of the classical computational homogenization scheme towards the multi-scale <b>computation</b> <b>of</b> material <b>quantities</b> like the Eshelby stresses and material forces. To this end, two approaches are elaborated and their consistency with respect to the virtual work principle, in terms of a Hill-Mandel type condition, is checked...|$|R
40|$|We {{investigate}} finite lattice approximations to the Wilson Renormalization Group in {{models of}} unconstrained spins. We discuss first {{the properties of}} the Renormalization Group Transformation (RGT) that control the accuracy of this type of approximations and explain different methods and techniques to practically identify them. We also discuss how to determine the anomalous dimension of the field. We apply our considerations to a linear sigma model in two dimensions in the domain of attraction of the Ising Fixed Point using a Bell-Wilson RGT. We are able to identify optimal RGTs which allow accurate <b>computations</b> <b>of</b> <b>quantities</b> such as critical exponents, fixed point couplings and eigenvectors with modest statistics. We finally discuss the advantages and limitations of this type of approach. Comment: LaTeX file, 38 pages, 21 eps figures, uses epsfi...|$|R
40|$|We {{consider}} {{flows in}} fractured media, described by Discrete Fracture Network (DFN) mod-els. We perform an Uncertainty Quantification analysis, assuming the fractures ’ transmissivity coefficients to be random variables. Two probability distributions (log-uniform and log-normal) are used within different laws that express the coefficients {{in terms of}} a family of independent stochastic variables; truncated Karhunen-Loève expansions provide instances of such laws. The approximate <b>computation</b> <b>of</b> <b>quantities</b> of interest, such as mean value and variance for outgoing fluxes, is based on a stochastic collocation approach that uses suitable sparse grids in the range of the stochastic variables (whose number defines the stochastic dimension of the problem). This produces a non-intrusive computational method, in which the DFN flow solver is applied as a black-box. A very fast error decay, related to the analytical dependence of the observed quantities upon the stochastic variables, is obtained in the low dimensional cases using isotropic sparse grids; comparisons with Monte Carlo results show a clear gain in efficiency for the proposed method. For increasing dimensions attained via successive truncations of Karhunen-Loève expansions, results are still good although the rates of convergence are progressively reduced. Resorting to suitably tuned anisotropic grids is an effective way to contrast such curse of dimensionality: in the explored range of dimensions, the resulting convergence histories are nearly independent of the dimension...|$|E
40|$|Planar maximally supersymmetric Yang-Mills theory (N= 4 SYM) is {{a special}} quantum field theory. A few of its {{remarkable}} features are conformal symmetry at the quantum level, evidence of integrability and, moreover, it {{is a prime example}} of the AdS/CFT duality. Triggered by Witten's twistor string theory, the past 15 years have witnessed enormous progress in reformulating this theory to make as many of these special features manifest, from the choice of convenient variables to recursion relations that allowed new mathematical structures to appear, like the Grassmannian. These methods are collectively referred to as on-shell methods. The ultimate hope is that, by understanding N= 4 SYM in depth, one can learn about other, more realistic quantum field theories. The overarching theme of this thesis is the investigation of how on-shell methods can aid the <b>computation</b> <b>of</b> <b>quantities</b> other than scattering amplitudes. In this spirit we study form factors and correlation functions, said to be partially and completely off-shell quantities, respectively. More explicitly, we compute form factors of half-BPS operators up to two loops, and study the dilatation operator in the SO(6) and SU(2 | 3) sectors using techniques originally designed for amplitudes. A second part of the work is dedicated to the study of scattering amplitudes beyond the planar limit, an area of research which is still in its infancy, and not much is known about which special features of the planar theory survive in the non-planar regime. In this context, we generalise some aspects of the on-shell diagram formulation of Arkani-Hamed et al. to take into account non-planar corrections. Comment: 206 pages, PhD thesis, references accounted for until 10 / 04 / 16 (submission date...|$|E
40|$|Double-diffusion {{refers to}} a class of instabilities that {{develops}} when the density of a fluid depends on two components with different diffusivities. For example, when warm, salty water is layered over cool, fresh water, there is relatively rapid diffusive transfer of heat from the upper layer to the lower layer, while there is negligible diffusive transfer of salt. This loss of heat results in localized parcels of fluid of increased density above the thermohaline interface, which sink out {{in the form of}} long, thin, convective cells called salt fingers. In parts of the ocean, salt fingers are known to be an important mechanism of heat and salt transport. However, they range from a few millimetres to centimetres wide, and are much smaller in scale than many oceanographic processes. The fine scale of these structures can also make their direct measurement in the environment using typical devices difficult. This thesis employs high-resolution direct numerical simulation for detailed examination of fine-scale double-diffusive features. An advantage of numerical simulation is the straightforward <b>computation</b> <b>of</b> <b>quantities</b> that cannot be measured directly through experiment, such as dissipation, stirring, and mixing. Two distinct types of flow are investigated. First, simulations of salt fingering gravity currents are examined, and the effect of different vertical boundary conditions and current volumes are analyzed. Second, a three-layer system resulting in double-diffusive Rayleigh-Taylor (RT) instabilities that transitions to double-diffusive turbulence in the absence of shear is presented. The flows are governed by the incompressible Navier-Stokes equations under the Boussinesq approximation, with salinity and temperature coupled to the equations of motion using a nonlinear approximation to the UNESCO equation of state. Flow dynamics are characterized using high-quality three-dimensional visualization techniques. In the gravity current simulations, it was observed that no-slip boundaries cause the current head to take the standard lobe-and-cleft shape, and encourage both a greater degree and an earlier onset of three-dimensionalization when compared to free-slip boundary cases. Additionally, numerical simulations with no-slip boundary conditions experience greater viscous dissipation, stirring, and mixing when compared to similar configurations using free-slip conditions. The Rayleigh-Taylor instabilities were observed to dominate the length scales of kinetic energy, while the length scales associated with the density field were dominated by double-diffusion. This was confirmed through spectral analysis, which also showed similarity between the dominant salinity and density scales. The standard eddy viscosity formulation was determined to be inappropriate for the salinity and temperature fluxes of this simulation. Due to the effects of double-diffusion, densities greater than the initial maximum value were observed in the RT instability simulations...|$|E
40|$|We {{consider}} linear {{systems with}} unspeci ed parameters that lie between given {{upper and lower}} bounds. Except for a few special cases, the <b>computation</b> <b>of</b> many <b>quantities</b> <b>of</b> interest for such systems can be performed only through an exhaustive search in parameter space. We present a general branch and bound algorithm that implements this search in a systematic manner {{and apply it to}} computing the minimum stability degree...|$|R
5000|$|This {{package was}} {{developed}} at Queen's University in Kingston, Ontario by Peter Musgrave, Denis Pollney and Kayll Lake. While {{there are many}} packages which perform tensor computations (including a standard Maple package), GRTensorII is particularly well suited for carrying out routine <b>computations</b> <b>of</b> useful <b>quantities</b> when working with (or searching for) exact solutions in general relativity. Its principal advantages include ...|$|R
40|$|We {{introduce}} rigorous definitions for two <b>quantities</b> <b>of</b> interest, {{diversity and}} degrees of freedom, {{that are used}} to quantify the advantages of a multiple antenna multiple-input multiple-output (MIMO) system when compared to a single-input single-output (SISO) system. These definitions are in a general setting which will allow the <b>computation</b> <b>of</b> these <b>quantities</b> for systems other than multiple antenna MIMO systems. We verify the effectiveness of the definitions by computing the <b>quantities</b> <b>of</b> interest for various existing examples...|$|R
40|$|Many {{problems}} in finance and risk management involve the <b>computation</b> <b>of</b> <b>quantities</b> related to rare-event analysis. As many financial problems are high-dimensional, the quan- tities of interest rarely have analytical forms {{and therefore they}} must be approximated using numerical methods. Plain Monte Carlo (MC) is a versatile simulation-based numer- ical technique suitable to high-dimensional problems as its estimation error converges to zero at a rate independent of the dimension of the problem. The weakness of plain MC is the high computational cost it requires to obtain estimates with small variance. This issue is especially severe for rare-event simulation as a very large number, often over millions, of samples are required to obtain an estimate with reasonable precision. In this thesis, we develop importance sampling (IS) and stratified sampling (SS) schemes for rare-event simulation problems to reduce the variance of the plain MC estimators. The main idea of our approach is to construct effective proposal distributions for IS and partitions of the sample space for SS by exploiting the low-dimensional structures that exist in many financial problems. More specifically, our general approach is to identify a low-dimensional transformation of input variables such that the transformed variables are highly correlated with the output, and then make the rare-event more frequent by twisting {{the distribution of the}} transformed variables by using IS and/or SS. In some cases, SS is used instead of IS as SS is shown to give estimators with smaller variance. In other cases, IS and SS are used together to achieve greater variance reduction than when they are used separately. Our proposed methods are applicable {{to a wide range of}} problems because they do not assume specific types of problems or distribution of input variables and because their performance does not degrade even in high dimension. Furthermore, our approach serves as a dimension reduction technique, so it enhances the effectiveness of quasi-Monte Carlo sampling methods when combined together. This thesis considers three types of low-dimensional structures in increasing order of generality and develops IS and SS methods under each structural assumption, along with optimal tuning procedures and sampling algorithms under specific distributions. The assumed low-dimensional structures are as follows: the output takes a large value when at least one of the input variables is large; a single-index model where the output depends on the input variables mainly through some one-dimensional projection; and a multi-index model where the output depends on the input mainly through a set of linear combinations. Our numerical experiments find that many financial problems possess one of the assumed low-dimensional structure. When applied to those {{problems in}} simulation studies, our proposed methods often give variance reduction factors of over 1, 000 with little additional computational costs compared to plain MC...|$|E
40|$|Abstract—Molecular {{geometric}} properties, such as volume, exposed surface area, and {{occurrence of}} internal cavities, are important inputs to many applications in molecular modeling. In this work we describe a very general and highly efficient approach for the accurate <b>computation</b> <b>of</b> such properties, which is applicable to arbitrary molecular surface models. The technique {{relies on a}} high performance ray casting framework {{that can be easily}} adapted to the <b>computation</b> <b>of</b> further <b>quantities</b> <b>of</b> interest at interactive speed, even for huge models. Keywords-ray tracing; surface area; volume; I...|$|R
40|$|This thesis {{treats the}} {{extension}} of the classical computational homogenization scheme towards the multi-scale <b>computation</b> <b>of</b> material <b>quantities</b> like the Eshelby stresses and material forces. To this end, microscopic body forces are considered in the scale-transition, which may emerge due to inhomogeneities in the material. Regarding the determination <b>of</b> material <b>quantities</b> based on the underlying microscopic structure different approaches are compared by means of their virtual work consistency. In analogy to the homogenization <b>of</b> spatial <b>quantities,</b> this consistency is discussed within Hill-Mandel type conditions...|$|R
40|$|Multi-loop {{computations}} in QCD {{are notoriously}} difficult {{due to the}} complexity of the integrals involved. Inspired by results from modern number theory and algebraic geometry, a lot of progress has recently been made regarding the <b>computation</b> <b>of</b> multi-loop integrals and scattering amplitudes. I will discuss various new approaches to the <b>computation</b> <b>of</b> loop integrals, and illustrate them on the first <b>computation</b> <b>of</b> a <b>quantity</b> at N 3 LO in perturbative QCD, the soft-virtual part of the inclusive Higgs-boson cross section in gluon fusion at N 3 LO...|$|R
40|$|This paper {{presents}} {{two different}} approaches for the efficient <b>computation</b> <b>of</b> beam coupling factors (BCF) in antenna arrays. First, an analytical expression is derived {{for the case}} of symmetric radiation patterns with sin (thetas) variation. Then, an efficient approach is presented for the numerical <b>computation</b> <b>of</b> such <b>quantities</b> in larger arrays. This approach {{is based on the}} combination of the Macro Basis Function (MBF) technique with the Method-of-Moments (MoM). Both impedance and scattering matrices formulations are considered to provide the BCF matrix. Anglai...|$|R
40|$|Stress-strain and {{durability}} information is often desirable for {{situations in which}} strain and temperature are changing simultaneously. To obtain such information, strain controlled uniaxial push-pull tests have typically been done. In order to control the mechanical strain, it is necessary in such tests to compute the mechanical strain from the total measured strain using measured temperature and the thermal expansion properties of the specimen. A system for conducting torsional thermomechanical tests is described which has the great advantage that the torsional strain is unaffected by the changing temperature and thus real time <b>computations</b> <b>of</b> <b>quantities</b> is not required {{for control of the}} test and the mechanical strain need not be determined from the subtraction of two measured qnantities {{as is the case in}} the uniaxial test. In addition to describing torsional thermomechanical tests, guidelines for software to be used in running biaxial thermomechanical tests will also be presented...|$|R
