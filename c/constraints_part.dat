21|508|Public
2500|$|The {{episode was}} filmed before the third episode, [...] "One Last Dance". The {{first day of}} filming {{involved}} the scenes between Head and Brownen Davies (Andrea), which was filmed in an art house. Due to timing <b>constraints,</b> <b>part</b> of the sequence had to be cut down. A few of the scenes where Tom reveals to Ellie were filmed with a hand-held camera; {{this is the first}} time such a technique was used in Spooks. The anti-globalisation riots in the teaser of the episode were originally going to be staged, but due to budget constraints most of the riot was edited from several pieces of real-life news footage - although part of the riot, particularly involving Salter, was staged. The scene was filmed by two separate units. The building Danny and Zoe used to observe overlooked the filmed riot.|$|E
40|$|Abstract: This is {{the seventh}} {{in a series}} of {{articles}} introducing the Visio-based database modeling component of Microsoft Visual Studio. NET Enterprise Architect. Part 1 showed how to create a basic ORM source model, map it to a logical database model, and generate a DDL script for the physical database schema. Part 2 discussed how to use the verbalizer, make an object type independent, objectify an association, and add some other ORM constraints to an ORM model. Part 3 showed how to add setcomparison constraints (subset, equality and exclusion) and how exclusive-or constraints combine exclusion and disjunctive mandatory <b>constraints.</b> <b>Part</b> 4 discussed the basics of modeling and mapping subtypes. Part 5 discussed mapping subtypes to separate tables, and occurrence frequency <b>constraints.</b> <b>Part</b> 6 discussed ring <b>constraints.</b> <b>Part</b> 7 discusses index constraints, constraint layers, and data types...|$|E
40|$|A {{new problem}} arises when an {{automated}} guided vehicle (AGV) is dispatched to visit a set of customers, which are usually located along a fixed wire transmitting signal to navigate the AGV. An optimal visiting sequence is desired {{with the objective of}} minimizing the total travelling distance (or time). When precedence constraints are restricted on customers, the problem is referred to as traveling salesman problem on path with precedence constraints (TSPP-PC). Whether or not it is NP-complete has no answer in the literature. In this paper, we design dynamic programming for the TSPP-PC, which is the first polynomial-time exact algorithm when the number of precedence constraints is a constant. For the problem with number of precedence <b>constraints,</b> <b>part</b> of the input can be arbitrarily large, so we provide an efficient heuristic based on the exact algorithm...|$|E
30|$|A {{decoupled}} solving {{method of}} number synthesis equation in GF set theory is proposed, which uses combinatorics {{to get the}} solution for both actuation <b>part</b> and <b>constraint</b> <b>part.</b>|$|R
40|$|We {{describe}} {{in this paper}} the GNU-Prolog system, a free system consisting of a Prolog compiler and a constraint solver on finite domains. GNU-Prolog {{is based on a}} low-level mini-assembly platform-independent language that makes it possible for efficient compilation time, and allows to produce small stand alone executable files {{as the result of the}} com-pilation process. Interestingly, the Prolog part is compliant to the ISO standard, and the <b>constraint</b> <b>part</b> includes sev-eral extensions, such as an efficient handling of reified con-straints. The overall system is efficient and comparable in performances with commercial systems, both for the Prolog and <b>constraint</b> <b>parts.</b> 1...|$|R
25|$|After the Cambridge {{project was}} {{abandoned}} due to financial <b>constraints,</b> <b>parts</b> {{of the project}} were {{picked up by the}} engineering firm Alfred McAlpine, and abandoned in the mid-1980s. The Tracked Hovercraft project and Professor Laithwaite's Maglev train system were contemporaneous, and there was intense competition between the two prospective British systems for funding and credibility.|$|R
3000|$|Suppose {{that there}} is no minimum wage in place and let the triplet (c_∗^i,n_∗^i), [...] where i[*]=[*]s, d, l, denote the optimal tax-and-transfer {{schedule}} that maximizes the welfare expression in (2) subject to the revenue constraint (3) and the incentive-compatibility constraints (4). The construction of the proof will be as follows. We will consider a small revenue-neutral perturbation to the optimal tax-and-transfer system. We will show that by imposing a binding minimum wage and further assuming that rationing is constrained efficient (as formally defined below) the suggested perturbation will violate none of the incentive-compatibility <b>constraints</b> (<b>Part</b> I). We will then demonstrate that the suggested perturbation results in a welfare gain (Part II). Finally, we will consider an extension to a more general class of rationing rules and demonstrate that the key result continues to hold when rationing is nearly constrained efficient (Part III).|$|E
40|$|Mixed microstructures {{consisting}} of ®ne plates of upper bainitic ferrite separated by thin ®lms of stable retained austenite have seen many applications in recent years. There {{may also be}} some martensite present, although carbides are avoided by the judicious use of silicon as an alloying element. The essential principles governing the optimisation of such microstructures are well established, particularly that large regions of unstable high carbon retained austenite must be avoided. With careful design, impressive combinations of strength and toughness have been reported for high silicon bainitic steels. The aim of the present work was to ascertain how far these concepts could be extended to achieve unprecedented combinations of strength and toughness in bulk samples subjected to continuous cooling transformation, consistent with certain hardenability and processing requirements. Thus, this paper �part 1 of a two part study) deals with the design, using phase transformation theory, {{of a series of}} bainitic alloys, given a set of industrial <b>constraints.</b> <b>Part</b> 2 of the study concerns the experimental veri®cation of the design process...|$|E
40|$|Mixed microstructures {{consisting}} of fine plates of upper bainitic ferrite separated by thin films of stable retained austenite have seen many applications in recent years. There {{may also be}} some martensite present, although carbides are avoided by the judicious use of silicon as an alloying element. The essential principles governing the optimisation of such microstructures are well established, particularly that large regions of unstable high carbon retained austenite must be avoided. With careful design, impressive combinations of strength and toughness have been reported for high silicon bainitic steels. The aim of the present work was to ascertain how far these concepts could be extended to achieve unprecedented combinations of strength and toughness in bulk samples subjected to continuous cooling transformation, consistent with certain hardenability and processing requirements. Thus, this paper (part 1 of a two part study) deals with the design, using phase transformation theory, {{of a series of}} bainitic alloys, given a set of industrial <b>constraints.</b> <b>Part</b> 2 of the study concerns the experimental verification of the design process. © 2001 IoM Communications Ltd. Peer Reviewe...|$|E
40|$|We {{describe}} {{in this paper}} the GNU-Prolog system, a free system consisting of a Prolog compiler and a constraint solver on finite domains. GNU-Prolog {{is based on a}} low-level mini-assembly platform-independent language that makes it possible for efficient compilation time, and allows to produce small stand alone executable files {{as the result of the}} compilation process. Interestingly, the Prolog part is compliant to the ISO standard, and the <b>constraint</b> <b>part</b> includes several extensions, such as an efficient handling of reified constraints. The overall system is efficient and comparable in performances with commercial systems, both for the Prolog and <b>constraint</b> <b>parts.</b> 1. INTRODUCTION GNU Prolog 1 is a free Prolog compiler supported by the GNU organization. It was released in April 1999 and more than 2500 copies have been downloaded up to now from the INRIA ftp mirror site 2. It is built on previous systems developed at INRIA, namely wamcc [4] for Prolog and clp(FD) [5] for constrain [...] ...|$|R
50|$|If {{the check}} {{constraint}} {{refers to a}} single column only, {{it is possible to}} specify the <b>constraint</b> as <b>part</b> of the column definition.|$|R
40|$|The {{eigenvalue}} {{problem for}} the Sen [...] Witten operator on closed spacelike hypersurfaces is investigated. The (square of its) eigenvalues are shown to be given exactly by the 3 -surface integral appearing in {{the expression of the}} total energy-momentum of the matter+gravity systems in Witten's energy positivity proof. A sharp lower bound for the eigenvalues, given in terms of the <b>constraint</b> <b>parts</b> of the spacetime Einstein tensor, i. e. the energy and momentum densities of the matter fields, is given. Comment: 12 page...|$|R
40|$|The {{design of}} a VLSI chip {{consists}} {{of a number of}} steps, and is usually subject to a number of constraints. During each step estimates of the final design properties are used to steer the design tasks. If the design does not meet the <b>constraints,</b> <b>part</b> of the design must be redone. This is very time-consuming, hence the need for good prediction methods. One of the most important aspects of a design is the interconnection complexity, which is mainly determined by the Rent characteristics and the net degree distribution. A main design task is logic synthesis, which consists of both logic optimization and technology mapping. In this paper the impact of these steps on the Rent characteristics and net degree distribution of the circuit graph is analyzed. A circuit mapped to a simple technology library results in a net degree distribution with relatively more two-terminal nets, and relatively less multi-terminal nets. Also, faster circuits have a more profound Rent region II, a higher average terminal count in region I, and sometimes even a higher Rent exponent in region I...|$|E
40|$|Abstract—We {{propose a}} branch flow {{model for the}} {{analysis}} and optimization of mesh as well as radial networks. The model leads to {{a new approach to}} solving optimal power flow (OPF) that consists of two relaxation steps. The first step eliminates the voltage and current angles and the second step approximates the resulting problem by a conic program that can be solved efficiently. For radial networks, we prove that both relaxation steps are always exact, provided there are no upper bounds on loads. For mesh networks, the conic relaxation is always exact but the angle relaxation may not be exact, and we provide a simple way to determine if a relaxed solution is globally optimal. We propose convexification of mesh networks using phase shifters so that OPF for the convexified network can always be solved efficiently for an optimal solution. We prove that convexification requires phase shifters only outside a spanning tree of the network and their placement depends only on network topology, not on power flows, generation, loads, or operating <b>constraints.</b> <b>Part</b> I introduces our branch flow model, explains the two relaxation steps, and proves the conditions for exact relaxation. Part II describes convexification of mesh networks, and presents simulation results...|$|E
40|$|We {{propose a}} branch flow {{model for the}} anal- ysis and {{optimization}} of mesh as well as radial networks. The model leads to {{a new approach to}} solving optimal power flow (OPF) that consists of two relaxation steps. The first step eliminates the voltage and current angles and the second step approximates the resulting problem by a conic program that can be solved efficiently. For radial networks, we prove that both relaxation steps are always exact, provided there are no upper bounds on loads. For mesh networks, the conic relaxation is always exact but the angle relaxation may not be exact, and we provide a simple way to determine if a relaxed solution is globally optimal. We propose convexification of mesh networks using phase shifters so that OPF for the convexified network can always be solved efficiently for an optimal solution. We prove that convexification requires phase shifters only outside a spanning tree of the network and their placement depends only on network topology, not on power flows, generation, loads, or operating <b>constraints.</b> <b>Part</b> I introduces our branch flow model, explains the two relaxation steps, and proves the conditions for exact relaxation. Part II describes convexification of mesh networks, and presents simulation results. Comment: A preliminary and abridged version has appeared in IEEE CDC, December 201...|$|E
5000|$|... "Definitional, personal, and {{mechanical}} <b>constraints</b> on <b>part</b> of speech annotation performance". Natural Language Engineering 12 (2006): xx-xx. Anna Babarczy and John Carroll (not John M Carroll) ...|$|R
50|$|Nightline was axed on Friday 25 July 2008 due {{to budget}} <b>constraints</b> as <b>part</b> of Nine's news and current affairs division. Wendy Kingston {{presented}} the final edition.|$|R
30|$|In this paper, we {{consider}} the mathematical programs with equilibrium constraints (MPECs) in Banach space. The objective function and functions in the <b>constraint</b> <b>part</b> {{are assumed to be}} lower semicontinuous. We study the Wolfe-type dual problem for the MPEC under the convexity assumption. A Mond-Weir-type dual problem is also formulated and studied for the MPEC under convexity and generalized convexity assumptions. Conditions for weak duality theorems are given to relate the MPEC and two dual programs in Banach space, respectively. Also conditions for strong duality theorems are established in an Asplund space.|$|R
40|$|One year shy of {{the fiftieth}} anniversary of Brown v. Board of Education, the Justices issued another {{equality}} ruling {{that is likely to}} become a historical landmark. In Lawrence v. Texas, the Court invalidated a state law that criminalized same-sex sodomy. This article contrasts these historic rulings along several dimensions, with the aim of shedding light on how Supreme Court Justices decide cases and how Court decisions influence social reform movements. Part I juxtaposes Brown and Lawrence to illustrate how judicial decisionmaking often involves an uneasy reconciliation of traditional legal sources with broader social and political mores and the personal values of the judges. Part II considers what these landmark decisions teach us about the relationship between Supreme Court decisions and movements for social reform. Part III examines the light these rulings shed on the strategic aspect of judicial decisionmaking: how courts sometimes temper their decisions in light of political <b>constraints.</b> <b>Part</b> IV considers the consequences of Brown and Lawrence (and Goodridge v. Department of Public Health) and, especially, the political backlashes they ignited. Part V analyzes the rulings from the perspective of Supreme Court Justices attempting to predict the future. A brief conclusion speculates as to what such decisions-and history 2 ̆ 7 s verdict upon them-teach us about the source of the Supreme Court 2 ̆ 7 s legitimacy...|$|E
40|$|This paper {{examines}} the global {{response to the}} Darfur crisis. The term global refers to non-Sudanese actors in Darfur crisis including but not limited to: international and multilateral governance institutions such as United Nations and its family agencies (hereafter UN), International Criminal Court (hereafter ICC), African Union (hereafter AU), European Union (hereafter EU), North Atlantic Organization (hereafter NATO), and the Arab League; countries such as United States of America (hereafter USA), Peoples Republic of China, Chad, and Central African Republic; and international NGOs such as Amnesty International, Human Rights Watch, International Crisis Group and many other relief and humanitarian organizations, countries. It has five parts. Part I provides brief history of Sudan, present situation of Darfur and {{the magnitude of the}} humanitarian crisis and its causes. Part II discusses the actors actively involved in the Darfur crisis. It also offers analysis of the interest of the main actors; and their role in the global response to Darfur crisis. This is vital input in the formulation of recommendation and a strategy as it considers ways for addressing the legitimate interests, provides ways to tackle those that are illegitimate, and means to remove the binding <b>constraints.</b> <b>Part</b> III looks at the Darfur Peace Processes and their shortcomings. The last part of the paper forwards recommendations on how to solve the Darfur crisis...|$|E
40|$|Cassava was {{introduced}} into Africa from South America {{in the latter}} half of the 16 th century. Since then it has become one of the most important food crops in Africa. Its starchy, tuberous roots are a valuable source of calories and in many parts of Africa, the leaves and tender shoots of cassava are also consumed as vegetables. Over two-thirds of the total production of cassava is consumed in various forms by humans. Its usage as a source of ethanol for fuel, energy in animal feed, and starch for industry is increasing. The crop is amenable to agronomic as well as genetic improvement, has a high yield potential under good conditions and performs better than other crops under sub-optimal conditions. Because of the agronomic and economic importance of cassava this book has been produced for those involved in the production and post-harvest technology of the crop. Designed both as a teaching aid for training sessions and as a convenient reference book, the manual is divided into four parts, and together these parts contain 12 units. Part I deals with production <b>constraints.</b> <b>Part</b> 11, which explains means of overcoming these constraints, consists of 6 units: morphology and physiology; breeding; rapid multiplication, tissue culture agronomy and crop protection. Part 111 deals with post-harvest technology and Part IV covers aspects of data collection and on-farm research. Although only a relatively small book, the informative text is well presented and is supplemented by diagrams, charts, tables and photographs. Cassava in tropical Atrica; a reference manual produced by the International Institute of Tropical Agriculture, 1990 176 pp, ISBN 978 131 041 3 Pbk IITA, PNB 5320 Oyo Road, Ibadan, NIGERIACassava in tropical Atrica; a reference manual produced by the International Institute of Tropical Agriculture, 1990 176 pp, ISBN 978 131 041 3 Pbk IITA, PNB 5320 Oyo Road, Ibadan, NIGERI...|$|E
40|$|AbstractWe {{introduce}} a higher-order constraint-based language for structured and declarative parallel programming. The language, called Goffin, systematically integrates constraints and user-defined functions within a uniform setting of concurrent programming. From {{the perspective of}} parallel programming methodology, the <b>constraint</b> <b>part</b> of Goffin provides a co-ordination language for the functional part, which takes {{on the role of}} the computation language. This conceptual distinction allows the structured formulation of parallel algorithms. Goffin is an extension of the purely functional language Haskell. The functional kernel is embedded in a layer based on concurrent constraints. Logical variables are bound by constraints which impose relations over expressions that may contain user-defined functions. Referential transparency is preserved by restricting the creation of logical variables to the <b>constraint</b> <b>part</b> and by suspending the reduction of functional expressions that depend on the value of an unbound logical variable. Hence, constraints are the means to organize the concurrent reduction of functional expressions. Moreover, constraint abstractions, i. e., functions over constraints, allow the definition of parameterized co-ordination forms. In correspondence with the higher-order nature of the functional part, abstractions in the <b>constraint</b> logic <b>part</b> are based on higher-order logic, leading to concise and modular specifications of behaviour. We {{introduce a}}nd explain Goffin together with its underlying programming methodology, and present a declarative as well as an operational semantics for the language. To formalize the semantics, we identify the essential core constructs of the language and characterize their declarative meaning by associating them with formulae of Church's simple theory of types. We also present a reduction system that captures the concurrent operational semantics of the core constructs. In the course of this paper, the soundness of this reduction system with respect to the declarative semantics is established...|$|R
30|$|Now, to {{investigate}} {{the performance of the}} proposed multi-objective model, the previous numerical example presented in ‘Illustrative numerical examples for the single-objective model’ section is solved by the epsilon-constrained method considering the two conflicting objectives Z 1 and Z 2 simultaneously. In ε-constraint method, one of the objectives is considered as the main objective function {{and the rest of them}} are incorporated in the <b>constraint</b> <b>part</b> of the model, assuming bounds for them. Therefore, the problem is changed to the single-objective problem. Then this single-objective model is solved, while the bounds of the new constraints are parametrically changed and the efficient solutions of the problem are obtained.|$|R
40|$|We connect two {{different}} extensions of Boltzmann's kinetic theory by requiring the same stationary solution. Non-extensive statistics {{can be produced}} by either using corresponding collision rates nonlinear in the one-particle densities or equivalently by using nontrivial energy composition rules in the energy conservation <b>constraint</b> <b>part.</b> Direct transformation formulas between key functions of the two approaches are given. Copyright EDP Sciences/Società Italiana di Fisica/Springer-Verlag 200625. 75. Nq Quark deconfinement, quark-gluon plasma production, and phase transitions, 05. 20. Dd Kinetic theory, 05. 90. +m Other topics in statistical physics, thermodynamics, and nonlinear dynamical systems (restricted to new topics in section 05), 02. 70. Ns Molecular dynamics and particle methods,...|$|R
40|$|In Haskell, programmers have {{a binary}} choice between omitting the type {{signature}} (and relying on type inference) or explicitly providing the type entirely; {{there are no}} intermediate options. Partial type signatures {{bridge the gap between}} the two extremes. In a partial type signature, annotated types can be mixed with inferred types. A type signature is written like before, but can now contain wildcards, written as underscores. Also, placing a wildcard in the <b>constraints</b> <b>part</b> of a type signature will allow the type checker to infer an arbi- trary number of constraints. The type checker will verify that the inferred type matches the form of the partial type signature, {{while at the same time}} it infers the types of the wildcards. E. g., a partial type signature _ => a -> _ could lead to the type Show a => a -> String. Partial type signatures give the programmer complete control over the amount of type infor- mation he wants to annotate. Partial type signatures can be useful in the development phase; the programmer can annotate the parts of the type signature he knows and replace the unknown parts with underscores. They can also be used to hide complex parts of a type signature or to emphasise the relevant annotated parts. We are currently working on integrating partial type signatures in GHC as a variant of Type- dHoles. Whereas TypedHoles allow holes in programs or expressions, PartialTypeSignatures allow holes in types. Depending on the extensions that are enabled, partial type signatures are considered as either temporary placeholders that need to be replaced with full signatures for the code to be successfully compiled or a permanent way to leave out uninteresting parts of types. In this presentation, I will give an overview of partial type signatures, and give some insights in the implementation. status: publishe...|$|E
40|$|This thesis uses {{documentary}} {{and interview}} data to advance a new {{interpretation of the}} development policies of the Tanzanian state. These policies are not {{a function of the}} interests of bureaucratic or other social classes acting in a context of national 'dependence', but rather the product of political-institutional factors which encourage beliefs that development problems are fundamentally 'political' and that external constraints are the principal causes of policy failures. The new explanation combines an analysis of Tanzania's political institutions in terms of a tripartite system of policy-making (Party, Government and Presidency) with an analysis of the policy-makers' conceptions of development. Three themes are investigated: -the relationship between Party and Government, the views of development held in the three organs of the state, and the consequences of a 'political' approach for the development process. Part One examines the emergence of two opposing views of development and the changing arrangements for policy formulation and policy implementation during the period 1962 - 66. It is argued that the division of labour created by the 1965 Constitution introduced great uncertainties into the development process. Part Two analyses the complex and changing relations between the three elements of the tripartite system during the years since 1967. In one sub-period (1969 - 74) the three organs of the state adopted views of development which had a common underlying theme, giving greater emphasis to political goals than to present economic <b>constraints.</b> <b>Part</b> Three investigates three major policy-areas: private capital, internal trade, and agriculture and rural development. The findings illustrate various ways in which Government and Party operated at cross-purposes with the Presidency alternating its support from one organ to the other. The practice of formulating policies without reference to economic considerations is shown to be a frequent cause of policy failure...|$|E
40|$|As traffic keeps increasing, En Route capacity, {{especially}} in Europe, becomes a serious problem. Aircraft conflict resolution, and resolution monitoring, are still done manually by controllers. Solutions to conflicts are empirical and, whereas aircraft are highly automated and optimized systems, tools provided for ATC control are very basic, even out of date. If we compare the current capacity {{and the standard}} separation {{to the size of}} controlled space, the conclusion is easy to draw: while ATC is overloaded, the sky is empty. It must be noticed that enhancing En Route capacity does not require optimal resolution of aircraft conflicts. The need for an automatic problem solver is also a serious concern when addressing the issues of free flight. It is still very much unclear how conflicts will be solved in free flight airspace. Human controllers highly rely on standard routes and traffic organization for solving conflicts; they are much quickly overloaded when controlling aircraft flying on direct routes. Free flight traffic, with a completely unorganized structure, might require automated, computer based, solvers. Moreover, one of the aim of free flight is to give aircraft optimal trajectories, whereas using ACAS techniques is certainly not optimal, and should only looked upon as a last issue system. In this paper, we present an optimal problem solver, based on a stochastic optimization technique (genetic algorithms). It builds optimal resolution for complex conflicts and also computes a large number of nearly optimal resolutions that do not violate separation <b>constraints.</b> <b>Part</b> 1 of the paper introduces the problem solver, its constraints and goals. Modelling is discussed in part 2. Part 3 introduces genetic algorithms techniques and the coding of the problem. Part 4 presents different examples of resolution of very complex test problems. In part 5 we present the complete ATC simulator, conflict detector and cluster builder used to benchmark the problem solver on real traffic; we also discuss weaknesses of the system and possible improvements...|$|E
40|$|The paper {{introduces}} mixed networks, a new {{framework for}} expressing and reasoning with probabilistic and deterministic information. The framework combines belief networks with constraint networks, defining the semantics and graphical representation. We also introduce the AND/OR search space for graphical models, {{and develop a}} new linear space search algorithm. This provides the basis for understanding the benefits of processing the constraint information separately, resulting in the pruning of the search space. When the <b>constraint</b> <b>part</b> is tractable or has {{a small number of}} solutions, using the mixed representation can be exponentially more effective than using pure belief networks which model constraints as conditional probability tables. ...|$|R
40|$|The Z {{notation}} is {{a formal}} specification language used for describing and mod-elling computing systems. Z notation was proposed by Abrial et al in 1974. Tools {{are available for}} simulation and validation of the model specified in Z. The Z no-tation is based upon set theory and mathematical logic. The mathematical logic is a first-order predicate calculus. Mathematical objects and their properties can be collected together in schemas- patterns of declarations and constraints [2]. The schema language {{can be used to}} describe the state of a system, {{and the ways in which}} a state may change. The state of the system can be modelled in Z on a component basis in a bottom up fashion. A schema specification modelling a component of the system state includes two parts- declaration part which specifies the fields or variables of the compo-nent (for example the declaration part of the schema Bag in Figure 1 (a) specifies elements as a set of natural numbers) and a <b>constraint</b> <b>part</b> which specifies the invariants on the component (for example the <b>constraint</b> <b>part</b> of the schema Bag in Figure 1 specifies that each member in the set elements is either 1 or 2). A state change in the system is modelled with a schema called operation schema. An operation schema specifies the state change w. r. t. the schema mod-elling the system state. The state change is specified as a relation between the pre-state and post-state of the schema modelling the system state. We call th...|$|R
5000|$|... (2007): <b>Constraints</b> in naming <b>parts</b> of the Tree of Life. Mol. Phylogenet. Evol. 42: 331-338.|$|R
40|$|The {{modern era}} of {{particle}} physics {{is driven by}} experimental anomalies. Experimental efforts have become increasingly diverse and are producing enormous volumes of data. In such a highly data-driven scientific environment theoretical models are necessary to understand this data and to help inform {{the development of new}} experimental approaches. In this dissertation I present two significant contributions to this effort relevant to the energy, intensity, and cosmic frontiers of modern particle physics research. Part 1 of this dissertation discusses methods to understand modern dark matter direct detection results. In particular I present an analysis under the hypothesis of inelastic dark matter, which supposes that dark matter must scatter inelastically, i. e. that it must gain or loose mass during a collision with atomic nuclei. This hypothesis is attractive because it can alleviate otherwise contradictory results from a number of dark matter detection facilities. The main conclusion of this work is a presentation of the analytical tools, along with a mathematica package {{that can be used to}} run the analysis, and the discovery that there are regions of inelastic dark matter parameter space which are consistent with all current experimental results, and <b>constraints.</b> <b>Part</b> 2 of this dissertation discusses a phenomenon of modern interest called kinetic mixing which allows particles from the standard model to spontaneously transform into particles which experience a new, as of yet undiscovered, force. This phenomenon is relatively common and well motivated theoretically and has motivated significant experimental effort. In this work, I present an analysis of a general case of kinetic mixing, called nonabelian kinetic mixing. This work shows that, In general, kinetic mixing predicts the existence of a new particle and that, under certain conditions, this particle could be detected at modern particle colliders. Furthermore, the mass of this particle is related to the strength of kinetic mixing. This relationship suggests novel ways to constrain kinetic mixing parameter space, and if observed would provide a very striking indication that such a model is realized in nature...|$|E
40|$|This thesis {{considers}} {{how best}} to administer redistribution policies. It focuses particularly on the information needed to assess relative circumstances, {{the implications of the}} government collecting such information, and processes by which the appropriate information may be assembled and assessed. In New Zealand, as with many other OECD nations, the Government's redistribution policies are administered through a range of different agencies, with duplication in some areas and gaps in others. An integrated approach to redistribution systems may offer a means to improve equity and efficiency. Part One discusses the assessment of relative well-being, and adopts the choice set as the intellectual device for this purpose. The time period for the assessment of income is examined in detail, with the conclusion that a long period should be used except where the individual is constrained to operate under a short time horizon. A new concept of "bankability" is developed as a means of identifying those operating under such <b>constraints.</b> <b>Part</b> Two uses the philosophical foundations of the value of privacy to develop a new statement of the right to privacy, such that everyone should be protected against the requirement to divulge information, unless that information is the "business" of another party. A view on the business of the state depends on one's ideology of the state. Since it is generally accepted in New Zealand {{in the late twentieth century}} that the state has a role in redistribution, the state has some right to collect information for that purpose. However, the rights of the state are moderated by the existence of a common law tradition of respect for individuals. A set of criteria for evaluating redistribution systems is devised in Part Three. These criteria, which include consideration of the information to be collected, individual control over personal information, and administrative simplicity, are then used to identify significant weaknesses in the systems currently used in New Zealand. The main problems identified are the collection of inadequate information, duplication, and complex institutional structures; the main virtue of the current systems is that information provided is only used for the purpose for which it was provided. An alternative approach is outlined which would address the problems while retaining the current protection of privacy interests. This thesis is a mix of inter-disciplinary academic enquiry and policy development. Part One is an amalgam of economic and philosophical approaches, Part Two involves philosophy and politics, and Part Three applies the theoretical considerations to issues of public administration...|$|E
40|$|Uncertainty is a {{fascinating}} subject, spanning {{a wide range of}} scientific and practicalknowledge domains. Economics, physics, decision theory and risk assessment are its traditional locations, while more recently it has also become common in modelling, numerical analysis, advanced statistics and computer science. However, the study of uncertainty has come into contact even with such subjects as epistemology, management science, psychology, public debate and theories of democracy. The focus of this book is narrower. As the product of a working group of practitioners representing a large panel of industrial branches, it is dedicated to the better understanding of the industrial constraints and best practices associated with the study of uncertainty. It is concerned with the quantification of uncertainties in the presence of data, model(s) and knowledge about the problem, and aims to make a technical contribution to decision-making processes. It should illustrate the optimal trade-offs between literature-referenced methodologies and the simplified approaches often inevitable in practice, owing to data, time or budget limitations, or simply to the current formulation of regulations and the cultural habits of technical decision-makers. The book is organized as follows. Part I introduces the common methodological framework which forms the backbone of the work: this is the essential generic structure unifying the industrial case studies (which are sketched briefly in Part I Chapter 2), notwithstanding the peculiarities of each due to sector, physical or decision-criteria specificities. Part II comprises a number of industrial case studies recently carried out in nuclear, oil, aerospace and mechanical industries or civil structures around Europe. The chapters follow a standard format in order to facilitate comparison, in line with the common methodological framework: they illustrate a number of practical difficulties encountered or solutions found, according to the level of complexity and regulatory or financial <b>constraints.</b> <b>Part</b> III is the scientific core of the book: it includes a review of methods which may be used in the main steps and towards the main goals of any uncertainty study. Citing a large and diverse academic literature, critical recommendations are developed from the applied point of view. The annexes gather supporting material for the practitioner. First, salient references and a few generic standards are provided in correspondence with the key steps and methods of the common methodological framework (Annex 1). Certain important items of software are then reviewed with a particular focus on their operational characteristics and features (Annex 2) : it is obviously a fast-moving reality for which completeness or permanence cannot be fully guaranteed. Once again, the purpose is to link the review to the salient features and key questions identified in the common methodological framework, rather than to reproduce the detailed specifications of each type of software. The chapters of the book may be read independently to a certain extent: readers familiar with the subject may go directly to any case study (in Part II) to find illustrations of industrial practices in a given domain; alternatively they may refer to the more specialized chapters of Part III for the most appropriate methods for a given purpose. It is suggested, however, that the reader begin at Part I for an introduction to the distinctive vocabulary and approach of the book. JRC. G. 9 -Econometrics and statistical support to antifrau...|$|E
30|$|We {{have studied}} {{mathematical}} programs with equilibrium constraints (MPECs). The objective function and {{functions in the}} <b>constraint</b> <b>part</b> {{are assumed to be}} lower semicontinuous. We studied the Wolfe-type dual problem for the MPEC under the convexity assumption. A Mond-Weir-type dual problem was also formulated and studied for the MPEC under convexity and generalized convexity assumptions. Conditions for weak duality theorems were given to relate the MPEC and two dual programs in Banach space, respectively. Also conditions for strong duality theorems were established in an Asplund space. We also discussed the cases when all the constraint functions are affine. Two numerical examples were given to illustrate the Wolfe-type duality and the Mond-Weir-type duality with our MPECs, respectively.|$|R
40|$|We {{propose a}} new way to mix {{constrained}} types and type inference, where the interaction between the two is minimal. By using local constraints embedded in types, {{rather than the other way}} round, we obtain a system which keeps the usual structure of an Hindley-Milner type system. In practice, this means that it is easy to introduce local constraints in existing type inference algorithms. Eventhough our system is notably weaker than general constraintbased type systems, making it unable to handle subtyping for instance, it is powerful enough to accomodate many features, from simple polymorphic records à la Ohori to Objective Caml’s polymorphic variants, and accurate typing of pattern matching (i. e. polymorphic message dispatch), all these through tiny variations in the <b>constraint</b> <b>part</b> of the system. 1...|$|R
40|$|International audienceThis paper {{provides}} a new constitutive model for rubber-like materials. The model {{adds to the}} 8 -chain density introduced by Arruda and Boyce, two phenomenological components: an original part made of an integral density and an interleaving <b>constraint</b> <b>part</b> represented by a logarithmic function as proposed by Gent and Thomas. The model contains six rheological parameters connected to the polymer chemistry and to the macroscopic behavior. Four sets of experimental data from the literature are used to identify the rheological parameters and to assess the proposed model. The model is able to reproduce with a good accuracy experimental data performed under different loading conditions such as uniaxial and equibiaxial tension, uniaxial compression, pure and simple shear {{as well as the}} Mooney plot...|$|R
