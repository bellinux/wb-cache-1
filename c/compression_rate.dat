1097|973|Public
25|$|Compression to storage depth is {{generally}} at a limited rate {{to minimize the}} risk of HPNS and compression arthralgia. Norwegian standards specifies a maximum <b>compression</b> <b>rate</b> of 1mswperminute, and a rest period at storage depth after compression and before diving.|$|E
25|$|The main {{advantage}} of press forging, {{as compared to}} drop-hammer forging, {{is its ability to}} deform the complete workpiece. Drop-hammer forging usually only deforms the surfaces of the work piece in contact with the hammer and anvil; the interior of the workpiece will stay relatively undeformed. Another advantage to the process includes the knowledge of the new part's strain rate. By controlling the <b>compression</b> <b>rate</b> of the press forging operation, the internal strain can be controlled.|$|E
25|$|Early {{studies of}} the {{phenomenon}} suggested one {{solution to the problem}} was to increase the <b>compression</b> <b>rate.</b> In this approach, the compression would be started and stopped so rapidly that the bulk of the plasma would not have time to move; instead, a shock wave created by this rapid compression would be responsible for compressing the majority of the plasma. This approach became known as fast pinch. The Los Alamos team working on the Columbus linear machine designed an updated version to test this theory.|$|E
50|$|In {{combination}} with good source material this format allows <b>compression</b> <b>rates</b> up to 50%. Because of {{that it is}} not comparable to other audio formats like MP3, which reach <b>compression</b> <b>rates</b> up to 90% because of their lossy compression algorithms. Therefore OSQ is used rather for archiving then for daily use.|$|R
40|$|In 1994 Burrows and Wheeler [3] {{described}} a universal data compression algorithm (BWalgorithm, for short) which achieved <b>compression</b> <b>rates</b> that {{were close to}} the best known <b>compression</b> <b>rates.</b> Due to it’s simplicity, the algorithm can be implemented with relatively low complexity. Fenwick [5] described ideas to improve the efficiency (i. e. the compressio...|$|R
50|$|DataStream Intelligence, {{a feature}} of Channel Gateway, {{separates}} virtual tape data and metadata inline for higher deduplication and <b>compression</b> <b>rates</b> when written to storage with deduplication capabilities, such as IBM ProtecTIER, HP StorOnce, Quantum DXi and EMC Data Domain systems. This configuration enables data <b>compression</b> <b>rates</b> upwards of 20:1 versus traditional tape hardware compression of 3:1.|$|R
2500|$|The {{engine was}} first seen in {{air-cooled}} form, equipped with three Mikuni VM carburettors, in the 1967 LC10 Suzuki Fronte 360. Displacement was , from a {{bore and stroke}} of 52.0 and 56.0mm. Originally developing , a [...] SS version soon appeared, with a stunning 101.1PS/L. For the conventionally laid out Fronte Van, Estate, and Custom a single carburettor version was used. Combined with a lower <b>compression</b> <b>rate</b> of 6.8:1, this meant a max power of [...] For 1971, the LC10 engine received Suzuki's new self-lubricating [...] "CCIS" [...] system (Cylinder Crank Injection and Selmix).|$|E
2500|$|An even sportier version {{appeared}} in November 1983, when the CT and CT-G Turbo versions were offered. The F5A Turbo was Suzuki's first forced induction car engine and produced [...] and a useful [...] It received an electronic carburetor {{and a lower}} <b>compression</b> <b>rate</b> of 8,6:1. Only a five-speed manual transmission was available, and front disc brakes were standard. The CT weighed in at , prices were ¥748,000 and ¥898,000 respectively. A two-tone red-and-black interior and dummy hood scoop added to the Turbo's sporty looks, while the CT-G also received a rev counter. Top speed has been quoted at [...]|$|E
2500|$|In May 1983, {{less than}} a year after introduction, the Cervo {{received}} a light facelift. In light of new regulations, wing mirrors were moved to the doors and the screw holes on the fenders were covered up. The mechanical changes were light: a slightly adjusted cam profile, the <b>compression</b> <b>rate</b> was upped to 9,7:1 (previously 9,5), the EGR and catalytic converter were improved and the engine received an automatic choke and transistor ignition. Power remained [...] There were also radial tired versions introduced, and disc brakes for the top-of-the-line CS-G. The lineup also received an overhaul, now looking as follows: ...|$|E
40|$|Data compression, in {{the process}} of Satellite Earth data transmission, is of great concern to improve the {{efficiency}} of data transmission. Information amounts inherent to remote sensing images provide a foundation for data compression in terms of information theory. In particular, distinct degrees of uncertainty inherent to distinct land covers result in the different information amounts. This paper first proposes a lossless differential encoding method to improve <b>compression</b> <b>rates.</b> Then a district forecast differential encoding method is proposed to further improve the <b>compression</b> <b>rates.</b> Considering the stereo measurements in modern photogrammetry are basically accomplished by means of automatic stereo image matching, an edge protection operator is finally utilized to appropriately filter out high frequency noises which could help magnify the signals and further improve the <b>compression</b> <b>rates.</b> The three steps were applied to a Landsat TM multispectral image and a set of SPOT- 5 panchromatic images of four typical land cover types (i. e., urban areas, farm lands, mountain areas and water bodies). Results revealed that the average code lengths obtained by the differential encoding method, compared with Huffman encoding, were more close to the information amounts inherent to remote sensing images. And the <b>compression</b> <b>rates</b> were improved to some extent. Furthermore, the <b>compression</b> <b>rates</b> of the four land cover images obtained by the district forecast differential encoding method were nearly doubled. As for the images with the edge features preserved, the <b>compression</b> <b>rates</b> are average four times as large as those of the original images...|$|R
30|$|We {{conducted}} transmission {{experiments in}} advance to see video quality differences versus <b>compression</b> <b>rates.</b>|$|R
50|$|Using this strategy, certain filetypes {{that so far}} {{achieved}} poor <b>compression</b> <b>rates</b> can be processed.|$|R
2500|$|The M3 {{operating}} sequence is as follows: the bolt is cocked {{to the rear}} using the cocking handle located {{on the right side}} of the ejector housing. When the trigger is pulled, the bolt is driven forward by the recoil springs, stripping a round from the feed lips of the magazine and guiding the round into the chamber. The bolt then continues forward and the firing pin strikes the cartridge primer, igniting the round, resulting in a high-pressure impulse, forcing the bolt back against the resistance of the recoil springs and the inertial mass of the bolt. By the time the bolt and empty casing have moved far enough to the rear to open the chamber, the bullet has left the barrel and pressure in the barrel has dropped to a safe level. The M3's comparatively low cyclic rate was a function of the relatively low pressure generated by the [...]45 ACP round, a heavy bolt, and recoil springs with a lighter-than-normal <b>compression</b> <b>rate.</b>|$|E
50|$|<b>Compression</b> <b>Rate</b> and Visual Quality: In a {{rendering}} system, {{lossy compression}} {{can be more}} tolerable than for other use cases. Some texture compression libraries, such as crunch, allow the developer to flexibly trade off <b>compression</b> <b>rate</b> vs. visual quality, using methods such as Rate-distortion optimization (RDO).|$|E
5000|$|Therefore, the <b>compression</b> <b>rate</b> of {{the spatial}} domain {{is as follows}} ...|$|E
5000|$|... 6 {{red book}} audio discs {{for the price}} of one Yellow Book (CD-ROM), {{depending}} on file <b>compression</b> <b>rates</b> ...|$|R
40|$|We present linearized {{stability}} {{analyses of}} the effect of slow anisotropic compression or expansion on the growth of perturbations at accelerated fluid interfaces in both planar and spherical geometries. The interface separates two fluids with different densities, compressibilities, and <b>compression</b> <b>rates.</b> We show that a perturbation of large mode number on a spherical interface grows at precisely the same rate as a similar perturbation on a planar interface subjected to the same normal and transverse <b>compression</b> <b>rates...</b>|$|R
5000|$|Writing {{many small}} {{blocks of data}} can even lead to {{negative}} <b>compression</b> <b>rates,</b> so {{it is essential for}} applications to use large write buffers.|$|R
5000|$|<b>Compression</b> <b>rate</b> of 4.71 bits/pixel (2K @ 24 frame/s), 2.35 bits/pixel (2K @ 48 frame/s), 1.17 bits/pixel (4K @ 24 frame/s) ...|$|E
5000|$|Conference paper - I {{have shown}} that the <b>compression</b> <b>rate,</b> {{asymptotically}} achieved by the “Sadeh Algorithm”, converges in probability to Shannon’s bound.|$|E
50|$|He {{have shown}} that the <b>compression</b> <b>rate,</b> {{asymptotically}} achieved by the “Sadeh Algorithm”, converges in probability to Shannon’s bound and showed suboptimal applications.|$|E
3000|$|Data {{visualization}} where loss in {{the quality}} and resolution in the reconstructed data is usually acceptable which indicates that large <b>compression</b> <b>rates</b> can be performed on the original data.|$|R
40|$|It is {{well known}} that {{real-time}} movie transmission over serial lines makes a tough problem, which can be solved only by huge image <b>compression</b> <b>rates.</b> The classic <b>compression</b> algorithms have serious limitations on that peculiar case because they are not "custom made" for images. We have developed here a way to get 16 : 1 up to 21 : 1 <b>compression</b> <b>rates,</b> using neural networks trained with advanced supervised backpropagation algorithms. These techniques, along with successive frames optimization, may open a way to real-time movie transmission across the Internet. 1 Background The classic non-destructive data compression algorithms applied on images may give <b>compression</b> <b>rates</b> of about 3 : 1. These algorithms are very good because they do not alter data, i. e. the uncompressed file resembles bit by bit the original one. On the other hand, the JPEG and neural networks compression algorithms cannot be used on anything else but images due to the inherent information loss during the compression/decomp [...] ...|$|R
3000|$|Low bitrate codec: the {{robustness}} against the low-rate codec was tested by using MPEG 1 Layer III compression (MP 3) with <b>compression</b> <b>rates</b> of 56, 64, 96, and 128 [*]kbps.|$|R
50|$|He {{presented}} {{performance analysis}} based on LDT (Large Deviations Theory) and presented the trade-off between <b>compression</b> <b>rate,</b> distortion level and probability of error.|$|E
5000|$|He {{presented}} {{performance analysis}} based on large deviations theory (LDT) and presented the trade-off between <b>compression</b> <b>rate,</b> distortion level, and probability of error.|$|E
50|$|US patent 5836003He {{had shown}} that the <b>compression</b> <b>rate,</b> {{asymptotically}} achieved by the “Sadeh Algorithm”, converges in probability to Shannon’s bound and showed suboptimal applications.|$|E
3000|$|... that {{corresponds}} to the point on the spiral closest to x. It is possible to achieve higher <b>compression</b> <b>rates</b> (i.e., N> 2) by extending (1) to generate more complex curves[15, 16].|$|R
40|$|This paper {{reports the}} {{improvements}} we made to our previously proposed hidden Markov model (HMM) based summarization method for multi-domain contact center dialogues. Since the method relied on Viterbi decoding for selecting utterances {{to include in}} a summary, it had the inability to control <b>compression</b> <b>rates.</b> We enhance our method by using the forward-backward algorithm together with integer linear programming (ILP) to enable the control of <b>compression</b> <b>rates,</b> realizing summaries that contain as many domain-related utterances and as many important words as possible within a predefined character length. Using call transcripts as input, we verify the effectiveness of our enhancement...|$|R
40|$|Traditional {{approaches}} to image sequence coding {{rely on the}} Discrete Cosine Transform (DCT) to achieve high <b>compression</b> <b>rates.</b> In this paper, an encoding scheme for image sequences is presented that trades off some image quality for high <b>compression</b> <b>rates</b> and low computational load. The coder is essentially integer-based and works as follows: i) removes temporal redundancy by a simple, block-based movement detection strategy; ii) provides a first image block approximation using tree-searched Vector Quantization (VQ); iii) then progres-sively transmits complementary information spread along a few frame periods if no further movement is detected. The coder also accepts the DCT in place of VQ...|$|R
50|$|Generic Diff Format (GDIFF) {{is another}} delta {{encoding}} format. It was submitted to W3C in 1997. In many cases, VCDIFF has better <b>compression</b> <b>rate</b> than GDIFF.|$|E
5000|$|The best {{image quality}} {{at a given}} bit-rate (or <b>compression</b> <b>rate)</b> is the main goal of image compression, however, there are other {{important}} properties of image compression schemes: ...|$|E
50|$|Conference paper - {{he showed}} {{by using the}} {{extended}} Kac’s Lemma, that the <b>compression</b> <b>rate,</b> asymptotically achieved by the “Sadeh Algorithm”, converges in probability to Shannon’s bound. The algorithm has been patented in the USA and Israel.|$|E
40|$|A {{compression}} method of phase-shifting digital holographic data is presented. Three interference patterns are recorded, and holographic information is extracted from them by phase-shifting interferometry. The scheme uses standard baseline Joint Photographic Experts Group (JPEG) or standard JPEG- 2000 image compression techniques on the recorded interference patterns {{to reduce the}} amount of data to be stored. High <b>compression</b> <b>rates</b> are achieved for good reconstructed object image quality. The utility of the proposed method is experimentally verified with real holographic data. Results for <b>compression</b> <b>rates</b> using JPEG- 2000 and JPEG of approximately 27 and 20, respectively, for a normalized root-mean-square error of ∼ 0. 7 are demonstrated...|$|R
40|$|Dielectric {{relaxation}} {{phenomena of}} 4 -cyano- 4 ¢-n-alkyl-biphenyl (nCB; n = 5 or 10) monolayers at the air–water interface {{have been investigated}} by measuring Maxwell displacement current (MDC; I) -area per molecule (A) isotherms and surface pressure (p) –A isotherms at several monolayer <b>compression</b> <b>rates.</b> It is found that I–A isotherms of 5 CB monolayer clearly depend on molecular <b>compression</b> <b>rates</b> (a) in the range (range 2) where the surface pressure is almost zero, and that in contrast, I–A isotherms of 10 CB monolayer fluctuate in range 2. These {{results indicate that the}} monolayer compression of 5 CB in range 2 is a non-equilibrium process. Ó 1998 Elsevier Scienc...|$|R
40|$|This paper investigates {{lossless}} coding of wideband speech {{by adding a}} lossless enhancement layer to the lossy baselayer produced by a standardised wideband speech coder. Both the ITU-T G. 722 and G. 722. 2 speech coders are examined. Entropy results show that potential <b>compression</b> <b>rates</b> are dependent on the type and bit rate of the baselayer coder {{as well as the}} symbol size used by the lossless coder. Higher <b>compression</b> <b>rates</b> were obtained by adding a decorrelation stage prior to lossless encoding. The resulting lossless speech coder operates at a bit rate that is approximately 58 % of the bit rate of original digitised wideband speech signal. 1...|$|R
