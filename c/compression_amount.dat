4|99|Public
40|$|The {{object of}} this paper is to study a {{cylindrical}} helical spring to be applied at high temperatures. The aim of this work is to study the regularity of relaxation stresses in spring and evaluate its long-term stresses. The work allowed us to establish relaxation dependencies of springs under high temperatures. According to the results of creep tests at 600 °, the theoretical equation of steel creep was defined concretely. It was then used for the analysis at 350 °. The paper presents a created finite element model of spring relaxation. It is the stainless steel 08 Х 18 Н 10 spring to be used at the temperature of 350 °. In this paper describes the basic theory of creep, considers the relationship between the creep speed and parameters. The changing compression force of springs is analyzed under fixed <b>compression</b> <b>amount.</b> The paper also analyzes the changing length of springs in the free state after various stages of high-temperature relaxation test. It determines the results of compression forces and free length under different amount of compression. The analysis to compare the theoretical calculation of the compression forces with the experimental results is conducted. Computer modeling is created in Abaqus for calculation. Spring relaxation experiments are carried out under fixed <b>compression</b> <b>amount</b> and at the temperature of 350 °. It is shown that the simulation results, which are carried out in Abaqus coincide with experimental results. The study shows that it is possible to use the creep equation parameters, based on the experimental results at high temperatures, to predict creep and relaxation properties of springs, which work at less high temperatures. The work results can be used as a basis in designing the springs working at high temperatures. </p...|$|E
40|$|AbstractCircle-to-square {{roll forming}} process of {{martensitic}} steel MS 980 was studied. A 3 D elastic-plastic roll forming {{finite element model}} was established with ABAQUS {{and the influence of}} material and process parameters was investigated. Four material with different strengths include MS 980, Fe 360 D, DP 590, MS 1500 were compared to study the effect of material strength. The result shows that as the material strength increase, the fillet radius of formed square tube decrease, corner thickness increase significantly, and the edge convexity is worse. Rolled <b>compression</b> <b>amount</b> distribution is an important parameter in roll forming process. Three distribution schemes were proposed, which were parabola distribution, decline distribution and equal distribution. The result shows that the equal distribution scheme can get good square shape and uniform force on each pass...|$|E
40|$|The paper {{deals with}} the single-machine {{scheduling}} problem in which job processing times as well as release dates are controllable parameters and they may vary within given intervals. While all release dates have the same boundary values, the processing time intervals are arbitrary. It is assumed {{that the cost of}} compressing processing times and release dates from their initial values is a linear function of the <b>compression</b> <b>amount.</b> The objective is to minimize the makespan together with the total compression cost. We construct a reduction to the assignment problem for the case of equal release date compression costs and develop an O(n 2) algorithm for the case of equal release date compression costs and equal processing time compression costs. For the bicriteria version of the latter problem with agreeable processing times, we suggest an O(n 2) algorithm that constructs the breakpoints of the efficient frontier. Department of Logistics and Maritime Studie...|$|E
50|$|Compression of solids {{has many}} {{implications}} in materials science, physics and structural engineering, for <b>compression</b> yields noticeable <b>amounts</b> {{of stress and}} tension.|$|R
40|$|The single machine {{scheduling}} problem with {{two types of}} controllable parameters, job processing times and release dates, is studied. It is assumed {{that the cost of}} compressing processing times and release dates from their initial values is a linear function of the <b>compression</b> <b>amounts.</b> The objective is to minimize the sum of the total completion time of the jobs and the total compression cost. For the problem with equal release date compression costs we construct a reduction to the assignment problem. We demonstrate that if in addition the jobs have equal processing time compression costs, then it can be solved in O(n 2) time. The solution algorithm can be considered as a generalization of the algorithm that minimizes the makespan and total compression cost. The generalized version of the algorithm is also applicable to the problem with parallel machines and to a range of due-date {{scheduling problem}}s with controllable processing times. Department of Logistics and Maritime Studie...|$|R
40|$|The inverse {{problem of}} fractal <b>compression</b> <b>amounts</b> to {{determining}} a contractive operator {{such that the}} corresponding fixed point approximates a given target function. The standard method based on the collage coding strategy is known to represent a suboptimal method. Why does one not search for optimal fractal codes? We will prove that optimal fractal coding, when considered as a discrete optimization problem, constitutes an NP-hard problem, i. e., it cannot be solved in a practical amount of time. Nevertheless, when the fractal code parameters are allowed to vary continuously, we show that one is able to improve on collage coding by fine-tuning some of the fractal code parameters {{with the help of}} dierentiable methods. The differentiability of the attractor as a function of its luminance parameters is established. We also comment on the approximating behavior of collage coding, state a lower bound for the optimal attractor error, and outline an annealing scheme for improved fractal codin [...] ...|$|R
40|$|Abstract. Belt and airbag are {{the most}} {{important}} protection devices in vehicle frontal crash, which can have the best protective performance when only they were well matched with vehicle body structure. In this paper the theoretical guidance for optimizing belt and airbag parameters is researched, in order to improve the effect and efficiency of parameters optimization of restraint system. Firstly, a simulation model for occupant restraint system is developed based on the finite element theory combined with multi rigid body theory, and its effectiveness for simulating the occupant dynamic response in frontal impact is validated. Then, the energy dissipation characteristics of occupant head and chest in typical frontal crash are analyzed based on the developed model. Lastly, the adaptive level of restraint system parameters are evaluated according to the dissipation characteristics of occupant energy, and theoretical guidance for parameters optimization are summarized based on the evaluation. The analysis results indicate that: (1) airbag with low stiffness cannot fully utilize the deformation of vehicle body to dissipate the energy of occupant head, but may increase the risk of head injury; (2) belt with high stiffness would apply a big force to occupant, which could increase the <b>compression</b> <b>amount</b> of chest and may increase the risk of chest injury...|$|E
30|$|The JPEG- 2000 {{algorithm}} {{is published by}} the Joint Photographic Experts Group (JPEG) {{as one of its}} still-image compression standards [7]. JPEG- 2000 uses state-of-the art compression techniques based on wavelets, unlike the more popular JPEG standard, which is based on the discrete cosine transform (DCT). JPEG- 2000 contains options that allow both lossless and lossy compression of imagery, as does JPEG. When using any lossy compression technique, some information is lost in the <b>compression</b> and the <b>amount</b> and type of information that is lost depends on several factors, including the algorithm used for <b>compression,</b> the <b>amount</b> of <b>compression</b> desired (which determines the size of the compressed file), and special options offered in the algorithm such as Region-of-Interest (ROI) processing. In ROI processing, select regions of the image are deemed more important than other areas such that less information is lost in those regions.|$|R
5000|$|... {{compression}} - {{the reduction}} of the dynamic range of a sound to avoid unintentional fluctuation in the dynamics. Level compression is {{not to be confused with}} audio data <b>compression,</b> where the <b>amount</b> of data is reduced without affecting the amplitude of the sound it represents.|$|R
50|$|Compression {{stockings}} or sleeves are {{a viable}} option to manage swelling of extremities with graduated <b>compression</b> (where the <b>amount</b> of <b>compression</b> decreases as {{the distance to the}} heart decreases). These garments are especially effective post-operatively and are used in virtually all hospitals to manage acute or chronic swelling, such as congestive heart failure.|$|R
50|$|A similar effect {{happens with}} image formats. Some formats such as JPEG achieve <b>compression</b> through small <b>amount</b> of {{information}} loss. If a lossless file, {{such as a}} BMP or PNG file, is converted to JPEG and back again then {{the result will be}} different from the original (although it may be visually very similar).|$|R
5|$|After the 1973 oil crisis, the Brazilian {{government}} made mandatory {{the use of}} ethanol blends with gasoline, and neat ethanol-powered cars (E100 only) were launched to the market in 1979, after testing with several prototypes developed by four carmakers. Brazilian carmakers modified gasoline engines to support ethanol characteristics and changes included <b>compression</b> ratio, <b>amount</b> of fuel injected, replacement of materials that would get corroded by the contact with ethanol, use of colder spark plugs suitable for dissipating heat due to higher flame temperatures, and an auxiliary cold-start system that injects gasoline from a small tank in the engine compartment to help starting when cold.|$|R
40|$|ABSTRACT: Slightly tapered {{truncated}} cones were manufactured from graphite/epoxy preimpregnated unidirectional {{tape and}} were loaded in <b>compression.</b> Different <b>amounts</b> of side loads were introduced by orienting the loading axis {{away from the}} central axis of the cone. The cones were crushed under quasi-static conditions, and their energy absorption was measured. For small amounts of side load, the energy absorbency was improved; however, {{as the amount of}} side load is increased further, the energy absorption capability of the structure is reduced significantly. Furthermore, a tendency for the specimen to topple is observed {{as a result of the}} moment induced by the side loads which reduces the energy absorption properties even further...|$|R
40|$|Image data {{compression}} {{provides a means}} for efficient storage and transmission of digitized industrial radiographs and can result in significant cost savings when the amount of data involved is large. Image compression methods are generally classified into two categories: lossless and lossy. In lossless compression, the image data are compressed {{in such a way}} that exact reconstruction of the original images is possible. But, the compression factors obtained are generally low and image dependent i. e., they cannot be fixed. In lossy <b>compression,</b> some <b>amount</b> of distortion is introduced in the reconstructed images. The compression factors are however larger and can be fixed irrespective of the activity levels of the images...|$|R
40|$|Fractal image <b>compression</b> reduces <b>amount</b> of {{redundancy}} {{to great}} extent using suitable affine transformations. This requires less {{number of bits}} to encode the same image. However the process of encoding requires enormous computational processing to generate required fractal codes. A distributed parallel method is proposed to reduce computational time by portioning and distributing the input image among different computing nodes, as each computing node performs encoding and block matching individually; which results in significant reduction in processing time to generate fractal codes. This paper presents a review of different parallel algorithms and architecture that {{has been applied to}} enhance the speedup also can be used for fractal image encoding task...|$|R
3000|$|To manage AC {{sensitivity}} to errors, authors of [6] proposed {{to use an}} extra symbol μ with probability ε > 0 to detect transmission errors. This symbol is introduced in the source alphabet but never transmitted. The forbidden symbol technique implies a reduction of the coding space {{by a factor of}} (1 -ε), thus, reduces <b>compression</b> efficiency. The <b>amount</b> of added rate redundancy is R [...]...|$|R
40|$|MPEG- 4 {{simple profile}} {{is being used}} as the video {{compression}} standard in mobile video communications. Video <b>compression</b> requires significant <b>amount</b> of processing power. Currently, ARM cores are widely used in mobile applications because of their low power consumption. As the processing power available is limited, optimization of the applications is inevitable. This paper describes in detail about the implementation of MPEG- 4 simple profile video decoder on ARM 7 TDMI...|$|R
30|$|Svärd et al. (2011) {{also used}} {{compression}} approach with dynamic page transfer reordering. This approach orders the page transfer {{in such a}} way that retransfer of frequently updated pages is reduced. Based on the number of times a page is being updated during migration, page weight for each page is calculated and pages are reordered accordingly. This results into reduced number of pages transferred during migration. For further improvement, authors combine this idea with delta compression technique which leads to reduction in both migration time as well as downtime. Delta <b>compression</b> reduces <b>amount</b> of data transferred by sending XOR deltas between previous and current page versions instead of full page. It is highly dependent on fast, efficient page privatization schemes and compression. Here, again overhead is high.|$|R
50|$|Compression {{is useful}} because it reduces {{resources}} required to store and transmit data. Computational resources are {{consumed in the}} compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space-time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough {{to be viewed as}} it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of <b>compression,</b> the <b>amount</b> of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.|$|R
5000|$|Compressors - A device which {{automatically}} varies {{the volume}} range of tracks being mixed, {{so that one}} track is not obscured by another when a low volume level on the primary track coincides with a high volume level on a secondary track. Compressors are equipped {{with a number of}} controls to vary the volume range over which the action of <b>compression</b> occurs, the <b>amount</b> of <b>compression,</b> and how quickly or slowly the compressor acts.|$|R
5000|$|Therefore, the scalar part [...] of [...] is {{a stress}} {{that may be}} {{observed}} when the material is being compressed or expanded {{at the same rate}} in all directions. It is manifested as an extra pressure that appears only while the material is being compressed, but (unlike the true hydrostatic pressure) is proportional to the rate of change of <b>compression</b> rather the <b>amount</b> of <b>compression,</b> and vanishes as soon as the volume stops changing.|$|R
50|$|After {{testing in}} {{government}} fleets with several prototypes developed by local subsidiaries of Fiat, Volkswagen, GM, and Ford, and compelled {{by the second}} oil crisis, the first 16 gasoline stations began supplying hydrous ethanol in May 1979 for a fleet of 2,000 neat ethanol adapted vehicles, and by July, the Fiat 147 was launched to the market, becoming the first modern commercial neat ethanol-powered car (E100) sold in the world. Brazilian carmakers modified gasoline engines to support hydrous ethanol characteristics. Changes included <b>compression</b> ratio, <b>amount</b> of fuel injected, replacement of materials subject to corrosion by ethanol, use of colder spark plugs suitable for dissipating heat due to higher flame temperatures, and an auxiliary cold-start system that injects gasoline from a small tank to aid cold starting. Six years later, approximately 75% of Brazilian passenger cars were manufactured with ethanol engines.|$|R
40|$|Contact no: 9468261582 ABSTRACT- Now a days Create, edit, and {{generate}} images {{in a very}} regular system for transmission is main priority. Original image data generated by the camera sensor {{is a very large}} store, and therefore is not efficient. It has become particularly troublesome to move or bandwidth-limited systems wherein the object is to be conservative bandwidth cost, such as the World Wide Web. This scenario requires the use of efficient image compression techniques, such as the JPEG algorithm technology, the quality of the compressed image height to which the perceived image with almost no loss. Today JPEG algorithms have become the de facto standard for image <b>compression.</b> The <b>amount</b> of hardware MATLAB code can be output to a quantized DCT version of the input image and techniques used to achieve expeditious manner JPEG algorithm were investigated procedures I...|$|R
40|$|Abstract: <b>Compression</b> {{reduces the}} <b>amount</b> {{of data to}} be sent to ground whilst {{preserving}} its content, allowing a lower bit-rate link to be used or more data to be sent over the same link. This paper presents the results of an investigation into image compression methods for multispectral images for use on-board small satellites. Suitable compression methods are described. An error resilience scheme is proposed and implemented. An improvement to a Neural-Network based data compression method is discussed. The investigated compression methods are compared in terms of error resilience, compression ratio and execution time. 1...|$|R
5000|$|Compressible: According to Miller, [...] "Uncompressed {{digital data}} is very large, {{and in its}} raw form would {{actually}} produce a larger signal (therefore {{be more difficult to}} transfer) than analog data. However, digital data can be compressed. <b>Compression</b> reduces the <b>amount</b> of bandwidth space needed to send information. Data can be compressed, sent and then decompressed at the site of consumption. This makes it possible to send much more information and result in, for example, digital television signals offering more room on the airwave spectrum for more television channels." ...|$|R
40|$|The aim of {{the present}} study was the {{determination}} of formulation factors and the in vitro evaluation of an extended release dosage form of a freely soluble weakly basic drug (alfuzosin hydrochloride). Binary mixer of one hydrophilic polymer (hydroxypropylmethylcellulose) and one directly compressible Eudragit (RS PO) was used in tablets prepared by direct <b>compression.</b> The <b>amounts</b> of both polymers were taken as independent variables for the 3 2 Factorial design. The percent drug releases at 1, 6, 12 and 20 h were selected as responses. The main effect and interaction terms were quantitatively evaluated using mathematical model. Dissolution data were fitted to zero order, first order, and Higuchi&#x 2032;s release kinetics to evaluate kinetic data. Both the diffusion and erosion mechanisms were responsible for drug release as shown by the power law. The release of Alfuzosin was prolonged for 20 h by binary mixer indicating the usefulness of the formulations for once daily dosage forms...|$|R
25|$|Rias {{eventually}} masters a new finishing attack {{during her}} training with Akeno. She later leaves for Romania with Kiba and Azazel {{to know more}} about Gasper's ability. In volume 16 she finally shows her new move, called Extinguished Star; Rias manipulates her Power of Destruction into a <b>compression</b> of unimaginable <b>amounts</b> of demonic power; takes form of an enormous sphere with a mixture of crimson and black aura radiating from inside of it that launches it toward her enemies in a slow velocity. It has the ability of a magnetic force; pulling the enemies towards it and get disintegrate by the latter.|$|R
2500|$|Volkswagen do Brasil {{produced}} and sold neat ethanol-powered, (E100 only), vehicles in Brazil, and production was discontinued {{only after they}} were supplanted by more modern Flex Fuel technology. As {{a response to the}} 1973 oil crisis, the Brazilian government began promoting bioethanol as a fuel, and the National Alcohol Program -Pró-Álcool- (...) was launched in 1975. Compelled by the 1979 energy crisis, and after development and testing with government fleets by the CTA at São José dos Campos, and further testing of several prototypes developed by the four local carmakers, including Volkswagen do Brasil, neat ethanol vehicles were launched in the Brazilian market. Gasoline engines were modified to support hydrous ethanol characteristics and changes included <b>compression</b> ratio, <b>amount</b> of fuel injected, replacement of materials that would get corroded by the contact with ethanol, use of colder spark plugs suitable for dissipating heat due to higher flame temperatures, and an auxiliary cold-start system that injects gasoline from a small tank in the engine compartment to help starting when cold. Within six years, around 75% of all Brazilian passenger cars were manufactured with ethanol engines.|$|R
40|$|A {{multi-gigabit}} {{internet protocol}} (IP) router may receive several million packets per second from each input link. For each packet, the router {{needs to find}} the longest matching prefix in the forwarding table {{in order to determine}} the packet's next-hop. An efficient hardware solution for the IP address lookup problem is presented. The problem is modelled as a searching problem on a binary-trie. The binary-trie is partitioned into fixed size non-overlapping subtrees. Each subtree is represented using a bit-vector and can be searched in parallel for the best matching prefix in a few nanoseconds. The address lookup is implemented using a hardware pipeline with a throughput of one lookup per memory access. A distinguishing feature of the design is that forwarding table entries are not replicated in the data structure. Hence, table updates can be done in constant time with only a few memory accesses. The approach can be extended to IPv 6. By applying path <b>compression,</b> the <b>amount</b> of memory required is upper bounded by O(N) where N is the number of prefixes in the routing table. link_to_subscribed_fulltex...|$|R
40|$|We {{present an}} idea for digital {{watermarking}} of still colour images. The scheme described is blind, so that the watermark detector does not require access to the unmarked image, or an undistorted copy of the watermarked image. We {{make use of the}} correlation of signals in the different colour components of a colour image, even after special desynchronisation attacks which usually defeat blind watermarks, using some components of the image to synchronise the others. We term this the dual channel approach to watermarking of colour images. A mathematical problem describing the difficulties with this approach is formulated, and a solution so simple as to be almost trivial is exhibited. We show how the solution can be used to motivate a practical watermarking scheme. An extremely crude implementation of this scheme is made. Despite its basic nature, this scheme performs well under some preliminary testing, exhibiting robustness to filtering attacks, JPEG <b>compression,</b> small <b>amounts</b> of rotation, scaling, and other linear transformations, and the StirMark tool even with greater than default parameters. ...|$|R
40|$|The {{purpose of}} this study is to {{investigate}} the flexural behavior of high workability high-performance concrete (HPC) beams with high-strength transverse reinforcement under static load. A total of 21 beam specimens were made in this study: seven were normal concrete beams and the rest were HPC beams. The parameters included concrete strength, amount of tension reinforcement, <b>amount</b> of <b>compression</b> reinforcement, <b>amount</b> of transverse reinforcement, and strength of transverse reinforcement. The results show that HPC beams have slightly higher strengths than normal concrete beams. In addition, an increase of tension reinforcement will increase the strength but decrease the ductility of the beams. An increase of compression steel and concrete strength will increase the ductility of the beams effectively HPC beams show better ductility than normal concrete beams. With the same rho (s) f(yh) value of transverse reinforcement, the beams can achieve the same ductility. HPC beams have better crack control over normal concrete beams. Beams with high-strength shear reinforcement exhibit the same crack control ability as beams with normal strength shear reinforcemen...|$|R
40|$|A {{learning}} context memory {{consisting of}} two main parts is presented. The first part performs lossy data <b>compression,</b> keeping the <b>amount</b> of stored data at a minimum by combining similar context attributes — the compression rate for the presented GPS data is 150 : 1 on average. The resulting data is stored in an appropriate data structure highlighting the level of compression. Elements {{with a high level}} of compression are used in the second part to form the start and end points of episodes capturing common activity consisting of consecutive events. The context memory is used to investigate how little context data can be stored containing still enough information to capture regular human activity...|$|R
40|$|Most {{processing}} {{tasks in}} three-dimensional (3 D) applications share common ground {{in terms of}} algorithm design and implementation. Furthermore, time consuming tasks like 3 D reconstruction of objects from generic multiview images and <b>compression</b> of massive <b>amounts</b> of 3 D image data, require enormous data processing power, {{and in many cases}} computations should be performed in real-time. In this paper we present a hardware architecture that jointly addresses both tasks. The proposed design features processing elements and memory modules that form a common compression and reconstruction datapath. Extensive pipelining minimizes throughput delays and memory access operations. Our implementation achieves real-time performance for both tasks, efficiently addressing demanding 3 D imaging and video applications. CC...|$|R
50|$|In computing, steganographically encoded package {{detection}} {{is called}} steganalysis. The simplest method to detect modified files, however, {{is to compare}} them to known originals. For example, to detect information being moved through the graphics on a website, an analyst can maintain known clean copies of the materials and then compare them against the current contents of the site. The differences, if the carrier is the same, comprise the payload. In general, using extremely high compression rates makes steganography difficult but not impossible. Compression errors provide a hiding place for data, but high <b>compression</b> reduces the <b>amount</b> of data available to hold the payload, raising the encoding density, which facilitates easier detection (in extreme cases, even by casual observation).|$|R
50|$|Volkswagen do Brasil {{produced}} and sold neat ethanol-powered, (E100 only), vehicles in Brazil, and production was discontinued {{only after they}} were supplanted by more modern Flex Fuel technology. As {{a response to the}} 1973 oil crisis, the Brazilian government began promoting bioethanol as a fuel, and the National Alcohol Program -Pró-Álcool- (Programa Nacional do Álcool) was launched in 1975. Compelled by the 1979 energy crisis, and after development and testing with government fleets by the CTA at São José dos Campos, and further testing of several prototypes developed by the four local carmakers, including Volkswagen do Brasil, neat ethanol vehicles were launched in the Brazilian market. Gasoline engines were modified to support hydrous ethanol characteristics and changes included <b>compression</b> ratio, <b>amount</b> of fuel injected, replacement of materials that would get corroded by the contact with ethanol, use of colder spark plugs suitable for dissipating heat due to higher flame temperatures, and an auxiliary cold-start system that injects gasoline from a small tank in the engine compartment to help starting when cold. Within six years, around 75% of all Brazilian passenger cars were manufactured with ethanol engines.|$|R
40|$|Adiabatic Compressed Air Energy Storage (A-CAES) {{represents}} a zero emission electrical storage technology together with acceptably high cycle efficiency. Therefore {{the application of}} internal heat storage becomes necessary. One main characteristic of such A-CAES is that the heat generated during <b>compression</b> exceeds the <b>amount</b> of usable heat for the expansion process afterwards. In real life cycling mode this {{could lead to a}} heat storage overload. For heat storage management four possible solutions are proposed and discussed quantitatively. A dynamic model of the whole A-CAES process is under development with the focus on the stratified high-temperature heat storage. After an introduction to basic mechanisms relevant for the understanding of heat management in an A-CAES context, a brief explanation of model structure is given...|$|R
