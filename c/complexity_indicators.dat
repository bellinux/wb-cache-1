38|64|Public
40|$|International audienceThis paper {{presents}} an original method to evaluate air traffic complexity metrics. Several <b>complexity</b> <b>indicators,</b> {{found in the}} litterature, were implemented and computed, using recorded radar data as input. A principal component analysis (PCA) provides some results on the correlations between these indicators. Neural networks are then used to find a relationship between <b>complexity</b> <b>indicators</b> and the actual sector configurations. Assuming that the decisions to group or split sectors are somewhat related to the controllers workload, this method allows to identify which types of <b>complexity</b> <b>indicators</b> are {{significantly related to the}} actual workload...|$|E
40|$|AbstractConstruction {{industry}} {{experts believe that}} project performance frequently suffers due to construction complexity issues. However, there are limited studies focusing on construction project complexity definition, characteristics and indicators. In this paper, a qualitative Delphi method was applied to identify and validate significant <b>complexity</b> <b>indicators.</b> To fulfill this purpose, ten Subject Matter Experts (SMEs) {{were invited to participate}} in a complexity assessment and management workshop. Through two rounds of the Delphi study, the top 30 <b>complexity</b> <b>indicators</b> were identified and ranked. It was concluded that “peak number of participants on the Project Management Team during engineering/design phase of the project”, “magnitude of change orders impacting project execution”, and “frequency of the workarounds” are the top three <b>complexity</b> <b>indicators</b> respectively. Furthermore, the impact weight associated with each of the <b>complexity</b> <b>indicators</b> were calculated. In {{the second part of the}} workshop, the Delphi methodology was again utilized to capture complexity management strategies and practices...|$|E
40|$|International audienceThis paper {{presents}} an original method to evaluate air traffic complexity metrics. In previous works, we applied a {{principal component analysis}} (PCA) to find the correlations among a set of 27 <b>complexity</b> <b>indicators</b> found in the literature. Neural networks were then used to find {{a relationship between the}} components and the actual airspace sector configurations. Assuming that the decisions to group or split sectors are somewhat related to the controllers workload, this method allowed us to identify which components were significantly related to the actual workload. We now focus on the subset of <b>complexity</b> <b>indicators</b> issued from these components, and use neural networks to find a simple relationship between these indicators and the sector status...|$|E
40|$|In this paper, a <b>complexity</b> <b>indicator</b> for 4 D flight trajectories is {{developed}} based on conflict probability. A 4 D trajectory is modeled as piecewise linear segments connected by waypoints. The position of each aircraft is modeled as a 2 D Gaussian random variable and an approximation {{of the conflict}} probability between two aircraft is deduced analytically over each segment. Based on such conflict probability, a <b>complexity</b> <b>indicator</b> is constructed for the whole trajectory. Numerical examples show that the proposed <b>complexity</b> <b>indicator</b> is able to reflect the complexity of 4 D trajectories perceived by air traffic controllers...|$|R
40|$|We {{consider}} {{the number of}} Bowen sets necessary to cover a large measure subset of the phase space. This introduces some <b>complexity</b> <b>indicator</b> characterizing different kinds of (weakly) chaotic dynamics. Since in many systems its value is given by a sort of local entropy, this indicator is quite simple to calculate. We give some examples of calculations in nontrivial systems (e. g., interval exchanges and piecewise isometries) and a formula {{similar to that of}} Ruelle-Pesin, relating the <b>complexity</b> <b>indicator</b> to some initial condition sensitivity indicators playing the role of positive Lyapunov exponents...|$|R
40|$|Abstract. We {{consider}} {{the number of}} Bowen sets which are necessary to cover a large measure subset of the phase space. This introduce some <b>complexity</b> <b>indicator</b> characterizing different kind of (weakly) chaotic dynamics. Since in many systems its value is given by a sort of local entropy, this indicator is quite simple to be calculated. We give some example of calculation in nontrivial systems (interval exchanges, piecewise isometries e. g.) and a formula similar to the Ruelle-Pesin one, relating the <b>complexity</b> <b>indicator</b> to some initial condition sensitivity indicators {{playing the role of}} positive Lyapunov exponents. 1...|$|R
3000|$|Finally, we {{evaluate}} the improvement, {{in terms of}} computational complexity reduction {{with respect to the}} MMSE detection rule, brought by the use of the simplified detection algorithms. As <b>complexity</b> <b>indicators,</b> we choose the numbers of additions and multiplications (referred to as [...]...|$|E
40|$|AbstractThis paper {{focuses on}} the {{modeling}} of assembly supply chain (ASC) systems {{in order to establish}} a framework generating topological classes of ASCs. Subsequently, structural parameters of the networks levels are explored with the aim to generate all possible non–repeated ASC structures. In this paper, we also define several structural complexity metrics indicators, such as the index of vertex degree that is adopted from the Shannon«s entropy theory as well as other pertinent <b>complexity</b> <b>indicators.</b> After the application of these indicators to ASC structures, we benchmark <b>complexity</b> <b>indicators</b> based on predefined criteria. Research results are especially applicable at the early configuration design stage {{to make a decision about}} a suitable networked manufacturing structure that will satisfy the production functional requirements and will make managerial tasks simpler and more cost effective...|$|E
40|$|AbstractProject Complexity Assessment and Management (PCAM) Tool was {{developed}} to help project teams identify, assess, and manage project complexity. This Excel-based tool was designed with a “Complexity Measurement Matrix” comprising of the indicators that are statistically significant to project complexity. The weight factors of these <b>complexity</b> <b>indicators</b> were developed based on the expert ranking result from a subject matter workshop. These factors function as the multipliers to place greater emphasis on the stronger <b>complexity</b> <b>indicators.</b> The comprehensive reports from the tool present the overall project complexity level, a series of radar diagrams describing the most important indicators, and associated management strategies. The outputs help project teams formulate a management plan for the most important contributors with sufficient flexibility to be deployed at multiple project stages and for different project sizes...|$|E
40|$|We {{propose a}} simple <b>complexity</b> <b>indicator</b> of {{classical}} Liouvillian dynamics, namely the separability entropy, which determines the logarithm {{of an effective}} number of terms in a Schmidt decomposition of phase space density with respect to an arbitrary fixed product basis. We show that linear growth of separability entropy provides stricter criterion of complexity than Kolmogorov-Sinai entropy, namely it requires that dynamics is exponentially unstable, non-linear and non-markovian. Comment: Revised version, 5 pages (RevTeX), with 6 pdf-figure...|$|R
40|$|Abstract. We {{study the}} Shannon {{information}} rate of accepting runs of {{various forms of}} automata. The rate is therefore a <b>complexity</b> <b>indicator</b> for executions of the automata. Accepting runs of finite automata and reversal-bounded nondeterministic counter machines, {{as well as their}} restrictions and variations, are investigated and are shown, in many cases, with computable execution rates. We also conduct experiments on C programs showing that estimating information rates for their executions is feasible in many cases. ...|$|R
40|$|This {{paper is}} about {{economic}} complexity, treated as interrelatedness between the parts or sectors of an economy, particularly one represented by an input-output system. The <b>complexity</b> <b>indicator</b> proposed capture two relevant features of interrelatedness {{that can be}} separately measured: a dependency effect and a network effect. This indicator {{can be used in}} two contexts: the direct connections given by the technical coefficient matrix, A and the total (direct plus indirect and induced) effects given by the Leontief inverse, (I-A) - 1. The first results of an empirical application to the Portuguese Case are presented, covering the period 1980 - 1999...|$|R
40|$|We prove a {{quantitative}} recurrence result which allow {{to estimate the}} speed of approaching of a generic orbit to the discontinuities of a map. This result {{is applied to the}} study of <b>complexity</b> <b>indicators</b> for individual orbits generated by a certain zero-entropy discontinuous maps which are related to polygonal billiards and quantum chaos. ...|$|E
40|$|Project {{complexity}} is a {{term that}} is not well understood {{in the construction industry}} in relation to academia and practice. Project complexity often refers to a measurement of the number of project elements and interactions between project elements or a relative comparison of difficulty to what was previously accomplished. Project complexity is a very critical factor that presents additional challenges to achieving project objectives. Impacts of project complexity can be negative if it is not assessed and managed appropriately. Developing a methodology to assess and measure project complexity can help project teams increase the likelihood of success and predictable project outcomes. This research proposed a constructive approach to assess and measure project complexity as a separate factor influencing projects including a model that helps identify the levels of project complexity. In this research, first, the concepts of complexity and its attributes were explored, and the literature relevant to defining and assessing project complexity was studied. A working definition of project complexity was then developed as a basis for describing project complexity. Project complexity was intentionally described in terms of managing projects rather than project physical features such as facility technology, types of materials, or project physical components to ensure that the research results could be generalized across a wide range of industry sectors. The possible project complexity attributes were then identified using complexity theory variables, the literature review results, and industry experience. The identified complexity attributes were used to develop the <b>complexity</b> <b>indicators</b> deemed to measure the associated attributes. The developed <b>complexity</b> <b>indicators</b> were then converted to survey questions for data collection purpose. The collected data was analyzed using statistical methods to test the significance of <b>complexity</b> <b>indicators</b> in differentiating low complexity projects from high complexity projects. The research result showed that thirty-seven <b>complexity</b> <b>indicators</b> associated with twenty-three complexity attributes were significant. These significant <b>complexity</b> <b>indicators</b> were considered to be truly representative of project complexity. The thirty seven significant indicators were then used as the input for the model development process. The model developed in this research is a binary logistic regression that predicts the probability of high complexity or low complexity given the values of <b>complexity</b> <b>indicators.</b> In the research, the multivariable analysis was used as the method to develop the model, and the univariate method was used as an approach to selecting a subset of explanatory variables. The univariate method resulted in a set of 27 <b>complexity</b> <b>indicators</b> out of 37 initial significant indicators that functioned as the measures of project complexity. To generate the required input for the proposed model, the variable reduction process called Principle Component Analysis (PCA) was applied during the model development process {{to reduce the number of}} explanatory variables. PCA process resulted in a significantly smaller number of principal components functioning as the moderating variables in the model (10 PCs). However, this number of moderating variables was still not small enough to create a stable model. Therefore, the univariate method was applied the second time to the set of principle components. The second application of univariate method resulted in the final set of 8 principle components. Those principle components functioned as the final set of moderating variables in the developed model as it was sufficient to generate a stable model. This process helped in generating a numerically stable model while the subject observations were limited. The developed model helps scholars and practitioners in the field of project management assess complexity level of a project based on the applicably identified complexity measures. Given the identified complexity levels, project practitioners can facilitate the management process and formulate a management plan by applying an appropriate complexity management strategy...|$|E
40|$|Software metrics {{have been}} shown to be {{indicators}} of software complexity. These <b>complexity</b> <b>indicators</b> identify error-prone, unmaintainable software, which if identified early can be rewritten and/or thoroughly tested. However, the integration of metric use into commercial situations faces practical resistance from budget and deadline restrictions. This paper presents an experiment that introduces a nondisruptive method for integrating metrics into a large-scale commercial software development environment...|$|E
40|$|This paper {{describes}} {{a novel approach}} for defining an air traffic <b>complexity</b> <b>indicator,</b> aimed at improving over the current operational definition that is only the number of aircraft present in a sector at a given time. The key concept in this work is that a level of disorder or equivalently unpredictability is the right indicator. Our approach is relevant for many applications in the air traffic management area, like sector design ([2]), traffic assignment ([1, 3]) and will be very well suited for predicting complex situations in a future autonomous aircraft environment. Moreover, since this modelling takes explicitly into account the trajectories of the aircraft, either observed or planned, it fits perfectly with 4 D based ATM like SESAR or NextGen...|$|R
40|$|We {{propose a}} {{deterministic}} network mobile automaton {{for the creation}} of planar trivalent networks (trinets) based on the application of only two simple rewrite rules, and we enumerate and explore the possible brownian dynamics of the control point. A useful behavioral <b>complexity</b> <b>indicator</b> is introduced, called revisit indicator, exposing a variety of emergent features, involving periodic, nested and random like dynamics. Regular structures obtained include 1 -D graphs, oscillating rings, and the 2 -D, hexagonal grid. In two cases only, out of over a thousand we have inspected, a remarkably fair, random-like revisit indicator is found, whose trinets exhibit a slow, square-root growth rate; some properties of these surprising computations are investigated. Finally, one 2 -D case is found that seems to be unique in the way regularity and randomness are mixed...|$|R
40|$|In {{many parts}} of the world, future air traffic demands are {{expected}} to exceed air traffic capabilities and, at times, the system does already become overloaded. In this context, in the United States, the Next Generation Air Transportation System (NGATS) vision arises, which calls for a set of system modifications, with the increase and addition of capabilities that are expected to allow a proper response to the future needs of the American air transportation system. As a consequence of these modifications, the controller's tasks and roles are going to change, but it is anticipated that the cognitive complexity is going to remain a limiting factor to the system capacity. Therefore it is necessary to understand the complexity's mechanisms and how it might behave under current and future operations. This work deals with the identification of the key factors that drive complexity and the development of cognitive complexity metrics for the current and future operations. It is presented the experimental setup designed for measuring effects of four-dimensional trajectories and different schedules on air traffic controller perceived complexity. A <b>complexity</b> measure <b>indicator</b> based on the implemented complexity subjective metrics has been derived. This indicator has shown higher sensibility to complexity behavior compared to any of the other metrics alone. Nonetheless, it was found that this <b>complexity</b> <b>indicator</b> has to be used together with metrics that capture aircraft individual complexity, in order to fully describe the underlying cognitive complexity. It is found that Time-Based control induces lower perceived complexity than Position-Based control. Optimized schedules prove to be proner to higher cognitive complexity levels, due to the difficulty of conforming to a tight throughput and executing aircraft swaps. Nevertheless, if optimized schedules are used under Time-Based control, the least complexity measures are detected. This suggests how automation tools and optimized schedules must be considered in the design of future air traffic control operations...|$|R
40|$|In this paper, {{we propose}} the {{application}} of discriminant analysis to select appropriately the algorithm that better solves an instance of the Job Shop Scheduling Problem. The discriminant analysis {{was used as a}} method of machine learning to find the relation between the characteristics of the problem (<b>complexity</b> <b>indicators)</b> and the performance of algorithms. The prediction of the classification obtained of the discriminant analysis was 60 %...|$|E
40|$|Original {{scientific}} paper This paper {{focuses on}} the comparison of different <b>complexity</b> <b>indicators</b> for complexity assessment of selected general process structures. The main objective {{in this study is}} to test their ability to uncover assumed differences in structural complexity among observed general process structures. The obtained results of this theoretical study show that all proposed indicators can be effectively used for analysing structural complexity of general process structures...|$|E
40|$|In this paper, {{we propose}} {{different}} <b>complexity</b> <b>indicators</b> for the well-known nurse scheduling problem (NSP). The NSP assigns nurses to shifts per day taking both hard and soft constraints into account. The {{objective is to}} maximize the nurses’ preferences and to minimize the total penalty cost from violations of the soft constraints. The problem {{is known to be}} NP-hard. Due to its complexity and relevance in practice, the operations research literature has been overwhelmed by different procedures to solve the problem. The complexity has resulted in the development of several (meta-) heuristic procedures, able to solve a NSP instance heuristically in an acceptable time limit. The practical relevance has resulted in a never-ending amount of different NSP versions, taking practical, case-specific constraints into account. The contribution of this paper is threefold. First, we describe our <b>complexity</b> <b>indicators</b> to characterize a nurse scheduling problem instance. Secondly, we develop a NSP generator to generate benchmark instances to facilitate the evaluation of existing and future research techniques. Finally, we perform some preliminary tests on a simple IP model to illustrate that the proposed indicators can be used as predictors of problem complexity. Nurse scheduling; Benchmark instances; Problem classification...|$|E
40|$|Size {{is one of}} {{the most}} {{fundamental}} measurements of software. For the past two decades, the source line of code (SLOC) and function point (FP) metrics have been dominating software sizing approaches. However both approaches have significant defects. For example, SLOC can only be counted when the software construction is complete, while the FP counting is time consuming, expensive, and subjective. In the late 1990 s researchers have been exploring faster, cheaper, and more effective sizing methods, such as Unified Modeling Language (UML) based software sizing. In this paper we present an empirical 14 -project-study of three different sizing metrics which cover different software life-cycle activities: requirement metrics (requirement), UML metrics (architecture), and SLOC metrics (implementation). Our results show that the software size in terms of SLOC was moderately well correlated with the number of external use cases and the number of classes. We also demonstrate that the number of sequence diagram steps per external use case is a possible <b>complexity</b> <b>indicator</b> of software size. However, we conclude that at least for this 14 -project eServices applications sample, the UML-based metrics were insufficiently well-defined and codified to serve as precise sizing metrics. 1...|$|R
40|$|The LMC <b>complexity,</b> an <b>indicator</b> of <b>complexity</b> {{based on}} a {{probabilistic}} description, is revisited. A straightforward approach allows us to establish the time evolution of this indicator in a near-equilibrium situation and gives us a new insight for interpreting the LMC complexity for a general non equilibrium system. Its relationship with the Renyi entropies is also explained. One {{of the advantages of}} this indicator is that its calculation does not require a considerable computational effort in many cases of physical and biological interest. Comment: 11 pages, 0 figure...|$|R
40|$|Most of {{the current}} CO 2 capture {{technologies}} are associated with large energy penalties that reduce their economic viability. Efficiency has therefore {{become the most important}} issue when designing and selecting power plants with CO 2 capture. Other aspects, like reliability and operability, have been given less importance, if any at all, in the literature. This article deals with qualitative reliability and operability analyses of an integrated reforming combined cycle concept. The plant reforms natural gas into a syngas, the carbon is separated out as CO 2 after a water-gas shift section, and the hydrogen-rich fuel is used for a gas turbine. The qualitative reliability analysis in the article consists of a functional analysis followed by a failure mode, effects, and criticality analysis (FMECA). The operability analysis introduces the comparative <b>complexity</b> <b>indicator</b> (CCI) concept. Functional analysis and FMECA are important steps in a system reliability analysis, as they can serve as a platform and basis for further analysis. Also, the results from the FMECA can be interesting for determining how the failures propagate through the system and their effects on the operation of the process. The CCI is a helpful tool in choosing the level of integration and to investigate whether or not to include a certain process feature. Incorporating the analytical approach presented in the article during the design stage of a plant can be advantageous for the overall plant performance...|$|R
40|$|AbstractProject {{complexity}} {{has been}} recognised {{as one of}} the main causes of failures in many energy megaprojects worldwide. This research aims to develop a Project Complexity Assessment (PCA) method, which consists of three components: a taxonomy of Project <b>Complexity</b> <b>Indicators</b> (PCIs), an integrated Delphi and Analytic Hierarchy Process (AHP) process to establish weights of the PCIs, and numerical rating criteria for all PCIs. An innovative aspect of the research is the effective consistency checking and consensus building method during the Delphi-AHP process. The developed PCA method is demonstrated in an energy megaproject case study...|$|E
40|$|There is {{increasing}} pressure to effectively treat patients with complex care needs from the mo-ment of {{admission to the}} general hospital. In this study, the authors developed a measurement strategy for hospital-based care complexity. The authors ’ four-factor model describes the interre-lations between <b>complexity</b> <b>indicators,</b> highlighting differences between length of stay (LOS), ob-jective complexity (such as medications or consultations), complexity ratings by the nurse, and complexity ratings by the doctor. Their findings illustrate limitations {{in the use of}} LOS as a sole indicator for care complexity. The authors show how objective and subjective complexity indica...|$|E
40|$|Due to its {{complexity}} and relevance in practice, many different procedures {{have been proposed}} in the operations research literature to solve the well-known nurse scheduling problem (NSP). The NSP assigns nurses to shifts per day maximizing {{the overall quality of}} the roster while taking various constraints into account. The often highly case-specific workplace conditions in hospital departments have resulted in the development of dedicated (meta-) heuristics to find a workable schedule in an acceptable time limit. However, in spite of research community posing a growing need for benchmarking, these procedures lack any base for comparison. In this paper, we propose a range of <b>complexity</b> <b>indicators</b> which characterize nurse scheduling problem instances, and a problem generator in order to construct a comparative test framework for various solution procedures for the NSP. We show that the different <b>complexity</b> <b>indicators</b> for the NSP presented in this paper predict the computational effort of a particular NSP instance for a particular solution procedure. Moreover, the comparison of procedures and good predictions of their performance allow the a priori selection of the best solution procedure, based on the simple calculation of the indicators. Hence, with the developed NSP generator those indicators can facilitate the evaluation of existing and future research techniques. Tests on a simple IP model illustrate the use of the proposed indicators. Nurse scheduling Problem characterization...|$|E
40|$|The <b>{{complexity}}</b> of <b>indicator’s</b> system {{derives from}} the very complexity of the health concept {{as defined by the}} World Health Organization that "Health is that state of good : physical, mental, social and consists not only in the absence of disease or infirmity". The indicators should cover both aspects of financing health services, technical equipment, human resources in healthcare, quality of healthcare services, issues that affect health, but also short and long term effects of work in this area. The paper analyzes the main indicators characterizing the health system in Romania, aimed at identifying potential strengths and its weaknesses. healthcare system, indicators, efficiency, competitiveness...|$|R
40|$|Kyungwook Kim, 1 Seul Lee, 2 Jong-Hoon Kim 1 – 3 1 Gachon University School of Medicine, 2 Department of Psychiatry, Gil Medical Center, Gachon University School of Medicine, Gachon University, 3 Neuroscience Research Institute, Gachon University, Incheon, Republic of Korea Background: Generalized anxiety {{disorder}} (GAD) is a chronic and highly prevalent disorder that {{is characterized by}} a number of autonomic nervous system symptoms. The {{purpose of this study was}} to investigate the linear and nonlinear complexity measures of heart rate variability (HRV), measuring autonomic regulation, and to evaluate the relationship between HRV parameters and the severity of anxiety, in medication-free patients with GAD. Methods: Assessments of linear and nonlinear complexity measures of HRV were performed in 42 medication-free patients with GAD and 50 healthy control subjects. In addition, the severity of anxiety symptoms was assessed using the State-Trait Anxiety Inventory and Beck Anxiety Inventory. The values of the HRV measures of the groups were compared, and the correlations between the HRV measures and the severity of anxiety symptoms were assessed. Results: The GAD group showed significantly lower standard deviation of RR intervals and the square root of the mean squared differences of successive normal sinus intervals values compared to the control group (P< 0. 01). The approximate entropy value, which is a nonlinear <b>complexity</b> <b>indicator,</b> was also significantly lower in the patient group than in the control group (P< 0. 01). In correlation analysis, there were no significant correlations between HRV parameters and the severity of anxiety symptoms. Conclusion: The present study indicates that GAD is significantly associated with reduced HRV, suggesting that autonomic neurocardiac integrity is substantially impaired in patients with GAD. Future prospective studies are required to investigate the effects of pharmacological or non-pharmacological treatment on neuroautonomic modulation in patients with GAD. Keywords: generalized {{anxiety disorder}}, heart rate variability, parasympathetic modulatio...|$|R
40|$|In this article, the {{refinement}} {{process is}} presented {{with respect to}} model list building using model generators. Performance criteria for built models are used to order the model lists according {{to the needs of}} modeling. Models are classified by means of performance and <b>complexity.</b> An aggregated <b>indicator</b> based on the two factors is proposed and analysed in model list ordering. A software structure for model refinement is presented. A case study shows practical aspects of using the aggregated indicator in model refinement...|$|R
40|$|Megaprojects are characterised {{by their}} {{large-scale}} capital expenditure, long duration and significant levels of technical and process complexity. Empirical {{data show that}} megaprojects in the energy sector experience alarming rates of failure, such as cost overruns, delays in completion and production shortfalls. One of the main causes of failure is their high level of complexity {{and the absence of}} effective tools to assess and manage it. Project complexity has received increasing attention in recent years, both in academia and the industry. However, there is still a lack of consensus on a clear definition for ‘project complexity’ or a comprehensive list of <b>complexity</b> <b>indicators,</b> specifically for energy megaprojects. Furthermore, there is also a lack of a widely accepted assessment method to measure project complexity in a quantitative manner. This study is carried out in response to these problems. First, it develops a taxonomy of project <b>complexity</b> <b>indicators</b> {{on the basis of a}} comprehensive review and synthesis of existing literature. It includes 51 internal and external Project <b>Complexity</b> <b>Indicators</b> (PCIs) in a logical hierarchical structure; these indicators specify the aspects that need to be measured when assessing project complexity. Second, weights for all indicators are established through an integrated Delphi-AHP method, with the participation of 20 international experts. Finally, the study specifies Numerical Scoring Criteria (NSCs) for all indicators based on a synthesis of existing knowledge about megaprojects. The criteria specify the scoring thresholds, on a 1 - 5 scale, for each indicator. These three components constitute a new Project Complexity Assessment (PCA) method, which is implemented as a spreadsheet PCA tool. The developed tool allows a project team to assess and score their project in each of the PCIs against the defined criteria. It then calculates two separate complexity indices for internal and external factors; the results indicate the complexity level of the project. Complexity profiles are also produced to illustrate the complexity scores of different categories of PCIs. The PCA method is tested using an energy megaproject case study. The results demonstrate not only that the tool can help a project team understand the complexity of their project, but also it can help the team to develop appropriate complexity management strategies by comparing the assessment results of different projects...|$|E
40|$|AbstractNowadays, {{competitiveness}} of supply chain companies {{is not just}} about optimizing supply chains within functions, but supply chains have to be optimized across functional elements as well. At this time, there are known several structural <b>complexity</b> <b>indicators</b> that have been validated for specific or/and generic supply chain models. Our intention in this paper is to test their ability to discern configuration dissimilarities between selected organizational concepts. The results of this theoretical study show that some of the indicators can be effectively used for identifying better configuration variant having feasible influence on organization performance while designing supply chain network...|$|E
40|$|Manufacturing {{systems can}} be {{considered}} as a network of machines/workstations, where parts are produced in flow shop or job shop environment, respectively. Such network of machines/workstations can be depicted as a graph, with machines as nodes and material flow between the nodes as links. The aim {{of this paper is to}} use sequences of operations and machine network to measure static complexity of manufacturing processes. In this order existing approaches to measure the static complexity of manufacturing systems are analyzed and subsequently compared. For this purpose, analyzed competitive <b>complexity</b> <b>indicators</b> were tested on two different manufacturing layout examples. A subsequent analysis showed relevant potential of the proposed method...|$|E
40|$|Thesis (Ph. D.), Computer Science, Washington State UniversityIn this thesis, {{we present}} information-theoretic metrics, {{thermodynamic}} metrics and their applications in computer science. First, {{we show that}} the information rate of the language accepted by a reversal-bounded deterministic counter machine is computable. For the nondeterministic case, we provide computable upper bounds. For the class of languages accepted by multi-tape deterministic finite automata, the information rate is computable as well. Second, we study the Shannon information rate of accepting runs of various forms of automata. This rate is a <b>complexity</b> <b>indicator</b> for executions of these automata. Accepting runs of finite automata and reversal-bounded nondeterministic counter machines, {{as well as their}} restrictions and variations, are investigated and are shown, in many cases, to have computable execution rates. Third, we analyze the bit rate of programs and use it to conduct dynamic program analysis. We consider a program as a device that generates discrete time signals, where a signal is an execution. Shannon information rate, or bit rate, of the signal may not be uniformly distributed across the signal so that testing strategies may benefit from the non-uniform distribution. When the program is specified by a finite state transition system, algorithms are provided in identifying information-rich components, on which test cases should focus. For a black-box program that has a partial specification or does not even have a specification, a bit rate signal and its spectrum are studied, which make use of data compression and the Fourier transform. The signal provides a bit-rate coverage for testing the black-box while its spectrum indicates a visual representation for execution’s information characteristics. Fourth, we present a free energy model to measure similarity between automata and formal languages. Last, we propose two heuristic algorithms for graph isomorphism problem. Washington State University, Computer ScienceBy student request, this dissertation cannot be exposed to search engines and is, therefore, only accessible to Washington State University users...|$|R
40|$|Many {{combinatorial}} search {{problems can}} be expressed as `constraint satisfaction problems', and this class of problems {{is known to be}} NP-complete in general. In this paper we investigate restricted classes of constraints which give rise to tractable problems. We show that any set of constraints must satisfy a certain type of algebraic closure condition in order to avoid NP-completeness. We also describe a simple test which can be applied to establish whether a given set of constraints satisfies a condition of this kind. The test involves solving a particular constraint satisfaction problem, which we call an `indicator problem'. Keywords: Constraint satisfaction problem, <b>complexity,</b> NP-completeness, <b>indicator</b> problem 1 Introduction Solving a constraint satisfaction problem is known to be an NP-complete problem in general [13] even when the constraints are restricted to binary constraints. However, many of the problems which arise in practice have special properties which allow them t [...] ...|$|R
40|$|Dynamic Probabilistic Risk Assessment (DPRA) is a {{powerful}} concept {{that is used to}} evaluate design and safety of complex industrial systems. A DPRA model uses a conceptual system representation as a formal basis for simulation and analysis. In this paper we consider an adaptive maintenance of DPRA models that consist in modifying and extending a simplified model to a real-size DPRA model. We propose an approach for quantitative maintainability assessment of DPRA models created with an industrial modeling tool called PyCATSHOO. We review and adopt some metrics from conceptual modeling, software engineering and OO design for assessing maintainability of PyCATSHOO models. On the example of well-known "Heated Room" test case, we illustrate how the selected metrics can serve as early indicators of model modifiability and <b>complexity.</b> These <b>indicators</b> would allow experts to make better decisions early in the DPRA model development life cycle. Comment: Technical repor...|$|R
