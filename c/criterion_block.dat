1|133|Public
40|$|International audienceThis paper {{proposes a}} signal-level turbo packet {{combining}} strategy for multi-antenna multi-rate relay-assisted systems operating over broadband channel. We derive a fixed rate equivalent multi-antenna system communication model where the multi-rate multi-node received signals {{can be viewed}} as direct retransmissions from a virtual node with a fixed transmission rate. Using virtual antenna concept, we introduce a frequency domain turbo packet combiner inspired by minimum mean square (MMSE) <b>criterion.</b> <b>Block</b> error rate (BLER) performance are provided to demonstrate the gains offered by the proposed combining scheme over the conventional soft information-based combining...|$|E
40|$|Handbook {{presents}} {{design and}} analysis of tracking, telemetry, and command functions utilized in these systems with particular emphasis on deep-space telecommunications. Antenna requirements are also discussed. Handbook provides number of tables outlining various performance <b>criteria.</b> <b>Block</b> diagrams and performance charts are also presented...|$|R
40|$|We {{consider}} multi-criteria decision problems {{where there}} is a lexicographically induced prioritization relationship over the criteria. We suggest that the prioritization between criteria can be modeled by making the weights associated with a criteria dependent upon the satisfaction of the higher priority criteria. We implement this using a prioritized scoring operator. We show how the lack of satisfaction to higher order <b>criteria</b> <b>block</b> the possibility of compensation by lower priority criteria. We show that in the special case where the prioritization relationship among the criteria satisfies a linear ordering we can use a prioritized averaging operator...|$|R
5000|$|The <b>criteria</b> of {{eligible}} <b>blocks</b> has been revised {{with effect from}} 1 April 2008 to include the following: ...|$|R
40|$|Description Algorithmic {{experimental}} designs. Calculates exact and approximate theory experimental {{designs for}} D,A, and I criteria. Very large designs may be created. Experimental designs may be blocked or blocked designs created from a candidate list, using several <b>criteria.</b> The <b>blocking</b> {{can be done}} when whole and within plot factors interact. License GPL (> = 2...|$|R
40|$|This paper {{presents}} xmx, a new symmetric {{block cipher}} optimized for public-key libraries and microcontrollers with arithmetic coprocessors. xmx has no S-boxes and uses only modular multiplications and xors. The complete scheme {{can be described}} {{by a couple of}} compact formulae that offer several interesting time-space trade-offs (number of rounds/key-size for constant security). In practice, xmx appears to be tiny and fast : 136 code bytes and a 121 kilo-bits/second throughput on a Siemens SLE 44 CR 80 s smart-card (5 MHz oscillator). 1 Introduction Since efficiency and flexibility are probably the most appreciated design <b>criteria,</b> <b>block</b> ciphers were traditionally optimized for either software (typically SAFER [4]) or hardware (DES [2]) implementation. More recently, autonomous agents and object-oriented technologies motivated the design of particularly tiny codes (such as TEA [9], 189 bytes on a 68 HC 05) and algorithms adapted to particular programming languages such as PERL. Surprisingl [...] ...|$|R
50|$|The Grade of Service can be {{measured}} using different sections of a network. When a call is routed {{from one end to}} another, it will pass through several exchanges. If the Grade of Service is calculated {{based on the number of}} calls rejected by the final circuit group, then the Grade of Service is determined by the final circuit group <b>blocking</b> <b>criteria.</b> If the Grade of Service is calculated based on the number of rejected calls between exchanges, then the Grade of Service is determined by the exchange-to-exchange <b>blocking</b> <b>criteria.</b>|$|R
50|$|No {{option is}} {{provided}} for users to request removing a block if a site ceases {{to meet the}} <b>blocking</b> <b>criteria</b> or has never met {{them in the first}} place.|$|R
40|$|Abstract — This paper {{proposes a}} new {{algorithm}} {{to identify and}} compose building blocks based on minimum mutual information <b>criterion.</b> Building <b>blocks</b> are interpreted as common subsequences between good individuals. The proposed algorithm can extract building blocks in population explicitly. The additively decomposable problems and hierarchical decomposable problems are used to validate the algorithm. The results are compared with Bayesian Optimizatio...|$|R
40|$|A {{continuum}} {{model for}} regular block structures is derived by replacing the difference quotients of the discrete equations by corresponding differential quotients. The homogenization procedure {{leads to an}} anisotropic Cosserat Continuum. For elastic block interactions the dispersion relations of the discrete and the continuous models are derived and compared. Yield <b>criteria</b> for <b>block</b> tilting and sliding are formulated. An extension of the theory for large deformation is proposed. (C) 1997 by John Wiley & Sons, Ltd...|$|R
30|$|Similar to the {{participants}} in the Keyboard condition, the number of errors was higher in EF training when compared with DE, CD, BC, and AB trainings for participants in the Mouse condition (P 6 through P 10). In EF training, P 6 achieved the <b>criterion</b> in four <b>blocks,</b> P 7 in 24 blocks, P 8 in 15 blocks, P 9 in 28 blocks, and P 10 in eight blocks. During DE training, P 6, P 9, and P 10 achieved the <b>criterion</b> after three <b>blocks,</b> P 7 after four blocks, and P 8 after five blocks. Three blocks were used by P 6, P 8, and P 10 for achieving the criterion during CD training, and four blocks were used by P 7 and P 9. For achieving the criterion in BC training three blocks were required for P 6, P 8, and P 10, and four blocks for P 7 and P 9. Finally, during AB training, P 6, P 7, and P 10 achieved the <b>criterion</b> after three <b>blocks</b> and P 8 and P 9 after four blocks. The mean number of blocks required by participants in the Mouse condition was 29.6.|$|R
40|$|Abstract- In {{this work}} a new {{schedule}} for XOR design {{is going to}} be considered. We will first implement SDES algorithm in MATALAB they possess the most popular design <b>criteria</b> for <b>block</b> ciphers. they will replace the normal XOR function with neural network XOR function in the design process the motivation for the proposal is ability of neural networks to perform complex mapping function from one domain to another. Also the non –linearity produced by neural network mapping is desired in cryptography...|$|R
40|$|International audienceAnalysis {{of traffic}} {{performance}} implies capturing the three-way relation between capacity, demand and performance. This relation is generally probabilistic expressing performance <b>criteria</b> like <b>blocking</b> probabilities or expected response times {{in terms of}} assumed statistical traffic characteristics. As well as being a requirement for cost effective sizing, a sound understanding of this relation is essential in designing network traffic controls and resource sharing schemes. In this talk we discuss {{the nature of the}} traffic performance relationship in multiservice wireless networks such as GPRS and UMTS [...] ...|$|R
40|$|Background: Multifocal motor {{neuropathy}} with conduction block (MMN) can {{be mistaken}} for motor neurone disease or other lower motor neurone syndromes, but is treatable with intravenous immunoglobulin (IvIg). Formal electrophysiological <b>criteria</b> for conduction <b>block</b> (CB) are so stringent that substantial numbers of patients may miss out on appropriate treatment...|$|R
40|$|Abstract. In this paper, a {{sufficient}} and {{necessary condition for}} judging block strictly α-diagonally dominant matrices is given firstly. According to the given condition, a new practical <b>criteria</b> for <b>block</b> H-matrices is obtained. Introduction. The block diagonally dominant matrices {{play an important role}} in many science and engineering calculation problems such as the convergence of block iterative schemes for linear systems from numerical solutions of Euler equation. Up to now, within the scope of the field, many researchers have obtained some valuable results in many fields, such as properties of block diagonally dominan...|$|R
40|$|The {{empirical}} {{assessment of}} test techniques {{plays an important}} role in software testing research. One common practice is to instrument faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow <b>criteria</b> (<b>Block,</b> Decision, C-use, P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria, as they show that the use of mutation operators is yielding trustworthy results: generated mutants can be used to predict the detection effectiveness of real faults. Applying such mutation analysis, we then investigate the relative cost and effectiveness of the above mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants which helps decrease the impact of random variation in our analysis and allows us to use a differen...|$|R
40|$|AbstractConditions for a nonsingular matrix {{to have a}} block Toeplitz inverse are obtained. A simpler <b>criterion</b> for a <b>block</b> Toeplitz matrix to have a block Toeplitz inverse is also given. The results generalize {{those of}} Huang and Cline for Toeplitz {{matrices}} with scalar entries, for which alternative statements and proofs are also indicated...|$|R
40|$|ABSTRACT- Two-hundred sixteen white {{laying hens}} {{were used to}} assess the ideal ratio of {{digestible}} threonine:lysine in diets for laying hens at 24 to 40 weeks of age. Birds were assigned to a randomized block design, with six treatments, six replicates per treatment and six birds per experimental unit. The cage was used as the <b>blocking</b> <b>criterion.</b> Experimental diet...|$|R
40|$|This {{paper is}} an attempt to study nX 3 flow shop {{production}} scheduling problems in which the processing time is associated with their respective probabilities transportation time and job <b>block</b> <b>criteria.</b> The objective of the study is to get optimal sequence of the jobs in order to minimize the rental cost using idle/waiting time operator through iterative algorith...|$|R
40|$|FRAMATOME TECHNOLOGIES This {{document}} is the non-proprietary {{version of the}} proprietary document BAW- 10236 P- 00. In order for this document to meet the non-proprietary <b>criteria,</b> certain <b>blocks</b> of information were withheld. The basis for determining what information to withhold {{was based on the}} three criteria listed below. Depending upon the applicable criteria, the criteria code, (b), (c), or (d), represents the withheld information. (b) (c) (d) The information reveals data or material concerning FTI research or development plans or programs of present or potential competitive advantage to FTI. The use of the information by a competitor would decrease his expenditures, in time or resources, in designing, producing o...|$|R
50|$|Squegging is a radio {{engineering}} term. It is the abbreviation of self-quenching. A squegging or self-blocking oscillator produces an intermittent or changing output signal. Wildlife tags for birds and little mammals use squegging oscillators. The Armstrong super-regenerative radio receiver uses a self-blocking oscillator, too. The receiver sensitivity rises while the oscillation builds up. The oscillation stops when the operation point no longer fulfills the Barkhausen stability <b>criterion.</b> The <b>blocking</b> oscillator recovers {{to the initial}} state and the cycle starts again.The receive frequency of the Armstrong Super-Regenerative receiver was some hundred kilohertz. The self-quenching frequency was ten kilohertz, just above the highest audio frequency the headphone could reproduce.|$|R
5000|$|In an {{open letter}} to Prime Minister Kevin Rudd, Reporters Without Borders states that the web filter is not the {{solution}} to combating child sex abuse, and the plan entails risks to freedom of expression. The blocking of websites by ACMA, rather than a Judge, is in contravention of laws. The <b>criteria</b> for <b>blocking</b> [...] "inappropriate" [...] websites is too vague, {{and it would be a}} dangerous censorship option to target [...] "Refused classification" [...] sites, many of which are unrelated to sexual abuse. Subjects such as abortion, anorexia, aborigines and legislation on the sale of marijuana would all risk being filtered, as would media reports on these subjects.|$|R
40|$|Abstract: This {{paper is}} an attempt to {{establish}} a linkage between a flowshop scheduling model having job <b>block</b> <b>criteria</b> with a parallel biserial queue network linked with a common channel in series. The arrival and service pattern both follows Poisson law in queue network. The generating function technique, law of calculus and statistical tools have been used to find out the various characteristics of queue. Further the completion time of jobs in a queue system form the set up time for the first machine in the scheduling model. A heuristic approach to find an optimal sequence of jobs with a job <b>block</b> <b>criteria</b> with minimum total flow time when the jobs are processed in a combined system with a queue network is discussed. The proposed method is easy to understand and also provide an important tool for the decision makers when the production is done in batches. A computer programme followed by a numerical illustration is given to justify the algorithm...|$|R
40|$|Abstract- This Paper {{provides}} a heuristic algorithm for two stage open shop scheduling problem {{to minimize the}} makespan in which processing times are associated with their respective probabilities including the concepts of job <b>block</b> <b>criteria</b> and transportation time. Further weights are attached with jobs to indicate their relative importance. A computer programme followed by a numerical illustration is given to justify the proposed algorithm...|$|R
40|$|The present {{paper is}} an attempt through {{heuristic}} method to obtain the optimal sequence for n jobs two stage open shop problem in which Set up time separated from processing times, each associated with respective probabilities Including job <b>block</b> <b>criteria</b> [...] The algorithm developed in this paper is very simple and easy to understand. A numerical illustration to clarify the algorithm is also given...|$|R
40|$|The {{empirical}} {{assessment of}} test techniques {{plays an important}} role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow <b>criteria</b> (<b>Block,</b> Decision, C-Use, and P-Use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators is yielding trustworthy results: Generated mutants can be used to predict the detection effectiveness of real faults. Applying such a mutation analysis, we then investigate the relative cost and effectiveness of the above-mentioned criteria by revisiting fundamental questions regarding the relationships between fault detection, test suite size, and control/data flow coverage. Although such questions have been partially investigated in previous studies, we can use a large number of mutants, which helps decrease the impact of random variation in our analysis and allows us to use a different analysis approach. Our results are then compared with published studies, plausible reasons for the differences are provided, and the research leads us to suggest a way to tune the mutation analysis process to possible differences in fault detection probabilities in a specific environment...|$|R
40|$|Abstract: The {{specific}} goal of {{this paper}} is to present an efficient heuristic for n-jobs, two machine open shop scheduling involving job <b>block</b> <b>criteria</b> and weights of jobs due to their relative importance, under the condition when probabilities are associate with processing time to obtain an optimal or near optimal sequence which minimize the makespan. The proposed heuristic developed in this paper is straightforward and provides an optimal sequence in an effective manner...|$|R
40|$|AbstractAt low bitrate {{and with}} {{acceptable}} quality in Fractal Image Compression (FIC) of the coded image, the encoding time is very large for most existing algorithms. In this paper, a fast fractal encoding system is proposed using {{particle swarm optimization}} (PSO) to reduce the encoding time. Here, an optimization technique {{is used for the}} MSE based on the stopping <b>criterion</b> between range <b>block</b> and domain block. This PSO technique can speedup the fractal encoder and preserve the image quality for medical imaging...|$|R
40|$|In {{this paper}} we {{introduce}} marginal cost criteria {{to determine the}} optimum time for preventively replacing a fixed group of components. The criteria have a clear interpretation and are very flexible: actual ages can {{be taken into account}} as well as discretely or continuously distributed lifetimes and any number of components. Since an overall optimal policy for the group replacement problem is difficult to establish, we compare the <b>criteria</b> with <b>block</b> replacement policies which replace the group at fixed intervals. Such policies, however, do not take failure renewals into account and may replace relatively new components. The performance of the criteria has been analysed both with discrete-time Markov decision chains and with simulation. In all cases considered the replacement criteria yield an improvement in average costs over the optimum block replacement policy varying between 0 % and 10 %, whil...|$|R
40|$|The {{concept of}} minimum {{aberration}} {{has been extended}} to choose blocked fractional factorial designs (FFDs). The minimum aberration <b>criterion</b> ranks <b>blocked</b> FFDs according to their treatment and block wordlength patterns, which are often obtained by counting words in the treatment defining contrast subgroups and alias sets. When the number of factors is large, there are {{a huge number of}} words to be counted, causing some difficulties in computation. Based on coding theory, the concept of minimum moment aberration, proposed by Xu (2003) for unblocked FFDs, is extended to blocked FFDs. A method is then proposed for constructing minimum aberration blocked FFDs without using defining contrast subgroups and alias sets. Minimum aberration blocked FFDs for all 32 runs, 64 runs up to 32 factors, and all 81 runs are given with respect to three combined wordlength patterns...|$|R
40|$|Entity Resolution In data {{engineering}} {{refers to}} searching for data records originating {{from the same}} entitles across different data sources. The solutions for Entity Resolution usually employ blocking and learning techniques to distinguish matching records from non-matching records. In this thesis, Density Monotonicity is first introduced to block data. Through clustering candidate data via the density information, most of the non-matches can be correctly detected and blocked. As a result, a more balanced dataset can be acquired. Compared to other blocking approaches that rely heavily on manually designed <b>blocking</b> <b>criteria,</b> the density-driven <b>blocking</b> approach can automatically find a suitable <b>blocking</b> <b>criterion</b> without the supervision of human experts. However, with the big-data era coming, the efficiency of data-intensive algorithms is challenged by large-scale datasets. To overcome the challenge from big data, parallel blocking Is a regular way to enhance blocking efficiency. With {{the fact that the}} density property still preserves in any randomly sampled dataset, the centralized blocking algorithm is upgraded to a distributed blocking algorithm. To improve efficiency, a probabilistic technique Is adopted to balance the speed and the effect of the distributed blocking algorithm. After the blocking process, to further retrieve matches from remaining dataset, active learning techniques are adopted in this thesis. With the density property, a novel approach is provided to initialize the classifier. The density-based approach can initialize a high-quality classifier without the involvement of human experts. Through the experiments on real-world datasets, the efficiency and effectiveness of the density-based approaches Is validated. The density-based matching algorithms can achieve a better blocking and learning performance than other state-of-art approaches. Compared to other measures used to detect duplicates, density Information can be attained more easily and cheaply Throughout this thesis, the discovery of the data property and the proposed techniques have been examined through many experiments on real-world data sets and on a real cloud. The experiments related to big data were run in Hadoop MapReduce and Spark Installed In the cloud. The experiments evidence the effectiveness and efficiency of the proposed techniques...|$|R
40|$|We are {{developing}} a blocking resistant, practical and usable system for anonymous web surfing. This means, the system tries to provide as much reachability and availability as possible, even to users in countries where {{the free flow of}} information is legally, organizationally and physically restricted. The proposed solution is an add-on to existing anonymity systems. First we give a classification of <b>blocking</b> <b>criteria</b> and some general countermeasures. Using these techniques, we outline a concrete design, which is based on the JAP-Web Mixes (aka AN. ON) ...|$|R
40|$|Abstract. Video {{compression}} coding {{technology is}} the key to implement network video transmission, and the increasing requirements of network content transmission quality accelerate the development of video compression coding technology. H. 264 is a highly compressed digital video codec standard, and is a hybrid coding mode of differential pulse code modulation(DPCM) and transform encoding. In this paper, we introduced several techniques of hybrid coding algorithm based on H. 264 standard, including differential coding, block matching motion prediction method, normalization <b>criteria,</b> variable-size <b>block</b> motion compensation, and multiple hypothesis motion compensation. We did extensive experiments on variable-size block motion compensation coding method. The experimental results show that, with the premise of the 1 / 4 pixel motion compensation accuracy and hybrid coding, 8 × 8 or larger block partition of content for normal video object, can further make use of space correlation and improve compression efficiency...|$|R
40|$|A fast full search block {{matching}} algorithm is developed. The matching criterion {{is the sum}} of absolute differences or the mean square error. The algorithm evaluates lower bounds for the matching <b>criteria</b> for subdivided <b>blocks</b> {{in order to reduce the}} number of search positions. It also uses the lower bounds for a fast calculation of the matching criterion for the remaining search positions. The computational complexity of the algorithm is evaluated and compared to the three-step search strategy. The search result of the algorithm is identical to the search result of the exhaustive search...|$|R
40|$|This paper {{presents}} a bicriteria in n-jobs, threemachines flow shop scheduling {{to minimize the}} total elapsedtime and rental cost of the machines taken on rent under aspecified rental policy in which the processing time,independent setup time each associated with probabilitiesincluding transportation time and job <b>block</b> <b>criteria.</b> Furtherthe concept of the break down interval for which the machinesare not available for the processing is included. A heuristicapproach to find optimal or near optimal sequence has beendiscussed. A computer programme followed by a numericalillustration is given to substantiate the algorithm...|$|R
40|$|The main design <b>criteria</b> for space-time <b>block</b> codes (STBCs) are {{the code}} rate, {{diversity}} order, coding gain, and low decoder complexity. In this letter, we propose a full-rate full-diversity STBC for 2 × 2 multiple-input multiple-output (MIMO) systems with a substantially lower maximum likelihood (ML) detection complexity {{than that of}} existing schemes. This makes the implementation of high-performance full-rate codes feasible for practical systems. Our numerical evaluation shows that the proposed code achieves significantly lower decoding complexity while maintaining a similar performance {{compared to that of}} existing rate- 2 STBCs...|$|R
