34|96|Public
50|$|It {{provides}} a <b>computerised</b> <b>method</b> for efficiently managing when staff {{are required to}} work and to ensure wards and departments have the right staff {{in the right place}} at the right time. It is used by over 80% of electronically rostered NHS trusts to plan and roster their staff. It permits bank nurses to check shifts that are available on their phones or computers at home.|$|E
50|$|In {{the case}} of robotically-assisted minimally-invasive surgery, instead of {{directly}} moving the instruments, the surgeon uses one of two methods to control the instruments; either a direct telemanipulator or through computer control. A telemanipulator is a remote manipulator that allows the surgeon to perform the normal movements associated with the surgery whilst the robotic arms carry out those movements using end-effectors and manipulators to perform the actual surgery on the patient. In computer-controlled systems the surgeon uses a computer to control the robotic arms and its end-effectors, though these systems can also still use telemanipulators for their input. One advantage of using the <b>computerised</b> <b>method</b> is that the surgeon {{does not have to}} be present, but can be anywhere in the world, leading to the possibility for remote surgery.|$|E
40|$|Objectives: The {{objective}} {{of this study was}} to evaluate the accuracy and reproducibility of three complete denture biofilm indices (Prosthesis Hygiene Index; Jeganathan et al. Index; Budtz-J circle divide rgensen Index) by means of a computerised comparison method. Background: Clinical studies into denture hygiene have employed a large number of biofilm indices among their outcome variables. However, the knowledge about the validity of these indices is still scarce. Materials and methods: Sixty-two complete denture wearers were selected. The internal surfaces of the upper complete dentures were stained (5 % erythrosine) and photographed. The slides were projected on paper, and the biofilm indices were applied over the photos by means of a scoring method. For the <b>computerised</b> <b>method,</b> the areas (total and biofilm-covered) were measured by dedicated software (Image Tool). In addition, to compare the results of the <b>computerised</b> <b>method</b> and Prosthetic Hygiene Index, a new scoring scale (including four and five graded) was introduced. For the Jeganathan et al. and Budtz-J circle divide rgensen indices, the original scales were used. Values for each index were compared with the <b>computerised</b> <b>method</b> by the Friedman test. Their reproducibility was measured by means of weighed kappa. Significance for both tests was set at 0. 05. Results: The indices tested provided similar mean measures but they tended to overestimate biofilm coverage when compared with the <b>computerised</b> <b>method</b> (p < 0. 001). Agreement between the Prosthesis Hygiene Index and the <b>computerised</b> <b>method</b> was not significant, regardless of the scale used. Jeghanathan et al. Index showed weak agreement, and consistent results were found for Budtz-Jorgensen Index (kappa = 0. 19 and 0. 39 respectively). Conclusion: Assessment of accuracy for the biofilm indices showed instrument bias that was similar among the tested methods. Weak inter-instrument reproducibility was found for the indices, except for the Budtz-J circle divide rgensen Index. This should be the method of choice for clinical studies when more sophisticated approaches are not possible. FAPESP (Fundacao de Amparo a Pesquisa do Estado de Sao Paulo...|$|E
50|$|Game {{statistics}} can {{be recorded}} using tally sheets, or computer software {{designed for the}} purpose. <b>Computerised</b> <b>methods</b> are increasingly being used for professional competition and bigger tournaments.|$|R
40|$|Objective: Reporting bias due {{to social}} {{desirability}} {{is an important}} consideration in carrying out surveys on sensitive issues. The study compared the frequency of self reported sensitive behaviours and response rates between the conventional "telephone interviewer method" (TIM) and a combined interviewer and <b>computerised</b> data capturing <b>method</b> (telephone interviewer and <b>computerised</b> questionnaire <b>method,</b> or TICQM) ...|$|R
40|$|How is mankind grow older, its {{knowledge}} about nature, which in it lives, is getting deeper. Unfortunately human brain in itself became short for solving modern problems {{and we have}} to help ourselves with computers and modern <b>computerised</b> <b>methods.</b> But computers themselves can not solve the problems. We need to equip them with adequate software. Goal of this work is to compare Octave and Matlab systems in part of compatibility and in the solvings of practical problems...|$|R
40|$|The {{preliminary}} {{results of a}} multidisciplinary research carried out at the University of Pisa between 1996 and 1999 are presented here. The aim was to reconstruct, through computer tomography data and virtual modelling techniques, three-dimensional models of the possible physiognomy of ancient Egyptian mummies. This new <b>computerised</b> <b>method</b> involved different disciplines: Egyptology, image processing, and anthropology...|$|E
40|$|OBJECTIVES: To {{evaluate}} the relative accuracy of a <b>computerised</b> <b>method</b> to quantitatively assess maximum urinary flow. METHODS: A total of 1147 uroflows {{were evaluated by}} the <b>computerised</b> <b>method</b> and by three experts from different European countries. The sample consisted of uroflows from the respective visits by a 20 % sample of randomly chosen patients (n = 223) with lower urinary tract symptoms with participation in two clinical trials in which the efficacy and safety of Permixon was evaluated. The proportions of automated maximum flow values included in the 10 % extended range of experts (and their 95 % confidence intervals) were assessed, {{as well as the}} concordance coefficients between experts and the <b>computerised</b> <b>method</b> and the paired Student's t-test for the average differences between experts and computer. RESULTS: The rate of agreement between experts and computer varied between about 95 and 100 % over factor levels for visit, type of machine and country. Concordance coefficients indicated good agreement between experts and the automated method. When looking at average differences between experts and the computer, the smallest differences were observed between experts 2, 3 and the computer (differences not statistically significant). Statistically significant average differences were observed between expert 1 and the other experts as well as between expert 1 and the computer. CONCLUSIONS: The computerised assessment decreases the fraction of variability of maximum urinary flow caused by artifacts as well as intra- and inter-expert variation. The computerised assessment of maximum urinary flow is an efficient, consistent and valid approach to quantitatively assess maximum urinary flow in clinical trial...|$|E
40|$|Thesis (MTech (Electrical Engineering)) [...] Peninsula Technikon, Cape Town, 1996 The Southern Nylon Spinning plant, at South African Nylon Spinners in Bellville - Cape Town - South Africa, {{is one of}} {{the oldest}} on the site and a need arose to upgrade the {{existing}} method used in speed monitoring in this particular plant. This system was unable to produce alarms on speed limits being exceeded (i. e. on under-speed or over-speed). There was no alarm logging or historical trending. Manual records on speed were either incomplete or non-existent. Thus the purpose of this study was to investigate the existing speed monitoring system and implement a suitable <b>computerised</b> <b>method</b> of speed monitoring...|$|E
40|$|The {{construction}} of a mass appraisal model requires the preliminary study {{of the real estate}} market, the sampling of sold properties, the development of a forecasting model and the verification of the appraisal results. They are generally <b>computerised</b> <b>methods,</b> that work with geo-referenced data. This experimental work has proceeded to build a mass appraisal model, collecting a data sample of sales of apartments in the city of Palermo, in the five years 2008 – 2012, using a multivariate statistical model (multilevel), testing the results and providing the operating applications in a scheme of online real estate valuations...|$|R
30|$|Our {{study has}} several limitations. First, a true golden {{standard}} {{for the assessment of}} breast density is lacking. There is no accurate way to determine breast density other than histopathologic analysis of mastectomy specimens. It is obvious that these specimens are not available for this study, and previously published studies assessing breast density with various <b>computerised</b> <b>methods</b> also lack this (true) golden standard. As we have shown in this study, our software programme acquired similar results to other, more validated programmes. This is why we have chosen to use our software programme as the reference standard (not golden standard) to which we compared the visual assessment of breast density.|$|R
40|$|Interpreting {{the results}} of <b>computerised</b> <b>methods</b> in {{archaeology}} cannot be done without a reference to theoretical archaeology. The main aim {{of this paper is}} to discuss the theoretical assumptions behind the use of GIS and visibility analysis in modelling controlled territories. An underlying assumption is that changing locations of settlements are related to changing needs of communities in their environment. The relationship between visible areas and those needed for subsistence is reviewed in a specific context. The case studies presented are those of Nepi and Gabii. The different position these sites had in central Italian settlement hierarchies is discussed in relation with the interwoven relationship between assumptions on and interpretations of {{the results of}} visibility analyses...|$|R
40|$|Picture {{archiving}} {{and communication}} system: prospective study Objectives. To evaluate {{the use of}} a picture archiving and communication system and user satisfaction in order to further improve its quality. Design. Prospective study. Setting. Medical college hospital, Japan. Materials and methods. An automated <b>computerised</b> <b>method</b> was used to collect the data from March 1999 to February 2000. Main outcome measures. Each workstation automatically recorded data on the rank of the user, purpose of use, use of postprocessing tools, and user satisfaction. Results. The number of resident users in the radiology reading room increased and those outside the reading room decreased, but the number of staff users changed little. The purpose of use and the use of postprocessing functions in the reading room were not significantly different from those outside it (P= 0. 179 and P= 0. 269, respectively). The average numbers of images accessed per workstatio...|$|E
40|$|A new <b>computerised</b> <b>method</b> of {{measurement}} of the anterior chamber in the human eye is described. Photographs of the anterior chamber were taken with a Zeiss slit-lamp 75 -SL and converted from an optical image into a true image by computer with a digitiser. We obtained data on both the anterior chamber volume and also the anterior chamber depth, peripheral anterior chamber volume, {{the diameter of the}} anterior chamber, and the degree of the iridocorneal angle. These data were simultaneously displayed several seconds after in-put by the digitiser. In mensuration by digitiser on three occasions from a single photograph of a given eye the mean deviation rate was about 0. 7 %, and the mean deviation rate of 10 successive exposures of one eye was 3. 5 %. By this image analysis technique an accurate profile of the anterior chamber is displayed of the pupil margin, iris-root insertion, and central anteroposterior pupillary axis. These data help us to understand the mechanism of primary angle-closure glaucoma...|$|E
40|$|To {{simplify the}} {{selection}} of tests for bacteriological typing methods, such as bacteriophage, bacteriocin, and biotyping, a <b>computerised</b> <b>method</b> was assessed. This uses a numerical index of discrimination (D) to facilitate {{the selection of}} an efficient typing set. The computer programs take the most discriminatory test as the initial test in the partial typing set, and then select the next test by combining each of the remaining candidates with the partial set and choosing the test which maximises D. This cycle is repeated until the remaining candidates do not increase the discriminatory power of the typing set. Options are provided for the investigator to pre-select certain tests for inclusion or exclusion from the typing set. It is concluded that the numerical index D is a simple means of test selection, {{but it must be}} emphasised {{that it is important to}} combine its use with data on the incidence of reaction in each test, on reproducibility, and on the similarity among tests...|$|E
40|$|Abstract. The paper {{outlines}} {{curriculum development}} activities {{that have been}} done in science education in the Slovak Republic {{as a result of an}} international collaboration within the frame of the Leonardo da Vinci II pilot project Computerised Laboratory in Science and Technology Teaching –“ComLab-SciTech”. The created teaching and learning materials include integration of science curricula in two meanings: an integration of knowledge and methodology of physics, chemistry and biology, as well as an integration of various true and virtual <b>computerised</b> <b>methods</b> of experiments. The materials contain suggestions for student investigative activities, in which life science processes are studied with the use of laboratory models. Key words: science education, life science,integration,investigations, computer-aided experiments...|$|R
40|$|BACKGROUND: Conjunctival {{ultraviolet}} autofluorescence (UVAF) photography {{was developed}} to detect and characterise pre-clinical sunlight-induced UV damage. The reliability of this measurement {{and its relationship to}} outdoor activity are currently unknown. METHODS: 599 people aged 16 - 85 years in the cross-sectional Norfolk Island Eye Study were included in the validation study. 196 UVAF individual photographs (49 people) and 60 UVAF photographs (15 people) of Norfolk Island Eye Study participants were used for intra- and inter-observer reliability assessment, respectively. Conjunctival UVAF was measured using UV photography. UVAF area was calculated using <b>computerised</b> <b>methods</b> by one grader on two occasions (intra-observer analysis) or two graders (inter-observer analysis). Outdoor activity category, during summer and winter separately, was determined with a UV questionnaire. Total UVAF equalled the area measured in four conjunctival areas (nasal/temporal conjunctiva of right and left eyes). RESULTS: Intra-observer (ρ_c= 0. 988, 95...|$|R
40|$|International audienceThe method {{presented}} here relies upon text-genealogical principles {{inspired by the}} Lachmannian or neo–Lachmannian tradition, and attempts to computerise them, following and extending the procedure first proposed by E. Poole in the 70 ’s. More than the application of <b>computerised</b> <b>methods</b> to philology, this method seeks to extend philology through {{the aid of the}} computer. It favours interaction between philologist and computer, and requires the former’s critical judgement at some points. After a careful selection of variant locations, needed to eliminate contamination and polygenesis (the two major factors that could impede the elaboration of a stemma), we then proceed to produce a stemma that is, at least at first, a simplification. The method is applied to two traditions: the artificial Parzival and the Bestiaires d’Amors by Richart de Fournival. The results obtained on the second are close to the hypotheses of C. Segre and J. Holmberg / G. B. Speroni...|$|R
40|$|The study {{continued}} validating the <b>computerised</b> <b>method</b> of implicit concept map-ping (Aidman & Egan, 1998), while extending it from {{assessing the}} map’s structural properties to content-based expert evaluation. The on-line concept mapping task (Aid-man & Egan, 1998) was modified to elicit similarity / contrast judgements {{for a set}} of basic personality concepts, in a group of 65 introductory psychology students. The re-sulting individual concept proximity matrices were scored for complexity and internal consistency, as well as individually factor- and cluster analysed. Hierarchical cluster tree and un-rotated factorial representations were generated for each individual map. Stu-dents were asked to interpret their own cluster trees and factor plots by naming the clusters and factor axes (a brief statement accompanying the name was allowed). Three independent experts (lecturers in the subject) rated the clarity and accuracy of these interpretations, as well as the soundness of cluster trees and the factorial representa-tions themselves. These data were compared with the overall grade the students ha...|$|E
40|$|An Origamic Architecture (OA) is {{a folded}} sheet of per-forated paper {{from which a}} {{three-dimensional}} structure “pops up ” when it is opened. It {{is similar to a}} “pop-up story book”, but its unique feature is that it is made purely by cutting a single piece of paper. Because of this limitation, designing an OA requires considerable experience. We pro-pose a <b>computerised</b> <b>method</b> which assists design of OAs. An OA is modelled using a set of planar polygons. This model must satisfy the conditions required of a valid, realis-able OA. A unique point of our method is the application of boolean set operations to the polygons on the unfolded pat-tern to guarantee that the model can be made from a single sheet of paper. We also present a procedure for checking the model’s validity. Additionally, we propose methods for cre-ating openings, for generating unfolded patterns, and for displaying folding animation. We have implemented a sys-tem based on these methods and demonstrated its usefulness for creating OA. Our system allows designers to intuitively design OA models and to easily generate the unfolded pat-terns. ...|$|E
40|$|Wireless capsule {{endoscopy}} (WCE) has revolutionised {{the diagnosis}} and treatment of gastrointestinal tract, especially the small intestine where traditional endoscopies cannot reach. However, this new technology leads to the inspection {{of a large number of}} images, which is a time-consuming process and also too hard by naked eyes for doctors. In this paper, we propose a new <b>computerised</b> <b>method</b> for bleeding detection in WCE images. We use the second component of CIE Lab colour space together with appropriate segmentation and enhancement techniques, involving an adaptive anisotropic diffusion (alike Perona–Malik diffusion). As a result of this procedure, it is possible to devise four functions to discriminate between bleeding and normal regions in WCE images. These four bleeding detectors rely on the eigenvalues of the Hessian and on the Laplacian of the modified enhanced image. Multiscale image analysis approach is also involved in the definition of these detectors for handling the maximum and minimum sizes at which the bleeding regions are expected to be found. Experimental results on several medical data-sets show that the new algorithm achieves a very good rate of success and promising performance for bleeding detection...|$|E
40|$|To use {{pressure}} {{transient test}} data in <b>computerised</b> <b>methods</b> for integrated reservoir charcterisation numerical simulations {{of the well}} tests are typically required. Numerical artifacts occurring in the simulation must be avoided {{as much as possible}} so that they do not adversely a#ect the reservoir characterisation. This work explores the advantages of a hybrid boundary element method known as the Green element method for modeling pressure transient tests. Boundary element methods are a natural choice for the problem because they are based on Green's functions, which are an established part of well test analysis. The classical boundary element method is limited to single phase flow in homogeneous media. This works presents formulations which give computationally efficient means to handle heterogeneity. The accuracy of the scheme is further enhanced by incorporating singularity programming. Comparisons of the proposed Green element approach to standard finite di#erence simulation show t [...] ...|$|R
40|$|Product {{modularity}} is {{an extremely}} powerful principle {{that can be used}} by industry to gain many advantages such as ease of product upgrade, maintenance, repair and disposal, increased product variety and greater product development speed. Researchers have outlined the advantages of modularity at various stages of the product lifecycle and have developed many modularity measures and identification techniques. However there are a lack of <b>computerised</b> <b>methods</b> that can be applied to optimise modularity for multiple lifecycle objectives. To this end a multi-objective methodology has been developed to search for optimal modules within a design structure matrix (DSM) representation of the product. Firstly, an analytic hierarchy process is applied to create a hierarchical weighting system for the modular driver based component interactions. Then an genetic algorithm(GA) with a goal based programming fitness function is applied to search for a set of module combinations that will maximise modularity for multiple product lifecycle objectives...|$|R
30|$|The {{borders of}} the MLV can be {{delineated}} visually or using various semi-automated techniques, however no single technique has proven optimal for all applications [6]. In most cases lesion delineation is still performed manually, based on visual interpretation of PET or CT images. This is prone to inter- and intra-operator variation, especially for PET due to its lower spatial resolution and inherent noise [8 – 15]. Multiple methods are used and proposed to decrease variation in lesion segmentation. These include using reference values to normalise the lesion- to- background uptake intensity by comparison to liver or mediastinal blood pool uptake [16 – 18] {{and the use of}} automated segmentation techniques. Automated segmentation techniques include thresholding techniques, gradient-based techniques and stochastic- and learning-based <b>computerised</b> <b>methods</b> [6]. Thresholds may be fixed or adaptive. Adaptive thresholds utilises image parameters, such as lesion-to-background ratio, mean background intensity and estimated lesion intensity in algorithms to define the threshold.|$|R
40|$|Direct student-patient contacts, {{during the}} {{professional}} clinical placement of a Master of Nutrition and Dietetics course, were collected and analysed {{for the first}} time using a <b>computerised</b> <b>method.</b> In the final eight-week hospital placement, 26 dietetic students submitted data on direct patient contacts which included: dietetic activities (e. g. assessing, counselling and reviewing); the primary nutritional condition of the patient (e. g. type 2 diabetes and liver disease); and the time spent in contact with patients. The most common dietetic activities were reviews, followed by collection of dietary information and counselling. The most common nutritional condition encountered by students was an inadequate nutrient intake, followed by patients receiving enteral nutrition. Contact time with patients increased over the placement, with proportionately more time spent by students seeing patients independently than when being observed by supervising dietitians. The data collected provided valuable informa tion on {{the amount of time spent}} by students in direct patient contacts, the range of dietetic activities undertaken and the amount of time student activities were directly observed. This information will be useful in the development of benchmarks for clinical skill development, hospital and university staff planning and the assessment of the impact of any changes to the format of student placement experience in the clinical setting. <br /...|$|E
40|$|The aims of {{this study}} were to {{validate}} a <b>computerised</b> <b>method</b> to detect muscle activity from surface electromyography (SEMG) signals in gait in patients with cervical spondylotic myelopathy (CSM), and to evaluate the test-retest reliability of the activation times designated by this method. SEMG signals were recorded from rectus femoris (RF), biceps femoris (BF), tibialis anterior (TA), and medial gastrocnemius (MG), during gait in 12 participants with CSM on two separate test days. Four computerised activity detection methods, based on the Teager-Kaiser Energy Operator (TKEO), were applied to a subset of signals and compared to visual interpretation of muscle activation. The most accurate method was then applied to all signals for evaluation of test-retest reliability. A detection method based on a combined slope and amplitude threshold showed the highest agreement (87. 5 %) with visual interpretation. With respect to reliability, the standard error of measurement (SEM) of the timing of RF, TA and MG between test days was 5. 5 % stride duration or less, while the SEM of BF was 9. 4 %. The timing parameters of RF, TA and MG designated by this method were considered sufficiently reliable for use in clinical practice, however the reliability of BF was questionable...|$|E
40|$|The {{diagnosis}} of autonomic neuropathy frequently depends on results of tests which elicit reflex changes in heart rate. Few well-documented normal ranges {{are available for}} these tests. The present {{study was designed to}} investigate the effect of age upon heart rate variability at rest and in response to a single deep breath, the Valsalva manoeuvre, and standing. A <b>computerised</b> <b>method</b> of measurement of R-R interval variation was used to study heart rate responses in 310 healthy subjects aged 18 - 85 years. Heart rate variation during each procedure showed a skewed distribution and a statistically significant negative correlation with age. Normal ranges (90 % and 95 % confidence limits) for subjects aged 20 - 75 years were calculated for heart rate difference (max-min) and ratio (max/min) and standard deviation (SD). Heart rate responses were less than the 95 th centile in {{at least one of the}} four procedures in 39 (12. 6 %) out of the 310 subjects, and were below this limit in two or more tests in five (1. 6 %) subjects. In view of the decline in heart rate variation with increasing age, normal ranges for tests of autonomic function must be related to the age of the subject...|$|E
40|$|AbstractThe {{morphology}} of cortical {{grey matter}} is commonly assessed using T 1 -weighted MRI together with automated <b>computerised</b> <b>methods</b> such as voxel-based morphometry (VBM) and cortical thickness measures. In the presented study we investigate how grey matter changes identified using voxel-based cortical thickness (VBCT) measures compare with local grey matter volume changes identified using VBM. We use {{data from a}} healthy aging population to perform the comparison, focusing on brain regions where age-related changes have been observed in previous studies. Our results show that overall, in healthy aging, VBCT and VBM yield very consistent results but VBCT provides a more sensitive measure of age-associated decline in grey matter compared with VBM. Our findings suggest that while VBCT selectively investigates cortical thickness, VBM provides a mixed measure of grey matter including cortical surface area or cortical folding, as well as cortical thickness. We therefore propose that used together, these techniques can separate the underlying grey matter changes, highlighting the utility of combining these complementary methods...|$|R
40|$|The {{value of}} {{scintigraphy}} in predicting {{development of new}} erosions in small peripheral joints was studied by visual evaluation of scintigrams and by three <b>computerised</b> <b>methods.</b> In 13 patients with newly diagnosed rheumatoid arthritis a total of 387 joints were examined clinically, scintigraphically, and radiographically. The follow up period was 24 months. Four eroded joints in three patients were found at the onset. Of the joints which were to become eroded, 46 / 47 were scintigraphically active at all the check ups. Erosions were detected earlier in foot joints than in finger joints. New erosions were especially prone to appear in joints with persisting and high scintigraphic activity. On the contrary, inactive joints by repeated scanning never eroded. Scintigraphic and clinical activity and radiographic erosiveness correlated significantly with each other. The sensitivity and specificity of visual scintigraphic assessment and the relative pixel activity method proved to be superior to the region of interest methods and clinical evaluation for prediction of erosiveness...|$|R
40|$|The {{morphology}} of cortical {{grey matter}} is commonly assessed using T 1 -weighted MRI together with automated <b>computerised</b> <b>methods</b> such as voxel-based morphometry (VBM) and cortical thickness measures. In the presented study we investigate how grey matter changes identified using voxel-based cortical thickness (VBCT) measures compare with local grey matter volume changes identified using VBM. We use {{data from a}} healthy aging population to perform the comparison, focusing on brain regions where age-related changes have been observed in previous studies. Our results show that overall, in healthy aging, VBCT and VBM yield very consistent results but VBCT provides a more sensitive measure of age-associated decline in grey matter compared with VBM. Our findings suggest that while VBCT selectively investigates cortical thickness, VBM provides a mixed measure of grey matter including cortical surface area or cortical folding, as well as cortical thickness. We therefore propose that used together, these techniques can separate the underlying grey matter changes, highlighting the utility of combining these complementary methods...|$|R
30|$|The {{diagnosis}} of brain tumours is usually made individually using MRI imaging. Its accuracy {{may be limited}} {{by the presence of}} a typical cases or by a radiologist’s insufficient clinical experience. A <b>computerised</b> <b>method</b> that is capable of providing objective information about an image may assist radiologists in the classification of brain tumours. A number of approaches have been used to segment and predict the grade and volume of the brain tumour. Vijayakumar et al. [74] proposed a computer-assisted method based on hierarchical multiresolution wavelet to perform segmentation of brain tumours on apparent diffusion coefficient (ADC) images. Kitajima et al. [75] developed an algorithm for differential diagnosis among pituitary adenoma, craniopharyngioma and Rathke’s cleft cyst with MRI images. The results showed high performance in differentiation when they used the computer output. Papageorgiou and Spyridonos [76] developed a fuzzy cognitive map (FCM) to find the grade value of the tumour. Authors used the soft computing method of fuzzy cognitive maps to represent and model experts knowledge of the FCM grading model. Ibrahim et al. [77] used the image mosaicking method in evaluating the MRI brain abnormalities segmentation study. Fifty-seven mosaic images were formed by cutting various shapes and sizes of abnormalities and pasting it onto normal brain tissue. Karpagam and Gowri [78] proposed a computer assistance-based algorithm for detection of tumour growth by advanced diameter technique from MRI data. To find the volume of brain tumour, they proposed diameter and graph-based methods.|$|E
40|$|OBJECTIVES [...] To {{improve the}} {{reproducibility}} {{and accuracy of}} joint space width (JSW) measurement as an assessment of cartilage loss in patients with osteoarthritis (OA) of the knee by determining how precision and accuracy of JSW measurement were altered by a <b>computerised</b> <b>method</b> of measurement, correction for radiographic magnification, radiography of the knee in the standing semiflexed view, and high definition macroradiography of the knee in the semiflexed view [...] taking JSW measurements from standard radiographs of OA knees in the extended view as the standard for comparison. METHODS [...] Twenty five OA and 10 nonarthritic knees were radiographed in the extended view and minimum JSW was measured manually. Conventional and x 5 macroradiographs were taken in the semiflexed view. All radiographs were taken twice {{on the same day}} and repeated two weeks later. Automated computerised measurement of minimum JSW was obtained from digitally stored images of all radiographs. RESULTS [...] For medial compartment JSW measurements, computerised was more accurate than manual, correction for radiographic magnification improved precision and accuracy, measurements in the semiflexed view were more precise and accurate, and macroradiography increased measurement precision. For the lateral compartment JSW measurements, correction for radiographic magnification improved precision and accuracy, and the semiflexed view improved precision only. CONCLUSIONS [...] Protocols defining radiographic and mensural procedures are essential for quality control of knee radiography in the semiflexed view to permit accurate and reproducible measurement of JSW. Macroradiography provides greater precision of JSW measurement...|$|E
40|$|Ready reckoners {{are used}} in the {{clinical}} setting as a tool for the estimation of nutrient intake. With increasing opportunities for nutrition research, ready reckoners may provide for a more rapid analysis of nutritional intake than computerised methods, often seen as the gold standard for nutritional analysis. This research aimed to determine the level of agreement between ready reckoner and computerised dietary analysis through a secondary analysis of clinical trial data. Participant food intakes were estimated by trained observers using the one-quarter method. Daily energy and protein intake were estimated by the healthcare network ready reckoner and computerised dietary analysis. Agreement between methods was tested using t-tests, correlations and Bland-Altman plots. A correlation between analysis methods was observed (r = 0. 9086 energy, r = 0. 8700 protein). Wide limits of agreement were observed for both energy and protein intake. Compared with the <b>computerised</b> <b>method,</b> ready reckoner analysis underestimated energy intake by 600 kJ and protein intake by 5 g. Mean energy and protein intake calculated by each method was significantly different (p < 0. 0001 energy; p < 0. 0001 protein). No time differences between analysis methods were observed. In the clinical setting, practitioners {{should be aware of the}} variability of a ready reckoner compared to computerised dietary analysis. Further investigation into the acceptability of ready reckoners as a reliable method of nutrient intake determination, particularly for analysis of nutrition research, is required...|$|E
40|$|The optimal modular {{configuration}} of a product’s architecture {{can lead to}} many advantages throughout the product lifecycle. Advantages such as: ease of product upgrade, maintenance, repair and disposal, increased product variety and greater product development speed. However, finding an optimal modular configuration is often difficult. Finding a solution will invariably mean trade-offs {{will have to be}} made between various lifecycle drivers. One of the main strengths of a computerised optimisation is that trade-off analysis becomes simple and straightforward and hence speeds up the product architecture decision making process. However, there are a lack of <b>computerised</b> <b>methods</b> that can be applied to optimise modularity for multiple lifecycle objectives. To this end, a genetic algorithm based optimisation framework has been developed to optimise modularity from a whole lifecycle perspective, namely, design, production, use and end of life. The paper will look briefly at the optimisation criteria then examine the optimisation framework - in particular the specialised developed genetic algorithm...|$|R
40|$|Project Abstract: Ancient Mesopotamia, {{birthplace of}} writing, has {{produced}} {{vast numbers of}} cuneiform tablets that {{only a handful of}} highly specialized scholars are able to read. The task of studying them is so labor intensive that the vast majority have not yet been translated, with the result that their contents are not accessible either to historians in other fields or to the wider public. This project will develop and apply new <b>computerised</b> <b>methods</b> to translate and analyse the contents of some 67, 000 highly standardised administrative documents from southern Mesopotamia from the 21 st century BC. By automating these basic but labor-intensive processes, we will free up scholars’ time. The tools that we will develop, combining machine learning, statistical and neural machine translation technologies, may then be applied to other ancient languages. Similarly, the translations themselves, and the historical, social and economic data extracted from them, will be made publicly available on the web...|$|R
30|$|Recently, a few {{automated}} <b>computerised</b> classification <b>methods</b> {{have been}} proposed to diagnose neurological diseases. They are sufficiently robust to handle data from different scanners for many applications. The numbers of developed CAD approaches are too large to review in a single article. Thus, in this section, we provide a brief review considering some of the essentials and recent researches of those for assisting neurologists in detection of neurological diseases.|$|R
