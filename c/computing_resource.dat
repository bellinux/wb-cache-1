699|8029|Public
5|$|The Mark 1 was {{to provide}} a <b>computing</b> <b>resource</b> within the university, to allow {{researchers}} to gain experience in the practical use of computers, but it very quickly also became a prototype on which the design of Ferranti's commercial version could be based. Development ceased at the end of 1949, and the machine was scrapped towards the end of 1950, replaced in February 1951 by a Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.|$|E
5000|$|Unable {{to switch}} {{if the system}} wrongly {{estimated}} the required <b>computing</b> <b>resource</b> ...|$|E
50|$|Avro Keyboard {{has been}} listed as useful Bengali <b>computing</b> <b>resource</b> by the Unicode consortium.|$|E
40|$|In this research, {{we suggest}} {{appropriate}} information technology (IT) governance structures {{to manage the}} cloud <b>computing</b> <b>resources.</b> The interest in acquiring IT resources a utility is gaining momentum. Cloud <b>computing</b> <b>resources</b> present organizations with opportunities to manage their IT expenditure on an ongoing basis, and are providing organizations access to modern IT resources to innovate and manage their continuity. However, cloud <b>computing</b> <b>resources</b> are no silver bullet. Organizations would need to have appropriate governance structures and policies in place to ensure its effective management and fit into existing business processes to leverage the promised opportunities. Using a mixed method design, we identified four possible governance structures for managing the cloud <b>computing</b> <b>resources.</b> These structures are a chief cloud officer, a cloud management committee, a cloud service facilitation centre, and a cloud relationship centre. These governance structures ensure appropriate direction of cloud <b>computing</b> <b>resources</b> from its acquisition {{to fit into the}} organizations business processes...|$|R
40|$|CompuP 2 P is an {{architecture}} for sharing of <b>computing</b> <b>resources</b> in peer-to-peer (P 2 P) networks. It provide resources, such as processing power, memory storage etc., to user applications that might require them. CompuP 2 P creates dynamic markets for different amounts of <b>computing</b> <b>resources</b> without relying on any trusted centralized entity {{to monitor the}} activities of nodes in those markets. Moreover, the pricing of <b>computing</b> <b>resources</b> takes into account selfishness of network users and uses ideas from game theory and microeconomics...|$|R
25|$|<b>Computing</b> <b>resources</b> are not {{administered}} centrally.|$|R
5000|$|CREAM- <b>Computing</b> <b>Resource</b> Execution And Management Service for job {{management}} operation at the computing element (CE) level ...|$|E
50|$|In computing, {{time-sharing}} is {{the sharing}} of a <b>computing</b> <b>resource</b> among many users by means of multiprogramming and multi-tasking at the same time.|$|E
5000|$|For instance, when {{sideband}} computing {{applies to}} the social computing based on each client creating or recreating social conventions and social contexts {{through the use of}} client’s <b>computing</b> <b>resource,</b> software and technology.|$|E
5000|$|... allows around-the-clock high {{utilization}} of expensive <b>computing</b> <b>resources</b> ...|$|R
5000|$|Distributed <b>computing</b> <b>resources</b> (Estonian Scientific <b>Computing</b> Infrastructure, Estonian Grid) ...|$|R
5000|$|... avoids idling the <b>compute</b> <b>resources</b> without minute-by-minute human {{supervision}} ...|$|R
5000|$|The Computer History Museum holds {{materials}} {{related to}} PROPHET, including two brochures about [...] "Prophet, a National <b>Computing</b> <b>Resource</b> for Life Science Research" [...] from BBN and a BBN Prophet II Design Manual.|$|E
50|$|The openQRM {{platform}} emphasizes {{a separation}} of hardware (physical servers and virtual machines) from software (operating system server-images). Hardware is treated agnostically as a <b>computing</b> <b>resource</b> which should be replaceable without the need to reconfigure the software.|$|E
5000|$|As part of {{the work}} of Calit2, he is Principal Investigator on the NSF OptIPuter LambdaGrid project, an [...] "optical {{backplane}} for planetary scale distributed computing" [...] and the CAMERA Project, a high-performance <b>computing</b> <b>resource</b> for genomic research.|$|E
5000|$|... #Subtitle level 3: UNICORE: {{easy access}} to <b>computing</b> <b>resources</b> ...|$|R
50|$|DreamHost's DreamCompute is {{a public}} cloud {{computing}} service that provides scalable <b>compute</b> <b>resources</b> for developers and entrepreneurs. DreamCompute users select the amount of <b>compute</b> <b>resources</b> and storage resources needed and define their own virtual networks. DreamCompute is powered by OpenStack and Ceph and is designed for scalability, resiliency, and security.|$|R
40|$|<b>Computing</b> <b>resources</b> {{supporting}} our {{activities are}} moving over from PC and workstations to the Clouds across Internet. Cloud is collective <b>computing</b> <b>resources,</b> which consist from computers, network, data storage, and data themselves. Cloud serves {{a large number}} of users and clients and it is pushing the demand for large-scale dat...|$|R
5000|$|Acorn Arcade is a <b>computing</b> <b>resource</b> {{web site}} {{with a focus}} on the [...] {{operating}} system and its gaming scene. It has been recognised by Acorn User magazine and was an award winner at the Wakefield Acorn Computer Show.|$|E
5000|$|In 2000, Sun {{acquired}} Gridware, Inc. {{a privately}} owned commercial vendor of advanced <b>computing</b> <b>resource</b> management software {{with offices in}} San Jose, Calif., and Regensburg, Germany. [...] Later that year, Sun offered a free version of Gridware for Solaris and Linux, and renamed the product Sun Grid Engine.|$|E
50|$|Device nodes are {{physical}} {{computing resources}} with processing memory {{and services to}} execute software, such as typical computers or mobile phones. An execution environment node (EEN) is a software <b>computing</b> <b>resource</b> that runs within an outer node and which itself provides a service to host and execute other executable software elements.|$|E
5000|$|Volunteer {{computing}} {{is a type}} of distributed computing,"an {{arrangement in}} which people, so-called volunteers, provide <b>computing</b> <b>resources</b> to projects, which use the resources to do distributed computing and/or storage". Thus, computer owners or users donate their <b>computing</b> <b>resources</b> (such as processing power and storage) to one or more [...] "projects".|$|R
30|$|CLUSTER: {{a set of}} <b>COMPUTE</b> <b>resources</b> being {{subject of}} auto-scaling.|$|R
30|$|To {{summarize}} it, Listing 2 {{contains a}} list of resource types {{that need to be}} provisioned (two <b>compute</b> <b>resources</b> and one storage). For the <b>compute</b> <b>resources,</b> information related to the vendor, sla, vm image, deployment information, security information are specified. Storage resource has only information related to the vendor and sla defined.|$|R
50|$|A {{problem which}} arises is that geo warping all {{measured}} radar video pixels {{is far too}} <b>computing</b> <b>resource</b> consuming as to be performed in real time. A possible solution is to use lookup tables for all points on the screen, but the lookup table re-computation after e.g. a display zoom operation still causes a noticeable delay for radar video visualization.|$|E
50|$|In <b>computing,</b> <b>Resource</b> Directory Description Language (RDDL) is an {{extension}} of XHTML Basic 1.0. An RDDL document, called a Resource Directory, provides a package of information about some target. The targets which RDDL was designed to describe are XML namespaces. The specification for RDDL has no official standing and has not been considered nor approved by any organization (e.g., W3C).|$|E
50|$|In 1970, the NBRF {{began its}} {{affiliation}} with the Georgetown University Medical Center. The university, which had allocated space for a biomedical computing facility {{that had never been}} built, provided office and laboratory space for the NBRF, while the NBRF would serve as a <b>computing</b> <b>resource</b> for the university as well as bring funding and prestige to the university through its research and development activities.|$|E
40|$|This {{viewgraph}} presentation {{provides information}} on NASA's geographically dispersed <b>computing</b> <b>resources,</b> and the various methods by which the disparate technologies are integrated within a nationwide computational grid. Many large-scale science and engineering projects are accomplished through the interaction of people, heterogeneous <b>computing</b> <b>resources,</b> information systems and instruments at different locations. The overall goal is to facilitate the routine interactions of these resources to reduce the time spent in design cycles, particularly for NASA's mission critical projects. The IPG (Information Power Grid) seeks to implement NASA's diverse <b>computing</b> <b>resources</b> in a fashion {{similar to the way}} in which electric power is made available...|$|R
5000|$|... #Caption: The carrier cloud synchronizes {{delivery}} of network and <b>compute</b> <b>resources</b> ...|$|R
5000|$|Database Resource Manager (DRM), which {{controls}} {{the use of}} <b>computing</b> <b>resources.</b>|$|R
50|$|From 1978 to 1981, the Rush Building was renovated for sole use by {{the library}} and {{information}} science programs. The project cost $2.4 million, $1 million of which came from a Pew Charitable Trusts grant. 2006 saw the renovation of the lobby and expansion of the Alumni Garden. In 2008, the <b>Computing</b> <b>Resource</b> Center (computer lab for use exclusively by IST students) was remodeled and renamed the iCommons.|$|E
50|$|The Holland Computing Center, often {{abbreviated}} to the HCC, {{is a high}} performance <b>computing</b> <b>resource</b> for the University of Nebraska System. The HCC has locations in both the University of Nebraska-Lincoln June and Paul Schorr III Center for Computer Science & Engineering and the University of Nebraska Omaha Peter Kiewit Institute.The center was named after Omaha businessman Richard Holland who donated considerably to the university for the project.|$|E
50|$|However, {{to realize}} the {{anticipated}} benefits of virtualization, network equipment vendors are improving IT virtualization technology to incorporate carrier-grade attributes required to achieve high availability, scalability, performance, and effective network management capabilities. To minimize {{the total cost of}} ownership (TCO), carrier-grade features must be implemented as efficiently as possible. This requires that NFV solutions make efficient use of redundant resources to achieve five-nines availability (99.999%), and of <b>computing</b> <b>resource</b> without compromising performance predictability.|$|E
50|$|A message {{queueing}} service {{also creates}} new value by providing reduced costs, enhanced performance and reliability. In {{order to provide}} those benefits, a message queueing service leverages cloud <b>computing</b> <b>resources</b> such as storage, network, memory and processing capacity. By using virtually unlimited cloud <b>computing</b> <b>resources,</b> a message queueing service provides an internet scale messaging platform.|$|R
5000|$|SRMs {{are used}} by TJNAF to provide the CLAS and Lattice QCD collaborations with remote access to the JASMine mass storage system. Such access has allowed {{researchers}} to utilize <b>computing</b> <b>resources</b> at universities and other collaborating institutions to process and analyze data weeks or months sooner than if done using only TJNAF <b>computing</b> <b>resources.</b>|$|R
40|$|Abstract: Applications using Grid {{computing}} infrastructure usually require resources allocation {{to satisfy}} their Quality of Service (QoS) requirements. Given that the Grid infrastructure {{is a set of}} <b>computing</b> <b>resources</b> geographically distributed, the support of Grid applications requires the allocation of <b>computing</b> <b>resources</b> and bandwidth to enable communication among these resources. The objective is to accommodate as many applications as possible while still satisfying their requirements. Ideally, we would like to accommodate a given Grid application using a set of <b>computing</b> <b>resources</b> (e. g., one server) that are not geographically distributed (e. g., in the same LAN); however, this is not always possible. Indeed, to increase the probability of accommodating Grid applications, we may need to use <b>computing</b> <b>resources</b> scattered all over the network; in this case, bandwidth allocation is required to enable communication among these resources. In this paper, we propose an optimization model that enables the “simultaneous ” allocation of <b>computing</b> <b>resources</b> and bandwidth for Grid application while maximizing the number of Grid applications being accommodated. A heuristic is proposed to solve the model with an acceptable response time; simulations show that the proposed approach outperforms existing classical approaches...|$|R
