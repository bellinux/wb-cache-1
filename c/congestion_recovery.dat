19|32|Public
40|$|This paper {{presents}} a new robust TCP congestion-recovery scheme to (1) handle bursty packet losses while preserving the self-clocking capability; (2) detect a TCP connection's new equilibrium during <b>congestion</b> <b>recovery,</b> thus improving both link utilization and e#ective throughput; and (3) make the TCP behavior during <b>congestion</b> <b>recovery</b> {{very close to}} that during congestion avoidance, thus "extending" the performance model for congestion avoidance to that for TCP loss recovery. Furthermore, the new recovery scheme requires only a slight modification to the sender side of TCP implementation, thus making it widely deployable. Th...|$|E
40|$|Gateways in {{very high}} speed internets {{will need to}} have low {{processing}} requirements and rapid responses to congestion. This has prompted {{a study of the}} performance of the Random Drop algorithm for <b>congestion</b> <b>recovery.</b> It was measured in experiments involving locul and long distance traffic using multiple gateways. For the most part, Random Drop did rwt improve the <b>congestion</b> <b>recovery</b> behavior of the gateways A surprising result was that its performance was worse in a topology with a single gateway bottleneck than in those with multiple bottlenecks. The experiments also showed that local trafic is affected by events at distant gateways. 1...|$|E
40|$|This paper {{presents}} a new robust TCP congestion-recovery scheme to (1) handle bursty packet losses while preserving the self-clocking capability; (2) detect a TCP connection’s new equilibrium during <b>congestion</b> <b>recovery,</b> thus improving both link utilization and effective throughput; and (3) make the TCP behavior during <b>congestion</b> <b>recovery</b> {{very close to}} that during congestion avoidance, thus “extending ” the performance model for congestion avoidance to that for TCP loss recovery. Moreover, the new recovery scheme requires only a slight modification to the sender side of TCP implementation, thus making it widely deployable. The performance of the proposed scheme is evaluated for scenarios with many TCP flows under the drop-tail and RED gateways {{in the presence of}} bursty packet losses. The evaluation results show that the new scheme achieves at least as much performance improvements as TCP SACK and consistently outperforms TCP New-Reno. Furthermore, its steady-state TCP behavior is close to the ideal TCP congestion behavior. Since the proposed scheme does not require selective acknowledgments nor receiver modifications, its implementation is simpler than SACK, and only the servers in the Internet need to be modified while keeping intact millions of clients scattered in the Internet...|$|E
30|$|Otherwise, if θ[*]<[*] 1, TDACKs are {{considered}} {{caused by a}} <b>congestion,</b> and fast <b>recovery</b> action is performed, by halving the congestion window and setting the slow start threshold to this new value. Fast recovery is also performed if θ[*]=[*] 1 and Jr[*]>[*]Th.|$|R
40|$|A {{recent study}} {{reported}} that the Macroscopic Fundamental Diagram of a medium size city exhibited a clockwise hysteresis loop on a day in which a major disturbance caused many drivers to switch to unfamiliar routes. This paper shows that clockwise loops {{are to be expected}} when there are disturbances, especially if the disturbances cause a significant fraction of the drivers to not change routes adaptively. It is shown that when drivers are not adaptive networks are inherently more unstable as they recover from congestion than as they are loaded. In other words, during <b>recovery</b> <b>congestion</b> tends more strongly toward unevenness because very congested areas clear more slowly than less congested areas. Since it is known that uneven congestion distributions reduce network flows, it follows that lower network flows should arise during recovery, resulting in clockwise loops. Fortunately, in sufficient numbers, drivers that choose routes adaptively to avoid congested areas help to even out <b>congestion</b> during <b>recovery,</b> increasing flow. Thus, clockwise loops are less likely to occur when driver adaptivity is high...|$|R
50|$|Connect:Direct {{supports}} the patented FASP™ transfer technology protocol originally developed by Aspera Software (an IBM company as of 2014). FASP (Fast And Secure Protocol) adds proprietary network <b>congestion</b> and packet <b>recovery</b> logic {{on top of}} UDP, which results in extremely fast transfers that do not interfere with TCP/IP network traffic.|$|R
40|$|This {{research}} investigated {{strategies for}} motorway congestion management {{from a different}} angle: that is, how to quickly recover motorway from congestion {{at the end of}} peak hours, given congestion cannot be eliminated due to excessive demand during the long peak hours nowadays. The project developed a zone recovery strategy using ramp metering for rapid <b>congestion</b> <b>recovery,</b> and a serious of traffic simulation investigations were included to evaluate the developed strategy. The results, from both microscopic and macroscopic simulation, demonstrated the effectiveness of the zone recovery strategy...|$|E
40|$|Active Networks give users {{the ability}} to program the network. We propose a {{management}} architecture that takes advantage of this ability combined with the potentials of Policy-Based Routing. It allows active applications to manipulate the routing configuration (e. g. routing table entries) {{of a group of}} active nodes communicating with each other using an overlay. These nodes may observe and constantly monitor the quality of Internet paths between them, and pick the path that has the best quality. We demonstrate the potential benefits of this architecture by deploying a <b>congestion</b> <b>recovery</b> application, able to detect and recover from congestion by routing the packets via an alternative path...|$|E
40|$|The paper {{describes}} fundamental {{features of}} RPR (Resilient Packet Ring - IEEE 802. 17 standard). It focuses on proposals {{how to improve}} fairness mechanism and to increase network efficiency in state of <b>congestion.</b> <b>Recovery</b> mechanisms are also discussed, with presented analytical and simulation results. The goals of paper are threefold. At first, we show RPR main features and describe its current status. Secondly, we present main recovery and resilience features of RPR and propose solutions for improving both fairness and congestion control. Finally, a new concept, the enhanced hold-off timer (EHOT) is introduced improving recovery actions in multilayer networks. Some simulation results are presented in order to illustrate advantages of proposed solution. Postprint (published version...|$|E
40|$|The special local {{features}} of the endothelio- and myoarchitectonics in the thoracic duct (TD) angiogionas of the dog have been studied firstly, and the general model of their construction has been represented; {{the presence of the}} "valval and muscular spirals" in their wall has been discovered, and their mathematical model has been represented. The transformations of the TD lymphangionas at lymph <b>congestion</b> and <b>recovery</b> operations on it have been studied firstly. The technique of the sutural-qlue procedures for recovery of the vessel integrity at its damage has been developed firstly at operations on the lymphatic vessels. The work results are used in the educational process. Available from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Ma onica accep line 8 witho contr g pri nt a {{input rate}} to the {{available}} bandwidth (Jacobson, 1988; Mascolo, 1999, 2000; Allman, Paxson & Stevens 1999; aims at avoiding the flooding {{of the network}} {{and is based on}} implicit feedback such as timeouts, duplicate congestion (i. e. 3 DUPACKs) by halving the <b>congestion</b> window (fast <b>recovery)</b> and sending again the missing packet (fast retransmit), and to heavy congestio...|$|R
30|$|Simulating the {{congestion}} {{propagation of}} urban rail transit system is challenging, especially under oversaturated conditions. This paper presents a congestion propagation model based on SIR (susceptible, infected, recovered) epidemic model for capturing the congestion prorogation process through formalizing the propagation by a <b>congestion</b> susceptibility <b>recovery</b> process. In addition, as congestion propagation {{is the key}} parameter in the congestion propagation model, a model for calculating congestion propagation rate is constructed. A gray system model is also introduced to quantify the propagation rate under the joint effect of six influential factors: passenger flow, train headway, passenger transfer convenience, time of congestion occurring, initial congested station and station capacity. A numerical example is used to illustrate the congestion propagation process and to demonstrate the improvements after taking corresponding measures.|$|R
40|$|Abstract — The {{innovative}} {{techniques of}} TCP Vegas {{have been the}} subject of much debate in recent years. Several studies have reported that TCP Vegas provides better performance than TCP Reno. However, the question which of the new techniques are responsible for the impressive performance gains remains unanswered so far. This paper presents a detailed performance evaluation of TCP Vegas. By decomposing TCP Vegas into the various novel mechanisms proposed and assessing the effect of each of these mechanisms on performance, we show that the reported performance gains are achieved primarily by TCP Vegas’s new techniques for slow-start and <b>congestion</b> <b>recovery.</b> TCP Vegas’s innovative congestion avoidance mechanism is shown to have only a minor influence on throughput. Furthermore, we find that the congestion avoidance mechanism exhibits fairness problems even if all competing connections operate with the same round trip time...|$|E
40|$|An {{extensive}} set of {{measurements were}} made in an OSI testbed to study the behavior of congestion control and avoidance. Testbed systems used the Connectionless Network Protocol (CLNP) and Transport Protocol Class 4 (TP 4), which had been modified to perform the CE-bit [10] congestion avoidance and the "CUTE" [6] <b>congestion</b> <b>recovery</b> algorithms. We found that two-way traffic has dynamics which can significantly decrease fairness among competing connections using congestion avoidance. We present experiments that demonstrate this problem and our analysis of how twoway traffic results in reduced fairness. This analysis led us to develop an effective modification to the congestion avoidance algorithms to maintain fairness with two-way traffic. Our analysis of experimental results also points to undesirable interactions between two-way traffic dynamics and a sending strategy that times data transmissions by the receipt of acknowledgements. These interactions reinforce burstiness of transmiss [...] ...|$|E
40|$|This paper {{presents}} a congestion control protocol for ad-hoc Wireless-LAN (WLAN) with Bandwidth-on Demand (BoD) access. The novelty {{of this paper}} is in the extensive use of model-based control methodologies to simultaneously compute the capacity requests necessary to access the network (BoD) and the capacity allocations required to regulate the rates of the traffic flows (congestion control). The proposed scheme allows one to compute upper-bounds of the queue lengths in all the network buffers (thus allowing proper buffer dimensioning and, therefore, overflow prevention), avoids that the assigned capacity is left unused (thus entailing full link utilization) and guarantees the recovery of a satisfactory traffic behaviour as soon as congestion situations terminate (<b>congestion</b> <b>recovery).</b> The high-speed WLAN considered in the paper has been developed within the European Union (EU) project Wireless Indoor Flexible High Bitrate Modern Architecture (WINDFLEX). Extensive simulations prove the effectiveness of the proposed scheme...|$|E
40|$|Rapid {{technological}} advances and innovations {{in the area of}} autonomous systems push the researchers towards autonomous networked systems with emphasis on Wireless Sensor Networks (WSNs). In WSN event-driven applications, it is critical to report the detected events in the area, resulting in sudden bursts of traffic due to occurrence of spatially-correlated or multiple events, causing loss of data. Also, nodes have very limited power due to hardware constraints. Packet losses and retransmissions resulting from congestion, cost precious energy and shorten the lifetime of sensor nodes. Till now, in WSNs, Congestion control techniques are based on detection of <b>congestion</b> and <b>recovery,</b> but they cannot eliminate or prevent the occurrence of congestion. Collision is a symptom of congestion in the wireless channel and can result in a timevariant channel capacity. Therefore, this research focuses on an efficient medium access control (MAC) technique to coordinate the access of nodes to the shared medium without interference. It uses the queue buffer length of the sensor nodes to estimate the congestion and then dynamically disseminates the traffic along with classifying them into different priority classes to provide a congestion-free routing path to the destination with improved Quality of Service (QoS) Â© 2013 IEEE...|$|R
40|$|This work uses {{regression}} models to analyze two characteristics of recurrent congestion: breakdown, {{the transition from}} freely flowing conditions to a congested state, and duration, the time between the onset and clearance of recurrent congestion. First, we apply a binary logistic regression model where a continuous measurement for traffic flow and a dichotomous categorical variable for time-of-day (AM- or PM-rush hours) is used to predict the probability of breakdown. Second, we apply an ordinary least squares regression model where categorical variables for time-of-day (AM- or PM-rush hours) and day-of-the-week (Monday–Thursday or Friday) are used to predict recurrent congestion duration. Models are fitted to data collected from a bottleneck on I- 93 in Salem, NH, {{over a period of}} 9 months. Results from the breakdown model, predict probabilities of recurrent congestion, are consistent with observed traffic and illustrate an upshift in breakdown probabilities between the AM- and PM-rush periods. Results from the regression model for congestion duration reveal the presences of significant interaction between time-of-day and day-of-the-week. Thus, the effect of time-of-day on congestion duration depends on the day-of-the-week. This work provides a simplification of recurrent <b>congestion</b> and <b>recovery,</b> very noisy processes. Simplification, conveying complex relationships with simple statistical summaries-facts, is a practical and powerful tool for traffic administrators to use in the decision-making process...|$|R
40|$|Carrier Ethernet is an {{important}} step forward that combines cost advantages of well known Ethernet technology with the needs of network providers: it extends Ethernet capabilities from the usual LAN domain to MAN and backbones. To accomplish this, scalability, traffic engineering and network management issues have been addressed by IEEE, ITU-T and MEF standardizing organizations, resulting in a series of new standards. The new Operation, Administration and Maintenance (OAM) functions play a key role to monitor the network status and performance, as they provide the foundations for load balancing on paths, <b>congestion</b> control, fault <b>recovery</b> and challenges a provider has to face. In this thesis a graphical interactive performance monitor for such a network is designed and implemented, with a special focus on point-to-multipoint topologies. Compared to classical command line interfaces, it enables a faster acknowledgement of problems and gives multiple parallel views of the network, from global to local...|$|R
40|$|The {{innovative}} {{techniques of}} TCP Vegas {{have been the}} subject of much debate in recent years. Several studies have reported that TCP Vegas provides better performance than TCP Reno. However, the question which of the new techniques are responsible for the impressive performance gains remains unanswered so far. This paper presents a detailed performance evaluation of TCP Vegas. By decomposing TCP Vegas into the various novel mechanisms proposed and assessing the effect of each of these mechanisms on performance, we show that the reported performance gains are achieved primarily by TCP Vegas's new techniques for slow-start and <b>congestion</b> <b>recovery.</b> TCP Vegas's innovative congestion avoidance mechanism is shown to have only a minor influence on throughput. Furthermore, we find that the congestion avoidance mechanism exhibits fairness problems even if all competing connections operate with the same round trip time. Keywords [...] - TCP Vegas, Congestion control, Transport protocols. I. INTRODUC [...] ...|$|E
40|$|Recurrent {{congestion}} {{caused by}} high commuter traffic is an irritation to motorway users. Ramp metering (RM) {{is the most}} effective motorway control means (M Papageorgiou & Kotsialos, 2002) for significantly reducing motorway congestion. However, given field constraints (e. g. limited ramp space and maximum ramp waiting time), RM cannot eliminate recurrent congestion during the increased long peak hours. This paper, therefore, focuses on rapid <b>congestion</b> <b>recovery</b> to further improve RM systems: that is, to quickly clear congestion in recovery periods. The feasibility of using RM for recovery is analyzed, and a zone recovery strategy (ZRS) for RM is proposed. Note that this study assumes no incident and demand management involved, i. e. no re-routing behavior and strategy considered. This strategy is modeled, calibrated and tested in the northbound model of the Pacific Motorway, Brisbane, Australia in a micro-simulation environment for recurrent congestion scenario, and evaluation results have justified its effectiveness...|$|E
30|$|When a {{duplicate}} ACK is received, {{the sender}} checks FirstUnsacked and HighSACK {{to investigate whether}} it is due to congestion. If FirstUnsacked is equal to HighACK, {{it means that the}} FirstUnsacked segment was reordered or dropped due to congestion (we assume that an ACK acknowledges the sequence number or the first byte of the expected data). (HighACK is defined as the sequence number of the highest byte of data that has been cumulatively ACKed at a given point in [34].) Otherwise, if FirstUnsacked is less than HighSACK, it means that the FirstUnsacked segment was reordered or dropped due to congestion while waiting for a new ACK for the segment retransmitted by an ELN. If the segment of HighACK is already SACKed, it means that the segment was dropped in the receiver queue. Therefore, the segment needs to be transmitted again. If the counter of received duplicate ACKs becomes three, the sender triggers the <b>congestion</b> <b>recovery</b> algorithms of fast retransmit and recovery. In fast retransmit phase, the sender retransmits the FirstUnsacked segment. The other operations of ESACK are equal to SACK. Therefore, if there is no wireless loss, ESACK behaves just like SACK.|$|E
40|$|This paper {{presents}} a prototype for single-rate reliable multicast congestion control, {{which has been}} built into an existing commercial whiteboard. The prototype was developed using a novel scheme that was engineered around conflicting industry provided requirements for collaborative workspaces. This required the scheme to be both low-weight when used with many senders and compatible with NAT, firewalls and reflectors. The key to overcome this conflict was to combine <b>congestion</b> control and <b>recovery</b> feedback. This differs from many current solutions {{in that they are}} often designed for use {{with a wide variety of}} protocols and thus operate independent of the recovery mechanism. This paper does not go into the detail required to specify a protocol but instead discusses a few important design requirements for multi-sender applications, which are generally not considered by current research, and describes an approach towards meeting these requirements. Validerad; 2002; 20060918 (ysko...|$|R
40|$|Rapid {{technological}} advances and innovations {{in the area of}} autonomous systems push the researchers towards autonomous networked systems with emphasis on Wireless Multimedia Sensor Networks (WMSNs). In WMSN event-driven applications, it is critical to report the detected events in the area, resulting in sudden bursts of traffic due to occurrence of spatially-correlated or multiple events, causing loss of data. Also, nodes have very limited power due to hardware constraints. Packet losses and retransmissions resulting from congestion, cost precious energy and shorten the lifetime of sensor nodes. Till now, in WMSNs, Congestion control techniques are based on detection of <b>congestion</b> and <b>recovery,</b> but they cannot eliminate or prevent the occurrence of congestion. Collision is a symptom of congestion in the wireless channel and can result in a time-variant channel capacity. The method in the proposed algorithm is that the routing algorithms do not precalculate the routes and the next step is chosen dynamically. Decisions are made based on the congestion degree on neighbor nodes; each node sees its own queue backlog and neighbor's queue backlog and chooses its own degree and route based on the queue backlogs obtained from its neighbors. If there is two or more data with the same condition in the backpressure routing, we use service differentiation to prioritize packets. The results obtained from simulation test done by NS- 2 simulator indicate that the proposed model is more innovative and presents better performance in compare with CCF and PCCP protocols...|$|R
40|$|Abstract — Advancements in technologies, {{semiconductor}} industry and communications resulted in construction of wireless sensors. Wireless sensors are self-managing and sensing devices having {{the capability to}} communicate with each other and sink node wirelessly. The sink node can then take decision according to the application they are being used for; upon the reception of messages from node. Generally, transport control protocols provides reliable data transfer with <b>congestion</b> control and <b>recovery</b> support. This support is very important in wireless networks where data is more likely to be lost than wired network. In wireless sensor networks (WSN), there are limited resources like energy, memory, and signal strength. Therefore in WSN, QoS is very important in order to reduce the impact of data loss and data reliability. This paper provides a comparative study on STCP and RMST transport layer protocol in order to study their effect on QoS. We used throughput, packet drop rate, delay and scaling in sensor nodes and source nodes is studied. Index Terms—QoS, STCP, RMST I...|$|R
40|$|Previous {{analytic}} {{models of}} TCP Vegas throughput {{have been developed}} for loss-free (all-Vegas) networks. This work develops a simple and accurate analytic model for the throughput of a TCP Vegas bulk transfer in the presence of packet loss, as a function of average round trip time, minimum round trip time, and loss rate for the transfer. Similar models have previously been developed for TCP Reno. However, several aspects of TCP Vegas need to be treated differently than their counterparts in Reno. The proposed model captures the key innovative mechanisms that Vegas employs during slow start, congestion avoidance, and <b>congestion</b> <b>recovery.</b> The results include (1) a simple, validated model of TCP Vegas throughput {{that can be used for}} equation-based rate control of other flows such as UDP streams, (2) a simple formula to determine, from the measured packet loss rate, whether the network buffers are overcommitted and thus the TCP Vegas flow cannot reach the specified target lower threshold on throughput, (3) new insights into the design and performance of TCP Vegas, and (4) comparisons between TCP Vegas and TCP Reno including new insights regarding incremental deployment of TCP Vegas...|$|E
40|$|In {{spite of}} the larger {{performance}} gain such as higher throughput and almost zero packet retransmissions compared to TCP Reno, TCP Vegas still has a few obstacles {{for it to be}} deployed in the Internet. Studies have shown unfair treatment to Vegas connections when they compete with Reno connections. Other issues identified with TCP Vegas are problems of rerouting, persistent congestion, and discrepancy in flow rate tied with starting times and link bandwidth. We reinvestigate these issues and propose modifications to the congestion avoidance mechanism of the TCP Vegas, with the slow-start and <b>congestion</b> <b>recovery</b> algorithms of Vegas remaining untouched. Unlike the solutions proposed in the recent past to deal with some of these issues, our solution is neither dependent on any critical parameter values nor on the buffer management scheme at the routers (e. g., RED). Our experiments show that the modified TCP Vegas (Vegas-A) is able to obtain a fairer share of the network bandwidth when competing with other TCP flows. We also show that Vegas-A can tackle rerouting issues and rectify Vegas’s bias against higher bandwidth flows. At the same time, our experiments prove that Vegas-A preserves the properties of Vegas that have made it a noteworthy protocol. I...|$|E
40|$|This paper proposes an {{algorithm}} {{to identify}} TCP spurious retransmission timeouts by post processing of packet traces monitored in operational networks. The operational {{principles of the}} algorithm and the assumptions behind its design are explained in detail {{as well as the}} situations in which the algorithm is prone to inaccuracies. By extensive measurements in a lab testbed using realistic round trip time characteristics as observed in operational wireless networks and FTP-like as well as Web-like traffic generators, it is shown that the algorithm is accurate in detection of spurious retransmission timeouts. Subsequently, the algorithm is applied to real traffic traces captured at several interfaces of an operational UMTS and GPRS network to analyze the frequency of spurious retransmission timeouts as well as the spurious timeout probability dependent on the load situation in the network and the flow size. This investigation, to our best knowledge the first on large scale TCP traffic traces monitored in an operational UMTS network, shows that spurious timeouts are infrequent events in the considered UMTS as well as the GPRS network. Among other findings, it is additionally shown that the ratio between spurious timeouts and other <b>congestion</b> <b>recovery</b> events experienced by TCP flows is low, indicating a negligible impact of spurious timeouts on TCP performance...|$|E
40|$|In recent years, {{there is}} growing {{literature}} concerning the cascading failure of network characteristics. The object {{of this paper is}} to investigate the cascade failures on road traffic network, considering the aeolotropism of road traffic network topology and road congestion dissipation in traffic flow. An improved coupled map lattice (CML) model is proposed. Furthermore, in order to match the <b>congestion</b> dissipation, a <b>recovery</b> mechanism is put forward in this paper. With a real urban road traffic network in Beijing, the cascading failures are tested using different attack strategies, coupling strengths, external perturbations, and attacked road segment numbers. The impacts of different aspects on road traffic network are evaluated based on the simulation results. The findings confirmed the important roles that these characteristics played in the cascading failure propagation and dissipation on road traffic network. We hope these findings are helpful to find out the optimal road network topology and avoid cascading failure on road network...|$|R
40|$|To {{interconnect}} {{a wireless}} sensor network (WSN) to the Internet, we propose to use TCP/IP as the standard protocol for all network entities. We present a cross layer designed communication architecture, which contains a MAC protocol, IP, a new protocol called Hop-to-Hop Reliability (H 2 HR) protocol, and the TCP Support for Sensor Nodes (TSS) protocol. The MAC protocol implements the MAC layer of beacon-less personal area networks (PANs) as defined in IEEE 802. 15. 4. H 2 HR implements hop-to-hop reliability mechanisms. Two acknowledgment mechanisms, explicit and implicit ACK are supported. TSS optimizes using TCP in WSNs by implementing local retransmission of TCP data packets, local TCP ACK regeneration, aggressive TCP ACK <b>recovery,</b> <b>congestion</b> and flow control algorithms. We show that H 2 HR increases the performance of UDP, TCP, and RMST in WSNs significantly. The throughput is increased and the packet loss ratio is decreased. As a result, WSNs can be operated and managed using TCP/IP...|$|R
40|$|The {{efficacy}} of end-to-end multicast transport protocols depends critically upon {{their ability to}} scale efficiently to {{a large number of}} receivers. Several research multicast protocols attempt to achieve this high scalability by identifying sets of co-located receivers in order to enhance loss <b>recovery,</b> <b>congestion</b> control and so forth. A number of these schemes could be enhanced and simplified by some level of explicit knowledge of the topology of the multicast distribution tree, the value of the bottleneck bandwidth along the path between the source and each individual receiver and the approximate location of the bottlenecks in the tree. In this paper, we explore the problem of inferring the internal structure of a multicast distribution tree using only observations made at the end hosts. By noting correlations of loss patterns across the receiver set and by measuring how the network perturbs the fine-grained timing structure of the packets sent from the source, we can determine both [...] ...|$|R
30|$|A {{satisfactory}} {{level of}} QoS particularly {{with respect to}} wireless links is highly dependent on traffic control strategies. The IP has traditionally been successfully used with best effort services. However, with the emerging multimedia communication, using IP as the network protocol traffic differentiation is required when using different QoS classes. This is particularly the case in order that real-time traffic can be given a higher precedence over nonreal time traffic. It is highly probable that the problem will be exacerbated by the fact that, in general, the various wireless infrastructures adopt different mechanisms in order to achieve QoS. The phenomenon of congestion is clearly one of the detrimental factors to the QoS as congestion has a negative influence on the delay (and hence the appearance of jitter), and also on the reliability. During the past number of years, there have been a variety of mechanisms developed to deal with congestion both in end hosts and at the network level (router based) [1 – 3]. However, in heterogeneous networks the assumption that a packet loss always indicates an appearance of congestion might be flawed. This is simply because (1) a wireless link has a much higher bit-error rate and (2) the Internet connection might be temporarily down due to handover. This may result in an unnecessary invocation of some <b>congestion</b> <b>recovery</b> algorithms and this will have an immediate effect on network performance such as a low link utilization and unsatisfactory transport protocol performance. Consequently, there has been a certain reluctance to deploy end-to-end congestion control mechanisms for networked multimedia applications in transport protocols.|$|E
40|$|In Optical Burst Switched networks, each {{light path}} carry {{huge amount of}} traffic, path {{failures}} may damage the user application. Hence fault-tolerance becomes an important issue on these networks. Blocking probability is a key index of quality of service in Optical Burst Switched (OBS) network. The Erlang formula has been used extensively in the traffic engineering of optical communication to calculate the blocking probability. A combined preventive/reactive control scheme improves the condition of packet loss due to congestion in networks. The transmission delay and the throttling rate are the major parameters which affect {{the performance of the}} reactive control. High throttling rates are most efficient for fast <b>congestion</b> <b>recovery,</b> although sometimes resulting in underutilization of the link. A combined reactive/preventive congestion control mechanism is investigated in this paper with emphasis on the Leaky Bucket (LB) mechanism chosen for source traffic policing in computer networks. The fluid-flow model is used to analyze the performance of both buffered and un-buffered LBs. It is proposed that one LB is not sufficient to manage all the source traffic parameters. If tight control, fast reaction time and a small queuing delay are required then according to the analysis done, the proposed triple LB mechanism is an effective solution. According to the delayed congestion feedback information received from the network the LB parameters are dynamically changed. The preventive control policy is compared with the adaptive control scheme. The results show that even for large propagation delays, major performance improvements are possible by using an appropriate feedback policy...|$|E
30|$|This paper {{presents}} a model based on SIR epidemic model that comprises a congestion propagation model and a propagation rate calculation method {{to utilize the}} propagation theories of the oversaturated conditions in rail transit. The application of the congestion propagation model aims to simulate the propagation process of the whole network under oversaturated conditions. In this paper, a new approach consisting of a separate method that calculates the congestion propagation rate is studied and presented. The congestion propagation rate model is used to generate measures to solve the oversaturated condition, improve congestion and finally enable us to analyze the incidents for future reference. The two models establish a solid foundation {{for the study of}} large-scale network congestion propagation. The influential factors of the propagation rate can be classified into six classes: passenger flow characteristic, train departure interval, passenger transfer convenience, the time of congestion occurring, the initial congested station and station capacity. Nevertheless, there are some other influential factors for these models. Hence, further research on other influential factors is suggested to extend the models. The congestion propagation model provides an extensive analysis of the propagation process related to congestion. Moreover, the model depicts the forecasting and trends of congestion so that traffic controllers obtain a quantitative observation of the process. The congestion propagation rate model distinguishes the efficiency of the alternatives related to congestion improvement measures. As a result, traffic controllers evaluate the outcome and subsequently select the optimal measure to resolve the oversaturated conditions. The expansion of urban transit networks magnifies the propagation of congestion and oversaturated conditions. The goal of the comprehensive quantitative model is to integrate congestion propagation analysis with efficiency differentiation of propagation rate to promote effective <b>congestion</b> <b>recovery</b> measures. The paper hopes to initialize a new approach in the quantitative model of the congestion propagation through the utilization of the two models and integrate the two results for the development of urban transit. Thus, the models proposed in this paper optimize the recovery measures and more importantly simulate the current circumstances and forecast the propagation trends in urban transit systems.|$|E
40|$|Abstract In {{wireless}} multimedi{{a sensor}} networks (WMSNs) a sensor node {{may have different}} types of sensor which gather different kinds of data. To support quality of service (QoS) requirements for multimedia applications having a reliable and fair transport protocol is necessary. One of the main objectives of the transport layer in WMSNs is congestion control. We observe that the information provided may have different levels of importance and argue that sensor networks {{should be willing to}} spend more resources in disseminating packets carrying more important information. In WMSN event-driven applications, it is critical to report the detected events in the area, resulting in sudden bursts of traffic due to occurrence of spatially-correlated or multiple events, causing loss of data. Also, nodes have very limited power due to hardware constraints. Packet losses and retransmissions resulting from congestion, cost precious energy and shorten the lifetime of sensor nodes. Till now, in WMSNs, Congestion control techniques are based on detection of <b>congestion</b> and <b>recovery,</b> but they cannot eliminate or prevent the occurrence of congestion. Collision is a symptom of congestion in the wireless channel and can result in a time-variant channel capacity. The main intention of this protocol is {{to be used as a}} mechanism for reducing congestion in the network by free resources to set accurate rates and priority data needs. If two nodes send their packets in the shortest path to the parent node in a crowded place, a source node must prioritize the data and uses data that have lower priorities of a suitable detour nodes consisting of low or non- active consciously. Proposed algorithm is applied to the nodes nea...|$|R
40|$|Presented in {{this paper}} is the {{solution}} to the problem that arises when the TCP/IP protocol suite is used to provide Internet connectivity through mobile terminals over emerging 802. 11 wireless links. Taking into consideration the strong drive towards wireless Internet access through mobile terminals, the problem of frequent disconnections causing serial timeouts is examined and analyzed, with the help of extensive simulations. After a detailed review of wireless link loss recovery mechanism and identification of related problems, a new scheme with modifications at link layer and transport layer is proposed. The proposed modifications which depend on interaction between two layers (i) reduce the idle time before transmission at TCP by preventing timeout occurrences and (ii) decouple the <b>congestion</b> control from <b>recovery</b> of the losses due to link failure. Results of simulation based experiments demonstrate considerable performance improvement with the proposed modifications over the conventional TCP, when a wireless sender is experiencing frequent link failures. Comment: 16 Pages, 11 Figures; International Journal of Computer Networks & Communications (IJCNC) Vol. 3, No. 6, November 201...|$|R
40|$|Abstract — The {{efficacy}} of end-to-end multicast transport protocols depends critically upon {{their ability to}} scale efficiently to {{a large number of}} receivers. Several research multicast protocols attempt to achieve this high scalability by identifying sets of co-located receivers in order to enhance loss <b>recovery,</b> <b>congestion</b> control and so forth. A number of these schemes could be enhanced and simplified by some level of explicit knowledge of the topology of the multicast distribution tree, the value of the bottleneck bandwidth along the path between the source and each individual receiver and the approximate location of the bottlenecks in the tree. In this paper, we explore the problem of inferring the internal structure of a multicast distribution tree using only observations made at the end hosts. By noting correlations of loss patterns across the receiver set and by measuring how the network perturbs the fine-grained timing structure of the packets sent from the source, we can determine both the underlying multicast tree structure as well as the bottleneck bandwidths. Our simulations show that the algorithm is robust and appears to converge to the correct tree with high probability. I...|$|R
