14|339|Public
50|$|In late 1983 {{the funding}} cost for {{continued}} development of Oric caused external funding to be sought, and {{eventually led to}} a sale to Edenspring Investments PLC. The Edenspring money enabled Oric International to release the Oric Atmos, which added a true keyboard and an updated V1.1 ROM to the Oric-1. The faulty tape error <b>checking</b> <b>routine</b> was still there.|$|E
5000|$|The {{original}} BinHex was {{a fairly}} simple format, one that was not very efficient because it expanded every byte of input into two, {{as required by the}} hexadecimal representation—an 8-to-4 bit encoding. For BinHex 2.0, Lempereur used a new 8-to-6 encoding that improved file size by 50% and took the opportunity to add a new CRC error <b>checking</b> <b>routine</b> in place of the earlier checksum. This new encoding used the first 64 ASCII printing characters, including the space, to represent the data, similarly to uuencode. Even though the new encoding was no longer hexadecimal in nature, the established name of the program was retained. The smaller files were incompatible with the older ones, so the extension became [...]hcx, c for compact.|$|E
3000|$|Therefore, one {{question}} that arises is when should we stop the <b>checking</b> <b>routine?</b> In[33], it is proved {{that for the}} cases where all task activations are simultaneously released at t= 0, the worst case load will occur again in multiples of H [...]...|$|E
40|$|This {{investigation}} {{proposes a}} method for increasing the quality of data from an automatic condition monitoring system for railway rolling stock wheels, in order to assure the right data quality for further use of the data. The data quality improvement is used to ensure a higher reliability of the data analysis and to propose a new <b>check</b> <b>routine</b> {{to ensure that the}} sensors generate the same data for the same sample. A case study on field data shows how the data from different measurement setups differ for three of four measurements and why this <b>check</b> <b>routine</b> is needed. The paper ends with a discussion and conclusions concerning the improvements that are presented. This paper presents {{a method for}} ensuring the quality of data from an automatic condition monitoring system for railway rolling stock wheels, in order to assure the right data quality for further use of the data. The quality assurance method presented is used to ensure a higher reliability of the data analysis and to propose a new <b>check</b> <b>routine</b> to ensure that the sensors generate the same data for the same sample. A case study on field data shows why this <b>check</b> <b>routine</b> is needed. The paper ends with a discussion and conclusions concerning the improvements that are presented. Godkänd; 2015; 20150829 (matasp...|$|R
30|$|We propose {{two basic}} {{policies}} {{to design the}} timings to re-plan. First, we activate re-plan modules only when we judge from map errors information that we cannot continue the present plan. Second, {{in order to keep}} real-time performance, we divide <b>checking</b> <b>routines</b> whether to activate the re-plan modules (with low calculation cost) and main routines of re-planning (with high calculation cost). These policies decrease the total number of applied modules, and in most cases, the calculation time may be shortened. Based on the above policies, we propose the algorithm shown in Figure  2, consisting of three <b>checking</b> <b>routines</b> and three re-plan modules.|$|R
40|$|The primarybackup with deallocation {{approach}} of {{is a strategy}} for the fault tolerant online scheduling of hard realtime tasks In this scheme tasks are either rejected {{within a short time}} after the request or guaranteed to be executed even in case of a processor failure In this paper several heuristics for the guarantee algorithm are investigated For the rst time di	erent processor selection strategies for guarantee algorithms with execution time constraints are compared In addition the concept of a decision deadline is introduced which then leads to an extension of the primary and backup <b>checking</b> <b>routines</b> The thus modied <b>checking</b> <b>routines</b> are shown to achieve a lower rejection ratio for tight task deadlines and constrained scheduler execution time...|$|R
40|$|The Very Portable Optimizer is a {{compiler}} {{tool for}} machine-independent program optimization. Optimizations are {{performed at the}} generalized Register Transfer List level so that any machine platform can be targeted. During this process, VPO utilizes a <b>checking</b> <b>routine</b> called &quot;gatekeeper &quot; to ensure that optimizations will translate to instructions for a given machin...|$|E
30|$|During {{the program}} execution, once a join point is identified, a method-like {{construct}} named advice may run, depending or not of some runtime <b>checking</b> <b>routine.</b> Advices {{can be of}} different types depending on the supporting technology. For example, in AspectJ, advices can be defined to run at three different moments when a join point is reached: before, after or around (in place of) it.|$|E
40|$|Note: {{before using}} this routine, please read the Users ’ Note for your {{implementation}} {{to check the}} interpretation of bold italicised terms and other implementation-dependent details. 1 Purpose G 13 ASF is a diagnostic <b>checking</b> <b>routine</b> suitable for use after fitting a Box–Jenkins ARMA model to a univariate time series using G 13 AEF or G 13 AFF. The residual autocorrelation function is returned along with an estimate of its asymptotic standard errors and correlations. Also, G 13 ASF calculates the Box– Ljung portmanteau statistic and its significance level for testing model adequacy...|$|E
40|$|The primary/backup with deallocation {{approach}} of [2] {{is a strategy}} for the fault-tolerant online scheduling of hard realtime tasks. In this scheme, tasks are either rejected {{within a short time}} after the request or guaranteed to be executed even in case of a processor failure. In this paper several heuristics for the guarantee algorithm are investigated. For the first time different processor selection strategies for guarantee algorithms with execution time constraints are compared. In addition, the concept of a decision deadline is introduced which then leads to an extension of the primary and backup <b>checking</b> <b>routines.</b> The thus modified <b>checking</b> <b>routines</b> are shown to achieve a lower rejection ratio for tight task deadlines and constrained scheduler execution times than the modification making use of task slack suggested in [2]. 1 Introduction Recently there has been some interest in the fault-tolerant scheduling of non-preemptive, aperiodic real-time tasks on a multiprocessor system [...] ...|$|R
30|$|The IRI UP method, {{embedding}} the F algorithm and the NQCR procedure, as described, respectively, in “Description of {{the filter}} implemented to select ionosonde data” and “On {{the choice of}} the best variogram model in the Universal Kriging procedure: a new quality <b>check</b> <b>routine</b> (NQCR)” sections, has been systematically tested over the 30 disturbed time intervals listed in Table  3, in order to investigate its performance during moderate, strong, and severe geomagnetic storms.|$|R
50|$|NOPs {{are often}} {{involved}} when cracking software that checks for serial numbers, specific hardware or software requirements, {{presence or absence}} of hardware dongles, etc. This is accomplished by altering functions and subroutines to bypass security checks and instead simply return the expected value being checked for. Because most of the instructions in the security <b>check</b> <b>routine</b> will be unused, these would be replaced with NOPs, thus removing the software's security functionality without attracting any attention.|$|R
40|$|Algorithms {{for finding}} large {{feasible}} n-dimensional intervals for constrained nonlinear optimization are presented. The n-dimensional interval is iteratively enlarged about a seed point while a <b>checking</b> <b>routine</b> maintains feasibility. Two checking routines are discussed: an interval subdivision method and a global optimization method. Both checking routines {{can be used}} in the overall methodology to generate a feasible suboptimal interval. Such an interval is useful when examining manufacturing tolerances in design optimization. Numerical results are presented for a practical application in the optimal design of a flat composite plate and a composite stiffened panel structure. Методология нахождения интервалов допуск...|$|E
40|$|This report {{describes}} the procedure and {{properties of the}} software upgrade for the Vibration Performance Recorder. The upgrade will check the 20 memory cards for proper read/write operation. The upgrade was successfully installed and uploaded into the Viper and the field laptop. The memory <b>checking</b> <b>routine</b> must run overnight to complete the test, although the laptop need only {{be connected to the}} Viper unit until the downloading routine is finished. The routine has limited ability to recognize incomplete or corrupt header and footer files. The routine requires 400 Megabytes of free hard disk space. There is one minor technical flaw detailed in the conclusion...|$|E
40|$|Detecting global predicates is an {{important}} task in testing and debugging distributed programs. In this paper, we propose an approach that effectively precludes useless events for global predicate detection, facilitating the process of an independent on-line <b>checking</b> <b>routine.</b> To identify more useless events than a simple causality-check method can do, our method tracks and maintains the precedence information of event intervals as a graph. To reduce the potentially expensive space and time cost as the graph expands, we propose an effective scheme to prune the graph. The performance of our method is analyzed and evaluated by simulations. The result shows that our approach outperforms conventional approaches {{in terms of the}} number of useless events found...|$|E
30|$|Based on a problem-partitioning structure, we {{designed}} rearrangement task realization that involve (a) design of three modules {{in consideration of}} characteristics of map errors {{and structure of the}} plan result, and (b) design of when they should be activated with the combination of <b>checking</b> <b>routines</b> and main re-planning routines. In addition to the proposition of a realization procedure, we constructed real robot systems. The proposed method and systems were tested by experiments in actual environments including transformation of obstacles. The results showed the effectiveness of our method.|$|R
30|$|In addition, a new quality <b>check</b> <b>routine</b> (NQCR), {{based on}} {{statistical}} tests carried out using the statistical variables Q 1, Q 2, and cR, {{built on the}} residuals [differences between the observed (experimental variogram) and the modeled (variogram model) semivariance values], has also been proposed {{with the intention of}} replacing the quality check based on the exponent s of the power variogram model. This allowed to select more objectively, and with a more acceptable degree of confidence, the “best” variogram model to be used.|$|R
50|$|The System/360 was {{designed}} to separate the system state from the problem state. This provided a basic level of security and recoverability from programming errors. Problem (user) programs could not modify data or program storage associated with the system state. Addressing, data, or operation exception errors made the machine enter the system state through a controlled routine so the operating system could try to correct or terminate the program in error. Similarly, it could recover certain processor hardware errors through the machine <b>check</b> <b>routines.</b>|$|R
40|$|The forward <b>checking</b> <b>routine</b> (FC) of Haralick and Elliott {{attempts}} to encourage early failures within the search tree of constraint satisfaction problems, {{leading to a}} reduction in nodes visited, which tends to result in reduced search effort. In contrast, Gaschnig's backmarking routine (BM) {{attempts to}} avoid performing redundant consistency checks. These two algorithms are combined to give us FC-BM, an algorithm that attempts to minimise the number of nodes visited, while avoiding redundant consistency checks. This algorithm is further enhanced such that it incorporates conflict-directed backjumping (CBJ) to give us FC-BM-CBJ. A series of experiments are then carried out on really hard problems in an attempt to position these new algorithms with respect to the known algorithms...|$|E
40|$|Abstract — E-cash {{algorithms}} offer strong protection on principal anonymity, transaction {{authenticity and}} quota enforcement of credit dispense {{but as of}} {{now there is no}} study on how to integrate them with the resource allocation schemes. In this paper, we propose a hypercube based tracking system for both credit dispense and resource allocation of computing assets using E-cash algorithms. Hypercube is chosen as the common references of the two major functions, so that the study results can be tailored {{for a wide range of}} data structures that could be used for both purposes. Based on the N-divisibility framework of disposable authentication, we develop the analysis techniques to assess the security properties using hypercube to track credit spending and for subcube allocation for two different schemes. The first scheme is highly secure yet has high computing costs. On the other hand, the second scheme can achieve the same security goals at much lower computing costs by relaxing anonymity protection rules but adding a simple anonymity hazard <b>checking</b> <b>routine</b> before a subcube can be spent/allocated. Anonymity hazard refers to the condition that the unique secret (commonly called the identity) assigned to a token can be deciphered from messages generated from transactions even when no double spending occurs. Simulations results show that well known subcube allocation schemes binary code and binary gray code are free of anonymity hazards because of their restrictive allocation rules. Anonymity hazards can occur to an unrestricted subcube allocation scheme if the anonymity hazard <b>checking</b> <b>routine</b> is not devised. Our study suggests that by decoupling of credit management and resource management, highly secure and efficient computing resource management schemes can be developed based on one integrated framework...|$|E
40|$|A {{mutualism}} quantum {{genetic algorithm}} (MQGA) is proposed for an integrated supply chain scheduling with the materials pickup, flow shop scheduling, and the finished products delivery. The {{objective is to}} minimize the makespan, that is, the arrival time of the last finished product to the customer. In MQGA, a new symbiosis strategy named mutualism is proposed to adjust the size of each population dynamically by regarding the mutual influence relation of the two subpopulations. A hybrid Q-bit coding method and a local speeding-up method are designed to increase the diversity of genes, and a <b>checking</b> <b>routine</b> is carried out to ensure the feasibility of each solution; that is, the total physical space of each delivery batch could not exceed {{the capacity of the}} vehicle. Compared with the modified genetic algorithm (MGA) and the quantum-inspired genetic algorithm (QGA), the effectiveness and efficiency of the MQGA are validated by numerical experiments...|$|E
40|$|AbstractIn {{this paper}} we {{describe}} and discuss a kernel for higher-dimensional computational geometry and we present its application in {{the calculation of}} convex hulls and Delaunay triangulations. The kernel is available in form of a software library module programmed in C++ extending LEDA. We introduce the basic data types like points, vectors, directions, hyperplanes, segments, rays, lines, spheres, affine transformations, and operations connecting these types. The description consists of a motivation for the basic class layout as well as topics like layered software design, runtime correctness via <b>checking</b> <b>routines</b> and documentation issues. Finally we shortly describe the usage of the kernel in the application domain...|$|R
50|$|In {{more recent}} events, on 25 June 2012, the United States Supreme Court handed down {{a ruling that}} upheld the {{practice}} of requiring immigration status <b>checks</b> during <b>routine</b> police stops in a 5-3 majority vote. This is {{goes a long way}} from previous immigration customs.|$|R
50|$|In May 2012, Australian Rail Tram and Bus Industry Union accused Metro Trains taking shortcuts {{in safety}} procedures, {{including}} not checking on-board CCTV and intercoms, and allow trains with cracked inner glass to take passengers. Metro Trains claim safety equipment is regularly <b>checked</b> during <b>routine</b> maintenance.|$|R
5000|$|The {{following}} {{table lists}} a few languages with repositories for contributed software. The [...] "Autochecks" [...] column describes the <b>routine</b> <b>checks</b> done.|$|R
50|$|Flight 3601 {{departed}} from Ekaterinenburg on August 18, 1996. The Il-76T landed at Belgrade Nikola Tesla Airport for refueling and a <b>routine</b> <b>check.</b>|$|R
5000|$|No {{preparations}} {{needed before}} sex, though <b>routine</b> <b>checking</b> {{of the device}} strings by patient and physician is advised to ensure proper placement remains intact ...|$|R
30|$|The {{present study}} was {{designed}} to investigate changes in HDL level values or ratios in patients with sepsis. HDL can be <b>checked</b> by <b>routine</b> clinical testing (receiving results during 1  h) and is inexpensive and easy. We also aimed to determine the best threshold to distinguish gram− sepsis infections from gram+ sepsis infections.|$|R
30|$|The {{data used}} and periods under study are {{presented}} in “Data used and periods under study” section. The description of the F algorithm implemented in the IRI UP method is provided in “Description of the filter implemented to select ionosonde data” section. A recall to the Kriging interpolation method is outlined in “The Kriging interpolation method: a brief recall” section. The description of the NQCR procedure is provided in “On {{the choice of the}} best variogram model in the Universal Kriging procedure: a new quality <b>check</b> <b>routine</b> (NQCR)” section. The validation of the IRI UP method and results obtained applying the NQCR procedure are the subject of “Validation of IRI UP method including the F algorithm and the NQCR procedure: some results” section. The discussion about the results and possible future developments is given in “Discussion, conclusions, and future developments” section.|$|R
40|$|We present Signal Spatio-Temporal Logic (SSTL), a {{modal logic}} {{that can be}} used to specify spatio-temporal {{properties}} in linear time and for a discrete space. The logic is equipped with a Boolean and a quantitative semantics, and with accompanying monitoring algorithms. As such, it is suitable for real-time verification of both white box and black box complex systems. It can also be integrated into stochastic model <b>checking</b> <b>routines.</b> The logic combines the until temporal modality with two spatial modality, one expressing that something is true somewhere nearby and the other capturing the notion of being surrounded by a region with a given property. The monitoring algorithms are implemented in an open source Java tool. In the paper we present the logic at work to analyse the formation of patterns in a Turing reaction-diffusion system. Comment: 28 pages with 9 figure...|$|R
40|$|Modeling and {{analysis}} of high consequence, high assurance systems requires special modeling considerations. System safety and reliability information must be captured in the models. Previously, high consequence systems were modeled using separate, disjoint models for safety, reliability, and security. The MultiGraph Architecture facilitates {{the implementation of a}} model integrated system for modeling {{and analysis}} of high assurance systems. Model integrated computing allows an integrated modeling technique to be applied to high consequence systems. Among the tools used for analyzing safety and reliability are a behavioral simulator and an automatic fault tree generation and analysis tool. Symbolic model checking techniques are used to efficiently investigate the system models. A method for converting finite state machine models to ordered binary decision diagrams allows the application of symbolic model <b>checking</b> <b>routines</b> to the integrated system models. This integrated approac [...] ...|$|R
40|$|Abstract: Because of data outsourcing, data holders {{store the}} data in cloud servers and clients {{can get to the}} data from cloud servers. This new data facilitating {{administration}} likewise presents new security issues in cloud. To weigh the data uprightness in the cloud it requires a free examining administration. There are some current respectability <b>checking</b> <b>routines</b> however they can serve static chronicle data and can be connected to the reviewing administration. So {{the data in}} the cloud can be alterably upgraded. To overcome from this a competent and secure evaluating protocol is liked toward fulfill data managers that the data are effectively put away in the cloud. Presenting a novel system which incorporates protection saving examining protocol for security of client data in cloud and backing the data dynamic operations which is effective and provably secure. Th...|$|R
40|$|The {{electronic}} management of documents and records is facing major challenges: {{the integration of}} document processes and <b>checks</b> in <b>routine</b> work processes; risk analysis; techniques and controls needed to manage records and digital evidence controls (metadata, digital signature, authenticity, integrity, preservation); and access restrictions for citizenship or customers. Information professionals are invited {{to take an active}} role in resolving these complex problems...|$|R
40|$|The images {{obtained}} by computed tomography constitute {{a new and}} interesting approach to lesions of the craniovertebral region. The criteria of basilar invagination must be systematically <b>checked</b> in <b>routine</b> examination. The main interest of CT scanning is the simultaneous view of the bony structures {{as well as the}} central nervous system in a completely innocuous way. © 1977 Springer-Verlag. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
50|$|All {{chemical}} {{shifts in}} RefDB have been computationally re-referenced to DSS (a common NMR chemical shift standard). RefDB is a continuously updated resource that uses web-bots to query public databases (BMRB, GenBank, Protein Data Bank) and fetch assignment, sequence and structure {{data on a}} weekly basis. It then applies a series of data <b>checking</b> <b>routines</b> (using keywords to remove paramagnetic or denatured proteins) followed {{by a series of}} calculations to identify and correct chemical shift referencing errors. RefDB is fully web-enabled database, it stores data in two standard formats (NMR-STAR and Shifty), it performs automated data updating, checking and validation and it provides open access to output data in a fully downloadable flat file format as well as in a hyperlinked browsable table (see Fig. 2). RefDB also supports keyword queries and sequence searches (using local BLAST). RefDB is usually updated on a weekly basis. The RefDB database, along with its associated software, is freely available at http://refdb.wishartlab.com and at the BMRB website.|$|R
50|$|Wrapper {{functions}} {{can be used}} {{to write}} error <b>checking</b> <b>routines</b> for pre-existing system functions without increasing the length of a code by a large amount by repeating the same error check for each call to the function. All calls to the original function can be replaced with calls to the wrapper, allowing the programmer to forget about error checking once the wrapper is written.A test driver is a kind of wrapper function that exercises a code module, typically calling it repeatedly, with different settings or parameters, in order to rigorously pursue each possible path. It is not deliverable code, but is not throwaway code either, being typically retained for use in regression testing.An interface adaptor is a kind of wrapper function that simplifies, tailors, or amplifies the interface to a code module, with the intent of making it more intelligible or relevant to the user. It may rename parameters, combine parameters, set defaults for parameters, and the like.|$|R
