344|465|Public
5000|$|A {{pattern of}} {{connectivity}} among units, represented by a matrix of real numbers indicating <b>connection</b> <b>strength.</b>|$|E
5000|$|... where [...] {{defines the}} {{synaptic}} weight or <b>connection</b> <b>strength</b> between the th input and th output neurons, [...] and [...] are the {{input and output}} vectors, respectively, and [...] is the learning rate parameter.|$|E
50|$|Using {{diffusion}} tensor sequences on MRI machines, {{the rate}} at which molecules diffuse {{in and out of a}} specific area of tissue, anisotropy can be measured and used as an indirect measurement of anatomical <b>connection</b> <b>strength.</b> These sequences have found consistent sex differences in human corpus callosal morphology and microstructure.|$|E
5000|$|... <b>connection</b> <b>strengths</b> {{are more}} plastic when the units being {{connected}} have activation probabilities intermediate between zero and one, {{leading to a}} so-called variance trap. The net effect is that noise causes the <b>connection</b> <b>strengths</b> to follow a random walk until the activities saturate.|$|R
3000|$|... {{denote the}} <b>connection</b> <b>strengths</b> which {{correspond}} to the neuronal gains associated with the neuronal activations; [...]...|$|R
3000|$|... {{are known}} {{constants}} denoting the synaptic <b>connection</b> <b>strengths</b> between the neurons {{in the two}} layers, respectively; [...]...|$|R
50|$|The {{initial phase}} of the SyNAPSE program {{developed}} nanometer scale electronic synaptic components capable of adapting the <b>connection</b> <b>strength</b> between two neurons in a manner analogous to that seen in biological systems (Hebbian learning), and simulated the utility of these synaptic components in core microcircuits that support the overall system architecture.|$|E
50|$|Intra-cellular {{electrophysiology}} recordings {{were conducted}} to verify whether increase in quantity of post-synaptic AMPA receptors equated to up-regulation of synaptic <b>connection</b> <b>strength.</b> Intracellular recordings show robust increase in mEPSC amplitude (approximately 130% above control values) following 4-5 hours of TTX treatment. Longer TTX treatments yielded a more noticeable increase in mEPSC amplitude. This form of AMPA receptor trafficking is hypothesized to be directed by local mRNA transcription.|$|E
50|$|Oja's {{learning}} rule, {{or simply}} Oja's rule, named after Finnish computer scientist Erkki Oja, {{is a model}} of how neurons in the brain or in artificial neural networks change <b>connection</b> <b>strength,</b> or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons.|$|E
5000|$|Memory {{is created}} by modifying the <b>strength</b> of the <b>connections</b> between neural units. The <b>connection</b> <b>strengths,</b> or [...] "weights", are {{generally}} represented as an N×N matrix.|$|R
5000|$|... {{the time}} the machine must be run in order to collect {{equilibrium}} statistics grows exponentially with the machine's size, and with {{the magnitude of the}} <b>connection</b> <b>strengths</b> ...|$|R
50|$|These centre {{surround}} {{structures are}} not physical {{in the sense}} that one cannot see them by staining samples of tissue and examining the retina's anatomy. The centre surround structures are logical (i.e., mathematically abstract) {{in the sense that}} they depend on the <b>connection</b> <b>strengths</b> between ganglion and bipolar cells. It is believed that the <b>connection</b> <b>strengths</b> between cells is caused by the number and types of ion channels embedded in the synapses between the ganglion and bipolar cells. See Receptive field for figures and more information on centre surround structures.|$|R
50|$|However, Hebb's {{rule has}} problems, namely {{that it has}} no {{mechanism}} for connections to get weaker and no upper bound for how strong they can get. In other words, the model is unstable, both theoretically and computationally. Later modifications gradually improved Hebb's rule, normalizing it and allowing for decay of synapses, where no activity or unsynchronized activity between neurons results in a loss of <b>connection</b> <b>strength.</b> New biological evidence brought this activity to a peak in the 1970s, where theorists formalized various approximations in the theory, {{such as the use of}} firing frequency instead of potential in determining neuron excitation, and the assumption of ideal and, more importantly, linear synaptic integration of signals. That is, there is no unexpected behavior in the adding of input currents to determine whether or not a cell will fire.|$|E
50|$|Cios has co-authored {{three books}} and over 200 journal and conference papers. His main {{contributions}} {{are in the}} areas of machine learning, computational neuroscience, and data mining; his book on the latter subject was the first one published in the U.S. (1998) and his article on uniqueness of medical data mining is frequently cited. In the area of neuroinformatics, he co-defined with Kevin Staley (Harvard Medical School) and others a dynamic Synaptic Activity Plasticity Rule, which in contrast to other plasticity rules is continuous and dynamic as it uses actual post-synaptic potential function to modify <b>connection</b> <b>strength</b> between neurons. Some of the machine learning algorithms developed with his students, such as CAIM and ur-CAIM discretization algorithms and rule algorithms for single-instance (DataSqueezer), multiple-instance (mi-DS) and one-class (OneClass-DS) learning, became popular and were implemented in open-source software platforms.|$|E
50|$|Synaptic {{plasticity}} is {{the ability}} of the brain to strengthen, weaken, destroy and create neural synapses and is the basis for learning. These molecular distinctions will identify and indicate the strength of each neural connection. The effect of a learning experience depends on the content of such an experience. Reactions that are favoured will be reinforced and those that are deemed unfavourable will be weakened. This shows that the synaptic modifications that occur can operate either way, {{in order to be able}} to make changes over time depending on the current situation of the organism. In the short term, synaptic changes may include the strengthening or weakening of a connection by modifying the preexisting proteins leading to a modification in synapse <b>connection</b> <b>strength.</b> In the long term, entirely new connections may form or the number of synapses at a connection may be increased, or reduced.|$|E
40|$|A linear non-Gaussian {{structural}} equation model called LiNGAM is an identifiable model for exploratory causal analysis. Previous methods estimate a causal ordering of variables and their <b>connection</b> <b>strengths</b> based on a single dataset. However, in many application domains, data are obtained under different conditions, that is, multiple datasets are obtained rather than a single dataset. In this paper, we present a new method to jointly estimate multiple LiNGAMs {{under the assumption that}} the models share a causal ordering but may have different <b>connection</b> <b>strengths</b> and differently distributed variables. In simulations, the new method estimates the models more accurately than estimating them separately. Comment: A revised version was accepted in Neurocomputin...|$|R
30|$|A signed {{weighted}} correlation {{network was}} constructed using 31 samples by first creating {{a matrix of}} pairwise correlations between all pairs of genes with annotation. The resulting Pearson correlation matrix {{was transformed into a}} matrix of <b>connection</b> <b>strengths</b> (e.g., an adjacency matrix) using a power of 12 (Langfelder and Horvath, 2008). Then the topological overlap was calculated to measure network interconnectedness (Yip and Horvath, 2007). For each dataset, we used average linkage hierarchical clustering to group genes {{on the basis of the}} topological overlap dissimilarity measure (1 -topological overlap) of their network <b>connection</b> <b>strengths.</b> Using a dynamic tree-cutting algorithm and merging threshold function at 0.25, we identified 9 modules.|$|R
40|$|In this study, we {{examined}} the accuracy of ancestral graphs (AGs) to study effective connectivity in the brain. Unlike most other methods that estimate effective connectivity, an AG is able to explicitly model missing brain regions in a network model. We compared AGs with the conventional structural equation models (SEM). We used both methods to estimate <b>connection</b> <b>strengths</b> between six regions of interest of the visual cortex based on {{functional magnetic resonance imaging}} data of a motion perception task. In order to examine which method is more accurate to estimate effective connectivity, we compared the <b>connection</b> <b>strengths</b> of the AG and SEM models with connection probabilities resulting from probabilistic tractography obtained from diffusion tensor images. This was done by correlating the <b>connection</b> <b>strengths</b> of the best fitting AG and SEM models with the connection probabilities of the probabilistic tractography models. We show that, in general, AGs result in more accurate models to estimate effective connectivity than SEM. The {{reason for this is that}} missing regions are taken into account when modeling with AG but not when modeling with SEM: AG can be used to explicitly test the assumption of missing regions. If the set of regions is complete, SEM and AG perform about equally well...|$|R
5000|$|Alex Nugent {{describes}} a physical neural network as {{one or more}} nonlinear neuron-like nodes used to sum signals and nanoconnections formed from nanoparticles, nanowires, or nanotubes which determine the signal strength input to the nodes. Alignment or self-assembly of the nanoconnections {{is determined by the}} history of the applied electric field performing a function analogous to neural synapses. Numerous applications for such physical neural networks are possible. For example, a temporal summation device [...] can be composed of one or more nanoconnections having an input and an output thereof, wherein an input signal provided to the input causes {{one or more of the}} nanoconnection to experience an increase in <b>connection</b> <b>strength</b> thereof over time. Another example of a physical neural network is taught by U.S. Patent No. 7,039,619 entitled [...] "Utilized nanotechnology apparatus using a neural network, a solution and a connection gap," [...] which issued to Alex Nugent by the U.S. Patent & Trademark Office on May 2, 2006.|$|E
5000|$|The {{connection}} densities, or neighbourhood {{densities of}} memory arrangements help distinguish which elements {{are a part}} of, or related to, the target memory. As the density of neural networks increases, the number of retrieval cues (associated nodes) also increases, which may allow for enhanced memory of the event. However, too many connections can inhibit memory in two ways. First, as described under the sub-section Spreading Activation, the total activation being spread from node 1 to connecting nodes is divided {{by the number of}} connections. With a greater number of connections, each connecting node receives less activation, which may result in too little activation for the memory cue to be brought to awareness. <b>Connection</b> <b>strength,</b> in which more strongly connected associations receive more activation than less-related associations, may also prevent specific connections from being brought to awareness due to being out-competed by the stronger associations. [...] Second, with more connections branching from various other nodes, there is a greater probability of linking associated connections of different memories together (transplant errors) so that memory errors occur and incorrect features are recalled.|$|E
5000|$|Based on the {{original}} pandemonium architecture, John Jackson has extended the theory to explain phenomena beyond perception. Jackson offered the analogy of an arena to account for [...] "consciousness". His arena consisted of a stand, a playing field, and a sub-arena. The arena was populated by a multitude of demons. The demons that were designated in the playing fields were the active demons, as they represent the active elements of human consciousness. The demons in the stands are to watch those in the playing field until something excites them; each demon is excited by different things. The more excited the demons get, the louder they yell. If a demon yells pass a set threshold, it gets to join the other demons in the playing field and perform its function, which may then excite other demons, and this cycle continues. The sub-arena in the analogy functions as the learning and feedback mechanism of the system. The learning system here is similar to any other neural styled networks, which is through modifying the <b>connection</b> <b>strength</b> between the demons; in other words, how the demons respond to each other’s yelling. This multiple agent approach to human information processing became the assumption for many modern artificial intelligence systems.|$|E
40|$|Training time {{decreases}} dramatically. In improved {{mathematical model}} of neural-network processor, temperature of neurons (in addition to <b>connection</b> <b>strengths,</b> also called weights, of synapses) varied during supervised-learning phase of operation according to mathematical formalism and not heuristic rule. Evidence that biological neural networks also process information at neuronal level...|$|R
40|$|Information {{geometry}} {{has been}} suggested to provide {{a powerful tool for}} analyzing multineuronal spike trains. Among several advantages of this approach, a significant property is the close link between information-geometric measures and neural network architectures. Previous modeling studies established that the first- and second-order information-geometric measures corresponded to the number of external inputs and the <b>connection</b> <b>strengths</b> of the network, respectively. This relationship was, however, limited to a symmetrically connected network, and the number of neurons used in the parameter estimation of the log-linear model needed to be known. Recently, simulation studies of biophysical model neurons have suggested that information geometry can estimate the relative change of <b>connection</b> <b>strengths</b> and external inputs even with asymmetric connections. Inspired by these studies, we analytically investigated the link between the information-geometric measures and the neural network structur...|$|R
5000|$|A {{multilayer}} {{neural network}} model by Linsker, having local connections from each cell layer to the next, whose <b>connection</b> <b>strengths</b> develop according to a Hebbian rule, generates orientation-selective cells and orientation columns. The resulting columnar arrangement contains fractures and [...] "pinwheel" [...] singularities of the same types as those found experimentally.|$|R
30|$|Signals {{are passed}} between nodes through {{connection}} links and each link has an associated weight that represents its <b>connection</b> <b>strength.</b>|$|E
30|$|Glued-in rod <b>connection</b> <b>strength</b> is {{determined}} by two different interfaces: one is the interface between glue and threaded rods which is enhanced by the increasing contacting area; {{and the other is}} the interface between base materials and inherent bonding (Fig.  2 a).When the glued-in rods are strong enough, the interface is critical to the <b>connection</b> <b>strength.</b> In terms of bamboo glulam, the glued rods with λ should be above the critical ratio.|$|E
40|$|BACKGROUND: The network {{approach}} to psychopathology conceives mental disorders as sets of symptoms causally impacting on each other. The {{strengths of the}} connections between symptoms are key elements {{in the description of}} those symptom networks. Typically, the connections are analysed as linear associations (i. e., correlations or regression coefficients). However, there is insufficient awareness of the fact that differences in variance may account for differences in <b>connection</b> <b>strength.</b> Differences in variance frequently occur when subgroups are based on skewed data. An illustrative example is a study published in PLoS One (2013; 8 (3) :e 59559) that aimed to test the hypothesis that the development of psychopathology through "staging" was characterized by increasing <b>connection</b> <b>strength</b> between mental states. Three mental states (negative affect, positive affect, and paranoia) were studied in severity subgroups of a general population sample. The <b>connection</b> <b>strength</b> was found to increase with increasing severity in six of nine models. However, the method used (linear mixed modelling) is not suitable for skewed data. METHODS: We reanalysed the data using inverse Gaussian generalized linear mixed modelling, a method suited for positively skewed data (such as symptoms in the general population). RESULTS: The distribution of positive affect was normal, but the distributions of negative affect and paranoia were heavily skewed. The variance of the skewed variables increased with increasing severity. Reanalysis of the data did not confirm increasing <b>connection</b> <b>strength,</b> except for one of nine models. CONCLUSIONS: Reanalysis of the data did not provide convincing evidence in support of staging as characterized by increasing <b>connection</b> <b>strength</b> between mental states. Network researchers should be aware that differences in <b>connection</b> <b>strength</b> between symptoms may be caused by differences in variances, in which case they should not be interpreted as differences in impact of one symptom on another symptom...|$|E
3000|$|We {{began by}} {{manually}} carrying out many numerical simulations of different coupled systems, varying {{the number of}} channels, connection topology, α and I, and whether the healthy or Parkinsonian fixed <b>connection</b> <b>strengths</b> were used. Each simulation was started from random initial conditions. During this experimental work, we found that for relatively low lateral coupling (e.g. when [...]...|$|R
30|$|The matrix H has {{constant}} columns, {{except for}} the diagonal, which reflects self-coupling from each cell onto itself. The parameters b_E and b_I give the ratios of self- to non-self-connection strengths for excitatory and inhibitory cells, respectively. We assume {{that the effect of}} self-coupling is to reduce <b>connection</b> <b>strengths,</b> that is, 0 < b_E, b_I< 1.|$|R
40|$|Wittmeyer’s pseudoinverse {{iterative}} {{algorithm is}} formulated as a dynamic connectionist Data Compression and Reconstruction (DCR) network, and subnets {{of this type}} are supplemented by the winner-take-all paradigm. The winner is selected upon the goodness-of-fit of the input reconstruction. The network can be characterised as a competitive-cooperative-competitive architecture {{by virtue of the}} contrast enhancing properties of the pseudoinverse subnets. The network is capable of fast learning. The adopted learning method gives rise to increased sampling in the vicinity of dubious boundary regions that resembles the phenomenon of categorical perception. The generalising abilities of the scheme allow one to utilise single bit <b>connection</b> <b>strengths.</b> The network is robust against input noise and contrast levels, shows little sensitivity to imprecise <b>connection</b> <b>strengths,</b> and is promising for mixed VLSI implementation with on-chip learning properties. The features of the DCR network are demonstrated on the NIST database of handprinted characters...|$|R
3000|$|... (t) {{stands for}} the {{accumulated}} external input to neuron i, and the corresponding synapses have <b>connection</b> <b>strength</b> of amplitude J [...]...|$|E
3000|$|... 1. The right {{term with}} a {{summation}} {{corresponds to a}} de-synchronizing influence among N Memory Units, with <b>connection</b> <b>strength</b> w [...]...|$|E
40|$|Proposed {{array of}} {{programmable}} synaptic connections for electronic neural network applications offers multiple quantized levels of <b>connection</b> <b>strength</b> using only simple, two-terminal, binary microswitch devices. Subgrids in fine grid of programmable resistive connections connected externally in parallel to form coarser synaptic grid. By selection of pattern of connections in each subgrid, <b>connection</b> <b>strength</b> of synaptic node represented by that subgrid set at quantized "gray level". Device structures promise implementations of quantized-"gray-scale" synaptic arrays with very high density...|$|E
3000|$|When {{studying}} the coupled channels model, {{we used the}} same values for the fixed parameters as {{were used in the}} isolated channel model (see Table  1). As before, the <b>connection</b> <b>strengths</b> were divided into a healthy set and a Parkinsonian set. However, since there is no known mechanism whereby STN neurons can excite other STN neurons, we chose to fix [...]...|$|R
50|$|Spike-timing-dependent {{plasticity}} (STDP) is {{a biological}} process that adjusts the <b>strength</b> of <b>connections</b> between neurons in the brain. The process adjusts the <b>connection</b> <b>strengths</b> {{based on the}} relative timing of a particular neuron's output and input action potentials (or spikes). The STDP process partially explains the activity-dependent development of nervous systems, especially with regards to long-term potentiation and long-term depression.|$|R
40|$|In this work, we {{investigate}} {{a model of}} an adaptive networked dynamical system, where the coupling strengths among phase oscillators coevolve with the phase states. It is shown that in this model the oscillators can spontaneously differentiate into two dynamical groups after a long time evolution. Within each group, the oscillators have similar phases, while oscillators in different groups have approximately opposite phases. The network gradually converts from the initial random structure with a uniform distribution of <b>connection</b> <b>strengths</b> into a modular structure which is characterized by strong intra connections and weak inter connections. Furthermore, the <b>connection</b> <b>strengths</b> follow a power law distribution, which is a natural consequence of the coevolution of the network and the dynamics. Interestingly, {{it is found that}} if the inter connections are weaker than a certain threshold, the two dynamical groups will almost decouple and evolve independently. These results are helpful in further understanding the empirical observations in many social and biological networks. Comment: 14 page, 5 figure...|$|R
