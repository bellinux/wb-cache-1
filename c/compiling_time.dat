10|2168|Public
5000|$|Originally, Rhino {{compiled}} all JavaScript code to Java bytecode in generated Java class files. This {{produced the}} best performance, often beating the C++ implementation of JavaScript run with just-in-time compilation (JIT), but suffered from two faults. First, <b>compiling</b> <b>time</b> was long since generating bytecode and loading the generated classes was a resource-intensive process. Also, the implementation effectively leaked memory since most Java Virtual Machines (JVM) didn't collect unused classes or the strings that are interned {{as a result}} of loading a class file. (This has changed in later versions of Java.) ...|$|E
50|$|LabVIEW {{includes}} a compiler that produces native code for the CPU Sunil platform. This aids performance. The graphical code is translated into executable machine code by interpreting the syntax and by compiling. The LabVIEW syntax is strictly enforced during the editing process and compiled into the executable machine code when requested to run or upon saving. In the latter case, the executable {{and the source}} code are merged into a single file. The executable runs {{with the help of}} the LabVIEW run-time engine, which contains some precompiled code to perform common tasks that are defined by the G language. The run-time engine reduces <b>compiling</b> <b>time</b> and provides a consistent interface to various operating systems, graphic systems, hardware components, etc. The run-time environment makes the code portable across platforms. Generally, LabVIEW code can be slower than equivalent compiled C code, although the differences often lie more with program optimization than inherent execution speed.|$|E
40|$|Abstract: Because of {{the rules}} of dynamic {{directive}} nesting and binding, some of the thread context in OpenMP programs can only be totally determined at runtime. However, by <b>compiling</b> <b>time</b> static analysis, nesting type can be partly determined and this information can be passed to other compiling phases to guide later translation and optimizations. Since the binding and nesting may span the procedure boundaries through calls, local and global analyses are not enough. It is the interprocedural analysis that provides the most required ability. By integrating information into traditional interprocedural analysis, the nesting type information of procedures is propagated along call graphs. And later translation and optimization phases can bind this global information with local information inside the procedure to determine the nesting types at <b>compiling</b> <b>time.</b> The results demonstrate that in typica...|$|E
5000|$|<b>Compile</b> <b>time</b> {{function}} execution: {{the evaluation}} of pure functions at <b>compile</b> <b>time</b> ...|$|R
5000|$|The LiteralExp {{must be a}} <b>compiled</b> <b>time</b> resolvable numeric expression, and {{may involve}} {{operators}}, as long as such operators involve <b>compile</b> <b>time</b> static value.|$|R
40|$|In {{this report}} we {{introduce}} the <b>compile</b> <b>time</b> constraint library of the pe physics engine. These <b>compile</b> <b>time</b> constraints are {{a valuable tool}} to detect <b>compile</b> <b>time</b> errors and {{in case of an}} error to abort the compilation process and report the error by emitting a comprehensible error message. Additionally, they offer the option to enforce certain design decisions. ...|$|R
40|$|This {{paper is}} {{concerned}} with designing efficient algorithms for determining data distribution and generating communication sets on distributed memory multicomputers. First, we propose a dynamic programming algorithm to automatically determine data distribution at <b>compiling</b> <b>time.</b> The proposed algorithm also can determine whether data redistribution is necessary between two consecutive DO-loop program fragments. Second, we propose closed forms to represent communication sets among processing elements for executing doall statements, when data arrays are distributed in a restricted block-cyclic fashion. Our methods can be included in current compilers and used when programmers fail to provide any data distribution directives. Experimental studies on a nCUBE- 2 multicomputer are also presented. 1 Introduction Arrays distribution and communication sets generation are two problems we must solve when dealing with the compilation of DO-loop program fragments for distributed memory multicomputer [...] ...|$|E
40|$|An {{efficient}} {{spatial data}} structure in a GIS system for database updating {{is required in}} order to minimising of spatial constraint violations and timesaving. An automated constraint checking procedure has been introduced to perform constraint violations check at <b>compiling</b> <b>time</b> before updating the database. Formal definitions of spatial data types were used in attempt to formulate novel equations and architectures to detect constraint violations and framework for spatial repository. A data structure called Semantic Spatial Outlier R-Tree (SSR O-Tree) was proposed for the Semantic Integrity Constraints Checking System to improve the functionality of the proposed method. The R-Tree or its variants have been widely-used data structures for this purpose and which are based on a heuristic optimization but unable to perform semantic spatial join queries at database updating. An experiment was conducted using actual spatial data and results revealed that the performance of SSR O-Tree is notably superior to the R*-Tree and R O-Tree for conducting semantic spatial join queries...|$|E
40|$|In {{the motion}} {{analysis}} of the coupler points of a four-bar mechanism, one can achieve reproductive results graphically for the displacement, {{but it is very}} time consuming. However, when the velocity and acceleration are determined graphically, the results are not very accurate. Rather than obtaining reproducible results one often gets an average value with a plus or minus around this value. Reproducible results are always received for the {{analysis of the}} displacement, velocity and acceleration of a coupler point using a digital computer. Since the computer, under normal conditions, stores a number which may have up to eight decimal places, one can be certain that the results will be accurate to as many significant figures as the input numbers are. As was mentioned earlier in plotting the center-point curve, a complete curve can be plotted graphically from one opposite pole quadrilateral. With four specified positions of a body, there are three distinct opposite pole quadrilaterals and therefore the complete center-point curve can be plotted three times. When this is done graphically, {{it is very difficult to}} achieve on curve, rather, the results are more likely to be a “band” or three distinct curves which are very close. This is significant because for the included example in the appendix one opposite side was 20 units long and yet the computer solution resulted in one curve for three plots. The program as presented may very well be improved upon by changing a part or parts of them. Although the center-point curve can be plotted from one quadrilateral, for this thesis results can be received from all three quadrilaterals. Since the complete curve can be plotted from one quadrilateral, two quadrilaterals could be eliminated from the program in order to save <b>compiling</b> <b>time.</b> The center-point curve results are not capable of being plotted by a computer curve plotter as they appear in this program; however, with a few changes one could adapt this program to a curve plotter. Advisor: James C. Wolfor...|$|E
50|$|Compile-time {{function}} execution (or <b>compile</b> <b>time</b> function evaluation, or general constant expressions) {{is the ability}} of a compiler, that would normally compile a function to machine code and execute it at run time, to execute the function at <b>compile</b> <b>time.</b> This is possible if the arguments to the function are known at <b>compile</b> <b>time,</b> and the function does not make any reference to or attempt to modify any global state (is a pure function).|$|R
50|$|As {{opposed to}} macros, {{templates}} are considered type-safe; that is, they require type-checking at <b>compile</b> <b>time.</b> Hence, the compiler can determine at <b>compile</b> <b>time</b> whether the type {{associated with a}} template definition can perform all of the functions required by that template definition.|$|R
50|$|To ensure compilers cannot pre-compute {{the results}} at <b>compile</b> <b>time</b> every {{operation}} in the benchmark derives a value that is not available at <b>compile</b> <b>time.</b> Furthermore, all code used within the timed portion of the benchmark {{is part of the}} benchmark itself (no library calls).|$|R
40|$|This paper {{presents}} {{a discussion of}} the predictive capacity of the implementation of the semi-distributed hydrological modeling system JGrass-NewAge. This model focuses on the hydrological budgets of medium scale to large scale basins as the product of the processes at the hillslope scale with the interplay of the river network. The part of the modeling system presented here deals with the: (i) estimation of the space-time structure of precipitation, (ii) estimation of runoff production; (iii) aggregation and propagation of flows in channel; (v) estimation of evapotranspiration; (vi) automatic calibration of the discharge with the method of particle swarming. The system is based on a hillslope-link geometrical partition of the landscape, combining raster and vectorial treatment of hillslope data with vector based tracking of flow in channels. Measured precipitation are spatially interpolated with the use of kriging. Runoff production at each channel link is estimated through a peculiar application of the Hymod model. Routing in channels uses an integrated flow equation and produces discharges at any link end, for any link in the river network. Evapotranspiration is estimated with an implementation of the Priestley-Taylor equation. The model system assembly is calibrated using the particle swarming algorithm. A two year simulation of hourly discharge of the Little Washita (OK, USA) basin is presented and discussed with the support of some classical indices of goodness of fit, and analysis of the residuals. A novelty with respect to traditional hydrological modeling is that each of the elements above, including the preprocessing and the analysis tools, is implemented as a software component, built upon Object Modelling System v 3 and jgrasstools prescriptions, that can be cleanly switched in and out at run-time, rather than at <b>compiling</b> <b>time.</b> The possibility of creating different modeling products by the connection of modules with or without the calibration tool, as for instance the case of the present modeling chain, reduces redundancy in programming, promotes collaborative work, enhances the productivity of researchers, and facilitates the search for the optimal modeling solution...|$|E
40|$|GPUs {{have been}} widely used to {{parallelize}} and accelerate applications for its high throughput. Traditionally, a GPU function can only be launched from the CPU side. This results in the fact that GPUs are preferable for those application which express a flat data parallelism, a simple data parallelism that is known at <b>compiling</b> <b>time</b> and can be easily distributed to different GPU blocks and threads. However, for those applications that contain nested data parallelism, which is not known a priori and can only be discovered at running time, it is difficult to write a GPU function that achieve high performance on parallelization and acceleration. One can easily end up with either a too coarse-grained or too fine-grained GPU function. Since Kepler architecture, Nvidia introduced a new feature [...] Dynamic Parallelism (DP), which enables the initiation of GPU functions from inside a GPU function. This makes the nested parallelism easy to be explored on GPU since one can program in a way that a new GPU function can be launched whenever a local nested parallelism is met during the execution. What is more, DP makes implementing recursion on GPU without the intervention of CPUs possible. Many computations exhibit a pattern of nested data parallelism and among those is parallel recursion. However, preliminary data shows that simple DP-based implementations of recursion result in poor performance. This work focus on how to efficiently exploit DP for parallel recursive applications on GPU. Specifically, the goal is to free the users from programming with the complexity of GPUs' hardware and software and to automatically generate high performance GPU recursive functions implemented with DP given the inputs of simple parallel CPU recursive functions. To this end, first, I propose several DP-based parallel recursive templates that can be generated from a serial CPU recursive function. I compare the parallel recursive templates with non DP-based counterparts (flat kernels) to see if using DP in parallel recursive application can be beneficial or not. Second, to reduce the overhead of DP, I propose compiler techniques that improve the efficiency of simple DP-based parallel recursive functions by performing workload consolidation. My evaluation shows that GPU kernels consolidated with the proposed code transformations achieve an average speedup in the order of 1500 x over basic implementations using DP and an average speedup of 3. 9 x over optimized flat GPU kernels for both tree traversal and graph based applications...|$|E
40|$|This thesis {{document}} was issued {{under the authority}} of another institution, not NPS. At the time it was written, a copy was added to the NPS Library Collection for reasons not now known. It has been included in the digital archive for its historical value to NPS. Not believed to be a CIVINS (Civilian Institutions) title. A simplified polyenergetic cell theory is formulated to determine spatially averaged energy dependent thermal fluxes in the moderator, cladding, and fuel regions within the unit cell of a reactor lattice. The derived spectra are then utilized in the calculations of the thermal integral parameters and average cross sections required for reactor computations. The cell theory, as formulated, postulates an infinite moderator region with the absorption cross section of this region appropriately modified to account for the neutron leakage into and absorption by the fuel element. The modifications to the moderator absorption cross section are formulated both in terms of the net current at the fuel element-moderator interface and in terms of energy dependent moderator and fuel element escape probabilities, the latter approach offering physical transparency and ease of calculation. Analytic expressions for the escape probabilities are presented, integral transport theory being applied to the fuel element region, while diffusion theory is utilized in the moderator region. Using these analytic expressions, the theory is applied to actual lattices {{in the form of the}} light water moderated and uranium dioxide fueled cores of the Cornell University Zero Power Reactor. Room temperature parameters and their temperature coefficients are determined using both the monatomic gas model and the Nelkin water kernel to describe the energy transfer process in the moderator. Calculations are made with PROGRAM COUTH, a Fortran- 63 program written for use with the Control Data Corporation 1604 digital computer. A typical lattice calculation including the computation of the spatially averaged fuel, cladding, and moderator spectra and the thermal integral properties and average cross sections takes approximately thirty-five seconds of computer time. This figure is exclusive of the <b>compiling</b> <b>time</b> and the time required to calculate the moderator scattering kernel. In an attempt to estimate the accuracy of the calculational results, the method is applied to the Brookhaven National Laboratory uranium dioxide cores and the results are then compared with those predicted by Honeck's THERMOS code. Disadvantage factors agree to within 1. 0 % while the thermal utilizations agree to within 0. 5 %. A study of the sensitivity of the calculated integral parameters to variations in the input data leads to the assignment of rather small uncertainties in the results calculated with the simplified cell theory. [URL] United States Nav...|$|E
5000|$|Package {{diagrams}} {{represent a}} <b>compile</b> <b>time</b> grouping mechanism.|$|R
5000|$|Constant folding is {{the process}} of {{recognizing}} and evaluating constant expressions at <b>compile</b> <b>time</b> rather than computing them at runtime. Terms in constant expressions are typically simple literals, such as the integer literal , but they may also be variables whose values are known at <b>compile</b> <b>time.</b> Consider the statement: ...|$|R
5000|$|The first IBM <b>Compile</b> <b>time</b> {{preprocessor}} {{was built}} by the IBM Boston Advanced Programming Center located in Cambridge, Mass, and shipped with the PL/I F compiler. The [...] statement was in the Standard, {{but the rest of}} the features were not. The DEC and Kednos PL/I compilers implemented much the same set of features as IBM, with some additions of their own. IBM has continued to add preprocessor features to its compilers. The preprocessor treats the written source program as a sequence of tokens, copying them to an output source file or acting on them. When a % token is encountered the following <b>compile</b> <b>time</b> statement is executed: when an identifier token is encountered and the identifier has been d, d, and assigned a <b>compile</b> <b>time</b> value, the identifier is replaced by this value. Tokens are added to the output stream if they do not require action (e.g. [...] ), as are the values of ACTIVATEd <b>compile</b> <b>time</b> expressions. Thus a <b>compile</b> <b>time</b> variable [...] could be declared, activated, and assigned using [...] Subsequent occurrences of [...] would be replaced by [...]|$|R
40|$|Purpose - The {{current work}} aims {{to present a}} {{parallel}} code using the open multi-processing (OpenMP) programming model for an adaptive multi-resolution high-order finite difference scheme for solving 2 D conservation laws, comparing efficiencies obtained with a previousmessage passing interface formulation for the same serial scheme and considering {{the same type of}} 2 D formulations laws. Design/methodology/approach - The serial version of the code is naturally suitable for parallelization because the spatial operator formulation is based on a splitting scheme per direction for which the flux components are numerically computed by a Lax-Friedrichs factorization independently for each row or column. High-order approximations for numerical fluxes are computed by the third-order essentially non-oscillatory (ENO) and fifth-order weighted essentially non-oscillatory (WENO) interpolation schemes, assuming sparse grids in each direction. The grid adaptivity is obtained by a cubic interpolating wavelet transform applied in each space dimension, associated to a threshold operator. Time is evolved by a third order TVD Runge-Kutta method. Findings - The parallel formulation is implemented automatically at <b>compiling</b> <b>time</b> by the OpenMP library routines, being virtually transparent to the programmer. This over simplifies any concerns about managing and/ or updating the adaptive grid when compared to what is necessary to be done when other parallel approaches are considered. Numerical simulations results and the large speedups obtained for the Euler equations in gas dynamics highlight the efficiency of the OpenMP approach. Research limitations/implications - The resulting speedups reflect the effectiveness of the OpenMP approach but are, to a large extension, limited by the hardware used (2 E 5 - 2620 Intel Xeon processors, 6 cores, 2 threads/core, hyper-threading enabled). As the demand for OpenMP threads increases, the code starts to make explicit use of the second logical thread available in each E 5 - 2620 processor core and efficiency drops. The speedup peak is reached near the possible maximum (24) at about 22, 23 threads. This peak reflects the hardware configuration and the true software limit should be located way beyond this value. Practical implications - So far no {{attempts have been made to}} parallelize other possible code segments (for instance, the ENO|-WENO-TVD code lines that process the different data components which could potentially push the speed up limit to higher values even further. The fact that the speedup peak is located close to the present hardware limit reflects the scalability properties of the OpenMP programming and of the splitting scheme as well. Consequently, it is likely that the speedup peak with the OpenMP approach for this kind of problem formulation will be close to the physical (and/or logical) limit of the hardware used. Social implications - This work is the result of a successful collaboration among researchers from two different institutions, one internationally well-known and with a long-term experience in applied mathematics for industrial applications and the other in a starting process of international academic insertion. In this way, this scientific partnership has the potential of promoting further knowledge exchange, involving students and other collaborators. Originality/value - The proposed methodology (use of OpenMP programming model for the wavelet adaptive splitting scheme) is original and contributes to a very active research area in the past years, namely, adaptive methods for conservation laws and their parallel formulations, which is of great interest for the entire scientific community...|$|E
5000|$|Here's {{an example}} of <b>compile</b> <b>time</b> {{function}} evaluation in C++14: ...|$|R
5000|$|Templates {{are written}} as <b>compile</b> <b>time</b> {{functions}} with type parameters.|$|R
50|$|In programming, string {{concatenation}} generally {{occurs at}} run time, as string values {{are not in}} general known until run time. However, {{in the case of}} string literals, the values are known at <b>compile</b> <b>time,</b> and thus string concatenation can be done at <b>compile</b> <b>time,</b> either via string literal concatenation or via constant folding.|$|R
50|$|Assertions {{that are}} checked at <b>compile</b> <b>time</b> are called static assertions.|$|R
5000|$|Execute {{parts of}} a program at <b>compile</b> <b>time</b> rather than runtime ...|$|R
5000|$|Java: Lightwolf javaflow (Requires {{bytecode}} manipulation at runtime or <b>compile</b> <b>time)</b> ...|$|R
40|$|Many keyword pattern {{matching}} algorithms use precomputation subroutines to produce lookup tables, {{which in turn}} are used to improve performance during the search phase. If the keywords to be matched are known at <b>compile</b> <b>time,</b> the precomputation subroutines can be implemented to be evaluated at <b>compile</b> <b>time</b> versus at run time. This will provide a performance boost to run time operations. We have started {{an investigation into the}} use of metaprogramming techniques to implement such <b>compile</b> <b>time</b> evaluation, initially for the Knuth-Morris-Pratt (KMP) algorithm. We present an initial experimental comparison of the performance of the traditional KMP algorithm to that of an optimised version that uses <b>compile</b> <b>time</b> precomputation. During implementation and benchmarking, it was discovered that C++ is not well suited to metaprogramming when dealing with strings, while the related D language is. We therefore ported our implementation to the latter and performed the benchmarking with that version. We discuss the design of the benchmarks, the experience in implementing the benchmarks in C++ and D, and the results of the D benchmarks. The results show that under certain circumstances, the use of <b>compile</b> <b>time</b> precomputation may significantly improve performance of the KMP algorithm...|$|R
50|$|It allows full checks at <b>compile</b> <b>time</b> between {{actual and}} dummy arguments.|$|R
5000|$|This example {{specifies}} a valid D function called [...] "factorial" [...] {{which would}} typically be evaluated at run time. The use of [...] tells the compiler that the initializer for the variables must be computed at <b>compile</b> <b>time.</b> Note that the arguments to the function {{must be able}} to be resolved at <b>compile</b> <b>time</b> as well.|$|R
500|$|Most of {{what happens}} in Perl's compile phase is compilation, and most {{of what happens}} in Perl's run phase is execution, but there are {{significant}} exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are <b>compile</b> <b>time</b> and run time. [...] Perl is in <b>compile</b> <b>time</b> at most points during the compile phase, but <b>compile</b> <b>time</b> may also be entered during the run phase. The <b>compile</b> <b>time</b> for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run <b>time</b> during the <b>compile</b> phase and spends most of the run phase in run time. [...] Code in BEGIN blocks executes at run time but in the compile phase.|$|R
5000|$|A few {{languages}} provide string literal concatenation, where adjacent string literals are implicitly joined into {{a single}} literal at <b>compile</b> <b>time.</b> This is a feature of C, C++, D, and Python, which copied it from C. Notably, this concatenation happens at <b>compile</b> <b>time,</b> during lexical analysis (as a phase following initial tokenization), and is contrasted with both run time string concatenation (generally with the [...] operator) and concatenation during constant folding, which occurs at <b>compile</b> <b>time,</b> but in a later phase (after phrase analysis or [...] "parsing"). Most languages, such as C#, Java and Perl, do not support implicit string literal concatenation, and instead require explicit concatenation, such as with the [...] operator (this is also possible in D and Python, but illegal in C/C++ - see below); in this case concatenation may happen at <b>compile</b> <b>time,</b> via constant folding, or may be deferred to run time.|$|R
5000|$|Most of {{what happens}} in Perl's compile phase is compilation, and most {{of what happens}} in Perl's run phase is execution, but there are {{significant}} exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are <b>compile</b> <b>time</b> and run time. Perl is in <b>compile</b> <b>time</b> at most points during the compile phase, but <b>compile</b> <b>time</b> may also be entered during the run phase. The <b>compile</b> <b>time</b> for code in a string argument passed to the [...] built-in occurs during the run phase. Perl is often in run <b>time</b> during the <b>compile</b> phase and spends most of the run phase in run time. Code in [...] blocks executes at run time but in the compile phase.|$|R
5000|$|A {{manifest}} {{expression is}} a <b>compile</b> <b>time</b> computable function which depends only on ...|$|R
5000|$|A type system {{enforcing}} null {{safety and}} list element existence at <b>compile</b> <b>time</b> ...|$|R
5000|$|A vastly {{improved}} argument-passing mechanism, allowing interfaces {{to be checked}} at <b>compile</b> <b>time</b> ...|$|R
5000|$|Functions, {{which are}} {{executed}} at <b>compile</b> <b>time,</b> {{can be used}} to define objects.|$|R
5000|$|Here's {{an example}} of <b>compile</b> <b>time</b> {{function}} evaluation in the D programming language: ...|$|R
