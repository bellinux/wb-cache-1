290|241|Public
5000|$|Geneformics, {{focused on}} {{computing}} for genomics (DNA sequencing). Geneformics aims to provide savings in the storage, communications and <b>cloud</b> <b>processing</b> of DNA sequencing.|$|E
50|$|CloudCompare is a 3D point <b>cloud</b> <b>processing</b> {{software}} (such {{as those}} obtained with a laser scanner). It can also handle triangular meshes and calibrated images.|$|E
50|$|Cloudgine is a Scottish {{software}} technology provider, based in Edinburgh. The company is mainly focused on providing <b>cloud</b> <b>processing</b> technologies for the entertainment and video game industry.|$|E
30|$|Several {{databases}} currently store sensitive data. Moreover, a {{vast number}} of sensors are constantly collecting new sensitive data and storing them in clouds. Privacy-preserving protocols are being designed and perfected to enhance user’s privacy in specific scenarios. Cultural interpretations of privacy, the variety of laws, big data from legacy systems in <b>clouds,</b> <b>processing</b> time, latency, key distribution and management, among other aforementioned are challenges for us to develop privacy-preserving protocols.|$|R
30|$|Chen Peng is {{currently}} a lecturer at School of Mechanical Engineering, Southwest Jiaotong University, China. His research interests include virtual design and manufacturing, human machine interaction, and point <b>cloud</b> data <b>processing.</b>|$|R
40|$|Abstract. For the {{construction}} of large-scale surface features 3 D point model, {{a large number of}} point <b>cloud</b> data <b>processing</b> calculations is needed. Previous model construction calculation was treated non-parallel manner successively and mostly with one by one point <b>cloud.</b> This data <b>processing</b> method is complex, low efficiency and requires vast computing resource. Accordance with the BSP parallel computing ideas, we design a point cloud data modeling algorithm based on BSP and build a Hama parallel computing cluster consisted of ordinary PCs. The results indicate that, large-scale 3 D point model BSP construction algorithm can improve the efficiency of modeling calculations and reduce computing resources requirements for processing construction computing...|$|R
5000|$|<b>Cloud</b> <b>processing</b> of edge-captured {{images is}} not sustainable. The sheer amount of visual data {{generated}} cannot {{be transferred to}} the cloud. Bandwith is not sufficient and cloud servers cannot cope with it.|$|E
50|$|The Point Cloud Library (PCL) is an {{open-source}} {{library of}} algorithms for point <b>cloud</b> <b>processing</b> tasks and 3D geometry processing, such as occur in three-dimensional computer vision. The library contains algorithms for feature estimation, surface reconstruction, registration, model fitting, and segmentation. It {{is written in}} C++ and released under the BSD license.|$|E
50|$|For {{any other}} {{software}} {{that is used}} on sites, {{one of the greatest}} concerns is due to issues arising regarding the balance between user experience and privacy. It is understood that the developers of Project Naptha are doing their best in attempting to allow the processing on the client side (i.e., within the browser). However, as text selected by users for extraction from the image are being processed in the cloud. This means that in order to achieve higher translation accuracy, there is still a need to rely on greater <b>cloud</b> <b>processing</b> and hence compromising on privacy. There is a default setting which helps to strike a delicate balance between having all the functionality made available and respecting user privacy. By default, when users begin selecting a text, a secure HTTPS request is sent. This is only contains the URL of the specific image and nothing else - no User Tokens, no Website Information, no Cookies or analytics and the requests are not logged. The server responds with a list of existing translations and OCR languages that have been done. This allows you to recognize text from an image with much more accuracy than otherwise possible.|$|E
30|$|A {{representative}} set {{of existing}} work related to resource management on <b>clouds</b> for <b>processing</b> workflows is presented next. A workflow is usually modelled using a {{directed acyclic graph}} (DAG) where each node in the graph represents a task in the workflow and {{the edges of the}} graph represent the precedence relationships among the tasks.|$|R
40|$|Abstract- The <b>cloud</b> data <b>{{processing}}</b> is {{the vision}} of the data processing as value, in which cloud consumer possibly its data into the cloud environment over enjoying the first-class quality servers and the fast nets, into which applications store and into the services of a divided pool of configurable operational resources. The advantages of the <b>cloud</b> data <b>processing</b> technology of the cloud cover need self-service mode, everywhere net entrance, independent resources of the location, which combine, fast resource elasticity, wages per the use, which on price estimation- resides, the transfer of the risk, etc [...] . It is large flexibility and economic savings is motivated individuals and the enterprises to outsource to their local complex cloud data environment administration system in the cloud. The security of the data in the cloud is at present a very hot topic. Most enterprises, which still possess sensitive metadata, hesitate...|$|R
40|$|The {{vision of}} the Internet of Things is to allow {{currently}} unconnected physical objects {{to be connected to}} the internet. There will be an extremely large number of internet connected devices that will be much more than the number of human being in the world all producing data. These data will be collected and delivered to the <b>cloud</b> for <b>processing,</b> especially with a view of finding meaningful information to then take action. However, ideally the data needs to be analysed locally to increase privacy, give quick responses to people and to reduce use of network and storage resources. To tackle these problems, distributed data analytics can be proposed to collect and analyse the data either in the edge or fog devices. In this paper, we explore a hybrid approach which means that both innetwork level and <b>cloud</b> level <b>processing</b> should work together to build effective IoT data analytics in order to overcome their respective weaknesses and use their specific strengths. Specifically, we collected raw data locally and extracted features by applying data fusion techniques on the data on resource constrained devices to reduce the data and then send the extracted features to the <b>cloud</b> for <b>processing.</b> We evaluated the accuracy and data consumption over network and thus show that it is feasible to increase privacy and maintain accuracy while reducing data communication demands. Comment: Accepted to be published in the Proceedings of the 7 th ACM International Conference on the Internet of Things (IoT 2017...|$|R
3000|$|Note that we {{have made}} no claims with respect to {{resource}} consumption on the cloud providers. Thus, it may be possible for a malfeasor to waste the resources of a <b>Cloud</b> <b>Processing</b> provider by replaying process‐cp messages from the C&C node. This could be countered by having the <b>Cloud</b> <b>Processing</b> provider store all nonces N [...]...|$|E
40|$|The {{formation}} of iron (Fe) nanoparticles and increase in Fe reactivity in mineral dust during simulated <b>cloud</b> <b>processing</b> was investigated using high-resolution microscopy and chemical extraction methods. <b>Cloud</b> <b>processing</b> of dust was experimentally simulated via an alternation of acidic (pH 2) and circumneutral conditions (pH 5 - 6) over periods of 24 h each on presieved...|$|E
30|$|The <b>cloud</b> <b>processing</b> {{platform}} calculates {{the number}} of allocated PRB for new small cell using formula (4 b).|$|E
40|$|With {{the advent}} of smart grid and smart sensors as well as {{advanced}} communication and devices, {{a huge amount of}} data is produced. To this end, an appropriate method needs to be selected to collect and process such huge amount of data. Different researches have been conducted so far. Some researchers considered <b>cloud</b> computing, parallel <b>processing,</b> and distributed processing as appropriate methods. In this article, we aim to introduce and explain some techniques using cloud computing for collecting and processing grid data. Key words: smart grid, <b>cloud</b> computing, parallel <b>processing,</b> smart sensor...|$|R
40|$|International audienceWe propose an {{automatic}} and robust approach to detect, segment and classify urban objects from 3 D point <b>clouds.</b> <b>Processing</b> {{is carried out}} using elevation images {{and the result is}} reprojected onto the 3 D point cloud. First, the ground is segmented and objects are detected as discontinuities on the ground. Then, connected objects are segmented using a watershed approach. Finally, objects are classified using SVM with geometrical and contextual features. Our methodology is evaluated on databases from Ohio (USA) and Paris (France). In the former, our method detects 98 % of the objects, 78 % of them are correctly segmented and 82 % of the well-segmented objects are correctly classified. In the latter, our method leads to an improvement of about 15 % on the classification step with respect to previous works. Quantitative results prove that our method not only provides a good performance but is also faster than other works reported in the literature...|$|R
40|$|Terrestrial laser {{scanning}} (TLS) technology has high speed of data acquisition, {{large amount of}} point cloud, long distance of measuring. However, there are some disadvantages such as distance limitation in target detecting, hysteresis in point <b>clouds</b> <b>processing,</b> low automation and weaknesses of adapting long-distance topographic survey. In this case, we put forward a method on long-range targets detecting in big point clouds orientation. The method firstly searches point cloud rings that contain targets according to their engineering coordinate system. Then the detected rings are divided into sectors to detect targets {{in a very short}} time so as to obtain central coordinates of these targets. Finally, the position and orientation parameters of scanner are calculated and point clouds in scanner's own coordinate system(SOCS) are converted into engineering coordinate system. The method is able to be applied in ordinary computers for long distance topographic(the distance between scanner and targets ranges from 180 to 700 m) survey in mountainous areas with targets radius of 0. 162 m...|$|R
40|$|Airborne Laser Scanning (ALS), {{often called}} LiDAR (Light Detection And Raging), {{delivers}} a point cloud as a survey result. This point cloud consist of topographic surface data and coating elements (e. g. vegetation, buildings) {{is used to}} build a Digital Surface Model (DSM). The point <b>cloud</b> <b>processing</b> can be represented in following steps: OBTAINING DATA → PRE-PROCESSING → MAIN PROCESSING → VISUALISATION. Existing methods of LiDAR point <b>cloud</b> <b>processing,</b> that leads to generation of Digital Terrain Model, are based on filtration algorithms designed especially for this purpose. The main task of these algorithms is to separate data from topographic surfaces and coating elements. This paper presents a modification of LiDAR point <b>cloud</b> <b>processing</b> with implementation of data set reduction (optimisation) algorithm in the PRE-PROCESSING stage [Błaszczak 2006, Błaszczak, Kamiński 2007]. The main goal of the research {{was to investigate the}} reliability and the efficiency of LiDAR point <b>cloud</b> <b>processing</b> methodology with implemented optimisation algorithm. The results confirm that, proposed modification improves the filtration algorithm by minimising the time needed for processing. It provides an effective way to generate DTM without loosin...|$|E
3000|$|The C&C node {{maintains}} a log of any outstanding {{requests sent to}} <b>cloud</b> <b>processing</b> providers, and will reject any unsolicited responses [...]...|$|E
30|$|The <b>cloud</b> <b>processing</b> {{platform}} updates {{the interference}} graph of adjacent cells {{according to the}} report on the relationship of the new small cell’s neighbors.|$|E
40|$|In this paper, {{we present}} a {{procedure}} which makes available an accurate historic BIM (HBIM) in the <b>cloud.</b> Data <b>processing</b> is carried out with a NURBS-based strategy {{to reduce the size}} of the final HBIM derived from images and laser scans, providing an accurate and reliable 3 D model with limited memory occupation. This guarantees a remote access with PCs and mobile devices connected through a cloud service...|$|R
40|$|International {{audience}} 3 D building segmentation is {{an important}} research issue in the remote sensing community with relevant applications to urban modeling, cloud-to-cloud and cloud-to-model registration, 3 D cartography, virtual reality, cultural heritage documentation, among others. In this paper, we propose automatic, parametric and robust approaches to segment façades from 3 D point <b>clouds.</b> <b>Processing</b> is carried out using elevation images and 3 D decomposition, and the final result can be reprojected onto the 3 D point cloud for visualization or evaluation purposes. Our methods are based on geometrical and geodesic constraints. Parameters are related to urban and architectural constraints. Thus, they {{can be set up}} to manage façades of any height, length and elongation. We propose two methods based on façade marker extraction and a third method without markers based on the maximal elongation image. This work is developed in the framework of TerraMobilita project [1]. The performance of our methods is proved in our experiments on TerraMobilita databases using 2 D and 3 D ground truth annotations...|$|R
40|$|Abstract—We {{have entered}} the big data era, where massive data are {{generated}} each single day. Most of these new generated big data are images and videos. Besides the fast-increasing data size, the image and video processing algorithms become much more complex, which poses great demands to data storage and computation power. Our image <b>processing</b> <b>cloud</b> project aims to support the image processing research by leveraging the cloud computing and big data analysis technology. In this paper, we present our design for image <b>processing</b> <b>cloud</b> architecture, and big data processing engine based on Hadoop. We also report the performance scalability and analysis on the cloud using several widely used image processing algorithms...|$|R
30|$|According to {{the formula}} (4 b), the <b>cloud</b> <b>processing</b> {{platform}} calculates {{the number of}} allocated PRBs for each small cell in the set of S^'.|$|E
40|$|Airborne {{data are}} {{presented}} {{on the impact}} of <b>cloud</b> <b>processing</b> on the aerosol mass light-scattering efficiency. The measurements, on marine stratocumulus, suggest that <b>cloud</b> <b>processing</b> significantly enhanced the mass light-scattering efficiency in three of the five cases analysed. Enhancements were of the order of 10 % for air detraining from the cloud deck relative to non-detraining air. A diagnostic modelling analysis suggested that the observed enhancements were consistent with the previously proposed explanation of in-cloud sulfate production in the particle size range for efficient light scattering. 1...|$|E
30|$|Since data is sent {{encrypted}} {{from the}} C&C node to the <b>Cloud</b> <b>Processing</b> provider, it cannot be observed by an adversary, who cannot determine the symmetric key used to encrypt the response, and thus cannot do a suppress-replay attack {{to replace the}} result with a bogus result. We are assuming the C&C node has verified public keys to the providers, which means only the selected provider sees the data, {{but as long as}} the relationship between user and data is kept secret, it does not really matter exactly which <b>Cloud</b> <b>Processing</b> provider handles the data.|$|E
40|$|Abstract: Current cloud {{computing}} systems continuously {{specialist in the}} high information measure native space network setting, for instance, Associate in nursing local area network of a company. This paper uses mobile agent to implement the software package and knowledge service for cloud user in web setting, and create the <b>cloud</b> automatic data <b>processing</b> system all-mains to figure in web setting, like a global corporation with branches everywhere the planet. The works during this paper includes 3 parts: (1) introducing mobile agent into <b>cloud</b> automatic data <b>processing</b> system Associate in Nursing presenting the mobile agent primarily based service for {{cloud computing}} system: Service as an Agent Service). The SaaAS uses mobile agents because the underlying facility to supply the service for user; (2) presenting a high performance code and knowledge of service load mechanism primarily based mobile agent for SaaAS, which might effectively cut back the serious communication overhead in Internet; (3) presenting a unique knowledge coherence mechanism for SaaS: Divided-Cloud and focused Coherence Mechanism (DC CM). The applying of mobile agent permits SaaS to be additional appropriate to figure in web setting than typical <b>cloud</b> automatic data <b>processing</b> system...|$|R
40|$|To be useful, the {{millions}} or even billions of 3 D points {{generated by a}} variety of active and passive sensors need to be stored, organised, combined, geo-referenced, measured, analysed and distributed within organisations or outward. Initially, the data is unorganised; software has been designed, developed and put on the market place to organise the unorganised and to extract information from the point clouds. In principle, the curved 2 D surface can represent any instance such as soil pollution, forest biomass, rainfall, terrain elevation or the seabed. In the field of geomatics, the phenomenon will usually be the terrain surface or the seabed. This paper focuses on point clouds from which DEMs or DSMs can be generated, stemming from sources including airborne Lidar; Terrestrial Laser Scanning (TLS); airborne and spaceborne Radar; close-range, airborne and spaceborne imagery; and sonar. This paper presents first the main features of point clouds and focusses next on the diverse functionalities of point <b>clouds</b> <b>processing</b> software presently on the market, including : data storage; geo-referencing; the filtering aspect of point-cloud creation; interpolation; and visualisation and editing. OTB ResearchArchitecture and The Built Environmen...|$|R
30|$|The {{purpose was}} {{to find out how}} fast tweets can be {{streamed}} into the <b>cloud</b> platform without <b>processing</b> of tweets. AsterixDB’s REST API and data feeds were experimented for data ingestion. Respectively, loading of tweets was implemented to Spark with TCP sockets.|$|R
40|$|This paper {{addresses}} {{the impact of}} aerosol <b>cloud</b> <b>processing</b> on the Angstrom expo-nent. It is a timely paper, as there is much research associated with the microphysics of aerosols in and around clouds, and our ability to detect aerosol-cloud interactions via remote sensing. Although there are many papers relating aerosol microphysics to <b>cloud</b> <b>processing</b> using cloud parcel models, I have not seen any that also relate the aerosol-cloud dynamic to the Angstrom exponent. This is important, as the Angstrom exponent {{is something that we}} can retrieve with satellite measurements [...] ...|$|E
40|$|Tellus 56 B 285 - 293. Airborne {{data are}} {{presented}} {{on the impact}} of <b>cloud</b> <b>processing</b> on the aerosol mass light-scattering efficiency. The measurements, on marine stratocumulus, suggest that <b>cloud</b> <b>processing</b> significantly enhanced the mass light-scattering efficiency in three of the five cases analysed. Enhancements were of the order of 10 % for air detraining from the cloud deck relative to non-detraining air. A diagnostic modelling analysis suggested that the observed enhancements were consistent with the previously proposed explanation of in-cloud sulfate production in the particle size range for efficient light scattering...|$|E
30|$|Each {{small cell}} scans the field {{strength}} and forms an interference diagram of its neighbor cells. Meanwhile, the small cell reports the interference diagram to the <b>cloud</b> <b>processing</b> platform.|$|E
40|$|A Nimbus- 7 <b>Cloud</b> Data <b>Processing</b> Team was {{established}} in 1982 in order to implement cloud-related studies {{for as long as}} the spacecraft's Temperature Humidity IR radiometer and Total Ozone Mapping Spectrometer continue to operate. It will soon be possible to correlate the Nimbus- 7 cloud cover information with International Satellite Cloud Climatology results. The production of validated Nimbus- 7 cloud products was scheduled to begin in November, 1983; each year of Nimbus- 7 cloud data should take about four months to produce...|$|R
40|$|Our work aims {{to explore}} {{interoperability}} of large-scale <b>cloud</b> data <b>processing</b> software in HPC environments, and involves clustering and 3 D visualization of gene sequence collections. We completed visualizing samples of 100 K – 400 K fungal sequences. This research involves major supercomputer computation as both clustering and visualization steps scale, even {{up to the}} square of the sample size. Our experiments were conducted on Indiana University’s Big Red II [1] supercomputer and in collaboration with bioinformatics and biology faculty at IU. communication by transforming map-reduc...|$|R
40|$|Abstract- Cloud {{computing}} {{has come}} {{out to be an}} interesting and beneficial way of changing the whole computing world. The traditional way for task scheduling cannot meet the cloud market well enough. This paper introduces an optimized algorithm for cost based scheduling in cloud computing and its implementation. This scheduling algorithm measures both resource cost and calculation performance, it also improves the calculation/communication ratio by grouping the user tasks according to a particular <b>cloud</b> resource’s <b>processing</b> capability and sends the grouped jobs to the resource...|$|R
