7245|214|Public
25|$|<b>Combinatorial</b> <b>optimization</b> is {{concerned}} with problems where the set of feasible solutions is discrete or {{can be reduced to}} a discrete one.|$|E
25|$|TSP is a touchstone {{for many}} general {{heuristics}} devised for <b>combinatorial</b> <b>optimization</b> such as genetic algorithms, simulated annealing, Tabu search, ant colony optimization, river formation dynamics (see swarm intelligence) and the cross entropy method.|$|E
25|$|Ellipsoid method: An {{iterative}} method for small problems with quasiconvex objective functions and of great theoretical interest, particularly {{in establishing the}} polynomial time complexity of some <b>combinatorial</b> <b>optimization</b> problems. It has similarities with Quasi-Newton methods.|$|E
40|$|This paper proposes an {{algorithm}} for <b>combinatorial</b> <b>optimizations</b> {{that uses}} reinforcement learning and estimation of joint probability distribution of promising solutions {{to generate a}} new population of solutions. We call it Reinforcement Learning Estimation of Distribution Algorithm (RELEDA). For the estimation of the joint probability distribution we consider each variable as univariate. Then we update the probability of each variable by applying reinforcement learning method...|$|R
40|$|We {{present a}} hyper-heuristic {{algorithm}} for solving <b>combinatorial</b> black-box <b>optimization</b> problems. The algorithm named CMA-VNS {{stands for a}} hybrid of variants of Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and Variable Neighborhood Search (VNS). The framework design and the design profiles of variants of CMA-VNS are introduced to enhance the intensification of searching for conventional CMA-ES solvers. We explain the parameter configuration details, the heuristic profile selection, and the rationale of incorporating machine learning methods during the study. Experimental tests {{and the results of}} the first and the second <b>Combinatorial</b> Black-Box <b>Optimization</b> Competitions (CB-BOC 2015, 2016) confirmed that CMA-VNS is a competitive hyper-heuristic algorithm...|$|R
50|$|Mallba is {{a library}} for <b>combinatorial</b> <b>optimizations</b> {{supporting}} exact, heuristic and hybrid search strategies. Each strategy is implemented in Mallba as a generic skeleton {{which can be}} used by providing the required code. On the exact search algorithms Mallba provides branch-and-bound and dynamic-optimization skeletons. For local search heuristics Mallba supports: hill climbing, metropolis, simulated annealing, and tabu search; and also population based heuristics derived from evolutionary algorithms such as genetic algorithms, evolution strategy, and others (CHC). The hybrid skeletons combine strategies, such as: GASA, a mixture of genetic algorithm andsimulated annealing, and CHCCES which combines CHC and ES.|$|R
25|$|The Research Institute for Discrete Mathematics {{focuses on}} {{discrete}} mathematics and its applications, in particular <b>combinatorial</b> <b>optimization</b> {{and the design}} of computer chips. The institute cooperates with IBM and Magma Design Automation. Researchers of the institute optimized the chess computer IBM Deep Blue.|$|E
25|$|Ant colony {{optimization}} algorithms {{have been}} applied to many <b>combinatorial</b> <b>optimization</b> problems, ranging from quadratic assignment to protein folding or routing vehicles {{and a lot of}} derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations.|$|E
25|$|Matroid theory borrows {{extensively}} {{from the}} terminology of linear algebra and graph theory, largely {{because it is}} the abstraction of various notions of central importance in these fields. Matroids have found applications in geometry, topology, <b>combinatorial</b> <b>optimization,</b> network theory and coding theory.|$|E
40|$|Abstract. This paper proposes an {{algorithm}} for <b>combinatorial</b> <b>optimizations</b> {{that uses}} reinforcement learning and estimation of joint probability distribution of promising solutions {{to generate a}} new population of solutions. We call it Reinforcement Learning Estimation of Distribution Algorithm (RELEDA). For the estimation of the joint probability distribution we consider each variable as univariate. Then we update the probability of each variable by applying reinforcement learning method. Though we consider variables independent of one another, the proposed method can solve problems of highly correlated variables. To compare the efficiency of our proposed algorithm with other Estimation of Distribution Algorithms (EDAs) we provide the experimental results of the two problems: four peaks problem and bipolar function. ...|$|R
40|$|Estimation of Distribution Algorithms (EDAs) is a {{new area}} of Evolutionary Computation. In EDAs there is neither {{crossover}} nor mutation operators. New population is generated by sampling the probability distribution, which is estimated from a database containing selected individuals of the previous generation. Different approaches have been proposed for the estimation of probability distribution. In this paper we provide a review of different EDA approaches and show how to apply UMDA with Laplace correction to Subset Sum, OneMax function and n-Queen problems of linear and <b>combinatorial</b> <b>optimizations.</b> The experimental results of the three problems comparing the performance of UMDA with that of Genetic Algorithm(GA) are provided. In our experiment UMDA outperforms GA for linear problems...|$|R
40|$|The CMA-VNS (Covariance Matrix Adaptation Variable Neighborhood Search) solver is a {{competitor}} {{for the first}} <b>Combinatorial</b> Black-Box <b>Optimization</b> Competition (CBBOC 20151). CMA-VNS is a hyper-heuristic which employs a CMA-ES (Covariance Matrix Adaptation Evolution Strategy) [Hansen et al., 2003] evolution which is followed by a...|$|R
25|$|<b>Combinatorial</b> <b>{{optimization}}</b> is {{the study}} of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research, algorithm theory and computational complexity theory.|$|E
25|$|At the {{beginning}} of the 1970s, it was observed that a large class of <b>combinatorial</b> <b>optimization</b> problems defined on graphs could be efficiently solved by non-serial dynamic programming as long as the graph had a bounded dimension, a parameter related to treewidth. Later, several authors independently observed, {{at the end of the}} 1980s, that many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently by dynamic programming for graphs of bounded treewidth, using the tree-decompositions of these graphs.|$|E
25|$|Integral linear {{programs}} are of central {{importance in the}} polyhedral aspect of <b>combinatorial</b> <b>optimization</b> since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.|$|E
40|$|Abstract—The {{problems}} with high complexity {{had been the}} challenge in combinatorial problems. Due to the none-determined and polynomial characteristics, these problems usually face to unreasonable searching budget. Hence <b>combinatorial</b> <b>optimizations</b> attracted numerous researchers to develop better algorithms. In recent academic researches, most focus on developing to enhance the conventional evolutional algorithms and facilitate the local heuristics, such as VNS, 2 -opt and 3 -opt. Despite the performances of {{the introduction of the}} local strategies are significant, however, these improvement cannot improve the performance for solving the different problems. Therefore, this research proposes a meta-heuristic evolutional algorithm which can be applied to solve several types of problems. The performance validates BBEA has the ability to solve the problems even without the design of local strategies...|$|R
40|$|We {{study the}} problem of satisfiability of {{randomly}} chosen clauses, each with K Boolean variables. Using the cavity method at zero temperature, we find the phase diagram for the K= 3 case. We show {{the existence of an}} intermediate phase in the satisfiable region, where the proliferation of metastable states is at the origin of the slowdown of search algorithms. The fundamental order parameter introduced in the cavity method, which consists of surveys of local magnetic fields in the various possible states of the system, can be computed for one given sample. These surveys can be used to invent new types of algorithms for solving hard <b>combinatorial</b> <b>optimizations</b> problems. One such algorithm is shown here for the 3 -sat problem, with very good performances. Comment: 38 pages, 13 figures; corrected typo...|$|R
40|$|The present article {{takes up}} some of the author’s {{research}} activities in the field of <b>combinatorial</b> and discrete <b>optimization</b> from 1975 till quite recently. It treats not only what the author has done but also what he still wants to do. They are written together with personal reminiscence and with the hope that this article will convey to researchers of younger generation the author’s enthusiasm in <b>combinatorial</b> and discrete <b>optimization</b> on the frontier of research in the past till recently...|$|R
25|$|The {{intersection}} {{of two or}} more matroids is the family of sets that are simultaneously independent in each of the matroids. The problem of finding the largest set, or the maximum weighted set, in the {{intersection of}} two matroids can be found in polynomial time, and provides a solution to many other important <b>combinatorial</b> <b>optimization</b> problems. For instance, maximum matching in bipartite graphs can be expressed as a problem of intersecting two partition matroids. However, finding the largest set in an intersection of three or more matroids is NP-complete.|$|E
25|$|A {{computationally}} hard problem, {{which is}} key for some relevant machine learning tasks, is {{the estimation of}} averages over probabilistic models {{defined in terms of}} a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging <b>combinatorial</b> <b>optimization</b> problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.|$|E
25|$|The D-Wave 2X system hosted at NASA Ames Research Center {{has been}} {{recently}} {{used for the}} learning of a special class of restricted Boltzmann machines that {{can serve as a}} building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for <b>combinatorial</b> <b>optimization</b> suggests the answer is not straightforward.|$|E
40|$|Designing {{distributed}} algorithms for optimizing system-wide {{performances of}} large scale communication networks is a challenging task. The {{key part of}} this design involves a lot of <b>combinatorial</b> network <b>optimization</b> problems, which are computationally intractable in general and hard to approximate even in a centralized manner. Inspired by the seminal work of Jiang-Walrand, Markov approximation framework was proposed for synthesizing distributed algorithms for general <b>combinatorial</b> network <b>optimization</b> problems. To provide performance guarantees, convergence properties of these distributed algorithms are of significance. In this thesis, we first review Markov approximation framework and further develop this framework by studying convergence properties of distributed algorithms. These system-wide algorithms consist of the designed Markov chain and resource allocation algorithms. We concentrate on two general scenarios: the designed Markov chain over resource allocation algorithms and resource allocation algorithms over the designed Markov chain. With imprecise measurement...|$|R
40|$|We present Moolloy, {{a general}} purpose, spreadsheet-like user {{interface}} for <b>combinatorial</b> multi-objective <b>optimization.</b> Current user interfaces for multi-objective optimization tend to either require some programming experience to use, or are narrowly focused on specific problems. Consequently, multi-objective optimization is usually only used by sophisticated technical workers, such as aerospace engineers. Moolloy makes multi-objective optimization accessible {{to a larger}} set of potential users. ...|$|R
5000|$|Markov random fields find {{application}} {{in a variety}} of fields, ranging from computer graphics to computer vision and machine learning. MRFs are used in image processing to generate textures as they can be used to generate flexible and stochastic image models. In image modelling, the task is to find a suitable intensity distribution of a given image, where suitability depends on the kind of task and MRFs are flexible enough to be used for image and texture synthesis, image compression and restoration, image segmentation, surface reconstruction, image registration, texture synthesis, super-resolution, stereo matching and information retrieval. They can be used to solve various computer vision problems which can be posed as energy minimization problems or problems where different regions have to be distinguished using a set of discriminating features, within a Markov random field framework, to predict the category of the region. [...] Markov random fields were a generalization over the Ising model and have, since then, been used widely in <b>combinatorial</b> <b>optimizations</b> and networks.|$|R
2500|$|Workshop on Approximation Algorithms for <b>Combinatorial</b> <b>Optimization</b> Problems (APPROX) ...|$|E
2500|$|... Reprinted in <b>Combinatorial</b> <b>Optimization</b> — Eureka, You Shrink!, Springer-Verlag, Lecture Notes in Computer Science 2570, 2003, pp.27–30, [...]|$|E
2500|$|Christos H. Papadimitriou and Kenneth Steiglitz <b>Combinatorial</b> <b>Optimization</b> : Algorithms and Complexity; Dover Pubns; (paperback, Unabridged edition, July 1998) [...]|$|E
40|$|In this article, {{constrained}} {{continuous and}} <b>combinatorial</b> vector <b>optimization</b> problems (VOPs) are {{considered in the}} setting of finite-dimensional Euclidean spaces. Equivalence results between constrained integer and continuous VOPs are established by virtue of that between a constrained VOP and its penalized problem. Finally, one of the established equivalences is applied to derive necessary optimality conditions for a constrained integer VOP. Department of Applied Mathematic...|$|R
30|$|In the literature, several {{randomized}} optimization algorithms {{based on}} the CE method have been proposed and {{have been shown to}} lead to good performances on numerous optimization problems [17 – 19], often outperforming other randomized algorithms [18]. Motivated by the effectiveness of the CE method for finding near-optimal solutions in huge search spaces, this article adopts the CE-based method to search for the near-optimal position of the pilot symbols with lower least square (LS) channel estimate MSE. The CE method is an iterative procedure for evaluation of rare event probabilities and also for <b>combinatorial</b> <b>optimizations.</b> Each iteration involves generating a random sample according to a probability distribution and then updating the parameters of the probability distribution in order to produce better samples in the next iteration. While applications of the CE-based methods to various disciplines of engineering have already been reported (see, e.g. [19 – 22]), {{to the best of our}} knowledge, they have not been employed yet to pilot symbol designs for channel estimation in conventional as well as NC-OFDM.|$|R
40|$|AbstractThis paper {{considers}} the well-known Quadratic Assignment Problem (QAP) for the study. It is NP-hard <b>combinatorial</b> <b>optimizations</b> {{that can be}} defined as follows. There is n facilities and n locations. A distance is specified for each pair of locations, and a flow is specifiedfor each pair of facilities. The objective of problem is to allocate all facilities to different locations such that the sum of the flows multiplied by the corresponding distances is minimized. We develop a data-guided lexisearch algorithm based on an existing reformulation to find exact solution to the problem. For this we first modify alphabet table according to the number of zeros in the rows of the surplus matrix, thus, renaming rows (facilities), and then we apply lexisearch algorithm. It is shown that before applying lexisearch algorithm, this minor preprocessing of the data improves computational time significantly. Finally, we present a comparative study between data-guided lexisearch algorithm and two existing algorithms on some QAPLIB instances of various sizes. The computational study shows the effectiveness of our proposed data-guided lexisearch algorithm...|$|R
2500|$|<b>Combinatorial</b> <b>optimization</b> {{is a good}} {{strategy}} to solve MSA problem. The idea of <b>combinatorial</b> <b>optimization</b> strategy is to transform the multiple sequence alignment into pair sequence alignment to solve this problem. Depending on its transformation strategy, the <b>combinatorial</b> <b>optimization</b> strategy {{can be divided into}} the tree alignment algorithm and the star alignment algorithm. For a given multi sequences set [...] ={,…, }, finding an evolutionary tree which has n leaf nodes and establishing one to one relationship between this evolutionary tree and the set S. By assigning the sequence to the internal nodes of the evolutionary tree, we calculate the total score of each edge and the sum of all edges’ score is the score of the evolutionary tree. The aim of tree alignment is to find an assigned sequence, which can obtain a maximum score, and get the final matching result by the evolutionary tree and its nodes’ assigned sequence.|$|E
2500|$|The {{travelling}} salesman problem [...] (TSP) asks the following question: [...] "Given {{a list of}} cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once {{and returns to the}} origin city?" [...] It is an NP-hard problem in <b>combinatorial</b> <b>optimization,</b> important in operations research and theoretical computer science.|$|E
2500|$|At the {{beginning}} of the 1970s, it was observed that a large class of <b>combinatorial</b> <b>optimization</b> problems defined on graphs could be efficiently solved by non serial dynamic programming as long as the graph had a bounded dimension, a parameter shown to be equivalent to treewidth by [...] Later, several authors independently observed {{at the end of the}} 1980s that many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently by dynamic programming for graphs of bounded treewidth, using the tree-decompositions of these graphs.|$|E
40|$|Brandão and Svore very {{recently}} gave quantum algorithms for approximately solving semidefinite programs, {{which in some}} regimes are faster than the best-possible classical algorithms {{in terms of the}} dimension n of the problem and the number m of constraints, but worse in terms of various other parameters. In this paper we improve their algorithms in several ways, getting better dependence on those other parameters. To this end we develop new techniques for quantum algorithms, for instance a general way to efficiently implement smooth functions of sparse Hamiltonians, and a generalized minimum-finding procedure. We also show limits on this approach to quantum SDP-solvers, for instance for <b>combinatorial</b> <b>optimizations</b> problems that have a lot of symmetry. Finally, we prove some general lower bounds showing that in the worst case, the complexity of every quantum LP-solver (and hence also SDP-solver) has to scale linearly with mn when m≈ n, which is the same as classical. Comment: v 2 : 74 pages, small clarifications. A 12 -page version will appear in the Proceedings of IEEE FOCS 201...|$|R
40|$|<b>Combinatorial</b> Multimodal <b>Optimization</b> Problems (CMOP) arising in the {{scheduling}} {{of manufacturing}} systems involve {{the determination of}} multiple integer solution vectors that optimize a given objective function with regard to some definite constraints. The genetic algorithm is an effective computational paradigm to search such a large optimization space for the best solutions. But simple genetic algorithms are notorious for their "premature convergence" to a unimodal (global or local) optimum because of genetic drift. Following the previous discussion in Part 1, a new Intelligent Genetic Algorithm is developed to include spatial structured population, relative fitness vector, absolute fitness value with dynamic fitness sharing function, super conservative selection strategy, intelligent recombination through speciation, optimal outbreeding and neural mutation. Experimental results and simple fitness landscape analysis illustrate that intelligent genetic algorithms can effectively solve a typical <b>combinatorial</b> multimodal <b>optimization</b> problem, which is a challenging problem for GA applications. These advanced intelligent genetic algorithms present a prospective arena for multi-objective optimization [Fonesca and Fleming., 1995 a,b; Shaw and Fleming, 1996]and multimodal optimization problems in evolutionary computation...|$|R
40|$|In this work, an {{alternative}} plant-wide control design approach based on oversizing analysis is presented. The overall strategy {{can be divided}} in two main sequential tasks: 1 - defining the optimal decentralized control structure, and 2 - setting the controller interaction degree and its implementation. Both problems represent <b>combinatorial</b> <b>optimizations</b> based on multi-objective functional costs and were solved efficiently by genetic algorithms. The first task defines the optimal selection of controlled and manipulated variables simultaneously, the input-output pairing, and the overall controller dimension in a sum of square deviations context. The second task analyzes the potential improvements by defining the controller interaction degree via the net load evaluation approach. In addition, some insights are given about the feasibility (implementation load) of these control structures for a decentralized or centralized framework. The well-known Tennessee Eastman (TE) process is selected here for sake of comparison with other multivariable control designs. Fil: Zumoffen, David Alejandro Ramon. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Rosario. Centro Internacional Franco Argentino de Ciencias de la Información y Sistemas; Argentin...|$|R
