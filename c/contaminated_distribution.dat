12|114|Public
3000|$|..., and Iε,i are all independent. Hence, this <b>contaminated</b> <b>distribution</b> is skewed {{with heavy}} right tails. The design is {{slightly}} unbalanced with n 1 = 45 and n 2 = 55. Without loss of generality β,θ, and β 0 {{were set to}} 0.|$|E
40|$|In this article, we {{consider}} a rank test for a randomized block design when the responses {{are from a}} <b>contaminated</b> <b>distribution.</b> A test statistic is proposed and the asymptotic distributions of the proposed test statistic under the null and local alternative are derived. A small sample simulation study is performed. Rank tests Friedman test...|$|E
40|$|Copyright © 2014 ISSR Journals. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: Today distribution method and atmospheric contamination emission muddling is vulnerable in city region because industrial units grow in underdeveloped countries and contaminated produced by this units. Atmospheric <b>contaminated</b> <b>distribution</b> muddling is method for estimate concentration level and contaminated concentration in different interval relative to emission source. This practice is restricted to step after events occurring without proper administering these programs and we have more timely and monetary cost. In this paper investigate concentration distribution study from 2 dimensional and non-permanent states in land surface. If there is barrier in front of chimney,there is more contaminated gas distribution after chimney and maximum contaminated density is less in earth. When there are 3 chimney, There is no effect on contaminated density maximum distance. But in situation there is barrier in front of chimney, this distance is 1000 meter next to chimney. Earth atmosphere attack all kinds of contaminated due to industrial grow development and citizen grow. Modeling and investigating this <b>contaminated</b> <b>distribution</b> in environment surface uneven and kind of cover has basic role in contaminating distribution method because inhalator this contaminated in soil and ware have non-compensate damage t...|$|E
3000|$|It is {{traditional}} {{for studies on}} outliers to use <b>contaminated</b> normal <b>distributions</b> (see Wellmann and Gather 1999 and Jain 1981). However, {{it is important to}} bear in mind that using <b>contaminated</b> normal <b>distributions</b> in order to model and explain real data is an inadvisable practice and instead distributions with smooth changes in skewness are recommended (Gleason 1993). In the current study this recommendation was followed by using outlier-prone distributions whose shape was manipulated by stepwise control of their parameters. Having said that, studying the Ueda’s method when dealing with artificially <b>contaminated</b> <b>distributions</b> could help reveal how the method performs in regards to outliers, inliers and masking effects. For example, a recursive outlier detection method known as KUR tends to detect inliers as outliers in one-sided <b>contaminated</b> <b>distributions</b> (Jain 1981). The example shown in Fig. 2 [...]...|$|R
30|$|The {{generation}} of outliers considers {{a certain percentage}} of samples {{to be replaced by a}} new sample which is drawn from a <b>contaminating</b> <b>distribution</b> (Gaussian or chi-square). The error rate is calculated based on the classified samples excluding any outliers. The displayed results represent the averages that are based on 100 Monte-Carlo runs.|$|R
40|$|Abstract. As a very {{important}} <b>distribution,</b> <b>contaminated</b> normal <b>distribution</b> play a great role in data processing. The probability density function (PDF) feature of the <b>contaminated</b> normal <b>distribution</b> was investigated. The Kullback-Leibler distance is suggested for measuring PDF difference between mean shift model and variance inflation model. Numerical calculations show that the PDF difference of two kinds of model is related to mean shift parameter λ and the variance inflation factor α closely when the main distribution is the standard normal distribution and the relationship is nonlinear proportional...|$|R
40|$|The {{most popular}} {{estimation}} methods in multivariate linear regression are the multivariate least squares estimation and the multivariate least absolute estimation. Each method repeats its univariate estimation method p, {{the number of}} response variables, times. Although they are relatively easy to apply, they do not employ the relationship between response variables. This study considers the multivariate least distance estimator ofÂ Bai etÂ al. (1990) that accounts for this relationship. We confirm its relative efficiency {{with respect to the}} multivariate least absolute estimator under the multivariate normal distribution and <b>contaminated</b> <b>distribution.</b> However, the asymptotic inference of the multivariate least distance estimator is shown to perform poorly in certain circumstances. We suggest the bootstrap method to infer the regression parameters and confirm its viability using Monte Carlo studies. ...|$|E
40|$|This {{study is}} to {{determine}} the concentrations of sixteen poly aromatic hydrocarbons in eighteen pooled samples of fish, shrimp, crab and bivalve from markets of Hormozgan province, Iran. The poly aromatic hydrocarbon levels varied from 16 ± 8. 4 to 28. 18 ± 3. 74 ng/g wet weight. The investigated samples were classified as minimally <b>contaminated.</b> <b>Distribution</b> patterns showed that PAHs with 4, 5 and 6 rings dominated, confirming the pyrogenic source of detected PAHs. Fish contributed more than other biota groups in transforming of PAHs to Hormozgan Province people. The average of B (a) Peq) values for the studied biota was 2. 71 ± 2. 28 ng/g that was greater than calculated local screen value. This finding was implemented in poor quality of studied biota and necessity for risk management...|$|E
40|$|Abstract—This {{study is}} to {{determine}} the concentrations of sixteen poly aromatic hydrocarbons in eighteen pooled samples of fish, shrimp, crab and bivalve from markets of Hormozgan province, Iran. The poly aromatic hydrocarbon levels varied from 16 ± 8. 4 to 28. 18 ± 3. 74 ng/g wet weight. The investigated samples were classified as minimally <b>contaminated.</b> <b>Distribution</b> patterns showed that PAHs with 4, 5 and 6 rings dominated, confirming the pyrogenic source of detected PAHs. Fish contributed more than other biota groups in transforming of PAHs to Hormozgan Province people. The average of B (a) Peq) values for the studied biota was 2. 71 ± 2. 28 ng/g that was greater than calculated local screen value. This finding was implemented in poor quality of studied biota and necessity for risk management. Index Terms—Hormozgan Province, PAH, risk assessment...|$|E
40|$|Data {{heterogeneity}} appears {{when the}} sample comes {{from at least}} two different populations. We analyze three types of situations. In the first and simplest case {{the majority of the}} data come from a central model and a few isolated observations come from a <b>contaminating</b> <b>distribution.</b> The data from the <b>contaminating</b> <b>distribution</b> are called outliers and they have been studied in depth in the statistical literature. In the second case we still have a central model but the heterogeneous data may appear in clusters of outliers which mask each other. This is the multiple outlier problem which is much more difficult to handle and it has been analyzed and understood in the last few years. The few Bayesian contributions to this problem are presented. In the third case {{we do not have a}} central model but instead different groups of data have been generated by different models. For multivariate normal this problem has been analyzed by mixture models under the name of cluster analysis, but a challengi [...] ...|$|R
40|$|Measurements of the {{equation}} of state of dark energy from surveys of thousands of Type Ia Supernovae (SNe Ia) will be limited by spectroscopic follow-up and must therefore rely on photometric identification, increasing the chance that the sample is contaminated by Core Collapse Supernovae (CC SNe). Bayesian methods for supernova cosmology can remove contamination bias while maintaining high statistical precision but {{are sensitive to the}} choice of parameterization of the <b>contaminating</b> distance <b>distribution.</b> We use simulations to investigate the form of the <b>contaminating</b> <b>distribution</b> and its dependence on the absolute magnitudes, light curve shapes, colors, extinction, and redshifts of core collapse supernovae. We find that the CC luminosity function dominates the distance distribution function, but its shape is increasingly distorted as the redshift increases and more CC SNe fall below the survey magnitude limit. The shapes and colors of the CC light curves generally shift the distance distribution, and their effect on the CC distances is correlated. We compare the simulated distances to the first year results of the SDSS-II SN survey and find that the SDSS distance distributions can be reproduced with simulated CC SNe that are ~ 1 mag fainter than the standard Richardson et al. (2002) luminosity functions, which do not produce a good fit. To exploit the full power of the Bayesian parameter estimation method, parameterization of the <b>contaminating</b> <b>distribution</b> should be guided by the current knowledge of the CC luminosity functions, coupled with the effects of the survey selection and magnitude-limit, and allow for systematic shifts caused by the parameters of the distance fit. Comment: 17 pages, 5 figures; accepted for publication in the Astrophysical Journa...|$|R
40|$|We {{investigate}} {{the reliability of}} estimated income poverty profiles for Albanian survey data. We find evidence {{that a significant number}} of households with low reported incomes have relatively high living standards and are consequently misclassified as poor. We extend the theory of <b>contaminated</b> <b>distributions</b> to incorporate direct measures of well-being as indicators of data contamination, and develop a new nonparametric approach for constructing bounds on conditional poverty rates. We find very large upward biases in measured income poverty under the assumption of independence between living standards and the misreporting propensity, but a wide range of uncertainty under more general conditions...|$|R
40|$|Abstract: Today {{distribution}} method and atmospheric contamination emission muddling is vulnerable in city region because industrial units grow in underdeveloped countries and contaminated produced by this units. Atmospheric <b>contaminated</b> <b>distribution</b> muddling is method for estimate concentration level and contaminated concentration in different interval relative to emission source. This practice {{is restricted to}} step after events occurring without proper administering these programs and we have more timely and monetary cost. In this paper investigate concentration distribution study from 2 dimensional and non-permanent states in land surface. If there is barrier in front of chimney, there is more contaminated gas distribution after chimney and maximum contaminated density is less in earth. When there are 3 chimney, There is no effect on contaminated density maximum distance. But in situation there is barrier in front of chimney, this distance is 1000 meter next to chimney...|$|E
40|$|Contamination of a sampled distribution, {{for example}} by a {{heavy-tailed}} distribution, can degrade {{the performance of}} a statistical estimator. We suggest a general approach to alleviating this problem, using a version of the weighted bootstrap. The idea is to "tilt" away from the <b>contaminated</b> <b>distribution</b> by a given (but arbitrary) amount, in a direction that minimises a measure of the new distribution's dispersion. This theoretical proposal has a simple empirical version, which results in each data value being assigned a weight according to an assessment of its influence on dispersion. Importantly, distance can be measured directly in terms of the likely level of contamination, without reference to an empirical measure of scale. This makes the procedure particularly attractive for use in multivariate problems. It has a number of forms, depending on the definitions taken for dispersion and for distance between distributions. Examples of dispersion measures include variance, and generalis [...] ...|$|E
40|$|We use the Maximum q-log-likelihood {{estimation}} for Least informative distributions (LID) {{in order}} to estimate the parameters in probability density functions (PDFs) efficiently and robustly when data include outlier(s). LIDs are derived by using convex combinations of two PDFs, f_ϵ=(1 -ϵ) f_ 0 +ϵ f_ 1. A convex combination of two PDFs is considered as a contamination f_ 1 as outlier(s) to underlying f_ 0 distributions and f_ϵ is a <b>contaminated</b> <b>distribution.</b> The optimal criterion is obtained by minimizing the change of Maximum q-log-likelihood function when the data have slightly more contamination. In this paper, we make a comparison among ordinary Maximum likelihood, Maximum q-likelihood estimations, LIDs based on _q and Huber M-estimation. Akaike and Bayesian information criterions (AIC and BIC) based on _q and LID are proposed to assess the fitting performance of functions. Real data sets are applied to test the fitting performance of estimating functions that include shape, scale and location parameters. Comment: 16 pages; 12 Figure...|$|E
40|$|Abstract. New {{algorithm}} for {{estimation of}} parameters of communication channel {{in the circumstances}} of existence of intensive impulse noise within measurement sequence is proposed in this paper. Proceeding from the theory of robust estimation, a simple, adaptive, practically applicable algorithm is derived that in the circumstances of <b>contaminated</b> normal <b>distribution</b> of measurement noise demonstrates high level of efficiency. QQ-plot technique {{is used as a}} framework for estimation of <b>contaminated</b> measurements <b>distribution</b> providing the algorithm adaptation. Application of proposed algorithm is broad, both in the field of wireless communications, equalization of transmitting channels, suppressing of noise and in modeling communication and control systems...|$|R
5000|$|The {{higher the}} {{breakdown}} point of an estimator, the more robust it is. Intuitively, {{we can understand}} that a breakdown point cannot exceed 50% because if {{more than half of}} the observations are contaminated, it is not possible to distinguish between the underlying <b>distribution</b> and the <b>contaminating</b> <b>distribution</b> [...] Therefore, the maximum breakdown point is 0.5 and there are estimators which achieve such a breakdown point. For example, the median has a breakdown point of 0.5. The X% trimmed mean has breakdown point of X%, for the chosen level of X. [...] and [...] contain more details. The level and the power breakdown points of tests are investigated in [...]|$|R
40|$|Tests for {{univariate}} normality, some of {{them not}} included in previous comparisons, are com-pared according to their power and simplicity, the validity of their reported p-values, their behavior under rounding, the information they provide and their availability in software. The power of each test was estimated by computer simulation for small, moderate and large sample sizes and {{a wide range of}} symmetric, skewed, <b>contaminated</b> and mixed <b>distributions.</b> A new omnibus test based on skewness and kurtosis is discussed. Key Words: Skewness, kurtosis, L-skewness, scale <b>contaminated,</b> mixed <b>distributions.</b> ...|$|R
40|$|Abstract: In this study, {{different}} optical, {{physical and}} chemical measurements were tested for their capacity to detect changes in water quality. The tests included UV-absorbance at 254 nm, absorbance at 420 nm, turbidity, particle counting, temperature, pH, electric conductivity (EC), free chlorine concentration and ATP concentration measurements. Special emphasis was given to investigating the potential for measurement tools to detect changes in bacterial concentrations in drinking water. Bacterial colony counts (CFU) and total bacterial cell counts (TBC) were used as reference methods for assessing the bacterial water quality. The study consists {{of a series of}} laboratory scale experiments: monitoring of regrowth of Pseudomonas fluorescens, estimation of the detection limits for optical measurements using Escherichia coli dilutions, verification of the relationships by analysing grab water samples from various distribution systems and utilisation of the measurements {{in the case of an}} accidentally <b>contaminated</b> <b>distribution</b> network. We found significant correlations between the tested measurements and the bacterial water quality. As the bacterial contamination of water often co-occurs with the intrusion of matrixes containing mainly non-bacterial components, the tested measurement tools can b...|$|E
40|$|Cluster {{analysis}} may {{be performed}} when {{one wishes to}} group similar objects into a given number of clusters. Several algorithms are available in order to construct these clusters. In this talk, {{focus will be on}} the generalized k-means algorithm, while the data of interest are assumed to come from an underlying population consisting of a mixture of two groups. Among the outputs of this clustering technique, a classi cation rule is provided in order to classify the objects into one of the clusters. When classi cation is the main objective of the statistical analysis, performance is often measured by means of an error rate ER(F; Fm) where F is the distribution of the training sample used to set up the classi cation rule and Fm (model distribution) is the distribution under which the quality of the rule is assessed (via a test sample). Under contamination, one has to replace the distribution F of the training sample by a contaminated one, F(eps) say (where eps corresponds to the fraction of contamination). In that case, the error rate will be corrupted since it relies on a contaminated rule, while the test sample may still be considered as being distributed according to the model distribution. To measure the robustness of classification based on this clustering proce- dure, influence functions of the error rate may be computed. The idea has already been exploited by Croux et al. (2008) and Croux et al. (2008) in the context of linear and logistic discrimination. In this setup, the <b>contaminated</b> <b>distribution</b> takes the form F(eps) = (1 -eps) *Fm + eps*Dx, where Dx is the Dirac distribution putting all its mass at x: After studying the influence function of the error rate of the generalized k- means procedure, which depends on the influence functions of the generalized k-means centers derived by Garcia-Escudero and Gordaliza (1999), a diagnostic tool based on its value will be presented. The aim is to detect observations in the training sample which can be influential for the error rate...|$|E
40|$|Classification {{analysis}} allows {{to group}} similar objects into a given {{number of groups}} {{by means of a}} classification rule. Many classification procedures are available : linear discrimination, logistic discrimination, etc. Focus in this poster will be on classification resulting from a clustering analysis. Indeed, among the outputs of classical clustering techniques, a classification rule is provided in order to classify the objects into one of the clusters. More precisely, let F denote the underlying distribution and assume that the generalized kmeans algorithm with penalty function is used to construct the k clusters C 1 (F),...,Ck(F) with centers T 1 (F),..., Tk(F). When one feels that k true groups are existing among the data, classification might be the main objective of the statistical analysis. Performance of a particular classification technique can be measured by means of an error rate. Depending on the availability of data, two types of error rates may be computed: a theoretical one and a more empirical one. In the first case, the rule is estimated on a training sample with distribution F while the evaluation of the classification performance may be done through a test sample distributed according to a model distribution of interest, Fm say. In the second case, the same data are used to set up the rule and to evaluate the performance. Under contamination, one has to replace the distribution F of the training sample by a contaminated one, F(eps) say (where eps corresponds to the fraction of contamination). In that case, thetheoretical error rate will be corrupted since it relies on a contaminated rule but it may still consider a test sample distributed according to the model distribution. The empirical error rate will be affected twice: via the rule and also via the sample used for the evaluation of the classification performance. To measure the robustness of classification based on clustering, influence functions of the error rate may be computed. The idea has already been exploited by Croux et al (2008) and Croux et al (2008) in the context of linear and logistic discrimination. In the computation of influence functions, the <b>contaminated</b> <b>distribution</b> takes the form F(eps) = (1 − eps) *Fm + eps* Dx, where Dx is the Dirac distribution putting all its mass at x. It {{is interesting to note that}} the impact of the point mass x may be positive, i. e. may decrease the error rate, when the data at hand is used to evaluate the error...|$|E
40|$|The {{approximate}} {{effects of}} measurement error {{on a variety}} of measures of inequality and poverty are derived. They are shown to depend on the measurement error variance and functionals of the error <b>contaminated</b> income <b>distribution,</b> but not on the form of the measurement error distribution, and to be accurate within a rich class of error free income distributions and measurement error distributions. The functionals of the error <b>contaminated</b> income <b>distribution</b> that approximate the measurement error induced distortions can be estimated. So it is possible to investigate the sensitivity of welfare measures to alternative amounts of measurement error and, when an estimate of the measurement error variance is available, to calculate corrected welfare measures. The methods are illustrated in an application using Indonesian household expenditure data. ...|$|R
40|$|In {{this paper}} we derive the exact Bahadur {{slope of the}} {{t-statistic}} based on a random sample from a <b>contaminated</b> normal <b>distribution,</b> using some results in large deviation theory. We also present a table of exact Bahadur slopes at various alternatives at several levels of contamination. Bahadur slope Large deviations Robustness Tukey model...|$|R
3000|$|... b could {{indicate}} that the Ueda’s method might behave similar to the KUR method; however, this claim is merely speculative {{and needs to be}} investigated. Thus, a systematic comparison study of the Ueda’s method to other available methods when dealing with <b>contaminated</b> <b>distributions</b> remains to be performed. In order to create smooth elongation in the tails of the distributions, it is here suggested that the contaminated normals be added in a stepwise fashion based on a measure of dispersion (e.g. adding contaminated normals from ± 1.5 SD from the mean up to ±kSD in steps of 0.5) and controlling for the proportion of contamination they add to the whole <b>distribution</b> (e.g. the <b>contaminated</b> normal could represent from 5 % up to 40 % of the whole distribution and this could be done in steps of 2.5 %).|$|R
40|$|We {{give some}} Monte Carlo {{results on the}} {{performance}} of two robust alternatives to least squares regression estimation - least absolute residuals and the one-step "sine" estimator. We show how to scale the residuals for the sine estimator to achieve constant efficiency at the Gaussian across various choices of X-matrix and give some results for the <b>contaminated</b> Gaussian <b>distribution.</b> ...|$|R
40|$|For {{probability}} distributions on ℝq, {{a detailed}} study of the breakdown properties of some multivariate M-functionals related to Tyler's [Ann. Statist. 15 (1987) 234] ‘distribution-free’ M-functional of scatter is given. These include a symmetrized version of Tyler's M-functional of scatter, and the multivariate t M-functionals of location and scatter. It is shown that for ‘smooth’ distributions, the (contamination) breakdown point of Tyler's M-functional of scatter and of its symmetrized version are 1 /q and inline image, respectively. For the multivariate t M-functional which arises from the maximum likelihood estimate for the parameters of an elliptical t distribution on ν ≥ 1 degrees of freedom the breakdown point at smooth distributions is 1 /(q + ν). Breakdown points are also obtained for general distributions, including empirical distributions. Finally, the sources of breakdown are investigated. It turns out that breakdown can only be caused by <b>contaminating</b> <b>distributions</b> that are concentrated near low-dimensional subspaces...|$|R
40|$|Two {{simple and}} {{frequently}} used capture–recapture {{estimates of the}} population size are compared: Chao's lower-bound estimate and Zelterman's estimate allowing for <b>contaminated</b> <b>distributions.</b> In the Poisson case it is shown that if there are only counts of ones and twos, the estimator of Zelterman is always bounded above by Chao's estimator. If counts larger than two exist, the estimator of Zelterman is becoming larger than that of Chao's, if only {{the ratio of the}} frequencies of counts of twos and ones is small enough. A similar analysis is provided for the binomial case. For a two-component mixture of Poisson distributions the asymptotic bias of both estimators is derived and it is shown that the Zelterman estimator can experience large overestimation bias. A modified Zelterman estimator is suggested and also the bias-corrected version of Chao's estimator is considered. All four estimators are compared in a simulation stud...|$|R
40|$|Despite Legionella spp. {{frequently}} <b>contaminate</b> water <b>distribution</b> {{systems of}} private and public buildings, the risk of infection/disease among workers is scarcely known. A multicentric survey was carried out to evaluate the seroprevalence of Legionella antibodies in different exposure risk groups accurately studying the related risk factors. Pneumonia events in the last 5 years and flu-like symptoms {{in the last year}} were also recorded...|$|R
40|$|In {{this article}} our {{objective}} is to evaluate the performance of different measures of associations for hypothesis testing purposes. We have consid-ered different measures of association (including some commonly used) in this study, {{one of which is}} parametric and others are non-parametric including three proposed modifications. Performance of these tests are compared un-der different symmetric, skewed and <b>contaminated</b> probability <b>distribution...</b>|$|R
40|$|For {{probability}} distributions on R -super-"q", {{a detailed}} study of the breakdown properties of some multivariate M-functionals related to Tyler's [Ann. Statist. 15 (1987) 234] 'distribution-free' M-functional of scatter is given. These include a symmetrized version of Tyler's M-functional of scatter, and the multivariate "t" M-functionals of location and scatter. It is shown that for 'smooth' distributions, the (contamination) breakdown point of Tyler's M-functional of scatter and of its symmetrized version are 1 /"q" and, respectively. For the multivariate "t" M-functional which arises from the maximum likelihood estimate for the parameters of an elliptical "t" distribution on "ν"[*]≥[*] 1 degrees of freedom the breakdown point at smooth distributions is 1 /("q" [*]+[*]"ν"). Breakdown points are also obtained for general distributions, including empirical distributions. Finally, the sources of breakdown are investigated. It turns out that breakdown can only be caused by <b>contaminating</b> <b>distributions</b> that are concentrated near low-dimensional subspaces. Copyright 2005 Board of the Foundation of the Scandinavian Journal of Statistics [...] ...|$|R
40|$|In {{this paper}} we treat {{the problem of}} robust entropy {{estimation}} given a multidimensional random sample from an unknown distribution. In particular, we consider estimation of the Renyi entropy of fractional order which is insensitive to outliers, e. g. high variance <b>contaminating</b> <b>distributions,</b> using the k-point minimal spanning tree (k-MST). A greedy algorithm for approximating the NP-hard problem of computing the k-minimal spanning tree is given which is a generalization of the potential function partitioning method of Ravi etal. 1 The basis for our approach is an asymptotic theorem establishing that the log of the overall length or weight of the greedy approximation is a strongly consistent estimator of the Renyi entropy. Quantitative robustness of the estimator to outliers is established using Hampel's method of in uence functions. 2 The structure of the in uence function indicates that the k-MST is {{a natural extension of}} the one dimensional -trimmed mean for multi-dimensional data...|$|R
40|$|The <b>contaminated</b> Gaussian <b>distribution</b> {{represents}} a simple heavy-tailed elliptical generalization of the Gaussian distribution, {{differently from the}} often-considered $t$-distribution, it also allows for automatic detection of outlying or "bad" points {{in the same way}} that observations are typically assigned to the groups in the finite mixture model context. Starting from this distribution, we propose the contaminated Gaussian factor analysis model as a method for data reduction and detection of bad points in high-dimensions. A mixture of contaminated Gaussian factor analyzers (MCGFA) model follows therefrom, and extends the recently proposed mixture of <b>contaminated</b> Gaussian <b>distributions</b> to high-dimensional data, i. e., where $p$ (number of dimensions) is large relative to $n$ (sample size). We introduce a family of eight parsimonious models formed by introducing constraints on the covariance structure of the general MCGFA model. We outline a variant of the classical expectation-maximization algorithm for parameter estimation. Various implementation issues are discussed, and the novel model is compared to competing models on both simulated and real data...|$|R
40|$|A {{class of}} two-step robust {{regression}} estimators that achieve a high relative efficiency for data from light-tailed, heavy-tailed, and <b>contaminated</b> <b>distributions</b> {{irrespective of the}} sample size is proposed and studied. In particular, the least weighted squares (LWS) estimator is combined with data-adaptive weights, which are determined from the empirical distribution or quantile functions of regression residuals obtained from an initial robust fit. Just like many existing two-step robust methods, the LWS estimator with the proposed weights preserves robust properties of the initial robust estimate. However, contrary to the existing methods and despite the data-dependent weights, the first-order asymptotic behavior of LWS is fully independent of the initial estimate under mild conditions. Moreover, the proposed estimation method is asymptotically efficient if errors are normally distributed. A simulation study documents these theoretical properties in finite samples; in particular, the relative efficiency of LWS with the proposed weighting schemes can reach 85 %- 100 % in samples of several tens of observations under various distributional models. Adaptive estimation Asymptotic efficiency Breakdown point Least weighted squares...|$|R
40|$|We propose an outlier {{robust and}} distributions-free {{test for the}} {{explosive}} AR(1) model with intercept based on simplicial depth. In this model, simplicial depth reduces to counting the cases where three residuals have alternating signs. Using this, it is shown that the asymptotic distribution of the test statistic is given by an integrated two-dimensional Gaussian process. Conditions for {{the consistency of the}} test are given {{and the power of the}} test at finite samples is compared with five alternative tests, using errors with normal <b>distribution,</b> <b>contaminated</b> normal <b>distribution,</b> and Frechet distribution in a simulation study. The comparisons show that the new test outperforms all other tests in the case of skewed errors and outliers. Although here we deal with the AR(1) model with intercept only, the asymptotic results hold for any simplicial depth which reduces to alternating signs of three residuals...|$|R
40|$|Kurtosis, usually as {{measured}} by the standardised fourth central moment, has been examined {{on a number of occasions}} by observing the effect of <b>contaminating</b> the <b>distribution,</b> that is, mixing in another distribution. However, superficial treatment can lead, and indeed has led, to misunderstandings. This paper considers, firstly for a symmetric <b>distribution</b> <b>contaminated</b> at two points symmetrically placed around its centre and then for a mixture of two continuous symmetric distributions, the behaviour of three measures of kurtosis. This is done in general and not just as the mixing proportion tends to zero as in the influence function approach. It is seen that when both scale and kurtosis change, the latter is not necessarily intuitive. It is also illustrated that parameter interpretation in terms of distributional properties such as shape can be misleading without the use of the appropriate distributional partial orderin...|$|R
40|$|The mean squared {{errors of}} various estimators of slope, intercept, and mean {{response}} in the simple linear regression problem are compared in a simulation study. A weighted median estimator of slope proposed by Sievers (1978) and Scholz (1978) and two intercept estimators based upon it are found to perform well for most error distributions studied. Theil's (1950) estimator of slope and two intercept estimators based on it are preferable for certain heavily <b>contaminated</b> error <b>distributions.</b> 1...|$|R
