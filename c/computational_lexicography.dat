41|10|Public
5000|$|Lenders, W. <b>Computational</b> <b>lexicography</b> and corpus {{linguistics}} until ca. 1970/1980, in: Gouws, R. H., Heid, U., Schweickard, W., Wiegand, H. E. (eds.) Dictionaries - An International Encyclopedia of Lexicography. Supplementary Volume: Recent Developments with Focus on Electronic and <b>Computational</b> <b>Lexicography.</b> Berlin: De Gruyter Mouton, 2013 ...|$|E
50|$|Computational {{lexicology}} is {{a branch}} of computational linguistics, which {{is concerned with the}} use of computers in the study of lexicon. It has been more narrowly described by some scholars (Amsler, 1980) as the use of computers in the study of machine-readable dictionaries. It is distinguished from <b>computational</b> <b>lexicography,</b> which more properly would be the use of computers in the construction of dictionaries, though some researchers have used <b>computational</b> <b>lexicography</b> as synonymous.|$|E
5000|$|Beryl T. (Sue) Atkins is a lexicographer, specialising in <b>computational</b> <b>lexicography,</b> who {{pioneered the}} {{creation}} of bilingual dictionaries from corpus data.|$|E
50|$|The Eagles Guidelines provide {{guidance}} for markup {{to be used}} with text corpora, particularly for identifying features relevant in <b>computational</b> linguistics and <b>lexicography.</b>|$|R
40|$|In this paper, {{we propose}} an {{approach}} to distributional semantics which can be formally related to a simple model-theoretic approach. We describe treatments {{of some of the}} traditional lexical semantic relationships within this framework, and also outline accounts of some phenomena which have been considered within Generative Lexicon theory. We further argue that distributions should be based on individual experience, rather than the type of text corpora currently used in <b>computational</b> linguistics and <b>lexicography...</b>|$|R
40|$|The Open Linguistics Working Group (OWLG) is an {{initiative}} of experts from different fields concerned with linguistic data, including academic linguistics (e. g. typology, corpus linguistics), applied linguistics (e. g. <b>computational</b> linguistics, <b>lexicography</b> and language documentation), and NLP (e. g. from the Semantic Web community). The primary {{goals of the}} working group are 1) promotion the idea of open linguistic resources 2) the development of means for their representation, and 3) encouraging the exchange of ideas across different disciplines. To a certain extent, {{the activities of the}} Open Linguistics Working Group converge towards the creation of a Linguistic Linked Open Data cloud, which is a topic addressed from different angles by several members of the Working Group. In this article, some of these currently on-going activities are presented and described...|$|R
50|$|EuroMatrix {{explored}} using {{linguistic knowledge}} in statistical machine translation. Statistical techniques were combined with rule-based approach, resulting in hybrid MT architecture. The project experimented with combining methods and resources from statistical MT, rule-based MT, shallow language processing and <b>computational</b> <b>lexicography</b> and morphology.|$|E
5000|$|Words and nonwords {{as basic}} units of a {{newspaper}} text corpus. In publication: COMPLEX 2001 / 6th Conference on <b>Computational</b> <b>Lexicography</b> and Corpus Research [...] "Computational Lexicography and New EU Languages", Mason Hall, Birmingham, 28 June-1 July 2001. - Birmingham: Centre for Corpus Linguistics, Department of English, University of Birmingham, 2001. - p. 49-65.|$|E
5000|$|In the 1990s, Fillmore taught {{classes in}} <b>computational</b> <b>lexicography</b> at the University of Pisa, {{where he met}} Sue Atkins, who was {{conducting}} frame-semantic analyses from a lexicographic perspective. In their subsequent discussions and collaborations, Fillmore came to acknowledge the importance of considering corpus data. They discussed the [...] "dictionary of the future", in which every word would be linked to example sentences from corpora.|$|E
40|$|Machine Learning {{techniques}} are useful tools for the automatic extension of existing lexical databases. In this paper, we review some symbolic machine learning methods {{which can be}} used to add new lexical material to the lexicon by automatically inducing the regularities implicit in lexical representations already present. We introduce the general methodology for the construction of inductive lexica, and discuss empirical results on extending lexica with two types of information: pronunciation and gender. 1. Introduction <b>Computational</b> lexicology and <b>lexicography</b> (the study of the structure, organization, and contents of computational lexica) have become central disciplines both in language engineering and in theoretical computational linguistics. Most language engineering applications are in need of rich lexical knowledge sources, and in computational linguistics theory, the role of the lexicon has become increasingly important in linguistic formalisms, such as GPSG, HPSG, and TAG [...] . ...|$|R
40|$|The present {{volume is}} a {{collection}} of thirteen papers resulting from a sympo-sium on the computer-aided production and publication of dictionaries held in Heidelberg in 1988 (p. 4). It is subdivided into two sections, Basics (Grundlagen), which discusses the impact of <b>computational</b> methods on <b>lexicography</b> in gen-eral, and Applications (Anwendungen), which contains descriptions of various existing computer-aided lexicographical projects. All of these projects deal with German as the object of lexicographical description {{and many of them are}} con-cerned with specialized scholarly dictionaries, often of historical stages of Ger-man. Thus two articles (Burch and Fournier; Plate and Recker) deal with Mid-dle High German and another one (Gloning and Welter) with the language of Goethe. Some articles, however, also describe projects dealing with present-day German and the design of general purpose dictionaries. Thus Petelenz dis-cusses information design in an electronic Polish–German dictionary and Haß-Zumkehr focuses on the organisation of the microstructure in the LEKSIS-data-base of the Institut für Deutsche Sprache in Mannheim...|$|R
40|$|Much {{research}} on {{natural language processing}} (NLP), <b>computational</b> linguistics and <b>lexicography</b> has relied and depended on linguistic corpora. In recent years, many organizations {{around the world have}} been constructing their own large corpora to achieve corpus representativeness and/or linguistic comprehensiveness. However, there is no reliable guideline as to how large machine readable corpus resources should be compiled to develop practical NLP software and/or complete dictionaries for humans and computational use. In order to shed some new light on this issue, we shall reveal the flaws of several previous researches aiming to predict corpus size, especially those using pure regression or curve-fitting methods. To overcome these flaws, we shall contrive a new mathematical tool: a piecewise curve-fitting algorithm, and next, suggest how to determine the tolerance error of the algorithm for good prediction, using a specific corpus. Finally, we shall illustrate experimentally that the algorithm presented is valid, accurate and very reliable. We are confident that this study can contribute to solving some inherent problems of corpus linguistics, such as corpus predictability, compiling methodology, corpus representativeness and linguistic comprehensiveness...|$|R
50|$|Professor Jonathan J. Webster is a {{professor}} in linguistics in the Department of Linguistics and Translation, City University of Hong Kong. He received his PhD from the State University of New York at Buffalo in Linguistics. Prior to coming to Hong Kong in 1987, he taught at the National University of Singapore in the Department of English Language and Literature. His research interests include text linguistics, <b>computational</b> <b>lexicography,</b> and example-based machine translation.|$|E
5000|$|Throughout Beeken’s career, her {{publications}} {{have covered}} a breadth of linguistic areas. She was {{also interested in}} solutions for academic unemployment. In 1994 she contributed briefly to Roger Blanpain book ‘The nine zeros of learning Flanders. From repetitive to creative jobs’, and in 1995 she addressed the feminisation of job titles for the Flemish social-economic council (Sociaal-Economische Raad van Vlaanderen) and the Ministries in Flanders and the Netherlands. [...] More recently her published work has focused on lexicology, <b>computational</b> <b>lexicography,</b> digitised language resources and spelling, including her contribution to ‘Hebrew and Yiddish words in Dutch’, and her involvement in the Dutch Medical Journal, publishing ‘The successful disclosure of a medical journal through a medical lexicon’.|$|E
50|$|Sue Atkins {{has been}} a {{professional}} lexicographer since 1966, first with Collins Publishers (now HarperCollins), where she was General Editor of the first 'modern' English-French dictionary, the Collins-Robert English-French Dictionary, then as Lexicographic Adviser to Oxford University Press, where she pioneered methodology {{for the creation of}} bilingual dictionaries from corpus data, ultimately resulting in the Oxford-Hachette English-French Dictionary. She was also Lexicographic Adviser to the FrameNet project at the International Computer Science Institute, Berkeley, California, {{and a member of the}} Advisory Board of the American National Corpus, and the International Journal of Lexicography. Among her most important contributions to corpus linguistics, Sue Atkins originated the idea of the British National Corpus.. In 2010, Sue Atkins founded the Lexicography MasterClass in partnership with Adam Kilgarriff and Michael Rundell, providing advice, consultancy services, and training for anyone involved in, or embarking on, a lexicographic project. Now retired, Sue Atkins has taught and consulted in lexicography, and participated in national and international research projects in the field of <b>computational</b> <b>lexicography,</b> and originated the idea of the British National Corpus.|$|E
40|$|Da bi se utvrdilo mjesto hrvatske leksikografije u okviru europske leksikografije, prikazat će se rezultati rada glavnih europskih leksikografa izneseni na godišnjim sastancima i znanstvenim skupovima ili objavljeni u časopisu Europskog društva za leksikografiju (European Association for Lexicography -EURALEX) 2 ̆ 7 International Journal ofLexicography 2 ̆ 7. Zatim će se pokazati kako se rad hrvatskih leksikografa uklapa u tu djelatnost. The {{analysis}} of topics discussed by European lexicographers {{has shown that}} Croatian lexicographers have analyzed similar problems and questions. At the first conference of European lexicographers LEXeter 2 ̆ 783 held in Exeter in 1983 the first contact between Croatian and European lexicography was established. The programme of all future conferences shows {{that there is a}} very close interest of both groups in <b>computational</b> lexicology and <b>lexicography,</b> particularly in the corpus and its use in lexicography. Quite a number of papers, read at the conference in Zagreb, held in November 1997, show that several lexicographic problems have been discussed by both Croatian and European lexicographers: the structure and function of the definition in a dictionary, lexicographic terminology, compounds, metaphors, grammar, neologisms and loan-words in a dictionary, verbal aspect in a dictionary, bilingual dictionary as an interpreter of foreign culture and a valency dictionary. The final conclusion is that Croatian lexicography from its beginnings has been developing parallelly with European lexicography...|$|R
40|$|Linguistics. The {{project is}} {{scheduled}} for six years and its aim {{is to create a}} solid base for a versatile computerized processing of Czech language serving both as a multifacetted source of material for empirical and theoretical linguistic research as well as for multifarious applications in the domain of text processing and information retrieval. The project is conceived of as consisting of three branches, running in parallel but in close interrelations (for a more detailed description of the project, see Hajičová 1996) : (i) the buildup of Czech National Corpus as the largest complex and representative Czech language data base, (ii) the research of present-day Czech based on contemporary methods and techniques of <b>computational</b> linguistics and <b>lexicography,</b> which includes the development of a tagging system for the corpus and formulation and implementation of tagging procedures, and (iii) continuation in the theoretical research in the domain of sentence and text structure, also with the perspective of possible applications in computerized system employing natural language processing. 2. The three layers of tagging of Czech The development of tagging systems for Czech is not a trivial task, {{in view of the fact}} that most of the existing tagging systems have been developed for languages typologically different from Czech: the highly inflectional character of Czech and its communicatively rather than grammatically based wor...|$|R
40|$|In this PhD we {{deal with}} lexical {{collocations}} in Modern Greek. Collocation is a widely used term mostly {{in the field of}} applied linguistics, <b>lexicography,</b> <b>computational</b> linguistics and language acquisition studies. Starting point for our research was the native and non-native speaker lexical production. Through our research it was found that while the advanced learner has acquired linguistic competence in Modern Greek, his phraseological/collocational competence in the language, competence in producing typical word sequences, is not in the same way developed. The native speaker has not acquired the phraseology of the language. Acquisition and production of collocations was not our only interest. In this PhD we focus, following a linguistic perspective, on verbal collocations, verb + noun (object) collocations such as δino prosoxi (pay attention), δino eksetasis (take an exam), perno mja apofasi (make a decision), etc. Phraseology (idioms, collocations, prefabricated patterns, similes, etc.) holds a wide spectrum from less to more stereotyped word combinations. A distinction between lexical collocations and idioms in the verb + noun structure is more than necessary and fundamental rixno mia matja (collocation) rixno δilitirio (idiom) Lexical collocations are semi-phrasemes, with restricted collocability due to, according to Cowie (1992), the specialized (figurative, technical) sense of one of their constituent, the verb. We argued that restricted collocability results from the bleached sense of the verb whereas the noun of the collocation carries its literal sense. We claimed that literal sense of the noun is the basic criterion for distinguishing collocations from idioms. Both of the constituents forming an idiom carry figurative sense or the combination as a whole is totally opaque in meaning. Lexical collocations permit lexical substitutability whereas idioms are absolutely frozen. Lexical decomposition of collocations has shown that the verb is a ditransitive verb requiring two internal arguments, a theme (y) {{on one side and a}} goal or source or location (z) on the other. Verb action is a transition, the event is divided into two subevents with a final point of change, referring to the physical world. The noun on the other side is an abstract entity. The ditransitive verbs of Modern Greek analyzed are the following δino, perno, vazo, vγazo, rixno, travao, pjano, arpazo, afino, δixno, ferno Lexical decomposition was based on argument structure-thematic roles and lexical conceptual structure of the predicates developed. Lexical decomposition of a wide range of lexical collocations brought to light a process concerning full, prototypical verb on one side and lexical collocations on the other side: verb of lexical collocations is, gradually, semantically bleached. Semantic bleaching of the verb is meant to be: a) loss of the control of the predicate arguments loss/abolishment of the verb’s arguments and new argument development in their place. ...|$|R
50|$|From 1990 to 2000, Hanks {{served as}} chief editor of current English {{dictionaries}} at Oxford University Press (OUP). In 1991 to 1992, he was joint principal investigator (with Mary-Claire van Leunen) of the HECTOR {{project at the}} Systems Research Center of Digital Equipment Corporation (DEC) in Palo Alto, CA. The HECTOR project was a collaboration between OUP and DEC, and although its results were never published, they served {{as a basis for}} the New Oxford Dictionary of English (1998), while the lexicographers working on it were also guinea-pig users in the development of one of the earliest search engines (AltaVista). On the basis of the COBUILD and HECTOR research in corpus analysis, Hanks began to develop his theory of Norms and Exploitations. From 2001 to 2005, he was adjunct professor of <b>computational</b> <b>lexicography</b> at Brandeis University in Waltham, MA, where he worked closely with James Pustejovsky. In 2003, he was appointed consultant and visiting scientist to the Collocations Project and Electronic Dictionary of the German Language (DWDS) at the Berlin-Brandenburg Academy of Sciences (BBAW) headed by Christiane Fellbaum. He has also served as a consultant on lexicographical methodology to the Institute of the Czech Language in Prague, to Patakis Publishers in Athens, and others.|$|E
40|$|In {{my thesis}} I shall deal with English {{monolingual}} dictionaries from their origin to present day. First of all I {{will look at}} how various writers describe computational linguistics as such, as <b>computational</b> <b>lexicography</b> {{is part of that}} field. Then I will follow by considering lexicography and <b>computational</b> <b>lexicography</b> itself. In the third part of the thesis I would like to give an account on the history of dictionaries, and then I will continue with outlining the history of electric dictionaries and will give detail about some computerized dictionaries. egyetemiangol nyelv és irodalo...|$|E
40|$|Trotter, D. A. (2013). ?Gallo-Romance II: Synchronic lexicography?. In R. H. Gouws, U. Heid, W. Schweickard, & H. E. Wiegand (Eds.), Dictionaries An International Encyclopedia of Lexicography Supplementary volume: Recent Developments with Special Focus on <b>Computational</b> <b>Lexicography.</b> (Supplement ed., pp. 27 - 36). Berlin: De Gruyter...|$|E
5000|$|... 1985: [...] "Monolingual & bilingual learners’ dictionaries: a comparison". In: Dictionaries, Lexicography & Language Learning, (ed.) R. F. Ilson. Oxford: Pergamon. 15-24.1986: (with J. Kegl & B. Levin) [...] "Implicit and Explicit Information in Dictionaries". In: Advances in Lexicology: Proceedings of the Second Annual Conference of the UW Centre for the New OED, 45-63, Waterloo, Canada.1987: (with H. Lewis, D. Summers, J. Whitcut) [...] "A {{research}} project into {{the use of}} learners’ dictionaries" [...] in The Dictionary and the Language Learner, A. P. Cowie (ed.), Niemeyer, Tübingen. 29-431987: [...] "Semantic-ID tags: corpus evidence for dictionary senses", in The Uses of Large Text Databases, proceedings of the Third Annual Conference of the New OED Centre, University of Waterloo, Canada. 17-36.1988: (with J. Kegl & B. Levin) [...] "Anatomy of a verb entry: from linguistic theory to lexicographic practice". In: International Journal of Lexicography, 1:2 Oxford University Press, Oxford. pp. 84-126. Also in Current Issues in Computational Linguistics: In Honour of Don Walker, Linguistica Computazionale IX-X. (eds.) A. Zampolli, N. Calzolari & M. Palmer (1994) Pisa: Giardini Editori. 237-266.1988: (with B. Levin) [...] "Admitting impediments". In: Information in Text, Proceedings of the Fourth Annual Conference of the New OED Centre, University of Waterloo, Canada. pp. 97-114. Also in Lexical Acquisition: Using On-Line Resources to Build a Lexicon, U. Zernik (ed.), Lawrence Erlbaum Assoc. Inc. (1991). pp. 233-262.1990: (with F. E. Knowles) [...] "Interim report on the EURALEX / AILA Research Project into Dictionary Use", in T. Magay & J. Zigany (eds) Proceedings of BudaLex ’88, Akadémiai Kiadó, Budapest. pp. 381-392.1991: [...] "Corpus lexicography: the bilingual dimension", in A. Zampolli & N. Calzolari (eds.) <b>Computational</b> Lexicology and <b>Lexicography</b> I, Giardini, Pisa. pp. 43-64.1991: [...] "Building a Lexicon: the Contribution of Lexicography", in B. K. Boguraev (ed.) International Journal of Lexicography, 4:3, pp. 163-204. Also in Challenges in Natural Language Processing, M. Bates and R. Weischedel (eds.), Cambridge University Press, Cambridge (1993). pp. 37-71992: [...] "Putting lexicography on the professional map", Proceedings of EURALEX ’90, M. Alvar Ezquerra (ed.), Bibliograf SA, Barcelona. pp. 519-526.1992: (with J. H. Clear & N. Ostler) [...] "Corpus design criteria". In: Journal of Literary and Linguistic Computing, (ed.) Gordon Dixon. Oxford: Oxford University Press. 1 - 16.1992: [...] "Tools for computer-aided corpus lexicography: the Hector project", in Papers in Computational Lexicography: Complex’92, F. Kiefer, G. Kiss and J. Pajsz (eds.) Hungarian Academy of Sciences, Budapest. pp. 1-60. Also in Acta Linguistica Hungarica 41, F. Kiefer (ed.) (1991), Hungarian Academy of Sciences, Budapest: Akadémiai Kiadó.1992: (with Charles J. Fillmore) [...] "Towards a Frame-based Lexicon: the Semantics of RISK and its Neighbors", in Frames, Fields and Contrasts: New Essays in Semantic and Lexical Organization, A. Lehrer & E. F. Kittay (eds.) Lawrence Erlbaum Associates: Hillsdale, New Jersey. 75-102.1992: (with N. Ostler) [...] "Predictable meaning shift: some linguistic properties of lexical implication rules" [...] in Lexical Semantics and Commonsense Reasoning, (eds.) J. Pustejovsky & S. Bergler, Springer-Verlag, NY. pp. 87-98.1993: [...] "Theoretical Lexicography {{and its relation to}} Dictionary-making". In: Dictionaries: the Journal of the Dictionary Society of North America, (guest editor) W. Frawley, DSNA, Cleveland Ohio. pp. 4-43.1994: (with Charles J. Fillmore) [...] "Starting where the dictionaries stop: the challenge of corpus lexicography". In Atkins & Zampolli. 350-3931994: (with Beth Levin & A. Zampolli) [...] "Computational Approaches to the Lexicon: An Overview". In Atkins & Zampolli. 17-451994: [...] "A corpus-based dictionary". In: Oxford-Hachette French Dictionary (Introductory section). Oxford: Oxford University Press. xix - xxxii1994: (with Charles J. Fillmore, J. B. Lowe & N. Urban) [...] "The Dictionary of the Future: a Hypertext Database". Presentation and on-line demonstration at the Xerox-Acquilex Symposium on the Dictionary of the Future, Uriage, France.1995: [...] "Analysing the verbs of seeing: a frame semantics approach to corpus lexicography". In: Proceedings of the Twentieth Annual Meeting of the Berkeley Linguistics Society, 1994, (eds.) S. Gahl, C. Johnson & A. Dolbey, BLS, UC Berkeley, CA.1995: (with Beth Levin) [...] "Building on a Corpus: A linguistic and lexicographical look at some near-synonyms". In: International Journal of Lexicography, 8 : 2. 85-114.1995: [...] "The Dynamic Database". In: Machine Tractable Dictionaries, (ed.) Cheng-ming Guo, Ablex Publishing Corporation, Norwood, NJ. 131-143.1995: (with Charles J. Fillmore and Ulrich Heid) [...] "Lexicographical Relevance in Corpus Evidence", Deliverable D-IX-2 of DELIS Project (LRE 61.034).1995: [...] "The role of the example in a frame semantics dictionary". In: Essays in Semantics and Pragmatics, in honor of Charles J. Fillmore. (eds.) Shibatani, Masayoshi & Sandra Thompson. Amsterdam: John Benjamins. 25-42.1996: [...] "Bilingual Dictionaries: Past, Present and Future". In: Euralex’96 Proceedings, (eds.) Gellerstam, M. , J. Järborg, S.-G. Malmgren, K. Norén, L. Rogström and C. R. Papmehl. Gothenburg: Gothenburg University, Department of Swedish. 515-590.1996: (with Beth Levin and Grace Song) [...] "Making Sense of Corpus Data: A Case Study". In: Euralex’96 Proceedings, (eds.) Gellerstam, M. , J. Järborg, S.-G. Malmgren, K. Norén, L. Rogström and C. R. Papmehl. Gothenburg: Gothenburg University, Department of Swedish. 345-354.1997: (with K. Varantola) [...] "Monitoring Dictionary Use". In International Journal of Lexicography, 10:1, 1-45, and reprinted in Atkins (1998).1997: (with Beth Levin and Grace Song) [...] "Making Sense of Corpus Data: A Case Study of Verbs of Sound". In: International Journal of Corpus Linguistics, 2:1 23-64.1998: (with Charles J. Fillmore) [...] "FrameNet and Lexicographic Relevance". In: Proceedings of the First International Conference On Language Resources And Evaluation, Granada, Spain, 28-30 May 19981998: (with K. Varantola) [...] "Language Learners Using Dictionaries: The Final Report of the EURALEX- and AILA-sponsored Research Project into Dictionary Use". In Atkins (1998).1998: (with Jan H. Hulstijn [...] ) [...] "Empirical research on dictionary use in foreign-language learning: an overview". In Atkins (1998).2000: (with Charles J. Fillmore) [...] "Describing polysemy : the case of crawl". In Polysemy: Linguistic and Computational Approaches, (eds) Yael Ravin and Claudia Leacock. Oxford: Oxford University Press.2002: (with Nuria Bel, Francesca Bertagna, Pierrette Bouillon, Nicoletta Calzolari, Christiane Fellbaum, Ralph Grishman, Alessandro Lenci, Catherine MacLeod, Martha Palmer, Gregor Thurmair, Marta Villegas, Antonio Zampolli) [...] "From Resources to Applications. Designing the Multilingual ISLE Lexical Entry", LREC2002, Las Palmas.2002: [...] "Then and Now: Competence and Performance in 35 Years of Lexicography". In Proceedings of the Tenth EURALEX International Congress, EURALEX 2002, (eds.) Braasch, A. and C. Povlsen. Copenhagen: Center for Sprogteknologi. 1-28.2003: (with Michael Rundell & Hiroaki Sato) [...] "The contribution of FrameNet to practical lexicography", in International Journal of Lexicography, guest editor2003: (with Charles Fillmore & Christopher Johnson) [...] "Lexicographic relevance: selecting information from corpus evidence", in International Journal of Lexicography, guest editor Thierry Fontenelle, Oxford, OUP: 16:3 251-2802004: Pierrette Bouillon, [...] "Relevance in dictionary-making", in Proceedings of the First International Symposium on Lexicography, Universitat Pompeu Fabra, Barcelona, May 2002; also in Text-based Studies: Lexicography, Terminology, Translation. In Honour of Ingrid Meyer (Ed.) Lynne Bowker; Ottawa, University of Ottawa Press, 2006.2006: (with Valerie Grundy) [...] "Lexicographic profiling: An aid to consistency in dictionary entry design". In Proceedings of the Twelfth EURALEX International Congress, EURALEX 2006, Alessandria Italy: Edizioni dell’Orso, 1097-1107.2007: [...] "Me Lexicographer, You Translators: or Context-free (vs. context-sensitive) translation and what it involves", in Proceedings of 4th International Maastricht-Lodz Duo Colloquium on Translation and Meaning, Maastricht, Netherlands: May 2005.(with Michael Rundell) [...] "Lexicography Training: An Overview" [...] in Dictionaries: An International Encyclopedia of Lexicography: Supplementary Volume: Recent Developments (eds.) R. H. Gouws, U. Heid, W. Schweickard & H.E. Wiegand (forthcoming from Mouton De Gruyter, Berlin).(with Michael Rundell) [...] "Criteria for the design of corpora for lexicography 1: monolingual dictionaries" [...] in Dictionaries: An International Encyclopedia of Lexicography: Supplementary Volume: Recent Developments (eds.) R. H. Gouws, U. Heid, W. Schweickard & H.E. Wiegand (forthcoming from Mouton De Gruyter, Berlin).|$|R
40|$|This {{case study}} focuses on various aspescts of the {{language}} technology based researches and improvements made in the Research Institute for Linguistics of the Hungarian Academy of Sciences. The study enumerates the key state-of-the-art technologic fields, such as <b>computational</b> <b>lexicography,</b> corpus linguistics, computational linguistics and machine translation and glimpses at its main improvements (resources and programs) ...|$|E
40|$|This paper {{deals with}} {{issues related to}} the design of {{structures}} for holding lexicographical and terminographical data, drawing from experiences gained during a terminology project. The issues include the structural differences between a typical dictionary entry and a typical terminographical entry, senses and concepts, semasiology and onomasiology, dictionary reversal, data conversion, polysemy and homonymy, and the grammatical labelling of multi-word items. 2 2. <b>Computational</b> <b>Lexicography</b> and Lexicology Finding the right structure for lexicographical data: experiences from a terminology project 1...|$|E
40|$|This book {{contains}} {{a selection of}} papers presented at the nineteenth meeting of Computational Linguistics in the Netherlands (CLIN). The meeting {{took place at the}} University of Groningen in the Netherlands on 22 January 2009 and featured 58 oral presentations and 9 posters. Of these 67 presentation nine papers have been selected for publication in these proceedings. The papers in these proceedings deal with a variety of topics in computational linguistics, like corpus linguistics, <b>computational</b> <b>lexicography,</b> event processing and modeling child language...|$|E
40|$|This paper {{examines}} <b>computational</b> <b>lexicography</b> {{and cognitive}} scientometry as specific {{tools in the}} history of science research. The main purpose, in this work, is to show those theoretical and fundamental conceptual frames related to this argument. An experimental analysis, concerning Claude Bernard’s (1813 - 1878) and Etienne Jules Marey’s (1830 - 1904) epistemological texts, has been achieved, successfully, using a particular software able to identify by graphic accounts, numerical tables, lemmatized concordances, the two different approaches in the study of physiology in the 19 th century...|$|E
40|$|Abstract: Onomasiological {{dictionaries}} are {{a simplified}} version of question answering systems when the user does not remember a term but its definition. Then, the query {{for this kind}} of dictionaries is a definition, in natural language, and the output is a set of the terms related to it. On this work we have taken a previously proven ono-masiological inference algorithm and we have tried to improve its ranking stage by combining different weighting techniques like simple frequency counting, Fuzzy Logic, and TF-IDF. Key–Words: Onomasiological search, ranking methods, <b>computational</b> <b>lexicography...</b>|$|E
40|$|Abstract. In {{this paper}} we {{describe}} and apply two statistical methods for extracting collocations from text corpora written in Modern Greek. The {{first one is}} the mean and variance method which calculates “offsets ” (distances) between words in a corpus and looks for patterns of distances with low spread. The second method {{is based on the}} X 2 test. Such an approach seems to be more flexible because it does not assume normally distributed probabilities of the words in the corpus. The two techniques produce interesting collocations that are useful in various applications e. g. <b>computational</b> <b>lexicography,</b> language generation and machine translation. ...|$|E
40|$|The last {{decades have}} seen an immense {{maturation}} of Natural Language Processing (NLP) and an increased interest to apply NLP techniques and resources to real-world applications in business and academia. This process has certainly been facilitated by the increased availability of language data in the internet age, and the subsequent paradigm shift to statistical approaches, but also it coincided with an increasing acceptance of empirical approaches in linguistics and related academic fields, including empirical approaches to typology (Greenberg, 1963), corpus linguistics (Francis and Kucera, 1979, Brown Corpus), and (<b>computational)</b> <b>lexicography</b> (Kucera, 1969), as well as th...|$|E
40|$|One of {{the main}} {{purposes}} in current <b>computational</b> <b>lexicography</b> is to determine methods to allow the acquisition of lexical knowledge. One of the facilities of the Acquilex Lexical Knowledge Base {{is the possibility of}} defining lexical rules. These rules are used for generating new lexical entries from the existing ones, capturing linguistic generalizations which will be reflected in the new entries. In this paper we focus on one kind of lexical rules which allows the generation of noun entries from verbs. We present the nominalization problem in Spanish and how to represent it in the LKB. The nouns created refer to the agent, patient, and action of the origin verbsPostprint (published version...|$|E
40|$|One of {{the main}} {{purposes}} in current <b>computational</b> <b>lexicography</b> is to determine methods to allow the acquisition of lexical knowledge. One of the facilities of the Acquilex Lexical Knowledge Base {{is the possibility of}} defining lexical rules. These rules are used for generating new lexical entries from the existing ones, capturing linguistic generalizations which will be reflected in the new entries. The main goal of this paper is the treatment of Spanish verbal subcategorization alternances (diatheses) by means of lexical rules. Different subcategorization schemes can be generated from the most basic (or general) verbal entry by applying lexical rules which show the diverse morphological, syntactic or semantic changes produced. Postprint (published version...|$|E
40|$|In {{the past}} years {{a large number}} of {{electronic}} text corpora for German have been created due to the increased availability of electronic resources. Appropriate filtering of lexical material in these corpora is a particular challenge for <b>computational</b> <b>lexicography</b> since machine readable lexicons alone are insufficient for systematic classification. In this paper we show – {{on the basis of the}} corpora of the DWDS – how lexical knowledge can be classified in a more fine-grained way with morphological and shallow syntactic parsing methods. One result of this analysis is that the number of different lemmas contained in the corpora exceeds the number of different headwords of current large monolingual German dictionaries by several times...|$|E
40|$|Lexicon schemas {{and their}} use are {{discussed}} in this paper {{from the perspective of}} lexicographers and field linguists. A variety of lex-icon schemas have been developed, with goals ranging from <b>computational</b> <b>lexicography</b> (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and com-pared to each other in terms of conversion and usability for this particular user group, using a common lexicon entry and providing examples for each schema under consideration. The formats are assessed and the final recommendation is given for the potential users, namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards, lexicographers and field linguists. 1...|$|E
40|$|The paper {{deals with}} the {{acquisition}} of static knowledge sources #ontology and the lexicons# for an NLP system. Acquiring the ontology and lexicon together enables immediate feedbackbetween the two acquisition teams, involving shared semi-automatic acquisition tools. The acquisition tools can be technological #mostly corpus-processing and interface-related# and conceptual #such {{as the use of}} lexical rules instead of sense enumeration#. We describe how both kinds of tools are used in our approach and presentanoverall knowledge acquisition methodology. We hope to demonstrate that knowledge acquisition for NLP may be more feasible than in other areas of AI. Content Areas: knowledge acquisition, ontology, <b>computational</b> <b>lexicography</b> 1. Knowledge Acquisition Sophisticated environments for knowledge acquisition are becoming increasingly commonplace. When used for practical applications, such tools are typically centered around a user interface. In NLP applications, the interfaces all [...] ...|$|E
40|$|The {{purpose of}} this study is to {{formulate}} a set of heuristics (problem solving procedures) for linguistic semantic description. The introduction builds the context of this dissertation by looking at sources for <b>computational</b> <b>lexicography.</b> A survey of heuristic research done in other fields is included in Chapter 2, but a survey of the discipline of linguistics reveals that there has been little research in this area. Chapter 3 features a discussion of this lack of problem solving methodology in semantics and lexicography, attributing it to the perceived 2 ̆ 2 unformalizability 2 ̆ 2 of meaning. Raskin 2 ̆ 7 s (1985) script-based semantic theory is proposed as a foundation for a set of linguistic semantic heuristics (LSHs) Script-based semantic theory describes and represents semantic competence in a formalizable way, and as such, makes a heuristics of meaning possible. An implementation of the LSHs [...] FrameBuilder [...] which produces computational semantic frames with native speaker assistance, is detailed in Chapter 4. The various components of the LSHs [...] linguistic formulation, branching design, semantic theory, world knowledge, and reasoning [...] are explored in Chapter 5. The second part of the chapter looks closely at a number of assumptions made by the formulation of the LSHs, including the nature of semantic speaker competence, the possibility (or impossibility) of communicating native speaker competence, and the role of the linguist in elicitation of speaker data. Chapter 6 is a study of how the formalism produced by the LSHs, called an 2 ̆ 2 inter-lingua, 2 ̆ 2 or ILT frame, compares with the semantic descriptions done by Fillmore (1981), Wierzbicka (1985), and Lehrer (1969). ^ A summary of these issues leads to a discussion of common problems in <b>computational</b> <b>lexicography,</b> and while the 2 ̆ 2 weaknesses 2 ̆ 2 of each formalism can be found to underlie these problems, the strengths of each can also be found to be part of the solution. Computer-assisted lexical acquisition, through the LSHs, is proposed as the other part of the solution. ...|$|E
40|$|Machine {{learning}} {{techniques can}} be used to make lexicons adaptive. The main problems in adaptation are the addition of lexical material to an existing lexical database, and the recomputation of sublanguage-dependent lexical information when porting the lexicon to a new domain or application. Inductive lexicons combine available lexical information and corpus data to alleviate these tasks. In this paper, we introduce the general methodology for the construction of inductive lexicons, and discuss empirical results on a case study using the approach: prediction of the gender of nouns in Dutch. 1. Introduction In <b>computational</b> <b>lexicography,</b> lexicons of language engineering applications should come with acceptable lexical coverage, and with the information necessary for the intended applications. They should also come equipped with methods for the automatic extension and adaptation of the lexicon with new or modified lexical entries. Computational lexicology should therefore try to solve t [...] ...|$|E
