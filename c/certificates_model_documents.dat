0|3030|Public
5000|$|Pennsylvania Land Trust Association <b>Model</b> <b>Documents</b> & Easements ...|$|R
5000|$|RFC 6087: Guidelines for Authors and Reviewers of YANG Data <b>Model</b> <b>Documents</b> ...|$|R
50|$|The utility <b>model</b> <b>certificate</b> {{is similar}} to a patent {{application}} but with less stringent procedures of application and shorter duration. The requirement needed for a utility <b>model</b> <b>certificate</b> is that it should be new and have industrial applicability. A utility <b>model</b> <b>certificate</b> expires after seven years of application without the possibility of a renewal. An application for patent can be converted to one for a utility <b>model</b> <b>certificate</b> and vice versa if prior to the grant or refusal of the application the prescribed fee for the latter is paid.|$|R
40|$|Certificate {{verification}} in PKI is {{a complex}} and time consuming process. In the classical PKI methodology, {{in order to obtain}} a public key and to accept a certificate as valid, a verifier needs to extract a certificate path from the PKI and to verify the certificates on this path recursively. Levi proposed a nested <b>certificate</b> <b>model</b> vvith the aim to simplify and speed up certificate verification. Such a nested certificate-based PKI significantly improves certificate verification, but it also requires a large {{increase in the number of}} issued certificates, which makes this model impractical for real life deployment. In order to solve this drawback of nested PKI, while retaining its speed in certificate verification, we propose in this paper the innovative concept of a compressed nested certificate, which is a significantly modified version of the nested <b>certificate</b> <b>model.</b> Compressed nested <b>certificate</b> PKI deploys compressed nested certificates which speed up and simplify certificate verification while keeping certificate load to a minimum, thus providing implementers the option of integrating it into the existing PKI model or building it separately as an independent model. <br /...|$|R
3000|$|... a {{declaration}} {{that this is}} a typeOf the OEM model (Fig.  3), thus enabling inheritance of the SensorML descriptions in the OEM <b>model</b> <b>documents,</b> and [...]...|$|R
50|$|The 421 {{was first}} {{certified}} on 1 May 1967 and shares a common type <b>certificate</b> with <b>models</b> 401, 402 411, 414 and 425.|$|R
5000|$|Although not {{certified}} until October 21, 1987, McDonnell Douglas {{had already}} applied for models DC-9-87 and DC-9-87F on February 14, 1985. The third derivative was similarly officially designated DC-9-87 (MD-87), although no nameplates were stamped DC-9-87. For the MD-88, {{an application for}} a type <b>certificate</b> <b>model</b> amendment was made after the earlier changes, so {{there was not a}} DC-9-88, which was certified on December 8, 1987. [...] The FAA's online aircraft registry database shows the DC-9-88 and DC-9-80 designations in existence but unused.|$|R
30|$|More {{details of}} the {{processing}} approach used for estimating the Swarm FAC products {{can be found in}} the Detailed Processing <b>Model</b> <b>Document</b> (Swarm Level 2 Processing System Consortium, 2012).|$|R
40|$|Currently deep {{learning}} has made great breakthroughs in visual and speech processing, mainly because it draws {{lessons from the}} hierarchical mode that brain deals with images and speech. In the field of NLP, a topic model {{is one of the}} important ways for <b>modeling</b> <b>documents.</b> Topic <b>models</b> are built on a generative model that clearly does not match the way humans write. In this paper, we propose Event Model, which is unsupervised and based on the language processing mechanism of neurolinguistics, to <b>model</b> <b>documents.</b> In Event <b>Model,</b> <b>documents</b> are descriptions of concrete or abstract events seen, heard, or sensed by people and words are objects in the events. Event Model has two stages: word learning and dimensionality reduction. Word learning is to learn semantics of words based on {{deep learning}}. Dimensionality reduction is the process that representing a document as a low dimensional vector by a linear mode that is completely different from topic models. Event Model achieves state-of-the-art results on document retrieval tasks...|$|R
40|$|OBJECTIVES: Compare the {{discrimination}} of risk-adjustment models for primary cesarean delivery derived from medical record data and birth certificate data and {{determine if the}} two types of models yield similar hospital profiles of risk-adjusted cesarean delivery rates. DATA SOURCES/STUDY SETTING: The study involved 29, 234 women without prior cesarean delivery admitted for labor and delivery in 1993 - 95 to 20 hospitals in northeast Ohio for whom data abstracted from patient medical records and data from birth certificates could be linked. STUDY DESIGN: Three pairs of multivariate models of the risk of cesarean delivery were developed using (1) the full complement of variables in medical records or birth certificates; (2) variables that were common to the two sources; and (3) variables for which agreement between the two data sources was high. Using each of the six models, predicted rates of cesarean delivery were determined for each hospital. Hospitals were classified as outliers if observed and predicted rates of cesarean delivery differed (p <. 05). PRINCIPAL FINDINGS: Discrimination of the full medical record and birth <b>certificate</b> <b>models</b> was higher (p <. 001) than {{the discrimination}} of the more limited common and reliable variable models. Based on the full medical record model, six hospitals were classified as statistical (p <. 01) outliers (three high and three low). In contrast, the full birth <b>certificate</b> <b>model</b> identified five low and four high outliers, and classifications differed for seven of the 20 hospitals. Even so, the correlation between adjusted hospital rates was substantial (r =. 71). Interestingly, correlations between the full medical record model and the more limited common (r =. 84) and reliable (r =. 88) variable birth <b>certificate</b> <b>models</b> were higher, and differences in classification of hospital outlier status were fewer. CONCLUSION: Birth certificates can be used to develop cesarean delivery risk-adjustment models that have excellent discrimination. However, using the full complement of birth certificate variables may lead to biased hospital comparisons. In contrast, limiting models to data elements with known reliability may yield rankings that are more similar to rankings based on medical record data...|$|R
50|$|Once a {{model is}} defined, {{one of the}} {{important}} operations on the model is to establish its validity. This involves checking whether all <b>model</b> <b>documents</b> satisfy the XML Schema and rule document constraints.|$|R
2500|$|TLA+ (pronounced as tee ell a plus, [...] ) is {{a formal}} {{specification}} language developed by Leslie Lamport. It {{is used to}} design, <b>model,</b> <b>document,</b> and verify concurrent systems. TLA+ {{has been described as}} exhaustively-testable pseudocode ...|$|R
50|$|<b>Document</b> <b>modelling</b> {{looks at}} the {{inherent}} structure in documents. It looks not at the structure in formatting which is the classic realm of word-processing tools, but at the structure in content. Because document content is typically viewed as the ad hoc result of a creative process, the art of <b>document</b> <b>modelling</b> {{is still in its}} infancy. Most <b>document</b> <b>modelling</b> {{comes in the form of}} document templates evidenced most often as word-processing documents, fillable PDF forms, and XML templates. The particular strength of XML in this context is its ability to <b>model</b> <b>document</b> components in a tree-like structure, and its separation of content and style.|$|R
30|$|The Segments class {{embodies the}} {{corresponding}} concept, <b>modeling</b> <b>documents</b> facets for the corresponding Description Units. The association between Description Units and Segments is visible as the Contents class. Segments are further specialized as text, image, video, and audio.|$|R
5000|$|Cultural Heritage: Collection of images, <b>documents</b> and 3D <b>models</b> <b>documenting</b> African {{heritage}} sites, including Timbuktu, Djenne, Lalibela, Kilwa Kisiwani, Lamu, and Elmina. This {{content area}} {{also includes a}} large collection of African Rock Art from many African nations.|$|R
5000|$|The {{definition}} of a battle lab provided by the DNBL operating <b>model</b> <b>document</b> is [...] "A combination of test capabilities brought together with operational end-users {{for the purpose of}} operator training and/or development/enhancement of operational concepts and procedures." ...|$|R
5000|$|... 2011 | Niranjan Damera-Venkata, José Bento and Eamonn O'Brien-Strain. Probabilistic <b>Document</b> <b>Model</b> for Automated <b>Document</b> Composition ...|$|R
50|$|A {{grade and}} tonnage model {{consists}} of the frequency distributions and relationships of the grades and sizes of completely explored individual mineral deposits of a given type. These <b>models</b> <b>document</b> how commonly different grades and tonnages occur by deposit type.|$|R
5000|$|Ability to do {{graphical}} modelling {{of business}} processes in Visual Studio, <b>model</b> <b>documents</b> with XML schemas, graphically mapping (with {{the assistance of}} functoids) between different schemas, and building pipelines to decrypt, verify, parse messages as they enter or exit the system via adapters.|$|R
40|$|Abstract. A {{very common}} issue of {{adaptive}} Web-Based systems is the <b>modeling</b> of <b>documents.</b> Such documents represent domain-specific information for a num-ber of purposes. Application {{areas such as}} Information Search, Focused Crawl-ing and Content Adaptation (among many others) benefit from several techniques and approaches to <b>model</b> <b>documents</b> effectively. For example, a document usu-ally needs preliminary processing {{in order to obtain}} the relevant information in an effective and useful format, so as to be automatically processed by the sys-tem. The objective of this chapter is to support other chapters, providing a basic overview of the most common and useful techniques and approaches related with <b>document</b> <b>modeling.</b> This chapter describes high-level techniques to <b>model</b> Web <b>documents,</b> such as the Vector Space Model and a number of AI approaches, such as Semantic Networks, Neural Networks and Bayesian Networks. This chapter is not meant to act as a substitute of more comprehensive discussions about the top-ics presented. Rather, it provides a brief and informal introduction to the main concepts of <b>document</b> <b>modeling,</b> also focusing on the systems that are presente...|$|R
40|$|International audienceThis paper proposes <b>models</b> for {{managing}} <b>documents</b> {{in a web}} engineering context. First, it proposes a <b>document</b> <b>model</b> to better manage multilingual composite documents. The approach, inspired by the FRBR report, is to group all versions, translations, formats, etc. of a document in a unique document tree, putting document data and metadata at the same level. Then it proposes a model for prototyping applications, {{using a combination of}} class-based inheritance and prototype programming principles. This <b>model</b> applies to <b>document</b> <b>models,</b> <b>documents</b> views and actions. Finally, it proposes a metadata management model, laying foundations for easier integration and management of information in web applications. The proposed models are implemented in the framework Sydonie and several applications are built with the model and framework...|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimited. This thesis solves a common issue in search applications. Typically, the user {{does not know}} exactly which terms are used in a document he is searching for. Several {{attempts have been made}} to overcome this issue by augmenting the <b>document</b> <b>model</b> and/or the query. In this thesis, a probabilistic topic <b>model</b> augments the <b>document</b> <b>model.</b> Probabilistic <b>document</b> <b>models</b> are formally introduced and inference methods are derived. It is shown how these models can be used for information retrieval tasks and how a search application can be implemented. A prototype was implemented and the implementation is tested and evaluated based on benchmark corpora. The evaluation provides empirical evidence that probabilistic <b>document</b> <b>models</b> improve the retrieval performance significantly, and shows which preprocessing steps should be made before applying the model. Outstanding ThesisGerman Army author...|$|R
40|$|Abstract: In text-based {{information}} retrieval, {{which is}} the predominant retrieval task at present, several <b>document</b> <b>models</b> have been proposed, such as boolean, probabilistic, or (extended) vector models [Baeza-Yates and Ribeiro-Neto 1999]. Interestingly, the suffix tree <b>document</b> <b>model</b> is usually not discussed {{in the literature on}} the subject though it comes along with a property that sets it apart from the other models: It encodes information about word order. The suffix tree <b>document</b> <b>model</b> owes much of its popularity from the Vivísimo search engine, which operationalizes on-the-fly categorization of Internet search results. While the classical <b>document</b> <b>models</b> can be considered as vectors of words, the suffix tree <b>document</b> <b>model</b> as well as the related similarity measures are graph-based. Both types of <b>document</b> <b>models</b> provide an efficient means to compute document similarities, and, according to various publications, both types of <b>document</b> <b>models</b> work well in practice. However, there is no comparison between both paradigms that explains the concepts of one in terms of the other, or that contrasts their advantages and disadvantages with respect to certain retrieval tasks. In this paper we start to tackle this gap by shading light on the following questions: (1) How does similarity computation work in the suffix tree <b>document</b> <b>model?</b> (2) Based on the insights of Question 1, is it possible to combine concepts of both <b>document</b> <b>model</b> types within classification or categorization tasks? (3) Which of the <b>document</b> <b>model</b> types is more powerful with respect to unsupervised document classification...|$|R
5000|$|TLA+ (pronounced as tee ell a plus, [...] ) is {{a formal}} {{specification}} language developed by Leslie Lamport. It {{is used to}} design, <b>model,</b> <b>document,</b> and verify concurrent systems. TLA+ {{has been described as}} exhaustively-testable pseudocode and blueprints for software systems; the TLA stands for [...] "Temporal Logic of Actions." ...|$|R
40|$|<b>Modeling</b> <b>document</b> {{structure}} {{is of great}} importance for discourse analysis and related applications. The goal {{of this research is}} to capture the document intent structure by <b>modeling</b> <b>documents</b> as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines. Comment: Accepted by AAAI 201...|$|R
40|$|Abstract: With {{the rapid}} growth of the {{wireless}} Internet service, the interest in security technology over the wireless Internet has been increased. Wireless Internet security technology provides users with confidentiality, authentication and non-repudiation based on WPKI(Wireless Public Key Infrastructure). To provide these services, the method that enables the wireless Internet using users to validate the other party’s certificate efficiently must be provided. But, there is no standard about the certificate validation using the mobile device over the wireless Internet environment. Therefore, we propose the <b>certificate</b> validation <b>model</b> applicable to the wireless Internet environment based on the previous <b>certificate</b> validation <b>model</b> based on the wired Internet environment. Key-Words: PKI, WPKI, mobile device, delegated certificate validation...|$|R
40|$|The {{following}} {{annotated bibliography}} lists documents {{prepared by the}} Department of Energy (DOE), and predecessor agencies, {{to meet the requirements}} of the National Environmental Policy Act (NEPA) for activities and facilities at Sandia National Laboratories sites. For each NEPA document summary information and a brief discussion of content is provided. This information may be used {{to reduce the amount of}} time or cost associated with NEPA compliance for future Sandia National Laboratories projects. This summary may be used to identify <b>model</b> <b>documents,</b> documents to use as sources of information, or documents from which to tier additional NEPA documents...|$|R
40|$|International audienceIn {{this paper}} {{we present a}} Neural Network approach, {{inspired}} by statistical physics of magnetic systems, to study fundamental problems of Natural Language Processing (NLP). The algorithm <b>models</b> <b>documents</b> as neural network whose Textual Energy is studied. We obtained good results on the application of this method to automatic summarization and Topic Segmentation...|$|R
5000|$|The award {{consists}} of a crystal <b>model,</b> <b>certificate,</b> and $10,000 honorarium. Deadline: 15 October of each year ...|$|R
40|$|Most legal tasks involve {{document}} {{preparation and}} review. Drafting effective texts {{is central to}} lawyering, judging, legislating, and regulating. How best to support that work with intelligent tools is an ancient topic in AI-and-Law research. For those tools to work, they must have good quality knowledge content to work with. Many alternative theories and techniques for <b>modeling</b> <b>documents</b> {{have been developed for}} particular kinds of situations. This article sketches a basic general theory of legal <b>document</b> <b>modeling,</b> with a focus on the key role of argumentation...|$|R
5000|$|Work {{products}} including architectures {{and architectural}} representations such as <b>models</b> and <b>documents</b> ...|$|R
40|$|We <b>model</b> <b>document</b> {{summarization}} as a nonlinear 0 - 1 programming problem {{where an}} objective function {{is defined as}} Heronian mean of the objective functions enforcing the coverage and diversity. The proposed model implemented on a multi-document summarization task. Experiments on DUC 2001 and DUC 2002 datasets showed that the proposed model outperforms the other summarization methods...|$|R
50|$|On 16 July 2015, the EPO and the Mexican Institute of Industrial Property (IMPI) {{agreed that}} as of January 2017, IMPI will {{classify}} patent documents currently being processed into the CPC, and begin to publish patent applications (A) and granted patent (B) and utility <b>model</b> <b>documents</b> in 2017. The data will be shared with the EPO.|$|R
50|$|Other notable {{features}} of RetrievalWare include distributed search servers, synchronizers for indexing external content management systems and relational databases, a heterogeneous security <b>model,</b> <b>document</b> categorization, real-time document-query matching (profiling), multi-lingual searches (queries containing terms from multiple languages searching for documents containing terms from multiple languages), and cross-lingual searches (queries in one language searching for documents {{in a different}} language).|$|R
50|$|While at Montpellier University in France, he {{took part}} in a program at the Massachusetts Institute of Technology (MIT) and {{received}} a <b>certificate</b> in <b>Modeling</b> and Simulation of Chemical Processes from MIT in 1977. A year later, he graduated with his PhD.|$|R
40|$|Abstract—In this {{position}} paper, we argue {{in favour of}} three points related to document-centric modelling of information systems: (i) information systems of some organizations may {{be understood in terms}} of documents, actions, actors, and <b>document</b> flow; (ii) <b>modelling</b> <b>document</b> flow may be a central step in modelling an information system; and (iii) as an approach to information system <b>modelling,</b> <b>document</b> flow <b>modelling</b> may be coupled with the form-based approach to information system development that is featured in IIS*Case, a model-driven devel-opment tool, because the form concept is semantically close to the document concept. With respect to these claims, we formulate a document-centric and model-driven approach to information system development, explain its particularities, and present a plan of activities that should lead to its implementation. By relying on domain-specific languages, the proposed approach should allow generation of information systems supporting document manipulation within a document flow and process mining. I...|$|R
