13|10000|Public
500|$|While {{the absence}} of {{ceramics}} appears anomalous, the presence of textiles is intriguing. Quipu (or khipu), string-based recording devices, have been found at Caral, suggesting a writing, or [...] "proto-writing", system at Norte Chico. (The discovery was reported by Mann in Science in 2005, but has not been formally published or described by Shady.) The exact use of quipu in this and later Andean cultures has been widely debated. Originally it {{was believed to be}} simply a mnemonic used to record numeric information, such as a <b>count</b> <b>of</b> <b>items</b> bought and sold. Evidence has emerged that the quipu may also have recorded logographic information in the same way writing does. Research has focused on the much larger sample of a few hundred quipu dating to Inca times; the Norte Chico discovery remains singular and undeciphered.|$|E
5000|$|The {{status bar}} of a file manager often shows the <b>count</b> <b>of</b> <b>items</b> {{in the current}} directory, their total size, or {{the size of the}} {{currently}} selected item.|$|E
50|$|Stem {{variables}} with numeric keys typically {{start at}} 1 {{and go up}} from there. The 0 key stem variableis used (by convention) as the <b>count</b> <b>of</b> <b>items</b> in the whole stem.|$|E
40|$|Spreadsheet {{containing}} a <b>count</b> <b>of</b> <b>item</b> {{records in the}} HELIN system by location, for the University Library, the CCE Library, and the Pell Marine Science Library. A count is presented on 11 dates between January 1999 and October 2007. Also included are partial <b>counts</b> <b>of</b> bibliographic records and holdings / checkin records...|$|R
25|$|The <b>count</b> <b>of</b> data <b>items</b> {{should be}} {{reported}} at the beginning (the first byte).|$|R
5000|$|... func Count(BMap) -> Univ_Integer is // Return <b>count</b> <b>of</b> number <b>of</b> <b>items</b> in map return BMap.Count; end func Count; ...|$|R
50|$|The selection-rejection {{algorithm}} {{developed by}} Fan et al. in 1962 requires single pass over data; however, it is a sequential algorithm and requires knowledge of total <b>count</b> <b>of</b> <b>items</b> n, {{which is not}} available in streaming scenarios.|$|E
50|$|Because {{counting}} sort uses key {{values as}} indexes into an array, {{it is not}} a comparison sort, and the Ω(n log n) lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the <b>count</b> <b>of</b> <b>items)</b> per bucket.|$|E
5000|$|The {{participants}} {{of the survey}} are {{divided into two groups}} at random. One group (the control group) is given a few harmless questions, while the other group gets one additional question (hence the name [...] "unmatched count"), the one about the property of interest. The respondents are to reveal only the number of [...] "yes"-answers they have given. Since the interviewer does not know how they arrived at that number, it is safe to answer the awkward question truthfully. Due to the unmatched <b>count</b> <b>of</b> <b>items,</b> {{the number of people who}} answered [...] "yes" [...] to the awkward question can be mathematically deduced.|$|E
5000|$|Binn {{structures}} {{consist of}} a list of elements. Each element has a type that can be followed by the size, <b>count</b> <b>of</b> internal <b>items,</b> and the data itself: ...|$|R
50|$|This {{application}} {{serves as}} the store's inventory mainframe. Using this program, pharmacy staff members can verify <b>counts</b> <b>of</b> various <b>items</b> and {{serves as the}} proprietary software for receiving and distribution within the company.|$|R
40|$|We {{present the}} Bitwise Bloom Filter, a data {{structure}} for maintaining counts {{for a large}} number <b>of</b> <b>items.</b> The bitwise filter {{is an extension of}} the Bloom filter, a space-efficient data structure for storing a large set efficiently by discarding the identity <b>of</b> the <b>items</b> being held while still being able to determine whether it is in the set or not, with high probability. We show how this idea can be extended to maintaining <b>counts</b> <b>of</b> <b>items</b> by maintaining a separate Bloom filter for every position in the bit representations <b>of</b> all the <b>counts.</b> We give both theoretical analysis of the accuracy of the Bitwise filter together with validation via experiments on real network data. ...|$|R
50|$|Overprecision is the {{excessive}} confidence that {{one knows the}} truth. For reviews, see Harvey (1997) or Hoffrage (2004).Much of the evidence for overprecision comes from studies in which participants are asked about their confidence that individual items are correct. This paradigm, while useful, cannot distinguish overestimation from overprecision; they {{are one and the}} same in these item-confidence judgments. After making a series of item-confidence judgments, if people try to estimate the number of items they got right, they do not tend to systematically overestimate their scores. The average of their item-confidence judgments exceeds the <b>count</b> <b>of</b> <b>items</b> they claim to have gotten right.One possible explanation for this is that item-confidence judgments were inflated by overprecision, and that their judgments do not demonstrate systematic overestimation.|$|E
5000|$|While {{the absence}} of {{ceramics}} appears anomalous, the presence of textiles is intriguing. Quipu (or khipu), string-based recording devices, have been found at Caral, suggesting a writing, or [...] "proto-writing", system at Norte Chico. (The discovery was reported by Mann in Science in 2005, but has not been formally published or described by Shady.) The exact use of quipu in this and later Andean cultures has been widely debated. Originally it {{was believed to be}} simply a mnemonic used to record numeric information, such as a <b>count</b> <b>of</b> <b>items</b> bought and sold. Evidence has emerged that the quipu may also have recorded logographic information in the same way writing does. Research has focused on the much larger sample of a few hundred quipu dating to Inca times; the Norte Chico discovery remains singular and undeciphered.|$|E
40|$|We {{introduce}} the measures share, coincidence and dominance as {{alternatives to the}} standard itemset methodology measure of support. We also redefine the confidence measure in this context. An itemset {{is a group of}} items bought together in a transaction. The support of an itemset is the ratio of transactions in which an itemset appears to the total number of transactions. The share of an itemset specifies the ratio of the <b>count</b> <b>of</b> <b>items</b> purchased together to the total <b>count</b> <b>of</b> <b>items</b> purchased in all transactions. The coincidence of an itemset is the ratio of the <b>count</b> <b>of</b> <b>items</b> in that itemset to the total of those same items in the database. The dominance of an item in an itemset specifies the extent to which that item dominates the total of all items in the itemset. Measures based on share have the advantage of reflecting accurately how many units are being moved by a business, a capability that current itemset methodology does not provide. In addition, the share can be extended to give an accurate picture of the financial impact of an itemset on the business bottom line...|$|E
40|$|This paper {{describes}} {{a model for}} studying the cache performance of algorithms in a direct-mapped cache. Using this model, we analyze the cache performance of several commonly occurring memory access patterns: (i) sequential and random memory traversals, (ii) systems of random accesses, and (iii) combinations of each. For each of these, we give exact expressions {{for the number of}} cache misses per memory access in our model. We illustrate the application of these analyses by determining the cache performance of two algorithms: the traversal of a binary search tree and the <b>counting</b> <b>of</b> <b>items</b> in a large array. Trace driven cache simulations validate that our analyses accurately predict cache performance. ...|$|R
40|$|Electricity market trade {{based on}} mobile {{intelligent}} device will extend {{the volume of}} transaction. For the massive and various trading data, transaction mining algorithm is very useful to find the relationship of correlative elements such as trade price and power capacity, and it always occurs between the power users and power generation enterprises. The novel FP-Table algorithm is proposed in this paper to solve the massive transaction mining problem. The FP-Table algorithm integrates the Hash table into FP-Growth algorithm, using two-dimension table saving frequency <b>count</b> <b>of</b> <b>item</b> pair, then mining the frequency <b>items</b> <b>of</b> electricity transactions efficiently. Applica-tion of mobile transaction mining is proved to be high efficiency and high value by performance experiment results...|$|R
50|$|A {{number of}} {{techniques}} {{have been used}} to correct phantom inventory problems, including physical cycle <b>counts,</b> RFID tagging <b>of</b> <b>items</b> and statistical modeling of phantom inventory conditions.|$|R
40|$|Abstract—the {{research}} was conducted using the self report of shoplifters who apprehended in the supermarket while stealing. 943 shoplifters in three years were interviewed right after the stealing act and before calling the police. The aim {{of the study is}} to know the shoplifting characteristics in Saudi Arabia, including the trait of shoplifters and the situation of the supermarkets where the stealing takes place. The analysis based on the written information about each thief as the documentary research method. Descriptive statistics as well as some inferential statistics were employed. The result shows that there are differences between genders, age groups, occupations, time of the day, days of the week, months, way of stealing, individual or group of thieves and other supermarket situations in the type of items stolen, total price and the <b>count</b> <b>of</b> <b>items.</b> The result and the recommendation will serve as a guide for retailers where, when and who to look at to prevent shoplifting. T Keywords—Shoplifting, stealing, theft, supermarket...|$|E
40|$|Multiple Sclerosis (MS) mainly affects {{people of}} working age. The Multiple Sclerosis Questionnaire for Job Difficulties (MSQ-Job) was {{designed}} to measure difficulties in work-related tasks. Our aim is to define cut-off score of MSQ-Job to identify potential critical situations that might require specific attention. A sample of patients with MS completed the MSQ-Job, WHODAS 2. 0 and MSQOL- 54 respectively for work difficulties, disability and health-related quality of life (HRQoL) evaluation. K-means Cluster Analysis was used to divide the sample in three groups {{on the basis of}} HRQoL and disability. ANOVA test was performed to compare the response pattern between these groups. The cut-off score was defined using the receiver operating characteristic (ROC) curve analyses for MSQ-Job total and count of MSQ-Job items scores ≥ 3 : a score value corresponding to the maximum of the sensitivity-to-specificity ratio was chosen as the cut-off. Out of 180 patients enrolled, twenty were clustered in the higher severity group. The area under the ROC curve was 0. 845 for the MSQ-Job total and 0. 859 for the count of MSQ-Job items scores ≥ 3 while the cut-off score was 15. 8 for MSQ-Job total and 8 for <b>count</b> <b>of</b> <b>items</b> scored ≥ 3. We recommend the use of MSQ-Job with this calculation as cut-off for identifying critical situations, e. g. in vocational rehabilitation services, where work-related difficulties have a significant impact in terms of lower quality of life and higher disability...|$|E
40|$|AbstractTwo main {{activities}} in retail business are {{to determine the}} amount of stock that should be maintained and the profit margin for each item. As the inventory principle stated, both processes required to make categories group of ‘fast moving’ and ‘slow moving’ and use it as the consideration for the processes. Citramart Minimarket as a retail business has not yet been used that grouping and consideration in their sales information system for processing their item's minimum stock level and profit margin. The process is still done manually by arbitrary observations from a stock division staff. The categories that is used to determine the minimum stock and profit margin are also not the moving speed of item but rather the kind of items. This study aim to support the process of determining the minimum stock and profit margin by building a model that can group items into categories ‘fast moving’ and slow moving’ using k-means clustering. K-means clustering is used in this study because the number of clusters required in categorization of items already set. The group cluster which has highest centroid will be the fast moving group, while the lowest centroid is the slow moving group. The data {{to be used in the}} research is taken from sales data for year 2014 and 2015. The clustering scenario uses combination of time, e. g. yearly and monthly, and variable, e. g. <b>count</b> <b>of</b> <b>items</b> and their transaction values. Using 3 clusters and delta value 0. 2, the resulting best scenario is using yearly time and transaction value variable. The test is conducted by calculating the xie-beny index which results 36. 265...|$|E
50|$|Devices {{have been}} used to aid {{computation}} for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented <b>counts</b> <b>of</b> <b>items,</b> probably livestock or grains, sealed in hollow unbaked clay containers. The use <b>of</b> <b>counting</b> rods is one example.The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.|$|R
40|$|Traditional {{attribute}} {{control charts}} {{are based on}} the monitoring of the number <b>of</b> nonconforming <b>items</b> in a sample of fixed size. In modern manufacturing processes, since items can be checked automatically, use of samples of a fixed subjective size with traditional charts is not suitable for on‐line continuous inspection. Furthermore, the sample size usually has to be large when the process fraction nonconforming is not reasonably high. If a decision is to be made only when a sufficient number <b>of</b> <b>items</b> are manufactured and inspected, many nonconforming items might have been produced. In this paper, a control scheme is presented based on the monitoring <b>of</b> cumulative <b>counts</b> <b>of</b> <b>items</b> inspected. This procedure will limit the number <b>of</b> consecutive nonconforming <b>items</b> to a small value when the process has suddenly deteriorated, can detect a sudden process shift quickly and is suitable for continuous inspection processes. The exact probability control limits are derived and the implementation procedure is described...|$|R
30|$|The eye {{movement}} data {{also suggested that}} the target search under high interference environment displayed the longer fixation time, more saccade counts and longer scan path, than under low interference environment. In addition, the first saccade time, the total fixation time and saccade <b>counts</b> <b>of</b> featured <b>items</b> search presented an increasing trend, {{which is the same}} as reaction.|$|R
40|$|The {{availability}} of large and rich quantities of text data {{is due to}} the emergence of the World Wide Web, social media, and mobile devices. Such vast data sets have led to leaps in the performance of many statistically-based problems. Given a large magnitude of text data available, it is computationally prohibitive to train many complex Natural Language Processing (NLP) models on large data. This motivates the hypothesis that simple models trained on big data can outperform more complex models with small data. My dissertation provides a solution to effectively and efficiently exploit large data on many NLP applications. Datasets are growing at an exponential rate, much faster than increase in memory. To provide a memory-efficient solution for handling large datasets, this dissertation show limitations of existing streaming and sketch algorithms when applied to canonical NLP problems and proposes several new variants to overcome those shortcomings. Streaming and sketch algorithms process the large data sets in one pass and represent a large data set with a compact summary, much smaller than the full size of the input. These algorithms can easily be implemented in a distributed setting and provide a solution that is both memory- and time-efficient. However, the memory and time savings {{come at the expense of}} approximate solutions. In this dissertation, I demonstrate that approximate solutions achieved on large data are comparable to exact solutions on large data and outperform exact solutions on smaller data. I focus on many NLP problems that boil down to tracking many statistics, like storing approximate counts, computing approximate association scores like pointwise mutual information (PMI), finding frequent items (like n-grams), building streaming language models, and measuring distributional similarity. First, I introduce the concept of approximate streaming large-scale language models in NLP. Second, I present a novel variant of the Count-Min sketch that maintains approximate counts of all items. Third, I conduct a systematic study and compare many sketch algorithms that approximate <b>count</b> <b>of</b> <b>items</b> with focus on large-scale NLP tasks. Last, I develop fast large-scale approximate graph (FLAG), a system that quickly constructs a large-scale approximate nearest-neighbor graph from a large corpus...|$|E
3000|$|In Line 1, {{scan the}} {{database}} D_y^' {{to obtain the}} support <b>count</b> <b>of</b> each <b>item</b> i, denoted as r_supp(i). In Line 2, compare r_supp(i) with the value of MRS(i) {{to determine whether the}} item i is frequent. Each rule-item with an r_supp(i) value {{greater than or equal to}} MRS(i) is inserted into frequent 1 -rule-item set L [...]...|$|R
50|$|A {{physical}} object (a possible referent of {{a concept}} or word) is considered concrete (not abstract) {{if it is}} a particular individual that occupies a particular place and time. However, in the secondary sense of the term 'abstraction', this physical object can carry materially abstracting processes. For example, record-keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented <b>counts</b> <b>of</b> <b>items,</b> probably livestock or grains, sealed in containers. According to , these clay containers contained tokens, the total of which were the <b>count</b> <b>of</b> objects being transferred. The containers thus served as something of a bill of lading or an accounts book. In order to avoid breaking open the containers for the count, marks were placed {{on the outside of the}} containers. These physical marks, in other words, acted as material abstractions of a materially abstract process of accounting, using conceptual abstractions (numbers) to communicate its meaning.|$|R
25|$|Devices {{have been}} used to aid {{computation}} for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented <b>counts</b> <b>of</b> <b>items,</b> probably livestock or grains, sealed in hollow unbaked clay containers. The use <b>of</b> <b>counting</b> rods is one example. The abacus was early used for arithmetic tasks. What we now call the Roman abacus was used in Babylonia as early as c. 2700–2300 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.|$|R
5000|$|Al-Masri {{was convicted}} in a British court on 8 February 2006 of several charges, {{including}} [...] "one <b>count</b> <b>of</b> possessing an <b>item</b> <b>of</b> potential use {{in an act of}} terrorism", referring to the Encyclopedia. He was sentenced to seven years in prison.|$|R
40|$|In high-yields process monitoring, the Geometric {{distribution}} is particularly useful {{to control the}} cumulative <b>counts</b> <b>of</b> conforming (CCC) <b>items.</b> However, in some instances the number of defects on a nonconforming observation is also of important application and must be monitored. For the latter case, {{the use of the}} generalized Poisson distribution and hence simultaneously implementation of two control charts (CCC & C charts) is recommended in the literature. In this paper, we propose an artificial neural network approach to monitor high-yields processes in which not only the cumulative <b>counts</b> <b>of</b> conforming <b>items</b> but also the number of defects on nonconforming items is monitored. In order to demonstrate the application of the proposed network and to evaluate the performance of the proposed methodology we present two numerical examples and compare the results with the ones obtained from the application of two separate control charts (CCC & C charts) ...|$|R
50|$|In {{the first}} pass, the {{algorithm}} <b>counts</b> occurrence <b>of</b> <b>items</b> (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances.Items {{in each instance}} have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly.Items in each instance that do not meet minimum coverage threshold are discarded.If many instances share most frequent items, FP-tree provides high compression close to tree root.|$|R
40|$|Observers saw 234 {{different}} {{pairs of}} stochastically organized dot patterns and indicated {{which of the}} two patterns appeared to be more numerous. All of the data can be accounted for by supposing that the choice of the more numerous pattern is based on the determination of the occupancy indices of both patterns. Each dot is posited to have an impact upon its neighborhood in a constant occupancy radius R. The area of the stimulus plane occupied collectively by all dots provides a basis for judging relative numerosity; the pattern with the larger occupancy value is chosen as more numerous. The occupancy model, besides providing a general explanation of known numerosity illusions in strictly quantitative terms, can explain some puzzling aspects of numerosity perception. Quantification {{is one of the most}} impressive acts of the human mind. On many occasions, however, the direct one-by-one <b>counting</b> <b>of</b> <b>items</b> is impossible: the number of objects is too large, the viewing time is too limited, the separation of already-counted objects from not-yetcounted ones is too difficult, and so forth. Nevertheless...|$|R
40|$|The {{challenges}} in mining frequent sequential patterns on data stream ap-plications include contending with limited memory for unlimited data, inability of mining algorithms to perform multiple scans of infinitely flowing original stream data set, to deliver current and accurate result on demand. Recent work on mining data streams exist for classification of stream data, clustering data streams, mining non-sequential frequent patterns over data streams, {{time series analysis}} on data stream. How-ever, only very limited {{attention has been paid}} to the problem of stream sequential mining in the literature. This paper proposes SSM-Algorithm (Sequential Stream Mining-algorithm) based on the efficient PLWAP se-quential mining algorithm, which uses three types of data structures (D-List, PLWAP tree and FSP-tree) to handle the complexities of mining frequent sequential patterns in data streams. It continuously summa-rizes frequency <b>counts</b> <b>of</b> <b>items</b> with the D-List, builds PLWAP tree and mines frequent sequential patterns of batches of stream records, main-tains mined frequent sequential patterns incrementally with FSP tree. Capabilities to handle varying time range querying, transaction fading mining, sliding window model are also added to the SSM-algorithm...|$|R
5000|$|For most purposes, Japanese uses Chinese numbers, {{rather than}} native numbers. However, native numbers {{are often used}} for <b>counting</b> numbers <b>of</b> <b>items</b> up to 10 - as in hitotsu, futatsu, mittsu (one item, two items, three items), notably days on the calendar, and with other Japanese counter words - and for various {{exceptions}} (fossils). These exceptions include 20 years old (hatachi), the 20th day of a month (hatsuka), greengrocer (literally [...] "800 store"), and {{last day of the}} year (literally [...] "big 30th day").|$|R
40|$|Many {{applications}} such as financial transactions data, customer click stream continuously generates huge data sets at very rapid rate. This is generally termed as data stream. These data sets are too huge to store on limited secondary storage. Therefore, it directs us to adopt a technique to maintain the sketch or summary of the data stream. This {{will allow us to}} answer any query, that we wish to perform on this data sets, approximately. Many applications on data streams such as <b>counting</b> <b>of</b> distinct <b>items,</b> estimating frequency moments [7, 8], the <b>counting</b> <b>of</b> frequent <b>items</b> [9, 3], clustering and the computation of histograms are of great interest among researchers. We consider k-median clustering problem in the geometric data stream. The task of the clustering is to partition a set of objects into disjoint group wherein the data objects in the same group are similar and objects in the different groups are dissimilar. We consider the k-median clustering algorithm in the context of data stream as proposed in [4]. We propose modified algorithm for 2 -dimension that achieve improved time and space bounds for k-median problem in [4]. We run the experiment by implementing the modified algorithm to see execution time...|$|R
40|$|The paper {{discusses}} the reasons of complexity rise in ERP system SAP R/ 3. It proposes {{a method for}} measuring complexity of SAP. Based on this method, the computer program in ABAP for measuring complexity of particular SAP implementation is proposed {{as a tool for}} keeping ERP complexity under control. The main principle of the measurement method is <b>counting</b> the number <b>of</b> <b>items</b> or relations in the system. The proposed computer program is based on <b>counting</b> <b>of</b> records in organization tables in SAP. </div...|$|R
