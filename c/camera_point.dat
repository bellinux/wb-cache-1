39|850|Public
25|$|The current stadium design {{offers an}} {{unobstructed}} {{view of the}} pitch from {{all areas of the}} ground. The Milburn stand is the location of the directors box and press boxes, and the main TV <b>camera</b> <b>point</b> for televised games.|$|E
5000|$|Texts: George Besson {{interviews}} artists including Auguste Rodin and Henri Matisse about Pictorial photography; Charles Caffin, [...] "The <b>Camera</b> <b>Point</b> of View in Painting and Photography"; miscellaneous others.|$|E
50|$|Rehearsals were single-take performances, filmed over fifteen days. Filmed in the mornings, {{with the}} actors fully involved, the footage was {{reviewed}} and {{discussed in the}} afternoons. Four separate monitors replayed each <b>camera</b> <b>point</b> of view simultaneously.|$|E
50|$|Fishcam {{refers to}} a {{broadcast}} consisting of a video <b>camera</b> <b>pointed</b> at a fish tank.|$|R
40|$|This paper {{presents}} a general strategy for assembling mosaics from numerous individual images where uncertainty {{exists in the}} position and orientation of those images. Both of the presented applications relate to remotely operated camera platforms, the first being the Galileo solid state imaging (SSI) camera presently in orbit around Jupiter, and the second being the Imager for Mars Pathfinder (IMP) stereo camera on Mars. A basic strategy in both applications {{is to determine the}} correct relative <b>camera</b> <b>pointing</b> followed by direct map projection of the images. It is assumed that approximate <b>camera</b> <b>pointing</b> exists sufficient to locate adjacent images and to place initial tiepoints within reach of the correlator. Spatial correlation is used to fix tiepoints whose initial locations are predicted by the <b>camera</b> <b>pointing.</b> We use either an fast fourier transform (fft) algorithm or a variant of Gruen's scheme permitting limited image rotation and skew. The Gruen correlator has three hierarchical modes: 1) A classical spatial least squares correlation on integral pixel boundaries used when rotation is small. 2) An annealing non-deterministic search used when rotations are unknown. A simplex deterministic search used for the end game. The correlation operation can be performed either interactively or autonomously. The final <b>camera</b> <b>pointing</b> solution relies upon a simplex downhill search in 2 n or 3 n dimensions where n is the number of images comprising the mosaic and the objective function to be minimized is the disagreement between tiepoint locations predicted from the <b>camera</b> <b>pointing</b> with those observed by the correlator. For Galileo the 3 n unknowns are euler angles defining <b>camera</b> <b>pointing</b> in planet coordinates, and for Mars Pathfinder they are 2 n unknowns representing commanded azimuth and elevation in the Lander coordinate system...|$|R
40|$|Introduction to {{the problem}} The image {{produced}} by a video <b>camera</b> <b>pointing</b> at black printing on a white sheet of paper is not truly black and white. This image is grey-scale (or colour) no matter where the <b>camera</b> is <b>pointed.</b> Unless lighting on the desk is carefully controlled, video images of paper lying on the desk are poo...|$|R
50|$|The current stadium design {{offers an}} {{unobstructed}} {{view of the}} pitch from {{all areas of the}} ground. The Milburn stand is the location of the directors box and press boxes, and the main TV <b>camera</b> <b>point</b> for televised games.|$|E
50|$|Bug AS is a Norwegian full-service {{production}} company established in 1995, By Gunnar Larsen in Bergen, Norway. In its startup period the company produced industrial 3d, Still Vizualisation and <b>Camera</b> <b>point</b> of view walkthrougs. Later such as idents for TV commercials and company presentations.|$|E
50|$|The {{object of}} the game is to collect stamps by going on rides {{throughout}} the park. To get on the rides, the player needs points, collected from picking up trash around the park and putting it into trash cans. The player can also meet and shake hands with costumed characters inside the park for additional points. The game uses a fixed camera that does not move or zoom in with the player. There are set camera vantages that the player moves in and out of by going outside of the field of view to go to the next <b>camera</b> <b>point.</b> Rides include Back to the Future: The Ride, Jaws, Jurassic Park River Adventure, and E.T. Adventure. Minigames include a Universal Studios quiz of film-related questions and puzzle games such as Concentration (memory match).|$|E
25|$|A modern data {{acquisition}} system usually contains a CCD/CMOS <b>camera</b> <b>pointed</b> to the screen for diffraction pattern visualization and a computer for data recording and further analysis.|$|R
50|$|To further deter {{employee}} theft {{the sale}} counter {{should also be}} equipped with a closed-circuit television <b>camera</b> <b>pointed</b> at the POS system to monitor and record all the activities.|$|R
40|$|We {{present a}} <b>camera</b> <b>pointing</b> system {{controlled}} by real-time calculations of sound source locations from a microphone array. Traditional audio localization techniques require explicit {{estimates of the}} spatial coordinates for each microphone in the array. In addition, positional information for the camera is needed to use such techniques to drive a <b>camera</b> <b>pointing</b> system. Sometimes this positioning {{can be done by}} hand, but for large aperture microphone arrays with many elements this is impractical. We show that in this setting, where elements are placed in an ad-hoc manner, explicitly learning the microphone positions is an unnecessary step. We give a calibration method whose focus is learning the mapping from time delays between pairs of microphones to the associated pan and tilt a PTZ-camera should be given to point at. This curtails the need to explicitly learn the microphone and camera positions. We use this method to calibrate a real-time <b>camera</b> <b>pointing</b> system used by the UCSD interactive display...|$|R
30|$|Use a CCD <b>camera</b> (<b>Point</b> Grey FL 2 - 08 S 2 M-C) with 4 -mm lens (uTron FV 0420) to take 12 {{photos of}} the model plane. The image {{resolution}} is 1024 × 768.|$|E
40|$|Abstract: The aim of {{this paper}} is to propose a new type of system used to monitor and {{determine}} the body torso movements and position without any physical contact and to demonstrate the validity and performance of our approach. Further, a genetic algorithm is used to compute, from the final result-ing image, the body position related to the video <b>camera</b> <b>point</b> of view...|$|E
3000|$|... [...]. To {{determine}} the extrinsic parameters of each projection, we fix the <b>camera</b> <b>point</b> of {{view to the}} spherical projection point of view, i.e., the local origin of the point cloud. We then sample the sphere with v equally distributed directions. The value of v depends on the required field of view and is estimated such that the images contained in the atlas completely cover the initial spherical image. Given the camera direction d [...]...|$|E
5000|$|Augmented Reality (AR) {{technology}} has been integrated into Dibidogs books and children's clothing. With a web <b>camera</b> <b>pointed</b> at a marker, children can interact with the characters on the screen.|$|R
40|$|Figure 1 : We {{present a}} novel yet simple {{mechanical}} technique for mitigating the interference when {{two or more}} Kinect <b>cameras</b> <b>point</b> at the same part of a physical scene. (a) Interference between overlapping structured light patterns from two regular Kinect <b>cameras</b> <b>pointing</b> at a person produces invalid and noisy depth pixels marked red. (b) Our method reduces noise and invalid pixels in the depth map. (c) The resulting point-cloud shows significant artifacts without our technique. (d) Point-cloud with our technique applied. (e) Our technique {{can be used to}} create an entire instrumented room with multiple overlapping Kinect cameras. (f) Meshed output accumulated from multiple Kinects shows reduced interference between cameras (color-coding indicates data from different cameras). We present a novel yet simple technique that mitigates the interference caused when multiple structured light depth <b>cameras</b> <b>point</b> at the same part of a scene. The technique is particularly useful for Kinect, where the structured light source is not modulated. Our technique requires only mechanical augmentation of the Kinect, without any need to modify the internal electronics, firmware or associated host software. It is therefor...|$|R
40|$|The MISR {{instrument}} {{consists of}} nine pushbroom cameras which measure radiance in four spectral bands. Global coverage is achieved in nine days. The cameras are arranged with one <b>camera</b> <b>pointing</b> toward the nadir, four <b>cameras</b> <b>pointing</b> forward and four <b>cameras</b> <b>pointing</b> aftward. It takes 7 minutes for all nine cameras {{to view the}} same surface location. The view angles relative to the surface reference ellipsoid, are 0, 26. 1, 45. 6, 60. 0, and 70. 5 degrees. The spectral band shapes are nominally gaussian, centered at 443, 555, 670, and 865 nm. The MISR Level 1 B 2 Geometric Parameters data product {{is part of the}} georectified radiance product. It contains the geometric parameters which measure the sun and view angles at the reference ellipsoid. [Location=GLOBAL] [Temporal_Coverage: Start_Date= 2000 - 02 - 24; Stop_Date=] [Spatial_Coverage: Southernmost_Latitude=- 90; Northernmost_Latitude= 90; Westernmost_Longitude=- 180; Easternmost_Longitude= 180] [Data_Resolution: Latitude_Resolution= 17. 6 km; Longitude_Resolution= 17. 6 km; Horizontal_Resolution_Range= 10 km - < 50 km or approximately. 09 degree - <. 5 degree; Temporal_Resolution=about 15 orbits/day; Temporal_Resolution_Range=Daily - < Weekly]...|$|R
40|$|This paper {{proposes a}} visual servoing {{algorithm}} for hand-eye robotic {{system based on}} epipolar geometry. The control law {{is based on the}} estimation of the epipoles position obtained by points correspondences extracted from the current and target images. The camera-robot motion is computed from the observation of the epipoles coordinates. Only the principal <b>camera</b> <b>point</b> is assumed to be known but not the other intrinsic parameters. Experimental results are reported to validate the visual servoing algorithm proposed...|$|E
40|$|Abstract. Quantitative SPECT imaging of 111 In is {{of growing}} {{importance}} in nuclear medicine applications like patient-specific radionuclide dosimetry. In this study, {{the quality of}} a Monte-Carlo based model of the imaging chain from nuclear decay to detection in a gamma camera is investigated. Important characteristics like the <b>camera</b> <b>point</b> spread function (PSF), the phantom attenuation and scatter effects are compared between simulation and experiment. The model shows a good agreement to the actual measurements and allows for further use in the developmentofquantitativeSPECTreconstructionwhichwillbemandatory for future oncology. ...|$|E
40|$|Part of the Hurley {{negative}} collection.; Condition: silvering.; See cover. "These three Interesting negatives {{look across}} to Qurna, Legendary Site of Garden of Eden. The three negatives, {{by a little}} manipulation {{can be made to}} join to form panorama. The picture at right looks up the great Tigris R. while the arm with boats is the Euphrates. The <b>Camera</b> <b>point</b> is at the Junction where the twin rivers meet to form the Shah-El-Arab. See diary page 133. " Only two negatives found. PIC FH/ 2601, left, PIC FH/ 2600, centre, right hand negative missing?.; Hurley series: Iraq, World War II...|$|E
40|$|Abstract: Unmanned aerial {{vehicles}} normally rely on GPS {{to provide}} pose information for navigation. In this work, we examine stereo visual odometry (SVO) {{as an alternative}} pose estimation method for situations in which GPS in unavailable. SVO is an incremental procedure that determines ego-motion by identifying and tracking visual landmarks in the environment, using cameras mounted on-board the vehicle. We present experiments demonstrating how SVO performance varies with <b>camera</b> <b>pointing</b> angle, for a robotic helicopter platform. Our results show that an oblique <b>camera</b> <b>pointing</b> angle produces better motion estimates than a nadir view angle, and that reliable navigation over distances of more than 200 meters is possible using visual information alone...|$|R
5000|$|For {{the last}} five years of RTÉ's 405-line simulcasting, a simple {{orthicon}} converter was used, essentially a 405-line <b>camera</b> <b>pointed</b> at a 625-line monitor, as the more expensive system converters that RTÉ had previously used were now inoperable.|$|R
50|$|A third {{alternate}} ending {{was written}} in which a possessed Katie would corner Micah and bludgeon him with his camera, while viewers watch from the <b>camera's</b> <b>point</b> of view. This version was deemed too complicated and too brutal to shoot.|$|R
40|$|We {{propose a}} method to recover the global {{structure}} with local details around a point. To handle a large scale of motion i. e. 360 degree around the point, we use an optimization-based algorithm to estimate the structure from the panorama around the fixed <b>camera</b> <b>point.</b> The global structure estimated can thus be used to initialize a structure from motion algorithm to recover the local details through simple camera motion such as panning. Synthetic as well as real data are {{used to test the}} validity of the algorithm. Our method can be used in applications such as authoring of virtual environments from real scene. 1...|$|E
40|$|Future 3 D Television {{applications}} {{will offer}} {{the viewer to}} freely choose his viewpoint during transmission. A lot of re-search {{in the field of}} 3 DTV therefore concentrated on cap-turing photo-realistic 3 D models of studio or movie sets. In this paper, however, we concentrate on the problem of self-localization within such scenes. As input we expect a 3 D model of an arbitrary environment. Therein, we are able to lo-calize a low-cost portable sensor-system based on a 3 D time-of-flight <b>camera.</b> <b>Point</b> clouds acquired with this system from arbitrary viewpoints are registered to the given model in order to estimate its position and orientation in the scene. Index Terms — Self-localization, 3 DTV, Scan registra...|$|E
40|$|In this paper, {{we address}} the problem of {{reflection}} removal and deblurring from a single image captured by a plenoptic camera. We develop a two-stage approach to recover the scene depth and high resolution textures of the reflected and transmitted layers. For depth estimation in the presence of reflections, we train a classifier through convolutional neural networks. For recovering high resolution textures, we assume that the scene is composed of planar regions and perform the reconstruction of each layer by using an explicit form of the plenoptic <b>camera</b> <b>point</b> spread function. The proposed framework also recovers the sharp scene texture with different motion blurs applied to each layer. We demonstrate our method on challenging real and synthetic images. Comment: ACCV 201...|$|E
50|$|Opposite the {{animator}} is {{a series}} of supporting arms and supports, on top of which is mounted a film or video <b>camera,</b> <b>pointing</b> down toward the artwork, which films the artwork, frame-by-frame, as it is slowly moved and changed by the operator.|$|R
50|$|Further sophistication in kite {{photography}} {{comes with}} live video and radio control features to control where the <b>camera</b> is <b>pointing.</b> This {{is superior to}} the minimal rig which simply clicks the camera every few minutes and must be hauled down to earth to change {{the direction in which}} the <b>camera</b> <b>points.</b> The penalty of the radio control rigs is weight, which requires higher winds to do photography. So in addition to clear skies, you must also have high winds, which will limit opportunities for photography.|$|R
5000|$|Rick Chernick, 1974, President and CEO, <b>Camera</b> Corner/Connecting <b>Point</b> ...|$|R
40|$|This paper {{presents}} {{the architecture of}} a pan/tilt/roll camera control system implemented on the Georgia Tech’s UAV research helicopter, the GTMax. The controller has currently three operating modes available: it can keep the camera at a fixed angle {{with respect to the}} helicopter, make the <b>camera</b> <b>point</b> {{in the direction of the}} helicopter velocity vector, or track a specific location. The camera is mounted in a large, but relatively light gimbal. Each axis is driven by a modified servo, and optical encoders measure the gimbal orientation. A PID controller with anti-windup and derivative filtering was designed in Simulink based on simple models of the servos and later implemented on the real system. A discussion of results obtained from Hardware-In-The-Loop tests and flight tests is given at the end of the paper. I...|$|E
40|$|Power {{consumption}} {{is a critical}} factor for the deployment of embedded computer vision systems. We explore the use of computational cameras that directly output binary gradient images to reduce {{the portion of the}} power consumption allocated to image sensing. We survey the accuracy of binary gradient cameras on a number of computer vision tasks using deep learning. These include object recognition, head pose regression, face detection, and gesture recognition. We show that, for certain applications, accuracy can be on par or even better than what can be achieved on traditional images. We are also the first to recover intensity information from binary spatial gradient images [...] useful for applications with a human observer in the loop, such as surveillance. Our results, which we validate with a prototype binary gradient <b>camera,</b> <b>point</b> to the potential of gradient-based computer vision systems...|$|E
40|$|Abstract — This paper {{describes}} a technique for the estima-tion of the translational and rotational velocities of a miniature helicopter using the video signals {{from a single}} onboard camera. For every two consecutive frames from the <b>camera,</b> <b>point</b> correspondences are identified and Epipolar Geometry based algorithms are used to find the likely estimates of the absolute rotations and relative displacements. Images from onboard camera are often corrupted with various types of noises; SIFT descriptors {{were found to be}} the best feature descriptors to be used for point correspondences. To speed up the processing, we introduce a new representation of these descriptors based on compressive sensing formalisms. To estimate the absolute displacement of the helicopter between frames, we use the measurements from a simulated IR sensor to find the true change in altitude of the body frame, scaling other translational dimensions accordingly, and later estimating the velocities. Experiments conducted using data from a real helicopter in an indoor environment demonstrate promising results. I...|$|E
40|$|Abstract – This paper {{presents}} three technical {{elements that}} we have developed to improve {{the accuracy of the}} visual target tracking for single-sol approach-and-instrument placement in future Mars rover missions. An accurate, straightforward method of rover mast calibration is achieved by using a total station, a camera calibration target, and four prism targets mounted on the rover. The method was applied to Rocky 8 rover mast calibration and yielded a 1. 1 -pixel rms residual error. <b>Camera</b> <b>pointing</b> requires inverse kinematic solutions for mast pan and tilt angles such that the target image appears right {{at the center of the}} camera image. Two issues were raised. Mast camera frames are in general not parallel to the masthead base frame. Further, the optical axis of the camera model in general does not pass through the center of the image. Despite these issues, we managed to derive non-iterative closed-form exact solutions, which were verified with Matlab routines. Actual <b>camera</b> <b>pointing</b> experiments over 50 random target image points yielded less than 1. 3 -pixel rms pointing error. Finally, a purely geometric method for camera handoff using stereo views of the target has been developed. Experimental test runs show less than 2. 5 pixels error on high-resolution Navcam for Pancam-to-Navcam handoff, and less than 4 pixels error on lower-resolution Hazcam for Navcam-to-Hazcam handoff. Index Terms- rover mast calibration, <b>camera</b> <b>pointing...</b>|$|R
5000|$|... 1:31 a.m. — A {{surveillance}} <b>camera</b> <b>pointed</b> along a {{chain-link fence}} around the substation recorded a streak of light that investigators from the Santa Clara County Sheriff's office think was a signal from a waved flashlight. It {{was followed by the}} muzzle flash of rifles and sparks from bullets hitting the fence.|$|R
50|$|Synthetic schlieren is {{a process}} that is used to {{visualize}} the flow of a fluid of variable refractive index. Named after the schlieren method of visualization, it consists of a digital camera or video <b>camera</b> <b>pointing</b> at the flow in question, with an illuminated target pattern behind. The method was first proposed in 1999.|$|R
