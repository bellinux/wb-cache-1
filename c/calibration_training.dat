29|94|Public
5000|$|<b>Calibration</b> <b>training</b> {{generally}} involves {{taking a}} battery of such tests. Feedback is provided between tests and the subjects refine their probabilities. <b>Calibration</b> <b>training</b> may also involve learning other techniques that help to compensate for consistent over- or under-confidence. Since subjects are better at placing odds when they pretend to bet money, subjects are taught how to convert calibration questions into a type of betting game which is shown to improve their subjective probabilities. [...] Various collaborative methods have been developed, such as prediction market, so that subjective estimates from multiple individuals can be taken into account.|$|E
5000|$|<b>Calibration</b> <b>training</b> {{improves}} subjective probabilities {{because most}} people are either [...] "overconfident" [...] or [...] "under-confident" [...] (usually the former). [...] By practicing {{with a series of}} trivia questions, it is possible for subjects to fine-tune their ability to assess probabilities. For example, a subject may be asked: ...|$|E
5000|$|Stochastic {{modeling}} {{methods such as}} the Monte Carlo method {{often use}} subjective estimates from [...] "subject matter experts". However, since research shows that such experts are very likely to be statistically overconfident, the model will tend to underestimate uncertainty and risk. The Applied Information Economics method systematically uses <b>calibration</b> <b>training</b> {{as part of a}} decision modeling process.|$|E
30|$|Four {{different}} {{cases were}} used to validate the developed correlation involving data that were not used for <b>calibration</b> or <b>training.</b> The results are compared with the available correlations in the literature, as described below.|$|R
30|$|Ten trainings {{were set}} for each network {{in order to}} improve chances of {{achieving}} the best <b>calibration.</b> The <b>training</b> map chosen for each structure was the one that provided the greatest efficiency of Nash-Sutcliffe (NASH) between calculated and observed sediment yields considering the calibration data set (Nash & Sutcliffe 1970).|$|R
50|$|Flattop is a natural-uranium-reflected, benchmarked, fixed-geometry {{critical}} assembly machine that can accommodate plutonium or uranium cores. The fast neutron spectrum {{is used to}} provide benchmarked neutronic measurements in spherical geometry with different fissile driver materials. Key missions for Flattop include fundamental reactor physics studies, sample irradiation for radiochemical research, actinide minimum critical mass studies, detector <b>calibration,</b> and <b>training.</b> The U-233 core is no longer usable because of its high gamma-ray activity.|$|R
5000|$|AIE as a whole, {{like many}} {{decision}} analysis and risk analysis methods, {{has little or}} no research showing the long term benefits of the method. [...] However, AIE itself is not new method and is based on previously-developed components that have a sound theoretical basis and/or have strong empirical evidence of improving on unaided intuition or other popular decision analysis methods. Among these components are Monte Carlo simulations, <b>calibration</b> <b>training,</b> information value calculations from decision theory, and widely accepted empirical methods used for scientific measurement (see references above).|$|E
40|$|The Occupational Requirements Survey (ORS) is an {{establishment}} {{survey conducted by}} the Bureau of Labor Statistics (BLS) for the Social Security Administration (SSA). The survey collects information on vocational preparation and the cognitive and physical requirements of occupations in the U. S. economy, as well as the environmental conditions in which those occupations are performed. <b>Calibration</b> <b>training</b> is a type of refresher training that compares interviewer performance against predetermined standards to assess rating accuracy, inter-rater reliability, and other measures of performance. This paper will review the results of three separate <b>calibration</b> <b>training</b> sessions that focused on a data collector’s ability to identify {{the presence or absence of}} physical demands and environmental conditions based on visual observation (assessed by watching job videos), assign Standard Occupational Classification (SOC) codes, and code Specific Vocational Preparation (SVP), which is a measure of the lapsed time required by a typical worker to reach average performance. Information obtained from these sessions was used to help evaluate training and mentoring programs, as well as to provide input into quality assurance procedures. However, the three <b>calibration</b> <b>training</b> sessions described in this paper generally showed minimal impact on performance measures used in the sessions...|$|E
40|$|Discovery of an {{accurate}} causal Bayesian network structure from observational {{data can be}} useful in many areas of science. Often the discoveries are made under uncertainty, which can be expressed as probabilities. To guide the use of such discoveries, including directing further investigation, it is important that those probabilities be well-calibrated. In this paper, we introduce a novel framework to derive calibrated probabilities of causal relationships from observational data. The framework consists of three components: (1) an approximate method for generating initial probability estimates of the edge types for each pair of variables, (2) the availability of {{a relatively small number of}} the causal relationships in the network for which the truth status is known, which we call a <b>calibration</b> <b>training</b> set, and (3) a calibration method for using the approximate probability estimates and the <b>calibration</b> <b>training</b> set to generate calibrated probabilities for the many remaining pairs of variables. We also introduce a new calibration method based on a shallow neural network. Our experiments on simulated data support that the proposed approach improves the calibration of causal edge predictions. The results also support that the approach often improves the precision and recall of predictions...|$|E
50|$|Supervised multivariate {{classification}} {{techniques are}} closely related to multivariate calibration techniques in that a <b>calibration</b> or <b>training</b> set is used to develop a mathematical model capable of classifying future samples. The techniques employed in chemometrics are similar to those used in other fields - multivariate discriminant analysis, logistic regression, neural networks, regression/classification trees. The use of rank reduction techniques in conjunction with these conventional classification methods is routine in chemometrics, for example discriminant analysis on principal components or partial least squares scores.|$|R
50|$|Jat Tehnika is {{approved}} {{in several countries}} and aviation authorities {{for the maintenance of}} aircraft, engines and components; also holds approvals for PART 21 Design Organisation, Equipement and Tools <b>Calibration</b> and Technical <b>Training.</b> The company has received the relevant certification from the countries/aviation authorities concerned.|$|R
40|$|This work {{addresses}} {{the problem of}} people counting in crowded situations, such as urban environments, in computer vision. As crowding density increases in a scene, it might become impossible to count people as single individuals: a global group-based approach is then preferable and in fact often necessary. A simple method for estimating the count of people in such tight crowds is here proposed, relying on accurate camera <b>calibration.</b> A <b>training</b> phase is also needed by the algorithm {{in order to learn}} the parameters needed for estimation...|$|R
40|$|Abstract: The {{purpose of}} this {{investigation}} was to evaluate the immediate effects of calibration on inter-rater agreement to a gold standard (GS) and to determine whether the effects can be sustained over a ten-week period. Valid criteria for a Class II amalgam preparation, a three-point rating scale, and a grade form were developed. Three tests were administered: prior to <b>calibration</b> <b>training,</b> immediately following training, and ten weeks later. Each test consisted of faculty independently evaluating ten prepared teeth. Agreement with GS scores for most of the grading criteria improved as a result of training and did not deteriorate over time. The overall percent agreement was 54. 5, 66. 9, and 64. 6 percent across test periods. The most impressive gains in agreement occurred when the criteria evaluated had a GS score of either “standard not met ” or “ideal. ” There was very little gain when the gold standard score was “acceptable. ” It is concluded that, with training, inter-rater agreement with a gold standard can improve and such improvement is reasonably resistant to deterioration after ten weeks. Nevertheless, future training ought to consider the use of a mastery approach in <b>calibration</b> <b>training</b> to ensure that a satisfactory degree of agreement with the GS is obtained...|$|E
40|$|In {{recent work}} on both {{generative}} and discriminative score to log-likelihood-ratio calibration, {{it was shown}} that linear trans-forms give good accuracy only for a limited range of operat-ing points. Moreover, these methods required tailoring of the <b>calibration</b> <b>training</b> objective functions in order to target the desired region of best accuracy. Here, we generalize the lin-ear recipes to non-linear ones. We experiment with a non-linear, non-parametric, discriminative PAV solution, as well as parametric, generative, maximum-likelihood solutions that use Gaussian, Student’s T and normal-inverse-Gaussian score dis-tributions. Experiments on NIST SRE’ 12 scores suggest that the non-linear methods provide wider ranges of optimal accu-racy and can be trained without having to resort to objective function tailoring. 1...|$|E
30|$|The overall {{objective}} of this research work {{is to find the}} atmospheric temperature of Tabuk using the other climatic factors. These minimum, maximum and mean values of monthly rainfall, relative humidity, wind speed, atmospheric temperature and atmospheric pressure data were fed as the inputs of the ANN model. Each data set was randomly partitioned into two sets, where 80 % of data out of 30  years data were used for model <b>calibration</b> (<b>training),</b> while the other 20 % was used for validation (testing). We developed an ANN model based on multilayer perceptron neural network architecture. The ANN model was trained using the Neural Network Toolbox in MATLAB. The Levenberg–Marquardt algorithm is used in this toolbox. The coefficient of determination (R 2) and the mean squared error (MSE) were used to test the developed ANN model.|$|E
40|$|Two chemometric methods, inverse {{least square}} and {{classical}} least square, {{were applied to}} simultaneous assay of clopidogrel bisulphate and aspirin in their combined dosage tablet formulation. Twelve mixed solutions were prepared for the chemometric <b>calibration</b> as <b>training</b> set and 10 mixed solutions were prepared as validation set. The absorbance data matrix was obtained by measuring the absorbance at 16 wavelength points, from 220 to 250 nm with the interval of 2 nm (Dl= 2 nm). The developed calibrations were successfully tested for laboratory mixtures as well as commercial tablet formulation for their clopidogrel bisulphate and aspirin concentration. Mean recoveries for clopidogrel bisulphate and aspirin {{were found to be}} in good agreement with the label claim...|$|R
40|$|The National Health and Nutrition Examination Survey (NHANES), {{developed}} by the Centers for Disease Control and Prevention, is a large and comprehensive health survey utilizing leading edge technologies to produce national estimates of health measures and the nutritional status of the United States population. Continuous quality assurance (QA) and quality control (QC) are the basic components to insure NHANES delivers high quality timely data. The QA activities before data collection consist of equipment <b>calibration</b> and <b>training,</b> while the QC activities during collection consist of automated software edits, data analysis of technician performance and analytic processing. These activities are tightly coupled in a continuous two-phase process through the data collection cycle, eventually leading to data release...|$|R
5000|$|Since the {{variables}} of interest are rarely observed within the time frame of the studies, out-of-sample validation mostly reduces to cross validation, whereby the model is initialized on {{a subset of the}} <b>calibration</b> variables (<b>training</b> set) and scored on the complimentary set (test set). The difficulty is in choosing the training/test set split. If the training set is small, then the ability to resolve expert performance is small and the PW of each training set poorly resembles the PW of the real study. If the test set is small then the ability to resolve differences in combination schemes is small. That said, [...] considered all splits of training/test sets, and showed that PW outperformed EW out-of-sample.|$|R
40|$|Periodontal probing depth is {{a measure}} of {{periodontitis}} severity. We develop a Bayesian hierarchical model linking true pocket depth to both observed and recorded values of periodontal probing depth, while permitting correlation among measures obtained from the same mouth and between duplicate examiners' measures obtained at the same periodontal site. Periodontal site-specific examiner effects are modeled as arising from a Dirichlet process mixture, facilitating identification of classes of sites that are measured with similar bias. Using simulated data, we demonstrate the model's ability to recover examiner site-specific bias and variance heterogeneity and to provide cluster-adjusted point and interval agreement estimates. We conclude with an analysis of data from a probing depth <b>calibration</b> <b>training</b> exercise. Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Learning {{probabilistic}} classification and prediction {{models that}} generate accurate probabilities {{is essential in}} many prediction and decision-making tasks in machine learning and data mining. One way {{to achieve this goal}} is to post-process the output of classification models to obtain more accurate probabilities. These post-processing methods are often referred to as calibration methods in the machine learning literature. This thesis describes a suite of parametric and non-parametric methods for calibrating the output of classification and prediction models. In order to evaluate the calibration performance of a classifier, we introduce two new calibration measures that are intuitive statistics of the calibration curves. We present extensive experimental results on both simulated and real datasets to evaluate the performance of the proposed methods compared with commonly used calibration methods in the literature. In particular, in terms of binary classifier calibration, our experimental results show that the proposed methods are able to improve the calibration power of classifiers while retaining their discrimination performance. Our theoretical findings show that by using a simple non-parametric calibration method, it is possible to improve the calibration performance of a classifier without sacrificing discrimination capability. The methods are also computationally tractable for large-scale datasets as they run in O(N log N) time, where N is the number of samples. In this thesis we also introduce a novel framework to derive calibrated probabilities of causal relationships from observational data. The framework consists of three main components: (1) an approximate method for generating initial probability estimates of the edge types for each pair of variables, (2) the availability of {{a relatively small number of}} the causal relationships in the network for which the truth status is known, which we call a <b>calibration</b> <b>training</b> set, and (3) a calibration method for using the approximate probability estimates and the <b>calibration</b> <b>training</b> set to generate calibrated probabilities for the many remaining pairs of variables. Our experiments on a range of simulated data support that the proposed approach improves the calibration of edge predictions. The results also support that the approach often improves the precision and recall of those predictions...|$|E
40|$|Abstract: This study {{evaluated}} {{the performance of}} dental students versus prosthodontics residents on a simulated caries removal exercise using a newly designed, 3 D immersive haptic simulator. The intent {{of this study was}} to provide an initial assessment of the simulator’s construct validity, which in the context of this experiment was defined as its ability to detect a statistically significant performance difference between novice dental students (n= 12) and experienced prosthodontics residents (n= 14). Both groups received equivalent <b>calibration</b> <b>training</b> on the simulator and repeated the same caries removal exercise three times. Novice and experienced subjects ’ average performance differed significantly on the caries removal exercise with respect to the percentage of carious lesion removed and volume of surrounding sound tooth structure removed (p< 0. 05). Experienced subjects removed a greater portion of the carious lesion, but also a greater volume of the surrounding tooth structure. Efficiency, defined as percentage of carious lesion removed over drilling time, improved significantly over the course of the experiment for both novice and experienced subjects (p< 0. 001). Within the limitations of this study, experienced subjects removed a greater portion o...|$|E
30|$|Artificial neural {{networks}} mimic their biological {{counterparts in the}} nervous system and the brains of animals and humans. They are used to estimate or approximate functions that depend on {{a large number of}} parameters, the effect of which is not clearly established or quantified. Due to their adaptive nature and ability to remember information introduced to them during <b>training</b> (<b>calibration),</b> ANNs can learn, generalize, categorize, and empirically predict values.|$|R
30|$|Imperfections in {{quadrature}} modulators (QMs), such as inphase and quadrature (IQ) imbalance, can severely impact {{the performance of}} power amplifier (PA) linearization systems, in particular in adaptive digital predistorters (PDs). In this paper, we first analyze the effect of IQ imbalance {{on the performance of}} a memory orthogonal polynomials predistorter (MOP PD), and then we propose a new adaptive algorithm to estimate and compensate the unknown IQ imbalance in QM. Unlike previous compensation techniques, the proposed method was capable of online IQ imbalance compensation with faster convergence, and no special <b>calibration</b> or <b>training</b> signals were needed. The effectiveness of the proposed IQ imbalance compensator was validated by simulations. The results clearly show the performance of the MOP PD to be enhanced significantly by adding the proposed IQ imbalance compensator.|$|R
50|$|After {{almost a}} month of trials, <b>calibrations,</b> and <b>training</b> {{along the coast of}} California, Skylark got {{underway}} for Pearl Harbor, Hawaii, on the morning of 20 December. The convoy arrived in Pearl Harbor ten days later, and Skylark remained in the Islands for another eleven days. On 10 January 1943, she stood out of Pearl Harbor and set course for Espiritu Santo in the New Hebrides, escorting another convoy. For the next year, Skylark escorted convoys around the various island groups in the South Pacific, the New Hebrides, Samoa, New Caledonia, and the Solomons, the conquest of which she was supporting. Often she shepherded supply echelons to Guadalcanal and to some of the other islands in the group, then would patrol the area for a week or two.|$|R
40|$|Autism {{spectrum}} disorder (ASD) currently affects nearly 1 in 160 children worldwide. In over {{two-thirds of}} evaluations, no validated diagnostics are used and gold standard diagnostic tools {{are used in}} less than 5 % of evaluations. Currently, the diagnosis of ASD requires lengthy and expensive tests, in addition to clinical confirmation. Therefore, fast, cheap, portable, and easy-to-administer screening instruments for ASD are required. Several {{studies have shown that}} children with ASD have a lower preference for social scenes compared with children without ASD. Based on this, eye-tracking and measurement of gaze preference for social scenes {{has been used as a}} screening tool for ASD. Currently available eye-tracking software requires intensive <b>calibration,</b> <b>training,</b> or holding of the head to prevent interference with gaze recognition limiting its use in children with ASD. In this study, we designed a simple eye-tracking algorithm that does not require calibration or head holding, as a platform for future validation of a cost-effective ASD potential screening instrument. This system operates on a portable and inexpensive tablet to measure gaze preference of children for social compared to abstract scenes. A child watches a one-minute stimulus video composed of a social scene projected on the left side and an abstract scene projected on the right side of the tablet's screen. We designed five stimulus videos by changing the social/abstract scenes. Every child observed all the five videos in random order. We developed an eye-tracking algorithm that calculates the child's gaze preference for the social and abstract scenes, estimated as the percentage of the accumulated time that the child observes the left or right side of the screen, respectively. Twenty-three children without a prior history of ASD and 8 children with a clinical diagnosis of ASD were evaluated. The recorded video of the child´s eye movement was analyzed both manually by an observer and automatically by our algorithm. This study demonstrates that the algorithm correctly differentiates visual preference for either the left or right side of the screen (social or abstract scenes), identifies distractions, and maintains high accuracy compared to the manual classification. The error of the algorithm was 1. 52 %, when compared to the gold standard of manual observation. This tablet-based gaze preference/eye-tracking algorithm can estimate gaze preference in both children with ASD and without ASD to a high degree of accuracy, without the need for <b>calibration,</b> <b>training,</b> or restraint of the children. This system can be utilized in low-resource settings as a portable and cost-effective potential screening tool for ASD...|$|E
40|$|Abstract: The {{purpose of}} this pilot {{study was to explore}} the impact of faculty <b>calibration</b> <b>training</b> on intra- and interrater reli-ability {{regarding}} calculus detection. After IRB approval, twelve dental hygiene faculty members were recruited from a pool of twenty-two for voluntary participation and randomized into two groups. All subjects provided two pre- and two posttest scorings of calculus deposits on each of three typodonts by recording yes or no indicating if they detected calculus. Accuracy and con-sistency of calculus detection were evaluated using an answer key. The experimental group received three two-hour training ses-sions to practice a prescribed exploring sequence and technique for calculus detection. Participants immediately corrected their answers, received feedback from the trainer, and reconciled missed areas. Intra- and interrater reliability (pre- and posttest) was determined using Cohen’s Kappa and compared between groups using repeated measures (split-plot) ANOVA. The groups did not differ from pre- to posttraining (intrarater reliability p= 0. 64; interrater reliability p= 0. 20). Training had no effect on reliability levels for simulated calculus detection in this study. Recommendations for future studies of faculty calibration when evaluating students include using patients for assessing rater reliability, employing larger samples at multiple sites, and assessing the impact on students ’ attitudes and learning outcomes...|$|E
40|$|Accurate {{prediction}} of water level fluctuation {{is important in}} lake management due to its significant impacts in various aspects. This study utilizes four model approaches to predict water levels in the Yuan-Yang Lake (YYL) in Taiwan: a three-dimensional hydrodynamic model, an artificial neural network (ANN) model (back propagation neural network, BPNN), a time series forecasting (autoregressive moving average with exogenous inputs, ARMAX) model, and a combined hydrodynamic and ANN model. Particularly, the black-box ANN model and physically based hydrodynamic model are coupled to more accurately predict water level fluctuation. Hourly water level data (a total of 7296 observations) was collected for model <b>calibration</b> (<b>training)</b> and validation. Three statistical indicators (mean absolute error, root mean square error, and coefficient of correlation) were adopted to evaluate model performances. Overall, the results demonstrate that the hydrodynamic model can satisfactorily predict hourly water level changes during the calibration stage {{but not for the}} validation stage. The ANN and ARMAX models better predict the water level than the hydrodynamic model does. Meanwhile, the results from an ANN model are superior to those by the ARMAX model in both training and validation phases. The novel proposed concept using a three-dimensional hydrodynamic model in conjunction with an ANN model has clearly shown the improved prediction accuracy for the water level fluctuation...|$|E
40|$|Imperfections in {{quadrature}} modulators (QMs), such as inphase and quadrature (IQ) imbalance, can severely impact {{the performance of}} power amplifier (PA) linearization systems, in particular in adaptive digital predistorters (PDs). In this paper, we first analyze the effect of IQ imbalance {{on the performance of}} a memory orthogonal polynomials predistorter (MOP PD), and then we propose a new adaptive algorithm to estimate and compensate the unknown IQ imbalance in QM. Unlike previous compensation techniques, the proposed method was capable of online IQ imbalance compensation with faster convergence, and no special <b>calibration</b> or <b>training</b> signals were needed. The effectiveness of the proposed IQ imbalance compensator was validated by simulations. The results clearly show the performance of the MOP PD to be enhanced significantly by adding the proposed IQ imbalance compensator...|$|R
40|$|The {{rejection}} of the image signal is a problem inherent to all receiver architectures. One {{of the benefits of}} the low-IF receiver is, that image rejection is realized by I/Q signal processing instead of a fixed analog filter, making it highly reconfigurable and cost-efficient. However, unavoidable imbalances between the I- and Q-branch lead to a limited image attenuation. In this paper a novel I/Q imbalance compensation scheme is presented, in which the unknown analog imbalance parameters are estimated digitally without the need for any <b>calibration</b> or <b>training</b> signal. Based on these estimates the interference by the image signal is effectively compensated, upgrading the vulnerable ordinary low-IF receiver to a powerful advanced receiver architecture, where the mean value of the image-to-signal ratio never exceeds a pre-defined maximum value, regardless of the image signal power. 1...|$|R
40|$|The {{problem of}} {{illumination}} estimation for colour constancy and automatic white balancing of digital color imagery {{can be viewed}} as the separation of the image into illumination and reflectance components. We propose using nonnegative matrix factorization with sparseness constraints (NMFsc) to separate the components. Since illumination and reflectance are combined multiplicatively, {{the first step is to}} move to the logarithm domain so that the components are additive. The image data is then organized as a matrix to be factored into nonnegative components. Sparseness constraints imposed on the resulting factors help distinguish illumination from reflectance. Experiments on a large set of real images demonstrate accuracy that is competitive with other illumination-estimation algorithms. One advantage of the NMFsc approach is that, unlike statistics- or learning-based approaches, it requires no <b>calibration</b> or <b>training...</b>|$|R
40|$|According to the EU {{directive}} {{on sustainable}} {{use of pesticides}} the pesticide users have to be trained on sprayer calibration. A high educative effect of trainings can only be achieved if trainees are motivated and interested in the training, and when the training is performed by the trainer with high competence and practical skills. A concept and programme of <b>calibration</b> <b>training</b> was developed, and then tested by performing the training for trainers. The event was organised in the Research Institute of Horticulture, Skierniewice, Poland, within the Safe Use Initiative (ECPA project). The 10 -hour training programme was composed so that the ratio of theory to practice was 40 %/ 60 %. The practical part was organized {{in a way to}} fully involve the trainees in calibration activities and by that let them gain skills and better understand the procedure. The programme included practical calibration of orchard sprayers by trainees divided into four 5 -person teams, followed by verification of the calibration effects during field experiment using water sensitive paper, analyzing he results, and making reports in form of PPT presentations. The elements of competition between the groups made the trainees active, creative and fully involved. The training was found to be instructive and enjoyable...|$|E
40|$|Abstract—Neural network {{classifiers}} {{have been}} shown to provide supervised classification results that significantly improve on traditional classification algorithms such as the Bayesian (maximum likelihood [ML]) classifier. While the predominant neural network architecture has been the feedforward multilayer perceptron known as backpropagation, Adaptive resonance theory (ART) neural networks offer advantages to the classification of optical remote sensing data for vegetation and land cover mapping. A significant advantage {{is that it does not}} require prior specification of the neural net structure, creating as many internal nodes as are needed to represent the <b>calibration</b> (<b>training)</b> data. The Gaussian ARTMAP classification algorithm bases the probability that input training samples belong to specific classes on the parameters of its Gaussian distributions: the means, standard deviations, and a priori probabilities. The performance of the Gaussian ARTMAP classification algorithm in terms of classification accuracy using independent validation data indicated was over 70 % accurate when applied to an annual series of monthly 1 -km advanced very high resolution radiometer (AVHRR) satellite normalized difference vegetation index (NDVI) data. The accuracies were comparable to those of fuzzy ARTMAP and a univariate decision tree, and significantly higher than a Bayesian classification algorithm. Algorithm testing is based on calibration and validation data developed and applied to Central America to map the International Geosphere-Biosphere Programme (IGBP) land cover classification system. Thus, it provides a realistic test of the algorithms for operational classification of a regional remote sensing and site dataset...|$|E
40|$|A chironomid-based <b>calibration</b> <b>training</b> set {{comprised}} of 100  lakes from south-western China was established. Multivariate ordination analyses {{were used to}} investigate {{the relationship between the}} distribution and abundance of chironomid species and 18  environmental variables from these lakes. Canonical correspondence analyses (CCAs) and partial CCAs showed that mean July temperature is one of the independent and significant variables explaining the second-largest amount of variance after potassium ions (K +) in 100  south-western Chinese lakes. Quantitative transfer functions were created using the chironomid assemblages for this calibration data set. The second component of the weighted-average partial least squares (WA-PLS) model produced a coefficient of determination (r 2 bootstrap) of 0. 63, maximum bias (bootstrap) of 5. 16 and root-mean-square error of prediction (RMSEP) of 2. 31  °C. We applied the transfer functions to a 150 -year chironomid record from Tiancai Lake (26 ° 38 ′ 3. 8  N, 99 ° 43 ′ E; 3898  m a. s. l.), Yunnan, China, to obtain mean July temperature inferences. We validated these results by applying several reconstruction diagnostics and comparing them to a 50 -year instrumental record from the nearest weather station (26 ° 51 ′ 29. 22 ′′ N, 100 ° 14 ′ 2. 34 ′′ E; 2390  m a. s. l.). The transfer function performs well in this comparison. We argue that this 100 -lake large training set is suitable for reconstruction work despite the low explanatory power of mean July temperature because it contains a complete range of modern temperature and environmental data for the chironomid taxa observed and is therefore robust...|$|E
50|$|Products {{produced}} by this segment {{are used by}} customers in their development of products and certain applications to characterize the products' mechanical properties. MTS' test systems simulate forces and motions that customers' products are expected to encounter in use. A test system includes a load frame to hold the prototype specimen, a hydraulic pump or electro-mechanical power source, piston actuators to create the force or motion, and a computer controller with specialized software to coordinate the actuator movement and record and manipulate results. MTS' Test segment sells a variety of accessories and spare parts, along with services that include installation, <b>calibration,</b> maintenance, <b>training</b> and consulting. The Test segment has a range of customers in the United States of America, Europe and Asia, which respectively represent approximately 30%, 30% and 40% of total orders.|$|R
40|$|In {{minimally}} invasive surgery (MIS), the ability to accurately interpret haptic information and apply appropriate force magnitudes onto soft tissue is critical for minimizing bodily trauma. Force perception in MIS is a dynamic {{process in which the}} surgeon’s administration of force onto tissue results in useful perceptual information which guides further haptic interaction and it is hypothesized that the compliant nature of soft tissue during force application provides biomechanical information denoting tissue failure. Specifically, the perceptual relationship between applied force and material deformation rate specifies the distance remaining until structural capacity will fail, or indicates Distance-to-Break (DTB). Two experiments explored the higher-order relationship of DTB in MIS using novice and surgeon observers. Findings revealed that observers could reliably perceive DTB in simulated biological tissues, and that surgeons performed better than novices. Further, through <b>calibration</b> feedback <b>training,</b> sensitivit...|$|R
40|$|Using {{a mobile}} device {{in a social}} context should not cause embarrassment and {{disruption}} to the immediate environment. Interaction with mobile and wearable devices needs to be subtle, discreet and unobtrusive. Therefore, we promote the idea of "intimate interfaces": discrete interfaces that allow control of mobile devices through subtle gestures {{in order to gain}} social acceptance. To achieve this goal, we present an electromyogram (EMG) based wearable input device which recognizes isometric muscular activity: activity related to very subtle or no movement at all. In the online experiment reported, the EMG device, worn on an armband around the bicep, was able to reliably recognize a motionless gesture without <b>calibration</b> or <b>training</b> across users with different muscle volumes. Hence, EMG-based input devices can provide an effective solution for designing mobile interfaces that are subtle and intimate, and therefore socially acceptable...|$|R
