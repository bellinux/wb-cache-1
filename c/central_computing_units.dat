1|1175|Public
40|$|This {{dissertation}} {{centers on}} residential city districts as source of flexible electrical energy demand and generation. More specifically, {{the focus is}} on the exploitation of operational flexibility provided by so-called electro-thermal heating systems like heat pumps (HPs) and combined heat and power (CHP) systems in combination with inexpensive thermal storage. Due to the high share of primary energy related to space heating in residential buildings, the electrification of the heating system is considered as a significant source of flexibility. First, this dissertation investigates the load shaping capability of clusters of Building Energy Systems (BESs) with different shares of electro-thermal HSs under a coordinated operation. The presented results show that heterogeneous clusters, comprising equally BESs with CHP and HP systems, provide the most load-shaping capability. Furthermore, the analyses revealed that clusters comprising less than 50 BESs are already sufficient in order to significantly provide load shaping. Thus, the clusters of BESs are capable to address local grid conditions like imbalances, related to the associated city district. Second, this thesis proposes a decentralized coordination approach, which has several advantages over existing approaches regarding data privacy, plug and play functionality and reduced demand for <b>central</b> <b>computing</b> <b>units.</b> In this approach, each BES, represented by a software agent, solves in a first step a local optimization problem, yielding a set of optimal or near-optimal operating schedules. In a second step, each agent determines by message exchange with other agents the actual schedule supporting the system objective the most. The simulation-based analysis shows that the algorithms exhibit a fast convergence and a good scalability. Third, a co-simulation platform is presented enabling the analysis of holistic simulations of city district energy systems. The simulation platform, referred to as MESCOS, allows simulating city district energy systems, comprising dynamic buildings models, energy supply infrastructure and control and energy management algorithms. In order to enable the simulation of large city district energy systems comprising hundreds of BESs, the platform exploits parallel computing features of modern simulation servers while at the same time reusing existing modeling and simulation tools. Finally, the simulation of a sample city district demonstrates the insights which can be gained by means of the introduced simulation platform...|$|E
5000|$|Most of the [...] "Hearables" [...] seen to date are Bluetooth {{devices that}} use phones or PCs as the <b>central</b> <b>computing</b> <b>unit.</b> Vinci smart headphones, {{announced}} in 2016, incorporated a dual-core CPU, local storage, WiFi and 3G connectivity that allow users to use without a phone.|$|R
40|$|Abstract- In {{this system}} using the {{features}} and technology of {{wireless sensor networks}} such as distributed and self-organization to build Heartbeat monitoring system, which can monitor the Heartbeat of human in real time. This system applies RF communication protocol and uses the 2. 4 GHZ Zigbee as RF transceiver. It has the characteristics of low power consumption, low cost, flexible structure and accurate measurement, and it can achieve the long- distance Heartbeat monitoring of human in real time. The wireless Heartbeat sensor node senses and transmits the variations in the human Heartbeat to the <b>central</b> <b>computing</b> <b>unit</b> within the range. The <b>central</b> <b>computing</b> <b>unit</b> receives the data and stores it in the file and plotting the variations simultaneously. The results are displayed in matlab. The system achieve heartbeat within the distance of 100 meters approximately and are successful to remove all wired logic for monitoring. this paper proposes a solution to upgrade existing health monitoring systems in hospitals by providing remote monitoring capability. Keywords- wsn, microcontroller, atmega, zigbee, biomedical, sensor node, MATLAB. 1...|$|R
40|$|CubeSat {{attitude}} control systems must be compact, fast, and accurate to achieve success in space missions with stringent control requirements. Nonlinear control strategies allow {{the creation of}} robust algorithms for orbit and {{attitude control}}, but can have higher processing requirements for effective operation. In addition, It is desirable to distribute components of a CubeSat control system across hardware {{as much as possible}} to decrease the load on a <b>central</b> <b>computing</b> <b>unit</b> and to make the system more tolerant to point failures. Field Programmable Gate Array technology has become a popular solution to the limited electronic space and demanding processing requirements present in new-generation CubeSat systems. This paper will focus on demonstrating the feasibility and effectiveness of a proposed nonlinear adaptive fuzzy controller implemented on a Field Programmable Gate Array as part of a highly-integrated space hardware system that is under development...|$|R
50|$|Mamram (ממר"ם), {{abbreviation}} for Center of Computing and Information Systems (מרכז מחשבים ומערכות מידע Merkaz Mahshevim UMa'arahot Meida), originally Center of Computing and Mechanized Registration (מרכז מחשבים ורישום ממוכן Merkaz Mahshevim VeRishum Memukhan) is the Israel Defense Forces' <b>central</b> <b>computing</b> system <b>unit,</b> providing {{data processing services}} for all arms and the general staff of the IDF. As of July 2015, Mamram is {{under the command of}} Colonel Talia Gazit.|$|R
40|$|Advanced motion {{tracking}} {{systems have been}} generally applied to movie production and digital entertainment. Due to the successful of Nintendo's Wii game machines, low cost motion tracking solutions are of particularly great demand in the computer gaming industry. In this paper, a low cost optical based motion capture system is proposed. The proposed system makes use of 2 high frame rate DLP projectors which project specially designed IR light patterns to the target object with IR sensing device attached. The pattern of illumination as recorded by the IR sensing device is converted to digital data words and sent via a wireless channel back to the <b>central</b> <b>computing</b> <b>unit</b> for estimating the 3 D coordinates of the target object. The system {{has the advantage of}} low cost with competitive performance in the update rate of the system. School of DesignDepartment of Electronic and Information EngineeringRefereed conference pape...|$|R
5000|$|Cohen {{was born}} in Jerusalem in 1968. During his {{military}} service {{he was part of}} the Israel Defense Forces' <b>central</b> <b>computing</b> system <b>unit,</b> Mamram, where he served in various positions for 6 years.In 1999, few years after his military service ended, Cohen founded CyberArk together with his high-school friend Udi Mokady. That year, Cohen invented and patented the Network Vault (US Patent 6,356,941), which serves as the basis for all of CyberArk’s products in the fields of Cyber Security, Data Security and Digital Warfare.Cohen was CyberArk’s Chief Executive Officer and later-on its Chairman of the Board of Directors for 14 years.|$|R
40|$|This {{paper is}} {{interested}} in maximizing the total throughput of cloud radio access networks (CRANs) in which multiple radio remote heads (RRHs) are connected to a <b>central</b> <b>computing</b> <b>unit</b> known as the cloud. The transmit frame of each RRH consists of multiple radio resources blocks (RRBs), and the cloud is responsible for synchronizing these RRBS and scheduling them to users. Unlike previous works that consider allocating each RRB to only a single user at each time instance, this paper proposes to mix the flows of multiple users in each RRB using instantly decodable network coding (IDNC). The proposed scheme is thus designed to jointly schedule the users to different RRBs, choose the encoded file sent in each of them, and {{the rate at which}} each of them is transmitted. Hence, the paper maximizes the throughput which is defined as the number of correctly received bits. To jointly fulfill this objective, we design a graph in which each vertex represents a possible user-RRB association, encoded file, and transmission rate. By appropriately choosing the weights of vertices, the scheduling problem is shown to be equivalent to a maximum weight clique problem over the newly introduced graph. Simulation results illustrate the significant gains of the proposed scheme compared to classical coding and uncoded solutions. Comment: 7 pages, 7 figure...|$|R
50|$|Mazin {{was born}} in Tel Aviv, Israel. During his {{military}} service in his early life, {{he was a member}} of Mamram, the Israel Defense Forces' <b>central</b> <b>computing</b> system <b>unit,</b> providing data processing services for all arms and the general staff of the IDF. In 1990, Mazin founded Memco Software, a now-defunct open-operating system security software company. In 1998, Memco was acquired by Platinum Technology in a stock-for-stock pooling of interests, valued at just more than $400 million. In 1999, Platinum was acquired by CA for $3.5 billion. After the acquisition, he moved on to establish GAMA Property. Later on, he co-founded Shadow Technologies with Eli Mashiah, the company hosting and managing Shadow.com. Ever since, Mazin has been investing in startups based in the technology industry.|$|R
40|$|We {{propose a}} consensus-based {{distributed}} voltage control (DVC), which solves {{the problem of}} reactive power sharing in autonomous meshed inverter-based microgrids with inductive power lines. Opposed to other control strategies available thus far, the DVC does guarantee reactive power sharing in steady-state while only requiring distributed communication among inverters, i. e. no <b>central</b> <b>computing</b> nor communication <b>unit</b> is needed. Moreover, we provide a necessary and sufficient condition for local exponential stability. In addition, {{the performance of the}} proposed control is compared to the usual voltage droop control [1] in a simulation example based on the CIGRE benchmark medium voltage distribution network...|$|R
40|$|Graduation date: 1973 The {{worst case}} {{analysis}} of rounding error of eleven numerical quadrature formulas on a computer with floating-point arithmetic, base b, and t digit mantissa is calculated. These results are applied to five different integrands using the digital computer CDC 3300 located at Oregon State University. <b>Central</b> processing <b>unit</b> <b>computing</b> time, and memory storage of the computer required by these eleven numerical quadrature formulas are examined also...|$|R
5000|$|Wilhelm Schickard {{designed}} and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. He {{may be considered}} the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he released his simplified arithmometer, {{which was the first}} calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and [...] "in less than two years, he had sketched out many of the salient features of the modern computer". [...] "A crucial step was the adoption of a punched card system derived from the Jacquard loom" [...] making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first computer program. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a <b>central</b> <b>computing</b> <b>unit.</b> When the machine was finished, some hailed it as [...] "Babbage's dream come true".|$|R
40|$|Abstract: In the {{beginning}} of the 1960 s it was decided that the multipurpose attack/fighter Saab 37 Viggen should be designed as a single seat aircraft. A central computer and a head-up display made it possible to dispense with the need for a human navigator. The computer was the <b>central</b> <b>computing</b> and integrating <b>unit</b> for all electronic equipment to support the pilot. This computer, CK 37 used in the Saab AJ 37, was the first airborne computer in the world to use integrated circuits (first generation ICs). Almost 200 computers were delivered 1970 - 1978. The function was reliable and the computers were still in operation, with upgrades, at the early beginning of 2000 `s...|$|R
40|$|We {{propose a}} consensus-based {{distributed}} voltage control (DVC) that solves {{the problem of}} reactive power sharing in autonomous inverter-based microgrids with dominantly inductive power lines and arbitrary electrical topology. Opposed to other control strategies available thus far, the control presented here does guarantee a desired reactive power distribution in steady state while only requiring distributed communication among inverters, i. e., no <b>central</b> <b>computing</b> nor communication <b>unit</b> is needed. For inductive impedance loads and under the assumption of small phase angle differences between the output voltages of the inverters, we prove that {{the choice of the}} control parameters uniquely determines the corresponding equilibrium point of the closed-loop voltage and reactive power dynamics. In addition, for the case of uniform time constants of the power measurement filters, a necessary and sufficient condition for local exponential stability of that equilibrium point is given. The compatibility of the DVC with the usual frequency droop control for inverters is shown and the performance of the proposed DVC is compared with the usual voltage droop control via simulation of a microgrid based on the Conseil International des Grands Réseaux Electriques (CIGRE) benchmark medium voltage distribution network...|$|R
40|$|The {{fleet of}} two 6 U CubeSats of the Terrestrial RaYs Analysis and Detection – TRYAD – project will explore the spatial {{structure}} of Terrestrial Gamma Ray Flashes produced by thunderstorms. At {{the core of}} this NSF supported project, is the ability to obtain simultaneous data from two CubeSats and VLF ground stations. Time synchronization is vital for correlating data from the three sources. The project is a collaboration between the University of Alabama in Huntsville (UAH) and Auburn University (AU). Also involved in the project are the NASA Goddard Space Flight Center and the Sci_Zone company. The UAH team (Drs. M. Briggs and P. Jenke) is in charge of developing the science instrument, determining the science data collection activities during flight, analyzing these data and publishing the results. The AU team (Drs. J-M Wersinger, M. Fogle, S. Biaz, and D. Harris) is in charge of developing the CubeSat platforms carrying the science instrument, of securing all interfaces, obtaining a launch, operating the satellites during flight, obtaining and forwarding data to UAH. NASA GSFC scientists and engineers, under the direction of Dr. G. de Nolfo are developing the gamma ray detector. Mr. A. Santangelo, CTO of Sci_Zone is providing and implementing the Linkstar radios (a duplex and a simplex) that will give access to science data in real time over 40 % of the orbit as well as a beacon everywhere along the orbit. The Linkstar radios are communicating through the network of Globalstar satellites. Mission success requires the ability to modify and control the separation of the two satellites along a common orbit. To this effect, the project is using differential drag. The two satellites, TRYAD 1 and TRYAD 2 are equipped with adjustable fins making them look like darts. Fin angle with the body of the satellites is adjusted through commands from the ground changing ionospheric drag on the satellites. The satellite with the largest drag loses more altitude than the other one, goes to a lower orbit and thus increases its speed. This speed difference allows for satellite separation modifications and control. At end of mission, both satellites’ fins are put into maximum drag configuration shortening the satellites’ lifetime in space. The satellite development is done by undergraduate students organized in technical and management teams. Each team has a lead and a deputy. New recruits are added twice a year, join a team and are trained by the senior members of their team. The project emphasizes good management practices and uses the NASA Systems Engineering approach. Most of the elements of the satellites are designed and built by the student teams. The only significant elements purchased are the Linkstar radios and DHV solar panels. The satellites carry over 100 sensors. The <b>central</b> <b>computing</b> <b>unit</b> is a BeagleBone Black (BBB) supplemented by two micro-controllers. Thermal control of satellite elements is obtained by radiative coupling with Earth through one of the large sides of the satellites and by heating elements controlled by the BBB...|$|R
40|$|A {{method and}} system for {{redundancy}} management is provided for a distributed and recoverable digital control system. The method uses unique redundancy management techniques to achieve recovery and restoration of redundant elements to full operation in an asynchronous environment. The system includes a first <b>computing</b> <b>unit</b> comprising {{a pair of}} redundant computational lanes for generating redundant control commands. One or more internal monitors detect data errors in the control commands, and provide a recovery trigger to the first <b>computing</b> <b>unit.</b> A second redundant <b>computing</b> <b>unit</b> provides the same features as the first <b>computing</b> <b>unit.</b> A first actuator control unit is configured to provide blending and monitoring of the control commands from {{the first and second}} <b>computing</b> <b>units,</b> and to provide a recovery trigger to each of the first and second <b>computing</b> <b>units.</b> A second actuator control unit provides the same features as the first actuator control unit...|$|R
40|$|The paper {{introduces}} {{our current}} research {{framework for the}} transition of functional <b>computing</b> <b>units</b> from legacy systems to a component-based software development environment. In this respect we discuss how various <b>computing</b> <b>units</b> from legacy information systems could be converted into independent software components. Based on this we formulate a framework within which extracted <b>computing</b> <b>units</b> could be gradually migrated as independent commercial off-the-shelf (COTS) ...|$|R
5000|$|The term EC2 <b>compute</b> <b>unit</b> (ECU) was {{introduced}} by Amazon EC2 as an abstraction of computer resources. Amazon's definition of an ECU notes that the company [...] "uses several benchmarks and tests to manage the consistency and predictability {{of the performance of}} an EC2 <b>Compute</b> <b>Unit.</b> One EC2 <b>Compute</b> <b>Unit</b> provides the equivalent CPU capacity of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor." ...|$|R
5000|$|Three {{to eight}} <b>Compute</b> <b>Units</b> (CUs) {{based on the}} revised GCN 2nd gen microarchitecture; 1 <b>Compute</b> <b>Unit</b> (CU) {{consists}} of 64 Unified Shader Processors : 4 Texture Mapping Units (TMUs) : 1 Render Output Unit (ROP) ...|$|R
30|$|Each Amazon Elastic Compute Cloud (EC 2) node {{provisioned}} has {{equivalent of}} 7.5 [*]GB of memory, 4 EC 2 <b>Compute</b> <b>Units</b> (2 virtual cores with 2 EC 2 <b>Compute</b> <b>Units</b> each), 850 [*]GB of local instance storage, 64 -bit platform.|$|R
40|$|While {{for single}} {{processor}} and SMP machines, memory is the allocatable quantity, for machines {{made up of}} large amounts of parallel <b>computing</b> <b>units,</b> {{each with its own}} local memory, the allocatable quantity is a single <b>computing</b> <b>unit.</b> Where virtual address management is used to keep memory coherent and allow allocation of more than physical memory is actually available, virtual communication channel references {{can be used to make}} <b>computing</b> <b>units</b> stay connected across allocation and swapping. Comment: 5 pages, 4 figure...|$|R
30|$|Tegra K 1 {{contains}} a recent GPGPU that adopts a unified shader architecture (Fig. 4). Unified shading architecture hardware {{is composed of}} an array of <b>computing</b> <b>units</b> (192 CUDA cores) which are capable of handling any type of shading tasks instead of dedicated vertex and fragment processor as in old GPUs. All <b>computing</b> <b>units</b> have the same characteristics. They can run either a fragment shader or a vertex shader. With a heavy vertex workload, we could allocate most <b>computing</b> <b>units</b> to run a vertex shader. In {{the case of a}} low vertex workload and a heavy fragment load, more <b>computing</b> <b>units</b> could be allocated to run fragment shader. In our work, we allocate more processing units to run the fragment shader to perform the desired parallel processing.|$|R
50|$|One <b>compute</b> <b>unit</b> {{combines}} 64 shader processors with 4 TMUs. The <b>compute</b> <b>unit</b> {{is separate}} from, but feed into, the Render output <b>units</b> (ROPs). Each <b>Compute</b> <b>Unit</b> {{consists of a}} CU Scheduler, a Branch & Message Unit, 4 SIMD Vector Units (each 16-lane wide), 4 64KiB VGPR files, 1 scalar unit, a 4 KiB GPR file, a local data share of 64 KiB, 4 Texture Filter Units, 16 Texture Fetch Load/Store Units and a 16 KiB L1 Cache. Four <b>Compute</b> <b>units</b> are wired to share an Instruction Cache 16 KiB in size and a scalar data cache 32KiB in size. These are backed by the L2 cache. A SIMD-VU operates on 16 elements at a time (per cycle), while a SU can operate on one a time (one/cycle). In addition the SU handles some other operations like branching.|$|R
50|$|Unified Shader Architecture allows more {{flexible}} {{use of the}} graphics rendering hardware. For example, in a situation with a heavy geometry workload the system could allocate most <b>computing</b> <b>units</b> to run vertex and geometry shaders. In cases with less vertex workload and heavy pixel load, more <b>computing</b> <b>units</b> could be allocated to run pixel shaders.|$|R
50|$|Tile {{processors}} are multicore or manycore {{chips that}} contain one-dimensional, or more commonly, two-dimensional arrays of identical tiles. Each tile comprises a <b>compute</b> <b>unit</b> (or a processing engine or CPU), caches and a switch. Tiles {{can be viewed}} as adding a switch to each core, where a core comprises a <b>compute</b> <b>unit</b> and caches.|$|R
40|$|Adaptive {{computers}} combine conventional software programmable processors with reconfigurable <b>compute</b> <b>units.</b> We present {{techniques that}} allow the high-performance realization of demand-paged, virtually addressed main memory shared between both of these processing elements. Furthermore, we have achieved low-latency communication between software running on the CPU and the reconfigurable <b>compute</b> <b>unit,</b> allowing even fine-grained hardware/software partitioning. A system-level evaluation quantifies the advantages of our approach...|$|R
5000|$|OpenCL views a {{computing}} system as {{consisting of a}} number of compute devices, which might be central processing units (CPUs) or [...] "accelerators" [...] such as graphics processing units (GPUs), attached to a host processor (a CPU). It defines a C-like language for writing programs. Functions executed on an OpenCL device are called [...] "kernels". A single compute device typically consists of several <b>compute</b> <b>units,</b> which in turn comprise multiple processing elements (PEs). A single kernel execution can run on all or many of the PEs in parallel. How a compute device is subdivided into <b>compute</b> <b>units</b> and PEs is up to the vendor; a <b>compute</b> <b>unit</b> {{can be thought of as}} a [...] "core", but the notion of core is hard to define across all the types of devices supported by OpenCL (or even within the category of [...] "CPUs"), and the number of <b>compute</b> <b>units</b> may not correspond to the number of cores claimed in vendors' marketing literature (which may actually be counting SIMD lanes).|$|R
40|$|Copy authorInternational audienceEmbedded {{computer}} vision based smart systems raise challenging issues in many research fields, including real-time vision processing, communication protocols or distributed algorithms. The {{amount of data}} generated by cameras using high resolution image sensors requires powerful computing systems to be processed at digital video frame rates. Consequently, the design of efficient and flexible smart cameras, with on-board processing capabilities, has become a key issue {{for the expansion of}} smart vision systems relying on decentralized processing at the image sensor node level. In this context, FPGA-based platforms, supporting massive data parallelism, offer large opportunities to match real-time processing constraints compared to platforms based on general purpose processors. In this paper, we describe the implementation, on such a platform, of a configurable object detection application, reformulated according to the dataflow model of computation. The application relies on the computation of the histogram of oriented gradients (HOG) and a linear SVM-based classification. It is described using the CAPH programming language, allowing efficient hardware descriptions to be generated automatically from high level dataflow specifications without prior knowledge of hardware description languages such as VHDL or Verilog. Results show that the performance of the generated code does not suffer from a significant overhead compared to handwritten HDL code. I. INTRODUCTION Traditional {{computer vision}}s system often operate in a centralized manner, even for multi-camera applications, where the sequences of frames output by each camera are sent to a <b>central</b> <b>computing</b> <b>unit.</b> This <b>central</b> unit gathers information from all the available cameras and processes it in order to extract significant features. However as the number of source nodes increases, such a centralized approach quickly becomes infeasible because the central node becomes a bottleneck. This is specially true when high resolution cameras with high acquisition rates are deployed, for instance in object detection applications. In this context, and in the current state of network technology, the necessity to meet real-time processing constraints rules out any kind of centralized approach. As a result, in the last years, many distributed video systems have been proposed. They aim at overcoming the above-mentioned bottleneck issue by distributing the computational intensive tasks on the camera nodes. Such nodes are generally called Smart Cameras (SC). Image processing capability is added by embedding processing units such as general purpose processors (GPP), specialized processors (DSP) or field programmable gate arrays (FPGA). The latter solution has drawn a lot of attention in the past years because it offers large opportunities for exploiting the fine grain, regular, parallelism that most of image processing applications exhibit at the lowest levels of processing. However, programming FPGA-based platforms is traditionally done using hardware description languages (HDLs) – see figure 1 – and therefore requires expertise in digital design. This, in practice, hinders the applicability of FPGA-based solutions. As a response, a lot of work has been devoted in the past decade to the design and development of high-level languages and tools, aiming at allowing FPGAs to be used by programmers who are not experts in digital design, such as Catapult-C [1], Stream-C [2] or Impulse-C [3]. Most of these tools propose a direct conversion of C or C++ code into HDL (VHDL or Verilog). While attractive, this approach suffers from several drawbacks. First, C programs often rely on features which are difficult, if not impossible, to implement in hardware (dynamic memory allocation for instance). This means that code frequently has to be rewritten to be accepted by the compilers. Practically, this rewriting cannot be carried out without understanding why certain constructs have to be avoided and how to replace them by " hardware-compatible " equivalents. So a minimum knowledge of hardware design principles is actually required. Second, C is intrinsically sequential whereas hardware is truly parallel. In the current state-of-the-art, this cannot be done in a fully automatic way and the programmer is required to put annotations (pragmas) in the code to help the compiler, whic...|$|R
5000|$|Central library which {{possesses}} rare books, <b>Central</b> <b>Computing</b> and Internet facilities.|$|R
5000|$|NVIDIA Tesla M2090 Graphical an {{emerging}} paradigm of parallel computing which uses GPUs as <b>computing</b> <b>units</b> ...|$|R
40|$|This paper accelerates a {{scalable}} GF(p) Montgomery inversion hardware. The hardware {{is made of}} {{two parts}} a memory and a <b>computing</b> <b>unit.</b> We modified the original memory unit to include parallel shifting of all bits which was a task handled by the <b>computing</b> <b>unit.</b> The new hardware modeling, simulating, and synthesizing is performed through VHDL for several 160 -bits designs showing interesting speedup to the inverse computation...|$|R
5000|$|... the Arctic Region Supercomputing Center, {{located within}} the Geophysical Institute, is the {{high-performance}} <b>computing</b> <b>unit</b> of UAF.|$|R
40|$|This paper {{presents}} {{implementation of}} one pulse coupled neural network (PCNN) neuron into Virtex 4 FPGA device on ML 403 development board. The implementation {{consists of three}} main units: <b>Computing</b> <b>unit</b> (CU), convolution <b>computing</b> <b>unit</b> (CCU) and MicroBlaze soft microprocessor. Neuron works with fixed point arithmetic, and is designed with the accent to effective utilization of the FPGA. The data transfer is effectively made by using DMA transfer unit...|$|R
30|$|Classical SLAM {{algorithms}} are too computationally intensive {{to run on}} an embedded <b>computing</b> <b>unit.</b> They require {{at least}} laptop-level performances. Gifford et al.[11] present a low-cost approach to autonomous multi-robot mapping and exploration for unstructured environments. The robot hosts a Gumstix <b>computing</b> <b>unit</b> (600 Mhz), 6 IR scanning range arrays, a 3 -axis gyroscope and odometers. Running DP-SLAM alone on the Gumstix with 15 particles takes on average 3 s per update. While using 25 particles, {{it takes more than}} 10 s per update. Authors have underlined the difficulty to find the right SLAM parameters to fit within the available computing power and the real-time processing. Magnenat et al.[12] present a system based on the co-design of a low-cost sensor (a slim rotating scanner), a SLAM algorithm, a <b>computing</b> <b>unit,</b> and an optimization methodology. The <b>computing</b> <b>unit</b> is based on an ARM processor (533 Mhz) running a FASTSLAM 2.0 algorithm[13]. Magnenat et al.[12] use an evolution strategy to find the best configuration of the algorithm and setting of the parameters.|$|R
40|$|The {{design of}} {{embedded}} systems {{is a complex}} activity that involves a lot of decisions. With high performance demands of present day usage scenarios and software, they often involve energy hungry state-of-the-art <b>computing</b> <b>units.</b> While focusing on power consumption of <b>computing</b> <b>units,</b> the physical properties of software are often ignored. Recently, {{there has been a}} growing interest to quantify and model the physical footprint of software (e. g. consumed power, generated heat, execution time, etc.), and a component based approach facilitates methods for describing such properties. Based on these, software architects can make energy-efficient software design solutions. This paper presents power consumption and execution time profiling of a component software that can be allocated on heterogeneous <b>computing</b> <b>units</b> (CPU, GPU, FPGA) of a tracked robot...|$|R
40|$|We present {{original}} {{approach to}} the design {{and implementation of the}} convolution <b>computing</b> <b>unit</b> in this article. Convolution <b>computing</b> <b>unit</b> (further CCU) is the part of the digital neuron structure and limits the speed of the computation cycle. For that reason we have designed the architecture of the CCU with high computing rate. System CCU was implemented in to the Virtex- 4 FX 12 FPGA. The convolution computing can work up to 200 MHz...|$|R
