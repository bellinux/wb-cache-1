18|10000|Public
5000|$|Substantive {{analytical}} procedures {{are used to}} obtain evidential matter about particular assertions related to account balances or <b>classes</b> <b>of</b> <b>transactions.</b>|$|E
5000|$|Assertions used by {{the auditor}} fall into the {{following}} categories:(a) Assertions about <b>classes</b> <b>of</b> <b>transactions</b> and events for the period ended: ...|$|E
50|$|It is {{stated in}} ISA 315 (paragraph A.124) that the auditor should use assertions for <b>classes</b> <b>of</b> <b>transactions,</b> account balances, and {{presentation}} and disclosures in sufficient detail {{to form a}} basis for the assessment ofrisks of material misstatement and the design and performance of further audit procedures.|$|E
5000|$|When {{the auditor}} {{is unable to}} obtain audit {{evidence}} regarding particular account balance, <b>class</b> <b>of</b> <b>transaction</b> or disclosure {{that does not have}} pervasive effect on the financial statements.|$|R
5000|$|When the {{financial}} statements are materially misstated due to misstatement in one particular account balance, <b>class</b> <b>of</b> <b>transaction</b> or disclosure {{that does not have}} pervasive effect on {{the financial}} statements.|$|R
50|$|Financial Reinsurance (or fin re), {{is a form}} of {{reinsurance}} {{which is}} focused more on capital management than on risk transfer. In the non-life segment of the insurance industry this <b>class</b> <b>of</b> <b>transactions</b> {{is often referred to as}} finite reinsurance.|$|R
50|$|Certain <b>classes</b> <b>of</b> <b>transactions</b> are exempt, {{or may be}} {{exempted}} on application. For example, established customers transacting amounts {{typical of}} their lawful business, such as for payroll, or retail or vending machine takings, etc. Motor vehicle traders are specifically not eligible for exemption, as are boats, farm machinery and aircraft traders.|$|E
5000|$|For audits {{performed}} by an outside audit firm, risk assessment {{is a crucial}} stage before accepting an audit engagement. According to ISA315 Understanding the Entity and its Environment and Assessing the Risks of Material Misstatement, [...] "the auditor should perform risk assessment procedures to obtain {{an understanding of the}} entity and its environment, including its internal control". Evidence relating to the auditor’s risk assessment of a material misstatement in the client’s financial statements. Then, the auditor obtains initial evidence regarding the <b>classes</b> <b>of</b> <b>transactions</b> at the client and the operating effectiveness of the client’s internal controls. Audit risk is defined as the risk that the auditor will issue a clean unmodified opinion regarding the financial statements, when in fact the financial statements are materially misstated, and therefore do not qualify for a clean unmodified opinion. As a formula, audit risk is the product of two other risks: Risk of Material Misstatement and Detection risk. This formula can be further broken down as follows: inherent risk × control risk × detection risk.|$|E
40|$|The rapid {{increase}} {{in the size of}} U. S. companies from the early twentieth century created the need for audit procedures based on the selection of a part of the total population audited to obtain reliable audit evidence, to characterize the entire population consists of account balances or <b>classes</b> <b>of</b> <b>transactions.</b> Sampling is not used only in audit – is used in sampling surveys, market analysis and medical research in which someone wants to reach a conclusion about a large number of data by examining only a part of these data. The difference is the “population” from which the sample is selected, ie that set of data which is intended to draw a conclusion. Audit sampling applies only to certain types of audit procedures...|$|E
50|$|Management implicitly {{assert that}} account {{balances}} and underlying <b>classes</b> <b>of</b> <b>transaction</b> {{do not contain}} any material misstatements: in other words, that they are materially complete, valid and accurate. Auditors gather evidence about these assertions by undertaking activities referred to as substantive procedures.|$|R
40|$|AbstractIt {{is often}} {{necessary}} to ensure that database transactions preserve integrity constraints that specify valid database states. While {{it is possible to}} monitor for violations of constraints at run-time, rolling back transactions when violations are detected, it is preferable to verify correctness statically,beforetransactions are executed. This can be accomplished if we can verify transaction safety with respect to a set of constraints by means of calculatingweakest preconditions. We study properties of weakest preconditions for a number <b>of</b> <b>transaction</b> and specification languages. We show that some simple transactions do not admit weakest preconditions over first-order logic and some of its extensions such as first-order logic with counting and monadicΣ 11. We also show that the <b>class</b> <b>of</b> <b>transactions</b> that admit weakest preconditions over first-order logic cannot be captured by any transaction language. We consider a strong local form of verifiability, and show that it is different from the general form. We define robustly verifiable transactions as those that can be statically analyzed regardless of extensions to the signature of the specification language, and we show that the <b>class</b> <b>of</b> robustly verifiable <b>transactions</b> over first-order logic is exactly the <b>class</b> <b>of</b> <b>transactions</b> that admit the local form of verifiability. We discuss the implications of these results for the design <b>of</b> verifiable <b>transaction</b> languages...|$|R
40|$|Relational update <b>transactions</b> {{consisting}} <b>of</b> line {{programs of}} inserts, deletes, and modifications are studied {{with respect to}} equivalence and simplification. A sound and complete set of axioms for proving transaction equivalence is exhibited. The axioms yield a set of simplification rules {{that can be used}} to optimize efficiently a large <b>class</b> <b>of</b> <b>transactions</b> <b>of</b> practical interest. The simplification rules are particularly well suited to a dynamic environment where transactions are presented in an on-line fashion, and where the time available for optimization may consist of arbitlcarily short and sparse intervals...|$|R
40|$|Evidence from {{research}} in psychology and auditor judgment {{has shown that}} perceptions that form early in a sequential judgment process can influence subsequent judgments. Auditing Standard 12 requires auditors to identify fraud risk factors and assess the risk of fraud {{as part of the}} process of assessing overall misstatement risk. While it is expected that fraud risk assessments should have a bearing on overall risk assessments, it is possible that perceptions formed from assessments of fraud risk can negatively affect the evaluation of any evidence reviewed thereafter. Because different <b>classes</b> <b>of</b> <b>transactions</b> may be affected by fraud risk factors in different ways, fraud risk assessments may differ across <b>classes</b> <b>of</b> <b>transactions.</b> These differences may make subsequent auditor judgments susceptible to the contrast effects bias, where subjects overreact to the differences such that the fraud risk assessments influence auditor judgment more than they should. This study examines whether auditors who learn that fraud risk is low for one class of transactions immediately after examining a class of transactions that has high fraud risk, can overreact to the contrast such that they reduce their sensitivity to evidence that suggests increased misstatement risk. The study also examines whether these contrast effects can be mitigated by acquiring information about fraud risk assessments later in the sequence of evidence, after auditors have reviewed and assimilated evidence related to other risks. The study finds that, as predicted, auditor judgments are influenced by contrast effects. Auditors who examined classes of accounts for which fraud risk assessments were different were less sensitive to evidence suggesting increased risk in accounts that had been identified as having low fraud risk. However, contrary to predictions, these contrast effects were not mitigated by evidence order...|$|E
40|$|International audienceHybrid Transactional Memory (TM) uses {{available}} hardware TM {{resources to}} execute language-level transactions, and falls {{back to a}} software TM implementation for those transactions that cannot complete in hardware. Ideally, a hybrid TM would allow hardware and software transactions to run concurrently, but would not waste hardware TM resources on coordination between the two <b>classes</b> <b>of</b> <b>transactions.</b> In addition, it should scale well, incur little latency, offer strong safety guarantees, and provide some degree of fairness. We introduce a new hybrid TM algorithm, “Hybrid Cohorts”, in which hardware transactions do not modify global metadata, and software transactions have ex- tremely low per-access overhead. The tradeoff is that hardware transactions cannot commit while software transactions are in flight. Evaluation on an 8 -thread Intel Haswell CPU shows competitive performance with the current state-of-the-art. Furthermore, it does so while providing acceptable levels of fairness and safety, and offering opportunities for hardware acceleration...|$|E
40|$|The {{most recent}} advancements in {{computer}} hardware and communications make the mobile computing paradigm tangible and feasible. One {{of the major}} factors affecting mobile computing is communication protocols efficiency. This paper proposes and discusses an Adaptive Queuing Protocol which targets advanced mobile database applications. The protocol has two main objectives. Firstly, {{to compensate for the}} relatively slow speed of some existing mobile communication links. Secondly, to reduce the cost of communications by reducing link usage. In achieving these aims, our goal has been to reduce the total data volume that the link must carry, {{and at the same time}} ensure adequate response time for all <b>classes</b> <b>of</b> <b>transactions.</b> Results of computer simulation are presented and discussed. 1. Introduction Recent advances in miniaturisation and cellular technology make the computing paradigm ubiquitous and are extending the scope of database applications. While distributed databases have been studied [...] ...|$|E
30|$|The second <b>class</b> <b>of</b> <b>transaction</b> risk is the ‘opportunism risks’, i.e. {{the risk}} which arises when another {{contracting}} party, with monopsonistic or monopolistic {{control over a}} complementary investment or service, removes, or threatens to remove, it from the supply chain after a player has made an investment that depends upon it. The {{findings of this study}} have shown that the bone and tallow value chains failed to grow and modernise due to limited options for business financing, and on several occasions, production had to be suspended until payments were received from distributors.|$|R
40|$|Transactional graph {{transformation}} systems (t-gtss) {{have been}} recently proposed as a mild {{extension of the}} standard dpo approach to graph transformation, equipping it with a suitable notion of atomic execution for computations. A typing mechanism induces a distinction between stable and unstable items, and a transaction {{is defined as a}} shift-equivalence <b>class</b> <b>of</b> computations such that the starting and ending states are stable and all the intermediate states are unstable. The paper introduces an equivalent, yet more manageable definition <b>of</b> <b>transaction</b> based on graph processes. This presentation is used to provide a universal characterisation for the <b>class</b> <b>of</b> <b>transactions</b> <b>of</b> a given t-gts. More specifically, we show that the functor mapping a t-gts to a graph transformation system having as productions exactly the <b>transactions</b> <b>of</b> the original t-gts is the right adjoint to an inclusion functor...|$|R
40|$|AbstractThe {{frequent}} connected subgraph mining problem, i. e., {{the problem}} of listing all connected graphs that are subgraph isomorphic to at least a certain number <b>of</b> <b>transaction</b> graphs <b>of</b> a database, cannot be solved in output polynomial time in the general case. If, however, the transaction graphs are restricted to forests then the problem becomes tractable. In this paper we generalize the positive result on forests to graphs of bounded tree-width. In particular, we show that for this <b>class</b> <b>of</b> <b>transaction</b> graphs, frequent connected subgraphs can be listed in incremental polynomial time. Since subgraph isomorphism remains NP-complete for bounded tree-width graphs, the positive complexity result of this paper shows that efficient frequent pattern mining is possible even for computationally hard pattern matching operators...|$|R
40|$|Approved {{for public}} release, {{distribution}} is unlimitedThe analysis of LAN performance {{is the main}} objective of this research. LANs can be configured in various ways combining different medium access control mechanisms and different physical layer specifications. Details on these alternatives are specified in IEEE 802. 3 through IEEE 802. 5. We study th performance {{of different types of}} LANs under various configurations of servers and stations. The queueing network model is one of the analytical tools to help investigate the performance characteristics of various LAN configurations. Since the analytical approach based on queueing network models is often too complicated to be practically used, we rely on simulations. Thus our analysis will be based on simulations, and SIMLAN II will be the simulation tool for our work. Our specification of simulation models involves three <b>classes</b> <b>of</b> <b>transactions,</b> and one or two servers. There are 24 simulation results in this thesis. These results, which are arranged in tables and figures, help compare th performance characteristics of various LAN configurations. Lieutenant Commander, Republic of China (Taiwan) Nav...|$|E
40|$|Analytic {{procedures}} allow auditors (or forensic accountants) {{to identify}} the presence of unexpected relationships among financial data, including unusual transactions or events, and to develop the scope of audit planning or investigation (Corbett and Clayton 2006, Bhattacharjee and Machuga 2004, Asare and Wright 2001). The effectiveness of analytic techniques e. g., trend analysis, ratio analysis, or a regression analysis depends on the precision of expectations developed (Kenyon and Tilton 2006). However, there is a caveat associated with these expectations. Specifically, Hitzig (2004) notes that “…because the expectation is specified by the auditor, and because the expectation’s parameters are estimated from data that may be misstated [or contain fraudulent transactions], {{there is a risk}} that reasonable expectations are incorrect… ” (p. 34). As such, the risk of failing to detect the presence of suspicious trends, patterns, or anomalies within the financial data likely exists. This problem is referred to herein as the risk of Type II error. On the other hand, analytic procedures infer propriety of individual transactions based on the reasonableness of aggregated account balances or <b>classes</b> <b>of</b> <b>transactions</b> (Stringer and Stewart 1986, p. 16). The percentage of misdiagnosis associated with these procedures ranges from 15 % to 50 % (Kogan et al. 2010) and research indicates that the use of daily or monthly disaggregated data improves th...|$|E
40|$|In {{this paper}} we shall propose {{a credit card}} {{transaction}} fraud detection framework which uses Hidden Markov Models, a well established technology that has not as yet been tested {{in this area and}} through which we aim to address the limitations posed by currently used technologies. Hidden Markov Models have for many years been effectively implemented in other similar areas. The flexibility offered by these models together with the similarity in concepts between Automatic Speech Recognition and credit card fraud detection has instigated the idea of testing the usefulness of these models in our area of research. The study performed in this project investigated the utilisation of Hidden Markov Models by means of proposing a number of different frameworks, which frameworks are supported through the use of clustering and profiling mechanisms. The profiling mechanisms are used in order to build Hidden Markov Models which are more specialised and thus are deployed on training data that is specific to a set of cardholders which have similar spending behaviours. Clustering techniques were used in order to establish the association between different <b>classes</b> <b>of</b> <b>transactions.</b> Two different clustering algorithms were tested {{in order to determine the}} most effective one. Also, different Hidden Markov Models were built using different criteria for test data. The positive results achieved portray the effectiveness of these models in classifying fraudulent and legitimate transactions through a resultant percentage value which indicates the prominence of the transaction being contained in the respective model. peer-reviewe...|$|E
5000|$|Discrete control procedures, or {{controls}} {{are defined by}} the SEC as: [...] "...a specific set of policies, procedures, and activities designed to meet an objective. A control may exist within a designated function or activity in a process. A control’s impact...may be entity-wide or specific to an account balance, <b>class</b> <b>of</b> <b>transactions</b> or application. Controls have unique characteristics - for example, they can be: automated or manual; reconciliations; segregation of duties; review and approval authorizations; safeguarding and accountability of assets; preventing or detecting error or fraud. Controls within a process may consist of financial reporting controls and operational controls (that is, those designed to achieve operational objectives)." ...|$|R
40|$|Acceptance rate = 104 / 521 = 20 %The {{frequent}} connected subgraph mining problem, i. e., {{the problem}} of listing all connected graphs that are subgraph isomorphic to at least a certain specified number of elements of a database <b>of</b> <b>transaction</b> graphs, cannot be solved in output polynomial time in the general case. If, however, the transaction graphs are restricted to trees then the problem becomes tractable. In this paper, we generalize the positive complexity result on trees to graphs of bounded treewidth. In particular, we show that for this <b>class</b> <b>of</b> <b>transaction</b> graphs, frequent connected subgraphs can be listed with incremental polynomial delay. Since subgraph isomorphism remains NP-complete for bounded treewidth graphs, the positive complexity result of this paper implies that efficient frequent pattern mining is possible even for computationally hard pattern matching operators. status: publishe...|$|R
25|$|Governments use {{different}} kinds of taxes and vary the tax rates. They do this in order to distribute the tax burden among individuals or <b>classes</b> <b>of</b> the population involved in taxable activities, such as the business sector, or to redistribute resources between individuals or classes in the population. Historically, taxes on the poor supported the nobility; modern social-security systems aim to support the poor, the disabled, or the retired by taxes on those who are still working. In addition, taxes are applied to fund foreign aid and military ventures, to influence the macroeconomic performance of the economy (a government's strategy for doing this is called its fiscal policy; see also tax exemption), or to modify patterns of consumption or employment within an economy, by making some <b>classes</b> <b>of</b> <b>transaction</b> more or less attractive.|$|R
40|$|There {{are several}} {{available}} paradigms {{to use when}} constructing the distributed systems of the future. Multidatabase systems (MDBSs) are becoming an increasingly popular paradigm: they offer a new promise for easing {{the construction of new}} distributed systems. Instead of developing a whole new distributed system, only a small portion needs to be developed, which integrates existing systems. We believe it is important to investigate the appropriateness of this approach to the construction of future distributed systems. In this paper we begin this investigation by focusing {{on one of the most}} important benefits of distributed systems: high availability. We study the problem of replication control in multidatabase systems in which members are autonomous. The requirement for local autonomy creates the need for a new model for replicating data in a MDBS. In this paper we first develop a model for replicated data in a MDBS. Subsequently, we formally characterize the <b>classes</b> <b>of</b> <b>transactions</b> which are allowed to commit, which allows us to formally derive availability upper bounds and define the performance limitations of the replication benefits for one-copy serializable (1 -sr) MDBS transactions. We then discuss the requirements of protocols that attempt to provide optimal availability and ensure 1 -sr. Finally, we examine some of the available commercial products and relate our formal models and results to them in an effort to show the applicability of the derived results. Because the availability and performance limitations are significant and the protocols that achieve high availability are costly, our results can be viewed in two ways. First, they show a way to replicate data in a 1 -sr MDBS, define the optimal availability and discuss the requirements of protocols which achieve it. Second, they constitute a formal justification for researching alternative notions of correctness in a MDBS...|$|E
40|$|This paper {{presents}} {{a new approach}} in evaluating risks of material misstatements in financial audit using dependency structure matrices (DSM). This perspective allows the identification of significant audit risks {{and can be used}} by audit managers to optimise resource allocation by focusing on higher risk areas. DSM matrix is widely used in other areas such as industrial production, design engineering and risk management. This approach is not used in financial audit so far. The financial crisis has diminished the activity of the audit clients and has imposed smaller audit fees. The auditors have to optimize their processes {{in order to maintain the}} quality of audit, even to improve it for the same audit remuneration. DSM matrix is a solution for this problem. This article points out have to use DSM matrix in financial audit process in order to optimize the allocation of resources, while maintaining audit quality. Our research aims to improve the risk evaluation stage in the financial audit process using DSM matrix to evaluate higher risk areas. We used the Project DSM Tool for representing significant accounts in the Purchase to pay process for a financial audit. Dependencies between accounts were used for creating a DSM matrix that depicts higher risk areas. Also, for each account, several resource allocation parameters such as costs and number of hours to be used for performing audit procedures on that account (from both audit team and client personnel). Our research suggests that DSM can provide useful information in detecting risk areas in significant <b>classes</b> <b>of</b> <b>transactions</b> identified in a risk based audit and we recommend using DSM matrix in the planning phase of the audit in order to avoid redundancies in the audit execution phase. This is important considering that the European Commission recommends in the Green Paper for Audit to improve the quality of audits following the setbacks to the profession caused by the financial crisis...|$|E
40|$|In {{this paper}} we extend our {{previous}} efforts to incorporate {{achievements of the}} New institutional and Transaction Cost Economics to the agrarian sphere. First, we demonstrate that Bulgarian agrarian economy is a Transaction cost economy, and clarify various types of transacting costs in transitional conditions. Next, we describe existing structures for governing of agrarian transactions, and evaluate their costs minimizing and incentive potential. Lastly, we estimate prospects for organizational modernization, and determine effective boundaries of market, private, public, and mixed modes for agrarian transacting. Neoclassical scenario for transformation of previous communist model （ 2 ̆ 2 free market plus private ownership 2 ̆ 2) has not worked in Bulgarian agriculture. Transition has changed 2 ̆ 2 {{rules of the game}} 2 ̆ 2 but it has not made agrarian agents 2 ̆ 2 more rational 2 ̆ 2 and 2 ̆ 2 less opportunistic 2 ̆ 2. Consequently costs for new property rights and institutional development, and for market and private models of individual transacting, have taken a good part of all social expenditures. High assets dependency, big uncertainty, low appropriability, and less frequency have determined a specific transitional structure of agrarian transacting. Less market transacting, big reliance on informal relationships at large scale, great extent of 2 ̆ 2 over 2 ̆ 2 integrated modes, part time farming and production cooperation phenomenon, block of all <b>classes</b> <b>of</b> <b>transactions</b> etc, all have come to existence. Besides, a large number of inefficient or contradictory third party （e. g. Government, Non-governmental organizations, international assistance etc） involvements in agrarian transacting have been in place. All this has deformed substantially emerging farming system, and domination of primitive and 2 ̆ 2 gray 2 ̆ 2 structure, little sustainability of large business and cooperative farms, significant distortion of national agrarian capital, and backward technological 2 ̆ 2 development 2 ̆ 2, have come to agenda. Low efficiency of public in-house organization and limited budget sources would restrict Government direct intervention in agrarian transactions. Agrarian policy should be toward exploring potential of market, and cooperative modes through new property rights provision, institutional and infrastructural support, improving law and contract enforcement, market information, extension education, assisting farmers association etc. Less expensive modes for trilateral governance （coordination, control on opportunism, incentives for specific investments） with active involvement of private sector and farmers organizations are to be preferred...|$|E
40|$|In {{this paper}} {{we present a}} {{framework}} for consistency checking of a database with respect to its integrity constraints, overcoming the restriction of considering stratified databases only. To reach this goal we have considered the well-founded and stable models semantics and then {{decided to use the}} well-founded one. The basic idea is that of finding the widest <b>class</b> <b>of</b> <b>transactions</b> for which it can be stated that the updated database D 2 ̆ 7 satisfies the integrity constraints IC, without having to compute its entire well-founded model, WF(D 2 ̆ 7). To this purpose the concept <b>of</b> conservative <b>transactions</b> has been introduced and furthermore a method to compute a (suitable approximation of the) minimal subset of WF(D 2 ̆ 7) that permits to decide the satisfiability of contraints is presented...|$|R
40|$|To be {{necessary}} {{means to be}} useful, indispensable for a particular purpose. To what extent is the samplingmethod necessary in the audit of financial statements? This paper aims to answer the question, why is samplingimportant, following {{the advantages and disadvantages}} of the two known sampling methods: statistical sampling andnon-statistical sampling. Using sampling saves time and effort for the team of auditors. Reduced costs for the auditorsand for the audited entity, resulting from the use of procedures only for the selected units, is another advantage ofusing this technique. The auditors should use their professional judgment to assess audit risk and to establishappropriate procedures for accountsbalances and <b>class</b> <b>of</b> <b>transactions</b> tested. Sampling should be used in order togive a correct opinion of the financial statements...|$|R
50|$|For example, an auditor may: {{physically}} examine inventory {{as evidence}} that inventory shown in the accounting records actually exists (existence assertion);inspect supporting documents like invoices to confirm that sales did occur (occurrence); arrange for suppliers to confirm in writing {{the details of the}} amount owing at balance date {{as evidence that}} accounts payable is a liability (rights and obligation assertion); and make inquires of management about the collectibility of customers' accounts as evidence that trade debtors are accurate as to its valuation. Evidence that an account balance or <b>class</b> <b>of</b> <b>transaction</b> is not complete, valid or accurate is evidence of a substantive misstatement but only becomes a material misstatement when it is large enough that it can be expected to influence the decisions of the users of the financial statement.|$|R
40|$|This report {{carries out}} a {{relative}} performance comparison between two DBMS architectures on the Multi Core, Single Die (MCSD) realization Niagara. The two DBMS architectures in question are Shared Nothing (SN) and Shared Everything (SE). The MCSD field is rapidly evolving, {{and we expect}} that this technology will become increasingly important in the near future. In order {{to carry out the}} comparison, the performance of the architectures must be calculated. This calculation depends on the cost figures associated with each architectural approach. To identify these costs, we present the design solutions made and results discovered in our previous work. Based on this, the most significant costs are determined and scheduled to be micro benchmarked. The natural next step is to examine possible techniques to implement the benchmarks. In order to do this, we first expand on the Niagara chip and the platform on which the micro benchmarks will run. Having a sufficient theoretical platform to continue, we move on to describe the implementation of each micro benchmark in detail. After benchmarking all the most significant costs, we thoroughly discuss the results, some of which are indeed surprising. The costs which are not benchmarked are based on assumptions from our previous work and recalculated to apply to Niagara. For both SN and SE, we evaluate the system for two <b>classes</b> <b>of</b> <b>transactions.</b> The first class is transactions touching one tuple (called simple), the second is transactions touching four tuples (called complex). Each class has two instances, read and update. In order to perform the subsequent analysis, the decomposition of each transaction is presented in detail. When analyzing the outcome of the calculations, interesting results emerge. First, we note that SE is the cheapest alternative when evaluating the simple transactions. This is because the SN approach includes an administrative overhead component that does not pay off when the transaction only touches one tuple. However, for complex transactions, the overhead component results in a parallel gain for SN which outperforms SE. Based on the most dominant costs of both architectures, we perform a sensitivity analysis. For SN, the analysis is based on the cost for message passing. For SE, {{it is based on the}} cost for synchronization. The goal of this analysis is two folded. First, it is interesting to see how the results vary. For example, what the ratio between the cost for message passing and the cost for synchronization must be in order to make the two approaches perform equally well. Second, the analysis indicate how error-prone each architecture is to erroneous estimation. The sensitivity analysis examine the performance of SN and SE when the ratio between the cost for message passing and the cost for synchronization is varied. This is done in both the read and the update cases. In addition to examining the simple and the complex transactions, we examine general transactions were the number of operations are not predetermined. The analysis of the general read transaction suggests that when the number of operations increases, the message passing and synchronization costs wipe out the impact of the other costs. It also suggests that when the cost of message passing is greater than 4 times the cost of synchronization, SE performs better when increasing the number of read operations. Similarly, if message passing is cheaper than 4 times the cost of synchronizing, SN is preferable. When increasing the number of update operations, the ratio is 3. 33. After concluding the analysis, we suggest a hybrid architecture that might combine the advantages of SN and SE. At the cost of introducing both message passing and synchronization, the architecture introduce parallelism in SE. Lastly, we identify suggestions for future work. Realized and applied to the DBMS model introduced in this report, we believe that several of these suggestions can shrink some of the costs presented. </p...|$|E
40|$|Thesis (PhD (Accounting)) [...] University of Stellenbosch, 2005. ENGLISH ABSTRACT: The South African {{long-term}} {{insurance industry}} is currently {{believed to be}} at an important crossroads in its existence. The industry is haunted by concerns about high cost structures, a lack of transparency in disclosure to policyholders, unfulfilled expectations of policyholders {{and the proliferation of}} available investment vehicles in the market. These concerns are exerting pressure on the existing products and practices of South African long-term insurers. The audits of these insurers are of a complex and high-risk nature {{as a result of the}} complexity of their operations and, in particular, the highly complex actuarial valuation process in respect of policy liabilities. The prevailing auditing standards in South Africa require auditors to include policy liabilities in the ambit of their audit opinions. Recent investigations into failed long-term insurers and their audits, including those of local Fedsure Life, British Equitable Life Assurance Society and Australian HIH Insurance, demonstrate the high risk involved in the audits of long-term insurers. Against this background, the objective of this research was to develop a best practice framework for the formulation of overall audit strategies for policy liabilities arising under insurance contracts and the related earnings of listed South African long-term insurers. To justify the focus of the research on the abovementioned components of the financial statements of listed South African long-term insurers, a questionnaire was developed and sent to auditors of all long-term insurers listed on the JSE Securities Exchange South Africa for completion. Responses were processed to calculate a Relative Inherent Risk Index specifically developed for use in this research, ranking various industry-specific account balances and <b>classes</b> <b>of</b> <b>transactions</b> on the basis of their potential exposure to inherent risk. The results of this process provided significant support for the hypotheses that policy liabilities and the related earnings are potentially exposed to the highest levels of inherent risk. The remainder of the research consequently focused on these components. A further very comprehensive questionnaire was developed to collect data with respect to respondents’ views of potential best practices for the audit of various aspects relating to policy liabilities arising under insurance contracts and the related earnings of listed South African long-term insurers, on the basis of their extensive experience in the industry. This questionnaire was sent to experienced auditors responsible for the audits of the five largest listed long-term insurers in South Africa for completion. Responses were received from four of the five potential respondents, resulting in an 80 % response rate, enabling meaningful analysis and interpretation of the data. Responses were analysed, interpreted and documented in the form of a detailed best practice framework for the formulation of overall audit strategies for policy liabilities arising under insurance contracts and the related earnings. The lack of a fifth response was compensated for by a review of the research findings by experienced auditors of Deloitte and the provision of their opinions thereon. Deloitte was selected for this purpose as the fact that this auditing firm is the only one of the so-called “Big Four” auditing firms that does not act as auditor of one of the selected target long-term insurers, resulted in the initial exclusion of the firm’s views from the research. The framework was updated to reflect these opinions and now incorporates input from all of the so-called “Big Four” auditing firms. The framework provides a comprehensive discussion of all possible types of audit procedures that may be relevant to the audit of all aspects of policy liabilities arising under insurance contracts and the related earnings of listed South African long-term insurers. As no such framework existed prior to this research, the development thereof made a significant contribution to existing knowledge. This contribution is the result of, inter alia, the method followed in designing the framework, resulting in it representing a synthesis of, inter alia, the following: • existing international and limited local guidance for auditors and, in particular, auditors of long-term insurers, customised for the South African environment; • best practices currently in use on the audits of listed South African long-term insurers; and • views of experienced practitioners on the abovementioned types of best practices that might not be employed at the moment, but that should, in their views, be employed in future. The valuable contribution of this research to existing knowledge is clear from the fact that numerous publications in popular professional as well as accredited academic journals, plus a paper delivered at a conference have resulted from it (refer to the source list and Appendix A). Furthermore, the South African Institute of Chartered Accountants has approved a project to update existing South African guidance for auditors of long-term insurers on the basis of the findings of this research...|$|E
40|$|The Internet has {{transformed}} the economics of communication, creating a spirited debate about {{the proper role of}} federal, state, and international governments in regulating conduct related to the Internet. Many argue that Internet communications should be entirely self-regulated because such communications cannot or should not be the subject of government regulation. The advocates of that approach would prefer a no-regulation zone around Internet communications, based largely on the unexamined view that Internet activity is fundamentally different in a way that justifies broad regulatory exemption. At the same time, some kinds of activity that the Internet facilitates undisputedly violate widely shared norms and legal rules. State legislatures motivated by that concern have begun to respond with Internet-specific laws directed at particular contexts, giving little or no credence to the claims that the Internet needs special treatment. This Article starts from the realist assumption that government regulation of the Internet is inevitable. Thus, instead of focusing on the naive question of whether the Internet should be regulated, this Article discusses how to regulate Internet-related activity {{in a way that is}} consistent with approaches to analogous offline conduct. The Article also assumes that the Internet 2 ̆ 7 s most salient characteristic is that it inserts intermediaries into relationships that could be, and previously would have been, conducted directly in an offline environment. Existing liability schemes generally join traditional fault-based liability rules with broad Internet-specific liability exemptions. Those exemptions are supported by the premise that in many cases the conduct of the intermediaries is so wholly passive as to make liability inappropriate. Over time, this has produced a great volume of litigation, mostly in the context of the piracy of copyrighted works, in which the responsibility of the intermediary generally turns on fault, as measured by the intermediary 2 ̆ 7 s level of involvement in the challenged conduct. This Article argues that the pervasive role of intermediaries calls not for a broad scheme of exoneration, premised on passivity, but rather for a more thoughtful development of principles for determining when and how it makes economic sense to allocate responsibility for wrongful conduct to the least cost avoider. The Internet 2 ̆ 7 s rise has brought about three changes that make intermediaries more likely to be least cost avoiders in the Internet context than they previously have been in offline contexts: (1) an increase in the likelihood that it will be easy to identify specific intermediaries for large <b>classes</b> <b>of</b> <b>transactions,</b> (2) a reduction in information costs, which makes it easier for the intermediaries to monitor the conduct of end users, and (3) increased anonymity, which makes remedies against end users generally less effective. Accordingly, in cases where intermediaries can feasibly control the conduct, this Article recommends serious attention to the possibility of one of three different schemes of intermediary liability: traditional liability for damages, takedown schemes in which the intermediary must remove offensive content upon proper notice, and 2 ̆ 7 hot list 2 ̆ 2 schemes in which the intermediary must avoid facilitation of transactions with certain parties. Part III of this Article uses that framework to analyze the propriety of intermediary liability for several kinds of Internet-related misconduct. This Article is agnostic about the propriety of any particular regulatory scheme, recognizing the technological and contextual contingency of any specific proposal. Because any such scheme will impose costs on innocent end users, selecting a particular level of regulation should depend on policymakers 2 ̆ 7 view of the net social benefits of eradicating the misconduct, taking into account the intermediaries 2 ̆ 7 and innocent users 2 ̆ 7 compliance costs associated with the regulation. Still, the analysis of this Article suggests three points. First, the practicality of peer-to-peer distribution networks for the activity in question is an important consideration because those networks undermine the regulatory scheme 2 ̆ 7 s effectiveness, thereby making regulation less useful. Second, the highly concentrated market structure of Internet payment intermediaries makes reliance on payment intermediaries particularly effective as a regulatory strategy because of the difficulty illicit actors have in relocating to new payment vehicles. Third, with respect to security harms, such as viruses, spam, phishing, and hacking, this Article concludes that the addition of intermediary liability in those cases is less likely to be beneficial because market incentives appear to be causing intermediaries to undertake substantial efforts to solve these problems without the threat of liability...|$|E
40|$|This article {{offers a}} {{critique}} and extension of recent work by Bruce Kogut and Udo Zander. Kogut and Zander are correct {{to argue that}} opportunism is unnecessary to explain {{the existence of the}} multinational, but wrong to infer that this indicates an absence of market failure. They have identified an important <b>class</b> <b>of</b> <b>transaction</b> costs and market failure which arises in the absence of opportunism, but do not cat doubt on the market failure approach to the theory of the multinational. Their argument that they have identified an important source of ‘ownership’ advantage is unconvincing and is not supported by the empirical evidence. However, Kogut and Zander's work is of considerable significance, but in areas different from those that they highlight. © 1995 JIBS. Journal of International Business Studies (1995) 26, 399 – 40...|$|R
40|$|This paper {{deals with}} the <b>transaction</b> {{management}} aspects <b>of</b> the R * distributed database system. It concentrates primarily on {{the description of the}} R * commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2 P) commit protocol. PA is optimized for read-only <b>transactions</b> and a <b>class</b> <b>of</b> multisite update <b>transactions,</b> and PC is optimized for other <b>classes</b> <b>of</b> multisite update <b>transactions.</b> The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R*‘s approach toward distributed deadlock detection and resolution...|$|R
40|$|We {{consider}} {{the use of}} a cluster system for Application Service Providers. To obtain high-performance and high-availability, we replicate databases (and DBMS) at several nodes, so they can be accessed in parallel through applications. Then the main problem is to assure the consistency of autonomous replicated databases. Preventive replication [8] provides a good solution that exploits the cluster's high speed network, without the constraints of synchronous replication. However, the solution in [8] assumes full replication and a restricted <b>class</b> <b>of</b> <b>transactions.</b> In this paper, we address these two limitations in order to scale up to large cluster configurations. Thus, the main contribution is a refreshment algorithm that prevents conflicts for partially replicated databases. We describe the implementation of our algorithm over a cluster of 32 nodes running PostGRESQL. Our experimental results show that our algorithm has excellent scale up and speed up...|$|R
