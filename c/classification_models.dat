2092|4543|Public
25|$|Another <b>classification</b> <b>models</b> is the Availability Environment Classification (AEC) of the Harvard Research Group, which {{ranges from}} AEC-0 to AEC-5.|$|E
2500|$|Given {{the success}} of ROC curves for the {{assessment}} of <b>classification</b> <b>models,</b> the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves [...] and the Regression ROC (RROC) curves. In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the error variance of the regression model.|$|E
5000|$|Out {{of sample}} {{prediction}} in regression and <b>classification</b> <b>models.</b>|$|E
40|$|Abstract. In {{order to}} reuse {{existing}} resources effectively {{and meet the}} specific demands of different users, {{it is necessary to}} study the <b>classification</b> <b>modeling</b> methods of parts for guiding product design and improving product quality. Firstly, the hierarchy theories of <b>classification</b> <b>modeling</b> are analyzed based on Design Structure Matrix(DSM). Then, the process of <b>classification</b> <b>modeling</b> is presented for parts of complex machinery product based on DSM modeling method. Meanwhile, the steps and notes to build classification structure were described in details. The results show that the theories and methods presented in this article could provid guidances for <b>classification</b> <b>modeling</b> of parts and realize efficient reuse and quick retrieval of product data for complex machinery manufactories. ...|$|R
30|$|<b>Classification</b> <b>model</b> {{considered}} as robust and accurate.|$|R
40|$|In {{this paper}} we derive {{asymptotic}} x 2 - tests for general linear hypotheses on variance components using repeated variance components models. In two examples, the two-way nested <b>classification</b> <b>model</b> and the two-way crossed <b>classification</b> <b>model</b> with interaction, we explicitly investigate {{the properties of}} the asymptotic tests in small sample sizes...|$|R
50|$|Decision trees (DT) are <b>classification</b> <b>models</b> where {{a series}} of {{questions}} and answers are mapped using nodes and directed edges.|$|E
50|$|Another <b>classification</b> <b>models</b> is the Availability Environment Classification (AEC) of the Harvard Research Group, which {{ranges from}} AEC-0 to AEC-5.|$|E
5000|$|In {{order to}} build the <b>classification</b> <b>models,</b> the samples {{belonging}} to each class need to be analysed using principal component analysis (PCA); only the significant components are retained.|$|E
30|$|The {{quality of}} the <b>classification</b> <b>model</b> that is {{developed}} by a classification algorithm in a reasonable (default) configuration or in an automatically optimized configuration provides an indication as to whether a reliable classification is possible at all, or not; for example. if such a <b>classification</b> <b>model</b> shows an area under the ROC curve of something close to 0.5, then it is rather unlikely to increase the {{quality of the}} <b>classification</b> <b>model</b> to a satisfying level just by adjusting and tuning the algorithms’ parameters. The more promising way is to adjust the input set of input variables.|$|R
5000|$|The ETIM <b>Classification</b> <b>model</b> {{is built}} using the {{following}} categories or entities: ...|$|R
5000|$|ISI Event Model (ISI-002): A {{comprehensive}} security event <b>classification</b> <b>model</b> (taxonomy + representation) ...|$|R
50|$|Fuzzy Logix {{released}} {{the first comprehensive}} library of in-database models, DB Lytix in 2008. The library had been under development since 1998. The library includes mathematical, statistical, data mining, simulation and <b>classification</b> <b>models.</b>|$|E
50|$|Some <b>classification</b> <b>models,</b> such as naive Bayes, {{logistic}} regression and multilayer perceptrons (when trained under an appropriate loss function) are naturally probabilistic. Other models such as support vector machines are not, but methods exist {{to turn them}} into probabilistic classifiers.|$|E
50|$|The DC 0-3R {{functions}} as a reference for the earlier manifestations of problems in infants and children, which can be connected to later problems in functioning. Secondly, the categorization focuses on types of difficulties in young children that are not addressed in other <b>classification</b> <b>models.</b>|$|E
40|$|This paper {{presents}} the Feature Decomposition Approach for improving supervised learning tasks. While in Feature Selection {{the aim is}} to identify a representative set of features from which to construct a <b>classification</b> <b>model,</b> in Feature Decomposition, the goal is to decompose the original set of features into several subsets. A <b>classification</b> <b>model</b> is built for each subset, and then all generated models are combined. This paper {{presents the}}oretical and practical aspects of the Feature Decomposition Approach. A greedy procedure, called DOT (Decomposed Oblivious Trees), is developed to decompose the input features set into subsets and to build a <b>classification</b> <b>model</b> for each subset separately...|$|R
40|$|In {{the data}} stream {{classification}} process, {{in addition to}} the solution of massive and real-time data stream, the dynamic changes of the need to focus and study. From the angle of detecting concept drift, according to the dynamic characteristics of the data stream. This paper proposes a new classification method for data stream based on the combined use of concept drift detection and <b>classification</b> <b>model.</b> The data stream <b>classification</b> <b>model</b> can’t adapt to concept drift problem to solve. Before the <b>model</b> <b>classification,</b> the use of information entropy to judge the data block concept drift, the concept of history to have appeared, the use of a classifier pool mechanism to save it, to makes the <b>classification</b> <b>model</b> has stronger resistance to concept drift...|$|R
2500|$|... {{provides}} a <b>classification</b> <b>model</b> {{taking into consideration}} the uncertainty associated with measuring replicate samples.|$|R
5000|$|For <b>classification</b> <b>models</b> {{that produce}} {{some kind of}} [...] "score" [...] on their outputs (such as a {{distorted}} probability distribution or the [...] "signed distance to the hyperplane" [...] in a support vector machine), there are several methods that turn these scores into properly calibrated class membership probabilities.|$|E
5000|$|Quantitative structure-activity {{relationship}} models (QSAR models) are regression or <b>classification</b> <b>models</b> {{used in the}} chemical and biological sciences and engineering. Like other regression models, QSAR regression models relate a set of [...] "predictor" [...] variables (X) to the potency of the response variable (Y), while classification QSAR models relate the predictor variables to a categorical value of the response variable.|$|E
50|$|The {{design of}} fitness {{functions}} for classification and logistic regression {{takes advantage of}} three different characteristics of <b>classification</b> <b>models.</b> The most obvious is just counting the hits, that is, if a record is classified correctly it is counted as a hit. This fitness function is very simple and works well for simple problems, but for more complex problems or datasets highly unbalanced it gives poor results.|$|E
30|$|While {{constructing}} the <b>classification</b> <b>model,</b> many variables may be included, {{but not all}} of these variables are actually important. Therefore, unimportant variables need to be eliminated in order to construct a simpler <b>classification</b> <b>model.</b> There is quite a number of ways to screen variables, of which the LASSO algorithm has shown excellent performance in reducing variables (Connor et al. 2015).|$|R
40|$|Churn is {{perceived}} as the behaviour of a customer to leave or to terminate a service. This behaviour causes the loss of profit to companies because acquiring new customer incurred high investment for advertisements and promotions compared to retaining existing ones. Thus, {{it is necessary to}} consider an efficient <b>classification</b> <b>model</b> to reduce the rate of churn. In the traditional approach of <b>classification</b> <b>modelling,</b> it do not produce straightforward result interpretation. Therefore, identifying the best <b>classification</b> <b>model</b> to reduce the rate of churn is indeed a challenging task. The main objective of this thesis is to propose a new <b>classification</b> <b>model</b> based on the Rough Set Theory to classify customer churn. This research utilized the Knowledge Discovery in Database (KDD) process involving data pre-processing, data discretization, attribute reduction, rule generation, classification process, as well as data analysis, using the Rough Set toolkit. The Rough Set theory elements consist of indiscernibility relation, lower and upper approximations, as well as reduction set. Those elements are applied to classify customer chum from uncertain and imprecise dataset. The results of the proposed model are compared with a few established existing approaches. The results of the study show that the proposed <b>classification</b> <b>model</b> outperformed the existing models and contributes to significant accuracy improvement. The model is tested using dataset form local telecommunication company which achieves 90. 32 %. In conclusion, the results proved that the <b>classification</b> <b>model</b> based on Rough Set Theory had been capable to classify customer chum compared to the existing model...|$|R
40|$|Classification is a Data Mining {{technique}} used {{for building a}} prototype of the data behaviour, using which an unseen data can be classified {{into one of the}} defined classes. Several researchers have proposed classification techniques but most of them did not emphasis much on the misclassified instances and storage space. In this paper, a <b>classification</b> <b>model</b> is proposed that takes into account the misclassified instances and storage space. The <b>classification</b> <b>model</b> is efficiently developed using a tree structure for reducing the storage complexity and uses single scan of the dataset. During the training phase, Class-based Closed Frequent ItemSets (CCFIS) were mined from the training dataset {{in the form of a}} tree structure. The <b>classification</b> <b>model</b> has been developed using the CCFIS and a similarity measure based on Longest Common Subsequence (LCS). Further, the Particle Swarm Optimization algorithm is applied on the generated CCFIS, which assigns weights to the itemsets and their associated classes. Most of the classifiers are correctly classifying the common instances but they misclassify the rare instances. In view of that, AdaBoost algorithm has been used to boost the weights of the misclassified instances in the previous round so as to include them in the training phase to classify the rare instances. This improves the accuracy of the <b>classification</b> <b>model.</b> During the testing phase, the <b>classification</b> <b>model</b> is used to classify the instances of the test dataset. Breast Cancer dataset from UCI repository is used for experiment. Experimental analysis shows that the accuracy of the proposed <b>classification</b> <b>model</b> outperforms the PSOAdaBoost-Sequence classifier by 7...|$|R
50|$|Platt scaling {{has been}} shown to be {{effective}} for SVMs as well as other types of <b>classification</b> <b>models,</b> including boosted models and even naive Bayes classifiers, which produce distorted probability distributions. It is particularly effective for max-margin methods such as SVMs and boosted trees, which show sigmoidal distortions in their predicted probabilities, but has less of an effect with well-calibrated models such as logistic regression, multilayer perceptrons and random forests.|$|E
50|$|In 2012, Fuzzy Logix {{released}} {{the first comprehensive}} library of GPU-based analytics designed to run on NVIDIA GPU Tesla and Kepler hardware. Tanay Zx contains for mathematical, statistical, data mining, simulation and <b>classification</b> <b>models</b> as well as equity, fixed income, foreign exchange, interest rate and time series models. Tanay Rx allows R users to call the models in Tanay Zx by writing R code. This allows R models to run using GPUs.|$|E
50|$|These {{functions}} {{based on}} the confusion matrix are quite sophisticated and are adequate to solve most problems efficiently. But there is another dimension to <b>classification</b> <b>models</b> which is key to exploring more efficiently the solution space and therefore results in the discovery of better classifiers. This new dimension involves exploring {{the structure of the}} model itself, which includes not only the domain and range, but also the distribution of the model output and the classifier margin.|$|E
3000|$|This paper {{presents}} a supervised <b>classification</b> <b>model,</b> where the indicators of correlation between dependent [...]...|$|R
40|$|This study {{examine the}} credit card fraud problem and adopt some actual {{transactional}} data with an online questionnaire transaction data to identify and prevent fraud. The ultimate {{aim of this study}} is to compare the effectiveness of generating personalized <b>classification</b> <b>model</b> to represent the spending behavior of individuals in identifying fraud as compared to the general <b>classification</b> <b>model</b> constructed from the mass data collected from all individuals...|$|R
40|$|Traditionally, {{word sense}} {{disambiguation}} (WSD) involves a different context <b>classification</b> <b>model</b> for each individual word. This paper presents a weakly supervised learning approach to WSD based on learning a word independent context pair <b>classification</b> <b>model.</b> Statistical models are not trained for classifying the word contexts, but for classifying a pair of contexts, i. e. determining if a pair of contexts of the same ambiguous word refers to the same or different senses. Using this approach, annotated corpus of a target word A can be explored to disambiguate senses of a different word B. Hence, only {{a limited amount of}} existing annotated corpus is required in order to disambiguate the entire vocabulary. In this research, maximum entropy modeling is used to train the word independent context pair <b>classification</b> <b>model...</b>|$|R
5000|$|Given {{the success}} of ROC curves for the {{assessment}} of <b>classification</b> <b>models,</b> the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves [...] and the Regression ROC (RROC) curves. In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the error variance of the regression model.|$|E
50|$|Also {{related to}} this new {{dimension}} of <b>classification</b> <b>models,</b> {{is the idea of}} assigning probabilities to the model output, which is what is done in logistic regression. Then it is also possible to use these probabilities and evaluate the mean squared error (or some other similar measure) between the probabilities and the actual values, then combine this with the confusion matrix to create very efficient fitness functions for logistic regression. Popular examples of fitness functions based on the probabilities include maximum likelihood estimation and hinge loss.|$|E
5000|$|Consider {{the problem}} of binary classification: for inputs , we want to {{determine}} whether they belong to one of two classes, arbitrarily labeled [...] and [...] We assume that the classification problem will be solved by a real-valued function , by predicting a class label [...] For many problems, it is convenient to get a probability , i.e. a classification that not only gives an answer, but also a degree of certainty about the answer. Some <b>classification</b> <b>models</b> do not provide such a probability, or give poor probability estimates.|$|E
30|$|The {{performance}} of a <b>classification</b> <b>model</b> is {{measured in terms of}} accuracy, sensitivity, specificity and error rate [11].|$|R
30|$|We then {{compared}} the outcomes between the elemental formula {{group and the}} control group using a mixed-effects logistic regression model [21] for binary outcomes and a linear-mixed regression model for continuous outcomes as the primary analysis, adjusted by the case-mix <b>classification</b> <b>model</b> established, with the random effects of hospital-level clustering. In the linear mixed-effects <b>model,</b> the case-mix <b>classification</b> <b>model</b> was inverse-logit-transformed to satisfy the homoscedasticity requirement for linear regression.|$|R
40|$|Abstract We propose {{different}} behaviour {{and interaction}} related indicators of artificial actors (bots) and show {{how they can}} be separated from natural users in a virtual dating market. A finite mixture <b>classification</b> <b>model</b> is applied on the different behavioural and interactional information to classify users into bot vs. non-botcategories. Finally the validity of the <b>classification</b> <b>model</b> and the impact of bots on sociodemographic distributions and scientific analysis is discussed. ...|$|R
