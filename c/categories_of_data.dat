207|10000|Public
5000|$|... a {{description}} of the category or <b>categories</b> <b>of</b> <b>data</b> subject and of the data or <b>categories</b> <b>of</b> <b>data</b> relating to them; ...|$|E
5000|$|QuickView Cards display <b>categories</b> <b>of</b> <b>data</b> as a {{mouse cursor}} rolls over a thumbnail.|$|E
5000|$|There are six 'lots', or <b>categories</b> <b>of</b> <b>data</b> available, {{defined by}} the NICE Framework. These are: ...|$|E
30|$|Confusion matrix: The {{confusion}} matrix {{is used to}} compare the mapping probabilities between the classification result and the true value. Each column of the {{confusion matrix}} represents a prediction <b>category</b> <b>of</b> <b>data,</b> and each row represents the true <b>category</b> <b>of</b> <b>data.</b>|$|R
5000|$|Accordingly, it is {{important}} to understand your requirement and whether or not the acquisition or the deliverables fit into these categories because it drives acquisition planning and required clauses and provisions that must be inserted into the solicitation. It is normal for MULTIPLE contract clauses and provisions to be inserted into the contract solicitation for the acquisition <b>of</b> a SINGLE <b>CATEGORY</b> <b>of</b> technical <b>data</b> (for example, DFARS 7013, Non-Commercial Technical Data, and 7017 clauses). However, it {{is important}} to note that if multiple <b>categories</b> <b>of</b> technical <b>data</b> (or even FAR data) is to be acquired, multiple <b>categories</b> <b>of</b> technical <b>data</b> clauses must be inserted and each contract line item number (CLIN) must be specifically drafted to cover each separate <b>CATEGORY</b> <b>of</b> technical <b>data</b> (e.g., commercial AND separately non-commercial software) ...|$|R
40|$|Abstract: The EU Directive 95 / 46 /EC {{specifically}} demarcates <b>categories</b> <b>of</b> sensitive <b>data</b> meriting special protection. It {{is important}} to review the continuing relevance <b>of</b> existing <b>categories</b> <b>of</b> sensitive <b>data</b> {{in the light of}} changes in societal structures and advances in technology. This paper draws on interviews with privacy and data protection experts from a range of countries and disciplines and findings from the Information Commissioner’s annual telephone survey of the British public in order to explore satisfaction with the current <b>categories</b> <b>of</b> sensitive <b>data.</b> It will be shown that the current classification <b>of</b> sensitive <b>data</b> appears somewhat outdated and thus ineffective for determining the conditions <b>of</b> <b>data</b> processing. Finally, possible reform proposals will be reviewed, including a purpose-based approach and context-based approach...|$|R
5000|$|Gellish is {{typically}} {{expressed in the}} form of Gellish Data Tables. There are three <b>categories</b> <b>of</b> <b>Data</b> Tables: ...|$|E
5000|$|Data-source marking is {{reflected}} in every sentence of the language. The three major grammatical <b>categories</b> <b>of</b> <b>data</b> source are: ...|$|E
50|$|There are {{currently}} five <b>categories</b> <b>of</b> <b>data</b> sets {{available on the}} GFW site, which are updated at various frequencies and available at various spatial resolutions.|$|E
30|$|The data {{gathered}} through different tools were analyzed as per <b>category</b> <b>of</b> <b>data</b> (quantitative and qualitative). Data analysis were made using descriptive statistics (mean, percentage), Chi-square, cross tabulation and regression {{analysis by the}} help of statistical package for social science (SPSS) version 20.0 software and Microsoft excel. The results were presented in tables and graphs.|$|R
30|$|Predictive {{analytics}} determines what {{is likely}} to happen in the future. This analysis is based on machine learning and statistical techniques as well as other more recently developed techniques that fall under the general <b>category</b> <b>of</b> <b>data</b> mining. The objective of these techniques is to be capable to provide predictions and forecasts {{about the future of the}} businesses activities.|$|R
30|$|In this part, S 3 {{analyzes}} the semantic information {{of a text}} to produce a candidate <b>category</b> <b>of</b> sensitive <b>data.</b>|$|R
5000|$|... {{processing}} on a {{large scale}} of special <b>categories</b> <b>of</b> <b>data</b> pursuant to Article 9 and personal data relating to criminal convictions and offences referred to in Article 10 ...|$|E
50|$|Different {{data has}} {{different}} census support, as different <b>categories</b> <b>of</b> <b>data</b> have (or have not yet) {{been released from}} the Canada 2016 Census. It is intended that all data presented here are the most current available census data.|$|E
5000|$|The Wind Energy GIS {{displayed}} wind energy-related {{spatial data}} {{with a standard}} web browser. The Wind Energy GIS features 10 <b>categories</b> <b>of</b> <b>data</b> layers: background; wind energy-related; transmission and infrastructure; geopolitical; hydrology; land ownership, designation and usage; elevation; land cover; ecosystem and climate; and soils.|$|E
30|$|In contrast, once we {{have the}} {{flexibility}} to address both code and data partitioning we have a more sophisticated tool available and so can explore moving data to the cloud, which obviously, will also have an affect on code placement. Figure  2 b shows the output of our cross-tier partitioning for doLogin. The figure shows the call-tree of function execution in the application-tier as well as data-tier query plans at the leaves. In the figure, we see four <b>categories</b> <b>of</b> components: (i) <b>data</b> on premise shown as black nodes, (ii) a new <b>category</b> <b>of</b> <b>data</b> in the cloud as square nodes, (iii) functions on premise as gray nodes, and (iv) functions in the cloud as white nodes.|$|R
50|$|While {{creation}} of new tables and columns to represent a new <b>category</b> <b>of</b> <b>data</b> is not especially labor-intensive, the programming of Web-based interfaces that support browsing or basic editing with type- and range-based validation is. In such a case, a more maintainable long-term solution {{is to create a}} framework where the class and attribute definitions are stored in metadata, and the software generates a basic user interface from this metadata dynamically.|$|R
50|$|Building {{information}} models represent another <b>category</b> <b>of</b> geo-spatial <b>data</b> {{that can}} be integrated into a 3D city model providing {{the highest level of}} detail for building components.|$|R
50|$|There are {{two types}} of <b>categories</b> <b>of</b> <b>data</b> mining. Predictive models use {{previous}} customer interactions to predict future events while segmentation techniques are used to place customers with similar behaviors and attributes into distinct groups. This grouping can help marketers to optimize their campaign management and targeting processes.|$|E
5000|$|Following {{discovery}} of the Church's Operation Snow White, the FBI's July 7, 1977 raids on the Church's offices produced, among other documents, an undated memo entitled [...] "PR General <b>Categories</b> <b>of</b> <b>Data</b> Needing Coding". This memo listed what it called [...] "Secret PR Front Groups" [...] which included the group Alliance for the Preservation of Religious Liberty (APRL), later renamed Americans Preserving Religious Liberty.|$|E
5000|$|The <b>categories</b> <b>of</b> <b>data</b> are numerous, growing or fluctuating, but {{the number}} of {{instances}} (records/rows) within each category is very small. Here, with conventional modeling, the database’s entity-relationship diagram might have hundreds of tables: the tables that contain thousands/ millions of rows/instances are emphasized visually to the same extent as those with very few rows. The latter are candidates for conversion to an EAV representation.|$|E
40|$|Visualization can {{be viewed}} as a process that {{transforms}} raw data (value) into views. There has been two major <b>category</b> <b>of</b> <b>data</b> process models that have been proposed to model the visualization transformation process. This paper seeks to compare the Data Flow Models and the Data State Models. Specifically, it proves that, in terms of expressiveness, anything that can represented using the Data Flow Model can also be represented using the Data State Model, and vice versa...|$|R
30|$|Clustering is {{the process}} of {{assigning}} a set of sensor nodes, with similar attributes, to a specified group or cluster. In our research, we have proposed a new energy efficient clustering algorithm that operates in two phases: preliminary and final clustering phase. In preliminary phase, sensor nodes sensing the same <b>category</b> <b>of</b> <b>data</b> are placed in a distinct cluster. In final phase, the remaining unclustered sensors estimate their divergence with respect to the clustered neighbors and ultimately join the least-divergent cluster.|$|R
40|$|Magidor (2013) {{argued that}} {{category}} mistakes are infelicitous due to presupposition failure. The case for {{this position is}} strengthened by the consideration of a previously unnoted <b>category</b> <b>of</b> <b>data,</b> namely multi-sentence discourses in which category mistake phenomenology arises {{at the end of}} the last sentence, but arguably due to content contained in a previous sentence. This phenomenon is analysed in terms of the previous sentence giving rise to a presupposition that is shown to be false only in the last sentence...|$|R
50|$|Nominal <b>categories</b> <b>of</b> <b>data</b> {{are often}} {{compared}} to ordinal and ratio data, {{to see if}} nominal categories {{play a role in}} determining these other factors. For example, the effect of race (nominal) on income (ratio) could be investigated by regressing the level of income upon one or more dummy variables that specify race. When nominal variables are to be explained, logistic regression or probit regression is commonly used.|$|E
5000|$|Section 13 of the Data Protection Act {{provides}} that the Office of the Data Protection Commissioner [...] "shall encourage trade associations and other bodies representing <b>categories</b> <b>of</b> <b>data</b> controllers to prepare {{codes of practice}} to be complied with by those categories in dealing with personal data." [...] The Office of the Data Protection Commissioner then formally approves such codes of practice, if such a code provides adequate data protection for individuals. The Office of the Data Protection Commissioner will then encourage its use in the sector concerned. The Office of the Data Protection Commissioner may also draw up such a code of practice on its own initiative.|$|E
5000|$|As noted above, EAV {{modeling}} {{makes sense}} for <b>categories</b> <b>of</b> <b>data,</b> such as clinical findings, where attributes are numerous and sparse. Where these conditions do not hold, standard relational modeling (i.e., one column per attribute) is preferable; using EAV does not mean abandoning common sense or principles of good relational design. In clinical record systems, the subschemas dealing with patient demographics and billing are typically modeled conventionally. (While most vendor database schemas are proprietary, VistA, the system used throughout the United States Department of Veterans Affairs (VA) medical system, known as the Veterans Health Administration (VHA), is open-source and its schema is readily inspectable, though it uses a MUMPS database engine rather than a relational database.) ...|$|E
5000|$|... 1) Server Hardware Failure - Preventing {{a server}} failure is very difficult, {{but it is}} {{possible}} to take precautions to avoid total server failure through the user of Redundant Power Supplies, RAID disk sets. [...] 2) Human Error - These disasters are major reasons for failure. Human error and intervention may be intentional or unintentional which can cause massive failures such as loss of entire systems or <b>data</b> files. This <b>category</b> <b>of</b> <b>data</b> loss includes accidental erasure, walkout, sabotage, burglary, virus, intrusion, etc.|$|R
40|$|Vector data {{represents}} one major <b>category</b> <b>of</b> <b>data</b> managed by GIS. This paper presents {{a new technique}} for vector-data display that is able to precisely and efficiently map vector data on 3 D objects such as digital terrain models. The technique allows the system to adapt the visual mapping to the context and user needs and enables users to interactively modify vector data through the visual representation. It represents a basic mechanism for GIS interface technology and facilitates the development of visual analysis and exploration tools...|$|R
5000|$|In October 2006, IBM {{announced}} {{the launch of}} IBM Information Server, the first entry into this new <b>category</b> <b>of</b> <b>data</b> integration tools. It is the first unified software platform able to deliver all of the functions to integrate, enrich and deliver trusted information for key business initiatives. IBM Information Server’s architecture meets all of the criteria that define it as an integrated software platform for information integration, and certainly establishes a functional benchmark for other vendors looking to say they have an information server too.|$|R
5000|$|The Church of Scientology uses front groups {{either to}} promote its {{interests}} in politics {{or to make}} its group seem more legitimate. The FBI's July 7, 1977 raids on the Church's offices (following discovery of the Church's Operation Snow White) turned up, among other documents, an undated memo entitled [...] "PR General <b>Categories</b> <b>of</b> <b>Data</b> Needing Coding". This memo listed what it called [...] "Secret PR Front Groups," [...] which included the group APRL, [...] "Alliance for the Preservation of Religious Liberty" [...] (later renamed [...] "Americans Preserving Religious Liberty"). The Cult Awareness Network (CAN) {{is considered by many}} to now be a front group for the Church of Scientology, which took the group over financially after bankrupting it in a series of lawsuits.|$|E
5000|$|NELL was {{programmed}} by its {{developers to}} be able to identify a basic set of fundamental semantic relationships between a few hundred predefined <b>categories</b> <b>of</b> <b>data,</b> such as cities, companies, emotions and sports teams. Since the beginning of 2010, the Carnegie Mellon research team has been running NELL around the clock, sifting through hundreds of millions of web pages looking for connections between the information it already knows and what it finds through its search process [...] - [...] to make new connections {{in a manner that is}} intended to mimic the way humans learn new information. For example, in encountering the word pair [...] "Pikes Peak", NELL would notice that both words are capitalized and deduce from the second word that it was the name of a mountain, and then build on the relationship of words surrounding those two words to deduce other connections.|$|E
50|$|The modern {{research}} author {{requires a}} reliable and standardized method to make research data available to other members of their community. This need {{has resulted in the}} development of a new form of scholarly communication known as data publishing. The process involves making data accessible, reusable and citable for long term use and is more elaborate than simply providing access to a data file. Data is becoming an important element of scholarship as a sharable source to be reused and shared. The same data can be accessed by multiple researchers to ask new questions or to replicate research for verification and augmentation. <b>Categories</b> <b>of</b> <b>data</b> differ among disciplines, as does its accessibility. Many publications have begun to offer incentives to scholarly researchers to publish their data and have developed the necessary infrastructure in support of e-research. Technical, policy, and institutional factors are becoming more established. The next phase will see the integration of the process into a standardized data publishing methodology.|$|E
50|$|Researchers have (semi-seriously) {{performed}} {{lossy compression}} on text by either using a thesaurus to substitute short words for long ones, or generative text techniques, although these sometimes {{fall into the}} related <b>category</b> <b>of</b> lossy <b>data</b> conversion.|$|R
50|$|Companies with an {{emphasis}} on marketing often focused their quality efforts on name and address information, but data quality is recognized as an important property of all types <b>of</b> <b>data.</b> Principles <b>of</b> <b>data</b> quality can be applied to supply chain data, transactional data, and nearly every other <b>category</b> <b>of</b> <b>data</b> found. For example, making supply chain data conform to a certain standard has value to an organization by: 1) avoiding overstocking of similar but slightly different stock; 2) avoiding false stock-out; 3) improving the understanding of vendor purchases to negotiate volume discounts; and 4) avoiding logistics costs in stocking and shipping parts across a large organization.|$|R
50|$|By {{defining}} an {{area as a}} CDP, that locality then {{appears in}} the same <b>category</b> <b>of</b> census <b>data</b> as incorporated places. This distinguishes CDPs from other census classifications, such as minor civil divisions (MCDs), which are in a separate category.|$|R
