3725|992|Public
5|$|This verbal {{prescription}} {{was replaced}} with coordinates in the CIE 1976 <b>color</b> <b>space</b> by the Coat of Arms Act of 31 January 1980.|$|E
5|$|To ensure {{baseline}} compatibility {{between different}} HDMI sources and displays (as well as backward compatibility with the electrically compatible DVI standard) all HDMI devices must implement the sRGB <b>color</b> <b>space</b> at 8 bits per component. Ability {{to use the}} YCbCr <b>color</b> <b>space</b> and higher color depths ("deep color") is optional. HDMI permits sRGB 4:4:4 chroma subsampling (8–16 bits per component), xvYCC 4:4:4 chroma subsampling (8–16 bits per component), YCbCr 4:4:4 chroma subsampling (8–16 bits per component), or YCbCr 4:2:2 chroma subsampling (8–12 bits per component). The color spaces {{that can be used}} by HDMI are ITU-R BT.601, ITU-R BT.709-5 and IEC 61966-2-4.|$|E
5|$|According to Chapter I, Article 28, {{paragraph}} 2 of the Constitution, {{the national}} colors of Poland are white and red. The Coat of Arms Act, Article 4, further specifies that {{the colors are}} white and red in two horizontal, parallel stripes of equal width, of which the top one is white and the bottom one is red. If the colors are displayed vertically, the white stripe {{is placed on the}} left from the onlooker's viewpoint. Attachment no. 2 to the Act shows the national colors in both horizontal and vertical alignment, as well as the official shades of both colors expressed as coordinates in the CIE xyY (CIE 1931) <b>color</b> <b>space</b> with the tolerated color differences (ΔE) specified in the CIE 1976 (L*, u*, v*) <b>color</b> <b>space</b> (CIELUV).|$|E
50|$|Adams {{chromatic}} valence <b>color</b> <b>spaces</b> are a {{class of}} <b>color</b> <b>spaces</b> suggested by Elliot Quincy Adams. Two important Adams chromatic valence spaces are CIELUV and Hunter Lab.|$|R
50|$|Calibrating all devices {{involved}} in the process chain (original, scanner/digital camera, monitor/printer) is required for an authentic color reproduction, because their actual <b>color</b> <b>spaces</b> differ device-specifically from the reference <b>color</b> <b>spaces.</b>|$|R
50|$|Occasionally, {{there are}} precise rules for {{converting}} between non-absolute <b>color</b> <b>spaces.</b> For example, HSL and HSV spaces {{are defined as}} mappings of RGB. Both are non-absolute, but the conversion between them should maintain the same color. However, in general, converting between two non-absolute <b>color</b> <b>spaces</b> (for example, RGB to CMYK) or between absolute and non-absolute <b>color</b> <b>spaces</b> (for example, RGB to L*a*b*) is almost a meaningless concept.|$|R
5|$|Note {{that the}} values {{given in the}} table {{correspond}} to CIE 1931 <b>color</b> <b>space.</b> Approximate RGB values for use may be taken to be: India saffron #FF9933, white #FFFFFF, India green #138808, navy blue #000080. Pantone values closest to this are 130 U, White, 2258 C and 2735 C.|$|E
5|$|The goal of Lost Coast was to {{demonstrate}} the new high-dynamic-range rendering implemented into the Source game engine. Valve first attempted to implement high-dynamic-range rendering in Source in late 2003. The first method stored textures in RGBA <b>color</b> <b>space,</b> allowing for multisample anti-aliasing and pixel shaders to be used, but this prevented alpha mapping and fog effects from working properly, as well as making textures appear sharp and jagged. The second method involved saving two versions of a texture: one with regular data, {{and the other with}} overbrightening data. However, this technique did not allow for multisample anti-aliasing and consumed twice as much video card memory, making it infeasible. The third method, shown at the E3 convention in 2005, used floating point data to define the RGB <b>color</b> <b>space,</b> allowing for reasonably efficient storage of the high-dynamic-range data. However, this method also did not allow for multisample anti-aliasing, and was only compatible with Nvidia video cards, leaving ATI cards unable to run high dynamic range. The fourth and final method compromised between the second and third methods, using overbrightening textures sparingly and allowing ATI cards to render HDR in a different way to the Nvidia ones while nearly producing the same end result.|$|E
5|$|DisplayPort uses a self-clocking, micro-packet-based {{protocol}} {{that allows}} for a variable number of differential LVDS lanes as well as flexible allocation of bandwidth between audio and video, and allows encapsulating multi-channel compressed audio formats in the audio stream. DisplayPort 1.2 supports multiple audio/video streams, variable refresh rate (FreeSync), Display Stream Compression (DSC), and Dual-mode LVDS/TDMS transmitters compatible with HDMI 1.2 or 1.4. Revision 1.3 increases overall transmission bandwidth to 32.4Gbit/s with the new HBR3 mode featuring 8.1Gbit/s per lane; it requires Dual-mode with mandatory HDMI 2.0 compatibility and HDCP 2.2. Revision 1.4 adds support BT.2020 <b>color</b> <b>space</b> and HDR10 extensions from CTA-861.3, including static and dynamic metadata.|$|E
40|$|Person re-identification is an {{important}} problem in visual surveillance where appearance plays a key role. Color {{is one of the}} widely used appearance features and utilizing more <b>color</b> <b>spaces</b> doesn’t imply benefit of performance enhancement. That’s because the poor performance <b>color</b> <b>spaces</b> influence on the high ones. So it is significant to evaluate the performance of different <b>color</b> <b>spaces</b> for person re-identification. In this paper, we propose a novel approach, called as random ensemble of color features (RECF), where we build a random forest to learn the similarity function of pairs of person images using color features from 6 kinds of popular <b>color</b> <b>spaces</b> (RGB, normalized RGB (NRGB), HSV, YCbCr, CIE XYZ and CIE Lab). We carry out experiments on the challenging dataset VIPeR to show the performances of different <b>color</b> <b>spaces</b> and their combination. We find out that the combination of NRGB, HSV, YCbCr and CIE Lab <b>color</b> <b>spaces</b> achieves the best performance, and our approach alleviates the over-fitting problem when there are limited training data. 1...|$|R
5000|$|ACES 1.0 {{defines a}} total of six <b>color</b> <b>spaces</b> {{covering}} the whole ACES framework as pertains generation, transport, processing, and storing of still and moving images. These <b>color</b> <b>spaces</b> all have a few common characteristics: ...|$|R
50|$|The HEVC {{standard}} supports <b>color</b> <b>spaces</b> such as generic film, NTSC, PAL, Rec. 601, Rec. 709, Rec. 2020, SMPTE 170M, SMPTE 240M, sRGB, sYCC, xvYCC, XYZ, and externally specified <b>color</b> <b>spaces.</b> HEVC supports <b>color</b> encoding representations such as RGB, YCbCr, and YCoCg.|$|R
5|$|From a user's perspective, an HDMI display can {{be driven}} by a single-link DVI-D source, since HDMI and DVI-D define an {{overlapping}} minimum set of allowed resolutions and framebuffer formats to ensure a basic level of interoperability. In the reverse case a DVI-D monitor {{would have the same}} level of basic interoperability unless there are content protection issues with High-bandwidth Digital Content Protection (HDCP) or the HDMI color encoding is in component <b>color</b> <b>space</b> YCbCr which is not possible in DVI, instead of RGB. An HDMI source such as a Blu-ray player may demand HDCP-compliance of the display, and refuse to output HDCP-protected content to a non-compliant display. A further complication {{is that there is a}} small amount of display equipment, such as some high-end home theater projectors, designed with HDMI inputs but not HDCP-compliant.|$|E
5|$|HDMI 2.0 {{increases}} the maximum TMDS clock to 600MHz (18.0Gbit/s). HDMI 2.0 uses 8b/10b encoding for video transmission like previous versions, {{giving it a}} maximum video bandwidth of 14.4Gbit/s. This enables HDMI 2.0 to carry 4K video at 60Hz with 24bit/px color depth. Other features of HDMI 2.0 include support for the Rec. 2020 <b>color</b> <b>space,</b> up to 32 audio channels, up to 1536kHz audio sample frequency, dual video streams to multiple users on the same screen, up to four audio streams, 4:2:0 chroma subsampling, 25 fps 3D formats, support for the 21:9 aspect ratio, dynamic synchronization of video and audio streams, the HE-AAC and DRA audio standards, improved 3D capability, and additional CEC functions.|$|E
5|$|HDMI 1.3 was {{released}} on June 22, 2006, and increased the maximum TMDS clock to 340MHz (10.2Gbit/s). Like previous versions, it uses 8b/10b encoding, giving it a maximum video bandwidth of 8.16Gbit/s (1920×1080 at 120Hz or 2560×1440 at 60Hz). It added support for 10bpc, 12bpc, and 16bpc color depth (30, 36, and 48bit/px), called deep color. It also added support for the xvYCC <b>color</b> <b>space,</b> {{in addition to the}} Rec. 601 and Rec. 709 color spaces supported by previous versions, and added the ability to carry metadata defining color gamut boundaries. It also optionally allows output of Dolby TrueHD and DTS-HD Master Audio streams for external decoding by AV receivers. It incorporates automatic audio syncing (audio video sync) capability. It defined cable Categories 1 and 2, with Category 1 cable being tested up to 74.25 MHz and Category 2 being tested up to 340 MHz. It also added the new type C Mini connector for portable devices.|$|E
50|$|AviSynth filters work {{in several}} <b>color</b> <b>spaces</b> {{including}} RGB, YUY2 and YV12 (Also YV16, YV24, YV411 and Y8 in Avisynth 2.6). This {{is necessary to}} allow all kinds of video input and output. Certain functions only work on specific <b>color</b> <b>spaces,</b> requiring conversion beforehand.|$|R
40|$|International audienceWe propose in {{this paper}} to study {{different}} <b>color</b> <b>spaces</b> for representing an image for the face authentication application. We used a generic algorithm based on a matching of keypoints using sift descriptors computed on one color component. Ten <b>color</b> <b>spaces</b> have been studied on four large and significant benchmark databases (ENSIB, FACES 94, AR and FERET). We show that all <b>color</b> <b>spaces</b> do not provide the same efficiency {{and the use of}} the color information allows an interesting improvement of verification results...|$|R
40|$|Normal human color {{perception}} {{is a product}} of three independent sensory systems. By mirroring this mechanism, full-color display devices create colors as mixtures of three primaries. Any displayable color can be described by the corresponding values of these primaries. Frequently it is more convenient to define various other <b>color</b> <b>spaces,</b> or coordinate systems, for color representation or manipulation. Several such <b>color</b> <b>spaces</b> are presented which are suitable for applications involving user specification of color, along with the defining equations and illustrations. The use of special <b>color</b> <b>spaces</b> for particular kinds of color computations is discussed. KEYWORDS...|$|R
5|$|Both HDMI and DVI use TMDS to send 10-bit {{characters}} that are encoded using 8b/10b encoding that {{differs from the}} original IBM form for the Video Data Period and 2b/10b encoding for the Control Period. HDMI adds the ability to send audio and auxiliary data using 4b/10b encoding for the Data Island Period. Each Data Island Period is 32 pixels in size and contains a 32-bit Packet Header, which includes 8 bits of BCH ECC parity data for error correction and describes {{the contents of the}} packet. Each packet contains four subpackets, and each subpacket is 64 bits in size, including 8 bits of BCH ECC parity data, allowing for each packet to carry up to 224 bits of audio data. Each Data Island Period can contain up to 18 packets. Seven of the 15 packet types described in the HDMI 1.3a specifications deal with audio data, while the other 8 types deal with auxiliary data. Among these are the General Control Packet and the Gamut Metadata Packet. The General Control Packet carries information on AVMUTE (which mutes the audio during changes that may cause audio noise) and Color Depth (which sends the bit depth of the current video stream and is required for deep color). The Gamut Metadata Packet carries information on the <b>color</b> <b>space</b> being used for the current video stream and is required for xvYCC.|$|E
25|$|The CIE 1931 RGB <b>color</b> <b>space</b> and CIE 1931 XYZ <b>color</b> <b>space</b> {{were created}} by the International Commission on Illumination (CIE) in 1931. They {{resulted}} {{from a series of}} experiments done in the late 1920s by William David Wright and John Guild. The experimental results were combined into the specification of the CIE RGB <b>color</b> <b>space,</b> from which the CIE XYZ <b>color</b> <b>space</b> was derived.|$|E
25|$|Tetrahedra {{are used}} in <b>color</b> <b>space</b> {{conversion}} algorithms specifically for {{cases in which the}} luminance axis diagonally segments the <b>color</b> <b>space</b> (e.g. RGB, CMY).|$|E
5000|$|Exposure {{control and}} curves in the L*a*b* and RGB <b>color</b> <b>spaces.</b>|$|R
5000|$|... #Caption: A {{comparison}} of the chromaticities enclosed by some <b>color</b> <b>spaces.</b>|$|R
5000|$|In the CIE 1976 L*a*b* and L*u*v* <b>color</b> <b>spaces,</b> the unnormalized chroma is the radial {{component}} of the cylindrical coordinate CIE L*C*h (lightness, chroma, hue) representation of the L*a*b* and L*u*v* <b>color</b> <b>spaces,</b> also denoted as CIE L*C*h(a*b*) or CIE L*C*h for short, and CIE L*C*h(u*v*). The transformation of [...] to [...] is given by: ...|$|R
25|$|The derived <b>color</b> <b>space</b> {{specified}} by x, y, and Y {{is known as}} the CIE xyY <b>color</b> <b>space</b> and is widely used to specify colors in practice.|$|E
25|$|ICC-compliant applications, such as Adobe Photoshop, {{use either}} the Lab <b>color</b> <b>space</b> or the CIE 1931 <b>color</b> <b>space</b> as a Profile Connection Space when {{translating}} between color spaces.|$|E
25|$|The CIE {{announced}} the XYZ <b>color</b> <b>space.</b>|$|E
5000|$|... #Subtitle level 2: Chroma in CIE 1976 L*a*b* and L*u*v* <b>color</b> <b>spaces</b> ...|$|R
2500|$|The CIE 1931 <b>color</b> <b>spaces</b> {{were the}} first defined {{quantitative}} links between distributions of wavelengths in the electromagnetic visible spectrum, and physiological perceived colors in human color vision. The mathematical relationships that define these <b>color</b> <b>spaces</b> are essential tools for color management, important when dealing with color inks, illuminated displays, and recording devices such as digital cameras.|$|R
40|$|In this paper, a new {{segmentation}} {{method for}} multicolor images based on Particle Swarm Optimization (PSO) is being proposed. The algorithm performs segmentation {{of an image}} with respect to swarm. The proposed algorithm randomly initializes each particle in the swarm for segmentation of images. PSO-based approach is proposed to tackle the color image quantization and spectral immixing problems. PSO algorithm works well in <b>color</b> <b>spacing</b> images to identify the reference image. The proposed work deals with {{the conversion of the}} captured color image into various <b>color</b> <b>spaces</b> and gray scale images. Then segmentation based on PSO algorithm is done at four different levels for each <b>color</b> <b>spaces</b> and gray scale image. The attributes of these <b>color</b> <b>spaces</b> are validated. This method can be used in medical image segmentation and image compression. The optimization method called Particle Swarm Optimization is applied to the field of pattern recognition and image processing...|$|R
25|$|A <b>color</b> <b>space</b> maps a {{range of}} {{physically}} produced colors from mixed light, pigments, etc. to an objective description of color sensations registered in the human eye, typically in terms of tristimulus values, but not usually in the LMS <b>color</b> <b>space</b> defined by the spectral sensitivities of the cone cells. The tristimulus values associated with a <b>color</b> <b>space</b> can be conceptualized as amounts of three primary colors in a tri-chromatic, additive color model. In some color spaces, including the LMS and XYZ spaces, the primary colors used are not real colors {{in the sense that}} they cannot be generated in any light spectrum.|$|E
25|$|As most {{definitions}} of color distance are distances within a <b>color</b> <b>space,</b> the standard means of determining distances is the Euclidean distance. If one presently has an RGB (Red, Green, Blue) tuple and wishes {{to find the}} color difference, computationally {{one of the easiest}} is to call R, G, B linear dimensions defining the <b>color</b> <b>space.</b>|$|E
25|$|The CIE 1931 color {{spaces are}} still widely used, {{as is the}} 1976 CIELUV <b>color</b> <b>space.</b>|$|E
50|$|Saturation is one {{of three}} {{coordinates}} in the HSL and HSV <b>color</b> <b>spaces.</b>|$|R
40|$|Perceptually uniform <b>color</b> <b>spaces</b> can be {{a useful}} tool for solving {{computer}} graphics color selection prob-lems. However, before they can be used effectively some basic principles of tristimulus colorimetry must be understood and the color reproduction device on which they are to be used must be properly adjusted. The Munsell Book of Color and the Optical Society of America (OSA) Uniform Color Scale are two uniform <b>color</b> <b>spaces</b> which provide a useful way of organizing the colors of a digitally controlled color television moni-tor. The perceptual uniformity of these <b>color</b> <b>spaces</b> can be used to select color scales to encode the vari-ations of parameters such as temperature or stress...|$|R
5000|$|... libvpx {{supports}} Rec. 601, Rec. 709, Rec. 2020, SMPTE-170, SMPTE-240, and sRGB <b>color</b> <b>spaces.</b>|$|R
