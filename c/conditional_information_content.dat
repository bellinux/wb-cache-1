2|10000|Public
5000|$|... {{conditional}} entropy, mean <b>conditional</b> <b>information</b> <b>content,</b> average <b>conditional</b> <b>information</b> <b>content</b> H(X|Y) ...|$|E
40|$|We {{describe}} a compression-based distance for genomic sequences. Instead {{of using the}} usual conjoint information content, as in the classical Normalized Compression Distance (NCD), it uses the <b>conditional</b> <b>information</b> <b>content.</b> To compute this Normalized Conditional Compression Distance (NCCD), we need a normal conditional compressor, that we built using a mixture of static and dynamic finite-context models. Using this approach, we measured chromosomal distances between Hominidae primates and also between Muroidea (rat and mouse), observing several insights of evolution that so far have not {{been reported in the}} literature. Comment: Full version of DCC 2014 paper "A conditional compression distance that unveils insights of the genomic evolution...|$|E
40|$|If the <b>conditional</b> <b>information</b> of a {{classical}} probability distribution of three random variables is zero, then it obeys a Markov chain condition. If the <b>conditional</b> <b>information</b> {{is close to}} zero, then {{it is known that}} the distance (minimum relative entropy) of the distribution to the nearest Markov chain distribution is precisely the <b>conditional</b> <b>information.</b> We prove here that this simple situation does not obtain for quantum <b>conditional</b> <b>information.</b> We show that for tri-partite quantum states the quantum <b>conditional</b> <b>information</b> is always a lower bound for the minimum relative entropy distance to a quantum Markov chain state, but the distance can be much greater; indeed the two quantities can be of different asymptotic order and may even differ by a dimensional factor. Comment: 14 pages, no figures; not for the feeble-minde...|$|R
40|$|This study {{examines}} {{the effects of}} systemic risk on global hedge fund returns. We consider systemic risk as a <b>conditional</b> <b>information</b> variable to predict the underlying exposures to various asset market returns and risk factors. This {{study examines}} a proxy for global systemic risk employed by investment professionals known as the Treasury/Eurodollar (TED) spread. The findings reveal that increases in systemic risk causes some hedge fund investment styles to dynamically reduce their equity and stock momentum exposures while others increase their exposures to investment grade bonds and commodities. The <b>information</b> <b>content</b> of systemic risk via the TED spread assists us in better understanding the behaviour of global hedge fund returns. Griffith Business School, Department of Accounting, Finance and EconomicsFull Tex...|$|R
40|$|We study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy {{that hold}} for {{distributions}} whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot {{be extended to}} any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: Submitted to the IEEE Transactions on Information Theor...|$|R
40|$|The aim of {{this paper}} is to present, by axiomatic way, an idea about the general <b>conditional</b> <b>information</b> of a single, fixed fuzzy set when the {{conditioning}} fuzzy event is variable. The properties of this <b>conditional</b> <b>information</b> are translated in a system of functional equations. Some classes of solutions of this functional system are founded...|$|R
5000|$|... is the <b>conditional</b> <b>information</b> entropy of the {{sequence}} of random variables. Equivalently, one has ...|$|R
40|$|We {{discuss the}} notions of mutual <b>information</b> and <b>conditional</b> <b>information</b> for noncomposite systems, {{classical}} and quantum; both the mutual <b>information</b> and the <b>conditional</b> <b>information</b> {{are associated with the}} presence of hidden correlations in the state of a single qudit. We consider analogs of the entanglement phenomena in the systems without subsystems related to strong hidden quantum correlations. Comment: 12 page...|$|R
40|$|We {{present an}} {{extension}} of the well-known information bottleneck framework, called <b>conditional</b> <b>information</b> bottleneck, which takes negative relevance information into account by maximizing a <b>conditional</b> mutual <b>information</b> score. This general approach can be utilized in a data mining context to extract relevant information that {{is at the same time}} novel relative to known properties or structures of the data. We present possible applications of the <b>conditional</b> <b>information</b> bottleneck in information retrieval and text mining for recovering non-redundant clustering solutions, including experimental results on the WebKB data set which validate the approach...|$|R
40|$|To {{appear in}} IEEE Transactions on Information Theory. An Early Access article is {{available}} in IEEE Xplore {{in advance of the}} final print version. International audienceWe study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy that hold for distributions whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot be extended to any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
40|$|International audienceIn 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we prove that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
40|$|In 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we show that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: v 4 : substantial corrections; 13 page...|$|R
40|$|The {{ability to}} {{anticipate}} forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations {{are critical to}} the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the <b>conditional</b> probability (and <b>information</b> <b>content)</b> of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation {{was found between the}} probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400 - 450 ms), (ii) beta band (14 - 30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity...|$|R
5000|$|Like mutual <b>information,</b> <b>conditional</b> mutual <b>information</b> can be {{expressed}} as a Kullback-Leibler divergence: ...|$|R
40|$|We {{show that}} the {{separability}} of states in quantum mechanics has a close counterpart in classical physics, and that <b>conditional</b> mutual <b>information</b> (a. k. a. <b>conditional</b> <b>information</b> transmission) is a very useful quantity {{in the study of}} both quantum and classical separabilities. We also show how to define entanglement of formation in terms of <b>conditional</b> mutual <b>information.</b> This paper lays the theoretical foundations for a sequel paper which will present a computer program that can calculate a decomposition of any separable quantum or classical state. 1...|$|R
40|$|The {{objective}} {{of this study is}} to investigate whether net income, net sale, and operating cash flow has incremental <b>information</b> <b>content.</b> This study also investigate whether net sales and operating cash flow have relative <b>information</b> <b>content.</b> Sample of this study is collected from Indonesian Capital Market (IDX). Collected sample is done during 2000 to 2004 in manufacturing industry. The result of this study is net income has not incremental <b>information</b> <b>content.</b> Net sale and operating cash flow have incremental <b>information</b> <b>content.</b> Net sale has not relative <b>information</b> <b>content</b> but operating cash flow has relative <b>information</b> <b>content...</b>|$|R
40|$|As digital terrain {{models are}} {{indispensable}} for visualizing and modeling geographic processes, terrain <b>information</b> <b>content</b> {{is useful for}} terrain generalization and representation. For terrain generalization, if the terrain information is considered, the generalized terrain may be of higher fidelity. In other words, the richer the terrain information at the terrain surface, the smaller the degree of terrain simplification. Terrain <b>information</b> <b>content</b> is also important for {{evaluating the quality of}} the rendered terrain, e. g., the rendered web terrain tile service in Google Maps (Google Inc., Mountain View, CA, USA). However, a unified definition and measures for terrain <b>information</b> <b>content</b> have not been established. Therefore, in this paper, a definition and measures for terrain <b>information</b> <b>content</b> from Digital Elevation Model (DEM, i. e., a digital model or 3 D representation of a terrain’s surface) data are proposed and are based on the theory of map <b>information</b> <b>content,</b> remote sensing image <b>information</b> <b>content</b> and other geospatial <b>information</b> <b>content.</b> The <b>information</b> entropy was taken as the information measuring method for the terrain <b>information</b> <b>content.</b> Two experiments were carried out to verify the measurement methods of the terrain <b>information</b> <b>content.</b> One is the analysis of terrain <b>information</b> <b>content</b> in different geomorphic types, and the results showed that the more complex the geomorphic type, the richer the terrain <b>information</b> <b>content.</b> The other is the analysis of terrain <b>information</b> <b>content</b> with different resolutions, and the results showed that the finer the resolution, the richer the terrain information. Both experiments verified the reliability of the measurements of the terrain <b>information</b> <b>content</b> proposed in this paper...|$|R
40|$|This study {{distinguishes between}} {{incremental}} and relative <b>information</b> <b>content.</b> Incremental comparisons ask whether one accounting measure provides <b>information</b> <b>content</b> beyond that provided by another, and apply when one measure {{is viewed as}} given and an assessment is desired regarding the incremental contribution of another (e. g., a supplemental disclosure). Relative comparisons ask which measure has greater <b>information</b> <b>content,</b> and apply when making mutually exclusive choices among alternatives, or when rankings by <b>information</b> <b>content</b> are desired (e. g., when comparing alternative disclosures). Questions of both incremental and relative <b>information</b> <b>content</b> arise frequently in accounting. However, few previous studies have examined questions of relative <b>information</b> <b>content.</b> Possible explanations include unfamiliarity with the relative versus incremental distinction, and the additional statistical complexity involved in testing for relative <b>information</b> <b>content.</b> First, we examine analytically the relation between incremental and relative <b>information</b> <b>content,</b> demonstrating that they address different research questions and require different tests for statistical significance. Second, we identify accounting research contexts in which questions of relative and incremental <b>information</b> <b>content</b> arise. Third, we propose a new regression-based test for relative <b>information</b> <b>content.</b> This test applies to both returns and valuation studies, generalizes to any number of predictor variables, {{and can be used}} in conjunction with White's (1980) adjustment for heteroskedasticity. Fourth, we illustrate tests for relative and incremental <b>information</b> <b>content</b> in a familiar research setting that compares the <b>information</b> <b>contents</b> of net income, cash flows, and net sales in 40 industries. link_to_subscribed_fulltex...|$|R
40|$|In {{this paper}} we propose new nonparametric estimators {{for a family of}} <b>conditional</b> mutual <b>information</b> and divergences. Our estimators are easy to compute; they only use simple k nearest {{neighbor}} based statistics. We prove that the proposed <b>conditional</b> <b>information</b> and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well. ...|$|R
40|$|This paper {{presents}} our extractive summarization {{systems at}} the update summarization track of TAC 2009. This system {{is based on}} our newly developed document summarization framework under the theory of <b>conditional</b> <b>information</b> distance among many objects. The best summary is defined in this paper {{to be the one}} which has the minimum information distance to the entire document set. The best update summary has the minimum <b>conditional</b> <b>information</b> distance to a document cluster given that a prior document cluster has already been read. Experiments on the TAC dataset have proved that our method has got a good performance in many categories. ...|$|R
40|$|In {{this paper}} two notions of <b>information</b> <b>content</b> for the {{characteristic}} sequences of sets are compared. One is the minimal-program {{complexity of the}} sequences and represents a quantitative <b>information</b> <b>content,</b> {{and the other is}} the degree of unsolvability of the underlying set and represents a qualitative <b>information</b> <b>content.</b> The major conclusion from this work is that with few exceptions these measures of <b>information</b> <b>content</b> are unrelated. Various tradeoffs between these measures are also demonstrated...|$|R
40|$|A simpler {{approach}} to the characterization of vanishing <b>conditional</b> mutual <b>information</b> is presented. Some remarks are given as well. More specifically, relating the <b>conditional</b> mutual <b>information</b> to a commutator is a very promising {{approach to}}wards the approximate version of SSA. That is, it is conjectured that small <b>conditional</b> mutual <b>information</b> implies small perturbation of quantum Markov chain. Comment: LaTex, 9 pages. Minor modifications are made. Any comments are welcome...|$|R
5000|$|The {{stronger}} {{properties of}} the [...] quantities, which allow the definition of <b>conditional</b> <b>information</b> and mutual information from communication theory, may be very important in other applications, or entirely unimportant, depending on those applications' requirements.|$|R
40|$|Black hole {{is called}} optimal if <b>information</b> <b>content</b> is minimal at the University region, {{consisting}} of usual substance and one(n) black hole(s). Optimal black hole mass {{does not depend}} on the mass of the Universe region. Optimal black holes can exist when at least the two types of substance are available in the Universe: with non-linear and linear correspondence between <b>information</b> <b>content</b> and mass. <b>Information</b> <b>content</b> of optimal black hole is proportional to squared coefficient correlating <b>information</b> <b>content</b> with mass in usual substance and in inverse proportion to coefficient correlating <b>information</b> <b>content</b> with black hole mass. Concentration of mass in optimal black hole minimizes <b>information</b> <b>content</b> in the system "usual substance - black holes". Minimal <b>information</b> <b>content</b> of the Universe consisting of optimal black holes only is twice as less as <b>information</b> <b>content</b> available of the Universe of the same mass filled with usual substance only. Under the radiation temperature T ≈ 1 E + 12 K the mass of optimal black holes that emerged in the systems "radiation - black hole" is equal to the mass of optimal black holes that emerged in the systems "hydrogen (protons) - black hole". Comment: 15 page...|$|R
40|$|Abstract Based on the {{indiscernible}} {{relation of}} rough set, {{the inevitability of}} superposition and inconsistency of data makes the reduction of attributes very important in information system. Rough set has difficulty in the difference of attribute reduction between consistent and inconsistent information system. In this paper, we propose the new uncertainty measure and attribute reduction algorithm by Bayesian posterior probability for correlation analysis between condition and decision attributes. We compare the proposed method and the <b>conditional</b> <b>information</b> entropy to address the uncertainty of inconsistent information system. As the result, our method has more accuracy than <b>conditional</b> <b>information</b> entropy in dealing with uncertainty via mutual information of conditio...|$|R
40|$|A {{measure of}} the <b>information</b> <b>content</b> of an {{evidence}} inducing a belief function or a possibility function is axiomatically defined. Its major property is to be additive for distinct evidences. Measures are developed of the <b>information</b> <b>content</b> of an evidence appropriate when belief or possibility functions are used. The additivity means that the <b>information</b> <b>content</b> derived from two distinct and non-conflictual evidences {{is the sum of}} the <b>information</b> <b>contents</b> of each evidence. Properties of these measures are studied. Refs. SCOPUS: NotDefined. jinfo:eu-repo/semantics/publishe...|$|R
5000|$|... {{relating}} the conditional expectation to the <b>conditional</b> mutual <b>information.</b> This {{is a simple}} consequence of Pinsker's inequality. (Note: the correction factor log 2 inside the radical arises because we are measuring the <b>conditional</b> mutual <b>information</b> in bits rather than nats.) ...|$|R
40|$|A {{measure is}} {{proposed}} {{based on the}} information theory and geostatistics to evaluate <b>information</b> <b>content</b> in remotely sensed images. The method is based on the additive noise model and maximum mutual information. These factors affecting the <b>information</b> <b>content</b> have been taken into account, such as noise, spatial correlation and so on. It is suitable for measuring the <b>information</b> <b>content</b> in optical images that have robust spatial correlation with different land cover types. An experiment was performed on a Landsat TM image with three different kinds of land cover types (city, farmland and mountain). The result shows that city has the most <b>information</b> <b>content.</b> It also proves that there is a log positive correlation between <b>information</b> <b>content</b> and the variance of the images...|$|R
50|$|Frank used {{agent-based}} {{models and}} the theory of autonomous agents with cognitive abilities (see multi-agent system) to operationalize measuring pragmatic <b>information</b> <b>content.</b> The transformation between the received message and the executed message {{is defined by the}} agents rules; the pragmatic <b>information</b> <b>content</b> is the <b>information</b> in the transformed message, measured by the methods given by Shannon. The general case can be split in (deterministic) actions to change the information the agent already has and the optimal decision using this information. To measure the pragmatic <b>information</b> <b>content</b> is relevant to assess the value of information received by an agent and influences the agents willingness to pay for information - not measured by Shannon communication <b>information</b> <b>content,</b> but by the received pragmatic <b>information</b> <b>content.</b>|$|R
40|$|How do {{ordinary}} indicative conditionals {{manage to}} convey <b>conditional</b> <b>information,</b> {{information about what}} might or must be if such-and-such is or {{turns out to be}} the case? An old school thesis is that they do this by expressing something iffy: ordinary indicatives express a two-place conditional operator and that is how they convey <b>conditional</b> <b>information.</b> How indicatives interact with epistemic modals seems to be an argument against iffiness and for the new school thesis that "if"-clauses are merely devices for restricting the domains of other operators. I will make the trouble both clear and general, and then explore a way out for fans of iffiness. doi: 10. 3765 /sp. 3. 4 BibTeX info</a...|$|R
40|$|The {{aim of this}} {{research}} is to investigate the <b>information</b> <b>content</b> of operating income and cash flow operating with stock return in information asymmetry situations in the accepted companies at Tehran stock exchange. This study was done according to data obtained from 70 companies during 2006 - 2010. This is an after event research. This research is calcified as the applicable research. And the hypothesis was examined, using liner multiple regression. The results of the examination reveal that operating income and cash flow operating contain the <b>information</b> <b>content</b> and they can explain the stock return and on the other hand, <b>information</b> <b>content</b> of operating income and cash flow operating are different from each other and <b>information</b> <b>content</b> of operating income is higher than cash flow operating. Also, the results indicated that information asymmetry effects on the income <b>information</b> <b>content</b> and cash flow operating and whatever the information asymmetry is higher, then <b>information</b> <b>content</b> of cash flow operating is increased...|$|R
5000|$|A {{familiar}} application my {{clarify the}} approach:Different car navigation systems produce different instructions {{but if they}} manage to guide you to the same location, their pragmatic <b>information</b> <b>content</b> must be the same (despite different <b>information</b> <b>content</b> when measured with Shannon's measure (SAME). A novice in the area may need all instructions received - the pragmatic <b>information</b> <b>content</b> and the (minimally encoded) <b>information</b> <b>content</b> of the message is the same. An experienced driver will ignore all [...] "follow the road" [...] and [...] "go straight" [...] instructions, thus the pragmatic <b>information</b> <b>content</b> is lower; for an driver {{with knowledge of the}} area, large parts of the instructions may be subsumed by simple instructions [...] "drive to X"; typically, only the last part ("the last mile") of the instructions are meaningful - the pragmatic <b>information</b> <b>content</b> is smaller, because much knowledge is already available (DIFF). Messages with more or less verbiage have for this user the same pragmatic content (SAME).|$|R
40|$|We {{consider}} {{the problem of}} selecting informative observations in Gaussian graphical models con-taining both cycles and nuisances. More specif-ically, we {{consider the}} subproblem of quantify-ing <b>conditional</b> mutual <b>information</b> measures that are nonlocal on such graphs. The ability to effi-ciently quantify the <b>information</b> <b>content</b> of obser-vations is crucial for resource-constrained data acquisition (adaptive sampling) and data process-ing (active learning) systems. While closed-form expressions for Gaussian mutual informa-tion exist, standard linear algebraic techniques, with complexity cubic in the network size, are in-tractable for high-dimensional distributions. We investigate the use of embedded trees for com-puting nonlocal pairwise mutual information and demonstrate through numerical simulations that the presented approach achieves a significant re-duction in computational cost over inversion-based methods. ...|$|R
5000|$|... {{based on}} the notion of <b>information</b> <b>content.</b> The <b>information</b> <b>content</b> of a concept (term or word) is the {{logarithm}} of the probability of finding the concept in a given corpus.|$|R
50|$|A {{combinatorial}} prediction {{market is}} a type of prediction market where participants can make bets on combinations of outcomes. The advantage of making bets on combinations of outcomes is that, in theory, <b>conditional</b> <b>information</b> can be better incorporated into the market price.|$|R
40|$|MaxEnt {{inference}} algorithm {{and information}} theory are relevant {{for the time}} evolution of macroscopic systems considered as problem of incomplete information. Two different MaxEnt approaches are introduced in this work, both applied to prediction of time evolution for closed Hamiltonian systems. The first one is based on Liouville equation for the conditional probability distribution, introduced as a strict microscopic constraint on time evolution in phase space. The conditional probability distribution is defined for the set of microstates associated with the set of phase space paths determined by solutions of Hamilton's equations. The MaxEnt inference algorithm with Shannon's concept of the <b>conditional</b> <b>information</b> entropy is then applied to prediction, consistently with this strict microscopic constraint on time evolution in phase space. The second approach {{is based on the}} same concepts, with a difference that Liouville equation for the conditional probability distribution is introduced as a macroscopic constraint given by a phase space average. We consider the incomplete nature of our information about microscopic dynamics in a rational way that is consistent with Jaynes' formulation of predictive statistical mechanics. Maximization of the <b>conditional</b> <b>information</b> entropy subject to this macroscopic constraint leads to a loss of correlation between the initial phase space paths and final microstates. Information entropy is the theoretic upper bound on the <b>conditional</b> <b>information</b> entropy, with the upper bound attained only in case of the complete loss of correlation. In this alternative approach to prediction of macroscopic time evolution, maximization of the <b>conditional</b> <b>information</b> entropy is equivalent to the loss of statistical correlation. In accordance with Jaynes, irreversibility appears as a consequence of gradual loss of information about possible microstates of the system. Comment: 25 page...|$|R
