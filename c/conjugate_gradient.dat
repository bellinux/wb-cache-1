4856|481|Public
5|$|In many {{practical}} situations {{additional information}} about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the <b>conjugate</b> <b>gradient</b> method.|$|E
25|$|Levenberg-Marquardt and <b>conjugate</b> <b>gradient</b> (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled <b>conjugate</b> <b>gradient).</b>|$|E
25|$|A {{disadvantage}} of the Crank–Nicolson method {{is that the}} matrix in the above equation is banded with a band width that is generally quite large. This makes direct solution {{of the system of}} linear equations quite costly (although efficient approximate solutions exist, for example use of the <b>conjugate</b> <b>gradient</b> method preconditioned with incomplete Cholesky factorization).|$|E
40|$|Iterative {{methods for}} {{arbitrary}} mesh discretizations of elliptic partial differential equations are surveyed. The methods discussed are preconditioned <b>conjugate</b> <b>gradients,</b> algebraic multigrid, deflated <b>conjugate</b> <b>gradients,</b> an element-by-element techniques, and domain decomposition. Computational results are included...|$|R
40|$|Abstract Background Possible {{methods for}} {{distinguishing}} receptor binding models and analysing their parameters are considered. Results and Discussion The <b>conjugate</b> <b>gradients</b> method {{is shown to}} be optimal for solving problems of the kind considered. Convergence with experimental data is rapidly achieved with the appropriate model but not with alternative models. Conclusion Lack of convergence using the <b>conjugate</b> <b>gradients</b> method {{can be taken to}} indicate inconsistency between the receptor binding model and the experimental data. Thus, the <b>conjugate</b> <b>gradients</b> method can be used to distinguish among receptor binding models. </p...|$|R
40|$|A derivative-free {{frame-based}} <b>conjugate</b> <b>gradients</b> {{algorithm is}} presented. Convergence is shown for C 1 functions, {{and this is}} verified in numerical trials. The algorithm is tested {{on a variety of}} low dimensional problems, some of which are ill-conditioned, and is also tested on problems of high dimension. Numerical results show that the algorithm is effective on both classes of problems. The results are compared with those from a discrete quasi-Newton method, showing that the <b>conjugate</b> <b>gradients</b> algorithm is competitive. The algorithm exhibits the <b>conjugate</b> <b>gradients</b> speed-up on problems for which the Hessian at the solution has repeated or clustered eigenvalues. The algorithm is easily parallelizable...|$|R
25|$|Iterative {{methods are}} more common than direct methods in {{numerical}} analysis. Some methods are direct in principle but are usually used {{as though they were}} not, e.g. GMRES and the <b>conjugate</b> <b>gradient</b> method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted {{in the same manner as}} for an iterative method.|$|E
25|$|Much {{effort has}} been put in the {{development}} of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and <b>conjugate</b> <b>gradient</b> method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.|$|E
25|$|The {{most common}} current {{homology}} modeling method takes its inspiration from calculations required {{to construct a}} three-dimensional structure from data generated by NMR spectroscopy. One or more target-template alignments are used to construct a set of geometrical criteria that are then converted to probability density functions for each restraint. Restraints applied to the main protein internal coordinates – protein backbone distances and dihedral angles – {{serve as the basis}} for a global optimization procedure that originally used <b>conjugate</b> <b>gradient</b> energy minimization to iteratively refine the positions of all heavy atoms in the protein.|$|E
40|$|AbstractWe {{present an}} {{efficient}} {{implementation of the}} <b>Conjugate</b> <b>Gradients</b> algorithm for Wiener-Hopf integral equations based on finite rank approximations of the integral operator and the corresponding preconditioner. The resulting algorithm is of linear complexity. Numerical experiments with this implementation of the preconditioned <b>Conjugate</b> <b>Gradients</b> algorithm show significant speed-up in the ill-conditioned case. This algorithm acts on ill-conditioned equations as a regularization algorithm...|$|R
40|$|Lanczos's major {{contributions}} to the numerical solution of linear equations are contained in two papers: ``An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators'' and ``Solutions of Linear Equations by Minimized Iterations,'' the second of which contains the method of <b>conjugate</b> <b>gradients.</b> In this note we retrace Lanczos's journey from Krylov sequences to <b>conjugate</b> <b>gradients.</b> (Also cross-referenced as UMIACS-TR- 91 - 47...|$|R
40|$|This paper {{concerns}} exact linesearch quasi-Newton {{methods for}} minimizing a quadratic function whose Hessian is positive definite. We show that by interpreting {{the method of}} <b>conjugate</b> <b>gradients</b> as a particular exact linesearch quasi-Newton method, necessary and sufficient conditions can be given for an exact linesearch quasi-Newton method to generate a search direction which is parallel {{to that of the}} method of <b>conjugate</b> <b>gradients.</b> We also analyze update matrices and give a complete description of the rank-one update matrices that give search direction parallel to those of the method of <b>conjugate</b> <b>gradients.</b> In particular, we characterize the family of such symmetric rank-one update matrices that preserve positive definiteness of the quasi-Newton matrix. This is in contrast to the classical symmetric-rank-one update where there is no freedom in choosing the matrix, and positive definiteness cannot be preserved. The analysis is extended to search directions that are parallel to those of the preconditioned method of <b>conjugate</b> <b>gradients</b> in a straightforward manner. Comment: The final publication is available at Springer via [URL]...|$|R
25|$|Currently {{there are}} two Poisson solvers {{implemented}} in BioMOCA based on the finite difference method. One uses the pre-conditioned <b>Conjugate</b> <b>Gradient</b> scheme (pCG) and is used by default. The later is borrowed from an APBS solver, which uses a V-multi-grid scheme. Other than the numerical approach to solve the Poisson equation, {{the main difference between}} the two solvers is on how they address the permittivity in the system. In the first solver, a dielectric value is assigned to each cell in the grid, while in the APBS solver the dielectric coefficients are defined on the grid nodes. As discussed earlier box integration method is used in the pCG solver, which allows us to treat the Poisson equation in the most accurate way. Even though a full multigrid solver based on box-integration method has been under development, there is a neat way to reuse the already exiting code and treat the ion channel systems.|$|E
2500|$|Bundle {{method of}} descent: An {{iterative}} method for small–medium-sized problems with locally Lipschitz functions, particularly for convex minimization problems. [...] (Similar to <b>conjugate</b> <b>gradient</b> methods) ...|$|E
2500|$|After {{applying}} {{one of the}} optimization {{methods to}} the value of the dual (such as Newton's method or <b>conjugate</b> <b>gradient)</b> we get the value of : ...|$|E
40|$|We discretize {{the shallow}} water {{equations}} with an Adams-Bashford scheme combined with the Crank-Nicholson scheme for the time derivatives and spectral elements for the discretization in space. The resulting coupled system of equations will {{be reduced to a}} Schur complement system with a special structure of the Schur complement. This system can be solved with a preconditioned <b>conjugate</b> <b>gradients,</b> where the matrix-vector product is only implicitly given. We derive an overlapping block preconditioner based on additive Schwarz methods for preconditioning the reduced system. Keywords: Shallow water equations, h-p nite elements, adaptive grids, multigrid, parallel computing, <b>conjugate</b> <b>gradients,</b> additive Schwarz preconditioner. 1...|$|R
40|$|We {{investigate}} a new algorithm for computing regularized solutions of the two-dimensional magnetotelluric inverse problem. The algorithm employs a nonlinear <b>conjugate</b> <b>gradients</b> (NLCG) scheme to minimize an objective function that penalizes data residuals and second spatial derivatives of resistivity. We compare this algorithm theoretically and numerically to two previous algorithms for constructing such 'minimum-structure' models: the Gauss-Newton method, which solves {{a sequence of}} linearized inverse problems and has been the standard approach to nonlinear inversion in geophysics, and an algorithm due to Mackie and Madden, which solves a sequence of linearized inverse problems incompletely using a (linear) <b>conjugate</b> <b>gradients</b> technique. Numerical experiments involving synthetic and field data indicate that the two algorithms based on <b>conjugate</b> <b>gradients</b> (NLCG and Mackie-Madden) are more efficient than the GaussNewton algorithm {{in terms of both}} computer memory requirements and CPU time needed to find accurate solutions to problems of realistic size. This owes largely {{to the fact that the}} conjugate gradients-based algorithms avoid two computationally intensive tasks that are performed at each step of a Gauss-Newton iteration: calculation of the full Jacobian matrix of the forward modeling operator, and complete solution of a linear system on the model space. The numerical tests also show that the Mackie-Madden algorithm reduces the objective function more quickly than our new NLCG algorithm in the early stages of minimization, but NLCG is more effective in the later computations. To help understand these results, we describe the Mackie-Madden and new NLCG algorithms in detail and couch each as a special case of a more general <b>conjugate</b> <b>gradients</b> scheme for nonlinear inversion...|$|R
40|$|A general {{algorithm}} for minimizing a quadratic {{function with}} bounds on the variables is presented. The new algorithm can use different unconstrained minimization techniques on different faces. At every face, the minimization technique can be chosen according to he {{structure of the}} Hessian and the dimension of the face. The strategy for leaving the face {{is based on a}} simple scheme that exploits the properties of the "chopped gradient" introduced by Friedlander and Mart'inez in 1989. This strategy guarantees global convergence even in the presence of dual degeneracy, and finite identification in the nondegenerate case. A slight modification of the algorithm satisfies, in addition, an identification property in the case of dual degeneracy. Numerical experiments combining this new strategy with <b>conjugate</b> <b>gradients,</b> gradient with retards and direct solvers are presented. Key words. Quadratic programming, <b>conjugate</b> <b>gradients,</b> gradient with retards, active set methods, sparse Cholesky factor [...] ...|$|R
2500|$|Iterative methods, such as <b>conjugate</b> <b>gradient</b> {{method and}} GMRES utilize fast {{computations}} of matrix-vector products , where matrix [...] is sparse. The use of preconditioners can significantly accelerate convergence of such iterative methods.|$|E
2500|$|<b>Conjugate</b> <b>gradient</b> methods: Iterative {{methods for}} large problems. (In theory, these methods {{terminate}} in {{a finite number}} of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.) ...|$|E
2500|$|An {{optimization}} algorithm can use {{some or all}} of , [...] and [...] to try to minimize the forces and this could in theory be any method such as gradient descent, <b>conjugate</b> <b>gradient</b> or Newton's method, but in practice, algorithms which use knowledge of the PES curvature, that is the Hessian matrix, are found to be superior. For most systems of practical interest, however, it may be prohibitively expensive to compute the second derivative matrix, and it is estimated from successive values of the gradient, as is typical in a Quasi-Newton optimization.|$|E
3000|$|... [...]. In this experiment, we set {{the mean}} squared {{relative}} error {{of the channel}} estimate to 10 dB. Note, that the <b>conjugated</b> <b>gradient</b> algorithm that follows the decoupling does not need the channel knowledge to reach the MMSE solution.|$|R
40|$|This {{thesis is}} based on recent work by Paige which gave a {{formalism}} for presenting and analyzing the class of algorithms which manipulate an appropriate Krylov subspace in solving large sparse systems of linear equations. This formalism [...] a way of dividing a method of solution into a Krylov process and an associated subproblem [...] is described and then applied to several {{of the more popular}} algorithms in use today including the methods of <b>Conjugate</b> <b>Gradients</b> and BiConjugate Gradients. The aim is to clarify these algorithms to make them easier to understand, analyze and use. Several of the methods presented in this thesis were developed in exactly this way [...] notably the Symmetric LQ method and the Generalized Minimum Residual method [...] and required little or no effort to characterize using the formalism. It was successfully applied to <b>Conjugate</b> <b>Gradients</b> and BiConjugate Gradients, already recognized as being closely related to the symmetric and unsymmetric Lanczos processes respectively. The newer algorithms such as <b>Conjugate</b> <b>Gradients</b> Squared and BiConjugate Gradients Stabilized, with less obvious relation to a specific Krylov process, provided more difficulty in their clarification...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 90 - 91). Issued also on microfiche from Lange Micrographics. In this study, the inversion of TEM sounding is investigated. I solved the over-determined and the under-determined inversion problems using the steepest descent and the <b>conjugate</b> <b>gradients</b> methods. The study depends on results from the inversion of synthetic responses generated by synthetic test models. In the over-determined problem, the inversion of layer conductivities and thicknesses are solved separately for few-layer Earth models. Out of 468 inversion runs, the results showed that the <b>conjugate</b> <b>gradients</b> method demands less calculation than the steepest descent method. The final model error and misfit values are generally better for the <b>conjugate</b> <b>gradients.</b> Although the Inversion for layer conductivities proved to be impractical in the over-determined problem, the inversion for layer thicknesses proved to be very reliable. I suggested a strategy to use both inversion types in mapping horizontal layers. The under-determined problem was solved by a regularized inversion. A total of 48 inversion runs of five synthetic responses and one real data set have been performed. The steepest descent and the <b>conjugate</b> <b>gradients</b> methods showed a comparable performance. The regularized under-determined inversion proved to be more satisfactory in constructing the original test models than the over-determined problem, in consistency with previous works. The inversion of real data from a geologically 1 -D site in Central Texas resulted in final models consistent with prior geological information about the site...|$|R
5000|$|Levenberg-Marquardt and <b>conjugate</b> <b>gradient</b> (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled <b>conjugate</b> <b>gradient).</b>|$|E
5000|$|In {{numerical}} optimization, the nonlinear <b>conjugate</b> <b>gradient</b> method generalizes the <b>conjugate</b> <b>gradient</b> {{method to}} nonlinear optimization. For a quadratic function : ...|$|E
5000|$|In most cases, {{preconditioning}} {{is necessary}} to ensure fast convergence of the <b>conjugate</b> <b>gradient</b> method. The preconditioned <b>conjugate</b> <b>gradient</b> method takes the following form: ...|$|E
3000|$|Hajarian [19] derived {{a simple}} and {{efficient}} matrix algorithm to solve the general coupled matrix equations ∑_j = 1 ^p A_ijX_jB_ij = C_i, i = 1, 2, [...]...,p (including several linear matrix equations as particular cases) based on the <b>conjugate</b> <b>gradients</b> squared (CGS) method.|$|R
40|$|Abstract: In {{this work}} the finite-difference schemes for coolant {{thermohydraulics}} equations and fuel pins temperature equations are presented. The fully implicit scheme for solving conservation equations is used. Solution of obtained systems of algebraic linear equations is made using <b>conjugate</b> <b>gradients</b> method. Note: Publication language:russia...|$|R
40|$|Abstract — Power grid {{analysis}} is a challenging problem for modern integrated circuits. For 3 -D systems fabricated using stacked tiers with TSVs, traditional power grid analysis methods for planar (2 -D) circuits do not demonstrate the same performance. An efficient IR drop analysis method for 3 -D large-scale circuits, called 3 -D voltage propagation method, is proposed in this paper. This method is compared with another widely used power grid analysis method, with preconditioned <b>conjugated</b> <b>gradients.</b> Simulation results {{demonstrate that the}} proposed method is more efficient for the IR drop analysis of large size 3 -D power grids. Speedups between 10 × to 20 × over the preconditioned <b>conjugated</b> <b>gradients</b> method are shown. Keywords – 3 -D integrated circuits; Through-silicon vias; Power grid analysi...|$|R
50|$|The <b>conjugate</b> <b>gradient</b> method can {{be applied}} to an {{arbitrary}} n-by-m matrix by applying it to normal equations ATA and right-hand side vector ATb, since ATA is a symmetric positive-semidefinite matrix for any A. The result is <b>conjugate</b> <b>gradient</b> on the normal equations (CGNR).|$|E
5000|$|Within {{a linear}} approximation, the {{parameters}} [...] and [...] {{are the same}} as in thelinear <b>conjugate</b> <b>gradient</b> method but have been obtained with line searches.The <b>conjugate</b> <b>gradient</b> method can follow narrow (ill-conditioned) valleys, where the steepest descent method slows down and follows a criss-cross pattern.|$|E
5000|$|... energy {{minimization}} (steepest {{descent and}} <b>conjugate</b> <b>gradient)</b> ...|$|E
40|$|AbstractThe <b>conjugate</b> <b>gradients</b> method generates {{successive}} approximations xi for {{the solution}} of the linear system Ax = b, where A is symmetric positive definite and usually sparse. It will be shown how intermediate information obtained by the <b>conjugate</b> <b>gradients</b> (cg) algorithm (or by the closely related Lanczos algorithm) can be used to solve f(A) x = b iteratively in an efficient way, for suitable functions f. The special case f(A) = A 2 is discussed in particular. We also consider the problem of solving Ax = b for different right-hand sides b. A variant on a well-known algorithm for that problem is proposed, which does not seem to suffer from the usual loss of orthogonality in the standard cg and Lanczos algorithms...|$|R
40|$|Abstract. Bundle {{adjustment}} for multi-view reconstruction is tradi-tionally done using the Levenberg-Marquardt algorithm with a direct linear solver, which is computationally very expensive. An alternative to {{this approach is}} to apply the <b>conjugate</b> <b>gradients</b> algorithm in the inner loop. This is appealing since the main computational step of the CG algorithm involves only a simple matrix-vector multiplication with the Jacobian. In this work we improve on the latest published approaches to bundle adjustment with <b>conjugate</b> <b>gradients</b> by {{making full use of}} the least squares nature of the problem. We employ an easy-to-compute QR factorization based block preconditioner and show how a certain property of the preconditioned system allows us to reduce the work per iteration to roughly half of the standard CG algorithm. ...|$|R
40|$|Abstract. The {{ranking of}} genes plays an {{important}} role in biomedical research. The GeneRank method of Morrison et al. [BMC Bioinformatics, 6 : 233 (2005) ] ranks genes based on the results of microarray experiments combined with gene expression information, for example from gene annotations. The algorithm is a variant of the well known PageRank iteration, and can be formulated as the solution of a large, sparse linear system. Here we show that classical Chebyshev semi-iteration can considerably speed up the convergence of GeneRank, outperforming other acceleration schemes such as <b>conjugate</b> <b>gradients.</b> Key words. GeneRank, computational genomics, Chebyshev semi-iteration, polynomials of best uniform approximation, <b>conjugate</b> <b>gradients</b> AMS subject classifications. 65 F 10, 65 F 50; 9208, 92 D 20 1. Introduction. Advance...|$|R
