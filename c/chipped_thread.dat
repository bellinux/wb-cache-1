0|22|Public
5000|$|Chip-level {{multiprocessing}} (CMP or multicore): integrates {{two or more}} processors {{into one}} <b>chip,</b> each executing <b>threads</b> independently.|$|R
40|$|Multi-core <b>chips</b> allow <b>thread</b> {{and program}} level {{parallelism}} thus increasing performance. How-ever, this {{comes with the}} cost of temperature problem. Multi-core chips require more power, cre-ating non uniform power map and hotspots. Thread migration is one of the solutions to distribute power in a more uniform manner over the chip. This work evaluates thread migration techniques and investigates issues such as thread fairness and migration overhead...|$|R
40|$|Multi-core <b>chips</b> allow <b>thread</b> {{and program}} level {{parallelism}} thus increasing performance. However, this {{comes with the}} cost of temperature problem. Multi-core chips require more power, creating non uniform power map and hotspots. Activity migration is one of the solutions to distribute power in a more uniform manner over the chip. Thermal Aware Multi Core Scheduler combines Dynamic Thermal Management (DTM) techniques, thread priorities and activity migration algorithms to achieve a uniform temperature map with the best performance...|$|R
40|$|In this bachelor’s thesis {{written on}} theme „Technology cutting {{external}} thread“ is mentioned basic overwiev of external threads and their labeling. Further there are described types of manufacturing external <b>threads</b> <b>chip</b> machining. For chosen type of machining is calculated unit machine time with it’s associated total costs to manufacture external thread...|$|R
5000|$|Olukotun is also {{a member}} of the board of advisors of UDC, a Nigerian venture capital firm. He was elected as a Fellow of the Association for Computing Machinery in 2006 for his [...] "contributions to multi{{processor}}s on a <b>chip</b> and multi <b>threaded</b> processor design". He became a Fellow of the IEEE in 2008.|$|R
50|$|SPARC T5 is {{the fifth}} {{generation}} multicore microprocessor of Oracle's SPARC T-Series family. It was first presented at Hot Chips 24 in August 2012, and was officially introduced with the Oracle SPARC T5 servers in March 2013. The processor is designed to offer high multithreaded performance (16 cores per <b>chip,</b> with 8 <b>threads</b> per core), as well as high single threaded performance from the same chip.|$|R
40|$|A {{new trend}} in {{processor}} design is increased on-chip support for multithreading {{in the form}} of both chip multiprocessors and simultaneous multithreading. Recent research in database systems has begun to explore increased thread-level parallelism made possible by these new multicore and multithreaded processors. The question of how best to use this new technology remains open, particularly as the number of cores per <b>chip</b> and <b>threads</b> per core increase. In this paper we use an existing massively multithreaded architecture, the Cray MTA- 2, to explore the implications that a larger degree of on-chip multithreading may have for parallelism in database operations. We find that parallelism in database operations is easy to achieve on the MTA- 2 and that, with little effort, parallelism can be made to scale linearly with the number of available processor cores. 1...|$|R
40|$|In {{computer}} science, concurrency is {{a property}} of systems in which several computations are executing simultaneously, and potentially interacting with each other. The computations may be executing on multiple cores {{in the same}} <b>chip,</b> preemptively time-shared <b>threads</b> on the same processor, or executed on physically separated processors. A number of mathematical models {{have been developed for}} general concurrent computation including Petri nets, process calculi, the Parallel Random Access Machine model, the Actor model and the Reo Coordination Language. Specific applications to static program analysis – design of automated tools to verify correctness etc. of a concurrent program regardless of specific timed execution...|$|R
40|$|International audienceThis work {{addresses}} the early exploration phase, before the hardware is available, {{of the design}} of a System on a <b>Chip.</b> We detect <b>threads</b> in C programs using a software only technique. The computed threads are used as a basis for partitioning the applications. The threads are built using profiling and hot-paths information. We use a speculative model that, contrary to previous approaches, does not assume a shared memory. The speculation is performed on control flow and data structure layout. The output of the proposed method is a set of threads characterized by their execution time, the amount of memory and communication required, etc. Preliminary results show that the approach is able to capture and to characterize the main computation kernels of embedded applications...|$|R
40|$|This paper proposes using a User-Level Memory Thread (ULMT) for {{correlation}} prefetching. In this approach, a user thread runs on a general-purpose processor in main memory, {{either in}} the memory controller chip or in a DRAM <b>chip.</b> The <b>thread</b> performs correlation prefetching in software, sending the prefetched data into the L 2 cache of the main processor. This approach requires minimal hardware beyond the memory processor: The correlation table is a software data structure that resides in main memory, while the main processor only needs a few modifications to its L 2 cache {{so that it can}} accept incoming prefetches. In addition, the approach has wide applicability, as it can effectively prefetch even for irregular applications. Finally, it is very flexible, as the prefetching algorithm can be customized by the user on an application basis. Our simulation results show that, through a new design of the correlation table and prefetching algorithm, our scheme delivers good results. Specifically, nine mostly-irregular applications show an average speedup of 1. 32. Furthermore, our scheme works well in combination with a conventional processor-side sequential prefetcher, in which case the average speedup increases to 1. 46. Finally, by exploiting the customization of the prefetching algorithm, we increase the average speedup to 1. 53...|$|R
40|$|The {{demand for}} high {{performance}} embedded processors, for consumer electronics, is rapidly increasing {{for the past}} few years. Many of these embedded processors depend upon custom built Instruction Ser Architecture (ISA) such as game processor (GPU), multimedia processors, DSP processors etc. Primary requirement for consumer electronic industry is low cost with high performance and low power consumption. A lot of research has been evolved to enhance the performance of embedded processors through parallel computing. But some of them focus superscalar processors i. e. single processors with more resources like Instruction Level Parallelism (ILP) which includes Very Long Instruction Word (VLIW) architecture, custom instruction set extensible processor architecture and others require more number of processing units on a single <b>chip</b> like <b>Thread</b> Level Parallelism (TLP) that includes Simultaneous Multithreading (SMT), Chip Multithreading (CMT) and Chip Multiprocessing (CMP). In this paper, we present a new technique, named C-slow, to enhance performance for embedded processors for consumer electronics by exploiting multithreading technique in single core processors. Without resulting into the complexity of micro controlling with Real Time Operating system (RTOS), C-slowed processor can execute multiple threads in parallel using single datapath of Instruction Set processing element. This technique takes low area & approach complexity of general purpose processor running RTOS...|$|R
40|$|In this paper, we {{introduce}} a novel garbage collector for Java {{to be used}} for processors with speculative threads support like the Hydra <b>chip</b> multiprocessor (CMP). <b>Thread</b> speculation permits parallel execution of sections of sequential code with data dependencies enforced in the hardware, eliminating the need for explicit locking. We have augmented Dijkstra’s classical on-the-fly mark and sweep collector {{to take advantage of the}} CMP’s thread-level speculation and low-latency interprocessor communication. The resulting collector can execute concurrently without an explicit collector thread or can increase the collection rate by executing in parallel on all free processors. A dynamically scalable collector provides more control to adjust collection behavior according to the total system load and real-time deadlines. Speculative threads ensured the scalable implementation was simple and required no additional overheads, but we were unable to observe additional performance gains from eliminated explicit synchronization. 1...|$|R
40|$|Abstract—there {{are many}} methodsofreducing {{the effects of}} {{transient}} and permanent faults that have been proposed recently. The redundant multi-threading (RMT) is a popular architectural level solution that obtains fault detection property using temporal and spatial redundancy. These methods execute two copies of the same thread at different times on various cores in a chip multiprocessor and finally compare the results. In this work we performed a gate-level simulation based fault injection analysis to obtain fault coverage for RMT. We selected <b>Chip</b> level Redundantly <b>Threaded</b> multiprocessor (CRT) that is a main implication of RMT to detect transient and permanent faults at the two-way chip multiprocessors environment. In order to determine fault coverage for this method we injected 5200 transient faults in a CRT dual-processor and analyzed the results. The experimental results show that CRT reduces the failure rate by 35 % whileincreases execution time by 29 %. I...|$|R
40|$|Much of the {{improvement}} in computer performance over {{the last twenty years}} has come from faster transistors and architectural advances that increase parallelism. Historically, parallelism has been exploited either at the instruction level with a grain-sie of a single instruction or by partitioning applications into coarse threads with grain-sies of thousands of instructions. Fine-grain threads fill the parallelism gap between these extremes by enabling tasks with run lengths as small as 20 cycles. As this fine-g rain parallelism is orthogonal to ILP and coarse threads, it complements both methods and provides an opportunity for greater speedup. This paper describes the efficient communication and synchronization mechanisms implemented in the Multi-ALU Processor (MAP) <b>chip,</b> including a <b>thread</b> creation instruction, register communication, and a hardware barrier. These register-based mechanisms provide 10 times faster communication and 60 times faster synchronization than mechanisms that operate via a shared onchip cache. With a three-processor implementation of the MAP, fine-grain speedups of 1. 2 - 2. 1 are demonstrated on a suite of applications...|$|R
40|$|Modern {{computer}} systems {{have a deep}} memory hierarchy consisting of caches and main memory. Unfortunately, while processor speeds have increased dramatically over the past decades, main memory access time has not kept pace, leading to a gap between processor and memory system performance. To make things worse, recent multi-core and multi-thread architectures that incorporate multiple processing cores per <b>chip</b> and multiple <b>threads</b> per core, have only increased the pressure on memory hierarchy. My {{research has focused on}} cache and main memory subsystems design and optimization for multi-core architectures. In the rest of this document, I will first describe my dissertation research, then describe research I have done outside of my dissertation, and finally, provide my future research agenda. 1 Dissertation Research: Thermal Modeling and Management of DRAM Systems With increasing speed and power density, high-performance memories, including FB-DIMM (Fully Buffered DIMM) and DDR 2 DRAM, now begin to require dynamic thermal management (DTM) as processors and hard drives did. The DTM of memories, nevertheless, is different in that it should take the processor performance and power consumption into consideration. Existing schemes have ignored that. We investigate a new approach that controls the memory thermal issues from the source generating memory activities – the processor [2]. It will smooth the program execution when compared with shutting dow...|$|R
40|$|Multi-Processors System on Chip (MPSoCs) and Massively Parallel Processors (MPPs) {{architectures}} are conceived to efficiently implement Thread Level Parallelism, {{a common}} characteristic of modern software applications targeted by embedded systems. Each core in a MPP environment {{is designed to}} execute a particular instructions flow, known as thread, in a completely self-sufficient manner, being {{able to communicate with}} the other cores in order to exchange shared data. The demand of parallelism in MPPs and MPSoCs entails the design of an efficient communication layer able to sustain it. This means that the interconnection medium has to be both scalable, to allow multiple accesses of the different cores to the shared resources, and optimized in terms of wiring. These are all native characteristics of Networks on Chip (NoCs). In MPSoCs and MPPs, it is necessary to provide: - a quick resolution of the interdependencies among different threads, single scalar data or even vectors. Interdependencies are responsible of completion time delay because prevent a thread from completion when not resolved; - load balancing support techniques to avoid hot spots and to efficiently exploit all the cores available on <b>chip.</b> When <b>threads</b> migration occurs, a regular and continuous traffic is generated, made up of long streams of data; - management of end-to-end small control data. Circuit Switching (CS) technique is the method by which a dedicated path, or circuit, is established prior the sending of the sensitive data. Circuit switched networks are suitable for guaranteed throughput applications, especially in case of real time communications. In Packet Switching (PS) methodologies the intermediate routers are responsible for routing the individual packets through the network, neither following a predefined nor a reserved path. Packet switched networks are suitable for best-effort services or for soft-timing constrained communications. In this chapter we will look at the possibility of combining CS and PS in order to support the heterogeneous traffic patterns coexisting in a MPP environment. Hybrid switching networks are designed to guarantee the benefits of both CS and PS consisting in a better usage of the available bandwidth and in a global increase of the overall throughput, at the price of a more complex hardware implementation. In this scope, the latest approaches in literature are presented, together with a particular NoC model able to provide dual-mode hybrid switching in a non-exclusive way, intended as the possibility of co-sharing the amount of available bandwidth between CS and PS communications...|$|R
40|$|Optimizing the {{performance}} of stencil algorithms {{has been the subject}} of intense research over the last two decades. Since many stencil schemes have low arithmetic intensity, most optimizations focus on increasing the temporal data access locality, thus reducing the data traffic through the main memory interface with the ultimate goal of decoupling from this bottleneck. There are, however, only few approaches that explicitly leverage the shared cache feature of modern multicore <b>chips.</b> If every <b>thread</b> works on its private, separate cache block, the available cache space can become too small, and sufficient temporal locality may not be achieved. We propose a flexible multi-dimensional intra-tile parallelization method for stencil algorithms on multicore CPUs with a shared outer-level cache. This method leads to a significant reduction in the required cache space without adverse effects from hardware prefetching or TLB shortage. Our Girih framework includes an auto-tuner to select optimal parameter configurations on the target hardware. We conduct performance experiments on two contemporary Intel processors and compare with the state-of-the-art stencil frameworks PLUTO and Pochoir, using four corner-case stencil schemes and a wide range of problem sizes. Girih shows substantial performance advantages and best arithmetic intensity at almost all problem sizes, especially on low-intensity stencils with variable coefficients. We study in detail {{the performance}} behavior at varying grid size using phenomenological performance modeling. Our analysis of energy consumption reveals that our method can save energy by reduced DRAM bandwidth usage even at marginal performance gain. It is thus well suited for future architectures that will be strongly challenged by the cost of data movement, be it in terms of performance or energy consumption...|$|R
40|$|The chip shape – {{besides the}} cutting forces, tool wear and surface {{roughness}} – {{is one of}} the most commonly used criteria for the evaluation of the machinability. The importance of the chip shape can be explained by its strong influence on process reliability. In consequence an automation of cutting operations is only possible for fa-vourable chip shapes. For longitudinal turning as well as for other processes with con-tinuous cutting edge contact, periodic chip breakage is necessary to avoid unfavourable ribbon or <b>thread</b> <b>chips.</b> Chip breakage is caused by a variety of mutually dependant in-fluencing factors related to workpiece, tool or process parameters. A possibility to predict chip breakage is to model these influences. The three-dimensional Finite Element Method (3 D-FEM) offers the possibility to take the complex chip breaker geometries of the cutting tool into account. For FE-modelling of chip breakage, a criterion is needed, capable of calculating material failure at the position of chip breakage. However, the definition of such a criterion requires research into the thermo-mechanical loads that cause chip breakage. Accordingly, this thesis aims to predict chip breakage using a damage model, which is based on the mechanical and thermal loads in the chip. It contributes to a more funda-mental understanding of cutting processes with geometrically defined cutting edges in general and contributes the prediction of chip breakage in longitudinal turning of AISI 1045. A model-based approach was chosen in order to calculate thermo-mechanical loads. Based on these loads, an adequate damage model could be defined. The localisation of chip breakage and the dominant failure mechanisms in the chip breakage zone were determined in empirical investigations of the cutting process. By applying a sensitivity analysis, the strongest tool- and process-related influences that cause chip breakage were identified and different process conditions for finishing and roughing tool geome-tries were determined. The process conditions included parameter sets with controlled chip breakage as well as sets with unfavourable chip shapes. For the parameter sets with controlled chip breakage, the time and position of material failure were identified using a high speed filming device. The analysis of the chip fracture faces revealed duc-tile material failure as the dominant failure mechanism. Different damage models for this type of material failure were presented and discussed regarding their potential and ap-plicability to predict chip breakage. The decision between the use of existing damage models and the definition of a new damage model was made on the basis of the thermo-mechanical loads in the chip breakage position. An analytic model was then de-veloped to transform the distribution of stress in a blocked chip to a status of plane stress. The maximum tensile stresses depending on the cutting conditions and tool ge-ometry were calculated based on this model. The model correlated with the real borders of controlled chip breakage in a wide range of cuttings depths and feed rates. Only for small cutting depths and large feed rates a significant difference compared to real chip breakage was identified. This deviation is caused by the complex three-dimensional chip flow under these process conditions. It is assumed that these cutting conditions require three-dimensional modelling of the chip flow. Consequently the chip formation, chip flow and expansion of the chip were modelled three-dimensionally using the Finite Element Method (FEM). In this model the current loads as well as their time-dependent developments could be determined. The evalua-tion of the existing damage criteria showed that none of the presented criteria offer suf-ficient reliability for the prediction of chip breakage. Therefore an independent damage criterion for chip breakage prediction was developed based on a damage model of Johnson and Cook. This criterion calculated the residual deformability of the material depending on the tolerated temperatures, strain rates and stress conditions. The calcu-lated damage value lead to a reduction of the material strength and to a local softening in the chip. The implementation of this damage criterion in the FEM-model enabled three-dimensional simulation of chip breakage. The model correlated well with the chip flow and breakage as well as with the empirically determined cutting forces and tem-peratures on the lower surface of the chip. The FEM-model developed enables system-atic investigation of the influence of tool geometries and cutting conditions on chip breakage for turning operations of AISI 1045...|$|R
40|$|Premi extraordinari doctorat curs 2009 - 2010, àmbit de les TICThe {{evolution}} of microprocessor {{design in the}} last few decades has changed significantly, moving from simple inorder single core architectures to superscalar and vector architectures in order to extract the maximum available instruction level parallelism. Executing several instructions from the same thread in parallel allows significantly improving the performance of an application. However, there is only a limited amount of parallelism available in each thread, because of data and control dependences. Furthermore, designing a high performance, single, monolithic processor has become very complex due to power and chip latencies constraints. These limitations have motivated the use of thread level parallelism (TLP) as a common strategy for improving processor performance. Multithreaded processors allow executing different threads at the same time, sharing some hardware resources. There are several flavors of multithreaded processors that exploit the TLP, such as chip multiprocessors (CMP), coarse grain multithreading, fine grain multithreading, simultaneous multithreading (SMT), and combinations of them. To improve cost and power efficiency, the computer industry has adopted multicore chips. In particular, CMP architectures have become the most common design decision (combined sometimes with multithreaded cores). Firstly, CMPs reduce design costs and average power consumption by promoting design re-use and simpler processor cores. For example, it is less complex to design a chip with many small, simple cores than a chip with fewer, larger, monolithic cores. Furthermore, simpler cores have less power hungry centralized hardware structures. Secondly, CMPs reduce costs by improving hardware resource utilization. On a multicore <b>chip,</b> co-scheduled <b>threads</b> can share costly microarchitecture resources that would otherwise be underutilized. Higher resource utilization improves aggregate performance and enables lower cost design alternatives. One of the resources that impacts most on the final performance of an application is the cache hierarchy. Caches store data recently used by the applications in order to take advantage of temporal and spatial locality of applications. Caches provide fast access to data, improving the performance of applications. Caches with low latencies have to be small, which prompts the design of a cache hierarchy organized into several levels of cache. In CMPs, the cache hierarchy is normally organized in a first level (L 1) of instruction and data caches private to each core. A last level of cache (LLC) is normally shared among different cores in the processor (L 2, L 3 or both). Shared caches increase resource utilization and system performance. Large caches improve performance and efficiency by increasing the probability that each application can access data from a closer level of the cache hierarchy. It also allows an application {{to make use of the}} entire cache if needed. A second advantage of having a shared cache in a CMP design has to do with the cache coherency. In parallel applications, different threads share the same data and keep a local copy of this data in their cache. With multiple processors, it is possible for one processor to change the data, leaving another processor's cache with outdated data. Cache coherency protocol monitors changes to data and ensures that all processor caches have the most recent data. When the parallel application executes on the same physical chip, the cache coherency circuitry can operate at the speed of on-chip communications, rather than having to use the much slower between-chip communication, as is required with discrete processors on separate chips. These coherence protocols are simpler to design with a unified and shared level of cache onchip. Due to the advantages that multicore architectures offer, chip vendors use CMP architectures in current high performance, network, real-time and embedded systems. Several of these commercial processors have a level of the cache hierarchy shared by different cores. For example, the Sun UltraSPARC T 2 has a 16 -way 4 MB L 2 cache shared by 8 cores each one up to 8 -way SMT. Other processors like the Intel Core 2 family also share up to a 12 MB 24 -way L 2 cache. In contrast, the AMD K 10 family has a private L 2 cache per core and a shared L 3 cache, with up to a 6 MB 64 -way L 3 cache. As the long-term trend of increasing integration continues, the number of cores per chip is also projected to increase with each successive technology generation. Some significant studies have shown that processors with hundreds of cores per chip will appear in the market in the following years. The manycore era has already begun. Although this era provides many opportunities, it also presents many challenges. In particular, higher hardware resource sharing among concurrently executing threads can cause individual thread's performance to become unpredictable and might lead to violations of the individual applications' performance requirements. Current resource management mechanisms and policies are no longer adequate for future multicore systems. Some applications present low re-use of their data and pollute caches with data streams, such as multimedia, communications or streaming applications, or have many compulsory misses that cannot be solved by assigning more cache space to the application. Traditional eviction policies such as Least Recently Used (LRU), pseudo LRU or random are demand-driven, that is, they tend to give more space to the application that has more accesses to the cache hierarchy. When no direct control over shared resources is exercised (the last level cache in this case), it is possible that a particular thread allocates most of the shared resources, degrading other threads performance. As a consequence, high resource sharing and resource utilization can cause systems to become unstable and violate individual applications' requirements. If we want to provide a Quality of Service (QoS) to applications, we need to enhance the control over shared resources and enrich the collaboration between the OS and the architecture. In this thesis, we propose software and hardware mechanisms to improve cache sharing in CMP architectures. We make use of a holistic approach, coordinating targets of software and hardware to improve system aggregate performance and provide QoS to applications. We make use of explicit resource allocation techniques to control the shared cache in a CMP architecture, with resource allocation targets driven by hardware and software mechanisms. The main contributions of this thesis are the following: - We have characterized different single- and multithreaded applications and classified workloads with a systematic method to better understand and explain the cache sharing effects on a CMP architecture. We have made a special effort in studying previous cache partitioning techniques for CMP architectures, in order to acquire the insight to propose improved mechanisms. - In CMP architectures with out-of-order processors, cache misses can be served in parallel and share the miss penalty to access main memory. We take this fact into account to propose new cache partitioning algorithms guided by the memory-level parallelism (MLP) of each application. With these algorithms, the system performance is improved (in terms of throughput and fairness) without significantly increasing the hardware required by previous proposals. - Driving cache partition decisions with indirect indicators of performance such as misses, MLP or data re-use may lead to suboptimal cache partitions. Ideally, the appropriate metric to drive cache partitions should be the target metric to optimize, which is normally related to IPC. Thus, we have developed a hardware mechanism, OPACU, which is able to obtain at run-time accurate predictions of the performance of an application when running with different cache assignments. - Using performance predictions, we have introduced a new framework to manage shared caches in CMP architectures, FlexDCP, which allows the OS to optimize different IPC-related target metrics like throughput or fairness and provide QoS to applications. FlexDCP allows an enhanced coordination between the hardware and the software layers, which leads to improved system performance and flexibility. - Next, we have made use of performance estimations to reduce the load imbalance problem in parallel applications. We have built a run-time mechanism that detects parallel applications sensitive to cache allocation and, in these situations, the load imbalance is reduced by assigning more cache space to the slowest threads. This mechanism, helps reducing the long optimization time in terms of man-years of effort devoted to large-scale parallel applications. - Finally, we have stated the main characteristics that future multicore processors with thousands of cores should have. An enhanced coordination between the software and hardware layers has been proposed to better manage the shared resources in these architectures. Award-winningPostprint (published version...|$|R
40|$|The {{evolution}} of microprocessor {{design in the}} last few decades has changed significantly, moving from simple inorder single core architectures to superscalar and vector architectures in order to extract the maximum available instruction level parallelism. Executing several instructions from the same thread in parallel allows significantly improving the performance of an application. However, there is only a limited amount of parallelism available in each thread, because of data and control dependences. Furthermore, designing a high performance, single, monolithic processor has become very complex due to power and chip latencies constraints. These limitations have motivated the use of thread level parallelism (TLP) as a common strategy for improving processor performance. Multithreaded processors allow executing different threads at the same time, sharing some hardware resources. There are several flavors of multithreaded processors that exploit the TLP, such as chip multiprocessors (CMP), coarse grain multithreading, fine grain multithreading, simultaneous multithreading (SMT), and combinations of them. To improve cost and power efficiency, the computer industry has adopted multicore chips. In particular, CMP architectures have become the most common design decision (combined sometimes with multithreaded cores). Firstly, CMPs reduce design costs and average power consumption by promoting design re-use and simpler processor cores. For example, it is less complex to design a chip with many small, simple cores than a chip with fewer, larger, monolithic cores. Furthermore, simpler cores have less power hungry centralized hardware structures. Secondly, CMPs reduce costs by improving hardware resource utilization. On a multicore <b>chip,</b> co-scheduled <b>threads</b> can share costly microarchitecture resources that would otherwise be underutilized. Higher resource utilization improves aggregate performance and enables lower cost design alternatives. One of the resources that impacts most on the final performance of an application is the cache hierarchy. Caches store data recently used by the applications in order to take advantage of temporal and spatial locality of applications. Caches provide fast access to data, improving the performance of applications. Caches with low latencies have to be small, which prompts the design of a cache hierarchy organized into several levels of cache. In CMPs, the cache hierarchy is normally organized in a first level (L 1) of instruction and data caches private to each core. A last level of cache (LLC) is normally shared among different cores in the processor (L 2, L 3 or both). Shared caches increase resource utilization and system performance. Large caches improve performance and efficiency by increasing the probability that each application can access data from a closer level of the cache hierarchy. It also allows an application {{to make use of the}} entire cache if needed. A second advantage of having a shared cache in a CMP design has to do with the cache coherency. In parallel applications, different threads share the same data and keep a local copy of this data in their cache. With multiple processors, it is possible for one processor to change the data, leaving another processor's cache with outdated data. Cache coherency protocol monitors changes to data and ensures that all processor caches have the most recent data. When the parallel application executes on the same physical chip, the cache coherency circuitry can operate at the speed of on-chip communications, rather than having to use the much slower between-chip communication, as is required with discrete processors on separate chips. These coherence protocols are simpler to design with a unified and shared level of cache onchip. Due to the advantages that multicore architectures offer, chip vendors use CMP architectures in current high performance, network, real-time and embedded systems. Several of these commercial processors have a level of the cache hierarchy shared by different cores. For example, the Sun UltraSPARC T 2 has a 16 -way 4 MB L 2 cache shared by 8 cores each one up to 8 -way SMT. Other processors like the Intel Core 2 family also share up to a 12 MB 24 -way L 2 cache. In contrast, the AMD K 10 family has a private L 2 cache per core and a shared L 3 cache, with up to a 6 MB 64 -way L 3 cache. As the long-term trend of increasing integration continues, the number of cores per chip is also projected to increase with each successive technology generation. Some significant studies have shown that processors with hundreds of cores per chip will appear in the market in the following years. The manycore era has already begun. Although this era provides many opportunities, it also presents many challenges. In particular, higher hardware resource sharing among concurrently executing threads can cause individual thread's performance to become unpredictable and might lead to violations of the individual applications' performance requirements. Current resource management mechanisms and policies are no longer adequate for future multicore systems. Some applications present low re-use of their data and pollute caches with data streams, such as multimedia, communications or streaming applications, or have many compulsory misses that cannot be solved by assigning more cache space to the application. Traditional eviction policies such as Least Recently Used (LRU), pseudo LRU or random are demand-driven, that is, they tend to give more space to the application that has more accesses to the cache hierarchy. When no direct control over shared resources is exercised (the last level cache in this case), it is possible that a particular thread allocates most of the shared resources, degrading other threads performance. As a consequence, high resource sharing and resource utilization can cause systems to become unstable and violate individual applications' requirements. If we want to provide a Quality of Service (QoS) to applications, we need to enhance the control over shared resources and enrich the collaboration between the OS and the architecture. In this thesis, we propose software and hardware mechanisms to improve cache sharing in CMP architectures. We make use of a holistic approach, coordinating targets of software and hardware to improve system aggregate performance and provide QoS to applications. We make use of explicit resource allocation techniques to control the shared cache in a CMP architecture, with resource allocation targets driven by hardware and software mechanisms. The main contributions of this thesis are the following: - We have characterized different single- and multithreaded applications and classified workloads with a systematic method to better understand and explain the cache sharing effects on a CMP architecture. We have made a special effort in studying previous cache partitioning techniques for CMP architectures, in order to acquire the insight to propose improved mechanisms. - In CMP architectures with out-of-order processors, cache misses can be served in parallel and share the miss penalty to access main memory. We take this fact into account to propose new cache partitioning algorithms guided by the memory-level parallelism (MLP) of each application. With these algorithms, the system performance is improved (in terms of throughput and fairness) without significantly increasing the hardware required by previous proposals. - Driving cache partition decisions with indirect indicators of performance such as misses, MLP or data re-use may lead to suboptimal cache partitions. Ideally, the appropriate metric to drive cache partitions should be the target metric to optimize, which is normally related to IPC. Thus, we have developed a hardware mechanism, OPACU, which is able to obtain at run-time accurate predictions of the performance of an application when running with different cache assignments. - Using performance predictions, we have introduced a new framework to manage shared caches in CMP architectures, FlexDCP, which allows the OS to optimize different IPC-related target metrics like throughput or fairness and provide QoS to applications. FlexDCP allows an enhanced coordination between the hardware and the software layers, which leads to improved system performance and flexibility. - Next, we have made use of performance estimations to reduce the load imbalance problem in parallel applications. We have built a run-time mechanism that detects parallel applications sensitive to cache allocation and, in these situations, the load imbalance is reduced by assigning more cache space to the slowest threads. This mechanism, helps reducing the long optimization time in terms of man-years of effort devoted to large-scale parallel applications. - Finally, we have stated the main characteristics that future multicore processors with thousands of cores should have. An enhanced coordination between the software and hardware layers has been proposed to better manage the shared resources in these architectures...|$|R

