22|87|Public
40|$|In {{this work}} we discuss a regularization {{framework}} to solve multi-category when the classes are described by an underlying <b>class</b> <b>taxonomy.</b> In particular we discuss {{how to learn}} the <b>class</b> <b>taxonomy</b> while learning a multi-category classifier. This research was sponsored by grants from DARPA (IPTO and DSO), National Science Foundation (NSF- 0640097, NSF- 0827427), AFSOR-THRL (FA 8650 - 05 -C- 7262) Additional support was provided by: Adobe, Honda Research Institute USA, King Abdullah University Science and Technology grant to B. DeVore, NEC, Sony and especially by the Eugene McDermott Foundatio...|$|E
40|$|Abstract. This paper {{presents}} how {{to build}} an ontology in the legal domain following the ontology development methodology METHONTOLOGY and using the ontology engineering workbench WebODE. Both {{of them have been}} widely used to develop ontologies in many other domains. The ontology used to illustrate this paper has been extracted from an existing <b>class</b> <b>taxonomy</b> proposed by Breuker, and adapted to the Spanish legal domain. 1...|$|E
40|$|Abstract. We {{consider}} classification {{problems in}} which the class labels are organized into an abstraction hierarchy {{in the form of}} a <b>class</b> <b>taxonomy.</b> We define a structured label classification problem. We explore two approaches for learning classifiers in such a setting. We also develop a class of performance measures for evaluating the resulting classifiers. We present preliminary results that demonstrate the promise of the proposed approaches. ...|$|E
25|$|Wu. F., Zhang, J., and Honavar, V. (2005) Learning Classifiers Using Hierarchically Structured <b>Class</b> <b>Taxonomies.</b> Proceedings of the Symposium on Abstraction, Reformulation, and Approximation (SARA 2005), Edinburgh, Berlin, Springer-Verlag. Vol. 3607. pp.313â€“320.|$|R
40|$|International audienceWe {{present in}} this paper audio {{classification}} schemes that we have experimented in order to perform musical genres classification. This type of classification {{is a part of}} a more general domain which is automatic semantic audio classification, the applications of which are more and more numerous in such fields as musical or multimedia databases indexing. Experimental results have shown that the feature set we have developed, based on Zipf laws, associated with a combination of classifiers organized hierarchically according to <b>classes</b> <b>taxonomy</b> allow an efficient classificatio...|$|R
40|$|As {{more and}} more data with <b>class</b> <b>taxonomies</b> emerge in diverse fields, such as pattern recognition, text {{classification}} and gene function prediction, we need to extend traditional machine learning methods to solve classification problem in such data sets, which presents more challenges over common pattern classification problems. In this paper, we define structured label classification problem and investigate two learning approaches that can learn classifier in such data sets. We also develop distance metrics with label mapping strategy to evaluate the results. We present experimental results that demonstrate {{the promise of the}} proposed approaches...|$|R
40|$|This paper {{presents}} how {{to build}} an ontology in the legal domain following the ontology development methodology METHONTOLOGY and using the ontology engineering workbench WebODE. Both {{of them have been}} widely used to develop ontologies in many other domains. The ontology used to illustrate this paper has been extracted from an existing <b>class</b> <b>taxonomy</b> proposed by Breuker, and adapted to the Spanish legal domain...|$|E
40|$|This {{contribution}} {{is concerned with}} the development of a modular structured concept to model the dynamical behaviour of bacterial metabolism. The metabolism is considered as two coupled and interacting networks: the metabolic and the regulatory network. By a decomposition of these networks modeling objects are defined. The similarities among these modeling objects are utilized to develop a <b>class</b> <b>taxonomy</b> of modeling objects. (8 References...|$|E
40|$|The {{paper at}} hand {{describes}} {{an approach to}} automatise {{the creation of a}} <b>class</b> <b>taxonomy.</b> Information about objects, e. g. "a tank is armored and moves by track", but no prior knowledge about taxonomy structure is presented to a connectionist system which organizes itself by means of activation spreading (McClelland and Rumelhart, 1981) and weight adjustments. The resulting connectionist network has a form of a taxonomy sought-after...|$|E
40|$|Abstract. Prediction {{problems}} with {{huge numbers of}} classes are becoming more common. While <b>class</b> <b>taxonomies</b> are available in certain cases, we have observed that simple flat learning and classification, via index learning and related techniques, offers significant effi-ciency and accuracy advantages. In the PASCAL challenge on large-scale hierarchical text classification, the accuracies we obtained {{ranked in the top}} three in all evaluations. We also found that using committees of a few learned models boosted accuracy, and observed a tradeoff between accuracy versus memory and time efficiency. This paper is a short report on our approach. ...|$|R
40|$|Abstract. As {{more and}} more data with <b>class</b> <b>taxonomies</b> emerge in diverse fields, such as pattern recognition, text {{classification}} and gene function prediction, we need to extend traditional machine learning methods to solve classification problem in such data sets, which presents more challenges over common pattern classification problems. In this paper, we define structured label classification problem and investigate two learning approaches that can learn classifier in such data sets. We also develop distance metrics with label mapping strategy to evaluate the results. We present experimental results that demonstrate {{the promise of the}} proposed approaches. ...|$|R
40|$|International audienceThis {{paper is}} {{concerned}} with the problem of learning a distance metric by considering meaningful and dis-criminative distance constraints in some contexts where rich information between data is provided. Classic metric learning approaches focus on constraints that involve pairs or triplets of images. We propose a general Mahalanobis-like distance metric learning framework that exploits distance constraints over up to four different images. We show how the integration of such constraints can lead to unsupervised or semi-supervised learning tasks in some applications. We also show the benefit on recognition performance of this type of constraints, in rich contexts such as relative attributes, <b>class</b> <b>taxonomies</b> and temporal webpage analysis...|$|R
40|$|Classification is a {{function}} that matches a new object {{with one of the}} predefined classes. Document classification is characterized by the large number of attributes involved in the objects (documents). The traditional method of building a single classifier to do all the classification work would incur a high overhead. Hierarchical classification is a more efficient method [...] - instead of a single classifier, we use a set of classifiers distributed over a <b>class</b> <b>taxonomy,</b> one for each internal node. However, once a misclassification occurs at a high level class, it may result in a class that is far apart from the correct one. An existing approach to coping with this problem requires terms also to be arranged hierarchically. In this paper, instead of overhauling the classifier itself, we propose mechanisms to detect misclassification and take appropriate actions. We then discuss an alternative that masks the misclassification based on a well known software fault tolerance technique. Our experiments show our algorithms represent a good trade-off between speed and accuracy in most applications. Keywords: Hierarchical document classification, naive Bayesian classifier, error control, <b>class</b> <b>taxonomy,</b> parallel algorithm...|$|E
40|$|Success in {{database}} schema integration {{depends on the}} ability to capture real world semantics of the schema objects, and to reason about the semantics. Earlier schema integration approaches mainly rely on heuristics and human reasoning. In this paper, we discuss an approach to automate {{a significant part of the}} schema integration process. Our approach consists of three phases. An attribute hierarchy is generated in the first phase. This involves identifying relationships (equality, disjointness and inclusion) among attributes. We discuss a strategy based on user-specified semantic clustering. In the second phase, a classification algorithm based on the semantics of class subsumption is applied to the class definitions and the attribute hierarchy to automatically generate a <b>class</b> <b>taxonomy.</b> This <b>class</b> <b>taxonomy</b> represents a partially integrated schema. In the third phase, the user may employ a set of well-defined comparison operators in conjunction with a set of restructuring operators, to further modify the schema. These operators as well as the automatic reasoning during the second phase are based on subsumption. The formal semantics and automatic reasoning utilized in the second phase is based on a terminological logic as adapted in the CANDIDE data model. Classes are completely defined in terms of attributes and constraints. Our observation is that the inability to completely define attributes and thus completely capture their real world semantics imposes a fundamental limitation on the possibility of automatically reasoning about attribute definitions. This necessitates human reasoning during the first phase of the integration approach...|$|E
40|$|Categories in multi-class {{data are}} often {{part of an}} {{underlying}} semantic taxonomy. Recent work in object classification has found interesting ways to use this taxonomy structure to develop better recognition algorithms. Here we propose a novel framework to learn similarity metrics using the <b>class</b> <b>taxonomy.</b> We show that a nearest neighbor classifier using the learned metrics gets improved performance over the best discriminative methods. Moreover, by incorporating the taxonomy, our learned metrics can also help in some taxonomy specific applications. We show that the metrics can help determine the correct placement of a new category that {{was not part of}} the original taxonomy, and can provide effective classification amongst categories local to specific subtrees of the taxonomy. 1...|$|E
40|$|Event-related {{potentials}} (ERP) are brain electrophysiological patterns {{created by}} averaging electroencephalographic (EEG) data, time-locking to events of interest (e. g., stimulus or response onset). In this paper, we propose a generic framework for mining and developing domain ontologies {{and apply it}} to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined according to the following steps: pattern decomposition, extraction of summary metrics for concept candidates, hierarchical clustering of patterns for <b>classes</b> and <b>class</b> <b>taxonomies,</b> and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128 -channel) ERP datasets. Results suggest good correspondence between mined concept...|$|R
5000|$|Mathiot, M. (1962). Noun <b>classes</b> {{and folk}} <b>taxonomy</b> in Papago. American Anthropologist, 64(2), 340-350.|$|R
40|$|RDF is {{increasingly}} {{being used to}} represent metadata. RDF Site Summary (RSS) is an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today does not scale well. In this paper we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS is particularly well suited for applications that deal with largevolume content distribution from diverse sources. RSS is an instance of the content distribution problem. G-ToPSS allows use of ontology {{as a way to}} provide additional information about the data. Furthermore, in this paper we show how G-ToPSS can support RDFS <b>class</b> <b>taxonomies.</b> We have implemented and experimentally evaluated G-ToPSS and we provide results in the paper demonstrating its scalability compared to alternatives...|$|R
40|$|We {{describe}} {{the problem of}} aggregating the label predictions of diverse classifiers using a <b>class</b> <b>taxonomy.</b> Such a taxonomy {{may not have been}} available or referenced when the individual classifiers were designed and trained, yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers. A hierarchical taxonomy representing some domain knowledge may be different from, but partially mappable to, the label sets of the individual classifiers. We present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy. Our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class. We experimentally validate our proposed method on image and text classification tasks. Comment: Under review by AISTATS 201...|$|E
40|$|Abstract. This paper proposes {{the use of}} ontologies {{to provide}} virtual environments with {{semantic}} information, that can be also used by intelligent virtual agents. We present our semantic architecture focusing on the ontology which mainly supports complex object representation and interaction. This approach helps the actors when sensing complex objects such as containers (e. g. cabinet, shelf, box) and it also allows them to interact with different sort of objects, looking at the <b>class</b> <b>taxonomy</b> defined by the ontology of the world. We finally show how the semantic information can be introduced in STRIPS based planners to enrich the actors activities that can be interesting for both narrative games and storytelling systems. 1 Introduction and related work Game narrative [4] and storytelling systems [2] have productively applied planning techniques to generate dynamic and interactive stories. Curiously, although these approaches are well known as knowledge intensive [2], no structure is normall...|$|E
40|$|Abstract. Virtual 3 D city models provide {{important}} information for {{different aspects of}} disaster management. In this context, up-to-dateness of and flexible access to 3 D city models are of utmost importance. Spatial Data Infrastructures (SDI) provide the appropriate framework to cover both aspects, integrating distributed data sources on demand. In this paper we present CityGML, a multi-purpose and multi-scale representation for the storage of and interoperable access to 3 D city models in SDIs. CityGML {{is based on the}} standard GML 3 of the Open Geospatial Consortium and covers the geometrical, topological, and semantic aspects of 3 D city models. The <b>class</b> <b>taxonomy</b> distinguishes between buildings and other man-made artifacts, vegetation objects, waterbodies, and transportation facilities like streets and railways. Spatial as well as semantic properties are structured in five consecutive levels of detail. Throughout the paper, special focus is on the utilization of model concepts with respect to different tasks in disaster management. 1...|$|E
40|$|Within {{an object}} {{knowledge}} representation formalism, <b>class</b> <b>taxonomies</b> are basic structures used {{to organize and}} query the knowledge base. It is therefore profitable to provide an object representation formalism with automatic means for taxonomy inference. Methods deriving classes from sets of unlabeled individuals have been developed within the data analysis and the machine learning fields but these methods use, in general, data descriptions which are less expressive than object models. A taxonomy building strategy based on object proximity estimations is {{presented in this paper}} which deals successfully with advanced features of object formalisms like inter-object relationships, multiple perspectives and generic typing. One of its basic tools is the topological object proximity model, which is universally applicable on object features and flexible, i. e. it includes a range of concrete dissimilarity functions. The output of the topological function are used by various cluste [...] ...|$|R
40|$|This paper {{analyzes}} and evaluates, in {{the context}} of Ontology learning, some techniques to identify and extract candidate terms to <b>classes</b> of a <b>taxonomy.</b> Besides, this work points out some inconsistencies that may be occurring in the preprocessing of text corpus, and proposes techniques to obtain good terms candidate to <b>classes</b> of a <b>taxonomy.</b> The development of this research is partially supported by projects CSO 2008 - 02627 and CSO 2009 - 13713 -C 05 - 04 of the Spanish Ministry of Science and Innovation...|$|R
40|$|This paper {{presents}} some advances {{towards the}} quantitative evaluation of design attributes of object-oriented software systems. We {{believe that these}} attributes can express the quality of internal structure, thus being strongly correlated with quality characteristics like analyzability, changeability, stability and testabilility, which are important to software developers and maintainers. An OO design metrics set is reviewed, along with its rationale. An experiment for collection and analysis of those metrics is described and several suppositions regarding the design are evaluated. A considerable number of <b>class</b> <b>taxonomies</b> written in the C++ language were used as a sample. A tool to collect those metrics was built and used for that purpose. Statistical analysis was performed to evaluate the collected data. Results show that some design heuristics can be derived and used to help guide the design process. It was also clear {{that a number of}} follow-up topics deserve further research. 1. I [...] ...|$|R
40|$|Completely {{data-driven}} grammar {{training is}} prone to over-fitting. Human-defined word class knowledge is useful to address this issue. However, the manual word <b>class</b> <b>taxonomy</b> may be unreliable and irrational for statistical natural language processing, aside from its insufficient linguistic phenomena coverage and domain adaptivity. In this paper, a formalized representation of function word subcategorization is developed for parsing in an automatic manner. The function word classification representing intrinsic features of syntactic usages is used to supervise the grammar induction, {{and the structure of}} the taxonomy is learned simultaneously. The grammar learning process is no longer a unilaterally supervised training by hierarchical knowledge, but an interactive process between the knowledge structure learning and the grammar training. The established taxonomy implies the stochastic significance of the diversified syntactic features. The experiments on both Penn Chinese Treebank and Tsinghua Treebank show that the proposed method improves parsing performance by 1. 6 % and 7. 6 % respectively over the baseline. ...|$|E
40|$|The goal of {{this paper}} is to {{describe}} an open-ended type system for Lisp with explicit and full control of bit-level data representations. This description uses a reflective architecture based on a metatype facility. This low-level formalism solves the problem of an harmonious design of a <b>class</b> <b>taxonomy</b> inside a type system. A prototype for this framework has been written in Le-Lisp and is used to build the integrated type and object systems of the EULISPproposal. Introduction Since the first circular definition of Lisp [8], self-descriptions have became more and more precise [11, 15] until sufficient to be now valuable implementation tools [2, 7, 14, 12]. But if control structures, variables scope or allocation extents are well studied aspects, data structures choice presents much difficulty. Accurate and powerful tools are still needed to (i) design bit-level representations for all Lisp primitive data types (conscells, arrays : : :) or more hidden embedded data structures (stack-f [...] ...|$|E
40|$|Input/output {{subsystem}} {{performance is}} currently receiving considerable research attention. Signi cant e ort {{has been focused}} on reducing average I/O response times and increasing throughput for a given workload. This work has resulted in tremendous advances in I/O subsystem performance. It is unclear, however, how these improvements will be re ected in overall system performance. The central problem {{lies in the fact that}} the current method of study tends to treat all I/O requests as equally important. We introduce a three <b>class</b> <b>taxonomy</b> of I/O requests based on their e ects on system performance. We denote the three classes time-critical, time-limited, andtime-noncritical. A system-level, trace-driven simulation model has been developed for the purpose of studying disk scheduling algorithms. By incorporating knowledge of I/O classes, algorithms tuned for system performance rather than I/O subsystem performance may bedeveloped. Tra-ditional I/O subsystem simulators would rate such algorithms unfavorably because they produce suboptimal subsystem performance. By studying the I/O subsystem via global, system-level simulation, one can more easily identify changes that will improve overall system performance. 0...|$|E
40|$|Abstract. Multi-label Classification (MC) often {{deals with}} hierarchi-cally {{organized}} <b>class</b> <b>taxonomies.</b> In contrast to Hierarchical Multi-label Classification (HMC), where the class hierarchy {{is assumed to}} be known a priori, we are interested in the opposite case where it is unknown and should be extracted from multi-label data automatically. In this case the predictive performance of a classifier can be assessed by well-known Per-formance Measures (PMs) used in flat MC such as precision and recall. The fact that these PMs treat all class labels as independent labels, in contrast to hierarchically structured taxonomies, is a problem. As an alternative, special hierarchical PMs can be used that utilize hierarchy knowledge and apply this knowledge to the extracted hierarchy. This type of hierarchical PM has only recently been mentioned in literature. The aim of this study is first to verify whether HMC measures do signifi-cantly improve quality assessment in this setting. In addition, we seek to find a proper measure that reflects the potential quality of extracted hier-archies in the best possible way. We empirically compare ten hierarchical and four traditional flat PMs in order to investigate relations between them. The performance measurements obtained for predictions of four multi-label classifiers ML-ARAM, ML-kNN, BoosTexter and SVM on four datasets from the text mining domain are analyzed by means of hierarchical clustering and by calculating pairwise statistical consistency and discriminancy...|$|R
40|$|Many of {{the current}} {{automatic}} differentiation (AD) tools have similar characteristics. Unfortunately, it {{is often the case}} that the similarities between these various AD tools can not be easily ascertained by reading the corresponding documentation. To clarify this situation, a taxonomy of AD tools is presented. The taxonomy places AD tools into the Elemental, Extensional, Integral, Operational and Symbolic <b>classes.</b> This <b>taxonomy</b> is used to classify twenty-nine AD tools. Each tool is examined individually with respect to the mode of differentiation used and the degree of derivatives computed. A list detailing the availability of the surveyed AD tools is provided in Appendix A...|$|R
5000|$|The International Committee on Anthropogenic Soils (ICOMANTH) defines {{its mission}} as follows. [...] "ICOMANTH {{is charged with}} {{defining}} appropriate <b>classes</b> in soil <b>taxonomy</b> for soils that have their major properties derived from human activities. The committee should establish which criteria significantly reflect human activities, or when a soil's properties are dominantly the result of human activities." ...|$|R
40|$|Learning {{mappings}} between arbitrary structured {{input and}} output variables is a fundamental problem in machine learning. It covers many natural learning tasks and challenges the standard model of learning a mapping from independently drawn instances to a small set of labels. Potential applications include classification with a <b>class</b> <b>taxonomy,</b> named entity recognition, and natural language parsing. In these structured domains, labeled training instances are generally expensive to obtain while unlabeled inputs are readily available and inexpensive. This thesis deals with semi-supervised learning of discriminative models for structured output variables. The analytical techniques and algorithms of classical semi-supervised learning are lifted to the structured setting. Several approaches based on different assumptions of the data are presented. Colearning, for instance, maximizes the agreement among multiple hypotheses while transductive approaches rely on an implicit cluster assumption. Furthermore, {{in the framework of}} this dissertation, a case study on email batch detection in message streams is presented. The involved tasks exhibit an inherent cluster structure and the presented solution exploits the streaming nature of the data. The different approaches are developed into semi-supervised structured prediction models and efficient optimization strategies thereof are presented. The novel algorithms generalize state-of-the-art approaches in structural learning such as structural support vector machines. Empirical results show that the semi-supervised algorithms lead to significantly lower error rates than their fully supervised counterparts in many application areas, including multi-class classification, named entity recognition, and natural language parsing...|$|E
40|$|With growing data volumes from {{synoptic}} surveys, astronomers {{must become}} more abstracted from the discovery and introspection processes. Given {{the scarcity of}} follow-up resources, there is a particularly sharp onus on the frameworks that replace these human roles to provide accurate and well-calibrated probabilistic classification catalogs. Such catalogs inform the subsequent follow-up, allowing consumers to optimize the selection of specific sources for further study and permitting rigorous treatment of purities and efficiencies for population studies. Here, we describe a process to produce a probabilistic classification catalog of variability with machine learning from a multi-epoch photometric survey. In addition to producing accurate classifications, we show how to estimate calibrated class probabilities, and motivate the importance of probability calibration. We also introduce a methodology for feature-based anomaly detection, which allows discovery of objects in the survey that do not fit within the predefined <b>class</b> <b>taxonomy.</b> Finally, we apply these methods to sources observed by the All Sky Automated Survey (ASAS), and unveil the Machine-learned ASAS Classification Catalog (MACC), which is a 28 -class probabilistic classification catalog of 50, 124 ASAS sources. We estimate that MACC achieves a sub- 20 % classification error rate, and demonstrate that the class posterior probabilities are reasonably calibrated. MACC classifications compare favorably to the classifications of several previous domain-specific ASAS papers and to the ASAS Catalog of Variable Stars, which had classified only 24 % of those sources into one of 12 science classes. The MACC is publicly available at [URL] 56 pages, 15 figures, 8 tables, submitted. The Machine-learned ASAS Classification Catalog is available at [URL]...|$|E
40|$|With {{the coming}} data deluge from {{synoptic}} surveys, {{there is a}} growing need for frameworks that can quickly and automatically produce calibrated classification probabilities for newly-observed variables based on a small number of time-series measurements. In this paper, we introduce a methodology for variable-star classification, drawing from modern machine-learning techniques. We describe how to homogenize the information gleaned from light curves by selection and computation of real-numbered metrics ("feature"), detail methods to robustly estimate periodic light-curve features, introduce tree-ensemble methods for accurate variable star classification, and show how to rigorously evaluate the classification results using cross validation. On a 25 -class data set of 1542 well-studied variable stars, we achieve a 22. 8 % overall classification error using the random forest classifier; this represents a 24 % improvement over the best previous classifier on these data. This methodology is effective for identifying samples of specific science classes: for pulsational variables used in Milky Way tomography we obtain a discovery efficiency of 98. 2 % and for eclipsing systems we find an efficiency of 99. 1 %, both at 95 % purity. We show that the random forest (RF) classifier is superior to other machine-learned methods in terms of accuracy, speed, and relative immunity to features with no useful class information; the RF classifier {{can also be used to}} estimate the importance of each feature in classification. Additionally, we present the first astronomical use of hierarchical classification methods to incorporate a known <b>class</b> <b>taxonomy</b> in the classifier, which further reduces the catastrophic error rate to 7. 8 %. Excluding low-amplitude sources, our overall error rate improves to 14 %, with a catastrophic error rate of 3. 5 %. Comment: 23 pages, 9 figure...|$|E
40|$|Motivation: Assigning {{functions}} for unknown genes based on diverse large-scale data {{is a key}} task in functional genomics. Previous work on gene function prediction has addressed this problem using independent classifiers for each function. However, such an approach ignores the structure of functional <b>class</b> <b>taxonomies</b> such as the Gene Ontology. Over a hierarchy of functional classes, a group of independent classifiers where each one predicts gene membership to a particular class can produce a hierarchically inconsistent set of predictions, where for a given gene a specific class may be predicted positive while its inclusive parent class is predicted negative. Taking the hierarchical structure into account resolves such inconsistencies, and {{provides an opportunity for}} leveraging all classifiers in the hierarchy to achieve higher specificity of predictions. Results: We developed a Bayesian framework for combining multiple classifiers based on the functional taxonomy constraints. Using a hierarchy of support vector machine (SVM) classifiers trained on multiple data types, we combined predictions in our Bayesian framework to obtain the most probable consistent set of predictions. Experiments show that over a 105 -node subhierarchy of the Gene Ontology, our Bayesian framework improves predictions for 93 nodes. As an additional benefit, our method also provides implicit calibration of SVM margin outputs to probabilities. Using this method, we make function predictions for multiple proteins, and experimentally confirm predictions for proteins involved in mitosis. Supplementary information: Results for the 105 selected GO classes and predictions for 1059 unknown genes are available at...|$|R
40|$|Abstract. Many of {{the current}} {{automatic}} dierentiation (AD) tools have similar characteristics. Unfortunately, it {{is often the case}} that the similarities between these various AD tools can not be easily ascertained by reading the corresponding documentation. To clarify this situation, a taxonomy of AD tools is presented. The taxonomy places AD tools into the Elemental, Extensional, Integral, Operational and Symbolic <b>classes.</b> This <b>taxonomy</b> is used to classify twenty-nine AD tools. Each tool is examined individually with respect to the mode of dierentiation used and the degree of derivatives computed. A list detailing the availability of the surveyed AD tools is provided in Appendix A...|$|R
40|$|In {{this paper}} we {{describe}} a <b>taxonomy</b> of object-oriented <b>classes</b> that catalogs each class in an application {{according to the}} characteristics of that class, including {{the properties of the}} data attributes and routines as well as the relationships with other <b>classes.</b> Our <b>taxonomy</b> is motivated {{by the fact that the}} current research literature contains no formal methodology for capturing the characteristics of a class. To illustrate the advantages of the taxonomy, we apply it to the problem of choosing implementation-based testing techniques and, more importantly, we show that our taxonomy can expose characteristics of a class that remain uncovered by the chosen testing technique. ...|$|R
