10|340|Public
5000|$|Contrary to {{the purely}} spatial {{computing}} model of FPGA, a reconfigurable computing platform that employs a temporal computing model (or {{a combination of}} both temporal and spatial) has also been investigated [...] in the context of improving performance and energy over conventional FPGA. These platforms, referred as Memory Based Computing (MBC), use dense two-dimensional memory array to store the LUTs. Such frameworks rely on breaking a complex function (f) into small sub-functions; representing the sub-functions as multi-input, multi-output LUTs in the memory array; and evaluating the function f over multiple cycles. MBC can leverage on the high density, low power and high performance advantages of nanoscale memory. [...] shows the high-level block diagram of MBC. Each computing element incorporates a two-dimensional memory array for storing LUTs, a small controller for sequencing evaluation of sub-functions and a set of temporary registers to hold the intermediate outputs from individual partitions. A fast, local routing framework inside each <b>computing</b> <b>block</b> generates the address for LUT access. Multiple such computing elements can be spatially connected using FPGA-like programmable interconnect architecture to enable mapping of large functions. The local time-multiplexed execution inside the computing elements can drastically reduce the requirement of programmable interconnects leading to large improvement in energy-delay product and better scalability of performance across technology generations. The memory array inside each computing element can be realized by Content-addressable memory (CAM) to drastically reduce the memory requirement for certain applications.|$|E
40|$|Tyt. z nagłówka. Bibliogr. s. 473 - 474. The paper {{presents}} the results of investigations concerning the possibility of using programm able logic devices (FPGA) to build virtual multi-core processors dedicated specifically towards particular applications. The paper shows the designed architecture of a multi-core processor specialized to perform a particular task, and it discusses its computational efficiency depending on the number of cores used. An evaluation of the results is also discussed. Dostępny również w formie drukowanej. KEYWORDS: microprocessor, FPGA, parallel <b>computing,</b> <b>block</b> cipher...|$|E
40|$|Abstract The paper {{deals with}} the vector control systems of the {{induction}} motor supplied from the failed and non-failed tandem converter. There are described the research results regarding the reconfiguration aspects of the control strategy for {{the transition from a}} control structure to another. Simulation results are presented for both control system topologies. Implementation of the general <b>computing</b> <b>block</b> in the Configurable Logic Cells ® of the Triscend’s Configurable System on Chip ® is presented together with the results of the path delay analyses. I...|$|E
40|$|One of {{the unique}} {{features}} of the REPORT procedure is the <b>Compute</b> <b>Block.</b> This PROC step tool allows the use of most DATA step statements, logic, and functions, and {{through the use of}} the <b>compute</b> <b>block</b> you can modify existing columns, create new columns, write text, and more! This provides the SAS programmer a level of control and flexibility that is unavailable in virtually all other procedures. Along with this flexibility comes complexity and this complexity often thwarts us as we try to write increasingly interesting <b>compute</b> <b>blocks.</b> The complexity of the <b>compute</b> <b>block</b> includes a number of column identification and timing issues that can confound the PROC REPORT user. Of course to make matters even more interesting, there can be multiple <b>compute</b> <b>blocks</b> that can interact with each other and these can execute for different portions of the report table. This tutorial will discuss the essential elements of the <b>compute</b> <b>block,</b> its relationship to the processing phases, and how it interacts with temporary variables and other <b>compute</b> <b>blocks.</b> We will discuss timing issues and naming conventions through a series of examples...|$|R
40|$|One of {{the unique}} {{features}} of the REPORT procedure is the <b>Compute</b> <b>Block.</b> Unlike most other SAS procedures, PROC REPORT {{has the ability to}} modify values within a column, to insert lines of text into the report, to create columns, and to control the content of a column. Through <b>compute</b> <b>blocks</b> it is possible to use a number of SAS language elements, many of which can otherwise only be used in the DATA step. While powerful, the <b>compute</b> <b>block</b> can also be complex and potentially confusing. This tutorial introduces basic <b>compute</b> <b>block</b> concepts, statements, and usages. It discusses a few of the issues that tend to cause folks consternation when firs...|$|R
40|$|In the <b>compute</b> <b>block</b> of the PROC REPORT step {{variables}} {{can be both}} {{created and}} used. They {{may be on the}} incoming data table, they may be computed in the REPORT step, they can be statistical summaries, and they may even be artificially created by the REPORT process. In the DATA step each variable has a name and that variable’s name is used whenever we want to address a particular value on the Program Data Vector. In the REPORT step things are not so simple. The origins and use of a variable will determine how it is to be named in a <b>compute</b> <b>block.</b> If you {{do not know how to}} properly address a variable or report item in the <b>compute</b> <b>block,</b> the <b>compute</b> <b>block</b> will often fail. This paper will discuss the various variable types, how they are used, and how they are addressed in the <b>compute</b> <b>block.</b> Once introduced the rules are fairly easy, and a full understanding of the naming conventions and how they are applied- “use what name when”, will go a long way to allowing you to create more complex and successful <b>compute</b> <b>blocks</b> in PROC REPORT...|$|R
30|$|The {{effect of}} {{different}} inter-view prediction directions on the coding efficiency of mixed spatial-resolution stereoscopic video coding is discussed. At low bitrates, mixed spatial-resolution stereoscopic video coding provides superior coding efficiency, when using full spatial-resolution frames rather than low spatial-resolution frames {{in the base}} view. This implies that full spatial-resolution and low spatial-resolution frames should use different reference frame selection and reference frame ordering processes. A comparison of different decimation and interpolation methods showed that the high-performance methods {{reduce the amount of}} time needed for both processes through filtering fewer samples than the conventional methods. The high-performance methods for decimation and interpolation were therefore used when <b>computing</b> <b>block</b> matching statistics and in the comparisons reported in Section 5.|$|E
40|$|A {{technique}} is presented here for directional gesture recognition by robots. The usual technique employed now is using camera vision and image processing. One major disadvantage {{with that is}} the environmental constrain. The machine vision system {{has a lot of}} lighting constrains. It is therefore only possible to use that technique in a conditioned environment, where the lighting is compatible with camera system used. The technique presented here is designed to work in any environment. It does not employ machine vision. It utilizes a set of sensors fixed on the hands of a human to identify the direction in which the hand is pointing. This technique uses cylindrical coordinate system to precisely find the direction. A programmed <b>computing</b> <b>block</b> in the robot identifies the direction accurately within the given range...|$|E
40|$|This paper {{describes}} from a {{new perspective}} the inverse-free spectral division methods for block generalized Schur decompositions and presents a more efficient, accurate and stable algorithm. Even in the case that only a right deflating subspace of a matrix pencil is of interest, as in many engineering application problems, the new algorithm can be used, with a low extra cost, to obtain posterior estimates on the backward accuracy of a computed orthonormal basis for the deflating subspace. The idea behind the new algorithm can be straightforwardly applied to non-inverse-free versions of spectral division algorithms. 1 Introduction In this paper we discuss and present modifications on spectral division algorithms for <b>computing</b> <b>block</b> Schur decompositions [12, 26] of a matrix pencil (A; B), where A and B are square matrices of order n. In particular, {{we are interested in}} computing two unitary matrices Q = (Q 1; Q 2) and Z = (Z 1; Z 2) such that Q h AZ = / A 11 A 12 0 A 22 ! [...] ...|$|E
30|$|The first {{implementation}} strategy {{is about the}} allocation of the GPU computing resources. According to the GPU compute unified device architecture (CUDA) programming principles, the GPU CUDA programming model consists of three programming hierarchy levels, i.e., GPU compute grid, GPU <b>compute</b> <b>block,</b> and GPU <b>compute</b> thread. At the top level of the programming hierarchy, all the algorithm computations are executed within one GPU compute grid. Meanwhile, at the second level of the programming hierarchy, the program tasks are allocated into a set of GPU <b>compute</b> <b>blocks.</b> The computation in different GPU <b>compute</b> <b>blocks</b> can be executed in parallel. Besides, at the third level of the programming hierarchy, the computational workloads are assigned {{to a series of}} GPU compute threads. The programs in different GPU compute threads are executed simultaneously, while the program instructions within one GPU compute thread are executed sequentially.|$|R
40|$|The authors {{present a}} novel {{architecture}} for implementing general-purpose fuzzy chips which allows fully-parallel rule processing employing a reduced number of mixed-signal <b>computing</b> <b>blocks</b> and minimum-sized digital memories. The resulting fuzzy processor can interact directly with continuous sensors and actuators {{and the subsequent}} digital processing system...|$|R
5000|$|If {{a shorter}} process arrives during another process' execution, the {{currently}} running process is interrupted (known as preemption), dividing that process into two separate <b>computing</b> <b>blocks.</b> This creates excess overhead through additional context switching. The scheduler must also place each incoming process into a specific {{place in the}} queue, creating additional overhead.|$|R
40|$|Point-in-Polygon (PIP) test is {{fundamental}} to spatial databases and GIS. Motivated by the slow response times in joining large-scale point locations with polygons using traditional spatial databases and GIS and the massively data parallel computing power of commodity GPU devices, we have designed and developed an end-to-end system completely on GPUs to associate points with the polygons that they fall within. The system includes an efficient module to generate point quadrants that have at most K points from large-scale unordered points, a simple grid-file based spatial filtering approach to associate point quadrants and polygons, and, a PIP test module to assign polygons to points in a GPU <b>computing</b> <b>block</b> using both the block and thread level parallelisms. Experiments on joining 170 million points with more than 40 thousand polygons {{have resulted in a}} runtime of 11. 165 seconds on an Nvidia Quadro 6000 GPU device. Compared with a baseline serial CPU implementation using state-of-the-art open source GIS packages which requires 15. 223 hours to complete, a speedup of 4, 910 X has been achieved. We further discuss several factors and parameters that may affect the system performance. 1...|$|E
40|$|We {{consider}} the block elimination problem Q ` A 1 A 2 ' = ` 0 ', where, given a matrix A 2 R m, A 11 2 R k, {{we try to}} find a matrix C with C T C = A T A and an orthogonal matrix Q that eliminates A 2. Sun and Bischof recently showed that any orthogonal matrix can be represented in the so-called basis-kernel representation Q = Q(Y; S) = I Γ Y ST T. Applying this framework to the block elimination problem, we show that there is considerable freedom in solving the block elimination problem and that, depending on A and C, we can find Y 2 R m, S 2 R r, where r is between rank(A 2) and k, to solve the block elimination problem. We then introduce the canonical basis Y = ` A 1 + C A 2 ' and the canonical kernel S = (A 1 + C) y C, which can be determined easily once C has been computed, and relate this view to previously suggested approaches for <b>computing</b> <b>block</b> orthogonal matrices. We also show that th [...] ...|$|E
40|$|Digital {{computation}} has penetrated {{diversity of}} {{applications such as}} audio visual communication, biomedical applications, industrial application, defense application, entertainment industries, remote sensing and control etc. Electrical systems having digital computing capability have {{become an integral part}} of daily life. This puts a thrust to design the systems with utmost portability. Portability of an electronic system not only depends on the physical size of computation blocks embedded inside it but also on the energy consumption by it. Thus, reduction of size of <b>computing</b> <b>block</b> along with the reduction of energy consumption has become prime necessity. As per ITRS (International Technology Roadmap for Semiconductors 2011, [URL] the SRAM occupies more than 70 % area of the SoC. Hence, the performance of the SRAM predominates the overall performance of the SoC. The basic operation of SRAM cells is simple and well-known. But the diversity among various applications requires different objectives to be achieved based on the field of application. Energy consumption, speed of operation, cell stability and area occupancy are the most important aspects of SRAM which need to be optimized for various applications. These parameters are commonly interdependent among each other. Thus improving one performance can potentially degrade the other one. Hence the objective which is most important for a particular application may be enhanced with compromising another performance index which may not be so critical for that particular application. In this thesis we have focused on various techniques to improve the performance of SRAM based memory...|$|E
40|$|El pdf del artículo es la versión post-print. The authors {{present a}} novel {{architecture}} for implementing general-purpose fuzzy chips which allows fully-parallel rule processing employing a reduced number of mixed-signal <b>computing</b> <b>blocks</b> and minimum-sized digital memories. The resulting fuzzy processor can interact directly with continuous sensors and actuators {{and the subsequent}} digital processing system. Peer Reviewe...|$|R
25|$|With the {{development}} of very-large-scale integration (VLSI) technology, Yannis Tsividis' group at Columbia University has been revisiting analog/hybrid computers design in standard CMOS process. Two VLSI chips have been developed, an 80th-order analog computer (250nm) by Glenn Cowan in 2005 and an 4th-order hybrid computer (65nm) developed by Ning Guo in 2015, both targeting at energy-efficient ODE/PDE applications. Glenn's chip contains 16 macros, {{in which there are}} 25 analog <b>computing</b> <b>blocks,</b> namely integrators, multipliers, fanouts, few nonlinear blocks. Ning's chip contains one macro block, in which there are 26 <b>computing</b> <b>blocks</b> including integrators, multipliers, fanouts, ADCs, SRAMs and DACs. Arbitrary nonlinear function generation is made possible by the ADC+SRAM+DAC chain, where the SRAM block stores the nonlinear function data. The experiments from the related publications revealed that VLSI analog/hybrid computers demonstrated about 1–2 orders magnitude of advantage in both solution time and energy while achieving accuracy within 5%, which points to the promise of using analog/hybrid computing techniques in the area of energy-efficient approximate computing.|$|R
30|$|The {{computing}} speed {{evaluation of}} the MV adaptive beamforming algorithm implementation comprised (a) the giga floating-point operations per second (GFLOPS), (b) the output image frame rate of the algorithm implementation, and (c) the computational speedup of the embedded GPU implementation over its ARM processor counterpart. The algorithm computing speed evaluation was conducted on Jetson TX 1 heterogeneous embedded development board. The evaluation test cases included various M and L combinations. While doing the evaluation tests, the value of M was first determined, which was chosen as 16, 32, 64, and 128, respectively. Then, the value of L was chosen from 1 to M/ 2. Therefore, there were 8 M and L combinations when M is 16, 16 M and L combinations when M is 32, 32 M and L combinations when M is 64, and 64 M and L combinations when M is 128. In the experiments, {{as the number of}} image pixels, 127 × 1000 pixels for one image, did not change during the tests, the number of the GPU <b>compute</b> <b>blocks</b> were not changed in the experiments. But the number of the GPU compute threads, a.k.a. GPU <b>compute</b> thread <b>block</b> size, was varying during the experiments. The algorithm computing speed of various GPU <b>compute</b> thread <b>block</b> size was tested so as to find the value of the GPU <b>compute</b> thread <b>block</b> size in the best practices. The GPU <b>compute</b> thread <b>block</b> size test cases covered the numbers from 32 to 1024 which were multiples of 32.|$|R
40|$|This thesis {{deals with}} {{spectral}} analysis. A {{number of new}} techniques are proposed which improve the computational efficiency of discrete orthogonal transform algorithms, and the accuracy of spectral analysis with applications to harmonic signal analysis. The techniques for computing discrete orthogonal transforms using adaptive filtering are systematically investigated. Firsdy, a general relationship between orthogonal transforms and adaptive filtering is established, which sets the foundation for these techniques. Secondly, the issue of computing block-based discrete orthogonal transforms using the adaptive Least Mean Square (LMS) algorithm is examined. Sufficient conditions for implementing block LMS-based discrete orthogonal transforms are proposed, {{and the performance of}} the techniques for <b>computing</b> <b>block</b> LMS-based discrete Walsh transforms and discrete cosine transforms is analysed. Thirdly, the thesis proposes LMS-based techniques for computing running orthogonal transforms, including running discrete Hartley transform, running discrete Cosine and Sine transforms, as well as running discrete W transforms. Finally, the thesis examines the possibility of orthogonal analysis using other adaptive processing algorithms. It is shown that the sample matrix inversion (SMI) algorithm can be used to compute all the discrete orthogonal transforms, while the adaptive Howells-Applebaum loop can be used to implement a spectral analyser for continuous signals. The running computation of discrete orthogonal transforms based on their shift properties is studied in detail. A number of discrete orthogonal transforms, including the discrete Hartley transform, the discrete cosine and sine transforms, and the discrete W transforms are considered. The shift properties of these transforms are developed, which are in effect recursive equations that connect the previous and updated transform coefficients. Both the first order shift properties and the second order shift properties are proposed for these transforms. As expected the first order shift properties are in the form of first order difference equations. These first order difference equations involve two transform coefficients of different transforms (for example, a discrete cosine transform and its corresponding discrete sine transform). This is a source of extra computational burden. For some transforms such as the discrete Hartley transform and the discrete W transform, this extra computational burden can be eliminated by using the reverse symmetrical properties of the transform coefficients. However, for the discrete cosine and sine transforms, the computation associated with the first order shift properties is not very efficient. The second order shift properties (that is, second order difference equations) are proposed, which can independently update transform coefficients thus reducing the computational burden. It is shown that for the discrete cosine and sine transforms, the computational burden associated with second order shift properties can be considerably reduced in comparison to the first order shift properties. A time domain interpolation pre-processing algorithm is proposed in an effort to reduce leakage effects associated with the DFT analysis of periodic signals. The leakage effect refers to the spreading of energy from one frequency bin into adjacent ones. To avoid leakage, the sampling frequency should be an integer multiple of the signal frequency. The basic idea of the proposed time domain interpolation pre-processing algorithm is to modify the actual samples towards an ideal sample sequence whose sampling frequency is an integer multiple of the signal frequency. The algorithm is based on a first order approximation of the Taylor 2 ̆ 7 s series. It is shown that the proposed algorithm can reduce quite significantly both the DFT leakage and the truncation error associated with digital wattmeter power measurement. Finally, frequency estimation techniques based on adaptive IIR notch filtering are considered. In order to improve the steady state error performance of existing techniques, a block-gradient based adaptive algorithm is proposed for adaptive IIR filtering. The input signal sequence is arranged into data blocks and the filter coefficients are kept constant within each block. The gradients are evaluated for each complete block of data and the filter coefficients are updated on a block by block basis. Application of the proposed block gradient algorithm to the problem of sinusoidal frequency estimation is studied. It reveals that the proposed algorithm is characterised by a lower steady state error as well as reduced computational complexity...|$|E
5000|$|HP Cloud Monitoring {{delivers}} fundamental <b>compute</b> and <b>block</b> storage metrics, providing visibility into resource utilization, application performance, {{and operational}} health.|$|R
40|$|Radio {{astronomy}} observatories {{with high}} throughput back end instruments require real-time data processing. While computing hardware continues to advance rapidly, development of real-time processing pipelines remains difficult and time-consuming, which can limit scientific productivity. Motivated by this, {{we have developed}} Bifrost: an open-source software framework for rapid pipeline development. Bifrost combines a high-level Python interface with highly efficient reconfigurable data transport and a library of <b>computing</b> <b>blocks</b> for CPU and GPU processing. The framework is generalizable, but initially it emphasizes the needs of high-throughput radio astronomy pipelines, {{such as the ability}} to process data buffers as if they were continuous streams, the capacity to partition processing into distinct data sequences (e. g., separate observations), and the ability to extract specific intervals from buffered data. <b>Computing</b> <b>blocks</b> in the library are designed for applications such as interferometry, pulsar dedispersion and timing, and transient search pipelines. We describe the design and implementation of the Bifrost framework and demonstrate its use as the backbone in the correlation and beamforming back end of the Long Wavelength Array station in the Sevilleta National Wildlife Refuge, NM. Comment: 25 pages, 13 figures, submitted to JAI. For the code, see [URL]...|$|R
40|$|Relay {{deployment}} in Orthogonal Frequency Division Multiple Access (OFDMA) based cellular networks {{helps in}} coverage extension and or capacity improvement. In OFDMA system, each user requires different number of subcarriers {{to meet its}} rate requirement. This resource requirement depends on the Signal to Interference Ratio (SIR) experienced by a user. Traditional methods to <b>compute</b> <b>blocking</b> probability cannot be used in relay based cellular OFDMA networks. In this paper, we present an approach to <b>compute</b> the <b>blocking</b> probability of such networks. We determine {{an expression of the}} probability distribution of the users resource requirement based on its experienced SIR and then classify the users into various classes depending upon their subcarrier requirement. We consider the system to be a multidimensional system with different classes and evaluate the blocking probability of system using the multi-dimensional Erlang loss formulas. Comment: 33 pages, 10 figure...|$|R
40|$|Recent {{years have}} seen the {{evolution}} of networks of tiny low power <b>computing</b> <b>blocks,</b> known as sensor networks. In one class of sensor networks a non-expert user, who {{has little or no}} experience with electronics or programming, is required to select, connect and/or configure one or more blocks such that the <b>blocks</b> <b>compute</b> a particular Boolean logic function of sensor values. We describe a series of experiments showing that non-expert users have much difficulty with a block based on Boolean logic truth tables, and that a logic block having a sentence-like structure with some configurable switches yields a better success rate. We also show that adding color to a truth table improves results over a traditional truth table. Author Keywords Sensor networks, Boolean logic, embedded computin...|$|R
40|$|Golub and Meurant {{have shown}} {{how to use}} the {{symmetric}} block Lanczos algorithm to <b>compute</b> <b>block</b> Gauss quadrature rules for the approximation of certain matrix functions. We describe new block quadrature rules that can be computed by the symmetric or nonsymmetric block Lanczos algorithms and yield higher accuracy than standard block Gauss rules after the same number of steps of the symmetric or nonsymmetric block Lanczos algorithms. The new rules are block generalizations of the generalized averaged Gauss rules introduced by Spalevi´c. Applications to network analysis are presente...|$|R
30|$|The {{memory access}} {{strategy}} of GPU implementation {{has an important}} impact on the overall GPU computational speed. There are basically three types of memory modules inside the GPU, i.e., global memory, shared memory, and register files. The lifetime of the data in the three memory types is associated to the GPU computing resources respectively. The lifetime of the data in the global memory is associated to the GPU compute grid, the lifetime of the data in the shared memory is associated to the GPU <b>compute</b> <b>block,</b> and the lifetime of the data in the register files is associated to the GPU compute thread.|$|R
40|$|Quality of Service (QoS) {{optimisation}} of converged networks {{has become}} an absolute necessity for mobile communication businesses to deliver business differentiation in a saturated market. In this paper, we present a resource allocation scheme for 3 G wireless systems that optimises QoS control by exploiting multi-component nature of multimedia applications. We build a Markovian model of our Multi-Component Resource Allocation Scheme (MCRAS) and analyse its performance by computing steady-state probability distributions. We <b>compute</b> <b>blocking</b> probability – the key QoS parameter – for different system configurations using MCRAS and make comparisons with Complete Partitioning (CP) and Complete Sharing (CSH) resource allocation schemes...|$|R
40|$|In this work, a Euclidean {{distance}} calculator is presented. The circuit comprises {{of simple}} <b>computing</b> <b>blocks,</b> their basic element being the floating gate MOSFET (FGMOS), exploiting {{the merits of}} this device in designing circuits with low-voltage and rail-to-rail operation. Therefore the overall circuit has the characteristics of modularity, low-voltage and rail-to-rail operation under a single supply voltage, accuracy and simplicity. The circuitis des#it 8 with 2 MIETEC CMOS technology and {{is used in the}} simulation of a hand-written digit recognition system using the nearest neighbour classification method. The simulation results presented, demonstrate the functionality of the circuit...|$|R
40|$|A fast control wrapper for a micropipeline with {{two-phase}} {{control is}} presented. The wrapper is implemented in an Artisan 0. 13 µ commercial standard library {{that has not}} been augmented with any special cells for asynchronous design. The wrapper is approximately 25 % faster than a more traditional approach that uses a Muller C-element. Introduction: Micropipelines [1] use control logic wrapped around <b>compute</b> <b>blocks</b> to implement asynchronous systems. Micropipelines have been used to implement significant designs, including complex microprocessors [2]. Four-phase control [3] means that the control lines between micropipeline stages undergo a low-to-high-to-low transition for each data movement between stages; while two-phase control implies either a single low-to-high or high-lo...|$|R
40|$|Hierarchical {{temporal}} memory (HTM) {{tries to}} mimic the computing in cerebral-neocortex. It identifies spatial and temporal patterns in the input for making inferences. This may require large number of computationally expensive tasks like, dot-product evaluations. Nano-devices that can provide direct mapping for such primitives are of great interest. In this work we show that the <b>computing</b> <b>blocks</b> for HTM can be mapped using low-voltage, fast-switching, magneto-metallic spin-neurons combined with emerging resistive cross-bar network (RCN). Results show possibility of more than 200 x lower energy as compared to 45 nm CMOS ASIC designComment: this work was submitted to IEEE Transactions on Neural Networks and Learning Systems. It is under review no...|$|R
40|$|Graver {{test sets}} for linear {{two-stage}} stochastic integer programs are studied. It is shown that test sets can be decomposed into finitely many building blocks whose number {{is independent of}} the number of scenarios of the sochastic program. A finite algorithm to <b>compute</b> the building <b>blocks</b> directly, without prior knowledge of test set vectors, is presented. Once <b>computed,</b> building <b>blocks</b> can be employed to solve the stochastic program by a simple augmentation scheme, again without explicit knowledge of test set vectors. Preliminary computational experience is reported...|$|R
40|$|We study Graver {{test sets}} for linear {{two-stage}} stochastic integer programs {{and show that}} test sets can be decomposed into finitely many building blocks whose number is independent {{on the number of}} scenarios of the stochastic program. We present a finite algorithm to <b>compute</b> the building <b>blocks</b> directly, without prior knowledge of test set vectors. Once <b>computed,</b> building <b>blocks</b> can be employed to solve the stochastic program by a simple augmentation scheme, again without explicit knowledge of test set vectors. Finally, we report preliminary computational experience...|$|R
40|$|A {{two-phase}} control wrapper for a micropipeline is presented. The wrapper {{is implemented}} in an Artisan 0. 13 µ standard cell library {{that has not}} been augmented with any special cells for asynchronous design. The wrapper supports early evaluation allowing the output to be updated after a subset of the inputs have arrived, thus improving the throughput of the micropipeline. Introduction: Micropipelines [1] use control logic wrapped around <b>compute</b> <b>blocks</b> to implement asynchronous systems. Micropipelines have been used to implement significant designs, including complex microprocessors [2]. Four-phase control [3] means that the control lines between micropipeline stages undergo a low-to-high-to-low transition for each data movement between stages; while two-phase control implies either a single low-to-high or high-lo...|$|R
40|$|Abstract — Many DSP, {{image and}} video {{processing}} applications use Finite Impulse Response (FIR) filters as basic <b>computing</b> <b>blocks.</b> Our paper introduces an efficient dynamically reconfigurable FIR {{system that can}} adapt the number of filter coefficients, and their values, in real time. Here, dynamic reconfiguration is used to switch between different, pre-computed, fixed-point realizations of different digital filters. Our platform relies {{on the use of}} Distributed Arithmetic blocks, mapped to the specific LUTs of the underlying FPGA. Dynamic reconfiguration of the coefficients is limited to changing a small number of relevant LUT contents, while leaving the rest of the architecture intact. We investigate the dynamic system throughput {{as a function of the}} dynamic reconfiguration rate...|$|R
40|$|International audienceWe analyze {{transient}} and stationary {{behaviors of}} multidimensional Markov chains defined on large state spaces. In this paper, we apply stochastic comparisons on partially ordered state {{which could be}} very interesting for performance evaluation of computer networks. We propose an algorithm for bounding aggregations in order to derive upper and lower performance measure bounds on a reduced state space. We study different queueing networks with rejection in order to <b>compute</b> <b>blocking</b> probability and end to end mean delay bounds. Parametric aggregation schemes are studied in order to propose an attractive solution: given a performance measure threshold, we vary the parameter values to obtain a trade-off between the accuracy of bounds and the computation complexity...|$|R
40|$|Abstract—There exist many {{recurrent}} {{neural networks}} for solving optimization-related problems. In this paper, {{we present a}} method for deriving such networks from existing ones by changing connections be-tween <b>computing</b> <b>blocks.</b> Although the dynamic systems may become much different, some distinguished properties may be retained. One example is discussed to solve variational inequalities and related optimization problems with mixed linear and nonlinear constraints. A new network is obtained from two classical models by this means, and its performance is comparable to its predecessors. Thus, an alternative choice for circuits implementation is offered to accomplish such computing tasks. Index Terms—Asymptotic stability, global convergence, linear program-ming (LP), optimization, quadratic programming (QP), recurrent neural network (RNN), variational inequality. I...|$|R
40|$|AbstractA {{polynomial}} time algorithm is presented for the founding question of Galois theory: determining solvability by radicals of a monic irreducible polynomial over the integers. Also a {{polynomial time}} algorithm which expresses a root in radicals {{in terms of a}} straightline program is given. Polynomial time algorithms are demonstrated for <b>computing</b> <b>blocks</b> of imprimitivity of roots of the polynomial under the action of the Galois group, and for computing intersections of algebraic number fields. In all of the algorithms it is assumed that the number field is given by a primitive element which generates it over the rationals, that the polynomial in question is monic, and that its coefficients are in the integers...|$|R
