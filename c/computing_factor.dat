7|1269|Public
50|$|Factor scores (also called {{component}} {{scores in}} PCA): are {{the scores of}} each case (row) on each factor (column). To compute the factor score for a given case for a given factor, one takes the case's standardized score on each variable, multiplies by the corresponding loadings of the variable for the given factor, and sums these products. <b>Computing</b> <b>factor</b> scores allows one to look for factor outliers. Also, factor scores {{may be used as}} variables in subsequent modeling. (Explained from PCA not from Factor Analysis perspective).|$|E
30|$|Using IBM SPSS Statistics 19.0, twenty-six {{observations}} with twelve {{factors were}} analyzed for data dimensionality reduction. Average values of 15 replicates for each observation were reserved for further analysis. In factor analysis, principal component method was used for data extraction, varimax method for rotation and regression method for <b>computing</b> <b>factor</b> scores. Factor scores for 26 observations were then utilized in the cluster analysis. Hierarchical cluster analysis was performed using between-group linkage method, and data were standardized by Z scores method to compute the squared Euclid distances.|$|E
40|$|Principal Component Analysis (PCA {{from now}} on) is a multivariate data {{analysis}} technique used for many different purposes {{and in many}} different contexts. PCA {{is the basis for}} low rank least squares approximation of a data matrix, for finding linear combinations with maximum or minimum variance, for fitting bilinear biplot models, for <b>computing</b> <b>factor</b> analysis approximations, and for studying regression with errors in variables. It is closely related to simple correspondence analysis (CA) and multiple corre- spondence analysis (MCA), which are discussed in Chapters XX and YY of this book. PCA is used wherever large and complicated multivariate data sets have to be reduced to a simpler form. We find PCA in microarray analysis, medical imaging, educational and psychological testing, survey analysis, large scale time series analysis, atmospheric sciences, high-energy physics, astronomy, and so on. Jolliffe [2002] is a comprehensive overview of the theory and applications of classical PCA...|$|E
3000|$|... {{respectively}} {{correspond to}} the unnormalized probability and the softmax-normalizing <b>factor.</b> <b>Computing</b> this <b>factor</b> z [...]...|$|R
3000|$|... {{value is}} set to 0 for the first cycle of {{bandwidth}} granting process. FPBWforecast indicates the <b>computed</b> <b>factor</b> point of BW forecast submodule while maxFP is the maximum factor point with 0.25 as its value.|$|R
30|$|If set I is empty, CQI {{prioritization}} is not required. Then, F_k^QCI_m= 1 ∀ k, and {{the algorithm}} is finished. If set I is not empty, CQI prioritization is triggered. Then, the algorithm starts to <b>compute</b> <b>factor</b> F_k^QCI_m for k = 1.|$|R
40|$|Determination {{of overall}} {{factor of safety}} of a design {{involves}} repeated calculation of factor of safety at critical points in the design. For a given stress state at a point, factor of safety is calculated by first finding the principal stresses and then comparing them with the maximum safe stress that can applied without causing failure of the material according to an appropriate failure theory. In this paper, we suggest quick and ready-to-use expressions and graphs for calculating factor of safety for biaxial stress states {{for a number of}} failure theories. These graphs can be directly used as design charts for <b>computing</b> <b>factor</b> of safety in engineering design activities. 1 Introduction In various mechanical design activities involving back-of-the-envelope estimates or using precise sophisticated computers, the overall factor of safety of the design is usually one of the primary concerns. Since factor of safety is computed at many critical points and are compared with each other to fi [...] ...|$|E
40|$|The {{objective}} {{of this paper is}} to study factors that affect potential online sales of specific product. In short, this study investigates consumer online behaviour, potential of online sales to identify those factors that influence online potential sales of a specific product by developing an instrument for investigating and understanding consumer's online apparel shopping behaviour of undergraduate student and staff at the University Malaysia Kelantan (UMK) located in the state of Kelantan, Malaysia. Customer's attitude towards online shopping is the key to the survival and profitability of internet retailers in the intensely competitive market. A five-level Likert scale ranging from one (strongly disagree) to five (strongly agree) was used to generate result for this research, and to make us understand more about online shopping, in order to determine attitudes towards online shopping. A self-administrated questionnaire, based on shopping orientation, perceived benefits, and attitude was developed. The sample population of this study consisted of fifty undergraduate students and fifty staff of University Malaysia Kelantan who were selected by using random sampling method and involved in the research study. The questionnaire consisted of four components. Out of the four components, one component referred to demographic question and three components referred to online shopping orientation, online shopping perceived benefits and attitude towards online shopping. The reliability of data and scale was tested by <b>computing</b> <b>factor</b> analysis, descriptive analysis and a reliability test. This instrument is offered to the research community as a tool that may be used in conducting future research related to online shopping behaviour...|$|E
40|$|Energy {{efficient}} infrastructures {{or green}} IT (Information Technology) has recently become a hot button issue for most corporations as they strive to eliminate every ineﬃciency from their enterprise IT systems and save capital and operational costs. Vendors of IT equipment now compete {{on the power}} eﬃciency of their devices, and as a result, {{many of the new}} equipment models are indeed more energy eﬃcient. Various studies have estimated the annual electricity consumed by networking devices in the U. S. in the range of 6 - 20 Terra Watt hours. Our research has the potential to make promising solutions solve those overuses of electricity. An energy-efficient data center network architecture which can lower the energy consumption is highly desirable. First of all, we propose a fair bandwidth allocation algorithm which adopts the max-min fairness principle to decrease power consumption on packet switch fabric interconnects. Specifically, we include power aware <b>computing</b> <b>factor</b> as high power dissipation in switches which is fast turning into a key problem, owing to increasing line speeds and decreasing chip sizes. This efficient algorithm could not only reduce the convergence iterations but also lower processing power utilization on switch fabric interconnects. Secondly, we study the deployment strategy of multicast switches in hybrid mode in energy-aware data center network: a case of famous Fat-tree topology. The objective is to ﬁnd the best location to deploy multicast switch not only to achieve optimal bandwidth utilization but also minimize power consumption. We show {{that it is possible to}} easily achieve nearly 50 % of energy consumption after applying our proposed algorithm. Finally, although there exists a number of energy optimization solutions for DCNs, they consider only either the hosts or network, but not both. We propose a joint optimization scheme that simultaneously optimizes virtual machine (VM) placement and network ﬂow routing to maximize energy savings. The simulation results fully demonstrate that our design outperforms existing host- or network-only optimization solutions, and well approximates the ideal but NP-complete linear program. To sum up, this study could be crucial for guiding future eco-friendly data center network that deploy our algorithm on four major layers (with reference to OSI seven layers) which are physical, data link, network and application layer to benefit power consumption in green data center...|$|E
50|$|The Real gas article {{features}} more theoretical methods to <b>compute</b> compressibility <b>factors.</b>|$|R
40|$|In {{this work}} we {{introduce}} a new method for <b>computing</b> Form <b>Factors</b> in Radiosity. We demostrate how our method improves on existing projective techniques such as the hemicube. We use the Nusselt analog to directly <b>compute</b> form <b>factors</b> by projecting the scene onto the unit circle. We compare our method with other form factor computation methods. The results show an improvement in the quality/speed ratio using our technique...|$|R
40|$|The {{clinical}} {{management of}} bacterial infection <b>computes</b> <b>factors</b> related to bacteria (e. g., resistance profile [1]), antibiotics (e. g., activity spectrum, distribution volume, etc.), host characteristics (e. g., vascularization of the infected tissue, effectiveness of host defenses, etc.), {{as well as}} pharmacokinetic (PK) parameters [2]. The serum inhibitory (SIT) and bactericidal titers (SBT) are labora-tory tests that simulate the interactions between antibio-tics and bacteria {{in the human body}} milieu...|$|R
40|$|University of Minnesota PhD dissertation. July 2011. Major: Work and Human Resource Education. Advisor: Gary N. McLean. 1 {{computer}} file (PDF) : x, 176 pages. A {{number of studies}} have found evidence supporting a link between the organizational environment and financial performance. However, several studies have found a mixed support or no support for this link. This study builds on these findings to address the question: Is there a relationship between organizational environment factors and financial performance? Organizational environment data for this study came from employees of a sales and service division of a global manufacturer located in the Midwest of the U. S. A sample of 1, 518 respondents, from a total population of 1, 615 employees organized in 100 teams, completed a 68 -item survey instrument for a response rate of 94 %. An exploratory factor analysis generated a model with 11 subscales using 52 items from the original instrument. The subscales are (1) operational effectiveness, (2) immediate manager/supervisor, (3) senior management, (4) mission, (5) valuing employees, (6) training, (7) involvement, (8) corporate social responsibility, (9) satisfaction, (10) teamwork, and (11) inclusion. Cronbach's alpha coefficients for all subscales on the survey were acceptable, ranging from. 85 to. 90. Team-level factor scores, the predictor variables, were generated by <b>computing</b> <b>factor</b> scores for individual respondents, followed by computing a mean of each of the factor scores from members of each team. This approach produced 11 factor scores for each team. Contribution margin ratio, a measure of profitability, was the outcome variable. This variable was calculated at the team level and is the quotient created when dividing operating income by revenue. This study used contribution margin ratios from five financial periods: four consecutive fiscal quarters and the fiscal year overall. This study found that team-level employee perceptions of organizational environment factors had no to weak relationships between various organizational environment factors and various measures of financial performance. The regression analyses, subsequently, found that organizational environment factors were able to explain only single-digit percentages of variation in financial performance. Implications of these findings with regard to organizational performance are discussed...|$|E
40|$|We present {{algorithms}} that <b>compute</b> all irreducible <b>factors</b> {{of degree}} â 8 ̆ 9 ¤ d of supersparse (lacunary) multivariate polynomials in n variables over an algebraic number field in deterministic polynomial-time in (l+d) n, where l {{is the size}} of the input polynomial. In supersparse polynomials, the term degrees enter logarithmically as their numbers of binary digits into the size measure l. The factors are again represented as supersparse polynomials. If the factors are represented as straight-line programs or black box polynomials, we can achieve randomized polynomial-time in (l + d) O(1). Our approach follows that by H. W. Lenstra, Jr., on <b>computing</b> <b>factors</b> of univariate supersparse polynomials over algebraic number fields. We generalize our ISSAC 2005 results for <b>computing</b> linear <b>factors</b> of supersparse bivariate polynomials over the rational numbers by appealing to recent lower bounds on the height of algebraic numbers and to a special case of the former Lang conjecture...|$|R
3000|$|... where x {{is number}} of cycle, and f(x) denotes the {{changes of the}} {{bandwidth}} request for x cycle. BWRequestχ- 1 and BWAllocatedχ- 1 are the bandwidth request and bandwidth allocation for x- 1 cycle, respectively. However, the f(x) {{is the amount of}} bandwidth request for the first cycle of bandwidth granting process. FPBRstatus indicates the <b>computed</b> <b>factor</b> point of BR status submodule while maxFP is the maximum factor point with 0.25 as its value.|$|R
40|$|We {{propose a}} {{corrected}} plug-in method for constructing confidence intervals of the conditional quantiles of an original response variable through a transformed regression with heteroscedastic errors. The interval {{is easy to}} <b>compute.</b> <b>Factors</b> affecting {{the magnitude of the}} correction are examined analytically through the special case of Box-Cox regression. Monte Carlo simulations show that the new method works well in general and is superior over the commonly used delta method and the quantile regression method. An empirical application is presented. [PUBLICATION ABSTRACT...|$|R
5000|$|Researchers gain extra {{information}} from a PCA approach, such as an individual’s score on a certain component - such information is not yielded from factor analysis. However, as Fabrigar et al. contend, the typical aim of factor analysis - i.e. to determine the factors accounting for {{the structure of the}} correlations between measured variables - does not require knowledge of factor scores and thus this advantage is negated. It is also possible to <b>compute</b> <b>factor</b> scores from a factor analysis.|$|R
5000|$|The {{security}} of an RSA {{system would be}} compromised if the number [...] could be factored or if [...] could be <b>computed</b> without <b>factoring</b> [...]|$|R
40|$|A {{very simple}} {{structure}} is sought when using factor analysis to develop measurement scales. The present article {{is about the}} SIMLOAD program; it <b>computes</b> measures of <b>factor</b> simplicity for rows and columns of loading matrices (usually the factor pattern) {{as well as some}} overall measures. These include Kaiser’s (1974) index of factor simplicity for variables (rows), Fleming’s scale fit index for factors (columns), Bentler’s (1977) scale-free matrix measure, and hyperplane counts. Routine use of these measures is recommended for multifactor scale development. The measures may also be useful in more general factor applications, and in confirmatory as well as  exploratory analysis. SIMLOAD additionally <b>computes</b> <b>factor</b> scale intercorrelations, scale alpha coefficients, including alpha when item removed, and sorted loadings for ease of interpretation...|$|R
30|$|When {{criterion}} 1 is {{used with}} tension cutoff, the <b>computed</b> <b>factor</b> of safety by ABAQUS obviously decreases but its variation with the dilation angle is unremarkable; when criterion 2 is {{used with the}} tension cutoff, ABAQUS is unable to show the plastic penetration zone so that no values are listed in those columns in Table  1, and in this case, criterion 3 was adopted to determine the factors of safety, which actually are around 0.94 for both tensile strengths σ t =  7.11 and 0.01  kPa and for all three dilation angles.|$|R
30|$|When {{criterion}} 1 is used without tension cutoff, the <b>computed</b> <b>factors</b> {{of safety}} {{with respect to}} the three dilation angles by ABAQUS are respectively 15 %, 14 %, and 6 % greater than the exact factor of safety of 1.00; when criterion 2 is used without tension cutoff, they are respectively 8 %, 3 % greater or 2 % less than 1.00. Similar results are obtained by the UMAT. Moreover, for both ABAQUS and the UMAT, the factors of safety by criterion 1 often deviate from 1.00 farther than those of by criterion 2.|$|R
30|$|Step 2 : <b>Compute</b> the perturbed <b>factor</b> as (25).|$|R
5000|$|Manin Classical computing, quantum <b>computing</b> and Shor´s <b>factoring</b> algorithm, ...|$|R
3000|$|Next we <b>compute</b> the <b>factor</b> {{by which}} the Y current has been enhanced. The bottom panel of Fig. 4 shows {{the ratio of the}} {{secondary}} current to the primary current, j [...]...|$|R
40|$|AbstractWe derive componentwise error {{bound for}} the {{factorization}} H = GJGT, where H is a real symmetric matrix, G has full column rank, and J is diagonal with ± 1 's on the diagonal. We also derive a componentwise forward error bound, that is, we bound {{the difference between the}} exact and the <b>computed</b> <b>factor</b> G, in the cases where such a bound is possible. We extend these results to the Hermitian case, and to the well-known Bunch-Parlett factorization. Finally, we prove bounds for the scaled condition of the matrix G, and show that the factorization can have the rank-revealing property...|$|R
40|$|The band {{spectrum}} of BaO has been obtained by spraying BaCl 2 soln. into a flame. The integrated intensities {{of the bands}} have been detd. by photographic photometry. The exptl. results along with the theoretically <b>computed</b> Franck-​Condon <b>factors</b> {{have been used to}} evaluate a relation between Re, the electronic transition moment, and r, the internuclear sepn., in the form, Re(r v'v'') = const. (1 - 0. 536 r) ​. This relation has been used to obtain improved Franck-​Condon <b>factors.</b> The theoretically <b>computed</b> Franck-​Condon <b>factors,</b> with and without the inclusion of Re variation, have been compared with the exptl. band strengths...|$|R
40|$|This paper proposes Bayesian {{methods for}} {{estimating}} the cointegration rank using Bayes factors. We consider natural conjugate priors for <b>computing</b> Bayes <b>factors.</b> First, we estimate the cointegrating vectors for each possible rank. Then, we <b>compute</b> the Bayes <b>factors</b> for each rank against 0 rank. Monte Carlo simulations show that using Bayes factor with conjugate priors produces fairly good results. The methods proposed here are also applied for selecting the appropriate lags and testing for over-identifying restrictions on cointegrating vectors...|$|R
3000|$|... {{between the}} {{upstream}} and downstream locations. If both can be determined (which is quite straightforward to obtain), one could <b>compute</b> platoon dispersion <b>factors</b> α and β. These parameters subsequently {{can be used to}} <b>compute</b> the smoothing <b>factor</b> R, from which the degree of how an upstream platoon will disperse at the downstream location can be computed.|$|R
40|$|We present {{algorithms}} that {{compute the}} linear and quadratic factors of supersparse (lacunary) bivariate polynomials over the rational numbers in polynomial-time in the input size. In supersparse polynomials, the term degrees can {{have hundreds of}} digits as binary numbers. Our algorithms are Monte Carlo randomized for quadratic factors and deterministic for linear factors. Our approach relies on the results by H. W. Lenstra, Jr., on <b>computing</b> <b>factors</b> of univariate supersparse polynomials over the rational numbers. Furthermore, we show {{that the problem of}} determining the irreducibility of a supersparse bivariate polynomial over a large finite field of any characteristic is co-NP-hard via randomized reductions. Categories and Subject Descriptor...|$|R
40|$|We {{present a}} {{deterministic}} polynomial-time algorithm which <b>computes</b> the multilinear <b>factors</b> of multivariate lacunary polynomials over number fields. It {{is based on}} a new Gap theorem which allows to test whether P(X) = ∑kj= 1 ajX αj(vX+ t) β j(uX+ w) γj is identically zero in polynomial time. Previous algorithms for this task were based on Gap Theorems expressed in terms of the height of the coefficients. Our Gap Theorem is based on the valuation of the polynomial and is valid for any field of characteristic zero. As a consequence we obtain a faster and more elementary algorithm. Furthermore, we can partially extend the algorithm to other situations, such as absolute and approximate factorizations. We also give a version of our Gap Theorem valid for fields of large characteristic, and deduce a randomized polynomial-time algorithm to <b>compute</b> multilinear <b>factors</b> with at least three monomials of mul-tivariate lacunary polynomials of finite fields of large characteristic. We provide NP-hardness results to explain our inability to <b>compute</b> binomial <b>factors...</b>|$|R
40|$|Solving sparse linear {{systems of}} {{equations}} {{is a common}} and important application of a multitude of scientific and engineering applications. The SPOOLES software package 1 provides this functionality {{with a collection of}} software objects. The first step to solving a sparse linear system is to find a good low-fill ordering of the rows and columns. The library contains several ways to perform this operation: minimum degree, generalized nested dissection, and multisection. The second step is to factor the matrix as a product of triangular and diagonal matrices. The library supports pivoting for numerical stability (when required), approximation techniques to reduce the storage for and work to <b>compute</b> the matrix <b>factors,</b> and the computations are based on BLAS 3 numerical kernels to take advantage of high performance computing architectures. The third step is to solve the linear system using the <b>computed</b> <b>factors.</b> The library is written in ANSI C using object oriented design. Good design and [...] ...|$|R
50|$|In <b>computing,</b> {{the form}} <b>factor</b> is the {{specification}} of a motherboard - the dimensions, power supply type, location of mounting holes, number of ports {{on the back}} panel, etc. Specifically, in the IBM PC compatible industry, standard form factors ensure that parts are interchangeable across competing vendors and generations of technology, while in enterprise <b>computing,</b> form <b>factors</b> ensure that server modules fit into existing rackmount systems. Traditionally, the most significant specification is for that of the motherboard, which generally dictates the overall size of the case. Small form factors have been developed and implemented.|$|R
40|$|Abstract. An {{important}} {{problem in}} applications of formal concept analysis is a possibly {{large number of}} clusters extracted from data. Factorization {{is one of the}} methods being used to cope with the number of clusters. We present an algorithm for <b>computing</b> a <b>factor</b> lattice of a concept lattice from the data and a user-specified similarity threshold a. The elements of the factor lattice are collections of clusters which are pairwise similar in degree at least a. The presented algorithm <b>computes</b> the <b>factor</b> lattice directly from the data, without first computing the whole concept lattice and then computing the collections of clusters. We present theoretical insight and examples for demonstration, and an open problem...|$|R
40|$|We present new {{results on}} Boolean matrix {{factorization}} {{and a new}} algorithm based on these results. The results emphasize the significance of factorizations that provide from-below approximations of the input matrix. While the previously proposed algorithms do not consider the possibly different significance of different matrix entries, our results help measure such significance and suggest where to focus when <b>computing</b> <b>factors.</b> An experimental evaluation of the new algorithm on both synthetic and real data demonstrates its good performance in terms of good coverage by the first k factors {{as well as a}} small number of factors needed for exact decomposition and indicates that the algorithm outperforms the available ones in these terms. We also propose future research topics...|$|R
40|$|We {{perform the}} non-perturbative (quenched) {{renormalization}} of the chromo-magnetic operator in Heavy Quark Effective Theory and its three-loop matching to QCD. At order 1 /m of the expansion, the operator {{is responsible for}} the mass splitting between the pseudoscalar and vector B-mesons. These new <b>computed</b> <b>factors</b> are affected by an uncertainty negligible in comparison to the known bare matrix element of the operator between B-states. Furthermore, they push the quenched determination of the spin splitting for the Bs-meson much closer to its experimental value than the previous perturbatively renormalized computations. The renormalization factor for three commonly used heavy quark actions and the Wilson gauge action and useful parametrizations of the matching coefficient are provided. Comment: 7 pages, 2 figure...|$|R
40|$|Abstract. The aim of {{this paper}} is to present {{experimental}} results on a recently developed method of factor analysis of data with graded, or fuzzy, attributes. The method utilizes formal concepts of data with graded attributes. In our previous papers, we described the factor model, the method, an algorithm to <b>compute</b> <b>factors,</b> and provided basic examples. In this paper, we perform a more extensive experimentation with this method. In particular, we apply the method to factor analysis of sports data. The aim of the paper is to demonstrate that the method yields reasonable factors, explain in detail how the factor model and the factors are to be understood, and to put forward new issues relevant to the method...|$|R
