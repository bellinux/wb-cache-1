5|330|Public
40|$|In {{the fashion}} market, the {{competition}} is ostensibly decided by the design, quality, and price for the article. Increasing globalisation and the resulting increase in uncertainties not only result in the much noticed purchasing, but also the logistics representing an important <b>correcting</b> <b>variable.</b> With this in mind, the Fraunhofer-Institut für Materialfluss und Logistik (IML), Dortmund (D), conducted a survey {{in order to bring}} some transparency into the special demands of the fashion supply chain...|$|E
40|$|The article {{discusses}} {{some aspects}} of the error correction model for the Norwegian consumption function proposed in Brodin and Nymoen (1991). The main focus is on the pursued simplification process since the simplified model contains an error <b>correcting</b> <b>variable</b> that includes a contemporaneous variable, and it further includes combinations of variables which may be hard to interpret. The economic implications of the alternative model for the Norwegian consumption, are that, apart from deterministics, consumption is effected by current income and wealth, and that the dynamics are established by an average of past changes in consumption/income ratios...|$|E
40|$|The paper {{presents}} a universal method of direct controller design. Similar to second order systems cascaded damping ratios are defined for third order systems, which allow a di-rect controller design for maximum bandwidth {{of the control}} circuit. Using the low frequency parameters of the plant {{the parameters of the}} process controller (PI, PID) may be calcu-lated for proportional plants of arbitrary lag order and dead-time. Large dead-time as well as nonminimum-phase characteristic can be handled without problems. A prescribed dynamic limiting of the <b>correcting</b> <b>variable</b> is easily accom-plished by introducing a time lag to the controller. Simplified design rules are derived referring only to the average resident time and approximate order of the plant to be controlled. A simple procedure of parameter estimation based on the in-flection tangent at the step response of the plant is outlined for parameter tuning using minimal process information. ...|$|E
30|$|At first, we are {{describing}} the first reason. Within a branch and price algorithm, assume a variable given that is, in theory, improving the current basic solution (i.e., it has negative reduced costs). In general, these variables {{are expected to}} enter the next basic solution in the simplex algorithm. Nevertheless, in the [FAPH 1] problem {{it may not be}} used in the next basic solution, i.e., its value remains at zero. This is {{based on the fact that}} the constraints are rather strict in the following sense: Assigning a value above zero to one variable may force other variables to change their values as well. These other <b>variables</b> are called <b>correcting</b> <b>variables</b> in the following, since they “correct” the changes the improving variable induces if used basic solution. In a column generation approach, these <b>correcting</b> <b>variables</b> may not be available yet such that the priced variable cannot be used. Additionally, these <b>correcting</b> <b>variables</b> may not have negative reduced costs in the current basic solution such that they are not recognized by the pricing problem. However, the reduced costs are updated after adding a new variable yielding that these variables will get negative reduced costs in the long run. Consequentially, solutions of the pricing problem are added successively until these <b>correcting</b> <b>variables</b> are all available. This may take arbitrarily many steps and is therefore not applicable for realistically sized instances. To our knowledge, no generic and efficient way of creating these <b>correcting</b> <b>variables</b> is known. In the following, we present some work around to this problem.|$|R
40|$|This paper investigates <b>correct</b> <b>variable</b> {{selection}} in finite samples via ℓ 1 and ℓ 1 +ℓ 2 type penalization schemes. The asymptotic consistency of variable selection immediately follows from this analysis. We focus on logistic and linear regression models. The following questions {{are central to}} our paper: give...|$|R
40|$|The integrality of Ooguri-Vafa disk invariants is {{verified}} using discrete symmetries of the superpotential of {{the mirror}} Landau-Ginzburg theory to calculate quantum corrections to the boundary variables. We show that these quantum corrections are completely determined {{if we assume that}} the discrete symmetry of the superpotential also holds in terms of the quantum <b>corrected</b> <b>variables.</b> We discuss the case of local P 2 blown up at three points and local F 2 blown up at two points in detail. Content...|$|R
40|$|In {{this paper}} we {{gradually}} construct a monthly encompassing monetary model {{on the basis}} of its two constituting components: a money demand and a loan demand model. Each of the three models pays special attention to the intermediation role of banks by modelling the relation between the retail bank interest rates and the short-term market interest rate. The encompassing monetary model accounts for the possible interactions between money and loans induced by the intermediation role of the banking sector, which is represented in this paper by its interest rates setting behaviour. Our analysis indicates that, over the period January 1981 -September 2001, our monthly money demand model corroborates the existing quarterly evidence. The same does not hold for our loan demand model where a <b>correcting</b> <b>variable</b> for the mergers and acquisitions wave of 1999 - 2000 is added as an exogenous variable to stabilise the loan demand equation. Our encompassing monetary model rejects the frequently used assumption of complete separability in the pricing of loans and deposits. It provides also some evidence on the existence of a bank lending channel in the euro area, although there is some indication of a possible instability in the link between money and loans towards the end of the sample period. The estimation of the Structural-VECM highlights very rich dynamics in the system. The common trends method results in the identification of seven shocks: an aggregate supply shock, an inflation objective shock, an institutional shock, a money demand shock, a loan demand shock, a banking shock and a monetary policy instrument shock. The first three shocks are permanent shocks, responsible for the main variability in the macro-economic variables in the long run; while the last four shocks are temporary ones, affecting the economy only in the short and medium run. Euro area, Cointegration, Structural VECM, Money demand, Loan demand, Banking intermediation. ...|$|E
40|$|The {{increasing}} demand of mobile applications of radio with low power demand {{led to the}} concept of the frequency modulation with constant envelopes. Those, with the production of integrated circuits arising costs depend on the used technology and the necessary chip area. Therefore the selection of the modulator must be met regarding these criteria. Today, almost all integrated circuits for applications of radio below 6 GHz are manufactured in complementary transistor logic. For highest integration and component density the structural widths of CMOS circuits until today are shrinked to under 100 Nm. The use of small technology structures is favourable in the reference to size and current consumption, but the fluctuations in the production process constantly increase. Thereby the actual sizes of the elements of the desired deviate. For capacities and resistances is to be counted on variations of 30 %. It is indispensable for the adherence to the respective radio specification that determines parameters precisely be adjusted can. In order to compensate the process variations is it necessarily to calibrate the circuit after the production. Naturally in principle the possibility exists to adjust the fluctuations by an external measurement and a following trim. This requires the measurement of each individual chip and is possible only in a defined operating condition (temperature, etc.). In order to minimize costs is it more meaningfully the calibration on the chip to be integrated. For this in past beginnings a separate circuit part was planned, which detects and compensates the fluctuations. A calibration cycle can be periodically implemented uniquely in principle, when each switching on, or. A goal of the available work was the development of a calibration concept that digitally detects the fluctuations of the loop gain of a phase rule loop and places behind automatically in accordance with defaults. Against this concept the demand was made to detect the process variations digitally. The additionally needed chip area, and thus the arising costs, should be reduced to a minimum. This objective presupposes an approach, which does not only supplement an existing concept, but fundamental modulation architectures on these criteria examines. As high performance the twopoint-Delta-Sigma- and the Predistortionmodulation pointed out. In both cases modulation is used, in order to modulate the data signals. The Predistortion points a significant advantage out in relation to the variant of point of two. The path needed for the high-pass modulation must be implemented always similarly. Thus the task of a calibration is the adjustment of the similar to the digital data path. This draws itself from as implementation intensive. Therefore the Predistortion-PLL can be selected as suitable architecture. Due to the function mode of the modulator complete, send-site signal processing must be regarded from digital transmit data to the high frequency, similar output signal. For a mathematical view the linear Model of the rule loop is used. A detailed investigation of the individual components of the PLL is implemented in this work. An analytic investigation of the intoxication processes was examined in the simulation environment MatLab and optimized for the treatment by Fractional N PLLs. Thus, compared with conventional estimations, fast and precise Noisesimulationen of the system could be accomplished. The essential point of this work is the simulation of the output phase of the voltage-controlled oscillator. This reinforcement, and/or slope varies strongly by the process-conditioned fluctuation of the capacities and inductances of the VCO. The challenge for the solution of this problem lay now in Substantial one in two nuclear aspects. On the one hand the variation had to be quantitatively seized. Further this value had to be impressed as <b>correcting</b> <b>variable</b> into the controlled system. In both points an optimal solution could be found. For the detection of the output phase two suitable procedures could be found and/or developed, which make a digital collection for the momentary phase possible. This new beginning is based on the synchronous subsampling of an asynchronous high frequency signal. The fact, a digital value of the phase curve too received makes a simple adjustment for the loop gain possible by the use of a switchable power source. This procedure became the developed calibration technology to the radio system DECT adapted. For this the rule loop was laid out regarding fast settling time when simultaneous adherence to the spectral requirements. In the context of this work additionally a procedure was developed, which makes a systematic draft possible of the loop filter on the basis the demanded specifications. The results show that the technology presented here can undercut the demanded accuracy of the loop gain of +- 1 %. In principle it is possible for the examined DECT system to detect the reinforcement up to 0. 0372 %. Such a precise attitude is not necessary however. In order to keep the expenditure to the implementation of the calibration circuit as small as possible, the system was laid out, in order to adjust variations of approx. +- 0, 5 %. The high frequency specifications could be thus completely kept. In summary it can be held that the results make a significant reduction of the necessary test procedures for highly integrated circuits for this work possible. A substantial step could be denied by this development for the economical production of integrated transmission concepts. The presented calibration technology points further a potential to the advancement into the direction of fully digital transmission concepts...|$|E
40|$|Abstract. According to the {{characteristics}} of non-linear, time-varying and large time delay in the control of grate cooler, a multi-mode intelligent control method is proposed in the paper. The multi-mode intelligent controller is builded up by variable integral PID control, fuzzy control, Bang-Bang control and expert control. This control method through a multi-mode intelligent control rules automatically identifies different modal and chooses a corresponding control algorithm to <b>correct</b> <b>variable</b> integral PID control. Industrial applications show that the control system has good robustness, achieved good control effect...|$|R
40|$|Forsaking the traditionnal hand-waving in the {{treatment}} of the motion allows to show that the ultra-relativistic approximation and the equality of kinematical variables are unnecessary ingredients in the derivation of the oscillation length using plane waves, at least in a two flavor world. It ensues that the formula is valid {{as it is in the}} non relativistic regime, provided one uses the <b>correct</b> <b>variable</b> which is found to be momentum, not energy, and that the precise production kinematics is irrelevant. Applications to more realistic three neutrino cases are briefly discussed...|$|R
5000|$|Like state observers, Kalman filters {{in general}} use {{multiple}} observed <b>variables</b> to <b>correct</b> state <b>variable</b> estimates, and these {{do not have}} to be direct measurements of individual system states.|$|R
40|$|International audienceThis paper {{presents}} an error compensation method for truncated multiplication. From two $n$-bit operands, the operator produces an n-bit product with small error {{compared to the}} 2 n-bit exact product. The method {{is based on a}} logical computation followed by a simplification process. The filtering parameter used in the simplification process helps to control the trade-off between hardware cost and accuracy. The proposed truncated multiplication scheme has been synthesized on an FPGA platform. It gives a better accuracy over area ratio than previous well-known schemes such as the constant <b>correcting</b> and <b>variable</b> <b>correcting</b> truncation schemes (CCT and VCT) ...|$|R
50|$|The light-front {{technique}} {{was brought into}} nuclear physics by the pioneering papers of Frankfurt and Strikman. The emphasis was on using the <b>correct</b> kinematic <b>variables</b> (and the corresponding simplifications achieved) in making correct treatments of high-energy nuclear reactions. This sub-section focuses on only a few examples.|$|R
40|$|Selection of {{covariates}} {{is among}} the most controversial and difficult tasks in epidemiologic analysis. <b>Correct</b> <b>variable</b> selection addresses the problem of confounding in etiologic research and allows unbiased estimation of probabilities in prognostic studies. The aim of this commentary is to assess how often different variable selection techniques were applied in contemporary epidemiologic analysis. It was of particular interest to see whether modern methods such as shrinkage or penalized regression were used in recent publications. Stepwise selection methods remained the predominant method for variable selection in publications in epidemiological journals in 2008. Shrinkage methods were not used in any of the reviewed articles. Editors, reviewers and authors have insufficiently promoted the new, less controversial approaches of variable selection in the biomedical literature, whereas statisticians may not have adequately addressed the method’s feasibility...|$|R
30|$|In this study, we are {{interested}} in supervised learning technique that finds the best described computer model from a dataset with the <b>correct</b> class <b>variable.</b> Support vector machine {{is one of the most}} well-known machine-learning techniques. The technique was proposed by Vladimir Vapnik for classification and regression [11, 16, 24].|$|R
30|$|Calculate the {{correction}} quantities ΔX(iter) by {{the correction}} equations and <b>correct</b> the unknown <b>variables</b> by X(iter +  1) = X(iter) + ΔX(iter). Update the iteration number iter = iter +  1.|$|R
40|$|Ó The Author(s) 2009. This {{article is}} {{published}} with open access at Springerlink. com Abstract Selection of covariates {{is among the}} most controversial and difficult tasks in epidemiologic analysis. <b>Correct</b> <b>variable</b> selection addresses the problem of confounding in etiologic research and allows unbiased estimation of probabilities in prognostic studies. The aim of this commentary is to assess how often different variable selection techniques were applied in contemporary epidemiologic analysis. It was of particular interest to see whether modern methods such as shrinkage or penalized regression were used in recent publications. Stepwise selection methods remained the predominant method for variable selection in publications in epidemiological journals in 2008. Shrinkage methods were not used in any of the reviewed articles. Editors, reviewers and authors have insufficiently promoted the new, less controversial approaches of variable selection in the biomedical literature, whereas statisticians may not have adequately addressed the method’s feasibility...|$|R
30|$|In the {{original}} publication, {{the unit of}} variable kj in Tables  4 and 7 has been incorrectly published as km/h. The <b>correct</b> unit of <b>variable</b> kj should be veh/h.|$|R
40|$|We {{discuss the}} {{difference}} between the distribution of secondaries measured in terms of pseudorapidity and that using the <b>correct</b> rapidity <b>variable.</b> We show a set of examples obtained using Monte Carlo simulations. We also consider the production of particles of low transverse momentum where coherence effects may occur, which are not yet included in the present Monte Carlos. Comment: 9 pages, 7 figure...|$|R
40|$|Expanded {{exploration}} of our Solar System {{will require more}} sophisticated autonomous assets to be developed and deployed. Model based Autonomous control system is a primary technology solution to this problem. A critical factor in the successful operations of these systems {{is to ensure that}} the models behave correctly. The Kennedy Space Center (KSC) has been pursing in conjunction with Ames Research Center the application of model-checking techniques for an Intelligent Systems Software for an In-Situ Resource Utilization (ISRU) plant for future manned Mars missions. Model checking is a formal technique which can exhaustively evaluate a finite state model for satisfiability of a logical property. The main goal of our current model checking effort is to develop tools and methodologies for efficient evaluation and certification of future Livingstone modeling applications which are declarative in form. As a result of this investigation a potential new re-usable specification pattern was derived which allows one to check a model for the existence of <b>correct</b> <b>variable</b> dependencies within the model...|$|R
40|$|We {{consider}} {{the problem of}} high-dimensional variable selection: given n noisy observations of a k-sparse vector β ∗ ∈ R p, estimate the subset of non-zero entries of β ∗. A significant body of work has studied behavior of ℓ 1 -relaxations when applied to random measurement matrices that are dense (e. g., Gaussian, Bernoulli). In this paper, we analyze sparsified measurement ensembles, and {{consider the}} trade-off between measurement sparsity, {{as measured by the}} fraction γ of nonzero entries, and the statistical efficiency, as measured by the minimal number of observations n required for <b>correct</b> <b>variable</b> selection with probability converging to one. Our main result is to prove {{that it is possible to}} let the fraction on non-zero entries γ → 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efficiency as dense ensembles. A variety of simulation results confirm the sharpness of our theoretical predictions...|$|R
40|$|The Dantzig {{selector}} performs {{variable selection}} and model fitting in linear regression. It uses an L 1 penalty {{to shrink the}} regression coefficients towards zero, {{in a similar fashion}} to the Lasso. While both the Lasso and Dantzig selector potentially {{do a good job of}} selecting the <b>correct</b> <b>variables,</b> they tend to over-shrink the final coefficients. This results in an unfortunate trade-off. One can either select a high shrinkage tuning parameter that produces an accurate model but poor coefficient estimates or a low shrinkage parameter that produces more accurate coefficients but includes many irrelevant variables. We extend the Dantzig selector to fit generalized linear models while also eliminating over-shrinkage of the coefficient estimates. In addition, we develop a computationally efficient algorithm, similar in nature to least angle regression, to compute the entire path of coefficient estimates. A detailed simulation study illustrates the advantages of our approach relative to several other possible methods. Finally, we apply the methodology to two real-world datasets...|$|R
40|$|While {{previous}} marketing {{activities were}} primarily focused on increasing market shares {{in terms of}} a mass marketing based on single transactions, the past few years saw a paradigmatic switch towards relationship management. This change in viewpoint is based on the finding that the establishment and maintenance of long-term relationships have a decisive influence on corporate success. In this study, the authors present a critical theoretical and empirical analysis of the contribution the Internet can make to successful relationship marketing. The study focuses on the influence that important characteristics of the World Wide Web such as its interactive structure and constant availability of information can have on central <b>correcting</b> <b>variables</b> of relationship marketing, i. e. commitment, satisfaction, and trust. For example, the authors provide empirical proof of potential withdrawal of trust {{on the part of the}} customers if their expectations with regard to these characteristics are not met. 1. Increased importance of long-term client relations While previous marketing activities were primarily focused on increasing market shares i...|$|R
40|$|A {{supersymmetry}} anomaly {{is found}} in the presence of non-perturbative fields. When the action is expressed in terms of the <b>correct</b> quantum <b>variables,</b> anomalous surface terms appear in its supersymmetric variation - one per each collective coordinate. The anomalous surface terms do not vanish in general when inserted in two- or higher-loop bubble diagrams, and generate a violation of the SUSY Ward identities. Comment: RevTeX, 4 page...|$|R
30|$|Note that {{misclassification}} also {{occurs when}} comparing different sources from the IEB. Wichert und Wilke (2012) compare job seekers’ histories (BewA) {{with data from}} the Employment Register (BeH) and obtain high misclassified results (Wichert and Wilke 2012). For example, the match between BewA and BeH data for the technical school degree variable is about 36 % after <b>correcting</b> the <b>variable</b> by using the imputation algorithm.|$|R
40|$|Minor bugfixes, including: Fixed a bug where DDTBOX crashed during group-level {{analysis}} of feature weights when using spatiotemporal decoding. Fixed a bug where feature weight analysis p-values output from the permutation test multiple comparisons correction were not copied into the <b>correct</b> MATLAB <b>variable.</b> Fixed a bug where the adjusted critical {{alpha for the}} Benjamini-Hochberg procedure for decoding performance analyses was erroneously copied into the wrong output variable...|$|R
50|$|Some {{of these}} {{documents}} can contain as many as 80 to 100 pages, with hundreds of optional paragraphs and data elements. Document automation software {{has the ability to}} automatically fill in the <b>correct</b> document <b>variables</b> based on the transaction data. In addition, some document automation software has the ability to create a document suite where all related documents are encapsulated into one file, making updates and collaboration easy and fast.|$|R
40|$|Radio Frequency Identification (RFID) {{technology}} uses radio-frequency {{waves to}} automatically identify people or objects. Despite an emergence of RFID technology, multiple tag identification, where a reader identifies a multiple number of tags {{in a very}} short time, is still a major problem. This is known as Collision problem and can be solved by using anti-collision scheme. The current tree-based anti-collision approach suffers from long identification delay, while the ALOHA-Based approach suffers from tag starvation problem due to inaccurate Frame-size. In this paper, we propose a ``Precise Tag Estimation Scheme'' for a Dynamic Framed-Slot ALOHA (DFSA), which estimates precise number of tags around the reader. In this empirical study, we compare our approaches with the original tag estimation in DFSA. The results indicate that the various parameters used by ``Precise Tag Estimation Scheme'', including /textit{empty slots} variables and/or /textit{collision slots} variables, have an impact on system efficiency. Thus, the number of frames and slots used by DFSA can be minimised by adjusting <b>correct</b> <b>variables.</b> Griffith Sciences, School of Information and Communication TechnologyFull Tex...|$|R
40|$|The dictionary-aided sparse {{regression}} (SR) {{approach has}} recently {{emerged as a}} promising alternative to hyperspectral unmixing (HU) in remote sensing. By using an available spectral library as a dictionary, the SR approach identifies the underlying materials in a given hyperspectral image by selecting a small subset of spectral samples in the dictionary to represent the whole image. A drawback with the current SR developments is that an actual spectral signature in the scene is often assumed to have zero mismatch with its corresponding dictionary sample, and such an assumption is considered too ideal in practice. In this paper, we tackle the spectral signature mismatch problem by proposing a dictionary-adjusted nonconvex sparsity-encouraging regression (DANSER) framework. The main idea is to incorporate dictionary <b>correcting</b> <b>variables</b> in an SR formulation. A simple and low per-iteration complexity algorithm is tailor-designed for practical realization of DANSER. Using the same dictionary correcting idea, we also propose a robust subspace solution for dictionary pruning. Extensive simulations and real-data experiments show that the proposed method is effective in mitigating the undesirable spectral signature mismatch effects...|$|R
40|$|The paper {{proposes a}} novel {{algorithm}} based on Retinex theory and color transfer to get illumination invariance among images, taking {{one of the}} images as a reference. Most of the algorithms focus on eliminating the illumination effect of one image, while our method <b>corrects</b> the <b>variable</b> illumination among images. Therefore, the algorithm {{can be applied to}} target tracking, recognition, matching and 3 D reconstruction to minish the illumination variations. Experiment results validate the method...|$|R
40|$|Program {{generation}} {{is the process}} of generating code in a high-level language (e. g., C, C++, Java) to implement an abstract specification of a program. Generated programs are created by synthesizing and composing code fragments. Binding identifiers in generated code with their <b>correct</b> <b>variable</b> declarations {{has been the focus of}} a lot of research work in the context of macro-expansion (e. g., hygienic macro expansion and syntactic closures mechanisms). The common solutions include automatically maintaining identifier environments, which determine the legal bindings for an identifier. In this paper we present generation scoping: an adaptation of hygienic macro-expansion techniques to general-purpose program generation. The conceptual novelty of generation scoping lies in making identifier environments first-class objects and organizing them explicitly into directed graphs. This approach yields significant benefits: We are able to express scoping relationships independently of the structure of the generated program. This way we get a stronger tool for detecting errors in the specification of generated code. Additionally, we are able to simplify the specification by employing powerful implicit qualification. Thus, generation scoping becomes a useful layer of infrastructure for implementing software generators: it both solves binding problems and makes code specification more convenient...|$|R
30|$|The {{fragments}} {{may also}} contain toggleable elements which are shown as gaps. For these fragments, the student must select, for example, the <b>correct</b> operator or <b>variable</b> name {{to fill the}} gap (see the segmented squares with question marks “??” in Fig.  4).|$|R
50|$|Since {{the timing}} of this process is {{determined}} mainly by exhaust system geometry, which is extremely difficult to make <b>variable,</b> <b>correct</b> timing and therefore optimum engine efficiency can typically only be achieved over {{a small part of the}} engine's range of operating speed.|$|R
50|$|Initiatives such {{as demand}} shaving, {{replacement}} of malfunctioning equipment, retrofits of inefficient equipment, and removal of unnecessary loads can be discovered and coordinated using the EMS. For example, an unexpected energy spike {{at a specific}} time each day may indicate an improperly set or malfunctioning timer. These tools {{can also be used}} for Energy Monitoring and Targeting. EMS uses models to <b>correct</b> for <b>variable</b> factors such as weather when performing historical comparisons to verify the effect of conservation and efficiency initiatives.|$|R
40|$|We {{describe}} an integer programming (IP) model {{that can be}} applied to the solution of all genome-rearrangement problems in the literature. No direct IP model for such problems had ever been proposed prior to this work. Our model employs an exponential number of variables, but it can be solved by column generation techniques. I. e., we start with a small number of variables and we show how the <b>correct</b> missing <b>variables</b> can be added to the model in polynomial time...|$|R
3000|$|Let us {{consider}} a test sample or a novel {{data to be}} classified. The problem is to predict whether the test data belongs {{to one of the}} considered classes. The training data is a set of examples of the form {x_i, y_i}([...] i= 1,...,l). x_i are called input vector. Each input vector has a number of features. [...] y_i∈{ 0, 1 }. [...] y_i are the response variables called also labels. These input vectors are paired with corresponding labels to find the <b>correct</b> class <b>variable.</b>|$|R
