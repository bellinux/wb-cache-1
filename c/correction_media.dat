0|19|Public
5000|$|HonestReporting has {{prompted}} many <b>corrections</b> in the <b>media</b> {{over the years}} including: ...|$|R
40|$|Australia's {{competitive}} {{energy market}} experienced sustained network tariff increases at multiples of {{the inflation rate}} from 2008 to 2012. This became a toxic issue for policymakers and in one region resulted in regulated tariffs in the competitive market being set below long-run costs. Doing so risks damaging competition, investment flows, and following any price <b>correction,</b> a subsequent <b>media</b> assault. Griffith Business School, Department of Accounting, Finance and EconomicsFull Tex...|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Copyright Notice Copyright (C) The Internet Society (1999). All Rights Reserved. This document specifies a payload format for generic forward error <b>correction</b> of <b>media</b> encapsulated in RTP. It is engineered for FEC algorithms based on the exclusive-or (parity) operation. The payload format allows end systems to transmit using arbitrary block lengths and parity schemes. It also allows for the recovery of both the payload and critical RTP header fields. Since FEC is sent as a separate stream, it is backwards compatible with non-FEC capabl...|$|R
40|$|Abstract—We {{investigate}} {{the performance of}} diffuse optical tomography to image highly heterogeneous media, such as breast tissue, {{as a function of}} background heterogeneity. To model the background heterogeneity, we have employed the functional information derived from Gadolinium-enhanced magnetic reso-nance images of the breast. We demonstrate that overall image quality and quantification accuracy worsens as the background heterogeneity increases. Furthermore we confirm the appearance of characteristic artifacts at the boundaries that scale with back-ground heterogeneity. These artifacts are very similar to the ones seen in clinical examinations and can be misinterpreted as actual objects if not accounted for. To eliminate the artifacts and improve the overall image reconstruction, we apply a data-correction algo-rithm that yields superior reconstruction results and is virtually independent of the degree of the background heterogeneity. Index Terms—Artifacts, <b>correction,</b> diffuse <b>media,</b> diffuse op-tical tomography, reconstruction. I...|$|R
40|$|We survey {{a number}} of packet-loss {{recovery}} techniques for streaming audio applications operating using IP multicast. We begin {{with a discussion of}} the loss and delay characteristics of an IP multicast channel and from this show the need for packet loss recovery. Recovery techniques may be divided into two classes: senderand receiver-based. We compare and contrast several sender-based recovery schemes: forward error <b>correction</b> (both <b>media</b> specific and media independent) interleaving and retransmission. In addition {{a number of}} error concealment schemes are discussed. We conclude with a series of recommendations for repair schemes to be used, based on application requirements and network conditions. 1 Introduction The development of IP multicast and the Internet multicast backbone has led to be emergence of a new class of scalable audio/video conferencing applications. These are based on the lightweight sessions model [11, 17] and provide efficient multi-way communication which scales fr [...] ...|$|R
40|$|The {{residential}} property market in New Zealand has been experiencing a boom and bubble period from 2001 through to mid 2007. Following {{a number of}} increases in the Official Cash Rate by the Reserve Bank {{and a decline in}} net migration numbers the housing market was perceived to be over inflated and due for a major <b>correction.</b> Numerous <b>media,</b> Government Departments, property experts and economists have been predicting significant reductions in the median price of {{residential property}} throughout New Zealand. This paper will analyse house prices in specific socio-economic locations within Christchurch over the past 12 months to determine how significant the current housing decline is. This study will review the change in residential property prices, variations in property listings since April 2008, sale volumes and days on the market across a range of housing sectors to determine the extent and range of any residential property downturn in the NZ recession. ...|$|R
5000|$|The goal in {{relative}} colorimetry {{is to be}} truthful to the specified color, with only a <b>correction</b> for the <b>media.</b> Relative colorimetry is useful in proofing applications, since you are using it {{to get an idea}} of how a print on one device will appear on a different device. Media differences are the only thing you really would like to adjust for. Obviously there has to be some gamut mapping going on also. Usually this is done in a way where hue and lightness are maintained at the cost of reduced saturation.|$|R
40|$|Nearly 80 {{years of}} {{accuracy}} {{research in the}} United States has documented that the press frequently errs, but empirical study about news accuracy {{elsewhere in the world}} is absent. This article presents an accuracy audit of Swiss and Italian daily regional newspapers. Replicating US research, the study offers a trans-Atlantic perspective of news accuracy. To compare newspaper accuracy in Switzerland and Italy to longitudinal accuracy research in the United States, the study followed closely the methodology pioneered by Charnley (1936) and adapted by Maier (2005). News sources found factual inaccuracy in 60 percent of Swiss newspaper stories they reviewed, compared to 48 percent of US and 52 percent of Italian newspapers examined. The results show that newspaper inaccuracy—and its corrosive effect on media credibility—transcends national borders and journalism cultures. Nowadays, digitization offers new ways of implementing <b>correction</b> policies. <b>Media</b> organizations need, however, to adapt to these changes and to adapt their structures in particular to new forms of participative and interactive two-way communication...|$|R
40|$|A new optical {{arrangement}} for performing dynamic focus in time domain {{optical coherence tomography}} (OCT) is demonstrated. Unlike previously reported schemes which require mechanical coupling of the object and reference arms, this method is confined to the object arm only and therefore does not impose design constraints on the OCT system layout. The scheme is tested on a high lateral resolution OCT system (NA= 0. 13) and it is shown that the effective depth of focus is extended from 200 m to better than 2 mm. The optimum <b>correction</b> is for <b>media</b> with a mean refractive index of 1. 4. Â© The Institution of Engineering and Technology 2009...|$|R
40|$|We derived {{the cone}} {{fundamentals}} for X-chromosome-linked anomalous trichromats for the wavelength range of 400 - 700 nm. Pigment templates were constructed from the cone fundamentals of normal trichromats after <b>correction</b> for ocular <b>media</b> absorption. The resultant retinal-level sensitivities had small irregularities in the short-wavelength region that were smoothed. The pigment templates, expressed as quantal sensitivities, were then shifted on a frequency abscissa to solve for the Amax of the pigments of anomalous trichromats needed to predict average anomaloscope matching data. We {{found that the}} protanomalous M- and U-cone pigments are separated by 10 nm and the deuteranomalous M'- and L-cone pigments are separated by 6 nm (rounded to the nearest nanometer), where M and L indicate middle- and long-wavelength sensitive, respectively. The triads o...|$|R
50|$|In February 2015, {{a wave of}} tributes (followed by <b>corrections)</b> hit social <b>media</b> {{sites on}} the World Wide Web {{over a period of}} two days, when an {{individual}} mistakenly read a 2009 report of Hart's death and, missing the dateline, published it as news on Facebook, from which it was later transferred to Twitter. Many social media posters hyperlinked to an article in The Guardian. The newspaper published a graph of the number of readers referred to its article for the period. Aardman Animations used its Twitter account in the name of Morph to point to a tribute to Tony Hart (a portrait of him being hung on a wall) that was included in the last episode of its forthcoming new set of episodes for the Morph television series.|$|R
40|$|This {{document}} specifies an Internet standards track {{protocol for}} the Internet community, and requests discussion {{and suggestions for}} improvements. Please refer to the current edition of the "Internet Official Protocol Standards " (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. This document specifies a payload format for generic Forward Error <b>Correction</b> (FEC) for <b>media</b> data encapsulated in RTP. It {{is based on the}} exclusive-or (parity) operation. The payload format described in this document allows end systems to apply protection using various protection lengths and levels, in addition to using various protection group sizes to adapt to different media and channel characteristics. It enables complete recovery of the protected packets or partial recovery of the critical parts of the payload depending on the packet loss situation. This scheme is completel...|$|R
40|$|As {{the number}} of prisons increases, so does the level of secrecy {{about what goes on}} inside them. The secret of the abuses perpetrated by the Criminal Justice System and Prison Industrial Complex can be heard in many stories told by many narrators, but only when they are allowed to speak. After a series of news stories and law-suits documenting egregious mistreatment of prisoners in 1993, the California Department of <b>Corrections</b> imposed a <b>media</b> ban on all of its facilities. This ongoing ban prohibits journalists from {{face-to-face}} interviews, eliminates prisoners rights to confidential correspon-dence with media representatives, and bars the use of cameras, recording devices, and writing instruments when conducting interviews. For the past three years, I have intentionally circumvented the California Prison media ban, gaining access to the incarcerated women by posing as a legal advocate. I have collaborated with non-profit, human rights organization, Justice Now. [1] Together we have been documenting conversations with women prisoners at the Central California Women's Facility (CCWF) in Chowchilla, CA, the largest female correctional facility in the United States, and publishing their statements online a...|$|R
40|$|International Telemetering Conference Proceedings / October 13 - 16, 1986 / Riviera Hotel, Las Vegas, NevadaToday I will {{describe}} {{the application of}} the Autocorrelation function to the Magnetic Recording Channel. I will explain what is an autocorrelated function, how does it behave and where may it be applied in the Magnetic Recording channel. There will be a brief description of Kodak San Diego’s Autocorrelator and how we apply this technology. If I have done my job well {{at the end of this}} presentation you will have enough knowledge about autocorrelation to access your own application. Before I start, let me give a brief overview on the application of an Autocorrelator. The Autocorrelator can be used to collect information on signals in a magnetic recording system and display this information graphically as a statistical plot. Autocorrelation, in the time domain, is the counter part to a spectrum analyzer in the frequency domain (Fourier Pair). The information about the signal of interest must be stored for post analysis. This information called a database must then be processed by a computer. The computer passes the database through the autocorrelation algorithm and produces a second database. This second database represents a plot of the autocorrelated function. The next step is to plot the database on a video screen. This plot can be examined for periodicities, randomness, and relational influences on a captured signal. In our application, this signal is an error flag or a dropout flag. We want a statistical picture of the magnitude of errors and their relative frequency. The information gained from Autocorrelation can aid in solutions for: Error <b>Correction</b> Codes <b>Media</b> Evaluation/Qualifications Media Process Defect Identification Mechanical Eccentricities Modulation Code Performances System’s Figure of Merit To use a cliche, “one picture is worth a thousand words,” is exactly the point of the Autocorrelator’s graphical display. It yields information useful to those disciplines which often find difficulty in describing an event in understandable terms...|$|R
40|$|Conversations {{on social}} media {{networks}} that discuss a crisis incident as it unfolds have become a norm in recent years. Left to its own devices, such conversations could quickly degenerate into rumor mills. Little research has thus far examined the correction of rumors {{on social media}}. Using the thirdperson effect as a theoretical underpinning, we developed a model of collective rumor <b>correction</b> on social <b>media</b> based on an incident surrounding the death hoax of a political figure. Tweets from Twitter were collected and analyzed for the period when a spike of circulating rumors speculating the demise of Singapore 2 ̆ 7 s first prime minister was detected. Corrections of the rumor also went viral on the same day. Our study reveals that corrective behavior during a death hoax situation on Twitter is characterized by affirmative and rational rebuttals verifiable by credible sources. While the inclusion of credible sources is essential for both rumor diffusion and corrections, correcting a rumor differs from its diffusion in that unambiguity and low emotional levels are crucial. Key characteristics of collective rumor correction identified by this study have implications for both theory and practice. We discussed these implications together with the study 2 ̆ 7 s limitations and suggestions for future research...|$|R
5000|$|In 2011, several {{companies}} including distribution network operators (ERDF, Enexis), meter vendors (Sagemcom, Landis&Gyr) and chip vendors (Maxim Integrated, Texas Instruments, STMicroelectronics) founded the G3-PLC Alliance to promote G3-PLC technology. G3-PLC is the low layer protocol to enable large scale infrastructure on the electrical grid. G3-PLC may operate on CENELEC A band (35 kHz to 91 kHz) or CENELEC B band (98 kHz to 122 kHz) in Europe, on ARIB band (155 kHz to 403 kHz) in Japan and on FCC (155 kHz to 487 kHz) for the US {{and the rest}} of the world. The technology used is OFDM sampled at 400 kHz with adaptative modulation and tone mapping. Error detection and correction is made by both a convolutional code and Reed-Solomon error <b>correction.</b> The required <b>media</b> access control is taken from IEEE 802.15.4, a radio standard. In the protocol, 6loWPAN has been chosen to adapt IPv6 an internet network layer to constrained environments which is Power line communications. 6loWPAN integrates routing, based on the mesh network LOADng, header compression, fragmentation and security. G3-PLC has been designed for extremely robust communication based on reliable and highly secured connections between devices, including crossing Medium Voltage to Low Voltage transformers. In December 2011, G3 PLC technology was recognised as an international standard at ITU in Geneva where it is referenced as G.9903. [...] Narrowband orthogonal frequency division multiplexing power line communicationtransceivers for G3-PLC networks.|$|R
40|$|Sprays {{and other}} industrially {{relevant}} turbid media can be quantitatively and qualitatively characterized using modern optical diagnostics. However, current laser based techniques generate {{errors in the}} dense region of sprays due to the multiple scattering of laser radiation e ected by the surrounding cloud of droplets. In most industrial sprays, the scattering of light occurs within the so-called intermediate scattering regime where {{the average number of}} scattering events is too great for single scattering to be assumed, but too few for the di usion approximation to be applied. An understanding and adequate prediction of the radiative transfer in this scattering regime is a challenging and non-trivial task that can significantly improve the accuracy and e ciency of optical measurements. A novel technique has been developed for the modelling of optical radiation propagation in inhomogeneous polydisperse scattering media such as sprays. The computational model is aimed to provide both predictive and reliable information, and to improve the interpretation of experimental results in spray diagnostics. Results from simulations are verified against the analytical approach and validated against the experiment by the means of homogeneous solutions of suspended polystyrene spheres. The ability of the technique to simulate various detection conditions, to di erentiate scattering orders and to generate real images of light intensity distributions with high spatial resolution is demonstrated. The model is used for the real case of planar Mie imaging through a typical hollow cone water spray. Versatile usage of this model is exemplified with its applications to image transfer through turbid <b>media,</b> <b>correction</b> of experimental Beer-Lambert measurements, the study of light scattering by single particles in the farfield region, and to simulate the propagation of ultra-short laser pulses within complex scattering media. The last application is fundamental for the development and testing of future optical spray diagnostics; particularly for those based on time-gating detection such as ballistic imaging. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|This {{final report}} {{summarizes}} the research work conducted under NASA's Physical Oceanography Program, entitled, GFO- 1 Geophysical Data Record And Orbit Verifications For Global Change Studies, {{for the investigation}} time period from December 1, 1997 through November 30, 2000. The primary objectives of the investigation include providing verification and improvement for the precise orbit, media, geophysical, and instrument corrections to accurately reduce U. S. Navy's Geosat-Followon- 1 (GFO- 1) mission radar altimeter data to sea level measurements. The status of the GFO satellite (instrument and spacecraft operations, orbital tracking and altimeter) is summarized. GFO spacecraft has been accepted by the Navy from Ball Aerospace and has been declared operational since November, 2000. We have participated in four official GFO calibration/validation periods (Cal/Val I-IV), spanning from June 1999 through October 2000. Results of verification of the GFO orbit and geophysical data record measurements both from NOAA (IGDR) and from the Navy (NGDR) are reported. Our preliminary results indicate that: (1) the precise orbit (GSFC and OSU) can be determined to approx. 5 - 6 cm rms radially using SLR and altimeter crossovers; (2) estimated GFO MOE (GSFC or NRL) radial orbit accuracy is approx. 7 - 30 cm and Operational Doppler orbit accuracy is approx. 60 - 350 cm. After bias and tilt adjustment (1000 km arc), estimated Doppler orbit accuracy is approx. 1. 2 - 6. 5 cm rms and the MOE accuracy is approx. 1. 0 - 2. 3 cm; (3) the geophysical and <b>media</b> <b>corrections</b> have been validated versus in situ measurements and measurements from other operating altimeters (T/P and ERS- 2). Altimeter time bias is insignificant with 0 - 2 ms. Sea state bias is about approx. 3 - 4. 5 % of SWH. Wet troposphere correction has approx. 1 cm bias and approx. 3 cm rms when compared with ERS- 2 data. Use of GIM and IRI 95 provide ionosphere correction accurate to 2 - 3 cm rms during medium to high solar activities; (4) {{the noise of the}} GFO altimeter data (uncorrected SSH) is about 15 mm, compared to 19 min for ERS- 2, and 12 min for TOPEX. It is anticipated that the operational GFO- 1 altimeter data will contribute to a number of researches in physical oceanography. A list of relevant presentations and publications is attached...|$|R
40|$|With {{the latest}} {{developments}} in video coding technology and fast deployment of end-user broadband internet connections, real-time media applications become increasingly interesting for both private users and businesses. However, the internet remains a best-effort service network unable to guarantee the stringent requirements of the media application, in terms of high, constant bandwidth, low packet loss rate and transmission delay. Therefore, efficient adaptation mechanisms must be derived in order to bridge the application requirements with the transport medium characteristics. Lately, different network architectures, e. g., peer-to-peer networks, content distribution networks, parallel wireless services, emerge as potential solutions for reducing the cost of communication or infrastructure, and possibly improve the application performance. In this thesis, we start from the path diversity characteristic of these architectures, in order to build a new framework, specific for media streaming in multipath networks. Within this framework we address important issues related to an efficient streaming process, namely path selection and rate allocation, forward error correction and packet scheduling over multiple transmission paths. First we consider a network graph between the streaming server and the client, offering multiple possible transmission paths to the media application. We are interested in finding the optimal subset of paths employed for data transmission, and the optimal rate allocation on these paths, in order to optimize a video distortion metric. Our in-depth analysis of the proposed scenario eventually leads to the derivation of three important theorems, which, in turn represent the basis for an optimal, linear time algorithm that finds the solution to our optimization problem. At the same time, we provide distributed protocols which compute the optimal solution in a distributed way, suitable for large scale network graphs, where a centralized solution is too expensive. Next, we address the problem of forward error <b>correction</b> for scalable <b>media</b> streaming over multiple network paths. We propose various algorithms for error protection in a multipath scenario, and we assess the opportunity of in-network error correction. Our analysis stresses the advantage of being flexible in the scheduling and error correction process on multiple network paths, and emphasizes the limitations of possible real systems implementations, where application choices are limited. Finally, we observe the improvements brought by in-network processing of transmitted media flows, in the case of heterogeneous networks, when link parameters vary greatly. Once the rate allocation and error correction issues are addressed, we discuss the packet scheduling problem over multiple transmission paths. We rely on a scalable bitstream packet model inspired from the media coding process, where media packets have different priorities and dependencies. Based on the concept of data pre-fetch, and on a strict time analysis of the transmission process, we propose fast algorithms for efficient packet scheduling over multiple paths. We ensure media graceful degradation at the client in adverse network conditions by careful load balancing among transmission paths, and by conservative scheduling which transparently absorb undetected network variations, or network estimation errors. The final part of this thesis presents a possible system for media streaming where our proposed mechanisms and protocols can be straightforwardly implemented. We describe a wireless setup where clients can access various applications over possibly multiple wireless services. In this setup, we solve the rate allocation problem with the final goal of maximizing the overall system performance. To this end, we propose a unifying quality metric which maps the individual performance of each application (including streaming) to a common value, later used in the optimization process. We propose a fast algorithm for computing a close to optimal solution to this problem and we show that compared to other traditional methods, we achieve a more fair performance, better adaptable to changing network environments...|$|R

