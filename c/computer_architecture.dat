2541|2344|Public
5|$|However, power {{consumption}} P by a chip {{is given by}} the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional {{to the number of}} transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second). Increases in frequency increase the amount of power used in a processor. Increasing processor {{power consumption}} led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant <b>computer</b> <b>architecture</b> paradigm.|$|E
5|$|Parallel {{computing}} {{is a type}} of computation {{in which}} many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in <b>computer</b> <b>architecture,</b> mainly in the form of multi-core processors.|$|E
5|$|The ENIAC (1946) was {{the first}} machine that was both {{electronic}} and general purpose. It was Turing complete, with conditional branching, and programmable to solve {{a wide range of}} problems, but its program was held in the state of switches in patchcords, not in memory, and it could take several days to reprogram. Researchers such as Turing and Zuse investigated the idea of using the computer's memory to hold the program as well as the data it was working on, and it was mathematician John von Neumann who wrote a widely distributed paper describing that <b>computer</b> <b>architecture,</b> still used in almost all computers.|$|E
40|$|The <b>Computers</b> <b>Architecture</b> {{hyperdocument}} {{was developed}} to support students learning. It has two main parts: the first one, the initiation level, focuses on <b>computers</b> <b>architecture</b> contents, and the second one, the advanced level, is organised according to Cognitive Flexibility Theory. This paper presents Cognitive Flexibility Theory and its application to the hyperdocument <b>Computers</b> <b>Architecture.</b> Then, it describes the hyperdocument and the usability tests conducted...|$|R
5000|$|Estrin's {{scheme to}} {{facilitate}} parallelization on modern <b>computer</b> <b>architectures</b> ...|$|R
5000|$|Advanced <b>computer</b> <b>architectures,</b> {{distributed}} {{and parallel}} computing, and computational science ...|$|R
5|$|Computer systems {{make use}} of caches—small and fast {{memories}} located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value {{in more than one}} location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping {{is one of the most}} common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in <b>computer</b> <b>architecture.</b> As a result, shared memory computer architectures do not scale as well as distributed memory systems do.|$|E
5|$|From {{the advent}} of very-large-scale {{integration}} (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in <b>computer</b> <b>architecture</b> was driven by doubling computer word size—the amount of information the processor can manipulate per cycle. Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than {{the length of the}} word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.|$|E
25|$|<b>Computer</b> <b>architecture</b> {{courses in}} {{universities}} and technical schools often study the MIPS architecture. The architecture greatly influenced later RISC architectures such as Alpha.|$|E
3000|$|... (i)There is a {{mismatch}} between performance-oriented <b>computer</b> <b>architectures</b> and worst-case analyzability.|$|R
40|$|Reduced feature sizes {{offer the}} {{possibility}} to build more powerful <b>computer</b> <b>architectures.</b> Unfortunately the rapid developments in VLSI technology which allow those improvements introduce another problem. As wire delays do not scale with smaller feature sizes, communication will become a major bottleneck in future processors. Current <b>computer</b> <b>architectures</b> are not suited to handle this problem...|$|R
50|$|Some {{high-level}} language <b>computer</b> <b>architectures</b> include hardware support for real-time garbage collection.|$|R
25|$|MSX is {{the name}} of a {{standardized}} home <b>computer</b> <b>architecture,</b> first announced by Microsoft on June 16, 1983 and marketed by Kazuhiko Nishi, then Vice-president at Microsoft Japan and Director at ASCII Corporation. Microsoft conceived the project as an attempt to create unified standards among various hardware makers of the period.|$|E
25|$|Alpha {{was also}} {{implemented}} in the Piranha, a research prototype developed by Compaq's Corporate Research and Nonstop Hardware Development groups at the Western Research Laboratory and Systems Research Center. Piranha was a multicore design for transaction processing workloads that contained eight simple cores. It was described at the 27th Annual International Symposium on <b>Computer</b> <b>Architecture</b> in June 2000.|$|E
25|$|All {{texts and}} papers {{excepting}} the four starred have been witnessed. These four {{are written in}} German and appear as references in Shepherdson-Sturgis (1963) and Elgot-Robinson (1964); Shepherdson-Sturgis (1963) offer a brief discussion of their results in Shepherdson-Sturgis' Appendix A. The terminology {{of at least one}} paper (Kaphengst (1959) seems to hark back to the Burke-Goldstine-von Neumann (1946-7) analysis of <b>computer</b> <b>architecture.</b>|$|E
5000|$|... some <b>computer</b> <b>architectures</b> such as IBM/360 use branch {{tables for}} {{dispatching}} interrupts ...|$|R
5000|$|Future {{trends in}} <b>computer</b> <b>architectures</b> Superscalar, Vector, Multi-thread and {{multicore}} processors; future trends.|$|R
5000|$|Hercules, a {{software}} {{implementation of the}} System/370, ESA/390, and z/Architecture mainframe <b>computer</b> <b>architectures.</b>|$|R
25|$|The {{depth of}} type {{constraints}} {{and the manner}} of their evaluation affect the typing of the language. A programming language may further associate an operation with varying concrete algorithms on each type {{in the case of}} type polymorphism. Type theory is the study of type systems, although the concrete type systems of programming languages originate from practical issues of <b>computer</b> <b>architecture,</b> compiler implementation, and language design.|$|E
25|$|In October 2006, a Transmeta {{lawsuit was}} filed against Intel for patent {{infringement}} on <b>computer</b> <b>architecture</b> and power efficiency technologies. The lawsuit was settled in October 2007, with Intel agreeing to pay US$150million initially and US$20million {{per year for}} the next five years. Both companies agreed to drop lawsuits against each other, while Intel was granted a perpetual non-exclusive license to use current and future patented Transmeta technologies in its chips for 10 years.|$|E
25|$|The {{company has}} {{engineering}} groups, working on <b>computer</b> <b>architecture,</b> systems integration, embedded operating {{systems such as}} OS-9, application-specific integrated circuit (ASIC) design, and middleware. Its platforms and building blocks are based on custom form factors as well as industry standards such as AdvancedTCA, COM Express, CompactPCI, and PCI. In 2009, Radisys' biggest customers were Philips Healthcare, Agilent, Fujitsu, Danaher Corporation, and Nokia Siemens Network (NSN). NSN was the largest single customer, totaling over 43% of revenues.|$|E
5000|$|IEEE Fellow, 2008: Awarded for his {{contributions}} in Parallel <b>Computer</b> <b>Architectures</b> and Compilers.|$|R
5000|$|Many (mostly historic) <b>computer</b> <b>architectures</b> are eight-bit, {{among them}} the Nintendo Entertainment System ...|$|R
50|$|BASHLITE {{is written}} in C, and {{designed}} to easily cross-compile to various <b>computer</b> <b>architectures.</b>|$|R
25|$|During development, the Palo Alto {{design team}} {{were working on}} a Unix-only {{workstation}} that originally included the PRISM. However, development of the workstation was well ahead of the PRISM, and the engineers proposed that they release the machines using the MIPS R2000 processor instead, moving its release date up considerably. DEC management doubted the need to produce a new <b>computer</b> <b>architecture</b> to replace their existing VAX and DECstation lines, and eventually ended the PRISM project in 1988.|$|E
25|$|There is a freely {{available}} MIPS32 simulator (earlier versions simulated {{only the}} R2000/R3000) called SPIM {{for use in}} education. EduMIPS64 is a GPL graphical cross-platform MIPS64 CPU simulator, written in Java/Swing. It supports a wide subset of the MIPS64 ISA and allows the user to graphically see {{what happens in the}} pipeline when an assembly program is run by the CPU. It has educational purposes and is used in some <b>computer</b> <b>architecture</b> courses in universities around the world.|$|E
25|$|Besides the Linux {{distributions}} {{designed for}} general-purpose use on desktops and servers, distributions may be specialized for different purposes including: <b>computer</b> <b>architecture</b> support, embedded systems, stability, security, localization {{to a specific}} region or language, targeting of specific user groups, support for real-time applications, or commitment to a given desktop environment. Furthermore, some distributions deliberately include only free software. , over four hundred Linux distributions are actively developed, with about a dozen distributions being most popular for general-purpose use.|$|E
50|$|Nowadays, its {{designs in}} <b>computer</b> <b>architectures</b> {{devoted to the}} bank {{services}} are a referent.|$|R
50|$|Most <b>computer</b> <b>architectures</b> which support paging {{also use}} pages {{as the basis}} for memory protection.|$|R
50|$|Queue {{machines}} offer {{a simple}} model {{on which to}} base <b>computer</b> <b>architectures,</b> programming languages, or algorithms.|$|R
25|$|Following {{the release}} of Miranda by Research Software Ltd, in 1985, {{interest}} in lazy functional languages grew. By 1987, {{more than a dozen}} non-strict, purely functional programming languages existed. Miranda was the most widely used, but it was proprietary software. At the conference on Functional Programming Languages and <b>Computer</b> <b>Architecture</b> (FPCA '87) in Portland, Oregon, there was a strong consensus that a committee be formed to define an open standard for such languages. The committee's purpose was to consolidate existing functional languages into a common one to serve as a basis for future research in functional-language design.|$|E
25|$|Upon {{agreement}} {{of a system}} design, RTL designers then implement the functional models in a hardware description language like Verilog, SystemVerilog, or VHDL. Using digital design components like adders, shifters, and state machines as well as <b>computer</b> <b>architecture</b> concepts like pipelining, superscalar execution, and branch prediction, RTL designers will break a functional description into hardware models of components on the chip working together. Each of the simple statements described in the system design can easily turn into thousands of lines of RTL code, {{which is why it}} is extremely difficult to verify that the RTL will do the right thing in all the possible cases that the user may throw at it.|$|E
25|$|Approaches that {{represent}} previous experiences directly {{and use a}} similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses {{in such a way}} that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional <b>computer</b> <b>architecture.</b>|$|E
5000|$|PCI Industrial Computer Manufacturers Group (PICMG) (an {{industry}} consortium developing Open Standards {{specifications for}} <b>computer</b> <b>architectures</b> [...] ) ...|$|R
5000|$|Server {{processors}} significantly {{contribute to}} the overall power signature of hybrid <b>computer</b> <b>architectures</b> and need appropriate cooling capacities.|$|R
40|$|Abstract — This {{paper is}} {{concerned}} with the analytical modeling of <b>computer</b> <b>architectures</b> to aid in the design of high-level language-directed <b>computer</b> <b>architectures.</b> High-level language-directed <b>computers</b> are computers that execute programs in a high-level language directly. The design procedure of these computers are at best described as being ad hoc. In order to systematize the design procedure, we introduce analytical models of computers that predict the performance of parallel computations on concurrent computers. We model computers as queueing networks and parallel computations as precedence graphs. The models that we propose are simple and lead to computationally efficient procedures of predicting the performance of parallel computations on concurrent computers. We demonstrate the use of these models in the design of high-level languagedirected <b>computer</b> <b>architectures.</b> I...|$|R
