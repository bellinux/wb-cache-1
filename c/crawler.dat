2711|1015|Public
5|$|A dungeon <b>crawler</b> game {{based on}} the story of the company's five founders was made. The game was housed in an arcade cabinet inside Obsidian.|$|E
5|$|Following Scott's {{death the}} band briefly {{considered}} quitting, but {{encouraged by the}} insistence from Scott's parents {{that he would have}} wanted them to go on, they eventually decided to continue and went about finding a new frontman. Various candidates were considered for his replacement, including: Buzz Shearman, ex-Moxy member, who was not able to join because of voice problems, ex-Back Street <b>Crawler</b> vocalist Terry Slesser and then Slade vocalist, Noddy Holder. The remaining AC/DC members finally decided on ex-Geordie singer Brian Johnson.|$|E
5|$|While it reprises {{the action}} {{role-playing}} elements of previous Mana games, such as real-time battle sequences, Children of Mana features an increased focus on user-friendliness. Unlike earlier Mana titles, Children is a heavily action-oriented dungeon <b>crawler,</b> {{in which the}} player progresses by completing randomly generated levels. Both the main plot and side-quests require the player to fight through dungeons and defeat boss monsters {{before returning to the}} central Mana Village. Like many of its predecessors, the game features a local cooperative multiplayer component.|$|E
5000|$|Focused <b>crawlers,</b> {{which are}} Web <b>crawlers</b> guided by page topic classifiers.|$|R
40|$|Web log mining is the {{extraction}} of web logs to analyze user behaviour at web sites. In addition to user information, web logs provide immense information about search engine traffic and behaviour. Search engine <b>crawlers</b> are highly automated programs that periodically visit the web site to collect information. The behaviour of search engines {{could be used in}} analyzing server load, quality of search engines, dynamics of search engine <b>crawlers,</b> ethics of search engines etc. The time spent by various <b>crawlers</b> is significant in identifying the server load as major proportion of the server load is constituted by search engine <b>crawlers.</b> A temporal analysis of the search engine <b>crawlers</b> were done to identify their behaviour. It was found that there is {{a significant difference in the}} total time spent by various <b>crawlers.</b> The presence of search engine <b>crawlers</b> at web sites on hourly basis was also done to identify the dynamics of search engine <b>crawlers</b> at web sites...|$|R
30|$|We {{develop a}} PathMarker {{prototype}} on an online open source forum website. We train SVM models {{based on the}} access log data collected from more than 100 normal users and 6 in-house <b>crawlers,</b> and then test the model using 6 open-source <b>crawlers</b> and another set of normal user data. The experimental results show that our anti-crawler technique can effectively detect all <b>crawlers.</b> Moreover, two external <b>crawlers,</b> Googlebot (Sexton) and Yahoo Slurp (Yahoo), are also detected, and PathMarker can successfully suppress these two distributed <b>crawlers.</b>|$|R
5|$|In {{the spring}} of 1912, {{debentures}} were sold to Wood and Gandy Co. for $17,700, so five new, steel road graders were purchased from Hamilton Machinery Co....World War II ended...Victory bonds were cashed and a <b>Crawler</b> tractor with a carry-all scraper was purchased. The first motor patrol operator was hired at $125 per month. Snow removal became necessary so a V-plow attachment {{and a set of}} chains were added... Highway #14 had been reconstructed so the R.M. posted new signs for the towns at junctions..|$|E
5|$|Treblinka {{was divided}} into two {{separate}} camps that were 2 kilometres apart. Two engineering firms, the Schoenbronn Company of Leipzig and the Warsaw branch of Schmidt–Munstermann, oversaw the construction of both camps. Between 1942 and 1943 the extermination centre was further redeveloped with a <b>crawler</b> excavator. New gas chambers made of brick and cement mortar were freshly erected, and mass cremation pyres were also introduced. The perimeter was enlarged to provide a buffer zone, {{making it impossible to}} approach the camp from the outside. The number of trains caused panic among the residents of nearby settlements. They would likely have been killed if caught near the railway tracks.|$|E
5|$|Development of System Shock 2 {{began in}} 1997 when Looking Glass Studios {{approached}} Irrational Games {{with an idea}} to co-develop a new game. The development team were fans of System Shock and sought to create a similar game. Early story ideas were similar to the novella Heart of Darkness. In an early draft, the player was tasked with assassinating an insane commander on a starship. The original title of the game, according to its pitch document, was Junction Point. The philosophy of the design was to continue to develop {{the concept of a}} dungeon <b>crawler,</b> like , in a science fiction setting, the basis for System Shock. However, the press mistook System Shock to be closer to a Doom clone which was cited for poor financial success of System Shock. With Junction Point, the goal was to add in significant role-playing elements and a persistent storyline as to distance the game from Doom.|$|E
40|$|<b>Crawlers</b> are {{software}} {{which can}} traverse {{the internet and}} retrieve webpages by hyperlinks. In {{the face of the}} inundant spam websites, traditional web <b>crawlers</b> cannot function well to solve this problem. Semantic focused <b>crawlers</b> utilize semantic web technologies to analyze the semantics of hyperlinks and web documents. This paper briefly reviews the recent studies on one category of semantic focused <b>crawlers</b> - ontology-based focused <b>crawlers,</b> which are a series of <b>crawlers</b> that utilize ontologies to link the fetched web documents with the ontological concepts (topics). The purpose of this is to organize and categorize web documents, or filtering irrelevant webpages with regards to the topics. A brief comparison are made among these <b>crawlers,</b> from six perspectives - domain, working environment, special functions, technologies utilized, evaluation metrics and evaluation results. The conclusion with respect to this comparison is made in the final section...|$|R
30|$|Besides {{the data}} of normal users, we include crawlers’ data in the {{training}} set by implementing 6 internal <b>crawlers</b> to crawl the system. We build internal <b>crawlers</b> provided by Frontera (Frontera 0.3), which relies on Scrapy (Scrapy 1.0). The 6 <b>crawlers</b> are depth-first, depth-first with delay, breadth-first, breadth-first with delay, random-like, and random-like with delay. The random-like <b>crawlers</b> randomly choose a link to visit from all links they gathered and put newly gained links in the link pool.|$|R
40|$|Nowadays, the {{research}} of <b>crawlers</b> moves closer to the semantic web, along {{with the appearance of}} increasing XML/RDF/OWL files and the rapid development of ontology mark-up languages. As an emerging concept, metadata abstraction <b>crawlers</b> are a series of <b>crawlers</b> that aim to abstract metadata from normal HTML documents, based on various semantic web technologies. In this paper, we make a general survey of the current situation of metadata abstraction <b>crawlers.</b> Fourteen cases in this field are chosen as typical examples, and classified in five clusters. From seven perspectives we horizontally compare and contrast the semantic web <b>crawlers</b> in each cluster, and draw our conclusion in the final section...|$|R
5|$|To {{convert the}} {{backlink}} data gathered by BackRub's web <b>crawler</b> into {{a measure of}} importance for a given web page, Brin and Page developed the PageRank algorithm, and realized {{that it could be}} used to build a search engine far superior to existing ones. The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one Web page to another, and allowed the number of links and their rank, to determine the rank of the page. Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network. After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.|$|E
5|$|In 2006 and 2007, {{four more}} games were {{released}} {{as part of}} the World of Mana subseries, an attempt by Square Enix to release games in a series over a variety of genres and consoles. These were Children of Mana (2006), an action-oriented dungeon <b>crawler</b> game for the Nintendo DS; Dawn of Mana (2006), a 3D action-adventure game for the PlayStation 2; Friends of Mana (2006), a Japan-only multiplayer role-playing game for mobile phones; and Heroes of Mana (2007), a real-time strategy game for the DS. Children was developed by Nex Entertainment and Heroes by Brownie Brown, founded by several developers of Legends, though Ishii oversaw development of all four games. Three more games have been released since the World of Mana subseries ended: Circle of Mana (2013), a Japan-only card battle game for the GREE mobile platform, Rise of Mana (2014), a Japan-only free-to-play action role-playing game for iOS, Android, and PlayStation Vita, and Adventures of Mana (2016), a 3D remake of Final Fantasy Adventure for the PlayStation Vita, iOS, and Android. In addition to the games, four manga series and one novelization have been released in the Mana franchise.|$|E
5|$|In the foundry, Joule is able {{to analyze}} the {{prismatic}} core. She can make out voices coming from the core – including her father's – thus deducing {{that it is a}} transmission of sorts. Heading to Eden Tower to decrypt the core's transmission, she agrees to meet Kai there. Once reunited, they are ambushed by Victor. Kai stays behind to provide Joule the time to escape. She finds a lost <b>crawler</b> filled with parietal art indicating that Victor had manipulated the other corebots to attack the maintenance habitats. Corebot Duncan enters, lamenting the death of his human companion, and unites with Joule against Victor. Joule approaches Eden Tower to activate it, only to be faced with Victor and his minions. She learns that the ships, once orbiting Far Eden while waiting for its terraforming to complete, are long gone, having been destroyed by Victor. He throws Kai's prosthetic leg to the ground, proclaiming him dead. Finally, Joule bests Victor in combat and activates the terraforming system, materializing a hologram of her father from parts of memories hidden in the cores. In the end, it becomes clear that Kai survived his confrontation with Victor.|$|E
5000|$|The Creepy <b>Crawlers</b> TV Show {{was based}} on ToyMax's Creepy <b>Crawlers</b> Activity toy. A line of 12 action figures were made by ToyMax in {{conjunction}} with the show, as well as the Goozooka Assault vehicle. A [...] "Creepy <b>Crawlers</b> Action Figure Playset" [...] was depicted in the 1994 ToyMax toy booklet, but was apparently not produced. Each figure came with a metal mold for use with the Creepy <b>Crawlers</b> toy oven, to make custom accessories for the figure using Plasti-Goop.|$|R
5000|$|The {{most common}} web {{archiving}} technique uses web <b>crawlers</b> to automate {{the process of}} collecting web pages. Web <b>crawlers</b> typically access web pages {{in the same manner}} that users with a browser see the Web, and therefore provide a comparatively simple method of remote harvesting web content. Examples of web <b>crawlers</b> used for web archiving include: ...|$|R
5|$|Temp: One of two <b>crawlers</b> (cockroaches) to {{join the}} quest. He {{is one of the}} first <b>crawlers</b> who {{encounters}} Boots and her brother and befriends the two.|$|R
5|$|The Binding of Isaac is a {{top-down}} dungeon <b>crawler</b> game, presented using two-dimensional sprites, {{in which}} the player controls Isaac or other unlockable characters as they explore the dungeons located in Isaac's basement. The characters differ in speed, amount of health, amount of damage they deal, and other attributes. The game's mechanics and presentation {{is similar to the}} dungeons of The Legend of Zelda, while incorporating random, procedurally-generated levels {{in the manner of a}} roguelike game. On each floor of the basement dungeon, the player must fight monsters in a room before continuing onto the next room. This is most commonly done by the character's tears as bullets in the style of a twin-stick shooter, but the player can also use a limited supply of bombs to damage enemies and clear out parts of the room. Other methods of defeating enemies become possible as the character gains power-ups, items that are automatically worn by the player-character when picked up that can alter the character's core attributes, such as increasing health or the strength of each tear, or cause additional side effects, such as for allowing charged tear shots to be fired after holding down a controller button for a short while, or a means to fire tears behind the character. Power-ups include passive items that improve the character's attributes automatically, active power-ups that can be used once before they are recharged by completing additional rooms in the dungeon, and single-use power-ups such as pills or Tarot cards that confer a one-time benefit when used, such as regaining full health, or increasing or decreasing all attributes of the character. The effect of power-ups stack, so that the player may come into highly-beneficial power-up combinations.|$|E
5|$|Unlike {{previous}} {{games in}} the series, which were more typical action role-playing games, Children of Mana is a dungeon <b>crawler,</b> {{and the majority of}} the gameplay takes place in selected locations rather than on an open world map. The player selects these areas on the world map to reach them. The primary objective in each location is to clear the dungeon of monsters. Each dungeon is divided into different randomly generated floors, and to progress between each zone, the player must find an item called a Gleamdrop, then carry it to a pillar of light called a Gleamwell. The player must repeat this process on each floor of the dungeon until the last floor is reached, where a boss monster lies. The player can not return to previous floors unless they die or leave the dungeon; upon returning, they start the dungeon over at the beginning. When not clearing dungeons, the player stays in the Mana Village, which contains shops to purchase equipment. Dungeons can be returned to later by accepting quests from townsfolk in the Dudbear shop. During these quests, the dungeon itself is slightly altered: the player's starting position may be different, the number of floors can change, and the monsters and boss monster contained may change. Like the main quests, Dudbear quests involve clearing the dungeon of monsters, sometimes to acquire an item {{from the end of the}} dungeon.|$|E
25|$|FAST <b>Crawler</b> is a {{distributed}} <b>crawler.</b>|$|E
50|$|Sarah, Juno, and Rios {{reach the}} exit, but are {{blocked by a}} group of <b>crawlers</b> led by their large leader. They try to quietly sneak past the <b>crawlers,</b> but Greg, who is dying from his injuries, appears and grabs Juno's leg in a last effort to save himself. This causes her to scream in {{surprise}} and attract the attention of the <b>crawlers.</b> Greg dies and the women are left to fight them off. After all the <b>crawlers</b> are killed, Sarah tries to rescue Juno from the leader, but it slashes Juno's stomach, mortally wounding her. Sarah then kills it before Juno dies in her arms. More <b>crawlers</b> arrive, but Sarah draws their attention to herself, giving Rios a chance to escape.|$|R
40|$|Web <b>crawlers</b> visit {{internet}} applications, collect data, {{and learn}} about new web pages from visited pages. Web <b>crawlers</b> have a long and interesting history. Early web <b>crawlers</b> collected statistics about the web. In addition to collecting statistics about the web and indexing the applications for search engines, modern <b>crawlers</b> {{can be used to}} perform accessibility and vulnerability checks on the application. Quick expansion of the web, and the complexity added to web applications have made the process of crawling a very challenging one. Throughout the history of web crawling many researchers and industrial groups addressed different issues and challenges that web <b>crawlers</b> face. Different solutions have been proposed to reduce the time and cost of crawling. Performing an exhaustive crawl is a challenging question. Additionally capturing the model of a modern web application and extracting data from it automatically is another open question. What follows is a brief history of different technique and algorithms used from the early days of crawling up to the recent days. We introduce criteria to evaluate the relative performance of web <b>crawlers.</b> Based on these criteria we plot the evolution of web <b>crawlers</b> and compare their performanc...|$|R
2500|$|As {{noted by}} Koster, {{the use of}} Web <b>crawlers</b> is useful {{for a number of}} tasks, but comes with a price for the general community. The costs of using Web <b>crawlers</b> include: ...|$|R
25|$|Cho uses 10 seconds as an {{interval}} for accesses, and the WIRE <b>crawler</b> uses 15 seconds as the default. The MercatorWeb <b>crawler</b> follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the <b>crawler</b> waits for 10t seconds before downloading the next page. Dill et al. use 1 second.|$|E
25|$|The {{objective}} of the <b>crawler</b> {{is to keep the}} average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the <b>crawler</b> is just concerned with how many pages are out-dated, while in the second case, the <b>crawler</b> is concerned with how old the local copies of pages are.|$|E
25|$|Coffman et al. {{worked with}} a {{definition}} of the objective of a Web <b>crawler</b> that is equivalent to freshness, but use a different wording: they propose that a <b>crawler</b> must minimize the fraction of time pages remain outdated. They {{also noted that the}} problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web <b>crawler</b> is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web <b>crawler.</b>|$|E
30|$|Machine {{learning}} detection. Our {{test set}} contains both data from users and <b>crawlers.</b> Besides the 6 <b>crawlers</b> {{we use to}} generate test data, we find two external <b>crawlers.</b> One is Googlebot {{and the other one}} is Yahoo Slurp. We believe they are the only two search engines that try to crawl our system by manually checking all public visitors that generate relatively abundant access logs.|$|R
40|$|WWW is a {{collection}} of hyperlink document available in HTML format [10]. This collection is very huge and thus difficult to refresh quickly because 40 % of the web pages changes daily. As the web has dynamic nature, so, to cover more and more pages on the web, the concept of parallel <b>crawlers</b> are used. These <b>crawlers</b> run in parallel and cover the wider area of the web. But the parallel <b>crawlers</b> consume more resources especially bandwidth of the network to keep the repository up to date. So this paper proposes an architecture that uses the concept of mobile <b>crawlers</b> to fetch the web page and download only those pages that are changed since the last crawl. These <b>crawlers</b> run in the specific domain and skip the irrelevant domains...|$|R
30|$|In this paper, {{we develop}} a new anti-crawler {{mechanism}} called PathMarker that aims to detect and constrain persistent distributed inside <b>crawlers,</b> which have valid user accounts to stealthily scrape valuable website content. Moreover, we manage to accurately detect those armoured <b>crawlers</b> at their earliest crawling stage. The basic idea is based on one observation that the normal users and malicious <b>crawlers</b> have different short-term and long-term download behaviours. In other words, <b>crawlers</b> have similar accessing patterns regarding to the path (e.g., depth-first, width-first, or random access) in both the short-term and the long-term; while human beings have obvious accessing patterns only {{in the short term}} and have no certain patterns in the long term. This is because <b>crawlers</b> are working based on certain crawling algorithms, and once the algorithm is chosen, the crawling paths would follow certain pattern.|$|R
25|$|Xapian, {{a search}} <b>crawler</b> engine, written in c++.|$|E
25|$|Dick Tracy Meets The Night <b>Crawler.</b> Whitman, hardcover, 1945.|$|E
25|$|Sphinx (search engine), a free search <b>crawler,</b> {{written in}} c++.|$|E
40|$|The {{growth of}} the World Wide Web (WWW) has seen it evolve into a rich {{information}} resource. It is constantly traversed {{with the aid of}} <b>crawlers</b> so as to harvest web content. When collecting data, <b>crawlers</b> have the potential of causing service denial to web servers. This paper proposes the application of sampling as a selection strategy in the design of structural analysis web <b>crawlers.</b> This has the benefit of alleviating the problems of bandwidth costs to web servers whilst retaining the quality of the data that is mined by <b>crawlers.</b> The initial results of this study are promising and are presented in this paper...|$|R
5000|$|Untethered robotic ILI <b>crawlers</b> are {{powered by}} onboard batteries; these tools {{transmit}} sensor data wirelessly to the tool operator or store {{the data for}} downloading upon tool retrieval. Untethered <b>crawlers</b> have the following advantages over tethered crawlers: ...|$|R
40|$|<b>Crawlers</b> are {{deployed}} by a Web search engine for collecting information from different Web servers {{in order to}} maintain the currency of its data base of Web pages. We present studies on the optimization of Web search engines from different perspectives. We first investigate the number of <b>crawlers</b> to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network. Both the static setting, where <b>crawlers</b> are always active, and the dynamic setting where, <b>crawlers</b> may be activated/deactivated as a function of the state of the system, are addressed. We then consider the optimal scheduling of the visits of these <b>crawlers</b> to the Web pages assuming these pages are modified at different rates. Finally, we briefly discuss some other optimization issues of Web search engines, including page ranking and system optimization...|$|R
