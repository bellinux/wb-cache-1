19|26|Public
50|$|In the eighties, chess play by email {{was still}} fairly novel. Latency with email {{was less than}} with {{traditional}} correspondence chess via paper letters. Often one could complete a dozen moves in a week. As network technology improved, public, widespread use of a <b>centralised</b> <b>server</b> for live play became a possibility.|$|E
50|$|It was {{originally}} developed for web deployment of applications, where a user would click on an 'Install with SoftwareValet' link on a website, and the BeOS web browser at the time, NetPositive, would launch SoftwareValet. It also handled product registration and software updates through a (now defunct) <b>centralised</b> <b>server,</b> BeDepot (bedepot.com).|$|E
50|$|The school {{maintains}} a central computer laboratory with a <b>centralised</b> <b>server.</b> In addition, {{there are four}} departmental modular computer laboratories with a separate broadband connection for every terminal. The computers are connected through the ERNET Internet backbone. The campus has an intranet facility for file sharing and communication, and has campus-wide Wi-Fi.|$|E
50|$|Pull {{requests}} {{form the}} foundation of network computing, where many clients request data from <b>centralised</b> <b>servers.</b> Pull is used extensively on the Internet for HTTP page requests from websites.|$|R
50|$|The {{centralised}} architecture {{model is}} a relatively simple and easy to manage solution. Because all media content is stored in <b>centralised</b> <b>servers,</b> {{it does not require}} a comprehensive content distribution system. Centralised architecture is generally good for a network that provides relatively small VOD service deployment, has adequate core and edge bandwidth and has an efficient content delivery network (CDN).|$|R
50|$|Wire Swiss GmbH {{launched}} the Wire app on 3 December 2014. Shortly after its launch, the company retracted a claim from their website that the app's messages and conversation history {{could only be}} read by the conversation participants. In August 2015, the company added group calling to their app. From its launch until March 2016, Wire's messages were only encrypted between the client and the company's server. In March 2016, the company added end-to-end encryption for its messaging traffic, {{as well as a}} video calling feature. Wire Swiss GmbH released the source code of the Wire client applications under the GPLv3 licence in July 2016. The company also published a number of restrictions that apply to users who have compiled their own applications. Among other things, they may not change the way the applications connect and interact with the company's <b>centralised</b> <b>servers.</b>|$|R
50|$|Where {{the subject}} is {{required}} to record data (e.g. daily symptoms) then a diary is provided for completion. Data management of this data requires {{a different approach to}} CRF data as, for example, it is generally not practical to raise data queries.Patient diaries may be developed in either paper or electronic (eDiary) formats. Such eDiaries generally {{take the form of a}} handheld device which enables the subject to enter the required data and transmits this data to a <b>centralised</b> <b>server.</b>|$|E
50|$|Centralisation is an {{important}} feature for organizations concerned with secure data being stored on remote devices such as notebook computers, and the associated risk for theft of the device and its data. Applications accessed via SGD run in the <b>centralised</b> <b>server</b> room, meaning that all data is backed up and secured via the normal datacenter practices of the organization. There is a potential for increased performance and effiiciency, since the actual computation is performed on larger systems with more resources; centralisation also makes resources considerably easier to manage.|$|E
5000|$|Compared to a tiled raster map, data {{transfer}} is also greatly reduced, as vector data is typically {{much smaller than}} a rendered bitmap. Also, styling can be applied later in the process, {{or even in the}} browser itself, allowing much greater flexibility in how data is presented. It is also easy to provide interactivity with map features, as their vector representation already exists within the client. Yet another benefit is that less <b>centralised</b> <b>server</b> processing power is required, since rasterisation can be performed directly in the client. This has been described as making [...] "rendering ... a last-mile problem, with fast, high-quality GPUs in everyone’s pocket".|$|E
40|$|Abstract. The {{increasing}} {{ubiquity of}} context aware services and sys-tems has been primarily underpinned {{by the use}} of <b>centralised</b> <b>servers</b> employing protocols that do no scale well for real time distribution and acquisition of neither sensor data nor dependent services. Any shift from this generic sensor framework mandated a new thinking where sensor data was capable of being propagated in real time using protocols and data models which serve to reduce unnecessary communication over-head. DCXP is proposed as an alternative architecture for the real time distribution of context information to ubiquitous mobile services. As a P 2 P based distributed protocol, it inherently poses the challenge of user anonymity across the system. In this paper we briefly present DCXP along with further work to enable the anonymised dissemination of sen-sor information within the architecture. Such a solution would have a negligible impact on the overall scalability and performance of DCXP. ...|$|R
40|$|The {{increasing}} {{popularity of}} Massively Multiplayer Online Games (MMOG) - games involving thousands of players participating simultaneously {{in a single}} virtual world - has highlighted the scalability bottlenecks present in centralised Client/Server (C/S) architectures. Researchers are proposing Peer-to-Peer (P 2 P) game technologies as a scalable alternative to C/S; however, P 2 P is more vulnerable to cheating as it decentralises the game state and logic to un-trusted peer machines, rather than using trusted <b>centralised</b> <b>servers.</b> Cheating {{is a major concern}} for online games, as a minority of cheaters can potentially ruin the game for all players. In this paper we present a review and classification of known cheats, and provide real-world examples where possible. Further, we discuss counter measures used by C/S game technologies to prevent cheating. Finally, we discuss several P 2 P architectures designed to prevent cheating, highlighting their strengths and weaknesses...|$|R
40|$|Efficiently {{executing}} large-scale, data-intensive workflows such as Montage {{must take}} into account the volume and pattern of communication. When orchestrating data-centric workflows, <b>centralised</b> <b>servers</b> common to standard workflow systems can become a bottleneck to performance. However, standards-based workflow systems that rely on centralisation, e. g., Web service based frameworks, have many other benefits such as a wide user base and sustained support. This paper presents and evaluates a light-weight hybrid architecture which maintains the robustness and simplicity of centralised orchestration, but facilitates choreography by allowing services to exchange data directly with one another. Furthermore our architecture is standards compliment, flexible and is a non-disruptive solution; service definitions {{do not have to be}} altered prior to enactment. Our architecture could be realised within any existing workflow framework, in this paper, we focus on a Web service based framework. Taking inspiration from Montage, a number of common workflow patterns (sequence, fan-in and fan-out), input to output data size relationships and network configurations are identified and evaluated. The performance analysis concludes that a substantial reduction in communication overhead results in a 2 – 4 fold performance benefit across all patterns. An end-to-end pattern through the Montage workflow results in an 8 fold performance benefit and demonstrates how the advantage of using our hybrid architecture increases as the complexity of a workflow grows...|$|R
40|$|This report gives a novel {{technique}} of image encryption and authentication by combining elements of Visual Cryptography and Public Key Cryptography. A prominent attack involving generation of fake shares to cheat honest users {{has been described}} and {{a demonstration of the}} proposed system employing a <b>centralised</b> <b>server</b> to generate shares and authenticate them on the basis of requests is made as a counter to the described attack...|$|E
40|$|The {{introduced}} {{concept of}} a gesture based sign language chat enables the communication of hearing impaired people over networks, including concerns regarding low throughput and bandwidth characteristics. Existing applications handle that task not appropriate, since either high bandwidth networks are necessary or the extensibility of the vocabulary is complicated for untrained users. Thereby our approach facilitates existing image recognition techniques for the determination of gestures which are captured by a standard camera. The recognised gestures then are mapped to a gesture Id that is transferred to the participants, where it is rendered. Additionally, the uncomplicated extension of the vocabulary by the users is supported. Therefore a <b>centralised</b> <b>server</b> updates the locally stored dictionaries, if necessary. ...|$|E
40|$|We {{propose to}} {{demonstrate}} a mobile server assisted P 2 P system for on-demand video streaming. Our proposed solution uses a combination of 3 G and ad-hoc Wi-Fi connections, to enable mobile devices to download content from a <b>centralised</b> <b>server</b> {{in a way that}} minimises the 3 G bandwidth use and cost. On the customised GUI, we show the corresponding reduction in 3 G bandwidth achieved by increasing the number of participating mobile devices in the combined P 2 P and ad-hoc Wi- Fi network, while demonstrating the good video playout quality on each of the mobiles. We also demonstrate the implemented trust mechanism which enables mobiles to only use trusted adhoc connections. The system has been implemented on Android based smartphones. Comment: Published as demo in Local Computer Network conference (LCN 2011...|$|E
40|$|The group {{intends to}} develop a Long-term Patient Monitoring Research Platform to support and assist health-care providers. The Research platform, which {{does not intend to}} rely on a central server or a {{continuously}} connected Internet connection, aims to act as a low-cost support tool to aid health-care providers of the future. In 2005, The World Health Organisation stated that 60 % of worldwide mortality is due to chronic disease [1], while 80 % of chronic diseases occur in low and middle income countries. Chronic disease, along with a global aging population, a population that is becoming obese [2], and worldwide pandemics (such as HIV/AIDS) combined with high-in-demand healthcare providers[3] and increased costs of healthcare [4], paint a disastrous picture for the future of healthcare. The research platform is therefore intended to not rely on a central server, but rather on a distributed intelligent network of the sensor nodes. By investigating modern parallel processing techniques [6] and distributed algorithms for data fusion (such as [7]), along with novel, low-cost sensors for physiological measurements, the research platform will be compared against traditional methods of <b>centralised</b> <b>servers</b> and the client-server model. The research platform's initial investigations will involve attempting to find patterns in signals for diagnosis and quickly identifying emergency events during long-term monitoring. For further information, please visi...|$|R
40|$|This paper {{describes}} {{the development of}} a distributed knowledge-based system. A software system, namely Distributed Algorithmic and Rule-based Blackboard System (DARBS), was developed from its predecessor ARBS, which lacked the distributed computing feature. ARBS has been used in solving a number of engineering problems [1 - 3]. DARBS now utilises client/server technology. It consists of a <b>centralised</b> database <b>server,</b> called the 'Blackboard' and a number of Knowledge Source Clients (experts). It distributes the workload to a number of clients which are rule-based or other AI systems with specific knowledge in various areas. DARBS is being applied to automatic interpretation of non-destructive evaluation (NDE) data and control of plasma deposition processes...|$|R
30|$|Another trend {{which is}} clear from Figure 4 is the shift from tools utilising a pull {{mechanism}} for collecting monitoring data to using a push model. This represents a move from tight coupling and manual configuration to loosely coupled tools which perform auto discovery and handle membership change gracefully. Pull mechanism provided a means for <b>centralised</b> monitoring <b>servers</b> to control {{the rate at which}} data was collected and control the volume of information produced at any given time. Push mechanism dispense with that means of control and either require an additional feedback mechanism to reintroduce this functionality or require the monitoring system to cope with variable and uncontrolled rates of monitoring data.|$|R
40|$|Current {{computing}} techniques {{using the}} cloud as a <b>centralised</b> <b>server</b> will become untenable as billions of devices get {{connected to the}} Internet. This raises the need for fog computing, which leverages computing {{at the edge of}} the network on nodes, such as routers, base stations and switches, along with the cloud. However, to realise fog computing the challenge of managing edge nodes will need to be addressed. This paper is motivated to address the resource management challenge. We develop the first framework to manage edge nodes, namely the Edge NOde Resource Management (ENORM) framework. Mechanisms for provisioning and auto-scaling edge node resources are proposed. The feasibility of the framework is demonstrated on a PokeMon Go-like online game use-case. The benefits of using ENORM are observed by reduced application latency between 20...|$|E
40|$|Abstract—When {{orchestrating}} data-centric workflows as {{are commonly}} found in the sciences, centralised servers can become a bottleneck {{to the performance of}} a workflow; output from service invocations are always transferred via a centralised orchestration engine, when they should be passed directly to where they are needed at the next service in the workflow. To address this performance bottleneck, this paper presents a lightweight Web services architecture and concrete API, based on a centralised control flow, distributed data flow model. Our architecture maintains the robustness and simplicity of centralised orchestration, but facilitates choreography by allowing services to exchange data directly with one another, reducing data that needs to be transferred through a <b>centralised</b> <b>server.</b> Furthermore, our architecture is a flexible, non-intrusive solution, as existing service definitions {{do not have to be}} altered prior to enactment. Index Terms—Systems architecture, workflow optimisation, Web services, decentralised orchestration...|$|E
40|$|AbstractThe hot-tempered {{development}} of hassle on big data processing make obligatory an intense load on computation, storage and networking in data centres. We suggested an approach of data centre node clustering for an efficient data placement and data retrieval which is unlike the routine in centralised architecture. The main objective {{for the proposed}} system is the shortcomings present in the conventional <b>centralised</b> <b>server</b> which is mainly the assumption that a single head is in the connectivity range of all other nodes. We proposed Hit Rate Geographical Locations Analysis Algorithm (HIRGLAA) for the dynamic election of cluster head based on the periodic hit rate analysis performance. We suggested candidate cluster heads containing redundant routing information to ensure data storage backup. Thus the proposed system assures Quality of Services (QoS) such as increased reliability, robustness, an energy efficient remote access and its efficiency can be validated by extensive simulation based studies...|$|E
40|$|This is a {{presentation}} showing how an intelligence software {{system could be}} employed to detect hidden cracks in a flat ferritic steel plates using the non-destructive evaulation technique. DARBS, a distributed blackboad system, had been successfully applied to the interpretation of ultrsound (B-scan) images from weld defects in flat ferritic steel plates. Based around the client/server model, DARBS comprises a <b>centralised</b> database <b>server,</b> i. e. the blackboard, {{and a number of}} knowledge source clients. As the clients are separate processes, possibly on separate networked computers, they can contribute to the solution of a problem whenever they have a contribution to make. DARBS therefore achieves the well-established but elusive ideal of opportunism. It behaves as a distributed agent-based system, with the proviso that all communication is via the blackboard...|$|R
40|$|Recently {{there has}} been an {{increasing}} deployment of content distribution networks (CDNs) offering hosting services to Web content providers. CDNs deploy a set of surrogate servers distributed throughout the Internet and replicate provider content across these servers to provide better performance and availability than <b>centralised</b> provider <b>servers</b> do. Problems regarding robustness, scalability, accessibility and efficiency however, brought the attention to peer-to-peer (P 2 P) architectures for use in CDNs. In P 2 P architectures, every node acts as a client and as a server. This way, intelligence is spread over the network, making self-organisation and automatic recovery more easy. In this paper, we present several replica placement algorithms (RPAs) for peer-to-peer CDNs. We will show that they enhance CDN performance by determining the optimal location of content replicas on the surrogate servers...|$|R
40|$|One of the {{fundamental}} challenges in building Peer-to-Peer (P 2 P) applications is to locate resources across a dynamic set of nodes without <b>centralised</b> <b>servers.</b> Structured overlay networks solve this challenge by proving a key-based routing (KBR) layer that maps keys to nodes. The performance of KBR is strongly influenced by the dynamic and unpredictable conditions of P 2 P environments. To cope with such conditions a node must maintain its routing state. Routing state maintenance directly influences both lookup latency and bandwidth consumption. The more vigorously that state information is disseminated between nodes, the greater the accuracy and completeness of the routing state and the lower the lookup latency, but the more bandwidth that is consumed. Existing structured P 2 P overlays provide a set of configuration parameters {{that can be used}} to tune the trade-off between lookup latency and bandwidth consumption. However, the scale and complexity of the configuration space makes the overlays difficult to optimise. Further, it is increasingly difficult to design adaptive overlays that can cope with the ever increasing complexity of P 2 P environments. This thesis is motivated by the vision that adaptive P 2 P systems of tomorrow, would not only optimise their own parameters, but also generate and adapt their own design. This thesis studies the effects of using an adaptive technique to automatically adapt state dissemination cost and lookup latency in structured overlays under churn. In contrast to previous adaptive approaches, this work investigates the algorithmic adaptation of {{the fundamental}} data dissemination protocol rather than tuning the parameter values of a protocol with fixed design. This work illustrates that such a technique can be used to design parameter-free structured overlays that outperform other structured overlays with fixed design such as Chord in terms of lookup latency, bandwidth consumption and lookup correctness. A large amount of experimentation was performed, more than the space allows to report. This thesis presents a set of key findings. The full set of experiments and data is available online at: [URL]...|$|R
40|$|As {{billions of}} devices get {{connected}} to the Internet, {{it will not be}} sustainable to use the cloud as a <b>centralised</b> <b>server.</b> The way forward is to decentralise computations away from the cloud towards the edge of the network closer to the user. This reduces the latency of communication between a user device and the cloud, and is the premise of 'fog computing' defined in this paper. The aim {{of this paper is to}} highlight the feasibility and the benefits in improving the Quality-of-Service and Experience by using fog computing. For an online game use-case, we found that the average response time for a user is improved by 20 % when using the edge of the network in comparison to using a cloud-only model. It was also observed that the volume of traffic between the edge and the cloud server is reduced by over 90 % for the use-case. The preliminary results highlight the potential of fog computing in achieving a sustainable computing model and highlights the benefits of integrating the edge of the network into the computing ecosystem. Comment: 8 page...|$|E
40|$|Hospitals {{represent}} a busy environment when considering energy {{consumption in the}} building sector. Electricity represents a crucial powering source in hospitals including, but not limited to, lighting, catering and medical equipment. Hence, {{it is important to}} conserve it to ensure high quality of services that would enhance the patients’ health. This would also contribute to the reduction of the hospitals’ carbon footprint as well as the harmful impacts on the environment. The studies carried out in hospitals considered the use of renewable energy or the use of more power efficient equipment to tackle the energy problem. The challenge remains open in how to tackle the energy problem in a hospital by influencing pro-environmental behaviour. This study represents a step into reducing electricity costs of Medway NHS Foundation Trust (MNFT) in the UK. The proposed idea is to use technology to persuade MNFT staff to monitor their behaviour and with the right motive, from selected and appointed energy delegates, sustain a pro-environmental behaviour. Through combining technology, in the form of feedback on energy consumption and monitoring occupancy rates, and behavioural factors, represented in motivation from energy delegates and spreading awareness; it is possible to develop and sustain a proenvironmental behaviour among MNFT staff. This paper aims at describing the methodology proposed to reduce energy costs in MNFT by inducing pro-environmental behaviour with the aid of technology. This involves a smart electricity sub-metering system to collect and communicate energy data to a <b>centralised</b> <b>server</b> that pushes the data onto a dedicated web interface. Furthermore, addressing the psychological factors by appointing energy delegates to monitor the consumption, in selected areas, as well as motivate the staff members. In addition, monitoring human dynamics and analysing it against energy data. The study will include a baseline data collection period, for reference, followed by an experimental period to test and evaluate the system...|$|E
40|$|An ad hoc {{network is}} a {{peer-to-peer}} network without no <b>centralised</b> <b>server.</b> Mobile Ad Hoc Network(MANETs) is a promising new wireless communications paradigm in which network device may move around and end hosts may {{function as a}} router. It is a key of success of being deployed to properly address the security problems. Although many research focus on how to deliver packets from one node to another, research into the security will surely overtake the trend and play the leading role. Current techniques of addressing security on the fixed structured wired network are only useful to protect the transmitted message on the end nodes, the security of routing information among the mobile nodes in the hostile environment where mobile Ad Hoc networks are usually used [4] has been inadequately addressed. Security and routing on the wired networks have been treated separately due to that Security is concerned with how to defend nodes from hackers attack, while routing is about how to efficiently determine optimal routes. Recent research shown that the security and routing on mobile Ad hoc networks {{need to be considered}} altogether, that is, the routing needs to be done with the capability of preventing various attacks from compromised or malicious nodes. Towards a solution of secure routing on MANETs, we discuss a new scheme, which is reliant on the fuzzy logic, capable of determining the most secure route during the routing discovery in this paper. Similar to the AODV or SAODV in the route discovery, the algorithm will decide a secure path among all the possible routing paths based on the knowledge about its neighbour nodes. The nodes will forward or reply the route with the highest security level. The feasibility of the proposed scheme of secure routing will be demonstrated by using simulation on NS 2. In addition, the performance of the new secure routing protocol on the simulation experiments are presented...|$|E
40|$|Over {{the last}} couple of years, multi-player games have become more and more popular. Additionally, new mobile devices now have {{sufficient}} resources to play these multi-player games in mobile and wireless networks. As the classic <b>centralised</b> game <b>server</b> design is unsuited for mobile ad-hoc networks, a group of nodes can take the role of a distributed game server. This group of nodes can provide the necessary redundancy which is needed in the dynamic environment of a mobile ad-hoc network. In this paper we present a modified dominating set algorithm for server selection which uses local information to determine well-suited nodes from the group of players. Our algorithm uses three phases (discovery, determination and marking) to calculate an initial server set and adjusts to network changes during the game. We implemented our server selection algorithm in NS- 2 and evaluated its behaviour in two different realistic scenarios for mobile games (schoolyard and train) as well as in an artificial stress scenario. 1...|$|R
40|$|Proving-ground {{testing of}} Active Safety Systems is {{typically}} based on scenarios in which relative positioning {{and speed of}} all vehicles on the track is strictly defined. Current testing methods are not designed for human driven or autonomous Vehicles Under Test as they rely on the predictability of path-following driving robots to precisely control these parameters. As Active Safety Systems are both increasingly integrated with vehicle autonomy and are further augmenting human driving, these testing methods become less useful. This paper presents an approach for preserving most of the control of current methods while allowing for some level of uncertainty generated by an autonomous Vehicle Under Test. Utilising the master/ slave architectural pattern, slaved mock-up vehicles, or Test Targets, are synchronised with the Vehicle Under Test via a <b>centralised</b> Master <b>Server.</b> Using a design science methodology, this approach is described and then evaluated against the existing best practice. It represents a first move towards integrating autonomous vehicles into existing Active Safety Systems testing...|$|R
40|$|Prior to this work, an {{algorithmic}} and rule-based blackboard system (ARBS) {{had been}} developed over a ten-year period. ARBS benefited from a versatile rule structure {{and the ability to}} mix computational styles either as separate knowledgesources or by embedding algorithms within rules. It was a serial system – any knowledge source that was able to contribute had to wait its turn. We report here on a new distributed system, DARBS, in which the knowledge sources are parallel processes. Based around the client/server model, DARBS comprises a <b>centralised</b> database <b>server,</b> i. e. the blackboard, and a number of knowledge source clients. As the clients are separate processes, possibly on separate networked computers, they can contribute to the solution of a problem whenever they have a contribution to make. DARBS therefore achieves the well-established but elusive ideal of opportunism. It behaves as a distributed agent-based system, with the proviso that all communication is via the blackboard. DARBS is currently being applied to automatic interpretation of nondestructive evaluation (NDE) data and control of plasma deposition processes...|$|R
40|$|Network {{infrastructures}} {{often need}} to stage content {{so that it}} is accessible to consumers. The standard solution, deploying the content on a <b>centralised</b> <b>server,</b> can be inadequate in several situations. Our thesis is that information encoded in social networks can be used to tailor content staging decisions to the user base and thereby build better data delivery infrastructures. This claim is supported by two case studies, which apply social information in challenging situations where traditional content staging is infeasible. Our approach works by examining empirical traces to identify relevant social properties, and then exploits them. The first study looks at cost-effectively serving the ``Long Tail'' of rich-media user-generated content, which need to be staged close to viewers to control latency and jitter. Our traces show that a preference for the unpopular tail items often spreads virally and is localised to some part of the social network. Exploiting this, we propose Buzztraq, which decreases replication costs by selectively copying items to locations favoured by viral spread. We also design SpinThrift, which separates popular and unpopular content based on the relative proportion of viral accesses, and opportunistically spins down disks containing unpopular content, thereby saving energy. The second study examines whether human face-to-face contacts can efficiently create paths over time between arbitrary users. Here, content is staged by spreading it through intermediate users until the destination is reached. Flooding every node minimises delivery times but is not scalable. We show that the human contact network is resilient to individual path failures, and for unicast paths, can efficiently approximate flooding in delivery time distribution simply by randomly sampling a handful of paths found by it. Multicast by contained flooding within a community is also efficient. However, connectivity relies on rare contacts and frequent contacts are often not useful for data delivery. Also, periods of similar duration could achieve different levels of connectivity; we devise a test to identify good periods. We finish by discussing how these properties influence routing algorithms. This work was supported by a St. John's College Benefactor's Scholarship and a Research Studentship from the Cambridge Philosophical Society...|$|E
40|$|Modern Computer-Supported Cooperative Work (CSCW) {{applications}} {{are driven by}} intensive graphical user interfaces. Groupware widgets, such as shared scrollbars and text editors, are common in these applications to facilitate synchronisation of shared information across a group of users. These widgets differ from their single-user counterparts {{in the way that}} actions performed on them must be readily reflected on the displays across a user group. Many of these groupware widgets are converted from their single-user counterparts by adding facilities for coupling the widget properties. The conversion involves determination on the properties by which groupware widgets should be synchronised, under what circumstances and to what extent they are synchronised. These properties define the coupling of groupware widgets. Flexibility in coupling, therefore, refers to the degrees of coupling that the CSCW system allows. A promising approach to supporting flexible coupling in CSCW systems is the use of notification service. In this approach, the shared data model is kept in a notification server and replicated by a number of replicas located at the client sides. Each replica receives state-change notification messages from the notification server and determines a specific presentation of the data. This approach requires notification event handling at the client sides. However, handling notification events at the client sides makes it difficult to have a central management over the coupling behaviours of a CSCW system. This central management is useful when the changes of the collaboration patterns are required to take place synchronously. In addition, the diversity of the collaboration behaviours, which are often affected by different characteristics of group behaviours, necessitates the ability of clients to respond to pattern changes. Therefore, it is beneficial to abstract the coupling pattern of the system and put this piece of information in a central repository. By means of this repository, dynamic changes in collaboration patterns can be achieved by modifying the installed coupling information in the course of collaboration. To support central management of coupling patterns and maintain a high degree of flexibility, a new framework for notification service that allows coupling behaviours to be dynamically controlled in a <b>centralised</b> <b>server</b> is proposed. In addition, this framework allows the coupling pattern to be utilised and modified over time, which enables the system to adapt to various collaboration needs and group tailoring. This thesis will highlight the details of the implementation of the proposed framework. A groupware web browser has also been developed on top of it to demonstrate how flexible coupling can be implemented and managed in practice. This thesis also evaluates the performance and discusses on the scalability issue...|$|E
40|$|The sale of prepaid {{electricity}} is prevalent in South Africa {{due to the}} current economic, social, and political conditions. The system currently used for the distribution of tokens for prepaid electricity, CVS, has a design flaw that leads to many security vulnerabilities. The design flaw is that the security devices that generate the tokens are distributed {{in the field and}} operate independently of centralised control. This was done because of the limited communication infrastructure in South Africa 10 years ago, but is no longer necessary. An improvement to the system is suggested that removes the security vulnerabilities by making the system on-line. By employing the communication infrastructure that is available today to provide access to the security devices, the security devices can be located in a secure environment. Changing the mode of operation to on-line also has other advantages such as simplifying auditing and removing synchronisation problems. This improved system works by communicating on-line with a <b>centralised</b> <b>server</b> and database for every transaction that a customer makes. By doing this, all of the parties involved are kept up to date with the most recent transactions. There can no longer be financial discrepancies and the risk of all parties involved is thus reduced. It is no longer meaningful to steal the vending machines because they no longer have the ability to generate tokens independently. In order to implement such a system, however, there are many security aspects that need to be addressed, such as the confidentiality of the information within the system and proving that a transaction did occur between two specific parties. To this end, cryptographic functions and protocols are selected that meet the requirements of the system. Public key cryptography was found to be a necessary ingredient in making the system work effectively and efficiently. In order to use public key cryptography in the new system, Public Key Infrastructure is required to manage public keys and provide authentication services. A suitable system is developed and described that employs certificate authorities and X. 509 certificates. The procedures that are required from each party are listed. A set of messages that is required for the functions of the system is given. For each message, the contents of the message is given, the parts of the message that must be encrypted are defined and the parts of the message that must be digitally signed are given. Finally, the security of the individual parts of the system is critically analysed to show that all of the design goals have been achieved. Particular attention is given to the authentication of parties involved in the communication. The security of the system as a whole is also evaluated with respect to the X. 810 security framework and it is shown that the system is robust from a security perspective. The result of the research is a system that meets the required functionality to replace the existing system, {{and at the same time}} meets all of the security requirements. It is shown that the proposed system does not have the security flaws of the existing system and thus is more effective in its purpose of vending prepaid electricity. Dissertation (MEng (Electronic)) [...] University of Pretoria, 2007. Electrical, Electronic and Computer Engineeringunrestricte...|$|E
40|$|The paper {{develops}} a distributed systems architecture for dependable Internet based online auctions, {{meeting the requirements}} of data integrity, responsiveness, fairness and scalability. Current auction services essentially rely on a <b>centralised</b> auction <b>server.</b> Such an approach is fundamentally restrictive with respect to scalability. It is well-known that a tree-based, recursive design approach caters well for scalability requirements. With this observation in mind, the paper {{develops a}}n approach that permits an auction service to be mapped on to globally distributed auction servers. The paper selects a suitable auction model that treats sellers and buyers symmetrically. This symmetry enables a computational node to play at one level of the tree {{the role of a}} seller by dealing with a group of potential buyers as well as {{to play the role of}} a potential buyer at the next higher level. Such a symmetric auction (also known as a double auction) is used for supporting a standard auction to be carried out in a hierarchic manner. An architecture is developed and basic algorithms and protocols are presented, together with correctness reasoning. Keywords and phrases: Double auctions, federated auction house, Bid servers, the Internet, synchronous/asynchronous communication, deadlines, auction fairness. 1...|$|R
40|$|As one of {{the most}} {{reliable}} technologies, network intrusion detection system (NIDS) allows the monitoring of incoming and outgoing traffic to identify unauthorised usage and mishandling of attackers in computer network systems. To this extent, this paper investigates the experimental performance of Snort-based NIDS (S-NIDS) in a practical network with the latest technology in various network scenarios including high data speed and/or heavy traffic and/or large packet size. An effective testbed is designed based on Snort using different muti-core processors, e. g., i 5 and i 7, with different operating systems, e. g., Windows 7, Windows Server and Linux. Furthermore, considering an enterprise network consisting of multiple virtual local area networks (VLANs), a centralised parallel S-NIDS (CPS-NIDS) is proposed with the support of a <b>centralised</b> database <b>server</b> to deal with high data speed and heavy traffic. Experimental evaluation is carried out for each network configuration to evaluate the performance of the S-NIDS in different network scenarios as well as validating the effectiveness of the proposed CPS-NIDS. In particular, by analysing packet analysis efficiency, an improved performance of up to 10 % is shown to be achieved with Linux over other operating systems, while up to 8 % of improved performance can be achieved with i 7 over i 5 processors...|$|R
40|$|Realtime {{telemonitoring}} of critical, {{acute and}} chronic patients {{has become increasingly}} popular {{with the emergence of}} portable acquisition devices and IP enabled mobile phones. During telemonitoring, enormous physiological signals are transmitted through the public communication network in realtime. However, these physiological signals can be intercepted with minimal effort, since existing telemonitoring practise ignores the privacy and security requirements. In this paper, to achieve end-to-end security, we first proposed an encoding method capable of securing Electrocardiogram (ECG) data transmission from an acquisition device to a mobile phone, and then from a mobile phone to a <b>centralised</b> medical <b>server</b> by concealing cardiovascular details as well as features in ECG data required to identify an individual. The encoding method not only conceals cardiovascular condition, but also reduces the enormous file size of the ECG with a compression ratio of up to 3. 84, thus making it suitable in energy constrained small acquisition devices. As ECG data transfer faces even greater security vulnerabilities while traversing through the public Internet, we further designed and implemented 3 phase encoding-compression-encryption mechanism on mobile phones using the proposed encoding method and existing compression and encryption tools. This new mechanism elevates the security strength of the system even further. Apart from higher security, we also achieved higher compression ratio of up to 20. 06, which will enable faster transmission and make the system suitable for realtime telemonitoring...|$|R
