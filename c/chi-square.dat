10000|5153|Public
25|$|However critics {{claim to}} have {{identified}} statistical errors in the conclusions published in Nature: including: the actual standard deviation for the Tucson study was 17 years, not 31, as published; the <b>chi-square</b> distribution value is 8.6 rather than 6.4, and the relative significance level (which measures {{the reliability of the}} results) is close to 1% – rather than the published 5%, which is the minimum acceptable threshold.|$|E
25|$|Hand {{elevation}} test The hand elevation test {{is performed}} by lifting both hands above the head, and if symptoms are reproduced in the median nerve distribution within 2 minutes, considered positive. The hand elevation test has higher {{sensitivity and specificity}} than Tinel's test, Phalen's test, and carpal compression test. <b>Chi-square</b> statistical analysis has shown the hand elevation test to be as effective, if not better than, Tinel's test, Phalen's test, and carpal compression test.|$|E
25|$|Benford's law was empirically tested {{against the}} numbers (up to the 10th digit) {{generated}} {{by a number of}} important distributions, including the uniform distribution, the exponential distribution, the half-normal distribution, the right-truncated normal, the normal distribution, the chi square distribution and the log normal distribution. In addition to these the ratio distribution of two uniform distributions, the ratio distribution of two exponential distributions, the ratio distribution of two half-normal distributions, the ratio distribution of two right-truncated normal distributions, the ratio distribution of two <b>chi-square</b> distributions (the F distribution) and the log normal distribution were tested.|$|E
5000|$|In statistics, {{the term}} <b>chi-squared</b> or [...] has various uses, {{including}} the <b>chi-squared</b> distribution, the <b>chi-squared</b> test, and <b>chi-squared</b> target models.|$|R
5000|$|It {{follows from}} the {{definition}} of the <b>chi-squared</b> distribution that the sum of independent <b>chi-squared</b> variables is also <b>chi-squared</b> distributed. Specifically, if {Xi}i=1n are independent <b>chi-squared</b> variables with {ki}i=1n degrees of freedom, respectively, then Y [...] X1 + ⋯ + Xn is <b>chi-squared</b> distributed with k1 + ⋯ + kn degrees of freedom.|$|R
5000|$|For {{samples of}} a {{reasonable}} size, the G-test and the <b>chi-squared</b> test {{will lead to}} the same conclusions. However, the approximation to the theoretical <b>chi-squared</b> distribution for the G-test is better than for the Pearson's <b>chi-squared</b> test. In cases where [...] for some cell case the G-test is always better than the <b>chi-squared</b> test.|$|R
2500|$|... {{for various}} {{specific}} tests (<b>chi-square,</b> Fisher's F-test, etc.).|$|E
2500|$|... where [...] is the p-th {{quantile}} of the <b>chi-square</b> distribution with k {{degrees of}} freedom, and [...] is the confidence level. [...] This {{is equivalent to}} the following: ...|$|E
2500|$|There {{are several}} methods for {{assessing}} fit, {{such as a}} <b>Chi-square</b> statistic, or a standardized version of it. [...] Two and three-parameter IRT models adjust item discrimination, ensuring improved data-model fit, so fit statistics lack the confirmatory diagnostic value found in one-parameter models, where the idealized model is specified in advance.|$|E
2500|$|If [...] is <b>chi-squared</b> {{distributed}} [...] then [...] is also non-central <b>chi-squared</b> distributed: ...|$|R
2500|$|From this representation, the noncentral <b>chi-squared</b> {{distribution}} {{is seen to}} be a Poisson-weighted mixture of central <b>chi-squared</b> distributions. Suppose that a random variable J has a Poisson distribution with mean , and the conditional distribution of Z given Jnbsp&=nbsp&i is <b>chi-squared</b> with knbsp&+nbsp&2i degrees of freedom. Then the unconditional distribution of Z is non-central <b>chi-squared</b> with k degrees of freedom, and non-centrality parameter [...]|$|R
5000|$|Noncentral beta {{distribution}} {{can be obtained}} as a transformation of <b>chi-squared</b> distribution and Noncentral <b>chi-squared</b> distribution ...|$|R
50|$|When {{two models}} are nested, models {{can also be}} {{compared}} using a <b>chi-square</b> difference test. The <b>chi-square</b> difference test is computed by subtracting the likelihood ratio <b>chi-square</b> statistics for the two models being compared. This value is then compared to the <b>chi-square</b> critical value at their difference in degrees of freedom. If the <b>chi-square</b> difference is smaller than the <b>chi-square</b> critical value, the new model fits the data significantly better and is the preferred model. Else, if the <b>chi-square</b> difference {{is larger than the}} critical value, the less parsimonious model is preferred.|$|E
5000|$|The minimum <b>chi-square</b> {{estimate}} of the population mean λ is the number that minimizes the <b>chi-square</b> statistic ...|$|E
50|$|In certain <b>chi-square</b> tests, one rejects a null {{hypothesis}} about a population distribution if a specified test statistic is too large, when that statistic would have approximately a <b>chi-square</b> distribution if the {{null hypothesis}} is true. In minimum <b>chi-square</b> estimation, one finds the values of parameters that make that test statistic as small as possible.|$|E
50|$|The sum of N <b>chi-squared</b> (1) random {{variables}} has a <b>chi-squared</b> distribution with N degrees of freedom.|$|R
50|$|Using the <b>chi-squared</b> {{distribution}} to interpret Pearson's <b>chi-squared</b> statistic requires one {{to assume that}} the discrete probability of observed binomial frequencies in the table can be approximated by the continuous <b>chi-squared</b> distribution. This assumption is not quite correct, and introduces some error.|$|R
5000|$|A <b>chi-squared</b> test, {{also written}} as [...] test, is any {{statistical}} hypothesis test wherein the sampling {{distribution of the}} test statistic is a <b>chi-squared</b> distribution when the null hypothesis is true. Without other qualification, 'chi-squared test' often is used as short for Pearson's <b>chi-squared</b> test.|$|R
5000|$|In the output, the [...] "block" [...] line {{relates to}} <b>Chi-Square</b> test {{on the set}} of {{independent}} variables that are tested and included in the model fitting. The [...] "step" [...] line relates to <b>Chi-Square</b> test on the step level while variables included in the model step by step. Note that in the output a step <b>chi-square,</b> {{is the same as the}} block <b>chi-square</b> since they both are testing the same hypothesis that the tested variables enter on this step are non-zero. If you were doing stepwise regression, however, the results would be different. Using forward stepwise selection, researchers divided the variables into two blocks (see METHOD on the syntax following below). LOGISTIC REGRESSION VAR=grade ...|$|E
50|$|Once {{the model}} of best fit is determined, the highest-order {{interaction}} is examined by conducting <b>chi-square</b> analyses {{at different levels of}} one of the variables. To conduct <b>chi-square</b> analyses, one needs to break the model down into a 2 &times; 2 or 2 &times; 1 contingency table.|$|E
5000|$|The Wald {{test can}} be {{evaluated}} against a <b>chi-square</b> distribution.|$|E
5000|$|The <b>chi-squared</b> test, {{when used}} with the {{standard}} approximation that a <b>chi-squared</b> distribution is applicable, has the following assumptions: ...|$|R
50|$|Just as the Wilson {{interval}} mirrors Pearson's <b>chi-squared</b> test, the Wilson interval with continuity correction {{mirrors the}} equivalent Yates' <b>chi-squared</b> test.|$|R
2500|$|The p-value {{was first}} {{formally}} introduced by Karl Pearson, in his Pearson's <b>chi-squared</b> test, using the <b>chi-squared</b> distribution and notated as capital P. The p-values for the <b>chi-squared</b> distribution (for various values of χ2 and degrees of freedom), now notated as P, was calculated in , collected in [...]|$|R
5000|$|In {{the above}} {{equation}} [...] represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the {{ratio of the}} fitted model to the saturated model) will produce a negative value, hence {{the need for a}} negative sign. [...] can be shown to follow an approximate chi-squared distribution. [...] Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a <b>chi-square</b> distribution, nonsignificant <b>chi-square</b> values indicate very little unexplained variance and thus, good model fit. Conversely, a significant <b>chi-square</b> value indicates that a significant amount of the variance is unexplained.|$|E
5000|$|Normality test using Jarque-Bera test, Shapiro-Wilk test, and <b>Chi-square</b> test methods.|$|E
5000|$|... where a is the {{estimated}} expected {{number in the}} [...] "> 8" [...] cell, and [...] "20" [...] appears {{because it is the}} sample size. The value of a is 20 times the probability that a Poisson-distributed random variable exceeds 8, and it is easily calculated as 1 minus the sum of the probabilities corresponding to 0 through 8. By trivial algebra, the last term reduces simply to a. Numerical computation shows that the value of λ that minimizes the <b>chi-square</b> statistic is about 3.5242. That is the minimum <b>chi-square</b> estimate of λ. For that value of λ, the <b>chi-square</b> statistic is about 3.062764. There are 10 cells. If the null hypothesis had specified a single distribution, rather than requiring λ to be estimated, then the null distribution of the test statistic would be a <b>chi-square</b> distribution with 10 − 1 = 9 degrees of freedom. Since λ had to be estimated, one additional degree of freedom is lost. The expected value of a <b>chi-square</b> random variable with 8 degrees of freedom is 8. Thus the observed value, 3.062764, is quite modest, and the null hypothesis is not rejected.|$|E
50|$|When {{the data}} {{consists}} of binary observations, the score statistic {{is the same}} as the <b>chi-squared</b> statistic in the Pearson's <b>chi-squared</b> test.|$|R
2500|$|In {{probability}} theory and statistics, the noncentral <b>chi-squared</b> or noncentral [...] distribution is a generalization of the <b>chi-squared</b> distribution. [...] This distribution often arises {{in the power}} analysis of statistical tests in which the null distribution is (perhaps asymptotically) a <b>chi-squared</b> distribution; important examples of such tests are the likelihood-ratio tests.|$|R
5000|$|... has a {{generalized}} <b>chi-squared</b> distribution {{of a particular}} form. The difference from the standard <b>chi-squared</b> distribution is that [...] are complex and can have different variances, and the difference from the more general generalized <b>chi-squared</b> distribution is that the relevant scaling matrix A is diagonal. If [...] for all i, then , scaled down by [...] (i.e. multiplied by [...] ), has a <b>chi-squared</b> distribution, , also known as an Erlang distribution. If [...] have distinct values for all i, then [...] has the pdf ...|$|R
50|$|The <b>chi-square</b> {{distribution}} if {{the number}} of degrees of freedom is >= 2.|$|E
50|$|Relative {{fit indices}} (also called “incremental fit indices” and “comparative fit indices”) compare the <b>chi-square</b> for the {{hypothesized}} model to {{one from a}} “null”, or “baseline” model. This null model almost always contains a model in which all of the variables are uncorrelated, and as a result, has a very large <b>chi-square</b> (indicating poor fit). Relative fit indices include the normed fit index and comparative fit index.|$|E
50|$|In statistics, minimum <b>chi-square</b> {{estimation}} is {{a method}} of estimation of unobserved quantities based on observed data.|$|E
2500|$|The {{square of}} X/σ has the noncentral <b>chi-squared</b> {{distribution}} with one degree of freedom: [...] If μ = 0, the distribution is called simply <b>chi-squared.</b>|$|R
40|$|A model-based windowed <b>chi-squared</b> {{procedure}} is proposed for identifying falsified sensor measurements. We employ the widely-used static <b>chi-squared</b> and the dynamic cumulative sum (CUSUM) fault/attack detection procedures as benchmarks {{to compare the}} performance of the windowed <b>chi-squared</b> detector. In particular, we characterize the state degradation that a class of attacks can induce to the system while enforcing that the detectors do not raise alarms (zero-alarm attacks). We quantify the advantage of using dynamic detectors (windowed <b>chi-squared</b> and CUSUM detectors), which leverages the history of the state, over a static detector (<b>chi-squared)</b> which uses a single measurement at a time. Simulations using a chemical reactor are presented to illustrate the performance of our tools...|$|R
2500|$|<b>Chi-squared</b> {{distribution}}, {{the distribution}} of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see <b>chi-squared</b> test) ...|$|R
