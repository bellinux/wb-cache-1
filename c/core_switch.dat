44|319|Public
25|$|The Security Extensions, {{marketed as}} TrustZone Technology, is in ARMv6KZ and later {{application}} profile architectures. It provides a low-cost alternative to adding another dedicated security core to an SoC, by providing two virtual processors backed by hardware based access control. This lets the application <b>core</b> <b>switch</b> between two states, {{referred to as}} worlds (to reduce confusion with other names for capability domains), {{in order to prevent}} information from leaking from the more trusted world to the less trusted world. This world switch is generally orthogonal to all other capabilities of the processor, thus each world can operate independently of the other while using the same core. Memory and peripherals are then made aware of the operating world of the core and may use this to provide access control to secrets and code on the device.|$|E
2500|$|Abrizio was a fabless {{semiconductor}} {{company which}} made switching fabric chip sets (integrated circuits for computer network [...] switches). [...] Their chip set, the TT1, {{was used by}} several large system development companies as the <b>core</b> <b>switch</b> fabric in their high value communication systems.|$|E
2500|$|On October 10, 2011, RIM {{experienced}} one of {{the worst}} service outages in the company's history. Tens of millions of BlackBerry users in Europe, the Middle East, Africa, and North America were unable to receive or send emails and BBM messages through their phones. The outage was caused {{as a result of a}} <b>core</b> <b>switch</b> failure, [...] "A transition to a back-up switch did not function as tested, causing a large backlog of data, RIM said." [...] Service was restored Thursday October 13, with RIM announcing a $100 package of free premium apps for users and enterprise support extensions.|$|E
40|$|We {{report on}} the {{switching}} of the magnetic vortex core in a Pac-man disk using a magnetic field pulse, investigated via micromagnetic simulations. The minimum <b>core</b> <b>switching</b> field is reduced by 72 % {{compared to that of}} a circular disk with the same diameter and thickness. However, the <b>core</b> <b>switches</b> irregularly with respect to both the field pulse amplitude and duration. This irregularity is induced by magnetization oscillations which arise due to excitation of the spin waves when the core annihilates. We show that the <b>core</b> <b>switching</b> can be controlled with the assist magnetic field and by changing the waveform. Comment: 9 pages, 4 figure...|$|R
50|$|Instead {{of using}} VLT between end-devices like servers {{it can also}} be used for uplinks between (access/distribution) <b>switches</b> and the <b>core</b> <b>switches.</b>|$|R
5000|$|ASI <b>Core</b> - Advanced <b>Switching</b> <b>Core</b> Architecture Specification ...|$|R
50|$|Extreme Networks {{introduced}} a four-port 100GbE module for the BlackDiamond X8 <b>core</b> <b>switch</b> in November 2012.|$|E
5000|$|Customization: support {{customization}} for proxy and <b>core</b> <b>switch</b> button, UEIP data feedback in setting-Privacy & Content, and auto-sync setting ...|$|E
50|$|Q-in-Q also {{introduces}} a scalability issue within {{the core of}} the carrier network, where every <b>core</b> <b>switch</b> needs to learn and maintain forwarding entries for every customer MAC address.|$|E
40|$|The {{universal}} {{criterion for}} ultrafast vortex <b>core</b> <b>switching</b> between core-up and -down vortex bi-states in soft magnetic nanodots was empirically investigated by micromagnetic simulations and {{combined with an}} analytical approach. Vortex-core switching occurs whenever the velocity of vortex core motion reaches a critical value, which is {nu}{sub c} = 330 {+-} 37 m/s for Permalloy, as estimated from numerical simulations. This critical velocity {{was found to be}} {nu}{sub c} = {eta}{sub c}{gamma} {radical}A{sub ex} with A{sub ex} the exchange stiffness, {gamma} the gyromagnetic ratio, and an estimated proportional constant {eta}{sub c} = 1. 66 {+-} 0. 18. This criterion does neither depend on driving force parameters nor on the dimension or geometry of the magnetic specimen. The phase diagrams for the vortex <b>core</b> <b>switching</b> criterion and its switching time with respect to both the strength and angular frequency of circular rotating magnetic fields were derived, which offer practical guidance for implementing vortex <b>core</b> <b>switching</b> into future solid state information storage devices...|$|R
3000|$|T,⏊ {{along the}} field direction, {{starting}} from 0 at zero field, and reaching saturation before the BLH Co <b>core</b> <b>switches.</b> A non-hysteretic field dependence of dI/dV(H) results for H < H [...]...|$|R
50|$|RTU. A VoIP {{multiservice}} platform & a class 5 softswitch {{intended to}} be the <b>core</b> <b>switching</b> point for residential and business telephony based on an IP network and to deliver Value Added Services (VAS) to end-users.|$|R
50|$|The major driver {{behind the}} {{development}} of the IN was the need for a more flexible way of adding sophisticated services to the existing network. Before the IN was developed, all new features and/or services had to be implemented directly in the <b>core</b> <b>switch</b> systems. This made for long release cycles as the software testing had to be extensive and thorough to prevent the network from failing. With the advent of the IN, most of these services (such as toll-free numbers and geographical number portability) were moved out of the <b>core</b> <b>switch</b> systems and into self-contained nodes, creating a modular and more secure network that allowed the service providers themselves to develop variations and value-added services to their networks without submitting a request to the <b>core</b> <b>switch</b> manufacturer and waiting for the long development process. The initial use of IN technology was for number translation services, e.g. when translating toll-free numbers to regular PSTN numbers; much more complex services have since been built on the IN, such as Custom Local Area Signaling Services (CLASS) and prepaid telephone calls.|$|E
5000|$|A compact {{form-factor}} platform delivering high-density 10/40 gigabit Ethernet connectivity, {{and targeted}} at mid-market through to mid-size enterprise <b>core</b> <b>switch</b> applications. [...] The VSP 8000 supports the Fabric Connect, Switch Cluster, Fabric Attach, and ID Engines technologies.|$|E
5000|$|Abrizio was a fabless {{semiconductor}} {{company which}} made switching fabric chip sets (integrated circuits for computer network switches). Their chip set, the TT1, {{was used by}} several large system development companies as the <b>core</b> <b>switch</b> fabric in their high value communication systems.|$|E
40|$|Reducing the {{interconnect}} size {{with each}} technology node and increasing speed with each generation increases IR-drop and Ldi/dt noise. In addition to this, {{the drive for}} more integration increases the average current requirement for modern ULSI design. Simultaneous <b>switching</b> of <b>core</b> logic blocks and I/O drivers produces large current transients due to power distribution network parasitics at high clock frequency. The current transients are injected into the power distribution planes thereby inducing noise in the supply voltage. The part of the noise that is caused by switching of the internal logic load is <b>core</b> <b>switching</b> noise. The <b>core</b> logic <b>switches</b> at much higher speed than driver speed whereas the package inductance {{is less than the}} on-chip inductance in modern BGA packages. The <b>core</b> <b>switching</b> noise is currently gaining more attention for three-dimensional integrated circuits where on-chip inductance is much higher than the board and package inductance due to smaller board, and package. The switching noise of the driver is smaller than the <b>core</b> <b>switching</b> noise due to small driver size and reduced capacitance associated with short on-board wires for three-dimensional integrated circuits. The load increases with the addition of each die. The power distribution TSV pairs to supply each extra die also introduce additional parasitic. The <b>core</b> <b>switching</b> noise may propagate through substrate and consequently through interconnecting TSVs to different dies in heterogeneous integrated system. <b>Core</b> <b>switching</b> noise may lead to decreased device drive capability, increased gate delays, logic errors, and reduced noise margins. The actual behavior of the on-chip load is not well known {{in the beginning of the}} design cycle whereas altering the design during later stages is not cost effective. The size of a three-dimensional power distribution network may reach billions of nodes with the addition of dies in a vertical stack. The traditional tools may run out of time and memory during simulation of a three-dimensional power distribution network whereas, the CAD tools for the analysis of 3 D power distribution network are in the process of evolution. Compact mathematical models for the estimation of <b>core</b> <b>switching</b> noise are necessary in order to overcome the power integrity challenges associated with the 3 D power distribution network design. This thesis presents three different mathematical models to estimate <b>core</b> <b>switching</b> noise for 3 D stacked power distribution networks. A time-domain-based mathematical model for the estimation of design parameters of a power distribution TSV pair is also proposed. Design guidelines for the estimation of optimum decoupling capacitance based on flat output impedance are also proposed for each stage of the vertical chain of power distribution TSV pairs. A mathematical model for tradeoff between TSV resistance and amount of decoupling capacitance on each DRAM die is proposed for a 3 D-DRAM-Over-Logic system. The models are developed by following a three step approach: 1) design physical model, 2) convert it to equivalent electrical model, and 3) formulate the mathematical model based on the electrical model. The accuracy, speed and memory requirement of the proposed mathematical model is compared with equivalent Ansoft Nexxim models. QC 20121015 </p...|$|R
50|$|The Dell Networking {{products}} {{will come in}} several families. The new naming system will partially follow the existing Force10 naming system: E-series for chassis-based modular (<b>core)</b> <b>switches,</b> C-series for chassis-based datacenter-access switches, S-series rack switches and Z-series for distributed core-switches.|$|R
40|$|We {{address the}} {{software}} costs of <b>switching</b> threads between <b>cores</b> in a multicore processor. Fast <b>core</b> <b>switching</b> enables {{a variety of}} potential improvements, such as thread migration for thermal man-agement, ne-grained load balancing, and exploiting asymmetric multicores, where performance asymmetry creates opportunities for more efcient resource utilization. Successful exploitation of these opportunities demands low core-switching costs. We describe our implementation of <b>core</b> <b>switching</b> in the Linux kernel, as well as software changes that can decrease switching costs. We use de-tailed simulations to evaluate several alternative implementations. We also explore how some simple architectural variations can re-duce switching costs. We evaluate system efciency using both real (but symmetric) hardware, and simulated asymmetric hardware, us-ing both microbenchmarks and realistic applications. 1...|$|R
50|$|The VSP 8000 is new {{category}} of high-performance Ethernet Switches developed by Avaya to leverage the latest generation {{application-specific integrated circuit}} chipsets. The Virtual Services Platform 8284XSQ is the first product in the VSP 8000 Series and is a fixed, compact form-factor Ethernet Switch designed to satisfy mainstream Campus <b>Core</b> <b>Switch</b> requirements.|$|E
5000|$|E8200 zl - (Released September 2007) <b>Core</b> <b>switch</b> offering, 12-module slot chassis with dual fabric modules {{and options}} for dual {{management}} modules and system support modules for high availability (HA). IPV6-ready, 692 Gbit/s fabric. Up to 48 10GbE ports, 288 Gb ports, or 288 SFPs. Powered {{by a combination}} of either 875W or 1500W PSU's, to provide a maximum of 3600W (5400W using additional powersupplies) of power for PoE.|$|E
50|$|Device Firmware or System Image Upgrades - Configuration and Orchestration Manager {{can be used}} to {{automate}} the task of device firmware upgrades, for operational or security enhancement purposes. Network Devices can be grouped together through multiple criteria such as core or network edge location. Upgrades can be performed from edge to core to ensure successful communication to switches being upgraded thereby preventing communications failures due to switches being randomly upgraded. This prevents the situation where the edge switch behind the <b>core</b> <b>switch</b> is attempted to be upgraded.|$|E
30|$|Some of the {{drawbacks}} of this design are oversubscription less than 1 : 1 due to prohibitive costs. An oversubscription of 1 : 1 {{means that all}} servers communicate with other arbitrary servers at full bandwidth of their network interface. Typical oversubscription in this topology is 2.5 : 1 or 8 : 1 (Al-Fares et al. 2008). Large data centers have multi-rooted <b>core</b> <b>switches</b> with multiple <b>core</b> <b>switches</b> which requires multipath routing techniques. This leads to oversubscription, limiting multiplicity of paths, and excessively large routing table entries increasing lookup latency. The most serious shortcoming of this topology is excessive cost due to 10  GigE switches in the upper layers. These shortcomings were identified and resolved by Al-Fares et al. (2008) who proposed fat-tree topology discussed below.|$|R
50|$|The {{intelligence}} {{is provided by}} network nodes on the service layer, distinct from the switching layer of the core network, as opposed to solutions based on intelligence in the <b>core</b> <b>switches</b> or telephone equipment. The IN nodes are typically owned by telecommunications operators (telecommunications service providers).|$|R
50|$|Dell's Force10 {{switches}} support 40 Gbit/s interfaces. These 40 Gbit/s fiber-optical interfaces using QSFP+ transceivers can {{be found}} on the Z9000 distributed <b>core</b> <b>switches,</b> S4810 and S4820 as well as the blade-switches MXL and the IO-Aggregator. The Dell PowerConnect 8100 series switches also offer 40 Gbit/s QSFP+ interfaces.|$|R
50|$|The S5000 is {{targeted}} for datacenter networking {{as either a}} 10G access-switch or a datacenter distribution switches. It {{can also be used}} as (routing) <b>core</b> <b>switch</b> in smaller datacenters. It fully supports Data Center Bridging (DCB) and can also be used as FCoE or Fibre Channel switch by using a FC interface module. It provides full FC logic allowing one to directly connect FC based SAN's to the switch to fully support FCoE or Converged Networking in combination with the other 10G switches in the Dell Networking range.|$|E
50|$|The {{product is}} {{typically}} positioned as the <b>Core</b> <b>Switch</b> in mid-market and small-to-medium Enterprise networks, {{or as an}} Aggregation/Distribution Switch in larger networks. Due to its ability to support both conventional and virtualized networking technologies - and operate both concurrently - the VSP 8000 is suitable in deployment scenarios that require high-availability, network segmentation, and dynamic provisioning. Additionally, the high-density of 10 Gigabit Ethernet (80 ports), plus support for 40 Gigabit Ethernet (4 ports), makes the VSP 8284XSQ model suitable {{for use as a}} Data Center Middle/End-of-Row Switch.|$|E
5000|$|It {{has strong}} IT {{infrastructure}} with around 256 Mbit/s broadband line from BTCL and BdREN with 1 Mbit/s from other ISP as backup for Internet facility, routers for routing, a firewall for internet security, five high configuration server as mails server, proxy server, database server, seven midlevel configuration of workstations for other servers and backup server, a <b>core</b> <b>switch,</b> several manageable and unmanageable switches for intranet connectivity. It is planned {{to increase the}} bandwidth and to expand and renovate the optical backbone network including student halls of residence and teacher residential area secure access of the academic and research materials very soon.|$|E
50|$|The {{computing}} nodes of MareNostrum 3 communicate {{primarily through}} a high bandwidth, low latency InfiniBand FDR10 network. The different nodes were interconnected via {{fibre optic cables}} and Mellanox 648-port FDR10 Infiniband <b>Core</b> <b>Switches.</b> In addition, there was a more traditional local area network consisting of Gigabit Ethernet adapters.|$|R
5000|$|The {{core network}} {{provides}} high-speed, highly redundant forwarding services to move packets between distribution-layer devices indifferent {{regions of the}} network. <b>Core</b> <b>switches</b> and routers are usually the most powerful, in terms of raw forwarding power, in theenterprise; core network devices manage the highest-speed connections, such as 10 Gigabit Ethernet.|$|R
5000|$|The {{femtocell}} gateway, comprising {{a security}} gateway that terminates {{large numbers of}} encrypted IP data connections from {{hundreds of thousands of}} femtocells, and a signalling gateway which aggregates and validates the signalling traffic, authenticates each femtocell and interfaces with the mobile network <b>core</b> <b>switches</b> using standard protocols, such as Luh.|$|R
5000|$|On October 10, 2011, RIM {{experienced}} one of {{the worst}} service outages in the company's history. Tens of millions of BlackBerry users in Europe, the Middle East, Africa, and North America were unable to receive or send emails and BBM messages through their phones. The outage was caused {{as a result of a}} <b>core</b> <b>switch</b> failure, [...] "A transition to a back-up switch did not function as tested, causing a large backlog of data, RIM said." [...] Service was restored Thursday October 13, with RIM announcing a $100 package of free premium apps for users and enterprise support extensions.|$|E
50|$|The AMS-IX {{platform}} is continually evolving {{due to its}} rapid growth in traffic and number of connected member ports. Up until end of 2009, it is using a redundant hub-spoke architecture using a <b>core</b> <b>switch</b> and multiple edge switches. This double-star topology brings {{the advantage of being}} able to perform maintenance on the network without any impact on customer traffic, and to anticipate on fiber and equipment problems by (automatically) switching to the backup topology as soon as a failure in one of the active components occurs. The active switching topology star is determined by means of the VSRP protocol. This topology is AMS-IX version 3.|$|E
5000|$|The Dell Networking PCT2800 web-managed {{switches}} are entry-level Ethernet switches {{that only}} offer a web-based GUI management interface. There are 4 models offering between 8 and 48 ports per switch. The interfaces on the switches are all copper-based gigabit Ethernet-ports and the 24 and 48 ports switches offer 2 or 4 'combo' ports where the last 2 (resp. 4) ports can use either the RJ45/UTP 1000BaseT copper-interface or a fiber SFP transceiver for uplinks to a distribution or <b>core</b> <b>switch.</b> All switches offer standard features like VLAN's, link-aggregation, auto-negotiation for speed- and duplex setting. The MAC address-table can {{hold up to}} 8000 MAC addresses in its forwarding table and have a 2Mb packet-buffering capacity ...|$|E
50|$|Commands on two <b>core</b> ERS-8600 <b>switches</b> in {{a switch}} cluster.|$|R
5000|$|NetIron MLXe-16 from Brocade Communications Systems as <b>core</b> IX <b>switch</b> ...|$|R
50|$|Broadvox has a 10G carrier-grade {{network that}} carries over 20 billion annual minutes of voice and data. The company uses high density {{hardware}} and technology from Sonus Networks, Juniper Networks, and Cisco Systems. It also has <b>core</b> <b>switching</b> centers in New York, Los Angeles, Chicago, Dallas, Miami, Atlanta, Denver, Toronto and Seattle.|$|R
