907|4|Public
25|$|The <b>cross-entropy</b> method (CE) generates {{candidates}} solutions via a parameterized probability distribution. The {{parameters are}} updated via <b>cross-entropy</b> minimization, {{so as to}} generate better samples in the next iteration.|$|E
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated principal components analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the <b>Cross-Entropy</b> Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
2500|$|In the {{engineering}} literature, MDI {{is sometimes called}} the Principle of Minimum <b>Cross-Entropy</b> (MCE) or Minxent for short. [...] Minimising the Kullback–Leibler divergence from m to p with respect to m is equivalent to minimizing the <b>cross-entropy</b> of p and m, since ...|$|E
2500|$|... 2004, Zlochin and Dorigo {{show that}} some {{algorithms}} are {{equivalent to the}} stochastic gradient descent, the <b>cross-entropy</b> method and algorithms to estimate distribution ...|$|E
2500|$|... {{which is}} {{appropriate}} {{if one is}} trying to choose an adequate approximation to p. However, this is just as often not the task one is trying to achieve. Instead, just as often it is m that is some fixed prior reference measure, and p that one is attempting to optimise by minimising DKL(p‖m) subject to some constraint. [...] This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining <b>cross-entropy</b> to be DKL(p‖m), rather than H(p,m).|$|E
2500|$|... {{a result}} known as Gibbs' inequality, with DKL(P‖Q) zero if {{and only if}} [...] almost everywhere. [...] The entropy [...] thus sets a minimum value for the <b>cross-entropy</b> H(P,Q), the {{expected}} number of bits required when using a code based on Q rather than P; and the Kullback–Leibler divergence therefore represents the expected number of extra bits that must be transmitted to identify a value x drawn from X, if a code is used corresponding to the probability distribution Q, rather than the [...] "true" [...] distribution P.|$|E
2500|$|The {{algorithm}} starts by a stochastic {{mapping of}} [...] to [...] through , {{this is the}} corrupting step. Then the corrupted input [...] passes through a basic auto-encoder process and is mapped to a hidden representation [...] From this hidden representation, we can reconstruct [...] In the last stage, a minimization algorithm runs {{in order to have}} z {{as close as possible to}} uncorrupted input [...] The reconstruction error [...] might be either the <b>cross-entropy</b> loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.|$|E
2500|$|For some {{versions}} of the algorithm, {{it is possible to}} prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of a convergence ant colony algorithm was made in 2000, the graph-based ant system algorithm, and then algorithms for ACS and MMAS. Like most metaheuristics, {{it is very difficult to}} estimate the theoretical speed of convergence. In 2004, Zlochin and his colleagues showed that COA-type algorithms could be assimilated methods of stochastic gradient descent, on the <b>cross-entropy</b> and estimation of distribution algorithm. They proposed these metaheuristics as a [...] "research-based model". A performance analysis of continuous ant colony algorithm based on its various parameter suggest its sensitivity of convergence on parameter tuning.|$|E
50|$|<b>Cross-entropy</b> {{minimization}} {{is frequently}} used in optimization and rare-event probability estimation; see the <b>cross-entropy</b> method.|$|E
5000|$|The <b>cross-entropy</b> (CE) method generates {{candidates}} solutions via a parameterized probability distribution. The {{parameters are}} updated via <b>cross-entropy</b> minimization, {{so as to}} generate better samples in the next iteration.|$|E
5000|$|There {{are many}} {{situations}} where <b>cross-entropy</b> {{needs to be}} measured but the distribution of [...] is unknown. An example is language modeling, where a model is created based on a training set , and then its <b>cross-entropy</b> is measured on a test set to assess how accurate the model is in predicting the test data. In this example, [...] is the true distribution of words in any corpus, and [...] is the distribution of words as predicted by the model. Since the true distribution is unknown, <b>cross-entropy</b> cannot be directly calculated. In these cases, an estimate of <b>cross-entropy</b> is calculated using the following formula: ...|$|E
5000|$|In the {{engineering}} literature, MDI {{is sometimes called}} the Principle of Minimum <b>Cross-Entropy</b> (MCE) or Minxent for short. Minimising the Kullback-Leibler divergence from m to p with respect to m is equivalent to minimizing the <b>cross-entropy</b> of p and m, since ...|$|E
5000|$|... #Subtitle level 2: <b>Cross-entropy</b> error {{function}} and logistic regression ...|$|E
5000|$|Rubinstein, R.Y., [...] "The <b>cross-entropy</b> {{method for}} {{combinatorial}} and continuous Optimization", Methodology and Computing in Applied Probability, 2, 127—190, 1999.|$|E
5000|$|Rubinstein R.Y. and D.P. Kroese, [...] "The <b>Cross-Entropy</b> Method: a Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning", Springer, 2004.|$|E
5000|$|... 2004, Zlochin and Dorigo {{show that}} some {{algorithms}} are {{equivalent to the}} stochastic gradient descent, the <b>cross-entropy</b> method and algorithms to estimate distribution ...|$|E
5000|$|Entropy-based methods {{result in}} {{algorithms}} {{that use the}} entropy of the foreground and background regions, the <b>cross-entropy</b> between the original and binarized image, etc.|$|E
50|$|The {{logistic}} loss {{is sometimes}} called <b>cross-entropy</b> loss. It {{is also known as}} log loss (In this case, the binary label is often denoted by {-1,+1}).|$|E
5000|$|Update the {{parameters}} of the random mechanism based on the data to produce a [...] "better" [...] sample in the next iteration. This step involves minimizing the <b>cross-entropy</b> or Kullback-Leibler divergence.|$|E
5000|$|He is {{well known}} as the founder of several {{breakthrough}} methods, such asthe score function method, stochastic counterpart method and <b>cross-entropy</b> method, [...] which have numerous applications in combinatorial optimization and simulation.|$|E
50|$|The softmax {{function}} {{is often used}} in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or <b>cross-entropy)</b> regime, giving a non-linear variant of multinomial logistic regression.|$|E
50|$|A large {{class of}} methods avoids relying on {{gradient}} information.These include simulated annealing, <b>cross-entropy</b> search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory {{and in the}} limit) a global optimum. In multiple domains they have demonstrated performance.|$|E
5000|$|However, as {{discussed}} in the article Kullback-Leibler divergence, sometimes the distribution [...] is the fixed prior reference distribution, and the distribution [...] is optimised to be as close to [...] as possible, subject to some constraint. In this case the two minimisations are not equivalent. This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining <b>cross-entropy</b> to be , rather than [...]|$|E
5000|$|The loss layer {{specifies}} how training penalizes {{the deviation}} between the predicted and true labels and is normally the final layer. Various loss functions appropriate for different tasks {{may be used}} there. Softmax loss is used for predicting a single class of K mutually exclusive classes. Sigmoid <b>cross-entropy</b> loss is used for predicting K independent probability values in [...] Euclidean loss is used for regressing to real-valued labels [...]|$|E
5000|$|The lowest perplexity {{that has}} been {{published}} on the Brown Corpus (1 million words of American English of varying topics and genres) as of 1992 is indeed about 247 per word, corresponding to a <b>cross-entropy</b> of log2247 = 7.95 bits per word or 1.75 bits per letter [...] using a trigram model. It is often possible to achieve lower perplexity on more specialized corpora, as they are more predictable.|$|E
5000|$|When {{comparing}} {{a distribution}} [...] against a fixed reference distribution , cross entropy and KL divergence are identical {{up to an}} additive constant (since [...] is fixed): both take on their minimal values when , which is [...] for KL divergence, and [...] for cross entropy. In the engineering literature, the principle of minimising KL Divergence (Kullback's [...] "Principle of Minimum Discrimination Information") is often called the Principle of Minimum <b>Cross-Entropy</b> (MCE), or Minxent.|$|E
5000|$|... {{which is}} {{appropriate}} {{if one is}} trying to choose an adequate approximation to p. However, this is just as often not the task one is trying to achieve. Instead, just as often it is m that is some fixed prior reference measure, and p that one is attempting to optimise by minimising DKL(p‖m) subject to some constraint. This has led to some ambiguity in the literature, with some authors attempting to resolve the inconsistency by redefining <b>cross-entropy</b> to be DKL(p‖m), rather than H(p,m).|$|E
5000|$|Loss {{functions}} are implemented as sub-classes of , {{which has a}} similar interface to [...] It also has [...] and [...] methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the Mean Squared Error criterion implemented in [...] and the <b>cross-entropy</b> criterion implemented in [...] What follows {{is an example of}} a Lua function that can be iteratively called to train an [...] Module on input Tensor , target Tensor [...] with a scalar : ...|$|E
5000|$|... {{a result}} known as Gibbs' inequality, with DKL(P‖Q) zero if {{and only if}} [...] almost everywhere. The entropy [...] thus sets a minimum value for the <b>cross-entropy</b> H(P,Q), the {{expected}} number of bits required when using a code based on Q rather than P; and the Kullback-Leibler divergence therefore represents the expected number of extra bits that must be transmitted to identify a value x drawn from X, if a code is used corresponding to the probability distribution Q, rather than the [...] "true" [...] distribution P.|$|E
5000|$|The {{algorithm}} starts by a stochastic {{mapping of}} [...] to [...] through , {{this is the}} corrupting step. Then the corrupted input [...] passes through a basic auto-encoder process and is mapped to a hidden representation [...] From this hidden representation, we can reconstruct [...] In the last stage, a minimization algorithm runs {{in order to have}} z {{as close as possible to}} uncorrupted input [...] The reconstruction error [...] might be either the <b>cross-entropy</b> loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.|$|E
50|$|The <b>cross-entropy</b> (CE) method {{attributed}} to Reuven Rubinstein {{is a general}} Monte Carlo approach tocombinatorial and continuous multi-extremal optimization and importance sampling. The method originated {{from the field of}} rare event simulation, wherevery small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems.The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema.|$|E
50|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The covariance matrix of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated principal components analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the <b>Cross-Entropy</b> Method are based on very similar ideas, but estimate (non-incrementally) the covariance matrix by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
5000|$|For some {{versions}} of the algorithm, {{it is possible to}} prove that it is convergent (i.e., it is able to find the global optimum in finite time). The first evidence of a convergence ant colony algorithm was made in 2000, the graph-based ant system algorithm, and then algorithms for ACS and MMAS. Like most metaheuristics, {{it is very difficult to}} estimate the theoretical speed of convergence. In 2004, Zlochin and his colleagues showed that COA-type algorithms could be assimilated methods of stochastic gradient descent, on the <b>cross-entropy</b> and estimation of distribution algorithm. They proposed these metaheuristics as a [...] "research-based model". A performance analysis of continuous ant colony algorithm based on its various parameter suggest its sensitivity of convergence on parameter tuning.|$|E
5000|$|Another idea, {{championed by}} Edwin T. Jaynes, {{is to use}} the {{principle}} of maximum entropy (MAXENT). The motivation is that the Shannon entropy of a probability distribution measures the amount of information contained in the distribution. The larger the entropy, the less information is provided by the distribution. Thus, by maximizing the entropy over a suitable set of probability distributions on X, one finds the distribution that is least informative {{in the sense that it}} contains the least amount of information consistent with the constraints that define the set. For example, the maximum entropy prior on a discrete space, given only that the probability is normalized to 1, is the prior that assigns equal probability to each state. And in the continuous case, the maximum entropy prior given that the density is normalized with mean zero and variance unity is the standard normal distribution. The principle of minimum <b>cross-entropy</b> generalizes MAXENT to the case of [...] "updating" [...] an arbitrary prior distribution with suitable constraints in the maximum-entropy sense.|$|E
30|$|This {{paper is}} devoted to {{formulate}} a fuzzy <b>cross-entropy</b> characterized by credibility measure. For this purpose, we organize this paper as follows. The ‘Preliminaries’ section recalls some useful definitions and properties about credibility theory. The ‘Fuzzy cross-entropy’ section defines the fuzzy <b>cross-entropy</b> and studies some useful properties. In the ‘Minimum <b>cross-entropy</b> principle’ section, the minimum <b>cross-entropy</b> principle is proposed. At {{the end of this}} paper, a brief summary is given.|$|E
40|$|In recent years, the <b>cross-entropy</b> {{method has}} been {{successfully}} applied {{to a wide range}} of discrete optimization tasks. In this paper we consider the <b>cross-entropy</b> method in the context of continuous optimization. We demonstrate the effectiveness of the <b>cross-entropy</b> method for solving difficult continuous multi-extremal optimization problems, including those with nonlinear constraints. Key words: <b>Cross-entropy,</b> continuous optimization, multi-extremal objective function, dynamic smoothing, constrained optimization, nonlinear constraints, acceptance–rejection, penalty function. 1...|$|E
