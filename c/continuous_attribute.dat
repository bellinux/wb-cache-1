57|359|Public
5000|$|When {{dealing with}} {{continuous}} data, a typical {{assumption is that}} the continuous values associated with each class are distributed according to a Gaussian distribution. For example, suppose the training data contains a <b>continuous</b> <b>attribute,</b> [...] We first segment the data by the class, and then compute the mean and variance of [...] in each class. Let [...] be {{the mean of the}} values in [...] associated with class c, and let [...] be the variance of the values in [...] associated with class c. Suppose we have collected some observation value [...] Then, the probability distribution of [...] given a class , , can be computed by plugging [...] into the equation for a Normal distribution parameterized by [...] and [...] That is, ...|$|E
30|$|Unlike SuLQ-based ID 3 and DiffP-C 4.5 algorithms, DiffGen uses {{information}} gain and max operator as scoring function to construct decision tree {{from top to}} bottom, and finally adds Laplace noise to the calculation value of released leaf nodes. But, for each recursion of DiffGen algorithm, {{it is necessary to}} assign a certain privacy budget to the <b>continuous</b> <b>attribute.</b> It uses the exponential mechanism to select a subdivision scheme from the <b>continuous</b> <b>attribute,</b> and then invokes the exponential mechanism together with the discrete attribute. Zhu et al. (2013) improved DiffGen algorithm. In each iteration of subdivision, all <b>continuous</b> <b>attribute</b> subdivision schemes were multiplied by corresponding weights and then combined with discrete attribute subdivision schemes to form a candidate scheme set. This algorithm reduces the number of calls to the exponential mechanism, thus increasing the utilization rate of privacy budget and improving the accuracy of classification model.|$|E
40|$|Abstract:- The {{concept of}} rough set, which an upper/lower {{approximation}} are involved in, is giving {{a powerful tool}} to extract rules from a database or examples. In {{order to determine the}} upper/lower approximation of a subset in an approximation space, each discrete attribute value (usually it appeals like digital figure) plays a very important role in the process of approximation. However, so far few papers pay attention to how to convert an originally <b>continuous</b> <b>attribute</b> values into some appropriate discrete values. Clearly, the better conversion we give, the better approximation accuracy we will get. To do so, we introduce fuzzy logic to divide the <b>continuous</b> <b>attribute</b> values, and further, genetic algorithm (GA) is adopted to obtain the most proper fuzzy division. In this paper, the detailed GA-based fuzzy modeling approach to extract rules from a database is given...|$|E
40|$|Abstract — Most {{real-world}} classification problems involve <b>continuous</b> (real-valued) <b>attributes,</b> as well as, nominal (discrete) attributes. The {{majority of}} Ant Colony Optimisation (ACO) classification algorithms have {{the limitation of}} only being {{able to cope with}} nominal attributes directly. Extending the approach for coping with <b>continuous</b> <b>attributes</b> presented by cAnt-Miner (Ant-Miner coping with <b>continuous</b> <b>attributes),</b> in this paper we propose two new methods for handling <b>continuous</b> <b>attributes</b> in ACO classification algorithms. The first method allows a more flexible representation of continuous attributes’ intervals. The second method explores the problem of attribute interaction, which originates from the way that <b>continuous</b> <b>attributes</b> are handled in cAnt-Miner, in order to implement an improved pheromone updating method. Empirical evaluation on eight publicly available data sets shows that the proposed methods facilitate the discovery of more accurate classification models...|$|R
40|$|While {{state-of-the-art}} kernels for graphs with discrete labels scale well to graphs {{with thousands}} of nodes, the few existing kernels for graphs with <b>continuous</b> <b>attributes,</b> unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with <b>continuous</b> <b>attributes</b> from discrete ones. The idea is to iteratively turn <b>continuous</b> <b>attributes</b> into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with <b>continuous</b> <b>attributes</b> and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation. Comment: IEEE ICDM 201...|$|R
40|$|AbstractIn {{real-time}} data mining applications discrete values play {{vital role in}} knowledge representation as {{they are easy to}} handle and very close to knowledge level representation than <b>continuous</b> <b>attributes.</b> Discretization is a major step in data mining process where <b>continuous</b> <b>attributes</b> are transformed into discrete values. However, most of the classifications algorithms are require discrete values as the input. Even though some data mining algorithms directly contract with <b>continuous</b> <b>attributes,</b> the learning process yields low quality results. In this paper, we introduce a new discretization method based on standard deviation technique called ‘z-score’ for <b>continuous</b> <b>attributes</b> on biomedical datasets. We compare performance of the proposed algorithm with the state-of- the-art discretization techniques. The experiment results show the efficiency in terms of accuracy and also minimize the classifier confusion for decision making process...|$|R
40|$|The {{performance}} of many {{machine learning algorithms}} can be substantially improved with a proper discretization scheme. In this paper we describe a theoretically rigorous approach to discretization of <b>continuous</b> <b>attribute</b> values, based on a Bayesian clustering framework. The method produces a probabilistic scoring metric for different discretizations, {{and it can be}} combined with various types of learning algorithms working on discrete data. The approach is validated by demonstrating empirically the performance improvement of the Naive Bayes classifier when Bayesian discretization is used instead of the standard equal frequency interval discretization. 1 INTRODUCTION Many algorithms developed in the machine learning and uncertain reasoning community focus on learning in nominal feature bases. On the other hand, many real world tasks involve <b>continuous</b> <b>attribute</b> domains. Consequently, {{in order to be able}} to use such algorithms, a discretization process is needed. Continuous variable d [...] ...|$|E
40|$|Nominal — {{values from}} an unordered set, e. g., color, {{profession}} Ordinal — values from an ordered set, e. g., military or academic rank Continuous — real numbers, e. g., integers or real numbers Data discretization: Divide {{the range of}} a <b>continuous</b> <b>attribute</b> into intervals Some classification algorithms only accept categorical attributes. Reduce data size by discretization Prepare for further analysi...|$|E
40|$|In many pattern {{recognition}} applications, first decision trees are used {{due to their}} simplicity and easily interpretable nature. In this paper, we propose a new decision tree learning algorithm called univariate margin tree where, for each <b>continuous</b> <b>attribute,</b> the best split is found using convex optimization. Our simulation results on 47 data sets show that the novel margin tree classifier performs at least a...|$|E
40|$|Nearest {{neighbor}} and instance-based learning techniques typically handle continuous and linear input values well, but {{often do not}} handle symbolic input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between symbolic attribute values, but it largely ignores <b>continuous</b> <b>attributes,</b> using discretization to map continuous values into symbolic values. This paper presents two heterogeneous distance metrics, called the Interpolated VDM (IVDM) and Windowed VDM (WVDM), that extend the Value Difference Metric to handle <b>continuous</b> <b>attributes</b> more appropriately. In experiments on 21 data sets the new distance metrics achieves higher classification accuracy in most cases involving <b>continuous</b> <b>attributes...</b>|$|R
40|$|This paper {{presents}} an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with <b>continuous</b> <b>attributes),</b> which incorporates an entropy-based discretization method {{in order to}} cope with <b>continuous</b> <b>attributes</b> during the rule construction process. By having {{the ability to create}} discrete intervals for <b>continuous</b> <b>attributes</b> "on-the-fly", cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rule...|$|R
5000|$|RULES-8 17 is an {{improved}} version {{that deals with}} <b>continuous</b> <b>attributes</b> online.|$|R
40|$|AbstractThe {{disturbance}} {{identification is}} a classification of data stream problem, VFDT (Very Fast Decision Tree) {{which is the}} classical data stream classification algorithm can only deal with discrete attribute values. However, the power grid data gathered by WAMS platform are mostly continuously present. This algorithm is improved on the VFDT, by introducing the sampling theorem, to make the <b>continuous</b> <b>attribute</b> discretization, and classify the common power grid disturbances. At last, the experiments prove that the new algorithm is feasible...|$|E
40|$|We {{propose a}} new algorithm, called CILA, for {{discretization}} of <b>continuous</b> <b>attribute.</b> The CILA algorithm {{can be used}} with any class labeled data. The tests performed using the CILA algorithm show that it generates discretization schemes with almost always the highest dependence between the class labels and the discrete intervals, and always with significantly lower number of intervals, when compared with other state-of-the-art discretization algorithms. The use of the CILA algorithm as a preprocessing step for a machine learning algorithm significantly improves the results in terms of the accuracy, which are better than by using other discretization algorithms...|$|E
40|$|There are {{two methods}} for GIS {{similarity}} measurement problem, one is cross-coefficient for GIS attribute similarity measurement, {{and the other}} is spatial autocorrelation that is based on spatial location. These methods can not calculate subzone similarity problem based on universal background. The rough measurement based on membership function solved this problem well. In this paper, we used rough sets to measure the similarity of GIS subzone discrete data, and used neighborhood rough sets to calculate continuous data’s upper and lower approximation. We used neighborhood particle to calculate membership function of <b>continuous</b> <b>attribute,</b> then to solve continuous attribute’s subzone similarity measurement problem...|$|E
40|$|Abstract. This paper {{presents}} an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with <b>continuous</b> <b>attributes),</b> which incorporates an entropy-based discretization method {{in order to}} cope with <b>continuous</b> <b>attributes</b> during the rule construction process. By having {{the ability to create}} discrete intervals for <b>continuous</b> <b>attributes</b> “on-the-fly”, cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rules. ...|$|R
40|$|Abstract. Nearest {{neighbor}} and instance-based learning techniques typically handle continuous and linear input values well, but {{often do not}} handle symbolic input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between symbolic attribute values, but it largely ignores <b>continuous</b> <b>attributes,</b> using discretization to map continuous values into symbolic values. This paper presents two heterogeneous distance metrics, called the Interpolated VDM (IVDM) and Windowed VDM (WVDM), that extend the Value Difference Metric to handle <b>continuous</b> <b>attributes</b> more appropriately. In experiments on 21 data sets the new distance metrics achieves higher classification accuracy in most cases involving <b>continuous</b> <b>attributes.</b> 1...|$|R
40|$|Classification {{trees have}} been {{successfully}} used in several application fields. However, <b>continuous</b> <b>attributes</b> cannot be used directly when building classification trees, but they must be first discretized with clustering techniques, which require some degree of subjectivity. We propose an approach to build classification trees that {{does not require the}} discretization of the <b>continuous</b> <b>attributes.</b> The approach is an extension of existing methods for building classification trees and is based on the information gain yielded by discrete and <b>continuous</b> <b>attributes.</b> Data from a software development case study are analyzed with both the proposed approach and C 4. 5 to show the approach's applicability and benefits over C 4. 5...|$|R
40|$|The {{majority}} of Ant Colony Optimization (ACO) algorithms for data mining {{have dealt with}} classification or clustering problems. Regression remains an unexplored research area {{to the best of}} our knowledge. This paper proposes a new ACO algorithm that generates regression rules for data mining applications. The new algorithm combines components from an existing deterministic (greedy) separate and conquer algorithm—employing the same quality metrics and <b>continuous</b> <b>attribute</b> processing techniques—allowing a comparison of the two. The new algorithm has been shown to decrease the relative root mean square error when compared to the greedy algorithm. Additionally a different approach to handling continuous attributes was investigated showing further improvements were possible...|$|E
40|$|Canada). The {{number of}} {{families}} required for detecting the familial aggregation of a <b>continuous</b> <b>attribute.</b> Am J Epidemiol 108 : 425 - 428, 1978. A formula Is presented and graphs are displayed that provide the approximate power of a test for intraclass correlation. The results are useful for estimating the {{number of families}} required for detecting the familial aggregation of a <b>continuous</b> <b>attribute.</b> biometry; family characteristics; statistics An investigation of intrafamily resem-blance is often the primary aim of an epi-demiologic study. The most common statis-tical tool for measuring intrafamily resem-blance {{with respect to a}} continuous attri-bute has been the coefficient of intraclass correlation yielded by a standard Model II analysis of variance. Table 1 shows this analysis of variance, where k is the number of families studied, and N is the total num-ber of observations. Let no = n- £*-i (n,- h) 2 /(k- 1) N, where n, is the number of individuals in the ith family, i = 1, 2, • • •, k, and h = £ £Li nj k = N/k, the average family size. Then the computed intraclass correlation coefficient n is given by r, = 1 /[1 + no MSW/(MSA-MSW) ]. A test of significance for n is pro-vided by the usual procedure of comparing the calculated value of F to Fo, the tabu-lated value of the F-distribution with k — 1 Received for publication November 28, 1977, and in final form April 21, 1978. Abbreviations: pi, true value of intraclass correla-tion coefficient; rt, sample intraclass correlation coef-ficien...|$|E
40|$|Abstract. In many pattern {{recognition}} applications, first decision trees are used {{due to their}} simplicity and easily interpretable nature. In this paper, we propose a new decision tree learning algorithm called univariate margin tree, where for each <b>continuous</b> <b>attribute,</b> the best split is found using convex optimization. Our simulation results on 47 datasets show that the novel margin tree classifier performs at least as good as C 4. 5 and LDT with a similar time complexity. For two class datasets it generates smaller trees than C 4. 5 and LDT without sacrificing from accuracy, and generates significantly more accurate trees than C 4. 5 and LDT for multiclass datasets with one-vs-rest methodology. ...|$|E
40|$|The paper {{addresses}} {{the problem of}} Discretization of <b>continuous</b> <b>attributes</b> in rough set. Discretization of <b>continuous</b> <b>attributes</b> {{is an important part}} of rough set theory because most of data that we usually gain are continuous data. In order to improve processing speed of discretization, we propose a FPGA-based discretization algorithm of <b>continuous</b> <b>attributes</b> making use of the speed advantage of FPGA. Combined attributes dependency degree of rough ret, the discretization system was divided into eight modules according to block design. This method can save much time of pretreatment in rough set and improve operation efficiency. Extensive experiments on a certain fighter fault diagnosis validate the effectiveness of the algorithm.  </p...|$|R
50|$|Many machine {{learning}} algorithms are known to produce better models by discretizing <b>continuous</b> <b>attributes.</b>|$|R
40|$|AbstractReal-life data {{usually are}} {{presented}} in databases by real numbers. On the other hand, most inductive learning methods require {{a small number of}} attribute values. Thus it is necessary to convert input data sets with <b>continuous</b> <b>attributes</b> into input data sets with discrete attributes. Methods of discretization restricted to single <b>continuous</b> <b>attributes</b> will be called local, while methods that simultaneously convert all <b>continuous</b> <b>attributes</b> will be called global. In this paper, a method of transforming any local discretization method into a global one is presented. A global discretization method, based on cluster analysis, is presented and compared experimentally with three known local methods, transformed into global. Experiments include tenfold cross-validation and leaving-one-out methods for ten real-life data sets...|$|R
40|$|Abstract—Because of the patient’s {{inconsistent}} data, uncertain Thyroid Disease dataset is {{appeared in}} the learning process: irrelevant, redundant, missing, and huge features. In this paper, Rough sets theory is used in data discretization for <b>continuous</b> <b>attribute</b> values, data reduction and rule induction. Also, Rough sets try to cluster the Thyroid relation attributes {{in the presence of}} missing attribute values and build the Modified Similarity Relation that is dependent on the number of missing values with respect to the number of the whole defined attributes for each rule. The discernibility matrix has been constructed to compute the minimal sets of reducts, which is used to extract the minimal sets of decision rules that describe similarity relations among rules. Thus, the rule associated strength is measured...|$|E
40|$|Abstract: Various anonymization {{techniques}} {{have been proposed}} for publishing a microdata. Anonymization techniques hide the sensitive data from the attackers. Examples of these techniques are slicing, bucketization and generalization. Generalization hides sensitive data but it loses lot of data. Bucketization make the difficult to detect sensitive attribute by randomizing sensitive attribute {{but it does not}} prevent relation between them. Slicing is better technique amongst all remaining technique because it cannot lose information and it is used for maintain the relation between attributes. In case of slicing, equal-width discretization is used to convert <b>continuous</b> <b>attribute</b> into categorical attribute. Equal-width discretization has very time consuming technique. To solve this problem we propose cluster based attribute slicing algorithm. Proposed technique does not take more to sort a data...|$|E
40|$|Because of the patient’s {{inconsistent}} data, uncertain Thyroid Disease dataset is {{appeared in}} the learning process: irrelevant, redundant, missing, and huge features. In this paper, Rough sets theory is used in data discretization for <b>continuous</b> <b>attribute</b> values, data reduction and rule induction. Also, Rough sets try to cluster the Thyroid relation attributes {{in the presence of}} missing attribute values and build the Modified Similarity Relation that is dependent on the number of missing values with respect to the number of the whole defined attributes for each rule. The discernibility matrix has been constructed to compute the minimal sets of reducts, which is used to extract the minimal sets of decision rules that describe similarity relations among rules. Thus, the rule associated strength is measured...|$|E
40|$|In {{this paper}} we propose an {{extension}} of classification algorithm based on ant colony algorithms to handle <b>continuous</b> valued <b>attributes</b> using the concepts of fuzzy logic. The ant colony algorithms transform <b>continuous</b> <b>attributes</b> into nominal attributes by creating clenched discrete intervals. This may lead to false predictions of the target attribute, especially if the attribute value history {{is close to the}} borders of discretization. <b>Continuous</b> <b>attributes</b> are discretized on the fly into fuzzy partitions that will be used to develop an algorithm called Fuzzy Ant-Miner. Fuzzy rules are generated by using the concept of fuzzy entropy and fuzzy fitness of a rule...|$|R
50|$|C4.5 improved: {{discrete}} and <b>continuous</b> <b>attributes,</b> missing attribute values, attributes {{with differing}} costs, pruning trees (replacing irrelevant branches with leaf nodes).|$|R
40|$|Instance-based {{learning}} techniques typically handle {{continuous and}} linear input values well, but {{often do not}} handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores <b>continuous</b> <b>attributes,</b> requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal <b>attributes,</b> <b>continuous</b> <b>attributes,</b> or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and <b>continuous</b> <b>attributes.</b> 1. Introduction Instance-Based Learning (IBL) (Aha, [...] ...|$|R
40|$|This {{research}} {{proposes a}} new approach to measuring and estimating Willingness to Pay (WTP) for a variety of non-market amenities. The continuous variation of attributes present in many non-market goods is utilized to collect higher resolution information on consumers' choices concerning their use decisions than is available through standard dichotomous choice questions. It does this without directly asking research participants to form explicit valuations - an unfamiliar and cognitively challenging task for most consumers. This generates data that can be estimated with a duration, or survival model consistent with Random Utility Theory, from which an expression for WTP {{as a function of the}} <b>continuous</b> <b>attribute</b> can be recovered. We apply this approach to estimate Delaware beach visitors' visual disamenity from the presence of offshore energy generation installations...|$|E
40|$|We {{propose a}} new genetic fuzzy {{discretization}} method with feature selection for the pattern classification problems. Traditional discretization methods categorize a <b>continuous</b> <b>attribute</b> {{into a number}} of bins. Because they are made on crisp discretization, there exists considerable information loss. Fuzzy discretization allows overlapping intervals and reflects linguistic classification. However, the number of intervals, the boundaries of intervals, and the degrees of overlapping are intractable to get optimized and a discretization process increases the total amount of data being transformed. We use a genetic algorithm with feature selection not only to optimize these parameters but also {{to reduce the amount of}} transformed data by filtering the unconcerned attributes. Experimental results showed considerable improvement on the classification accuracy over a crisp discretization and a typical fuzzy discretization with feature selection...|$|E
40|$|We {{propose a}} novel Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) {{framework}} for person re-identification. Re-identifications from multiple cameras {{are regarded as}} related tasks to exploit shared information to improve re-identification accuracy. Both low level fea-tures and semantic/data-driven attributes are utilized. Since attributes are generally correlated, we introduce a low rank attribute embedding into the MTL formulation to embed original binary attributes to a <b>continuous</b> <b>attribute</b> space, where incorrect and incomplete attributes are rectified and recovered to better describe people. The learning objective function consists of a quadratic loss regarding class labels and an attribute embedding error, which is solved by an al-ternating optimization procedure. Experiments on four per-son re-identification datasets have demonstrated that MTL-LORAE outperforms existing approaches by a large margin and produces promising results. 1...|$|E
30|$|There are uses of {{probability}} distribution function on the <b>continuous</b> <b>attributes</b> in the dataset in the model built using Naive Bayes classification.|$|R
5000|$|Handling both <b>continuous</b> and {{discrete}} <b>attributes</b> - In {{order to}} handle <b>continuous</b> <b>attributes,</b> C4.5 creates a threshold and then splits the list into those whose attribute value {{is above the}} threshold {{and those that are}} {{less than or equal to}} it.|$|R
40|$|Due to {{the large}} volume of data set as well as complex and dynamic {{properties}} of data instances, several data mining algorithms have been applied for mining complex data streams in the last decades. Now a day, knowledge extraction from data streams is getting more complex because {{the structure of the}} data instance does not match the attribute values when considering the tabulated data, texts, web, images or videos etc. In this paper, we address some difficulties of mining complex data streams such as dealing with <b>continuous</b> <b>attributes,</b> input attribute selection, and classifier construction. The proposed discretization algorithm finds the possible cut points in <b>continuous</b> <b>attributes</b> using information gain heuristic and naïve Bayesian classifier that can separate the class distributions. We evaluate the proposed algorithms on several benchmark data sets from UCI machine learning repository. The experimental results demonstrate that the proposed method improves the quality of discretization of <b>continuous</b> <b>attributes</b> and scales up the classification accuracy for different types of classification problem. </span...|$|R
