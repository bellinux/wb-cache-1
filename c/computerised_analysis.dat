62|200|Public
5|$|Due to {{its status}} an {{international}} language, English is expeditious {{when it comes}} adopting foreign words, and borrows vocabulary from {{a large number of}} other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora, collections of actual written texts and spoken passages. Many statements published {{before the end of the}} 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new <b>computerised</b> <b>analysis</b> of linguistic corpus data becomes available.|$|E
50|$|In 2011, the {{hospital}} achieved Stage 7 of the Healthcare Information and Management Systems Society Analytics Europe′s Electronic Medical Record Adoption Model. This was awarded for achieving a paperless medical record environment coupled with significant <b>computerised</b> <b>analysis</b> of clinical data.|$|E
50|$|Due to {{its status}} an {{international}} language, English is expeditious {{when it comes}} adopting foreign words, and borrows vocabulary from {{a large number of}} other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora, collections of actual written texts and spoken passages. Many statements published {{before the end of the}} 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new <b>computerised</b> <b>analysis</b> of linguistic corpus data becomes available.|$|E
50|$|The {{outstanding}} {{geophysical survey}} results provide the main data set for an important archaeological application of <b>computerised</b> network <b>analysis.</b>|$|R
5000|$|<b>Computerised</b> EEG <b>analysis</b> of Penicillin induced seizure {{threshold}} {{in developing}} rats S. K. Sharma, W. Selvamurthy, M. Behari, N. C. Maheswari and T. P. Singh Indian J Med Res, 86: 775-782, 1987 ...|$|R
5000|$|Kainic acid induced epileptogenesis in {{developing}} and undernourished rats: A <b>computerised</b> EEG <b>analysis</b> S. K. Sharma, W. Selvamurthy, M. C. Maheshwari and T. P. Singh Indian J Med Res, 32: 456-466, 1990 ...|$|R
40|$|This {{study was}} {{conducted}} to assess the incidence and impact of additional findings from magnetic resonance imaging (MRI) on the workup of patients eligible for breast-conserving therapy (BCT) and to optimise the specificity of further workup by combining radiological reading with <b>computerised</b> <b>analysis.</b> One hundred and sixteen patients eligible for BCT underwent preoperative MRI where the gold standard was histology or follow-up (median 35 months, range 23 - 48). The incidence of additional findings and impact on treatment (wider excision/conversion to mastectomy) were assessed. The specificity of referral to further workup was also assessed without and with <b>computerised</b> <b>analysis.</b> Additional findings from MRI occurred in 41 % of patients, requiring workup in 78 %. In 22 % the findings were malignant, causing change in treatment. Specificity was 33 % (10 / 30) for radiological reading alone, and 97 % (29 / 30) combined with computer analysis. Our findings show that additional findings preoperative MRI required workup in approximately one-third of patients and we suggest that combining radiological reading with computer analysis has the potential to accurately exclude benign lesions from further worku...|$|E
40|$|This article adapts a new {{technique}} for the <b>computerised</b> <b>analysis</b> of political texts, previously used to analyse party manifestos, {{to the analysis of}} speeches made in a legislature. The benefits of computerised text analysis come from the ability to analyse, for the first time, complex and daunting electronic sources of text, such as the parliamentary record. This allows the systematic estimation of the policy positions of individual political actors, with huge benefits both for theory development and empirical analysis. In this article, the technique is used to analyse all 58 English language speeches made in the October 1991 confidence debate {{on the future of the}} incumbent Fianna Fáil-PD coalition. The task was to use the words spoken in the debate to locate every one of the individual speakers on a 'pro- versus anti-government' dimension. The purpose was, first, to examine the validity of computerised text analysis when applied to legislative speeches and, second, to answer substantively interesting questions about the positions of individual Irish legislators in 1991. The results vindicate the use of <b>computerised</b> <b>analysis</b> in the context of legislative speeches and locate all speakers in the 1991 debate in a substantively interesting policy space...|$|E
40|$|Sir: I {{should like}} to comment on Stevens et all 2 two elegant {{investigations}} correlat-ing abnormal behaviour and abnormal mental state in chronic schizophrenics with the <b>computerised</b> <b>analysis</b> of telemetered EEG signals. In the first of these reports Stevens et all recorded continuously the EEGs of 40 schizophrenics, most of whom were unmedicated, by radio-telemetry for periods ranging between 2 and 24 hours. The patients were compared to 12 normal, age-matched controls. It was found that the schizophrenics had more delta, and less alpha power than the controls. Further, during hallucinations, the patients showe...|$|E
40|$|Ready reckoners {{are used}} in the {{clinical}} setting as a tool for the estimation of nutrient intake. With increasing opportunities for nutrition research, ready reckoners may provide for a more rapid analysis of nutritional intake than computerised methods, often seen as the gold standard for nutritional analysis. This research aimed to determine the level of agreement between ready reckoner and <b>computerised</b> dietary <b>analysis</b> through a secondary analysis of clinical trial data. Participant food intakes were estimated by trained observers using the one-quarter method. Daily energy and protein intake were estimated by the healthcare network ready reckoner and <b>computerised</b> dietary <b>analysis.</b> Agreement between methods was tested using t-tests, correlations and Bland-Altman plots. A correlation between analysis methods was observed (r = 0. 9086 energy, r = 0. 8700 protein). Wide limits of agreement were observed for both energy and protein intake. Compared with the computerised method, ready reckoner analysis underestimated energy intake by 600 kJ and protein intake by 5 g. Mean energy and protein intake calculated by each method was significantly different (p < 0. 0001 energy; p < 0. 0001 protein). No time differences between analysis methods were observed. In the clinical setting, practitioners {{should be aware of the}} variability of a ready reckoner compared to <b>computerised</b> dietary <b>analysis.</b> Further investigation into the acceptability of ready reckoners as a reliable method of nutrient intake determination, particularly for analysis of nutrition research, is required...|$|R
5000|$|... congs/m-l2 - 2013-2015 - Dynamic sound <b>analysis,</b> <b>computerised</b> {{transcription}} of {{data from}} various single and multi-lane traffic congestions in a unidirectional system - Grand piano and C++granular synthesis. study:12 ...|$|R
40|$|The corneal {{endothelium}} of 42 clear corneal transplants was {{studied with}} a specular microscope. The endothelial cell morphology was analysed {{by using a}} <b>computerised</b> image <b>analysis</b> system. A quantitative index was developed to study the degree of variation in cell size. By applying this objective index we observed that pleomorphism is independent of age, and considerable alteration occurs in cell morphology during healing...|$|R
40|$|A {{system for}} <b>computerised</b> <b>analysis</b> of ultrasonographic {{prostate}} images (AUDEX=Automated Urologic Diagnostic EXpert system) {{for the detection}} of prostate carcinoma was developed. The ultimate goal is to develop a system that is reliable and non-observer dependent. Results of an earlier study with a small group were encouraging and this study describes the results of the <b>computerised</b> <b>analysis</b> in a larger group. Sixty-two patients who were scheduled to undergo a radical prostatectomy were prospectively analysed. The radical prostatectomy specimens were step-sectioned in the transverse plane, corresponding to the ultrasound pictures. Malignant regions identified by each study were quantified and compared by computer calculation. No correlation was observed between ultrasound analysis and pathology result. For the AUDEX analysis an overall sensitivity of 85 % and a specificity of 18 % with only a diagnostic accuracy of 57 % was noticed when presence or absence of malignancy was evaluated by octant (total 496). When applying a cut-off value of 0. 5 ml the numbers were 71 %, 33 % and 55 %, respectively. Correlation was significantly better for the ventral octants. In this study the earlier results of our AUDEX system could not be confirmed. Although sensitivity was good, specificity and especially diagnostic accuracy were lower than expected. We have to conclude that the current settings are inappropriate for routine clinical use. Prostate Cancer and Prostatic Diseases (2001) 4, 56 - 6...|$|E
40|$|A {{scoring system}} using <b>computerised</b> <b>analysis</b> of digital stored images of knee radiographs has been developed. Measurement {{is based on}} the {{assessment}} of joint space size. It is sensitive, rapid, and reproducible. Plain radiographs are positioned on an acetate grid; a computer generated grid is superimposed on a digital image of the radiograph viewed on a closed circuit television monitor and the joint space measured automatically. Area and distance have been assessed; area measurements are more reproducible. Application of microcomputer based digital image analysis to radiological scoring systems is an important step in understanding the nature and progression of arthritis...|$|E
40|$|Systems {{developed}} {{in the past for}} the <b>computerised</b> <b>analysis</b> of well tests are based on mathematical models of the reservoir that can be solved using analytical methods. Unfortunately, for many geothermal well tests the flows are non-isothermal and often involve phase changes. This means that the corresponding mathematical models cannot be solved analytically and traditional methods of well test analysis, either manual or computerised, do not work very well. In the system discussed here, instead of analytical techniques very fast numerical methods are used to solve the reservoir models. These fast solvers are linked to an optimiser, which carries out the model fitting task. The system includes a range o...|$|E
40|$|In {{the last}} twenty years, an {{international}} group of political scientists has coded nearly 2000 party manifestos {{with the help of}} one single coding scheme based on 56 categories which covers all main topics of these documents. However, there is a growing awareness of the shortcomings of the underlying coding scheme, such as overlapping and missing categories, which cannot be repaired without coding all manifestos all over again. Some have presented an alternative for manifesto-research by means of expert opinions on party policy positions, but these are unable to provide reliable time series for subsequent election years. The unborn solution to some of the problems with the coding scheme would be the <b>computerised</b> content <b>analysis</b> on digitalised party manifestos. This would open up a new universe of infinite possibilities for recodings and reanalyses. The extended consequences from full computerisation of textual analysis are mind boggling. But at the present, these possibilities are merely potentials as the computerised techniques are still underdeveloped. This article explores the possibilities for <b>computerised</b> content <b>analysis</b> {{in such a way that}} all postwar manifestos in established democracies can be compared with each other with the help of flexible coding schemes...|$|R
500|$|Allardyce is a keen {{proponent of}} sports science and using {{technology}} and innovative techniques in coaching his teams, such as <b>computerised</b> performance <b>analysis</b> and yoga. Martin Hardy of The Independent {{described him as}} [...] "one of the pioneers of sports science in English football". Former players and pundits have cited his preparation as his main strength, which allows his teams to have better organisation and defensive stability. Former Bolton player Kevin Davies also highlighted Allardyce's man-management skills as a strength.|$|R
40|$|Facial morphometry using <b>computerised</b> image <b>analysis</b> was {{performed}} on patients with growth hormone receptor deficiency (Laron syndrome) from an inbred population of southern Ecuador. Morphometrics were compared for 49 patients, 70 unaffected relatives, and 14 unrelated persons. Patients with growth hormone receptor deficiency showed significant decreases in measures of vertical facial growth as compared to unaffected relatives and unrelated persons with short stature from other causes. This report validates and quantifies the clinical impression of foreshortened facies in growth hormone receptor deficiency...|$|R
40|$|Background: Measuring and {{describing}} {{the effects of}} aphasia on the informativeness of language is a complex process. Due to technological advances in the recent years, the processes involved in the measurement of language can be automated {{through the use of}} computerised analyses. In the present research, the Computerized Propositional Idea Density Rater (CPIDR 3. 2) provides an automated method for calculating Propositional Idea Density (PD), a measure which {{has been shown to be}} sensitive to the effects of ageing and dementia. The measure of PD quantifies the proportion of words within a text that are semantically intrinsic to its overall meaning. Aims: This research investigated the extent to which PD measures were different in aphasic and non-aphasic discourse, and the extent to which PD correlated with the severity of aphasia and with the established measures of other aspects of informativeness. Given the previously reported high levels of agreement between the <b>computerised</b> <b>analysis</b> and human raters, it was hypothesised that there would be high levels of agreement between the <b>computerised</b> <b>analysis</b> and human judgements for aphasic (as well as non-aphasic) discourse. Methods & Procedures: Data from the Goals in Aphasia Project were analysed for the purposes of the present research. De-identified transcriptions of 50 interviews with individuals with aphasia and 49 interviews with their family members were stripped of all interviewer data, leaving only conversational contributions made by the participants. These formatted transcripts were analysed using two automated, computerised language analysis tools: CPIDR 3. 2) and Systematic Analysis of Language Transcripts (SALT Version 8) for a range of other discourse measures. Outcomes & Results: Results showed a significant difference in PD (...|$|E
40|$|The {{variability}} of pulmonary arterial pressure, {{the relation of}} pulmonary pressure to systemic pressure, pulmonary pressure responses to stimuli (exercise, hypoxia, smoking, free ambulation), and plasma catecholamine responses were assessed in five patients with primary pulmonary hypertension. Ambulatory monitoring techniques provided data for the <b>computerised</b> <b>analysis</b> of continuous, beat-to-beat, direct recordings of both pulmonary and systemic arterial pressures for 8 to 10 hours. The absolute {{variability of}} pulmonary arterial pressure and the magnitude of absolute changes in this variable in response to stimuli were increased in primary pulmonary hypertension. The variability of systemic pressure {{was similar to that}} in healthy volunteers. Basal and stimulated plasma catecholamine values were normal, suggesting preservation of normal sympathetic nervous system activity in primary pulmonary hypertension...|$|E
40|$|Alcohol abuse {{leads to}} impotence, infertility, and feminisation. Patients with chronic {{alcoholism}} may have impaired hypothalamic-pituitary function. The release of luteinising hormone {{was investigated in}} men with alcoholic cirrhosis with and without hypogonadism and controls. Blood was sampled every 15 minutes for six or eight hours and luteinising hormone concentrations measured by radioimmunoassay. Data were analysed by iterative <b>computerised</b> <b>analysis</b> and spectral analysis to assess pulsatile release {{and the length of}} the cycle, respectively. Pulsatile release of luteinising hormone was shown in all the control subjects; in the men with alcoholic liver disease it was normal in those with subclinical primary testicular failure but absent or grossly attenuated in those with overt combined central and primary gonadal failure. The impaired release of luteinising hormone in the men with overt gonadal failure might be due to a hypothalamic defect...|$|E
40|$|The {{application}} of a <b>computerised</b> image <b>analysis</b> system to simultaneously measure longitudinal and axial strain in naturally available anisotropic-viscoelastic material like leather is described. The results are used to calculate Poisson’s ratio and indicate that this parameter varies with position and orientation on the hide and depends {{on the degree of}} pre-existing fibre orientation. The origins of this dependency can be explained by a simple microstructural model. The practical implication of the findings for detecting lines of tightness (maximum fibre’s orientation) is discusse...|$|R
5000|$|Allardyce is a keen {{proponent of}} sports science and using {{technology}} and innovative techniques in coaching his teams, such as <b>computerised</b> performance <b>analysis</b> and yoga. Martin Hardy of The Independent {{described him as}} [...] "one of the pioneers of sports science in English football". Former players and pundits have cited his preparation as his main strength, which allows his teams to have better organisation and defensive stability. Former Bolton player Kevin Davies also highlighted Allardyce's man-management skills as a strength.|$|R
40|$|OBJECTIVES: To {{evaluate}} inter-observer {{agreement for}} microscopic measurement of inflammation in synovial tissue using manual quantitative, semiquantitative and <b>computerised</b> digital image <b>analysis.</b> METHODS: Paired serial sections of synovial tissue, obtained at arthroscopic biopsy {{of the knee}} from patients with rheumatoid arthritis (RA), were stained immunohistochemically for T lymphocyte (CD 3) and macrophage (CD 68) markers. Manual quantitative and semiquantitative scores for sub-lining layer CD 3 + and CD 68 + cell infiltration were independently derived in 6 international centres. Three centres derived scores using <b>computerised</b> digital image <b>analysis.</b> Inter-observer agreement was evaluated using Spearman's Rho and intraclass correlation coefficients (ICCs). RESULTS: Paired tissue sections from 12 patients were selected for evaluation. Satisfactory inter-observer agreement was demonstrated for all 3 methods of analysis. Using manual methods, ICCs for measurement of CD 3 + and CD 68 + cell infiltration were 0. 73 and 0. 73 for quantitative analysis and 0. 83 and 0. 78 for semiquantitative analysis, respectively. Corresponding ICCs of 0. 79 and 0. 58 were observed {{for the use of}} digital image analysis. All ICCs were significant at levels of p < 0. 0001. At each participating centre, use of <b>computerised</b> image <b>analysis</b> produced results that correlated strongly and significantly with those obtained using manual measurement. CONCLUSION: Strong inter-observer agreement was demonstrated for microscopic measurement of synovial inflammation in RA using manual quantitative, semiquantitative and computerised digital methods of analysis. This further supports the development of these methods as outcome measures in R...|$|R
40|$|<b>Computerised</b> <b>analysis</b> of {{the contact}} of gear teeth is {{currently}} dependent on numerical solution techniques involving implicit multi-equation systems. These present inherent convergence problems when the initial values are not {{close enough to}} the real solution and require significant computational effort. Here a comprehensive new solution is presented using a modified form for the fundamental gear meshing equations in two dimensions. This formulation allows the analytical reduction of the system of meshing equations to a single scalar equation, which is solved using a fast unconditionally stable numerical method. The need for careful determination of initial values for the numerical solution is eliminated and test runs on real gear geometries verify solution accuracy, stability and speed. Application of the algorithm to profile-modified involute gears and Geneva-type mechanisms and related results are shown. (C) 2011 Elsevier Ltd. All rights reserved...|$|E
40|$|The present {{research}} conducted a <b>computerised</b> <b>analysis</b> {{of the content}} of all lyrics from the United Kingdom’s weekly top 5 singles sales charts (Study 1, 1962 - 2011), and considered their macroeconomic correlates (Study 2, 1960 - 2011). Study 1 showed that coverage of interpersonal relationships consistently reflected a self-centred and unsophisticated approach; coverage of violence featured predominantly anti-authoritarian denial rather than overt depictions; and more recent lyrics were more stimulating. Study 2 showed no evidence that variations in lyrical optimism predicted future variations in economic optimism and subsequently GDP; but, consistent with the environmental security hypothesis, economic turbulence (defined as volatility in the closing price of the London Stock Exchange) was associated with the later popularity of lyrics concerning certainty and succour. These findings are discussed in terms of the advantages and limitations of computerised coding of lyrics. Open Acces...|$|E
40|$|OBJECTIVE: To {{investigate}} {{the effect of}} syntocinon augmentation on the fetal cardiotocogram (CTG) using <b>computerised</b> <b>analysis.</b> We hypothesised that syntocinon will have no direct effects on the fetal heart rate if used correctly. STUDY DESIGN: A retrospective, nested case-control study. SETTING: Intrapartum CTG records from the digital archive at the John Radcliffe Hospital, Oxford, UK. SUBJECTS: 110 women with singleton pregnancies of > 36 weeks gestation, no known congenital abnormality, spontaneous onset of labour and syntocinon augmentation for failure to progress, with start time of syntocinon recorded, from between August 1998 and December 1999, extensively matched to 110 controls who had normally progressing labours. METHODS: Eight different CTG features were measured during four time points with OxSys, a computerised numerical analysis system. STATISTICAL ANALYSIS: Differences in the CTG features over time in cases and controls using ANOVA and Friedman's ANOVA and {{at each time point}} between case-control pairs using Student's t-test and the Wilcoxon signed rank test. RESULTS: After administration, syntocinon increased the frequency, decreased the duration and decreased the resting time between contractions (p< 0. 001), resulting in no significant difference between normally progressing labours and those requiring augmentation. The case group had a significantly higher signal stability index (SSI) and fewer decelerations compared to the control group - differences which disappeared after augmentation was commenced (p= 0. 025 and 0. 033 respectively). Syntocinon did not affect the baseline heart rate, short term variability (STV) or phase rectified signal averaging (PRSA) (p= 0. 518, 0. 215 and 0. 138) in comparison with controls. There was {{a significant increase in the}} PRSA in babies born with acidaemia (arterial pH≤ 7. 05) 60 - 120 min after syntocinon was commenced that was not seen with in babies with a normal pH (p= 0. 002). CONCLUSION: Syntocinon "normalises" ineffective uterine activity without any direct effect on the fetal heart rate. Therefore its administration does not confound objective <b>computerised</b> <b>analysis.</b> There may be a specific response in PRSA shortly after commencing syntocinon augmentation in the fetus which is subsequently born acidaemic which requires further investigation...|$|E
40|$|To-date no {{large scale}} {{studies have been}} {{published}} that have used player tracking technology to investigate continuous time-motion analysis {{in the modern era}} of Women’s field hockey during Elite level International competition to investigate positional differences and inform fitness training and testing. A new <b>computerised</b> time-motion <b>analysis</b> method, Trak Performance was used to analyse individual player movement (n = 54) from 18 International Women’s hockey matches (18 defenders, 18 midfielders, 18 forwards). Overall analysis identified distance covered 9. 1 ± 1. 6 km, of which 74. 7 ± 9. 0...|$|R
40|$|The {{application}} {{and interpretation of}} <b>computerised</b> spectral <b>analysis</b> of the neonatal electroencephalogram (EEG) using the Neuroscience Berg Fourier Analyser (BFA) is described. Recordings are immediately available at the cotside. Electrophysiological changes can be recognised by individuals with no previous experience in EEG technology. The compact nature of the analysis allows long periods of recording to be viewed within minutes. In addition to the unequivocal demonstration of both clinical and subclinical seizures, the BFA is useful {{in the evaluation of}} interseizure activity [...] that is, disturbance of sleep patterns, electrical output, and hemisphere asymmetry...|$|R
40|$|Blog {{post for}} the Software Sustainability Institute. Excerpt: "For Open Science, it is {{important}} to cite the software you use in your research, as has been mentioned in previous articles on this blog. Particularly, you should cite any software that made a significant or unique impact on your work. Modern research relies heavily on <b>computerised</b> data <b>analysis,</b> and we should elevate its standing to a core research activity with data and software as prime research artefacts. Steps must be taken to preserve and cite software in a sustainable, identifiable and simple way. This is how digital repositories like Zenodo can help. ...|$|R
40|$|Electroencephalograms of 12 {{patients}} with a recent lacunar infarct, 12 {{patients with}} a recent cortical infarct and 12 control patients were studied without previous information about the clinical features and the corresponding CT scan findings. EEG assessment included both visual and <b>computerised</b> <b>analysis,</b> in both the eyes closed and the eyes open condition. The specificity and {{the sensitivity of the}} EEG in the diagnosis of lacunar infarction in this study were both 0. 8 (95 % confidence limits 0. 5 - 1). The positive predictive value of diagnosing a lacunar infarct {{on the basis of the}} EEG was 0. 7 (95 % confidence limits 0. 4 - 0. 9). The chance-corrected coefficient of agreement (kappa) between CT scanning and EEG was 0. 75. Thus, in contrast to the results of previous studies, most EEGs of patients with recent lacunar infarction show rather specific abnormalities...|$|E
40|$|When {{transacting}} {{and interacting}} through open computer networks, traditional methods {{used in the}} physical world for establishing trust can no longer be used. Creating virtual network substitutes with which people, organisations and software agents can derive trust in other parties requires <b>computerised</b> <b>analysis</b> of the underlying trust networks. This article describes an approach to trust network analysis using subjective logic (TNA-SL), that consists of the three following elements. Firstly it uses a concise notation with which trust transitivity and parallel combination of trust paths can be expressed. Secondly it defines a method for simplifying complex trust networks {{so that they can be}} expressed in this concise form. Finally it allows trust measures to be expressed as beliefs, so that derived trust can be automatically and securely computed with subjective logic. We compare our approach with trust derivation algorithms that are based on normalisation such as PageRank and EigenTrust. We also provide a numerical example to illustrates how TNA-SL can be applied...|$|E
40|$|Accurate {{thresholding}} and segmentation of three-dimensional structures within thick biological specimens {{is particularly}} difficult to achieve. However, {{there exists a}} vast array of possible methods and approaches with which to tackle this problem. In this paper, we describe the problems associated with cellular segmentation and <b>computerised</b> <b>analysis</b> of confocal derived data from living blood vessels. In addition, current segmentation methods are examined and discussed in relation to their use with biological samples. Finally, a novel iterative multilevel thresholding and splitting method for semiautomated 3 D segmentation of objects of different brightness and intensity homogeneity is presented. The method is particularly suited to segmentation of vascular cell volumes. The method has been tested on three typical confocal data sets, each of which have features common to 3 D volumes of living biological tissue. The segmenter has been estimated to produce results which are accurate to within 90 % of the “ground truth” measurements...|$|E
40|$|Wood fibres are {{the main}} {{constituent}} of paper and are also used to alter properties of plastics in wood-fibre—based composite materials. The manufacturing of these materials involves numerous parameters that determine {{the quality of the}} products. The link between the manufacturing parameters and the final products can often be found in properties of the microstructure, which calls for advanced characterisation methods of the materials. <b>Computerised</b> image <b>analysis</b> is the discipline of using computers to automatically extract information from digital images. <b>Computerised</b> image <b>analysis</b> can be used to create automated methods suitable for the analysis of large data volumes. Inherently these methods give reproducible results and are not biased by individual analysts. In this thesis, three-dimensional X-ray computed tomography (CT) at micrometre resolution is used to image paper and composites. Image analysis methods are developed to characterise properties of individual fibres, properties of fibre—fibre bonds, and properties of the whole fibre networks based on these CT images. The main contributions of this thesis is the development of new automated image-analysis methods for characterisation of wood-fibre—based materials. This include the areas of fibre—fibre contacts and the free—fibre lengths. A method for reduction of phase contrast in mixed mode CT images is presented. This method retrieves absorption from images with both absorption and phase contrast. Curvature calculations in volumetric images are discussed and a new method is proposed that is suitable for three-dimensional images of materials with wood fibres, where the surfaces of the objects are close together...|$|R
40|$|A largely {{subjective}} provisional {{classification of}} virgin indigenous forests has been produced for the South Island of New Zealand. Three geographical zones were recognised, and within each zone forest types were identified {{from a combination}} of two approaches. Canopy-tree stocking data collected during the systematic line-plot sampling of the National Forest Survey of New Zealand (1946 - 55) were examined with the assistance of a <b>computerised</b> cluster <b>analysis</b> technique. In areas not covered by the National Forest Survey plot sampling, forest descriptions from more recent work were used. The resultant 94 forest types were distributed between 10 forest classes to produce a three-level classification...|$|R
40|$|Graph {{classification}} is {{an increasingly}} important step in numerous application domains, such as function prediction of molecules and proteins, <b>computerised</b> scene <b>analysis,</b> and anomaly detection in program flows. Among the various approaches proposed in the literature, graph classification based on frequent subgraphs is a popular branch: Graphs are represented as (usually binary) vectors, with components indicating whether a graph contains a particular subgraph that is frequent across the dataset. On large graphs, however, one faces the enormous problem {{that the number of}} these frequent subgraphs may grow exponentially with the size of the graphs, but only few of them possess enough discriminative power to make the...|$|R
