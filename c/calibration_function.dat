223|262|Public
5000|$|... #Caption: Analysis of food-dyes: (A) {{photo of}} HPTLC plate (developed from both sides), (B) multi-wavelength scan of mix 1, (C) <b>calibration</b> <b>function,</b> (D) mass spectra of {{selected}} zones, (E) results ...|$|E
50|$|The basic {{approach}} is to form a weighted average of the 353 clock CpGs, which is then transformed to DNAm age using a <b>calibration</b> <b>function.</b> The <b>calibration</b> <b>function</b> reveals that the epigenetic clock has a high ticking rate until adulthood, after which it slows to a constant ticking rate. Using the training data sets, Horvath used a penalized regression model (Elastic net regularization) to regress a calibrated version of chronological age on 21,369 CpG probes that were present both on the Illumina 450K and 27K platform and had fewer than 10 missing values. DNAm age is defined as estimated ("predicted") age. The elastic net predictor automatically selected 353 CpGs. 193 of the 353 CpGs correlate positively with age while the remaining 160 CpGs correlate negatively with age. R software and a freely available web-based tool {{can be found at}} the following webpage.|$|E
50|$|Biosensors {{can be used}} to {{indirectly}} measure BOD via a fast (usually <30 min) to {{be determined}} BOD substitute and a corresponding calibration curve method (pioneered by Karube et al., 1977). Consequently, biosensors are now commercially available, but they do have several limitations such as their high maintenance costs, limited run lengths due to the need for reactivation, and the inability to respond to changing quality characteristics as would normally occur in wastewater treatment streams; e.g. diffusion processes of the biodegradable organic matter into the membrane and different responses by different microbial species which lead to problems with the reproducibility of result (Praet et al., 1995). Another important limitation is the uncertainty associated with the <b>calibration</b> <b>function</b> for translating the BOD substitute into the real BOD (Rustum et al., 2008).|$|E
40|$|Soil {{moisture}} content and dry density of unbound granular pavement materials are important properties for compaction control that influence pavement performance under cyclic loading. Under these loading conditions, increasing {{moisture content}} can accelerate {{significant changes in}} density. Time domain reflectometry (TDR) is a method for measuring the moisture content and density of soils with rod probe sensors. This paper introduces new <b>calibration</b> <b>functions</b> for TDR measurements using these rod probe sensors embedded in the soil. TDR measurements were taken in the laboratory for a typical road base material at two basically different conditions: at constant moisture content with different dry densities and at constant dry density with different moisture contents. In this study, a relationship was developed between the voltage drop occurring for the passage of an electromagnetic wave through the soil and the bulk density. The permittivity of the soil sample obtained from the travel time of TDR signals was {{used to calculate the}} volumetric moisture content. Finally, the gravimetric moisture content was obtained from the volumetric moisture content and bulk density relationship. For the validation of the <b>calibration</b> <b>functions,</b> rod probe sensors were installed in a road to obtain in situ moisture content and density under field conditions. Laboratory results indicate that the <b>calibration</b> <b>functions</b> are independent of moisture and density, and the field test shows the applicability of the method. The newly developed <b>calibration</b> <b>functions</b> allow for the monitoring of the long-term pavement performance, leading {{to a better understanding of}} the time-dependent evolution of, for example, rutting of roads...|$|R
30|$|Further unique {{points of}} the PWE are indebted to the {{abundance}} and flexibility of the observation modes and <b>calibration</b> <b>functions</b> utilizing the onboard software combining with the hardware specifications. Detailed specifications and performance of the onboard software are presented in Matsuda et al. (2018).|$|R
40|$|This paper {{deals with}} design and {{prototype}} {{development of an}} Active Pixel Sensor – based miniature sun sensor and a laboratory facility for its indoor test and calibration. The miniature sun sensor is described and the laboratory test facility is presented in detail. The major focus of the paper is on tests and calibration of the sensor. Two different <b>calibration</b> <b>functions</b> have been adopted. They are based, respectively, on a geometrical model, which has required least-squares optimisation of system physical parameters estimates, and on neural networks. Calibration results are presented for the above solutions, showing that accuracy {{in the order of}} 0. 01 ° has been achieved. Neural <b>calibration</b> <b>functions</b> have attained better performance thanks to their intrinsic auto-adaptive structure...|$|R
50|$|The {{detector}} used is broadband; it {{responds to}} all the light that reaches it. If {{a significant amount of}} the light passed through the sample contains wavelengths that have much lower extinction coefficients than the nominal one, the instrument will report an incorrectly low absorbance. Any instrument will reach a point where an increase in sample concentration will not result in an increase in the reported absorbance, because the detector is simply responding to the stray light. In practice the concentration of the sample or the optical path length must be adjusted to place the unknown absorbance within a range that is valid for the instrument. Sometimes an empirical <b>calibration</b> <b>function</b> is developed, using known concentrations of the sample, to allow measurements into the region where the instrument is becoming non-linear.|$|E
40|$|Abstract − Calibration {{procedures}} are widely implemented in metrology. This paper considers {{the evaluation of}} measurement uncertainty {{associated with the use}} of a <b>calibration</b> <b>function.</b> Three methods for uncertainty evaluation are described and the differences in the results returned are discussed. A simple <b>calibration</b> <b>function</b> is used to illustrate these differences...|$|E
30|$|App 05 {{performs}} {{data processing}} for special functions (e.g., onboard <b>calibration</b> <b>function).</b>|$|E
40|$|Abstract. We {{analyze the}} co-alignment between Hinode’s BFI-Gband images and {{simultaneous}} SP maps {{with the aim}} of characterizing the general offsets between them and the second order non-linear effects in SP’s slit scanning mechanism. We provide <b>calibration</b> <b>functions</b> and parameters to correct for the nominal pixel scales and positioning...|$|R
40|$|Preliminary {{calibration}} of whole-sky irradiance for the Southern Great Plains (SGP) Cloud and Radiation Testbed (CART) site Ground Radiation Measurement System (GRAMS) and GRAMSCAL {{instruments were}} carried out relative to Broadband Outdoor Radiometer CALibration (BORCAL) data. Three one-month periods, two with eight days and one with nine days of about 1. 5 % absolute accuracy BORCAL data are available for comparison under relatively clear-sky conditions. GRAMS data is at hand for {{all three of these}} periods and GRAMSCAL data is available for two of them. Simple <b>calibration</b> <b>functions</b> are obtained that yield irradiance accurate to 5 % or better for both instruments as long as the sun is higher than 10 to 20 degrees above the horizon. The <b>calibration</b> <b>functions</b> seem to be very stable over the 13 -month period for which data has been collected and, therefore, presumably for longer...|$|R
50|$|The {{magnitude}} of an earthquakes can be roughly estimated {{by measuring the}} area affected by intensity level III or above in square kilometres and taking the logarithm. A more acurrate estimate relies {{on the development of}} regional <b>calibration</b> <b>functions</b> derived using many isoseismal radii. Such approaches allow magnitudes to be estimated for historical earthquakes.|$|R
40|$|In {{conditional}} copula models, the copula parameter is deterministically {{linked to}} a covariate via the <b>calibration</b> <b>function.</b> The latter is of central interest for inference and is usually estimated nonparametrically. However, when a parametric model for the <b>calibration</b> <b>function</b> is appropriate, the resulting estimator exhibits significant gains in statistical efficiency and requires smaller computational costs. We develop methodology for testing a parametric formulation of the <b>calibration</b> <b>function</b> against a general alternative and propose a generalized likelihood ratio-type test that enables conditional copula model diagnostics. We derive the asymptotic null distribution of the proposed test and study its finite sample performance using simulations. The method is applied to two data examples...|$|E
40|$|Liquid chromatography–mass {{spectrometry}} {{has become}} a powerful analytical tool, with high selectivity and sensitivity. Usually in this technique, the <b>calibration</b> <b>function</b> is estimated from the molecular peak signal. This report describes the improvement in sensitivity when the signals from several fragments {{in addition to the}} molecular peak are used to establish the <b>calibration</b> <b>function.</b> The influence of the dwell time has also been analysed as an important instrumental parameter that influences the signal range, and consequently, the sensitivity. The <b>calibration</b> <b>function</b> obtained by adding fragment signals was used to estimate the instrumental detection limit using three different procedures, comparing and discussing the results obtained. © 2006 Elsevier B. V. All rights reserved...|$|E
40|$|A {{new method}} for {{determining}} the temperature <b>calibration</b> <b>function</b> for a high-temperature X-ray diffraction stage is described. This method utilizes the relative thermal expansion of two diffraction peaks, either for peaks from a mixture of phases, or from a single phase that has anisotropic thermal expansion. A <b>calibration</b> <b>function</b> is derived from data at many temperatures and collected over a narrow angular range. Because only the relative separations of closely spaced peaks are used, this method is fast and insensitive to geometric aberrations. Materials used for calibration can include phase transition standards for an independent check on the <b>calibration</b> <b>function.</b> A new type of single-point temperature calibration standard can also be constructed that relies only on the observation of peak-crossing events at specific temperatures that are caused by relative thermal expansion...|$|E
40|$|The thesis {{deals with}} the {{calibration}} of a digital camera for a purpose of luminance measurement. The aim is to create <b>calibration</b> approximation <b>functions</b> allowing the calculation of luminance. The first part analyses the basic light-technical parameters, {{as well as the}} principle of luminance measurement and the features of digital cameras for this purpose. An important issue, introduced in the present thesis, is to adapt a digital camera as a luminance meter. Furthermore, there is a description of the preparation of a site for the calibration, including necessary devices. The last part is dedicated to define the acquired <b>calibration</b> <b>functions</b> for the respective light sources and their mutual comparison...|$|R
40|$|A smart {{pressure}} transducer with a piezo resistive sensor microbridge, digitally controlled readout electronics, and a nonlinear temperature compensation based on spline functions, is integrated {{on a single}} CMOS chip. The chip also contains a temperature sensor and control hardware for test and <b>calibration</b> <b>functions.</b> The <b>calibration</b> process of the sensing and temperature compensation parameters has been optimized for high volume production...|$|R
40|$|Description chemCal {{provides}} simple {{functions for}} plotting linear <b>calibration</b> <b>functions</b> and estimating standard errors for measurements {{according to the}} Handbook of Chemometrics and Qualimetrics: Part A by Massart et al. There are also functions estimating the limit of detection (LOD) and limit of quantification (LOQ). The functions work on model objects from- optionally weighted- linear regression (lm) or robust linear regression (rlm from the MASS package) ...|$|R
30|$|A <b>calibration</b> <b>function</b> from 100 to 1000  µg/l {{for each}} sugar ester was {{prepared}} for quantification of the extracted targets (see Additional file 1).|$|E
40|$|This study {{investigates the}} {{accuracy}} of spatially resolved skin friction velocity ut measurements using {{a large number of}} Irwin sensors (Irwin, 1981). A fast and simple procedure to calibrate the sensors against a two-component hot-film anemometer is introduced which leads to a universal <b>calibration</b> <b>function</b> that can then be used for all sensors. Insignificantly lower accuracies are obtained using the universal <b>calibration</b> <b>function</b> instead of individual calibration functions for each sensor. The overall accuracy of the skin friction velocity measurements averages about ± 5...|$|E
40|$|ABSTRACT: The {{general purpose}} of this study is the {{determination}} of J-integral when dealing with fracture of a penny-shaped crack on the end of the stiff cylindrical cord embedded in rubber matrix. The dimensional analysis is applied to derive a general equation of J-integral, and then it is assumed that the equation of J-integral can be separated into the deformation and geometry functions, the validity of which is proved by using separation parameter. The J-integral is expressed in a multiplicative form in which the geometry <b>calibration</b> <b>function</b> (or factor) is introduced in order to take into account the finite dimensions. The values of the J-integral for the rubber-cord composites with various crack and specimen radius are obtained by using finite element analysis (FEA), and these results are used for concretely determining the geometry <b>calibration</b> <b>function,</b> which, in this work, is expressed in a polynomial form of fourth order. The deformation <b>calibration</b> <b>function,</b> which is a constant for a linear elastic materials, characterizing the large deformable nonlinear effect in rubbery materials is obtained by comparing the equation of J-integral with the ones for the linear elastic deformation. As we approach the infinitesimal strain, the value of the deformation <b>calibration</b> <b>function</b> comes close to the result of linear elastic fracture mechanics (LEFM) ...|$|E
40|$|We {{analyze the}} co-alignment between Hinode's BFI-Gband images and {{simultaneous}} SP maps {{with the aim}} of characterizing the general off-sets between them and the second order non-linear effects in SP's slit scanning mechanism. We provide <b>calibration</b> <b>functions</b> and parameters to correct for the nominal pixel scales and positioning. Comment: 4 pages, 1 figure, proceedings of the Second Hinode Science Meeting, held in Boulder, CO, Sept 30 - Oct 3, 200...|$|R
40|$|In {{addition}} to residual stresses {{sheet metal forming}} induces characteristic crystallographic texture, hence, the material behavior is anisotropic. In general, the standard evaluation procedures of residual stress analysis techniques are limited to isotropic material states. In the present paper deep drawn steel cups of dual phase steel DP 600 are analyzed by using a recently proposed calibration approach for residual stress analysis {{by means of the}} incremental hole-drilling method for highly textured material states. It is based on the differential method, which is enhanced with four case specific <b>calibration</b> <b>functions.</b> The multiple case specific <b>calibration</b> <b>functions</b> are determined by means of finite element simulations using the orientation distribution function (ODF) in combination with Hills assumption and single crystal elastic constants of iron to calculate the effective elasticity tensor to account for elastic anisotropy. Supplementary, the deep drawing process is simulated using a finite element model based on the Hill 48 yield criterion. Finally, the comparison shows that the numerical results are in satisfactory agreement to the experimental data...|$|R
40|$|The Gravity Recovery and Climate Experiment {{launched}} March 17, 2002. The GPS {{data for}} this experiment are processed {{to contribute to the}} recover long wavelength gravity field; remove errors due to long term on-board oscillator drift; and align K/Ka-band measurments between the two spacecraft to 0. 1 ns. This paper will concentrate on the use of GPS for these timing and <b>calibration</b> <b>functions</b> and will not address the recovery of the gravity field...|$|R
40|$|In this paper, a Bimorph Accelerometer with {{symmetric}} bimorphs {{and central}} supporting pole configuration is developed and analytically characterized. <b>Calibration</b> <b>function</b> is defined {{to describe the}} system dynamics of the accelerometer. Closed-form expression for the <b>calibration</b> <b>function</b> is then derived based on piezoelectric constitutive equations and beam theory. The analysis {{takes into account the}} effect of device geometry and material properties, and agrees well with the results obtained by the finite element analysis. Besides producing accurate predications, it is found that with appropriate geometrical dimensions, both high sensitivity and broad frequency bandwidth, two most important parameters in evaluating accelerometers, can be achieved. ...|$|E
40|$|Evidence {{suggests}} the calibration of hypothetical and actual behavior is good-specific. We examine whether clustering commodities into mutual categories {{can reduce the}} burden. While we reject a common calibration across sets of commodities, a sport-specific <b>calibration</b> <b>function</b> cannot be rejected. Valuation, Calibration, Clusters...|$|E
40|$|A {{fundamental}} component of any accountability system for nuclear materials is a tank calibration equation that relates {{the height of}} liquid in a tank to its volume. Tank volume calibration equations are typically determined from pairs of height and volume measurements taken {{in a series of}} calibration runs. After raw calibration data are standardized to a fixed set of reference conditions, the calibration equation is typically fit by dividing the data into several segments [...] corresponding to regions in the tank [...] and independently fitting the data for each segment. The estimates obtained for individual segments must then be combined to obtain an estimate of the entire <b>calibration</b> <b>function.</b> This process is tedious and time-consuming. Moreover, uncertainty estimates may be misleading because it is difficult to properly model run-to-tun variability and between-segment correlation. In this paper, they describe a model whose parameters can be estimated simultaneously for all segments of the calibration data, thereby eliminating the need for segment-by-segment estimation. The essence of the proposed model is to define a suitable polynomial to fit to each segment and then extend its definition to the domain of the entire <b>calibration</b> <b>function,</b> so that it (the entire <b>calibration</b> <b>function)</b> can be expressed as the sum of these extended polynomials. The model provides defensible estimates of between-run variability and yields a proper treatment of between-segments correlations. A portable software package, called TANCS, has been developed to facilitate the acquisition, standardization, and analysis of tank calibration data. The TANCS package was used for the calculations in an example presented to illustrate the unified modeling approach described in this paper. With TANCS, a trial <b>calibration</b> <b>function</b> can be estimated and evaluated in a matter of minutes...|$|E
40|$|We {{describe}} a calibration technique for a hand-immersive virtual-reality environment [...] the Reachin Display [1]. The technique we present {{allows us to}} calibrate both the haptics device and the stereo display environment simultaneously where calibration of the stereo display utilises the haptics device and vice versa. The calibration procedure is described and we present a modular architecture for constructing <b>calibration</b> <b>functions</b> and show how fine control over the individual calibration parameters that are used can be obtained...|$|R
40|$|Abstract: In the {{preliminary}} aerodynamic aircraft design, optimum calculation methods should be used, which enable engineers {{to perform a}} vast number of test runs in a reasonably short time. Such tools should not only be fairly simple but also reliable. In the first part, this paper briefly presents aerodynamic analyses performed by a 3 D vortex lattice method (VLM), with an aim to verify its capabilities to give results that coincide well with the experimental data of an existing airplane. Since this computational model is based on inviscid flow concept, effectiveness of control surfaces and flaps are inherently overestimated. In order to compensate for the omitted boundary layer influence, a set of calibration diagrams for effectiveness and circulation influence has been successfully derived and good agreements with wind tunnel test data have been achieved. After several necessary adjustments, <b>calibration</b> <b>functions</b> have been applied to VLM analysis within a new light aircraft conceptual study. Those results have been compared with results obtained by well known Datcom method and verygood agreements have been achieved, proving that VLM computations with properly defined <b>calibration</b> <b>functions</b> can be both efficient and reliable tools in preliminary aerodynamic design...|$|R
40|$|The paper {{deals with}} the use of neural {{networks}} for the determination of pressure altitude and Mach number of a fly-by-wire high-performance aircraft during flight. In previous works the authors developed a methodology based on polynomial <b>calibration</b> <b>functions</b> for the determination of such flight parameters, together with the angles of attack and sideslip. Such an approach provided successful results, but the use of different polynomial functions in different areas was needed to map the entire flight envelope. The fading methodologies for the management of polynomial functions overlap and considerably increased both procedure complexity and the time to spent for the procedure tuning. In particular, the <b>calibration</b> <b>functions</b> related to the Mach number and static-pressure estimation are susceptible to these problems because of their high nonlinearity. The alternative approach studied in this paper, based on neural networks, provides a level of accuracy comparable with that of polynomial functions. However, such an approach is simpler, because it allows the entire flight envelope to be mapped by means of a single network for each output parameter, and so it eliminates the fading problems. In addition, the new procedure is extremely easier to tune when new data from flight tests are available. This is a very important point, because several versions of the air data computation algorithms are generally to be developed in parallel with the flight-envelope enlargement of a new aircraft...|$|R
40|$|ABSTRACT: Measurement of particles, whether grains on a {{microscope}} slide or {{image of a}} muck or stock pile, is two dimensional. Principles of geometric probability and stereology {{can be used to}} reconstruct or unfold a three dimensional size distribution. This analytical solution or unfolding function can be calibrated with an empirical <b>calibration</b> <b>function.</b> ...|$|E
40|$|A simple linear <b>calibration</b> <b>function</b> {{can be used}} over a wide {{concentration}} range for the Inductively Coupled Plasma (ICP) spectrometer due to its linear response. The random errors over wide {{concentration range}}s are not constant, and constant variance regression {{should not be used}} to estimate the <b>calibration</b> <b>function.</b> Weighted regression techniques are appropriate if the proper weights can be obtained. Use of the calibration curve to estimate the concentration of one or more unknown samples is straightforward, but confidence interval estimation for multiple use of the calibration curve is less obvious. We describe a method for modeling the error along the ICP calibration curve and using the estimated parameters from the fitted model to calculate weights for the calibration curve fit. Multiple and single-use confidence interval estimates are obtained and results along the calibration curve are compared...|$|E
40|$|Dose resolution, DΔp, is {{becoming}} a common method for characterizing {{the performance of a}} gel dosimeter. In this note we examine how the goodness of fit of the <b>calibration</b> <b>function</b> affects DΔp and show that its inclusion in the calculation of DΔp is essential to avoid overestimating the performance of the gel. 8 page(s...|$|E
40|$|A fiber Bragg grating (FBG) {{interrogation}} {{system based}} on an intensity demodulation and demultiplexing of an arrayed waveguide grating (AWG) module is examined in detail. The influence of the spectral line shape of the FBG on the signal obtained from the AWG device is discussed by accomplishing the measurement and simulation of the system. The simulation of the system helps to create quickly and precisely <b>calibration</b> <b>functions</b> for nonsymmetric, tilted, or nonapodized FBGs. Experiments show that even small sidebands of nonapodized FBGs have strong influences on the signal resulted by an AWG device with a Gaussian profile...|$|R
40|$|International audienceTo enable post-processing, {{the output}} of a support vector data {{description}} (SVDD) should be a calibrated probability as done for SVM. Standard SVDD does not provide such probabilities. To create probabilities, we first generalize the SVDD model and propose two <b>calibration</b> <b>functions.</b> The first one uses a sigmoid model {{and the other one}} is based on a generalized extreme distribution model. To estimate calibration parameters, we use the consistency property of the estimator associated with a single SVDD model. A synthetic dataset and datasets from the UCI repository are used to compare the performance against a robust kernel density estimator...|$|R
40|$|Abstract: The paper {{comprises}} {{design and}} a calibration campaign of the low-cost APS-based miniature digital sun sensor {{which has been}} developed at the University of Naples “Federico II”. Design of the miniature sun sensor is described and the laboratory test facility is presented in details. The paper is focused on tests and calibration of the sensor. A set of different <b>calibration</b> <b>functions</b> have been adopted. They are based, respectively, on various interpolation and approximation techniques and on neural networks. Calibration results are presented for the above solutions showing that accuracy {{of the order of}} 0. 01 ° has been achieved. Note: Publication language:russia...|$|R
