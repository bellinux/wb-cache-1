79|9|Public
25|$|ATSDR has a <b>computational</b> <b>toxicology</b> {{laboratory}} that conducts {{research and}} modeling {{on the effects}} of toxic substances on human health. The agency's toxicology work involves pharmacokinetic/pharmacodynamic modeling, quantitative structure–activity relationship methods, and benchmark dose modeling, as well as establishing minimal risk levels for human exposure to hazardous substances. One model developed by the toxicology laboratory showed that children were much more susceptible than adults to chemical exposure from inhalation and oral exposure. In the aftermath of chemical spills and emergencies, the laboratory also conducts research for state and local health departments on the health effects of the chemicals involved.|$|E
2500|$|Providing {{bioinformatics}} and <b>computational</b> <b>toxicology</b> {{support to}} National Toxicology Program and NIEHS projects, especially {{those related to}} Tox21 ...|$|E
2500|$|National Center for <b>Computational</b> <b>Toxicology,</b> National Center for Environmental Assessment, National Center for Environmental Research, National Exposure Research Laboratory, National Health and Environmental Effects Research Laboratory, National Homeland Security Research Center, National Risk Management Research Laboratory ...|$|E
50|$|BIOSCI, {{also known}} as Bionet, {{is a set of}} {{electronic}} communication forum used by life scientists around the world. It includes the Bionet Usenet newsgroups and parallel e-mail lists, with public archives since 1992 at www.bio.net. BIOSCI/Bionet provides public, open access biology news and discussion for areas such as molecular biology methods and reagents, bioinformatics software and <b>computational</b> biology, <b>toxicology,</b> and several organism communities including yeast, C.elegans and annelida (worms), the plant arabidopsis, fruitfly, maize (corn), and others.|$|R
40|$|We {{present the}} open source {{components}} for drug discovery {{that has been}} developed and integrated into the graphical workbench Bioclipse. Building on a solid open source cheminformatics core, Bioclipse has advanced functionality for managing and visualizing chemical structures and related information. The features presented here include QSAR/QSPR modeling, various predictive solutions such as decision support for chemical liability assessment, site-of-metabolism prediction, virtual screening, and knowledge discovery and integration. We demonstrate {{the utility of the}} described tools with examples from <b>computational</b> pharmacology, <b>toxicology,</b> and ADME. Bioclipse is used in both academia and industry, and {{is a good example of}} open source leading to new solutions for drug discovery...|$|R
40|$|Selecting most {{rigorous}} quantitative {{structure-activity relationship}} (QSAR) approaches is of great importance {{in the development of}} robust and predictive models of chemical toxicity. To address this issue in a systematic way, we have formed an international virtual collaboratory consisting of six independent groups with shared interests in <b>computational</b> chemical <b>toxicology.</b> We have compiled an aqueous toxicity data set containing 983 unique compounds tested in the same laboratory over a decade against Tetrahymena pyriformis. A modeling set including 644 compounds was selected randomly from the original set and distributed to all groups that used their own QSAR tools for model development. The remaining 339 compounds in the original set (external set I) as well as 110 additional compounds (external set II) published recently by the same laboratory (after this computational study was already in progress) were used as two independent validation sets to assess the external predictive power of individual models. In total, our virtual collaboratory has developed 15 different types of QSAR models of aquatic toxicity for the training set. The interna...|$|R
5000|$|Providing {{bioinformatics}} and <b>computational</b> <b>toxicology</b> {{support to}} National Toxicology Program and NIEHS projects, especially {{those related to}} Tox21 ...|$|E
5000|$|Bois F., 2012, Bayesian inference, in <b>Computational</b> <b>Toxicology</b> vol. II, Reisfeld B., Mayeno A.N. Eds., Methods in Molecular Biology Series, 930:597-636, Humana Press, New-York, doi: 10.1007/978-1-62703-059-5_25.|$|E
50|$|Medicinal chemists and {{structural}} biologists study toxicophores {{in order to}} predict (and hopefully avoid) potentially toxic compounds early in the drug development process. Toxicophores can also be identified in lead compounds and removed or replaced later in the process with less toxic moieties. Both techniques, in silico (predictive) and a posteriori (experimental), are active areas of chemoinformatics research and development, within the field known as <b>Computational</b> <b>Toxicology.</b> For example, in the United States, the EPA's National Center for <b>Computational</b> <b>Toxicology</b> sponsors several toxicity databases based on predictive modeling as well as high-throughput screening experimental methods.|$|E
40|$|Presented at CMTPI 2007 : <b>Computational</b> Methods in <b>Toxicology</b> and Pharmacology Integrating Internet Resources (Moscow, Russia, September 1 – 5, 2007). Results from {{extensive}} 70 ns all-atom {{molecular dynamics}} simulations of catechol-O-methyltransferase (COMT) enzyme are reported. The simulations were performed with explicit TIP 3 P water and Mg 2 ¿+ ions. Four different crystal structures of COMT, {{with and without}} different ligands, were used. These simulations {{are among the most}} extensive of their kind and as such served as a stability test for such simulations. On the methodological side we found that the initial energy minimization procedure may be a crucial step: particular hydrogen bonds may break, and this can initiate an irreversible loss of protein structure that becomes observable in longer time scales of the order of tens of nanoseconds. This has important implications for both molecular dynamics and quantum mechanics–molecular mechanics simulations. Keywords: catechol-O-methyltransferase, COMT, molecular dynamics, hydrogen bonds, stability, protein dynamic...|$|R
40|$|Systems {{biology and}} {{synthetic}} biology are emerging disciplines which {{are becoming increasingly}} utilised in several areas of bioscience. Toxicology is beginning to benefit from systems biology and we suggest {{in the future that}} is will also benefit from synthetic biology. Thus, a new era is on the horizon. This review illustrates how a suite of innovative techniques and tools can be applied to understanding complex health and toxicology issues. We review limitations confronted by the traditional <b>computational</b> approaches to <b>toxicology</b> and epidemiology research, using polycyclic aromatic hydrocarbons (PAHs) and their effects on adverse birth outcomes as an illustrative example. We introduce how systems toxicology (and their subdisciplines, genomic, proteomic, and metabolomic toxicology) will help to overcome such limitations. In particular, we discuss {{the advantages and disadvantages of}} mathematical frameworks that computationally represent biological systems. Finally, we discuss the nascent discipline of synthetic biology and highlight relevant toxicological centred applications of this technique, including improvements in personalised medicine. We conclude this review by presenting a number of opportunities and challenges that could shape the future of these rapidly evolving disciplines...|$|R
40|$|Copyright © 2015 Mark T. Mc Auley et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License 3. 0, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Systems biology and synthetic biology are emerging disciplines which are becoming increasingly utilised in several areas of bioscience. Toxicology is beginning to benefit from systems biology and we suggest {{in the future that}} is will also benefit from synthetic biology. Thus, a new era is on the horizon. This review illustrates how a suite of innovative techniques and tools can be applied to understanding complex health and toxicology issues. We review limitations confronted by the traditional <b>computational</b> approaches to <b>toxicology</b> and epidemiology research, using polycyclic aromatic hydrocarbons (PAHs) and their effects on adverse birth outcomes as an illustrative example. We introduce how systems toxicology (and their subdisciplines, genomic, proteomic, and metabolomic toxicology) will help to overcome such limitations. In particular, we discuss {{the advantages and disadvantages of}} mathematical frameworks that computationally represent biological systems. Finally, we discuss the nascent discipline of synthetic biology and highlight relevant toxicological centred applications of this technique, including improvements in personalised medicine. We conclude this review by presenting a number of opportunities and challenges that could shape the future of these rapidly evolving disciplines...|$|R
50|$|The US Environmental Protection Agency's The Distributed Structure-Searchable Toxicity (DSSTox) Database Network is {{a project}} of EPA's <b>Computational</b> <b>Toxicology</b> Program. The {{database}} provides SDF molecular files {{with a focus on}} carcinogenic and otherwise toxic substances.|$|E
5000|$|National Center for <b>Computational</b> <b>Toxicology,</b> National Center for Environmental Assessment, National Center for Environmental Research, National Exposure Research Laboratory, National Health and Environmental Effects Research Laboratory, National Homeland Security Research Center, National Risk Management Research Laboratory ...|$|E
5000|$|Dorne J.L., Amzal P., Bois F., Crepet A., Tressou J., Verger P., 2012, Population {{effects and}} variability, in <b>Computational</b> <b>Toxicology</b> vol. I, Reisfeld B., Mayeno A.N. Eds., Methods in Molecular Biology Series, 929:521-581, Humana Press, New-York, doi: 10.1007/978-1-62703-050-2_20.|$|E
40|$|The prompt {{identification}} of chemical molecules with potential effects on liver may help in drug discovery and in raising {{the levels of}} protection for human health. Besides in vitro approaches, <b>computational</b> methods in <b>toxicology</b> are drawing attention. We built a structure-activity relationship (SAR) model for evaluating hepatotoxicity. After compiling a data set of 950 compounds {{using data from the}} literature, we randomly split it into training (80 %) and test sets (20 %). We also compiled an external validation set (101 compounds) for evaluating the performance of the model. To extract structural alerts (SAs) related to hepatotoxicity and non-hepatotoxicity we used SARpy, a statistical application that automatically identifies and extracts chemical fragments related to a specific activity. We also applied the chemical grouping approach for manually identifying other SAs. We calculated accuracy, specificity, sensitivity and Matthews correlation coefficient (MCC) on the training, test and external validation sets. Considering the complexity of the endpoint, the model performed well. In the training, test and external validation sets the accuracy was respectively 81 %, 63 % and 68 %, specificity 89 %, 33 % and 33 %, sensitivity 93 %, 88 % and 80 % and MCC 0. 63, 0. 27 and 0. 13. Since it is preferable to overestimate hepatotoxicity rather than not to recognize unsafe compounds, the model’s architecture followed a conservative approach. As it was built using human data, it might be applied without any need for extrapolation from other species. This model will be freely available in the VEGA platform...|$|R
40|$|AbstractThe VirtualToxLab is an {{in silico}} {{technology}} for estimating the toxic potential – endocrine and metabolic disruption, {{as well as}} aspects of carcinogenicity and cardiotoxicity – of drugs, chemicals and natural products. The technology {{is based on an}} automated protocol that simulates and quantifies the binding of small molecules towards a series of currently 16 proteins, known or suspected to trigger adverse effects. The simulations are conducted at the atomic level and explicitly allow for a mechanistic interpretation of the results (in real-time 3 D/ 4 D), thereby complying with the Setubal principles put forward in 2002 for <b>computational</b> approaches to <b>toxicology.</b> Moreover, the underlying “ab-initio” protocol is independent from any training data and makes the approach universal with respect to the applicability domain. The VirtualToxLab runs in client–server mode and is freely available to academic and non-profit organizations. As the underlying technology yields a thermodynamic estimate of the binding affinity, the associated ligand–protein complexes have been challenged by molecular-dynamics simulations to probe their kinetic stability. Human African trypanosomiasis is a neglected tropical disease caused by two subspecies of Trypanosoma brucei. The control of this parasitic infection relies on a few chemotherapeutic agents, most of which were discovered decades ago and pose many challenges including adverse side effects, poor efficacy, and the occurrence of drug resistances. Natural products, on the other hand, offer a high potential for the discovery of new drug leads due to their chemical diversity. In this in silico study, we analyze a series of 89 natural products and derivatives displaying anti-trypanosomal activity for their potential to trigger adverse effects. Our results indicate a moderate potential for a number of those compounds to bind to nuclear receptors and thereby ease the development of endocrine disregulation. A few others would seem to inhibit enzymes of the cytochrome P 450 family and, hence, sustain drug–drug interactions...|$|R
50|$|Georgopoulos {{has been}} {{a faculty member at}} Rutgers University since 1989. He has held {{teaching}} positions in the department of environmental sciences, the department of chemical and biochemical engineering, and the department of environmental and occupational health. He {{is also a member of}} the Environmental and Occupational Health Sciences Institute in Piscataway, New Jersey, and has served as director of the institute's occupational chemodynamics laboratory and its ozone research center. He has also served as director for the Informatics and <b>Computational</b> <b>Toxicology</b> Core of the NIEHS Center for Environmental Exposures and Disease at EOHSI, and is an associate member of the Rutgers Cancer Institute. In 2010, he became co-director of the Environmental Bioinformatics and <b>Computational</b> <b>Toxicology</b> Center, a Rutgers - Princeton - USFDA Research Consortium.|$|E
50|$|Sean Ekins is a British {{pharmacologist}} {{and expert}} {{in the fields of}} ADME/Tox, <b>computational</b> <b>toxicology</b> and cheminformatics at Collaborations in Chemistry, a division of corporate communications firm Collaborations in Communications. He is also the editor of four books and a book series for John Wiley & Sons.|$|E
50|$|It is the {{official}} journal of Center for Alternatives to Animal Testing, the American Society for Cellular and <b>Computational</b> <b>Toxicology,</b> the European consensus platform for alternatives, the European Society for Alternatives to Animal Testing, and the transatlantic think tank for toxicology. It has been an open access journal since 2011 under a Creative Commons Attribution 4.0 license.|$|E
5000|$|In 2009, EPA {{reported}} that it developed a system called ACToR (Aggregated <b>Computational</b> <b>Toxicology</b> Resource) to expose living cells or isolated proteins to chemicals. It pooled chemical research, data and screening tools from multiple federal agencies including the National Toxicology Program/ National Institute of Environmental Health Science, National Center for Advancing Translational Sciences and the Food and Drug Administration.|$|E
50|$|<b>Computational</b> <b>toxicology</b> is a {{discipline}} that develops mathematical and computer-based models {{to better understand}} and predict adverse health effects caused by chemicals, such as environmental pollutants and pharmaceuticals. Within the Toxicology in the 21st Century project, the best predictive models were identified to be Deep Neural Networks, Random Forest, and Support Vector Machines, which can reach the performance of in vitro experiments.|$|E
5000|$|Dr. Faqi holds a doctor’s {{degree in}} Veterinary Medicine from the Somali National University, Diploma of Specialization in Experimental Pharmacology from the University of Milan and a PhD in Toxicology from the University of Leipzig. He is a Diplomate of American Board of Toxicology (DABT) and a Fellow Academy of Toxicological Sciences (FATS). [...] Furthermore, he served as:• Board of Scientific Counselors (BOSC) <b>Computational</b> <b>Toxicology</b> at the United States Environmental Protection Agency (US EPA) from September, 2009-September, 2010.|$|E
5000|$|Toxicology in Vitro is a {{bimonthly}} peer-reviewed scientific journal covering {{in vitro}} toxicology. It {{is published by}} Elsevier and an official journal of the European Society of Toxicology in Vitro and affiliated with the American Association for Cellular and <b>Computational</b> <b>Toxicology.</b> The editors-in-chief are Daniel Acosta (University of Cincinnati), Frank A. Barile (St. John's University), and Bas J. Blaauboer (Utrecht University). According to the Journal Citation Reports, the journal has a 2014 impact factor of 2.903, ranking it 29th out of 87 journals in the category [...] "Toxicology".|$|E
50|$|ATSDR has a <b>computational</b> <b>toxicology</b> {{laboratory}} that conducts {{research and}} modeling {{on the effects}} of toxic substances on human health. The agency's toxicology work involves pharmacokinetic/pharmacodynamic modeling, quantitative structure-activity relationship methods, and benchmark dose modeling, as well as establishing minimal risk levels for human exposure to hazardous substances. One model developed by the toxicology laboratory showed that children were much more susceptible than adults to chemical exposure from inhalation and oral exposure. In the aftermath of chemical spills and emergencies, the laboratory also conducts research for state and local health departments on the health effects of the chemicals involved.|$|E
5000|$|In 2007 the National Academy of Sciences {{published}} a report called [...] "Toxicity Testing in the 21st Century: A Vision and a Strategy" [...] which {{opened with a}} statement: [...] "Change often involves a pivotal event that builds on previous history {{and opens the door}} to a new era. Pivotal events in science include the discovery of penicillin, the elucidation of the DNA double helix, and the development of computers. ...Toxicity testing is approaching such a scientific pivot point. It is poised {{to take advantage of the}} revolutions in biology and biotechnology. Advances in toxicogenomics, bioinformatics, systems biology, epigenetics, and <b>computational</b> <b>toxicology</b> could transform toxicity testing from a system based on whole-animal testing to one founded primarily on in vitro methods that evaluate changes in biologic processes using cells, cell lines, or cellular components, preferably of human origin." [...] As of 2010 that vision was still unrealized. [...] As of 2014 that vision was still unrealized.|$|E
40|$|Importance of the field: The {{methods and}} tools of <b>computational</b> <b>toxicology</b> an {{essential}} and integrating pillar {{in the new}} paradigm of predictive toxicology which seeks to develop more efficient and effective means of assessing chemical toxicity, while also reducing animal testing. Areas covered in this review: The increasingly prominent role of <b>computational</b> <b>toxicology</b> {{in the implementation of}} European chemicals legislation is described, along with initiatives by the European Commission¿s Joint Research Centre to promote the acceptance and use of computational methods. Outstanding needs and scientific challenges are also outlined. What the reader will gain: an awareness of the current situation regarding the application of computational methods in regulatory toxicology. Take home message: In recent years, there has been impressive scientific and technological advances in <b>computational</b> <b>toxicology.</b> However, considerable progress is still needed to increase the acceptance of computational methods, and in particular to develop a deeper and common understanding of how to apply <b>computational</b> <b>toxicology</b> in regulatory decision making. JRC. I. 5 -Systems Toxicolog...|$|E
30|$|Thomas Hartung is a {{consultant}} with Underwriters Laboratories (UL) on <b>computational</b> <b>toxicology</b> and has a share in their respective sales; he also is part of Apple’s Green Chemistry Advisory Board. The other authors declare {{that they have no}} competing interests.|$|E
40|$|Assessing {{the risks}} of {{engineered}} nanomaterials (ENMs) {{solely on the basis}} of experimental assays is time-consuming, resource intensive, and constrained by ethical considerations (such as the principles of the 3 Rs of animal testing). The adoption of <b>computational</b> <b>toxicology</b> in this field is a high priority. <b>Computational</b> <b>toxicology</b> is able to contribute to the prediction of the extent of toxic effects of untested ENMs, to the hazard categorization and labeling of ENMs, and to the establishment of hazard threshold values that are sufficiently protecting the ecosystem with respect to the ENMs of concern. These three steps are listed by the European Chemicals Agency (ECHA) as the three elements in evaluating the hazards of ENMs. This study has expanded the use of <b>computational</b> <b>toxicology</b> in the hazard assessment with regard to the safe handling of ENMs. The results obtained contribute to the integration and evaluation of toxicity data, the identification of research gaps on ENM-related modeling, and the development of nano-SARs and SSDs for metallic ENMs. Despite the uncertainties that are associated with our results, as mainly due to limited data quality and availability, we managed to take this field one step forwards and contribute to better-informed regulatory decisions of ENMs...|$|E
40|$|Abstract: <b>Computational</b> <b>toxicology</b> {{combines}} {{data from}} high-throughput test methods, chemical structure analyses and other biological domains (e. g., genes, proteins, cells, tissues) {{with the goals}} of predicting and understanding the underlying mechanistic causes of chemical toxicity and for predicting toxicity of new chemicals and products. A key feature of such approaches is their reliance on knowledge extracted from large collections of data and data sets in computable formats. The U. S. Environmental Protection Agency (EPA) has developed a large data resource called ACToR (Aggregated <b>Computational</b> <b>Toxicology</b> Resource) to support these data-intensive efforts. ACToR comprises four main repositories: core ACToR (chemical identifiers and structures, and summary data on hazard, exposure, use, and other domains), ToxRefDB (Toxicity Reference Database, a compilation of detailed in vivo toxicity data from guideline studies), ExpoCastDBInt. J. Mol. Sci. 2012, 13 180...|$|E
40|$|This mini-review {{is based}} on presentations and discussions at the International Science Forum on <b>Computational</b> <b>Toxicology</b> that was {{sponsored}} by the Office of Research and Development of the US Environmental Protection Agency and held in Research Triangle Park, NC on May 21 - 23, 2007. The complete agenda and copies of the individual presentations from the Forum are available on the Interne...|$|E
40|$|<b>Computational</b> <b>toxicology</b> is the {{development}} of quantitative structure activity relationship (QSAR) models that relate a quantitative measure of chemical structure to a biological effect. In silico QSAR tools are widely accepted as a faster alternative to time-consuming clinical and animal testing methods for regulatory risk assessment of xenobiotics used in consumer products. However, different QSAR tools often make contrasting predictions for a new xenobiotic and may also vary in their predictive ability for different class of xenobiotics. This makes their use challenging, especially in regulatory applications, where transparency and interpretation of predictions {{play a crucial role in}} {{the development}} of safety assessment decisions. Recent efforts in <b>computational</b> <b>toxicology</b> involve the use of in vitro data, which enables better insight into the mode of action of xenobiotics and identification of potential mechanism(s) of toxicity. To ensure that in silico models are robust and reliable before they can be used for regulatory applications, the registration, evaluation, authorization and restriction of chemicals (REACH) initiative and the organization for economic co-operation and development (OECD) have established legislative guidelines for their validation. This dissertation addresses the limitations in the use of current QSAR tools for regulatory risk assessment within REACH/OECD guidelines. The first contribution is an ensemble model that combines the predictions from four QSAR tools for improving the quality of predictions. The model presents a novel mechanism to select a desired trade-off between false positive and false negative predictions. The second contribution is the introduction of quantitative biological activity relationship (QBAR) models that use mechanistically relevant in vitro data as biological descriptors for development of <b>computational</b> <b>toxicology</b> models. Two novel applications are presented that demonstrate that QBAR models can sufficiently predict carcinogenicity when QSAR model predictions may fail. The third contribution {{is the development}} of two novel methods which explore the synergistic use of structural and biological similarity data for carcinogenicity prediction. Two applications are presented that demonstrate the feasibility of proposed methods within REACH/OECD guidelines. These contributions lay the foundation for development of novel mechanism based in silico tools for mechanistically complex toxic endpoints to successfully advance the field of <b>computational</b> <b>toxicology...</b>|$|E
40|$|Information Management {{effort is}} {{designed}} to support several of the strategic projects of the overall <b>Computational</b> <b>Toxicology</b> program. In particular, we are developing databases and tools to support the ToxCast program for screening and prioritization; the Virtual Liver and Developmental Toxicity projects in NCCT; and agency-wide efforts in genomics and chemical data management. Here we outline the most important current projects. ACToR: (Aggregated <b>Computational</b> <b>Toxicology</b> Resource) [1, 2] is a database and set of software applications that bring into one central location many types and sources of data on environmental chemicals. Currently, the ACToR chemical database contains information on chemical structure, in vitro bioassays and in vivo toxicology assays derived from more than 150 sources including the EPA, CDC, FDA, NIH, state agencies, corresponding government agencies in Canada, Europe and Japan, universities, WHO and NGOs. In particular, the ACToR database includes all data from ToxRefDB, DSSTox (both described below) and ToxCast. ACToR uses a MySQL database and has a web-based front end for searching and browsing, and is being used within the Agency...|$|E
40|$|<b>Computational</b> <b>toxicology</b> is the applica-tion of high-powered {{computing}} to man-age and detect {{patterns and}} interactions in large {{biological and chemical}} data sets. <b>Computational</b> <b>toxicology</b> takes advantage of three significant technological breakthroughs: high- information-content data streams (e. g., from micro array or in vitro high-throughput screening experiments), novel biostatistical methods, and the computational power to analyze these data (Judson et al. 2009; Nigsch et al. 2009). Life scientists are {{acutely aware of the}} technologies that produce large data sets, but the steady increase in computational power is of equal importance in supporting discoveries at a systems level in understanding the interaction of environmental agents with biological systems, and how those interactions may produce adverse consequences. Perhaps because computer technology is {{so much a part of}} our daily lives, we have overlooked the fact that it is becoming a crucial element in the next great leap in our understanding of how exogenous agents affect living systems. In this commentary we reflect on the out-comes of the National Academies ’ Standin...|$|E
40|$|This report aims to {{describe}} the main outcomes of an IHCP Exploratory Research Project carried out during 2005 by the European Chemicals Bureau (<b>Computational</b> <b>Toxicology</b> Action). The original aim of this project {{was to develop a}} computational method to facilitate the classification of chemicals into similarity-based chemical categories, which would be both useful for building (Q) SAR models (research application) and for defining chemical category proposals (regulatory application). JRC. I-Institute for Health and Consumer Protection (Ispra...|$|E
40|$|Teaching, research, {{and service}} in the {{following}} fields: <b>computational</b> <b>toxicology</b> (including molecular modeling; classical, sequential, and inverse docking; molecular dynamics; cheminformatics; toxicophore mapping; QSAR; and pathway analysis); ligand-receptor interactions and allostery; drug discovery and development; biomarkers and biosensors; counterterrorism and chemical/biological defense; mechanisms of chemical inactivation of viruses; nanotechnology; exposure and risk assessment of single or multiple agents (mixtures); pathogenesis and mechanisms of neurodegenerative disease; organophosphorus compounds; serine hydrolases; mechanisms of oxidative stress and protein oxidation...|$|E
