7725|10000|Public
5|$|The {{statistics}} for team play, including team skins play, are listed below. The percentages are <b>calculated</b> <b>for</b> <b>each</b> player by rating their shots in each game. Each shot the player attempts is scored {{out of four}} based on how well the shot is made.|$|E
5|$|A {{percentage}} {{of each group}} working in less contaminated areas was badged. Eventually, 18,875 film-badge dosimeters were issued to about 15% of the total work force. On {{the basis of this}} sampling, a theoretical total exposure was <b>calculated</b> <b>for</b> <b>each</b> person who did not have a personal badge. As expected, exposures for target ship crewmen who reboarded their ships after Baker were higher than those for support ship crews. The hulls of support ships that entered the lagoon after Baker became so radioactive that sleeping quarters were moved toward the center of each ship. Of the total mass of radioactive particles scattered by each explosion, 85% was unfissioned plutonium which produces alpha radiation not detected by film badges or Geiger counters. There was no method of detecting plutonium in a timely fashion, and participants were not monitored for ingestion of it.|$|E
5|$|Several {{articles}} published {{near the end}} of 2011 examined the effects of mephedrone, compared to the similar drugs MDMA and amphetamine in the nucleus accumbens of rats, as well as examining the reinforcing potential of mephedrone. Dopamine and serotonin were collected using microdialysis, and increases in dopamine and serotonin were measured using HPLC. Reward and drug seeking are linked to increases in dopamine concentrations in the nucleus accumbens, and drug half-life plays a role in drug seeking, as well. Based on histological examination, most of the author's probes were in the nucleus accumbens shell. Mephedrone administration caused about a 500% increase in dopamine, and about a 950% increase in serotonin. They reached their peak concentrations at 40 minutes and 20 minutes, respectively, and returned to baseline by 120 minutes after injection. In comparison, MDMA caused a roughly 900% increase in serotonin at 40 minutes, with an insignificant increase in dopamine. Amphetamine administration resulted in about a 400% increase in dopamine, peaking at 40 minutes, with an insignificant increase in serotonin. Analysis of the ratio of the AUC for dopamine (DA) and serotonin (5-HT) indicated mephedrone was preferentially a serotonin releaser, with a ratio of 1.22:1 (serotonin vs. dopamine). Additionally, half-lives for the decrease in DA and 5-HT were <b>calculated</b> <b>for</b> <b>each</b> drug. Mephedrone had decay rates of 24.5 minutes and 25.5 minutes, respectively. MDMA had decay values of 302.5 minutes and 47.9 minutes, respectively, while amphetamine values were 51 minutes and 84.1 minutes, respectively. Taken together, these findings show mephedrone induces a massive increase in both DA and 5-HT, combined with rapid clearance. The rapid rise and subsequent fall of DA levels could explain some of the addictive properties mephedrone displays in some users.|$|E
5000|$|Based on the {{reported}} valuations, Mech <b>calculates,</b> <b>for</b> <b>each</b> player, his equilibrium strategy in Mech.|$|R
3000|$|... a–d we {{are able}} to <b>calculate,</b> <b>for</b> <b>each</b> {{measurement}} scenario, the probabilities P that the corresponding Δ [...]...|$|R
3000|$|Calculate {{elemental}} distances: <b>Calculate</b> <b>for</b> <b>each</b> intersected element separately the nodal distances {{following the}} above introduced discontinuous distance calculation.|$|R
25|$|The water {{balances}} are <b>calculated</b> <b>for</b> <b>each</b> reservoir separately {{as shown}} in the article Hydrology (agriculture). The excess water leaving one reservoir is converted into incoming water for the next reservoir.|$|E
25|$|Where a {{party had}} more than one {{candidate}} in one or both of a pair of successive elections change is <b>calculated</b> <b>for</b> <b>each</b> individual candidate, otherwise change is based on the party vote.|$|E
25|$|The salt {{balances}} are <b>calculated</b> <b>for</b> <b>each</b> reservoir separately. They {{are based}} on their water balances, using the salt concentrations of the incoming and outgoing water. Some concentrations must be given as input data, like the initial salt concentrations {{of the water in}} the different soil reservoirs, of the irrigation water and of the incoming ground water in the aquifer.|$|E
50|$|The {{reaching}} definition analysis <b>calculates</b> <b>for</b> <b>each</b> program {{point the}} set of definitions that may potentially reach this program point.|$|R
5000|$|<b>Calculating,</b> <b>for</b> <b>each</b> digit {{position}}, {{whether that}} position {{is going to}} propagate a carry if one comes in from the right.|$|R
3000|$|... ([...] f 2) : <b>calculates</b> <b>for</b> <b>each</b> {{neighbor}} of ‘s’ {{the number of}} tourist services provided which differs with those provided by the agent source‘s’.|$|R
25|$|A {{principal}} {{benefit of}} modeling {{is the ability}} to explicitly compare models: Rather than simply returning a value for each component, the modeler can compute confidence intervals on parameters, but, crucially, can drop and add paths and test the effect via statistics such as the AIC. Thus, for instance to test for predicted effects of family or shared environment on behavior, an AE model can be objectively compared to a full ACE model. For example, we can ask of the figure above for height: Can C (shared environment) be dropped without significant loss of fit? Alternatively, confidence intervals can be <b>calculated</b> <b>for</b> <b>each</b> path.|$|E
25|$|Researchers from OPERA {{measured}} the remaining delays and calibrations {{not included in}} the CERN calculation: those shown in Fig.4. The neutrinos were detected in an underground lab, but the common clock from the GPS satellites was visible only above ground level. The clock value noted above-ground had to be transmitted to the underground detector with an 8km fiber cable. The delays associated with this transfer of time had to be accounted for in the calculation. How much the error could vary (the standard deviation of the errors) mattered to the analysis, and had to be <b>calculated</b> <b>for</b> <b>each</b> part of the timing chain separately. Special techniques were used to measure the length of the fiber and its consequent delay, required as part of the overall calculation.|$|E
25|$|However, {{the biggest}} change to the {{previous}} Finnish ice class rules {{was the way the}} structural requirements were determined. Instead of percentages and experience the minimum requirements were based on plastic deformation theory and pressure loads determined from observations of past ice damages in the Baltic Sea. The ships were divided into three areas (bow, midship, aft) and the pressure loads were <b>calculated</b> <b>for</b> <b>each</b> area {{as a function of the}} ship's displacement and engine output. The rules regarding rudders, engines and the propulsion system were also changed accordingly, and the propulsion system was to be designed so that its strength increased towards the engine. This minimized the repair costs as the parts most likely to break, the propeller blades, were also the easiest and cheapest to replace. Furthermore, a minimum power requirement was given so that the ice-strengthened ships would be powerful enough to follow the icebreakers and not slow down the traffic.|$|E
3000|$|The {{operation}} of D* Lite {{is inspired by}} the A*, <b>calculating,</b> <b>for</b> <b>each</b> node s of the graph (indicating {{a position on the}} environment), the distance to reach the goal (s [...]...|$|R
5000|$|The {{procedure}} <b>calculates</b> <b>for</b> <b>each</b> pair the {{studentized range}} statistic: [...] where [...] is {{the larger of}} the two means being compared, [...] is the smaller, and [...] is the standard error of the data in question.|$|R
5000|$|The {{second step}} of the DUDE scheme is to <b>calculate,</b> <b>for</b> <b>each</b> two-sided context [...] {{observed}} in the previous step along , and <b>for</b> <b>each</b> symbol [...] observed in each context (namely, any [...] such that [...] is a substring of [...] ) the Bayes response to the vector , namely ...|$|R
500|$|Edward Wright (baptised 8 October 1561; died November 1615) was an English {{mathematician}} and cartographer {{noted for}} his book Certaine Errors in Navigation (1599; 2nd ed., 1610), which {{for the first time}} explained the mathematical basis of the Mercator projection, and set out a reference table giving the linear scale multiplication factor as a function of latitude, <b>calculated</b> <b>for</b> <b>each</b> minute of arc up to a latitude of 75°. [...] This was in fact a table of values of the integral of the secant function, and was the essential step needed to make practical both the making and the navigational use of Mercator charts.|$|E
500|$|However, as {{the number}} of {{sequences}} increases and especially in genome-wide studies that involve many MSAs it is impossible to manually curate all alignments. Furthermore, manual curation is subjective. And finally, even the best expert cannot confidently align the more ambiguous cases of highly diverged sequences. In such cases it is common practice to use automatic procedures to exclude unreliably aligned regions from the MSA. For the purpose of phylogeny reconstruction (see below) the Gblocks program is widely used to remove alignment blocks suspect of low quality, according to various cutoffs on the number of gapped sequences in alignment columns. However, these criteria may excessively filter out regions with insertion/deletion events that may still be aligned reliably, and these regions might be desirable for other purposes such as detection of positive selection. A few alignment algorithms output site-specific scores that allow the selection of high-confidence regions. Such a service was first offered by the SOAP program, which tests the robustness of each column to perturbation in the parameters of the popular alignment program CLUSTALW. The T-Coffee program uses a library of alignments {{in the construction of the}} final MSA, and its output MSA is colored according to confidence scores that reflect the agreement between different alignments in the library regarding each aligned residue. [...] Its extension, [...] : (Transitive Consistency Score), uses T-Coffee libraries of pairwise alignments to evaluate any third party MSA. Pairwise projections can be produced using fast or slow methods, thus allowing a trade-off between speed and accuracy. Another alignment program that can output an MSA with confidence scores is FSA, which uses a statistical model that allows calculation of the uncertainty in the alignment. The HoT (Heads-Or-Tails) score can be used as a measure of site-specific alignment uncertainty due to the existence of multiple co-optimal solutions. The GUIDANCE program calculates a similar site-specific confidence measure based on the robustness of the alignment to uncertainty in the guide tree that is used in progressive alignment programs. An alternative, more statistically justified approach to assess alignment uncertainty is the use of probabilistic evolutionary models for joint estimation of phylogeny and alignment. A Bayesian approach allows calculation of posterior probabilities of estimated phylogeny and alignment, which is a measure of the confidence in these estimates. In this case, a posterior probability can be <b>calculated</b> <b>for</b> <b>each</b> site in the alignment. Such an approach was implemented in the program BAli-Phy.|$|E
2500|$|The pulse load must be <b>calculated</b> <b>for</b> <b>each</b> application. A {{general rule}} for calculating the power {{handling}} of film capacitors {{is not available}} because of vendor-related internal construction details. To prevent the capacitor from overheating the following operating parameters have to be considered: ...|$|E
50|$|The live {{variable}} analysis <b>calculates</b> <b>for</b> <b>each</b> program {{point the}} variables that may be potentially read afterwards before their next write update. The result is typically used bydead code elimination to remove statements that assign to a variable whose value is not used afterwards.|$|R
40|$|A {{method for}} {{controlling}} four semi-active suspensions {{of a vehicle}} comprising the steps of: determining, <b>for</b> <b>each</b> semi-active suspension, a first and a second signal representative of the acceleration and speed of the sprung mass; determining, {{for a pair of}} semi-active suspensions arranged {{on one side of the}} vehicle a third and a four signal representative of the acceleration and pitch speed; <b>calculating</b> <b>for</b> <b>each</b> semi-active suspension, a first damping coefficient as a function of the difference between the first and second signal squared; <b>calculating</b> <b>for</b> <b>each</b> semi-active suspension, a second damping coefficient as a function of the difference between the third and the four signal squared; <b>for</b> <b>each</b> semi-active suspension, comparing the first and the second damping coefficient for determining the higher coefficient;; applying to each force generator device, an electronic control signal indicative of the respective high damping coefficient...|$|R
50|$|In {{compiler}} theory, live variable analysis (or simply liveness analysis) is {{a classic}} data-flow analysis performed by compilers to <b>calculate</b> <b>for</b> <b>each</b> program point the variables that may be potentially read before their next write, that is, the variables that are live {{at the exit from}} each program point.|$|R
2500|$|A player's plusminus {{statistic}} is <b>calculated</b> <b>for</b> <b>each</b> game played, {{to provide}} a more meaningful measure over a full season. [...] The statistic is directly affected by overall team performance, influenced by both the offensive and defensive performance of the team as a whole.|$|E
2500|$|Where a {{party had}} more than one {{candidate}} in one or both of a pair of successive elections change is <b>calculated</b> <b>for</b> <b>each</b> individual candidate, otherwise change is based on the party vote. Change figures at by-elections are from the preceding general election or the last intervening by-election. [...] Change figures at general elections are from the last general election.|$|E
2500|$|The UUA {{requests}} annual {{contributions from}} its member congregations. [...] The requested contribution, known as Fair Share, is <b>calculated</b> <b>for</b> <b>each</b> congregation by multiplying an annually determined membership fee {{times the number}} of registered members of that congregation. [...] The UUA also has alternative modes of raising funds. [...] In order for congregations to participate in certain programming, they will pay a nominal fee. [...] Some funds are earned through charitable gifts or estate planning. [...] Additionally, the UUA pools together investment funds from congregations or other constituents and manages them for a small percentage.|$|E
5000|$|... {{so that it}} {{is enough}} to <b>calculate</b> <b>for</b> <b>each</b> {{polynomial}} only about half of the coefficients. The recursion begins with two initial polynomials driven from the sum and difference of the tested polynomial and its reciprocal, then each subsequent polynomial of reduced degree is produced from the last two known polynomials.|$|R
30|$|Occupational skill {{requirements}} {{are based on}} the activities that employees have to perform at the workplace. We pool these activities into five task categories, and each occupation has a value <b>for</b> <b>each</b> task category. The task categories are: non-routine analytical tasks, non-routine interactive tasks, routine cognitive/analytical tasks, routine manual tasks, and non-routine manual tasks (for detailed information, see Table  3 in the Appendix). We <b>calculate</b> <b>for</b> <b>each</b> occupation (2 -digits) the working time spent within a certain task category and use this as an approximation for all workers in this occupation. Particularly, we use data information to <b>calculate</b> <b>for</b> <b>each</b> occupation (2 -digits) a vector that describes how important each of the 5 task categories is for the job. We rate this procedure adequate to use full information about task allocation compared to a simple one to one classification of each occupation to one dominant task (see Table  4).|$|R
5000|$|The tax was <b>calculated</b> {{separately}} <b>for</b> <b>each</b> mining project interest, {{according to}} the formula ...|$|R
2500|$|The Floyd–Warshall {{algorithm}} typically only {{provides the}} lengths of the paths between all pairs of vertices. With simple modifications, {{it is possible}} to create a method to reconstruct the actual path between any two endpoint vertices. While one may be inclined to store the actual path from each vertex to each other vertex, this is not necessary, and in fact, is very costly in terms of memory. Instead, the shortest-path tree can be <b>calculated</b> <b>for</b> <b>each</b> node in [...] time using [...] memory to store each tree which allows us to efficiently reconstruct a path from any two connected vertices.|$|E
2500|$|The {{very first}} {{generation}} computational phantoms {{were developed to}} address the need to better assess organ doses from internally deposited radioactive materials in workers and patients. [...] Until the late 1950s, the ICRP still used very simple models. In these calculations, each organ of the body {{was assumed to be}} represented as a sphere with an [...] "effective radius". The radionuclide of interest was assumed to be located {{at the center of the}} sphere and the [...] "effective absorbed energy" [...] was <b>calculated</b> <b>for</b> <b>each</b> organ. Phantoms such as the Shepp-Logan Phantom were used as models of a human head in the development and testing of image reconstruction algorithms. However, scientists attempted to model individual organs of the body and ultimately the entire human body in a realistic manner, the efforts of which led to stylized anthropomorphic phantoms that resemble the human anatomy.|$|E
2500|$|A {{recently}} developed glacier balance model based on Monte Carlo principals is a promising supplement to both manual field measurements and geodetic methods of measuring mass balance using satellite images. [...] The PTAA [...] (precipitation-temperature-area-altitude) model requires only daily observations of precipitation and temperature collected at usually low-altitude weather stations, and the area-altitude {{distribution of the}} glacier. [...] Output are daily snow accumulation (Bc) and ablation (Ba) for each altitude interval, which is converted to mass balance by Bn = Bc – Ba. Snow Accumulation (Bc) is <b>calculated</b> <b>for</b> <b>each</b> area-altitude interval based on observed precipitation at one or more lower altitude weather stations located in the same region as the glacier and three coefficients that convert precipitation to snow accumulation. [...] It is necessary to use established weather stations that have a long unbroken records so that annual means and other statistics can be determined. [...] Ablation (Ba) is determined from temperature observed at weather stations near the glacier. [...] Daily maximum and minimum temperatures are converted to glacier ablation using twelve coefficients.|$|E
30|$|Reputation Response: To {{check the}} sincere {{execution}} of the reputation protocol, <b>each</b> node <b>calculates</b> <b>for</b> <b>each</b> neighbour the number of reputation responses received divided {{by the number of}} times this neighbour was asked for reputation information. This way, nodes that do not cooperate in the {{execution of the}} reputation protocol are assigned lower trust values.|$|R
40|$|Al metal foams {{manufactured}} by the powder-method have been investigated. Compression tests {{were performed on}} the same sample at increasing deformation steps: at each stage the sample was observed by X-ray computerized tomography. A geometric evaluation of porosity on many sections was performed by <b>calculating,</b> <b>for</b> <b>each</b> pore, its area, equivalent diameter, perimeter and circularity...|$|R
3000|$|... s(.) <b>calculates</b> <b>for</b> <b>each</b> p^k_q {{the number}} of links in common with already {{established}} links and stores this in the num_used_link variable. The p^k_q path with {{the maximum number of}} links in common is selected as the candidate LSP. The variable num_used_link indicates path p^k_q contains at least one link that is already established and, therefore, can be reused.|$|R
