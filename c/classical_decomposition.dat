83|230|Public
3000|$|Open {{image in}} new window called {{analytic}} projections. They are intimately related with the <b>classical</b> <b>decomposition</b> of [...]...|$|E
3000|$|One {{simple method}} of {{describing}} a series {{is that of}} <b>classical</b> <b>decomposition.</b> The notion is that the series can be decomposed into four elements: [...]...|$|E
3000|$|... [...]. Finally, {{note that}} a {{user-defined}} step-size is also necessary in dual decomposition and, as we discuss later, {{this is a serious}} drawback of <b>classical</b> <b>decomposition</b> methods in practice.|$|E
5000|$|A {{closely related}} {{variant of the}} <b>classical</b> Cholesky <b>decomposition</b> is the LDL decomposition, ...|$|R
40|$|We {{formulate}} {{and prove}} relative versions of several <b>classical</b> <b>decompositions</b> {{known in the}} theory of Chevalley groups over commutative rings. As an application we obtain upper estimates for the width of principal congruence subgroups in terms of several families of generators. Some of our results are new even in the absolute case and were previously studied only for groups over finite fields...|$|R
5000|$|This {{decomposition}} {{is related}} to the <b>classical</b> Cholesky <b>decomposition,</b> of the form LL*, as follows: ...|$|R
40|$|This work {{presents}} a tutorial {{to teach a}} quantitative methodology for forecasting known as <b>Classical</b> <b>Decomposition,</b> used in business {{to predict the future}} behavior of time series with different explicative components. A brief Literature Review was developed in relation to the already known and widespread theoretical aspects of the methodology. Then, the tutorial itself is presented - which is composed of a text (in itsentirety) and a spreadsheet (with screens and graphics for illustration). The tutorial can be used by students, researchers and managers who need to learn <b>Classical</b> <b>Decomposition</b> and can achieve the midpoint between a purely theoretical approach and a very technical guide for implementation of the method. Through its format, this study aims to encourage students to actively participate in the learning process...|$|E
40|$|With {{the view}} of {{stressing}} the importance of forecasting and discovering underlying patterns in Tourism data series -for the purpose of assisting management and policy makers in the Tourism Industry - a multiple regression model and one <b>classical</b> <b>decomposition</b> model were constructed in order to investigate Outbound Tourism from Australia to Greece...|$|E
3000|$|With this purpose, we {{tackle the}} <b>classical</b> <b>decomposition</b> of the MSE in {{variance}} and bias terms. First, we obtain preliminary {{results that are}} needed {{for the analysis of}} the average measure π _t^M × N. In particular, we prove that the random non-normalised measure ρ _t^N produced by the bootstrap filter (Algorithm 1) is unbiased and attains L [...]...|$|E
40|$|International audienceIn {{remotely}} sensed Synthetic Aperture Radar (SAR) images, scattering from {{a target}} {{is often the}} result of a mixture of different scattering mechanisms. Fully polarimetric data offers the possibility to separate and to interpret them. To achieve this task, several target decomposition techniques have been proposed in the literature. In particular, the advantage of <b>classical</b> target <b>decomposition</b> techniques applied to fully polarimetric SAR data {{is due to the fact}} that by evaluating the scattering matrix, various scattering mechanisms and target properties can be identified. Aim of this paper is to evaluate a novel approach based on the use of Nonlinear Principal Component Analysis for the target decomposition. In fact, differently from <b>classical</b> target <b>decomposition</b> techniques, the proposed method is based on the decorrelation of the polarimetric SAR data to extract the inherent information content related to the different scattering mechanisms present in the image. An assessment of the effectiveness of the nonlinear principal component analysis method for target decomposition has been carried out by comparing it with the <b>classical</b> <b>decompositions...</b>|$|R
40|$|We {{investigate}} analytically the non-equilibrium {{spatial phase}} segregation process of {{a mixture of}} alkali Bose-Einstein condensates. Two stages (I and II) are found in analogy to the <b>classical</b> spinodal <b>decomposition.</b> The coupled non-linear Schrödinger equations enable us to give a quantitative picture of the present dynamical process in a square well trap. Both time and length scales in the stage I are obtained. We further propose that the stage II {{is dominated by the}} Josephson effect between different domains of same condensate different from scenarios in the <b>classical</b> spinodal <b>decomposition.</b> Its time scale is estimated. Comment: revte...|$|R
40|$|AbstractWe {{study the}} compresion {{properties}} of ENO-type nonlinear multiresolution transformations on digital images. Specific error control algorithms {{are used to}} ensure a prescribed accuracy. The numerical results reveal that these methods strongly outperform the more <b>classical</b> wavelet <b>decompositions</b> {{in the case of}} piecewise smooth geometric images...|$|R
40|$|This paper {{describes}} a Decision Support System to provide indicators to support budget plan decisions, {{in a local}} government organization, the municipality of Lagoa - S. Miguel, Azores. The work includes system modeling, using the UML notation, {{the development of a}} MySQL relational database, algorithms for data collection using PHP, and forecasting models using R functions, such as exponential smoothing, <b>classical</b> <b>decomposition</b> with linear trend, and ARIMA models. Users have access to predictions made by different models for several indicators, being suggested to use the models with closest to zero errors. From the analysis performed considering 12 years data, it is concluded that for most of indicators, the <b>classical</b> <b>decomposition</b> model is the most successful. However, for some indicators, {{it was found that the}} two error measures used are not consistent. In these cases, the final decision is left to the decision-maker, taking advantage of his domain knowledge. info:eu-repo/semantics/publishedVersio...|$|E
40|$|Reinertsen, a Swedish {{consulting}} engineering firm, is {{dissatisfied with the}} accuracy of its qualitative forecast of chargeable hours. This thesis investigates whether <b>classical</b> <b>decomposition,</b> Holt-Winters, or ARIMA can perform more accurate forecasts of chargeable hours than the qualitative method currently used at Reinertsen. This thesis also attempts to explain {{why or why not}} these forecasting methods improve the forecasting accuracy at Reinertsen. The purpose of this thesis is twofold: (1) to identify a suitable manpower forecasting method for Reinertsen; and (2) to contribute to previous literature on forecasting by further assessing the performance and the applicability of the chosen forecasting methods. The data applied was monthly numbers of chargeable hours which covered the period between 2007 and 2011. The first 48 monthly observations were used to generate the forecasts while the remaining 12 monthly observations were used to evaluate the forecasts. The data contains trend and strong monthly fluctuations. The results indicate that ARIMA and <b>classical</b> <b>decomposition</b> are inappropriate forecasting methods to forecast chargeable hours at Reinertsen. The relatively poor performance of <b>classical</b> <b>decomposition</b> and ARIMA is believed to be attributable to these methods inability to forecast varying fluctuations. The results also show that Holt-Winters yield the most accurate forecasts amongst the evaluated forecasting methods. The forecasted time series fluctuates much and the Holt-Winters method, which focuses on recent observations, might be better suited to capture these fluctuations. Consequently, the Holt-Winters method has the potential to improve the forecasting of chargeable hours at Reinertsen...|$|E
40|$|We use {{relative}} zeta functions {{technique of}} W. Muller [19] {{to extend the}} <b>classical</b> <b>decomposition</b> of the zeta regularized partition function of a finite temperature quantum field theory on a ultrastatic space-time with compact spatial section {{to the case of}} non compact spatial section. As an application, we study the case of Schrödinger operators with delta like potential, as described by Albeverio & alt. in [1]...|$|E
5000|$|The {{concept of}} the {{polyphase}} matrix allows matrix decomposition. For instance the decomposition into addition matrices leads to the lifting scheme. [...] However, <b>classical</b> matrix <b>decompositions</b> like LU and QR decomposition cannot be applied immediately, because the filters form a ring with respect to convolution, not a field.|$|R
40|$|In {{this note}} we mainly study the fine Jordan-Chevalley decomposition: a {{refinement}} of the <b>classical</b> Jordan-Chevalley <b>decomposition</b> of a matrix and we pay a {{particular attention to}} the field of the coefficients of the matrix. Moreover we obtain some further additive and multiplicative decompositions of a matrix under suitable conditions...|$|R
5000|$|Or {{given the}} <b>classical</b> Cholesky <b>decomposition</b> [...] the [...] form {{can be found}} by using the {{property}} that the diagonal of L must be 1 and that both the Cholesky and the [...] form are lower triangles,if S is a diagonal matrix that contains the main diagonal of [...] then ...|$|R
40|$|The paper {{deals with}} the inverse {{gravimetric}} problem generalizing a <b>classical</b> <b>decomposition</b> of mass distributions into a harmonic component and another component that produces a zero external field. After a review and {{an extension of the}} well-known L 2 theory, mass distributions in Lp and in H(−s, 2) are considered and proved to undergo an analogous decomposition. Examples will make the theory easier to grasp. Conclusions follow...|$|E
40|$|Over {{a perfect}} field k, let G be an {{extension}} of an abelian variety by the multiplicative group _m. We compute the motive of G in Voevodsky's category of etale motivic complexes with rational coefficients. The result is a decomposition similar to the <b>classical</b> <b>decomposition</b> of the Chow motive of an abelian variety obtained by Deninger/Murre, Kuennemann, Shermenev and Scholl, and other authors. Comment: Ph. D. Thesis, 176 page...|$|E
40|$|In {{this note}} we prove the <b>classical</b> <b>decomposition</b> theorems for certain subgroups of the {{automorphism}} {{group of the}} Bruhat-Tits tree associated to PGL_ 2 (F), where F is a local field. The results {{can be used to}} understand the representation theory of these subgroups. Moreover, we can also use the results to obtain similar decompositions for any closed transitive subgroup of the automorphism group of any Bruhat-Tits tree. Comment: 14 page...|$|E
40|$|We {{study the}} Grassmann {{manifold}} G_k of all k-dimensional subspaces of R^n. The Cartan embedding G_k⊂ O(n) realizes G_k as a subspace of Sl_n(R) and we study the decomposition G_k=∐_w (BwB∩ G_k) {{inherited from the}} <b>classical</b> Bruhat <b>decomposition.</b> We prove that this defines a CW structure on G_k and determine the incidence numbers between cells...|$|R
40|$|We prove an {{analogue}} of the <b>classical</b> Davis' <b>decomposition</b> for martingales in noncommutative L_p-spaces, {{involving the}} square functions. We also determine the dual {{space of the}} noncommutative conditioned Hardy space _ 1. We further extend this latter result to the case 1 <p< 2. Comment: To be published in Journal of London Math. So...|$|R
40|$|An n-tuple of {{operators}} (V_ 1, [...] .,V_n) {{acting on}} a Hilbert space H {{is said to be}} isometric if the operator [V_ 1. ̇. V_n]:H^n→ H is an isometry. We prove a decomposition for an isometric tuple of operators that generalizes the <b>classical</b> Lebesgue-von Neumann-Wold <b>decomposition</b> of an isometry into the direct sum of a unilateral shift, an absolutely continuous unitary and a singular unitary. We show that, as in the <b>classical</b> case, this <b>decomposition</b> determines the weakly closed algebra and the von Neumann algebra generated by the tuple. Comment: 30 pages; significant change...|$|R
30|$|The {{remainder}} of the article is organized as follows. Section 2 formulates the type of problems that we deal with and it also reviews the <b>classical</b> <b>decomposition</b> techniques. Section 3 describes the proposed CDM and proves its convergence to the optimal solution whereas Section 4 provides further analysis on the proposed method when the problem is particularized. Finally, Section 5 presents numerical examples of the proposed method and Section 6 concludes the article.|$|E
30|$|The {{superposition}} of Arima {{time series}} models {{forms the basis}} of two dominant approaches to the <b>classical</b> <b>decomposition</b> of a univariate time series into trend, cyclical, seasonal and irregular components: the reduced form “model-based” decomposition analysed by Box et al. (1978) and Pierce (1978) and further extended by Agustín Maravall and his co-authors, and the so-called “structural time series” models studied by Nerlove (1967), Engle (1978) and Nerlove et al. (1979) and subsequently developed by Andrew Harvey and his co-authors.|$|E
40|$|Abstract. We {{consider}} an outwardly β-biased random walk Xn on a Galton-Watson tree with {{leaves in the}} sub-ballistic regime. We prove that Xn/n γ convergences in law and we characterize the limit law. The exponent γ ∈ (0, 1) is explicit and is a decreasing function of β. Key tools for the proof are <b>classical</b> <b>decomposition</b> results for Galton-Watson trees, a new variant of regeneration times and the careful analysis {{of the time the}} walker spends in leaves. 1...|$|E
40|$|The <b>classical</b> Nagy-Foiaş-Langer <b>decomposition</b> of an {{ordinary}} contraction is generalized {{in the context of}} the operators T on a complex Hilbert space H which, relative to a positive operator A on H, satisfy the inequality T^*AT < A. As a consequence, a version of the <b>classical</b> von Neumann-Wold <b>decomposition</b> for isometries is derived in this context. Also one shows that, if T^*AT=A and AT=A^ 1 / 2 TA^ 1 / 2, then the decomposition of H in normal part and pure part relative to A^ 1 / 2 T is just a von Neumann-Wold type decomposition for A^ 1 / 2 T, which can be completely described.  As applications, some facts on the quasi-isometries recently studied in [4], [5], are obtained...|$|R
40|$|This {{paper is}} devoted to certain {{applications}} of <b>classical</b> Whitney <b>decomposition</b> of the upper half space R^n+ 1 to various problems in harmonic function spaces in the upper half space. We obtain sharp new assertions on embeddings,distances and traces for various spaces of harmonic functions New sharp theorems on multipliers for harmonic function spaces in the unit ball are also presented. Comment: 17 page...|$|R
40|$|We {{establish}} {{the theory of}} the Dulmage-Mendelsohn decomposition for $b$-matchings. The original Dulmage-Mendelsohn <b>decomposition</b> is a <b>classical</b> canonical <b>decomposition</b> of bipartite graphs, which describes the structures of the maximum $ 1 $-matchings and the dual optimizers, i. e., the minimum vertex covers. In this paper, we develop analogical properties, and thus obtain the structure of the maximum $b$-matchings and characterizes the family of $b$-verifying set...|$|R
40|$|The aim of {{this paper}} is to analyze the {{regional}} disparities of six decentralized countries using LIS microdata. In order to determine the extent of the territorial variable in the explanation of income inequality, we carry out two complementary analyses. On the one hand, we perform the <b>classical</b> <b>decomposition</b> by population subgroups of different inequality measures. On the other hand, we implement a semiparametric decomposition analysis based on the method proposed by DiNardo, Fortin and Lemieux...|$|E
40|$|Regularizing {{algorithms}} {{according to}} Maslov for incorrect problems are investigated {{in the paper}} aiming at the bases development for the theory of M-regularizing algorithms. As a result necessary and sufficient conditions {{for the existence of}} a continuous M-regularizing algorithm for the mapping have been found. Existence conditions of decompositions on a base for the solution of Ax=y equation, decompositions generalizing the <b>classical</b> <b>decomposition</b> of Hilbert-Shmidt, have been obtainedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|AbstractWe {{establish}} decompositions of a uniformly convex and uniformly smooth Banach space B and dual space B∗ in {{the form}} B=M⊎J∗M⊥ and B∗=M⊥⊎JM, where M is an arbitrary subspace in B, M⊥ is its annihilator (subspace) in B∗, J:B→B∗ and J∗:B∗→B are normalized duality mappings. The sign ⊎ denotes the James orthogonal summation (in fact, it is the direct sums of the corresponding subspaces and manifolds). In a Hilbert space H, these representations coincide with the <b>classical</b> <b>decomposition</b> in a shape of direct sum of the subspace M and its orthogonal complement M⊥: H=M⊕M⊥...|$|E
40|$|International audienceWe propose in {{this paper}} a new Dantzig-Wolfe master model based on Lagrangian decomposition. We {{establish}} the relationship with <b>classical</b> Dantzig-Wolfe <b>decomposition</b> master problem and propose an alternative proof of the dominance of Lagrangian decomposition on Lagrangian relaxation dual bound. As illustration, we give the corresponding models and numerical results for two standard mathematical programs: the 0 - 1 bidimensional knapsack problem and the generalized assignment problem...|$|R
40|$|There are {{two major}} {{approaches}} to the synthesis of logic circuits. One {{is based on a}} predominantly algebraic factorization leading to AND/OR logic optimization. The other is based on <b>classical</b> Reed-Muller <b>decomposition</b> method and its related decision diagrams, which {{have been shown to be}} efficient for XOR-intensive arithmetic functions. Both approaches share the same characteristics: while one is strong at one class of functions, it is weak at the other's...|$|R
40|$|During {{the last}} decades, several polynomial-time {{algorithms}} {{have been designed}} that decide if a graph has treewidth (resp., pathwidth, branchwidth, etc.) at most k, where k is a fixed parameter. Amini et al. (to appear in SIAM J. Discrete Maths.) use the notions of partitioning-trees and partition functions as a generalized view of <b>classical</b> <b>decompositions</b> of graphs, namely tree-decomposition, path-decomposition, branch-decomposition, etc. In this paper, we propose a set of simple sufficient conditions on a partition function Φ, that ensures {{the existence of a}} linear-time explicit algorithm deciding if a set A has Φ-width at most k (k fixed). In particular, the algorithm we propose unifies the existing algorithms for treewidth, pathwidth, linearwidth, branchwidth, carvingwidth and cutwidth. It also provides the first Fixed Parameter Tractable linear-time algorithm deciding if the q-branched treewidth, defined by Fomin et al. (Algorithmica 2007), of a graph is at most k (k and q are fixed). Our decision algorithm can be turned into a constructive one by following the ideas of Bodlaender and Kloks (J. of Alg. 1996) ...|$|R
