76|64|Public
50|$|The {{steps and}} {{principles}} involved in originating VoIP telephone calls {{are similar to}} traditional digital telephony and involve signaling, channel setup, digitization of the analog voice signals, and encoding. Instead of being transmitted over a circuit-switched network; however, the digital information is packetized, and transmission occurs as IP packets over a packet-switched network. They transport audio streams using special media delivery protocols that encode audio and video with audio codecs, and video codecs. Various codecs exist that optimize the media stream based on application requirements and network bandwidth; some implementations rely on narrowband and <b>compressed</b> <b>speech,</b> while others support high fidelity stereo codecs. Some popular codecs include μ-law and a-law versions of G.711, G.722, a popular open source voice codec known as iLBC, a codec that only uses 8 kbit/s each way called G.729, and many others.|$|E
40|$|This {{paper is}} {{concerned}} with the role of <b>compressed</b> <b>speech</b> in research related to auditory attention within the human information processing system. This brief review of some of the research using <b>compressed</b> <b>speech</b> shows that the primary research emphasis has been in the applied areas. It also suggests some areas where <b>compressed</b> <b>speech</b> research might provide valuable insight into the nature of attention and listening comprehension. An 89 -item bibliography is appended. (RB) U S DEPARTMENT OF HEALTH...|$|E
40|$|This {{review of}} the {{literature}} on rate-controlled or <b>compressed</b> <b>speech</b> begins by tracing the historical development of <b>compressed</b> <b>speech</b> with emphasis on earlier approaches to the problem, e. g., changing speaking rate, simplifying text, changing tape speed, or using a sampling technique. An overview of research from 1917 to 1974 highlights the major contributors and important issues and findings. Four main categories of research are discussed: (1) comprehension and intelligibility; (2) trainability; (3) retention; and (4) applications of <b>compressed</b> <b>speech</b> to various instructional situations. Current basic and applied research is then reviewed, including research related to the use of <b>compressed</b> <b>speech</b> with the blind, visually impaired, and handicapped; its use in reading and language instruction; and its use in other instructional applications. Contemporary uses of rate-modified materials are summarized, and new research directions are suggested. An extensive 105 -item bibliography is included. (LRM) Reproductions supplied by EDRS are. the best that can be made from the original documeht. u. i. OEPARTMVIT OF FoucAifot...|$|E
50|$|Codec 2 is a low-bitrate speech {{audio codec}} (speech coding) that is patent {{free and open}} source. Codec 2 <b>compresses</b> <b>speech</b> using {{sinusoidal}} coding, a method specialized for human speech. Bit rates of 3200 to 700 bit/s have been successfully created. Codec 2 {{was designed to be}} used for amateur radio and other high compression voice applications.|$|R
5000|$|The van {{was also}} {{the site of the}} first {{experiments}} (primarily by Earl Craighill and Tom Magill) with Voice over IP through ARPA's Network Speech Compression Program around 1977 to 1978. They focused on how to <b>compress</b> <b>speech</b> such that it still sounded [...] "natural" [...] and arrived in a timely manner. The phone in the van was a Mickey Mouse phone.|$|R
40|$|Abstract- This paper {{proposes a}} new perceptual hashing {{algorithm}} for speech content identification with compressed domain based on MDCT (Modified Discrete Cosine Transform) Spectrum Entropy. It aims primarily {{to solve problems}} of large computational complexity and poor real-time performance that appear when applying traditional identification methods to the <b>compressed</b> <b>speeches.</b> The process begins by extracting the MDCT coefficients, which are the intermediately decoded results o...|$|R
40|$|Abstract: — In this paper, we have {{proposed}} a cryptosystem using {{state of the art}} technology. We used the minimum possible <b>compressed</b> <b>speech</b> file for encryption. Additionally we applied data compression to speech files and added further compression. Then we applied data encryption techniques to minimize security threats. We applied the symmetric and asymmetric encryption algorithms to the <b>compressed</b> <b>speech</b> files. This was followed by hiding these files into cover images using steganography. This camouflages the secret data and reduces chances of eavesdropping. By using compression, the capacity has been increased. However, after applying encryption, the size of the encrypted data becomes much greater than the input file size, therefore hiding it in a cover demands more capacity. We have analyzed the size of the cipher texts of the <b>compressed</b> <b>speech</b> data for further enhancement of the capacity...|$|E
40|$|Background and Aim: Time <b>compressed</b> <b>speech</b> test {{is one of}} {{the most}} common types of {{monaural}} central auditory processing tests assesses the temporal resolution. The aim of the present study was to evaluate the impact of compression rate and sex on results of this test among the young individuals with normal hearing. Methods: The Persian version of time <b>compressed</b> <b>speech</b> test with six lists of 25 monosyllabic words was prepared in three compression rates of 0, 40 and 60 percent. 36 young people with normal hearing and age range of 18 to 30 years were assessed with time <b>compressed</b> <b>speech</b> test in the most comfortable level in each ear separately. Then scores were compared between two ears, genders and three compression rates. Results: There was a significant difference between word recognition scores of three compression rates (p< 0. 0001). Significant difference was revealed between word recognition scores of both ears, just in the 60 % compression rates. No significant difference was found between the word recognition scores of the two genders. Conclusion: Word recognition scores decreases with increase of the presentation rate of speech stimuli. Time <b>compressed</b> <b>speech</b> test accompany with other audiologic tests can be used to examin the auditory temporal processing and speed of speech processing as a test battery...|$|E
40|$|When {{listeners}} {{first experience}} speech that is either spoken very rapidly or has been artificially compressed, {{there is usually}} {{a short period of}} time during which they find it difficult to understand the speaker. However, with experience, they improve. This improvement reflects adjustments that may be occurring at a number of different levels of speech processing. The purpose of the current study was to further investigate the nature of the adjustments that occur when listeners are asked to recognize highly <b>compressed</b> <b>speech.</b> The first experiment examined adjustment to <b>compressed</b> <b>speech</b> as a function of the amount of exposure, using two different speakers and two different compression rates. The results of this experiment confirm that adjustment to highly <b>compressed</b> <b>speech</b> occurs over a number of sentences. Moreover, the gradient of adjustment depends on the compression rate. Lower compression rates required less experience before full adjustment was obtained. A second and third exper [...] ...|$|E
40|$|Conjugate {{structure}} algebraic CELP (G. 729) is a voice codec that <b>compresses</b> <b>speech</b> signal {{based on}} model parameter of human voice. G. 729 is a 8 kbits/s speech coder. This paper {{deals with the}} reduction ofthe bit rate in G. 729 coder while maintaining the same quality of speech. It proposes two methods; one is to send less number bits {{if there is no}} voice detection in the signal. Another is conditional search in fixed codebook which improves the speech quality...|$|R
40|$|Conjugate {{structure}} algebraic CELP (G. 729) is avoice codec that <b>compresses</b> <b>speech</b> signal {{based on}} modelparameter of human voice. This paper deals withimplementation of a speech-coding algorithm CS-ACELP usingITU-T’s G. 729 recommendation and optimize it for real-timeimplementation on a Very Long Instruction Word (VLIW) Digital Signal Processor (DSP) Central Processing Unit (CPU). Very long instruction word or VLIW {{refers to a}} CPUarchitecture designed {{to take advantage of}} instruction levelparallelism (ILP). A processor that executes every instructionone after the other (i. e. a non-pipelined scalar architecture) mayuse processor resources inefficiently, potentially leading to poorperformanc...|$|R
40|$|Low bit rate, {{high quality}} speech coding {{is a vital}} part in voice {{telecommunication}} systems. The introduction of CELP (1982) (Codebook Excited Linear Prediction) speech coding provides a feasible way to <b>compress</b> <b>speech</b> data to 4. 8 kbps with high quality, but the formidable computational complexity required for real-time processing has prevented its wide application. In this thesis, we reduce the computational complexity to 5 MIPS (million instructions per second), which can be handled by even inexpensive DSP chips, while maintaining the same high quality. We hope our contribution can finally make CELP coding a widely applicable technology...|$|R
40|$|Graduation date: 1980 The {{primary purpose}} of this {{research}} {{study was to determine}} whether significant differences occurred in the amount of knowledge achieved as a result of using Braille or <b>compressed</b> <b>speech</b> as learning modes. Biographic/demographic characteristics such as present age, highest educational level achieved, age at loss of vision, age at initial use of Braille and of talking books were analyzed to investigate which characteristics were able to predict success for Braille or <b>compressed</b> <b>speech.</b> Procedure Thirty legally blind adults residing in the Willamette Valley, Oregon, comprised the study population. All were Braille, grade two, readers. The adults in this group were randomly assigned either Brailled material or <b>compressed</b> <b>speech</b> material on diabetes as the experimental study treatment. Instruments chosen to assess knowledge achievement of the health education material on diabetes were developed and field tested, during the research pilot study. Both the group using Braille, grade two, and the group using <b>compressed</b> <b>speech</b> material were pre- and post-tested using the field tested multiple-choice instrument. A personal data questionnaire was given to collect biographic/demographic data, which were then statistically analyzed for a predictive model of successful correlates when using Braille or <b>compressed</b> <b>speech.</b> The one way analysis of covariance test was employed for the statistical inquiry of the data for hypothesis I, using the F statistic. A t-test was utilized to investigate the means of each group; pre-test scores were used as the covariate. Statistical inquiry into the second hypothesis used a multiple discriminant analysis procedure which determined whether the biographic/demographic characteristics could be distinguished from each other in their assessment of each variable category. Variables were analyzed and ranked according to mean scores to illustrate the degree of relative importance placed on each variable or biographic/demographic characteristic. This hypothesis was also resolved {{with the use of a}} stepwise multiple-regression analysis, which also yields a correlation coefficient. Scores for each variable were used to measure variable relationships. Findings The findings of this study indicated the following: 1. No significant differences were noted in achievement scores when using Braille or <b>compressed</b> <b>speech</b> as learning modes; and 2. Analysis of the biographic/demographic variables were not predictive of a success characteristic for using Braille or <b>compressed</b> <b>speech.</b> Based on these findings, it was concluded that legally blind adults who read Braille, grade two, may use either Braille or <b>compressed</b> <b>speech</b> with success. The use of either Braille or <b>compressed</b> <b>speech</b> becomes the choice of the user and depends upon the material studied. It is notable that the participants in this study were highly educated. Eighty percent had at least some college background; twentyseven percent hadamaster's or a doctoral degree. The study participants' level of income attainment and job securement did not match their respective educational levels achieved. The majority of the population were between the ages of 20 and 35 years; 40 percent were adventitiously blind and 60 percent were congenitally blind. One implication, based on this study's findings, suggests researching whether Braille should be considered a second written language structure. Braille is a learned tactual skill, but the imagery of Braille contractions and alphabet letter analogs make it suspect as does the fact that only five to ten percent of all visually impaired invididuals read Braille. Further, it should be researched why the other 90 to 95 percent do not read Braille in any form. Statistical significance is different from practical importance and the respondents' open -ended answers to questions asked on the personal data questionnaire revealed concern that: 1. Braille is being phased out of private and public school programs with "mainstreaming situations"; 2. Not all technology is suitable for all visually impaired persons; 3. An individual still needs to know and use Braille for such advanced technology as the "paperless Braille" machines; 4. The need remains for retaining Braille in self-communication; 5. Spelling, punctuation and composition are learned with Braille usage while syntax and grammar may be acquired through aural means (<b>compressed</b> <b>speech,</b> talking book cassettes, and discs); 6. <b>Compressed</b> <b>speech</b> becomes valuable in reducing time spent in listening to taped material, lectures, etc., and is especially helpful to speed up a slow speaker; 7. The speed of reading Braille needs further research; and 8. The use of either Braille or <b>compressed</b> <b>speech</b> is individualized; however, both could be used as complementary learning techniques. Recommendations for further study include using sensory-integration research to measure tactile and aural learning pathways as well as to predict success for the blind learner with either tactile or aural integration preference. This research should also include investigation into such questions as: 1. How does blindness effect motor outputs and perceptual skills, thus how does a visual impairment affect learning? 2. Of the primary sensory inputs, do the congenitally or adventitiously blind have a greater or lesser functional ability with one specific sensory modality? Studies considering personality characteristics of successful Braille or <b>compressed</b> <b>speech</b> users should be investigated. It is also recommended that this research include independence and dependence traits with respect to success with tactual or aural preference. It also appears appropriate to investigate the inconsistency of educational levels and paucity of job equivalency or opportunities. Research regarding the use of Braille and <b>compressed</b> <b>speech</b> in job related tasks might be informative data to gather...|$|E
40|$|Transform (DWT) for {{real-world}} speech compression {{design by}} using the Field Programmable Gate Array (FPGA) device. Compared with many reports which only focused on the DWT architecture design, this paper gives the speech compression system design detail on how to interface DWT block with SDRAM and audio codec chip. Speech compression was achieved by keeping only the approximation part of the DWT result. The <b>compressed</b> <b>speech</b> signal was read back after upsampling was performed. The resulting <b>compressed</b> <b>speech</b> can be heard clearly with some introduced background noise Future work is to reduce noise. Index Terms—Audio codec, discrete wavelet, FPGA, speech compression...|$|E
40|$|This is an Open Access article {{distributed}} {{under the}} Creative Commons Attribution (CC BY) licence. Time-compressed speech is an artificial form of rapidly presented speech. Training with time <b>compressed</b> <b>speech</b> {{in a second}} language leads to adaptation toward time-compressed speech in a second language and toward time <b>compressed</b> <b>speech</b> in different languages. However, the effects of training with time-compressed speech of a second language (TCSSL) on diverse cognitive functions and neural mechanisms beyond time compressed speech-related activation are unknown. We investigated the effects of 4 weeks of training with TCSSL on the fractional amplitude of spontaneous low-frequency fluctuations (fALFF) of 0. 01 – 0. 08 Hz, resting-state functional connectivity (RSFC) with the left superior temporal gyrus (STG), fractional anisotropy (FA), and regional gray matter volume (rGMV) of young adults by magnetic resonance imaging. There {{were no significant differences}} in change of performance of measures of cognitive functions or second language skills after training with TCSSL compared with that of the active control group. However, compared with the active control group, training with TCSSL was associated with increased fALFF, RSFC, and FA and decreased rGMV involving areas in the left STG. These results lacked evidence of a far transfer effect of time <b>compressed</b> <b>speech</b> training on a wide range of cognitive functions and second language skills in young adults. However, these results demonstrated effects of time <b>compressed</b> <b>speech</b> training on gray and white matter structures as well as on resting-state intrinsic activity and connectivity involving the left STG, which plays a key role in listening comprehension...|$|E
40|$|Code excited linear {{prediction}} coding is {{an efficient}} technique for <b>compressing</b> <b>speech</b> sequences. Communications quality of speech {{can be obtained}} at bit rates below 8 Kb/s. However, relatively large coding delays are necessary to buffer the input speech in order to perform the LPC analysis. A low delay 8 Kb/s CELP coder is introduced in which the short term predictor is based on past synthesized speech. A new distortion measure that improves the tracking of the formant filter is discussed. Formal listening tests showed that {{the performance of the}} backward adaptive coder is almost as good as the conventional CELP coder...|$|R
40|$|Abstract Conjugate {{structure}} algebraic CELP (G. 729) is a voice codec that <b>compresses</b> <b>speech</b> signal {{based on}} model parameter of human voice. This paper deals with {{implementation of a}} speech-coding algorithm CS-ACELP using ITU- 7 ¶V * UHFRPPHQGDWLRQ DQG RSWLPL]H LW IRU UHDO-time implementation on a Very Long Instruction Word (VLIW) Digital Signal Processor (DSP) Central Processing Unit (CPU). Very long instruction word or VLIW refers to a CPU architecture designed {{to take advantage of}} instruction level parallelism (ILP). A processor that executes every instruction one after the other (i. e. a non-pipelined scalar architecture) may use processor resources inefficiently, potentially leading to poor performanc...|$|R
50|$|While the {{software}} itself comes at no cost, the developers investigated ways to monetize it, with potential options including paid customization options such as emoji or stickers. In January 2017, the first paid features were released with 'Discord Nitro'. For a monthly subscription fee users {{can get an}} animated avatar, use custom emojis across all servers, an increased maximum file size on file uploads (from 8 MB to 50 MB), and a unique profile badge. The developers have claimed that while they will {{look for ways to}} monetize {{the software}}, it will never lose its core features. Discord uses the Opus audio format, which is low-latency and designed to <b>compress</b> <b>speech.</b>|$|R
40|$|AbstractHumans {{can adapt}} {{to a wide range}} of {{variations}} in the speech signal, maintaining an invariant representation of the linguistic information it contains. Among them, adaptation to rapid or time-compressed speech has been well studied in adults, but the developmental origin of this capacity remains unknown. Does this ability depend on experience with speech (if yes, as heard in utero or as heard postnatally), with sounds in general or is it experience-independent? Using near-infrared spectroscopy, we show that the newborn brain can discriminate between three different compression rates: normal, i. e. 100 % of the original duration, moderately compressed, i. e. 60 % of original duration and highly compressed, i. e. 30 % of original duration. Even more interestingly, responses to normal and moderately <b>compressed</b> <b>speech</b> are similar, showing a canonical hemodynamic response in the left temporoparietal, right frontal and right temporal cortex, while responses to highly <b>compressed</b> <b>speech</b> are inverted, showing a decrease in oxyhemoglobin concentration. These results mirror those found in adults, who readily adapt to moderately compressed, but not to highly <b>compressed</b> <b>speech,</b> showing that adaptation to time-compressed speech requires little or no experience with speech, and happens at an auditory, and not at a more abstract linguistic level...|$|E
30|$|The {{automatic}} {{recognition of}} MP 3 <b>compressed</b> <b>speech</b> presents {{a challenge to}} the current systems due to the lossy nature of compression which causes irreversible degradation of the speech wave. This article evaluates the performance of a recognition system optimized for MP 3 <b>compressed</b> <b>speech</b> with current state-of-the-art acoustic modelling techniques and one specific front-end compensation method. The article concentrates on acoustic model adaptation, discriminative training, and additional dithering as prominent means of compensating for the described distortion in the task of phoneme and large vocabulary continuous speech recognition (LVCSR). The experiments presented on the phoneme task show a dramatic increase of the recognition error for unvoiced speech units {{as a direct result of}} compression. The application of acoustic model adaptation has proved to yield the highest relative contribution while the gain of discriminative training diminished with decreasing bit-rate. The application of additional dithering yielded a consistent improvement only for the MFCC features, but the overall results were still worse than those for the PLP features.|$|E
40|$|Click on the DOI link {{below to}} access the article (may not be free) In this paper, the results of both normal-hearing, and profoundly hearing-impaired adults, tested with {{spectrum}} <b>compressed</b> <b>speech</b> via the modified chirp-z algorithm, with and without visual stimuli, are reported. Ten normal-hearing adult listeners and five profoundly hearing-impaired adult listeners {{were asked to identify}} nonsense syllables presented auditorily and bimodally (audition and vision) via video tape in two conditions: lowpass filtered or unprocessed, and spectrum compressed. The lowpass filtered and spectrum <b>compressed</b> <b>speech</b> occupies the same spectrum width of 840 Hz; at 900 Hz and above, the attenuation is at least 60 dB. The spectrum compression is performed by means of a modified chirp-z algorithm, and is described in this paper. The testing results are significant and are reported in this paper. While the signal processing approach is somewhat intensive, the realtime throughput delay is small. Recent advances in hardware speed suggest that realization in a hearing aid is feasible. Peer reviewe...|$|E
40|$|Abstract- The {{human voice}} {{communication}} through GSM is general part of life. Voice speech requires encoding & decoding of signal, which determine {{the quality of}} the recovered speech & the capacity of the system. To make voice speech at lower bit rate as well as making coder complexity simpler, waveform coding has been used. Waveform coding of signal is able to represent any signal in time and frequency simultaneously. Since voiced speech is low frequency of speech, waveform coding is able to interpret all low frequency content and suppressing higher frequency content which is unvoiced speech of signal and other residual. The Code Excited Linear Predictor (CELP) speech coder and a new Wavelet transform method is used to <b>compress</b> <b>speech...</b>|$|R
40|$|G. 729 or Conjugate {{structure}} algebraic CELP is a audio voice codec that <b>compresses</b> <b>speech</b> signal {{based on}} model characteristics of human voice. This paper {{deals with the}} reduction of the computational complexity for estimating the open loop pitch of the CS-ACELP codec, described in ITU recommendation G. 729. For reduction in computation of open loop pitch analysis using Matlab 7. 4, the weighted delta-LSP function is used. This depth first tree search is also used in G. 729 for reducing the search complexity with minimum effort. In experimental study of our paper we are showing the comparing graphical result of Open Loop Pitch in Matlab 7. 4, we are trying to prove that our proposed method save the computational time for calculation of open loop pitc...|$|R
5000|$|Mount Sims is {{responsible}} for Madonna's remixes of her song [...] "Nobody Knows Me," [...] from her 2003 album American Life, {{in which he was}} noted to time <b>compress</b> a <b>speech</b> by Charles Manson in order to construct different electronic drums, which he then placed in the remix.|$|R
3000|$|... {{cannot be}} {{scrambled}} directly by Φ∈Rm×n without dimensional variation. To solve this problem, one feasible {{way is to}} select a group of random atoms from Φ to form an m×m scrambling matrix. But the selection schedule will bring about additional communication load. As a consequence, based on the <b>compressed</b> <b>speech</b> sensing, we have designed a new paradigm of scrambling matrix that alias and scramble two volumes of compressed data [...]...|$|E
40|$|Abstract — The paper {{gives the}} {{details about the}} speech {{compression}} using discrete wavelet transform in FPGA. In today’s world multimedia files are used, storage space required for these files is more and sound files have no option so ultimate solution for this is compression. Compression is nothing but high input stream of data converted into smaller size. Compression is done for all, such as image, data; signals. Here speech compression technique is used and done using DWT. For this purpose only single level implementation is done to get compressed signal, and this is implemented in FPGA by using VHDL code. In this technique DWT code is written in VHDL that include separation of high level component and low level component from given input wav file and after separating these components down sampling is done and we get the <b>compressed</b> <b>speech</b> signal by keeping only approximation part of the result. The <b>compressed</b> <b>speech</b> signal was read back after up-sampling was performed. The resulting compressed signal is with some noise and future work is to reduce noise...|$|E
40|$|Includes bibliographical {{references}} (pages 23 - 25) The {{purpose of}} this study was to compare Ss??? silent reading comprehension with listening comprehension to see if low-level readers could here speech past the normal speaking rates of between 125 and 175 wpm. A further {{purpose of this}} study was to find out if <b>compressed</b> <b>speech</b> could be used as an educational method with low level readers. Ss consisted of 44 male secondary students reading below grade level; 73 percent read under 125 wpm. Ss??? silent reading rate were tested through a silent reading test which was scored for speed (wpm) and accuracy. The auditory mode was tested through a tape of time <b>compressed</b> <b>speech</b> ranging from 0 to 50 percent compression. Comparisons were made of Ss' M reading rate with Ss' best listening rate; the total percentage accuracy of silent reading with the total percentage accuracy of total listening comprehension; and the total percentage accuracy of silent reading with the best percentage accuracy of listening comprehension. (See more in text...|$|E
40|$|A new {{algorithm}} is proposed to <b>compress</b> <b>speech</b> signals using wavelet transform and linear predictive coding. Signal compression based {{on the concept of}} selecting a small number of approximation coefficients after they are compressed by the wavelet decomposition (Haar and db 4) at a suitable chosen level and ignored details coefficients, and then approximation coefficients are windowed by a rectangular window and fed to the linear predictor. Levinson Durbin {{algorithm is}} used to compute LP coefficients, reflection coefficients and predictor error. The compress files contain LP coefficients and previous sample. These files are very small in size compared {{to the size of the}} original signals. Compression ratio is calculated from the size of the compressed signal relative to the size of the uncompressed signal. The proposed algorithms where fulfilled with the use of Matlab package...|$|R
40|$|Abstract — This {{research}} exploits a {{lossless compression}} method based on Hamming codes to <b>compress</b> <b>speech</b> signals with different bit resolutions and sampling rates. This compression is performed directly to signal in its Pulse Coded shape PCM. For the tested data; the average {{performance for the}} compressor reached up to the ratio of 2. 3. The resulting compression rates were compared to the classic entropy compression methods (Run-Length/Golomb-Rice) which is implemented in the FLAC suite. The performance of the Classic entropy methods were close {{to the performance of}} the Hamming code compressor over 16 bits resolutions, the compression ratio for these cases was around 1. 4 average. The best performance levels for the Hamming code compressor was at 8 bit resolution and achieved the ratio of 2. 3...|$|R
40|$|INTRODUCTION Modern speech coding schemes {{have been}} {{developed}} to address the demand for economical spoken language telecommunication of acceptable quality. A variety of speech coding algorithms have been described, which <b>compress</b> <b>speech</b> to facilitate efficient transmission of spoken language over communication networks [2, 3, 4]. Most such speech coding algorithms are lossy {{in the sense that the}} "processed" speech is not identical to the original speech. As a result, some distortion is invariably introduced with any lossy speech coding strategy. For this reason, candidate coders undergo detailed evaluation to ensure that the associated speech output is of acceptable quality [1]. Potential limitations of previous evaluations of the performance and acceptability of coding techniques are the restriction of the speech dataset used in the evaluation to normal speech and the restriction of the listeners to persons who have normal or superior hearing. Assumptions regardin...|$|R
40|$|This study {{investigated}} the perceptual adjustments that occur when listeners recognize highly <b>compressed</b> <b>speech.</b> In Experiment 1, adjustment was examined {{as a function of}} the amount of exposure to <b>compressed</b> <b>speech</b> by use of 2 different speakers and compression rates. The results demonstrated that adjustment takes place over a number of sentences, depending on the compression rate. Lower compression rates required less experience before furl adjustment occurred. In Experiment 2, the impact of an abrupt change in talker characteristics was investigated; in Experiment 3, the impact of an abrupt change in compression rate was studied. The results of these 2 experiments indicated that sudden changes in talker characteristics or compression rate had tittle impact on the adjustment process. The findings are discussed with respect to the level of speech processing at which such adjustment might occur. It is well known that the relationship between the speech signal and underlying phonological representations is extremely complex. This complexity is the result of several different factors. First, no one-to-one mapping between cue...|$|E
40|$|Twenty {{subjects}} {{listened to}} CNC word lists compressed at a 2 : 1 compression ratio using four different discard intervals, 40, 50, 60, and 70 milliseconds. The stimuli {{were presented to}} the subjects both dichotically and diotically. Results indicated a clear advantage in word intelligibility of time-compressed speech at discard intervals from 40 to 70 milliseconds when the signal was presented dichotically. Intelligibility was found to decline rapidly, however, under diotic presentation when the discard interval exceeded 50 to 60 milliseconds. Intelligibility scores for all discard intervals, dichotic presentation, remained above 90 %. In 1965, Scott proposed that the intelligibility of time-compressed speech might be improved by presenting the compressed signal to only one ear and the intervals discarded by the sampling (i. e., Fairbanks) method abutted {{in time to the}} other ear (cf. Fig. 1). He reasoned that such a dichotic presentation of <b>compressed</b> <b>speech</b> would restore to the listener the information that otherwise would have been discarded. In 1968 Gerber subjected Scott’s technique to quantitative intelligibility measurement. He found that <b>compressed</b> <b>speech</b> at ratios up to 2 : 1 was always more intelligible b...|$|E
30|$|This article {{investigates the}} {{performance}} of current state-of-the-art acoustic modelling (AM) and feature extraction techniques in the task of phoneme and large vocabulary continuous speech recognition of MP 3 <b>compressed</b> <b>speech.</b> It is organized as follows: the next section gives a short overview of related works on this topic, followed by a theoretical analysis of distortion for spectral-based speech features, a description of used techniques, and a section detailing the experimental setup and achieved results. The article concludes with a discussion.|$|E
40|$|This {{final report}} {{presents}} {{the latest research}} activity in voice compression. We have designed a non-real time simulation system that is implemented around the IBM-PC where the IBM-PC {{is used as a}} speech work station for data acquisition and analysis of voice samples. A real-time implementation is also proposed. This real-time Voice Compression Board (VCB) is built around the Texas Instruments TMS- 3220. The voice compression algorithm investigated here was described in an earlier report titled, Low Cost Voice Compression for Mobile Digital Radios, by the author. We will assume the reader is familiar with the voice compression algorithm discussed in this report. The VCB <b>compresses</b> <b>speech</b> waveforms at data rates ranging from 4. 8 K bps to 16 K bps. This board interfaces to the IBM-PC 8 -bit bus, and plugs into a single expansion slot on the mother board...|$|R
40|$|Low bit rate, {{high quality}} speech coding {{is a vital}} part in voicetelecommunication systems. The {{introduction}} of CELP (1984) (Codebook Excited Linear Prediction) speech coding provided a feasible way to <b>compress</b> <b>speech</b> data to 4. 8 kbps with high quality, but the formidable computational complexity required for real-time processing has prevented its wide application. Using the new deterministic codebook, we reduce the computational complexity of codebook search, which originally accounted for 2 / 3 of the computational complexity, to negligible. Based on this reduction, we produce an algorithm with complexity of about 5 MIPS. It can be implemented in even inexpensive DSP chips, while maintaining the same high quality. In addition to extremely simpleencoding and decoding schemes, this codebook also provides optimalerror tolerance and it doesn't require codebook storage. We hope that this contribution can finally make CELP speech coding a widely applicable and practical technology...|$|R
40|$|AbstractSpeech coding {{has been}} {{major issue in}} the area of digital speech processing. Speech coding is the process of {{transforming}} the speech signal in a more compressed form, which can then be transmitted with few numbers of binary digits. It is not possible to access unlimited bandwidth of a channel each time we send a signal across it which leads to code and <b>compress</b> <b>speech</b> signals. Speech compression is applied in long distance communication, high-class speech storage, and message encryption. Speech coding is a lossy type of coding and hence the output signal does not exactly sound like the input. Speech coding techniques discussed here are Linear predictive coding, waveform coding, Code excited linear predictive coding, etc. Linear Predictive Coding and Code Excited Linear Predictive Coding techniques are studied with the help of MATLAB to check their performance measures like compression ratio and speech audible quality...|$|R
