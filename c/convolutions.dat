2652|10000|Public
5|$|Gyromitra esculenta has a {{wrinkled}} surface (similar to brainlike <b>convolutions),</b> not wavy or bumpy like G.infula. Gyromitra ambigua {{is very similar}} in appearance, and although it is usually not possible to discern between the two species without examining microscopic characteristics, G.ambigua {{is said to have}} more pronounced purple tints in the stipe. G.ambigua has larger spores that are about 22–30µm long. The saddle-shaped cap of G.infula might also lead to confusion with some species of Helvella, but these latter fungi typically have grayer colors and thinner, fluted stipes.|$|E
5|$|Anatomically, {{the human}} {{cerebellum}} has {{the appearance of}} a separate structure attached {{to the bottom of the}} brain, tucked underneath the cerebral hemispheres. Its cortical surface is covered with finely spaced parallel grooves, in striking contrast to the broad irregular <b>convolutions</b> of the cerebral cortex. These parallel grooves conceal the fact that the cerebellar cortex is actually a continuous thin layer of tissue tightly folded in the style of an accordion. Within this thin layer are several types of neurons with a highly regular arrangement, the most important being Purkinje cells and granule cells. This complex neural organization gives rise to a massive signal-processing capability, but almost all of the output from the cerebellar cortex passes through a set of small deep nuclei lying in the white matter interior of the cerebellum.|$|E
5|$|Slow lorises have a {{round head}} because their skull is shorter {{than in other}} living strepsirrhine. Like other lorisids, its snout does not taper towards {{the front of the}} face as it does in lemurs, making the face appear less long and pointed. Compared to the slender lorises, the snout of the slow loris is even less pointed. As with other members of Lorisidae, its interorbital {{distance}} is shorter than in lemurs. The skull has prominent crests (ridges of bone). A distinguishing feature of the slow loris skull is that the occipital bone is flattened and faces backward. The foramen magnum (hole through which the spinal cord enters) faces directly backward. The brains of slow lorises have more folds (<b>convolutions)</b> than the brains of galagos.|$|E
30|$|<b>Convolution</b> {{layer is}} the core of the <b>convolution</b> neural network, and its core {{operation}} is <b>convolution</b> operation. The <b>convolution</b> layer contains many <b>convolution</b> kernel functions, which extract the image features from the input original image, and determine the location relationship of the image by these local features. The <b>convolution</b> kernel function corresponds to the image feature one-to-one. Each <b>convolution</b> kernel function generates a feature graph. The <b>convolution</b> kernel function values extracted from different feature graphs are different, but they can be shared in the same feature graph. The result of the <b>convolution</b> is convoluted by a learnable <b>convolution</b> kernel with a bias term, and the final output can be obtained by an activation function. Each output characteristic graph can be combined to <b>convolution</b> multiple feature maps. As shown in Formula 2.|$|R
30|$|<b>Convolution</b> {{layer of}} <b>convolution</b> neural network, {{also known as}} feature {{extraction}} layer, consists of two parts. The first part is the real <b>convolution</b> layer in which the main role is to extract the input data features. The characteristics {{of each of the}} different <b>convolution</b> kernels extracting input data are different. The more the <b>convolution</b> layer’s <b>convolution</b> kernels, the more features of the input data can be extracted. The second part is the pooling layer, also known as the downsampling layer, in which the main purpose is to retain useful information on the basis of reducing the amount of data processing and speed up the training network. Generally, convolutional neural networks contain at least four layers of <b>convolution</b> (in this case, the real <b>convolution</b> layer and the lower sampling layer are collectively called <b>convolution</b> layer), namely <b>convolution</b> layer, pooling layer, <b>convolution</b> layer, and pooling layer. The more the <b>convolution</b> level, the more abstract features can be extracted {{on the basis of the}} previous layer.|$|R
3000|$|... [...]. The <b>convolution,</b> Kronecker <b>convolution</b> and Hadamard <b>convolution</b> {{products}} are matrix functions defined for [...]...|$|R
25|$|Digestive system: The {{formula of}} the radula is: 62-73/ × 138-157. The {{intestine}} has six <b>convolutions</b> and is without a caecum. Of the six <b>convolutions</b> of the intestine, four are imbedded in the liver, and two hang freely {{in the body}} cavity.|$|E
25|$|Reciprocal space {{methods have}} been used {{extensively}} {{for their ability to}} evaluate enormous numbers of configurations. They lose their speed advantage if torsional changes are introduced. Another drawback is {{that it is impossible to}} make efficient use of prior knowledge. The question also remains whether <b>convolutions</b> are too limited a class of scoring function to identify the best complex reliably.|$|E
25|$|Each of the {{proteins}} may be {{represented as a}} simple cubic lattice. Then, for the class of scores which are discrete <b>convolutions,</b> configurations {{related to each other}} by translation of one protein by an exact lattice vector can all be scored almost simultaneously by applying the convolution theorem. It is possible to construct reasonable, if approximate, convolution-like scoring functions representing both stereochemical and electrostatic fitness.|$|E
40|$|Abstract—Novel one-and {{two-dimensional}} systolic {{structures are}} designed for computation of circular <b>convolution</b> using distributed arithmetic (DA). The proposed structures involve sig-nificantly less memory and less area-delay complexity compared with the existing DA-based structures for circular <b>convolution.</b> Besides, it is shown that the proposed systolic designs for circular <b>convolution</b> {{can be used for}} computation of linear <b>convolution</b> as well. Index Terms—Linear <b>convolution,</b> circular <b>convolution,</b> sys-tolic array, VLSI. I...|$|R
40|$|Relatively simple low-pass-filtering {{algorithm}} suppresses high-frequency {{noise in}} digitally sampled signal. Algorithm derived from <b>convolution</b> of signal with causal exponential. Filter implemented by algorithm does not introduce phase shift. Algorithm applies forward <b>convolution</b> in time followed by backward <b>convolution</b> in time, and phase shift in second <b>convolution</b> cancels phase shift in first <b>convolution...</b>|$|R
40|$|Students {{often have}} a {{difficult}} time understanding what <b>convolution</b> is. Students can often evaluate the <b>convolution</b> integral (continuous time case), <b>convolution</b> sum (discrete-time case), or perform graphical <b>convolution</b> but may not have a good grasp of what is happening. In other words, students can solve the formula but often do not understand the result or why they get that result. Most engineering texts explain <b>convolution</b> by giving the <b>convolution</b> integral (and/or <b>convolution</b> sum) and doing some mathematical and graphical examples. They often do not attempt to explain how <b>convolution</b> corresponds with what is happening between the system and the input to give the output response. In this paper, a more intuitive explanation of <b>convolution</b> is given and MATLAB and SIMULINK simulations of physical systems are used to give a more intuitive approach to understanding <b>convolution</b> from a systems perspective...|$|R
25|$|The Fourier {{variants}} {{can also}} be generalized to Fourier transforms on arbitrary locally compact Abelian topological groups, which are studied in harmonic analysis; there, the Fourier transform takes functions on a group to functions on the dual group. This treatment also allows a general formulation of the convolution theorem, which relates Fourier transforms and <b>convolutions.</b> See also the Pontryagin duality for the generalized underpinnings of the Fourier transform.|$|E
25|$|Since Starčevo-Körös pottery was {{earlier than}} the LBK and was located in a {{contiguous}} food-producing region, the early investigators looked for precedents there. Much of the Starčevo-Körös pottery features decorative patterns composed of convolute bands of paint: spirals, converging bands, vertical bands, and so on. The LBK appears to imitate and often improve these <b>convolutions</b> with incised lines; hence the term, linear, to distinguish painted band ware from incised band ware.|$|E
25|$|For general non-abelian locally compact groups, {{harmonic}} analysis {{is closely related}} to the theory of unitary group representations. For compact groups, the Peter–Weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. This choice of harmonics enjoys some of the useful properties of the classical Fourier transform in terms of carrying <b>convolutions</b> to pointwise products, or otherwise showing a certain understanding of the underlying group structure. See also: Non-commutative {{harmonic analysis}}.|$|E
30|$|Equation  26 {{indicates}} that by adding and removing chirp CP, the linear <b>convolution</b> is turned into circular <b>convolution</b> (It is not fractional circular <b>convolution).</b> It is the circular <b>convolution</b> about h(n, l) and one chirp periodic extension sequence x((n))p,N.|$|R
5000|$|A {{probability}} distribution F together with all <b>convolution</b> powers of F, with <b>convolution</b> as the operation. This {{is called a}} <b>convolution</b> semigroup.|$|R
50|$|The {{operation}} of applying such a matrix transformation is called <b>convolution.</b> Thus the matrix is also called <b>convolution</b> matrix or a <b>convolution</b> kernel. In {{the case of}} simple series of data points (rather than a multi-dimensional image), the <b>convolution</b> kernel is a one-dimensional vector.|$|R
25|$|May 1968 in France {{introduced}} a partial return to {{representation in the}} work of Jean Messagier. His landscapes were reduced to a point where only a trace of gesture remained. This spontaneous passage would replace the depth of field, while the dynamic imprint of the brushstrokes, turbulent and interlacing, move across the canvas in vast <b>convolutions.</b> Titles, often inscribed directly into the painted surface, are a fundamental aspect of Messagier's paintings. The title becomes part of the work, opening the viewers imagination to the pleasure the artist wishes to share.|$|E
25|$|Gauss died in Göttingen, (then Kingdom of Hanover and now Lower Saxony) on 23 February 1855 and is interred in the Albani Cemetery there. Two {{individuals}} gave eulogies at his funeral: Gauss's son-in-law Heinrich Ewald and Wolfgang Sartorius von Waltershausen, who was Gauss's {{close friend}} and biographer. His brain was preserved and was studied by Rudolf Wagner who found its mass to be 1,492grams (slightly above average) and the cerebral area equal to 219,588 square millimeters (340.362 square inches). Highly developed <b>convolutions</b> were also found, which in the early 20th century were suggested as the explanation of his genius.|$|E
25|$|Classical lissencephaly, {{also known}} as type I or {{generalized}} agyria-pachygyria, is a severe brain malformation of a smooth cerebral surface, abnormally thick (10-20mm) cortex with four layers, widespread neuronal heterotopia, enlarged ventricles, and agenesis or malformation of the corpus callosum. Classical lissencephaly can range from agyria to regional pachygyria and is usually present along with subcortical band heterotopia (known as ‘double cortex’ to describe the circumferential bands of heterotopic neurons located beneath the cortex). Subcortical band heterotopia is a malformation slightly different from lissencephaly that is now classified under the agyria-pachygyria-band spectrum because it consists of a gyral pattern consistent with broad <b>convolutions</b> and an increased cortical thickness.|$|E
5000|$|... #Subtitle level 3: Multidimensional <b>Convolution</b> (M-D <b>Convolution)</b> ...|$|R
3000|$|We {{note that}} Hadamard <b>convolution</b> product {{differs from the}} <b>convolution</b> product of {{matrices}} in many ways. One important difference is the commutativity of Hadamard <b>convolution</b> multiplication [...]...|$|R
40|$|Abstract. <b>Convolution</b> {{method is}} studied to analyze {{statistical}} tolerance for linear dimension chain and nonlinear dimension chain. Hybrid <b>convolution</b> method is proposed, {{which is the}} integration of analytical <b>convolution</b> and numerical <b>convolution.</b> In {{order to reduce the}} algorithm errors, improved <b>convolution</b> method is proposed. Comparing with other statistical tolerance analysis methods, this method is faster and accurate. At last, an example is used to demonstrate the method proposed in this paper...|$|R
500|$|The Plancherel formula {{extends to}} all [...] By a theorem of Jacques Dixmier and Paul Malliavin, every smooth compactly {{supported}} function on [...] is a finite sum of <b>convolutions</b> of similar functions, the inversion formula holds for such [...] It {{can be extended}} to much wider classes of functions satisfying mild differentiability conditions.|$|E
500|$|Casablanca {{received}} [...] "consistently good reviews". Bosley Crowther of The New York Times wrote, [...] "The Warners... have {{a picture}} which makes the spine tingle and the heart take a leap." [...] He applauded the combination of [...] "sentiment, humor and pathos with taut melodrama and bristling intrigue". Crowther noted its [...] "devious <b>convolutions</b> of the plot", and praised the screenplay quality as [...] "of the best" [...] and the cast's performances as [...] "all of the first order".|$|E
500|$|Gyromitra brunnea {{is similar}} in {{appearance}} to G.caroliana, and has an overlapping geographical range. G.brunnea is distinctly lobed, and lacks ribs and cross-ribs. Consequently, [...] "seams" [...] can usually be found where the undersurface is [...] exposed. In contrast, G.caroliniana is almost never lobed and thus lacks seams. Its tightly wrinkled and attached cap mostly hide the undersurface. G.korfii has a more block-like or square appearance, and its yellowish-brown to reddish-brown cap surface has fewer wrinkles, folds, and <b>convolutions.</b> G.fastigiata is a European species that resembles the North American G.brunnea. The common and widespread G.esculenta has a loosely lobed, irregularly shaped, brainlike cap. It has shorter spores measuring 21–25 by 12–13μm.|$|E
40|$|Abstract. In {{an earlier}} paper [1], the author {{introduced}} {{the notion of}} <b>convolution</b> of Riemannian manifolds. In [1] he also provided some examples and applications of <b>convolution</b> manifolds. In this paper we use tensor product to construct more examples of <b>convolution</b> manifolds and investigate fundamental properties of con-volution manifolds. In particular, we study the relationship between <b>convolution</b> manifolds and the gradient of their scale functions. Moreover, we obtain a neces-sary and sufficient condition for a factor of a <b>convolution</b> Riemannian manifold to be totally geodesic. We also completely classify flat <b>convolution</b> Riemannian surfaces...|$|R
5000|$|... #Subtitle level 3: Multidimensional <b>Convolution</b> with One-Dimensional <b>Convolution</b> Methods ...|$|R
40|$|<b>Convolution</b> is a {{critical}} component in modern deep neural networks, thus several algorithms for <b>convolution</b> have been developed. Direct <b>convolution</b> is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im 2 col-based <b>convolution,</b> FFT-based <b>convolution,</b> or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient <b>convolution</b> or MEC with compact lowering, which reduces memory-overhead substantially and accelerates <b>convolution</b> process. MEC lowers the input matrix in a simple yet efficient/compact way (i. e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get <b>convolution</b> completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect <b>convolution</b> algorithms. Comment: ICML 201...|$|R
500|$|Verpa bohemica is {{a species}} of fungus in the family Morchellaceae. Commonly known as the early morel (or early false morel) or the {{wrinkled}} thimble-cap, {{it is one of}} several species known informally as a [...] "false morel". The mushroom has a pale yellow or brown thimble-shaped cap— in diameter by [...] long—that has a surface wrinkled and ribbed with brain-like <b>convolutions.</b> The cap hangs from the top of a lighter-colored, brittle stem that measures up to [...] long by [...] thick. Microscopically, the mushroom is distinguished by its large spores, typically 60–80 by 15–18µm, and the presence of only two spores per ascus.|$|E
500|$|... 1880: French {{neurologist}} Désiré-Magloire Bourneville had {{a chance}} encounter with the disease that would bear his name. He was working as an unofficial assistant to Jean Martin Charcot at La Salpêtrière. While substituting for his teacher, Louis J.F. Delasiauve, he attended to Marie, a 15-year-old girl with psychomotor retardation, epilepsy and a [...] "confluent vascular-papulous eruption of the nose, the cheeks and forehead". She {{had a history of}} seizures since infancy and was taken to the children's hospital aged three and declared a hopeless case. She had learning difficulties and could neither walk nor talk. While under Bourneville's care, Marie had an ever-increasing number of seizures, which came in clusters. She was treated with quinquina, bromide of camphor, amyl nitrite, and the application of leeches behind the ears. On 7 May 1879 Marie died in her hospital bed. The post-mortem examination disclosed hard, dense tubers in the cerebral <b>convolutions,</b> which Bourneville named Sclérose tubéreuse des circonvolutions cérébrales. He concluded they were the source (focus) of her seizures. In addition, whitish hard masses, one [...] "the size of a walnut", were found in both kidneys.|$|E
2500|$|Lissencephaly (to which pachygyria is {{most closely}} linked) is {{associated}} with severe mental retardation, epilepsy, and motor disability. Two characteristics of lissencephaly include its absence of <b>convolutions</b> (agyria) and decreased presence of <b>convolutions</b> (pachygyria). [...] The types of seizures associated with lissencephaly include: ...|$|E
3000|$|... before <b>convolution</b> {{to avoid}} repeating ‘whitening operation’ on each patch in the <b>convolution</b> process. The {{algorithm}} for image feature activation acquisition with 2 D <b>convolution</b> in convolutional autoencoders is summarized as follows.|$|R
5000|$|For {{one-dimensional}} signals, the <b>Convolution</b> Theorem {{states that}} the Fourier transform of the <b>convolution</b> between two signals {{is equal to the}} product of the Fourier Transforms of those two signals. Thus, <b>convolution</b> in the time domain is equal to multiplication in the frequency domain. Mathematically, this principle is expressed via the following:This principle is directly extendable to dealing with signals of multiple dimensions.This property is readily extended to the usage with the Discrete Fourier transform (DFT) as follows (note that linear <b>convolution</b> is replaced with circular <b>convolution</b> where [...] is used to denote the circular <b>convolution</b> operation of size [...] ): ...|$|R
5000|$|In mathematics, the <b>convolution</b> theorem {{states that}} under suitableconditions the Fourier {{transform}} of a <b>convolution</b> is the pointwise product of Fourier transforms. In other words, <b>convolution</b> in one domain (e.g., time domain) equals point-wise multiplication {{in the other}} domain (e.g., frequency domain). Versions of the <b>convolution</b> theorem are true for various Fourier-related transforms.Let [...] and [...] be two functions with <b>convolution</b> [...] (Note that the asterisk denotes <b>convolution</b> in this context, and not multiplication. The tensor product symbol [...] is sometimes used instead.)Let [...] denote the Fourier transform operator, so [...] and [...] are the Fourier transforms of [...] and , respectively.Then ...|$|R
