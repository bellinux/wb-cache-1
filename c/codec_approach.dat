1|13|Public
40|$|Information {{security}} {{is an issue}} of utmost importance in modern time. In order to protect the safety of communication, data encryption and decryption (also called codec) are in-troduced into the communication system. Traditional cryptography focuses on how to encode the secret message into unrecognizable cipher text during transmission. On the other hand, information hiding, one branch of modern cryptography, mainly studies on how to hide secret message into other open message and use this open message as the in-termediate agent of transmission. Nowadays, the commonly used message codec methods are usually designed based on one of these technologies. This thesis introduces a new <b>codec</b> <b>approach</b> that combines the two technologies. Based on the properties of the human vision and the traditional cryptography, messages are graphically encoded and hidden into the picture displayed on an LCD screen. The mes-sage recovery and decoding are achieved through manipulating the corresponding picture captured by a digital camera, using digital image processing methods based on machine vision. The thesis focuses on the design, implementation and verification of the codec...|$|E
40|$|A new rate-constrained self-organising map (SOM) {{learning}} algorithm, {{incorporating a}} noise-mixing model, {{is presented as}} a vector quantiser for very low bit-rate video <b>codecs.</b> A SOM-based <b>approach</b> will exhibit a higher resilience against local minima under low resolution conditions. Practical implementation details and results are also described...|$|R
40|$|Abstract—A novel {{approach}} to speech coding using the hybrid architecture is presented. Advantages of parametric and perceptual coding methods are utilized together {{in order to}} create a speech coding algorithm assuring better signal quality than in traditional CELP parametric <b>codec.</b> Two <b>approaches</b> are discussed. One is based on selection of voiced signal components that are encoded using parametric algorithm, unvoiced components that are encoded perceptually and transients that remain unencoded. The second approach uses perceptual encoding of the residual signal in CELP codec. The algorithm applied for precise transient selection is described. Signal quality achieved using the proposed hybrid codec is compared to quality of some standard speech codecs. Keywords—CELP residual coding; hybrid codec architecture; perceptual speech coding; speech codecs comparison. I...|$|R
40|$|Distributed Video Coding (DVC), {{has been}} an {{attractive}} alternative to the conventional video coding {{for a number of}} applications because of its flexibility for designing extremely low complex video encoders. DVC based video codecs proposed in the literature generally include a dynamic feedback channel between the encoder and the decoder. However it is observed that this dynamic feedback mechanism is a practical hindrance in a number of practical consumer electronics applications. In this paper, a novel transform domain Unidirectional Distributed Video Codec (UDVC) without a feedback channel is proposed. A simple encoder rate control algorithm is used in order to eliminate the feedback channel of the existing DVC codecs while keeping the encoder complexity as low as possible. Moreover, an improved reconstruction algorithm is utilised in order {{to improve the quality of}} the reconstructed frame. Simulation results show that the performance of the proposed <b>codec</b> can <b>approach</b> to that of DVC with feedback, which is assumed to be known as the upper bound...|$|R
3000|$|Most modern speech codecs {{are based}} on the {{principle}} of CELP coding [13]. They exploit a simple source/filter model of speech production, where the source corresponds to the vibration of the vocal cords or/and to a noise produced at a constriction of the vocal tract, and the filter corresponds to the vocal/nasal tracts. Based on the quasi-stationary property of speech, the filter coefficients are estimated by linear prediction and regularly updated (20 ms corresponds to a typical value). Since the beginning of the seventies and the [...] "LPC- 10 " [...] <b>codec</b> [14], numerous <b>approaches</b> were proposed to effectively represent the source.|$|R
40|$|In {{this paper}} we present an audio {{tampering}} detection method {{based on the}} analysis of discontinuities in the framing grid, caused either by manipulations within the same recording or across recordings even with <b>codec</b> changes. The <b>approach</b> extends {{state of the art}} methods for MP 3 framing grid detection with respect to efficiency and robustness, and multi-codec support, adding mp 3 PRO, AAC, and HE-AAC schemes. An evaluation has been carried out using a publicly available dataset. A high performance is reported on both detecting tampering and codecs showing the usefulness of the approach in audio forensics...|$|R
40|$|The Daala {{project is}} a royalty-free video codec that {{attempts}} {{to compete with the}} best patent-encumbered codecs. Part of our strategy is to replace core tools of traditional video <b>codecs</b> with alternative <b>approaches,</b> many of them designed to take perceptual aspects into account, rather than optimizing for simple metrics like PSNR. This paper documents some of our experiences with these tools, which ones worked and which did not, and what we've learned from them. The result is a codec which compares favorably with HEVC on still images, and is on a path to do so for video as well. Comment: 10 page...|$|R
40|$|The Daala {{project is}} a royalty-free video codec that {{attempts}} {{to compete with the}} best patent-encumbered codecs. Part of our strategy is to replace core tools of traditional video <b>codecs</b> with alternative <b>approaches,</b> many of them designed to take perceptual aspects into account, rather than optimizing for simple metrics like PSNR. This paper documents some of our experiences with these tools, which ones worked and which did not. We evaluate which tools are easy to integrate into a more traditional codec design, and show results {{in the context of the}} codec being developed by the Alliance for Open Media. Comment: 19 pages, Proceedings of SPIE Workshop on Applications of Digital Image Processing (ADIP), 201...|$|R
40|$|Synchronous Data ow (SDF) is a {{powerful}} analysis tool for regular, cyclic, parallel task graphs. The behaviour of SDF graphs however is static and therefore not always able to accurately capture the behaviour of modern, dynamic data ow applications, such as embedded multimedia <b>codecs.</b> An <b>approach</b> to tackle this limitation is by means of scenarios. In this paper we introduce a technique and a tool to automatically analyse a scenario-aware data ow model for its worstcase performance. A system is speci??ed {{as a collection of}} SDF graphs representing individual scenarios of behaviour and a ??nite state machine that speci??es the possible orders of scenario occurrences. This combination accurately captures more dynamic applications and this way provides tighter results than an existing analysis based on a conservative static data ow model, which is too pessimistic, while looking only at the `worst-case' individual scenario, without considering scenario transitions, can be too optimistic. We introduce a formal semantics of the model, in terms of (max; +) linear system-theory and in particular (max; +) automata. Leveraging existing results and algorithms from this domain, we give throughput analysis and state space generation algorithms for worst-case performance analysis. The method is implemented in a tool and the e??ectiveness of the approach is experimentally evaluated...|$|R
40|$|A new {{content-based}} {{approach for}} improved H. 264 /MPEG 4 -AVC video coding is presented. The framework is generic {{because it is}} based on a closed- loop texture analysis by synthesis algorithm that can automatically identify and recover from video quality impairments through artifact detectors and appropriate countermeasures. The algorithm is flexible, for it can in principle be integrated into any standards-compliant video codec. The fundamental assumption of our approach is that many video scenes can be classified into subjectively relevant and irrelevant textures. The texture categorization is thereby done by a texture analyzer (encoder side), while the corresponding texture synthesizer performs the replacement of the subjectively irrelevant textures (decoder side), given the side information generated by the texture analyzer. When implementing the proposed approach into an H. 264 /MPEG 4 - AVC codec, bit rate savings of up to 33. 3 % compared to an H. 264 /MPEG 4 - AVC video <b>codec</b> without our <b>approach</b> are reported...|$|R
40|$|Abstract—In this paper, a {{low power}} joint bus and error {{correction}} coding is proposed to provide reliable and energy-efficient interconnection for network-on-chip (NoC) in nano-scale technology. The proposed self-corrected “green ” (low power) coding scheme is constructed by two stages, which are triplication error correction coding (ECC) stage and green bus coding stage. Triplication ECC {{provides a more}} reliable mechanism to advanced technologies. Moreover, in view of lower latency of decoder, it has rapid correction ability to reduce the physical transfer unit size of switch fabrics by self-corrected technique in bit level. The green bus coding employs more energy reduction by a joint triplication bus power model for crosstalk avoidance. In addition, the circuitry of green bus coding is more simple and effective. Based on UMC 90 nm CMOS technology, the simulation results show self-corrected green coding can achieve 34. 4 % energy reduction with small <b>codec</b> overhead. This <b>approach</b> not only makes the NoC applications tolerant against transient malfunctions, but also realizes energy efficiency. I...|$|R
40|$|Content-based {{approaches}} to motion compensation offer {{the advantage of}} being able to adapt to the spatial and temporal characteristics of a scene. Three such motion compensation techniques are described in detail, with one of the methods being integrated into a video <b>codec.</b> The first <b>approach</b> operates by performing spatio-temporal segmentation of a frame. A split and merge approach is then used to ensure that motion character-istics are relatively homogeneous within each region. Region shape information is coded (by approximating the boundaries with polygons) and a triangular mesh is generated within each region. Translational and affine motion estima-tion are then performed on each triangle within the mesh. This approach offers an improvement in quality when compared to a regular mesh of the same size. However, it is difficult to control the number of triangles, since this depends on the segmentation and polygon approximation stages. As a result, this approach is difficult to integrate into a rate-distortion framework. The second method involves the use of variable-size blocks, rather than a trian...|$|R
40|$|Content-based {{approaches}} to motion compensation offer {{the advantage of}} being able to adapt to the spatial and temporal characteristics of a scene. Three such motion compensation techniques are described in detail, with one of the methods being integrated into a video <b>codec.</b> The first <b>approach</b> operates by performing spatio-temporal segmentation of a frame. A split and merge approach is then used to ensure that motion characteristics are relatively homogeneous within each region. Region shape information is coded (by approximating the boundaries with polygons) and a triangular mesh is generated within each region. Translational and affine motion estimation are then performed on each triangle within the mesh. This approach offers an improvement in quality when compared to a regular mesh of the same size. However, it is difficult to control the number of triangles, since this depends on the segmentation and polygon approximation stages. As a result, this approach is difficult to integrate into a rate-distortion framework. The second method involves the use of variable-size blocks, rather than a triangular mesh. Once again, a frame is first segmented into regions of homogeneous motion, which are then approximated with polygons. A grid of blocks is created in each region, with the block size inversely proportional to the motion compensation error for that region. This ensures that regions with complex motion are populated by smaller blocks. Following this, bi-directional translational and affine motion parameters are estimated for each block. In contrast to the mesh-based approach, this method allows the number of blocks to be easily controlled. Nevertheless, the number and shape of regions remains very sensitive to the segmentation parameters used. The third technique also uses variable size blocks, but the spatio-temporal segmentation stage is replaced with a simpler and more robust binary block partitioning process. If a particular block does not allow for accurate motion compensation, then it is split into two using the horizontal or vertical line that achieves the maximum reduction in motion compensation error. Starting with the entire frame as one block, the splitting process is repeated until a large enough binary tree of blocks is obtained. This method causes partitioning to occur along motion boundaries, thus substantially reducing blocking artifacts compared to regular block matching. In addition, small blocks are placed in regions of complex motion, while large blocks cover areas of uniform motion. The proposed technique provides significant gains in picture quality when compared to fixed size block matching at the same total rate. The binary partition tree method has been integrated into a hybrid video codec. (The codec also has the option of using fixed-size blocks or H. 264 /AVC variable- size blocks.) Results indicate that the binary partition tree method of motion compensation leads to improved rate-distortion performance over the state-of- the-art H. 264 /AVC variable-size block matching. This advantage is most evident at low bit-rates, and also in the case of bi-directionally predicted frames. Keywords: motion estimation, motion compensation, video coding, video compression, content-based, variable-size block matching, binary partition tree...|$|R
40|$|Aggressive {{technology}} scaling in the nano-scale regime makes chips {{more susceptible}} to failures. This causes multiple reliability challenges {{in the design of}} modern chips, including manufacturing defects, wear-out, and parametric variations. By increasing the number, amount, and hierarchy of on-chip memory blocks in emerging computing systems, the reliability of the memory sub-system becomes an increasingly challenging design issue. The limitations of existing resilient memory design schemes motivate us to think about new approaches considering scalability, interconnect-awareness, and cost-effectiveness as major design factors. In this thesis, we propose different approaches to address resilient on-chip memory design in computing systems ranging from traditional single-core processors to emerging many-core platforms. We classify our proposed approaches in five main categories: 1) Flexible and low-cost approaches to protect cache memories in single-core processors against permanent faults and transient errors, 2) Scalable fault-tolerant approaches to protect last-level caches with non-uniform cache access in chip multiprocessors, 3) Interconnect-aware cache protection schemes in network-on-chip architectures, 4) Relaxing memory resiliency for approximate computing applications, and 5) System-level design space exploration, analysis, and optimization for redundancy-aware on-chip memory resiliency in many-core platforms. We first propose a flexible fault-tolerant cache (FFT-Cache) architecture for SRAM-based on-chip cache memories in single-core processors working at near-threshold voltages. Then, we extend the technique proposed in FFT-Cache, to protect shared last-level cache (LLC) with Non-Uniform Cache Access (NUCA) in chip multiprocessor (CMP) architectures, proposing REMEDIATE that leverages a flexible fault remapping technique while considering the implications of different remapping heuristics in the presence of cache banking, non-uniform latency, and interconnected network. Then, we extend REMEDIATE by introducing RESCUE with the main goal of proposing a design trend (aggressive voltage scaling + cache over-provisioning) that uses different fault remapping heuristics with salable implementation for shared multi-bank LLC in CMPs to reduce power while exploring a large design space with multiple dimensions and performing multiple sensitivity analysis. Considering multibit upsets, we propose a low-cost technique to leverage embedded erasure coding (EEC) to tackle soft errors as well as hard errors in data caches of a high-performance as well as an embedded processor. Considering non-trivial effect of interconnection fabric in memory resiliency of network-on-chip (NoC) platforms, we then propose a novel fault-tolerant scheme that leverages the interconnection network to protect the LLC cache banks against permanent faults. During a LLC access to a faulty area, the network detects and corrects the faults, returning the fault-free data to the requesting core. In another <b>approach,</b> we propose <b>CoDEC,</b> a Co-design <b>approach</b> to error coding of cache and interconnect in many-core architectures to reduce the cost of error protection compared to conventional methods. Proposing a system-wide error coding scheme, CoDEC guarantees end-to-end protection of LLC data blocks throughout the on-chip network against errors. Observing available tradeoffs among reliability, output fidelity, performance, and energy in emerging error-resilient applications in approximate computing era motivates us to consider application-awareness in resilient memory design. The key idea is exploiting the intrinsic tolerance of such applications to some level of errors for relaxing memory guard-banding to reduce design overheads. As an exemplar we propose Relaxed-Cache, in which we relax the definition of faulty block depending on the number and location of faulty bits in a SRAM-based cache to save energy. In this part of thesis, we aim at cross-layer characterization and optimization of on-chip memory resiliency over the system stack. Our first contribution toward this approach is focusing more on scalability of memory resiliency as a system-level design methodology for scalable fault-tolerance of distributed on-chip memories in NoCs. We introduce a novel reliability clustering model for effective shared redundancy management toward cost-efficient fault-tolerance of on-chip memory blocks. Each cluster represents a group of cores that have access to shared redundancy resources for protection of their memory blocks...|$|R

