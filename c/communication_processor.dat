51|664|Public
25|$|Many second-generation CPUs {{delegated}} {{peripheral device}} communications to a secondary processor. For example, while the <b>communication</b> <b>processor</b> controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took {{at least two}} memory cycles; one for the instruction, one for the operand data fetch.|$|E
5000|$|... #Caption: Siemens Simatic S7-400 {{system in}} a rack, left-to-right: power supply unit (PSU), CPU, {{interface}} module (IM) and <b>communication</b> <b>processor</b> (CP).|$|E
50|$|Some {{models have}} a network {{interface}} processor {{in the form}} of a <b>communication</b> <b>processor</b> module (CPM) and serial communications controllers (SCC) which can be interfaced to Ethernet or HDLC busses.|$|E
40|$|Book chapterCommunication {{processors}} are processors {{with specific}} optimizations to support <b>communication</b> sys-tems. <b>Communication</b> <b>processors</b> {{exist in a}} wide variety of forms and can be categorized based on the communication system, such as wired or wireless and based on the layer in the communication system, such as the physical layer, the medium access control layer or the network layer. <b>Communication</b> <b>processors</b> can be further categorized based on the application, such as audio, video or data and the end system requiring the communication system such as a laptop, a cell phone or a personal computer. In this book chapter, we present a brief outline of the different types of <b>communication</b> <b>processors</b> and the need and requirements of each of these processors. However, we will focus on the challenges in the physical layer design in <b>communication</b> <b>processors</b> with the increase in data rates, increase in algorithm complexity, need for flexibility to adapt to different protocols and environments, need to optimize over varying con-straints such as area, power, performance and the need for supporting multiple interfaces, devices and applications...|$|R
40|$|Data rate {{traffic and}} {{communication}} capacity demand have been increased continuously. Therefore, a highly advanced 4 G wireless system {{is required to}} meet a high demand for modern mobile terminals. For getting a further improvement for 4 G communication systems, new paradigms of design, analysis tools and applications for 4 G <b>communication</b> <b>processors</b> are necessary. In this paper, some of these new paradigms are discussed. Furthermore, a single-step discrete cosine transform truncation (DCTT) method is proposed for the modeling-simulation in signal integrity verification for high-speed <b>communication</b> <b>processors.</b> © 2011 IEEE. published_or_final_versio...|$|R
40|$|This work {{describes}} a parallel neural network emulator which uses standard DSPs and application-specific VLSI <b>communication</b> <b>processors</b> with an integrated hardware routing algorithm. The use of DSPs as programmable processing elements enables the emulation {{of different types}} of neurons including biologically inspired models with learnable synaptic weights and delays, variable neuron gain, and static and dynamic thresholding. Locally interconnected <b>communication</b> <b>processors</b> attached to each DSP can span up a 2 D- or 3 D-computing grid and thus form a highly parallel network topology capable of global packet switching routing...|$|R
50|$|The CEVA-XC is a {{low-power}} <b>communication</b> <b>processor</b> family {{designed for}} wireless communications processing in mobile handsets {{as well as}} wireless infrastructure applications. A single CEVA-XC core is capable of handling complete transceiver paths for multiple air interfaces in software.|$|E
50|$|The CPM {{features}} its own RISC microcontroller (<b>Communication</b> <b>Processor),</b> {{separate from}} the actual Central Processing Unit IP core. The RISC microcontroller communicates with the core using dual-ported RAM, special command, configuration and event registers as well as via interrupts.|$|E
50|$|My Cloud uses a Mindspeed Comcerto 2000 (M86261G-12) dual-core ARM Cortex-A9 <b>Communication</b> <b>Processor</b> {{running at}} 650 MHz. The Gigabit Ethernet port is a Broadcom BCM54612E Gigabit Ethernet Transceiver. Other {{components}} include 256 MB of Samsung K4B2G1646E DDR3 RAM and 512 KB of Winbound 25X40CL flash. The drive is a WD Red 2 TB (WD20EFRX).|$|E
50|$|Merit renamed its Communication Computers to be Primary <b>Communication</b> <b>Processors</b> (PCPs) {{and created}} LSI-11 based Secondary <b>Communication</b> <b>Processors</b> (SCPs). PCPs formed {{the core of}} the network and were {{attached}} to each other over Ethernet and dedicated synchronous data circuits. SCPs were attached to PCPs over synchronous data circuits. PCPs and SCPs would eventually include Ethernet interfaces and support local area network (LAN) attachments. PCPs would also serve as gateways to commercial networks such as GTE's Telenet (later SprintNet), Tymnet, and ADP's Autonet, providing national and international network access to MTS. Later still the PCPs provided gateway services to the TCP/IP networks that became today's Internet.|$|R
40|$|This work {{describes}} a parallel neural network emulator which combines use of application-specific VLSI <b>communication</b> <b>processors</b> and standard DSPs as programmable processing elements. Locally interconnected <b>communication</b> <b>processors</b> attached to each DSP can span up 2 D- or 3 D-grids containing {{large number of}} computing nodes and thus form highly parallel multiprocessor networks capable of global pipelined packet switched routing. The use of standard DPSs as processing element enables the emulation {{of different types of}} neurons. These include biologically inspired models with learnable synaptic weights and delays, variable neuron gain, and static and dynamic thresholding. We describe applications of the emulator that include neural robot control as well as temporal signal processing, e. g. beamforming...|$|R
50|$|A second {{hardware}} technology {{initiative in}} 1983 produced the smaller Secondary <b>Communication</b> <b>Processors</b> (SCP) based on DEC LSI-11 processors. The first SCP was {{installed at the}} Michigan Union in Ann Arbor, creating UMnet, which extended Merit's network connectivity deeply into the U-M campus.|$|R
50|$|QsNetII's core {{design is}} based on two ASICs: Elan4 and Elite4. Elan4 is a <b>communication</b> <b>processor</b> that forms the {{interface}} between a high-performance multistage network and a processing node {{with one or more}} CPUs. Elite4 is a switching component that can switch eight bidirectional communications links, each of which carrying data in both directions simultaneously at 1.3 GB/s.|$|E
50|$|The {{network was}} {{initially}} {{based upon a}} dedicated modular packet switch using DCC's TP 4000 <b>communication</b> <b>processor</b> hardware. The operating system and the packet switching software was developed by Telenet (later on GTE Telenet). At the time of PSS's launch this was in advance of both Telenet's own network and most others that used general purpose mini-computers as packet switches.|$|E
50|$|In 2013, My Cloud NAS {{has been}} {{released}} by Western Digital. My Cloud uses a Mindspeed Comcerto 2000 (M86261G-12) dual-core ARM Cortex-A9 <b>Communication</b> <b>Processor</b> running at 650 MHz. The Gigabit Ethernet port is a Broadcom BCM54612E Gigabit Ethernet Transceiver. Other components include 256 MB of Samsung K4B2G1646E DDR3 RAM and 512 KB of Winbound 25X40CL flash. The drive is a WD Red 2 TB (WD20EFRX).|$|E
40|$|Introduction Communication {{is central}} in any {{computer}} system {{in order to}} enable subsystems to cooperate. The diversication of o-the-shelf components and busses gives designers more freedom in designing specic optimized applications. But designers have to guarantuee that the selected components can {{communicate with each other}} or with other words that they observe specic communication protocols. Additional components called <b>communication</b> <b>processors</b> are needed that convert incoming signals to outgoing ones by doing the job of a protocol converter. Typically a converter has only a small data path and is therefore control dominated. It has to observe timing constraints like timeouts or minimal signal durations. We describe a set of design tools dedicated to the design of such <b>communication</b> <b>processors</b> (Coprodes 1) and being produced by a consortium of partners 2 from dierent European countries. Th...|$|R
40|$|High {{bandwidth}} {{of networks}} demands high performance <b>communication</b> <b>processors</b> that integrate application processing, network processing, and system support functions into a single, low cost System-On-Chip solution. However, conventional processors, {{when used in}} network related applications, are beset by the overhead of save/restore of register context, cache misses due to fetching interrupt handler from memory, {{and the possibility of}} NIC buffer overflow. Therefore, this paper analyzes the effectiveness of multithreading to service interrupts on an embedded processor simulator enhanced with a multithreaded hardware execution model. Our simulation results reveal that multithreading for interrupts from a single NIC brings only a modest improvement to processing performance. However, our analysis also shows that multithreading for interrupts has a lot of potential when applied to <b>communication</b> <b>processors</b> with multiple interrupt sources, such as Ethernet, ATM, USB, and HDLC...|$|R
40|$|The {{number of}} design {{decisions}} for connecting processor nodes within MIMD systems is rather large. This paper systematically introduces {{the most important}} design parameters for <b>communication</b> <b>processors</b> in MIMD systems. Together, these parameters span a multidimensional design space. Points in this space are clarified through classification {{of a number of}} existing <b>communication</b> <b>processors.</b> The design choices made for these processors are reviewed and their performance is evaluated. Suitable choices of the design parameters are highly influenced by application behavior. Ideally one would like to design processors which cover a whole area in this design space. A companion paper describes a scalable and flexible design currently being realized at our laboratory. 1 Introduction Many problems require far more computing power than can be offered by single processor systems; e. g. scientific computations doing many calculations on very large data spaces, and artificial intelligence application [...] ...|$|R
5000|$|The PAR Data Processor—with Central Logic and Control {{including}} redundant Processor, Program Store, and Variable Store units—provided missile/satellite {{track data}} for communications equipment {{to transfer to}} NORAD, etc. and was listed as a separate procurement item from the Perimeter Acquisition Radar by the Congressional Record. [...] For the Advanced Data Communication Control Procedure, the ADCCP <b>communication</b> <b>processor</b> invented in the 1980s by Lynn O Kesler [...] "translates messages between" [...] the PARCS data transmission controller and Cheyenne Mountain.https://www.google.com/patents/US4773043 ...|$|E
5000|$|Powermouse {{was another}} {{scalable}} system {{that consisted of}} modules and individual components. It was a starightforward extension of the x'plorer-system. Each module (dimensions: 9 cm x 21 cm x 45 cm) contained four MPC 604 processors (200/300 MHz) and 64 MB RAM attaining a peak performance of 2.4 Gflop/s.A separate <b>communication</b> <b>processor</b> T425) equipped with 4 MB RAM, controlled the data flow in four directions to other modules in the system. The bandwidth of a single node was 9 MB/s ...|$|E
50|$|Another {{solution}} for Arduino {{has been presented}} in an Application Note published by KissBox for their RTP-MIDI OEM module. This solution {{is similar to the}} one used by the MIDIBox community (external <b>communication</b> <b>processor</b> board, connected over a fast SPI link). This solution allows to use Arduino boards with very limited amount of RAM, like the Arduino Uno, and frees completely the microprocessor for user's tasks, since the external module implements the whole communication stack and act as buffer between the network and the Arduino.|$|E
5000|$|Nvidia's family {{includes}} primarily graphics, wireless <b>communication,</b> PC <b>processors</b> and automotive hardware/software. Some {{families are}} listed below: ...|$|R
50|$|RapidIO fabrics enjoy {{dominant}} {{market share}} in global deployment of cellular infrastructure 3G, 4G & LTE networks with millions of RapidIO ports shipped into wireless base stations worldwide. RapidIO fabrics were originally designed to support connecting different types of processors from different manufacturers together in a single system. This flexibility has driven {{the widespread use of}} RapidIO in wireless infrastructure equipment where {{there is a need to}} combine heterogeneous, DSP, FPGA and <b>communication</b> <b>processors</b> together in a tightly coupled system with low latency and high reliability.|$|R
5000|$|The 440 core is {{also used}} in the Cray XT3, XT4 and XT5 supercomputers, where its SeaStar, SeaStar2 and SeaStar2+ <b>communication</b> <b>processors</b> closely couples HyperTransport memory {{interface}} with routing to other nodes in supercomputer clusters. The SeaStar device provides a 6.4 GB/s connection to the Opteron based processors across HyperTransport (together making a processing element, PE), as well as six 7.6 GB/s links to neighboring PEs. SeaStar2+ offers 9.6 GB/s intra-node bandwidth and error correcting functionality to intercept errors en route between computing nodes.|$|R
50|$|Many second-generation CPUs {{delegated}} {{peripheral device}} communications to a secondary processor. For example, while the <b>communication</b> <b>processor</b> controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took {{at least two}} memory cycles; one for the instruction, one for the operand data fetch.|$|E
50|$|The Cyber 18 was a 16-bit {{minicomputer}} {{which was}} a successor to the CDC 1700 minicomputer. It was mostly used in real-time environments. One noteworthy application is {{as the basis of}} the 2550—a communications processor used by CDC 6000 series and Cyber 70/Cyber 170 mainframes. The 2550 was a product of CDC's Communications Systems Division, in Santa Ana, California (STAOPS). STAOPS also produced another <b>communication</b> <b>processor</b> (CP), used in networks hosted by IBM mainframes. This M1000 CP, later renamed C1000, came from an acquisition of Marshall MDM Communications. A three-board set was added to the Cyber 18 to create the 2550.|$|E
50|$|The MPC8xx {{family was}} Motorolas first PowerPC based {{embedded}} processors, suited for network processors and system-on-a-chip devices. The core is an original {{implementation of the}} PowerPC specification. It is a single issue, four stage pipelined core with MMU and branch prediction unit with speeds up to 133 MHz. The MPC821 was introduced in 1995 together with MPC860 with a complete QUICC engine. A slimmed down version, MPC850 with reduced caches and IO ports came in 1997. The QUICC <b>communication</b> <b>processor</b> module (CPM) offloads networking tasks from the CPU, thus branding this family as PowerQUICC. All processors in the family differ in on-chip features like USB, serial, PCMCIA, ATM and Ethernet controllers and different amount of L1 caches ranging from 1 KiB up to 16 KiB.|$|E
5000|$|... <b>processor</b> <b>{{communication}}</b> {{object is}} used for interprocessor communication ...|$|R
40|$|Although a prime goal of CAD {{frameworks}} is {{to facilitate}} cost effective, efficient, and seamless incorporation of tools into design systems little support {{is given to}} tool inte-grators. We present a new methodology called execution protocols that allows to abstract tool integration from a particular framework or tool. The actual design step is performed by an execution protocol engine that is guided by language and <b>communication</b> <b>processors</b> generated from an abstract specification of the relevant information content of data and start-up files. The execution protocol methodology may {{be regarded as a}} natural evolutionary step from today’s wrapper encapsulation. 1...|$|R
40|$|This paper {{describes}} and evaluates protocols for optimizing strided non-contiguous communication on the Quadrics QsNetII high-performance network interconnect. Most {{of previous}} related studies {{focused primarily on}} NIC-based or host-based protocols. This paper discusses merits for using both approaches and tries to determine for types and data sizes in the communication operations these protocols should be used. We focus on the Quadrics QsNetII-II network which offers powerful <b>communication</b> <b>processors</b> on the network interface card (NIC) and practical and flexible opportunities for exploiting them in context of user. Furthermore, the paper focuses on non-contiguous data remote memory access (RMA) transfers and performs th...|$|R
40|$|We {{have built}} a {{high-speed}} local-area network called Nectar that uses programmable communication processors as host interfaces. In contrast to most protocol engines, our communication processors have a flexible runtime system that supports multiple transport protocols as well as applicationspecific activities. In particular, we have implemented the TCP/IP protocol suite and Nectar-specific communication protocols on the <b>communication</b> <b>processor.</b> The Nectar network currently has 25 hosts {{and has been in}} use for over a year. The flexibility of our <b>communication</b> <b>processor</b> design does not compromise its performance. The latency of a remote procedure call between application tasks executing on two Nectar hosts is less than 500 ¯sec. The same tasks can obtain a throughput of 28 Mbit/sec using either TCP/IP or Nectar-specific transport protocols. This throughput is limited by the VME bus that connects a host and its <b>communication</b> <b>processor.</b> Application tasks executing on two communication proc [...] ...|$|E
40|$|Abstract: 2 ̆ 2 We {{have built}} a {{high-speed}} local-area network called Nectar that uses programmable communication processors as host interfaces. In contrast to most protocol engines, our communication processors have a flexible runtime system that supports multiple transport protocols as well as application-specific activities. In particular, we have implemented the TCP/IP protocol suite and Nectar-specific communication protocols on the <b>communication</b> <b>processor.</b> The Nectar network currently has 25 hosts {{and has been in}} use for over a year. The flexibility of our <b>communication</b> <b>processor</b> design does not compromise its performance. The latency of a remote procedure call between application tasks executing on two Nectar hosts is less than 500 [mu]sec. The same tasks can obtain a throughput of 28 Mbit/sec using either TCP/IP or Nectar-specific transport protocols. This throughput is limited by the VME bus that connects a host and its <b>communication</b> <b>processor.</b> Application tasks executing on two communication processors can obtain 90 Mbit/sec of the possible 100 Mbit/sec physical bandwidth using Nectar-specific transport protocols. 2 ̆...|$|E
40|$|Multithreaded {{architectures}} {{are becoming}} commercially diffused. The {{work of this}} thesis aims at investigating a possible use of this technology in high performance computing for supporting interprocess communication in shared memory systems by emulating the functionalities of a <b>communication</b> <b>processor</b> {{when it is not}} physically available...|$|E
40|$|The {{continued}} {{increase in}} Internet throughput {{and the emergence}} of broadband access networks drive the development of <b>communication</b> <b>processors.</b> Other developing arenas for the application of intelligent I/O are storage area networks and system area networks used to cluster computers and mass storage systems, respectively. However, given all advantages of such devices, they have a common memory bottleneck originating from the internal bus that connects the I/O ports with the internal processor's core. This paper presents a novel intelligent I/O architecture that eliminates this bottleneck by implementing a novel data transfer controller that grants a four-fold improvement over conventional designs. I...|$|R
50|$|MPPA is a MIMD (Multiple Instruction streams, Multiple Data) architecture, with {{distributed}} memory accessed locally, not shared globally. Each processor is strictly encapsulated, accessing only its own code and memory. Point-to-point <b>communication</b> between <b>processors</b> is directly {{realized in the}} configurable interconnect.|$|R
5000|$|In the beginning, Parsytec {{had participated}} in the GPMIMD (General Purpose MIMD) project under the {{umbrella}} of the ESPRIT project, both being funded by the European Commission's Directorate for Science.However, after substantial divisions with the other participants, Meiko, Parsys, Inmos and Telmat, as regards the choice of a common physical architecture, Parsytec left the project and announced a T9000-based machine of their own, i.e. the GC. But due to Inmos' problems with the T9000, they were forced to change to the ensemble Motorola MPC 601 CPUs and Inmos T805. This led to Parsytec's [...] "hybrid" [...] systems (e.g. GC/PP) degrading transputers to <b>communication</b> <b>processors</b> whilst the compute work was offloaded to the PowerPCs.|$|R
