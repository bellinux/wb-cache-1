12|25|Public
2500|$|In 2006, AIBO {{was added}} into the Carnegie Mellon University Robot Hall of Fame with the {{description}} [...] "the Sony AIBO represents {{the most sophisticated}} product ever offered in the <b>consumer</b> <b>robot</b> marketplace." ...|$|E
2500|$|Famed {{engineer}} Dr. Toshitada Doi {{is credited}} as AIBO’s original progenitor: in 1994 {{he had started}} work on robots with artificial intelligence expert Masahiro Fujita within CSL. Fujita would write that the robot's behaviors will need to “be sufficiently complex or unexpected so that people keep an interest in watching or taking care of it”. Fujita argued that entertainment robots might be viable as [...] "A robot for entertainment can be effectively designed using various state-of-the-art technologies, such as speech recognition and vision, even though these technologies may not be mature enough for applications where they perform a critical function. While there exists special and difficult requirements in entertainment applications themselves, limited capabilities in the speech and vision systems {{may turn out to}} be an interesting and attractive feature for appropriately designed entertainment robots." [...] His early monkey-like prototype [...] "MUTANT" [...] included behaviors that would become part of AIBOs including tracking a yellow ball, shaking hands, karate strikes and sleeping. Fujita would later receive the IEEE Inaba Technical Award for Innovation Leading to Production for [...] "AIBO, the world's first mass-market <b>consumer</b> <b>robot</b> for entertainment applications".|$|E
5000|$|In 2006, AIBO {{was added}} into the Carnegie Mellon University Robot Hall of Fame with the {{description}} [...] "the Sony AIBO represents {{the most sophisticated}} product ever offered in the <b>consumer</b> <b>robot</b> marketplace." ...|$|E
5000|$|... iRobot Corporation is an American {{advanced}} technology company founded in 1990 by three MIT graduates who designed robots for space exploration and military defense. [...] Incorporated in Delaware, the company designs and builds <b>consumer</b> <b>robots</b> for {{inside and outside}} of the home, including a range of autonomous home vacuum cleaners (Roomba), floor moppers (Braava), and other autonomous cleaning solutions.|$|R
40|$|Speech {{technologies}} {{nowadays available}} on mobile devices show an increased performance {{both in terms}} of the language that they are able to capture and in terms of reliability. The availability of performant speech recognition engines suggests the deployment of vocal interfaces also in <b>consumer</b> <b>robots.</b> In this paper, we report on our current work, by specifically focussing on the difficulties that arise in grounding the user's utterances in the environment where the robot is operating. © 2013 Springer-Verlag...|$|R
5000|$|A {{differentially}} steered robot {{is similar}} to the differential gears used in automobiles in that both the wheels can have different rates of rotations, but unlike the differential gearing system, a differentially steered system will have both the wheels powered. Differential wheeled robots are used extensively in robotics, since their motion is easy to program and can be well controlled. Virtually all <b>consumer</b> <b>robots</b> on the market today use differential steering primarily for its low cost and simplicity.|$|R
50|$|The WonderBorg (lead=yes) is a {{programmable}} <b>consumer</b> <b>robot</b> kit first released for the Bandai WonderSwan and Microsoft Windows PCs in 2000. It {{is intended}} to match both the external appearance and mode of transport of a beetle, with functioning antennae and a six-legged design.|$|E
50|$|The {{self-driving}} STANLEY and JUNIOR cars, led by Sebastian Thrun, won the DARPA Grand Challenge {{and came}} {{second in the}} DARPA Urban Challenge in the 2000s, and included SLAM systems, bringing SLAM to worldwide attention. Mass-market SLAM implementations can now be found in <b>consumer</b> <b>robot</b> vacuum cleaners such as the Neato XV11. Self-driving cars by Google and others have now received licenses to drive on public roads in some US states.|$|E
5000|$|In the 2000s, Sorayama's first-generation AIBO design (the {{robotics}} {{of which}} were developed by Sony's Toshitada Doi) received the Grand Prize of Best Design award, the highest design award conferred by Japan. AIBO has since {{been included in the}} permanent collections of the Museum of Modern Art (MOMA) and the Smithsonian Institution. MOMA published a book, Objects of Design by Paola Antonelli, which included AIBO along with other noteworthy designs where form and function combine in harmony. AIBO was the first [...] "artificially intelligent" [...] mass-market <b>consumer</b> <b>robot</b> for entertainment applications. It {{has been the subject of}} special studies at Carnegie Mellon University and other academic institutions. Also in this decade, Soryama produced nine more books, and the Nike [...] "White Dunk Project" [...] included Sorayama's art among the 25 most inspiring Japanese artists.|$|E
5000|$|Precision {{depends upon}} sensor precision, data {{granularity}} and calculation speed. Range-finding lasers may have +/-1 cm accuracy while digital stereo camera accuracy {{is limited to}} [...]25 pixel and thus is range-dependent. Vision-based systems require more computational resources than simple range-finding systems such as lasers, but may employ a digital signal processor embedded with the camera. Cost/precision trade-offs led to less expensive vision-based systems on <b>consumer</b> <b>robots</b> while commercial and industrial robots and automated guided vehicles (AGVs) tend to use laser-based systems.|$|R
40|$|The rapidly {{increasing}} number of elderly people {{has led to the}} development of in-home assistive robots for assisting and monitoring elderly people in their daily life. To these ends, indoor scene and human activity recognition is fundamental. However, image processing is an expensive process, in computational, energy, storage and pricing terms, which can be problematic for <b>consumer</b> <b>robots.</b> For this reason, we propose the use of computer vision cloud services and a Naive Bayes model to perform indoor scene and human daily activity recognition. We implement the developed method on the telepresence robot Double to make it autonomously find and approach the person in the environment as well as detect the performed activity...|$|R
40|$|Mobile robotic mapping is now {{considered}} to be a sufficiently mature field with demonstrated successes in various domains. While much {{progress has been made in}} the development of computationally efficient and consistent mapping schemes, it is still murky, at best, on how these maps can be evaluated. We are motivated by the absence of an accepted standard for quantitatively measuring the performance of robotic mapping systems against user-defined requirements. It is our belief that the development of standardized methods for quantitatively evaluating existing robotic technologies will improve the utility of mobile robots in already established application areas, such as vacuum cleaning, robot surveillance, and bomb disposal. This approach will also enable the proliferation and acceptance of such technologies in emerging markets. This chapter summarizes our preliminary efforts by bringing together the research community towards addressing this important problem which has ramifications not only from researchers perspective but also from <b>consumers,</b> <b>robot</b> manufacturers, and developers viewpoints...|$|R
5000|$|Famed {{engineer}} Dr. Toshitada Doi {{is credited}} as AIBO’s original progenitor: in 1994 {{he had started}} work on robots with artificial intelligence expert Masahiro Fujita within CSL. Fujita would write that the robot's behaviors will need to “be sufficiently complex or unexpected so that people keep an interest in watching or taking care of it”. Fujita argued that entertainment robots might be viable as [...] "A robot for entertainment can be effectively designed using various state-of-the-art technologies, such as speech recognition and vision, even though these technologies may not be mature enough for applications where they perform a critical function. While there exists special and difficult requirements in entertainment applications themselves, limited capabilities in the speech and vision systems {{may turn out to}} be an interesting and attractive feature for appropriately designed entertainment robots." [...] His early monkey-like prototype [...] "MUTANT" [...] included behaviors that would become part of AIBOs including tracking a yellow ball, shaking hands, karate strikes and sleeping. Fujita would later receive the IEEE Inaba Technical Award for Innovation Leading to Production for [...] "AIBO, the world's first mass-market <b>consumer</b> <b>robot</b> for entertainment applications".|$|E
50|$|The Falcon {{in essence}} is a <b>consumer</b> <b>robot.</b> It {{consists}} of its grip connected via three arms to a roughly conical body, which {{sits on a}} U-shaped base. Each of the three arms moves {{in and out of}} the Falcon's body. The default grip is a small spherical grip with 4 buttons on the top. The buttons are the Novint Logo for the primary button (which is similar to an 'N'), an upside down triangle (similar to a 'V'), a lightning bolt (similar to an 'N'), and a plus (similar to a 'T'), which collectively make the letters 'NVNT', the consonants in Novint's name and its ticker symbol as a public company. At the front flattened point of the Falcon's conical housing is a Novint Falcon logo that lights up in different colors to indicate the state of the device. The body contains 3 motors, each attached to one of the Falcon's arms by a cable that is wrapped around a capstan on the motor. As each of the 3 arms moves, an optical sensor attached to each motor keeps track of the movements of the arm. A mathematical function called a Jacobian is then used to determine the position of a three-dimensional cursor in Cartesian coordinates based on the positions of the arms. The position of that haptic cursor is therefore controlled by the Falcon's movements, and is used by the Falcon's software to determine the forces to be applied to the user. Currents are sent to the motors at the 1 kHz servo rate to present the user with an accurate sense of touch. In this way, a force can be applied to the grip in any direction, up to the maximum force (over 2 pounds of force), every 1/1000 of a second.|$|E
40|$|International audienceIn {{this paper}} we {{address the problem of}} {{recognizing}} everyday sound events in indoor environments with a <b>consumer</b> <b>robot.</b> Sounds are represented in the spectro-temporal domain using the stabilized auditory image (SAI) representation. The SAI is well suited for representing pulse-resonance sounds and has the interesting property of mapping a time-varying signal into a fixed-dimension feature vector space. This allows us to map the sound recognition problem into a supervised classification problem and to adopt a variety of classifications schemes. We present a complete system that takes as input a continuous signal, splits it into significant isolated sounds and noise, and classifies the isolated sounds using a catalogue of learned sound-event classes. The method is validated with a large set of audio data recorded with a humanoid robot in a house. Extended experiments show that the proposed method achieves state-of-the-art recognition scores with a twelve-class problem, while requiring extremely limited memory space and moderate computing power. A first real-time embedded implementation in a <b>consumer</b> <b>robot</b> show its ability to work in real conditions...|$|E
50|$|It {{was founded}} in 1979. In 2000, it moved from Aston Science Park to the University of Warwick. In 2001, around 1600 robots were sold to UK {{industry}}; ten years later it was around 800. The food industry and automotive industry in the UK are large <b>consumers</b> of <b>robots.</b> By 2002, the organisation estimated that there were around 13,500 industrial robots in the UK.|$|R
50|$|VxWorks {{is used by}} {{products}} {{over a wide range}} of market areas: aerospace and defense, automotive, industrial such as <b>robots,</b> <b>consumer</b> electronics, medical area and networking. Several notable products also use VxWorks as the onboard operating system.|$|R
40|$|Abstract- Developing an {{environment}} that enables optimal and flexible design of robot manipulators using reconfigurable links, joints, actuators, and sensors is an essential concept towards efficient robot design and prototyping. Such {{an environment}} should have {{a complex set of}} software and hardware subsystems for designing the physical parts and the controllers, and for the algorithmic control of the robot modules (kinematics, inverse kinematics, dynamics, trajectory planning, analog control and digital computer control). Specifying object-based communications and catalog mechanisms between the software modules, controllers, physical parts, CAD designs, and actuator and sensor components is a necessary step in the prototyping activities. In this project, we propose a web interface based prototyping environment for robot manipulators with the required subsystems and interfaces between the different components of this environment. The goal is to build a system of components that allows potential customers (located anywhere geographically) to input through a web interface a set of request / design parameters and specifications (such as torque, dexterity, repeatability, velocity etc), that could be analyzed, simulated and converted to specific manufacturing information that can be ultimately used by an automated manufacturing plant. The plant would be able to build <b>consumer</b> <b>robots</b> tailored to specific requirements and deliverable to customers flexibly. I...|$|R
40|$|Abstract – Many indoor {{robotics}} systems use laser rangefinders {{as their}} primary sensor for mapping, localization, and obstacle avoidance. The cost and power of such systems is a major roadblock to the deployment of low-cost, efficient <b>consumer</b> <b>robot</b> platforms for home use. In this paper, we describe a compact, planar laser distance sensor (LDS) that has capabilities comparable to current laser scanners: 3 cm accuracy out to 6 m, 10 Hz acquisition, and 1 degree resolution over a full 360 degree scan. The build cost of this device, using COTS electronics and custom mechanical tooling, is under $ 30. I...|$|E
40|$|In this paper, a new {{localization}} system utilizing afocal {{optical flow}} sensor (AOFS) based sensor fusion for indoor service robots in low luminance and slippery environment is proposed, where conventional localization systems do not perform well. To accurately estimate the moving {{distance of a}} robot in a slippery environment, the robot was equipped with an AOFS along with two conventional wheel encoders. To estimate {{the orientation of the}} robot, we adopted a forward-viewing mono-camera and a gyroscope. In a very low luminance environment, it is hard to conduct conventional feature extraction and matching for localization. Instead, the interior space structure from an image and robot orientation was assessed. To enhance the appearance of image boundary, rolling guidance filter was applied after the histogram equalization. The proposed system was developed to be operable on a low-cost processor and implemented on a <b>consumer</b> <b>robot.</b> Experiments were conducted in low illumination condition of 0. 1 lx and carpeted environment. The robot moved for 20 times in a 1. 5 × 2. 0 m square trajectory. When only wheel encoders and a gyroscope were used for robot localization, the maximum position error was 10. 3 m and the maximum orientation error was 15. 4 °. Using the proposed system, the maximum position error and orientation error were found as 0. 8 m and within 1. 0 °, respectively...|$|E
40|$|To allow wide-spread {{adoption}} of <b>consumer</b> robotics, <b>robots</b> {{must be able}} to adapt to theirenvironment by learning new skills and communicating with humans. Each chapter explains acontribution to achieve this goal. Chapter One covers a stochastic And-Or knowledgerepresentation framework for robotic manipulations. Chapter Two further expands thisestablished system for robustly learning from perception. Chapter Three unifies perception withnatural language for a joint real-time processing of information. We've successfully tested thegeneralizability and faithfulness of our robotic knowledge acquisition and inference pipeline. Wepresent proof of concepts {{in each of the three}} chapters...|$|R
40|$|Domestic {{service robots}} such as lawn mowing and vacuum {{cleaning}} robots {{are the most}} numerous <b>consumer</b> <b>robots</b> in existence today. While early versions employed random exploration, recent systems fielded {{by most of the}} major manufacturers have utilized range-based and visual sensors and user-placed beacons to enable robots to map and localize. However, active range and visual sensing solutions have the disadvantages of being intrusive, expensive, or only providing a 1 D scan of the environment, while the requirement for beacon placement imposes other practical limitations. In this paper we present a passive and potentially cheap vision-based solution to 2 D localization at night that combines easily obtainable day-time maps with low resolution contrast-normalized image matching algorithms, image sequence-based matching in two-dimensions, place match interpolation and recent advances in conventional low light camera technology. In a range of experiments over a domestic lawn and in a lounge room, we demonstrate that the proposed approach enables 2 D localization at night, and analyse the effect on performance of varying odometry noise levels, place match interpolation and sequence matching length. Finally we benchmark the new low light camera technology and show how it can enable robust place recognition even in an environment lit only by a moonless sky, raising the tantalizing possibility of being able to apply all conventional vision algorithms, even in the darkest of nights...|$|R
40|$|Robots {{are commonplace}} today in {{factories}} and on battlefields. The <b>consumer</b> market for <b>robots</b> is rapidly catching up. A worldwide survey of robots by the United Nations in 2006 revealed 3. 8 million in operation, 2. 9 million {{of which were}} for personal or service use. By 2007, there were 4. 1 million robots working just in people’s homes (Singe...|$|R
5000|$|Amaryllo robots {{are linked}} to Google Services. If {{consumers}} receive an email, Amaryllo robots will say [...] "You've got mail." [...] If consumers have appointments at 3 PM, Amaryllo robots will say [...] "You have an appointment at 3 pm," [...] minutes before the meeting to remind <b>consumers.</b> These <b>robots</b> can also say [...] "Hello", [...] "Good Morning", [...] "Good Afternoon", etc. when they detect events. These events could be motion, audio, or face detection pre-determined by users. These robots are wirelessly connected to networks, so {{they are aware of}} local time and can report time on an hourly fashion. For example, it will say [...] "It's 5 PM," [...] acting as a regular clock. More interactive voice communications are reported.|$|R
5000|$|Subsequently, the <b>consumer</b> goods <b>robots</b> rose {{up against}} the Quintessons in a {{rebellion}} that ultimately forced the aliens off the planet. Sentinel Major survived this clash, but he {{and the rest of}} his race—now dubbed [...] "Autobots"—were robbed of any chance of peace when the military hardware robots, now known as [...] "Decepticons", set their sights on conquering Cybertron for themselves. Two Autobot leaders fell during the war that ensued, and Sentinel Major inherited the Matrix of Leadership and became Sentinel Prime, leader of the Autobots. Under his leadership, the Autobots realized that they would never be able to defeat the Decepticons with firepower, and they turned instead to stealth, reconfiguring their bodies to be able to transform into other forms. With the power of his new ability, Sentinel Prime defeated the Decepticons' leader, and the Autobots won the war.|$|R
40|$|The {{travel range}} of a mobile robot is {{directly}} linked to its energy consumption. Apart from the actuators, the computing resources are the main energy <b>consumers.</b> Thus, <b>robots</b> that can navigate with minimal computational resources {{would be able to}} travel longer distances and, hence, would be valuable tools in applications such as search and rescue or planetary exploration. Inspired from the navigational abilities of insects, we developed the Trail-Map as a data structure for biologically inspired homing that can easily be scaled in case of memory or computational shortage. The Trail-Map can be built in constant time and enables constant time homing vector calculation. In this paper, we evaluate the Trail-Map-based homing performance of a simulated mobile robot equipped with an omnidirectional camera under the presence of sensor noise, such as odometry errors and observation errors. Further, we will show that Trail-Map-based homing outperforms SLAM methods in terms of computational resources while achieving a comparable homing performance...|$|R
30|$|The {{mobile robot}} placing in a {{workspace}} represents {{the definition of}} the position and orientation possibilities that can be achieved in that space. Controllability of a mobile robot defines the possible trajectories from its workspace. In order to move in the workspace, the mobile robots need certain mechanical components to allow precise movement (or movement would be chaotic). Currently, there are several components, including wheel, track and legs. The most popular designs are the wheeled mobile robots. The wheeled mobile robots are also better controlled than other types of robots. Disadvantages of wheeled robots are that they cannot navigate well over obstacles such as rocky terrain, sharp slopes, or areas with low friction. Usually, robots of this type are used during competitions (e.g., a soccer game, a sumo fight etc.) and are most popular in the <b>consumer</b> market. <b>Robots</b> can have any number of wheels, but three wheels are sufficient for static and dynamic balance. Additional wheels can add to balance; however, additional mechanisms will be required to keep all the wheels on the ground when the terrain is not flat.|$|R
30|$|Unfortunately, {{although}} {{the idea of}} making use of a preliminary selection of salient facial components and a subsequent emotion recognition phase based on geometrical or textural matching has been widely investigated, the achieved classification performances do not fulfill the demanding requirements of the technologies that a FER system has to serve. The main unresolved issues concern the alignment of components in different facial images, especially in case of extreme expressions. Moreover, they experienced high computational time due to the load for the fine extraction of the facial components (especially when iterative strategies are used) and then they appear to be not suitable for real world applications especially if low-power systems are involved (e.g. assistive <b>robot,</b> <b>consumer</b> analysis devices) (Sadeghi et al. 2013).|$|R
40|$|In Companion Projects I am {{specifically}} contesting {{the domestic}} robots marketed {{to people of}} all ages by creating robots with complicated psychological states similar to those that people might have, such as anxiety, neuroses, and psychoses. Omo, Amo, and Umo are three series of soft, lap sized, egg shaped, organ-like robots with secret lives, strange reactions, uncanny communications and neurotic propensities. They are held by people, and communicate through body signals. They are responsive to people (and animals, and machines) interacting with them, but unlike machines designed as <b>consumer</b> items, these <b>robots</b> are designed to be provocatively unstable. They are not emotionally subordinate, soothing, and "perfect" all the time - even if a person shows them affection they maintain the ability to misbehave. This presents people with emotional dilemmas and contradictions, as people vitally do with each other...|$|R
40|$|Abstract — The {{batteries}} of many <b>consumer</b> products, includ-ing <b>robots,</b> {{are often}} both {{a substantial portion}} of the product’s cost and commonly a first point of failure. Accurately predicting remaining battery life can lower costs by reducing unnecessary battery replacements. Unfortunately, battery dynamics are extremely complex, and we often lack the domain knowledge required to construct a model by hand. In this work, we take a data-driven approach and aim to learn a model of battery time-to-death from training data. Using a Dirichlet process prior over mixture weights, we learn an infinite mixture model for battery health. The Bayesian aspect of our model helps to avoid over-fitting while the nonparametric nature of the model allows the data to control the size of the model, preventing under-fitting. We demonstrate our model’s effectiveness by making time-to-death predictions using real data from nickel-metal hydride battery packs. I...|$|R
40|$|Abstract Date:                                     2017 / 06 / 05 Level:                                    Master thesis in Business Administration, 15 credits Institution:                                       School of Business, Society and Engineering, Mälardalen University Authors:                              Wilma de Boer                                                Jenny-Maria Åström                                                   (91 / 10 / 29)                                                        (87 / 06 / 30) Title:                                      Robots of {{the future}} are coming, are you ready? Tutor:                                    Cecilia Lindh Keywords:                          Robotics, {{technology}} acceptance, future robots, trust, innovation, UTAUT Research questions:                          - How can <b>consumer</b> acceptance of <b>robots</b> be studied as an international phenomenon?                                                   - What {{are the implications of}} <b>robot</b> acceptance for <b>consumers?</b> Purpose:                             The {{purpose of this study is}} to investigate the acceptance-level internationally, because {{of the future}} increase of new technology in the form of robotics. Method:                             A quantitative research method was conducted in this study. The data collection was done by a survey, via non-probability sampling. Conclusion:                       The findings of this study show that trust, anxiety and personal innovativeness influence the acceptance of robots internationally, while social influence does not affect the acceptance of the new technology in the context of robotics...|$|R
40|$|Thesis (Master's) [...] University of Washington, 2016 - 03 Early {{producers}} of robots {{in the emerging}} social robotics space rely heavily upon polysemy to present their products as both possessing an anthropomorphic ontology and being consumer products for sale. The receptional fragments of early purchasers of social robots suggest they too engage in {{a reading of the}} robots as both social beings and technological products. However, both producers and <b>consumers</b> of social <b>robots</b> are able to sustain these disparate readings only because one interpretation is perceived axiomatically as more “real” than the other. In the following paper, the textual, material, visual, and receptional rhetorics surrounding two robots anticipated to arrive in homes in 2016 are examined. Through close reading and analysis of numerous rhetorical devices—including identification, interpellation, narrative and metaphor—a new form of relational polysemy is revealed. Characterized as a both/and/only reading, the polysemy expressed by both producers and consumers in the rhetoric of social robots indicates consumerism as the defining force in the ontological categorization of social robots...|$|R
40|$|The {{research}} interests and applicability of robotics have diversified and seen a tremendous growth in recent years. There {{has been a}} shift from industrial robots operating in constrained settings to <b>consumer</b> <b>robots</b> working in dynamic environments associated closely with everyday human activities. Personal service robots to assist elderly, compliant robots with advanced perception skills for flexible manufacturing and autonomous driving vehicles for safe transportation are among the promising directions. In all these cases, robots have to work in close cooperation with human users and an intuitive higher level interaction between robots and layman users is essential for its widespread acceptability. Hence in this thesis, development of cognitive and perceptual skills in humans is studied and applied {{to the development of}} robot’s perceptual skills, especially based on visual information from a user interaction point of view. A physical robot is developed from scratch considering the aspects of affordability and user acceptability. A 9 DoF robot, LEA which incorporates a differential drive base, 4 DoF arm with a gripper and a pan-tilt neck supporting the robot’s head. The entire mechanics and control electronics are custom developed leading to decreased mechanical complexity and increased flexibility in physical dimensions. All the components are well integrated with a socially appealing industrial design which has been well received by the public and media. The limitations arising from simplified mechanics and affordable hardware are compensated by advanced adaptive vision algorithms to achieve the required functionalities of a service robot. A generic human centric architecture for highly autonomous and interactive robots is proposed to integrate various capabilities of a robot that are triggered by user interaction. A specific case of object recognition is investigated, as many tasks faced by such robots involve perception and manipulation of different household objects. An intuitive non-verbal interaction between a user and a robot for conveying objects of interest to the robot is developed. The developed spatial grounding model can detect the object of user interest independent of the relative position between the robot, the user and the object and without any prior training. This is achieved by a hybrid attention system combining bottom-up color saliency with depth image and top-down cues comprising user’s pointing direction and gaze. Robustness of gaze based attention system is improved by automatically switching between a keypoint based and a color based approach depending on objects’ texture. The recognition of these objects is achieved with a three layered semantic recognition framework that can incorporate multiple modalities of information. Developed based on studies of human perception, this method achieves recognition robustness in unconstrained domestic environments while providing semantic grounding with human users. Modalities of color, shape and object location have been incorporated into this recognition model while maintaining flexibility to include additional modalities. The first layer consists of semantic grounding modules that abstract raw sensory information into a probability distribution over meaningful semantic concepts familiar to humans. A second layer operates on these semantic features to obtain an object hypothesis based on every individual modality. The last layer performs knowledge association to estimate combined probability over known objects to obtain the final inference. A novel algorithm to track contours of objects and persons to allow exploration from different viewpoints is developed. Visual model of the target is refined by considering only the dominant 3 D cluster within the initial bounding box. A tracking-by detection algorithm constrains the search space in the image by removing regions based on metric size constancy of the object and other structural patterns like perpendicular planes. A feature based on Color Naming System has been used with an online learning classifier to obtain a color probability map while the depth probability map is obtained by using a Gaussian model of the object’s depth distribution. An optimal fusion of different object modalities using a target-background dissimilarity measure is developed and is used in a graphcut framework to continuously obtain contours of the target object. The reliability of recognition of these objects in challenging domestic environments is enhanced using visual appearances from multiple views while incorporating the spatial relations between these viewpoints as well. A Sequence Alignment algorithm has been used with vector quantized features from each view to achieve view point correlation in object recognition. A fast Visual Odometry estimation has been used to obtain viewpoint relations in an unsupervised manner and this has been incorporated with segmentation to provide a standalone system {{that can be used in}} real world scenario. This system is made generic to be used with different feature vectors and a benchmark is created to compare the performance improvement achieved by the developed system with respect to single view object recognition using different feature vectors. Object recognition in service robots can be augmented by incorporating the context of objects’ use within the developed semantic recognition framework. The utility of an object can be understood by the actions performed by the user on the object and hence an Action Recognition system based on human skeletal tracking with a novelty detection method is developed to facilitate the incremental learning of new actions. Compact representations of skeletal structure are obtained using a Torso-PCA transform and are used as observations for a HMM based system to recognize user actions. Uncertainty in predictions, quantified as confidence measures are thresholded to detect unknown actions. These confidence measures are obtained through background models and different methods are evaluated with respect to sensitivity and specificity of recognition performance. Various algorithms are developed to enhance the reliability of object perception overcoming challenges posed by dynamic environments and affordable hardware by incorporating different modalities of information available to a robot. The development of algorithms in this direction is significant as these concepts can be readily extended to incorporate user and environment recognition to complete the perceptual capabilities of robots …Intelligent Vehicles & Cognitive Robotic...|$|R
40|$|Carbon {{nanotubes}} (CNT) exhibit extraordinary {{mechanical properties}} and unique electronic properties and therefore, have received much attention {{for more than}} a decade now for a variety of applications ranging from nanoelectronics, composites to meeting needs in energy, environmental and other sectors. In this talk, we focus on some near term potential of CNT applications for both NASA and other Agency/societal needs. The most promising and successful application to date is a nano chem sensor at TRL 6 that uses a 16 - 256 sensor array in the construction of an electronic nose. Pristine, doped, functionalized and metal-loaded SWCNTs are used as conducting materials to provide chemical variation across the individual elements of the sensor array. This miniaturized sensor has been incorporated in an iPhone for homeland security applications. Gases and vapors relevant to leak detection in crew vehicles, biomedical, mining, chemical threats, industrial spills and others have been demonstrated. SWCNTs also respond to radiation exposure via a change in conductivity and therefore, a similar strategy is being pursued to construct a radiation nose to identify radiation sources (gamma, protons, neutrons, X-ray, etc.) with their energy levels. Carbon nanofibers (CNFs) grown using plasma enhanced CVD typically are vertical, individual, freestanding structures and therefore, are ideal for construction of nanoelectrodes. A nanoelectrode array (NEA) can be the basis for an affinity-based biosensor to meet the needs in applications such as lab-on-a-chip, environmental monitoring, cancer diagnostics, biothreat monitoring, water and food safety and others. A couple of demonstrations including detection of e-coli and ricin will be discussed. The NEA is also useful for implantation in the brain for deep brain stimulation and neuroengineering applications. Miniaturization of payload such as science instrumentation and power sources is critical to reduce launch costs. High current density (greater than 100 mA/per square centimeters) field emission capabilities of CNTs can be exploited for construction of electron gun for electron microscopy and X-ray tubes for spectrometers and baggage screening. A CNT pillar array configuration has been demonstrated, not only meeting the high current density needs but more importantly providing long term emitter stability. Finally, supercapacitors hold the promise to combine the high energy density of a battery with the high power density of capacitors. Traditional graphite electrodes have not delivered this promise yet. A novel design and processing approach using MWCNTs has shown a record 550 F/g capacitance along with significant device endurance. This supercapacitor is suitable for railgun launch application for NASA, powering rovers and <b>robots,</b> <b>consumer</b> electronics and future hybrid vehicles...|$|R

