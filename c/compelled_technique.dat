0|39|Public
40|$|Many {{apparently}} <b>compelling</b> <b>techniques</b> for automatic label placement use sophisticated heuristics for capturing cartographic knowledge, but, {{as noted}} by Zoraster (1991), also use inferior optimization strategies for nding good tradeo s between the variety of competing concerns involved in typical labeling problems. These techniques use procedural methods o...|$|R
40|$|Abstract. Knowledge {{compilation}} is a <b>compelling</b> <b>technique</b> {{for dealing}} with the intractability of propositional reasoning. One particularly effective target language is Deterministic Decomposable Negation Normal Form (d-DNNF). We exploit recent advances in #SAT solving in order to produce a new state-ofthe-art CNF â†’ d-DNNF compiler: DSHARP. Empirical results demonstrate that DSHARP is generally an order of magnitude faster than C 2 D, the de facto standard for compiling to d-DNNF, while yielding a representation of comparable size. ...|$|R
30|$|Nanoimprint {{lithography}} (NIL) is a <b>compelling</b> <b>technique</b> for {{low cost}} nanoscale device fabrication. The precise and repeatable replication of nanoscale patterns {{from a single}} high resolution patterning step makes the NIL technique much more versatile than other expensive techniques such as e-beam or even helium ion beam lithography. Furthermore, the use of mechanical deformation during the NIL process enables grayscale lithography with only a single patterning step, not achievable with any other conventional lithography techniques. These strengths enable the fabrication of unique nanoscale devices by NIL {{for a variety of}} applications including optics, plasmonics and even biotechnology. Recent advances in throughput and yield in NIL processes demonstrate the potential of being adopted for mainstream semiconductor device fabrication as well.|$|R
40|$|Dual-comb {{interferometry}} is {{a particularly}} <b>compelling</b> <b>technique</b> that relies on the phase coherence of two laser frequency combs for measuring broadband complex spectra. This method is rapidly advancing the field of optical spectroscopy and empowering new applications, from nonlinear microscopy to laser ranging. Up to now, most dual-comb interferometers were based on modelocked lasers, whose repetition rates have restricted the measurement speed to ~kHz. Here we demonstrate a dual-comb interferometer {{that is based on}} electrooptic frequency combs and measures consecutive complex spectra at an ultra-high refresh rate of 25 MHz. These results pave the way for novel scientific and metrology applications of frequency comb generators beyond the realm of molecular spectroscopy, where the measurement of ultrabroadband waveforms is of paramount relevance...|$|R
40|$|Today, service-orientation is {{considered}} as a <b>compelling</b> <b>technique</b> for developing agile software systems that best align with business. It is also {{considered as a}}n important approach for information technology (IT) service management, especially for service delivery and support. However, {{there is still some}} confusions and misunderstanding between people as sometimes they are referring to information technology service management (ITSM) services and sometimes to service-oriented architecture (SOA) services. So, the combination of both SOA and ITSM approaches arises. This paper deals with this combination and it provides some principles, policies and models that guide and explain the connections that may exist between the two worlds. Specifically, it provides a service model and governance, which cover the multitude necessary activities for specifying and managing how services are defined and supported within large enterprise...|$|R
40|$|The {{femtosecond}} laser frequency comb has enabled the 21 st century revolution in optical synthesis and metrology. A particularly <b>compelling</b> <b>technique</b> {{that relies on}} the broadband coherence of two laser frequency combs is dual-comb interferometry. This method is rapidly advancing the field of optical spectroscopy and empowering new applications, from nonlinear microscopy to laser ranging. Up to now, most dual-comb interferometers were based on modelocked lasers, whose repetition rates have restricted the measurement speed to ~ kHz. Here we demonstrate a novel dual-comb interferometer {{that is based on}} electrooptic frequency comb technology and measures consecutive complex spectra at a record-high refresh rate of 25 MHz. These results pave the way for novel scientific and metrology applications of frequency comb generators beyond the realm of molecular spectroscopy, where the measurement of ultrabroadband waveforms is of paramount relevance...|$|R
40|$|Some {{apparently}} powerful algorithms for automatic label placement on maps use heuristics {{that capture}} considerable cartographic expertise but are hampered by provably inefficient methods of search and optimization. On the other hand, no approach to label placement {{that is based}} on an efficient optimization technique has been applied to the production of general cartographic maps [...] - those with labeled point, line, and area features [...] - and shown to generate labelings of acceptable quality. We present an algorithm for label placement that achieves the twin goals of practical efficiency and high labeling quality by combining simple cartographic heuristics with effective stochastic optimization techniques. To appear in Cartographica. 1 Introduction Many apparently <b>compelling</b> <b>techniques</b> for automatic label placement use sophisticated heuristics for capturing cartographic knowledge, but, as noted by Zoraster (1991), also use inferior optimization strategies for finding good tradeoffs betwe [...] ...|$|R
40|$|Data with {{hierarchical}} structure arise in many fields. Estimating global effect sizes from nested data, and testing effects against global null hypotheses, is, however, more challenging {{than in the}} traditional setting of independent and identically distributed data. In this paper, we review statistical approaches to deal with nested data following either a fixed-effect or a random-effects model. We focus on methods {{that are easy to}} implement, such as group-level t-tests and Stouffer's method. The properties of these approaches are discussed within the context of neuroimaging applications, quantitatively assessed on simulated data, and demonstrated on real human neurophysiological data from a simulated-driving experiment. With what we call the inverse-variance-weighted sufficient-summary-statistic approach, we highlight a particularly <b>compelling</b> <b>technique</b> that combines desirable statistical properties with computational simplicity, and we provide step-by-step instructions to apply it to a number of popular measures of effect size. Comment: 14 pages, 4 figure...|$|R
40|$|Phage {{display is}} a <b>compelling</b> <b>technique</b> of {{synthetic}} binding protein engineering {{which allows for}} a firm genotype-phenotype linkage which {{can be applied to}} selection and evolution of engineered binding protein scaffolds. Mutations can be made to the pVIII protein providing capability to significantly increase the surface display of large proteins on a phage particle 1. Random and rational mutagenesis of the major coat protein, pVIII, was performed on the first 30 residues in 10 -mer segments. The array of mutations imbedded into the wild type pVIII was mass produced to construct a phage display library. Promising mutants will selected by the screening of avid activity by ELISA selection. The effective clones were paired with DNA from the fibronectin Gr 2 library, and captured on ELISA plates by binding interaction with hen egg lysozyme for quantification by plate reader. Initial experiments yielded no avid clones, but this is likely due to the weak affinity of the first Fn chosen for capture ELISA in these experiments. Adjustments were made and are being tested to discover pVIII clones with avid functionality. This research was supported by the Undergraduate Research Opportunities Program (UROP) ...|$|R
40|$|Understanding the {{governing}} mechanism of solar magnetism remains an outstanding challenge in astrophysics. Seismology {{is the most}} <b>compelling</b> <b>technique</b> with which to infer the internal properties of the Sun and stars. Waves in the Sun, nominally acoustic, {{are sensitive to the}} emergence and cyclical strengthening of magnetic field, evidenced by measured changes in resonant oscillation frequencies that are correlated with the solar cycle. The inference of internal Lorentz stresses from these measurements has the potential to significantly advance our appreciation of the dynamo. Indeed, seismological inverse theory for the Sun is well understood for perturbations in composition, thermal structure and flows but, is not fully developed for magnetism, owing to the complexity of the ideal magnetohydrodynamic (MHD) equation. Invoking first-Born perturbation theory to characterize departures from spherically symmetric hydrostatic models of the Sun and applying the notation of generalized spherical harmonics, we calculate sensitivity functions of seismic measurements to the general time-varying Lorentz stress tensor. We find that eigenstates of isotropic (i. e. acoustic only) background models are dominantly sensitive to isotropic deviations in the stress tensor and much more weakly so to anisotropic stresses (and therefore challenging to infer). The apple cannot fall far from the tree. Comment: 18 pages, 4 figures; MNRA...|$|R
40|$|The {{delivery}} of amino acids {{to the early}} Earth by interplanetary dust particles, comets, and carbonaceous meteorites {{could have been a}} significant source of the early Earth's prebiotic organic inventory. Amino acids are central to modern terrestrial biochemistry as major components of proteins and enzymes and were probably vital in the origin of life. A variety of amino acids have been detected in the CM carbonaceous meteorite Murchison, many of which are exceptionally rare in the terrestrial biosphere including a-aminoisobutyric acid (AIB) and isovaline. AIB has also been detected in a small percentage of Antarctic micrometeorite grains believed {{to be related to the}} CM meteorites We report on progress in optimizing a nanoflow liquid chromatography separation system with dual detection via laser-induced-fluorescence time of flight mass spectrometry (nLC-LIF/ToF-MS) for the analysis of o-phthaldialdehydelN-acetyl-L-cysteine (OPA/NAC) labeled amino acids in cosmic dust grains. The very low flow rates (0. 1 ml/min) combined with 4 orders of magnitude lower than traditional GC-MS techniques), and specificity (compounds identities are determined by both retention time and exact mass) makes this a <b>compelling</b> <b>technique.</b> However, the development of an analytical method to achieve separation of compounds as structurally similar as amino acid monomers and produce the sharp peaks required for maximum sensitivity is challenging...|$|R
40|$|Recent {{technological}} advances in input sensing, {{as well as}} ultra-small projectors, have opened up new opportunities for interaction â€“ {{the use of the}} body itself as both an input and output platform. Such on-body interfaces offer new interac-tive possibilities, and the promise of access to computation, communication and information literally in the palm of our hands. The unique context of on-body interaction allows us to take advantage of extra dimensions of input our bodies naturally afford us. In this paper, we consider how the arms and hands can be used to enhance on-body interactions, which is typically finger input centric. To explore this op-portunity, we developed Armura, a novel interactive on-body system, supporting both input and graphical output. Using this platform as a vehicle for exploration, we proto-typed many applications and interactions. This helped to confirm chief use modalities, identify fruitful interaction approaches, and in general, better understand how interfaces operate on the body. We highlight the most <b>compelling</b> <b>techniques</b> we uncovered. Further, this paper is the first to consider and prototype how conventional interaction issues, such as cursor control and clutching, apply to the on-body domain. Finally, we bring to light several new and unique interaction techniques. ACM Classification: H. 5. 2 [Information interfaces an...|$|R
40|$|A {{proof of}} {{ownership}} based on digital forensics evidence {{is the way}} forward in solving ownership disputes in the ever growing cyberspace that confronts us today. A strong proof beyond all reasonable doubts with the good standing of the law provides victory for a case before a competent court of law. The Society we live in is governed by laws and laws need evidence to pass judgments and not any kind of evidence but evidence compelling enough to attract the passing of judgment. Lessons learnt from the past of {{the failures of the}} court to provide and use strong verified evidence due to limitations of scientific evidence such as DNA, surveillance videos, authenticable images etc. have cost human lifeâ€™s freedom and having access to a fair trial. We cannot wait till certain loopholes are exploited in our digital cyber evolution before we get it fixed. Hence stronger and a more <b>compelling</b> <b>techniques</b> are needed for image authentication and identification. In our work, we proposed a spatial domain watermarking approach for digital images based on image features built on formal concepts analysis. We adopt the use of formal concept analysis {{due to the fact that}} a change in a pixel value can result in a change in the lattice generated from the image. We presented our results and they showed to be very effective...|$|R
40|$|Sensing static or slowly varying {{magnetic}} fields with high sensitivity and spatial resolution {{is critical to}} many applications in fundamental physics, bioimaging and materials science. Several versatile magnetometry platforms have emerged over the past decade, such as electronic spins associated with Nitrogen Vacancy (NV) centers in diamond. However, their high sensitivity to external fields also makes them poor sensors of DC fields. Indeed, the usual method of Ramsey magnetometry leaves them prone to environmental noise, limiting the allowable interrogation time to the short dephasing time T 2 *. Here we introduce a hybridized magnetometery platform, consisting of a sensor and ancilla, that allows sensing static {{magnetic fields}} with interrogation times up to the much longer T 2 coherence time, allowing significant potential gains in field sensitivity. While more generally applicable, we demonstrate the method for an electronic NV sensor and a nuclear ancilla. It relies on frequency upconversion of transverse DC fields through the ancilla, allowing quantum lock-in detection with low-frequency noise rejection. In our experiments, we demonstrate sensitivities better than 6 uT/vHz, comparable to the Ramsey method, and narrow-band signal noise filtering better than 64 kHz. With technical optimization, we expect more than an one order of magnitude improvement {{in each of these}} parameters. Since our method measures transverse fields, in combination with the Ramsey detection of longitudinal fields, it ushers in a <b>compelling</b> <b>technique</b> for sensitive vector DC magnetometry at the nanoscale. Comment: 12 + 3 page...|$|R
40|$|ABSTRACT. Information {{visualization}} is a <b>compelling</b> <b>technique</b> for {{the exploration}} {{and analysis of}} the large and complex data sets generated by computer applications. Visualization takes advantage of the immense power, bandwidth, and pattern recognition capabilities of the human visual system. It enables users to see large amounts of data in a single display, and to discover patterns, trends, and outliers within the data. Visualization is becoming increasingly important in Semantic Web tools. In particular, visualization is used in tools that support the development of ontologies, such as ontology extraction tools or ontology editors. The majority of existing works is, concerned by the creation of ontologies, once the consensus is achieved (called post-consensus ontologies); but the main problem is how to reach this consensus? This work tries to answer this question by proposing to create a pre-consensus ontology starting from the opinions of various actors. For instance, we insist on the necessity to keep different definitions of the same concept and multimedia illustrations of the concepts. In this paper, our work {{within the framework of the}} TOWNTOLOGY project aims at writing urban ontologies in a visual form. This visualization is a way to be explored towards a co-operative system for reaching consensus. From the descriptive point of view of ontologies, an extension of XML is designed and used. As results, we build a Towntology prototype system (the Towntology Tool suite) in order to store the ontology, to display the ontology in visual graphic form, to navigate in the ontology, to query it and to update it...|$|R
40|$|Crowdsourcing is {{the process}} of {{outsourcing}} numerous tasks to many untrained individuals. Our aim was to assess the performance and repeatability of crowdsourcing for the classification of retinal fundus photography. One hundred retinal fundus photograph images with pre-determined disease criteria were selected by experts from a large cohort study. After reading brief instructions and an example classification, we requested that knowledge workers (KWs) from a crowdsourcing platform classified each image as normal or abnormal with grades of severity. Each image was classified 20 times by different KWs. Four study designs were examined to assess the effect of varying incentive and KW experience in classification accuracy. All study designs were conducted twice to examine repeatability. Performance was assessed by comparing the sensitivity, specificity and area under the receiver operating characteristic curve (AUC). Without restriction on eligible participants, two thousand classifications of 100 images were received in under 24 hours at minimal cost. In trial 1 all study designs had an AUC (95 %CI) of 0. 701 (0. 680 - 0. 721) or greater for classification of normal/abnormal. In trial 1, the highest AUC (95 %CI) for normal/abnormal classification was 0. 757 (0. 738 - 0. 776) for KWs with moderate experience. Comparable results were observed in trial 2. In trial 1, between 64 - 86 % of any abnormal image was correctly classified by over half of all KWs. In trial 2, this ranged between 74 - 97 %. Sensitivity was â‰¥ 96 % for normal versus severely abnormal detections across all trials. Sensitivity for normal versus mildly abnormal varied between 61 - 79 % across trials. With minimal training, crowdsourcing represents an accurate, rapid and cost-effective method of retinal image analysis which demonstrates good repeatability. Larger studies with more comprehensive participant training are needed to explore the utility of this <b>compelling</b> <b>technique</b> in large scale medical image analysis...|$|R
40|$|International audiencePhysical {{simulation}} {{has emerged}} as a <b>compelling</b> animation <b>technique,</b> yet current approaches to coupling simulations of fluids and solids with irregular boundary geometry are inefficient or cannot handle some relevant scenarios robustly. We propose a new variational approach which allows robust and accurate solution on relatively coarse Cartesian grids, allowing possibly orders of magnitude faster simulation. By rephrasing the classical pressure projection step as a kinetic energy minimization, broadly similar to modern approaches to rigid body contact, we permit a robust coupling between fluid and arbitrary solid simulations that always gives a well-posed symmetric positive semi-definite linear system. We provide several examples of efficient fluid-solid interaction and rigid body coupling with sub-grid cell flow. In addition, we extend the framework with a new boundary condition for free-surface flow, allowing fluid to separate naturally from solids...|$|R
40|$|Textile {{wastewater}} is hard {{to treat}} because they are originate from multi processes which produce multi segments of wastewater. Expansive measure of suspended solids, high color and high chemical oxygen demand of the wastewater can cause serious ecological issues. Adsorption is a <b>compelling</b> <b>technique</b> for bringing down the convergence of dissolved dyes in the effluent resulting in colour removal. Thus, there is a developing enthusiasm for utilizing material such as agricultural by-product for instance, rubber wood sawdust since they are abundantly available and kenaf fibre. The crude materials, kenaf core fibre (KCF) and rubber wood sawdust (RWS) were carbonized for an hour at 500 oC, soaked in H 3 PO 4 and activated by microwave processing with input power of 500 W and irradiation time of 6 min. Impregnation of RWAC and KFAC with CuO powder was carried out through solid state reaction of mechanochemical activation processing followed by carbothermal reduction. 0. 4 g of activated carbon produced from KFAC and RWAC of 60 % acid loading were used to treat 50 ml of effluent textile wastewater with the pH of 11. 2 with 4 different contact time. The optimum contact time was 90 minutes then {{used to test the}} best dosage of activated carbon for adsorption of dye in effluent wastewater. 2. 0 g of KFAC and RWAC then used to treat 50 ml of effluent wastewater for 90 minutes to study the best pH for dye adsorption. The best pH was pH 3. Then the best parameters from the treatment then used to treat wastewater with CuO assisted KFAC and RWAC. CuO assisted activated carbon proven can enhance reaction activity. The study shows that microwave heating is an effective and facilitate method which can be used for preparation of activated carbon and the effectiveness of activated carbon produced by microwave processing proven can be use to remove dye from textile wastewater...|$|R
40|$|Mobile devices (e. g., smart phones) and {{interactive}} surfaces (e. g., digital tabletops) represent two distinct device classes: One is personal and portable, the other shared and stationary. They excel at different tasks, {{and are less}} suited for others. Combining their distinct characteristics can realize various synergies. In doing so, we recently introduced PhoneTouch, a system that allows users to directly touch a digital surface with their phones, much like a stylus, for immediate cross-device interaction. The purpose might be to seamlessly transfer photos in both directions, for example. We developed a series of <b>compelling</b> interaction <b>techniques</b> and applications to illustrate the fluidity of our approach. Throughout our design iterations, all test users exposed to the system were enthused by the nature and simplicity of interaction. Author Keywords Mobile devices, smart phones, interactive surfaces, tabletops, synergies, integration. Figure 1. A novel cross-device interaction style that seamlessly bridges the gap between handheld devices {{and interactive}} surfaces. Copyright is held by the author/owner(s) ...|$|R
40|$|Figure 1 : Left: A solid {{stirring}} smoke runs at interactive rates, two {{orders of}} magnitude faster than previously. Middle: Fully coupled rigid bodies of widely varying density, with flow visualized by marker particles. Right: Interactive manipulation of immersed rigid bodies. Physical simulation {{has emerged as a}} <b>compelling</b> animation <b>technique,</b> yet current approaches to coupling simulations of fluids and solids with irregular boundary geometry are inefficient or cannot handle some relevant scenarios robustly. We propose a new variational approach which allows robust and accurate solution on relatively coarse Cartesian grids, allowing possibly {{orders of magnitude}} faster simulation. By rephrasing the classical pressure projection step as a kinetic energy minimization, broadly similar to modern approaches to rigid body contact, we permit a robust coupling between fluid and arbitrary solid simulations that always gives a wellposed symmetric positive semi-definite linear system. We provide several examples of efficient fluid-solid interaction and rigid body coupling with sub-grid cell flow. In addition, we extend the framework with a new boundary condition for free-surface flow, allowing fluid to separate naturally from solids...|$|R
40|$|The recent {{application}} of electron tomography {{to the study}} of biomaterial interfaces with bone has brought about an awareness of nano-osseointegration and, to a further extent, demanded increasingly advanced characterization methodologies. In this study, nanoscale osseointegration has been studied via laser-modified titanium implants. The micro- and nano-structured implants were placed in the proximal tibia of New Zealand white rabbits for six months. High-resolution transmission electron microscopy (HRTEM), analytical microscopy, including energy dispersive X-ray spectroscopy (EDXS) and energy-filtered TEM (EFTEM), as well as electron tomography studies were used to investigate the degree of nano-osseointegration in two- and three-dimensions. HRTEM indicated the laser-modified surface encouraged the formation of crystalline hydroxyapatite in the immediate vicinity of the implant. Analytical studies suggested the presence of a functionally graded interface at the implant surface, characterized by the gradual intermixing of bone with oxide layer. Yet, the most <b>compelling</b> of <b>techniques,</b> which enabled straightforward visualization of nano-osseointegration, proved to be segmentation of electron tomographic reconstructions, where thresholding techniques identified bone penetrating into the nanoscale roughened surface features of laser-modified titanium. Combining high-resolution, analytical and three-dimensional electron microscopy techniques has proven to encourage identification and understanding of nano-osseointegration...|$|R
40|$|Nothing {{replaces the}} {{importance}} of the sweat, blood and tears of a live simulated training experience in which all your senses (visual, audio, haptic, olfactory, gastronomy, etc.) play into a physical, mental and emotional life-anddeath scenario in a fully three dimensional, real-time world. When Virtual Reality is provided as an alternative, it can pale in comparison, as it is a disembodied experience no matter how much artistry has been applied to the aesthetic display and emotional thrill. This statement is applicable even to military simulations that drive complex and intricate training, and yet rarely cause trainees to break a sweat. There is a need for systems that integrate training scenarios into physically responsive live environments, enhanced by <b>compelling</b> entertainment <b>techniques.</b> Such systems must support the delivery {{of a wide range of}} simulation applications including vehicular, dismounted and constructive simulation planning. This paper covers recent developments in integrating multi-modal functionality into Mixed Reality (MR), the blending of real and virtual sight, sound and special effects. More specifically, we present an overview of an MR research project and the multi-modal training engine (versus game engine) produced as a consequence of this research. This engine composes real and synthetic sensory stimulations into an interactive, multi-sensory, non-linear, immersive experience...|$|R
50|$|As a {{short story}} writer, Indu Menon {{can be said to}} have succeeded Kamala Das, who traversed the worlds of poetry and fiction with ease. This, however, {{is not to suggest that}} she is a story-teller cast in the â€˜Kamala Das mould'. Far from plodding along the trodden path, she courageously explores new areas.Indu Menon's themes as well as <b>technique</b> <b>compel</b> attention. Going beyond issues such as gender and sexuality, which young women writers are {{generally}} preoccupied with, she challenges Kerala society that is showing unmistakable signs of regression, at various other levels too. For instance, she takes on the forces of communalism in a way few other writers, man or woman, have done. Hers is the voice of the â€˜New Woman', which is yet to make itself heard in the public space.|$|R
40|$|In this paper, {{we present}} a {{thorough}} and realistic analysis of audio conferencing over application-level multicast (ALM). Through flexibility and ease-of-deployment, ALM is a <b>compelling</b> alternative group-communication <b>technique</b> to IP Multicast â€” which has yet to see wide-scale deployment in the Internet. However, proposed ALM techniques suffer from inherent latency inefficiencies, which we show, through realistic simulation and exploration of perceived quality in multi-party conversation, to be greatly problematic for the realisation of truly-scalable audio-conferencing systems over ALM. In this work, we propose to adapt dynamically the application-level distribution structure to the conversational pattern of the audio conference. The contribution {{of this paper is}} threefold: we develop a novel perceptual quality model for multi-party audio conversations; we provide dynamic adaptation via a simple next-speaker prediction technique and we validate the proposed approach by using a large and detailed corpus of real multi-party conversations...|$|R
40|$|Background In December 2017 {{following}} a consultation process, the Committee of Advertising Practice, who regulate UK non-broadcast advertising announced that new restrictions for non-broadcast advertising for foods high in fat, {{sugar and salt}} (HFSS) would be introduced to protect {{the health and well-being}} of children and young people. Ahead of submitting to this consultation, we set out to garner the views of young people on the proposed regulations. Methods We conducted 15 focus groups with sixty-five 12 - 15 year olds living in the UK. Participants were recruited using snowball sampling from initial local adult contacts and all groups were audio-recorded. Topics related to the consultation areas of interest. We presented broadcast and non-broadcast advertising to stimulate discussion. Transcripts were analysed thematically. Results We found that young people recalled advertising for products HFSS across a range of non-broadcast media. They reported rarely viewing live TV broadcasts, or receiving parental oversight of their viewing habits. Although they were sceptical of marketing practices, and discussed strategies to avoid advertising, they stated that they were influenced by celebrities, <b>compelling</b> advertising <b>techniques,</b> competitions and peer endorsement. Young people were also exposed to marketing in settings outside the remit of the code through product packaging, restaurant competitions and price promotions. Conclusions We conclude that existing rules surrounding non-broadcast advertising were too lax and that these needed to be strengthened to fully protect children. Changes to the rules will not cover all settings where young people are exposed to marketing...|$|R
40|$|Many {{important}} classification {{problems are}} polychotomies, i. e. {{the data are}} organized into K classes with K ? 2. Given an unknown function F :Î© ! f 1; : : :; Kg representing a polychotomy, an algorithm aimed at "learning" this polychotomy will produce an approximation of F, based on the knowledge {{of a set of}} pairs f(x p; F (x p)) g P p= 1. Although in the wide variety of learning tools there exist some learning algorithms capable of handling polychotomies, many of the interesting tools were designed by nature for dichotomies (K = 2). Therefore, many researchers are <b>compelled</b> to use <b>techniques</b> to decompose a polychotomy into a series of dichotomies in order to apply their favorite algorithms to the resolution of a general problem. A decomposition method based on error-correcting codes has been lately proposed and shown to be very efficient. However, this decomposition is designed only on the basis of K without taking the data into account. In this paper, we explore alter [...] ...|$|R
40|$|In this paper, {{we present}} a {{thorough}} and realistic analy-sis of voice (i. e. audio conferencing) over application-level multicast (ALM). Through flexibility and ease-of-deployment, ALM is a <b>compelling</b> alternative group-communication <b>technique</b> to IP Multicast â€” which has yet to see wide-scale deploy-ment in the Internet. However, proposed ALM techniques suffer from inherent latency inefficiencies, which we show, through realistic simulation and exploration of perceived quality in multi-party conversation, to be greatly problem-atic for the realisation of truly-scalable audio-conferencing systems over ALM. By incorporating talkspurt data from a large and de-tailed corpus of multi-party conversation, and through using network-simulation techniques based on actual In-ternet latency measurements, we develop our previous work on the Application-Level Network Audio-Conferenc-ing (ALNAC) routing protocol into a thorough analysis of the problem, leading to a novel model for assessing the perceptual quality of multi-party conversation and to novel techniques for speaker prediction. We show that through adaptation to conversational patterns, the ALNAC pro-tocol can achieve perceptual quality for large-scale audio conferencing that, with little cost to each end-system node, is comparable to IP Multicast. ...|$|R
40|$|Hierarchical routing {{has often}} been {{mentioned}} as an appealing point-to-point routing technique for wireless sensor networks (sensornets). While there is a volume of analytical and high-level simulation results demonstrating its merits, {{there has been little}} work evaluating it in actual sensornet settings. This article bridges the gap between theory and practice. Having analyzed a number of proposed hierarchical routing protocols, we have developed a framework that captures the common characteristics of the protocols and identifies design points at which the protocols differ. We use a sensornet implementation of the framework in TOSSIM and on a 60 -node testbed to study various trade-offs that hierarchical routing introduces, as well as to compare the performance of hierarchical routing with the performance of other routing techniques, namely shortest-path routing, compact routing, and beacon vector routing. The results show that hierarchical routing is a <b>compelling</b> routing <b>technique</b> also in practice. In particular, despite only logarithmic routing state, it can offer small routing stretch: An average of ~ 1. 25 and a 99 th percentile of 2. It can also be robust, minimizing the maintenance traffic or the latency of reacting to changes in the network. Moreover, the trade-offs offered by hierarchical routing are attractive for many sensornet applications when compared to the other routing techniques. For example, in terms of routing state, hierarchical routing can offer scalability at least an order of magnitude better than compact routing, and at the same time, in terms of routing stretch, its performance is within 10 - 15 % of that of compact routing; in addition, this performance can further be tuned to a particular application. Finally, we also identify a number of practical issues and limitations of which we believe sensornet developers adopting hierarchical routing should be aware. Â© 2012 ACM...|$|R
40|$|Abstract. Many {{important}} classi cation {{problems are}} polychotomies, i. e. {{the data are}} organized into K classes with K> 2. Given an unknown function F:!f 1 ï¿½:::ï¿½Kg representing a polychotomy, an algorithm aimed at " this polychotomy will produce an approximation of F, based on the knowledge {{of a set of}} pairs f(x p ï¿½F(x p)) g P p= 1. Although in the wide variety of learning tools there exist some learning algorithms capable of handling polychotomies, many ofthe interesting tools were designed by nature for dichotomies (K = 2). Therefore, many researchers are <b>compelled</b> to use <b>techniques</b> to decompose a polychotomy into a series of dichotomies in order to apply their favorite algorithms to the resolution of a general problem. A decomposition method based on error-correcting codes has been lately proposed and shown to be very e cient. However, this decomposition is designed only on the basis of K without taking the data into account. In this paper, we explore alternatives to this method, still based on the fruitful idea of errorcorrecting codes, but where the decomposition is inspired by the data at hand. The e ciency of this approach, both for the simplicity of the model and for the generalization, is illustrated by some numerical experiments...|$|R
40|$|If {{magnetic}} {{frustration is}} most commonly known for undermining long-range order, as famously illustrated by spin liquids, {{the ability of}} matter to develop new collective mechanisms in order to fight frustration is no less fascinating, providing an avenue for the exploration and discovery of unconventional properties of matter. Here we study an ideal minimal model of such mechanisms which, incidentally, pertains to the perplexing quantum spin ice candidate Yb 2 Ti 2 O 7. Specifically, we explain how thermal and quantum fluctuations, optimized by order-by-disorder selection, conspire to expand the stability region of an accidentally degenerate continuous symmetry U(1) manifold against the classical splayed ferromagnetic ground state that is displayed by the sister compound Yb 2 Sn 2 O 7. The resulting competition gives rise to multiple phase transitions, in striking similitude with recent experiments on Yb 2 Ti 2 O 7 [Lhotel et al., Phys. Rev. B 89 224419 (2014) ]. Considering the effective Hamiltonian determined for Yb 2 Ti 2 O 7, we provide, by combining a gamut of numerical <b>techniques,</b> <b>compelling</b> evidence that such multiphase competition is the long-sought missing key to understanding the intrinsic properties of this material. As a corollary, our work offers a pertinent illustration {{of the influence of}} chemical pressure in rare-earth pyrochlores. Comment: 9 page...|$|R
40|$|The {{cascading}} order variability from downstream trumping up the upstream site of {{the supply}} chain network indicates the deleterious effect {{to the performance of}} the fast moving consumer goods industry. The fundamental likelihood to optimization in this industry requires dexterous flows of quasi-real-time information, as well as reliable product availability. In this context, this study analyzes the challenges of bullwhip effect on the perspective of ingenious optimization strategies, and further contemplates to establish the engineering patterns of interrelationships on the magnitude of pooling the resources to advance supply chain capabilities. The suppression of bullwhip effect on underlying optimization strategies is sought to elevate accelerated responsiveness, improve network demand visibility and reduce volatility in frequencies to inventory replenishment. A rigorous and disciplined quantitative approach afforded the tentatively development of pattern of interrelated supply chain dimensions. The factor analysis method was used on 448 responses and insightful findings were produced from the <b>compelling</b> purposive sampling <b>technique.</b> The findings indicate that the magnitude of better ameliorating bullwhip effect, the value of competitive economic information and strength of selected optimization strategies depend on the model of unified engineering patterns. This paper provides insights to FMCG industry on using innovative strategies and modern technology to enhance supply chain visibility through integrated systems networks...|$|R
40|$|Pilgrimage {{is perhaps}} the most {{characteristic}} and <b>compelling</b> literary <b>technique</b> of the medieval period. In spite of the diversity of instantiations of pilgrimage in medieval literature, critics have examined pilgrimage almost exclusively as an allegorical apparatus signifying spiritual, semiotic, or historical progress. This project re-evaluates the allegorical significance of pilgrimage and also explores how the non-allegorical significance of pilgrimage influenced premodern literary strategy and history in England. ^ Pilgrimage is a fundamentally spatial phenomenon, and yet it has barely been considered in the context of critical mark on medieval space and place. Moreover, the cultural significance of English pilgrimage has been particularly neglected. Pilgrimage was a peculiarly vexed practice in England, where orthodox and heterodox Christian antipathy towards the ritual culminated in Henry VIII 2 Ì† 7 s prohibition of pilgrimage in the Injunctions of l 536 and 1538. This proscription, along with England 2 Ì† 7 s location marginal to the renowned pilgrimage routes through Europe and Palestine, has resulted in the uniquely interrupted and expurgated history and geography of insular pilgrimage. I examine pilgrimage as a spatial, spiritual, and anthropological phenomenon to theorize and historicize medieval English place. Moreover, I investigate how the fundamentally spatial phenomenon of pilgrimage inflects the dynamics between premodern place and premodern literature. ^ Throughout the dissertation I consider {{the ways in which the}} operations of pilgrimage and place complicate our understanding of allegory in premodern texts, attending to the significance of particular pilgrimage destinations in each of the texts I analyze. In my Prologue of Place, I explore how the allegorization of place instantiated at Walsingham, Norfolk 2 Ì† 7 s 2 Ì† 2 Newe Nazareth, 2 Ì† 2 engages both the operations of late medieval allegory and late medieval conceptions of place to illuminate a connection between the literal and the local. Each subsequent chapter explores how the historical, cultural, and spatial phenomenon of pilgrimage informs the textual strategies of pilgrimage in The Vision of Piers Plowman, The Siege of Thebes, and The Book of Margery Kempe respectively. In my Epilogue of Place, I return to study the case of Walsingham to consider pilgrimage and periodization. I examine how pilgrimage 2 Ì† 7 s prohibition ultimately affected English place and transformed the literary technique of pilgrimage. ...|$|R
40|$|The wide {{burden of}} {{anaerobic}} bacteria colonizing human body comprises about 90 % {{of its total}} biomass. The biotic relationshipÂ between humans and its microbiota sets reciprocal benefits, albeit with pathogenic potencial for the human being in particular dysbiosisÂ situations. Infections adjacent to or originating from the skin or mucous membranes of the intestinal, genitourinary and upper respiratoryÂ tracts are often polymicrobial in nature, whereby should anaerobes be invariably included in the etiological differential diagnosis ofÂ these conditions. Gram negative bacilli such as Bacteroides fragilis group, Fusobacterium spp., Porphyromonas spp., Prevotella spp. Â and Gram positive cocci such as Peptostreptococcus spp. stand out for their high virulence and frequence of isolation in suppurativeÂ infections and abcesses with metastatic or contiguous relation to human microbiota. The fastidious nature of anaerobic bacteria,Â especially of less aerotolerant species, <b>compels</b> to particular <b>techniques</b> of sample collection, transport and cultural isolation thatÂ challenge clinicians and microbiologists for a full efficient practice. Such requirements bring on a poor identification of anaerobic bacteriaÂ in the clinical practice and undervaluation of its aetiopathogenic potential amongst common polymicrobial infections. An approach overÂ microbial floraâ€™s composition in the different human anatomical sites is a primary goal of the present article. Clinicians are intended toÂ recognize the variability and proportion of likely involved anaerobic microorganisms in certain infectious processes related to humanÂ microbiota, in order to optimize samples processing {{and the establishment of}} an appropriate empirical antibiotic therapy, mindful ofÂ anaerobic coverage and according to known susceptibility profiles...|$|R
40|$|Studies on {{heart rate}} {{variability}} (HRV) have become popular {{and the possibility of}} diagnosis based on non-invasive <b>techniques</b> <b>compels</b> us to overcome the difficulties originated on the environmental changes that canaffect the signal. We perform a nonparametric segmentation which consists of locating the points where the sig-nal can be split into stationary segments. By finding stationary segments we are able to analyze the size of thesesegments and evaluate how the signal changes from one segment to another, looking at the statistical momentsgiven in each patch, for example, mean and variance. We analyze HRV data for 15 patients with congestive heartfailure (CHF; 11 males, 4 females, age 56 &# 177; 11 years), 18 elderly healthy subjects (EH; 11 males, 7 females, age 50 &# 177; 7 years), and 15 young healthy subjects (YH; 11 females, 4 males, age 31 &# 177; 6 years). Our results confirmhigher variance for YH, and EH, while CHF displays diminished variance with p-values < 0. 01, when comparedto the healthy groups, presenting higher HRV in healthy subjects. Moreover, it is possible to distinguish betweenYH and EH with p < 0. 05 through the segmentation outcomes. We found high correlations between the resultsof segmentation and standard measures of HRV analysis and a connection to results of detrended fluctuationanalysis. The segmentation applied to HRV studies detects aging and pathological conditions effects on thenonstationary behavior of the analyzed groups, promising to contribute in complexity analysis and providingrisk stratification measures...|$|R
40|$|ObjectiveHybrid {{thoracic}} endovascular aortic repair (TEVAR) {{has expanded}} the surgical management of complex thoracic aneurysms. Aortic arch debranching generally requires a sternotomy. We describe our experience performing a right anterior minithoracotomy for hybrid TEVAR. MethodDuring a 3 -year period, 7 patients (aged 76 Â± 15 years; 57 % were male) with aortic arch aneurysms underwent hybrid TEVAR via a right anterior minithoracotomy. Of all with prior thoracic or abdominal aortic surgery, 4 had a prior sternotomy. All patients {{included in this}} series had an American Society of Anesthesiology score of 4 or greater. ResultsRepairs were performed via a 5 -cm incision at the third to fourth intercostal space to access the ascending arch. A Satinsky clamp on the ascending aorta facilitated bypass with the 10 -mm arm of a bifurcated 10 / 12 -mm graft to the innominate artery or right common carotid artery (12 -mm arm: endoprosthesis conduit). The remaining arch vessels were bypassed as needed; subsequently, a thoracic stent graft was deployed by the 12 - or 14 -mm arm. Primary technical success was 86 % (6 patients); 1 patient required conversion to sternotomy secondary to bleeding. Complications included cerebrovascular accident in 2 patients (28 %) and respiratory failure in 2 patients (28 %). The average length of stay was 12 days with no wound infection. One death occurred during the 30 -day period. ConclusionsRight anterior minithoracotomy is a <b>compelling,</b> less invasive <b>technique</b> for hybrid TEVAR. Further experience {{will be necessary to}} completely evaluate the merits of this approach...|$|R
40|$|Cellular phones offer a {{whole range}} of {{interesting}} and exciting possibilities for entertainment systems coupled with a very resource-constrained environment. In this article we consider the possibilities currently achievable through the example of a networked sports service. Applications that keep users up-to-date with sports results and playing fantasy team games, {{based on the results of}} actual events, are well established on the Internet; they attract millions of subscribers world-wide. As yet, the sports results services on cellular phones, even within countries that offer 3 G services, are by and large SMS or WAP-based, and there are no dedicated fantasy team game services for cellular phones. In this article we present a novel application using GPRS that not only keeps users up-to-date, wherever they are, with the events of the English Premier Football League, but also provides the opportunity of playing a real-time fantasy football game as these events transpire. As we are seeing moves by cellular phone manufacturers to adopt standardized operating systems, we compare application development in Symbian, Brew, and J 2 ME. Although J 2 ME is not an operating system, it has, up until recently, been the only means of cross-platform application development for cellular phonesThis application not only takes advantage of the "nature" of the cellular network, rather than porting existing services off the Internet, but radically improves currently available services in terms of cost and efficiency. It also highlights the fact that resource constraints on a cellular phone are not a bar to creating <b>compelling</b> content. This <b>technique</b> could be applied to {{a whole range}} of sports from Formula 1 to baseball...|$|R
