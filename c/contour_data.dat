149|212|Public
50|$|The {{availability}} of compact fast computers and high-capacity storage, {{combined with the}} {{availability of}} global digital elevation maps, has mitigated this problem, as TERCOM data is no longer limited to small patches, {{and the availability of}} side-looking radar allows much larger areas of landscape <b>contour</b> <b>data</b> to be acquired for comparison with the stored <b>contour</b> <b>data.</b>|$|E
50|$|The FCC calculates FM and TV {{contours}} {{based on}} {{effective radiated power}} (ERP) in a given direction, the radial height above average terrain (HAAT) in a given direction, the FCC's propagation curveshttp://www.fcc.gov/mb/audio/bickel/curves.html, and the station's class. AM contours {{are based on the}} standard ground wave field strength pattern, the frequency, and the ground conductivity in the area. While the FCC makes FM and TV service <b>contour</b> <b>data</b> readily available, the AM, while unavailable as a separate data file, can be obtained through an AM Query in the resulting 'maps' section of each record (when using the 'detailed output' output option).|$|E
50|$|In practice, {{surveyors}} first sample {{heights in}} an area, then use these {{to produce a}} Digital Land Surface Model {{in the form of}} a TIN. The DLSM can then be used to visualize terrain, drape remote sensing images, quantify ecological properties of a surface or extract land surface objects. Note that the <b>contour</b> <b>data</b> or any other sampled elevation datasets are not a DLSM. A DLSM implies that elevation is available continuously at each location in the study area, i.e. that the map represents a complete surface. Digital Land Surface Models should not be confused with Digital Surface Models, which can be surfaces of the canopy, buildings and similar objects. For example, in the case of surface models produces using the lidar technology, one can have several surfaces - starting from the top of the canopy to the actual solid earth. The difference between the two surface models can then be used to derive volumetric measures (height of trees etc.).|$|E
30|$|Here {{we present}} an {{approach}} to apply the recurrence plot method on circular <b>contour</b> line <b>data</b> by using a modified embedding, where the <b>contour</b> line <b>data</b> are augmented by recycled elements. The resulting RP is the basis to get a first glimpse about usefulness of RQA scalars as features for automated classification systems. For comparison we used two different image sets. The first set is composed of geometric forms, while the second is compiled from images of plankton specimens and marine snow taken under arbitrary angles and showing high morphological variability.|$|R
40|$|We {{consider}} robust {{methods of}} likelihood and frequentist inference for the nonlinear parameter, say "&agr;", in conditionally linear nonlinear regression models. We derive closed-form expressions for robust conditional, marginal, profile and modified profile likelihood functions for "&agr;" under elliptically <b>contoured</b> <b>data</b> distributions. Next, we develop robust exact-F confidence intervals for "&agr;" and consider robust Fieller intervals for ratios of regression parameters in linear models. Several well-known examples are considered and Monte Carlo simulation results are presented. Copyright (c) 2007 Board of the Foundation of the Scandinavian Journal of Statistics [...] ...|$|R
30|$|The first {{principle}} task of {{this study}} was to apply the well-established methods of Recurrence Plots (RP) and Recurrence Quantification Analysis (RQA) in the new context of circular <b>contour</b> line <b>data</b> of an imaged object’s outer hull. To set up the circular <b>contour</b> line <b>data</b> for the proposed methods, each point of the contour line was enumerated and its distance to an arbitrary point was calculated. This arbitrary reference point was static. In contrast to traditional RP and RQA investigations, we augmented the <b>contour</b> line distance <b>data</b> during the embedding process. Thus, the distance data are recycled to allow creating a number of embedding vectors equal to the number of contour lines points. By this, opposite sides of the RP wrap up. This allowed the introduction of the eye structure quantification (ESQ).|$|R
50|$|To {{construct}} a contour boxplot, data ordering {{is the first}} step. In functional data analysis, each observation is a real function, therefore data ordering {{is different from the}} classical boxplot where scalar data are simply ordered from the smallest sample value to the largest. More generally, data depth, gives a center-outward ordering of data points, and thereby provides a mechanism for constructing rank statistics of various kinds of multidimensional data. For instance, functional data examples can be ordered using the method of band depth or a modified band depth. In <b>contour</b> <b>data</b> analysis, each observation is a feature-set (a subset of the domain), and therefore not a function. Thus, the notion of band depth and modified band depth is further extended to accommodate features that can be expressed as sets but not necessarily as functions. Contour band depth allows for ordering feature-set data from the center outwards and, thus, introduces a measure to define functional quantiles and the centrality or outlyingness of an observation. Having the ranks of feature-set data, the contour boxplot is a natural extension of the classical boxplot which in special cases reduces back to the traditional functional boxplot.|$|E
40|$|Project of {{building}} country wide DTM for Serbia and Montenegro {{is coming to}} its final phase. The project is based on scanning and vectorization of contours from existing 1 : 25000 topographic maps. Supplementing <b>contour</b> <b>data</b> with additional data sets available on map layers, such as hydrography and spot heights, is also planned within the project. Procedures and software tools for data acquisition, verification and processing were developed and implemented. Special attention was dedicated {{to the problem of}} high quality terrain surface modeling using <b>contour</b> <b>data</b> and all the other available information. The main concern was reconstruction and usage of all implicit information contained in <b>contour</b> <b>data.</b> Procedures based on TIN data structure were designed and implemented. Also, procedures and initial tests for the evaluation of the DTM quality are designed and started within the project. The paper reviews applied procedures, algorithms, current project status and some results. 1...|$|E
40|$|The main {{objective}} {{of this paper is}} to discuss on the effectiveness of visualising terrain draped with Unmanned Aerial Vehicle (UAV) images generated from different contour intervals using Unity 3 D game engine in online environment. The study area that was tested in this project was oil palm plantation at Sintok, Kedah. The <b>contour</b> <b>data</b> used for this study are divided into three different intervals which are 1 m, 3 m and 5 m. ArcGIS software were used to clip the <b>contour</b> <b>data</b> and also UAV images data to be similar size for the overlaying process. The Unity 3 D game engine was used as the main platform for developing the system due to its capabilities which can be launch in different platform. The clipped <b>contour</b> <b>data</b> and UAV images data were process and exported into the web format using Unity 3 D. Then process continue by publishing it into the web server for comparing the effectiveness of different 3 D terrain data (<b>contour</b> <b>data)</b> draped with UAV images. The effectiveness is compared based on the data size, loading time (office and out-of-office hours), response time, visualisation quality, and frame per second (fps). The results were suggest which contour interval is better for developing an effective online 3 D terrain visualisation draped with UAV images using Unity 3 D game engine. It therefore benefits decision maker and planner related to this field decide on which contour is applicable for their task...|$|E
2500|$|Robinson and Dadson refined {{the process}} in 1956 to obtain {{a new set of}} equal-loudness curves for a frontal sound source {{measured}} in an anechoic chamber. The Robinson-Dadson curves were standardized as ISO 226 in 1986. In 2003, [...] was revised as equal-loudness <b>contour</b> using <b>data</b> collected from 12 international studies.|$|R
40|$|A {{real-time}} <b>contour</b> detector and <b>data</b> {{acquisition system}} is described for an angiographic apparatus having a video scanner for converting an X-ray {{image of a}} structure characterized by a change in brightness level compared with its surrounding into video format and displaying the X-ray image in recurring video fields. The real-time <b>contour</b> detector and <b>data</b> acqusition system includes track and hold circuits; a reference level analog computer circuit; an analog compartor; a digital processor; a field memory; and a computer interface...|$|R
3000|$|... are {{weighting}} parameters. Regularization {{forces are}} most likely tangent to the principal axis of the <b>contour</b> whereas <b>data</b> fidelity forces are vertical to this axis, attracting the contour towards target edges. It is tempting to notice that if we associate the principal axis of the tensor ellipsoid with the principal axis of the contour, the regularization weight w [...]...|$|R
40|$|A {{method for}} {{manufacturing}} smart pressure suits, comprises steps of: measuring physical properties of lycra materials and select a lycra material suitable {{for making the}} smart pressure monitored suits; measuring body <b>contour</b> <b>data</b> of a patient and computing a pressure value applied to a disease's part of the patient; inputting the body <b>contour</b> <b>data</b> into a computer and creating a pattern of the smart pressure monitored suits by the body <b>contour</b> <b>data</b> and the pressure value processed by a drawing process module in the computer; plotting a blueprint of the smart pressure monitored suits with a plotter controlled by the computer; and sewing the smart pressure monitored suits according to the selected lycra material and the blueprint. Said method makes whole manufacturing process shorten and the manufactured smart pressure monitored suits has advantages of aesthetic, comfort, good air permeability, less deformation after it is wore {{in a period of}} time. Department of Rehabilitation SciencesUS 8386065; US 8386065 B 2; US 8386065 B 2; US 8, 386, 065; US 8, 386, 065 B 2; 8386065; Appl. No. 12 / 341, 499 U...|$|E
40|$|As the {{representation}} of terrain surface and height information, the <b>contour</b> <b>data</b> has the strict constraint relationship with the distribution of river network. In spatial data integration and matching, the inconsistency usually occurs between the river network and contour generating “river climbing uphill”. This study presents a method to build the matching relationship and to correct the inconsistency between river network and <b>contour</b> <b>data.</b> Based on Delaunay triangulation, the terrain landform features are extracted and by bend analysis build the matching relations between river network and contour. According to different inconsistency situations, we offer two correction approaches depending on which data is precise, which includes the river network displacement referenced to the contour and the opposite...|$|E
40|$|We present two new {{pre-processing}} {{techniques that}} improve thin plate Digital Elevation Model (DEM) approximations from grid-based <b>contour</b> <b>data.</b> One method computes gradients from an initial interpolated or approximated surface. The aspects {{are used to}} create gradient lines that are interpolated using Catmull-Rom splines. The computed elevations {{are added to the}} initial <b>contour</b> <b>data</b> set. Thin plate methods are applied to all of the data. The splines allow information to flow across contours, improving the final surface. The second method successively computes new, intermediate contours in between existing isolines, which provide additional data for subsequent thin plate processing. Both methods alleviate artifacts visible in previous thin plate methods. The surfaces are tested with published methods to show qualitative and quantitative improvements over previous methods. 2...|$|E
40|$|The {{presented}} {{paper is}} motivated by seeking an efficient encoding scheme for arbitrarily shaped objects using linear, quadratic and cubic spline wavelets. A new <b>contour</b> image <b>data</b> compression method is described and its efficiency evaluated from the achieved bit-per-contour-pixel ratio point of view. K e y w o r d s: wavelet transform, spline wavelets, contour coding...|$|R
30|$|The DIPS {{software}} program {{was used to}} generate the stereonets from the scanline survey data collected. For each region a <b>contoured</b> pole <b>data</b> stereonet with joint set windows was constructed. The stereonets then were analysed according to procedures given in (Hoek and Bray 1981) {{in order to assess}} potential modes of failure, thereby allowing potential failure zones to be identified.|$|R
40|$|A {{new method}} for {{automatic}} delineation of drainage basins from <b>contour</b> elevation <b>data</b> is presented. As a preprocessing step, contour line vertices {{are used to}} construct Delaunay and Voronoi diagrams along with other useful structures known in computational geometry as the crust and the skeleton or medial axis transform. Using the skeleton of contour lines, a recursive algorithm is then developed to solve critical topographic structures such as ridges, saddles, and peaks in a fully-automated and accurate manner. Numerical experiments based on high- accuracy <b>contour</b> elevation <b>data</b> of real terrains (generated from LiDAR surveys) show that the proposed method is able to process automatically any topographic structure and to produce results that are comparable to those that can be interpreted visually from contour lines. The gain in accuracy over state-of-the-art solutions is generally {{found to be significant}} and to increase as the contour interval increases. Finally, it is shown how the proposed method can be easily applied to construct accurate flow nets in a fully-automated manner. Skeleton construction techniques allow the morphological information implicitly present in <b>contour</b> elevation <b>data</b> to be explicitly revealed and appropriately processed by a computer program, and therefore appear useful means for improving the accuracy with which physiographic features of drainage basins are determined. The proposed method can be used to advance the construction of flow nets and contour-based digital elevation models (as outlined in this study) and to test the reliability of algorithms for the analysis of more efficient and straightforward, gridded or triangulated, elevation data (as shown in a companion study) ...|$|R
40|$|Groundwater {{modeling}} of a 156, 000 hectare basin in IOWA {{was made possible}} by intersection of hypsography (contours) and hydrography (river) data layers to determine elevations of the rivers. The elevation of the river is needed as input prescribed heads for the groundwater model. In order to intersect the river and <b>contour</b> <b>data</b> the river and <b>contour</b> <b>data</b> were exported into a GEN file using ArcMAP and a plug-in called ET GeoWizards. The exported files resemble the ArcInfo Generate (GEN) format. A new procedure was devised to intersect the river and contour polylines. The non-trivial logic had been worked out by others at www. faqs. org. The data used for the study is hosted on the IOWA Department of Natural Resources (DNR) Web site (ftp. igsb. uiowa. edu) ...|$|E
40|$|We {{describe}} several algorithms for reconstructing a variational {{implicit surface}} (VIS) from <b>contour</b> <b>data.</b> These algorithms are applied on {{two kinds of}} data, polygons and grey-level images. We show that VIS are a natural choice for reconstruction purposes since they provide an automatic interpolation of sample points, and produce smooth surfaces...|$|E
40|$|This paper {{explores the}} {{sensitivity}} of analytical results using contours as the topographic data. In visualizing the topography of a landscape using contours, the two parameters under investigation are {{the size of the}} contour interval and the base contour chosen. Different contour intervals will generate different descriptions of the landscape. This is labeled as the interval effect. Choosing different base contours to compile the contour database can produce different landscape characteristics. This is called the placement effect. Using viewshed analysis in GIS as an application example, this paper assesses the interval effect and placement effect systematically based upon two study areas in northern Virginia, US. <b>Contour</b> <b>data</b> of 5 -foot interval were resampled to derive data of different intervals and of different base contours. In general, larger contour intervals overestimate visible areas but visible locations are not consistently visible with increasing intervals. Results from using different base contours do not exhibit identifiable patterns, but the placement effect can truncate the edge of the landscape by excluding low lying areas. Finally, a probability viewshed approach is suggested to handle the two effects in analyzing <b>contour</b> <b>data.</b> This paper demonstrates that expert and novice users alike using <b>contour</b> <b>data</b> for topographical analysis have to realize that the result {{is only one of many}} possible outcomes and this issue is independent of GIS software adopted. link_to_subscribed_fulltex...|$|E
5000|$|Despite {{the fact}} that the conjunct-final {{elements}} in these examples do not contrast, the sentences can be acceptable given an appropriate intonation <b>contour.</b> Some similar <b>data</b> from German reinforce the point: ...|$|R
30|$|It {{could be}} shown, that the {{principle}} of recurrence plots and subsequent analyses {{can be applied to}} <b>contour</b> line <b>data</b> of imaged and pre-segmented objects. The tailored embedding algorithm enabled our application to derive new image features for automated classification systems of plankton organisms. Additionally, a new set of features was derived by measurement of contiguous elements of given phase space dissimilarity (eye-structures in the recurrence plots).|$|R
40|$|AbstractFor multivariate data, the halfspace depth {{function}} {{can be seen}} as {{a natural}} and affine equivariant generalization of the univariate empirical cdf. For any multivariate data set, we show that the resulting halfspace depth function completely determines the empirical distribution. We do this by actually reconstructing the data points from the depth <b>contours.</b> The <b>data</b> need not be in general position. Moreover, we prove the same property for regression depth...|$|R
40|$|An {{image space}} {{algorithm}} for morphological interpolation between contours is presented. Image space interpolation avoids {{the need to}} represent or store <b>contour</b> <b>data</b> using intermediate data structures. The algorithm makes use of basic morphological transforms such as dilation and erosion and interimage operations such as XOR and union. Morphological interpolation is applied successfully {{to a variety of}} synthetic contours as well as naturally occurring contours such as those found in medical images or topographic maps [17]. The algorithm interpolates between nested, overlapping, nonoverlapping, or branching contours in a general way although nonoverlapping or minimally overlapping contours require initial registration. The algorithm is particularly appropriate for generation of digital elevation maps or whenever the original <b>contour</b> <b>data</b> is derived from a regular sampling grid. Image space morphological interpolation exploits pipeline architectures allowing simultaneous generation of int [...] ...|$|E
40|$|A new {{programming}} {{method has}} been developed for grinding robots. Instead of using the conventional jog-and-teach method, the workpiece contour is automatically tracked by the robot. During the tracking, the robot position is stored in the robot control system every 8 th millisecond. After filtering and reducing this <b>contour</b> <b>data,</b> a robot program is automatically generated...|$|E
40|$|We propose an {{approach}} to the reconstruction of point samples into smooth models based upon the generation of isocurves, with concentration on <b>contour</b> <b>data.</b> We show how to build an isocurve through any point and how to choose which isocurves are computed. The isocurves are used to build a tensor product surface for each component of the model isomorphic to a cylinder...|$|E
40|$|New {{methods for}} {{automatic}} delineation of drainage basins from <b>contour</b> elevation <b>data</b> are presented. As a fundamental preprocessing step, the points defining {{a set of}} contour lines are used to compute the Delaunay triangulation, the Voronoi diagram, and other structures known in computational geometry as the crust and the skeleton (or medial axis transform). By exploiting the skeleton extracted from contour lines, a recursive algorithm is then developed to solve critical topographic structures such as ridges, saddles, and peaks in a fully automated and accurate manner. Finally, the algorithm is further extended {{to deal with the}} construction of flow nets. Numerical experiments based on high-accuracy <b>contour</b> elevation <b>data</b> of real terrains show that the proposed methods are able to process automatically complex topographic structures and to produce results comparable to those that can be interpreted visually from contour lines. The gain in accuracy over current state-of-the-art solutions is generally found to be significant and to increase as the contour interval increases. Copyright 2008 by the American Geophysical Union...|$|R
25|$|As LIDAR-surveying advances, {{base maps}} {{consisting}} of 1 meter <b>contours</b> and other <b>data</b> {{derived from the}} LIDAR data get more common. As these base maps contain large amounts of information the cartographic generalization becomes important in creating a readable map.|$|R
40|$|For multivariate data, the halfspace depth {{function}} {{can be seen}} as {{a natural}} and affine equivariant generalization of the univariate empirical cdf. For any multivariate data set, we show that the resulting halfspace depth function completely determines the empirical distribution. We do this by actually reconstructing the data points from the depth <b>contours.</b> The <b>data</b> need not be in general position. Moreover, we prove the same property for regression depth. location depth, multivariate ranking, reconstruction algorithm, regression depth...|$|R
40|$|Optical {{contouring}} techniques which incorporate holography {{have the}} potential for providing high resolution noncontacting topographic measurements of surface relief. In addition, holographic recording provides a means by which <b>contour</b> <b>data</b> may be archived indefinitely. Contouring may be applied to specular and optically rough surfaces and, through the use of pulsed holographic recording, may be applied where contour changes occur at very high speed...|$|E
40|$|This {{article was}} study about {{decision}} process data meaning with algoritma ID 3. Data meaning is an atomatic extraction process of large data. It {{was found with}} a <b>contour</b> <b>data.</b> Data meaning have a function to produce a different contour wich other. The function of classification of data meaning is helping write decision tree, the function with algoritma ID 3. Key words: data meaning, classification, decision tre...|$|E
40|$|Nozzle <b>contour</b> <b>data</b> for untruncated Bell nozzles with {{expansion}} area ratios to 6100 and a specific heat ratio of 1. 2 are provided. Curves for optimization of nozzles for maximum thrust coefficient {{within a given}} length, surface area, or area ratio are included. The nozzles are two dimensional axisymmetric and calculations were performed using the method of characteristics. Drag due to wall friction {{was included in the}} final thrust coefficient...|$|E
40|$|We study a {{degenerate}} nonlinear parabolic equation with moving boundaries which {{arises in}} the study of the technique of contour enhancement in image processing. In order to obtain mass concentration at the <b>contour,</b> singular <b>data</b> are imposed at the free boundary, leading to a nonstandard free boundary problem. Our main results are: (i) the well-posedness for the singular problem, without monotonicity assumptions on the initial datum, and (ii) the convergence of the approximation by means of combustion-type free-boundary problems...|$|R
50|$|Older {{methods of}} {{generating}} DEMs often involve interpolating digital contour maps {{that may have}} been produced by direct survey of the land surface. This method is still used in mountain areas, where interferometry is not always satisfactory. Note that <b>contour</b> line <b>data</b> or any other sampled elevation datasets (by GPS or ground survey) are not DEMs, but may be considered digital terrain models. A DEM implies that elevation is available continuously at each location in the study area.|$|R
40|$|Half-space depth (also called Tukey depth or {{location}} depth) {{is one of}} {{the most}} commonly studied data depth measures because it possesses many desirable properties for data depth functions. The <b>data</b> depth <b>contours</b> bound regions of increasing depth. For the sample case, there are two competing definitions of contours: the rank-based contours and the cover-based contours. In this paper, we present three dynamic algorithms for maintaining the half-space depth of points and contours: The first maintains the half-space depth of a single point in a data set in O(n) time per update (insertion/deletion) and overall linear space. By maintaining such a data structure for each data point, we present an algorithm for dynamically maintaining the rank-based contours in O(n· n) time per update and overall quadratic space. The third dynamic algorithm maintains the cover-based contours in O(n·^ 2 n) time per update and overall quadratic space. We also augment our first algorithm to maintain the local cover-based <b>contours</b> at <b>data</b> points while maintaining the same complexities. A corollary of this discussion is a strong structural result of independent interest describing the behavior of dynamic cover-based <b>contours</b> near <b>data</b> points. Comment: 31 page...|$|R
