18|10000|Public
40|$|In {{this report}} {{we discuss the}} {{treatment}} of statistical errors in cut efficiencies. The two commonly used methods for the calculation of the errors, Poissonian and Binomial, are shown to be defective. We derive {{the form of the}} underlying probability density function and characterize its mean, mode, and variance. A method for the <b>calculation</b> <b>of</b> <b>errors</b> based on the variance of the distribution is discussed. Comment: This paper has been withdrawn by the author...|$|E
40|$|This Bachelor´s thesis {{deals with}} {{accuracy}} testing of chosen building laser tools. At first, features {{of these devices}} are described. Then the measurement procedure is explained. By this measurement the values for the <b>calculation</b> <b>of</b> <b>errors</b> are obtained. These errors are compared with the maximum deviations prescribed in the manual by the manufacturer. In conclusion, the practical applicability of selected devices is described in relation to measurement experiences. All measured values, calculations and charts are located in attachments of this thesis...|$|E
40|$|This paper {{describes}} a simple method for the <b>calculation</b> <b>of</b> <b>errors</b> {{due to the}} finite length and separation of X-array hot-wire anemometers. Results of computations for a typical wire array are presented for total and spectral turbulence measurements. Although the actual errors are slightly underestimated due to the simplifying assumptions used to reduce the computational effort, {{it is found that}} the wire length and separation effect is very significant. The analysis does not require complete correlation or spectral functions of the flow and includes the effect of nonuniform wire sensitivity. Copyrigh...|$|E
40|$|The chief {{advantage}} of the instrument lies {{in the degree of}} accuracy obtainable with suitably flexible capsule (dynamic pressure recorder with small test range) and in its sensitivity for recording static-pressure changes. A description and hook-up of the instrument is provided along with <b>calculations</b> <b>of</b> <b>error...</b>|$|R
40|$|Abstract: We {{present a}} {{normalization}} that facilitates <b>calculation</b> <b>of</b> <b>error</b> vector magnitude (EVM) from measurements. We derive {{the definition of}} EVM for a common industry standard from a more basic equation. We compare EVM for various modulation types for a given average symbol power under simple distortion conditions...|$|R
40|$|This {{zip file}} {{includes}} {{most of the}} analysis in Sims and Zha(1999), "Error Bands for Impulse Responses", Econometrica, vol 67, no. 5, pp 1113 - 1156. This includes <b>calculation</b> <b>of</b> <b>error</b> bands using Monte Carlo integration, bootstrapping and also includes IRF's for a structural VAR. VAR, bootrapping, Monte Carlo integration...|$|R
40|$|This paper {{proposes a}} set of higher-order {{modified}} moments for estimation of the fundamental frequency of speech and explores {{the impact of the}} speech window length on pitch estimation error. The pitch extraction methods are evaluated in a range of noise types and SNRs. For <b>calculation</b> <b>of</b> <b>errors,</b> pitch reference values are calculated from manually-corrected estimates of the periods obtained from laryngograph signals. The results obtained for the 3 rd and 4 th order modified moment compare well with methods based on correlation and magnitude difference criteria and the YIN method; with improved pitch accuracy and less occurrence of large error...|$|E
40|$|The {{scheme is}} {{presented}} for <b>calculation</b> <b>of</b> <b>errors</b> of dry matter values which occur during approximation of data with growth curves, {{determined by the}} analytical method (logistic function) and by the numerical method (Richards function). Further formulae are shown, which describe absolute errors of growth characteristics: Growth rate (GR), Relative growth rate (RGR), Unit leaf rate (ULR) and Leaf area ratio (LAR). Calculation examples concerning the growth course of oats and maize plants are given. The critical analysis of the estimation of obtained results has been done. The purposefulness of joint application of statistical methods and error calculus in plant growth analysis has been ascertained...|$|E
40|$|Techniques for remote {{measurement}} of sea-surface physical temperature and salinity using radiometric measurements from aircraft or satellite are reviewed. Studies {{have been conducted}} to determine {{the sensitivity of the}} errors in surface temperature and salinity to errors in the measured brightness temperatures using combinations of UHF, L, S, and C-band measurements. These investigations were made using values of conductivity, static dielectric constant, and relaxation time derived from the regression equations of Klein and Swift (1977). Results of the error sensitivity study are presented in the form of error contour plots which permit the <b>calculation</b> <b>of</b> <b>errors</b> in the estimation of the physical parameters for given errors in the raw radiometric measurements...|$|E
40|$|Abstract: The paper {{presents}} the SADCOM computer program for profile measurement accuracy evaluation. The program analyses by comparison the measurements performed {{by means of}} various measuring devices. The software allows, among others, statistical and harmonic analyses <b>of</b> profiles, <b>calculation</b> <b>of</b> correlation coefficients of the compared profiles, <b>calculation</b> <b>of</b> <b>error</b> <b>of</b> method, graphical representation of roundness profiles in a traditional form and as an amplitude and phase spectrum. The program {{can be used for}} the roundness profiles comparison performed on CMM and traditional reference and non – reference devices. Key words: CMM, accuracy evaluation, measurement...|$|R
3000|$|Expected Poisson noise calculation, {{fitting to}} {{minimize}} the noise <b>of</b> counting, and <b>calculation</b> <b>of</b> fitting <b>error</b> [...]...|$|R
40|$|This paper {{explores the}} use of large {{ensembles}} of model runs with randomly perturbed initial conditions for the <b>calculation</b> <b>of</b> <b>error</b> covariance fields, initial condition sensitivity fields, and perturbation impact fields. The <b>calculation</b> <b>of</b> <b>error</b> covariances from ensembles is familiar from ensemble Kalman filter (EnKF) techniques, but the <b>calculation</b> <b>of</b> sensitivity and impact fields from ensembles is new. This work is unlike previous EnKF work in that the ensemble members are randomly perturbed in each degree of freedom (DOF) of the model, rather than having perturbation fields based on expected errors in the analysis. In this work, all the DOFs of the model (or a subset) are independently and simultaneously perturbed and {{the response of the}} model to each perturbed DOF is sought by statistical technique. Sensitivity results are conceptually comparable to adjoint calculations. Error covariance, impact, and sensitivity fields constitute the three distinct kinds of fields that can be found from an ensemble. These fields can be found equivalently to first order as either covariances or partial derivatives from regression analysis. This paper makes use of ensembles of 2000 members of a mesoscale model, run for 6 hours over a domain in th...|$|R
40|$|These devices with {{imaginary}} prism eliminate {{friction between}} the part generators and prism flanks, to rotate for control the work piece. This is especially beneficial to control large parts and / or high mass. Moreover, the new concept, we have proposed eliminating sources of error caused by the elements that shall enter into relations of <b>calculation</b> <b>of</b> <b>errors</b> characteristic of this means of control. This becomes possible by using an adjustable system {{to which it is}} associated the safe assembly principle, AS (principle patented) and by final processing method of the active surfaces of the device, in assembled. By this method it fully satisfy requirements imposed by a robust design, in that the device analyzed retains its initial parameters precision, even after some adjustments, fitting and removing repeated, giving thus evidence of an exceptional reliability, translated by robustness and insensitivity to the external "noise factors"...|$|E
40|$|This paper {{proposes a}} set of higher-order {{modified}} moments as alternative objective criteria for pitch extraction and explores {{the impact of the}} speech window length on pitch estimation error. To obtain the Kth order modified moment, each speech frame is split into a positive-valued signal and a negative-valued signal. The magnitudes of the Kth order moments for the positive and the negative valued signals are obtained and combined. The proposed objective criteria form a relatively sharp peak around the true pitch value compared to the correlation function. For <b>calculation</b> <b>of</b> <b>errors,</b> pitch reference (`ground truth') values are calculated from manually-corrected estimates of the periods obtained from laryngograph signals. The results obtained for the third order modified moment are compared with the results for correlation and magnitude difference criteria and the YIN method. The modified moments provide improved pitch accuracy with less occurrence of large errors (e. g. half or double pitch estimation errors) ...|$|E
40|$|A prime goal {{of quantum}} {{tomography}} {{is to provide}} quantitatively rigorous characterisation of quantum systems, be they states, processes or measurements, particularly {{for the purposes of}} trouble-shooting and benchmarking experiments in quantum information science. A range of techniques exist to enable the <b>calculation</b> <b>of</b> <b>errors,</b> such as Monte-Carlo simulations, but their quantitative value is arguably fundamentally flawed without an equally rigorous way of authenticating the quality of a reconstruction to ensure it provides a reasonable representation of the data, given the known noise sources. A key motivation for developing such a tool is to enable experimentalists to rigorously diagnose the presence of technical noise in their tomographic data. In this work, I explore the performance of the chi-squared goodness-of-fit test statistic as a measure of reconstruction quality. I show that its behaviour deviates noticeably from expectations for states lying near the boundaries of physical state space, severely undermining its usefulness as a quantitative tool precisely in the region which is of most interest in quantum information processing tasks. I suggest a simple, heuristic approach to compensate for these effects and present numerical simulations showing that this approach provides substantially improved performance. Comment: 16 pages, 9 figure...|$|E
3000|$|... 12 Since {{individual}} {{observations are}} used in a regression with establishment information, establishment clusters adjust for the sample size in the <b>calculation</b> <b>of</b> standard <b>errors.</b>|$|R
40|$|The {{one-dimensional}} new Mersenne number transform (NMNT) {{was proposed}} for the <b>calculation</b> <b>of</b> <b>error</b> free convolutions and correlations for signal processing purposes. The aim {{of this paper is}} to develop the split-radix decimation-in-time algorithm for fast <b>calculation</b> <b>of</b> the one-dimensional NMNT with a sequence length equal to a power of two. The arithmetic complexity of this algorithm is analysed and the number of multiplications and additions is calculated. An example is given to prove the validity of the algorithm and the exact nature of this transform...|$|R
40|$|In {{the article}} the {{question}} of designing of new training algorithm for the convolution neural networks. The study found the new rule for <b>calculation</b> <b>of</b> neuron?s <b>error.</b> ? ?????? ?????????? ?????? ?????????? ?????? ????????? ???????? ??? ?????????? ????????? ?????. ? ???? ???????????? ?????????? ????? ??????? ??? ?????????? ?????? ???????...|$|R
40|$|In this paper, {{data from}} {{spaceborne}} radar, lidar and infrared radiometers on the “A-Train” of satellites are combined in a variational algorithm to retrieve ice cloud properties. The method allows a seamless retrieval between regions where both radar and lidar {{are sensitive to}} the regions where one detects the cloud. We first implement a cloud phase identification method, including identification of supercooled water layers using the lidar signal and temperature to discriminate ice from liquid. We also include rigorous <b>calculation</b> <b>of</b> <b>errors</b> assigned in the variational scheme. We estimate the impact of the microphysical assumptions on the algorithm when radiances are not assimilated by evaluating the impact of the change in the area-diameter and the density-diameter relationships in the retrieval of cloud properties. We show that changes to these assumptions affect the radar-only and lidar-only retrieval more than the radar-lidar retrieval, although the lidar-only extinction retrieval is only weakly affected. We also show that making use of the molecular lidar signal beyond the cloud as a constraint on optical depth, when ice clouds are sufficiently thin to allow the lidar signal to penetrate them entirely, improves the retrieved extinction. When infrared radiances are available, they provide an extra constraint and allow the extinction-to-backscatter ratio to vary linearly with height instead of being constant, which improves the vertical distribution of retrieved cloud properties. ...|$|E
40|$|We {{present a}} {{derivation}} of a generalized optimally-weighted estimator for the weak lensing magnification signal, including a <b>calculation</b> <b>of</b> <b>errors.</b> With this estimator, {{we present a}} local method for optimally estimating the local effects of magnification from weak gravitational lensing, using a comparison of number counts in an arbitrary region of space to the expected unmagnified number counts. We show that when equivalent lens and source samples are used, this estimator is simply related to the optimally-weighted correlation function estimator used in past work and vice-versa, but this method has the benefits that it can calculate errors with significantly less computational time, that it can handle overlapping lens and source samples, and that it can easily be extended to mass-mapping. We present a proof-of-principle test of this method on data from the CFHTLenS, showing that its calculated magnification signals agree with predictions from model fits to shear data. Finally, we investigate how magnification data {{can be used to}} supplement shear data in determining the best-fit model mass profiles for galaxy dark matter haloes. We find that at redshifts greater than z ~ 0. 6, the inclusion of magnification can often significantly improve the constraints on the components of the mass profile which relate to galaxies' local environments relative to shear alone, and in high-redshift, low- and medium-mass bins, it can have a higher signal-to-noise than the shear signal. Comment: 20 pages, 10 figures, submitted to MNRAS, first revisio...|$|E
40|$|The {{algorithm}} of {{the simulation}} graph of industrial enterprises reactive load at different {{values of the}} correlation functions parameters have been developed. The simulated graph has 172 values: 162 are used for training {{of each of the}} prediction methods, and the remaining 10 for comparison with the predicted values and for <b>calculation</b> <b>of</b> <b>errors.</b> A two-layer network with back-propagation of a mistake, with seven neurons in the hidden layer, one input and one output were used in predicting with neural networks in Matlab package. The activation function of the first layer is Hyperbolic tangent sigmoid transfer function, and the second one is a linear function. The Lewenberg-Markvart function was selected for training of the model. Predicting by using statistical methods was made by extrapolation. In this paper, predicting based on extrapolation is implemented in the Mathcad program. The problem reduces to determining the value of a parameter outside the region in which the value of this parameter is known. This function is implemented in Mathcad by the command "predict". The last of the studied methods widely used today in industrial enterprises is predicting of the mean value. Its essence lies in finding the arithmetic mean value of the reactive load for the previous day and tuning the compensating devices to the power. The conclusions of the feasibility of each of the methods of the reactive loads predicting for different values of the attenuation coefficient of the correlation function are based on the comparison of the errors of predicting method...|$|E
40|$|Computational problem <b>of</b> gross <b>errors</b> {{estimation}} {{is discussed}} {{based on the}} mean shift model, and the gross <b>errors</b> estimation formulas <b>of</b> the observed statistical correlation data snooping method are given. The relationships <b>of</b> gross <b>errors</b> estimation <b>of</b> the data snooping method, the method of simultaneous locating and evaluating multidimensional gross errors (LEGE), quasi-accurate detection <b>of</b> gross <b>errors</b> (QUAD) method and the partial least-squares (PLS) method are discussed. It is proved that &# 9312; in the case <b>of</b> correlated observations, <b>calculation</b> <b>of</b> gross <b>errors</b> estimation <b>of</b> the PLS method and the QUAD method are equivalent. However, these two methods are different with the data snooping method and the LEGE method; &# 9313; {{in the case of}} uncorrelated and unequal weight observations, <b>calculation</b> <b>of</b> gross <b>errors</b> estimation <b>of</b> the QUAD method, the PLS method and the data snooping method are equivalent, but these three methods are different with the LEGE method; &# 9314; in the case of uncorrelated and equal weight observations, <b>calculation</b> <b>of</b> gross <b>errors</b> estimated value <b>of</b> these four methods are equivalent. Finally, the case studies verify the conclusions...|$|R
40|$|Prepared for the ILO by Patrick Belser, Michaelle de Cock, and Farhad Mehran, {{this is a}} {{technical}} document that provides {{a detailed account of}} the methodology used in the ILO’s first minimum estimate of forced labour in the world, prepared in 2005. It also includes a thorough evaluation of the results, with <b>calculation</b> <b>of</b> margins <b>of</b> <b>error</b> and comparison with external sources...|$|R
30|$|In {{estimating}} the source locations of tremor, we adopted a time window of 30  s for assessing RMS amplitude at each station and shifted the window by 15 -s increments from 11 : 45 to 12 : 10 on 27 September. The {{beginning of the}} time window was set for each station to accommodate its travel time from the assumed tremor source (Kumagai et al. 2010). The definition and <b>calculation</b> <b>of</b> <b>error</b> bars {{are the same as}} for the earthquake location explained in the previous section.|$|R
40|$|This study aims at {{describing}} {{the types of}} susceptibility of interlanguage system in SMP Muhammadiyah 4 Surakarta, explaining the source influences in student interlanguage and {{describing the}} frequency of influences in students interlanguage. The type {{of this research is}} descriptive qualitative research. In collecting the data, the writer uses elicitation technique and documentation. There are four steps to collect data, namely: the writer gives instructions to the students to write English composition, the writer reads every composition accurately, the writer writes again all of the errorneous sentences of students composition, the writer writes all the erroneous sentences into a list and used them as data. In this research the writer uses descriptive analysis by Celce Murcia and frame work of Error Analysis by Shridar. the writer analyzes the collected data using following steps: errors Identification is the writer collects the errorneous sentences in students English composition appropriate with English linguistics rules,errors classification is the writer classifies the errors in students‟ composition based on linguistic category, strategy taxonomy outside, and also based on degree of mother tongue and target language system, <b>calculation</b> <b>of</b> <b>errors</b> is each type of errors is calculated to find out the total number of each type of errors and to get clear picture of the frequency of their occurrence, description of influences degree is the writer describes influences degree towards Indonesian system and English system reflected students interlanguage system, conclusion is the last step concluded from the research results to answer the problems statements in this research. The results of the study is influence of target language in SMP Muhammadiyah 4 Surakarta greater (61. 75...|$|E
40|$|In {{this paper}} {{a model for}} the {{measuring}} process of sonic anemometers (ultrasound pulse based) is presented. The differential equations that describe the travel of ultrasound pulses are solved in the general case of non-steady, non-uniform atmospheric flow field. The concepts of instantaneous line-average and travelling pulse-referenced average are established and employed to explain and calculate the differences between the measured turbulent speed (travelling pulse-referenced average) and the line-averaged one. The limit k 1 l= 1 established by Kaimal in 1968, as the maximum value which permits the neglect of the influence of the sonic measuring process on the measurement of turbulent components is reviewed here. Three particular measurement cases are analysed: A non-steady, uniform flow speed field, a steady, non-uniform flow speed field and finally an atmospheric flow speed field. In the first case, for a harmonic time-dependent flow field, Mach number, M (flow speed to sound speed ratio) and time delay between pulses have revealed themselves to be important parameters in the behaviour of sonic anemometers, within the range of operation. The second case demonstrates how the spatial non-uniformity of the flow speed field leads to an influence of the finite transit time of the pulses (M≠ 0) {{even in the absence of}} non-steady behaviour of the wind speed. In the last case, a model of the influence of the sonic anemometer processes on the measurement of wind speed spectral characteristics is presented. The new solution is compared to the line-averaging models existing in the literature. Mach number and time delay significantly distort the measurement in the normal operational range. Classical line averaging solutions are recovered when Mach number and time delay between pulses go to zero in the new proposed model. The results obtained from the mathematical model have been applied to the <b>calculation</b> <b>of</b> <b>errors</b> in different configurations of practical interest, such as an anemometer located on a meteorological mast and the transfer function of a sensor in an atmospheric wind. The expressions obtained can be also applied to determine the quality requirements of the flow in a wind tunnel used for ultrasonic anemometer calibrations...|$|E
40|$|The <b>calculation</b> <b>of</b> <b>errors</b> in a {{least squares}} {{estimate}} of unit-cell dimensions A LEAST squares technique for evaluating unit-cell dimensions from {{measurements of the}} d-spacings of indexed powder lines {{has been described by}} Smith (1956). The relationship between d-spacing and indices is given in the general triclinic ease by the equation Q (hkl) = d ~ = h~a* 2 § lc~b* 2 § ~- 2 klb*c * cos a * § 21 hc*a * cos fi * + 2 hka*b * cos ~* which can be written in the form Q (hkl) = h 2 xl + k 2 x 2 + l~x 3 § klx ~ + lhxs § hkx 6. Smith derived least squares estimates of xl, [...] ., x~, together with the esti-mated standard eviation ~ of Q for the measured lines, from the expression: ~ 2 = ~ (Qobs__Qe~lc) 2 /(M__N), where M is the number of lines measured and N is the number of variables to be estimated (6 for the triclinie). Smith then evaluates the standard eviation, ai, of the least squares estimates of each x i from the relationship ai ~/cii, where c ~ is the appropriate lement on the principal diagonal of the covariance matrix (Smith, 1956, p. 56). It would be more interesting to have estimates of the standard eviations of the calculated parameters of the direct lattice, a, b, c, a,]~, and ~. The errors in x t are interdependent, the relationship between the standard eviations of a function F of the least square estimates of 6 6 xl, [...] ., % being given by @ = ~ 2. ~ k~ 2,= 1 cjk. (OF/Oxj). (0 F/exk) (Deming, j = l 1938, p. 167), where c~. k is the element in the jth row of the kth column of the eovarianee matrix. If the function F is taken to be each of the direct lattice parameters in turn, the standard eviation of the calcu-lated parameters a, b, c, a, fl, and}, can be evaluated from this expression. The partial derivatives of the direct lattice parameters with respect to the estimated variables are set down in table I. The calculation of each of the six al~'s involves 36 triple products, which reduces to 21 since cjk = ckj. A programme for calculating least squares estimates of unit-cell dimensions and their estimated standard eviations wa...|$|E
2500|$|The {{advantage}} of interval arithmetic {{is that after}} each operation there is an interval that reliably includes the true result. The distance between the interval boundaries gives the current <b>calculation</b> <b>of</b> rounding <b>errors</b> directly: ...|$|R
40|$|The {{formulas}} for {{account of}} a spectrum of function of distribution and second power spectrum <b>of</b> an <b>error</b> with the given confidential probability are found. The got formulas can be utillized for conducting <b>of</b> engineerings <b>calculations</b> <b>of</b> random <b>error</b> terms at their quantile estimation. ??????? ??????? ??? ??????? ??????? ??????? ????????????? ? ??????? ??????????????? ??????? ??????????? ? ???????? ????????????? ????????????. ?????????? ??????? ????? ???? ???????????? ??? ?????????? ?????????? ???????? ????????? ???????????? ??? ?? ??????????? ??????...|$|R
40|$|In {{order to}} find the optimum {{locations}} for new seismic stations in Yugoslavia, a number of points were provisionally assumed and the <b>calculation</b> <b>of</b> <b>error</b> estimation <b>of</b> seismic focal parameters was carried out. If the location is adequately chosen even a single new station considerably improves {{the accuracy of the}} determination of epicenter position and origin time of earthquakes. If two or more stations can be set up the error will decrease as much as 35 %. The calculation was based on the Monte Carlo method. |ユーゴスラヴィアはさきにスコピエの地震に見まわれ, 1967 年にも破壊的地震があるなど,その対策が真剣に考えられているが,地震観測の向上も一つの重要課題となつている. それで現存する 8 個所の観測所に加えて,新しく 1 観測所を建設するならば震央及び発震時の決定精度がどのようになるかについてさきに計算を行ない,概略の結果はすでに得られている. 今回はこれに下に記すような改良を加えて計算を行なつた,(モソテ・カルロ法を使用する基本的な方針と計算手続は前と全く同様である. ...|$|R
40|$|In {{this work}} a new machine for the {{examination}} of ion-molecule-reactions was planned and built. The focus was the analysis of reaction constants and correspondingly cross sections at low c. m. -energies. To achieve this goal {{it was necessary to}} connect an ion-guide with a quadrupole mass spectrometer, where the ion-guide was operated in energy-conserving (adiabatic) mode. The four poles of the ion-guide, a quadrupole itself, were constructed with 15 wires each, arranged in a hyperbolic form (Chapter 3. 2). The core piece of the ion-guide is the rf-generator (Chapter 3. 3). It was constructed by an efficient design using two radio tubes. Frequency and amplitude were adjusted according {{to the needs of the}} experiment. Compared to semiconductors, tubes have some major advantages. On one hand they are more powerful and short-circuit-proof and on the other hand they are much cheaper. Furthermore, a rf-generator with radio tubes can be built with much less components than one with semiconductors; this considerably simplifies the tuning process and troubleshooting. The analysis of the effective c. m. -energy was more difficult than originally assumed. Multiple effects can influence the c. m. -energy: besides the acceleration energy of the ions, there is (i) the radial energy by the rf-field and (ii) the Doppler broadening because of the Maxwellian velocity distribution of the target molecules (Chapter 4. 3. 2). The amount of radial energy considerably depends on the point of ionisation (Fig. 4 8) and has to be redetermined with each change in the experimental setup. Since a direct measurement of the radial energy is not possible one has to rely on calculations with SIMION 3 D (Fig. 4 10). The Doppler broadening always occurs when the target molecules have a Maxwellian velocity distribution. Both the Doppler broadening effect, as well as the radial energy influence have been incorporated in the <b>calculation</b> <b>of</b> <b>errors</b> for the c. m. -energy (Chapter 4. 3. 2). The second part of the present study involves the investigation of four ion-molecule reactions. With the help of these reactions (i) the functional efficiency of the machine could be proofed and (ii) initial measurements of k-values at low c. m. -energies (< 1 eV) were made. The first reaction, HBr+ + HBr à H 2 Br+ + Br·, was used to determine an accurate k-value at a defined c. m. -energy, and ultimately to verify the functional efficiency of the assembly. The results of this experiment are summarised in Tab. 6 1. The k-value calculated from this work is in agreement with the theory and k-value given by Zare and co-workers. In addition the error margin could be reduced by approximately 10 % (Chapter 4. 1). The second reaction, HBr+ + CO à HCO+ + Br·, could be also investigated. However, both this and the first reaction had to be discontinued since HBr corroded the turbo molecular pumps, which subsequently resulted in their breakdown. For this reason the less corrosive system with ammonia was selected. The third reaction, NH 3 + + NH 3 à NH 4 + + NH 2, was utilised to measure the first k-values at c. m. -energies from 0. 06 eV to 3. 5 eV. The results are summarised in Abb. 6 1 (comp. Fig. 4 14). The exponential fit of the experimental data correlates well with Langevin (Eq. 4 4) and Locked Dipole (Eq. 4 5) theoretical calculations. At c. m. -energies lower than 0. 5 eV, a greater spreading of the k-values was observed. To overcome this problem in future investigations, the c. m. -energy control should be improved by using the molecular beam technique and better-stabilized power supplies for polebias, acceleration lenses and rf-amplitude. The fourth reaction, NH 3 + + H 2 CO à CH 2 NH+ + H 2 O, was not observed, even though the reaction is exothermic at - 0. 21 eV. According to calculations from Walch[108], this reaction has a non-negligible second reaction barrier of approximately 35 kcal/mol = 1. 513 eV. This implies that the reaction conditions originally planned were not suitable to form CH 2 NH+ in quantities large enough to detect...|$|E
40|$|Introduction: In recent years, {{due to the}} {{reduction}} in surface water, utilization of groundwater has been increased to meet the growing demand of irrigation water. The quality of these water resources is continually changing, due to the geological formations, the amount of utilization, and climatic parameters. In many developing countries, the irrigation water is obtained from poor quality groundwater resources, which in turn, creates unfavorable circumstances for plant growth and reduces the agricultural yield. Providing adequate water resources for agricultural utilization {{is one of the}} most important steps needed to achieve the developmental targets of sustainable agriculture. Thus, this necessitates the assessment and evaluation of the quality of irrigation water. There are many proposed methods to determine the suitability of water for different applications, such as Piper, Wilcox, and Schoeller diagrams. Zoning of quality and suitability of irrigation water could represent the prone and critical areas to groundwater exploitation. Garmsar alluvial fan {{is one of the most}} sensitive areas in the country where traditional agriculture practices had turned into modern techniques and excessive exploitation of groundwater has caused an intensepressure on aquifers and increased water salinity. The aim of this study is to evaluate the suitability of groundwater for irrigation in a 10 -year period (2002 - 2012) and its changes in this basin. Materials and Methods: Garmsar alluvial fan is located in the North-West of Semnan Province. Semnan is situated in the Southern hillside of the Alborz Mountains, in North of Iran. The study area includes the agricultural land on this alluvial fan and covers over 3750 hectares of this basin. In order to evaluate the quality of groundwater in this area, the electrical conductivity and sodium absorption ratio of 42 sample wells were calculated. The raster maps of these indicators were obtained using Geo-statistical techniques. The suitability of irrigation water was determined by Wilcox diagram. Upon evaluating the data distribution and testing the data from Klomogrov-Smirnov normality test, normalization of the data was performed in SPSS software. Spatial correlation and spatial structure of variables were analyzed by drawing their semi-variograms in GS+ software. The most accurate variogram model was selected according to the lowest Residual Sums of Squares (RSS) and the highest correlation coefficient (R 2). Interpolation and zoning of the indicators were performed in ArcGIS software and the Quality classes were determined. Results and Discussion: According to the results of Kolmogorov-Smirnov test, none of the data series had normal distribution. Therefore, they were normalized through calculating the logarithm of variables. Fitting and the selection of variograms were performed in GS+ software and after the <b>calculation</b> <b>of</b> <b>errors,</b> kriging method with Guassian model was determined as the best fitting model. The correlation coefficient was 0. 896 for electrical conductivity and 0. 99 for sodium absorption ratio. Interpolation of indicators in ArcGIS implied fewer measurements of these indicators in north of the study area (Hableh-Rood inlet). The maximum measurement of indicators was observed on the western edge of the alluvial fan. In total, the values of both electrical conductivity and a sodium absorption ratio indicators in the western half of the area, in the vicinity of the third period domes, were more than the eastern half. The result of the water classification using Wilcox diagram represented the unsuitability of groundwater for irrigation in all of the study area. The area with unusable groundwater for irrigation has increased over the 2005 – 2009 period. Conclusion: In this study, relying on the use of GIS and Geo-statistical methods, the quality of Garmsar basin groundwater has been evaluated. The electrical conductivity was applied to monitor water salinity, and Sodium absorption ratio was used to monitor alkalinity. The interpolation of these indicators was performed by Kriging method and Guassian fitting model. Likewise, in other studies, the Kriging method was introduced as an appropriate method for the interpolation of chemical parameters of the groundwater. The accuracy of various fitting models in the prediction of interpolated values differed according to the number and the distribution of sample points. In the current study, the Guassian fitting model was determined as the best model to interpolate both of the indicators. According to the maps, it seems that the third period domes in the western margin of the study area have a great influence on the quality of Garmsar’s surface water and groundwater. In total, the groundwater of Garmsar basin didn’t poss high suitability for irrigation, and was classified into two unsuitable and unusable classes. Moreover, according to the maps, the maximum area of unusable groundwater for irrigation in the area was observed in 2008...|$|E
40|$|Based on the {{definition}} and assessment methods proposed in Chapter 2, {{an analysis of}} accuracy at the phonetic, spelling, lexical, syntactic, semantic, and textual levels is given, together with {{an examination of the}} impact of extra-linguistic factors, such as the time and place of translation, initiator's policy, and translators. The last section defines the texts for the present study. Chapter 1 is {{a review of the literature}} on translation quality assessment in China and in the West. Chapter 2 reviews the study of accuracy in scientific and technical translation, including an introduction to accuracy, research on accuracy, and definition of accuracy, which involves completeness in information transfer, consistency in terminology, grammaticality in the encoding of the target text, and methods of assessing linguistic accuracy. Chapter 3 proposes a new theoretical framework and presents texts for the study of accuracy. Apart from complete transfer of information, consistency in terminology, and grammaticality in the encoding of the target text, accuracy should also be assessed by the function of the target text and the impact of extra-linguistic factors. Chapter 4 is a contrastive analysis of the source and target texts on the basis of the above theoretical framework. This analysis includes features of the source and target texts, methods in translating terms in traditional Chinese medicine, methods in assessing semantic accuracy, types of Chinese medicine formulas and their methods of translation, types of errors in the target texts, and the impact of extra-linguistic factors on translation. Chapter 5 discusses methods of assessing accuracy with the use of computer-aided tools, illustrating them with examples and offering explanations on the terms and statistics. The software used are Wordsmith 3. 0 (Scott, 1998) and MonoConc Pro 2. 0 (Barlow, 2000). Chapter 6 examines computer-aided assessment of the translation of The Pharmacopoeia with a more comprehensive error analysis of the target texts. It is shown that compared with human analysis, computer analysis has a higher speed in search, a broader range of texts, a faster production of wordlists, and a faster and better <b>calculation</b> <b>of</b> <b>errors.</b> This is a more comprehensive assessment of translation accuracy. It concludes with an analysis of the possible impact of the extra-linguistic factors on linguistic encoding. Chapter 7 proposes methods to improve accuracy through the construction of the bilingual term bank, and offers suggestions to improve the translation by the construction of a translation memory with the computer-aided tools provided by Trados. A comparison of the new translations with the old ones shows marked improvements in the former. Chapter 8 concludes with the major findings in this thesis. Firstly, computer-aided assessment of accuracy in scientific and technical translation is faster and broader than human assessment. Secondly, methods for assessing accuracy with computer-aided tools are important in filling up gaps in human assessment. Lastly, computer-aided accuracy assessment has a great impact on other relevant areas. In scientific and technical translation, accuracy is most important. However, the operative aspect of accuracy assessment still eludes us. This is a thorny issue in translation studies and practice. The purpose of the present study is to use computer tools, such as Wordsmith 3. 0 (Scott, 1998) and MonoConc Pro 2. 0 (Barlow, 2000), to help to assess accuracy in the English translation of Chinese pharmacological writings, a specific area of scientific and technical translation. The texts used for this study have been selected from the bilingual versions of The Pharmacopoeia of the People's Republic of China (Beijing: Chemical Industry Press, 2000; The Pharmacopoeia hereafter). The theoretical framework adopted in this thesis is the translation quality assessment model proposed by Reiss (1971 / 2000) and Vermeer (1996). The emphasis is on {{the definition}} and assessment of "accuracy" so as to provide an operative method to assess accuracy in translation, which is supplemented by a computer-aided approach to accuracy. This is done in the following ways: (1) The creation of a bilingual term bank of 1, 500 entries and a translation corpus based on the source texts, target texts and 8 Chinese-English dictionaries of medicine. (2) The categorization of the formula names and linguistic items in the source texts and the definition of accuracy and its assessment methods based on research findings in scientific and technical translation. (3) The quantitative and qualitative analysis of translation errors based on the above definition and methods. (4) The comparison of new translations, produced with the aid of computer tools, with the old ones to demonstrate the validity and applicability of the new approach to accuracy assessment. This thesis is divided into 8 chapters. 錢多秀. 呈交日期: 2005 年 12 月. 論文(哲學博士) [...] 香港中文大學, 2006. 參考文獻(p. 198 - 211). Cheng jiao ri qi: 2005 nian 12 yue. Adviser: Sin-Wai Chan. Source: Dissertation Abstracts International, Volume: 67 - 11, Section: A, page: 4165. Electronic reproduction. Hong Kong : Chinese University of Hong Kong, [2012] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. [Ann Arbor, MI] : ProQuest Information and Learning, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Electronic reproduction. Ann Arbor, MI : ProQuest Information and Learning Company, [200 -] System requirements: Adobe Acrobat Reader. Available via World Wide Web. Abstracts in Chinese and English. School code: 1307. Lun wen (zhe xue bo shi) [...] Xianggang Zhong wen da xue, 2006. Can kao wen xian (p. 198 - 211). Qian Duoxiu...|$|E
40|$|Variational {{methods have}} been {{employed}} with considerable success in computer vision, particularly for surface reconstruction problems. Formulations of this type require the solution of computationally complex Euler–Lagrange partial differential equations (PDEs) to obtain the desired reconstructions. Further, the <b>calculation</b> <b>of</b> reconstruction <b>error</b> covariances for such approaches are usually neglected. In this paper we describe a computationally efficient multiscale approach to surface reconstruction which differs fundamentally from other multiresolution methods {{that are used to}} solve the Euler– Lagrange PDEs. Instead, we interpret the variational problem as a statistical estimation problem in order to define a nearby, but slightly different, multiscale estimation problem that admits efficient solutions for both surface reconstruction and the <b>calculation</b> <b>of</b> <b>error</b> statistics. In particular, the membrane and thin-plate variational models for surfaces are interpreted as 1 /f 2 prior statistical models for the surface and its gradients, respectively. Such 1 /f 2 behavior is then achieved using a recently introduced class of multiresolution models that admits algorithms with constant per-pixel computational complexity. c ○ 1998 Academic Pres...|$|R
40|$|SummaryA {{measure of}} the {{accuracy}} of the Retail Sales Index (RSI) has been produced by estimating the standard <b>errors</b> <b>of</b> index movements. This article reports on the <b>calculation</b> <b>of</b> standard <b>errors</b> for one‐month and 12 ‐month movements in the RSI. It provides an overview <b>of</b> standard <b>errors</b> and their meaning {{in the context of the}} RSI. ...|$|R
40|$|Medication errors {{should be}} {{amenable}} to epidemiological analysis, giving {{insights into the}} causes <b>of</b> <b>error</b> {{and the effects of}} interventions to prevent them or reduce harm from them. There are formidable difficulties in establishing the rates <b>of</b> medication <b>errors.</b> There is no agreement on a clear operational definition of the condition. The methods used to enumerate cases so far have been unreliable or incomplete or both. There is disagreement about whether cases <b>of</b> <b>error</b> that do not cause harm should be included in <b>calculations</b> <b>of</b> <b>error</b> rates. When harm occurs in association with drug therapy, it is often unclear whether the harm might have been prevented, and its occurrence should therefore be considered to result from error. The denominator for calculating the rate <b>of</b> <b>error</b> is both ill-defined and inconsistently measured. Better definitions, more complete evaluation, and more thorough impact assessment may improve matters...|$|R
