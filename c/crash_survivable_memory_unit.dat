1|1309|Public
5000|$|<b>Crash</b> <b>Survivable</b> <b>Memory</b> <b>Unit</b> - a {{resilient}} {{storage medium}} used in military aircraft ...|$|E
30|$|Generation {{of output}} {{is given by}} another network as shown in Figure 2 b. One position-related <b>Memory</b> <b>Unit</b> and one motion-related <b>Memory</b> <b>Unit</b> {{converge}} to a coincidence detection unit which can activate the corresponding position-related <b>Memory</b> <b>Unit</b> as an output. The correct mappings between inputs and outputs have been trained beforehand.|$|R
30|$|At first, {{there is}} {{synchronization}} between a position-related and a motion-related <b>Memory</b> <b>Units,</b> {{while there is}} another position-related <b>Memory</b> <b>Unit</b> which is anti-phase (blue magnification in Figure 7). Assume this binding is wrong. The exact mechanism of error detection by the brain {{is beyond the scope}} of this study. Here, our goal is to show that the dynamics of partial synchronization can be adjusted by the Central Unit. At time = 3 sec, the Central Unit is shut down by setting Kc= 0. As a result, the only force remained in the network is the de-synchronizing influence among the 3 <b>Memory</b> <b>Units.</b> Then at time = 4 sec, the Central Unit is reactivated again by setting Kc= 5. The system is reconfigured and this time, the motion-related <b>Memory</b> <b>Unit</b> synchronizes with another position-related <b>Memory</b> <b>Unit</b> (pink magnification in Figure 7).|$|R
40|$|Internal {{representations}} of the environment are often invoked to explain performance in tasks in which an organism must make a detour around an obstacle to reach a target and the organism can {{lose sight of the}} target along the path to the target. By simulating a detour task in evolving populations of robots (Khepera) we show that neural networks with <b>memory</b> <b>units</b> perform better than networks without <b>memory</b> <b>units</b> in this task. However, the content of the <b>memory</b> <b>units</b> need not be interpreted as an internal representation of the position of target. The <b>memory</b> <b>units</b> send a time-varying internally generated input to the network's hidden units that allows the network to generate the appropriate behavior even when there is no external input. Networks without <b>memory</b> <b>units</b> do not have this internal input and this explains their inferior performance. 1 Detour behavior Imagine a robot (an artificial organism) that must reach a target object located somewhere in the environment. The robot has a [...] ...|$|R
3000|$|... 0 = 0 or 20 {{corresponding}} {{to whether the}} <b>Memory</b> <b>Unit</b> is quiet or activated, which {{is controlled by the}} visual inputs. The middle term corresponds to a synchronizing influence from the Central <b>Unit</b> to the <b>Memory</b> <b>Units,</b> with connection strength w [...]...|$|R
30|$|Agreement with {{temporal}} correlation hypothesis. This is {{an extension}} of the traditional temporal correlation hypothesis. The function of partial synchronization is to bind relevant <b>Memory</b> <b>Units</b> according to the task, and resetting the Central Unit can control which group of <b>Memory</b> <b>Units</b> to be synchronized.|$|R
3000|$|Next, let us {{consider}} a network of one Central <b>Unit</b> and 4 <b>Memory</b> <b>Units.</b> The Central Unit receives a constant current of Kc= 5 and oscillates at theta frequency, while the <b>Memory</b> <b>Units</b> are oscillating at alpha frequency due to a higher constant current K [...]...|$|R
3000|$|... 0 = 20 (shown {{as green}} arrows in Figure 2 a). The model needs to {{memorize}} this extra object {{until the end}} of task. Apart from that, the subject also needs to do another task. At time = 1 sec, another visual signal appears briefly and activates another position-related <b>Memory</b> <b>Unit</b> (second panel in Figure 6). At time = 2 sec, a visual signal of an arrow appears briefly, which activates a motion-related <b>Memory</b> <b>Unit</b> (third panel in Figure 6). This motion-related unit sends a “go” signal to the Central Unit (top panel in Figure 6) by a current of Kc= 5 (shown as pink arrows in Figure 2 a). The Central Unit tries to synchronize the <b>Memory</b> <b>Units</b> while the <b>Memory</b> <b>Units</b> try to de-synchronize each other. By choosing w [...]...|$|R
5000|$|Instrument {{interface}} and <b>memory</b> <b>unit</b> (BIP). Developed by Russia.|$|R
2500|$|Original style {{consoles}} {{also have}} two front-mounted {{memory card slots}} for the system's proprietary <b>Memory</b> <b>Unit.</b> These {{can be used to}} transfer profile and game data from one Xbox 360 to another. <b>Memory</b> <b>Units</b> up to 512MB are available from Microsoft. The [...] "Arcade" [...] model formerly came with a 256MB <b>Memory</b> <b>Unit,</b> but with the Jasper motherboard revision of September 2008, the [...] "Arcade" [...] model began to include 256MB of built-in flash memory. This was later increased to 512MB. The memory card slots were replaced with USB ports on the newer Xbox 360 S models.|$|R
50|$|The {{control unit}} fetches the instruction's address from the <b>memory</b> <b>unit.</b>|$|R
5000|$|Review flight {{software}} {{stored in}} mass <b>memory</b> <b>units</b> and display systems ...|$|R
3000|$|As a side note, {{it should}} be pointed out the fact that the mapping of (19) can also reduce the memory spaces of R in (18). Obviously, R needs 9 or 18 <b>memory</b> <b>units</b> to store a 3 × 3 real or complex matrix, whereas 4 or 8 <b>memory</b> <b>units</b> are enough for storing R [...]...|$|R
40|$|There is {{provided}} {{a method of}} detecting offset in a sense amplifier of an SRAM <b>memory</b> <b>unit.</b> The method comprises using a sense amplifier of the SRAM <b>memory</b> <b>unit</b> to implement a read of a first data value stored in a memory cell of the SRAM <b>memory</b> <b>unit,</b> and measuring a first time for the sense amplifier to read the first data value. The method further comprises using the sense amplifier to implement a read of a second data value stored in a memory cell of the SRAM <b>memory</b> <b>unit,</b> and measuring a second time for the sense amplifier to read the second data value. The method then comprises calculating {{a difference between the}} first time and the second time, and determining whether an offset adjustment should be applied to the sense amplifier in dependence upon the difference between the first time and the second time...|$|R
5000|$|Sega Visual <b>Memory</b> <b>Unit</b> (1998) - Dreamcast memory card/portable mini console in one.|$|R
40|$|This paper {{presents}} a technique for the efficient compiler management of software-exposed heterogeneous memory. In many lower-end embedded chips, {{often used in}} micro-controllers and DSP processors, heterogeneous <b>memory</b> <b>units</b> such as scratch-pad SRAM, internal DRAM, external DRAM and ROM are visible directly to the software, without automatic management by a hardware caching mechanism. Instead the <b>memory</b> <b>units</b> are mapped to different portions of the address space. Caches are avoided because of their cost and power consumption, and because they {{make it difficult to}} guarantee real-time performance. For this important class of embedded chips, the allocation of data to different <b>memory</b> <b>units</b> to maximize performance {{is the responsibility of the}} softwar...|$|R
40|$|Power {{consumed}} by memory systems becomes {{a serious issue}} as {{the size of the}} memory installed increases. With various low power modes that can be applied to each <b>memory</b> <b>unit,</b> the operating system can reduce the number of active <b>memory</b> <b>units</b> by collocating active pages onto a few <b>memory</b> <b>units.</b> This paper presents a memory management scheme based on this observation, which differs from other approaches in that all of the memory space is considered, while previous methods deal only with pages mapped to user address spaces. The buffer cache usually takes {{more than half of the}} total memory and the pages access patterns are different from those in user address spaces. Based on an analysis of buffer cache behavior and its interaction with the user space, our scheme achieves up to 63 percent more power reduction. Migrating a page to a different <b>memory</b> <b>unit</b> increases <b>memory</b> latencies, but it is shown to reduce the power {{consumed by}} an additional 4. 4 percent. close 102...|$|R
40|$|Power {{consumed}} by the memory system becomes a serious issue as the size of installed memory increases. With various low power modes {{that can be applied}} to each <b>memory</b> <b>unit,</b> the operating system can reduce the number of active <b>memory</b> <b>units</b> by collocating active pages onto a few <b>memory</b> <b>units.</b> This paper presents a memory management scheme based on this observation, which differs from other approaches in that all the memory space is considered while previous ones deal only with pages mapped to user address spaces. The buffer cache usually takes {{more than half of the}} total memory, and the pages access patterns are different from those made in user address spaces. Based on an analysis of buffer cache behavior and its interaction with user space our scheme achieves up to 63 % more power reduction. Migrating a page to a different <b>memory</b> <b>unit</b> increases <b>memory</b> latencies, but it is shown to reduce additional 4. 4 % of consumed power...|$|R
40|$|International audienceWe study an {{indexing}} architecture {{to store}} and search in {{a database of}} high-dimensional vectors {{from the perspective of}} statistical signal processing and decision theory. This architecture is composed of several <b>memory</b> <b>units,</b> each of which summarizes a fraction of the database by a single representative vector. The potential similarity of the query to one of the vectors stored in the <b>memory</b> <b>unit</b> is gauged by a simple correlation with the <b>memory</b> <b>unit's</b> representative vector. This representative optimizes the test of the following hypothesis: the query is independent from any vector in the <b>memory</b> <b>unit</b> vs. the query is a simple perturbation of one of the stored vectors. Compared to exhaustive search, our approach finds the most similar database vectors significantly faster without a noticeable reduction in search quality. Interestingly, the reduction of complexity is provably better in high-dimensional spaces. We empirically demonstrate its practical interest in a large-scale image search scenario with off-the-shelf state-of-the-art descriptors...|$|R
40|$|Abstract—Power {{consumed}} by memory systems becomes {{a serious issue}} as {{the size of the}} memory installed increases. With various low power modes that can be applied to each <b>memory</b> <b>unit,</b> the operating system can reduce the number of active <b>memory</b> <b>units</b> by collocating active pages onto a few <b>memory</b> <b>units.</b> This paper presents a memory management scheme based on this observation, which differs from other approaches in that all of the memory space is considered, while previous methods deal only with pages mapped to user address spaces. The buffer cache usually takes {{more than half of the}} total memory and the pages access patterns are different from those in user address spaces. Based on an analysis of buffer cache behavior and its interaction with the user space, our scheme achieves up to 63 percent more power reduction. Migrating a page to a different <b>memory</b> <b>unit</b> increases <b>memory</b> latencies, but it is shown to reduce the power {{consumed by}} an additional 4. 4 percent...|$|R
40|$|We study an {{indexing}} architecture {{to store}} and search in {{a database of}} high-dimensional vectors {{from the perspective of}} statistical signal processing and decision theory. This architecture is composed of several <b>memory</b> <b>units,</b> each of which summarizes a fraction of the database by a single representative vector. The potential similarity of the query to one of the vectors stored in the <b>memory</b> <b>unit</b> is gauged by a simple correlation with the <b>memory</b> <b>unit's</b> representative vector. This representative optimizes the test of the following hypothesis: the query is independent from any vector in the <b>memory</b> <b>unit</b> vs. the query is a simple perturbation of one of the stored vectors. Compared to exhaustive search, our approach finds the most similar database vectors significantly faster without a noticeable reduction in search quality. Interestingly, the reduction of complexity is provably better in high-dimensional spaces. We empirically demonstrate its practical interest in a large-scale image search scenario with off-the-shelf state-of-the-art descriptors. Comment: Accepted to IEEE Transactions on Big Dat...|$|R
40|$|Whereas {{clustered}} microarchitectures {{themselves have}} been extensively studied, the <b>memory</b> <b>units</b> for these clustered microarchitectures have received relatively little attention. This article discusses {{some of the}} inherent challenges of clustered <b>memory</b> <b>units</b> and shows how these can be overcome. Clustered memory pipelines work well with the late allocation of load/store queue entries and physically unordered queues. Yet this approach has characteristic problems such as queue overflows and allocation patterns that lead to deadlocks. We propose techniques to solve each of these problems and show that a distributed <b>memory</b> <b>unit</b> can offer significant energy savings and speedups over a centralized unit. For instance, compared to a centralized cache with a load/store queue of 64 / 24 entries, our four-cluster distributed <b>memory</b> <b>unit</b> with load/store queues of 16 / 8 entries each consumes 31 percent less energy and performs 4, 7 percent better on SPECint and consumes 36 percent less energy and performs 7 percent better for SPECfp. Peer ReviewedPostprint (author's final draft...|$|R
5000|$|... #Caption: Comparison of the I/O <b>memory</b> {{management}} <b>unit</b> (IOMMU) to the <b>memory</b> management <b>unit</b> (MMU).|$|R
5000|$|If the {{instruction}} is a load then: execute {{as soon as}} the <b>memory</b> <b>unit</b> is available ...|$|R
40|$|Modular {{inversion}} is {{a fundamental}} process in several cryptographic systems. It can be computed in software or hardware, but hardware computation has been proven to be faster and more secure. This research focused on improving an old scalable inversion hardware architecture proposed in 2004 for finite field GF(p). The architecture comprises two parts, a computing <b>unit</b> and a <b>memory</b> <b>unit.</b> The <b>memory</b> <b>unit</b> holds all the data bits of computation whereas the computing unit performs all the arithmetic operations in word (digit) by word bases such that the design is scalable. The main objective {{of this paper is}} to show the cost and benefit of modifying the <b>memory</b> <b>unit</b> to include shifting, which was previously one of the tasks of the scalable computing unit. The study included remodeling the entire hardware architecture removing the shifter from the scalable computing part and embedding it in the non-scalable <b>memory</b> <b>unit</b> instead. This modification resulted in a speedup to the complete inversion process with an area increase due to the new <b>memory</b> shifting <b>unit.</b> Several design schemes have been compared giving the user the complete picture to choose from depending on the application need...|$|R
5000|$|... #Caption: The C.mmp <b>memory</b> <b>unit,</b> {{with three}} racks visible, {{including}} the front {{panel of the}} crossbar switch.|$|R
40|$|This article {{presents}} a technique for the efficient compiler management of software-exposed heterogeneous memory. In many lower-end embedded chips, {{often used in}} microcontrollers and DSP processors, heterogeneous <b>memory</b> <b>units</b> such as scratch-pad SRAM, internal DRAM, external DRAM, and ROM are visible directly to the software, without automatic management by a hardware caching mechanism. Instead, the <b>memory</b> <b>units</b> are mapped to different portions of the address space. Caches are avoided due to their cost and power consumption, and because they {{make it difficult to}} guarantee real-time performance. For this important class of embedded chips, the allocation of data to different <b>memory</b> <b>units</b> to maximize performance {{is the responsibility of the}} software. Current practice typically leaves it to the programmer to partition the data among different <b>memory</b> <b>units.</b> We present a compiler strategy that automatically partitions the data among the <b>memory</b> <b>units.</b> We show that this strategy is optimal, relative to the profile run, among all static partitions for global and stack data. For the first time, our allocation scheme for stacks distributes the stack among multiple <b>memory</b> <b>units.</b> For global and stack data, the scheme is provably equal to or better than any other compiler scheme or set of programmer annotations. Results from our benchmarks show a 44. 2 % reduction in runtime from using our distributed stack strategy vs. using a unified stack, and a further 11. 8 % reduction in runtime from using a linear optimization strategy for allocation vs. a simpler greedy strategy; both in the case of the SRAM size being 20 % of the total data size. For some programs, less than 5 % of data in SRAM achieves a similar speedup...|$|R
5000|$|The entire {{core memory}} {{was in the}} IBM 1625 <b>memory</b> <b>unit.</b> <b>Memory</b> cycle time was halved {{compared}} to the Model I's (internal or 1623 <b>memory</b> <b>unit),</b> to 10 µs (i.e., the cycle speed was raised to 100 kHz) by using faster cores. A Memory Address Register Storage (MARS) core memory read, clear, or write operation took 1.5 µs and each write operation was automatically (but not necessarily immediately) preceded by a read or clear operation of the same [...] "register(s)" [...] during the 10 µs memory cycle.|$|R
50|$|Side Note: A VMU (Visual <b>Memory</b> <b>Unit)</b> was {{released}} for the Sega Dreamcast in 1999 themed around Gamera.|$|R
50|$|For example, if one {{clock pulse}} latches a value into a {{register}} or begins a calculation, {{it will take}} some time for the value to be stable at the outputs of the register or for the calculation to complete. As another example, reading an instruction out of a <b>memory</b> <b>unit</b> cannot be done {{at the same time that}} an instruction writes a result to the same <b>memory</b> <b>unit.</b>|$|R
5000|$|... #Caption: 7-track {{magnetic}} tape <b>memory</b> <b>units</b> (CDC 604) at the Rechenzentrum (Computer Center) of RWTH Aachen University, Germany (1970) ...|$|R
40|$|The modular {{inversion}} is {{a fundamental}} process in several cryptographic systems. It can be computed in software or hardware, but hardware computation proven to be faster and more secure. This research focused on improving an old scalable inversion hardware architecture proposed in 2004 for finite field GF(p). The architecture {{has been made of}} two parts, a computing <b>unit</b> and a <b>memory</b> <b>unit.</b> The <b>memory</b> <b>unit</b> is to hold all the data bits of computation whereas the computing unit performs all the arithmetic operations in word (digit) by word bases known as scalable method. The main objective of this project was to investigate the cost and benefit of modifying the <b>memory</b> <b>unit</b> to include parallel shifting, {{which was one of the}} tasks of the scalable computing unit. The study included remodeling the entire hardware architecture removing the shifter from the scalable computing part embedding it in the <b>memory</b> <b>unit</b> instead. This modification resulted in a speedup to the complete inversion process with an area increase due to the new <b>memory</b> shifting <b>unit.</b> Quantitative measurements of the speed area trade-off have been investigated. The results showed that the extra hardware to be added for this modification compared to the speedup gained, giving the user the complete picture to choose from depending on the application need...|$|R
50|$|Île de Sein {{arrived at}} the crash site on 26 April, and during its first dive, the Remora 6000 found the flight data {{recorder}} chassis, although without the crash-survivable <b>memory</b> <b>unit.</b> On 1 May the <b>memory</b> <b>unit</b> was found and lifted on board the Île de Sein by the ROV. The aircraft's cockpit voice recorder was found on 2 May 2011, and was raised and brought on board the Île de Sein the following day.|$|R
3000|$|... 1. The right {{term with}} a {{summation}} {{corresponds to a}} de-synchronizing influence among N <b>Memory</b> <b>Units,</b> with connection strength w [...]...|$|R
5000|$|... a dual <b>memory</b> <b>unit</b> {{consisting}} of two sets of 64 mercury acoustic delay lines of eight words capacity on each line ...|$|R
5000|$|Else, if the {{instruction}} is a store then: {{wait for the}} value to be stored before sending it to the <b>memory</b> <b>unit</b> ...|$|R
