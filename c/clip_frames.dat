3|62|Public
50|$|Other {{styles are}} <b>clip</b> <b>frames</b> (not really a frame at all), box frames and shadow boxes. A digital photo frame {{is an example}} of the {{changing}} technology of the 21st century.|$|E
5000|$|The Vulcan IDIC pendant was {{designed}} by Gene Roddenberry as a marketing premium long before the third season. As early {{as the end of}} the first season, fans of the show had begun writing in asking for copies of the scripts, film <b>clip</b> <b>frames,</b> etc., and these were soon sold through Roddenberry's mail-order company, Lincoln Enterprises, founded by Bjo Trimble and her husband John before they were fired by Roddenberry and the business turned over to Majel Barrett. As evidenced in some of his letters and memos, Roddenberry was fond of circle-and-triangle designs and had wanted to use them for purposes of theatrical unity as early as the first season's [...] "The Return of the Archons". As reported by editor Ruth Berman (issue #1, Inside Star Trek, July 1968, pp. 15-16), [...] "ardent rock hound and amateur lapidary" [...] Roddenberry came up with the Vulcan philosophy after he presented Leonard Nimoy with a unique [...] "hand-crafted piece of jewelry", a [...] "pendent" [...] (sic) of polished yellow gold (circle) and florentined white gold (triangle), with a stone of brilliant white fabulite—an artificial gem [...] "developed by the laser industry and used in space mechanisms for its optical qualities", and thus well-suited as a gift for an actor in a science fiction show. Readers were encouraged to submit their interest in such a product to the then-Star Trek Enterprises mail order firm. It was noted that [...] "less expensive materials" [...] would keep costs down.|$|E
40|$|Motion graphs {{have been}} widely {{successful}} in the synthesis of human motions. However, {{the quality of the}} generated motions depends heavily on the connectivity of the graphs and the quality of transitions in them. Achieving both of these criteria simultaneously though is difficult. Good connectivity requires transitions between less similar poses, while good motion quality requires transitions only between very similar poses. This paper introduces a new method for building motion graphs. The method first builds a set of interpolated motion clips, which contains many more similar poses than the original data set. The method then constructs a well-connected motion graph (wcMG), by using as little of the interpolated motion <b>clip</b> <b>frames</b> as necessary to provide good connectivity and only smooth transitions. Based on experiments, wcMGs outperform standard motion graphs across different measures, generate good quality motions, allow for high responsiveness in interactive control applications, and do not even require post-processing of the synthesized motions...|$|E
50|$|The liveliness of Lively {{graphics}} {{becomes even}} more apparent when manipulating the scale and rotation handles for objects and text. The whole code browser can be used when tilted 20 degrees on its side. Because the text editor is made up entirely of lively graphics, it works perfectly well when rotated or scaled, just as do the scroll bars, <b>clipping</b> <b>frames,</b> {{and the rest of}} the entire user interface.|$|R
50|$|The Lively Kernel uses a Morphic {{graphics}} {{model to}} add behavior to a scene graph built from browser graphics. Simple graphics are thus assembled into such standard widgets as sliders, scroll bars, text views, lists and <b>clipping</b> <b>frames.</b> A simple window system built from these widgets offers object inspectors, file browsers and code browsers. Even the rudimentary demo pages thus {{have the ability}} to edit and test new code in a simple code browser while the system is running.|$|R
40|$|This {{dissertation}} {{presents a}} method capable of smoothing {{the silhouette of}} a 3 D model using interpolation to find smooth edges. The method has as goal to be used with normal mapping to improve the performance and give a better result with a low polygonal count. To do this the lines located on {{the silhouette of a}} model is interpolated to find a curve that is used as <b>clipping</b> <b>frame</b> in the stencil buffer. This method is able to modify the silhouette for the better. The amount of interpolation is rather limited...|$|R
30|$|The {{proposed}} dataset includes 31 {{video clips}} or 44, 000 individual frames with {{the resolution of}} 554  ×  235. The video <b>clips</b> and <b>frames</b> were recorded at 30 frames per second using a fixed video recorder elevated at a height, viewing individuals moving.|$|R
40|$|We are {{studying}} about automatic production of soccer sports videos for easy understanding by using digital camera work on camera fixed videos. The digital camera {{work is a}} movie technique which uses virtual panning and zooming by <b>clipping</b> <b>frames</b> from hi-resolution images and controlling the frame size and position. We have studied so far digital panning. In this paper, we propose a method of digital zooming by automatically recognizing the game situation or events such as penalty kick and free kick based on player and ball tracking. These recognition results are used as key indices to retrieve the event scenes from soccer videos. We compared the proposed technique with a conventional technique by AHP method that can reflect an individual subjectivity. 1...|$|R
5000|$|Mix and Match - put <b>clips</b> of any <b>frame</b> rate, compression, scan mode {{or video}} format {{on the same}} {{timeline}} ...|$|R
40|$|Audio has a key index {{in digital}} videos {{that can provide}} useful {{information}} for video editing, such as capturing conversations only, clipping only talking people, and so on. In this paper, we are studying about video editing based on audio with a two-channel (stereo) microphone that is standard equipment on video cameras, where the video content is automatically recorded without a cameraman. In order to capture only a talking person on video, a novel voice/non-voice detection algorithm using AdaBoost, which can achieve extremely high detection rates in noisy environments, is used. In addition, the sound source direction is estimated by the CSP (Crosspower-Spectrum Phase) method in order to zoom in on the talking person by <b>clipping</b> <b>frames</b> from videos, where a two-channel (stereo) microphone is used to obtain information about time differences between the microphones. ...|$|R
50|$|When {{used in a}} CinemaDNG movie <b>clip,</b> each <b>frame</b> is encoded {{using the}} above DNG image format. The clip's image stream can then be stored {{in one of two}} formats: either as video essence using {{frame-based}} wrapping in an MXF file, or as a sequence of DNG files in a specified file directory.|$|R
5000|$|<b>Clips</b> of Strange <b>Frame</b> had debut {{screening}} in Los Angeles at Bent-Con on December 3, 2011. [...] The film premiered at Sci-Fi-London on May 3, 2012.|$|R
500|$|Rolling Stone, {{called the}} video [...] "powerful" [...] and a found [...] "the final crescendo {{reaching}} a breaking point of true sorrow". Chris Payne, from Billboard called the music video [...] "somber". Nicole Sia of Spin praised [...] "the song's visual". According to Lansky of Idolator, [...] "the clip, {{which is basically}} just Mars...sitting and singing at the piano...works". He continued, [...] "While he certainly could have gone for {{something a little more}} high-concept, the <b>clip’s</b> elegant <b>framing</b> just draws attention to...the song".|$|R
50|$|The Moore City Council {{proposed}} a measure making twelve changes to its residential building codes, include requiring that new home {{construction in the}} city include hurricane <b>clips</b> or <b>framing</b> anchors, continuous plywood bracing and wind-resistant garage doors in order for homes to withstand winds up to 135 mph (equivalent to a high-end EF2 tornado). When the measure was passed in a unanimous vote held on March 17, 2014, Moore became the first city in the United States to adopt a building code addressing the effects of tornadoes on homes, which exceed the national standards set by the National Association of Home Builders.|$|R
5000|$|In {{two-dimensional}} graphics, a clip region may {{be defined}} so that pixels are only drawn {{within the boundaries of}} a window or <b>frame.</b> <b>Clip</b> regions {{can also be used to}} selectively control pixel rendering for aesthetic or artistic purposes. In many implementations, the final clip region is the composite (or intersection) of one or more application-defined shapes, as well as any system hardware constraints ...|$|R
40|$|Video <b>clip</b> {{consists}} of <b>frames,</b> and each frame {{can be considered}} as a transformed picture of the reference frame. In this paper, we briefly discuss a framelet method for highresolution image reconstruction to enhance the resolution of video clips. The detailed discussion can be found in [10]. Experiments on an actual video clip show that our method can provide information that are not discernable from the given video clip. 1...|$|R
50|$|In December 2002, {{the show}} was relaunched by Granada Productions, makers of You've Been Framed!, with Tony Blackburn and Tara Palmer-Tomkinson now co-hosting the show together, {{as well as a}} new logo and jazzed up theme tune. It is {{possible}} that now it was produced by Granada, it was more likely You've Been <b>Framed!</b> <b>clips</b> may have been used. There was seven episodes in this era which lasted until their final episode in July 2005.|$|R
40|$|This paper {{presents}} {{a technique to}} reduce the peak-to-average power ratio (PAPR) in orthogonal frequency-division multiplexing (OFDM) systems. The presented scheme combines two well-known techniques, namely selected mapping (SLM) and clipping, in an adaptive way. Allowing an affordable PAPR, which might be exceeded with very small probability, the proposed method aims at minimizing the instantaneous number of clipped peaks per OFDM frame to limit the resulting transmit signal distortion. Using Compressed Sensing based algorithms which account for the potential variation {{of the number of}} <b>clips</b> per <b>frame,</b> this <b>clipping</b> strategy directly translates into an improved receiver-side peak reconstruction compared to choosing a fixed number of clipped peaks. It is shown that in this way a receiver-side bit error probability reduction is achieved while guaranteeing the same maximum PAPR at the transmitter...|$|R
40|$|This paper {{presents}} the results of a study which investigated the impact of cognitive styles on perceptual multimedia quality. More specifically, we examine the different preferences demonstrated by verbalizers and imagers when viewing multimedia content presented with different Quality of Service (QoS) levels pertaining to frame rates and color depth. Recognizing multimedia’s infotainment duality, we used the Quality of Perception (QoP) metric to characterize perceived quality. Results showed that in terms of low and high dynamisms <b>clips,</b> the <b>frame</b> rate at which multimedia content is displayed influences the levels of information assimilated by Imagers. Whilst black and white presentations are shown to be beneficial for both Biomodals and Imagers in order to experience enhanced levels of information assimilation, Imagers were shown to enjoy presentations in full 24 -bit colou...|$|R
50|$|At {{the present}} day, the bough {{has become more}} {{elaborate}} with ribbons, nuts, apples and candles. The whole frame is covered with greenery. Red apples or oranges may be hung from ribbons in the centre and mistletoe is tied below. Additionally candles may be <b>clipped</b> to the <b>frame</b> and bright streamers {{are attached to the}} top. Another form that the Kissing Bough can take is that of a crown with a structure composed of only {{the top half of the}} globe.|$|R
40|$|In this paper, we {{describe}} the search algorithm for movie files. The algorithm is first divides the video <b>clip</b> into different <b>frames.</b> Full search algorithm in video compression. The algorithm takes 16 pixels window left,right,upper and lower. Find out the best possible matching block of current frame to reference frame,then it find out the motion vectors. Since algorithm works pixel by pixel basis,for each window it performs 33 X 33 X 8 X 8 multiplications and 33 X 33 X 8 X 8 x 2 subtractions so speed is low...|$|R
40|$|TV estabilishes {{televised}} {{worlds in}} which a society is devised, which for its part devises television. It makes it mainly through <b>clipping,</b> assembling and <b>framing</b> cultural fragments and remains, giving them a new significance in typically televised experienced pictures: the overlapped frames. In those pictures, the televised otherness gain existence. The Brazility as well as TV itself, amongst other ethicities, are in this manner stated, with certain senses, in practices {{which tend to be}} homological. Those senses have to be searched in televised grammars even more than in something called contents of television programs...|$|R
40|$|This is the post-print {{version of}} the Article. The {{official}} published version can be accessed from the link below - Copyright @ 2008 ElsevierThis paper {{presents the results of}} a study which investigated the impact of cognitive styles on perceptual multimedia quality. More specifically, we examine the different preferences demonstrated by verbalizers and imagers when viewing multimedia content presented with different quality of service (QoS) levels pertaining to frame rates and color depth. Recognizing multimedia’s infotainment duality, we used the quality of perception (QoP) metric to characterize perceived quality. Results showed that in terms of low and high dynamisms <b>clips,</b> the <b>frame</b> rate at which multimedia content is displayed influences the levels of information assimilated by Imagers. Whilst black and white presentations are shown to be beneficial for both Biomodals and Imagers in order to experience enhanced levels of information assimilation, Imagers were shown to enjoy presentations in full 24 -bit colour...|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Surveillance videos are {{recorded}} continually and the retrieval of such videos currently still relies on human operators. Automatic retrieval has not reached a satisfactory accuracy. As an intermediate representation, this work develops multiple original temporal profiles of video to convey accurate temporal {{information in the}} video while keeping certain spatial characteristics. These are effective methods to visualizes surveillance video contents efficiently in a 2 D temporal image, suitable for indexing and retrieving a large video database. We are aiming to provide a compact index that is intuitive and preserves {{most of the information}} in the video in order to avoid browsing extensive video <b>clips</b> <b>frame</b> by frame. By considering some of the properties of static surveillance videos, we aim at accentuating the temporal dimension in our visualization. We have introduced our framework as three unique methods that visualize different aspects of a surveillance video, plus an extension to non-static surveillance videos. In our first method "Localized Temporal Profile", by knowing that most surveillance videos are monitoring specific locations, we try to emphasize the other dimension, time, in our solution. we focus on describing all the events only in critical locations of the video. In our next method "Multi-Position Temporal Profile", we generate an all-inclusive profile that covers all the events in the video field of view. In our last method "Motion Temporal Profile" we perform in-depth analysis of scene motion and try to handle targets with non-uniform, non-translational motion in our temporal profile. We then further extend our framework by loosening the constraint that the video is static and including cameras with smooth panning motion as such videos are widely used in practice. By performing motion analysis on the camera, we stabilize the camera to create a panorama-like effect for the video, allowing us to utilize all of the aforementioned methods. The resulting profiles allows temporal indexing to each video frame, and contains all spatial information in a continuous manner. It also shows the actions and progress of events in the temporal profile. Flexible browsing and effective manipulation of videos can be achieved using the resulting video profiles...|$|R
40|$|Surveillance videos are {{recorded}} continually and the retrieval of such videos currently still relies on human operators. Automatic retrieval has not reached a satisfactory accuracy. As an intermediate representation, this work develops multiple original temporal profiles of video to convey accurate temporal {{information in the}} video while keeping certain spatial characteristics. These are effective methods to visualizes surveillance video contents efficiently in a 2 D temporal image, suitable for indexing and retrieving a large video database. We are aiming to provide a compact index that is intuitive and preserves {{most of the information}} in the video in order to avoid browsing extensive video <b>clips</b> <b>frame</b> by frame. ^ By considering some of the properties of static surveillance videos, we aim at accentuating the temporal dimension in our visualization. We have introduced our framework as three unique methods that visualize different aspects of a surveillance video, plus an extension to non-static surveillance videos. ^ In our first method 2 ̆ 2 Localized Temporal Profile 2 ̆ 2, by knowing that most surveillance videos are monitoring specific locations, we try to emphasize the other dimension, time, in our solution. we focus on describing all the events only in critical locations of the video. In our next method 2 ̆ 2 Multi-Position Temporal Profile 2 ̆ 2, we generate an all-inclusive profile that covers all the events in the video field of view. In our last method 2 ̆ 2 Motion Temporal Profile 2 ̆ 2 we perform in-depth analysis of scene motion and try to handle targets with non-uniform, non-translational motion in our temporal profile. We then further extend our framework by loosening the constraint that the video is static and including cameras with smooth panning motion as such videos are widely used in practice. By performing motion analysis on the camera, we stabilize the camera to create a panorama-like effect for the video, allowing us to utilize all of the aforementioned methods. The resulting profiles allows temporal indexing to each video frame, and contains all spatial information in a continuous manner. It also shows the actions and progress of events in the temporal profile. Flexible browsing and effective manipulation of videos can be achieved using the resulting video profiles. ...|$|R
40|$|Television (TV) {{services}} have been becoming increasingly important {{in our daily}} life. With the advent of communication technologies, a large variety of TV content is delivered to customers through various channels, and an important challenge for content providers is to efficiently manage such abundant content. In this chapter, we first review key techniques in semantic multimedia (audio-visual sequences) content analysis and two important applications, namely content browsing and summarization. Subsequently, we propose a hierarchical analysis framework for modeling the semantic importance of different units in an audio-visual sequence, i. e. sequence, scene, shot, and frame, by using low-level as well as high-level audio-visual cues. Based on the semantic analysis framework, a feasible solution to generate TV content summaries, which can efficiently extract the most important <b>clips</b> and <b>frames</b> from the original sequences, is proposed for rapid browsing of multimedia content. Subjective evaluations and comparison to two state-of-the-art summarization methods have demonstrated that the proposed TV content summarization solution can efficiently extract the most representative excerpts from the original sequences, {{and it can be}} further used in rapid TV content browsing. ...|$|R
3000|$|... [...]. Furthermore, a {{rectangular}} region including one human is <b>clipped</b> from each <b>frame,</b> {{and they are}} regularized to the same size. In this article, we assume that this rectangular region has previously been obtained. Deciding the rectangular regions including humans might be difficult. However, there are several methods for extracting/deciding human regions from video sequences [26, 27]. These methods achieved accurate human region detection by combining visual information and sensor information such as kinect,d using a stereo-camera, or using a camera for which position is calibrated. Although we extract the rectangular region manually for simplicity, we consider that a certain precision can be guaranteed using these methods.|$|R
40|$|Abstract. This paper {{presents}} {{an approach to}} extracting dominant feature vectors from an individual audio clip and then proposes a new similarity measure based on the dominant feature vectors. Instead of using the {{mean and standard deviation}} of frame features in most conventional methods, the most salient characteristics of an audio clip are represented in the form of several dominant feature vectors. These dominant feature vectors give a better description of the fundamental properties of an audio <b>clip,</b> especially when <b>frame</b> features change a lot along the time line. Evaluations on a content-based audio retrieval system indicate an obvious improvement after using the proposed similarity measure, compared with some other conventional methods. ...|$|R
40|$|Abstract—The modern {{telecommunication}} industry demands higher capacity networks {{with high}} data rate. Orthogonal frequency division multiplexing (OFDM) is a promising technique for {{high data rate}} wireless communications at reasonable complexity in wireless channels. OFDM has been adopted for many types of wireless systems like wireless local area networks such as IEEE 802. 11 a, and digital audio/video broadcasting (DAB/DVB). The proposed research focuses on a concatenated coding scheme that improve the performance of OFDM based wireless communications. It uses a Redundant Residue Number System (RRNS) code as the outer code and a convolutional code as the inner code. Here, a direct conversion of analog signal to residue domain is done to reduce the conversion complexity using sigma-delta based parallel analog-to-residue converter. The bit error rate (BER) performances of the proposed system under different channel conditions are investigated. These include the effect of additive white Gaussian noise (AWGN), multipath delay spread, peak power <b>clipping</b> and <b>frame</b> start synchronization error. The simulation {{results show that the}} proposed RRNS-Convolutional concatenated coding (RCCC) scheme provides significant improvement in the system performance by exploiting the inherent properties of RRNS. Keywords—Analog-to-residue converter, Concatenated codes...|$|R
40|$|This paper {{describes}} a novel methodology for implementing video search {{functions such as}} retrieval of near-duplicate videos and recognition of actions in surveillance video. Videos are divided into half-second <b>clips</b> whose stacked <b>frames</b> produce 3 D space-time volumes of pixels. Pixel regions with consistent color and motion properties are extracted from these 3 D volumes by a threshold-free hierarchical space-time segmentation technique. Each region is then described by a high-dimensional point whose components represent the position, motion and, when possible, color of the region. In the indexing phase for a video database, these points are assigned labels that specify their video clip of origin. All the labeled points for all the clips are stored into a single binary tree for efficient k-nearest neighbor retrieval. The retrieva...|$|R
40|$|This work {{proposes a}} simple {{pipeline}} to classify and temporally localize activities in untrimmed videos. Our system uses features from a 3 D Convolutional Neural Network (C 3 D) as input to train a a recurrent neural network (RNN) that learns to classify video <b>clips</b> of 16 <b>frames.</b> After <b>clip</b> prediction, we post-process {{the output of}} the RNN to assign a single activity label to each video, and determine the temporal boundaries of the activity within the video. We show how our system can achieve competitive results in both tasks with a simple architecture. We evaluate our method in the ActivityNet Challenge 2016, achieving a 0. 5874 mAP and a 0. 2237 mAP in the classification and detection tasks, respectively. Our code and models are publicly available at: [URL] ReviewedPostprint (published version...|$|R
30|$|The {{data set}} of YTF {{contains}} 3425 videos of 1595 subjects {{collected from the}} website of YouTube. There are huge variations of pose, expressions, appearances, occlusions, colors, illumination, and resolutions of the frames of these videos. The average length of each video <b>clip</b> is 181.3 <b>frames.</b> The standard protocol for evaluation {{of the performance of}} the unconstrained face verification includes 5000 video pairs of this database. These pairs are equally divided into 10 -folds, and each fold has almost equal numbers of intra-personal pairs and inter-personal pairs. The facial parts of the frames are detected using the Viola-Jones detector, and the aligned frames are provided by the authors of [68]. In the experiments, we use the grayscale version of the cropped frames of size 150 × 120 by considering the center coordinate of the aligned frames as the face center.|$|R
40|$|In this study, {{international}} and national assistant referees (ARs) were tested on offside decision-making accuracy {{and accuracy of}} memory. Eye movements were tracked during the observation of video <b>clips</b> and frozen <b>frames.</b> The international ARs clearly performed better than the national ARs for response accuracy on the video clips, {{but not for the}} frame recognition. Interestingly, for the incorrect responses, the international ARs chose more frames behind the offside line, and the national ARs more frames ahead. The eye movement data only revealed a significant longer observation length on the frames for the international ARs. These observations lead us to believe that international ARs used a strategy of compensation for the forward displacement of the attacker induced by the flash-lag effect. Future research should focus on the awareness of the perceptual illusion and the compensation strategy of the ARs. status: publishe...|$|R
40|$|In {{this paper}} we {{introduce}} “clipping, ” a new method of syntactic approximation which {{is motivated by}} and {{works in conjunction with}} a sound and decidable denotational model for a given programming language. Like slicing, clipping reduces the size of the source code in preparation for automatic verification; but unlike slicing it is an imprecise but computationally inexpensive algorithm which does not require a whole-program analysis. The technique of <b>clipping</b> can be <b>framed</b> into an iterated refinement cycle to arbitrarily improve its precision. We first present this rather simple idea intuitively with some examples, then work out the technical details {{in the case of an}} Algol-like programming language and a decidable approximation of its gamesemantic model inspired by Hankin and Malacaria’s “lax functor ” approach. We conclude by presenting an experimental model checking tool based on these ideas and some toy programs. 1...|$|R
50|$|The market {{houses the}} {{headquarters}} of many jewelry institutions of India including Jagawat Sons(of Mankhush Jagawat) and Tribhovandas Bhimji Zaveri, {{one of the biggest}} jewelry retailers of India, established in 1864. All kinds of gems and precious stones are available in the market as well as ornaments of traditional Indian designs to modern designs made of every possible gem and precious metal. Zaveri bazaar is regarded as being secured because it has CCTVs installed in order to avoid almost any crime, private protection involved for every shop, industry is under 24 hr vigilance because it does a huge trade in diamond and also other mettle jewelry. Zaveri Bazaar is also famous for photo <b>frames,</b> <b>clips,</b> tea-sets, dinnerware, toys and other luxury lifestyle articles crafted out of expensive metals. Zaveri Bazaar gets the maximum credit of jewelry and gems export from the country.|$|R
40|$|Databases were {{introduced}} to remove redundancy from conventional file systems to encourage sharing of information. The same idea is extended in this study to support sharing for continuous media data types (i. e., video and audio). Sharing in conventional databases results in update anomalies when information is modified. With presentations (movies) sharing <b>clips</b> (sequence of <b>frames),</b> continuous display becomes challenging as well. To ensure a continuous display, a system should retrieve data at a pre-specified rate. Otherwise, a display might suffer from disruptions or delays, termed hiccups. To ensure a continuous display using a multi-disk hardware platform, a video object is striped {{into a number of}} subobjects. The system enforces a regular schedule on retrieval of each subobject by controlling the placement of the subobjects across the disks. Now if different presentations share subobjects, each presentation will enforce its own restrictions on the placement of the data. This migh [...] ...|$|R
