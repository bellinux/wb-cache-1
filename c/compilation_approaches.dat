24|169|Public
40|$|We {{provide a}} {{perspective}} on knowledge compilation which calls for analyzing different <b>compilation</b> <b>approaches</b> according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We argue that such analysis is necessary for placing new <b>compilation</b> <b>approaches</b> {{within the context of}} existing ones...|$|E
40|$|We {{propose a}} {{perspective}} on knowledge compilation which calls for analyzing different <b>compilation</b> <b>approaches</b> according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes {{a large number of}} existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new <b>compilation</b> <b>approaches</b> within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages. ...|$|E
40|$|Abstract: Knowledge {{compilation}} {{deals with}} the computational intractability of reasoning problems. To overcome this difficulty we provide three different <b>compilation</b> <b>approaches</b> to a first order knowledge base. The knowledge base Σ is preprocessed by unit resolution into an approximate knowledge base CKB(Σ) from which a subset of possible queries can be answered by unit refutation. The number of clauses obtained by {{each of the three}} methods is less than the number of clauses in the prime implicate set...|$|E
40|$|Tomorrow’s {{embedded}} devices {{need to run}} highresolution multimedia as well as need to support multistandard wireless systems which require an enormous computational complexity with a very low energy consumption and very high performance constraints. In this context, the register file {{is one of the}} key sources of power consumption and performance bottleneck, and its inappropriate design and management can severely affect the performance of the system. In this paper, we present a new <b>compilation</b> <b>approach</b> to mitigate the performance implications of technology variation in the shared register file in upcoming embedded VLIW architectures with several processing units. The <b>compilation</b> <b>approach</b> is based on a redefined register assignment policy and a set of architectural modifications to this device. Experimental results show up to a 67 % performance improvement with our technique. ...|$|R
40|$|Coarse-grained {{reconfigurable}} architectures {{can enhance}} the performance of critical loops and computation-intensive functions. Such architectures need efficient compilation techniques to map algorithms onto customized architectural configurations. A new <b>compilation</b> <b>approach</b> uses a generic reconfigurable architecture to tackle the memory bottleneck that typically limits the performance of many applications. close 396...|$|R
40|$|We {{present an}} {{approach}} for compiling a device into a propositional sentence and then show {{how to answer}} key diagnostic queries based on such a device compilation. We present two key results about our <b>compilation</b> <b>approach,</b> which makes it {{of interest to the}} broad community. First, the complexity of answering diagnostic queries is linear {{in the size of the}} propositional sentence constituting the device compilation. Second, the time to generate, and space to store, a device compilation are linear in the device size when the device structure has a bounded tree-width, where tree-width is a graph-theoretic parameter that measures the device connectivity. The main practical value of our <b>compilation</b> <b>approach</b> is in the simplicity of the device compilation and its associated algorithms, which allows for efficient and cost effective implementations on a variety of (primitive) software and hardware platforms. 1 Introduction The field of model-based diagnosis has been concerned with the automatic d [...] ...|$|R
40|$|Programs {{written in}} the {{synchronous}} programming language Esterel may contain statically cyclic dependencies of signals, which inhibits the application of certain <b>compilation</b> <b>approaches</b> that rely on static scheduling. This talk proposes an algorithm which, given a constructive synchronous program, performs a semantics-preserving source-level code transformation that removes cyclic signal dependencies. The transformation exploits the monotonicity of constructive programs, and is illustrated {{in the context of}} Esterel, but should be applicable to other synchronous languages as well...|$|E
40|$|Abstract—The {{performance}} gap for high performance applications has been widening over time. High level program transformations {{are critical to}} improve applications ’ performance, many of which concern the determination of optimal values for transformation parameters, such as loop unrolling and blocking. Static approaches achieve these values based on analytical models {{that are hard to}} achieve because of increasing architecture complexity and code structures. Recent iterative <b>compilation</b> <b>approaches</b> achieve it by executing different versions of the program on actual platforms and select the one that renders best performance, outperforming static <b>compilation</b> <b>approaches</b> significantly. But the expensive compilation cost has limited their application scope to embedded applications and a small group of math kernels. This paper proposes a combinative approach [...] Combining Model and Iterative Compilation for Program Performance Optimization (CMIC). Such an approach first constructs a program optimization transformation model based on hardware performance counters to decide how and when to apply transformations, and then selects the optimal transformation parameters using Nelder-Mead simplex algorithm. Experimental results show that our approach can effectively improve programs ’ floating-point performance, reducing programs ’ runtime, therefore, lessening the {{performance gap}} for high-performance applications...|$|E
40|$|Control flow {{compilation}} is {{a hybrid}} between classical WAM compilation and meta-call, {{limited to the}} compilation of non-recursive clause bodies. This approach is used successfully for the execution of dynamically generated queries in an inductive logic programming setting (ILP). Control flow compilation reduces compilation times up to an order of magnitude, without slowing down execution. A lazy variant of control flow compilation is also presented. By compiling code by need, it removes the overhead of compiling unreached code (a frequent phenomenon in practical ILP settings), and thus reduces {{the size of the}} compiled code. Both dynamic <b>compilation</b> <b>approaches</b> have been implemented and were combined with query packs, an efficient ILP execution mechanism. It turns out that locality of data and code is important for performance. The experiments reported in the paper show that lazy control flow compilation is superior in both artificial and real life settings. status: publishe...|$|E
5000|$|The Gospel According to Barnabas (1992) - CD <b>compilation</b> of <b>Approaching</b> Light Speed and Feel the Fire ...|$|R
40|$|Abstract: Within {{this paper}} {{attempt was made}} to present minaral {{potential}} of the Republic of Macedonia. With <b>compilation</b> <b>approach</b> is made review of the natural mineral resources, some of the data are results from the geological investigations done in the perennial period. Separate presentation was made for metalic, non – metalic and energetics mineral raw materials, and for some of them are present perspective areas and directions for further investigations...|$|R
40|$|AbstractGuaranteeing {{correctness}} of compilation {{is a vital}} {{precondition for}} correct software. Code generation {{can be one of}} the most error-prone tasks in a compiler. One way to achieve trusted compilation is certifying compilation. A certifying compiler generates for each run a proof that it has performed the compilation run correctly. The proof is checked in a separate theorem prover. If the theorem prover is content with the proof one can be sure that the compiler produced correct code. This paper reports on the construction of a certifying code generation phase for a compiler. It is part of a larger project aimed at guaranteeing the correctness of a complete compiler. We emphasize on demonstrating the feasibility of the certifying <b>compilation</b> <b>approach</b> to code generation and focus on the implementation and practical issues. It turns out that the checking of the certificates is the actual bottleneck of certifying compilation. We present a proof schema to overcome this bottleneck. Hence we show the applicability of the certifying <b>compilation</b> <b>approach</b> for small sized programs processed by a compiler's code generation phase...|$|R
40|$|AbstractIt {{has been}} {{observed}} previously that Random 3 -SAT exhibits a phase transition at a critical ratio of constraints to variables, where the average frequency of satisfiability falls abruptly from near 1 to near 0. In this paper we look beyond satisfiability to implicates and prime implicates of non-zero length and show experimentally that, for any given length, these exhibit their own phase transitions. All of these phase transitions appear to share the same critical point as the well-known satisfiability phase transition. We also find a rich, regular pattern, in which phase transitions for longer implicates or prime implicates are less steep at a given problem size and all the phase transitions sharpen with increasing problem size. Implicates correspond in a one-to-one way to nogoods, and prime implicates correspond similarly to minimal nogoods. Knowledge about these phase transitions helps us to understand more about the behavior of search algorithms and knowledge <b>compilation</b> <b>approaches</b> {{in the context of}} Random 3 -SAT...|$|E
40|$|Due {{to their}} {{increasing}} resource densities, field programmable gate arrays (FPGAs) have become capable of efficiently implementing large scale scientific applications involving floating point computations. In this paper FPGAs are {{compared to a}} high end microprocessor with respect to sustained performance for a popular floating point CPU performance benchmark, namely LINPACK 1000. A set of translation and optimization steps have been applied to transform a sequential C description of the LINPACK benchmark, based on a monolithic memory model, into a parallel Handel-C description that utilizes the plurality of memory resources available on a realistic reconfigurable computing platform. The experimental {{results show that the}} latest generation of FPGAs, programmed using Handel-C, can achieve a sustained floating point performance up to 6 times greater than the microprocessor while operating at a clock frequency that is 60 times lower. The transformations are applied in a way that could be generalized, allowing efficient <b>compilation</b> <b>approaches</b> for the mapping of high level descriptions onto FPGAs. 1...|$|E
40|$|We {{describe}} an {{implementation of the}} PowerPC architecture using dynamic compilation techniques to an optimized VLIW target architecture called BOA. BOA is a variable length VLIW architecture with priority given not to minimizing cycles per instruction (CPI) but to maximizing processor frequency. By limiting the size of individual processor cores, multiples {{of them can be}} placed on a single die for SMP-on-a-chip configurations. BOA's dynamic optimization offers significant advantages over purely static <b>compilation</b> <b>approaches</b> like those Intel and Hewlett-Packard currently propose for the IA- 64 architecture. Reliance on purely static profiling makes it impossible to adapt to changes in program usage. In addition, static profiling requires that independent software vendors (ISVs) perform extensive profiling and generate different executables optimized for a particular processor generation. Thus, the use of dynamic optimization enables two goals, (1) achieving efficient adaptation to specific workload behavior, and (2) optimizing for a specific implementation of the target platform...|$|E
40|$|Abstract. We {{present a}} logic-based {{framework}} for automated skill matching, {{able to return}} a ranked referral list and the related ranking explanation. Thanks to a Knowledge <b>Compilation</b> <b>approach,</b> a knowledge base in Description Logics is translated into a relational database, without loss of information. Skill matching inference services are then efficiently executed via SQL queries. Experimental results for scalability and turnaround times on large scale data sets are reported, confirming {{the validity of the}} approach. ...|$|R
40|$|In this paper, {{we present}} a {{software}} <b>compilation</b> <b>approach</b> for microprocessor/FPGA platforms that partitions a software binary onto custom hardware implemented in the FPGA. Our approach imposes less restrictions on software tool flow than previous compiler approaches, allowing software designers to use any software language and compiler. Our approach uses a back-end partitioning tool that utilizes decompilation techniques to recover important high-level information, resulting in performance comparable to high-level compiler-based approaches. 1...|$|R
25|$|Java {{bytecode}} {{can either}} be interpreted at run time by a virtual machine, {{or it can be}} compiled at load time or runtime into native code which runs directly on the computer's hardware. Interpretation is slower than native execution, and compilation at load time or runtime has an initial performance penalty for the compilation. Modern performant JVM implementations all use the <b>compilation</b> <b>approach,</b> so after the initial startup time the performance is equivalent to native code.|$|R
40|$|Abstract—Computational {{complexity}} {{has been}} the primary challenge of many VLSI CAD applications. The emerging multicore and many-core microprocessors {{have the potential to}} offer scalable performance improvement. How to explore the multicore resources to speed up CAD applications is thus a natural question but also a huge challenge for CAD researchers. Indeed, decades of work on general-purpose <b>compilation</b> <b>approaches</b> that automatically extracts parallelism from a sequential program has shown limited success. Past work has shown that programming model and algorithm design methods have a great influence on usable parallelism. In this paper, we propose a methodology to explore concurrency via nondeterministic transactional algorithm design, and to program them on multicore processors for CAD applications. We apply the proposed methodology to the min-cost flow problem which has been identified as the key problem in many design optimizations, from wire-length optimization in detailed placement to timing-constrained voltage assignment. A concurrent algorithm and its implementation on multicore processors for min-cost flow have been developed based on the methodology. Experiments on voltage island generation in floorplanning demonstrated its efficiency and scalable speedup over different number of cores...|$|E
40|$|This book {{describes}} {{for researchers}} {{in the fields of}} compiler technology, design and test, and electronic design automation the new area of digital microfluidic biochips (DMBs), and thus offers a new application area for their methods.   The authors present a routing-based model of operation execution, along with several associated <b>compilation</b> <b>approaches,</b> which progressively relax the assumption that operations execute inside fixed rectangular modules.   Since operations can experience transient faults during the execution of a bioassay, the authors show how to use both offline (design time) and online (runtime) recovery strategies. The book also presents methods for the synthesis of fault-tolerant application-specific DMB architectures. ·         Presents the current models used for the research on compilation and synthesis techniques of DMBs in a tutorial fashion; ·         Includes a set of “benchmarks”, which are presented in great detail and includes the source code of most of the techniques presented, including solutions to the basic compilation and synthesis problems; ·         Discusses several new research problems in detail, using numerous examples. ...|$|E
30|$|Very long {{instruction}} word- (VLIW-) based processors {{have become}} widely adopted {{as a basic}} building block in modern System-on-Chip designs. Advances in clustered VLIW architectures have extended the scalability of the VLIW architecture paradigm to {{a large number of}} functional units and very-wide-issue widths. A central challenge with wide-issue clustered VLIW architecture is the availability of programming and automated compiler methods that can fully utilize the available computational resources. Existing <b>compilation</b> <b>approaches</b> for clustered-VLIW architectures are based on extensions of previously developed scheduling algorithms that primarily focus on the maximization of instruction-level parallelism (ILP). However, many applications do not have sufficient ILP to fully utilize a large number of functional units. On the other hand, many applications in digital communications and multimedia processing exhibit enormous amounts of data-level parallelism (DLP). For these applications, the streaming programming paradigm has been developed to explicitly expose coarse-grained data-level parallelism as well as the locality of communication between coarse-grained computation kernels. In this paper, we investigate the mapping of stream programs to wide-issue clustered VLIW processors. Our work enables designers to leverage their existing investments in VLIW-based architecture platforms to harness the advantages of the stream programming paradigm.|$|E
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceIn this paper, we present a {{software}} <b>compilation</b> <b>approach</b> for microprocessor/FPGA platforms that partitions a software binary onto custom hardware implemented in the FPGA. Our approach imposes less restrictions on software tool flow than previous compiler approaches, allowing software designers to use any software language and compiler. Our approach uses a back-end partitioning tool that utilizes decompilation techniques to recover important high-level information, resulting in performance comparable to high-level compiler-based approaches...|$|R
50|$|Java {{bytecode}} {{can either}} be interpreted at run time by a virtual machine, {{or it can be}} compiled at load time or runtime into native code which runs directly on the computer's hardware. Interpretation is slower than native execution, and compilation at load time or runtime has an initial performance penalty for the compilation. Modern performant JVM implementations all use the <b>compilation</b> <b>approach,</b> so after the initial startup time the performance is equivalent to native code.|$|R
40|$|DOSE {{is unique}} among structu. re editor {{generators}} in its interpretive approach. This approach leads to very fast turn-around time for changes and provides multi-language facilities for no additional effort or cost. This article compares the interpretive <b>approach</b> to the <b>compilation</b> <b>approach</b> of other structu. re editor generators. It describes {{some of the}} design and implementation decisions made and remade during this project and the lessons learned. It emphasizes {{the advantages and disadvantages}} of DOSE with respect to other structure editing systems...|$|R
40|$|This paper {{introduces}} a novel method for automatically tuning {{the selection of}} compiler flags to optimize the performance of software intended to run on embedded hardware platforms. We begin by developing our approach on code compiled by the GNU C Compiler (GCC) for the ARM Cortex-M 3 (CM 3) processor; and we show how our method outperforms the industry standard -O 3 optimization level across a diverse embedded benchmark suite. First we quantify the potential gains by using existing iterative <b>compilation</b> <b>approaches</b> that time-intensively search for optimal configurations for each benchmark. Then we adapt iterative compilation to output a single configuration that optimizes performance across the entire benchmark suite. Although this is a time-consuming process, our approach constructs an optimized variation of -O 3, which we call -Ocm 3, that realizes nearly two thirds of known available gains on the CM 3 and significantly outperforms a more complex state-of-the-art predictive method in cross-validation experiments. Finally, we demonstrate our method on additional platforms by constructing two more optimization levels that find even more significant speed-ups on the ARM Cortex-A 8 and 8 -bit AVR processors...|$|E
40|$|The {{evaluation}} of a query over a probabilistic database boils down to computing {{the probability of a}} suitable Boolean function, the lineage of the query over the database. The method of query <b>compilation</b> <b>approaches</b> the task in two stages: first, the query lineage is implemented (compiled) in a circuit form where probability computation is tractable; and second, the desired probability is computed over the compiled circuit. A basic theoretical quest in query compilation is that of identifying pertinent classes of queries whose lineages admit compact representations over increasingly succinct, tractable circuit classes. Fostering previous work by Jha and Suciu (2012) and Petke and Razgon (2013), we focus on queries whose lineages admit circuit implementations with small treewidth, and investigate their compilability within tame classes of decision diagrams. In perfect analogy with the characterization of bounded circuit pathwidth by bounded OBDD width, we show that a class of Boolean functions has bounded circuit treewidth if and only if it has bounded SDD width. Sentential decision diagrams (SDDs) are central in knowledge compilation, being essentially as tractable as OBDDs but exponentially more succinct. By incorporating constant width SDDs and polynomial size SDDs, we refine the panorama of query compilation for unions of conjunctive queries with and without inequalities...|$|E
40|$|Reconfigurable {{computing}} {{has been}} an active field of research {{for the past two}} decades. Coarse-Grained Reconfigurable Architectures (CGRAs) are gaining interest for embedded systems and multimedia applications, which demand a flexible but highly efficient platform. A CGRA comprises a network of simple programmable processing elements (PEs). CG RAs exploit the inherent parallelism and repetitive computations found in these applications and can adapt themselves to diverse computations by dynamically changing configurations. Although CGRAs have the potential to exploit both hardware like efficiency and software like extensibility, the absence of proper <b>compilation</b> <b>approaches</b> is an obstacle to their widespread use. [...] In this thesis a novel approach for compiling parallel applications to a target CGRA will be presented. The application will be written in HARPO/L, a parallel object oriented language suitable for hardware. HARPO/L is first compiled to a Data Flow Graph (DFG) representation. The remaining compilation steps are a combination of three tasks: scheduling, placement and routing. For compiling cyclic portions of the application, we have adapted a modulo scheduling algorithm: modulo scheduling with integrated register spilling, which incorporates register spilling with instruction scheduling. For scheduling, the nodes of the DFG are ordered using the hypernode reduction modulo scheduling (HRMS) method. The placement and routing is done using the neighborhood relations of the PEs...|$|E
40|$|Currently, {{run-time}} {{operating systems}} {{are widely used}} to implement concurrent embedded applications. This runtime approach to multi-tasking and inter-process communication can introduce significant overhead to execution times and memory requirements [...] prohibitive in many cases for embedded applications where processor and memory resources are scarce. In this paper, we present a static <b>compilation</b> <b>approach</b> that generates ordinary C programs at compile-time that can be readily retargeted to different processors, without including or generating a runtime scheduler. Our method {{is based on a}} novel Petri net theoretic approach...|$|R
50|$|Comparisons of {{election}} campaign communication {{can either be}} conducted by one team looking at different countries (leading to theoretical diversity) or by scholars compiling case studies and drawing conclusions from them. Drawing conclusions from a compilation of case studies can lead to methodological problems. Country experts provide single report about the election campaign communication in different countries, eventually following guidelines designed by the editors. The methodological problem refers {{to the lack of}} a unified data set, i.e. the research within the countries does not use equivalent methods and measurements especially designed for the sample of countries (either using a more etic or emic approach to achieve equivalence). To deal with methodological problems rising from a <b>compilation</b> <b>approach,</b> several aspects have to be considered: creation of chapter homogeneity by following a guideline, awareness for the logic of comparative research, and creation of a systematized synthesis when drawing conclusions. Nevertheless, the validity of cross-national conclusions remains comprised, since no aggregate data set, derived from identical research conduction, exists. Nevertheless, the <b>compilation</b> <b>approach</b> can provide an insight into communication differences in election campaigns around the globe. Swanson and Mancini published such a compilation in 1996 to compare election campaign styles of eleven countries.|$|R
40|$|CSL is a {{programming}} language designed {{for use in the}} field of complex logical problems. The basic approach adopted is that groups of items or objects sharing similar qualities can best be represented intensively, by recording the names of these items in predicative groups or sets, rather than by listing them extensively and coding their qualities numerically. The first application of the language has been in the field of Monte Carlo simulation work, for which special facilities are provided. An unusual <b>compilation</b> <b>approach</b> has been followed, involving translation into FORTRAN as an intermediate stage...|$|R
40|$|International audienceHigh-level {{concurrency}} constructs and abstractions {{have several}} well-known software engineering advantages {{when it comes}} to programming concurrency protocols among threads in multicore applications. To also explore their complementary performance advantages, in ongoing work, we are developing compilation technology for a high-level coordination language, Reo, based on this language’s formal automaton semantics. By now, as shown in our previous work, our tools are capable of generating code that can compete with carefully hand-crafted code, at least for some protocols. An important prerequisite to further advance this promising technology, now, is {{to gain a better understanding}} of how the significantly different <b>compilation</b> <b>approaches</b> that we developed so far, which vary in the amount of parallelism in their generated code, compare against each other. For instance, to better and more reliably tune our compilers, we must learn under which circumstances parallel protocol code, with high throughput but also high latency, outperforms sequential protocol code, with low latency but also low throughput. In this paper, we report on an extensive performance comparison between these approaches for a substantial number of protocols, expressed in Reo. Because we have always formulated our compilation technology in terms of a general kind of communicating automaton (i. e., constraint automata), our findings apply not only to Reo but, in principle, to any language whose semantics can be defined in terms of such automata...|$|E
40|$|Very long {{instruction}} word- (VLIW-) based processors {{have become}} widely adopted {{as a basic}} building block in modern Systemon-Chip designs. Advances in clustered VLIW architectures have extended the scalability of the VLIW architecture paradigm to {{a large number of}} functional units and very-wide-issue widths. A central challenge with wide-issue clustered VLIW architecture is the availability of programming and automated compiler methods that can fully utilize the available computational resources. Existing <b>compilation</b> <b>approaches</b> for clustered-VLIW architectures are based on extensions of previously developed scheduling algorithms that primarily focus on the maximization of instruction-level parallelism (ILP). However, many applications do not have sufficient ILP to fully utilize a large number of functional units. On the other hand, many applications in digital communications and multimedia processing exhibit enormous amounts of data-level parallelism (DLP). For these applications, the streaming programming paradigm has been developed to explicitly expose coarse-grained data-level parallelism as well as the locality of communication between coarse-grained computation kernels. In this paper, we investigate the mapping of stream programs to wide-issue clustered VLIW processors. Our work enables designers to leverage their existing investments in VLIWbased architecture platforms to harness the advantages of the stream programming paradigm. Copyright © 2008 S. Yan and B. Lin. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|E
40|$|Abstract—Microfluidic-based biochips are {{replacing}} the con-ventional biochemical analyzers, {{and are able}} to integrate on-chip all the necessary functions for biochemical analysis using microfluidics. The digital microfluidic biochips are based on the manipulation of liquids not as a continuous flow, but as discrete droplets on an array of electrodes. Microfluidic operations, such as transport, mixing, split, are performed on this array by routing the corresponding droplets on a series of electrodes. Several approaches have been proposed for the compilation of digital microfluidic biochips, which, starting from a biochemical application and a given biochip architecture, determine the allocation, resource binding, scheduling, placement and routing of the operations in the application. To simplify the compilation problem, researchers have assumed an abstract droplet size of one electrode. However, the droplet size abstraction is not realistic and it impacts negatively the execution of the biochemical application, leading in most cases to its failure. Hence the existing <b>compilation</b> <b>approaches</b> have to be revisited to consider the size of the droplets. In this paper we take the first step towards a droplet size-aware compilation by proposing a routing algorithm that considers the droplet size. Our routing algorithm is developed for a novel digital microfluidic biochip architecture based on Active Matrix Electrowetting on Dielectric, which uses a thin film transistor array for the electrodes. We also implement a simulator that allows us to perform the needed adaptations and to validate the proposed routing algorithm. I...|$|E
40|$|Knowledge {{compilation}} {{by using}} the extension rule (KCER) has been recognized as a novel <b>compilation</b> <b>approach,</b> although this method can only deal with static knowledge bases. This paper proposes a novel incremental knowledge compilation method {{by using the}} extension rule {{so that it can}} tackle knowledge compilation problems in the dynamic environment. The method does not recompile the whole knowledge base, while it is to use the information of compiling original knowledge base to speed up the compiling process. The experimental results show that this method can deal with dynamic knowledge bases efficientl...|$|R
40|$|Interactive configurators are {{decision}} support systems assist-ing users in selecting values for parameters that respect given constraints. The underlying {{knowledge can be}} conveniently formulated as a Constraint Satisfaction Problem where the constraints are propositional formulas. The problem of in-teractive configuration was originally inspired by the prod-uct configuration problem {{with the emergence of}} the mass-customization paradigm in product manufacturing, but has also been applied to other tasks requiring user interaction, such as specifying services or setting up complex equipment. The user-friendly requirements of complete, backtrack-free and real-time interaction makes the problem computationally challenging. Therefore, it is beneficial to compile the con-figuration constraints into a tractable representation such as Binary Decision Diagrams (BDD) (Bryant 1986) to support efficient user interaction. The compilation deals with the NP-hardness such that the online interaction is in polynomial time {{in the size of the}} BDD. In this paper we address the problem of extending con-figurators so that a user can interactively limit configura-tion choices based on a maximum cost (such as price or weight of a product) of any valid configuration, in a com-plete, backtrack-free and real-time manner. The current BDD <b>compilation</b> <b>approach</b> is not adequate for this purpose, since adding the total cost information to the constraints descrip-tion can dramatically increase the size of the compiled BDD. We show how to extend this <b>compilation</b> <b>approach</b> to solve the problem while keeping the polynomial time guarantees...|$|R
40|$|Abstract—Hardware {{supported}} multithreading can mask memory latency {{by switching}} the execution to ready threads, which is particularly effective on irregular applications. FPGAs {{provide an opportunity}} to have multithreaded data paths customized toeach individual application. In this paper we describe the compiler generation of these hardware structures from a C subset targeting a Convey HC- 2 ex machine. We describe how this <b>compilation</b> <b>approach</b> differs from other C to HDL compilers. We use the compiler to generate a multithreaded sparse matrix vector multiplication kernel and compare its performance to existing FPGA, and highly optimized software implementations. I...|$|R
