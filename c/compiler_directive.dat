23|132|Public
25|$|These are, if necessary, {{converted}} {{to the middle}} end's input representation, called GENERIC form; the middle end then gradually transforms the program towards its final form. Compiler optimizations and static code analysis techniques (such as FORTIFY_SOURCE, a <b>compiler</b> <b>directive</b> that attempts to discover some buffer overflows) are applied to the code. These work on multiple representations, mostly the architecture-independent GIMPLE representation and the architecture-dependent RTL representation. Finally, machine code is produced using architecture-specific pattern matching originally based on an algorithm of Jack Davidson and Chris Fraser.|$|E
5000|$|... • Counts the <b>compiler</b> <b>directive</b> SLOC (using CountDirectiveSLOC method).|$|E
5000|$|... #Caption: FORTRAN Port-A-Punch card. <b>Compiler</b> <b>directive</b> [...] "SQUEEZE" [...] {{removed the}} {{alternating}} blank columns from the input.|$|E
5000|$|<b>Compiler</b> <b>directives</b> for {{recommended}} {{distributions of}} array data ...|$|R
5000|$|In Ada, <b>compiler</b> <b>directives</b> {{are called}} pragmas (short for [...] "pragmatic information").|$|R
5000|$|Verilog HDL : The backquote is {{used at the}} {{beginning}} of <b>compiler's</b> <b>directives.</b>|$|R
50|$|A {{reserved}} word is any identifier or symbol that the Action! compiler recognizes as something special. It {{can be an}} operator, a data type name, a statement, or a <b>compiler</b> <b>directive.</b>|$|E
5000|$|A pragma is a <b>compiler</b> <b>directive</b> that conveys {{information}} to the compiler to allow specific manipulation of compiled output. [...] Certain pragmas are built into the language while other are implementation-specific.|$|E
5000|$|The GNAT Ada {{compiler}} can automate conformance {{checks for}} some GPL software license issues via a <b>compiler</b> <b>directive.</b> Use [...] {{to activate the}} check against the Modified GPL. The GNAT Reference Manual [...] documents the License [...] pragma along with other compiler directives.|$|E
5000|$|Oracle Corporation's PL/SQL {{procedural}} language includes {{a set of}} <b>compiler</b> <b>directives,</b> known as [...] "pragmas".|$|R
50|$|In the C and C++ {{programming}} languages, a header file is a file whose text {{is included}} in another source file by the compiler, usually {{by the use of}} <b>compiler</b> <b>directives</b> {{at the beginning of the}} source file. A prefix header differs from a normal header file in that it is automatically included at the beginning of every source file by the compiler, without the use of any <b>compiler</b> <b>directives.</b>|$|R
40|$|A {{commonly}} used approach to develop parallel programs is to augment a sequential program with <b>compiler</b> <b>directives</b> that indicate which program blocks may potentially be executed in parallel. This paper develops a verification technique to prove correctness of <b>compiler</b> <b>directives</b> combined with functional correctness of the program. We propose syntax and semantics {{for a simple}} core language, capturing the main forms of deterministic parallel programs. This language distinguishes three kinds of basic blocks: parallel, vectorized and sequential blocks, which can be composed using three different composition operators: sequential, parallel and fusion composition. We show that it is sufficient to have contracts for the basic blocks to prove correctness of the <b>compiler</b> <b>directives,</b> and moreover that functional correctness of the sequential program implies correctness of the parallelized program. We formally prove correctness of our approach. In addition, we define a widely-used subset of OpenMP that can be encoded into our core language, thus effectively enabling the verification of OpenMP <b>compiler</b> <b>directives,</b> and we discuss automated tool support for this verification process...|$|R
5000|$|... @interface Pen : NSObject@property (copy) NSColor *colour; // The [...] "copy" [...] {{attribute}} {{causes the}} object's copy to be // retained, {{instead of the}} original.@end@implementation Pen@synthesize colour; // <b>Compiler</b> <b>directive</b> to synthesise accessor methods. // It can be left behind in Xcode 4.5 and later.@end ...|$|E
5000|$|In fact it {{was often}} the case that several stropping {{conventions}} might be in use within one language. For example, in ALGOL 68, the choice of stropping convention can be specified by a <b>compiler</b> <b>directive</b> (in ALGOL terminology, a [...] "pragmat"), namely POINT, UPPER, QUOTE, or RES: ...|$|E
5000|$|Visual Basic {{uses them}} {{whenever}} the variable is of type Object and the <b>compiler</b> <b>directive</b> [...] "Option Strict Off" [...] is in force. This is the default {{setting for a}} new VB project. Prior to version 9, only [...]NET and COM objects could be late bound. With VB 10, this has been extended to DLR-based objects.|$|E
40|$|This paper {{presents}} {{a model to}} evaluate the performance and overhead of parallelizing sequential code using <b>compiler</b> <b>directives</b> for multiprocessing on distributed shared memory (DSM) systems. With increasing popularity of shared address space architectures, {{it is essential to}} understand their performance impact on programs that benefit from shared memory multiprocessing. We present a simple model to characterize the performance of programs that are parallelized using <b>compiler</b> <b>directives</b> for shared memory multiprocessing. We parallelized the sequential implementation of NAS benchmarks using native Fortran 77 <b>compiler</b> <b>directives</b> for an Origin 2000, which is a DSM system based on a cache-coherent Non Uniform Memory Access (ccNUMA) architecture. We report measurement based performance of these parallelized benchmarks from four perspectives: efficacy of parallelization process; scalability; parallelization overhead; and comparison with hand-parallelized and -optimized version of the same benchmarks. Our results indicate that sequential programs can conveniently be parallelized for DSM systems using <b>compiler</b> <b>directives</b> but realizing performance gains as predicted by the performance model depends primarily on minimizing architecture-specific data locality overhead...|$|R
40|$|In this report, {{we present}} X-Kaapi’s {{programming}} model. A X-Kaapi parallel {{program is a}} C or C++ sequential program with code annotation using #pragma <b>compiler</b> <b>directives</b> that allow to create tasks. A specific source to source <b>compiler</b> translates X-Kaapi <b>directives</b> to runtime calls. Key-words: parallel computing, data flow graph, scheduling, X-Kaap...|$|R
5000|$|Another notable token is [...] (C# directives), {{respectively}} [...] (Visual Basic directives), used in Microsoft Visual Studio Code Editor. These {{are treated}} syntactically as <b>compiler</b> <b>directives,</b> {{though they do}} not affect compilation.|$|R
5000|$|With {{the release}} of version 3.0 of NSIS, Unicode support can be {{implemented}} using the <b>compiler</b> <b>directive</b> [...] "Unicode true". This gives full Unicode support with no further code changes, but the installer will not run under Windows 95/98/Me. As of 2016 before the 3.0 release NSIS was available in the PortableApps format for Unicode 2.46.5 Rev 3 and ANSI 2.51.|$|E
50|$|These are, if necessary, {{converted}} {{to the middle}} end's input representation, called GENERIC form; the middle end then gradually transforms the program towards its final form. Compiler optimizations and static code analysis techniques (such as FORTIFY_SOURCE, a <b>compiler</b> <b>directive</b> that attempts to discover some buffer overflows) are applied to the code. These work on multiple representations, mostly the architecture-independent GIMPLE representation and the architecture-dependent RTL representation. Finally, machine code is produced using architecture-specific pattern matching originally based on an algorithm of Jack Davidson and Chris Fraser.|$|E
5000|$|The {{section of}} code that {{is meant to}} run in {{parallel}} is marked accordingly, with a <b>compiler</b> <b>directive</b> that will cause the threads to form before the section is executed. Each thread has an id attached to it which can be obtained using a function (called [...] ). The thread id is an integer, and the master thread has an id of 0. After {{the execution of the}} parallelized code, the threads join back into the master thread, which continues onward {{to the end of the}} program.|$|E
50|$|Many languages, {{including}} Java {{and functional}} languages, {{do not provide}} language constructs for inline functions, but their compilers or interpreters often do perform aggressive inline expansion. Other languages provide constructs for explicit hints, generally as <b>compiler</b> <b>directives</b> (pragmas).|$|R
5000|$|Function {{definitions}} {{may include}} <b>compiler</b> <b>directives,</b> known as declarations, which provide hints to the compiler about optimization settings or the data types of arguments. They may also include documentation strings (docstrings), which the Lisp system may use to provide interactive documentation: ...|$|R
5|$|Mainstream {{parallel}} programming languages remain either explicitly parallel or (at best) partially implicit, {{in which a}} programmer gives the <b>compiler</b> <b>directives</b> for parallelization. A few fully implicit {{parallel programming}} languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.|$|R
5000|$|C and C++ have an [...] keyword, which {{functions}} {{both as a}} <b>compiler</b> <b>directive</b> - specifying that inlining {{is desired}} but not required - and also changes the visibility and linking behavior. The visibility change is necessary to allow the function to be inlined via the standard C toolchain, where compilation of individual files (rather, translation units) is followed by linking: for the linker {{to be able to}} inline functions, they must be specified in the header (to be visible) and marked [...] (to avoid ambiguity from multiple definitions).|$|E
5000|$|Compilers use {{a variety}} of {{mechanisms}} to decide which function calls should be inlined; these can include manual hints from programmers for specific functions, together with overall control via command-line options. Inlining is done automatically by many compilers in many languages, based on judgment of whether inlining is beneficial, while in other cases it can be manually specified via compiler directives, typically using a keyword or <b>compiler</b> <b>directive</b> called [...] Typically this only hints that inlining is desired, rather than requiring inlining, with the force of the hint varying by language and compiler.|$|E
5000|$|In the C and C++ {{programming}} languages, {{an inline}} function is one qualified with the keyword this serves two purposes. Firstly, {{it serves as}} a <b>compiler</b> <b>directive</b> that suggests (but does not require) that the compiler substitute the body of the function inline by performing inline expansion, i.e. by inserting the function code at the address of each function call, thereby saving the overhead of a function call. In this respect it is analogous to the [...] storage class specifier, which similarly provides an optimization hint.The second purpose of [...] is to change linkage behavior; the details of this are complicated. This is necessary due to the C/C++ separate compilation + linkage model, specifically because the definition (body) of the function must be duplicated in all translation units where it is used, to allow inlining during compiling, which, if the function has external linkage, causes a collision during linking (it violates uniqueness of external symbols). C and C++ (and dialects such as GNU C and Visual C++) resolve this in different ways.|$|E
50|$|Like in OpenMP, the {{programmer}} can annotate C, C++ and Fortran {{source code}} {{to identify the}} areas that should be accelerated using <b>compiler</b> <b>directives</b> and additional functions. Like OpenMP 4.0 and newer, code can be started on both the CPU and GPU.|$|R
50|$|Mainstream {{parallel}} programming languages remain either explicitly parallel or (at best) partially implicit, {{in which a}} programmer gives the <b>compiler</b> <b>directives</b> for parallelization. A few fully implicit {{parallel programming}} languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.|$|R
40|$|OpenMP is an {{application}} programming interface for parallelizing sequential programs written in C, C++, and Fortran on shared-memory platforms. It provides a collection of <b>compiler</b> <b>directives,</b> a runtime library, and environment variables to enable programmers to specify the parallelism they desire to exploit in a program. DISCUSSIO...|$|R
40|$|Syntax Three (AST) [16]. Pretty [...] {{printing}} of code is performed during AST walking: FORTRAN keywords {{are written in}} capital letters, line printing is nested for statements within the body of language constructs, and comments {{are not included in}} the produced code since they are discarded by the NANOSCOMPILER front-end. Of course directive lines are kept for <b>compiler</b> <b>directive</b> processing...|$|E
30|$|A single <b>compiler</b> <b>directive</b> for the Storm- 1 {{processor}} can specify up to 64 concurrent data-parallel computations (four per lane) {{by using}} its 8 -bit packed-data mode. This allows simultaneous processing of four check nodes per lane which yields {{a total of}} 64 concurrent row updates across the 16 lanes of the processor. Thus the row-update data parallelism available with the compiler and the architecture in 8 -bit mode is fully exploited if the dimension of the permutation submatrices is an integer multiple of 64.|$|E
40|$|This paper {{presents}} the results of a recent collaborative project between EPCC at the University of Edinburgh and Scottish engineering consultants Weidlinger Associates Ltd. The aim of this work was to extend the solver capabilities, and in particular the parallel solver capabilities, of an industrial modelling code, PZFlex. Two key solvers were compared, a preconditioned conjugate gradient (PCG) iterative solver and a state-of-the-art direct solver (MFACT). The PCG solver was parallelised using OpenMP, the new standard for <b>compiler</b> <b>directive</b> based programming of shared memory multiprocessor systems. Our results show that OpenMP delivers parallel efficiencies of up to 80 % for the PCG solver. It also offers good scalability, allowing the simple PCG algorithm to beat the complex MFACT direct solver for typical simulation runs of PZFlex. ...|$|E
5000|$|Many {{programming}} languages support conditional compilation. Typically <b>compiler</b> <b>directives</b> define or [...] "undefine" [...] certain variables; other directives test {{these variables}} and modify compilation accordingly. For example, not using an actual language, the compiler may {{be set to}} define [...] "Macintosh" [...] and undefine [...] "PC", and the code may contain: ...|$|R
50|$|In {{addition}} to libraries, <b>compiler</b> <b>directives,</b> CUDA C/C++ and CUDA Fortran, the CUDA platform supports other computational interfaces, including the Khronos Group's OpenCL, Microsoft's DirectCompute, OpenGL Compute Shaders and C++ AMP. Third party wrappers {{are also available}} for Python, Perl, Fortran, Java, Ruby, Lua, Haskell, R, MATLAB, IDL, and native support in Mathematica.|$|R
50|$|OpenMP (Open Multi-Processing) is an {{application}} programming interface (API) that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, on most platforms, instruction set architectures and operating systems, including Solaris, AIX, HP-UX, Linux, macOS, and Windows. It consists {{of a set of}} <b>compiler</b> <b>directives,</b> library routines, and environment variables that influence run-time behavior.|$|R
