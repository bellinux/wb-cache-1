16|51|Public
50|$|Power {{states are}} <b>computer</b> <b>dependent</b> and will vary from {{manufacturer}} to manufacturer. Power savings {{can be made}} {{in a number of ways}} including slowing/stopping the processor clock speed or shutting off power to complete sub-systems.|$|E
40|$|<b>Computer</b> <b>dependent</b> {{design space}} is {{illustrated}} {{with the recent}} international competition project for Kowloon Arts Center. The spatial condition of design resulted from the deployment of Euclidian geometry {{and use of the}} non-rational, complex surfaces. Another aspect of <b>computer</b> <b>dependent</b> space was rooted in the contemporary work situation where architects located on the three different continents collaborated in distributed mode over the internet. The resulting project confirms the creative potential of the above aspects of the digital space in contemporary design...|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Philosophy of Loughborough University. This research was initiated to investigate the syndrome of computer dependency, and to ascertain whether there was any foundation to the apocryphal stories which suggested that 'obsessive' dependency by some people upon computers and computing was detrimental to their psychological and social development. National publicity brought forth volunteers who considered themselves to be dependent upon computers. As a group they did not form a cross-section {{of the general population}} but consisted in the main of very well educated, adult males. (An additional study showed that there were distinct differences between the sexes in attitudes held towards computers to account for this lack of balance). As the <b>computer</b> <b>dependent</b> individuals were unrepresentative of the general population, control groups were established with whom comparisons could be made, matched with them on the criteria of sex, age and highest educational level. One control group was formed from computer owners who were not <b>computer</b> <b>dependent</b> and the other from people Who did not own a computer. Thus three groups were studied; a <b>computer</b> <b>dependent</b> group and two controls. The results established that the two computer-owning groups differed significantly from each other in their preferred computing activities, both quantitatively and qualitatively. As anticipated, the <b>computer</b> <b>dependent</b> individuals spent significantly more time computing than the others, but they were also found to use computers in a more exploratory and self-educational manner, rarely having a definite end-product in mind. All three groups were found to have enjoyed different types of hobbies throughout their lives. The <b>computer</b> <b>dependent</b> group had shown interests in technological and scientific artefacts before school age and rarely partook of either the social or physical activities of interest to the control groups. The dependent group had found in the computer the ultimate hobby; one which was constantly stimulating and exciting and which matched their psychological needs. Investigation of the social and psychological issues suggested that the group of <b>computer</b> <b>dependent</b> people had experienced different types of parenting from the control groups, leading them to become object- rather than people-centred at an early age. This bias had been perpetuated throughout life, leaving them shy and unable to form satisfactory relationships; they neither trusted humans nor needed them in many cases. Their lives had become dominated by task- and object-related activities, with the computer offering them a controllable form of interaction Which they had been unable to find elsewhere. Deleterious effects occurred within some marriages Where one spouse had become <b>computer</b> <b>dependent,</b> but only en very rare occasions did individuals express distress about their dependency. Tb the contrary, the positive benefits gained by their use of computers far outweighed any disadvantages. The research disproved the hypothesis that computer dependency was in general detrimental to the individuals' social and psychological development, and suggested that computer dependency was in fact therapeutic by providing an outlet for their high levels of curiosity and originality. Computing had brought them intellectual stimulation rarely found when interacting with the majority of humans and had provided a level of fulfilment to which many would aspire...|$|E
5000|$|I {{know that}} kids play on <b>computers.</b> (a <b>dependent</b> (subordinate) clause, but still finite) ...|$|R
5000|$|A {{fat client}} (also called heavy, rich or thick client) is a {{computer}} (client) in client-server architecture or networks that typically provides rich functionality independent of the central server. Originally known as just a [...] "client" [...] or [...] "thick client" [...] the name is contrasted to thin client, which describes a <b>computer</b> heavily <b>dependent</b> on a server's applications.|$|R
50|$|With {{the small}} {{transistor}} at their hands, electrical engineers of the 1950s saw {{the possibilities of}} constructing far more advanced circuits. However, as the complexity of circuits grew, problems arose. One problem {{was the size of}} the circuit. A complex circuit like a <b>computer</b> was <b>dependent</b> on speed. If the components were large, the wires interconnecting them must be long. The electric signals took time to go through the circuit, thus slowing the computer.|$|R
40|$|The exact FORTRAN {{computer}} program BUNVIS-RG which uses an exact stiffness matrix method to find eigenvalues {{and modes of}} three-dimensional planes, is presented in detail. It is shown that BUNVIS-RG solution times can be predicted fairly accurately from two <b>computer</b> <b>dependent</b> constants, the number of nodes and members in the frame, and the maximum node number difference of any pair of connected nodes. The program's use of repetitive geometry, stayed column substructures and nonuniform member options was found to enhance efficiency and range of application...|$|E
40|$|Educated {{people are}} aware that {{disaster}} planning is a must in today’s <b>computer</b> <b>dependent</b> society. Nevertheless, when Hurricane Katrina hit the Gulf Coast of the United States in August of 2005, many businesses were surprised by the severity of this natural disaster. Disaster planning has been redefined by this major event and several lessons learned have enabled companies to be better prepared for future events. In today’s increasingly <b>computer</b> <b>dependent</b> society, {{it is not a}} question of if disaster will happen, it is a question of when. With regard to Hurricane Katrina, perhaps one of the greatest sources of economic hardship and loss came from the disruption to the businesses in the Gulf Coast region that were faced with damaged facilities, displaced employees, and business interruption. A surprising number of companies were not prepared for such a disruption and had not tested their total plan for disaster recovery. Why were businesses not more prepared to recover quickly and continue operating in the face of this disaster? Why did they not have plans in place to account for and protect employees and their families, keeping them safe and productive? This paper discusses basic principles of disaster planning that have been enhanced by lessons learned from Katrina, and offers solutions for the future...|$|E
40|$|Accelerometer {{location}} {{analysis for}} the modal survey test of the International Space Station Node is described. Three different approaches were utilized: (1) Guyan reduction; (2) Iterative Guyan reduction; and (3) The average driving point residue (ADPR) method. Both Guyan approaches worked well, but poor results were observed for the ADPR method. Although the iterative Guyan approach appears {{to provide the best}} set of sensor locations, it is intensive computationally, becoming impractical for large initial location sets. While this is <b>computer</b> <b>dependent,</b> it appears that initial sets larger than about 1500 degrees of freedom are impractical for the iterative technique...|$|E
40|$|This {{research}} {{was done at}} Polytechnic State of Sriwijaya and aimed at understanding the Influences of computer anxiety and computer experience on computer self-efficacy [...] This study uses data collected by a survey on active student at Polytechnic State of Sriwijaya, using simple random sampling method with amount of sample 100 respondent. The data analysis using SPSS software and AMOS. The independent variables are computer anxiety and <b>computer</b> experience. <b>Dependent</b> variable is <b>computer</b> self-efficacy. The study results indicate that computer anxiety had significant and negative influence on computer self-efficacy. Then, computer experience had significant and positive influence on computer self-efficacy...|$|R
40|$|In this study, {{finite element}} {{simulation}} of tube extrusion {{process has been}} carried out considering different mesh adaptivity schemes. A comparison of these schemes has been made based on stress, strain distribution, and load-stroke curves. Based on the finite element results, it is observed that the success of the <b>computer</b> simulation is <b>dependent</b> on the mesh refinement criteria...|$|R
40|$|The {{purpose for}} {{conducting}} {{this study was}} to develop a practitioner list of computer competencies for use in the training of preservice teachers. The competencies were developed through a survey of preservice teachers, cooperating-teachers, and faculty members in Teachers Colleges at the Universities of Nebraska in Lincoln, Omaha, and Kearney. ^ By use of a stratified random sampling method, 50 teacher educators, 50 preservice teachers, and 50 cooperating teachers were selected. A total of 150 surveys were mailed to the sample [...] 50 to preservice teachers, 50 to cooperating teachers, and 50 to teacher educators. A total of 131 responses were returned [...] 42 from the teacher educators, 45 from the cooperating teachers, and 44 from the preservice teachers. Thus, the rate of return for the survey was 87 percent. ^ All respondents thought the ability to initialize and format diskettes, make backups of files and disks, and operate a mouse or trackball; the ability to set up and print; and the ability to load a program from a diskette and save it were the most important competencies. Computer programming competencies were rated as least important. To answer the second research question, a MANOVA procedure was used. Statistically significant differences were found for the three groups among the five <b>dependent</b> variables (<b>computer</b> programming, software, hardware, computer literacy, and computer application) (p 3 ̆c. 10, multivariate F = 1. 82). The Wilks test value was 0. 868. ^ A one-way analysis of variance (ANOVA) indicated that the type of respondent was significant for the <b>computer</b> application <b>dependent</b> variable only. The results of a Tukey follow-up test for the <b>computer</b> application <b>dependent</b> variable showed the mean scores of faculty members and cooperating teachers were significantly different at the 0. 05 level. Faculty members perceived the computer applications competencies to be significantly more important than cooperating teachers. ...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedThe computer {{program for the}} analysis of linearly elastic plane-stress or plane-strain problems devised by Felippa in his work on "Refined Finite Element Analysis of Linear and Nonlinear Two-dimensional Structures" has been modified to include the use of initial displacement boundary conditions. In addition the original IBM 7094 <b>computer</b> <b>dependent</b> program has been adapted for use on the IBM 360 / 65 computer. In both programs the FORTRAN IV language has been used. Problems involving "Poor fit" displacement boundary conditions and refined mesh analysis using coarse mesh analysis input displacements, which could not have been done with the original program, are now possible with the modified version presented herein. [URL] United States Nav...|$|E
40|$|A UCSD Pascal {{program was}} {{developed}} which can analyze nucle-ic acid dot matrices {{of up to}} 9500 x 9500 in size on the Apple II computer. Although matrices of such size consume large amounts of computer memory, this program minimizes these problems by ana-lyzing only small strips of the matrix at a time, and then trans-ferring the results to a floppy disk or printer. Compression and memory efficient code further enhance {{the size of the}} matrix that can be analyzed. By generating an image of the dot matrix using software, and sending this image directly to an Epson dot matrix printer, a very detailed print out may be produced. The program has a number of user selectable options which allow a great deal of control over the analysis. The program contains no <b>computer</b> <b>dependent</b> code, and thus should work on all systems that can run UCSD Pascal...|$|E
40|$|AbstractThe aim of {{the present}} reasearch is to expose a {{psychological}} profile of children from children's homes and social orphanages prone to computer dependency. 302 teenagers at the age from 11 to 16 from children's homes and social orphanages of the Republic of Tatarstan, the Russian Federation, {{took part in the}} empirical research, 80 of them were identified by the selected methods as <b>computer</b> <b>dependent</b> and inclined to get into a risk group. Besides, there were recruited 30 educators, social workers and teachers of these institutions. There has been applied the complex of diagnostic techniques to diagnose the level of teenagers’ computer dependency, to study their personal qualities and specific character of social intelligence, social and psychological adaptation, interpersonal relations, value-motivational, communicative and emotional spheres, self-assessment. On the basis of the results obtained by the research there has been defined a psychological profile of inclined to computer dependency teenagers brought up in children's homes and social orphanages. Empirical research statistical data processing was carried out by means of standard methods of mathematical statistics (Student's t-criteria of differences, methods of correlation data analysis) ...|$|E
40|$|New {{formulation}} for Nystrom integrators offers increased {{precision and}} speed of computation. Formulation takes advantage of capabilities of modern electronic <b>computers.</b> Independent and <b>dependent</b> variables and various partial and total derivatives expressed via compact tensor notation. In this approach, typical set of constraint equations derived in about 1 day. Solution of constraint equations speeded by SEARCH computer program. Is general-purpose optimization program, which maximizes or minimizes cost function defined by user. Used successfully, to make least-squares fit of about 4, 000 equations in 12 unknowns...|$|R
40|$|The {{widespread}} use of microcomputers has introduced the issue of computer ethics to a larger and more diverse population than {{was the case with}} traditional mainframe computing. An ever-increasing larger number of people must now deal on a daily basis with ethical issues involving the use of information technology. This paper addresses {{the question of whether the}} ethical views of <b>computer</b> users is <b>dependent</b> on the type of computer used. The results should be of value in focusing institutional efforts on addressing the ethical uses of technology...|$|R
40|$|Abstract, The {{successful}} {{development of}} quantum <b>computers</b> is <b>dependent</b> on identifying quantum systems {{to function as}} qubits. Paramagnetic states of point defects in semiconductors or insulators {{have been shown to}} provide an effective implementation, with the nitrogen-vacancy center in diamond being a prominent example. The spin- 1 ground state of this center can be initialized, manipulated, and read out at room temperature. Identifying defects with similar properties in other materials would add flexibility in device design and possibly lead to superior performance or greater functionality. A systematic search for defect-based qubits has been initiated, starting from a list of physical criteria that such centers and their hosts should satisfy. First-principles calculations of atomic and electronic structure are essential in supporting this quest: They provide a deeper understanding of defects that are already being exploited and allow efficient exploration of new materials systems and "defects by design. ...|$|R
40|$|Let _n(q) {{denote the}} unitriangular group of {{unipotent}} n× n upper triangular matrices over a finite field with cardinality q and prime characteristic p. It {{has been known}} for some time that when p is fixed and n is sufficiently large, _n(q) has "exotic" irreducible characters taking values outside the cyclotomic field (ζ_p). However, all proofs of this fact to date have been both non-constructive and <b>computer</b> <b>dependent.</b> In a preliminary work, we defined a family of orthogonal characters decomposing the supercharacters of an arbitrary algebra group. By applying this construction to the unitriangular group, we are able to derive by hand an explicit description of a family of characters of _n(q) taking values in arbitrarily large cyclotomic fields. In particular, we prove that if r is a positive integer power of p and n> 6 r, then _n(q) has an irreducible character of degree q^ 5 r^ 2 - 2 r which takes values outside (ζ_pr). By the same techniques, we are also able to construct explicit Kirillov functions which fail to be characters of _n(q) when n> 12 and q is arbitrary. Comment: 24 pages, 3 figure...|$|E
40|$|Academics in {{business}} science and elsewhere {{have begun to}} look at what Vaughan (1999) called the “dark side of organisations” and started to engage with the fact that people connected to cyber systems can be the source of great opportunity for exploitation. MacGillivray (2014) notes that organisations should be seen as socio-technical: where infrastructure and systems shape and are shaped by the people that work with them. The current paper puts these sociotechnical systems at the heart of cyber-attack and defence - where we see ‘cyber’ as being shorthand for any <b>computer</b> <b>dependent</b> technologies used to achieve dark effects on the human mind and subsequent behaviour (e. g. SMS received on a mobile phone). There is no logic to restrictively focussing on user behaviour around laptops and desktops. This paper provides some unconventional examples of cyber-attack. Our concern is with enabling decision-makers (or those supporting them) to challenge their assumptions about information received, adjust behaviours accordingly and thereby render them and their organisations increasingly resilient to the efforts of creative adversaries, no matter whether those adversaries motivation is commercial, political or personal. From such target-hardening arises organisational competitive advantage...|$|E
40|$|In {{this paper}} I {{describe}} a simple model of photon transport. This simple model includes: tabulated cross sections and average expected energy losses for all elements between hydrogen (Z = 1) and fermium (Z = 100) over the energy range 10 eV to 1 GeV, simple models to analytically describe coherent and incoherent scattering, {{and a simple}} model to describe fluorescence. This is all of the data that is required to perform photon transport calculations. Each of these simple models is first described in detail. Then example results are presented to illustrate the accuracy and importance of each model. These models have now been implemented in the Epic (Electron Photon Interaction Code). All of the figures and results presented here are from Epicshow, an interactive program to allow access to the Epic data bases, and Epicp, a simple photon transport code designed to develop optimum algorithms for later use in Epic. Epicp {{is made up of}} four parts: 1) a simple unoptimized driver to perform transport calculations, 2) an i/o package to handling reading of the binary, random access data files, 3) a physics package to handle kinematics of all processes, 4) a utility package containing all <b>computer</b> <b>dependent</b> routines, e. g., define running time, initialize random number sequence, etc. The focus is on optimizing parts 2) and 3) for later use in Epic; these are the part...|$|E
40|$|The Australian Government has a multibillion dollar {{national}} policy, the National Broadband Network {{to provide}} internet access for all Australians. eLearning and educational technology are significant {{new approaches to}} delivering higher education to Australian higher education students. Such approaches are dependent on National Broadband Networks to deliver the resources using computer based learning. Worldwide Blackboard Learn, a provider of <b>computer</b> technology <b>dependent</b> on eLearning management systems, claims {{to have more than}} 20 million plus users of their learning management system. With the current emphasis on eLearning some important questions need to be asked and answered. These questions are, is this new learning paradigm a help or a hindrance to student learning in higher education, are the clams made by the providers of such resources supported by independent research evidence indicates and what effects actually occur with the end users, such as lecturers and students, the actuators of the new learning paradigm in higher education...|$|R
40|$|The {{independence}} axiom recommends independence {{among all}} functional requirements. Modern machines, however, are all driven by electrical power and follow commands from <b>computers</b> with algorithms <b>dependent</b> on instrumentation signals; electrical functions interfere with all mechanical functional requirements. Moreover, a typical machine loses its entire function when its single electrical system fails. The Fukushima- 1 accident followed this exact scenario; the tsunami destroyed all power supplies and switchboards, then all pumps and valves turned inoperable {{from the control}} room. Delayed counteractions led {{to a loss of}} cooling functions and eventually to core damage. This interference is a fundamental design problem with modern machines...|$|R
40|$|An {{evolutionary}} {{process of the}} fast magnetic reconnection in ``free space'' which is free from any influence of outer circumstance has been studied semi-analytically, and a self-similarly expanding solution has been obtained. The semi-analytic solution {{is consistent with the}} results of our numerical simulations performed in our previous paper (see Nitta et al. 2001). This semi-analytic study confirms the existence of self-similar growth. On the other hand, the numerical study by time <b>dependent</b> <b>computer</b> simulation clarifies the stability of the self-similar growth with respect to any MHD mode. These results confirm the stable self-similar evolution of the fast magnetic reconnection system. Comment: 15 pages, 7 figure...|$|R
40|$|A {{law firm}} {{associate}} has prepared a continuing legal education PowerPoint presentation that resides {{on the hard}} drive of the associate 2 ̆ 7 s laptop. Another associate has served as an expert witness at a U. S. congressional hearing and the testimony {{is available on the}} GPO 2 ̆ 7 s website. The law firm 2 ̆ 7 s annual report from last year is stored on the intranet on the firm 2 ̆ 7 s web server. The firm 2 ̆ 7 s librarian has delivered an educational presentation at a professional meeting that is available on the web as a podcast. How can all of these diverse items be captured, archived, organized and readily accessible on the web in one location for public access? An institutional repository can provide the perfect solution. In our current technological age, most communications and scholarship are born digital and are often scattered across various servers and hard drives. Most of these virtual items are not as carefully archived or preserved as are traditional print publications. Librarians have a unique opportunity to fill a void by taking a leadership role in organizing and preserving digital information. In today 2 ̆ 7 s <b>computer</b> <b>dependent</b> environment, our extensive archival expertise is timely and germane. One particularly effective means for filling the void and seizing the opportunity is to establish an institutional repository to collect the intellectual output of your institution...|$|E
40|$|This {{document}} reports {{survey data}} collected from the U. S. Department of Energy, Richland Operations Office (DOE-RL), Project Hanford Management Contract (PHMC) companies, and the PHMC enterprise companies for purposes of characterizing the Hanford Local Area Network (HLAN) user profile. Telephone, radio, and pager data are also provided. The data reveal that job tasks of the 8, 500 Hanford Site workers who use the HLAN are highly, if not completely, <b>computer</b> <b>dependent.</b> Employees use their computers as their pens and paper, calculators, drafting tables and communication devices. Fifty eight percent of the survey respondents predict 90 to 100 % loss in productivity if they had no access to a computer. Additionally, 30 % of the users felt {{they would have a}} 50 to 80 % loss in productivity without computers; and more than 68 % use their computers between 4 and 8 hours per day. The profile also shows th at the software packages used most heavily are cc:Mail` the Windows version, Hanford Information, WordPerfece, Site Forms and Look-up. Use of Windows-based products is very high. Regarding the productivity tools that are seldom used, 49 % of the respondents say they ``never use`` the Hanford Help and Hints (HUH). The use of the external intemet by Hanford has shown a large increase. The survey indicates that users rate the intranet and the ability to access other sources of information as the fourth most important computer application. The Microsoft System Management Server (SMS 4) data show that more than 60 % of the computers on the HLAN need replacement or upgrades to run the Windows 95 Operating System, which has been selected as the PHMC standard. Although data also show that 77 % of the PHMC machines are running the current standard Windows for Workgroup version 3. 1 1, {{they do not have the}} memory and/or the hard disk space to upgrade to Windows 95. The survey results indicate that telephone system use is also high and regarded as a useful tool. Pager use is very high and has continued to rise even as the numbers of employees dropped. The number of radios is decreasing as the employee numbers drop...|$|E
40|$|Optimal {{operation}} of reservoir systems {{is necessary for}} better utilizing the limited water resources and to justify the high capital investments associated with reservoir projects. However, finding optimal policies for real-life problems of reservoir systems operation (RSO) is a challenging task as the available analytical methods can not handle the arbitrary functions {{of the problem and}} almost all methods employed are numerical or iterative type that are <b>computer</b> <b>dependent.</b> Since the computer resources in terms of memory and CPU time are limited, a limit exists for the size of the problem, in terms of arithmetic and memory involved, that can be handled. This limit is approached quickly as the dimension and the nonlinearity of the problem increases. In encountering the complex aspects of the problem all the traditionally employed methods have their own drawbacks. Linear programming (LP), though very efficient in dealing with linear functions, can not handle nonlinear functions which is the case mostly in real-life problems. Attempting to approximate nonlinear functions to linear ones results in the problem size growing enormously. Dynamic programming (DP), though suitable for most of the RSO problems, requires exponentially increasing computer resources as the dimension of the problem increases and at present many high dimensional real-life problems can not be solved using DP. Nonlinear programming (NLP) methods are not known to be efficient in RSO problems due to slow rate of convergence and inability to handle stochastic problems. Simulation methods can, practically, explore {{only a small portion of}} the search region. Many simplifications in formulations and adoption of approximate methods in literature still fall short in addressing the most critical aspects, namely multidimensionality, stochasticity, and additional complexity in conjunctive operation, of the problem. As the problem complexity increases and the possibility of arriving at the solution recedes, a near optimal solution with the best use of computational resources can be very valuable. In this context, genetic algorithms (GA) can be a promising technique which is believed to have an advantage in terms of efficient use of computer resources. GA is a random search method which find, in general, near optimal solutions using evolutionary mechanism of natural selection and natural genetics. When a pool of feasible solutions, represented in a coded form, are given fitness according to a objective function and explored by genetic operators for obtaining new pools of solutions, then the ensuing trajectories of solutions come closer and closer to the optimal solution which has the greatest fitness associated with it. GA can be applied to arbitrary functions and is not excessively sensitive to the dimension of the problem. Though in general GA finds only the near optimal solutions trapping in local optima is not a serious problem due to global look and random search. Since GA is not fully explored for RSO problems two such problems are selected here to study the usefulness and efficiency of GA in obtaining near optimal solutions. One problem is conjunctive {{operation of}} a system consisting of a surface reservoir and an aquifer, taken from the literature for which deterministic and stochastic models are solved. Another problem is real-time operation of a multipurpose reservoir, operated for irrigation (primary purpose) and hydropower production, which is in the form of a case study. The conjunctive operation problem consists of determining optimal policy for a combined system of a surface reservoir and an aquifer. The surface reservoir releases water to an exclusive area for irrigation and to a recharge facility from which it reaches the aquifer in the following period. Another exclusive area is irrigated by water pumped from the aquifer. The objective is to maximize the total benefit from the two irrigated areas. The inflow to the surface reservoir is treated as constant in deterministic model and taken at 6 different classes in stochastic model. The hydrological interactions between aquifer and reservoir are described using a lumped parameter model in which the average aquifer water table is arrived at based on the quantity of water in the aquifer, and local drawdown in pumping well is neglected. In order to evaluate the GA solution both deterministic and stochastic models are solved using DP and stochastic DP (SDP) techniques respectively. In the deterministic model, steady state (SS) cyclic (repetitive) solution is identified in DP as well as in GA. It is shown that the benefit from GA solution converges to as near as 95 % of the benefit from exact DP solution at a highly discounted CPU time. In the stochastic model, the steady state solution obtained with SDP consists of converged first stage decisions, which took a 8 -stage horizon, for any combination of components of the system state. The GA solution is obtained after simplifying the model to reduce the number of decision variables. Unlike SDP policy which gives decisions considering the state of the system in terms of storages, at reservoir, aquifer, and recharge facility, and previous inflow at the beginning of that period, GA gives decisions for each period of the horizon considering only the past inflow state of the period. In arriving at these decisions the effect of neglected state information is approximately reflected in the decisions by the process of refinement of the decisions, to conform to feasibility of storages in reservoir and aquifer, carried out in a simplified simulation process. Moreover, the validity of the solution is confirmed by simulating the operation with all possible inflow sequences for which the 8 -stages benefit converged up to 90 % of the optimum. However, since 8 stages are required for convergence to SS, a 16 -stage process is required for GA method in which the first 8 stages policy is valid. Results show that GA convergence to the optimum is satisfactory, justifying the approximations, with significant savings in CPU time. For real-time operation of a multipurpose reservoir, a rule curve (RC) based monthly operation is formulated and applied on a real-life problem involving releases for irrigation as well as power production. The RC operation is based on the target storages that have to be maintained, at each season of the year, in the reservoir during normal hydrological conditions. Exceptions to target storages are allowed when the demands have to be met or for conserving water during the periods of high inflows. The reservoir in the case study supplies water to irrigation fields through two canals where a set of turbines each at the canal heads generate hydropower. A third set of turbines operate on the river bed with the water let out downstream from the dam. The problem consists of determining the the RC target storages that facilitate maximum power production while meeting the irrigation demands up to a given reliability level. The RC target storages are considered at three different levels, corresponding to dry, normal, and wet conditions, according to the system state in terms of actual (beginning of period) storage of the reservoir. That is, if the actual beginning storage of the reservoir is less than some coefficient, dry-coe, times the normal target storage the target for the end of the period storage is taken at the dry storage target (of the three sets of storages). Similarly the wet level is taken for the end of the period target if the actual beginning storage is greater than some coefficient, wet-coe, times the normal storage. For other conditions the target is the normal storage level. The dry-coe and wet-coe parameters are obtained by trial and error analysis working on a small sequence of inflows. The three sets of targets are obtained from optimization over a 1000 year generated inflow sequence. With deterministic DP solutions, for small sequences of inflows, the optimization capability of GA-RC approach, in terms of objective function convergence, and generalization or robustness capability of GA-RC approach, for which the GA-RC benefit is obtained by simulating the reservoir operation using the previously obtained GA-RC solution, are evaluated. In both the cases GA-RC approach proves to be promising. Finally a 15 year real-time simulation of the reservoir is carried out using historical inflows and demands and the comparison with the historical operation shows significant improvement in benefit, i. e. power produced, without compromising irrigation demands throughout the simulation period...|$|E
40|$|This {{project is}} {{the design of a}} {{computer}} speech recognition system. A off-line, speaker <b>dependent</b> <b>computer</b> based speech recognizer, with a vocabulary of ten digits was implemented on an HP/UX workstation. The system consisted of two subunits: parameter extraction and parameter correlation. The parameter extraction subunit could extract the following parameters from a digitally recorded speech signal: Zero crossing rate, Autocorrelation coefficients, LPC coefficients, Cepstral coefficients, and PARCOR coefficients. The parameter correlation subunit could compare two speech segments using either Linear time normalization or Dynamic time warping method. In real-time situations, the system produced results with a lag of about 5 seconds for each recognition...|$|R
40|$|In today’s job market, {{computer}} skills {{are part of}} the prerequisites for many jobs. In this paper, we report on a study of readiness to work with <b>computers</b> (the <b>dependent</b> variable) among unemployed women (N= 54) after participating in a unique, web-supported training focused on {{computer skills}} and empowerment. Overall, the level of participants’ readiness to work with computers was much higher {{at the end of the}} course than it was at its begin-ning. During the analysis, we explored associations between this variable and variables from four categories: log-based (describing the online activity); computer literacy and experience; job-seeking motivation and practice; and training satisfaction. Only two variables were associated with the dependent variable: knowledge post-test duration and satisfaction with content. After building a prediction model for the dependent variable, another log-based variable was highlighted: total number of actions in the course website along the course. Overall, our analyses shed light on the predominance of log-based variables over variables from other categories. These findings might hint at the need of developing new assessment tools for learners and trainees that take into consideration human-computer interaction when measuring self-efficacy variables...|$|R
40|$|Abstract — Automation {{is the use}} {{of control}} system and {{information}} technology to reduce the need of human work in the production of goods and services. As one of the primary activities in <b>computer</b> science and <b>dependent</b> heavily on the rapid development in computer technology, Expert systems have been eagerly adopted by industries and applied {{to a wide range of}} applications. Expert systems belongs to a field of intelligence knowledge based systems the constitute one of the principle field of activity of computational intelligence, a field which been referred to as the science that attempts to reproduced human intelligence using computational means. In this paper we present the need of expert system, the element of expert system and expert system paradigm in industries...|$|R
40|$|Ion {{diffusion}} induced current, Nanochannel, Concentration gradientIn this article, {{a theoretical}} model {{that takes into}} consideration both the diffusion and electric osmosis fluxes to estimate the diffusion current through a nanochannel is proposed. Equivalent impedance of a nanochannel is modeled as a series connection of finite RC circuits. The impedances of the electrical double layer and buffer solution in each infinitesimal RC circuit are represented by a capacitor and a resister, respectively. Due to the electrical double layer effect, the diffusion induced current in the proposed model is position <b>dependent.</b> <b>Computer</b> simulations and experiments using a nanoporous anodic aluminum oxide (AAO) thin film as the filter to separate electrolysis with ion concentration gradient are conducted. A high degree coincidence between the theoretical and experimental data is observed...|$|R
40|$|In this article, a {{theoretical}} model {{that takes into}} consideration both the diffusion and electric osmosis fluxes to estimate the diffusion current through a nanochannel is proposed. Equivalent impedance of a nanochannel is modeled as a series connection of finite RC circuits. The impedances of the electrical double layer and buffer solution in each infinitesimal RC circuit are represented by a capacitor and a resister, respectively. Due to the electrical double layer effect, the diffusion induced current in the proposed model is position <b>dependent.</b> <b>Computer</b> simulations and experiments using a nanoporous anodic aluminum oxide (AAO) thin film as the filter to separate electrolysis with ion concentration gradient are conducted. A high degree coincidence between the theoretical and experimental data is observed. 1...|$|R
40|$|The {{effect of}} dynamic optical {{properties}} on the {{spatial distribution of}} light in laser therapy is studied via numerical simulations. A two-dimensional, time <b>dependent</b> <b>computer</b> program called LATIS is used. Laser light transport is simulated with a Monte Carlo technique including anisotropic scattering and absorption. Thermal heat transport is calculated with a finite difference algorithm. Material properties are specified on a 2 -D mesh and can be arbitrary functions of space and time. Arrhenius rate equations are solved for tissue damage caused by elevated temperatures. Optical properties are functions of tissue damage, as determined by previous measurements. Results are presented for the time variation of the light distribution and damage within the tissue as the optical properties of the tissue are altered...|$|R
40|$|A {{model of}} Brownian {{particles}} {{with the ability}} to take up energy from the environment, to store it in an internal depot, and to convert internal energy into kinetic energy of motion, is discussed. The general dynamics outlined in Sect. 2 is investigated for the deterministic and stochastic particle's motion in a non-fluctuating ratchet potential. First, we discuss the attractor structure of the ratchet system by means of <b>computer</b> simulations. <b>Dependent</b> on the energy supply, we find either periodic bound attractors corresponding to localized oscillations, or one/two unbound attractors corresponding to directed movement in the ratchet potential. Considering an ensemble of particles, we show that in the deterministic case two currents into different directions can occur, which however depend on a supercritical supply of energy. Considering stochastic influences, we find the current only in one direction. We further investigate how the current reversal depends on the strength of the stochastic force and the asymmetry of the potential. We find both a critical value of the noise intensity for the onset of the current and an optimal value where the net current reaches a maximum. Eventually, the dynamics of our model is compared with other ratchet models previously suggested. Comment: 24 pages, 11 Figs., For related work see [URL]...|$|R
40|$|Efforts {{to bridge}} the digital divide have {{concentrated}} on community <b>computer</b> centers <b>dependent</b> on subsidy and constant supervision. This thesis considers the design of public digital interfaces that are physically and financially autonomous while establishing an adaptable structure for community networking. These pedestrian interfaces generate income from retail and advertising already common on our streets. In turn they can provide free wireless networking and serve as community computer centers. The network of public computers is targeted to travelers along existing transportation infrastructures: streets, highways, train and bus lines. By offering services such as directions, e-mail, job-searching and web-surfing, these computer centers will provide incentive to develop digital literacy. The interfaces are climate-controlled secure street shelters. Many include a small store, an automated vending machine or a public bathroom. A flexible system of wireless input and output modules allow each interface {{to take on a}} number of public and private uses through the course of a day. The small buildings adapt continually to a user's needs to create an accessible, intuitive interface. The pedestrian interfaces are suited to current technology, and the ergonomic envelope is designed to accommodate future technologies as they become feasible. by Leonardo Amerigo Bonanni. Thesis (M. Arch.) [...] Massachusetts Institute of Technology, Dept. of Architecture, 2003. Includes bibliographical references (leaf 51) ...|$|R
40|$|A time <b>dependent</b> <b>computer</b> {{model of}} radiative-convective-conductive heat {{transfer}} in the Martian ground-atmosphere system was refined by incorporating an intermediate line strength CO 2 band absorption which {{together with the}} strong-and weak-line approximation closely simulated the radiative transmission through a vertically inhomogeneous stratification. About 33, 000 CO 2 lines were processed to cover the spectral range of solar and planetary radiation. Absorption by silicate dust particulates, was taken into consideration to study {{its impact on the}} ground-atmosphere temperature field as a function of time. This model was subsequently attuned to IRIS, IR-radiometric and S-band occultation data. Satisfactory simulations of the measured IRIS spectra were accomplished for the dust-free condition. In the case of variable dust loads, the simulations were sufficiently fair so that some inferences into the effect of dust on temperature were justified...|$|R
