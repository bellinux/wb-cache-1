605|42|Public
25|$|<b>Computability</b> <b>theory</b> {{for digital}} {{computation}} is well developed. <b>Computability</b> <b>theory</b> is less well developed for analog computation {{that occurs in}} analog computers, analog signal processing, analog electronics, neural networks and continuous-time control theory, modelled by differential equations and continuous dynamical systems (Orponen 1997; Moore 1996).|$|E
25|$|The {{diagonal}} lemma {{is closely}} related to Kleene's recursion theorem in <b>computability</b> <b>theory,</b> and their respective proofs are similar.|$|E
25|$|After World War II, {{mathematical}} logic branched into four inter-related but separate areas of research: model theory, proof theory, <b>computability</b> <b>theory,</b> and set theory.|$|E
5000|$|Candidate of Sciences (Charles University, 1964), thesis: Vyčíslitelnost ve vztahu k teoriím On <b>Computability</b> w.r.t. <b>Theories</b> ...|$|R
50|$|In <b>computability</b> and {{complexity}} <b>theory,</b> ALL is {{the class of}} all decision problems.|$|R
40|$|The {{notion of}} Limit-Computable Mathematics (LCM) will be introduced. LCM is a {{fragment}} of classical mathematics in which the law of excluded middle is restricted to 1 0 2 -formulas. We can give an accountable computational interpretation to the proofs of LCM. The computational content of LCM-proofs is given by Gold's limiting recursive functions, which is the fundamental notion of learning theory. LCM {{is expected to be}} a right means for "Proof Animation," which was introduced by the first author [10]. LCM is related not only to learning theory and recursion theory, but also to many areas in mathematics and computer science such as computational algebra, <b>computability</b> <b>theories</b> in analysis, reverse mathematics, and many others...|$|R
25|$|Some {{commentators}} {{argue that}} both the names recursion theory and <b>computability</b> <b>theory</b> fail to convey {{the fact that most}} of the objects studied in recursion theory are not computable.|$|E
25|$|In {{computer}} science, combinatory {{logic is}} used as a simplified model of computation, used in <b>computability</b> <b>theory</b> and proof theory. Despite its simplicity, combinatory logic captures many essential features of computation.|$|E
25|$|In <b>computability</b> <b>theory,</b> the halting {{problem is}} the problem of determining, from a {{description}} of an arbitrary computer program and an input, whether the program will finish running or continue to run forever.|$|E
40|$|Gene {{assembly}} in stichotrichous ciliates happening during sexual reproduction {{is one of}} the most involved DNA manipulation processes occurring in biology. This biological process is of high interest from the computational and mathematical points of view due to its close analogy with such concepts and notions in theoretical computer science as permutation and linked list sorting and string rewriting. Studies on computational properties of gene {{assembly in}} ciliates represent a good example of interdisciplinary research contributing to both computer science and biology. We review here a number of general results related both to the development of different computational methods enhancing our understanding on the nature of gene assembly, as well as to the development of new biologically motivated computational and mathematical models and paradigms. Those paradigms contribute in particular to combinatorics, formal languages and <b>computability</b> <b>theories...</b>|$|R
500|$|Nick Pippenger: Computing {{researcher}} and theorist, IBM Fellow {{and professor at}} Harvey Mudd College. Known {{for his work on}} the foundations of computer science, including complexity, <b>computability</b> and communications <b>theory</b> ...|$|R
40|$|Theoretical Computer Science classically {{aimed to}} develop a {{mathematical}} understanding of capabilities and limits of traditional computing architecture (Boole, von Neuman, Turing, Church, Godel), investigating in <b>computability,</b> complexity <b>theory</b> and algorithmics. Now it seems more natural to revisit classical computer science notions under a new game- theoretic model. The purpose of this work is to investigate some themes {{at the intersection of}} algorithmics and game theory, emphasizing both mathematical and technological issues. computer science, game theory, network, protocol...|$|R
25|$|The {{period after}} World War II, when {{mathematical}} logic branched into four inter-related but separate areas of research: model theory, proof theory, <b>computability</b> <b>theory,</b> and set theory, and its ideas and methods began to influence philosophy.|$|E
25|$|In <b>computability</b> <b>theory,</b> the μ operator, {{minimization}} operator, or unbounded search operator {{searches for}} the least natural number with a given property. Adding the μ-operator to the five primitive recursive operators {{makes it possible to}} define all computable functions.|$|E
25|$|A {{cellular}} automaton (pl. cellular automata, abbrev. CA) is a discrete model studied in <b>computability</b> <b>theory,</b> mathematics, physics, complexity science, theoretical biology and microstructure modeling. Cellular automata are also called cellular spaces, tessellation automata, homogeneous structures, cellular structures, tessellation structures, and iterative arrays.|$|E
50|$|Nerode is Goldwin Smith Professor of Mathematics at Cornell University. His {{interests}} are in mathematical logic, the <b>theory</b> of automata, <b>computability</b> and complexity <b>theory,</b> the calculus of variations, and distributed systems. With John Myhill, Nerode proved the Myhill-Nerode theorem specifying necessary and sufficient {{conditions for a}} formal language to be regular.|$|R
40|$|This paper {{considers}} {{the relevance of}} the concepts of observability and <b>computability</b> in physical <b>theory.</b> Observability is related to verifiability which is essential for effective computing and as physical systems are computational systems it is important even where explicit computation is not the goal. Specifically, we examine two problems: observability and computability for quantum computing, and remote measurement of time and frequency. Comment: 8 pages, 1 figur...|$|R
25|$|A {{programming}} {{language is a}} machine-readable artificial language. Programming languages {{can be used to}} create programs that specify the behavior of a machine, to express algorithms precisely, or as a mode of human communication. The first {{programming language}}s predate the modern computer. In mathematical logic and computer science, lambda calculus, also written as λ-calculus, is a formal system designed to investigate function definition, function application and recursion. It was invented by Alonzo Church and Stephen Cole Kleene in the 1930s as part of an investigation into the foundations of mathematics, but has now emerged as a useful tool in the investigation of problems in <b>computability,</b> recursion <b>theory,</b> and as a fundamental basis and a modern paradigm to computer programming and software languages.|$|R
25|$|The {{interpretability}} {{method is}} often used to establish undecidability of theories. If an essentially undecidable theory T is interpretable in a consistent theory S, then S is also essentially undecidable. This is closely related to the concept of a many-one reduction in <b>computability</b> <b>theory.</b>|$|E
25|$|In <b>computability</b> <b>theory,</b> the Ackermann function, {{named after}} Wilhelm Ackermann, {{is one of}} the {{simplest}} and earliest-discovered examples of a total computable function that is not primitive recursive. All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.|$|E
25|$|The {{concept of}} Turing degree is {{fundamental}} in <b>computability</b> <b>theory,</b> where sets of natural numbers are often regarded as decision problems. The Turing degree {{of a set}} {{is a measure of}} {{how difficult it is to}} solve the decision problem associated with the set, that is, to determine whether an arbitrary number is in the given set.|$|E
40|$|This paper {{discusses}} {{an economic}} equilibrium <b>theory</b> with <b>computability</b> constraints {{on both the}} magnitudes (prices, quantities, etc.) and the operations (to perceive, evaluate, choose, communicate, etc) that agents can use. Prices and quantities are computable numbers (Turing (1936)), and preference relations, utility functions, demand functions are computable (in the sense of Moschovakis (1964)). We present sharper versions of several traditional assertions on utility representation, existence of consumer demand functions, the fundamental welfare theorems, characterizations of market excess demands. We also give a "computable counterexample " {{to the existence of}} a competitive equilibrium. These results can be interpreted as possibility and impossibility results in computability-bounded rationality, and in computational economics. Keywords: Bounded rationality, <b>Computability,</b> Consumer <b>theory,</b> General equilibrium analysis, Recursive analysis 1. INTRODUCTION In classical economic mod [...] ...|$|R
5000|$|Harel is {{best known}} for his work on dynamic logic, <b>computability,</b> {{database}} <b>theory,</b> software engineering and modelling biological systems. In the 1980s he invented the graphical language of Statecharts for specifying and programming reactive systems, which has been adopted as part of the UML standard. Since the late 1990s he has concentrated on a scenario-based approach to programming such systems, launched by his co-invention (with W. Damm) of Live Sequence Charts. He has published expository accounts of computer science, such as his award winning 1987 book [...] "Algorithmics: The Spirit of Computing" [...] and his 2000 book [...] "Computers Ltd.: What They Really Can’t do", and has presented series on computer science for Israeli radio and television. He has also worked on other diverse topics, such as graph layout, computer science education and the analysis and communication of odors.|$|R
40|$|AbstractA {{programming}} approach to <b>computability</b> and complexity <b>theory</b> yields more natural definitions and proofs of central results than the classical approach. Further, some new {{results can be}} obtained using this viewpoint. This paper contains new intrinsic characterizations of the well-known complexity classes PTIME and LOGSPACE, with no externally imposed resource bounds on time or space. LOGSPACE is proven identical with the decision problems solvable by read-only imperative programs on Lisp-like lists; and PTIME is proven identical with the problems solvable by recursive read-only programs...|$|R
25|$|<b>Computability</b> <b>theory,</b> {{also called}} {{recursion}} theory, is {{a branch of}} mathematical logic, of computer science, and {{of the theory of}} computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since grown to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.|$|E
25|$|Recursion theorists in {{mathematical}} logic often study {{the theory of}} relative computability, reducibility notions and degree structures described in this article. This contrasts with the theory of subrecursive hierarchies, formal methods and formal languages that is common {{in the study of}} <b>computability</b> <b>theory</b> in computer science. There is considerable overlap in knowledge and methods between these two research communities, however, and no firm line can be drawn between them.|$|E
25|$|Computable {{functions}} {{are the basic}} objects of study in <b>computability</b> <b>theory.</b> Computable {{functions are}} the formalized analogue of the intuitive notion of algorithm, {{in the sense that}} a function is computable if there exists an algorithm that can do the job of the function, i.e. given an input of the function domain it can return the corresponding output. Computable functions are used to discuss computability without referring to any concrete model of computation such as Turing machines or register machines. Any definition, however, must make reference to some specific model of computation but all valid definitions yield the same class of functions.|$|E
40|$|This review volume {{consists}} of an indispensable set of chapters written by leading scholars, scientists and researchers {{in the field of}} Randomness, including related subfields specially but not limited to the strong developed connections to the <b>Computability</b> and Recursion <b>Theory.</b> Highly respected, indeed renowned in their areas of specialization, many of these contributors are the founders of their fields. The scope of "Randomness Through Computation" is novel. Each contributor shares his personal views and anecdotes on the various reasons and motivations which led him to the study...|$|R
40|$|This article {{gives an}} {{overview}} of recent work on the theory of selection functions. We explain the intuition behind these higher-type objects, and define a general notion of sequential game whose optimal strategies can be computed via a certain product of selection functions. Several instances of this game are considered {{in a variety of}} areas such as fixed point theory, topology, game <b>theory,</b> higher-type <b>computability,</b> and proof <b>theory.</b> These examples are intended to illustrate how the fundamental construction of optimal strategies based on products of selection functions permeates several research areas...|$|R
40|$|We {{define and}} study a new notion called k-{{immunity}} that lies between immunity and hyperimmunity in strength. Our interest in k-immunity is {{justified by the}} result that # # does not k-tt reduce to a k-immune set, which improves a previous result by Kobzev [7, 13]. We apply the result to show that # # does not btt-reduce to MIN, the set of minimal programs. Other applications include the set of Kolmogorov random strings, and retraceable and regressive sets. We also give a new characterization of e#ectively simple sets and show that simple sets are not btt-cuppable. Keywords: <b>Computability,</b> Recursion <b>Theory,</b> bounded reducibilities, minimal programs, immunity, k-immune, regressive, retraceable, e#ectively simple, cuppable. 1 Introduction There {{seems to be a}} large gap between immunity and hyperimmunity (h-immunity) that is waiting to be filled. What happens, one wonders if the disjoint strong arrays that try to witness that a set is not h-immune are subjected to additional conditions [...] ...|$|R
25|$|Theoretical {{computer}} science seeks to understand which computational {{problems can be}} solved by using a computer (<b>computability</b> <b>theory)</b> and how efficiently (computational complexity theory). Traditionally, {{it is said that}} a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.|$|E
25|$|Fundamental {{results of}} <b>computability</b> <b>theory</b> {{show that there}} are {{functions}} that can be precisely defined but are not computable. Moreover, {{in the sense of}} cardinality, almost all functions from the integers to integers are not computable. The number of computable functions from integers to integers is countable, because the number of possible algorithms is. The number of all functions from integers to integers is higher: the same as the cardinality of the real numbers. Thus most functions from integers to integers are not computable. Specific examples of uncomputable functions are known, including the busy beaver function and functions related to the halting problem and other undecidable problems.|$|E
25|$|Church's {{system for}} {{computation}} developed {{into the modern}} λ-calculus, while the Turing machine became a standard model for a general-purpose computing device. It was soon shown that many other proposed models of computation were equivalent in power to those proposed by Church and Turing. These results led to the Church–Turing thesis that any deterministic algorithm that {{can be carried out}} by a human can be carried out by a Turing machine. Church proved additional undecidability results, showing that both Peano arithmetic and first-order logic are undecidable. Later work by Emil Post and Stephen Cole Kleene in the 1940s extended the scope of <b>computability</b> <b>theory</b> and introduced the concept of degrees of unsolvability.|$|E
40|$|Broad in coverage, mathematically sophisticated, {{and up to}} date, {{this book}} {{provides}} an introduction to <b>theories</b> of <b>computability.</b> It treats not only 2 ̆ 2 the 2 ̆ 2 <b>theory</b> of <b>computability</b> (the <b>theory</b> created by Alan Turing {{and others in the}} 1930 s), but also a variety of other theories (of Boolean functions, automata and formal languages) as <b>theories</b> of <b>computability.</b> These are addressed from the classical perspective of their generation by grammars and from the more modern perspective as rational cones. The treatment of the classical theory of computable functions and relations {{takes the form of a}} tour through basic recursive function theory, starting with an axiomatic foundation and developing the essential methods in order to survey the most memorable results of the field. This authoritative account, written by one of the leading lights of the subject, will be required reading for graduate students and researchers in theoretical computer science and mathematics. [URL]...|$|R
40|$|In this study, {{we present}} an {{original}} theoretical approach to prove π (n) -li(n) =o(M(n) √(li(n))), where M(n) is an arbitrary function such that M(n) →+∞. This {{implies that the}} Riemann Hypothesis almost certainly stands without any assumptions. To prove this, we apply Turing <b>computability</b> to probability <b>theory</b> {{and the distribution of}} arbitrarily large prime numbers. We prove that for any Turing machine prime numbers require an arbitrarily large number of computational steps to identify when primes tend to infinity. This nature of prime numbers is related to probability theory. Comment: Revision of the description over the whole range of the thesi...|$|R
40|$|A basic {{dichotomy}} {{concerning the}} structure of the orbit space of a transformation group has been discovered by Glimm [G 12] in the locally compact group action case and extended by Effros [E 1, E 2] in the Polish group action case when additionally the induced equivalence relation is Fσ. It is {{the purpose of this paper}} to extend the Glimm-Effros dichotomy to the very general context of an arbitrary Borel equivalence relation (not even necessarily induced by a group action). Despite the totally classical descriptive set-theoretic nature of our result, our proof requires the employment of methods of effective descriptive set theory and thus ultimately makes crucial use of <b>computability</b> (or recursion) <b>theory</b> on the integers. ...|$|R
