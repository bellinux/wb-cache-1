2|35|Public
50|$|The {{game was}} {{developed}} in Unreal Engine 4. Development began {{with the concept of}} a dystopian society embellished with 1980s and '90s references to Eastern European culture and architecture. The ever-changing level design in hacking sequences features <b>compression</b> <b>artefact</b> effects. The soundtrack was written by composer Arkadiusz Reikowski. The involvement of Rutger Hauer, who plays the lead, was revealed in July 2017.|$|E
40|$|International audienceThe {{quantization}} of {{data from}} individual block-based Discrete Cosine Transform generates the blocking effect estimated as the most annoying <b>compression</b> <b>artefact.</b> It appears as an artificial structure caused by noticeable changes in pixel values along the block boundaries. Due to the masking effect, the blocking artefact is more annoying in flat areas than in textured or detailed areas. Existing low-cost algorithms propose strong low-pass filters to correct this artefact in flat areas. Nevertheless, they are confronted to a limitation based on their filter length. This limitation can introduce other artefacts such as ghost boundaries. We propose a new principle to detect and correct the boundaries on flat areas without being limited to a fix number of pixels. This principle can be easily implemented in a low-cost post processing algorithm and completed with other corrections for perceptible boundaries on non-flat areas. This new method produces results which are perceived as more pleasing for the human eye than the other traditional low-cost methods...|$|E
40|$|Abstract. In {{this paper}} we extend the {{recently}} proposed DCT-mod 2 feature extraction technique (which utilizes polynomial coefficients derived from 2 D DCT coefficients obtained from horizontally & vertically neighbouring blocks) via {{the use of}} various windows and diagonally neighbouring blocks. We also propose enhanced PCA, where traditional PCA feature extraction is combined with DCT-mod 2. Results using test images corrupted by a linear and a non-linear illumination change, white Gaussian noise and <b>compression</b> <b>artefacts,</b> show that use of diagonally neighbouring blocks and windowing is detrimental to robustness against illumination changes while being useful for increasing robustness against white noise and <b>compression</b> <b>artefacts.</b> We also show that the enhanced PCA technique retains all {{the positive aspects of}} traditional PCA (that is, robustness against white noise and <b>compression</b> <b>artefacts)</b> while also being robust to illumination changes; moreover, enhanced PCA outperforms PCA with histogram equalisation pre-processing. ...|$|R
30|$|The Mondial Marmi dataset {{contains}} {{images of}} slabs of Italian marble. The textured surfaces are rotated prior to image acquisition {{to allow for}} studying rotation invariance in texture analysis. The imaging is done with a compact camera storing the images in JPEG format with notable <b>compression</b> <b>artefacts.</b>|$|R
40|$|Abstract: Determining the {{compression}} quality {{of an image}} is important for photo forensics and image enhancement algorithms. Unfortunately, {{there are a number}} of issues involved in determining {{the compression}} quality of an image from its metadata or quantization tables. A compression quality estimation algorithm based on visual inspection of detected <b>compression</b> <b>artefacts</b> is presented. This method detects and extracts feature samples around compression block corners. These feature samples are then pre-filtered to enhance the discontinuities produced by <b>compression</b> <b>artefacts.</b> The feature samples are then classified using a constricted Neural Network. The local quality estimations are then combined using robust statistics to estimate the maximum likelihood compression quality. This method was shown to accurately estimate the compression quality of an image without prior knowledge of the original uncompressed image...|$|R
40|$|Abstract:- Biometric {{identity}} management {{based only on}} the single biometric modality is not accurate or robust enough {{to be used in}} uncontrolled environments. This paper describes a fusion of face and voice biometric traits, based on fuzzy logic approach for speaker identity verification. In this approach, a scheme based on membership function and fuzzy integral is proposed to fuse information from the two modalities. Equal Error rate is used to evaluate the fusion scheme. Experimental results show the fusion scheme improves identity verification performance substantially and makes the system robust to environmental degradations such as acoustic noise and visual <b>compression</b> <b>artefacts...</b>|$|R
30|$|The large {{families}} of small cubic arrays {{presented in this}} paper solve this problem. Here, we show how we propose to do this. First, we present some results demonstrating the feasibility and effectiveness of our method of watermarking and use of arrays produced by our constructions. The first requirement of watermarking is imperceptibility. The left of Fig.  3 shows a frame from an unmarked video, while the right shows the same frame after marking with the array described above. The <b>compression</b> <b>artefacts</b> due to H 264 processing in the left frame are not too dissimilar to the patterns due to the watermark, which are superimposed on the <b>compression</b> <b>artefacts</b> in the watermarked frame on the right. The frames were quite dark, and the intensity curves had to be adjusted to bring out the patterns. This resulted in a mismatch in contrast and hue. There were also H 264 synchronization issues, due to variable buffer delays in Windows, which resulted in loss of sync between I frames within a GOP. All the same, Fig.  6. demonstrates that the watermark was unobtrusive or even imperceptible. The second requirement of watermarking is that the watermark can be successfully extracted from the media in which it was embedded with sufficiently low probability of missed or false detection. Figure  1 shows an unmistakable autocorrelation peak, which is clearly distinguishable from the cross-correlation with the video. These results were obtained without matched filtering.|$|R
40|$|In {{this paper}} we {{evaluate}} the effectiveness of two likelihood normalization techniques, the background model set (BMS) and the universal background model (UBM), for improving performance and robustness of four face authentication systems utilizing a Gaussian mixture model (GMM) classifier. The systems differ in the feature extraction method used: eigenfaces (PCA), 2 -D DCT, 2 -D Gabor wavelets and DCT-mod 2. Experiments on the VidTIMIT database, using test images corrupted either by an illumination change or <b>compression</b> <b>artefacts,</b> suggest that likelihood normalization has little effect when using PCA derived features, while providing significant performance improvements when using the remaining features. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
40|$|This thesis {{presents}} {{a framework of}} test methodology to assess spatial domain <b>compression</b> <b>artefacts</b> produced by image and intra-frame coded video codecs. Few researchers have studied this broad range of artefacts. A taxonomy of image and video <b>compression</b> <b>artefacts</b> is proposed. This {{is based on the}} point of origin of the artefact in the image communication model. This thesis presents objective evaluation of distortions known as artefacts due to image and intra-frame coded video compression made using synthetic test patterns. The American National Standard Institute document ANSI T 1 801 qualitatively defines blockiness, blur and ringing artefacts. These definitions have been augmented with quantitative definitions in conjunction with test patterns proposed. A test and measurement environment is proposed in which the codec under test is exercised using a portfolio of test patterns. The test patterns are designed to highlight the artefact under study. Algorithms have been developed to detect and measure individual artefacts based on the characteristics of respective artefacts. Since the spatial contents of the original test patterns form known structural details, the artefact distortion metrics based on the characteristics of those artefacts are clean and swift to calculate. Distortion metrics are validated using a human vision system inspired modern image quality metric. Blockiness, blur and ringing artefacts are evaluated for representative codecs using proposed synthetic test patterns. Colour bleeding due to image and video compression is discussed with both qualitative and quantitative definitions for the colour bleeding artefacts introduced. The image reproduction performance of a few codecs was evaluated to ascertain the utility of proposed metrics and test patterns...|$|R
40|$|A device-independent {{algorithm}} for {{the estimation}} of an enhanced resolution image from a low-resolution compressed sequence is proposed. The algorithm utilises least squares matching to extract the interdependence of the low-resolution images. This algorithm also {{has the effect of}} attenuating <b>compression</b> <b>artefacts.</b> Improving the spatial resolution of the image sequence is not the only goal of the enhancement algorithm, as the enhanced images in turn lead to digital elevation models (DEMs) of improved accuracy. Experimental results illustrate the algorithm's performance as a tool for digital photogrammetric applications. Stereoscopic sets of left and right images were taken of objects of known geometry and DEMs were created using both the original coarse images and compressed images enhanced by the algorithm...|$|R
30|$|In this paper, we {{investigate}} {{and compare the}} following: local rotation invariance, global rotation invariance, including rotations in the training data, {{and the effect of}} the different interpolation methods when rotating textures, in the setting of retaining discriminant texture information. In order to do this, we introduce a new texture dataset, the Kylberg Sintorn Rotation Dataset. The dataset includes images of hardware-rotated textures as well as texture images rotated by the interpolation kernels: nearest neighbour, linear, third order cubic, cubic B-spline and Lanczos 3. The dataset has 25 classes of different types of textured surfaces and is publicly available [13]. The images in the dataset are acquired in raw format avoiding <b>compression</b> <b>artefacts.</b>|$|R
30|$|Assessment {{algorithms}} typically {{assign a}} score to a photo {{based on some}} metric. This allows {{the creation of an}} ordering based on the returned metric values. Assessing (or ranking) a photo is a very difficult and controversial task, especially when dealing with consumer ones. Two main aspects can be evaluated: the quality and the aesthetics of the photo. While the image quality analysis, in this survey, is understood as the assessment of the degradation of the image (e.g., sensor noise, resolution, and <b>compression</b> <b>artefacts),</b> the aesthetics analysis is related to the visual appearance and appeal of the photo (e.g., the color harmony and photo composition). IQA is out of the scope of this survey.|$|R
40|$|Signal {{processing}} on digitally sampled vowel sounds for {{the detection}} of pathological voices has been firmly established. This work examines <b>compression</b> <b>artefacts</b> on vowel speech samples that have been compressed using the adaptive multi-rate codec at various bit-rates. Whereas previous work has used the sensitivity of machine learning algorithm to test for accuracy, this work examines the changes in the extracted speech features themselves and thus report new findings on the usefulness of a particular feature. We believe this work will have potential impact for future research on remote monitoring as the identification and exclusion of a ill-defined speech feature that has been hitherto used, will ultimately increase the robustness of the system...|$|R
40|$|International audienceWith the fastly {{developing}} {{standards for}} video compression, {{the availability of}} the video content at a large scale in the compressed form has become an everyday reality. Compressed video streams whatever is the compression standard contain primary information which without full decompression of the content can serve for its indexing and retrieval. This primary features optimized for the compression purposes are very often noisy and subject to <b>compression</b> <b>artefacts.</b> Hence their re-use for analysis and indexing purposes remains a challenge. In this paper we present the methodology which starting from the early low - resolution standards of MPEG family up to the new high-qualityhigh definition video compression approaches aims at answering the same question: how to re-use noisy primary features for fast and efficient video indexing...|$|R
50|$|A <b>compression</b> {{artifact}} (or <b>artefact)</b> is {{a noticeable}} distortion of media (including images, audio, and video) {{caused by the}} application of lossy compression.|$|R
40|$|This paper {{introduces}} a Bayesian restoration method for low-resolution images {{combined with a}} smoothness prior and a newly proposed adaptive bimodal prior. The bimodal prior {{is based on the}} fact that an edge pixel has a colour value that is typically a mixture of the colours of two connected regions, each having a dominant colour distribution. Local adaptation of the parameters of the bimodal prior is made to handle neighbourhoods which have typically more than two dominant colours. The maximum a posteriori estimator is worked out to solve the problem. Experimental results confirm the effectiveness of the proposed bimodal prior and show the visual superiority of our reconstruction scheme to other traditional interpolation and reconstruction methods for images with a strong colour modality like cartoons: noise and <b>compression</b> <b>artefacts</b> are removed very well and our method produces less blur and other annoying artefacts...|$|R
5000|$|The region 1 DVD of Gosford Park was {{released}} on 25 June 2002, with the region 2 release on 3 December 2002. The critic Ed Gonzalez reviewed the DVD negatively, calling the picture quality [...] "atrocious on the small screen", going {{on to say that}} [...] "the image quality of this video transfer is downright lousy from start to finish." [...] However, reviewer Robert Mack generally wrote favourably of the picture quality, noting excellence in the shots' detail and sharpness and the lack of <b>compression</b> <b>artefacts,</b> but describing an unfavourable darkness to scenes filmed within the manor house. Both reviewers commented positively on the film's score and soundtrack. Gonzalez wrote that [...] "Gosford Park sounds amazing for a film so dialogue-dependent" [...] and Mack that [...] "the audio transfer is about as good as it can get on a movie of this style." ...|$|R
40|$|During acquisition, processing, {{compression}} and transmission, images may be corrupted by multiple distortions such as blur, noise or <b>compression</b> <b>artefacts.</b> However, {{most of the}} existing image quality assessment (IQA) methods are designed for images degraded by a single distortion type. This paper proposes a reduced-reference (RR) IQA method for quality assessment of multiply distorted images. The method extracts a number of quality-characterizing features from the reference and the distorted images for quality prediction. Based on internal generative mechanism (IGM) theory, the images are decomposed first into their predicted and disorderly portions. Next, a number of quality-characterizing features are extracted from each portion and feature differences are computed between the reference and distorted images. Finally, support vector regression (SVR) is adopted to obtain a quality score. Experimental results on public multiply-distorted image databases, namely MDID 2015 and MLIVE, show that the proposed method is well-correlated with subjective ratings and outperforms several IQA methods...|$|R
40|$|The way image {{sequences}} are encoded by technological systems, that is video, {{is fundamentally}} {{tied to the}} way in which the human eye and brain interpret images and motion. This includes such aspects as resolution, colour, dynamic range, frame rates and spatial and temporal compression techniques. On the contrary, object identification algorithms are commonly based on single image analysis, such as the extraction of a single video frame from a sequence. This mismatch of, in particular, temporal processing paradigms means that most object analysis algorithms are not well suited to the data with which they are presented. In order to bridge this gap we investigate the temporal preconditioning of video data through a biologically-inspired vision model, based on multi-stage processing analogous to the vision systems of insects. In doing so, we argue that such an approach can lead to improved object identification through the enhancement of object perimeters and the amelioration of lighting and <b>compression</b> <b>artefacts</b> such as shadows and blockiness. S. Poursoltan, R. Brinkworth and M. Sorel...|$|R
40|$|Mars Express is {{in orbit}} {{for over two}} years and has {{returned}} more than 1. 000 image strips to earth taken by the multiple line scanner camera HRSC (High Resolution Stereo Camera). The three-dimensional position of the spacecraft is continually determined by the Flight Dynamics Team (FDT) at ESOC (European Space Operations Centre) in Darmstadt, Germany. The HRSC experiment with its multiple stereo lines is designed with the goal in mind to improve these nominal values of exterior orientation by means of photogrammetric techniques. First, tie points are determined automatically via image matching. After that a bundle adjustment is carried out to improve the exterior orientation. Unfortunately image quality is frequently degraded by noise from the incoming signal and camera electronics and by <b>compression</b> <b>artefacts.</b> In case of a low signal-to-noise ratio it is difficult or even impossible to derive correct and an adequate amount of tie points with image matching techniques. This paper deals with image restoration using anisotropic diffusion to improve tie point extraction. The results of the matching, the distribution of the tie points in the images and the achieved accuracy are presented and evaluated on the basis of some selected Mars Express orbits...|$|R
40|$|Closed access. Owing to its wide-interoperability, {{stereoscopic}} 3 D video format in High Definition (HD) is {{a popular}} choice for 3 D entertainment media distribution. However, the delivery over bandwidth constrained networks exhibits challenges in terms of intermittent congestions in the network traffic, which enforce the delivery system to perform perception-aware coding to save bandwidth. In the scope of stereoscopic 3 D video, asymmetric quality adaptation {{has proved to be}} an effective method in terms of maintaining the perceived quality while reducing the required transmission bandwidth. On the other hand, Region-Of-Interest (ROI) based coding in accordance with the visual attention cues, which offers non-uniform quality assignment to regions of different saliency levels has not been widely studied in combination with asymmetric coding of stereoscopic 3 D video. In this work, the effectiveness of using visual attention aided non-uniform asymmetric 3 D video coding is explored. The importance of incorporating <b>compression</b> <b>artefacts</b> in the formulation of visual attention model is also revealed. The discussions in this paper are based on a comprehensive subjective test with 8 stereoscopic video sequences of different spatial and temporal characteristics at different conditions...|$|R
5000|$|Limerick's cable TV system descends from Westward Cable (later Irish Multichannel) and {{installation}} of the network {{began in the early}} 1980s. This covers most of the city as far as Castletroy on the south side and Caherdavin on the north side. Unfortunately it never reached some areas of the city (such as around Clare Street), and most housing developments built since then have not been linked to the network. [...] only analogue CATV is available on the system, with 18 unencrypted channels (including Channel 6 and Chorus TV) and some encrypted channels (such as Sky Sports 1 and Sky Movies 1) - customers are supplied with cumbersome old Jerrold/General Instrument decoders to avail of these extra channels. Signal quality varies greatly in areas, and is often quite poor due to the age of the cables and related equipment. MPEG <b>compression</b> <b>artefacts</b> are also noticeably high on some channels which are sourced from digital MMDS signals. Interruption of service is also quite frequent. Channels previously on VHF Band I frequencies were recently moved to Band III positions - this is {{to make way for the}} introduction of the cable broadband service.|$|R
40|$|Abstract. Compression {{systems like}} JPEG include {{optional}} pre-processing with filtering to avoid <b>compression</b> <b>artefacts.</b> At higher <b>compression</b> ratios a stronger filtering is needed that impacts the large scale image content. To preserve the large scale {{information we have}} previously proposed to use non-linear diffusion as a pre-processing for filtering out small scale details irrelevant at a given compression ratio and acting as noise. Now we compare typical diffusion processes applied before the blockwise DCT compression using the peak signal to noise ration (PSNR) as an objective quality measure. We give a simple measure of artefact reduction in terms of PSNR, and show that a considerable artefact reduction is achieved by pre-processing at the same bit rate as and with no greater error than the original compression. We did tests {{to see if the}} above artefact reduction implies a better subjective impression of quality. The images processed with the PSNR-based algorithm had nearly the same but greater PSNR value as the original compression. Subjects preferred noisy image content to the lack of small scale details, so the subjective preference of the images with reduced artefact is worse that of the original compression. Results suggest however that non-linear diffusion is more efficient for artefact reduction than non-adaptive smoothing like Gaussian filtering in terms of the subjective preference. ...|$|R
40|$|Compression {{schemes for}} 3 D images and -video gain {{much of their}} {{efficiency}} from transformations that convert the signal into forms suitable for quantization. The achieved compression efficiency is principally determined by rate-distortion analysis using objective quality evaluation metrics. In 2 D, quality evaluation metrics operating in the pixel domain implicitly assumes an ideal display modelled as a unity transformation. Similar simplifications are not feasible in 3 D analysis and different coding schemes introduce significantly different <b>compression</b> <b>artefacts</b> even though operating at the same rate-distortion ratio.   In this paper we have performed a subjective assessment {{of the quality of}} compressed 3 D images presented on an autostereoscopic display. In the qualitative part of the assessment different properties of the induced coding artefacts was identified with respect to image depth, pixelation, and zero-parallax distortion. The quantitative part was conducted using a group of non-expert observers that assessed the 3 D quality. In the results we show how the compression schemes introduce specific groups of artefacts manifesting with significantly different characteristics. In addition, each characteristic is derived from the transformation domains and the relationships between coding scheme and distortion property are presented. Moreover, the characteristics are related to the image quality assessment produced by the observation group...|$|R
40|$|This paper {{presents}} an alternative spatial image compression method {{that can be}} directly applied to compressing certain classes of grey-scale and/or colour (RGB, 24 bits) imagery of any size. The process involves the vectorisation of digital images into contour maps with subsequent converting of the contours to a pixel format. Contours are often approximated by polygons (linear or otherwise), and only the vertices of the polygons need to be retained to recreate the original contour lines. The final amount of storage depends crucially on {{the complexity of the}} image and on the degree of approximation desired. In this context, the compression process is based on filtering and eliminating those contours that may contain redundant information. Contours extracted from digital images may contain multiple redundant data (i. e. intersecting or nested contours), any of which might logically be used as a basis for discrimination and/or used in the reconstruction of an original captured image. Depending on the applications, and the amount of contour lines employed in the reconstruction of an image, the process allows for various levels of image accuracy while preserving visual integrity and reducing <b>compression</b> <b>artefacts.</b> The proposed method does not require special hardware and, if combined with existing encoding schemes, can be efficiently used for image transmission purposes. ...|$|R
40|$|The {{outer ear}} is an {{emerging}} biometric trait that has drawn {{the attention of}} the research community for more than a decade. The unique structure of the auricle is long known among forensic scientists and has been used for the identification of suspects in many cases. The next logical step towards a broader application of ear biometrics is to create automatic ear recognition systems. This work focuses on the usage of texture (2 D) and depth (3 D) data for improving the performance of ear recognition. We compare ear recognition systems using either texture or depth data with respect to segmentation and recognition accuracy, but also in the context of robustness to pose variations, signal degradation and throughput. We propose a novel segmentation method for ears where texture and surface information are fused in the feature space. We also provide a reliable method for geometric normalization of ear images and present a comparative study of different texture description method and the impact of their parametrization and the capture settings of a dataset. In this context, we propose a fusion scheme, were fixed length spectral histograms are created from texture and surface information. The proposed ear recognition system is integrated into a demonstrator system {{as a part of a}} novel identification system for forensics. The system is benchmarked against a challenging dataset that comprises of 3 D head models, mugshots and CCTV videos from four different perspectives. As a result of this work, we outline limitations of current ear recognition systems and provide possible direction for future applied research. Having a complete ear recognition system with optimized parameters, we measure impact of image quality on the accuracy during ear segmentation and ear recognition. These experiments focus on noise, blur and <b>compression</b> <b>artefacts</b> and are hence only conducted on 2 D data. We show that blur has a smaller impact on the system performance than noise. In scenarios where we work with compressed images, we show that the performance can be improved by optimizing the size of local image patches for feature extraction and the size of the <b>compression</b> <b>artefacts.</b> This thesis is concluded by work on automatic classification of ears for the purpose of narrowing the search space in large datasets. We show that classification of ears using texture descriptors is possible. Furthermore, we show that the class label is influenced by the skin tone, but also by the capture settings of the dataset. In further work, we propose a method for the extraction of binary feature vectors of texture descriptors and their application in a 2 -stage search system. We show that our 2 -stage system improves the recognition performance, because it removes images from the search space that would otherwise have caused recognition errors in the second stage...|$|R
40|$|Information and {{entertainment}} signals presented through a vehicle's sound system are superposed by environmental noises. Consequently, {{a mixture of}} dynamically changing audio signals and dynamically changing noises is received by the listener's ears in the car cabin. In addition to this variant character of signal-to-noise ratio - making audibility and speech intelligibility extremely difficult - today's vehicle infotainment systems offer the capability to reproduce {{a wide variety of}} entertainment signal sources. As a result, signals fed to the infotainment-system through the air, by cable, or by data storage media feature wide ranges of input levels and dynamic compressions. Additionally, human perception of loudness is not linearly dependent on level. Elaborate methods are necessary for comfortable listening in cars, ensuring no need of continuous re-adjusting of volume settings. This paper presents an approach to solve these issues by dynamical alignment of sound sig nals to an average loudness, depending on surrounding noises and input audio signal loudness. The method guarantees all audio signals being presented within a loudness corridor while preserving the dynamics of music. Only gain is adjusted, since signal analysis and alignment is done in full band processing. As a result no spectral coloration or dynamic <b>compression</b> <b>artefacts</b> are generated by the algorithm; a computationally cheap implementation was realized, as also depicted in [1] and [2]. This work was performed at the department "Center of Competence Multimedia", Harman/Becker Automotive Systems, Karlsbad, Germany. Â© VDE VERLAG GMBH â¢ Berlin â¢ Offenbach...|$|R
40|$|International audienceOrganisms entombed in amber for {{millions}} of years are usually preserved with unmatched fidelity, i. e. showing minute details and without <b>compression</b> <b>artefacts.</b> But often the amber matrix is not perfectly clear, or the view of the fossil is obscured by bubbles, cracks, debris, or other inclusions, without possibility of a further polishing of the amber surface. Three-dimensional digital restoration of amber inclusions using microtomography is increasingly used to overcome these limitations. Most studies have focused on one or few particular fossils visualized in more or less translucent amber prior to X-ray analyses, to help with the systematic description and phylogenetic analyses. But amber deposits usually contain several thousands of pieces, for which the screening of the fossil content can last for years. In some deposits also, a significant portion of amber is fully opaque, and inclusions are invisible with conventional microscopes, leading to evident bias in paleoecological studies. Propagation phase-contrast X-ray synchrotron imaging techniques have been developed at the European Synchrotron Radiation Facility (ESRF; Grenoble, France), that are powerful tools to access inclusions regardless of the amber opacity. Among these, an optimized microradiographic protocol allows a rapid survey of large amounts of amber pieces. The technique can be used for the overview of any deposit with opaque or transparent pieces of amber, followed by 3 D microtomographic reconstructions applied to the most valuable fossils. The ESRF uniquely maintains an open-access paleontological database ([URL] with all tomographic data (original stacks of slices and processed data) of each specimen published...|$|R
40|$|BACKGROUND: Guidelines {{recommend}} 2 min of CPR after defibrillation attempts {{followed by}} ECG analysis during chest compression pause. This pause {{may reduce the}} likelihood of return of spontaneous circulation (ROSC) and survival. We have evaluated the possibility of analysing the rhythm earlier in the CPR cycle in an attempt to replace immediate pre-shock rhythm analysis. METHODS AND RESULTS: The randomized Circulation Improving Resuscitation Care (CIRC) trial included patients with out of hospital cardiac arrest of presumed cardiac aetiology. Defibrillator data were used to categorize ECG rhythms as shockable or non-shockable 1 min post-shock and immediately before next shock. ROSC was determined from end-tidal CO 2, transthoracic impedance (TTI), and patient records. TTI was used to identify chest <b>compressions.</b> <b>Artefact</b> free ECGs were categorized during periods without chest compressions. Episodes without ECG or TTI data or with undeterminable ECG rhythm were excluded. Data were analyzed using descriptive statistics. Of 1657 patients who received 3409 analysable shocks, the rhythm was shockable in 1529 (44. 9 %) cases 1 min post-shock, 13 (0. 9 %) of which were no longer shockable immediately prior to next possible shock. Of these, three had converted to asystole, seven to PEA and three to ROSC. CONCLUSION: While a shockable rhythm 1 min post-shock was present also immediately before next possible defibrillation attempt in most cases, three patients had ROSC. Studies are needed to document if moving the pre-shock rhythm analysis will increase shocks delivered to organized rhythms, and if it will increase shock success and survival...|$|R
40|$|Nowadays, video {{surveillance}} systems apply video compression to reduce bandwith and storage cost. However, generally, these video sequences are the input, after decoding, for video analysis modules. H. 264 /AVC {{is the newest}} video standard and {{is assumed to be}} omnipresent in {{video surveillance}} systems in the near future. Since the video <b>compression</b> introduces <b>artefacts,</b> which influence the performance of these analysis modules, it is important to make a quantitative evaluation of this effect. Hence, in this paper we present the first quantitative analysis of the effect that H. 264 /AVC compression has upon a generally accepted moving object detection technique. We analyze different encoding schemes and show the influence on the object detection results for different representative sequences...|$|R
40|$|This {{thesis is}} {{concerned}} with maximizing the coding efficiency, random accessibility and visual performance of scalable compressed video. The unifying theme behind this work {{is the use of}} finely embedded localized coding structures, which govern {{the extent to which these}} goals may be jointly achieved. The first part focuses on scalable volumetric image compression. We investigate 3 D transform and coding techniques which exploit inter-slice statistical redundancies without compromising slice accessibility. Our study shows that the motion-compensated temporal discrete wavelet transform (MC-TDWT) practically achieves an upper bound to the compression efficiency of slice transforms. From a video coding perspective, we find that most of the coding gain is attributed to offsetting the learning penalty in adaptive arithmetic coding through 3 D code-block extension, rather than inter-frame context modelling. The second aspect of this thesis examines random accessibility. Accessibility refers to the ease with which a region of interest is accessed (subband samples needed for reconstruction are retrieved) from a compressed video bitstream, subject to spatiotemporal code-block constraints. We investigate the fundamental implications of motion compensation for random access efficiency and the compression performance of scalable interactive video. We demonstrate that inclusion of motion compensation operators within the lifting steps of a temporal subband transform incurs a random access penalty which depends on the characteristics of the motion field. The final aspect of this thesis aims to minimize the perceptual impact of visible distortion in scalable reconstructed video. We present a visual optimization strategy based on distortion scaling which raises the distortion-length slope of perceptually significant samples. This alters the codestream embedding order during post-compression rate-distortion optimization, thus allowing visually sensitive sites to be encoded with higher fidelity at a given bit-rate. For visual sensitivity analysis, we propose a contrast perception model that incorporates an adaptive masking slope. This versatile feature provides a context which models perceptual significance. It enables scene structures that otherwise suffer significant degradation to be preserved at lower bit-rates. The novelty in our approach derives from a set of "perceptual mappings" which account for quantization noise shaping effects induced by motion-compensated temporal synthesis. The proposed technique reduces wavelet <b>compression</b> <b>artefacts</b> and improves the perceptual quality of video...|$|R
30|$|In {{this article}} the {{challenges}} of breast imaging practice are divided into three areas: technical performance, quality of practices, and the patient-centred way of working. The challenges related to technical performance and quality of practices like the issues of positioning, breast <b>compression,</b> contrast, noise, <b>artefacts,</b> sharpness, and labelling, have been {{known for a long}} time, and identified also in the studies performed outside the European context [26, 32 – 34, 40 – 43]. The introduction of digital mammography in the practice brought challenges in breast positioning, mainly due to a bigger breast support according to radiographers [32]. Small breasts and shorter clients are difficult to position without the superimposition of the arm and/or the abdominal wall.|$|R
40|$|Long interruptions of {{cardiopulmonary}} resuscitation (CPR) {{in case of}} a sudden cardiac arrest result in higher failure rate of resuscitation. The current work concerns the filtering of the chest <b>compression</b> (CC) <b>artefacts</b> during CPR, which is essential for the CPR continuation during electrocardiogram (ECG) analysis by automated external defibrillators (AEDs). We have studied two possible approaches - one based on high-pass filter (HPF), and another using band-stop filter (BSF) with adjustable cut-off frequency. The purpose is {{to improve the quality of}} the signal provided to the ECG analysis module, aiming at a reliable decision to Stop CC if VF is present or to Continue CC for all other rhythms, including asystole (ASYS) or 'normal' rhythms with ventricular complexes (NR). The two filters are tested with artificially constructed ECG+CC signals, as well as with real ECGs recorded during CPR. The HPF passes the high-frequency components of the QRS complexes and effectively suppresses CC artefacts. This allows correct recognition of NR and ASYS. However, HPF suppresses the VF amplitude thus compromising the VF detection sensitivity. The BSF is favorable for detection of NR and VF but presents problems for ASYS detection because there are often attending residual high-frequency components belonging to the CC artefacts...|$|R
40|$|Image {{and video}} <b>compression</b> {{introduces}} distortions (<b>artefacts)</b> to the coded image. The most prominent artefacts added are blockiness and blurriness. Many existing quality meters are normally distortion-specific. This paper proposes an objective quality meter for quantifying the combined blockiness and blurriness distortions in frequency domain. The model first applies edge detection and cancellation, then spatial masking {{to mimic the}} characteristics of the human visual system. Blockiness is then estimated by transforming image into frequency domain, followed by finding the ratio of harmonics to other AC components. Blurriness is determined by comparing the high frequency coefficients of the reference and coded images {{due to the fact that}} blurriness reduces the high frequency coefficients. Then, both blockiness and blurriness distortions are combined for a single quality metric. The meter is tested on blocky and blurred images from the LIVE image database, with a correlation coefficient of 95 - 96 %...|$|R
40|$|Identity {{verification}} {{systems are}} an important part of our every day life. A typical example is the Automatic Teller Machine (ATM) which employs a simple identity verification scheme: the user is asked to enter their secret password after inserting their ATM card; if the password matches the one prescribed to the card, the user is allowed access to their bank account. This scheme suffers from a major drawback: only the validity of the combination of a certain possession (the ATM card) and certain knowledge (the password) is verified. The ATM card can be lost or stolen, and the password can be compromised. Thus new verification methods have emerged, where the password has either been replaced by, or used in addition to, biometrics such as the person's speech, face image or fingerprints. Apart from the ATM example described above, biometrics can be applied to other areas, such as telephone & internet based banking, airline reservations & check-in, as well as forensic work and law enforcement applications. Biometric systems based on face images and/or speech signals have been shown to be quite effective. However, their performance easily degrades in the presence of a mismatch between training and testing conditions. For speech based systems this is usually in the form of channel distortion and/or ambient noise; for face based systems it can be in the form of a change in the illumination direction. A system which uses more than one biometric at the same time is known as a multi-modal verification system; it is often comprised of several modality experts and a decision stage. Since a multi-modal system uses complimentary discriminative information, lower error rates can be achieved; moreover, such a system can also be more robust, since the contribution of the modality affected by environmental conditions can be decreased. This thesis makes several contributions aimed at increasing the robustness of single- and multi-modal verification systems. Some of the major contributions are listed below. The robustness of a speech based system to ambient noise is increased by using Maximum Auto-Correlation Value (MACV) features, which utilize information from the source part of the speech signal. A new facial feature extraction technique is proposed (termed DCT-mod 2), which utilizes polynomial coefficients derived from 2 D Discrete Cosine Transform (DCT) coefficients of spatially neighbouring blocks. The DCT-mod 2 features are shown to be robust to an illumination direction change as well as being over 80 times quicker to compute than 2 D Gabor wavelet derived features. The fragility of Principal Component Analysis (PCA) derived features to an illumination direction change is solved by introducing a pre-processing step utilizing the DCT-mod 2 feature extraction. We show that the enhanced PCA technique retains all the positive aspects of traditional PCA (that is, robustness to <b>compression</b> <b>artefacts</b> and white Gaussian noise) while also being robust to the illumination direction change. Several new methods, for use in fusion of speech and face information under noisy conditions, are proposed; these include a weight adjustment procedure, which explicitly measures the quality of the speech signal, and a decision stage comprised of a structurally noise resistant piece-wise linear classifier, which attempts to minimize the effects of noisy conditions via structural constraints on the decision boundary. Keywords: biometrics, speaker recognition, speaker verification, face recognition, face verification, feature extraction, multi-modal verification, fusion, noise resistance, gaussian mixture model...|$|R
40|$|Rapid {{and ongoing}} {{developments}} {{in the field of}} computer-generated imagery (CGI) have reached a point where the synthetic image strives to become almost indistinguishable from the real. This desire for realism in the filmic image has seen CGI thrive in mainstream popular culture: from Hollywood cinema through to advertising. Large visual effects (VFX) and CGI animation companies are often at the vanguard of research and development in this area, driving CGI development toward the production of realism. This over-investment in ‘realistic’ imaging has all but erased the potential for broad experimentation in CGI techniques and aesthetics, and has seen the emergence of an aesthetic realism manifested by the eradication of error and materiality, wherein all traces of the process of production are excised from the image. This research project explores the technical and aesthetic effects of the erasure of error in contemporary CGI and how this affects the capacity of CGI to approach the abject as defined by Julia Kristeva. Focusing on the ‘trace’ in motion capture and its relationship {{to the power of the}} close-up image in film and photographic contexts, I look at how digital artefacts can exist as a critical part of a process of experimentation in CGI. These ‘errors’ take the form of unprocessed motion capture data, unfiltered, <b>compression</b> image <b>artefacts</b> and low-sample rate renders, as well as reduced polygonal modelling techniques. Exploring the production of images that embrace these ‘errors’ affords the possibility to introduce ‘fallibility’ and embodied artefacts into the system of image production, and reveal both a material trace of the technology as well as the artist. This research identifies technical and aesthetic processes that introduce a greater degree of haptic visuality into CGI images, as a means of approaching abjection. These experiments in motion capture techniques, code transformations, and close-up lenticular imagery, introduce error and materiality into CGI in order to interrogate the material field and affective (or abjective) power of the CGI close-up. Through these practical experiments, I argue that the viewer is afforded greater access to haptic visuality: a tactile experience of images which is crucial to experiencing the abject, and which moves beyond mainstream CGI's use of grotesque imagery. Further, these experiments reveal the possibilities for the creation of new and engaging CGI aesthetic forms that embrace the digital artefact, and which re-insert the abject body and trace into the digital realm...|$|R
