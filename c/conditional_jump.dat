75|95|Public
25|$|One new {{technology}} {{included in the}} design is Macro-Ops Fusion, which combines two x86 instructions into a single micro-operation. For example, a common code sequence like a compare followed by a <b>conditional</b> <b>jump</b> would become a single micro-op. Unfortunately, this technology does not work in 64-bit mode.|$|E
2500|$|In {{interpreted}} programming languages, for-loops can {{be implemented}} in many ways. Oftentimes, the for-loops are directly translated to assembly-like compare instructions and <b>conditional</b> <b>jump</b> instructions. However, {{this is not always}} so. In some interpreted programming languages, for-loops are simply translated to while-loops. For instance, take the following Mint/Horchata code: ...|$|E
2500|$|To {{these four}} both Wang (1954, 1957) and then C.Y. Lee (1961) added another {{instruction}} from the Post set { ERASE }, {{and then a}} Post's unconditional jump { JUMP_to_ instruction_z } (or to make things easier, the <b>conditional</b> <b>jump</b> JUMP_IF_blank_to_instruction_z, or both. Lee named this a [...] "W-machine" [...] model: ...|$|E
40|$|The overall prrformance of {{supercomputers}} is slow {{compared to}} the speed of their underly-ing logic technology. This discrepancy is due to several bottlenecks: memories are slower than the CPU, <b>conditional</b> <b>jumps</b> limit the usefulness of pipelining and pre-fetching mechanisms, and functional-unit parallelism {{is limited by the}} speed of hardware scheduling. This paper describes a supercomputer architecture called Ring of Pre-fetch Elements (ROPE) that attempts to solve the problems of memory latency and <b>conditional</b> <b>jumps,</b> without hardware scheduling. ROPE consists of a very pipelined CPU data path with a new instruction pre-fetching mechanism that supports general multi-way <b>conditional</b> <b>jumps.</b> An optimizing compiler based on a global code transformation technique (Percolation Scheduling or PS) gives high performance without scheduling hardware...|$|R
40|$|Supercomputer {{architectures}} {{are not as}} fast as logic technology allows because {{memories are}} slow than the CPU, <b>conditional</b> <b>jumps</b> limit the usefulness of pipelining and prefetching mechanisms, and functional-unit parallelism {{is limited by the}} speed of hardware scheduling. We propose a supercomputer architecture called Ring Of Prefetch Elements (ROPE) that attempts to solve the problems of memory latency and <b>conditional</b> <b>jumps</b> without hardware scheduling. ROPE consists of a pipelined CPU or very-large-instruction-word data path with a new instruction prefetching mechanism that supports general multi-way <b>conditional</b> <b>jumps.</b> To get high-performance without scheduling hardware, ROPE relies on an optimizing compiler based on a global code transformation technique (Percolation Scheduling). This paper describes both the promise and the limitations of ROPE...|$|R
5000|$|Jump threading: In this pass, {{consecutive}} <b>conditional</b> <b>jumps</b> predicated entirely or partially on {{the same}} condition are merged.|$|R
2500|$|Beyond general {{algorithms}} {{and their}} implementation on an abstract machine, concrete source code level choices {{can make a}} significant difference. For example, on early C compilers, while(1) was slower than for( [...] ;;) for an unconditional loop, because while(1) evaluated 1 and then had a <b>conditional</b> <b>jump</b> which tested if it was true, while for ( [...] ;;) had an unconditional jump [...] Some optimizations (such as this one) can nowadays be performed by optimizing compilers. This depends on the source language, the target machine language, and the compiler, and can be both difficult to understand or predict and changes over time; this is a key place where understanding of compilers and machine code can improve performance. Loop-invariant code motion and return value optimization are examples of optimizations that {{reduce the need for}} auxiliary variables and can even result in faster performance by avoiding round-about optimizations.|$|E
50|$|A <b>conditional</b> <b>jump</b> that {{controls}} a loop is best predicted {{with a special}} loop predictor. A <b>conditional</b> <b>jump</b> {{in the bottom of}} a loop that repeats N times will be taken N-1 times and then not taken once. If the <b>conditional</b> <b>jump</b> is placed at the top of the loop, it will be not taken N-1 times and then taken once. A <b>conditional</b> <b>jump</b> that goes many times one way and then the other way once is detected as having loop behavior. Such a <b>conditional</b> <b>jump</b> can be predicted easily with a simple counter. A loop predictor is part of a hybrid predictor where a meta-predictor detects whether the <b>conditional</b> <b>jump</b> has loop behavior.|$|E
5000|$|Two-way {{branching}} {{is usually}} implemented with a <b>conditional</b> <b>jump</b> instruction. A <b>conditional</b> <b>jump</b> {{can either be}} [...] "not taken" [...] and continue execution with the first branch of code which follows immediately after the <b>conditional</b> <b>jump,</b> {{or it can be}} [...] "taken" [...] and jump to a different place in program memory where the second branch of code is stored. It is not known for certain whether a <b>conditional</b> <b>jump</b> will be taken or not taken until the condition has been calculated and the <b>conditional</b> <b>jump</b> has passed the execution stage in the instruction pipeline (see fig. 1).|$|E
50|$|The {{multiplication}} algorithm {{is based on}} addition and subtraction, uses the function G and does not have <b>conditional</b> <b>jumps</b> nor branches. Cryptoleq encryption is based on Paillier cryptosystem.|$|R
40|$|We {{establish}} a lower bound of 2 n <b>conditional</b> <b>jumps</b> for deciding the satisfiability of the conjunction of any two Boolean formulas from a set called a full representation of Boolean functions of n variables- a set containing a Boolean formula to represent each Boolean function of n variables. The contradiction proof first assumes that {{there exists a}} RAM program that correctly decides the satisfiability of the conjunction of any two Boolean formulas from such a set by following an execution path that includes fewer than 2 n <b>conditional</b> <b>jumps.</b> By using multiple runs of this program, with one run for each Boolean function of n variables, the proof derives a contradiction by showing that this program is unable to correctly decide the satisfiability of the conjunction {{of at least one}} pair of Boolean formulas from a full representation of n-variable Boolean functions if the program executes fewer than 2 n <b>conditional</b> <b>jumps.</b> This lower bound of 2 n <b>conditional</b> <b>jumps</b> holds for any full representation of Boolean functions of n variables, even if a full representation consists solely of minimized Boolean formulas derived by a Boolean minimization method. We discuss why the lower bound fails to hold for satisfiability of certain restricted formulas, such as 2 CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate the lower bound to 3 CNF satisfiability. ...|$|R
25|$|Improving on {{the basic}} Z2 machine, he built the Z3 in 1941. On 12 May 1941 Zuse {{presented}} the Z3, built in his workshop, to the public. The Z3 was a binary 22-bit floating point calculator featuring programmability with loops but without <b>conditional</b> <b>jumps,</b> with memory and a calculation unit based on telephone relays. The telephone relays used in his machines were largely collected from discarded stock. Despite the absence of <b>conditional</b> <b>jumps,</b> the Z3 was a Turing complete computer. However, Turing-completeness was never considered by Zuse (who had practical applications in mind) and only demonstrated in 1998 (see History of computing hardware).|$|R
5000|$|... { Increment (r), Decrement (r), Clear (r); Copy (rj,rk), <b>conditional</b> <b>Jump</b> if {{contents}} of r=0, <b>conditional</b> <b>Jump</b> if rj=rk, unconditional Jump, HALT } ...|$|E
50|$|The {{first time}} a <b>conditional</b> <b>jump</b> {{instruction}} is encountered, {{there is not much}} information to base a prediction on. But the branch predictor keeps records of whether branches are taken or not taken. When it encounters a <b>conditional</b> <b>jump</b> that has been seen several times before then it can base the prediction on the history. The branch predictor may, for example, recognize that the <b>conditional</b> <b>jump</b> is taken more often than not, or that it is taken every second time.|$|E
50|$|A local branch {{predictor}} has {{a separate}} history buffer for each <b>conditional</b> <b>jump</b> instruction. It may use a two-level adaptive predictor. The history buffer is separate for each <b>conditional</b> <b>jump</b> instruction, while the pattern history table may be separate {{as well or}} it may be shared between all conditional jumps.|$|E
5000|$|The LZMA SDK {{comes with}} the BCJ / BCJ2 {{preprocessor}} included, so that later stages are able to achieve greater compression: For x86, ARM, PowerPC (PPC), IA-64 Itanium, and ARM Thumb processors, jump targets are normalized before compression by changing relative position into absolute values. For x86, this means that near <b>jumps,</b> calls and <b>conditional</b> <b>jumps</b> (but not short <b>jumps</b> and <b>conditional</b> <b>jumps)</b> are converted from the machine language [...] "jump 1655 bytes backwards" [...] style notation to normalized [...] "jump to address 5554" [...] style notation; all jumps to 5554, perhaps a common subroutine, are thus encoded identically, making them more compressible.|$|R
50|$|<b>Conditional</b> <b>jumps</b> {{should be}} used in the syntax:{{condition}}''''statement 1:statement 2When condition is true, statement 1 is executed, then statement 2 is executed. If condition is false, statement 1 is skipped and statement 2 is executed.|$|R
50|$|Most authors {{pick one}} or the other of the <b>conditional</b> <b>jumps,</b> e.g. Shepherdson-Sturgis (1963) use the above set minus JE (to be {{perfectly}} accurate they use JNZJump if Not Zero in place of JZ; yet another possible convenience instruction).|$|R
50|$|Among the x86 instructions, {{some use}} {{implicit}} registers {{for one of}} the operands or results (multiplication, division, counting <b>conditional</b> <b>jump).</b>|$|E
50|$|When {{a branch}} is evaluated, the {{corresponding}} state machine is updated. Branches evaluated as not taken decrement the state toward strongly not taken, and branches evaluated as taken increment the state toward strongly taken. The {{advantage of the}} two-bit counter over a one-bit scheme is that a <b>conditional</b> <b>jump</b> has to deviate twice from what it has done most in the past before the prediction changes. For example, a loop-closing <b>conditional</b> <b>jump</b> is mispredicted once rather than twice.|$|E
5000|$|NUS (sr. Nula-Skok, en. Zero Jump) {{performs}} a <b>conditional</b> <b>jump</b> {{to the address}} specified by the parameter if the current value of the accumulator is zero ...|$|E
25|$|While Babbage's {{machines}} were mechanical and unwieldy, their basic architecture {{was similar to}} a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make <b>conditional</b> <b>jumps,</b> and the machine had a separate I/O unit.|$|R
50|$|Program flow may be {{influenced}} by special 'jump' instructions that transfer execution to an instruction other than the numerically following one. <b>Conditional</b> <b>jumps</b> are taken (execution continues at another address) or not (execution continues at the next instruction) depending on some condition.|$|R
50|$|This is {{particularly}} useful {{in connection with}} jumps, because typical jumps are to nearby instructions (in a high-level language most if or while statements are reasonably short). Measurements of actual programs suggest that an 8 or 10 bit offset is large enough for some 90% of <b>conditional</b> <b>jumps</b> (roughly &plusmn;128-&plusmn;512 bytes).|$|R
5000|$|NES (sr. Negativni Skok, en. Negative Jump) {{performs}} a <b>conditional</b> <b>jump</b> {{to the address}} specified by the parameter if the current value of the accumulator is negative ...|$|E
50|$|Flow {{control is}} {{facilitated}} through {{a group of}} one unconditional and twelve <b>conditional</b> <b>Jump</b> instructions. Jump targets are relative to PC with an offset of -128 to +127 word addresses.|$|E
50|$|The Intel Pentium MMX, Pentium II and Pentium III have local branch {{predictors}} {{with a local}} 4-bit {{history and}} a local pattern history table with 16 entries for each <b>conditional</b> <b>jump.</b>|$|E
50|$|As shown, much of {{the text}} of a typical PDP-8 program focuses not on the author's {{intended}} algorithm but on low-level mechanics. An additional readability problem is that in <b>conditional</b> <b>jumps</b> {{such as the one}} shown above, the conditional instruction (which skips around the JMP) highlights the opposite of the condition of interest.|$|R
5000|$|Previously {{executed}} {{instructions were}} saved in an eight-word cache, called the [...] "stack". In-stack jumps were quicker than out-of-stack jumps because no memory fetch was required. The stack was flushed by an unconditional jump instruction, so unconditional jumps {{at the ends}} of loops were conventionally written as <b>conditional</b> <b>jumps</b> that would always succeed.|$|R
50|$|The {{parity flag}} is usually used in <b>conditional</b> <b>jumps,</b> where e.g. the JP {{instruction}} jumps to the given target when the parity flag is {{set and the}} JNP instruction jumps {{if it is not}} set. The flag may be also read directly with instructions such as PUSHF, which pushes the flags register on the stack.|$|R
50|$|Arithmetic-based Turing-complete {{machines}} use an {{arithmetic operation}} and a <b>conditional</b> <b>jump.</b> Like {{the two previous}} universal computers, this class is also Turing-complete. The instruction operates on integers which may also be addresses in memory.|$|E
5000|$|<b>Conditional</b> <b>jump</b> if r is positive; i.e. IF r > 0 THEN jump to {{instruction}} z else continue in sequence (Cook and Reckhow call this: [...] "TRAnsfer control to line m if Xj > 0") ...|$|E
50|$|Without branch prediction, the {{processor}} {{would have to}} wait until the <b>conditional</b> <b>jump</b> instruction has passed the execute stage before the next instruction can enter the fetch stage in the pipeline. The branch predictor attempts to avoid this waste of time by trying to guess whether the <b>conditional</b> <b>jump</b> is most likely to be taken or not taken. The branch that is guessed to be the most likely is then fetched and speculatively executed. If it is later detected that the guess was wrong then the speculatively executed or partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.|$|E
50|$|This {{scheme is}} only {{better than the}} saturating counter scheme for large table sizes, and it is rarely as good as local prediction. The history buffer must be longer {{in order to make}} a good prediction. The size of the pattern history table grows {{exponentially}} with the size of the history buffer. Hence, the big pattern history table must be shared among all <b>conditional</b> <b>jumps.</b>|$|R
5000|$|Given that, [...] {{generates the}} code below (example.c).The {{contents}} of the comment [...] are substituted with a deterministic finite automatonencoded {{in the form of}} <b>conditional</b> <b>jumps</b> and comparisons; the rest of the program is copied verbatim into the output file.There are several code generation options; normally re2c uses [...] statements,but it can use nested [...] statements (as in this example with [...] option),or generate bitmaps and jump tables.Which option is better depends on the C compiler;re2c users are encouraged to experiment.|$|R
2500|$|The 68000 comparison, arithmetic, {{and logic}} {{operations}} set bit flags in a status register {{to record their}} results for use by later <b>conditional</b> <b>jumps.</b> The bit flags are [...] "zero" [...] (Z), [...] "carry" [...] (C), [...] "overflow" [...] (V), [...] "extend" [...] (X), and [...] "negative" [...] (N). The [...] "extend" [...] (X) flag deserves special mention, because it is separate from the carry flag. This permits the extra bit from arithmetic, logic, and shift operations {{to be separated from}} the carry for flow-of-control and linkage.|$|R
