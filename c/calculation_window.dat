11|61|Public
30|$|According to {{the actual}} effect test, by {{properly}} selecting the <b>calculation</b> <b>window,</b> the 3 [*]×[*] 3 target centroid accuracy is better than 0.25 pixels. Compared with the classical gravity center algorithm, the weighted centroid method is simple, practical, and has strong anti-interference ability.|$|E
40|$|The transverse-magnetic to transverse-electric (TM-TE) mode {{coupling}} properties and the lateral leakage loss {{behavior of the}} propagating TM-like mode in silicon-on-insulator thin-ridge waveguides and directional couplers are investigated. Accurate calculation of the lateral leakage is performed by a mode matching technique in which the <b>calculation</b> <b>window</b> is left fully open in the lateral direction...|$|E
40|$|Bibliometric {{indicators}} increasingly affect careers, funding, {{and reputation}} of individuals, their institutions and journals themselves. In contrast to author self-citations, {{little is known}} about kinetics of journal self-citations. Here we hypothesized that they may show a generalizable pattern within particular research fields or across multiple fields. We thus analyzed self-cites to 60 journals from three research fields (multidisciplinary sciences, parasitology, and information science). We also hypothesized that the kinetics of journal self-citations and citations received from other journals of the same publisher may differ from foreign citations. We analyzed the journals published the American Association for the Advancement of Science, Nature Publishing Group, and Editura Academiei Române. We found that although the kinetics of journal self-cites is generally faster compared to foreign cites, it shows some field-specific characteristics. Particularly in information science journals, the initial increase in a share of journal self-citations during post-publication year 0 was completely absent. Self-promoting journal self-citations of top-tier journals have rather indirect but negligible direct effects on bibliometric indicators, affecting just the immediacy index and marginally increasing the impact factor itself as long as the affected journals are well established in their fields. In contrast, other forms of journal self-citations and citation stacking may severely affect the impact factor, or other citation-based indices. We identified here a network consisting of three Romanian physics journals Proceedings of the Romanian Academy, Series A, Romanian Journal of Physics, and Romanian Reports in Physics, which displayed low to moderate ratio of journal self-citations, but which multiplied recently their impact factors, and were mutually responsible for 55. 9 %, 64. 7 % and 63. 3 % of citations within the impact factor <b>calculation</b> <b>window</b> to the three journals, respectively. They did not receive nearly any network self-cites prior impact factor <b>calculation</b> <b>window,</b> and their network self-cites decreased sharply after the impact factor <b>calculation</b> <b>window.</b> Journal self-citations and citation stacking requires increased attention and elimination from citation indices...|$|E
5000|$|GraphPad StatMate {{performs}} {{power and}} sample size <b>calculations</b> (<b>Windows</b> and Mac).|$|R
5000|$|Sliding <b>window</b> <b>calculations</b> for packet {{acknowledgement}} and congestion control.|$|R
40|$|Digital Image Correlation (DIC) {{has been}} {{proven to be a}} highly {{reliable}} framework for the full-field displacement and strain measurement of materials that undergo deformation when subjected to physical stresses. This paper presents a new method that extends the popular Newton-Raphson algorithm through the inclusion of spatial regularization in the minimization process used to obtain the motion data. The basic principle is that the motion data is calculated between corresponding blocks in the reference and deformed images using adaptively previously obtained motion estimates in the immediate vicinity of the respective location along with the local block-based image information. The results indicate significant accuracy improvements over the classic approach especially when the block sizes and strain <b>calculation</b> <b>windows</b> used for motion and strain estimation decrease in size...|$|R
30|$|Both Fourier Modal Method (FMM) [16 – 18] and Finite Difference based mode solver, OptiMode by OptiWave [19, 20], {{are used}} in this work to {{determine}} the optical mode profiles of the waveguides (slab or channels). In both cases a <b>calculation</b> <b>window</b> of 10  μm[*]×[*] 3.5  μm is taken into account with a mesh size of 20  nm in x-direction and 5  nm in y-direction. For FMM calculations, 5 and 200 harmonics are considered to increase the precision of the result and, as mentioned above, reduce the ripples coming from Gibbs phenomenon.|$|E
40|$|An {{approximate}} entropy {{feature is}} tested with parameters appropriate for online BCI - a short <b>calculation</b> <b>window</b> {{and use of}} the running standard deviation of the EEG signal. Features are extracted from self-paced real movement data, with various values of the embedding dimension and tolerance of comparison. Two alternative features, band power and reflection coefficients, are extracted for comparative purposes. Class separability is measured using classiffication results from k-means clustering for individual features and linear discriminant analysis for multiple features, as selected by sequential forward floating search. Results show this method of calculating approximate entropy to be a candidate for online movement detection in self-paced BCI systems...|$|E
30|$|The image {{post-processing}} technology {{based on}} MATLAB was applied in oil-film interferometry {{to calculate the}} skin friction coefficient of smooth and roughness flat plate. This method can recognize the bright and dark fringes efficiently and robustly by the standard deviation-based algorithm. Furthermore, it can process low-quality images and extract fringe spacing by converted grayscale image and the proper sliding window can catch the discrete objectives precisely by multiple selected windows. In addition, the <b>calculation</b> <b>window</b> size can adjust automatically, which improved the robustness of the algorithm greatly. By comparing the experimental results with the numerical results, it is proved that the method of the image post-processing based on MATLAB can accurately measure the skin friction in hypersonic flow.|$|E
3000|$|In {{the full}} cycle <b>window</b> <b>calculation</b> mode, one {{complete}} cycle data (i.e., half cycle data before and half cycle data after the fault incipient) of the E [...]...|$|R
3000|$|... {{are passed}} through the TFR technique, where as in the half cycle <b>window</b> <b>calculation</b> mode, half cycle data (i.e., quarter cycle data before and quarter cycle data after fault incipient) of the E [...]...|$|R
50|$|Braina is an {{intelligent}} personal assistant application for Microsoft Windows developed by Brainasoft. Braina uses {{natural language interface}} and speech recognition to interact with its users and allows users to use English language sentences to perform various tasks on their computer. The application can find information from the internet, play songs and videos of user's choice, take dictation, find and open files, set alarms and reminders, performs math <b>calculations,</b> controls <b>windows</b> and programs etc. Braina's Android app {{can be used to}} interact with the system remotely over a Wi-Fi network.|$|R
40|$|Quantification {{of surface}} sciences. Slopes {{measured}} from digital elevation models or other topographic data sets depend strongly {{on the length}} scale or window size used in the slope calculations. The spectrum of slope distributions {{as a function of}} length scale is related to the variation relief as a function of scale, and may reflect the length scales the processes acting to form the surface morphology. Small window sizes, approaching the grid spacing, can yield reliable slope determinations if good quality data sets are available. There is a strong variation in the data quality among DEM's, with a second DEM poorly representing slopes in the Hills, California area, The slopes measured from a standard 30 -m DEM and high-resolution TOPSAR DEM match each other well at length-scales greater than 30 means for the rugged Verdugo Hills change from 15 to 26 as the <b>calculation</b> <b>window</b> size decreases from 270 m to 15 m. Importance of Slope The accurate measurement of the slope angle of the [...] ...|$|E
40|$|In telecommunication, optical chips modulate, switch or amplify light, {{enabling}} a {{large amount}} of data to be transmitted through optical fibers. Moreover, such chips are also used in very sensitive medical and environmental sensors. Instead of electrons, optical chips handle photons; they manipulate light on micro- and nanoscales. Designers need to simulate how light behaves in their chips, but in most cases, calculating fully three-dimensional vectorial solutions of Maxwell's equations is computationally too expensive. This thesis attempts to reduce the computational cost of such simulations by globally expanding the field in one spatial direction in the modes of some cross-section(s). A variational procedure is applied to obtain a system of coupled partial differential equations in the other one or two directions - effectively reducing the dimensionality of the simulations by one. The system is solved by means of semi-analytical or finite element methods. The procedure is applied to scalar and vectorial mode solvers, and 2 D and 3 D scattering problems. For the scattering problems, special attention is paid to the boundary conditions; the boundaries of the <b>calculation</b> <b>window</b> are transparent for outgoing radiation, while allowing influx to be prescribed. When using one or a low number of modes in the expansion, the methods prove to be an improvement on other approximate techniques like the Effective Index Method, while using more modes yields results that converge to those of rigorous methods...|$|E
40|$|A transient-state chamber was {{developed}} to measure canopy gas exchange of single trees in the field. The chamber, with a volume of 41. 6 m 3, is designed to enclose a medium-size orchard tree; chamber top and windows can be left open, causing minimum disturbance to the tree environment. Transitory closures allow simultaneous measurement of CO 2 exchange and transpiration of the enclosed tree. The chamber was tested during a 2 -year study in an olive orchard submitted to different irrigation treatments: control with no water stress (CI) and regulated deficit irrigation (RDI). Leakage had a minimal impact on flux calculations (0. 8 % min- 1); adsorption was not detectable. Maximum increases in canopy temperature of 0. 58 °C min- 1 for CI and 1. 3 °C min- 1 for RDI generated very small effects on fluxes. Changes in the transpiration rate induced by the chamber's modification of the canopy environment were evaluated by continuous sap flow measurements with heat pulse gauges inserted {{in the trunk of}} two trees enclosed by chambers. Results showed a sap flow decrease of about 8 % after 180 s of chamber closure. The artificial turbulence generated by fans into the chamber to facilitate air mixing did not alter the transpiration rate. The enclosure had a very small impact on the tree canopy conductance (Gc). The initial lag and mixing time was estimated as 30 s; the optimal duration of the <b>calculation</b> <b>window</b> was 70 s. Hourly carbon assimilation (A), transpiration (E), and water use efficiency (WUE) for two olive trees in the field subjected to different levels of water stress were measured. © 2009 Elsevier B. V. All rights reserved. This work was funded by projects AGL- 2004 - 05717 of Ministerio de Educación y Ciencia of Spain and P 08 -AGR- 4202 of Junta de Andalucía. Peer Reviewe...|$|E
40|$|The basic {{types of}} the sun baffle for {{optoelectronic}} star trackers, feature of their dimensional calculations are analyzed. Mathematical model of baffle work and illuminance of output baffle <b>window</b> <b>calculation</b> techniques are offered. Necessary parameters for baffle modeling, optimization and performance analysis are chosen. The optimum design {{of the sun}}-protection baffle for wide-field star tracker is offered. ???????????????? ???????? ???? ?????????????? ????? ??????-??????????? ???????? ?????????, ??????????? ?? ??????????? ???????. ?????????? ?????????????? ?????? ?????? ?????? ? ???????? ????????????????? ??????? ???????????? ????????? ???? ??????. ??????? ??????????? ????????? ??? ?????????????, ??????????? ? ??????? ????????????? ?????. ?????????? ??????????? ??????????? ?????????????? ?????? ??? ?????????????? ????????? ????????...|$|R
3000|$|The {{operations}} in 2 D NUFFT and 2 D adjoint NUFFT {{can be classified}} into three types: (1) element-wise operations of matrices; (2) 2 D FFT; (3) <b>calculation</b> of <b>window</b> functions φ(x) and φ̂([...] k [...]). The parallel strategy of type 1 {{is the same as}} the strategy described in Section “Parallelizing element-wise operations of matrices.” For type 2, to achieve a high performance FFT, we took advantage of the NVIDIA’s FFT library, CUFFT (NVIDIA Corp 2007). Since ICON is an iterative algorithm, 2 D NUFFT and 2 D adjoint NUFFT will be repeated many times. To cut down the time of calculation and memory transfer, we pre-computed the window functions for once and stored them in the device memory.|$|R
50|$|I {{tried to}} induce Roothaan {{to do his}} Ph.D. thesis on Hückel-type {{calculations}} on substituted benzenes. But after carrying out some very good calculations on these he revolted against the Hückel method, threw his excellent <b>calculations</b> out the <b>window,</b> and for his thesis developed entirely independently his now well known all-electron LCAO SCF self-consistent-field method for the calculation of atomic and molecular wave functions, now appropriately referred to, I believe, as the Hartree-Fock-Roothaan method.|$|R
40|$|Numerous {{studies have}} {{successfully}} used topographic variables for {{the prediction of}} plant species distributions in mountainous areas. A current question is which scale optimizes the predictive power of these variables. Due {{to the development of}} airborne raclar/laser scanning, very high resolution digital elevation models (VHR DEMs) with a typical resolution of 1 m have emerged, allowing an accurate estimation of surface derivatives on a fine scale. Improved understanding of the relationship between scale and predictive power, through a comparative assessment of the significance of topographic variables computed at different scales, is needed. It should help researchers in selecting an adequate scale for the computation of topographic factors and further also increase the precision of models for predicting plant species distributions, and find direct applications, such as a better estimation of potential refuges for plant species in a climate change scenario. In this context, it is worth exploring the use of VHR DEM's techniques for data acquisition. In this study, we use a 1 m resolution laser DEM and a standard 25 m resolution DEM to compute four terrain variables (slope, aspect, plan curvature and profile curvature) at six different scales and compare their predictive ability. Univariate logistic regression models are fitted to presence/absence data of 117 alpine plant species collected at 125 sites in the Western Alps of Switzerland. The results show that slope and aspect, through northness = cos(aspect), are strongly correlated to the presence/absence data of many surveyed species. Using a laser VHR DEM significantly improves the predictive ability of aspect. The predictive power of slope and aspect is optimized when these variables are derived using a <b>calculation</b> <b>window</b> of about 100 m x 100 m (slope) and 20 m x 20 m (aspect). Curvature variables are much less significant. Studies that use them should begin with a careful analysis of their behaviour according to the scale used to compute the...|$|E
40|$|Malaysia {{is a hot}} {{and humid}} {{tropical}} country that has yearly mean temperature between 24 °C and 33 °C and relative humidity between 70 % and 90 % throughout the year. Thus, overheating is a paramount problem in residential buildings. Although air conditioning can solve this problem, it will consume {{a large amount of}} electricity and also causes environmental problems. Moreover, the concern of sick building syndrome has resulted in a resurgence of interest in naturally ventilated rooms. The action of opening a window is the most intuitive and simple response to controlling overheating in a room. Even so, if the window opening dimension is not properly designed, direct sunlight through the window will contribute by far the largest heat gain in houses. The survival of vernacular houses especially in the tropics should be reviewed to identify window design strategies that had been applied for natural ventilation and daylight. This study provides an evaluation on the window design performance in urban area and evaluation of impact of microclimate on indoor thermal comfort of the inhabitants based on case study on a selected British Colonial residences in Kuala Lumpur, Malaysia. The result suggests optimum window design for similar residences in urban area. The evaluation was conducted based on the <b>calculation</b> of <b>window</b> to wall ratio, field measurement, and simulation test using AIOLOS. Architecture and window design approaches of the selected residences were analyzed and the data collected from each residence were applied in the <b>calculation</b> of <b>window</b> to wall ratio. The validations of the window to wall ratio were then examined by field measurement and simulation using AIOLOS software. The research reveals that the thermal condition of the selected residence was higher than Malaysia standard requirements. In addition, existing surrounding buildings imposed a deep impact on the microclimate of JKR 989 which directly influenced the windows performance of the building...|$|R
40|$|We give an {{overview}} of how windows are modeled in the EnergyPlus whole-building energy simulation program. Important features include layer-by-layer input of custom glazing, ability to accept spectral or spectral-averaged glass optical properties, incidence angle-dependent solar and visible transmission and reflection, iterative heat balance solution to determine glass surface temperatures, calculation of frame and divider heat transfer, and modeling of movable interior or exterior shading devices with user-specified controls. Example results of EnergyPlus <b>window</b> <b>calculations</b> are shown...|$|R
30|$|In {{the present}} study, CC results for records {{obtained}} by OBSs deployed off the Kii Peninsula {{were used to}} carry out a cluster analysis of earthquakes. We proposed methods to deal with problems intrinsic to OBS data, namely the predominance of monotone frequencies and varying site effects among stations. These methods involved multiple CC <b>calculations,</b> using time <b>windows</b> with different lengths, and appropriately determining the CC threshold, which varied among OBSs. By applying the appropriate threshold, we identified clusters of similar earthquakes.|$|R
40|$|Abstract—Guided {{wave imaging}} {{techniques}} employed for structural health monitoring (SHM) can be computationally demanding, especially for adaptive {{techniques such as}} minimum variance distortionless response (MVDR) imaging, which requires a matrix inversion for each pixel <b>calculation.</b> Instantaneous <b>windowing</b> {{has been shown in}} previous work to improve guided wave imaging performance. The use of instantaneous windowing has the additional benefit of significantly reducing the compu-tational requirements of image generation. This paper derives a formula-tion for MVDR imaging using instantaneous windowing and shows that the matrix inversion associated with MVDR imaging can be optimized, reducing the computational complexity to that of conventional delay-and-sum imaging algorithms. Additionally, a vectorized approach is presented for implementing guided wave imaging algorithms, including delay-and-sum imaging, in matrix-based software packages. The improvements in computational efficiency are quantified by measuring computation time for different array sizes, windowing assumptions, and imaging methods. Index Terms—minimum variance, MVDR, instantaneous windowing. I...|$|R
2500|$|In a {{computer}} implementation, {{as the three}} s'j sums become large, {{we need to consider}} round-off error, arithmetic overflow, and arithmetic underflow. The method below calculates the running sums method with reduced rounding errors. This is a [...] "one pass" [...] algorithm for calculating variance of n samples without the need to store prior data during the calculation. Applying this method to a time series will result in successive values of standard deviation corresponding to n data points as n grows larger with each new sample, rather than a constant-width sliding <b>window</b> <b>calculation.</b>|$|R
40|$|This paper {{presents}} the results of the analysis on the influence of different RMS calculation methods to the detection and characterization of voltage dips. The analytical calculation of measurement parameters for voltage dips and the systematic deviations in characterization due to the dip detection algorithms are discussed for the general RMS <b>calculation</b> methods, sliding <b>window</b> and half cycle methods. Stochastic processes are carried out with different fault types, fault locations and event occurring instants in a real medium voltage network, an evaluation study is presented with the statistic results...|$|R
5000|$|In a {{computer}} implementation, {{as the three}} sj sums become large, {{we need to consider}} round-off error, arithmetic overflow, and arithmetic underflow. The method below calculates the running sums method with reduced rounding errors. This is a [...] "one pass" [...] algorithm for calculating variance of n samples without the need to store prior data during the calculation. Applying this method to a time series will result in successive values of standard deviation corresponding to n data points as n grows larger with each new sample, rather than a constant-width sliding <b>window</b> <b>calculation.</b>|$|R
40|$|New {{applications}} in {{fields such as}} augmented or virtualized reality have created a demand for dense, accurate real-time stereo reconstruction. Our goal is to reconstruct a user and her office environment for networked tele-immersion, which requires accurate depth values in a relatively large workspace. In order {{to cope with the}} combinatorics of stereo correspondence we can exploit the temporal coherence of image sequences by using coarse optical flow estimates to bound disparity search ranges at the next iteration. We use a simple flood fill segmentation method to cluster similar disparity values into overlapping windows and predict their motion over time using a single optical flow <b>calculation</b> per <b>window.</b> We assume that a contiguous region of disparity represents a single smooth surface which allows us to restrict our search to a narrow disparity range. The values in the range may vary over time as objects move nearer or farther away in Z, but we can limit the number of [...] ...|$|R
3000|$|... ratio, we {{consider}} it as congestion loss and enter the rate adjustment state. Otherwise, the loss event is regarded as wireless loss and JSCTP enters immediate retransmit state. The procedure of two states can refer to Algorithm 1 which shows the pseudocode of duplicated SACKs that occurred on JSCTP. Rate adjustment state refers the optimal congestion <b>window</b> <b>calculation</b> of TABE. First, it set ssthresh to optimal congestion window. Subsequently, sender sets cwnd to ssthresh if the transmission is in congestion avoidance. In the immediate retransmit state, JSCTP retransmits lost packets without adjusting current congestion window size.|$|R
40|$|TCP {{being the}} most widely used routing {{protocol}} for wired network considers packet loss as an indicator of congestion and calculates its congestion window according to that, but this approach is not suitable for wireless network where packet loss occurs due to various reasons other than congestion. Taking into consideration heterogeneous network, in this paper we explore a new variant of TCP, which has only sender side modification, end-to-end reliability and dynamic <b>window</b> <b>calculation</b> technique, which gives better result compared to existing known variants of TCP. We call it TCP Rcc. Here, we have used fixed window concept because it proves to produce better result...|$|R
40|$|We {{present a}} new {{algorithm}} for the 2 D Radix- 2 Sliding Window Fourier Transform (SWFT). Our algorithm avoids repeating <b>calculations</b> in overlapping <b>windows</b> {{by using a}} tree representation of the Cooley-Tukey Fast Fourier Transform (FFT). For an N_ 0 × N_ 1 array and n_ 0 = 2 ^m_ 0 × n_ 1 = 2 ^m_ 1 windows, our algorithm takes O(N_ 0 N_ 1 n_ 0 n_ 1) operations, which is faster than taking a 2 D FFT in each window. We provide a C implementation of the algorithm, compare ours with existing algorithms, and show how the algorithm extends to higher dimensions. Comment: 10 pages, 3 figures, submitted to ACM TOM...|$|R
40|$|Data from {{nighttime}} {{measurements of}} the net heat flow through several types of skylights is presented. A well-known thermal test facility was reconfigured to measure the net heat flow through {{the bottom of a}} skylight/light well combination. Use of this data to determine the U-factor of the skylight is considerably more complicated than the analogous problem of a vertical fenestration contained in a test mask. Correction of the data for heat flow through the skylight well surfaces and evidence for the nature of the heat transfer between the skylight and the bottom of the well is discussed. The resulting measured U-values are presented and compared with <b>calculations</b> using the <b>WINDOW</b> 4 and THERM programs...|$|R
40|$|Transiting planet discoveries {{have yielded}} {{a plethora of}} {{information}} regarding the internal structure and atmospheres of extrasolar planets. These discoveries have been restricted to the low-periastron distance regime due to the bias inherent in the geometric transit probability. Monitoring known radial velocity planets at predicted transit times is a proven method of detecting transits, and presents an avenue through which to explore the mass-radius relationship of exoplanets in new regions of period/periastron space. Here we describe transit <b>window</b> <b>calculations</b> for known radial velocity planets, techniques for refining their transit ephemerides, target selection criteria, and observational methods for obtaining maximum coverage of transit windows. These methods are currently being implemented by the Transit Ephemeris Refinement and Monitoring Survey (TERMS) ...|$|R
40|$|The photoion {{spectrum}} of atomic potassium was measured over the 3 s -> np excitation region with the photoion time-of-flight method and monochromatized synchrotron radiation. An unusual spectrum with paired windows structure was found {{instead of a}} simple regular Rydberg series. Such subsidiary windows have not been observed in the 3 s -> np resonances of Ar, which has a closed outer shell. Based on Dirac-Fock <b>calculations,</b> the dual <b>window</b> structure at 36. 7 eV and at 37. 4 eV {{was assigned to the}} 3 s^{- 1 } 3 p^{ 6 } 4 s 4 p resonance. The line shape can be fitted by Fano's formula and the Fano parameters were obtained. Comment: 7 pages, 1 figur...|$|R
40|$|Abstract: In this {{research}} the congestion prediction in nodal packet switches {{such as the}} routers is concerned. We introduce and verify a novel scheme for the congestion prediction of burst traffic based on the Haar wavelet energy coefficients. It is a dynamic scheme. There have been several solutions for the problem in which parametric models are used. The measured traffic data adjusts the parameters in a prediction formula. But these methods are very complex. Haar wavelets are easy to analysis and have no complexity; in addition they have information in various resolutions. In this work, we introduce a new application of wavelet in traffic congestion prediction. The proposed method is simulated by Network Simulator 2 and evaluated. The result show that algorithm predict correctly more than 80 %. One major problem in prediction algorithms is window length (time interval) in which prediction is done. This parameter depends on applications of algorithm (adaptive bandwidth control, admission control), and trade off between QoS parameters. In {{this research}}, moreover, a method for <b>calculation</b> of <b>window</b> is introduced. We assume a self similar behavior for the traffic and its long term correlation is used as the <b>window</b> length <b>calculation</b> basis...|$|R
40|$|Abstract. Recently, {{applications}} of integrated large composite structures have been attempted to many structures of vehicles. In {{order to improve}} the cost performance and reliability, {{it is necessary to}} judge the structural integrity of composite structures. Fracture simulation techniques using FEM have been developed for the purpose. Since a number of iterations of finite element analysis are required in the fracture simulation, the simulation techniques consume many memory resources and much calculation time. In this study, a personal computer cluster (PC cluster) and the domain decomposition method were incorporated into a fracture simulation system. <b>Calculations</b> using a <b>Windows</b> PC cluster were carried out to confirm the efficiency of the proposed simulation system. As a result, it is concluded that adopting the domain decomposition method and the computer cluster is remarkably efficient to reduce calculation time...|$|R
40|$|Thesis (M. Sc. (Nuclear Engineering) [...] North-West University, Potchefstroom Campus, 2009. The {{applicability}} of the Monte Carlo N-Particle code (MCNP) to evaluate reactor shielding applications is greatly improved {{through the use of}} variance reduction techniques. This study deals with the analysis of variance reduction methods, more specifically, variance reduction methods applied in MCNP such as weight windows, geometry splitting and source biasing consistent with weight windows. Furthermore, different cases are presented to show how to improve the Figure of Merit (FOM) of an MCNP <b>calculation</b> when weight <b>windows</b> and source biasing consistent with weight windows are used. Various methodologies to generate weight windows are clearly defined in this dissertation. All the above-mentioned concepts are used to analyse a system similar to {{the upper part of the}} Pebble Bed Modular Reactor’s (PBMR) bottom reflector. Master...|$|R
