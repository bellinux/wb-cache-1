42|255|Public
50|$|A <b>clustered</b> <b>file</b> {{system is}} a file system which is shared by being {{simultaneously}} mounted on multiple servers. There are several approaches to clustering, most of which do not employ a <b>clustered</b> <b>file</b> system (only direct attached storage for each node). <b>Clustered</b> <b>file</b> systems can provide features like location-independent addressing and redundancy which improve reliability or reduce {{the complexity of the}} other parts of the cluster. Parallel file systems are a type of <b>clustered</b> <b>file</b> system that spread data across multiple storage nodes, usually for redundancy or performance.|$|E
5000|$|Blue Whale <b>Clustered</b> <b>file</b> system (BWFS) is {{a shared}} disk file system (also called <b>clustered</b> <b>file</b> system, shared storage file systems or SAN file system) made by Tianjin Zhongke Blue Whale Information Technologies Company in China.|$|E
50|$|Scale out File Services (SoFS) is {{a highly}} scalable, grid-based NAS {{implementation}} developed by IBM. It is based on IBM's high-performance shared-disk <b>clustered</b> <b>file</b> system GPFS. SoFS exports the <b>clustered</b> <b>file</b> system through industry standard protocols like SMB, NFS, FTP and HTTP. Released in 2007, SoFS is a second generation file services architecture used within IBM since 2001.|$|E
50|$|Oracle Cloud File System (CloudFS) is {{a storage}} {{management}} suite developed by Oracle Corporation. CloudFS {{consists of a}} <b>cluster</b> <b>file</b> system called ASM <b>Cluster</b> <b>File</b> System (ACFS), and a cluster volume manager called ASM Dynamic Volume Manager (ADVM) initially released in August 2007.|$|R
50|$|The <b>Cluster</b> <b>File</b> System {{provides}} cache coherency and POSIX compliance across nodes, so {{that data}} changes are atomically seen by all cluster nodes simultaneously. Because <b>Cluster</b> <b>File</b> System shares the same binaries and same on-disk layout as single instance VxFS, moving between cluster and single instance mode is straightforward.|$|R
5000|$|OCFS2 Oracle <b>Cluster</b> <b>File</b> System a shared-disk {{file system}} for Linux ...|$|R
5000|$|Commodity {{hardware}} with storage logic abstracted into {{a software}} layer. This is also {{described as a}} <b>clustered</b> <b>file</b> system for converged storage.|$|E
5000|$|File: The file driver allows using {{files that}} can reside in any Linux file system or <b>clustered</b> <b>file</b> system as backstores for export via LIO.|$|E
50|$|A common {{performance}} measurement of a <b>clustered</b> <b>file</b> {{system is the}} amount of time needed to satisfy service requests. In conventional systems, this time consists of a disk-access time and a small amount of CPU-processing time. But in a <b>clustered</b> <b>file</b> system, a remote access has additional overhead due to the distributed structure. This includes the time to deliver the request to a server, the time to deliver the response to the client, and for each direction, a CPU overhead of running the communication protocol software.|$|E
40|$|<b>Cluster</b> <b>file</b> {{system is}} a key {{component}} of system software of clusters. It attracts more and more attention in recent years. In this paper, we introduce the design and implementation of DCFS 1 (the Dawning <b>Cluster</b> <b>File</b> System) — a <b>cluster</b> <b>file</b> system developed for Dawning 4000 -L. DCFS is a global file system sharing among all cluster nodes. Applications see a single uniform name space, and can use system calls to access DCFS files. The features of DCFS include its scalable architecture, metadata policy, server-side optimization, flexible communication mechanism and easy management. Performance tests of DCFS on Dawning 4000 -L show that DCFS can provide high aggregate bandwidth and throughput...|$|R
40|$|Abstract—Performance problems, {{which can}} stem from dif-ferent system components, such as network, memory, and storage devices, are {{difficult}} to diagnose and isolate in a <b>cluster</b> <b>file</b> system. In this paper, we present an online performance anomaly detector which is able to efficiently detect performance anomaly and accurately identify the faulty sources in a system node of a <b>cluster</b> <b>file</b> system. Our method exploits the stable relationship between workloads and system resource statistics to detect the performance anomaly and identify faulty sources which cause the performance anomaly in the system. Our preliminary experimental results demonstrate the efficiency and accuracy of the proposed performance anomaly detector. Keywords-performance anomaly detector; <b>cluster</b> <b>file</b> system; I...|$|R
40|$|Lack of {{a highly}} {{scalable}} and parallel metadata service is the Achilles heel for many <b>cluster</b> <b>file</b> system deployments in both the HPC world and the Internet services world. This is because most <b>cluster</b> <b>file</b> systems have focused on scaling the data path, i. e. providing high bandwidth parallel I/O to files that are gigabytes in size. But with proliferation of massively parallel applications that produce metadata-intensive workloads, such as large number of simultaneous file creates and large-scale storage management, <b>cluster</b> <b>file</b> systems also need to scale metadata performance. To realize these goals, this paper makes a case for a scalable metadata service middleware that layers on existing <b>cluster</b> <b>file</b> system deployments and distributes file system metadata, including the namespace tree, small directories and large directories, across many servers. Our key idea is to effectively synthesize a concurrent indexing technique to distribute metadata with a tabular, on-disk representation of all file system metadata...|$|R
50|$|OpenSSI can use SAN based <b>clustered</b> <b>file</b> {{systems for}} its root {{provided}} {{they provide a}} POSIX compatible file system interface. Currently Lustre and GFS have been tested.|$|E
50|$|With a <b>clustered</b> <b>file</b> system, each node mounts {{the file}} system in {{parallel}} {{and access to}} the files goes directly from the node to the file system.|$|E
50|$|However, {{the use of}} a <b>clustered</b> <b>file</b> {{system is}} {{essential}} in modern computer clusters. Examples include the IBM General Parallel File System, Microsoft's Cluster Shared Volumes or the Oracle Cluster File System.|$|E
50|$|Oracle ASM Dynamic Volume Manager {{provides}} {{the foundation for}} the ASM <b>Cluster</b> <b>File</b> System (ACFS).|$|R
40|$|In {{this paper}} {{we present a}} novel <b>cluster</b> <b>file</b> system - - TH-CluFS [...] in {{distributed}} service environment, especially in Internet service area, and use the file system to construct a scalable web server cluster. TH-CluFS has a consistent file view across several servers connected by a high-speed local area network and out-performs the traditional distributed file systems. Due to the advantages of high-speed LAN, the system provides stronger support for the fast file access service than the previous distributed systems, such as NFS, AFS and Coda. In this paper we describe the key issues in <b>cluster</b> <b>file</b> systems {{and focus on the}} improvement from traditional distributed file system to current <b>cluster</b> <b>file</b> system [...] TH-CluFS...|$|R
40|$|With the {{emergence}} of Storage Networking, distributed file systems that allow data sharing through shared disks will become vital. We refer to <b>Cluster</b> <b>File</b> Systems as a distributed file systems optimized for environments of clustered servers. The requirements such file systems is that they guarantee file systems consistency while allowing shared access from multiple nodes in a shared-disk environment. In this paper we evaluate three approaches for designing a <b>cluster</b> <b>file</b> system - conventional client/server distributed file systems, symmetric shared file systems and asymmetric shared file systems. These alternatives are considered by using our prototype <b>cluster</b> <b>file</b> system, HAMFS (Highly Available Multi-server File System). HAMFS is classified as an asymmetric shared file system. Its technologies are incorporated into our commercial <b>cluster</b> <b>file</b> system product named SafeFILE. SafeFILE offers a disk pooling facility that supports off-the-shelf disks, and balances file load across these disks automatically and dynamically. From our measurements, we identify the required disk capabilities, such as multi-node tag queuing. We also identify the advantages of an asymmetric shared file system over other alternatives...|$|R
50|$|The PanFS <b>clustered</b> <b>file</b> system {{creates a}} single pool of storage under a global {{filename}} space to support multiple applications and workflows {{in a single}} storage system with high performance for technical applications.PanFS supports DirectFlow (pNFS), NFS and CIFS data access protocols simultaneously.|$|E
50|$|HP-UX 11i {{offers a}} common root disk for its <b>clustered</b> <b>file</b> system. HP Serviceguard is the cluster {{solution}} for HP-UX. HP Global Workload Management adjusts workloads to optimize performance, and integrates with Instant Capacity on Demand so installed resources can {{be paid for}} in 30-minute increments as needed for peak workload demands.|$|E
50|$|The Nasan <b>Clustered</b> <b>File</b> System is {{a shared}} disk file system {{created by the}} company DataPlow. Nasan {{software}} enables high-speed access to shared files located on shared, storage area network (SAN)-attached storage devices by utilizing the high-performance, scalable data transfers inherent to storage area networks and the manageability of network attached storage (NAS).|$|E
40|$|Design of <b>cluster</b> <b>file</b> {{system is}} very {{important}} for building a general-purpose cluster with commodity components. To provide scalable high I/O performance needed in the scientific computing, engineering computing, Internet/Intranet services, I/O intensive commercial database and multimedia application, we designed a high-performance <b>cluster</b> <b>file</b> system named COSMOS. This paper presents the basic characteristics of the <b>cluster</b> <b>file</b> system and its performance analysis. The performance of the COSMOS file system is evaluated on the platform of Dawning/ 2000 superserver, using standard I/O performance benchmarks. Our tests show satisfactory bandwidth and overall performance of COSMOS, and the system scales well. Based on implementation experience and the raw data, we discuss the bottleneck of the existing system and propose methods to improve the system...|$|R
50|$|CFS, the OpenSSI <b>Cluster</b> <b>File</b> System {{provides}} transparent inter-node {{access to}} an underlying real file system on one node.|$|R
50|$|In total Brutus {{contains}} 19,872 cores, {{plus a few}} hundreds in the <b>clusters</b> <b>file</b> servers, login {{nodes and}} management nodes.|$|R
50|$|Nasan derives {{its name}} from the {{combination}} of network attached storage (NAS) and storage area network (SAN). Nasan <b>clustered</b> <b>file</b> sharing is an extension of traditional LAN file sharing yet utilizes storage area networks for data transfers. Deploying a Nasan cluster entails configuring LAN file sharing, installing Nasan file system software, and connecting computers and storage devices to the SAN.|$|E
5000|$|The SAN File System (SFS) is a high-performance, <b>clustered</b> <b>file</b> system {{created by}} the company DataPlow. SFS enables fast access to shared files located on shared, storage area network (SAN)-attached storage devices. [...] SFS {{utilizes}} the high-speed, scalable data transfers inherent to storage area networks and is a general-purpose file system {{for a wide variety}} of environments, including scientific computing, finance, healthcare, entertainment, defense, broadcast, and aerospace.|$|E
5000|$|VMware VMFS (Virtual Machine File System) is VMware, Inc.'s <b>clustered</b> <b>file</b> {{system used}} by the company's {{flagship}} server virtualization suite, vSphere. It was developed to store virtual machine disk images, including snapshots. Multiple servers can read/write the same filesystem simultaneously while individual virtual machine files are locked. VMFS volumes can be logically [...] "grown" [...] (non-destructively increased in size) by spanning multiple VMFS volumes together.|$|E
40|$|Noncontiguous I/O {{access is}} the main access pattern in many {{scientific}} applications. Noncontiguity exists both in access to files and in access to target memory regions on the client. This characteristic imposes a requirement of native noncontiguous I/O access support in <b>cluster</b> <b>file</b> systems for high performance. In this paper, we address noncontiguous data transmission between the client and the I/O server in <b>cluster</b> <b>file</b> systems over a high performance network. We propos...|$|R
5000|$|<b>Cluster</b> <b>file</b> systems, {{which are}} file systems that {{maintain}} data or indexes in a coherent fashion across a whole computing cluster; ...|$|R
40|$|Abstract — The {{concept of}} Remote Procedure Call (RPC) was {{proposed}} more than 30 years ago. Although various RPC {{systems have been}} studied and implemented, the existing RPC systems lack many crucial features and flexibility required for building modern <b>cluster</b> <b>file</b> systems. This paper presents FlexRPC, a flexible user-level RPC system that enables to develop high-performance <b>cluster</b> <b>file</b> systems easily. FlexRPC ensures client-side thread-safeness and fully supports multithreaded RPC servers. Parallel and serial multicasting mechanisms allow for implementing sophisticated replication in modern <b>cluster</b> <b>file</b> systems. The remote procedure can be invoked using both UDP and TCP transports with at-most-once semantics. The concurrent call requests are handled {{by a set of}} worker threads on the client and server side where the number of workers varies dynamically according to the request rate. In addition, the semantics and the specification of remote procedures are designed to be {{as close as possible to}} SunRPC. The experimental results show that FlexRPC improves both latency and bandwidth significantly in spite of added functionalities. We also demonstrate the performance and the flexibility provided by FlexRPC by building working prototype of <b>cluster</b> <b>file</b> system called Kadoop on top of FlexRPC. I...|$|R
50|$|The nodes of iPSC/2 ran the {{proprietary}} NX/2 operating system, {{while the}} host machine ran System V or Xenix.Nodes could be configured like the iPSC/1 without and local disk storage, or {{use one of}} the Direct-Connect Module connections with a <b>clustered</b> <b>file</b> system (called concurrent file system at the time).Using both faster node computing elements and the interconnect system improved application performance over the iPSC/1.An estimated 140 iPSC/2 systems were built.|$|E
50|$|Xsan is Apple Inc.'s {{storage area}} network (SAN) or <b>clustered</b> <b>file</b> system for macOS. Xsan enables {{multiple}} Mac desktop and Xserve systems to access shared block storage over a Fibre Channel network. With the Xsan file system installed, these computers can read and write to the same storage volume at the same time. Xsan is a complete SAN solution that includes the metadata controller software, the file system client software, and integrated setup, management and monitoring tools.|$|E
50|$|The Digitronix PC (DTX PC) of 1992 is an {{inexpensive}} OEM personal computer {{that can also}} serve as a business capable workstation or a <b>clustered</b> <b>file</b> server. It boasted 100 megabytes of ram, an HDTV monitor and optical disk storage, possibly based on the NeXTcube's magneto-optical drive. It also had built in compatibility with NTSC\PAL formats and had built in composite video out, as well as advanced digital audio and video editing capacity on-chip, possibly as coprocessors similar to the Amiga.|$|E
30|$|To {{deal with}} the {{challenges}} of storing and accessing big data, one distributed cluster platform is necessary. Such a system must provide large storage space (petabyte) and location transparent access to data files to the servers on the <b>cluster.</b> Hadoop Distributed <b>File</b> System (HDFS) [28] {{is an example of}} <b>cluster</b> <b>file</b> system which is designed for reliably storing large amount of various structure or no structure data across machines in a large scale cluster. Interestingly, HDFS was originally derived from Google Files System (GFS) paper [29]. It has ability to deliver an open source <b>cluster</b> <b>file</b> system similar to GFS.|$|R
40|$|Replication in grid file {{systems can}} {{significantly}} im-proves I/O performance of data-intensive applications. However, most of existing replication techniques apply to individual files, which may introduce inefficient replication overheads {{for a large}} number of files. We propose a <b>file</b> <b>clustering</b> based replication algorithm for grid file systems. Our algorithm groups files according to a relationship of simultaneous accesses between files and stores the replicas of the <b>clustered</b> <b>files</b> into storage nodes, to satisfy expected most of future read access times to the <b>clustered</b> <b>files</b> and replication times for individual files being minimized under the given storage capacity limitation. Our experiments on a given grid environment, 20 nodes of 5 sites, suggest that the proposed algorithm achieves accurate <b>file</b> <b>clustering</b> and ef-ficient replica management; our clustering policy with the <b>file</b> <b>cluster</b> size limit of 5120 MB and storage capacity limit for replicas of 10240 MB exhibits 1. 58 times efficiency than the policy that never <b>cluster</b> related <b>files.</b> The results also indicate that the overheads required for introducing our al-gorithm significantly affect I/O performance of running ap-plications. 1...|$|R
50|$|Veritas Storage Foundation {{was also}} {{packaged}} in bundles such as Veritas Storage Foundation Veritas Cluster Server, for databases, for Oracle RAC, and Veritas <b>Cluster</b> <b>File</b> System.|$|R
