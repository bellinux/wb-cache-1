30|0|Public
40|$|The report {{specifically}} discusses time dissemination techniques, including epoch determination, frequency determination, and ambiguity resolution. It also discusses operational considerations including equipment, path selection, {{and adjustment}} procedure. epoch (the actual location or timing of periodic events) {{is shown to}} be both maintainable and <b>calibratable</b> by the techniques described to better than 3 -microsecond accuracy; and frequency (the uniformity of the time scale) to about one part in 10 to the 12 th power...|$|E
40|$|Designing a {{hypothesis}} test {{to determine the}} best of two machine learning algorithms with only a small data set available is not a simple task. Many popular tests suffer from low power (5 x 2 cv [2]), or high Type I error (Weka’s 10 x 10 cross validation [11]). Furthermore, many tests show a low level of replicability, so that tests performed by different scientists with the same pair of algorithms, the same data sets and the same hypothesis test still may present different results. We show that 5 x 2 cv, resampling and 10 fold cv suffer from low replicability. The main complication is due to the need to use the data multiple times. As a consequence, independence assumptions for most hypothesis tests are violated. In this paper, we pose the case that reuse of the same data causes the effective degrees of freedom to be much lower than theoretically expected. We show how to calibrate the effective degrees of freedom empirically for various tests. Some tests are not <b>calibratable,</b> indicating another flaw in the design. However the ones that are <b>calibratable</b> all show very similar behavior. Moreover, the Type I error of those tests is on the mark {{for a wide range of}} circumstances, while they show a power and replicability that is a considerably higher than currently popular hypothesis tests. 1...|$|E
40|$|In {{the course}} of the next few years the {{extracted}} beam in-tensities of the SIS (the heavy ion synchroton at GSI) will reach 10 ” particles per second, corresponding to bearn currents up to the PA-range. Especially in the region above 1 nA a non-destructive and <b>calibratable</b> measure-ment is required to determine extraction efficiences in reg-ular intervals. Therefore a new type of beam transformer has been developed using the principle of a Cryogenic Cur-rent Comparator. The paper will give an overview on the construction and the design criteria of the experimental set-up and discuss the first results of measurements. ...|$|E
40|$|ABSTRACT The {{computation}} of phylogenetic {{diversity and}} phylogenetic community structure demands an accurately calibrated, high-resolution phylogeny, which reflects current knowledge regarding diversification {{within the group}} of interest. Herein we present the angiosperm phylogeny R 20160415. new, {{which is based on}} the topology proposed by the Angiosperm Phylogeny Group IV, a recently released compilation of angiosperm diversification. R 20160415. new is <b>calibratable</b> by different sets of recently published estimates of mean node ages. Its application for the computation of phylogenetic diversity and/or phylogenetic community structure is straightforward and ensures the inclusion of up-to-date information in user specific applications, as long as users are familiar with the pitfalls of such hand-made supertrees...|$|E
40|$|This paper {{develops}} a simple equilibrium model of CEO pay. CEOs have different talents and are matched to firms {{in a competitive}} assignment model. In market equilibrium, a CEO%u 2019 s pay changes one for one with aggregate firm size, while changing much less {{with the size of}} his own firm. The model determines the level of CEO pay across firms and over time, offering a benchmark for <b>calibratable</b> corporate finance. The sixfold increase of CEO pay between 1980 and 2003 can be fully attributed to the six-fold increase in market capitalization of large US companies during that period. We find a very small dispersion in CEO talent, which nonetheless justifies large pay differences. The data broadly support the model. The size of large firms explains many of the patterns in CEO pay, across firms, over time, and between countries. ...|$|E
40|$|Physical {{systems can}} be modeled at many levels of approximation. The right model depends on the problem or subproblem to be solved. In many cases, a {{combination}} of models will {{be more effective than}} a single model alone. Our research investigates this idea in the context of engineering design optimization. We present a family of strategies for using multiple models to optimize engineering designs. The strategies are useful when multiple approximations of an objective function can be implemented by compositional modeling techniques. We show how a compositional modeling library can be used to construct a variety of locally <b>calibratable</b> approximation schemes that can be incorporated into the optimization strategies. We analyze the optimization strategies and approximation schemes to formulate and prove sufficient conditions for correctness and convergence. We also report experimental tests of our methods in the domain of sailing yacht design. Our results demonstrate a dramatic reduction i [...] ...|$|E
40|$|As an {{alternative}} to the pecking order, we develop a dynamic <b>calibratable</b> model where the firm avoids mispricing via signaling. The model is rich, featuring endogenous invest-ment, debt, default, dividends, equity flotations, and share repurchases. In equilibrium, firms with negative private information have negative leverage, issue equity, and overin-vest. Firms signal positive information by substituting debt for equity. Default costs induce such firms to underinvest. Model simulations reveal that repeated signaling can account for countercyclical leverage, leverage persistence, volatile procylical investment, and correla-tion between size and leverage. The model generates other novel predictions. Investment rates are the key predictor of abnormal announcement returns in simulated data, with lever-age only predicting returns unconditionally. Firms facing asymmetric information actually exhibit higher mean Q ratios and investment rates. (JEL G 32) Three decades have passed since Leland and Pyle (1977) and Ross (1977) de-veloped the signaling theory of corporate finance. Although their work has been extended, signaling models remain static and qualitative, making it im-possible for empiricists to assess the theory’s ability to match observed time...|$|E
40|$|Abstract—Low current applications, like neuromorphic circuits, where {{operating}} currents {{can be as}} low as a few nanoamperes or less, {{suffer from}} huge transistor mismatches, resulting in around or less than 1 -bit precisions. Recently, a neuromorphic programmable-kernel 2 -D convolution chip has been reported where each pixel included two compact calibrated digital-to-analog converters (DACs) of 5 -bit resolution, for currents down to picoamperes. Those DACs were based on MOS ladder structures, which although compact require 3 + 1 unit transistors (is the number of calibration bits). Here, we present a new calibration approach not based on ladders, but on individually <b>calibratable</b> current sources made with MOS transistors of digitally adjustable length, which require only-sized transistors. The scheme includes a translinear circuit-based tuning scheme, which allows us to expand the operating range of the calibrated circuits with graceful precision degradation, over four decades of operating currents. Experimental results are provided for 5 -bit resolution DACs operating at 20 nA using two different translinear tuning schemes. Maximum measured precision is 5. 05 and 7. 15 b, respectively, for the two DAC schemes. Index Terms—Analog, calibration, mismatch, subthreshold. I...|$|E
40|$|This paper {{provides}} an extensive {{analysis of the}} performance of a six-port based direct conversion receiver (SPR) in terms of signal quality, dynamic range, noise figure, ports matching, isolation, bandwidth, and cost. Calibration technique using multimemory polynomials has been adopted in order to improve the signal quality of the six-port receiver. The performances of the calibrated receiver are then compared with the performances of a commercially available I-Q demodulator used as a low-IF receiver. The main advantages and disadvantages of the SPR compared to the low-IF receiver are highlighted. The major advantages of the SPR come in terms of its available input frequency bandwidth and the low power requirement. The SPR system requires no external bias supply but suffers in terms of the available conversion gain. A better port matching of the SPR can be guaranteed over a wide frequency bandwidth, which mixer based receiver systems lack. The main component limiting the performance of a SPR is the diode detector. A faster and a better diode detector will alleviate some of the problems highlighted in this paper. The SPR system is <b>calibratable</b> and its error-vector-magnitude performance can be made better than the I-Q demodulator used as a low-IF receiver...|$|E
40|$|The {{interfacing}} of man-made {{electronic components}} with specifically-folded biomacromolecules lies central {{not only to}} the development of sensory interfaces and potential new molecular-scale devices, but also enables us to analyse processes of great biological importance in a refined and controllable manner. Recent advances in both available technology, most notably optical and scanning probes in nature, and our understanding of suitable methodologies, have led us {{to the point where the}} characteristics of single biological molecules can be interrogated with good levels of reproducibility. We review here the application of scanning probe microscopy to the analysis of and experimentation on biological redox systems. Within this paper the tunnel transport characteristics, as assayed by both scanning tunnelling microscopy (STM) and conducting probe atomic force microscopy (AFM), of single metalloproteins are discussed. In a specific case study the electron transfer characteristics of the blue copper metalloprotein, azurin, are reported. The modulation of these properties under the influence of <b>calibratable</b> compressional force has also been examined in some detail. Work such as this enables one to reproducibly establish the conductance, barrier height, environmental sensitivity and electromechanical properties of these molecules...|$|E
40|$|Type Ia supernovae are {{the biggest}} {{thermonuclear}} explosions in the modern universe and responsible for making about 2 / 3 of the iron in our blood. They also play a special role as <b>calibratable</b> standard candles in cosmology, yet our understanding of them is primitive. We discuss reecent attempts in theory and simulation to describe more physically the ignition of the runaway in a carbon-oxygen white dwarf; the possible transition of burning to detonation; and the light curves, nucleosynthesis, and spectra of multi-dimensional models. The convection prior to ignition sets up a dipole-flow that implies off-center, lopsided ignition with an offset {{that depends on the}} rotation of the white dwarf. Once the flame ignites, an extended period of subsonic burning is followed by a transition to a detonation that happens when the burning enters the “stirred flame” regime. In terms of combustion parameters, the required conditions for detonation are Karlovitz numbers much greater than 10 and Damköhler numbers of approximately 10. Multi-dimensional models employing these ignition and detonation criteria give good agreement with the observed light curves and spectra of supernovae and with the observed width-luminosity relation. The data base generated by these models will be useful in planning future SN Ia survey missions. PoS(NIC X) 04...|$|E
40|$|This paper {{develops}} a simple competitive model of CEO pay. A {{large part of}} the rise in CEO compensation in the US economy is explained without assuming managerial entrenchment, mishandling of options, or theft. CEOs have observable managerial talent and are matched to assets in a competitive assignment model. The marginal impact of a CEO’s talent is assumed to increase with the value of the assets under his control. Under very general assumptions, using results from extreme value theory, the model determines the level of CEO pay across firms and over time, and the pay-sensitivity relations. The model predicts the cross-sectional Cobb-Douglas relation between pay and firm size. It also predicts that the level of CEO compensation should increase one for one with the average market capitalization of large firms in the economy. Therefore, the six-fold increase of CEO pay between 1980 and 2000 can be fully attributed to the increase in market capitalization of large US companies. The model {{can also be used to}} study other large changes at the top of the income distribution, and offers a benchmark for <b>calibratable</b> corporate finance. The empirical evidence is broadly supportive of our model. The size of large firms explain many of the patterns in CEO pay, in the time series, across industries and across countries...|$|E
40|$|Realizing the {{potential}} of 21 cm tomography to statistically probe the intergalactic medium {{before and during the}} Epoch of Reionization requires large telescopes and precise control of systematics. Next-generation telescopes are now being designed and built to meet these challenges, drawing lessons from first-generation experiments that showed the benefits of densely packed, highly redundant arrays [...] in which the same mode on the sky is sampled by many antenna pairs [...] for achieving high sensitivity, precise calibration, and robust foreground mitigation. In this work, we focus on the Hydrogen Epoch of Reionization Array (HERA) as an interferometer with a dense, redundant core designed following these lessons to be optimized for 21 cm cosmology. We show how modestly supplementing or modifying a compact design like HERA's can still deliver high sensitivity while enhancing strategies for calibration and foreground mitigation. In particular, we compare the imaging capability of several array configurations, both instantaneously (to address instrumental and ionospheric effects) and with rotation synthesis (for foreground removal). We also examine the effects that configuration has on calibratability using instantaneous redundancy. We find that improved imaging with sub-aperture sampling via "off-grid" antennas and increased angular resolution via far-flung "outrigger" antennas is possible with a redundantly <b>calibratable</b> array configuration. Comment: 19 pages, 11 figures. Revised to match the accepted ApJ versio...|$|E
40|$|An {{electro-optical}} infrared (EO-IR) sensor’s Operational Envelope is the {{parameter space}} where the sensor can physically operate. The Valid Operating Region (VOR) is {{a subset of the}} Operational Envelope and is the parameter {{space where the}} sensor can return <b>calibratable</b> data. The traditional VOR (TVOR) is a small subset of the VOR that is the intended on-orbit operating range for each parameter. Usually, sensors are calibrated only over their TVOR. We describe a method to produce radiometric and Line-of-Sight (LOS) calibrations over the entire VOR, enabling a sensor to perform its mission and return calibrated data through changing operating conditions, such as are experienced early on-orbit or during spacecraft anomalies. This method expands the calibration to compensate for all known input variations, rather than the traditional approach of minimizing input variations. The ground test method described builds a statistical calibration model as ground test data is collected, where the model determines if there is sufficient data in a particular region of the parameter space and where additional data may need to be collected to meet mission LOS and radiometric accuracy requirements. Although more data will need to be collected to span the VOR compared to a calibration process only over the TVOR, this statistical calibration model can guide the ground test and enable optimal data collection...|$|E
40|$|WO 2009030361 A 1 UPAB: 20090403 NOVELTY - The sensor (100) has an exciter line (108) {{arranged}} {{with respect to}} a sensor element arrangement (104), and two spatial axes running along linearly independent position vectors. The sensor element arrangement detects magnetic field components (By, Bz) with measurement field components (BMy, BMz) and/or calibration field components (BKy, BKz). A pair of different predefined calibration field components in sensor elements (104 a, 104 b) is produced in the sensor element arrangement with respect to one of the spatial axes when a predefined current (Ik 1) is injected into the exciter line. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: (1) a method for detecting spatial components of a magnetic field at a reference point (2) a computer program product including a set of instructions to perform a method for detecting spatial components of a magnetic field at a reference point. USE - <b>Calibratable</b> multidimensional magnetic field sensor i. e. hall sensor, for detecting spatial components of a magnetic field at a reference point, for current measurement, and for use in a contactless signal generator for wear-free detection of a position of a switch or an actuator. ADVANTAGE - The sensor detects the spatial components of the magnetic field at the reference point in an efficient manner. The sensor is designed in such a manner that tolerance of the sensor is efficiently compensatable, and is calibrated in an efficient and cost-effective manner...|$|E
40|$|We {{would like}} {{to find a way}} to improve the {{determination}} of galaxy star formation history from integrated light spectroscopy. To this end, several classes of chemically peculiar (CP) stars arise during the course of normal evolution in single stars and noninteracting binary stars. An aging stellar population has periods of time in which CP stars contribute to the integrated light, and others in which the contributions fade. The HgMn stars, for example, occupy a narrow temperature range of 10500 to 16000 K, which maps to a narrow range of ages. Wolf-Rayet stars, He-poor stars, Bp-Ap stars, Am-Fm stars, and C stars all become very common in a normal stellar population at various ages between zero and several Gyr, fading in and out in a way that is analogous to features used in stellar spectral classification. We examine population fractions and light fractions in order to assess the feasibility of using CP stars as age tracers. We find that, even though CP stars do not usually dominate in number, there are enough of them so that the CP spectral features are detectable in high-quality integrated spectra of young and intermediate age stellar populations. The new technique should be <b>calibratable</b> and useful. Furthermore, using CP signatures as age dating tools sidesteps reliance on photometry that is susceptible to dust and Balmer features that are susceptible to nebular fill-in. Comment: 4 pages, 3 figures, accepted for publication in A&A Letter...|$|E
40|$|Low current applications, like neuromorphic circuits, where {{operating}} currents {{can be as}} low as a few nanoamperes or less, {{suffer from}} huge transistor mismatches, resulting in around or less than 1 -bit precisions. Recently, a neuromorphic programmable- kernel 2 -D convolution chip has been reported where each pixel included two compact calibrated digital-to-analog converters (DACs) of 5 -bit resolution, for currents down to picoamperes. Those DACs were based on MOS ladder structures, which although compact require unit transistors (is the number of calibration bits). Here, we present a new calibration approach not based on ladders, but on individually <b>calibratable</b> current sources made with MOS transistors of digitally adjustable length, which require only -sized transistors. The scheme includes a translinear circuit-based tuning scheme, which allows us to expand the operating range of the calibrated circuits with graceful precision degradation, over four decades of operating currents. Experimental results are provided for 5 -bit resolution DACs operating at 20 nA using two different translinear tuning schemes. Maximum measured precision is 5. 05 and 7. 15 b, respectively, for the two DAC schemes. This work was supported by Spanish Research Grants TEC 2006 - 11730 -C 03 - 01 (SAMANTA 2), TEC- 417 (Brain System), and EU Grant IST- 2001 - 34124 (CAVIAR). The work of J. A. Leñero-Bardallo {{was supported by the}} Spanish Ministry of Education and Science through an I 3 P national scholarship. Peer reviewe...|$|E
40|$|In an {{integrated}} world capital market with perfect information, {{all forms of}} capital flows are indistinguishable. Information frictions and incomplete risk sharing are important elements that needed to differentiate between equity and debt flows, and between different types of equities. This survey put together models of debt, FDI, Fpi flows to help explain the composition of capital flows. With information asymmetry between foreign and domestic investors, a country which finances its domestic investment through foreign debt or foreign equity portfolio issue, will inadequately augment its capital stock. Foreign direct investment flows, however, have the potential of generating an efficient level of domestic investment. In the presence of asymmetric information between sellers and buyers in the capital market, foreign direct investment is associated with higher liquidation costs due to the adverse selection. Thus, the exposure to liquidity shocks determines the volume of foreign direct investment flows relative to portfolio investment flows. In particular, the information-liquidity trade-off helps explain the composition of equity flows between developed and emerging countries, {{as well as the}} patterns of FDI flows during financial crises. The asymmetric information between domestic investors (as borrowers) and foreign investors (as lenders) with respect to investment allocation leads to moral hazard and thus generate an inadequate amount of borrowings. The moral hazard problem, coupled with limited enforcement, can explain why countries experience debt outflows in low income periods; in contrast to the predictions of the complete-market paradigm. Finally, we analyze a risk-diversification model, where bond holdings hedge real exchange rate risks, while equities hedge non-financial income fluctuations. An equity home bias emerges as a <b>calibratable</b> equilibrium outcome. ...|$|E
40|$|The work {{presented}} here {{is intended to}} develop a simple <b>calibratable</b> simulation model of a long-term software evolution process. This base model will act as a test-bed for the examination of effects of long-term software process ‘improvement ’ proposals, since {{it will be possible}} to modify it to reflect the effects on the process of any particular proposed change. The use of an existing simulation in this way is not itself novel. Existing work on the same lines (Tveldt and Collofello 1995; Haberlein, 2003) use Abdel-Hamid and Madnick’s model (1991) as a base. However, they are examining the effect on a single development project as against the longer-term effect over many years and many releases considered here. In addition, the latter is a complex model, with many variables and a considerable number of inputs to calibrate. The base model {{presented here}} is intended to be as simple as possible whilst allowing the crucial variables to be manipulated. The simplicity has a number of advantages: • the base model is easier to calibrate, requiring fewer inputs to be quantified; • in the same way, it should be easier to reflect proposed process changes, since fewer changes to the model structure and/or fewer calibration inputs need to be taken into account; • there is less potential for the model to fail to reflect reality after changes to go wrong when simulating a process change, since this inevitably results in taking a model outside its known behaviour envelope; and • the evaluation and interpretation of results of proposed process changes is more easily understandable. On the other hand, some of the subtleties in process changes may be missed with this approach...|$|E
40|$|The {{interdisciplinary}} and multi-institute GENTOO (Gliders: Excellent New Tools for Observing the Ocean) project {{undertook a}} cruise to the northwestern Weddell Sea in early 2012. During the cruise net samples, acoustic transects and CTD casts were undertaken. In addition surface drifters and three ocean gliders (iRobot Seagliders) were deployed. One of the gliders was {{fitted with a}} bespoke 120 kHz single-beam, <b>calibratable</b> echo sounder, an instrument appropriate for detecting the presence and measuring the biomass of Antarctic krill (Euphausia superba) by means of measuring mean volume backscatter. Antarctic krill are a key species in the ecosystem of the Southern Ocean where they are consumed by a very large numbers of higher trophic level predators. The Weddell Sea is a potentially important area of spawning of Antarctic krill with juvenile stages exploiting the winter ice habitat before being advected into the Scotia Sea when the ice retreats. Understanding the distribution of krill in the Weddell Sea is problematic, however, due to the difficulty and expense of accessing this remote and often ice-covered region and the limited seasonal window of Antarctic operations. The development of sophisticated autonomous vehicle offers the possibility for long deployments, spanning several months and remote monitoring via satellite telemetry of collected data. We discuss the calibration of the echo sounder using known targets and the validation of krill swarm identification by mounting the echo sounder on a net. The analysis of the acoustic data collected during the glider deployment is presented and we consider the potential and challenges of using Seagliders as platforms for estimating krill biomass and advective flux in Antarctic waters...|$|E
30|$|Swank and Schreuder (1974) {{compared}} stratified two-phase sampling, two-phase sampling with {{a regression}} estimator, and two-phase sampling with a ratio-of-means estimator. They found the stratified two-phase sampling {{as the most}} precise and appropriate method for estimating surface area and biomass for a young eastern white pine forest. Temesgen (2003) found that stratified random sampling produced the lowest mean squared error value in comparing five sampling designs to quantify tree leaf area. Stratification in branch biomass sampling {{can be done in}} many different ways. Snowdon (1986) showed improved accuracy of estimates by stratification based on crown position compared to those obtained by simple random sampling, especially at low sampling intensities. Their findings suggest that stratification by whorl was slightly but not significantly inferior to stratification based on crown position or branch diameter. Another approach used in selecting branches for estimating crown biomass is to divide the bole into sections and pile up the branches from each section into different size classes and randomly select a number of branches proportional to the total number of branches in each size class (e.g. Harrison et al. 2009, Devine et al. 2013). In an evaluation of ten different sampling strategies, Temesgen et al. (2011) found that systematic sampling with ratio estimation as the most efficient estimate of individual tree foliage biomass. de-Miguel et al. (2014 a) developed generalized, <b>calibratable,</b> mixed-effects meta-models for large-scale biomass prediction. One of their objectives was to investigate and demonstrate how the biomass prediction differed when calibration trees were selected using different sampling strategies. They found that a stratified sampling was better compared to the simple random sampling. Thus there is no strong rationale to support one method as being superior to another.|$|E
40|$|Exemplary {{embodiments}} {{describe a}} method for determining an exciter conductor distance between an exciter conductor 15, 16 of an exciter conductor structure 14 and a sensor element 20 a of a <b>calibratable</b> magnetic field sensor 10, wherein the exciter conductor structure 14 has a first exciter conductor 15 and a second exciter conductor 16 {{at a distance from}} the latter, and wherein the sensor element 20 a can be calibrated using the first exciter conductor 15 or the second exciter conductor 16. The method has a step of injecting 100 a first electrical current I 0 into the first exciter conductor 15 of the exciter conductor structure 14 in order to produce a first magnetic field component B 0,X in the sensor element 20 a of the magnetic field sensor 10 and a step of determining 110 a variable, which depends on the first magnetic field component B 0,X, using the sensor element 20 a.; The method also has a step of injecting 120 a second electrical current I 1 into the second exciter conductor 16 of the exciter conductor structure 14 in order to produce a second magnetic field component B 1,X in the sensor element 20 a of the magnetic field sensor 10 and a step of determining 130 a variable, which depends on the second magnetic field component B 1,X, using the sensor element 20 a. The method also comprises a step of determining 140 the exciter conductor distance between the exciter conductor 15, 16 and the sensor element 20 a of the magnetic field sensor 10 on the basis of an exciter conductor intermediate distance between the first exciter conductor 15 and the second exciter conductor 16 at a distance from the latter and the variables which depend on the first and second magnetic field components B 0,X and B 1,X...|$|E
40|$|The {{development}} and implementation of a new structure for data-driven models for NOX and soot emissions is described. The model structure is a linear regression model, where physically relevant input signals are used as regressors, and all the regression parameters are defined as grid-maps in the engine speed/injected fuel domain. The method of using grid-maps in the engine speed/injected fuel domain for all the regression parameters enables the models to be valid for changes in physical parameters that affect the emissions, without having to include these parameters as input signals to the models. This is possible for parameters that are dependent only on the engine speed and the amount of injected fuel. This means that models can handle changes for different parameters in the complete working range of the engine, without having to include all signals that actually effect the emissions into the models. The approach possibly also enables for the model to handle the main differences between steady-state engine operation and transient engine operation, thus possibly being able to use steady-state engine measurement data to calibrate the model, but still achieve acceptable performance for transient engine operation. This, however, is not evaluated in this study. The model structure has been used to create models for NOX and soot emissions. These models have been calibrated using measured steady-data from a 5 cylinder Volvo passenger car diesel engine with a displacement volume of 2. 4 liters, equipped with a turbocharger, an exhaust gas recirculation system, and a common rail injection system. The models estimate NOX mass flow with a root mean square error of 0. 0021 g/s and soot mass flow with a root mean square error of 0. 59 mg/s for the steady-state engine data used in this study. The models are capable of reacting to different <b>calibratable</b> engine parameters, and they are also fast to execute. This makes them suitable for development of engine management system optimization. The models could also be implemented directly into an engine management system. For comparison, three other fast models of different types for NOX and soot emissions have been implemented and evaluated...|$|E
40|$|Direct fuel {{injection}} technology is increasingly being {{applied to the}} spark ignition internal combustion engine {{as one of the}} many actions required to reduce the CO 2 emissions from road transport. Whilst the potential for CO 2 reductions is compelling, the technology is not without disadvantages. Early examples typically emitted over an order of magnitude more Particulate Matter (PM) than vehicles with conventional spark ignition engines. Consequently, future revisions to European and North American exhaust emissions legislation are likely to regulate the particulate emissions from vehicles with direct injection gasoline engines. This thesis undertakes to investigate a) instrumentation capable of simultaneously resolving the number concentration and size distribution of particles in the 5 - 1000 nm size range and b) the factors affecting the PM emissions from spark ignition engines with direct {{fuel injection}}. The first objective is achieved by evaluation and comparison of a differential mobility spectrometer; photo-acoustic soot sensor; condensation particle counter and electrical low pressure impactor. To address the second question, a differential mobility spectrometer is applied to quantify the PM emissions from a number of direct injection gasoline engines, together with investigation of their dependence on various <b>calibratable</b> parameters, operating temperature and fuel composition. The differential mobility spectrometer showed good agreement with the other more established instruments tested. Moreover, it exhibited a faster time response and finer resolution in particle size. The number weighted size distribution of the PM emitted was typically lognormal with either one or two modes located between 20 and 100 nm. Chemical analysis of PM samples showed the presence of elemental carbon, volatile organic material and sulphates. Transient PM measurements enabled short time-scale events such as mode switching between homogeneous and stratified mixture preparation to be identified. PM number concentrations in stratified mode exceeded those in homogeneous mode by a factor of 10 - 100. Dynamometer based experiments showed that PM emissions increase for rich air fuel ratios, retarded fuel injection and advanced ignition events. They also demonstrated a strong dependence on fuel composition: the highest PM emissions were measured with an aromatic fuel, whereas blending alcohols such as methanol or ethanol tended to suppress PM emissions, particularly in the accumulation mode size range. These measurements are amongst the first of their kind and demonstrate the applicability of the differential mobility spectrometer to the measurement of ultra-fine particulate emissions from engines with direct fuel injection systems. Numerous explanations are put forward to describe the data obtained, together with suggestions for future work on PM control and abatement. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|To meet {{customer}} {{demands for}} vehicle performance and to satisfy increasingly stringent emission standard, powertrain control strategies {{have become more}} complex and sophisticated. As a result, controller development and calibration have presented a time-consuming and costly challenge to the automotive industry. This thesis aims to develop new control methodologies with reduced calibration effort. Internal model control (IMC) lends itself to automotive applications for its intuitive control structure with simple tuning philosophy. A few applications of IMC to the boost-pressure control problem have been reported, however, none offered an implementable and easy-to-calibrate solution. Motivated {{by the need to}} develop robust and easily <b>calibratable</b> control technologies for boost-pressure control of turbocharged gasoline engines, this thesis developed new control design methodologies in the IMC framework. Two directions are pursued: adaptive IMC (AIMC) and nonlinear IMC. A plant model and a plant inverse are explicit components of IMC. In the presence of plant-model uncertainty, combining the IMC structure with parameter identification through the certainty equivalence principle leads to adaptive IMC (AIMC), where the plant model is identified and the plant inverse is derived by inverting the model. We propose the composite AIMC (CAIMC), which identifies the model and the inverse in parallel, and reduces the tracking error through the online identification. ``Composite" refers to the simultaneous identifications. The constraint imposed by the stability of an n-th order model is nonconvex, and it is re-parameterized as a linear matrix inequality. The parameter identification problem with the stability constraint is reformulated as a convex programming problem. Stability proof and asymptotic performance are established for CAIMC of a general n-th order plant. CAIMC is applied to the boost-pressure control problem of a turbocharged gasoline engine. It is first validated on a physics-based high-order and nonlinear proprietary turbocharged gasoline engine Simulink model, and then validated on a turbocharged 2 L four-cylinder gasoline engine on a Ford Explorer EcoBoost. Both simulations and experiments show that CAIMC is not only effective, but also drastically reduces the calibration effort compared to the traditional PI controller with feedforward. Nonlinear IMC is presented {{in the context of the}} boost-pressure control of a turbocharged gasoline engine. To leverage the available tools for linear IMC design, the quasi-linear parameter varying (quasi-LPV) models are explored. A new approach for nonlinear inversion, referred to as the structured quasi-LPV model inverse, is developed and validated. A fourth-order nonlinear model which sufficiently describes the dynamic behavior of the turbocharged engine is used as the design model, and the IMC controller is derived based on the structured quasi-LPV model inverse. The nonlinear IMC is applicable when the nonlinear system has a special structural property and has not been generalized yet. Simulations on a high-fidelity turbocharged engine model are carried out to show the feasibility of the proposed nonlinear IMC...|$|E
40|$|The {{requirement}} for scatterometer-combined transmit-receive gain variation knowledge is typically addressed by sampling {{a portion of}} the transmit signal, attenuating it with a known-stable attenuation, and coupling it into the receiver chain. This way, the gain variations of the transmit and receive chains are represented by this loop-back calibration signal, and can be subtracted from the received remote radar echo. Certain challenges are presented by this process, such as transmit and receive components that are outside of this loop-back path and are not included in this calibration, as well as the impracticality for measuring the transmit and receive chains stability and post fabrication separately, without the resulting measurement errors from the test set up exceeding the {{requirement for}} the flight instrument. To cover the RF stability design challenge, the portions of the scatterometer that are not calibrated by the loop-back, (e. g., attenuators, switches, diplexers, couplers, and coaxial cables) are tightly thermally controlled, and have been characterized over temperature to contribute less than 0. 05 dB of calibration error over worst-case thermal variation. To address the verification challenge, including the components that are not calibrated by the loop-back, a stable fiber optic delay line (FODL) was used to delay the transmitted pulse, and to route it into the receiver. In this way, the internal loopback signal amplitude variations can be compared to the full transmit/receive external path, while the flight hardware is in the worst-case thermal environment. The practical delay for implementing the FODL is 100 s. The scatterometer pulse width is 1 ms so a test mode was incorporated early in the design phase to scale the 1 ms pulse at 100 -Hz pulse repetition interval (PRI), by a factor of 18, to be a 55 s pulse with 556 s PRI. This scaling maintains the duty cycle, thus maintaining a representative thermal state for the RF components. The FODL consists of an RF-modulated fiber-optic transmitter, 20 km SMF- 28 standard single-mode fiber, and a photodetector. Thermoelectric cooling and insulating packaging are used to achieve high thermal stability of the FODL components. The chassis was insulated with 1 -in. (. 2. 5 -cm) thermal isolation foam. Nylon rods support the Micarta plate, onto which are mounted four 5 -km fiber spool boxes. A copper plate heat sink was mounted on top of the fiber boxes (with thermal grease layer) and screwed onto the thermoelectric cooler plate. Another thermal isolation layer in the middle separates the fiberoptics chamber from the RF electronics components, which are also mounted on a copper plate that is screwed onto another thermoelectric cooler. The scatterometer subsystem fs overall stability was successfully verified to be <b>calibratable</b> to within 0. 1 dB error in thermal vacuum (TVAC) testing with the fiber-optic delay line, while the scatterometer temperature was ramped from 10 to 30 C, which is a much larger temperature range than the worst-case expected seasonal variations...|$|E
40|$|The Michaelis–Menten {{kinetics}} and {{the reverse}} Michaelis–Menten kinetics are two popular mathematical formulations {{used in many}} land biogeochemical models to describe how microbes and plants would respond to changes in substrate abundance. However, the criteria of when to use {{either of the two}} are often ambiguous. Here I show that these two kinetics are special approximations to the equilibrium chemistry approximation (ECA) kinetics, which is the first-order approximation to the quadratic kinetics that solves the equation of an enzyme–substrate complex exactly for a single-enzyme and single-substrate biogeochemical reaction with the law of mass action and the assumption of a quasi-steady state for the enzyme–substrate complex and that the product genesis from enzyme–substrate complex is much slower than the equilibration between enzyme–substrate complexes, substrates, and enzymes. In particular, I show that the derivation of the Michaelis–Menten kinetics does not consider the mass balance constraint of the substrate, and the reverse Michaelis–Menten kinetics does not consider the mass balance constraint of the enzyme, whereas both of these constraints are taken into account in deriving the equilibrium chemistry approximation kinetics. By benchmarking against predictions from the quadratic kinetics {{for a wide range of}} substrate and enzyme concentrations, the Michaelis–Menten kinetics was found to persistently underpredict the normalized sensitivity ∂ ln v / ∂ ln k 2 + of the reaction velocity v with respect to the maximum product genesis rate k 2 +, persistently overpredict the normalized sensitivity ∂ ln v / ∂ ln k 1 + of v with respect to the intrinsic substrate affinity k 1 +, persistently overpredict the normalized sensitivity ∂ ln v / ∂ ln [E] T of v with respect the total enzyme concentration [E] T, and persistently underpredict the normalized sensitivity ∂ ln v / ∂ ln [S] T of v with respect to the total substrate concentration [S] T. Meanwhile, the reverse Michaelis–Menten kinetics persistently underpredicts ∂ ln v / ∂ ln k 2 + and ∂ ln v / ∂ ln [E] T, and persistently overpredicts ∂ ln v / ∂ ln k 1 + and ∂ ln v / ∂ ln [S] T. In contrast, the equilibrium chemistry approximation kinetics always gives consistent predictions of ∂ ln v / ∂ ln k 2 +, ∂ ln v / ∂ ln k 1 +, ∂ ln v / ∂ ln [E] T, and ∂ ln v / ∂ ln [S] T, indicating that ECA-based models will be more <b>calibratable</b> if the modeled processes do obey the law of mass action. Since the equilibrium chemistry approximation kinetics includes advantages from both the Michaelis–Menten kinetics and the reverse Michaelis–Menten kinetics and it is applicable for almost the whole range of substrate and enzyme abundances, land biogeochemical modelers therefore no longer need to choose when to use the Michaelis–Menten kinetics or the reverse Michaelis–Menten kinetics. I expect that removing this choice ambiguity will make it easier to formulate more robust and consistent land biogeochemical models...|$|E
40|$|Author(s) 2015. The Michaelis-Menten {{kinetics}} and {{the reverse}} Michaelis-Menten kinetics are two popular mathematical formulations {{used in many}} land biogeochemical models to describe how microbes and plants would respond to changes in substrate abundance. However, the criteria of when to use {{either of the two}} are often ambiguous. Here I show that these two kinetics are special approximations to the equilibrium chemistry approximation (ECA) kinetics, which is the first-order approximation to the quadratic kinetics that solves the equation of an enzyme-substrate complex exactly for a single-enzyme and single-substrate biogeochemical reaction with the law of mass action and the assumption of a quasi-steady state for the enzyme-substrate complex and that the product genesis from enzyme-substrate complex is much slower than the equilibration between enzyme-substrate complexes, substrates, and enzymes. In particular, I show that the derivation of the Michaelis-Menten kinetics does not consider the mass balance constraint of the substrate, and the reverse Michaelis-Menten kinetics does not consider the mass balance constraint of the enzyme, whereas both of these constraints are taken into account in deriving the equilibrium chemistry approximation kinetics. By benchmarking against predictions from the quadratic kinetics {{for a wide range of}} substrate and enzyme concentrations, the Michaelis-Menten kinetics was found to persistently underpredict the normalized sensitivity & part; ln v / & part; ln k 2 + of the reaction velocity v with respect to the maximum product genesis rate k 2 +, persistently overpredict the normalized sensitivity & part; ln v / & part; ln k 1 + of v with respect to the intrinsic substrate affinity k 1 +, persistently overpredict the normalized sensitivity & part; ln v / & part; ln [E] T of v with respect the total enzyme concentration [E] T, and persistently underpredict the normalized sensitivity & part; ln v / & part; ln [S] T of v with respect to the total substrate concentration [S] T. Meanwhile, the reverse Michaelis-Menten kinetics persistently underpredicts & part; ln v / & part; ln k 2 + and & part; ln v / & part; ln [E] T, and persistently overpredicts & part; ln v / & part; ln k 1 + and & part; ln v / & part; ln [S] T. In contrast, the equilibrium chemistry approximation kinetics always gives consistent predictions of & part; ln v / & part; ln k 2 +, & part; ln v / & part; ln k 1 +, & part; ln v / & part; ln [E] T, and & part; ln v / & part; ln [S] T, indicating that ECA-based models will be more <b>calibratable</b> if the modeled processes do obey the law of mass action. Since the equilibrium chemistry approximation kinetics includes advantages from both the Michaelis-Menten kinetics and the reverse Michaelis-Menten kinetics and it is applicable for almost the whole range of substrate and enzyme abundances, land biogeochemical modelers therefore no longer need to choose when to use the Michaelis-Menten kinetics or the reverse Michaelis-Menten kinetics. I expect that removing this choice ambiguity will make it easier to formulate more robust and consistent land biogeochemical models...|$|E
40|$|AC {{susceptibility}} is {{an important}} characterisation technique measuring the time dependent magnetisation and dynamics of a magnetic system. It is capable of yielding information on thermodynamic phase transitions, relaxation processes and losses {{in a variety of}} interesting magnetic and superconducting materials. In particular it is a powerful probe of the mixed state of superconductivity providing insight into the ux dynamics at play and determination of a number of physical properties such as the critical temperature Tc, field Hc and characteristic length scales. Application of pressure can tune materials through multiple phases and interesting phenomena. The thesis describes the design of a <b>calibratable</b> susceptometer in a piston cylinder pressure cell, achieving AC susceptibility measurements of the same accuracy as a SQUID magnetometer but under pressure. This is used to make measurements on an electrostatically doped capacitance device, a single chain magnet and a heavy fermion superconductor. These studies are summarised below. Electric double layer (EDL) devices provide a means of continuous tuning through a materials phase diagram by applying an electric field, including inducing superconductivity. Application of pressure in tandem with electrostatic doping could improve the efficiency of these devices and provide a second tuning parameter. An EDL capacitor was constructed and measured with the above susceptometer aiming to shift the Tc of a doped high temperature superconducting cuprate La 1 : 9 Sr 0 : 1 CuO 4. The Tc shifts proved irreproducible already at ambient conditions. Indeed {{during the course of this}} research further experimental evidence emerged in the literature indicating EDL devices may very well work due to electrochemical doping rather than electrostatic, possibly accounting for the lack of repeatability. Work therefore focused on mapping the ionic liquid DEME-TFSI's glass-liquid phase diagram over the 1 GPa pressure range, rather than extending the study of the EDLC device to high pressure. Single chain magnets (SCM) are an interesting class of material consisting of a one-dimensional molecular magnet chain manifesting magnetic hysteresis and slow relaxation best characterised by AC susceptibility. The susceptometer was used to study the SCM [Co(NCS) 2 (pyridine) 2]n to investigate the effect of pressure on its characteristic magnetic relaxation time and energy barrier. A secondary signal appears at 0. 44 GPa which is attributed to the development of an additional structural phase that has been independently observed in X-ray crystallographic measurements. The heavy fermion superconductor U 6 Fe has the highest Tc 4 K of all the U-based compounds and large critical fields of 10 - 12. 5 T, depending on direction, which increase on initial application of pressure. It exhibits a coexisting charge density wave (CDW) below 10 K making it a promising candidate for the modulated superconductivity of the theorised Fulde-Ferrel-Larkin-Ovchinnikov (FFLO) state. A feature at 110 K is also evident in Mossbauer, resistivity and specific heat measurements, the origin of which has not yet been clearly identified. Evidence for the FFLO state was sought by mapping the upper critical field Hc 2 along with the peak effect through AC susceptibility measurements up to pressures of 1 GPa. The data is accounted for by an evolution of collective pinning and superconducting parameters, with no clear evidence for an FFLO state although an enhancement of the reduced field is observed...|$|E
40|$|The European Water Framework Directive imposes on {{member states}} to reach a good water quality status of all their river bodies by 2015. To asses if this goal will be reached and, if not reached, which {{measures}} can be undertaken to remediate the situation, an integrated hydrological-hydrodynamic-water quality river model is required that can simulate the concentrations of relevant water quality state variables in the river under given boundary conditions and for long term rainfall series. Since so many different inputs and processes determine these concentrations, the resulting integrated (physically based) model has an excessively high calculation time. Calibration of the parameters that govern the water quality transformation processes has serious practical difficulties. However, when the detailed model {{is transformed into a}} conceptual model by placing non-linear reservoirs in series to represent the river, the computational times can be reduced with a factor 5. 103. This approach is applied to the Grote Laak, a river of 13 km situated in the North East of Belgium. To test the robustness of the conceptual model, and thus asses the validity of this global sensitivity analysis approach, 10 random parameter sets are ran both with the detailed and the conceptual model. Comparison of the conceptual with the detailed model results show that the degree of similarity between both models is such that the conceptual model can be used for time consuming processes such as the calibration of model parameters, scenario runs and long term simulations for statistical processing. The resulting conceptual model is used for sensitivity analysis by means of Monte Carlo simulations to determine the most sensitive parameters of the 23 <b>calibratable</b> parameters that define the water quality transformation processes. More specifically, Latin hypercube sampling was used to select 5000 random parameter sets from their probability distribution as determined by expert judgment. The simulated 90 th and 50 th percentile of dissolved oxygen (DO), biological oxygen demand (BOD), ammonia (NH 4), nitrate (NO 3) and temperature (Temp) is calculated as this value is used to determine compliance to water quality standards. A multiple linear regression is performed on the standardized inputs and outputs of the model to obtain the standardized regression coefficient (SRC) for each of the parameters. The absolute values of these SRC’s allow for a ranking of the parameters. From this analysis {{it is clear that the}} first order BOD decay rate is the most important parameter in the uncertainty of the model result as it ranks in the top three of most influential parameters for both DO, BOD and NH 4 at all locations. The third most important parameter in terms of magnitude of sensitivity is the sedimentation rate of BOD. Both parameters indicate the need for a good knowledge on the composition of the BOD in the river and thus could indicate were additional measurement efforts should be spend on. The global sensitivity analysis indicates that from the 23 model parameters, only 9 have a significant contribution to the uncertainty in the model output result. Further calibrate should thus focus on these 9 parameters while keeping the values of the other parameters constant at the recommended value. status: publishe...|$|E

