14|73|Public
40|$|We propose DFSM's as an {{extension}} of finite state machines, explore some of their properties, and indicate how they can be used to formalize naturally occurring linguistic systems. We feel that this implementation of two-level rules may be more linguistically natural and easier to work with computationally. We provide complexity results that shed light on the <b>computational</b> <b>situation...</b>|$|E
40|$|Human {{computation}} tackles many difficult-to-automate problems. However, {{the involvement}} of human solvers in computation process leads to unstable performance, especially when solvers recruited via Internet. By considering human’s attributes and <b>computational</b> <b>situation,</b> the research model of factors of affecting the effectiveness and efficiency of human computation algorithm is proposed in this paper, together with seven proxies of human’s attributes and <b>computational</b> <b>situation.</b> For testing the proposed mod-el, we collect data from an implemented Web application testing system using a crowdsourcing approach. There are 87 volunteers recruited via social network and the corresponding user-session data. The results show that more human solvers with lower ability level, higher engagement, more personal bias and lower difficulty level of tasks lead to shorter completion time. Human solvers with higher ability level, higher engage-ment, more personal bias and lower difficulty level of tasks lead to more accurate output, but number of solvers does not show significant correlation with the correctness of hu-man computation algorithm...|$|E
40|$|We {{describe}} {{a novel approach}} {{to the analysis of}} pronominal anaphora in Turkish. A computational medium which is based on situation theory is used as our implementation tool. The task of resolving pronominal anaphora is demonstrated in this environment which employs situation-theoretic constructs for processing. Zusammenfassung. Wir beschreiben einen neuartigen Ansatz fur die Analyse pronominaler Anaphern im Turkischen. Fur die Implementation wird eine situationstheoretisch fundierte Entwicklungsumgebung verwendet. Mithilfe dieser Umgebung, die situationstheoretische Konstrukte unterstutzt, demonstrieren wir die Auflosung pronominaler Anaphern. Keywords. <b>Computational</b> <b>situation</b> theory, situation semantics, anaphora 1 Introduction In written/spoken discourse, people use certain instruments for `pointing back' in the discourse context to individuals, objects, events, times, and concepts mentioned previously. Such anaphoric mechanisms comprise pronouns, definite noun phrases, and elli [...] ...|$|E
50|$|For {{technical}} purposes, injective sheaves {{are usually}} {{superior to the}} other classes of sheaves mentioned above: they can do almost anything the other classes can do, and their theory is simpler and more general. In fact, injective sheaves are flabby (flasque), soft, and acyclic. However, there are situations where the other classes of sheaves occur naturally, and {{this is especially true}} in concrete <b>computational</b> <b>situations.</b>|$|R
40|$|A {{metalanguage}} for {{denotational semantics}} {{is given a}} logical interpretation: types are interpreted as propositional theories; terms (in an extended typed λ-calculus, denoting elements of types) are embedded in an endogenous program logic, which characterises their behaviour in terms of which properties they satisfy; terms (in {{an extension of the}} algebraic metalanguage of cartesian closed categories, denoting morphisms between types) are embedded in an exogenous logic, which characterises their behaviour as predicate transformers or modal operators. This interpretation is related to the standard domain-theoretic one via the machinery of Stone duality, and an exact correspondence is obtained. Some applications to logics for specific <b>computational</b> <b>situations</b> (e. g. concurrency) are mentioned. ...|$|R
40|$|This volume {{presents}} {{a collection of}} activities and games for use in elementary school mathematics laboratories. The activities and games included were submitted by classroom teachers and were selected for their use of manipulative materials or their reliance on student interactions. Several of the activities included have been described in "The Arithmetic Teacher, " but many are first published in this volume. Activities described are in eight subject matter categories: (1) number concepts, (2) addition and subtraction, (3) multiplication and division, (4) number skills review, (5) measurement,*(6) fractions, (7) graphs and functions, and (8) geometric concepts. The goals of activities range from increasing speed and power in <b>computational</b> <b>situations</b> to concept learning, development of strategies, and discovery. Each activity description is in outline form and includes statements of goals and purposes materials needed, procedures involved, and activity source. Many descriptions include diagrams and instructions for making any necessary materials. (SD) I...|$|R
40|$|This paper desribes a {{study that}} {{assessed}} the feasibility of developing an adaptive pilot/vehicle interface (PVI) prototype that uses measures of pilot workload and <b>computational</b> <b>situation</b> assessment models to drive the content, format, and modality of military cockpit displays. The system architecture consists of three distinct modules: 1) an on-line situation assessor that generates a "picture" of the tactical situation; 2) a pilot state estimator module that uses physiological signals and other measures to estimate pilot workload; and 3) a PVI adaptation module, which uses the assessed situation to determine the pilot's information requirements, and the pilot state estimate to determine the most appropriate modality and format for conveying that information. The prototype display is being integrated with the Synthetic Immersion Research Environments (SIRE) simulator at the Armstrong Laboratory of Wright Patterson Air Force Base...|$|E
40|$|Abstract—A formal {{approach}} {{to the design of}} situation analysis and decision support systems is justified and unavoidable if one is interested in reproducibility/traceability of results, satisfaction of constraints, and a language to represent and reason about dy-namic situations. In this paper, we propose the integration of two multiagent modeling paradigms, Abstract State Machines and Interpreted Systems, to develop a comprehensive framework for <b>computational</b> <b>Situation</b> Analysis (SA) as a basis for design and development of decision support systems. Due to the similarities of the underlying modeling concepts, a systematic integration of the two paradigms seems sensible, as each one has its particular focus and strength, complementing each other in several respects. Our approach builds on multiagent systems theories to formalize the distributed aspect, allows for reasoning about knowledge, uncertainty and belief change, and enables rapid prototyping of abstract executable decision support system models...|$|E
40|$|This paper {{describes}} an effort {{under way to}} develop pilot/vehicle interface (PVI) concepts that use <b>computational</b> <b>situation</b> assessment models and pilot workload metrics to drive the content, format, and modality of cockpit displays. The goal {{of this research is}} to develop PVI concepts that support a tactical pilot's situation awareness and decision-making. The envisioned system is driven by two key information streams: 1) a content path, driven by a tactical situation assessment module that uses avionics system outputs to determine the aircraft's current tactical situation and the pilot's information needs based on the situation; and 2) a format path, which uses an estimate of the pilot's state (workload level, attentional focus, etc.) to determine the most appropriate modality for conveying the required information to the pilot. The system will be integrated into the SIRE simulator at the Armstrong Laboratory. Introduction Advances in aircraft performance and weapons capabili [...] ...|$|E
5000|$|A {{family of}} Prolog-like {{concurrent}} message passing systems using unification of shared variables and data structure streams for messages {{were developed by}} Keith Clark, Hervé Gallaire, Steve Gregory, Vijay Saraswat, Udi Shapiro, Kazunori Ueda, etc. Some of these authors made claims that these systems were based on mathematical logic. However, like the Actor model, the Prolog-like concurrent systems were based on message passing and consequently were subject to indeterminacy in the ordering of messages in streams that {{was similar to the}} indeterminacy in arrival ordering of messages sent to Actors. Consequently Carl Hewitt and Gul Agha 1991 concluded that the Prolog-like concurrent systems were neither deductive nor logical. They were not deductive because computational steps did not follow deductively from their predecessors and they were not logical because no system of mathematical logic was capable of deriving the facts of subsequent <b>computational</b> <b>situations</b> from their predecessors ...|$|R
40|$|Mathematics, 2000), {{but little}} {{direction}} is given {{as to how}} children make the choice as to which form of computation to use in any given situation. In setting the standard for computation in Australia the National Statement on Mathematics (Australian Education Council, 1991) included the following comments. All school leavers should feel confident in their capacity {{to deal with the}} <b>computational</b> <b>situations</b> which they meet daily, and number work should reflect the balance of number techniques in regular adult use … Students should develop the ability to judge the level of accuracy needed, learn to estimate and approximate, and use mental, calcu-lator and paper-and-pencil strategies effectively and appropriately in different situations … This requires that they: • decide what operations to perform (formulate the calcu-lation); • select a means of carrying out the operation (choose a method of calculation); • perform the operation (carry out the calculation); • make sense of the answer (interpret the results of the calculation). (p. 108...|$|R
40|$|Abstract:The {{objective}} of the research work is to propose a software service compliance management model towards portability fault tolerance (PFT) using RESTFUL web services exchanges in distributed systems. The compliance management is achieved through correct identification of the various non-compliances while deploying and executing software services on various hardware and system platforms. The portability faults are due to operational platform changes (OPC), service audit constraints (SAC) and multiple service contracts (MSC). These portability faults can be detected and identified through the proposed set of services, namely SARALA {{that are based on}} <b>computational</b> <b>situations,</b> resources and locations of the needed services. The software services in HIPAA framework for mobile health care system are to be portable but also accountable where both are also to be compliant with existing government policies and regulations. The incorrect portability attributes which lead to non-compliances in the network protocol, the third party software and data format are checked to detect the causes of policy violations and activate the necessary enforcement actions to meet the healthcare acts...|$|R
40|$|We propose DFSM's as an {{extension}} of finite state machines, explore some of their properties, and indicate how they can be used to formalize naturally occurring linguistic systems. We feel that this implementation of two-level rules may be more linguistically natural and easier to work with computationally. We provide complexity results that shed light on the <b>computational</b> <b>situation.</b> 1. Introduction Two-level phonology combines the computational advantages of finite state technology with a formalism that permits phenomena to be described with familiar-looking rules. The problem with such a scheme is that, in practice, the finite state machines (FSM's) can grow too large to be manageable; one wants to describe them and to run them without having to deal with them directly. The Kimmo approach 1 seeks to achieve this by (1) decomposing the computational process into a battery of parallel finite state machines and (2) compiling rules (which notationally resemble familiar phonologic [...] ...|$|E
40|$|This paper proposes {{the use of}} {{situation}} theory as a basic semantic formalism for defining general semantic theories. Astl, a <b>computational</b> <b>situation</b> theoretic language, is described which goes some way to offering such a system. After a general description of Discourse Representation Theory an encoding of DRT in astl is given. Advantages and disadvantages of this method are then discussed. Topic: computational formalisms in semantics and discourse Introduction The {{purpose of this paper}} is to show how a computational language based on situation theory can be used as a basic formalism in which general semantic theories can be implemented. There are many different semantic theories which, because of their different notations, are difficult to compare. A general language which allows those theories to be implemented within it would offer an environment where similar semantic theories could be more easily evaluated. Situation Theory (ST) has been developed over the last ten years [2]. [...] ...|$|E
40|$|Situation {{theory is}} a {{mathematical}} theory of meaning introduced by Jon Barwise and John Perry. It has evoked great theoretical interest and motivated {{the framework of}} a few `computational' systems. PROSIT is the pioneering work in this direction. Unfortunately, {{there is a lack of}} real-life applications on these systems and this study is a preliminary attempt to remedy this deficiency. Here, we solve a group of epistemic puzzles using the constructs provided by PROSIT. Keywords: <b>computational</b> <b>situation</b> theory, epistemic puzzles, common knowledge, PROSIT 1 Introduction Situation theory is a principled programme to develop a mathematical theory of meaning which aims to clarify and resolve some formidable problems in the study of language, information, logic, and philosophy of mind. It was introduced by Jon Barwise and John Perry in their Situations and Attitudes [3] and stimulated great interest. The theory matured within the last ten years or so [6, 7] and various versions of it have b [...] ...|$|E
30|$|However, one {{may argue}} that {{surgeons}} and computational scientist are practitioners in their own scientific field. Computational science is the ‘clinical’ activity of a mathematician. Each <b>computational</b> model <b>situation</b> is different by its parameter setting, boundary conditions, and often nonlinear behavior. Most often, simulation is achieved in unknown territory for the mathematical theory, i.e., existence and uniqueness of the solution are not guaranteed by a theorem! The (full 3 D) Navier Stokes equation, known for a century, is a classical example.|$|R
40|$|Primary sex {{determination}} in placental mammals {{is a very}} well studied developmental process. Here, we aim to investigate the currently established scenario and to assess its adequacy to fully recover the observed phenotypes, in the wild type and perturbed <b>situations.</b> <b>Computational</b> modelling allows clarifying network dynamics, elucidating crucial temporal constrains as well as interplay between core regulatory modules. Fundação Calouste Gulbenkian...|$|R
40|$|The {{well-known}} Jarzynski equality, often {{written in}} the form e^-βΔ F=〈 e^-β W〉, provides a non-equilibrium means to measure the free energy difference Δ F of a system at the same inverse temperature β based on an ensemble average of non-equilibrium work W. The accuracy of Jarzynski's measurement scheme {{was known to be}} determined by the variance of exponential work, denoted as var(e^-β W). However, it was recently found that var(e^-β W) can systematically diverge in both classical and quantum cases. Such divergence will necessarily pose a challenge in the applications of Jarzynski equality because it may dramatically reduce the efficiency in determining Δ F. In this work, we present a deformed Jarzynski equality for both classical and quantum non-equilibrium statistics, in efforts to reuse experimental data that already suffers from a diverging var(e^-β W). The main feature of our deformed Jarzynski equality is that it connects free energies at different temperatures and it may still work efficiently subject to a diverging var(e^-β W). The conditions for applying our deformed Jarzynski equality may be met in experimental and <b>computational</b> <b>situations.</b> If so, then {{there is no need to}} redesign experimental or simulation methods. Furthermore, using the deformed Jarzynski equality, we exemplify the distinct behaviors of classical and quantum work fluctuations for the case of a time-dependent driven harmonic oscillator dynamics and provide insights into the essential performance differences between classical and quantum Jarzynski equalities. Comment: 24 pages, 1 figure, accepted version to appear in Entropy (Special Issue on "Quantum Thermodynamics"...|$|R
40|$|For a {{roll gap}} of {{continuous}} plastic forming, {{the determination of}} mechanical parameters of a rolling processing involves a multi-body elasto-plastic frictional contact problem. As {{one of the three}} important numerical analytic methods, the Boundary Element Method (BEM) is suitable for the solution of contact problems, and it shows superiority to the Finite Element Method (FEM) and Finite Difference Method (FDM) in these cases. However, when the contact objects become very complicated and large-scale discrete nodes are generated, there are inherent difficulties for the BEM, such as time-consuming problem, low efficiency, and so on. To solve these problems, a kind of Fast Multi-pole Boundary Element Method (FM-BEM) is proposed. Combining the Fast Multi-pole Method (FMM) with BEM opens up a new <b>computational</b> <b>situation,</b> especially when a high efficient solver named Generalized Minimal Residual Algorithm (GMRES (m)) is introduced. Then a node-to-surface frictional contact model and a programming-iteration algorithm are developed. On a PVM network parallel platform, the cold rolling process of 2030 four-high mill with a width-to-thickness ratio reaching 1850 is successfully simulated. The total freedom is 18414 and the CPU time is 42 hours and 24 minutes. For this rolling problem, both the high precision and the high computational efficiency are impossible for other numerical analytic methods...|$|E
40|$|AbstractAbramsky, S., Domain {{theory in}} logical form, Annals of Pure and Applied Logic 51 (1991) 1 – 77. •Domain theory, the {{mathematical}} theory of computation introduced by Scott {{as a foundation}} for detonational semantics•The theory of concurrency and systems behaviour developed by Milner, Hennesy based on operational semantics. •Logics of programsStone duality provides a junction between semantics (spaces of points=detonations of computational processes) and logics (lattices of properties of processes). Moreover, the underlying logic is geometric, which can be computationally interpreted as the logic of observable properties—i. e., properties which can be determined to hold of a process {{on the basis of a}} finite amount of information about its execution. These ideas lead to the following programme. (1) A metalanguage is introduced, comprising•types = universes of discourse for various computational situations;•terms = programs = syntactic intensions for models or points. (2) A standard denotational interpretation of the metalanguage is given, assigning domains to types and domain elements to terms. (3) The metalanguage is also given a logical interpretation, in which types are interpreted as propositional theories and terms are interpreted via a program logic, which axiomatises the properties they satisfy. (4) The two interpretations are related by showing that they are Stone duals of each other. Hence, semantics and logic are guaranteed to be in harmony with each other, and in fact each determines the other up to isomorphism. (5) This opens the way to a whole range of applications. Given a denotational description of a <b>computational</b> <b>situation</b> in our metalanguage, we can turn the handle to obtain a logic for that situation...|$|E
40|$|Domain Theory, the {{mathematical}} theory of computation introduced by Scott {{as a foundation}} for denotational semantics. The theory of concurrency and systems behaviour developed by Milner, Hennessy based on operational semantics. Logics of programs. Stone duality provides a junction between semantics (spaces of points = denotations of computational processes) and logics (lattices of of processes). Moreover, the underlying logic is, which can be computationally interpreted as the logic of properties [...] i. e. properties which can be determined to hold of a process {{on the basis of a}} finite amount of information about its execution. These ideas lead to the following programme: 1. A metalanguage is introduced, comprising types = universes of discourse for various computational situations. terms = programs = syntactic intensions for models or points. 2. A standard denotational interpretation of the metalanguage is given, assigning do-mains to types and domain elements to terms. 3. The metalanguage is also given a interpretation, in which types are interpreted as propositional theories and terms are interpreted a program logic, which ax- iomatizes the properties they satisfy. 4. The two interpretations are related by showing that they are Stone duals of each other. Hence, semantics and logic are guaranteed to be in harmony with each other, and in fact each determines the other up to isomorphism. 5. This opens the way to a whole range of applications. Given a denotational description of a <b>computational</b> <b>situation</b> in our meta-language, we can turn the handle to obtain a logic for that situation...|$|E
40|$|Stagnation is {{a common}} problem that all ant {{algorithms}} suffer from regardless of their application domain. This paper proposes a new algorithmic approach that can effectively be used to tackle combinatorial optimization problems with {{a good chance to}} control the stagnation. The new approach utilizes multiple ant colonies with certain techniques that efficiently organize the work of these colonies to avoid the stagnation <b>situations.</b> <b>Computational</b> tests show that the proposed approach is competitive with other state of art ant algorithms...|$|R
40|$|The {{constraint}} {{paradigm is}} a model of computation in which values are deduced whenever possible, under the limitation that deductions be local in a certain sense. One may visualize a constraint 'program' as a network of devices connected by wires. Data values may flow along the wires, and computation is performed by the devices. A device computes using only locally available information (with a few exceptions), and places newly derived values on other, locally attached wires. In this way computed values are propagated. An advantage of the constraint paradigm (not unique to it) is that a single relationship can be used in more than one direction. The connections to a device are not labelled as inputs and outputs; a device will compute with whatever values are available, and produce as many new values as it can. General theorem provers are capable of such behavior, but tend to suffer from combinatorial explosion; it is not usually useful to derive all the possible consequences of a set of hypotheses. The constraint paradigm places a certain kind of limitation on the deduction process. The limitations imposed by the constraint paradigm are not the only one possible. It is argued, however, that they are restrictive enough to forestall combinatorial explosion in many interesting <b>computational</b> <b>situations,</b> yet permissive enough to allow useful computations in practical situations. Moreover, the paradigm is intuitive: It is easy to visualize the computational effects of these particular limitations, and the paradigm is a natural way of expressing programs for certain applications, in particular relationships arising in computer-aided design. A number of implementations of constraint-based programming languages are presented. A progression of ever more powerful languages is described, complete implementations are presented and design difficulties and alternatives are discussed. The goal approached, though not quite reached, is a complete programming system which will implicitly support the constraint paradigm to the same extent that LISP, say, supports automatic storage management...|$|R
40|$|Abstract. Vision is {{a crucial}} sensor. It {{provides}} a very rich collection of informa-tion about our environment. However, not everything in a visual scene is relevant for the task at hand. Feature-based attention has been suggested for guiding vi-sion towards the objects of interest in a visual search <b>situation.</b> <b>Computational</b> models of visual attention have implemented different concepts of feature-based attention. We will discuss these approaches and present a solution {{which is based on}} population-based inference. We illustrate the proposed mechanism with sim-ulations using real world-scenes. ...|$|R
40|$|We {{survey on}} the ongoing {{research}} that relates the combinatorics of parity games to the algebra of categories with finite products, finite coproducts, initial algebras and final coalgebras of definable functors, i. e. µ-bicomplete categories. We argue that parity {{games with a}} given starting position {{play the role of}} terms for the theory of µ-bicomplete categories. We show that the interpretation of a parity game in the category of sets and functions is the set of deterministic winning strategies for one player in the game. We discuss bounded memory communication strategies between two parity games and their computational significance. We describe how an attempt to formalize them within the algebra of µ-bicomplete categories leads to develop a calculus of proofs that are allowed to contain cycles. This paper is a survey on our recent work lifting results on free µ-lattices [1, 2] to a categorical setting. A µ-lattice is a lattice with enough least and greatest fixed points to interpret formal µ-terms. A generalization of this notion leads to consider categories with finite products, finite coproducts, and enough initial algebras and final coalgebras of functors. We call these categories µ-bicomplete. The outcome of this research is so far described in [3, 4, 5]. A main goal for us is to understand how the algebra of µ-bicomplete categories describes a <b>computational</b> <b>situation</b> through the combinatorics of games; when attempting to achieve this goal, computational logic and proof-theory become unavoidable ingredients. It is the aim of this note to give insights on how these four worlds – categories, games, computation and logic – relate in this context. As the need of a mathematical formalization has too often hidden these relationships, we shall present here only informal arguments. The reader will find formal proofs of the statements in the references cited above...|$|E
40|$|The {{mathematical}} {{framework of}} Stone duality {{is used to}} synthesize a number of hitherto separate developments in Theoretical Computer Science: - Domain Theory, the mathematical theory of computation introduced by Scott {{as a foundation for}} denotational semantics. - The theory of concurrency and systems behaviour developed by Milner, Hennessy et al. based on operational semantics. - Logics of programs. Stone duality provides a junction between semantics (spaces of points = denotations of computational processes) and logics (lattices of properties of processes). Moreover, the underlying logic is geometric, which can be computationally interpreted as the logic of observable properties [...] -i. e. properties which can be determined to hold of a process {{on the basis of a}} finite amount of information about its execution. These ideas lead to the following programme: 1. A metalanguage is introduced, comprising - types = universes of discourse for various computational situations. - terms = programs = syntactic intensions for models or points. 2. A standard denotational interpretation of the metalanguage is given, assigning domains to types and domain elements to terms. 3. The metalanguage is also given a logical interpretation, in which types are interpreted as propositional theories and terms are interpreted via a program logic, which axiomatizes the properties they satisfy. 4. The two interpretations are related by showing that they are Stone duals of each other. Hence, semantics and logic are guaranteed to be in harmony with each other, and in fact each determines the other up to isomorphism. This opens the way to a whole range of applications. Given a denotational description of a <b>computational</b> <b>situation</b> in our meta-language, we can turn the handle to obtain a logic for that situation. Comment: 235 pages. Ph. D thesis, 1988, Queen Mary College, University of Londo...|$|E
40|$|This {{bachelor}} thesis {{deals with}} {{the evaluation of the}} energy performance of elementary school in the village Líbeznice. The theoretical part addresses the current legislative code for the energy performance of buildings and the description of the energy required written work with graphical outputs that are used in practical <b>situations.</b> <b>Computational</b> and practical part contains an evaluation of the energy performance certificate in the form of building energy performance. In connection with the proposed austerity measures and then selected the best option. Economic and ecological standpoint evaluates energy assessment...|$|R
40|$|The paper {{presents}} {{a way for}} encoding representations as concurrent processes. Based on linguistic data, we first argue representations constitute {{a part of the}} meaning of sentences. Then, we regard representations as actions rather than static picture of things. To implement such an idea, we turn to the pi-calculus and encode situation-theoretic objects into the calculus. Through the encoding, <b>computational</b> aspects of <b>Situation</b> Theory are investigated. We also show how sentences can be analyzed in our approach. Finally, we discuss what representations are and what outcome our approach would bring about...|$|R
40|$|Serious {{thinking}} about the <b>computational</b> aspects of <b>situation</b> theory is just starting. There have been some recent proposals in this direction (viz. PROSIT and ASTL), {{with varying degrees of}} divergence from the ontology of the theory. We believe that a programming environment incorporating bona fide situation-theoretic constructs is needed and describe our very recent BABY-SIT implementation. A detailed critical account of PROSIT and ASTL is also offered in order to compare our system with these pioneering and influential frameworks. Comment: 30 pages, also published in the University of Tuebingen Technical Report Serie...|$|R
40|$|A {{dynamical}} {{systems approach}} for modeling changing spa-tial environments is formalised. The formalisation adheres to the representational and <b>computational</b> semantics of <b>situation</b> calculus {{and includes a}} systematic account of all aspects nec-essary to implement a domain-independent qualitative spa-tial theory that is applicable across diverse application areas. Foundational to the formalisation is a situation calculus based causal theory and a generalised view of qualitative spatial cal-culi that encompass one or more spatial domains. Further-more, aspects considered inherent to dynamic spatial systems are also accounted for and the relevant computational tasks addressed by the proposed formalisation are stated explicitly...|$|R
40|$|Disasters {{can be very}} {{stressful}} events. However, computational models of stress require data that might {{be very difficult to}} collect during disasters. Moreover, personal experiences are not repeatable, so {{it is not possible to}} collect bottom-up information when building a coherent model. To overcome these problems, we propose the use of computational models and virtual reality integration to recreate disaster situations, while examining possible dynamics in order to understand human behavior and relative consequences. By providing realistic parameters associated with disaster <b>situations,</b> <b>computational</b> scientists can work more closely with emergency responders to improve the quality of interventions in the future...|$|R
40|$|Abstract. <b>Computational</b> {{models of}} <b>situation</b> {{awareness}} {{can be of}} interest for different purposes, varying {{from the study of}} human cognition in demanding circumstances to the development of human-like virtual opponents in serious gaming applications. This paper presents a novel model of situation awareness, which extends previous work at a number of points. In particular, the model incorporates qualitative time references, offers the possibility to use Allen [1]‟s temporal relations, and features an explicit representation of Endsley [4]‟s three phases of situation awareness. The behaviour of the model has been tested within a simulation environment for F- 16 pilots, and the resulting behaviour has been found satisfactory using a formal verification tool...|$|R
40|$|Several {{norms for}} how people should assess a question's {{usefulness}} have been proposed, notably Bayesian diagnosticity, information gain (mutual information), Kullback-Liebler distance, probability gain (error minimization), and impact (absolute change). Several probabilistic models of previous experiments on categorization, covariation assessment, medical diagnosis, and the selection task are shown to not discriminate among these norms as descriptive models of human intuitions and behavior. <b>Computational</b> optimization found <b>situations</b> in which information gain, probability gain, and impact strongly contradict Bayesian diagnosticity. In these situations, diagnosticity's claims are normatively inferior. Results {{of a new}} experiment strongly contradict the predictions of Bayesian diagnosticity. Normative theoretical concerns also argue against use of diagnosticity. It is concluded that Bayesian diagnosticity is normatively flawed and empirically unjustified...|$|R
40|$|Project and {{the growing}} {{computational}} challenges presented by {{the large amount of}} genomic data available today, machine learning is becoming an integral part of biomedical research and {{plays a major role in}} the emerging fields of bioinformatics and <b>computational</b> biology. This <b>situation</b> offers unparalleled opportunities and unprecedented challenges to machine learning research in general and to Bayesian learning methods in particular. This paper outlines some of the opportunities and the challenges of this endeavor, it describes where the efforts of &quot;cracking the code of life &quot; can most benefit from a Bayesian approach, and it identifies some potential applications of Bayesian machine learning methods to the genomic analysis of squamous cell carcinomas of the head and neck...|$|R
40|$|Abstract. The {{numerical}} computation of the exponentiation {{of a real}} matrix has been intensively studied. The main objective of a good numerical method is to deal with round-off errors and <b>computational</b> cost. The <b>situation</b> is more complicated when dealing with interval matrices exponentiation: Indeed, the main problem will now be the dependency loss of the different occurrences of the variables due to interval evaluation, which may lead to so wide enclosures that they are useless. In this paper, the problem of computing a sharp enclosure of the interval matrix exponential is proved to be NP-hard. Then the scaling and squaring method is adapted to interval matrices and shown to drastically reduce the dependency loss w. r. t. the interval evaluation of the Taylor series. 1...|$|R
40|$|Abstract. This article extends a {{framework}} intended {{to simplify the}} generation of use cases. The framework allows a concrete and systematic way of describing actions {{by means of a}} use case pattern and simple natural language sentences. The framework is composed of a pattern for specifying a use case and a set of guidelines for writing the sentences describing it. These guidelines help the specifier to write natural language sentences that may be easy and uniformly understood. The extension presented in this article uses situation theory to formalise the use cases as situations and to translate the simple sentences into situation theoretic concepts. This article refers to PROSIT, a <b>computational</b> implementation of <b>situation</b> theory, to show how use cases can be described in terms of situation theory. ...|$|R
