9|41|Public
40|$|Traveltime {{computation}} is {{an important}} part of seismic imaging algorithms. Conventional implementations of Kirchhoff migration require precomputing traveltime tables or include traveltime calculation in the innermost <b>computational</b> <b>loop.</b> The cost of traveltime computations is especially noticeable in the case of 3 -D prestack imaging where the input data size increase...|$|E
40|$|Independent Contact Regions allow {{a robust}} finger {{placement}} on the object, despite of potential errors in finger position. They are computed without considering the kinematics of the end-effector, {{and are usually}} applied to off-line grasp planners. This paper presents an approach to obtain Reachable Independent Contact Regions by including the hand kinematics in the <b>computational</b> <b>loop.</b> The regions are computed in a short time, which allows real-time applications in virtual grasping. Potential applications of the proposed approach include regrasp planning, and dual-hand manipulation of objects...|$|E
40|$|This paper {{describes}} a control strategy to achieve high ﬁdelity dynamics simulation rendered on admittance controlled robotic facilities. It explores {{the reasons for}} an increasing energy found in the virtual dynamics of a free-ﬂoating satellite rendered on a six degree of freedom robot, which can lead the system to become unstable and proposes a method to cope with it. The proposed method identifies the sources of intrinsic instability provoked by time delays that {{are found in the}} <b>computational</b> <b>loop</b> of the rendered dynamics and counteracts their destabilizing effects using the passivity criteria. The performance of the system and the benefits of the method are shown in simulations and are verified experimentally...|$|E
40|$|The JPL energy {{consumption}} computer program {{developed as a}} useful tool in the on-going building modification studies in the DSN energy conservation project is described. The program simulates building heating and cooling loads and computes thermal and electric {{energy consumption}} and cost. The accuracy of computations are not sacrificed, however, since the results lie within + or - 10 percent margin compared to those read from energy meters. The program is carefully structured to reduce both user's time and running cost by asking minimum information from the user and reducing many internal time-consuming <b>computational</b> <b>loops.</b> Many unique features were added to handle two-level electronics control rooms not found in any other program...|$|R
40|$|INTRODUCTION Every {{recursive}} algorithm has a maximum sample frequency {{determined by the}} <b>computational</b> <b>loops</b> in the algorithm. The algorithm may, however, be modified {{in order to reach}} the true maximal sample frequency. In this paper, we introduce a technique using the distributive and associative properties of the operations to modify the algorithms. 2. MAXIMALLY FAST RECURSIVE ALGORITHMS The minimal sample period for a {{recursive algorithm}} that is described by a fully specified signal-flow graph is: a b c d e z [...] 1 z [...] 1 f Loop 1 Loop 2 T min = max i { T opi N i } where T opi is the total delay due to the arithmetic operations, etc. in the directed loop i. N i is the number of delay elements in the directed loop i. The loops that yield T<F 6. 79...|$|R
40|$|The {{roadblock}} to wide {{acceptance of}} asynchronous methodology is poor CAD support. Current asynchronous design tools require a significant re-education of designers, and their capabilities are far behind synchronous commercial tools. This paper considers the testing methodology {{for a particular}} subclass of asynchronous circuits (Null Convention Logic or NCL) that entirely relies on conventional CAD tools available at today’s market. It is shown that for acyclic NCL pipelines a test pattern generation for stuckat faults could be effectively solved through the construction and checking of the synchronous circuit {{with a set of}} faults “equivalent ” to the original NCL circuit. This result is extended to arbitrary NCL structures by applying the partial scan technique to break <b>computational</b> <b>loops.</b> The method guarantees 100 % stuck-at fault coverage in NCL systems, which is confirmed by experimental data. 1...|$|R
40|$|Abstract. Human {{has always}} {{been a part of the}} <b>computational</b> <b>loop.</b> The goal of human-centered {{multimedia}} computing is to explicitly address human factors at all levels of multimedia computations. In this chapter, we have incorporated a novel visual analytics framework to design a human-centered multimedia com-puting environment. In the loop of image classifier training, our visual analytics framework can allow users to obtain better understanding of the hypotheses, thus they can further incorporate their personal preferences to make more suitable hypotheses for achieving personalized classifier training. In the loop of image retrieval, our visual analytics framework can also allow users to gain a deep in-sights of large-scale image collections at the first glance, so that they can specify their queries more precisely and obtain the most relevant images quickly. By sup-porting interactive image exploration, users can express their query intentions explicitly and our system can recommend more relevant images adaptively. Key words: hypotheses visualization, similarity-based image visualization and exploration, personalized classifier training, visual analytics. ...|$|E
40|$|Compiler transformations can {{significantly}} improve data locality for many scientific programs. In this paper, we show iterative solvers for partial differential equations (PDEs) {{in three dimensions}} require new compiler optimizations not needed for 2 D codes, since reuse along the third dimension cannot fit in cache for larger problem sizes. Tiling is a program transformation compilers can apply to capture this reuse, but successful application of tiling requires selection of non-conflicting tiles and/or padding array dimensions to eliminate conflicts. We present new algorithms and cost models for selecting tiling shapes and array pads. We explain why tiling is rarely needed for 2 D PDE solvers, but can be helpful for 3 D stencil codes. Experimental results show tiling 3 D codes can reduce miss rates and achieve performance improvements of 17 [...] 121 % for key scientific kernels, including a 27 % average improvement for the key <b>computational</b> <b>loop</b> nest in the SPEC/NAS benchmark MGRID. 1 Introduc [...] ...|$|E
40|$|Multilevel {{parallel}} {{versions of}} CFL 3 D {{have been created}} and their performance has been evaluated for a complete aircraft problem on NAS platforms. CFL 3 D is a three-dimensional multi-block Navier-Stokes flow solver for structured grids and the official version of the code employs MPI to execute calculations in parallel over the discrete geometrical regions. A second version of the code employing the MLP library was created to compare its performance {{to that of the}} MPI library. The major modification to CFL 3 D, involving the aggressive exploitation of parallelism in the main <b>computational</b> <b>loop,</b> was added to both versions of the code using the OpenMP library. Elimination of the output routine and reassembly of the output after execution increased the effectiveness of all code improvements. On the SGI 3000 architecture, the overall improvements have produced a speedup of 8. 5 in the most timeconsuming iterations and implementation on the SGI Altix has further increased this speedup by a factor of 5. 5. The MLP library demonstrates a performance somewhat superior to the MPI library because of its superior load balancing capabilities. 1...|$|E
40|$|The {{development}} of the new generation elsA software package was initiated in 1997 at ONERA. The aim is to group a very broad range of CFD capabilities together in an inter-operable and evolving software package, designed both {{as a tool to}} tackle industrial problems and as a plat-form for further innovative CFD developments. To meet this challenge, the project team has chosen an Object Ori-ented (OO) design method as well as an OO implementa-tion. The paper describes the main features of the OO de-sign, discuss some performance issues associated with dif-ferent codings of the kernel <b>computational</b> <b>loops,</b> presents an overview of elsA transition modeling capability, and ends with several application results dealing with external and internal aerodynamics (missiles, re-entry vehicles, he-licopters, turbomachinery). The broadness of the exam-ples given here, together with good CPU efficiency, clearly demonstrates the benefits of a full OO approach. ...|$|R
40|$|The work {{consists}} of an optimization study of {{the design of a}} complete chemical plant. The process chosen is the manufacture of acetic anhydride by thermal cracking of acetone. There are involved fourteen design variables, two major recycles and six iterative, <b>computational</b> <b>loops.</b> The process includes the most important unit operations of chemical engineering. Emphasis is placed in two areas: developing computer procedures which perform the design of individual items of plant in considerable detail and in producing an optimization program for the integrated plant. An improved version of the Pattern Search method is presented, known as MOSP, and it is shown to be competitive with the best Direct Search techniques available. A new approach is offered for achieving global rather than local optima. The results show clearly the feasibility of optimization in process design and give quantitative informations, for the chosen example, of the optimum conditions...|$|R
40|$|Efficiently {{exploiting}} GPUs {{is increasingly}} essential in scientific computing, as many current and upcoming supercomputers are built using them. To facilitate this, {{there are a}} number of programming approaches, such as CUDA, OpenACC and OpenMP 4, supporting different programming languages (mainly C/C++ and Fortran). There are also several compiler suites (clang, nvcc, PGI, XL) each supporting different combinations of languages. In this study, we take a detailed look at some of the currently available options, and carry out a comprehensive analysis and comparison using <b>computational</b> <b>loops</b> and applications from the domain of unstructured mesh computations. Beyond runtimes and performance metrics (GB/s), we explore factors that influence performance such as register counts, occupancy, usage of different memory types, instruction counts, and algorithmic differences. Results of this work show how clang’s CUDA compiler frequently outperform NVIDIA’s nvcc, performance issues with directive-based approaches on complex kernels, and OpenMP 4 support maturing in clang and XL; currently around 10 % slower than CUDA...|$|R
40|$|Abstract—Due to the {{inherent}} {{characteristics of the}} visualization process, {{most of the problems}} in this field have strong ties with human cognition and perception. This makes the human brain and sensory system the only appropriate platform for evaluating and fine-tuning a new visualization method or paradigm. However, getting humans to volunteer for these purposes has always been a significant obstacle, and thus this phase of the development process has traditionally formed a bottleneck, slowing down progress in visualization research. We propose {{to take advantage of the}} newly emerging field of Human Computation (HC) to overcome these challenges. HC promotes the idea that rather than considering humans as users of the computational system, they can be made part of a hybrid <b>computational</b> <b>loop</b> consisting of traditional computation resources and the human brain and sensory system. This approach is particularly successful in cases where part of the computational problem is considered intractable using known computer algorithms but is trivial to common sense human knowledge. In this paper, we focus on HC from the perspective of solving visualization problems and also outline a framework by which humans can be easily seduced to volunteer their HC resources. We introduce a purpose-driven game entitled “Disguise ” which serves as a prototypical example for how the evaluation of visualization algorithms can be mapped into a fun and addictive activity, allowing this task to be accomplished in an extensive yet cost effective way. Finally, we sketch out a framework that transcends from the pure evaluation of existing visualization methods to the design of new ones. Index Terms — Human Computation, perception, evaluation, color blending...|$|E
40|$|Volumetric {{data sets}} have become common in {{medicine}} and many sciences through technologies such as computed x-ray tomography (CT), magnetic resonance (MR), positron emission tomography (PET), confocal microscopy and 3 D ultrasound. When presented with 2 D images humans immediately and unconsciously begin a visual analysis of the scene. The viewer surveys the scene identifying significant landmarks and building an internal mental model of presented information. The identification of features is strongly influenced by the viewers expectations based upon their expert knowledge of what the image should contain. While not a conscious activity, the viewer makes a series of choices about how to interpret the scene. These choices occur in parallel with viewing the scene and effectively {{change the way the}} viewer sees the image. It is this interaction of viewing and choice which is the basis of many familiar visual illusions. This is especially important in the interpretation of medical images where it is the expert knowledge of the radiologist which interprets the image. For 3 D data sets this interaction of view and choice is frustrated because choices must precede the visualization of the data set. It is not possible to visualize the data set with out making some initial choices which determine how the volume of data is presented to the eye. These choices include, view point orientation, region identification, color and opacity assignments. Further compounding the problem is the fact that these visualization choices are defined in terms of computer graphics as opposed to language of the experts knowledge. The long term goal of this project is to develop an environment where the user can interact with volumetric data sets using tools which promote the utilization of expert knowledge by incorporating visualization and choice into a tight <b>computational</b> <b>loop.</b> The tools will support activities involving the segmentation of structures, construction of surface meshes and local filtering of the data set. To conform to this environment tools should have several key attributes. First, they should be only rely on computations over a local neighborhood of the probe position. Second, they should operate iteratively over time converging towards a limit behavior. Third, they should adapt to user input modifying they operational parameters with time...|$|E
40|$|Protein {{flexibility}} {{is critical to}} the molecular recognition process between a protein and a ligand. Flexible loop regions have been shown to {{play a crucial role in}} many biological functions such as protein-ligand recognition, enzymatic catalysis, and proteinprotein association. In the context of structure-based drug design, using or predicting an incorrect loop configuration can be detrimental to the study if the loop is capable of interacting with the ligand. To date, most <b>computational</b> <b>loop</b> prediction methods only focus on individual loop regions. However, loop regions are often in close proximity spatially to one another and their mutual interactions stabilize their configuration. We have developed a new method, titled CorLps, that predicts such interacting loop regions. First an ensemble of individual loop conformations is generated for each loop region. The members of the individual ensembles are then combined and are accepted or rejected based on a steric clash filter. After a subsequent side chain optimization step, the resulting interacting loop configurations are initially ranked by the statistical scoring function DFIRE. Our results show that predicting interacting loops with CorLps is superior to sequential prediction of the two interacting loop regions. ^ We applied CorLps to three protein systems, each with at least one flexible loop region capable of interacting with bound ligands; a six residue loop region from phosphoribosylglycinamide formyltransferase (GART), two nine residue loop regions from CYP 119, and an eleven residue loop region from enolase were selected for loop prediction. To optimize the accuracy of loop prediction, different scoring functions were used to re-rank the predicted loops. In general, single snapshot MM/GBSA scoring provided the best ranking of native-like loop configurations. Based on the scoring function analyses presented, the optimal ranking of native-like loop configurations is still a difficult challenge and the choice of the “best” scoring function appears to be system dependent. ^ Protein flexibility can also influence the success of site of metabolism (SOM) prediction studies. We have combined molecular dynamics, AutoDock Vina docking, the neighboring atom type (NAT) reactivity model, and a solvent-accessible surface-area term to form a reactivity-accessibility model capable of predicting SOM for cytochrome P 450 2 C 9 substrates. To investigate the importance of protein flexibility during the ligand binding process, the results of SOM prediction using a static protein structure for docking were compared to SOM prediction using multiple protein structures in ensemble docking. Our results indicate that ensemble docking increases the number of ligands that can be docked in a bioactive conformation but only leads to a slight improvement in predicting an experimentally known SOM in the top- 1 position for a ligand library of 75 CYP 2 C 9 substrates. However, further classifying the substrate library according to Km values leads to an improvement in SOM prediction for substrates with low Km values. Although the current predictive power of the reactivity-accessibility model still leaves significant room for improvement, the results illustrate the usefulness of this method to identify key protein-ligand interactions and guide structural modifications of the ligand to increase its metabolic stability. ...|$|E
5000|$|The goal of loop {{unwinding}} is {{to increase}} a program's speed by reducing or eliminating instructions that control the loop, such as pointer arithmetic and [...] "end of loop" [...] tests on each iteration; reducing branch penalties; as well as hiding latencies including the delay in reading data from memory. To eliminate this <b>computational</b> overhead, <b>loops</b> can be re-written as a repeated sequence of similar independent statements.|$|R
40|$|A {{predictive}} analysis of combustion product composition is provided. Discrete reaction model of sooting combustion is proposed {{on the grounds}} of multi-stage representation of oxidation chemistry. By the restrictions on the number of product species involved, and the assumption of fast reaction rates the set of algebraic equations is deduced for the mass fraction of product species. The solutions obtained are shown to be linked to the elemental mass composition with associated limiting requirements. An advanced conserved-scalar approach has then emerged, for which, it is shown that the product mass fractions, including sooting intermediates, can be obtained as a conditional functions of the global mixture fraction. The application of the methodology is illustrated using the example of an industrial carbon black furnace. It is demonstrated that the predictions are in fair agreement to the measured data, and show the correct trends without making any adjustments to the soot modeling concept. The algebraic nature of the model relationships makes it easy bringing them into the <b>computational</b> <b>loops</b> of available predictive tools, so that it is believed the present model has the potential to supplant or complement the similar methods in the engineering computational analysis of combustion...|$|R
40|$|Abstract-A {{huge amount}} of on-line {{information}} {{is available on the}} web, and is still growing. While search engines were developed to deal with this huge volume of documents, even they output a large number of documents for a given user's query. Under these circumstances it became very difficult for the user to find the document he actually needs, because most of the users are reluctant to make the cumbersome effort of going through each of these documents. Therefore systems that can automatically summarize one or more documents are becoming increasingly desirable. A summary can be loosely defined as a text that is produced from one or more texts. Automatic summarization is to use automatic mechanism to produce a finer version for the original document. This paper presents the results of an experimental study of K-means document clustering techniques. We have implemented the query dependant single document summarization by using clustering approach. We have implemented k-means algorithm for only. txt file. The performance of the algorithm is analyzed on different evolution factors like execution time, number of words in summary, number of <b>computational</b> <b>loops</b> etc. Keywords- K-means Clustering, Summarization, weighted Document graph,Clustered graph...|$|R
40|$|The ventral visual pathway {{implements}} {{object recognition}} and categorization in {{a hierarchy of}} processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy {{raises the possibility that}} normal visual processing relies on the use of <b>computational</b> <b>loops.</b> It is not known, however, whether object recognition can be performed at all without such loops (i. e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msec [...] -and often not consciously perceived [...] -can fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward "sweep. " Therefore, feedback loops {{do not appear to be}} "mandatory" for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep. &...|$|R
40|$|The {{performance}} of High Performance Computing (HPC) applications highly {{depends on the}} memory subsystem due to the huge data sets used that do not fit into the cache hierarchy. Besides, energy efficiency has become a main design factor and, consequently, both performance and energy efficiency are primary goals in HPC designs. As a result, energy efficient high performance memory subsystems designs should be explored. In this paper, we extend the architecture of general-purpose processors by adding a software managed local memory (LM) and a very simple programmable DMA controller (PDC). We demonstrate that with these extensions –together with an efficient runtime management – we improve performance and energy consumption factors. We perform an LM design space exploration study for a Intel R ○ Pentium R ○ 4 platform: we analyze performance, energy and energy-delay product (EDP) {{for a total of}} 27 <b>computational</b> <b>loops</b> of the NAS benchmarks. We show a 1. 2 x performance speedup factor and an energy reduction of 6. 21 % on average when using a constrained 32 KB LM with commodity memory bandwidths (6. 4 GB/s). More aggressive configurations (i. e., 256 KB LM + 12. 8 GB/s) show at least 2. 14 x performance speedup factors and energy savings of 42. 07 % on average...|$|R
40|$|Software-based thread-level parallelization {{has been}} widely studied for {{exploiting}} data parallelism in purely <b>computational</b> <b>loops</b> to improve program performance on multiprocessors. However, none of the previous efforts deal with efficient parallelization of hybrid loops, i. e., loops that contain a mix of computation and I/O operations. In this paper, we propose a set of techniques for efficiently parallelizing hybrid loops. Our techniques apply DOALL parallelism to hybrid loops by breaking the cross-iteration dependences caused by I/O operations. We also support speculative execution of I/O operations to enable speculative parallelization of hybrid loops. Helper threading is used to reduce the I/O bus contention caused by the improved parallelism. We provide an easy-to-use programming model for exploiting parallelism in loops with I/O operations. Parallelizing hybrid loops using our model requires few modifications to the code. We have developed a prototype implementation of our programming model. We have evaluated our implementation on a 24 -core machine using eight applications, including a widely-used genomic sequence assembler and a multi-player game server, and others from PARSEC and SPEC CPU 2000 benchmark suites. The hybrid loops in these applications take 23 %– 99 % of the total execution time on our 24 -core machine. The parallelized applications achieve speedups of 3. 0 x– 12. 8 x with hybrid loop parallelization over the sequential {{versions of the same}} applications. Compared to the versions of applications where only computation loops are parallelized, hybrid loop parallelization improves the application performance by 68 % on average...|$|R
40|$|Fine-grain {{parallelism}} {{offered by}} VLIW and superscalar processors can be effectively exploited in <b>computational</b> intensive <b>loops.</b> In this paper {{we present a}} new Software Pipelining technique and show how an efficient VLIW code can be automatically generated using a hamiltonian recurrence in the dependence graph. The dependence graph, extended with a scheduling recurrence, describes {{the characteristics of the}} schedule: the number of functional units required and the efficiency achieved (in terms of parallelism) can be known prior to generating the physical scheduling. In this paper we consider single nested loops without conditionals and multifunctional processing units with a unit latency; extensions to multi-cycle and specific processing units are straightforward...|$|R
40|$|Cuttings {{transportation}} in micro-borehole annulus in coiled tubing drilling for mineral exploration was studied. The effect of cuttings size and mud properties {{as well as}} hole inclination was simulated physically using a flow <b>loop.</b> <b>Computational</b> fluid dynamics was applied to simulate lab experiments and do sensitivity analysis of various parameters. The results show significant differences in cuttings transport response in mineral exploration comparing to {{the oil and gas}} drilling applications...|$|R
40|$|Modern {{processors}} and compilers hide long memory latencies through non-blocking loads or explicit software prefetching instructions. Unfortunately, each mechanism has potential drawbacks. Non-blocking loads can significantly increase register pressure by extending the lifetimes of loads. Software prefetching increases {{the number of}} memory instructions in the loop body. For a loop whose execution time is bound {{by the number of}} loads/stores that can be issued per cycle, software prefetching exacerbates this problem and increases the number of idle <b>computational</b> cycles in <b>loops...</b>|$|R
40|$|In this {{document}} {{we describe the}} parallel package within DYNARE (called the ``Parallel DYNARE'' hereafter). The parallel methodology has been developed taking into account two different perspectives: the ``User perspective'' and the ``Developers perspective''. The fundamental requirement of the ``User perspective'' is to allow DYNARE users to use the parallel routines easily, quickly and appropriately. Under the ``Developers perspective'', on the other hand, {{we need to build}} a core of parallelizing routines that are sufficiently abstract and modular to allow DYNARE software developers to use them easily as a sort of `parallel paradigm', for application to any DYNARE routine or portion of code containing <b>computational</b> intensive <b>loops</b> suitable for parallelization. The Parallel DYNARE comes with the official DYNARE installation package, so the preprocessor part required to interpret the cluster definition is built-in the standard DYNARE installation. JRC. G. 3 -Econometrics and applied statistic...|$|R
40|$|We {{consider}} the load-balancing problems which arise from parallel scienti c codes containing multiple <b>computational</b> phases, or <b>loops</b> over subsets of the data, which {{are separated by}} global synchronisation points. We motivate, derive and describe the implementation of an approach which we {{refer to as the}} multiphase mesh partitioning strategy to address such issues. The technique is tested on example meshes containing multiple computational phases and it is demonstrated that our method can achieve high quality partitions where a standard mesh partitioning approach fails...|$|R
40|$|Abstract — Reconfigurable systems combine {{flexibility}} with programmability and {{capitalize on}} the strengths of hardware and software. In the Chameleon project we developed the Montium: a course grain reconfiguration processing tile with high performance and low power consumption as its characteristic. Low power consumption is mainly achieved by exploiting locality of reference and high performance is obtained by exploiting parallelism. This paper presents a resource allocation method to allocate variables and arrays to storage places and to schedule data movements. The presented algorithm allocates memories as evenly as possible and stores arrays and variables local to the ALUs which need them. Data movements are scheduled such that as few as possible clock cycles are needed. The resource allocation method exploits locality of reference of the Montium architecture as well as parallelism. The focus {{of this paper is}} resource allocation for <b>computational</b> intensive <b>loops.</b> I...|$|R
40|$|Unlike the {{secondary}} structure elements that connect in protein structures, loop fragments in protein chains are often highly mobile even in generally stable proteins. The structural variability of loops is {{often at the}} center of a protein’s stability, folding, and even biological function. Loops are found to mediate important biological processes, such as signaling, protein-ligand binding, and protein-protein interactions. Modeling conformations of a loop under physiological conditions remains an open problem in computational biology. This article reviews <b>computational</b> research in <b>loop</b> modeling, highlighting progress and challenges. Important insight is obtained on potential directions for future research...|$|R
40|$|This paper {{presents}} a parametric approach to an integrated and performance-oriented design, from the conceptual design phase towards materialization. The novelty {{occurs in the}} use of parametric models as a way of integrating multidisciplinary design constraints, from daylight optimization to the additive manufacturing process. The work focuses on the case of a customized sun-shading system that tailors daylighting effects for a fully glazed façade of the alleged PULSE building. The overall workflow includes preliminary analysis on simplified models and an initial parametric model to run <b>computational</b> optimization <b>loops.</b> The output consists of individually unique sun-shading panels, optimized for varying daylighting requirements based on programmatic distribution and specified viewing areas. The resulting geometric complexity was resolved through subsequent detailed parametric models; implementing the structural design requirements and integrating the constraints dictated by the additive manufacturing process, including the necessity to minimize material and 3 D-printing time. This paper focuses on a particular part of the overall workflow, describing the support provided by parametric modelling to control geometric complexity and multi-disciplinary requirements. Design Informatic...|$|R
40|$|Abstract—From the {{perspective}} of physical system feedback control, the cyber or computer system’s role has been to sample and compute control inputs sufficiently fast to maintain accept-able reference command tracking and disturbance rejection in the physical system. This strategy has been successful given the relatively low computational overhead for most control laws compared to computational resource availability. However, in many emerging applications this requirement may be insufficient, not because the computer is incapable of high-speed compu-tations but instead because either more complex computations are required or because processor or network speed must be minimized to conserve energy. We propose the augmentation of traditional physical state models with a computational model to enable a cyber-physical system to co-regulate physical and computational actuation. Ultimately, {{our goal is to}} balance resources of the cyber system with quality of control of the physical system to provide a more energy-conscious CPS. As a first step, we propose a continuous-time representation of computational state and derive a continuous “dynamics ” model approximation. Next, we propose the addition of a computational state into the closed-loop control law for the physical system states. Finally, we augment the derived cyber model with a second-order oscillator and demonstrate control via a LQR controller. In our simulation results, <b>computational</b> state and <b>loop</b> execution rate and oscillator “force ” are regulated closed-loop at each control cycle based both on physical and computational state reference commands and errors. Results show that both physical and cyber state can be successfully regulated with the expected degradation in tracking performance as reference <b>computational</b> state (control <b>loop</b> rate) is slowed to values near the stability threshold...|$|R
40|$|This article {{introduces}} {{an important}} concept: Transparency {{by way of}} Humanistic Intelligence as a human right, and in particular, Big/Little Data and Sur/Sous Veillance, where “Little Data” is to sousveillance (undersight) as “Big Data” is to surveillance (oversight). Veillance (Sur- and Sous-veillance) is a core concept not just in human–human interaction (e. g. people watching other people) but {{also in terms of}} Human–Computer Interaction. In this sense, veillance is the core of Human-in-the-loop Intelligence (Humanistic Intelligence rather than Artificial Intelligence), leading us to the concept of “Sousveillant Systems” which are forms of Human–Computer Interaction in which internal computational states are made visible to end users, allowing users (but not requiring them) to “jump” into the <b>computational</b> feedback <b>loop</b> whenever or wherever they want. An important special case of Sousveillant Systems is that of scientific exploration: not only is (big/little) data considered, but also due consideration must be given to how data is captured, understood, explored, and discovered, and in particular, to the use of scientific instruments to collect data and to make important new discoveries, and learn about the world. Science is a domain where bottom-up transparency is of the utmost importance, and scientists have the right and responsibility to be able to understand the instruments that they use to make their discoveries. Such instruments must be sousveillant systems...|$|R
40|$|We {{consider}} the load-balancing problems which arise from parallel scientific codes containing multiple <b>computational</b> phases, or <b>loops</b> over subsets of the data, which {{are separated by}} global synchronisation points. We motivate, derive and describe the implementation of an approach which we {{refer to as the}} multiphase mesh partitioning strategy to address such issues. The technique is tested on several examples of meshes, both real and artificial, containing multiple computational phases and it is demonstrated that our method can achieve high quality partitions where a standard mesh partitioning approach fails. Keywords: graph-partitioning, mesh-partitioning, load-balancing, parallel multiphysics. 1 Introduction The need for mesh partitioning arises naturally in many finite element (FE) and finite volume (FV) computational mechanics (CM) applications. Meshes composed of elements such as triangles or tetrahedra are often better suited than regularly structured grids for representin [...] ...|$|R
40|$|AbstractAccurately {{modeling}} protein loops is {{an important}} step to predict three-dimensional structures as well as to understand functions of many proteins. Because of their high flexibility, modeling the three-dimensional structures of loops is difficult and is usually treated as a “mini protein folding problem” under geometric constraints. In the past decade, there has been remarkable progress in template-free loop structure modeling due to advances of computational methods as well as stably increasing number of known structures available in PDB. This mini review provides an overview on the recent <b>computational</b> approaches for <b>loop</b> structure modeling. In particular, we focus on the approaches of sampling loop conformation space, which is a critical step to obtain high resolution models in template-free methods. We review the potential energy functions for loop modeling, loop buildup mechanisms to satisfy geometric constraints, and loop conformation sampling algorithms. The recent loop modeling results are also summarized...|$|R
40|$|Fine-grain {{parallelism}} {{available in}} VLIW and superscalar processors can be mainly exploited in <b>computational</b> intensive <b>loops.</b> Aggressive scheduling techniques {{are required to}} fully exploit this parallelism. In this paper we present a new Software Pipelining technique based on Graph Traverse Scheduling, a parallelizing technique originally proposed for multiprocessor systems that generates parallel threads automatically using a hamiltonian recurrence in the dependence graph of the loop. Explicit synchronizations required in multiprocessors to guarantee data dependences are now substituted in the VLIW architecture by the correct allocation of loop operations and nop-operations in the lock-step execution. The technique proposed here shows how an efficient VLIW code can be automatically generated using a hamiltonian recurrence in the dependence graph. The NP-hardness of the scheduling problem is restricted here to a problem of smaller size than related techniques. The dependence graph, extended with a scheduling recurrence, describes {{the characteristics of the}} schedule: the number of functional units required and the efficiency achieved, in terms of parallelism, can be known prior to generate the physical scheduling. In this paper we consider single-nested loops without conditionals and multifunctional processing units with a unit latency; extensions for multi-cycle processing units are straightforward. Finally we also show how other Software Pipelining techniques can be interpreted by means of scheduling recurrences in the dependence graph. In this sense our technique encompasses these other techniques since the schedules they obtaine can be also achieved with the technique described here...|$|R
40|$|A {{challenge}} in computational protein folding is to assemble secondary structure elements-helices and strands-into well-packed tertiary structures. Particularly difficult is {{the formation of}} beta-sheets from strands, because they involve large conformational searches {{at the same time}} as precise packing and hydrogen bonding. Here we describe a method, called Geocore- 2, that (1) grows chains one monomer or secondary structure at a time, then (2) disconnects the loops and performs a fast rigid-body docking step to achieve canonical packings, then (3) in the case of intrasheet strand packing, adjusts the side-chain rotamers; and finally (4) reattaches <b>loops.</b> <b>Computational</b> efficiency is enhanced by using a branch-and-bound search in which pruning rules aim to achieve a hydrophobic core and satisfactory hydrogen bonding patterns. We show that the pruning rules reduce computational time by 10 (3) - to 10 (5) -fold, and that this strategy is computationally practical at least for molecules up to about 100 amino acids long...|$|R
40|$|Aircraft {{dynamics}} at high {{angles of}} attack due to loss {{of stability and}} control essentially limits its manoeuvrability. Modern control systems implement flight envelope protection {{at the cost of}} maneuverability to improve safety in these conditions. Flight envelope boundaries, which are set taking into account deterioration of stability and controllability due to separated flow, can be expanded by appropriate design of control laws. However, such a design requires extensive analysis of the maneuver envelope of the airframe and its utilization by the flight envelope protection laws. The reliability of this analysis depends on the adequate aerodynamic modeling which captures nonlinear unsteady variation of aerodynamic loads in these flight regimes. Two novel models for unsteady aerodynamics at low and high subsonic Mach numbers are described. These models and prototyping control laws are used for closed <b>loop</b> <b>computational</b> analysis. The computational methodology of clearing flight control laws for flight envelope expansion of a Generic Tailless Aircraft (GTA) is addresse...|$|R
