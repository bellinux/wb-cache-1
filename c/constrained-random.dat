25|0|Public
40|$|This thesis {{analyses}} {{messages that}} come from NYSE Arca and ISE exchanges and provides a description of design of stock exchange updates generator which is capable of generating <b>constrained-random</b> messages. It {{can be used for}} testing software that handles messages from an electronic stock exchange. Techniques of coverage-driven verification and <b>constrained-random</b> stimulus generation are discussed. Message generation is based on XML template and because of that the generator can be adjusted for various exchanges...|$|E
40|$|ASIC {{complexity}} {{is increasing}} {{so rapidly that}} designer productivity is not coping with the growth. Verification presents about 60 - 70 % of the total design effort and only advances in verification methodology can improve the time to market considerably. Directed tests and ‘golden ’ reference files will soon become the primitive tools of the modern test environment. Verification engineers are consequently looking towards new methodologies like <b>Constrained-Random</b> approach to reduce test bench development time, and speed-up {{the time it takes}} to achieve complete verification of their ASIC or SoC. Test bench automation tools for <b>constrained-random</b> stimulus generation and functional coverage create tests for corner cases that even engineers who designed the system may not anticipate and hence find bugs early in the development cycle. This thesis describes the study and implementation of the <b>Constrained-Random</b> concept in the Functional verification of a 32 -bit ALU core using Specman...|$|E
40|$|The {{miniaturization}} of transistors {{in recent}} technology nodes requires tremendous back-end tuning and optimizations, making bug fixing at later design stages more expensive. Therefore, {{it is imperative}} to find design bugs as early as possible. The first defense against bugs is block-level testing performed by designers, and <b>constrained-random</b> simulation is the prevalent method. However, this method may miss corner-case scenarios. In this paper we propose an innovative methodology that reuses existing constrainedrandom testbenches for formal bug hunting. To support the methodology, we present several techniques to enhance RTL symbolic simulation, and integrate state-of-the-art word-level and Boolean-level verification techniques into a common framework called BugHunter. From case studies DLX, Alpha and FIR, BugHunter found more bugs than <b>constrained-random</b> simulation using fewer cycles, including four new bugs in the verified design previously unknown to the designer. The results demonstrate that the proposed techniques provide a flexible, scalable and robust solution for bug hunting...|$|E
40|$|This paper {{describes}} a SystemVerilog transaction-based testbench compliant to the Verification Methodology Manual (VMM). It explains by example the VMM methodology {{in the creation}} of a comprehensive <b>constrained-random</b> verification environment using a transaction-based approach. This includes generation of transactions and consumption of them via transactors. The paper also addresses through graphical explanations how VMM macros and classes are used in the makeup of a transaction-based verification testbench. The DUT used for this purpose is a synchronous FIFO model with assertions. The testbench models and results are demonstrated. The complete verification model is available for download...|$|E
40|$|Abstract. Adopting the Verification Methodology Manual’s (VMM) {{hierarchical}} structure, {{this paper}} presents a design of available verification platform based on System Verilog adopted. The platform completed can implement <b>constrained-random</b> test, directed test, and error stimulus test with high efficiency; moreover, gain maximum code reuse. Using Direct Programming Interface (DPI), the verification platform can conveniently link C++ with the model that realized the function of Design Under Test (DUT), and then to test it. At last the paper shows the experiment results to prove the effectiveness and practicality of the platform by verification sample of HOG chip...|$|E
40|$|<b>Constrained-random</b> {{simulation}} is {{the predominant}} ap-proach {{used in the}} industry for functional verification of complex digital designs. The effectiveness of this approach depends on two key factors: the quality of constraints used to generate test vectors, and the randomness of solutions gen-erated from a given set of constraints. In this paper, {{we focus on the}} second problem, and present an algorithm that sig-nificantly improves the state-of-the-art of (almost-) uniform generation of solutions of large Boolean constraints. Our al-gorithm provides strong theoretical guarantees on the unifor-mity of generated solutions and scales to problems involving hundreds of thousands of variables. ...|$|E
40|$|Abstract — Optimizing {{blocks in}} a System-on-Chip (SoC) circuit is {{becoming}} more and more important nowadays due to the use of third-party Intellectual Properties (IPs) and reused design blocks. In this paper, we propose techniques and methodologies that utilize abundant external don’t-cares that exist in an SoC environment for block optimization. Our symbolic codestatement reachability analysis can extract don’t-care conditions from <b>constrained-random</b> testbenches or other design blocks to identify unreachable conditional blocks in the design code. Those blocks can then be removed before logic synthesis is performed to produce smaller and more power-efficient final circuits. Our results show that we can optimize designs under different constraints and provide additional flexibility for SoC design flows. I...|$|E
40|$|Designers {{of complex}} digital systems (ASIC, application-specific/general-purpose {{microprocessors}} (MP), etc.) need verification methods and tools {{to guarantee a}} perfect design before the process of its manufacturing is started. Verification presents about 60 - 70 % of the total design effort and only advances in verification methodology can improve the time to market considerably. Directed tests and 'golden' reference files will soon become the primitive tools of the modern test environment. Verification engineers are consequently looking towards new methodologies like <b>Constrained-Random,</b> Coverage-Driven and Reuse approach to reduce testbench development time, and speed-up {{the time it takes}} to achieve complete verification of their ASIC or SoC. Testbench automation tools for <b>constrained-random</b> stimulus generation and functional coverage create tests for corner cases that even engineers who designed the system may not anticipate and hence find bugs early in the development cycle. There exist several commercial offerings addressing the area of SoC functional verification. In this context we will show Specman Elite and its language e of Cadence Inc, and we compare it to SystemVerilog that represent today one of the most used tool for functional verification. This thesis is structured to provide information on the state of the art in the area of functional verification and we will focus on existing methodologies, tools, and practical approaches based on the development of a verification component for testing some Networks-on-Chip features of a STNoC Network Interface (AST Microelectronics Grenoble approach to NoCs) using Specman. We also present another approach to build a verification environment by using an e Verification Component, just development from Cadence using the eRM, ready to use and configurable to work in any environment in a plug and play manner...|$|E
40|$|As the {{complexity}} of current hardware systems rises, {{it is challenging to}} harden these systems against faults and to complete their verification and manufacturing test. Not only that verification and testing take a considerable amount of time but the number of design errors, faults and manufacturing defects increases with the rising complexity as well. In this paper we performed a detailed analysis of two approaches devoted to generation of input test vectors with respect to detection of stuck-at faults: the first one is based on classical Automatic Test Pattern Generation, the second one on <b>Constrained-random</b> Stimulus Generation. We evaluated their qualities as well as their drawbacks and introduced ideas about their combination {{in order to create a}} new promising approach for testing reliable systems...|$|E
40|$|Constrained {{sampling}} and counting are two fundamental problems in artificial intelligence with a {{diverse range of}} applications, spanning probabilistic reasoning and planning to <b>constrained-random</b> verification. While the theory of these problems was thoroughly investigated in the 1980 s, prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability. Recently, we proposed a novel approach that combines universal hashing and SAT solving and scales to formulas {{with hundreds of thousands}} of variables without giving up correctness guarantees. This paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances. Comment: Appears in proceedings of AAAI- 16 Workshop on Beyond N...|$|E
40|$|The work {{presents}} a configurable network interface (NI) macrocell {{to be integrated}} in Spidergon network-on-chip (NoC) infrastructures, and addresses the problem of its functional verification. The NI architecture supports multiple native bus for the IP cells connected to the NoC and the conversion of data size, protocol and frequency between the NoC and each IF. Differently from many state-of-art NI designs the proposed macrocell features also the hardware implementation of advanced networking features such as security, order handling, error management, store and forward transmission, memory remapping, power management. Such a configurable and complex design poses several challenges in terms of functional verification. Direct HDL testbenches fails covering corner cases and typically are based on handwritten testbenches that are error-prone. In formal methods the verification engineer tries to extract deterministic laws/relationships internal to the HDL description, and then to prove theorems to check the netlist functional behavior. However in complex designs the state explosion problem limits model checking, {{and the cost of}} theorem proving becomes prohibitive because of the amount of skilled manual guidance it requires. To overcome such issues a <b>constrained-random</b> coverage-driven approach is presented and customized to be applied to the novel NI as design under test (DUT). Starting from DUT specifications, a software verification platform is created performing these tasks: generating traffic patterns which are <b>constrained-random,</b> i. e. random within variations ranges specified by the user; monitoring the DUT outputs and checking them according to pre-programmed rules; parsing collected outputs into a functional coverage scheme to check if all possible cases have been stressed and covered by the tests. This enables a coverage-driven verification: the user continues developing and running tests until there are no holes left in the coverage plan. As result of this verification strategy full code and functional coverage is achieved. Implementation results of the verified NI core in 45 nm and 65 nm CMOS technologies are also provided and compared to state-of-art NI designs. (C) 2011 Elsevier B. V. All rights reserved...|$|E
40|$|Abstract — The {{verification}} of modern computing systems {{has grown to}} dominate the cost of system design, often with limited success as designs continue to be released with latent bugs. This trend is accelerated {{with the advent of}} highly integrated systemon-a-chip (SoC) designs, which feature multiple complex subcomponents connected by simultaneously active interfaces. In this paper, we introduce a closed-loop feedback technique targeting the {{verification of}} multiple components connected by parallel interfaces. We utilize an environment with hierarchical Markov models, where top-level submodels specify overarching simulation goals of the system, while lower-level submodels specify the detailed component-level input generation. Test accuracy is improved through the use of depth-driven random test generation. The approach allows users to specify correctness properties and key activity nodes in the design to be exercises. We examine three non-trivial designs, two microprocessors and a chipmultiprocessor router switch, and we demonstrate that our technique finds many more bugs than <b>constrained-random</b> test generation technique and reduces the simulation effort in half, compared to previous Markov-model based solutions. I...|$|E
40|$|With {{the growing}} {{complexity}} of modern digital systems and embedded system designs, {{the task of}} verification has become the key to achieving the faster time-to-market requirement for such designs. This paper describes a graduate level, Verification of Digital Systems using SystemVerilog, offered at Boise State University {{as a part of}} the Master of Science program in Computer Engineering,. This course does not only teach syntax and semantics but also coverage-driven, <b>constrained-random,</b> and assertion-based verification methodologies employing the advanced features of SystemVerilog to ensure that designs meet the required specifications. The course also emphasizes the practical aspects of verification methodologies through providing students with hands-on experience on commercial verification tools such as QuestaSim, the Advanced Functional Verification suite from Mentor Graphics. Course goals are explained along with course content, format, and benefits to students. The course is designed around small practical exercises to illustrate the main concepts, tools, and language usage. A mid-term and a final project are also offered that require an automated verification environment to be designed and tested on given designs...|$|E
40|$|Submitted {{on behalf}} of EDAA ([URL] audienceFunctional {{verification}} of microprocessors {{is one of the}} most complex and expensive tasks in the current system-on-chip design process. A significant bottleneck in the validation of such systems is the lack of a suitable functional coverage metric. This paper presents a functional coverage based test generation technique for pipelined architectures. The proposed methodology makes three important contributions. First, a general graph-theoretic model is developed that can capture the structure and behavior (instruction-set) of a wide variety of pipelined processors. Second, we propose a functional fault model that is used to define the functional coverage for pipelined architectures. Finally, test generation procedures are presented that accept the graph model of the architecture as input and generate test programs to detect all the faults in the functional fault model. Our experimental results on two pipelined processor models demonstrate that the number of test programs generated by our approach to obtain a fault coverage is an order of magnitude less than those generated by traditional random or <b>constrained-random</b> test generation techniques...|$|E
40|$|Abstract. <b>Constrained-random</b> {{verification}} (CRV) {{is widely}} used in in-dustry for validating hardware designs. The e↵ectiveness of CRV depends on the uniformity of test stimuli generated from a given set of constraints. Most existing techniques sacrifice either uniformity or scalability when generating stimuli. While recent work based on random hash functions has shown {{that it is possible}} to generate almost uniform stimuli from constraints with 100, 000 + variables, the performance still falls short of today’s industrial requirements. In this paper, we focus on pushing the performance frontier of uniform stimulus generation further. We present a random hashing-based, easily parallelizable algorithm, UniGen 2, for sampling solutions of propositional constraints. UniGen 2 provides strong and relevant theoretical guarantees in the context of CRV, while also of-fering significantly improved performance compared to existing almost-uniform generators. Experiments on a diverse set of benchmarks show that UniGen 2 achieves an average speedup of about 20 ⇥ over a state-of-the-art sampling algorithm, even when running on a single core. More-over, experiments with multiple cores show that UniGen 2 achieves a near-linear speedup in the number of cores, thereby boosting performance even further. ...|$|E
40|$|Given the {{increasing}} adoption and maturity of SystemVerilog {{as a viable}} HVL (High-level Verification Language), Cadence has released two flavors of the Incisive Plan-to-Closure methodology: 1) SystemVerilog Module-Based Universal Reuse Methodology, and, 2) SystemVerilog Class-Based Reuse Methodology. Both flavors draw heavily from the proven e Reuse Methodology (eRM). According to the documentation, the module-based approach is targeted towards design teams who quickly want to get started {{with the creation of}} block- and chip-level environments. On the other hand, the class-based approach is geared towards verification teams who can take advantage of advanced object-oriented software techniques to implement <b>constrained-random,</b> plan-driven verification environments. From a user perspective, this begs the question of whether to use a class-based approach or a module-based approach. Both design and verification teams are intimately involved in a verification project. So, should one recommend creating two separate environments, one for the design team to do module level testing, and leave the heavy-duty verification effort to the verification team using a class-based approach? Alternatively, does it make sense to do it all in one approach? Even better, should the design team create a module-based environment, which is then reused by the class-based approach? T...|$|E
40|$|Springer-Verlag Berlin Heidelberg 2015. <b>Constrained-random</b> {{verification}} (CRV) {{is widely}} used in industry for validating hardware designs. The effectiveness of CRV depends on the uniformity of test stimuli generated from a given set of constraints. Most existing techniques sacrifice either uniformity or scalability when generating stimuli. While recent work based on random hash functions has shown {{that it is possible}} to generate almost uniform stimuli from constraints with 100, 000 + variables, the performance still falls short of today’s industrial requirements. In this paper, we focus on pushing the performance frontier of uniform stimulus generation further. We present a random hashing-based, easily parallelizable algorithm, UniGen 2, for sampling solutions of propositional constraints. UniGen 2 provides strong and relevant theoretical guarantees in the context of CRV, while also offering significantly improved performance compared to existing almostuniform generators. Experiments on a diverse set of benchmarks show that UniGen 2 achieves an average speedup of about 20 × over a state-ofthe- art sampling algorithm, even when running on a single core. Moreover, experiments with multiple cores show that UniGen 2 achieves a near-linear speedup in the number of cores, thereby boosting performance even further...|$|E
40|$|Abstract — Verification {{remains an}} {{integral}} and crucial phase of today’s microprocessor design and manufacturing process. Unfortunately, with soaring design complexities and decreasing time-to-market windows, today’s verification approaches {{are incapable of}} fully validating a microprocessor before its release to the public. Increasingly, post-silicon validation is deployed to detect complex functional bugs in addition to exposing electrical and manufacturing defects. This {{is due to the}} significantly higher execution performance offered by post-silicon methods, compared to pre-silicon approaches. Validation in the postsilicon domain is predominantly carried out by executing <b>constrained-random</b> test instruction sequences directly on a hardware prototype. However, to identify errors, the state obtained from executing tests directly in hardware must be compared to the one produced by an architectural simulation of the design’s golden model. Therefore, the speed of validation is severely limited by the necessity of a costly simulation step. In this work we address this bottleneck in the traditional flow and present a novel solution for post-silicon validation that exposes its native high performance. Our framework, called Reversi, generates random programs {{in such a way that}} their correct final state is known at generation time, eliminating the need for architectural simulations. Our experiments show that Reversi generates tests exposing more bugs faster, and can speed up post-silicon validation by 20 x compared to traditional flows. I...|$|E
40|$|Increasing {{complexity}} {{coupled with}} time-to-market pressure create a critical {{need to raise}} the abstraction level for System-on-Chip (SoC) designs. Functional validation is widely acknowledged as a major bottleneck {{due to lack of}} automated techniques and limited reuse of validation efforts between abstraction levels. Simulation is the most widely used form of validation using random or <b>constrained-random</b> tests. Directed tests are very promising for simulation since only fewer directed tests are required compared to billions of random tests to achieve a coverage goal. Currently, directed test generation is performed manually which is time-consuming and error-prone. This dissertation presents a novel top-down methodology for automatically generating directed tests from high-level specifications and reuse them across different abstraction levels. The objective is to reduce the overall functional validation effort. My research has four major contributions: i) it proposes a method that can extract formal models from high-level SoC specifications; ii) it presents an approach that can automatically derive properties based on fault models; iii) it proposes efficient clustering, learning and decomposition techniques to reduce the directed test generation time; and iv) it provides validation refinement approaches to enable reuse of the system-level validation efforts for low-level implementation validation as well as to check the consistency between different abstraction layers. Our experimental results using both software and hardware benchmarks demonstrate that the proposed approaches can significantly reduce the overall validation effort...|$|E
40|$|The {{challenge}} of verification of multi-core and multi-processor designs grows dramatically {{with each new}} generation of systems produced today. Validation of memory coherence and memory consistency of the entire system, which includes multiple levels of cache and complex protocols, remains a major fraction of this difficult task. Unfortunately, current tools are incapable of addressing these new challenges, leading to an unacceptably high risk that critical bugs could slip into designs, and make software behave unpredictably or cause wrong computation results. In this work we present a scalable approach to the verification of memory coherence and consistency protocols in large multi-core and multi-processor systems. We accomplish this task with multiple cooperating agents, which feed the cores or processors with stimuli, attempting to both achieve their own verification goals and support other agents on their. The agents can dynamically change the stimuli that they generate based on coverage and pressure observed during the validation. Since each agent has a minimal knowledge of the entire system, their communication and decision process is greatly simplified. Moreover, since the agents’ view {{of the system is}} independent of the number of nodes in it, our approach can be efficiently scaled to target large multi-processor systems. Our experimental results on two common memory coherence protocols demonstrate that this technique can reach 100 % coverage of the individual agents’ verification goals and of the system-level coherence protocol FSM much faster than a <b>constrained-random</b> approach. 1...|$|E
40|$|The goal of {{this thesis}} is to analyze and to find {{solutions}} of optimization problems derived from automation of functional verification of hardware using artificial neural networks. Verification of any integrated circuit (so called Design Under Verification, DUV) using technique called coverage-driven verification and universal verification methodology (UVM) is carried out by sending stimuli inputs into DUV. The verification environment continuously monitors percentual coverage of DUV functionality given by the specification. In current context, coverage stands for measurable property of DUV, like count of verified arithemtic operations or count of executed lines of code. Based on the final coverage, {{it is possible to}} determine whether the coverage of DUV is high enough to declare DUV as verified. Otherwise, the input stimuli set needs to change in order to achieve higher coverage. Current trend is to generate this set by technique called <b>constrained-random</b> stimulus generation. We will practice this technique by using pseudorandom program generator (PNG). In this paper, we propose multiple solutions for following two optimization problems. First problem is ongoing modification of PNG constraints {{in such a way that}} the DUV can be verified by generated stimuli as quickly as possible. Second one is the problem of seeking the smallest set of stimuli such that this set verifies DUV. The qualities of the proposed solutions are verified on 32 -bit application-specific instruction set processors (ASIPs) called Codasip uRISC and Codix Cobalt...|$|E
40|$|Modern Integrated Circuit (IC) {{design is}} {{characterized}} by a strong trend of Intellectual Property (IP) core integration into complex system-on-chip (SOC) architectures. These cores require thorough verification of their functionality to avoid erroneous behavior in the final device. Formal verification methods are capable of detecting any design bug. However, due to state explosion, their use remains limited to small circuits. Alternatively, simulation-based verification can explore hardware descriptions of any size, although the corresponding stimulus generation, as well as functional coverage definition, must be carefully planned to guarantee its efficacy. In general, static input space optimization methodologies have shown better efficiency and results than, for instance, Coverage Directed Verification (CDV) techniques, although they act on different facets of the monitored system and are not exclusive. This work presents a <b>constrained-random</b> simulation-based functional verification methodology where, {{on the basis of the}} Parameter Domains (PD) formalism, irrelevant and invalid test case scenarios are removed from the input space. To this purpose, a tool to automatically generate PD-based stimuli sources was developed. Additionally, we have developed a second tool to generate functional coverage models that fit exactly to the PD-based input space. Both the input stimuli and coverage model enhancements, resulted in a notable testbench efficiency increase, if compared to testbenches with traditional stimulation and coverage scenarios: 22 % simulation time reduction when generating stimuli with our PD-based stimuli sources (still with a conventional coverage model), and 56 % simulation time reduction when combining our stimuli sources with their corresponding, automatically generated, coverage models. Sao Paulo Research Foundation FAPESP, BrazilNational Council of Technological and Scientific Development CNPq, Brazi...|$|E
40|$|Elastic {{constants}} for tantalum single crystals {{have been}} calculated by Orlikowski, et al. [1] {{for a broad}} range of temperatures and pressures. These moduli can be utilized directly in continuum crystal simulations or dislocation dynamics calculations where the individual grains of the polycrystalline material are explicitly represented. For simulations on a larger size scale, the volume of material represented by the quadrature points of the simulation codes includes many grains, and average moduli are needed. Analytic bounding and averaging schemes exist, but since these do not account for nonuniform stress and strain within the interacting grains, the upper and lower bounds tend to diverge as the crystal anisotropy increases. Local deformation and stress equilibrium accommodate the anisotropic response of the individual grains. One method of including grain interactions in shear modulus averaging calculations is through a highly-descretized finite element model of a polycrystal volume. This virtual test sample (VTS) can be probed to determine the average response of the polycrystal. The desire to obtain isotropic moduli imposes attributes on the VTS. The grains should be equiax and the crystal orientation distribution function should be random. For these simulations, a cube, 300 {micro}m on a side, was discretized with 1 million finite elements on a regular rectangular mesh. The mesh was seeded with 1000 grains generated using a <b>constrained-random</b> placement algorithm, Figure 1. Since the orientations were simply painted in the mesh, the grain boundaries are irregular. The orientation distribution function is shown as pole figure in Figure 2. It has the appearance of being random. Analysis of the simulation results will be used to determine if the randomness of the texture and number of grains are adequate...|$|E
40|$|Validation {{of hybrid}} systems is {{challenging}} {{due to the}} combination of both discrete and continuous dynamics. Simulation is {{the most widely used}} form of system validation using a combination of random and <b>constrained-random</b> tests. Directed tests are promising since orders-of-magnitude less number of directed tests can achieve same coverage goal as random simulation. Automated methods for development of directed tests are required {{to keep pace with the}} increasing design complexity. This thesis developed efficient directed test generation methods for hybrid systems. The test generation methods are based on two popular techniques used by current researchers namely, RRT (Rapidly Exploring Random Tree) and model checking. In contrast to current RRT methods that explore the state space in forward direction (from initial region to functional scenario), my method explores the state space in the reverse direction (from functional scenario to initial region). Experimental results demonstrate that reverse RRT technique is upto 33 times (average 10 times) faster than currently used forward RRT methods. I also developed a model checking based test generation method that addressed the tradeoff between accuracy of the result and test generation time for affine-hybrid systems. The reverse reachability based method produces a reasonably accurate testcase (within 18 % error margin) in a fraction of time (15 times faster) normally required for generating a high accuracy testcase. To further reduce the test generation time, this thesis proposed a clustering and learning technique to improve test generation time involving similar functional scenarios. The learning based test generation method provides a significant reduction in test generation time (1. 6 times on average) by exploiting similarity across test generation instances. The proposed approaches can significantly reduce the overall validation of hybrid systems...|$|E

