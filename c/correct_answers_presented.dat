0|7349|Public
40|$|Linear Relational Embedding (LRE) is a {{new method}} of {{learning}} a distributed representation of concepts from data consisting of binary relations between concepts. The final goal of LRE {{is to be able}} to generalize, i. e. to infer new relations among the concepts. The version presented here is capable of handling incomplete information and multiple <b>correct</b> <b>answers.</b> We <b>present</b> results on two simple domains, that show an excellent generalization performance...|$|R
40|$|Indeterminate {{problems}} are {{problems that can}} be written with κ equations with more than κ unknowns and have been used since ancient times from many civilizations. Problem solving constitutes {{a critical part of}} Mathematics Educations, in which emphasis is given on the Curricula of Mathematics. Open-ended problems may have several <b>correct</b> <b>answers</b> or differed ways of finding the <b>correct</b> <b>answer.</b> In the <b>present</b> study the way students of the 5 th grade manage an open-ended problem is examined and also elements of the way they solve it are presented...|$|R
40|$|Situated {{question}} answering {{is the problem}} of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the <b>correct</b> <b>answer.</b> We <b>present</b> Parsing to Probabilistic Programs (P 3), a novel situated {{question answering}} model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines. Comment: EMNLP 2016, 11 page...|$|R
5000|$|Same Sound Name Round (Season 1 and UK) - Each {{contestant}} {{is given}} three placards, each depicting a picture corresponding {{to a particular}} word, with all three words rhyme with each other. Chudd's questions this round will have an answer that will either match or have the same pronunciation as one of these three placards. Contestants are only awarded points for presenting the correct placard: giving the <b>correct</b> <b>answer</b> verbally while <b>presenting</b> the wrong placard does not award points.|$|R
40|$|In {{the past}} years, {{the number of}} service {{requests}} through e-mail has shown an explosive growth. To cope with the increased numbers of received e-mails, current call centres are “transformed ” into so-called contact centres, handling both e-mail and telephone calls. Personal answering (each e-mail is written again by an agent) costs way too much, so most companies use a set of predefined answers that cover most of the e-mail service requests. Incoming e-mails are analysed, where after a set of (maximal) 10 possible <b>answers</b> is <b>presented</b> to the agent. If the selection is successful, writing an answer is replaced by selecting the <b>correct</b> (predefined) <b>answer.</b> Because this selection process works much faster, the number of e-mails that can be handled increases significantly. This is true, {{as long as the}} <b>correct</b> <b>answer</b> is <b>presented</b> within the set of suggested answers. If not, the agent has to find the <b>correct</b> <b>answer</b> by typing in those keywords that lead to the right set of suggestions: a time and money consuming process. Therefore, the focus of this study is on the “improvement of relevant answer suggestions”. We try to tackle the answer suggestion problem by transformin...|$|R
40|$|Suppose we {{know that}} an object is in a sorted table {{and we want to}} {{determine}} the index of that object. To achieve this goal we could perform a binary search. However, suppose it is time-consuming to determine the relative position of that object to any other objects in the table. In this scenario, we might want to resort to an incomplete solution: we could device an algorithm that quickly predicts the result of comparing two objects, and replace the actual comparison with this algorithm during a binary search. The question then is how far away are the results yielded by the imperfect binary search from the <b>correct</b> <b>answers.</b> We <b>present</b> two quick lemmas that answer this question. Comment: 2 page...|$|R
40|$|Abstract. Ontology-based {{semantic}} query answering algorithms {{suffer from}} high computational complexity and become impractical {{in most cases}} that OWL {{is used as a}} framework for data access in the Semantic Web. For this reason, most semantic query answering systems prefer to lose some possible <b>correct</b> <b>answers</b> of user queries rather than being irresponsible. Here, we present a method that follows an alternative direction that we call progressive semantic query answering. The idea is to start giving the most relevant <b>correct</b> <b>answers</b> to the user query as soon as possible and continue by giving additional answers with decreasing relevance until you find all the <b>correct</b> <b>answers.</b> We first <b>present</b> a systematic analysis that formalises the notion of answer relevance and that of query answering sequences that we call strides, providing a formal framework for progressive semantic query answering. Then, we describe a practical algorithm performing sound and complete progressive query answering for the W 3 C’s OWL 2 QL Profile...|$|R
40|$|Abstract — In this {{position}} paper, {{we present a}} powerful and distributed spatio-temporal query processing framework, coined HUB-K. Our framework can be utilized to promptly answer queries of the form: “Report the objects (i. e., trajectories) that follow a similar spatio-temporal motion to Q, where Q is some query trajectory. ” HUB-k, relies on an in-situ data storage model, where spatio-temporal data remains on the smartphone that generated the given data, as well a state-of-the-art top-k query processing algorithms, which exploit distributed trajectory similarity measures {{in order to identify}} the <b>correct</b> <b>answers</b> promptly. We <b>present</b> preliminary design choices, an outline of our preliminary implementation and an outlook to future challenges. I...|$|R
40|$|Suppose you’re on a game show, and you’re {{given the}} choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2 ? ” Is it to your {{advantage}} to switch your choice? The answer is “yes ” but the literature offers many reasons why this is the <b>correct</b> <b>answer.</b> The <b>present</b> paper argues that the most common reasoning found in introductory statistics texts, depending on making a number of “obvious ” or “natural ” assumptions and then computing a conditional probability, is a classical example of solution driven science. The best reason to switch {{is to be found}} in von Neumann’s minimax theorem from game theory, rather than in Bayes’ theorem. ...|$|R
30|$|In this study, we {{implemented}} a distractor generation method introduced by Jiang and Lee (2017) as a baseline because {{their work is}} the latest state-of-the-art method that targets the most similar task to the current study. Although their method generates Chinese fill-in-the-blank vocabulary questions, the method is independent of the language because it takes a corpus-based approach. We can hence adapt the method for English by replacing the corpus. Another difference is the question type to generate, i.e. fill-in-the-blank questions versus closest-in-meaning questions. These questions differ in whether the target word {{is present in the}} options as a <b>correct</b> <b>answer</b> (fill-in-the-blank) or <b>present</b> in the reading passage (closest-in-meaning). There is no difference in the characteristics of the distractors in both types of vocabulary questions.|$|R
40|$|In many systems, sensors {{are used}} to acquire {{information}} from external environments such as temperature, pressure and locations. Due to continuous changes in these values, and limited resources (e. g., network bandwidth and battery power), it is often infeasible for the database to store the exact values at all times. Queries that uses these old values can produce invalid results. In order to manage the uncertainty between the actual sensor value and the database value, we propose a system called U-DBMS. U-DBMS extends the database system with uncertainty management functionalities. In particular, each data value is represented as an interval and a probability distribution function, {{and it can be}} processed with probabilistic query operators to produce imprecise (but <b>correct)</b> <b>answers.</b> This demonstration <b>presents</b> a PostgreSQL-based system that handles uncertainty and probabilistic queries for constantly-evolving data. ...|$|R
40|$|Internal {{boundaries}} and interfaces {{are an important}} part of many fluid and solid modeling problems. Front Tracking contains a general interface framework, closely related to the non-manifold geometry used in CAD solid modeling packages, to support numerical simulation of such fluid problems. It can thus be considered to be a systematic application of the ideas of computational geometry to computational fluid dynamics. It is based on the principle that finite differences behave best when applied to differentiable functions, and that weak derivatives of nondifferentiable functions can be replaced by regularized expressions such as jump conditions. Front Tracking offers superior resolution for fluid problems with important discontinuities and interfaces, and in some cases, it has provided the unique method to obtain <b>correct</b> <b>answers.</b> Here we <b>present</b> Computer Science issues which have contributed to the success of Front Tracking: software design and organization [...] modularity, data struct [...] ...|$|R
40|$|This study {{addresses}} {{the question of}} how pre-service elementary teachers can be induced to change their attitudes to mathematics. Pre-service elementary teachers want to get mathematical answers right. They want to know which formulas to use, and how to get the <b>correct</b> <b>answer.</b> The paper <b>presents</b> transcriptions of student writing that illustrate and support this point. Changing what students value in mathematics is a much harder challenge than teaching them mathematical procedures and applications of formulas. An antidote is needed to a severely procedural orientation to mathematics focused on "correct answers " that prospective teachers have learned to value above all. How can we explicitly emphasize connections, and assist students to construct relationships between parts of mathematics that they see as different? (Contains 21 references.) (Author/MM) Reproductions supplied by EDRS are the best that can be made from the original document. C 71 tr) What mathematical knowledge do pre-service elementary teachers value and remember?...|$|R
40|$|To {{answer the}} {{question}} in machine comprehension (MC) task, the models need to establish {{the interaction between the}} question and the context. To tackle the problem that the single-pass model cannot reflect on and <b>correct</b> its <b>answer,</b> we <b>present</b> Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct an query-aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by a substantial margin, and matches or surpasses the performance of all other published systems. Comment: 10 pages, 6 figure...|$|R
40|$|Context: In {{developing}} countries many chronic conditions including periodontitis {{are on the}} rise. Oral health attitudes and beliefs are important factors affecting oral health behavior. Aims: The aim of this pilot study {{was to assess the}} existing knowledge about periodontal disease and its impact on treatment seeking behavior in a group of population visiting the out-patient Department of Periodontics, Yenepoya Dental College, India. This study also attempted to identify deficit in the knowledge if present. Settings and Design: This is a written questionnaire based pilot study. 143 subjects (89 male and 54 female) {{agreed to participate in the}} study. Simple random sampling was used for recruitment. Subjects and Methods: A written questionnaire consisting of 18 questions was given to the patients. Only one <b>correct</b> <b>answer</b> was <b>present</b> and the score given was + 1. The knowledge of the subjects was reflected by their ability to select a <b>correct</b> <b>answer</b> from the number of distractors (multiple choices, prespecified answers). Statistical Analysis Used: SPSS software version 15. 0 is used for all statistical analysis. The Chi-square test was employed to assess the passive knowledge of the participants in relation to their age. Results: We found a deficit in the knowledge in all the topics investigated. No consistent relationship between age and gender was found. Female respondents had better knowledge about oral hygiene compared to males. Conclusion: We made an attempt to assess the knowledge of periodontitis among the participants of this study. Knowledge deficit was found in the population surveyed. This knowledge deficit could be one of the reasons why patients do not seek periodontal treatment routinely unless there are acute symptoms. There is urgent need to educate the patients about the periodontal disease, the need for the treatment of periodontitis and advanced treatment modalities available...|$|R
50|$|The {{contestant}} {{is asked}} a question with multiple answers, and must give as many <b>correct</b> <b>answers</b> as possible {{over the course of}} the ride (a total of 24 seconds in length). The first <b>correct</b> <b>answer</b> is worth $50; each additional <b>correct</b> <b>answer</b> is worth $50 more than the previous one (for example, giving 5 <b>correct</b> <b>answers</b> wins $750, or $50+$100+$150+$200+$250).|$|R
50|$|After no <b>correct</b> <b>answer</b> {{was given}} by viewers, the <b>correct</b> <b>answer</b> was {{revealed}} to be 292.|$|R
30|$|Finally, in Phase 3, the {{participants}} were presented with all the questions again {{and were asked to}} provide the factually <b>correct</b> <b>answer</b> to each question, along with their confidence in the truth value of their answer. After {{the participants}} in Phase 3 provided what they believed was the <b>correct</b> <b>answer,</b> they were then shown the factually <b>correct</b> <b>answer</b> along with the words: “The <b>correct</b> <b>answer</b> is: _____.” This final, corrective feedback to all questions was included to ensure that participants left the experiments having been told, unambiguously, the <b>correct</b> <b>answers</b> to all questions.|$|R
3000|$|Too {{similar to}} the <b>correct</b> <b>answer</b> or the target word. Comments {{explicitly}} state that a distractor is too {{similar to the}} <b>correct</b> <b>answer,</b> e.g. “the distractor ‘overcome’ {{is too close to}} the <b>correct</b> <b>answer</b> or ‘refined’ is too similar to the correct answer” [...]...|$|R
40|$|In this paper, we {{introduce}} new features for question-answering systems. These features {{are inspired by}} the fact that justification of the <b>correct</b> <b>answer</b> (out of many candidate <b>answers)</b> may be <b>present</b> in multiple passages. Our features attempt to combine evidence from multiple passages retrieved for a candidate <b>answer.</b> We <b>present</b> results on two data-sets: Jeopardy! and Doctor’s Dilemma. In both data-sets, our features are ranked highest in correlation with gold class (in the training data) and significantly improve the performance of our existing QA system, Watson...|$|R
5000|$|Each {{subsequent}} <b>correct</b> <b>answer</b> {{is worth}} 50,000 yen, up {{to a maximum}} running total of 500,000 yen after 15 total <b>correct</b> <b>answers.</b>|$|R
30|$|The {{questionnaire}} had 14 items {{related to}} knowledge of prostate cancer. The level of knowledge was categorised as low (0 – 4), moderate (5 – 9) or high (10 +), based on {{the total number of}} <b>correct</b> <b>answers.</b> A score of zero to four <b>correct</b> <b>answers</b> was regarded as a low level of knowledge, five to nine <b>correct</b> <b>answers</b> as a moderate level of knowledge, and ten or more <b>correct</b> <b>answers</b> as a high level of knowledge.|$|R
30|$|This {{histogram}} represents learners’ response latency {{in accordance}} with their duration of correct responses and their ZPD level. The blue bars manifest learners’ response latency of <b>correct</b> <b>answer</b> in association with the offered mediations, while orange bars depict learners’ ZPD level. Learners with larger ZPD demonstrate to reach the <b>correct</b> <b>answer</b> with shorter response latency. For example, the learners who correctly answered the question with the first attempt (with no mediation) elicited shorter processing information than those who required mediations in arriving at the <b>correct</b> <b>answer.</b> On the other hand, learners who were not able to tackle the <b>correct</b> <b>answer,</b> despite the presentation of all mediations, demonstrated the longest response latency which was the result of longer information processing. As it is apparent in Fig.  2, the hierarchal presentation of mediation via C-DA caused fluctuation in learners’ response latency. Learners who tackled the <b>correct</b> <b>answer</b> through the first mediation, the most implicit one, indicated shorter latency in response for <b>correct</b> <b>answer</b> than those who reached the <b>correct</b> <b>answer</b> with the second and third mediations. With the gradual increase of mediations’ explicitness for failed learners, the learners’ response latency for <b>correct</b> <b>answer</b> gradually increased. Successful learners with the use of fifth mediation, the most explicit one, had the longest response latency of the <b>correct</b> <b>answer</b> while failures (out of ZPD) at the last attempt had the longest latency in responses.|$|R
30|$|Their {{negative}} {{comments on the}} <b>correct</b> <b>answers</b> include that the <b>correct</b> <b>answer</b> is “too obvious thus makes the question too easy”, “could not find which one is the correct answer”, “it needs improvement”, and so on. On the positive side, they mentioned that the <b>correct</b> <b>answer</b> is “appropriate”, “advanced”, “well-made”, and the like.|$|R
5000|$|<b>Correct</b> <b>answers</b> can be {{filtered}} from false positives {{by relying}} on the <b>correct</b> <b>answer</b> to appear more times in the documents than instances of incorrect ones.|$|R
30|$|It was not {{possible}} to compute γ correlations for the condition in which participants originally generated the <b>correct</b> <b>answer</b> and then were provided with the <b>correct</b> <b>answer,</b> because —not surprisingly—participants nearly always got these <b>answers</b> <b>correct.</b>|$|R
30|$|The {{second factor}} is the {{semantic}} similarity between the <b>correct</b> <b>answer</b> and distractors. Distractors in a multiple-choice question act as a lure to distract the test takers from finding the <b>correct</b> <b>answer.</b> The vocabulary question {{used in the present}} study asks for the word with closest meaning to that of the target word; thus, a distracting distractor would be one with a close but different meaning from the <b>correct</b> <b>answer.</b> Hence, the similarity between the <b>correct</b> <b>answer</b> and distractors is considered to be a factor affecting item difficulty.|$|R
50|$|This was {{repeated}} for three mouths and noses, with the partners alternating. The second <b>correct</b> <b>answer</b> added another $50, {{and the third}} <b>correct</b> <b>answer</b> earned another $250 {{for a total of}} $500.|$|R
50|$|All {{four players}} now competed on a toss-up basis with {{the player who}} gave the last <b>correct</b> <b>answer</b> {{selecting}} the half-questions. If a player buzzes in with a <b>correct</b> <b>answer,</b> that player would then {{have a chance at}} a “Double Play” by selecting another half-question to answer on their own. Each <b>correct</b> <b>answer</b> was worth $50 in Round Two and $100 in Round Three.|$|R
50|$|From 2005 to 2006, {{there was}} one Joker box. Its value {{depended}} on correct answers: if a contestant answered correctly, €10,000 was added into the Joker box. In this case, if all 22 contestants gave the <b>correct</b> <b>answers,</b> the Joker box was worth €220,000. On some occasions, each <b>correct</b> <b>answer</b> was worth €30,000, {{that could result in}} a €660,000 box. On June 2, 2006, a <b>correct</b> <b>answer</b> was worth €50,000, which could boost the value of the Joker box to €1,100,000. But, in this show there were only 4 <b>correct</b> <b>answers,</b> so the Joker box, at the time, was worth €200,000.|$|R
50|$|Each <b>correct</b> <b>answer</b> {{is worth}} 10 points. If a student buzzes early with a <b>correct</b> <b>answer,</b> the host will {{acknowledge}} the answer and then summarize {{the reading of}} the question for the viewing audience.|$|R
30|$|Although the {{proposed}} method removes synonyms {{of the target}} word and <b>correct</b> <b>answer</b> from the distractor candidates, the expert-based evaluation showed that it still produces problematic distractors that are too similar to the <b>correct</b> <b>answer.</b> The distractors of this category can make a generated question invalid because {{it appears to have}} multiple <b>correct</b> <b>answers.</b> The questions generated by {{the proposed}} method still need human validation before using for a real test.|$|R
30|$|Table 10 {{indicates}} a tendency that the MI <b>correct</b> <b>answers</b> {{appear in the}} good-rated question items {{more than in the}} bad-rated items, while it {{indicates a}}n opposite tendency for MD <b>correct</b> <b>answers.</b> Note that the MI options are favourable for the <b>correct</b> <b>answers.</b> This means that the result of the ICRP analysis based on the test taker responses (evaluation 1) is consistent with the judgement by the human experts (evaluation 2).|$|R
50|$|The {{final round}} {{took the form}} of a 3-minute quick-fire quiz, where the owners had to answer animal-related trivia questions. For every <b>correct</b> <b>answer,</b> a {{cardboard}} representation of their animal moved along a race track; five <b>correct</b> <b>answers</b> were required to reach the end. The first player {{to the end of the}} track (or the one with the most <b>correct</b> <b>answers</b> after 3 minutes) was declared the winner.|$|R
50|$|This game {{introduced}} by the 2016-2017, sees the two finalists contestants that {{have to answer to}} five open <b>answer</b> questions. Every <b>correct</b> <b>answer</b> is worth a point {{and at the end of}} the five ordinary questions will win who's done more <b>correct</b> <b>answers.</b> In case of a tie, the game will be continued until one of the two players will do the <b>correct</b> <b>answer</b> and the other misses.|$|R
50|$|The {{contestant}} {{in control}} chose one answer at a time; each <b>correct</b> <b>answer</b> awarded money, while finding a Wipeout reset the score to zero and ended his/her turn. After each <b>correct</b> <b>answer,</b> he/she could either choose again or pass {{control to the}} next contestant. The first <b>correct</b> <b>answer</b> of the round was worth $25, {{and the value of}} each subsequent answer increased by $25, with the last one worth $275.|$|R
