0|86|Public
3000|$|..., where M is {{the larger}} matrix {{dimension}} [32]. As a <b>comparison,</b> <b>again</b> assuming all users have N receive antennas, the regularized BD method of [15] performs an SVD of a (K 0 - 1)N × M [...]...|$|R
3000|$|For a fair <b>comparison,</b> once <b>again,</b> {{the total}} {{transmit}} {{power is the}} same for the two protocols, i.e., ε [...]...|$|R
50|$|The enigmatic Cambrian and Ordovician animals Heliomedusa, Marocella and Conchopeltis warrant <b>comparison,</b> {{although}} <b>again</b> large {{differences exist}} between these taxa.|$|R
40|$|Abstract. We {{study the}} problem of {{navigating}} through a database of similar objects using comparisons under heterogeneous demand, a prob-lem closely related to small-world network design. We show that, under heterogeneous demand, the small-world network design problem is NP-hard. Given the above negative result, we propose a novel mechanism for small-world network design and provide an upper bound on its perfor-mance under heterogeneous demand. The above mechanism has a natural equivalent {{in the context of}} content search through <b>comparisons,</b> <b>again</b> under heterogeneous demand; we use this to establish both upper and lower bounds on content search through comparisons. ...|$|R
5000|$|By contrast, Monte Carlo {{simulations}} {{sample from}} a probability distribution for each variable to produce {{hundreds or thousands}} of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of a spreadsheet cost construction model run using traditional “what if” scenarios, and then running the <b>comparison</b> <b>again</b> with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the “what if” analysis. This is because the “what if” analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called [...] "rare events".|$|R
50|$|Every {{possible}} {{bit combination}} {{is either a}} NaN or a number with a unique value in the affinely extended real number system with its associated order, except for the two bit combinations negative zero and positive zero, which sometimes require special attention (see below). The binary representation has the special property that, excluding NaNs, any two numbers can be compared as sign and magnitude integers (endianness issues apply). When comparing as 2's-complement integers: If the sign bits differ, the negative number precedes the positive number, so 2's complement gives the correct result (except that negative zero and positive zero should be considered equal). If both values are positive, the 2's complement <b>comparison</b> <b>again</b> gives the correct result. Otherwise (two negative numbers), the correct FP ordering {{is the opposite of}} the 2's complement ordering.|$|R
30|$|Despite millennia {{of human}} soil use (agriculture and forestry) and {{perceived}} abuse (mining etc.), most of Europe’s soils {{are on the}} continental scale more strongly influenced by natural forces (primarily geology/lithology) than by human impact. Even in large conurbations (e.g., Berlin), baseline values can be encountered in the city centre and in spatial proximity to highly contaminated sites. In a country like Brazil, where significant human impact started only about 500 [*]years ago, challenges appear more demanding, due to the very old age of most soils and a radical depletion of nutrients. Yet, relatively recent site-specific to local contamination appears almost negligible in <b>comparison,</b> <b>again</b> corroborating the dominant role of natural forces and processes (geology, climate). Both regions, representing significant northern and southern hemispheric conditions, show for most chemical elements a very wide natural concentration range.|$|R
40|$|We {{study the}} problem of {{navigating}} through a database of similar objects using comparisons under heterogeneous demand, a problem closely related to small-world network design. We show that, under heterogeneous demand, the small-world network design problem is NPhard. Given the above negative result, we propose a novel mechanism for small-world network design and provide an upper bound on its performance underheterogeneous demand. Theabovemechanism hasanatural equivalent {{in the context of}} content search through <b>comparisons,</b> <b>again</b> under heterogeneous demand; we use this to establish both upper and lower bounds on content search through comparisons. These bounds are intuitively appealing, as they depend on the entropy of the demand as well as its doubling constant, a quantity capturing the topology of the set of target objects. Finally, we propose an adaptive learning algorithm for content search that meets the performance guarantees achieved by the above mechanisms...|$|R
40|$|Abstract: The {{conventional}} wisdom that larger buyers have more “countervailing power ” (i. e., receive lower prices from suppliers) than small buyers has motivated a growing theoretical literature. We test the theories using data on wholesale prices for antibiotics sold to large and small drugstores in the United States during the 1990 s. We find that large drugstores obtain small price discounts under intermediate levels of supplier competition but no price discounts from monopoly suppliers. These findings support the subset of theories that identify supplier competition as {{a necessary condition for}} buyers to have countervailing power. To further investigate the importance of supplier competition, we compare prices paid by drugstores relative to hospitals and HMOs, which, unlike drugstores, can induce supplier competition through the institution of restrictive formularies. We find larger discounts in these <b>comparisons,</b> <b>again</b> consistent with theories stressing the importance of supplier competition...|$|R
30|$|In {{the table}} of Fig.  13, the slotframe lengths are given {{together}} with the number and ratio of reception slots at the sink v 0. For Orchestra SBD, every neighbor of v 0 has only one slot, so v 0 has a long phase of inactivity. For the traffic-aware schedules, the inner nodes get more slots because of their higher traffic load. Thus, the sink can receive more packets per slotframe resulting in a higher throughput. The missing slot in the last row {{is due to the}} shared slot. In the multi-channel schedule, the sink can even potentially receive data traffic in every slot, apart from the first one. This <b>comparison</b> <b>again</b> explains the difference in throughput for the different network sizes, because comparative to the network with 19 nodes, the RX ratio is significantly lower for N= 37 and the Orchestra SBD and single-channel schedules, while it is even larger for the multi-channel schedule where the shared slot has a lower impact due to the longer slotframe length.|$|R
40|$|The thesis {{discusses}} a novel off-line and {{on-line learning}} approach for Fully Recurrent Neural Networks (FRNNs). The most popular algorithm for training FRNNs, the Real Time Recurrent Learning (RTRL) algorithm, employs the gradient descent technique for finding the optimum weight vectors in the recurrent neural network. Within {{the framework of}} the research presented, a new off-line and on-line variation of RTRL is presented, that is based on the Gauss-Newton method. The method itself is an approximate Newton’s method tailored to the specific optimization problem, (non-linear least squares), which aims to speed up the process of FRNN training. The new approach stands as a robust and effective compromise between the original gradient-based RTRL (low computational complexity, slow convergence) and Newton-based variants of RTRL (high computational complexity, fast convergence). By gathering information over time in order to form Gauss-Newton search vectors, the new learning algorithm, GN-RTRL, is capable of converging faster to a better quality solution than the original algorithm. Experimental results reflect these qualities of GN-RTRL, as well as the fact that GN-RTRL may have in practice lower computational cost in <b>comparison,</b> <b>again,</b> to the original RTRL. ii ACKNOWLEDGMENT...|$|R
40|$|We {{present the}} results of {{high-level}} ab initio calculations on the electron affinity of B- 2. Our new best estimate of 1. 93 +/- 0. 03 eV is in agreement with previous calculations {{as well as the}} sole existing experimental estimate of 1. 8 eV, as derived from quantities with an uncertainty of 0. 4 eV. The electron affinity of atomic boron, which is much smaller, is also calculated for <b>comparison</b> and <b>again</b> found to be in good agreement with experiment...|$|R
30|$|In {{order to}} feed the BMM running in NEURON with {{continuous}} or discrete input, inter-process communication between the SIM and the BMM was implemented with an existing first-in, first-out (FIFO) queue facility in Python. This queue provides easy-to-use operations such as putting/getting items into/from the queue and locking. For online mode simulations, the queue in SIM supports two cases: either the simulation is slower or faster than the rate of input of spiking data. The SIM assumes that input data is coming in real time. Therefore, in online mode, to recognize if the simulation is faster or slower than real-time, the SIM compares the timestamp in NEURON with the most recent input. If the difference is greater than a constant value, latency requirement (LR), which is fixed by user, the simulation is slow, so the SIM discards an item in the queue and executes the <b>comparison</b> <b>again.</b> Otherwise, the simulation is not slow and the item is processed. Discarding spikes allows the simulation {{to catch up with}} the data rate as less spikes are processed. We evaluate the consequences of this in the 'Results' section. For offline mode simulations, LR is set to a value bigger than simulation time (e.g., 1, 000 s).|$|R
30|$|In this work, we have {{dissipated}} this {{complexity and}} have a new <b>comparison</b> result that <b>again</b> gives the null solution {{a central role in}} the comparison fractional order differential system. This result creates many paths for continuing research by direct application and generalization [15, 19, 20, 22].|$|R
30|$|Therefore, in this work, we have {{dispersed}} this intricacy {{and a new}} <b>comparison</b> {{result that}} <b>again</b> gives the null solution {{a central role in}} the comparison fractional order differential system. The direct application and generalization of this result in qualitative method have created many paths for continuing research in this direction.|$|R
40|$|This master’s {{thesis is}} aimed at process of enzymatic {{hydrolysis}} of lignocellulosic material – waste paper {{as a source of}} raw material for production of liquid biofuels. In the theoretical part of this work are summarized previously used methods of hydrolysis and lignocellulosic materials used for the process of hydrolysis as a source of fermentable sugars for fermentation technology. The different types of waste paper are evaluated from the composition and usability with consideration to the papermaking process in order to select the appropriate type of waste paper for the enzymatic hydrolysis process. In the next part of this work are suggested technological premises and procedures for the preparation of raw materials and the subsequent enzymatic hydrolysis of these pre–treated materials. In the experimental part were optimized parameters of enzymatic hydrolysis using the Novozymes company enzyme package. Enzymatic degradation of cellulose to reducing sugars was observed using Somogyi – Nelson method. For the verification of hydrolysis conditions were used model materials with high cellulose content – pulp and filter paper. Conditions, which seems to be the best after testing on the model materials, were verified on specific waste paper materials – offset cardboard, recycled paper, matte MYsol paper and for <b>comparison</b> <b>again</b> on model materials – pulp and filter paper. The highest yields was achieved with the use of cardboard, which was further tested using various combinations of pretreatment to material for purpose of increase the yields of hydrolysis...|$|R
50|$|In 1776, Banks sent Masson abroad again, {{this time}} to Madeira, Canary Islands, the Azores and the Antilles. Whilst in Grenada, Masson was {{captured}} and imprisoned by the French, a traumatic experience which haunted him {{for the rest of}} his life. Although he was eventually released, his collections deteriorated during the delay in securing a passage home, and a hurricane in St. Lucia destroyed almost all of what little had survived. Returning to Kew, Masson found the gardening life tedious by <b>comparison,</b> and <b>again</b> turned to Banks for another opportunity to collect abroad. However, the war with France had made such ventures increasingly difficult.|$|R
50|$|The {{processing}} rule check {{gives no}} indication about the functionality {{or even the}} structure of the produced good or device. In the area of semiconductor device fabrication, the techniques of semiconductor process simulation / TCAD can provide an idea about the produced structures. To support this ’virtual fabrication’, a PDES is able to manage simulation models for process steps. Usually the simulation results are seen as standalone data. To rectify this situation PDESs are able to manage the resulting files in combination with the process flow. This enables the engineer to easily compare the expected results with the simulated outcome. The knowledge gained from the <b>comparison</b> can <b>again</b> be used to improve the simulation model.|$|R
40|$|This {{report was}} {{prepared}} {{in the context}} of the LPCC "Electroweak Precision Measurements at the LHC WG" and summarizes the activity of a subgroup dedicated to the systematic comparison of public Monte Carlo codes, which describe the Drell-Yan processes at hadron colliders, in particular at the CERN Large Hadron Collider (LHC). This work represents an important step towards the definition of an accurate simulation framework necessary for very high-precision measurements of electroweak (EW) observables such as the $W$ boson mass and the weak mixing angle. All the codes considered in this report share at least next-to-leading-order (NLO) accuracy in the prediction of the total cross sections in an expansion either in the strong or in the EW coupling constant. The NLO fixed-order predictions have been scrutinized at the technical level, using exactly the same inputs, setup and perturbative accuracy, in order to quantify the level of agreement of different implementations of the same calculation. A dedicated <b>comparison,</b> <b>again</b> at the technical level, of three codes that reach next-to-next-to-leading-order (NNLO) accuracy in quantum chromodynamics (QCD) for the total cross section has also been performed. These fixed-order results are a well-defined reference that allows a classification of the impact of higher-order sets of radiative corrections. Several examples of higher-order effects due to the strong or the EW interaction are discussed in this common framework. Also the combination of QCD and EW corrections is discussed, together with the ambiguities that affect the final result, due to the choice of a specific combination recipe. Comment: 92 pages, report of the working group on precision calculations for Drell-Yan processe...|$|R
5000|$|Misnamed {{after the}} 1970 film Zabriskie Point, {{the band was}} formed by school friends Matthew Durbridge (vocals, guitar), Iwan Morgan (electronica/keyboards), and Gareth Richardson (guitar) in Carmarthen. They {{relocated}} to Cardiff and took on further members, Rhun Lenny (bass) and Owain Jones (drums), both of the band Topper. Their debut album, Screen Memories was released on their own Microgram label, {{and led to a}} deal with Ankst. In March 2001, the band's Ankst debut, Yeti was released, described by the South Wales Echo as [...] "a collection of 10 tracks just bristling with ambition", and the band also recorded a session for John Peel's BBC Radio 1 show. Yeti drew comparisons with such varied artists as New Order, Aphex Twin, and Flaming Lips. The same year, they supported Gorky's Zygotic Mynci on their UK tour, and they also toured several times with Super Furry Animals. They were nominated for three Welsh Music Awards in 2001. The band's third album, Koala Ko-ordination was issued in 2002 now with Kris Jenkins on keyboards, with <b>comparisons</b> <b>again</b> to Flaming Lips, and also to Super Furry Animals and Gorky's Zygotic Mynci. The album was described as [...] "Very strange. Rather marvellous" [...] by the Sunday Times, and [...] "a beguiling album that is both smart and warm" [...] by the Daily Express. The band's last album, Ill Gotten Game was released in 2005, and was described by the BBC as [...] "a many faceted work of wonder", but disappointing sales contributed to the band splitting up in 2007. The band's final performance was at the Gwyl Macs Fest in Carmarthen on 1 September 2007.|$|R
50|$|As {{documents}} {{changed and}} evolved, so did document comparison solutions. The {{second generation of}} documents began utilizing tables to manage a multiplicity of document layouts. Many document comparison solutions had difficulty comparing tables in document versions. These solutions first converted tables to text arrays and then compared the created arrays. In many cases, not enough due diligence on the software’s part was conducted; users would not be informed of sections that were not successfully compared. In the second generation, Microsoft’s Track Changes option was also introduced. With Track Changes, all changes made to documents were captured and stored inside the document. Flaws in the functionality of Track Changes could render the documents unusable and some <b>comparison</b> offerings <b>again</b> had difficulty managing the complex process of comparing in a Track Changes environment.|$|R
40|$|International audienceThis {{report was}} {{prepared}} {{in the context}} of the LPCC Electroweak Precision Measurements at the LHC WG ([URL]) and summarizes the activity of a subgroup dedicated to the systematic comparison of public Monte Carlo codes, which describe the Drell–Yan processes at hadron colliders, in particular at the CERN Large Hadron Collider (LHC). This work represents an important step towards the definition of an accurate simulation framework necessary for very high-precision measurements of electroweak (EW) observables such as the W boson mass and the weak mixing angle. All the codes considered in this report share at least next-to-leading-order (NLO) accuracy in the prediction of the total cross sections in an expansion either in the strong or in the EW coupling constant. The NLO fixed-order predictions have been scrutinized at the technical level, using exactly the same inputs, setup and perturbative accuracy, in order to quantify the level of agreement of different implementations of the same calculation. A dedicated <b>comparison,</b> <b>again</b> at the technical level, of three codes that reach next-to-next-to-leading-order (NNLO) accuracy in quantum chromodynamics (QCD) for the total cross section has also been performed. These fixed-order results are a well-defined reference that allows a classification of the impact of higher-order sets of radiative corrections. Several examples of higher-order effects due to the strong or the EW interaction are discussed in this common framework. Also the combination of QCD and EW corrections is discussed, together with the ambiguities that affect the final result, due to the choice of a specific combination recipe. All the codes considered in this report have been run by the respective authors, and the results presented here constitute a benchmark that should be always checked/reproduced before any high-precision analysis is conducted based on these codes. In order to simplify these benchmarking procedures, the codes used in this report, together with the relevant input files and running instructions, can be found in a repository at [URL]...|$|R
40|$|The paper {{reflects}} the research {{performed by the}} authors, in destined {{to find out the}} proper beam model, in concordance with its geometrical characteristics. This knowledge is needed to establish the adequate relations in the difficult process of damage detection. First, we calculated the natural frequencies for two cases: the Euler-Bernoulli model and the Share model. Afterwards, using FEM simulations we determined these frequencies <b>again.</b> <b>Comparison</b> between analytic results and FEM analysis allow seeing which theoretical models best fit for the various investigated cases...|$|R
50|$|Inside, Jo and Stephanie try to mend their {{strained}} friendship. Initially, Jo apologizes for {{not believing}} {{her when she}} told Amelia about her childhood. Jo continues by saying how they {{have more in common}} than they first realized. Jo overcame a crappy childhood, and Stephanie overcame an illness. Once the comparison was made, Stephanie turns the table and finds fault in the <b>comparison.</b> Stephanie once <b>again</b> becomes angry with Jo, because she thinks that Jo has to “level” the playing field instead of just admitting that Stephanie is better than she is.|$|R
40|$|In {{the first}} experiment, adult male Swiss-Webster mice were systemically {{injected}} {{with a standard}} dose of morphine. Compared {{to the influence of}} vehicle, the motor activity of morphine-injected mice was increased. Neither phenytoin sodium nor carbamazepine alone facilitated motor activity, but pretreatment with both drugs further facilitated the increased motor activity produced by morphine. In a second experiment, mice were injected centrally with a long-acting analog of leu-enkephalin. It also increased motor activity in <b>comparison</b> with vehicle. <b>Again,</b> both phenytoin sodium and carbamazepine further facilitated this response. Both experiments suggest a facilitatory interaction between some aspects of these anticonvulsants and opiate-induced motor activity...|$|R
40|$|In {{this paper}} we extend our {{previous}} work, first presented in, 1 to handle effectively non-Gaussian processes and long-time integration in unsteady simulations of compressible flows. Specifically, we apply the generalized polynomial chaos (GPC) method {{to solve the}} one-dimensional stochastic Euler equations. We present systematic verification studies against an analytical solution of the stochastic piston problem for different correlation lengths of the time-dependent random piston motion, which may follow a Gaussian or a uniform distribution. A new multi-element decomposition of the random space is presented that provides more robustness and resolution capability, and <b>comparisons</b> are made <b>agains...</b>|$|R
40|$|The {{qualitative}} {{behavior of a}} perturbed fractional-order differential equation with Caputo's derivative that differs in initial position and initial time {{with respect to the}} unperturbed fractional-order differential equation with Caputo's derivative has been investigated. We compare the classical notion of stability to the notion of initial time difference stability for fractional-order differential equations in Caputo's sense. We present a <b>comparison</b> result which <b>again</b> gives the null solution {{a central role in the}} comparison fractional-order differential equation when establishing initial time difference stability of the perturbed fractional-order differential equation with respect to the unperturbed fractional-order differential equation...|$|R
40|$|Action {{and energy}} flux-tube {{profiles}} are computed, in SU(2) with beta= 2. 4, 2. 5, for two quarks up to 1 fm apart {{and for which}} the colour fields are in their ground state (A_ 1 g) and the first (E_u) and higher (A'_ 1 g) excited gluonic states. When these profiles are integrated over all space, a scaling comparison is made between the beta= 2. 4 and 2. 5 data. Using sum rules, these integrated forms also permit an estimate {{to be made of}} generalised beta-functions giving b(2. 4) =- 0. 312 (15), b(2. 5) =- 0. 323 (9), f(2. 4) = 0. 65 (1) and f(2. 5) = 0. 68 (1). When the profiles are integrated only over planes transverse to the interquark line and assuming underlying string features, scaling <b>comparisons</b> are <b>again</b> made near the centres of the interquark line for the largest interquark distances. For the A'_{ 1 g} case, some of the profiles exhibit a 'dip-like' structure characteristic of the Isgur-Paton model. Comment: 3 pages, 6 eps figures. Presented at LATTICE 9...|$|R
40|$|Objective: Data on the {{prevalence}} of emerging bacterial pathogens like extended-spectrum-lactamase-building (ESBL) Gram negative organisms, multiresistant Pseudomonas and Acinetobacter species or toxin-building Clostridium difficile in German hospitals are sparse. To provide data for different regions in Germany, a one-day point prevalence study with five tertiary care hospitals and four secondary care hospitals {{was conducted on the}} 10 th of February 2010. Method: For participating hospitals, the level of care (primary/secondary/tertiary), staffing with infection prevention personnel, availability of a MRSA-screening, microbiological support and {{the prevalence}} of five emerging bacterial pathogens in intensive care, surgical and medical wards was assessed by questionnaire. Results: Overall, 3411 patients were included. In tertiary hospitals, the following prevalences were given: MRSA 1. 8 %, ESBL E. coli 0. 45 %, ESBL Klebsiella spp. 0. 41 %, multiresistant Pseudomonas 0. 53 %, multiresistant Acinetobacter species 0. 15 %, VRE 0. 49 % und Clostridium difficile 1. 01 %. In secondary hospitals, as prevalences resulted for MRSA 3. 48 %, ESBL E. coli 0. 4 %, ESBL Klebsiella spp. 0. 4 %, multiresistant Pseudomonas 0 %, multiresistant Acinetobacter species 0 %, VRE 0. 13 % und Clostridium difficile 1. 34 %. Discussion: The prevalence of MRSA found is comparable to other prevalence studies published in the last years, but remarkably higher than reported by the German National Surveillance System (KISS). As no prevalence data for other pathogens as MRSA could be found, only data from the ITS-KISS are available for <b>comparison.</b> <b>Again,</b> the prevalences found in the present study are much higher than reported by the KISS. Whether this is by chance or indicates a systematic underreporting in the KISS remains unclear. Conclusion: The results from this one day point prevalence study show that prevalences of emerging bacterial pathogens differ markedly between regions, departments and hospitals. This can be explained by regional, methodical and other difference associated with the level of care provided by these hospitals. Still, the prevalences found fit well to other prevalence studies from the last years but are remarkably higher than to be expected by the KISS. As questionnaire-based one-day prevalence studies {{have been shown to be}} inexpensive and feasible, such studies, using a fixed day and protocol, should be extendedly used in the future to collect representative data for Germany. By such initiatives, scientific societies as the DGKH can take part in collecting valuable epidemiological data of emerging bacterial pathogens...|$|R
40|$|Bu tez çalışması kapsamında uzman hekimlerin iş tatmini ve örgütsel bağlılık düzeyleri incelenmiş olup; iş tatmininin örgütsel bağlılık üzerindeki etkisinin ölçülmesi hedeflenmiştir. Araştırma Isparta il merkezindeki yedi hastanede çalışan uzman hekimleri kapsamaktadır. Bu hastanelerden biri Üniversite hastanesi, üç tanesi Sağlık Bakanlığı. na bağlı hastane, geri kalan üçü de özel hastanelerdir. Veri toplama işlemi üç bölümden ve toplam 50 sorudan oluşan bir anket aracılığıyla gerçekleştirilmiştir. Anketin ilk kısmı Paul Spector. ın İş Tatmin Ölçeği, ikinci kısım Mowday, Steers ve Porter. ın Örgütsel Bağlılık Anketi ve son kısım da demografik sorulardan oluşmaktadır. Toplam 179 uzman hekimin katılımıyla gerçekleştirilen araştırma kapsamında öncelikle iş tatmini boyutları oluşturulmuş ve hekimlerin iş tatmin düzeyleri ölçülmüştür. Daha sonra demografik değişkenlerin iş tatmin boyutları üzerindeki etkileri irdelenmiştir. Uzman hekimlerin örgütsel bağlılık düzeyleri de ölçülmüş ve benzer biçimde demografik değişkenlere göre karşılaştırması yapılmıştır. Son olarak da iş tatmini boyutlarının örgütsel bağlılık üzerindeki etkisi araştırılmıştır. Within {{the context}} of this study we {{examined}} the job satisfaction and organizational commitment levels of speacilist physicians. We also aimed to test the effect of job satisfaction on organizational commitment. The study took place at the 7 hospitals located in the city center of Isparta, Turkey. One of the hospitals {{that took place in the}} study was a university hospital, three of them were Ministry of Health hospitals and the rest three were private hospitals. Data collection was done through a questionnaire which consisted of three parts and a total of 50 questions. The first part of the questionnaire was Paul Spector. s Job Satisfaction Scale, the second part was Mowday, Steers and Porter. s Organizational Commitment Questionnaire and the last part was the demographic questions. 179 specialist physicians participated in this study. As part of the study the facets of job satisfaction were formed and the satisfaction levels of physicians were measured. Then the results were compared according to the demographic variables. The organizational commitment levels of the participants were also measured and <b>comparisons</b> <b>again</b> with the demographic variables were made. At last the effects of job satisfaction facets on organizational commitment were examined. According to the results it was found that both the satisfaction and the commitment levels of physicians were at the medium level. Along with this it was found out that the ownership was an important factor in the forming of satisfaction and commitment of the physicians. Another important finding was that 63...|$|R
40|$|Sea loan or pecunia traiecticia {{belongs to}} the {{heritage}} of Roman legal thought. It seems to occupy in the conceptual framework of private law a specific position which few researchers are interested to investigate. One of them is Z. Benincasa who has analyzed the topic in her general work on risk in the maritime journeys till the II century AD. Present article was inspired by her book, however {{it is also the}} consequence of own studies on sea loan not only in the ancient Roman law but also in medieval, modern and contemporary legal thought. Thanks to broad insight into the history of sea loan it was possible to take up an approach which was only mentioned before. Namely, Roman sea loan seems to operate in the similar way as today's Project Finance. It was reasonable to start the broad <b>comparison</b> <b>again</b> from ancient Roman law. Present work uncovers some important characteristics of pecunia traiecticia. First of all, sea loan served not only as a method of taking over the risk by creditor, but also was a kind of speculative investment and opportunity to gain a great profit from maritime trade. At the same it enabled debtor to organize the risky journey. There were two kinds of sea loan. One was a loan under the condition that debtor will successfully reach the port of destination. The other one was a loan with the same condition, but there was added the time limit to the loan, e. g. 200 days of navigation – so called dies incertus sensu stricto. Secondly, the profit of creditor was strictly attached to the gains from maritime trade and depended on the success of the maritime journey. On the one hand, debtors personal liability was moved as further as possible, in order to satisfy creditor firstly from the things brought from the trade expedition. On the other hand, there was a flexible way to enter into the contract, to attach high interests and finally to sue the debtor and his heirs. Thirdly, caesars were interested in sea loan and provided very balanced position of creditor and debtor. It can suggest that pecunia traiecticia was important for Roman economy, maybe the same as Project Finance for our times. Present work seeks to broaden the previous studies on the western legal tradition and Roman law and is an attempt to find out whether the Roman concept of sea loan is applicable also nowadays...|$|R
30|$|In this paper, we have {{investigated}} that initial time difference boundedness criteria and Lagrange stability for fractional order differential equation in Caputo's sense are unified with Lyapunov-like functions to establish comparison result. The qualitative {{behavior of a}} perturbed fractional order differential equation with Caputo's derivative that differs in initial position and initial time {{with respect to the}} unperturbed fractional order differential equation with Caputo's derivative has been investigated. We present a <b>comparison</b> result that <b>again</b> gives the null solution {{a central role in the}} comparison fractional order differential equation when establishing initial time difference boundedness criteria and Lagrange stability of the perturbed fractional order differential equation with respect to the unperturbed fractional order differential equation in Caputo's sense.|$|R
40|$|Commons Attribution License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. The qualitative {{behavior of a}} perturbed fractional-order differential equation with Caputo’s derivative that differs in initial position and initial time {{with respect to the}} unperturbed fractional-order differential equation with Caputo’s derivative has been investigated. We compare the classical notion of stability to the notion of initial time difference stability for fractional-order differential equations in Caputo’s sense. We present a <b>comparison</b> result which <b>again</b> gives the null solution {{a central role in the}} comparison fractional-order differential equation when establishing initial time difference stability of the perturbed fractional-order differential equation with respect to the unperturbed fractional-order differential equation. 1...|$|R
40|$|The use of microelectrodes for {{the study}} of the {{kinetics}} and mechanism of photoinduced processes is described with reference to two examples. First, the rate of light-induced electron transfer between anthraquinone and the tetraphenylborate anion is quantified using phototransient experiments at microband electrodes and the resulting data are shown to be in good agreement with independent measurements. Second, microdisc experiments are used to characterize the rate of photofragmentation of electrogenerated radical anions of 1 -bromoanthraquinone. <b>Again,</b> <b>comparison</b> with independent measurements shows excellent agreement. Finally the ability of using microelectrodes for quantitative voltammetry in the near absence of supporting electrolyte is exploited to demonstrate that in the photoelectrochemical reduction of 1 -bromoanthraquinone to anthraquinone the source of the required H atoms is the supporting electrolyte. © 1995...|$|R
5000|$|More formally, the {{algorithm}} begins at alignment , so {{the start of}} [...] is aligned with the start of [...] Characters in [...] and [...] are then compared starting at index [...] in [...] and [...] in , moving backward. The strings are matched {{from the end of}} [...] to the start of [...] The comparisons continue until either the beginning of [...] is reached (which means there is a match) or a mismatch occurs upon which the alignment is shifted forward (to the right) according to the maximum value permitted by a number of rules. The <b>comparisons</b> are performed <b>again</b> at the new alignment, and the process repeats until the alignment is shifted past the end of , which means no further matches will be found.|$|R
40|$|Indiana University-Purdue University Indianapolis (IUPUI) Mammographic {{density is}} an {{important}} risk factor for breast cancer, detecting and screening {{at an early stage}} could help save lives. To analyze breast density distribution, a good segmentation algorithm is needed. In this thesis, we compared two popularly used segmentation algorithms, EM-MPM and K-means Clustering. We applied them on twenty cases of synthetic phantom ultrasound tomography (UST), and nine cases of clinical mammogram and UST images. From the synthetic phantom segmentation comparison we found that EM-MPM performs better than K-means Clustering on segmentation accuracy, because the segmentation result fits the ground truth data very well (with superior Tanimoto Coefficient and Parenchyma Percentage). The EM-MPM is able to use a Bayesian prior assumption, which takes advantage of the 3 D structure and finds a better localized segmentation. EM-MPM performs significantly better for the highly dense tissue scattered within low density tissue and for volumes with low contrast between high and low density tissues. For the clinical mammogram, image segmentation <b>comparison</b> shows <b>again</b> that EM-MPM outperforms K-means Clustering since it identifies the dense tissue more clearly and accurately than K-means. The superior EM-MPM results shown in this study presents a promising future application to the density proportion and potential cancer risk evaluation...|$|R
