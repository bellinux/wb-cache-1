398|678|Public
5000|$|... 2014 Deconstructing the Subject Condition {{in terms}} of {{cumulative}} <b>constraint</b> <b>violation.</b> The Linguistic Review 31(1): 73 - 150. Co-authored with Liliane Haegeman (Ghent) and Ángel Jiménez-Fernández (Sevilla.https://idus.us.es/xmlui/bitstream/handle/11441/23439/tlr-2013-0022%281%29.pdf?sequence=1 ...|$|E
5000|$|In {{computer}} science, constrained clustering is a {{class of}} semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should {{be associated with the}} same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of <b>constraint</b> <b>violation</b> should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions.|$|E
50|$|As a {{constraint}} matures in {{the design}} flow, it tends to work its way {{from the end of}} the flow to the beginning.As it does this, it also tends to increase in complexity and in the degree that it contends with other constraints.Constraints tend to move up in the flow due to one of the basic paradoxes of design: accuracy vs.influence. Specifically, the earlier in a design flow a constraint is addressed, the more flexibility there is toaddress the constraint. Ironically, the earlier one is in a design flow, the more difficult it is to predict compliance.For example, an architectural decision to pipeline a logic function can have a far greater impact ontotal chip performance than any amount of postrouting fix-up. At the same time, accurately predicting theperformance impact of such a change before the chip logic is synthesized, let alone placed or routed, is verydifficult. This paradox has shaped the evolution of the design closure flow in several ways. First, it requiresthat the design flow is no longer composed of a linear set of discrete steps. In the early stages of VLSI it wassufficient to break the design into discrete stages, i.e., first do logic synthesis, then do placement, then dorouting. As the number and complexity of design closure constraints has increased, the linear design flowhas broken down. In the past if there were too many timing constraint violations left after routing, it wasnecessary to loop back, modify the tool settings slightly, and reexecute the previous placement steps. If theconstraints were still not met, it was necessary to reach further back in the flow and modify the chip logicand repeat the synthesis and placement steps. This type of looping is both time consuming and unable toguarantee convergence i.e., it is possible to loop back in the flow to correct one <b>constraint</b> <b>violation</b> only tofind that the correction induced another unrelated violation.|$|E
40|$|Abstract. The {{most common}} reason for plan repair are the {{violation}} of a plan’s temporal constraints. Air Traffic Control {{is an example of}} an area in which violations of the plan’s temporal constraints is rather a rule than an exception. In such domains {{there is a need for}} identifying the underlying causes of the <b>constraint</b> <b>violations</b> in order to improve plan repairs and to anticipate future <b>constraint</b> <b>violations.</b> This paper presents a model for identifying the causes of the temporal <b>constraint</b> <b>violations.</b> ...|$|R
40|$|This paper explores a {{mechanism}} underlying cuereadiness in insight problem-solving. Cue-readiness {{is concerned with}} situations where previously neglected information suddenly and unexpectedly becomes illuminative. From the view point of dynamic constraint relaxation theory (Suzuki & Hiraki, 1997), this {{can be explained by}} constraint relaxation caused by noticing failures. The theory predicts that <b>constraint</b> <b>violations</b> increase during the problem-solving process, and that a specific combination of <b>constraint</b> <b>violations</b> takes place which leads people to an insight. In this paper, we examined the time-course differences of frequencies of <b>constraint</b> <b>violations,</b> and of sensitivity to the crucial information using a rating task. Although Experiment 1 di...|$|R
3000|$|In this {{feasible}} situation, the <b>constraint</b> <b>violations</b> of COPs {{with zero}} are equivalent {{to be one}} of the unconstrained optimization problems because <b>constraint</b> <b>violations</b> of every individual are zero. Hence, only objective function is required to be considered, and Eq. (19) can be also used as a criterion to select better individuals because G [...]...|$|R
30|$|R_b^v: average <b>constraint</b> <b>violation.</b>|$|E
40|$|We present {{efficient}} new randomized and deterministic {{methods for}} transforming optimal solutions for {{a type of}} relaxed integer linear program into provably good solutions for the corresponding NP-hard discrete optimization problem. Without any <b>constraint</b> <b>violation,</b> the ε-approximation problem for many problems of this type is itself NP-hard. Our methods provide polynomial-time ε-approximations while attempting to minimize the packing <b>constraint</b> <b>violation.</b> Our method...|$|E
30|$|In {{the end of}} this section, we {{are going}} to {{investigate}} the degree of <b>constraint</b> <b>violation</b> for the proposed method. By simulation, in MATLAB 6.5, 48 samples of all stochastic parameters are generated. Thus, we get 48 optimization problems. Next, we are going to investigate the degree of <b>constraint</b> <b>violation</b> for the proposed method in this paper and the expectation method presented in [9].|$|E
40|$|Dynamic service {{composition}} {{is suitable for}} on-demand business requests. For autonomic computing, service composition needs to deal with runtime environment faults, but also with business <b>constraint</b> <b>violations</b> which result from business requirements. We propose an approach for integrated handling of business <b>constraint</b> <b>violations</b> and runtime environment faults for dynamic service composition. We introduce a loosely coupled implementation architecture to maintain the platform-independent nature...|$|R
3000|$|... are {{the penalty}} {{multipliers}} for penalizing the <b>constraint</b> <b>violations.</b> Based on EDT[17], there exist finite α∗ ≥ 0 and [...]...|$|R
40|$|Repairing <b>violations</b> of {{integrity}} <b>constraints</b> in databases {{can be seen}} as an interleaving diagnostic/repair process. In this paper we introduce a new approach on repairing <b>constraint</b> <b>violations</b> by adopting existing techniques from model-based diagnosis. <b>Violations</b> {{of integrity}} <b>constraints</b> observed in an inconsistent database state are diagnosed and repair actions are deduced from diagnoses. By interleaving diagnosing <b>constraint</b> <b>violations</b> and performing repair actions, transactions are computed which restore the consistency of the database...|$|R
30|$|Through {{comparing}} two infeasible solutions, {{the chosen}} {{is the one}} with the lower sum of <b>constraint</b> <b>violation.</b>|$|E
40|$|Abstract:- This paper {{proposes a}} {{strategy}} for checking timing <b>constraint</b> <b>violation</b> in wireless sensor networks. We present the process for checking timing <b>constraint</b> <b>violation</b> and the techniques for calculation of clock drift, collection of sending and receiving time, and calculation of data transfer time. Through the techniques of this paper, the user can know about data transfer time on each node and total data transfer time from a sensor node to the server. Then, the user can confirm whether the expected data transfer time is satisfied by analyzing the information. If the expected data transfer time is not satisfied in the sensor network, the user can {{find out where the}} delay has occurred. Real-time data processing is a key factor in sensor networks, so the sensing data should be transferred to the server in right time in order to perform the correct action at proper time. The proposed strategy will help the user to maintain correct real-time processing in the sensor network because the user can debug timing <b>constraint</b> <b>violation.</b> Key-Words:- debugging, timing <b>constraint,</b> <b>violation</b> checking, sensor networks...|$|E
30|$|In {{traditional}} filter method, originally {{proposed by}} Fletcher and Leyffer [3], {{the acceptability of}} iterates is determined by comparing the value of <b>constraint</b> <b>violation</b> and the objective function with previous iterates collected in a filter. Define the violation function h(x) by h(x)=c(x)^+_∞, where c_i(x)^+={c_i(x), 0, i∈ I}. Obviously, h(x)= 0 {{if and only if}} x is a feasible point. So a trial point should either reduce the value of <b>constraint</b> <b>violation</b> or the objective function f.|$|E
40|$|AbstractWe {{present a}} class of trust region {{algorithms}} without using a penalty function or a filter for nonlinear inequality constrained optimization and analyze their global and local convergence. In each iteration, the algorithms reduce the value of objective function or the measure of <b>constraints</b> <b>violation</b> according {{to the relationship between}} optimality and feasibility. A sequence of steps focused on improving optimality is referred to as an f-loop, while some restoration phase focuses on improving feasibility and is called an h-loop. In an f-loop, the algorithms compute trial step by solving a classic QP subproblem rather than using composite-step strategy. Global convergence is ensured by requiring the <b>constraints</b> <b>violation</b> of each iteration not to exceed an progressively tighter bound on <b>constraints</b> <b>violation.</b> By using a second order correction strategy based on active set identification technique, Marato’s effect is avoided and fast local convergence is shown. The preliminary numerical results are encouraging...|$|R
40|$|An {{efficient}} {{spatial data}} structure in a GIS system for database updating {{is required in}} order to minimising of spatial <b>constraint</b> <b>violations</b> and timesaving. An automated constraint checking procedure has been introduced to perform <b>constraint</b> <b>violations</b> check at compiling time before updating the database. Formal definitions of spatial data types were used in attempt to formulate novel equations and architectures to detect <b>constraint</b> <b>violations</b> and framework for spatial repository. A data structure called Semantic Spatial Outlier R-Tree (SSR O-Tree) was proposed for the Semantic Integrity Constraints Checking System to improve the functionality of the proposed method. The R-Tree or its variants have been widely-used data structures for this purpose and which are based on a heuristic optimization but unable to perform semantic spatial join queries at database updating. An experiment was conducted using actual spatial data and results revealed that the performance of SSR O-Tree is notably superior to the R*-Tree and R O-Tree for conducting semantic spatial join queries...|$|R
40|$|Robust {{optimization}} is {{a methodology}} {{that can be}} applied to problems that are affected by uncertainty in the problem’s parameters. The classical robust counterpart (RC) of the problem requires the solution to be feasible for all uncertain parameter values in a so-called uncertainty set, and offers no guarantees for parameter values outside this uncertainty set. The globalized robust counterpart (GRC) extends this idea by allowing controlled <b>constraint</b> <b>violations</b> in a larger uncertainty set. The <b>constraint</b> <b>violations</b> are controlled by the distance of the parameter to the original uncertainty set. We derive tractable GRCs that extend the initial GRCs in the literature: our GRC is applicable to nonlinear constraints instead of only linear or conic constraints, and the GRC is more flexible with respect to both the uncertainty set and distance measure function, which are used to control the <b>constraint</b> <b>violations.</b> In addition, we present a GRC approach {{that can be used to}} provide an extended trade-off overview between the objective value and several robustness measures...|$|R
40|$|We present {{efficient}} new randomized and deterministic {{methods for}} transforming optimal solutions for {{a type of}} relaxed integer linear program into provably good solutions for the corresponding NP-hard discrete optimization problem. Without any <b>constraint</b> <b>violation,</b> the ε-approximation problem for many problems of this type is itself NP-hard. Our methods provide polynomial-time-approximations while attempting to minimize the packing <b>constraint</b> <b>violation.</b> Our methods lead to the first known approximation algorithms with provable performance guarantees for the s-median problem, the tree pruning problem, and the generalized assignment problem. These important problems have numerous applications to data compression, vector quantization, memory-based learning, computer graphics, image processing, clustering, regression, network location, scheduling, and communication. We provide evidence via reductions that our approximation algorithms are nearly optimal {{in terms of the}} packing <b>constraint</b> <b>violation.</b> We also discuss some recent applications of our techniques to scheduling problems...|$|E
3000|$|... {{for each}} user. Therefore, {{there will be}} no <b>constraint</b> <b>violation</b> when each user updates the {{transmission}} power locally, while minimizing (15) in a distributed operation.|$|E
40|$|We {{consider}} {{the question of}} global convergence of iterative methods for nonlinear programming problems. Traditionally, penalty functions {{have been used to}} enforce global convergence. In this paper we review a recent alternative, so-called filter methods. Instead of combing the objective and <b>constraint</b> <b>violation</b> into a single function, filter methods view nonlinear optimization as a biobjective optimization problem that minimizes the objective and the <b>constraint</b> <b>violation.</b> We outline the main ideas and convergence results of filter methods and indicate other areas where filter methods have been used successfully...|$|E
30|$|In [1, 5 – 9, 16], {{the authors}} propose {{approaches}} for reliable rule based smart environments. They first consider {{a set of}} errors that can occur within {{a set of rules}} (e.g., conflicts, circularities, <b>constraints</b> <b>violations,</b> redundancy). Then, they propose methods to detect (e.g., pairwise comparison, model checking) and solve (e.g., priority) the considered errors. These approaches do not enable the detection of implicit errors. Indeed, they do not consider the effects of the rules on the environment. Our approach prevents from both implicit conflicts and <b>constraint</b> <b>violations,</b> by considering the devices effects on the environment.|$|R
40|$|The Job-Shop Scheduling Problem (JSSP) {{deals with}} the {{allocation}} of resources over time to factory operations. Allocations are subject to various constraints (e. g., production precedence relationships, factory capacity constraints, and limits on the allowable number of machine setups) which must be satisfied for a schedule to be valid. The identification of <b>constraint</b> <b>violations</b> and the monitoring of constraint threats plays {{a vital role in}} schedule generation in terms of the following: (1) directing the scheduling process; and (2) informing scheduling decisions. This paper describes a general mechanism for identifying <b>constraint</b> <b>violations</b> and monitoring threats to the satisfaction of constraints throughout schedule generation...|$|R
40|$|A service-based {{scientific}} workflow can {{be exposed}} as a composite service {{that consists of}} a set of logically connected sub-services. A critical issue in this area is how to achieve overall optimized end-to-end QoS requirements by effectively coordinating individual QoS constraints of single service. Unfortunately, this issue has not been well addressed. In this paper, we propose a Reverse Order-based approach to gradually remove QoS <b>Constraint</b> <b>violations</b> for building an optimized path to execute a scientific workflow. With our approach, an initial execution path for a scientific workflow is first built by employing the local optimization policy without considering user-defined end-to-end QoS constraints. Based on this path, global QoS computing models can be used to calculate the global QoS values for each quality attribute. Then, QoS <b>constraint</b> <b>violations</b> can be detected by comparing global QoS values with end-to-end user- defined QoS <b>constraints.</b> For each <b>violation,</b> a Reverse Order-based correction algorithm by gradually removing QoS <b>constraint</b> <b>violations</b> is proposed to recursively correct it by reselecting critical service execution instances. As a result, an optimized execution path can be rebuilt to meet overall end-to-end QoS requirements. Comparison and simulation further demonstrate the feasibility and performance of our approach...|$|R
30|$|If the {{constraint}} of (14) is violated, the CMD algorithm {{will reduce}} {{the magnitude of the}} adaptive filter frequency response in proportion to the level of <b>constraint</b> <b>violation.</b>|$|E
30|$|The CMD {{algorithm}} normalizes {{the weight}} update {{in a manner}} similar to the normalized-LMS with leakage. The amount of leakage is dependent on the level of <b>constraint</b> <b>violation.</b>|$|E
3000|$|Selecting one TS {{according}} to the first proposed method in Section  3, implementing the load flow, and studying the network constraints. If no <b>constraint</b> <b>violation</b> exists, go to step 5; [...]...|$|E
40|$|Recent {{approaches}} to integrity enforcement in active databases suggest {{not only to}} check for inconsistencies by triggers but also to utilize triggers to perform repair actions on <b>constraint</b> <b>violations.</b> Typically, respective repairing triggers are derived automatically from constraint specification following almost fixed derivation strategies. However, in order to incorporate more semantic knowledge of the application these approaches often require refinements or even revisions of already derived integrity checking and inconsistency repairing triggers by the designer. In this paper we argue that analysing and specifying repair actions on <b>constraint</b> <b>violations</b> should be also a design task and exclusively be {{carried out in the}} conceptual design. For this purpose, we provide a declarative specification language for repair actions on inconsistencies with an operational semantics, suitable to express most of the designers' intentions on the behavior on <b>constraint</b> <b>violations.</b> We describe a design methodology for reaction specifications and show how integrity checking and inconsistency repairing triggers can be derived from these specifications. (orig.) SIGLEAvailable from TIB Hannover: RR 3280 (1993, 2) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|Techniques are {{developed}} for projecting the solutions of symmetric hyperbolic evolution systems onto the constraint submanifold (the constraint-satisfying {{subset of the}} dynamical field space). These optimal projections map a field configuration to the ``nearest'' configuration in the constraint submanifold, where distances between configurations are measured with the natural metric on the space of dynamical fields. The construction and use of these projections is illustrated for a new representation of the scalar field equation that exhibits both bulk and boundary generated <b>constraint</b> <b>violations.</b> Numerical simulations on a black-hole background show that bulk <b>constraint</b> <b>violations</b> cannot be controlled by constraint-preserving boundary conditions alone, but are effectively controlled by constraint projection. Simulations also show that <b>constraint</b> <b>violations</b> entering through boundaries cannot be controlled by constraint projection alone, but are controlled by constraint-preserving boundary conditions. Numerical solutions to the pathological scalar field system are shown to converge to solutions of a standard representation of the scalar field equation when constraint projection and constraint-preserving boundary conditions are used together. Comment: final version with minor changes; 16 pages, 14 figure...|$|R
40|$|Abstract- The aim of {{this work}} is towards a better {{understanding}} of the effect of using <b>constraint</b> <b>violations</b> in guiding evolutionary search for nonlinear programming problems. Different penalty functions, based on <b>constraint</b> <b>violations,</b> create different search biases. However, this bias may he eliminated when treating the nonlinear programming problem as a multiobjective task. The different search behaviors are illustrated using a new artificial test function. The effectiveness of the multiobjective approach is also compared with the standard penalty function method on a number of commonly used benchmark problems. It is shown that in practice multi-objective methods are not an efficient or effective approach to constrained evolutionary optimization. ...|$|R
3000|$|GSA This {{algorithm}} retains all slack at {{the coordinator}} node. It helps to resolve violations in {{the phase of}} PRP, but it will increase the probability of <b>constraint</b> <b>violation</b> on monitoring nodes.|$|E
40|$|In this paper, chaos {{control is}} {{proposed}} for the output- constrained system with uncertain control gain and time delay and {{is applied to the}} brushless DC motor. Using the dynamic surface technology, the controller overcomes the repetitive differentiation of backstepping and boundedness hypothesis of pre-determined control gain by incorporating radial basis function neural network and adaptive technology. The tangent barrier Lyapunov function is employed for time-delay chaotic system to prevent <b>constraint</b> <b>violation.</b> It is proved that the proposed control approach can guarantee asymptotically stable in the sense of uniformly ultimate boundedness without <b>constraint</b> <b>violation.</b> Finally, the effectiveness of the proposed approach is demonstrated on the brushless DC motor example...|$|E
40|$|Random convex {{programs}} (RCPs) are convex optimization problems {{subject to}} {{a finite number of}} constraints that are extracted at random according to some probability distribution. The optimal objective of an RCP, and its associated optimal solution (when it exists), are random variables: RCP theory is mainly concerned with providing probabilistic assessments on the probability of objective and <b>constraint</b> <b>violation</b> in random convex programs. In a recent contribution, the authors provide a tight upper bound on the <b>constraint</b> <b>violation</b> probability for a restricted class of random convex programs that are assumed to be feasible and attain an optimal solution in every possible problem realization. Here, we remove this restriction and we provide a result holding for general random convex program...|$|E
40|$|Abstract. As [5] and [11] have shown, some {{applications}} of Optimality Theory can be modelled using finite state algebra {{provided that the}} constraints are regular. However, their approaches suffered from an upper bound {{on the number of}} <b>constraint</b> <b>violations.</b> We present a method to construct finite state transducers which can handle an arbitrary number of <b>constraint</b> <b>violations</b> using a variant of the tropical semiring as its weighting structure. In general, any Optimality Theory system whose constraints can be represented by regular relations, can be modelled this way. Unlike [16], who used roughly the same idea, we can show, that this can be achieved by using only the standard (weighted) automaton algebra. ...|$|R
30|$|Multidimensional schemas {{are used}} to model data {{warehouse}} systems, a special type of large databases dedicated for decision support. Several approaches have been proposed in this domain to ensure the satisfaction of decision makers needs and {{the accuracy of the}} generated schemas. This paper presents an approach for the validation of multidimensional star schema assisted by repair solutions. Our approach aims to assist designers by detecting <b>constraint</b> <b>violations</b> and proposing repair solutions based on a number of error-based rules formalized in Prolog. Its efficiency stems from using both linguistic similarity computation and a set of heuristics developed by bottom-up design methods to produce warnings about semantic <b>constraint</b> <b>violations</b> and their impact {{on the quality of the}} analysis results.|$|R
40|$|Object-centered {{constraints}} are a computationally practical type of constraint {{that can}} be defined on any network of data. These constraints can be incrementally maintained by searching for <b>constraint</b> <b>violations</b> about points of change within a database. It may be infeasible to define a constraint on the base data or base relationships of a database, but preferable to define it on a view. When attempting to detect <b>constraint</b> <b>violations,</b> {{some part of the}} view will need to be instantiated. This partial view instantiation can be performed by evaluating the query that defines that view as a search about the point of change in the network. This can significantly reduce the number of pages accesses needed to determine if the constraint is violated...|$|R
