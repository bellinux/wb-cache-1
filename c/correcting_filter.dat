1|142|Public
40|$|The {{operating}} {{characteristics of a}} synchronous electric motor are discussed. A system of phase stabilization of the instantaneous angular velocity of rotation of a synchronous-reaction motor is diagrammed. A mathematical model is developed to show the parameters which affect {{the operation of the}} motor. The selection of a <b>correcting</b> <b>filter</b> to use with the motor {{in order to reduce the}} reaction of the system to interference is explained...|$|E
30|$|Why is {{important}} to do a <b>correct</b> <b>filtering</b> of ionospheric (or atmospheric in general) parameters? We live in the Earth {{and we want to}} understand and predict the atmosphere behavior which is essential for human life. A <b>correct</b> <b>filtering</b> would guarantee a good result from which right conclusions regarding variations of small amplitudes such as long-term trends are extracted.|$|R
5000|$|ISO 11562 Surface texture: Profile method - Metrological {{characteristics}} of phase <b>correct</b> <b>filters</b> ...|$|R
5000|$|Blowtorch goggles: These {{protect the}} eyes from glare and flying sparks and hot metal {{splashes}} while using or near a blowtorch. They {{are not the}} <b>correct</b> <b>filters</b> for arc welding.|$|R
5000|$|After convolving {{with the}} <b>correct</b> matched <b>filter,</b> the {{resulting}} signal, [...] is, ...|$|R
50|$|Damon and Laut {{stated that}} when the graphs are <b>corrected</b> for <b>filtering</b> errors, the sensational {{agreement}} with the recent global warming, which drew worldwide attention, totally disappeared.|$|R
50|$|With premultiplied alpha, the RGB {{components}} {{represent the}} color of the object or pixel, adjusted for its opacity by multiplication. A more obvious advantage of this is that, in certain situations, it can save a subsequent multiplication (e.g. if the image is used many times during later compositing). However, the most significant advantages of using premultiplied alpha are for correctness and simplicity rather than performance: premultiplied alpha allows <b>correct</b> <b>filtering</b> and blending. In addition, premultiplied alpha allows regions of regular alpha blending and regions with additive blending mode to be encoded within the same image.|$|R
40|$|This work {{presents}} an input {{stage for a}} cardiac pacemaker fully integrated in 0. 35 µm CMOS technology. The system can acquire and digitize to 8 bits both atrial and ventricular electrical activity. Log-domain circuits are exploited to amplify and filter the input signal, while Σ ∆ modulation is exploited to convert it. The design is power optimized, indeed the power consumption is limited to 4. 8 µW with a power supply down to 1. 8 V. The total area is 2. 2 mm 2 and experimental data prove <b>correct</b> <b>filtering</b> and a total DR of at least 47 dB. ...|$|R
5000|$|Ability to move, rename, and delete the <b>correct</b> file in <b>filtered</b> lists.|$|R
40|$|Abstract. According to {{statistical}} analysis of the actual situation and the existing data, {{the expression of the}} variable filtration ratio of the filter is given. Based on contamination control balancing equations of the hydraulic system, the variation curves of contamination level changing with time are obtained by Matlab/simulink. Through analysis of the example, the final expression of the variable filtration ratio of the filter is determined by fitting simulation results to the experimental data repeatedly, and the reliability of the expression is verified by compared with traditional fixed filtration ratio. For determining the true filtering performance of the filter and selecting the <b>correct</b> <b>filter,</b> this article has good theoretical guidance...|$|R
5000|$|... the 80A, 80B and 80C {{photographic}} <b>filters</b> <b>correct</b> for excessive redness under tungsten lighting ...|$|R
30|$|The {{recognition}} {{method of}} combining rules and statistics. Some mainstream. Named-entity recognition systems combine {{the rules and}} the statistics. First, they use the statistical methods to the image to recognize the entities, and then, <b>correct</b> and <b>filter</b> them by the rules.|$|R
30|$|There {{are three}} key steps of SRAF: getting {{coefficient}} matrix and desired response vector; performing ill-posed analysis; and solving equation for filtering vector. Convolution kernel {{method is adopted}} to construct coefficient matrix; it makes the product between matrix K and vector F be equivalent to the convolution of Equation (1). Spectral condition is introduced to estimate whether Equation (2) represents an ill-posed system; its value is large means that the equation is ill posed. TV method is implemented to solve Equation (2) for <b>correct</b> <b>filtering</b> vector because the least square method is not feasible to resolve ill-posed equation. With no restriction of solution to be smooth, TV ensures that solving equation process is stable enough to obtain appropriate filtering vector.|$|R
30|$|From the {{previous}} section, finding filtering vector F {{is the key}} of apodization filtering. As {{can be seen from}} Equations (1) and (2), getting F is essentially to solve a first kind integral equation which generally represents ill-posed system, so that the stability of adopted method is as important as its efficiency. Original realization of apodization filtering (ORAF) is introduced in [22]. It has successfully been implemented for random noise radar in order to suppress the range sidelobes by applying projection method to resolve the ill-posed problem. However, its stability is not enough because inappropriate solution may be obtained. Improper filtering can lead to obvious distortion of output response. Therefore, a modified method SRAF is proposed in this article to acquire <b>correct</b> <b>filtering</b> vector with following processing details.|$|R
30|$|SRAF is {{executed}} {{to suppress}} range sidelobes of correlation output and matched filtering output for noise SAR and general SAR, respectively. Indeed, <b>correct</b> <b>filtering</b> vector {{can be obtained}} within several iterative steps. Sidelobe reductions of 12 dB and greater {{can be achieved by}} this method; moreover, mainlobe energy is preserved well, which is contrast to obvious mainlobe broadening after amplitude weighting. With increasing filter length, the performance of sidelobe suppression is improved at the cost of increasing running time, so {{that it is important to}} select a balanced filter length. Noise disturbance is also considered in simulations. As SNR degenerates from infinite to 20 dB, the performance of filtered outputs have no obvious degradation. In the cases of SNR = - 0.5 and - 10 dB, SRAF is still able to depress sidelobes effectively.|$|R
40|$|Four {{performance}} measures {{were used to}} evaluate the fitting characteristics of 18 models of N 95 filtering-facepiece respi-rators: (1) the 5 th percentile simulated workplace protection factor (SWPF) value, (2) the shift average SWPF value, (3) the h-value, and (4) the assignment error. The effect of fit-testing on the level of protection provided by the respirators was also evaluated. The respirators were tested on a panel of 25 subjects with various face sizes. Simulated workplace protec-tion factor values, determined from six total penetration (face-seal leakage plus filter penetration) tests with re-donning be-tween each test, were used to indicate respirator performance. Five fit-tests were used: BitrexTM, saccharin, generated aerosol <b>corrected</b> for <b>filter</b> penetration, PortaCount©R Plus <b>corrected</b> for <b>filter</b> penetration, and the PortaCount Plus with the N 95 -CompanionTM accessory. Without fit-testing, the 5 th percentil...|$|R
40|$|This article {{proposes a}} formal {{framework}} for Multi-Agent Systems {{in the context}} of Information Discovery. Information Discovery is a synthesis of Information Retrieval and Information Filtering. The Information Discovery Paradigm is given. In addition, the different types of agents needed in Information Discovery applications are described in terms of the operations they support and the knowledge and information they use. A <b>correct</b> <b>filtering</b> topology, consisting of sound filter paths, is identified. It is also shown how Information Retrieval and Information Filtering benefit from their synthesis. The research described in this article was conducted for the Profile - Information Filtering Project of the University of Nijmegen, the Netherlands. For more information, see [URL] 1 Introduction The amount of information made available through different media is growing rapidly. In parallel, our need for accurate information increases as well. Theref [...] ...|$|R
40|$|The goal of HLA Data Distribution Management (DDM) {{services}} is {{to limit the}} messages received by federates in large distributed federations to those messages of interest {{in order to reduce}} (1) the data set required to be processed by the receiving federate and (2) the message traffic over the network. If this functionality is provided {{in a manner that is}} efficient, it can significantly improve the performance and scalability of large distributed federations. We provide an overview of the fundamental DDM mechanisms, routing spaces with update and subscription regions, and show how DDM services interoperate with other HLA services to allow federates to discover objects and attributes of interest. We proceed to derive strategies for implementing physically <b>correct</b> <b>filters</b> to account for network latencies and object movement in the virtual environment. These strategies rely on mathematically derived extensions of update and subscription regions to optimize filter efficiency. 1. INTRODUCT [...] ...|$|R
40|$|Many {{real-world}} applications are addressed through a linear least-squares problem formulation, whose solution is calculated {{by means of}} an iterative approach. A huge amount of studies has been carried out in the optimization field to provide the fastest methods for the reconstruction of the solution, involving choices of adaptive parameters and scaling matrices. However, in presence of an ill-conditioned model and real data, the need of a regularized solution instead of the least-squares one changed the point of view in favour of iterative algorithms able to combine a fast execution with a stable behaviour with respect to the restoration error. In this paper we analyze some classical and recent gradient approaches for the linear least-squares problem by looking at their way of filtering the singular values, showing in particular the effects of scaling matrices and non-negative constraints in recovering the <b>correct</b> <b>filters</b> of the solution. An original analysis of the filtering effect for the image deblurring problem with Gaussian noise on the data is also provided...|$|R
40|$|Abstract. We {{present a}} novel image filter {{generation}} method and an image filter retrieval algorithm and analyse their properties. Based on an original image and a filtered {{version of the}} original, the retrieval algorithm can find, to a high probability, which filter {{was applied to the}} original from a large pre-defined list of filters, without having to apply all filters to the original image, which is usually a time consuming task when the number of filters is large. This is achieved by pre-computing image annotations for a set of filtered images obtained by applying the pre-defined filters to a database of 50 images. Using standard imagebased annotation techniques, we show that the filter retrieval can be achieved by taking the closest images to the original from the database and analysing those known images instead. The retrieval algorithm has a set of parameters and we present results of experiments with these values to maximise the probability of retrieving the <b>correct</b> <b>filter.</b> ...|$|R
40|$|An integrated-optic {{dispersion}} compensator {{that uses}} chirped waveguide gratings is designed and fabricated. The 7 -mm-long chirped grating is realized by a recently proposed method of curving a waveguide through a uniform grating. At 800 nm, the fabricated device exhibits a reflection bandwidth {{in excess of}} 0. 5 nm and a nearly quadratic phase response corresponding to a fiber dispersion–length product of 58 ps�nm. The phase response reported is measured interferometrically with a narrow-band, tunable Ti:sapphire laser. Extending this technology to 5 -cm-long gratings should permit dispersion compensation of 50 -km-long fiber lengths at 1. 55 mm. The current trend in optical communication systems is toward higher bit rates and longer distances, where fiber chromatic dispersion becomes a performance-limiting factor. For systems operating at 1. 5 mm on standard telecommunicationsgrade optical fiber, the dispersion is approximately 20 ps��km nm�. Thus a 20 -ps Gaussian pulse will expand to 60 ps after it propagates through only 17 km of fiber. Therefore, <b>correcting</b> <b>filters</b> ar...|$|R
40|$|This paper {{describes}} the architecture, design flow and verification {{process for the}} FPGA implementation of timing recovery circuits for QAM waveforms. We achieve sample timing alignment by phase selection of a polyphase matched filter. The challenge in realizing these circuits in hardware {{is not in the}} construction of the multirate filter architecture, but rather the complex control circuitry that marshals data from the receiver front-end processor (e. g. digital down-converter) into the timing recovery compute engine. This engine must select the <b>correct</b> <b>filter</b> path to align the output sample position with the maximum eyeopening in the face of sample clock phase and frequency offsets and drift. The design and FPGA implementation of this control plane, filter architecture, timing error detector and memory management sub-system is described, along with implementation considerations for Xilinx Virtex- 4 FPGAs. A model-based FPGA design flow called System Generator, based on the Mathworks Simulink visual programming environment, was used for our implementation. The FPGA resource utilization and performance is also reported. 1...|$|R
50|$|The type of filter {{incorporated}} into a PAPR must be appropriate to the contaminants {{that need to be}} removed. Some respirators are designed to remove fine particulate matter such as the dust created during various woodworking processes. When used with high-efficiency particulate air (HEPA) filters, airborne particles containing pathogens (viruses, bacteria) smaller than 5 microns will be removed. (PAPRs are typically required for lab workers in BSL-3 facilities, and sometimes in BSL-2 as well.) When used in combination with the <b>correct</b> <b>filters,</b> PAPRs are suitable for working with volatile organic compounds such as those used in many spray paints. At the same time filters that are suitable for volatile substances must typically have their filter elements replaced more often than a particulate filter. In addition there is some confusion over terminology. Some literature and users will refer to a particulate filtering unit as a dust mask or filter and then use the term respirator to mean a unit that can handle organic solvents.|$|R
3000|$|... of {{the filter}} {{transfer}} function. An approach to <b>correct</b> the <b>filter</b> bias by estimating the noise cross-power density {{was presented in}} [21]. Another issue with speech enhancement solely based on the LMS approach is that the speech signals at the microphone inputs may only be weakly correlated for some frequencies as shown in Section 2. Consequently, these frequency components will be attenuated in the output signals.|$|R
40|$|Filters {{provide an}} aid to visual {{interpretation}} of images. So the design of filters in image processing is important to de-noise the images, to emphasize edges and to transform the <b>correct</b> images <b>Filters</b> depend {{on the type of}} noise present on the images. A new hybrid Filter DWT filters followed by Average filter is introduced to de-noise the speckle noise and investigated the performance on the images...|$|R
40|$|Modern {{operating}} systems run multiple interpreters in the kernel, which enable user-space applications to add new functionality or specialize system policies. The correct-ness of such interpreters {{is critical to}} the overall system security: bugs in interpreters could allow adversaries to compromise user-space applications and even the kernel. Jitk is a new infrastructure for building in-kernel in-terpreters that guarantee functional correctness as they compile user-space policies down to native instructions for execution in the kernel. To demonstrate Jitk, we im-plement two interpreters in the Linux kernel, BPF and INET-DIAG, which are used for network and system call filtering and socket monitoring, respectively. To help ap-plication developers write <b>correct</b> <b>filters,</b> we introduce a high-level rule language, along with a proof that Jitk correctly translates high-level rules all the way to native machine code, and demonstrate that this language can be integrated into OpenSSH with tens of lines of code. We built a prototype of Jitk on top of the CompCert verified compiler and integrated it into the Linux kernel. Ex-perimental results show that Jitk is practical, fast, and trustworthy. ...|$|R
50|$|In a point-sampled mask, the {{coverage}} bit for each multisample is only set if the multisample is located inside the rendered primitive. Samples are never taken from outside a rendered primitive, so images produced using point-sampling will be geometrically <b>correct,</b> but <b>filtering</b> quality may be low because {{the proportion of}} bits set in the pixel’s coverage mask may not be equal to {{the proportion of the}} pixel that is actually covered by the fragment in question.|$|R
5000|$|The {{director}} of photography, DoP or DP, {{is the chief}} of the camera and lighting crew of the film. The DoP makes decisions on lighting and framing of shots in conjunction with the film's director. Typically, the director tells the DoP how they want a shot to look, and the DoP chooses the <b>correct</b> lens, <b>filter,</b> lighting and composition to achieve the desired aesthetic effect. The DoP is the senior creative crew member after the director.|$|R
40|$|A computer‐based {{platform}} {{based on}} signal processing {{has been developed}} using a modular programming approach, to enable the selection and the processing of earthquake ground motion records. Structural engineers during the design phase use different specific applications to perform simple tasks such as <b>correcting,</b> <b>filtering,</b> displaying and comparing ground motion records, searching for seismic records that satisfy certain criteria such as spectral matching. The proposed computer‐based platform combines in unified environment different features such as: (i) selection of ground motion records using both spectral and waveform matching, (ii) signal processing, (iii) response spectra analysis, (iv) soil response analysis etc. The novelty of the platform is {{the implementation of a}} procedure that allows the selection of earthquake records based on spectral matching criteria using different target spectra such as the Conditional Mean Spectra which has been build {{for the first time on}} the Italian national territory. Using the proposed platform a set of ground motion from three different databases has been selected and processed. The results show the advantage of processing multiple records simultaneously and of guiding the user toward the complete definition of the seismic input that will be used in structural analyse...|$|R
30|$|Pulse {{response}} of radar system always suffers from high sidelobe level resulting in resolution degradation. Investigated {{here is a}} sidelobe suppression method based on apodization filtering technique for range responses of synthetic aperture radar (SAR) and noise SAR systems. The core of apodization filtering is finding an appropriate filtering vector in time domain. Compared with original apodization filtering, the proposed method could be realized stably because it could get <b>correct</b> <b>filtering</b> vector efficiently. This method contains three important steps: constructing coefficient matrix and desired response vector; performing ill-posed analysis; and solving equation to find filtering vector. In these steps, convolution kernel method is adopted to construct coefficient matrix; spectral condition is introduced as an indicating function for ill-posed analysis; and total variation method is used to resolve ill-posed equation for getting filtering vector. Elaborate theoretical derivation is presented to demonstrate the feasibility of this method. In order to test its effect, simulation experiments are implemented. Simulation results {{show that there is}} a great suppression of range sidelobes after processed by this method. With increasing filter length, the performance of filtered output is improved but time cost is increasing correspondingly. Furthermore, the proposed method is also effective with noise disturbance.|$|R
40|$|Abstract: Steerable {{filters are}} {{oriented}} filters generally used in vision and image processing tasks, such as texture analysis, edge detection, image data compression, motion analysis, and image enhancement. Steerable filters with oriented property can be examined {{as a function}} of both orientation and phase. In the case of <b>correct</b> <b>filter</b> set and interpolation rules, it is possible to determine the response of a filter of arbitrary orientation without applying that filter directly. In this paper, we have first applied steerable filters to synthetic examples and after satisfactory results are obtained; we have evaluated the tectonic setting of the Gelibolu Peninsula in the western region of Turkey using potential fields. The gravity and magnetic anomaly maps of Gelibolu used which was obtained from Turkish Petroleum Research Institute (TPAO). A parallel fault branch on the North-West of Anafartalar fault was detected. Also other tectonic structures of area were modelled. The results are confirmed by the TPAO seismic data. Further it is shown that steerable filters can be considered as a compromising approach in the evaluation of geophysical data. In particular, they can be used to delineate the strike of faults and to locate roughly ruins of foundation walls...|$|R
40|$|The recent {{development}} of Sequence Capture methodology represents a powerful strategy for enhancing data generation to assess genetic variation of targeted genomic regions. Here, we present SUPER-CAP, a bioinformatics web tool aimed at handling Sequence Capture data, fine calculating the allele frequency of variations and building genotype-specific sequence of captured genes. The dataset {{used to develop}} this in silico strategy consists of 378 loci and related regulative regions {{in a collection of}} 44 tomato landraces. About 14, 000 high-quality variants were identified. The high depth (> 40 ×) of coverage and adopting the <b>correct</b> <b>filtering</b> criteria allowed identification of about 4, 000 rare variants and 10 genes with a different copy number variation. We also show that the tool is capable to reconstruct genotype-specific sequences for each genotype by using the detected variants. This allows evaluating the combined effect of multiple variants in the same protein. The architecture and functionality of SUPER-CAP makes the software appropriate for a broad set of analyses including SNP discovery and mining. Its functionality, together with the capability to process large data sets and efficient detection of sequence variation, makes SUPER-CAP a valuable bioinformatics tool for genomics and breeding purposes...|$|R
40|$|This work {{addresses}} moving targets {{detection and}} imaging using airborne SAR data. Targets are selected according to user-defined radial velocity and direction. The computational burden is drastically reduced because azimuth compression is done only for ranges with apositivemoving target indication. Furthermore, {{the knowledge of}} the velocity sign allows to tune the detection and focusing of moving targets with higher SNR than the usual methods. Range migration is easily tackled because radial velocity is known, enabling the <b>correct</b> compression <b>filter</b> design...|$|R
40|$|Formation flying {{missions}} require autonomous relative navigation, with {{an accuracy}} depending {{both on the}} mission goals and on {{the capabilities of the}} involved platform in terms of sensors and computation. The emerging, successful standard of Cubesat spacecraft represents a clear example of a platform introducing severe constraints. This paper aims to investigate the performance attainable by means of a simple solution for the relative navigation of the formations in Low Earth Orbit, especially suitable for moderate cost, moderate performance missions (like the ones exploited by cubesats). The solution builds on the radiofrequency inter-satellite link, and exploits both the received signal strength as well as the received signal shift in frequency. Related hardware is normally available in on board receivers, so that no additional equipment is required. Observables are fed to an estimator (Extended Kalman Filter) which on its turn poses really limited computational burden. The solution can be therefore conveniently intended as an inexpensive back-up or even as the primary solution for missions with rough requirements. Simulations for a number of interesting cases provide an insight about the actual performance to be attained with current hardware and <b>correct</b> <b>filter</b> tuning by means of this simple, traditional technique applied to formation relative navigation...|$|R
50|$|Apart {{from these}} {{individual}} pore sizes, the same CFP measurement permits {{the representation of}} the cumulative filter flow distribution vs the pore size, which provides information about the percentage of the cumulative total flow through the sample that goes through pores of a larger size than a certain value. Another information that can be obtained from the measurements is the <b>corrected</b> differential <b>filter</b> flow, which shows the flow distribution per unit of change in size, i.e. the increase in flow rate per unit increase in pore diameter. It is also defined as pore size distribution.|$|R
40|$|Particle {{filters are}} Monte Carlo methods that aim to {{approximate}} the optimal filter of a partially observed Markov chain. In this paper, we study {{the case in}} which the transition kernel of the Markov chain depends on unknown parameters: we construct a particle filter for the simultaneous estimation of the parameter and the partially observed Markov chain (adaptive estimation) and we prove the convergence of this <b>filter</b> to the <b>correct</b> optimal <b>filter,</b> as time and the number of particles go to infinity. The filter presented here generalizes Del Moral's Monte Carlo particle filter...|$|R
