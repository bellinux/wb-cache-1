27|14|Public
5000|$|... eXtremeDB Financial Edition {{provides}} {{features for}} managing market data (tick data) in {{applications such as}} algorithmic trading and order matching. [...] A “sequences” data type supports <b>columnar</b> <b>data</b> layout and enables eXtremeDB to offer {{the benefits of a}} column-oriented database in handling time series data. The Financial Edition also provides a library of vector-based statistical functions to analyze data in sequences, and a performance monitor.|$|E
50|$|Letter Gothic is a {{monospaced}} sans-serif typeface. It {{was created}} between 1956 and 1962 by Roger Roberson for IBM in their Lexington, Kentucky Lexington plant. It was initially {{intended to be}} used in IBM Selectric typewriter Selectric electric typewriters. It is readable and is recommended for technical documentation and for sheets including <b>columnar</b> <b>data.</b> Gayaneh Bagdasaryan designed a proportional font called New Letter Gothic, based on Letter Gothic, for ParaType.|$|E
5000|$|On June 26, 2014, at {{the annual}} ACM SIGMOD/PODS Conference, Kersten {{received}} the ACM SIGMOD Edgar F. Codd Innovations Award for innovative and significant contributions to database systems and databases.Two years later, May 26, 2016, {{at the annual}} ACM SIGMOD/PODS Conference, Kersten received the SIGMOD Systems Award for the design and implementation of MonetDB, a pioneering main-memory database system based on a <b>columnar</b> <b>data</b> organization.In the same year, December 8, 2016, Kersten became [...] ACM Fellow.|$|E
40|$|Microbial ureolysis-induced calcite {{precipitation}} {{may offer}} an in situ remediation for heavy metal and radionuclide contamination, {{as well as}} an alternative to traditional soil strengthening techniques. A microbially mediated calcite precipitation model was built in TOUGHREACT v 2 and calibrated to batch and <b>columnar</b> experimental <b>data.</b> Kinetic ureolysis and calcite precipitation-rate expressions were parameterized by coupling TOUCHREACT with UCODE...|$|R
50|$|Storage server {{software}} also implements Smart Flash Cache, Columnar Flash Cache, Flash Logging and Write Back Flash Cache routines {{that use}} flash storage to improve I/O response times. I/O Resource Management allocates I/O bandwidth to databases or workloads according to specified priorities. Lastly, decompression of Hybrid <b>Columnar</b> Compressed <b>data</b> may {{be performed in}} Exadata storage servers.|$|R
50|$|When the Oracle Database Appliance is {{connected}} to a ZS3 storage array, the DBA can leverage Hybrid <b>Columnar</b> Compression for <b>data</b> stored on the ZS3 array. This can enable not only tiered storage, but also compression ratios exceeding 20x.|$|R
50|$|Apache Parquet can be {{compared}} with RCFile and Optimized RCFile (ORC) file formats as all the three {{fall under the category}} of <b>columnar</b> <b>data</b> storage within the Hadoop ecosystem. They all have better compression and encoding with improved read performance at the cost of slower writes. In addition to these features, Apache Parquet supports limited schema evolution where the schema can be modified according to the changes in the data. It also provides the ability to add new columns {{at the end of the}} file structure. As of now, only Apache Hive and Cloudera Impala are able to query such newly added columns and the other frameworks like Apache Pig are working it.|$|E
5000|$|Comparisons between row-oriented and column-oriented {{databases}} {{are typically}} {{concerned with the}} efficiency of hard-disk access for a given workload, as seek time is incredibly long {{compared to the other}} bottlenecks in computers. For example, a Serial ATA (SATA) hard drive has a maximum transfer rate of 600 MB/second (Megabytes per second) [...] while DDR3 SDRAM Memory can reach transfer rates of 17 GB/s (Gigabytes per second), nearly 30 times as fast. Clearly, a major bottleneck in handling big data is disk access. Columnar databases boost performance by reducing the amount of data that needs to be read from disk, both by efficiently compressing the similar <b>columnar</b> <b>data</b> and by reading only the data necessary to answer the query.|$|E
50|$|IBM BLU Acceleration is a {{collection}} of technologies from the IBM Research and Development Labs for analytical database workloads. BLU Acceleration integrates a number of different technologies including in-memory processing of <b>columnar</b> <b>data,</b> Actionable Compression (which uses approximate Huffman encoding to compress and pack data tightly), CPU Acceleration (which exploits SIMD technology and provides parallel vector processing), and Data Skipping (which allows data that's of no use to the current active workload to be ignored). The term ‘BLU’ does not stand for anything in particular; however it has an indirect play on IBM's traditional corporate nickname Big Blue. (Ten IBM Research and Development facilities around the world filed more than 25 patents while working on the Blink Ultra project, which has resulted in BLU Acceleration.)BLU Acceleration does not require indexes, aggregates or tuning. BLU Acceleration is integrated in Version 10.5 of IBM DB2 for Linux, Unix and Windows,(DB2 for LUW) and uses the same storage and memory constructs (i.e., storage groups, table spaces, and buffer pools), SQL language interfaces, and administration tools as traditional DB2 for LUW databases. BLU Acceleration is available on both IBM POWER and x86 processor architectures.|$|E
40|$|Prediction (NCEP), {{describes}} "NCEP improved " {{ocean surface}} wind speed data at 20 m above the sea surface, <b>columnar</b> liquid water <b>data,</b> and <b>columnar</b> water vapor <b>data</b> retrieved from the SSM/I sensor on the DSMP satellites {{which is being}} prepared for distribution through AWIPS, FOS, and GTS in BUFR format. These data are expected to become operational in late March 2000. They will be issued 4 {{times a day and}} contain 6 hours of data centered on the synoptic hours. The issuance times will be approximately 0400, 1000, 1600, and 2200 UTC. Data from all the currently operational DSMP satellites will be included. Currently, there are 3 operational satellites (f 11, f 13, and f 14) in polar orbit. The orbit times are approximately 102 minutes with two orbits per day over any given area (one ascending and one descending) per satellite. A passive microwave sensor (SSM/I) is used with 7 channels of brightness temperature data available. Each satellite covers a swath of 1400 km with a spacing and footprint resolution of 25 km...|$|R
40|$|A new sodium salt, sodium triphenylacetate {{has been}} {{synthesized}} and characterized by single-crystal X-ray diffraction: triclinic, P- 1 with a = 8. 3846 (2) angstrom, b = 13. 5992 (3) angstrom, c = 15. 5288 (3) angstrom, a = 76. 7270 (11) degrees, b = 74. 4500 (11) degrees, c = 84. 7410 (13) degrees, M-W = 656. 65, V = 1659. 42 (6) angstrom(3), Z = 2. The triphenylacetic molecules bridge the metallic ions forming an infinite <b>columnar</b> structure. Calorimetric <b>data</b> {{does not support}} the existence of a mesophase induced by temperature...|$|R
40|$|Both {{the sun and}} {{the moon}} exert {{influences}} on the ionosphere, causing fluctuations in its electron content. The small lunar effects, though not negligible, are difficult to analyze because their periodicities differ little from the periodicity of the dominant solar effects. A finite duration impulse response filter was perfected, permitting the efficient splitting of our <b>columnar</b> electron content <b>data</b> into a solar, a lunar, and a residual component. The solar component plus the lunar component and the solar component alone were processed by a dynamic ionospheric simulation program that yields values of vertical plasma drifts when electron content data are used as input. The difference between the two plasma drifts so obtained was taken as being the plasma drift caused by the electric field generated by the lunar tides in the dynamo region. This technique appears {{to be the first to}} allow a direct estimation of the lunar-induced electric fields in the ionosphere...|$|R
40|$|Big Data query systems {{represent}} {{data in a}} columnar {{format for}} fast, selective access, {{and in some cases}} (e. g. Apache Drill), perform calculations directly on the <b>columnar</b> <b>data</b> without row materialization, avoiding runtime costs. However, many analysis procedures cannot be easily or efficiently expressed as SQL. In High Energy Physics, the majority of data processing requires nested loops with complex dependencies. When faced with tasks like these, the conventional approach is to convert the <b>columnar</b> <b>data</b> back into an object form, usually with a performance price. This paper describes a new technique to transform procedural code so that it operates on hierarchically nested, <b>columnar</b> <b>data</b> natively, without row materialization. It {{can be viewed as a}} compiler pass on the typed abstract syntax tree, rewriting references to objects as columnar array lookups. We will also present performance comparisons between transformed code and conventional object-oriented code in a High Energy Physics context. Comment: 10 pages, 2 figures, submitted to IEEE Big Dat...|$|E
40|$|The four <b>columnar</b> <b>data</b> tape drives for Rice University’s R 1 computer. Also shown, in {{the lower}} right foreground, is a {{teletype}} machine. Photograph by Bob Roosth. Original resource is {{a black and white}} photograph. Donated by Rice graduate Bill Harris, class of 1971, friend of the photographer and fellow Rice graduate, Bob Roosth, Will Rice College class of 1971...|$|E
30|$|ADAM {{focuses on}} backend processing, and hence, user-facing {{applications}} {{need to be}} implemented as, for example, Scala or Python scripts. ADAM uses Spark to scale out parallel processing. The data are stored in Parquet, a <b>columnar</b> <b>data</b> storage using Avro serialized file formats that reduce I/O load by providing in-memory data access for the Spark pipeline implementation. The pipeline is implemented as a Spark program.|$|E
40|$|NOTE: Text or symbols not renderable {{in plain}} ASCII are {{indicated}} by [ [...] . ]. Abstract is included in. pdf document. The solar-wind turbulence near {{the sun is}} investigated with data obtained near the superior conjunctions of Mariners 6, 7, and 9. The data are time histories {{of the change in}} the electron columnar content between the earth and spacecraft. The data were obtained with a group-phase technique which is sensitive only to the change in the columnar content. The measurement technique is discussed. The theory of power spectra is outlined. The relationship between the temporal power spectrum of the <b>columnar</b> content <b>data</b> and the comoving wave-number spectrum of the solar wind is derived. It is found that comoving spectrum is well represented by a power-law [ [...] . ] of index [ [...] . ]= 3. 9 [ [...] . ] 0. 2. Comparison of the overall average spectral amplitude near the sun [ [...] . ] to that near 1 a. u. shows that the turbulence declines with heliocentric distance as [ [...] . ], ignoring time variations. In the region near the sun (0. 07 [ [...] . ] r [ [...] . ] 0. 22 a. u.) [ [...] . ] declines more slowly. It is suggested that there is a region of enhanced turbulence near the sun. The Mariner 9 spectral amplitudes correlate with Zurich sunspot number. The data are used to investigate the relationship between McMath calcium plage regions and density enhancements intersecting the line of sight. The relationship of the present observations to theories of solar wind heating and to interplanetary scintillation observations is discussed...|$|R
40|$|An {{extraordinary}} aerosol situation over Leipzig, Germany in April 2002 {{was investigated}} with a comprehensive set of ground-based volumetric and <b>columnar</b> aerosol <b>data,</b> combined with aerosol profiles from lidar, meteorological data from radiosondes and air mass trajectory calculations. Air masses were identified {{to stem from}} the Arctic, partly influenced by the greater Moscow region. An evaluation of ground-based measurements of aerosol size distributions during these periods showed that the number concentrations below about 70 nm in diameter were below respective long-term average data, while number, surface and volume concentrations of the particles larger than about 70 nm in diameter were higher than the long-term averages. The lidar aerosol profiles showed that the imported aerosol particles were present up to about 3 km altitude. The particle optical depth was up to 0. 45 at 550 nm wavelength. With a one-dimensional spectral radiative transfer model top of the atmosphere (TOA) radiative forcing of the aerosol layer was estimated for a period with detailed vertical information. Solar aerosol radiative forcing values between - 23 and - 38 W m(- 2) were calculated, which are comparable to values that {{have been reported in}} heavily polluted continental plumes outside the respective source regions. The present report adds weight to previous findings of aerosol import to Europe, pointing to the need for attributing the three-dimensional aerosol burden to natural and anthropogenic sources as well as to aerosol imports from adjacent or distant source regions. In the present case, the transport situation is further complicated by forward trajectories, indicating that some of the observed Arctic haze may have originated in Central Europe. This aerosol was transported to the European Arctic before being re-imported in the modified and augmented form to its initial source region...|$|R
40|$|International audienceThere {{is little}} {{information}} on the function of epididymal basal cells. These cells secrete prostaglandins, can metabolize radical oxygen species, and have apical projections that are components of the blood-epididymis barrier. The objective {{of this study was}} to develop a reproducible protocol to isolate rat epididymal basal cells and to characterize their function by gene expression profiling. Integrin-alpha 6 was used to isolate a highly purified population of basal cells. Microarray analysis indicated that expression levels of 552 genes were enriched in basal cells relative to other cell types. Among these genes, 45 were expressed at levels of 5 -fold or greater. These highly expressed genes coded for proteins implicated in cell adhesion, cytoskeletal function, ion transport, cellular signaling, and epidermal function, and included proteases and antiproteases, signal transduction, and transcription factors. Several highly expressed genes have been reported in adult stem cells, suggesting that basal cells may represent an epididymal stem cell population. A basal cell culture was established that showed that these basal cells can differentiate in vitro from keratin (KRT) 5 -positive cells to cells that express KRT 8 and connexin 26, a marker of <b>columnar</b> cells. These <b>data</b> provide novel information on epididymal basal cell gene expression and suggest that these cells can act as adult stem cells...|$|R
40|$|A {{close-up}} {{view of the}} reel of tape used {{on one of the}} <b>columnar</b> <b>data</b> tape drives for Rice University’s R 1 computer. The tape is labeled “Pubic sys. tp. 1 / 24 / 69 ”. There are two puncture holes in the glass covering the reels. Photograph taken by Bob Roosth. Original resource is a black and white photograph. Donated by Rice graduate Bill Harris, class of 1971, friend of the photographer and fellow Rice graduate, Bob Roosth, Will Rice College class of 1971...|$|E
40|$|Dremel is a scalable, {{interactive}} ad hoc query {{system for}} analysis of read-only nested data. By combining multilevel execution trees and <b>columnar</b> <b>data</b> layout, {{it is capable of}} running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReducebased computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system. 1...|$|E
40|$|Abstract: The {{purpose of}} this paper is to {{investigate}} the application of logical tools such as inference trees and <b>columnar</b> <b>data</b> flow diagrams in the information system (IS) analysis and design context. Seventeen students at an institution of higher education were observed during the design and analysis of information systems and their experiences were evaluated through a focus group interview, observations and documents analysis. This research was based on a qualitative, action research approach (Yin 1994; Merriam 1998). The most important findings were: the columnar method empowers students ’ motivational and cognitive skills enhancing their vision and system design skills; inference trees improve detection and correction of reasoning errors overcoming students ’ limited information processing capacity...|$|E
40|$|BACKGROUND: Epithelial {{shedding}} {{processes in}} airway inflammation and defence may produce damaged areas where basal cells {{are the main}} remaining epithelial cell type. The present study examines the capacity of basal cells to form an epithelial barrier structure after loss of columnar epithelial cells. METHODS: A technique was developed which allows selective removal of columnar epithelial cells from isolated airways. A drop of tissue adhesive glue was applied on the mucosal surface shortly after excision of guinea pig trachea and human bronchus. Gentle removal of the glue, together with attached columnar cells, left a single layer of cobbled, solitary basal cells. The tissue was kept in culture media. Morphological changes of the basal cells were monitored by immuno-histochemistry and scanning and transmission electron microscopy at several time points. RESULTS: After 20 minutes the basal cells had undergone extensive flattening and established contact with each other. The basement membrane thus became covered by a poorly differentiated epithelium in both guinea pig and human airways. Abundant interdigitating cytoplasmic protrusions were observed at cell borders. CONCLUSIONS: Basal cells promptly flatten out to cover the basement membrane at loss of neighbouring <b>columnar</b> cells. These <b>data</b> may explain why the epithelial barrier function may be uncompromised in desquamative airway diseases. Furthermore, they suggest the possibility that sacrificial release of columnar epithelial cells and prompt creation of a barrier structure constitute important roles of basal cells in airway defence against severe insults. ...|$|R
40|$|It {{has been}} {{suggested}} that the number and position of epidermal stem cells are related to the units of columnar structure in the upper epidermal strata and that the cells of each unit are derived from a single stem cell. Studies of cell lineage in developing tissues have been facilitated by the use of retroviral transduction to provide inherited expression of a histochemically demonstrable foreign gene product. To provide direct evidence about the clonal nature of epidermal units, murine epidermal keratinocytes were transduced with a replication-deficient retroviral vector carrying the β-galactosidase gene. Subepidermal injection of virus in vivo led to infrequent transduction with only transient presence of β-gal-staining keratinocytes within the epidermis. Transduction of keratinocytes in vivo and transplantation back to in vivo sites permitted demonstration of the transduced gene in clusters of cells within the reformed epidermis throughout a 12 -wk period. The epidermis redeveloped an ordered columnar structure with restriction of transduced cells to individual columnar units. This clonal appearance is compatible with derivation of each epidermal unit from a single stem cell but is not compatible with a random pattern of cell proliferation. Transduced epidermal sheets that were recombined with oral mucosal connective tissue also redeveloped normal columnar structure with restriction of β-gal staining to individual <b>columnar</b> units. These <b>data</b> suggest that the establishment of an epidermal stem cell pattern related to units of structure is an intrinsic property of the epithelium and is not dependent on regionally-specific connective tissue influences...|$|R
40|$|International audienceIn {{the present}} work {{we use the}} NASA-JPL global ionospheric maps of total {{electron}} content (TEC), firstly to construct TEC maps (TEC vs. magnetic local time MLT, and magnetic latitude MLAT) in the interval from 1999 to 2005. These TEC maps were, in turn, used to estimate the annual-to-mean amplitude ratio, A 1, and the semiannual-to-mean amplitude ratio, A 2, {{as well as the}} latitudinal symmetrical and asymmetrical parts, A' and A" of A 1. Thus, we investigated in detail the TEC climatology from maps of these indices, with an emphasis on the quantitative presentation for local time and latitudinal changes in the seasonal, annual and semiannual anomalies of the ionospheric TEC. Then we took the TEC value at 14 : 00 LT to examine various anomalies at a global scale following the same procedure. Results reveal similar features appearing in NmF 2, such as that the seasonal anomaly is more significant in the near-pole regions than in the far-pole regions and the reverse is true for the semiannual anomaly; the winter anomaly has least a chance to be observed at the South America and South Pacific areas. The most impressive feature is that the equinoctial asymmetry is most prominent at the East Asian and South Australian areas. Through the analysis of the TIMED GUVI <b>columnar</b> [O/N 2] <b>data,</b> we have investigated to what extent the seasonal, annual and semiannual variations can be explained by their counterparts in [O/N 2]. Results revealed that the [O/N 2] variation is a major contributor to the daytime winter anomaly of TEC, and it also contributes to some of the semiannual and annual anomalies. The contribution to the anomalies unexplained by the [O/N 2] data could possibly be due to the dynamics associated with thermospheric winds and electric fields...|$|R
30|$|Most {{pipelines}} save data in {{a traditional}} file system since most analysis tools are implemented {{to read and write}} files in POSIX file systems. GESALL provides a layer that enables using HDFS for data storage by wrapping tools and providing optimized genomic data-specific mapping between POSIX and HDFS. ADAM uses a different, data-oriented approach, with a layered architecture for data storage and analysis that exploits recent advancement in big data analysis systems. Like ADAM, GATK 4 is also built on top of Spark and a <b>columnar</b> <b>data</b> storage system. ADAM requires re-implementing the analysis tools, which may be practical for the most commonly used tools and pipelines such as the GATK reference pipelines, but is often considered impractical for the many other tools and hence pipelines.|$|E
40|$|Abstract—Performance and {{responsiveness}} {{of visual}} ana-lytics sytems for exploratory data analysis of large datasets {{has been a}} long standing problem. We propose a method for incrementally computing visualizations in a distributed fashion by combining a modified MapReduce-style algorithm with a compressed <b>columnar</b> <b>data</b> store, resulting in signif-icant improvements in performance and responsiveness for constructing commonly encountered information visualizations, e. g. bar charts, scatterplots, heat maps, cartograms and parallel coordinate plots. We compare our method with one that queries three other readily available database and data warehouse systems — PostgreSQL, Cloudera Impala and the MapReduce-based Apache Hive — in order to build visualizations. We show that our end-to-end approach allows for greater speed and guaranteed end-user responsiveness, {{even in the face of}} large, long-running queries. Keywords-incremental visualization; online aggregation; in-formation visualization; MapReduce; columnar storage; I...|$|E
40|$|Column-oriented {{database}} {{systems have}} been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users {{with the possibility of}} answering ad hoc queries over large datasets in an interactive manner. In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill 1. It combines the advantages of <b>columnar</b> <b>data</b> layout with other known techniques (such as using composite range par-titions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in pro-cessing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to pro-cessing a trillion values in the underlying dataset. 1...|$|E
40|$|The {{exchange}} {{of carbon dioxide}} between the atmosphere and the polar caps on Mars creates a seasonal cycle of growth and retreat of the polar caps. As the major component of the Martian atmosphere, CO 2 condenses in the polar regions of the planet during the winter seasons and precipitates as CO 2 frost. It then sublimes during {{the spring and summer}} seasons in response to solar radiation. Through natural radioactivity or when exposed to cosmic rays, elements in the Martian near-subsurface (uppermost meter) emit gamma rays with distinct, characteristic energies. The Gamma Ray Spectrometer (GRS) onboard the 2001 Mars Odyssey satellite is used to measure the gamma rays coming from the Martian regolith to calculate elemental distributions, abundances, and temporal variations in the gamma ray flux. Changes in the CO 2 frost over time can be quantified by observing attenuation effects of H (2223 keV hydrogen) and 40 K (1461 keV potassium) gamma ray signals transmitted through various depths of polar CO 2 overburden throughout the Martian seasons. Conclusions are drawn about the spatial extent, column density, and mass of Mars' seasonal polar caps as a function of time utilizing GRS <b>data.</b> <b>Columnar</b> thickness and mass results are discussed and plotted for latitudes including +/- 60 degrees and poleward. GRS observations are compared to predictions from the NASA Ames Research Center Mars General Circulation Model (ARC GCM) and to similar experimental results from the Mars Odyssey High Energy Neutron Detector (HEND) and the Neutron Spectrometer (NS). Models for north and south polar atmosphere and regolith distributions are incorporated, and the results indicate that the assumption of a 100 % H 2 O-ice residual cap underlying the seasonal frost in the north is accurate. The GRS CO 2 frost observations are in good agreement with the other studies mentioned, in particular for the timing of the beginning of frost deposition to the complete sublimation of surface CO 2 back into the atmosphere. The total amount of condensed CO 2 mass derived from GRS data is on the order of 6. 0 x 10 ^ 15 kg and verifies previous reports that ~ 25 % of the total Martian exchangeable-CO 2 reservoir participates in the ground-atmosphere cycle...|$|R
40|$|In modern column-oriented databases, {{compression}} {{is important}} for improving I/O throughput and overall database performance. Many string <b>columnar</b> <b>data</b> cannot be compressed by special-purpose algorithms such as run-length encoding or dictionary compression, and the typical choice for them is the LZ 77 -based compression algorithms such as GZIP or Snappy. These algorithms treat data as a byte block and do not exploit the columnar nature of the data. In this thesis, we develop a compression algorithm using frequent string patterns directly mined from a sample of a string column. The patterns are used as the dictionary phrases for compression. We discuss some interesting properties of frequent patterns {{in the context of}} compression, and develop a pruning method to address the cache inefficiencies in indexing the patterns. Experiments show that our compression algorithm outperforms Snappy in compression ratio while retains compression and decompression speed...|$|E
40|$|Arguably data {{is the new}} natural {{resource}} in the enterprise world with an unprecedented degree of proliferation. But to derive real-time actionable insights from the data, {{it is important to}} bridge the gap between managing the data that is being updated at a high velocity (i. e., OLTP) and analyzing a large volume of data (i. e., OLAP). However, there has been a divide where specialized solutions were often deployed to support either OLTP or OLAP workloads but not both; thus, limiting the analysis to stale and possibly irrelevant data. In this paper, we present Lineage-based Data Store (L-Store) that combines the real-time processing of transactional and analytical workloads within a single unified engine by introducing a novel lineage-based storage architecture. By exploiting the lineage, we develop a contention-free and lazy staging of <b>columnar</b> <b>data</b> from a write-optimized form (suitable for OLTP) into a read-optimized form (suitable for OLAP) in a transactionally consistent approach that also supports querying and retaining the current and historic data. Our working prototype of L-Store demonstrates its superiority compared to state-of-the-art approaches under a comprehensive experimental evaluation. Comment: 22 pages, 10 figures, 9 table...|$|E
40|$|Exploratory data {{analysis}} tools must respond quickly to a user's questions, {{so that the}} answer to one question (e. g. a visualized histogram or fit) can influence the next. In some SQL-based query systems used in industry, even very large (petabyte) datasets can be summarized on a human timescale (seconds), employing techniques such as <b>columnar</b> <b>data</b> representation, caching, indexing, and code generation/JIT-compilation. This article describes progress toward realizing such a system for High Energy Physics (HEP), focusing on the intermediate problems of optimizing data access and calculations for "query sized" payloads, such as a single histogram or group of histograms, rather than large reconstruction or data-skimming jobs. These techniques include direct extraction of ROOT TBranches into Numpy arrays and compilation of Python analysis functions (rather than SQL) to be executed very quickly. We will also discuss the problem of caching and actively delivering jobs to worker nodes that have the necessary input data preloaded in cache. All of these pieces of the larger solution are available as standalone GitHub repositories, and could be used in current analyses. Comment: 6 pages, 2 figures, proceedings for ACAT 201...|$|E
40|$|Data {{models are}} a central piece in {{information}} systems, being the relational data models very popular and extensively used. In Big Data, {{and due to}} {{the characteristics of the}} NoSQL databases, the data modeling task is seen in another perspective, as those databases are considered schema-free. Nevertheless, these databases also need data models that ensure the proper storage and querying of the data. Considering the vast amount of relational databases and the ever-increasing volume of data, the importance of data models in Big Data increases. In this work, a specific set of rules is proposed for the automatic transition between a traditional and a Big Data environment, considering two specific objectives: the identification of a <b>columnar</b> <b>data</b> model for HBase supporting operational needs and the identification of a tabular data model for Hive supporting analytical needs. The obtained results show the applicability of the proposed rules and their relevance for data modeling in Big Data environments. This work has been supported by COMPETE: POCI- 01 - 0145 - FEDER- 007043 and FCT, Fundação para a Ciência e Tecnologia, within the Projects UID/CEC/ 00319 / 2013 (ALGORITMI) and MITP-TB/CS/ 0026 / 2013 (SusCity) ...|$|E
40|$|Background: The {{display of}} N-glycan {{carbohydrate}} structures {{is an essential}} part of glycoinformatics. Several tools exist for building such structures graphically, by selecting from a palette of symbols or sugar names, or else by specifying a structure in one of the chemical naming schemes currently available. Findings: In the present work we present two tools for displaying N-glycans found in the mammalian CHO (Chinese hamster ovary) cell line, both of which take as input a 9 -digit identifier that uniquely defines each structure. The first of these, GlycoForm, is designed to display a single structure automatically from an identifier entered by the user. The display is updated in real time, using symbols for the sugar residues, or in text-only form. Structures can be added to a library, which is recorded in a preference file and loaded automatically at start. Individual structures can be saved in a variety of bitmap image formats. The second program, Glycologue, reads a file containing <b>columnar</b> <b>data</b> of nine-digit codes, which can be displayed on-screen and printed at high resolution. Conclusion: A key advantage of both programs is the speed and facility with which carbohydrate structures can be drawn. It is anticipated that these programs will be useful to glycobiologists, systems biologists and biotechnologists interested in N-glycosylation systems in mammalian cells...|$|E
40|$|Abstract Background The {{display of}} N -glycan {{carbohydrate}} structures {{is an essential}} part of glycoinformatics. Several tools exist for building such structures graphically, by selecting from a palette of symbols or sugar names, or else by specifying a structure in one of the chemical naming schemes currently available. Findings In the present work we present two tools for displaying N -glycans found in the mammalian CHO (Chinese hamster ovary) cell line, both of which take as input a 9 -digit identifier that uniquely defines each structure. The first of these, GlycoForm, is designed to display a single structure automatically from an identifier entered by the user. The display is updated in real time, using symbols for the sugar residues, or in text-only form. Structures can be added to a library, which is recorded in a preference file and loaded automatically at start. Individual structures can be saved in a variety of bitmap image formats. The second program, Glycologue, reads a file containing <b>columnar</b> <b>data</b> of nine-digit codes, which can be displayed on-screen and printed at high resolution. Conclusion A key advantage of both programs is the speed and facility with which carbohydrate structures can be drawn. It is anticipated that these programs will be useful to glycobiologists, systems biologists and biotechnologists interested in N -glycosylation systems in mammalian cells. </p...|$|E
40|$|The {{research}} activity {{presented in this}} manuscript deals {{with the implementation of}} a methodology to merge in an optimal way atmospheric modelling and observations at different spatial scales. In particular, we approached the problem of assimilation of ground measurements and satellite <b>columnar</b> <b>data</b> and how the Data Assimilation (DA) could improve the chemical transport model (CTMs) and correct biases and errors in the chemical species forecast. The work focused on tropospheric ozone and the species linked to its formation, since they {{play a crucial role in}} chemical processes during photochemical pollution events. The study was carried out implementing and applying an Optimal Interpolation (OI) DA technique in the air quality model BOLCHEM and the CHIMERE CTM. The OI routine was chosen because it has given satisfactory results in air quality modelling and because it is relatively simple and computationally inexpensive. In the first part of the study we evaluated the improvement in the capability of regional model BOLCHEM to reproduce the distribution of tropospheric pollutants, using the assimilation of surface chemical observations. Among the many causes of uncertainties of CTMs simulations, a particular focus is given by uncertainties in emissions, that are known to be high. The scientific purpose was to analyse the efficacy of DA in correcting the biases due to perturbed emission. The work was performed using an Observing System Simulation Experiment (OSSE), which allowed the quantification of assimilation impact, through comparison with a reference state. Different sensitivity tests were carried out in order to identify how assimilation can correct perturbations on O 3, induced by NOx emissions biased in flux intensity and time. Tests were performed assimilating different species, varying assimilation time window length and starting hour of assimilation. Emissions were biased quantitatively up to ± 50...|$|E
40|$|Due to the {{considerable}} differences between transactional and analytical workloads, a “one size {{does not fit}} all” paradigm is typically applied to isolate transactional and analytical data into separate database management systems. Even though the separation has its advantages, it compromises real-time analytics. To blur boundaries between analytical and transactional data management systems, hybrid transactional/analytical processing (HTAP) systems are turned into reality. HTAP systems mostly rely on in-memory computation to present profound performance. Also, <b>columnar</b> <b>data</b> layout has become popular specifically for analytical use-cases. In this thesis, a quantitative empirical research is conducted {{with the goal of}} evaluating the performance of an HTAP system with a transactional workload. HANA (High-Performance Analytic Appliance), an in-memory HTAP system, is used as the underlying data management system for the research; HANA comes with two data stores: a columnar and a row data store. Firstly, the performance of HANA’s columnar store is compared with the row store. To generate the required workload, an industry-grade transactional benchmark (TPC-E) is implemented. Secondly, a profiling tool is employed to analyze primary cost drivers of the HTAP system while running the benchmark. Finally, it is investigated how optimal an HTAP-oriented stored procedure language (SQLScript) is for the transactional workload. To investigate this matter, several transactions are designed on top of TPC-E schema; the transactions then are implemented with and without using SQLScript iterative constructs. The transactions are studied regarding the response time and growth rate. The experiment shows that the row data store achieves 26 % higher throughput compared to its counterpart for the transactional workload. Furthermore, the profiling results demonstrate that the transactional workload mainly breaks down into eight components of HANA including query compilation and validation, data store access and predicate evaluation, index access and join processing, memory management, sorting operation, data manipulation language (DML) operations, network transfer and communication, and SQLScript execution. Lastly, the experiment reveals that the native SQL set-based operations outperform the iterative paradigm offered by SQLScript...|$|E
