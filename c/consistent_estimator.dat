783|970|Public
5|$|The maximum spacing {{estimator}} is a <b>consistent</b> <b>estimator</b> {{in that it}} converges in probability to {{the true}} value of the parameter, θ0, as the sample size increases to infinity. The consistency of maximum spacing estimation holds under much more general conditions than for maximum likelihood estimators. In particular, {{in cases where the}} underlying distribution is J-shaped, maximum likelihood will fail where MSE succeeds. An example of a J-shaped density is the Weibull distribution, specifically a shifted Weibull, with a shape parameter less than 1. The density will tend to infinity as x approaches the location parameter rendering estimates of the other parameters inconsistent.|$|E
25|$|Efficiency, i.e., it {{achieves}} the Cramér–Rao {{lower bound}} when {{the sample size}} tends to infinity. This means that no <b>consistent</b> <b>estimator</b> has lower asymptotic mean squared error than the MLE (or other estimators attaining this bound).|$|E
25|$|Though non-parametric, L-estimators are {{frequently}} used for parameter estimation, {{as indicated by}} the name, though they must often be adjusted to yield an unbiased <b>consistent</b> <b>estimator.</b> The choice of L-estimator and adjustment depend on the distribution whose parameter is being estimated.|$|E
5000|$|McCulloch JH. Simple <b>consistent</b> <b>estimators</b> {{of stable}} {{distribution}} parameters, 1986, 191 cites.|$|R
40|$|This paper gives three easily {{computed}} highly outlier resistant robust $sqrt{n}$ <b>consistent</b> <b>estimators</b> of multivariate {{location and}} dispersion for elliptically contoured distributions with fourth moments. When {{the data is}} from a multivariate normal distribution, the dispersion <b>estimators</b> are also <b>consistent</b> <b>estimators</b> of the covariance matrix. Outlier detection and robust canonical correlation analysis are presented as applications. </p...|$|R
50|$|The {{method of}} moments is fairly simple and yields <b>consistent</b> <b>estimators</b> (under very weak assumptions), though these estimators are often biased.|$|R
2500|$|In statistics, a <b>consistent</b> <b>estimator</b> or {{asymptotically}} [...] <b>consistent</b> <b>estimator</b> is an estimator—a {{rule for}} computing estimates of a parameter θ0—having the property {{that as the}} number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to θ0. This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to θ0 converges to one.|$|E
2500|$|For example, {{dividing}} the IQR by [...] (using the error function) {{makes it an}} unbiased, <b>consistent</b> <b>estimator</b> for the population variance if the data follow a normal distribution.|$|E
2500|$|... {{finally found}} a {{rigorous}} derivation for the more robust weights, , as they are defined in the CMA-ES (weights are often zero for [...] ). They are formulated as the <b>consistent</b> <b>estimator</b> for the CDF of [...] at the point , composed with a fixed monotonous decreased transformation , that is, ...|$|E
40|$|We {{consider}} two <b>consistent</b> <b>estimators</b> for {{the parameters}} of the linear predictor in the Poisson regression model, where the covariate is measured with errors. The measurement errors are assumed to be normally distributed with known error variance sigma_u^ 2. The SQS estimator, based on a conditional mean-variance model, takes the distribution of the latent covariate into account, and this is here assumed to be a normal distribution. The CS estimator, based on a corrected score function, does not use the distribution of the latent covariate. Nevertheless, for small sigma_u^ 2, both estimators have identical asymptotic covariance matrices up to the order of sigma_u^ 2. We also compare the <b>consistent</b> <b>estimators</b> to the naive estimator, which is based on replacing the latent covariate with its (erroneously) measured counterpart. The naive estimator is biased, but has a smaller covariance matrix than the <b>consistent</b> <b>estimators</b> (at least up to the order of sigma_u^ 2.) ...|$|R
40|$|Longitudinal studies often feature {{incomplete}} {{response and}} covariate data. Likelihood-based {{methods such as}} the EM algorithm give <b>consistent</b> <b>estimators</b> for model parameters when data are missing at random provided that the response model and the missing covariate model are correctly specified; {{but we do not}} need to specify the missing data mechanism. An alternative method is the weighted estimating equation which gives <b>consistent</b> <b>estimators</b> if the missing data and response models are correctly specified; but we do not need to specify the distribution of the covariates that have missing values. In this paper we develop a doubly robust estimation method for longitudinal data with missing response and missing covariate when data are missing at random. This method is appealing in that it can provide <b>consistent</b> <b>estimators</b> if either the missing data model or the missing covariate model is correctly specified. Simulation studies demonstrate that this method performs well in a variety of situations...|$|R
40|$|We study {{a multivariate}} ultrastructural {{measurement}} error (MUME) model {{with more than}} one response variable. This model is a synthesis of multivariate functional and structural models. Three <b>consistent</b> <b>estimators</b> of regression coefficients, satisfying the exact linear restrictions have been proposed. Their asymptotic distributions are derived under the assumption of a non-normal measurement error and random error components. A simulation study is carried out to investigate the small sample properties of the estimators. The effect of departure from normality of the measurement errors on the estimators is assessed. Measurement error Multivariate regression Reliability matrix Linear restrictions <b>Consistent</b> <b>estimators...</b>|$|R
2500|$|If the {{conditions}} of the law of large numbers hold for the squared observations, s2 is a <b>consistent</b> <b>estimator</b> ofσ2. One can see indeed that the variance of the estimator tends asymptotically to zero. [...] An asymptotically equivalent formula was given in Kenney and Keeping (1951:164), Rose and Smith (2002:264), and Weisstein (n.d.).|$|E
2500|$|This is a <b>consistent</b> <b>estimator</b> (it converges in {{probability}} to {{the population}} value {{as the number of}} samples goes to infinity), and is the maximum-likelihood estimate when the population is normally distributed. However, this is a biased estimator, as the estimates are generally too low. The bias decreases as sample size grows, dropping off as 1/N, and thus is most significant for small or moderate sample sizes; for [...] the bias is below 1%. Thus for very large sample sizes, the uncorrected sample standard deviation is generally acceptable. This estimator also has a uniformly smaller mean squared error than the corrected sample standard deviation.|$|E
50|$|For instance, {{in order}} to use the median {{absolute}} deviation (MAD) to estimate the standard deviation of the normal distribution, one must multiply it by the factorwhere Φ−1 is the quantile function (inverse of the cumulative distribution function) for the standard normal distribution. (See MAD for details.) That is, the MAD is not a <b>consistent</b> <b>estimator</b> for the standard deviation of a normal distribution, but 1.4826... MAD is a <b>consistent</b> <b>estimator.</b> Similarly, the average absolute deviation needs to be multiplied by approximately 1.2533 to be a <b>consistent</b> <b>estimator</b> for standard deviation. Different factors would be required to estimate the standard deviation if the population did not follow a normal distribution.|$|E
50|$|As {{proponent of}} <b>consistent</b> <b>estimators</b> and causally {{identified}} models using econometrics and {{structural equation modeling}} techniques, he has also written strong critiques of Partial least squares path modeling, which he states should be abandoned.|$|R
2500|$|It {{has been}} {{established}} that the plug-in and smoothed cross validation selectors (given a single pilot bandwidth G) both converge at a relative rate of Op(n−2/(d+6)) [...] i.e., both these data-based selectors are <b>consistent</b> <b>estimators.</b>|$|R
3000|$|... is {{the average}} of the unobserved effects over time, but it is {{standard}} to assume that this is time-invariant. If cohort averages are based on a large number of individuals, this is a reasonable assumption, and one can obtain <b>consistent</b> <b>estimators</b> with an FE model (Verbeek 2008). Another condition to obtain <b>consistent</b> <b>estimators</b> is that the cohort averages show genuine time variation. It should be a balance between the number and size of the cohorts. More cohorts will refrain from small sample problems in the estimators, but fewer individuals in each cohort will reduce the reliability of the averages taken in the cohort 7.|$|R
5000|$|The whitened STA is a <b>consistent</b> <b>estimator,</b> i.e., it converges to {{the true}} linear subspace, if ...|$|E
5000|$|If the {{covariance}} of {{the errors}} [...] is unknown, {{one can get}} a consistent estimate of , say , using an implementable version of GLS known as the feasible generalized least squares (FGLS) estimator. In FGLS, modeling proceeds in two stages: (1) the model is estimated by OLS or another consistent (but inefficient) estimator, and the residuals are used to build a <b>consistent</b> <b>estimator</b> of the errors covariance matrix (to do so, one often needs to examine the model adding additional constraints, for example if the errors follow a time series process, a statistician generally needs some theoretical assumptions on this process {{to ensure that a}} <b>consistent</b> <b>estimator</b> is available); and (2) using the <b>consistent</b> <b>estimator</b> of the covariance matrix of the errors, one can implement GLS ideas.|$|E
5000|$|In {{order to}} use the MAD as a <b>consistent</b> <b>estimator</b> for theestimation of the {{standard}} deviation σ, one takes ...|$|E
40|$|This study {{presents}} an alternative method for estimating gravity models by {{multiple linear regression}} {{that is based on}} proxy variables, thus circumventing the endogeneity problems arising when least-squares estimators are used. The proxy variable approach generates <b>consistent</b> <b>estimators</b> for a gravity model without endogeneity bias. The presence of endogeneity is tested for using statistical tests developed specifically for our application. We conclude that proxy variables eliminate the endogeneity and produce <b>consistent</b> <b>estimators</b> in gravity models estimated using least squares. We also find, however, that endogeneity bias has no significant impact either on gravity model prediction or on urban transportation system planning processes based on such models. Gravity model Proxy variables Endogeneity Bias Consumer surplus...|$|R
3000|$|... i,, so {{they are}} no longer identifiable. But using the fixed-effects model allows me to obtain the “within-group” <b>consistent</b> <b>estimators</b> of other {{variables}} in the gravity model. They can be compared with the results of pooled OLS regressions to obtain a robust conclusion.|$|R
40|$|For the {{estimation}} of coefficients in a measurement error model, the least squares method utilizing original observations and averaged observations over replications provides inconsistent estimators. Based on these, <b>consistent</b> <b>estimators</b> are formulated and asymptotic properties are analyzed. Ultrastructural model Measurement errors Replicated observations...|$|R
5000|$|... is a {{consistent}} and unbiased estimator of MD(X). The statistic:is a <b>consistent</b> <b>estimator</b> of RMD(X), but is not, in general, unbiased.|$|E
5000|$|... is a <b>consistent</b> <b>estimator</b> of the {{population}} Gini coefficient, but is not, in general, unbiased. Like G, [...] has a simpler form: ...|$|E
50|$|In statistics, a <b>consistent</b> <b>estimator</b> or {{asymptotically}} <b>consistent</b> <b>estimator</b> is an estimator—a {{rule for}} computing estimates of a parameter θ0—having the property {{that as the}} number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to θ0. This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to θ0 converges to one.|$|E
40|$|This paper {{identifies}} {{and estimates}} the relative average treatment {{effect in the}} presence of misclassification. We propose <b>consistent</b> <b>estimators</b> based on nonparametric methods. The simulation results reported illustrate the performance of the proposed estimators. Misclassification Treatment effect Relative effect Policy intervention Estimation...|$|R
40|$|This paper derives {{the sharp}} pointwise bounds of the {{distribution}} function of the difference of two treatments' outcomes. The asymptotic distributions of the <b>consistent</b> <b>estimators</b> are discontinuous at nuisance parameters due to two reasons, which make bootstrap estimations inconsistent. Bootstrap Treatment effects Boundary problem Subsampling...|$|R
25|$|Other {{desirable}} {{properties for}} estimators include: UMVUE estimators {{that have the}} lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and <b>consistent</b> <b>estimators</b> which converges in probability to the true value of such parameter.|$|R
5000|$|By definition, a <b>consistent</b> <b>estimator</b> B converges in {{probability}} to {{its true}} value β, {{and often a}} central limit theorem {{can be applied to}} obtain asymptotic normality: ...|$|E
5000|$|For example, {{dividing}} the IQR by [...] (using the error function) {{makes it an}} unbiased, <b>consistent</b> <b>estimator</b> for the population variance if the data follow a normal distribution.|$|E
5000|$|Mathematically, a {{sequence}} of estimators {tn; n ≥ 0} is a <b>consistent</b> <b>estimator</b> for parameter θ if and only if, for all ϵ > 0, no matter how small, we have ...|$|E
40|$|In {{this paper}} we {{consider}} a sieve bootstrap method for constructing nonparametric prediction intervals {{for a general}} class of linear processes. We show that the sieve bootstrap provides <b>consistent</b> <b>estimators</b> of the conditional distribution of future values given the observed data. Sieve bootstrap Prediction intervals Linear processes...|$|R
40|$|Stein-rule {{procedure}} is a known technique for yielding biased but efficient estimators of parameters. This article demonstrates {{that it can}} be utilized for overcoming the inconsistency of least squares estimators in measurement error models and therefrom providing a class of <b>consistent</b> <b>estimators.</b> Inconsistency Measurement errors Stein procedure...|$|R
50|$|Other {{desirable}} {{properties for}} estimators include: UMVUE estimators {{that have the}} lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and <b>consistent</b> <b>estimators</b> which converges in probability to the true value of such parameter.|$|R
