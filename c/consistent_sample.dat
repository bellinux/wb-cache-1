50|813|Public
50|$|Concentric nebulizers have {{a central}} {{capillary}} with the liquid and an outer capillary with the gas. The gas draws the liquid into the gas stream through induction, and the liquid is broken into a fine mist {{as it moves}} into the gas stream. In theory, the gas and liquid may be switched with the gas {{in the center and}} the liquid in the outer capillary, but generally they work better with the gas outside and the liquid inside. The first Canadian concentric patent was Canadian Patent #2405 of April 18, 1873. It was designed to deliver a better spray of oil into a burner. The design is larger but essentially the same as modern analytical nebulizers. The first one developed for spectrometers was a glass design developed by Dr. Meinhard of California in 1973. His design enabled early ICP users to have a <b>consistent</b> <b>sample</b> introduction nebulizer, but it plugged easily. Today many companies produce glass concentrics, and since 1997, Teflon concentrics have become available.|$|E
40|$|Abstract: Given two {{dendrogram}}s (rooted tree diagrams) {{which have}} some {{but not all}} of their basepoints in common, a supertree is a dendrogram from which each of the original trees can be regarded as samples The distinction is made between inconsistent and <b>consistent</b> <b>sample</b> trees, defined by whether or not the samples provide contradictory information about the supertree An algorithm for obtaining the strict consensus supertree of two <b>consistent</b> <b>sample</b> trees is presented, as are procedures for merging two inconsistent sample trees Some suggestions for future work are mad...|$|E
30|$|More {{research}} is needed to fully elucidate the influence of exercise on CAR and address a number of gaps in the literature, including controlling exercise load, <b>consistent</b> <b>sample</b> collection, and CAR calculation and analysis.|$|E
40|$|There {{are many}} good methods for {{sampling}} Markov chains via streams of independent U[0, 1] random variables. Recently some non-random and some random but dependent driving sequences {{have been shown}} to result in <b>consistent</b> Markov chain <b>sampling,</b> sometimes with considerably improved accuracy. The key to <b>consistent</b> <b>sampling</b> is for the driving sequence to be completely uniformly distributed (CUD) or weakly CUD. This paper gives some sufficient conditions for an infinite sequence to be (W) CUD. The earlier theory did not incorporate acceptance-rejection sampling. We show by a coupling argument that a strategy due to Liao (1998) for inserting IID points into a WCUD sequence leads to <b>consistent</b> <b>sampling.</b> We also introduce a notion of (W) CUD triangular arrays for finite samples, and show that a lattice sampling construction of Niederreiter (1977) produces CUD triangular arrays. ...|$|R
40|$|Abstract. <b>Consistent</b> <b>sampling</b> is a {{technique}} for specifying, in small space, a subset S of a potentially large universe U such that the elements in S satisfy a suitably chosen sampling condition. Given a subset I ⊆ U {{it should be possible}} to quickly compute I ∩ S, i. e., the elements in I satisfying the <b>sampling</b> condition. <b>Consistent</b> <b>sampling</b> has important applications in similarity estimation, and estimation of the number of distinct items in a data stream. In this paper we generalize <b>consistent</b> <b>sampling</b> to the setting where we are interested in sampling size-k subsets occurring in some set in a collection of sets of bounded size b, where k is a small integer. This can be done by applying standard <b>consistent</b> <b>sampling</b> to the k-subsets of each set, but that approach requires time Θ(bk). Using a carefully designed hash function, for a given sampling probability p ∈ (0, 1], we show how to improve the time complexity to Θ(bdk/ 2 e log log b + pbk) in expectation, while maintaining strong concentration bounds for the sample. The space usage of our method is Θ(bdk/ 4 e). We demonstrate the utility of our technique by applying it to several well-studied data mining problems. We show how to efficiently estimate the number of frequent k-itemsets in a stream of transactions and the number of bipartite cliques in a graph given as incidence stream. Further, building upon a recent work by Campagna et al., we show that our approach can be applied to frequent itemset mining in a parallel or distributed setting. We also present applications in graph stream mining. ...|$|R
40|$|<b>Consistent</b> <b>sampling</b> is a {{technique}} for specifying, in small space, a subset S of a potentially large universe U such that the elements in S satisfy a suitably chosen sampling condition. Given a subset I ⊆ U {{it should be possible}} to quickly compute I ∩ S, i. e., the elements in I satisfying the <b>sampling</b> condition. <b>Consistent</b> <b>sampling</b> has important applications in similarity estimation, and estimation of the number of distinct items in a data stream. In this paper we generalize <b>consistent</b> <b>sampling</b> to the setting where we are interested in sampling size-k subsets occurring in some set in a collection of sets of bounded size b, where k is a small integer. This can be done by applying standard <b>consistent</b> <b>sampling</b> to the k-subsets of each set, but that approach requires time Θ(b k). Using a carefully designed hash function, for a given sampling probability p ∈ (0, 1], we show how to improve the time complexity to Θ(b ⌈k/ 2 ⌉ log log b + pb k) in expectation, while maintaining strong concentration bounds for the sample. The space usage of our method is Θ(b ⌈k/ 4 ⌉). We demonstrate the utility of our technique by applying it to several well-studied data mining problems. We show how to efficiently estimate the number of frequent k-itemsets in a stream of transactions and the number of bipartite cliques in a graph given as incidence stream. Further, building upon a recent work by Campagna et al., we show that our approach can be applied to frequent itemset mining in a parallel or distributed setting...|$|R
40|$|The limit {{behavior}} of the conditional probability of error of linear and quadratic discriminant analyses is studied under wide assumptions on the class conditional distributions. Results obtained may help to explain analytically the behavior in applications of linear and quadratic discrimination techniques. Bayes error Conditional probability of misclassification <b>Consistent</b> <b>sample</b> discriminant rules Inverse regression models Plug-in sample discriminant rules...|$|E
3000|$|... 14 An {{earlier version}} of this paper (Lundberg, 2010) {{reported}} models in which openness remained significant in the male marriage models for the younger cohort. The minor discrepancies between these versions of the empirical results are due to changes in sampling (the small guestworker sample was excluded in the earlier version, and this version uses a <b>consistent</b> <b>sample</b> with non-missing variables for all models).|$|E
30|$|We use the PRODY {{measures}} {{calculated by}} Hausmann et al. (2007). According {{to the authors}} they “constructed the PRODY measure for a <b>consistent</b> <b>sample</b> of countries that reported trade data {{in each of the}} years 1999 – 2001. These indexes are the result of an average of 3 years” (Hausmann et al. 2007). Because the chosen years are previous to the Chinese accession to the World Trade Organization, the possibility of a downward bias in the ranking of the machinery goods (in particular, final goods, given the increase in multinationals assembling their final products China) is minimized.|$|E
40|$|These Guidelines for the Study of the Epibenthos of Subtidal Environments {{document}} a {{range of}} sampling gears and procedures for epibenthos studies that meet a variety of needs. The importance of adopting <b>consistent</b> <b>sampling</b> and analytical practices is highlighted. Emphasis is placed on ship‐based techniques for surveys of coastal and offshore shelf environments, but diver‐assisted surveys are also considered...|$|R
40|$|The {{management}} of communication networks increasingly requires detailed knowledge of network usage, acquired by direct measurement. We {{report on the}} design and implementation of a backend system for Trajectory Sampling, a method for <b>consistent</b> <b>sampling</b> of packets across a network domain. This Trajectory Engine collects trajectory samples and stores them after appropriate preprocessing. It provides a querying and visualization tool to aid in traffic engineering and troubleshooting...|$|R
40|$|Locality-sensitive hashing (LSH) is a {{fundamental}} technique for similarity search and similarity estimation in high-dimensional spaces. The basic idea is that similar objects should produce hash collisions with probability significantly larger than objects with low similarity. We consider LSH for objects that can be represented as point sets in either one or two dimensions. To make the point sets finite size we consider the subset of points on a grid. Directly applying LSH (e. g. min-wise hashing) to these point sets would require time proportional {{to the number of}} points. We seek to achieve time that is much lower than direct approaches. Technically, we introduce new primitives for range-efficient <b>consistent</b> <b>sampling</b> (of independent interest), and show how to turn such samples into LSH values. Another application of our technique is a data structure for quickly estimating the size of the intersection or union of a set of preprocessed polygons. Curiously, our <b>consistent</b> <b>sampling</b> method uses transformation to a geometric problem. Comment: A shorter version appears in Proceedings of ISAAC 201...|$|R
40|$|We {{present an}} {{embedding}} of stochastic optimal control problems, of {{the so called}} path integral form, into reproducing kernel Hilbert spaces. Using <b>consistent,</b> <b>sample</b> based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e. g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided. ...|$|E
3000|$|... 9 Of the 12, 686 {{individuals}} {{interviewed in}} the initial 1979 wave, these data provide information for respondents on a yearly basis from 1979 to 1994 and biyearly afterwards. The initial wave is comprised of a core civilian cross-section of 6, 111 and an oversample of 5, 295 black, Hispanic, and economically disadvantaged individuals born between January 1, 1957 and Dec. 31 1964. This is further supplemented by a military sample of 1, 280 individuals born in the same period. We only keep the core cross-sectional sample of the NLSY 79 {{in order to maintain}} a <b>consistent</b> <b>sample</b> between the NLSY 79 and the CPS.|$|E
40|$|High-energy zero-norm states (HZNS) {{in the old}} {{covariant}} first quantized (OCFQ) {{spectrum of}} the 26 D open bosonic string are used to derive infinitely many linear relations among 4 -point functions of different string states. As a result, ratios among all high-energy scattering amplitudes of four arbitrary string states can be calculated algebraically and the leading order amplitudes can be {{expressed in terms of}} that of four tachyons as conjectured by Gross in 1988. A dual calculation can also be performed and equivalent results are obtained by taking the high-energy limit of Virasoro constraints. Finally, as a <b>consistent</b> <b>sample</b> calculation, we compute all high-energy scattering amplitudes of three tachyons and one massive state at the leading order by saddle-point approximation to justify our results...|$|E
40|$|We first {{introduce}} {{the concept of}} <b>consistent</b> <b>sampling</b> in infinite dimensional spaces and derive a general mathematical framework. The underlying linear reconstruction scheme coincides with the so-called oblique projection, which turns into an ordinary orthogonal projection when adapting the inner product. The inner product of interest is, in general, not unique. We characterize the inner products and corresponding positive operators for which the new geometrical interpretation applies. Finally, we study explicit constructions of such positive operators by means of Riesz bases. 1 <b>Consistent</b> <b>Sampling</b> Suppose we are given measurements ci of an unknown signal f belonging to a linear class H endowed with an inner product 〈·, ·〉. The measurements {{are assumed to be}} of the form ci = 〈f, si〉 for some function f ∈ H and a set of sampling vector {si} that span a subspace S ⊆ H. We want to reconstruct f from the observations ci using a given set of reconstruction vectors {wi} that span a subspace W ⊆ H [1, 4]. For designing the reconstruction algorithm we start with the following natural requirements on the sampling and reconstruction: 1. uniqueness of signal ˆ f ∈ W with 〈 ˆ f, si 〉 = ci, 2. <b>consistent</b> <b>sampling</b> (interpolation) [6] in the sense that 〈 ˆ f, si 〉 = 〈f, si 〉. The first requirement is a requirement on the sampling process. Specifically, we want the sampling vectors si to be such that if 〈f 1, si 〉 = 〈f 2, si 〉 for all i, where f 1, f 2 ∈ W, then f 1 = f 2 or, equivalently, To satisfy (1), we must have that ∀f 1, f 2 ∈ W, 〈f 1 − f 2, si 〉 = 0 ⇒ f 1 = f 2. (1...|$|R
40|$|Document {{sketching}} using Jaccard similarity {{has been}} a workable effective technique in reducing near-duplicates in Web page and image search results, and has also proven useful in file system synchronization, compression and learning applications [6, 4, 5]. Min-wise sampling {{can be used to}} derive an unbiased estimator for Jaccard similarity and taking a few hundred independent <b>consistent</b> <b>samples</b> leads to compact sketches which provide good estimates of pairwise-similarity. Early sketching papers handled weighted similarity, for integer weights, by trans-forming an element of weight w into w elements of unit weight, each requiring their own hash function evaluation in the <b>consistent</b> <b>sampling.</b> Subsequent work [12, 19, 14] removed the integer weight restric-tion, and showed how to produce samples using a constant number of hash evaluations for any element, independent of its weight. Another drastic speedup for sketch computations was given by Li, Owen and Zhang [17] who showed how to compute such (near-) independent samples in one shot, requiring only a constant number of hash function evaluations per element. Unfortunately this latter improvement works only for the unweighted case. In this paper we give a simple, fast and accurate procedure which reduces weighted sets to unweighte...|$|R
40|$|Database {{sampling}} {{is widely}} used in many database applications when, for eciency reasons, an entire database cannot be used. This paper analyses the use of <b>consistent</b> <b>sampling,</b> that is, sampling according to certain criteria (e. g. integrity constraints) {{used to evaluate the}} consistency of the resulting sample. This alternative to random sampling, the most common sampling strategy, is particularly appropriate in the context of constructing prototype databases to support Information System Development. The paper rstly presents a framework for evaluation of prototype database construction methods. Then a general description of the <b>Consistent</b> Database <b>Sampling</b> Process is introduced. Finally the paper outlines a sampling tool which implements this Process. Keywords: Information System, Database Prototyping, Database Sampling, Integrity Constraints, Legacy Migration 1 Introduction Database Sampling is commonly used {{in a wide range of}} applications when the size of the database makes impr [...] ...|$|R
40|$|This study {{investigates the}} impact of {{choosing}} a particular strategic focus or foci on operational productivity {{by means of the}} Malmquist Productivity Index as applied to a <b>consistent</b> <b>sample</b> (for the period 1999 - 2003) of 69 general freight motor carriers. The results show that general freight motor carriers, regardless of the strategic focus or foci pursued, did not link these strategic positions to an operational posture that reflected both operating efficiency and technological change. An interesting bifurcation is found with regard to the strategic foci of the LTL market niche and firm size growth, with the former of these two strategic foci having a positive impact on operating efficiency and the latter having a positive impact on technological change. Motor carriers Operating efficiency Technological change Malmquist productivity index Strategic focus...|$|E
40|$|Most {{existing}} correlation filter-based tracking algorithms, {{which use}} fixed patches and cyclic shifts as training and detection measures, {{assume that the}} training samples are reliable and ignore the inconsistencies between training samples and detection samples. We propose to construct and study a consistently sampled correlation filter with space anisotropic regularization (CSSAR) to solve these two problems simultaneously. Our approach constructs a spatiotemporally <b>consistent</b> <b>sample</b> strategy to alleviate the redundancies in training samples caused by the cyclical shifts, eliminate the inconsistencies between training samples and detection samples, and introduce space anisotropic regularization to constrain the correlation filter for alleviating drift caused by occlusion. Moreover, an optimization strategy based on the Gauss-Seidel method was developed for obtaining robust and efficient online learning. Both qualitative and quantitative evaluations demonstrate that our tracker outperforms state-of-the-art trackers in object tracking benchmarks (OTBs) ...|$|E
40|$|This paper {{provides}} {{a review of}} the current knowledge of temporomandibular joint total replacement systems. An electronic Medline search was performed to identify all the relevant English-language, peer-reviewed articles published during 1990 - 2006. Twenty-eight references were considered for review, seven of which were reviews, 17 clinical trials or case series, and four single-patient case reports. Therapeutic outcomes were encouraging for all three total prosthetic systems for which follow-up data from a <b>consistent</b> <b>sample</b> of patients exist. A lack of homogeneity between studies in patient selection and indications for the intervention was noted. A better integration between clinical and research settings is needed to achieve a standardized definition of the rationale and indications for total temporomandibular joint replacement. Findings from the available studies are promising, and need to be confirmed by multicenter trials taking into account interoperator variability...|$|E
40|$|Report on the {{condition}} of the Nation’s Lakes � Statistically valid design that represents {{the condition}} of all lakes � Regional and national estimates of the condition of lakes, option for State-scale estimates � Use <b>consistent</b> <b>sampling</b> and analysis procedures to ensure the results can be compared across the country � Promote State and Tribal capacity for monitoring and assessment � Promote collaboration across jurisdictional boundaries in the assessment of water quality 4 - 15 - 09 National Lakes Assessment – Chicago Lakes Meeting – NLA Worksho...|$|R
40|$|Although many {{foundations}} faced continuing asset {{losses in}} 2002, in general they held steady to their core programs. Among {{the more than}} 1, 000 larger foundations included in the Foundation Center's 2002 grants database sample, shares of funding for major fields from the arts, to education, to human services remained largely <b>consistent.</b> <b>Sampled</b> foundations provided a record high share of operating support grants, which help sustain nonprofits in a difficult economic environment. At the same time, foundations continued to reduce their spending for endowments and other capital grants...|$|R
40|$|The {{rheometric}} {{data measured}} using three various sensor geometries (PP 60, ZZ 40 DIN and KK-sensor) are compared in this contribution {{at the case}} of water solution of polymer welan. The KK-sensor, constructed at our department, combines advantages of geometry both plate-plate and concentric cylinder geometry. It is shown that KK-sensor is comparable to commercial sensors. It is presented for water kaolin dispersions that apparent wall slip does not disappeared neither for low consistent dispersions. The values of slip coefficients are even higher {{in the case of}} less <b>consistent</b> <b>samples...</b>|$|R
40|$|Premise of the study: The {{chemical}} {{diversity of}} land plants ensures {{that no single}} DNA isolation method results in high yield and purity with little effort for all species. Here we evaluate a new technique originally developed for forensic science, based on MagnaCel paramagnetic cellulose particles (PMC), to determine its efficacy in extracting DNA from 25 plant species representing 21 families and 15 orders. Methods and Results: Yield and purity of DNA isolated by PMC, DNeasy Plant Mini Kit (silica column), and cetyltrimethylammonium bromide (CTAB) methods were compared among four individuals for each of 25 plant species. PMC gave a two-fold advantage in average yield, and the relative advantage of the PMC method was greatest for samples with the lowest DNA yields. PMC also produced more <b>consistent</b> <b>sample</b> purity based on absorbance ratios at 260 : 280 and 260 : 230 nm. Conclusions: PMC technology is a promising alternative for plant DNA isolation...|$|E
40|$|High-energy {{limit of}} zero-norm states (HZNS) {{in the old}} {{covariant}} first quantized (OCFQ) spectrum of the 26 D open bosonic string, together with the assumption of a smooth behavior of string theory in this limit, are used to derive infinitely many linear relations among the leading high-energy, fixed angle behavior of four point functions of different string states. As a result, ratios among all high-energy scattering amplitudes of four arbitrary string states can be calculated algebraically and the leading order amplitudes can be {{expressed in terms of}} that of four tachyons as conjectured by Gross in 1988. A dual calculation can also be performed and equivalent results are obtained by taking the high-energy limit of Virasoro constraints. Finally, as a <b>consistent</b> <b>sample</b> calculation, we compute all high-energy scattering amplitudes of three tachyons and one massive state at the leading order by saddle-point approximation to justify our results. Comment: 10 pages, no figure, modifications of text and reference...|$|E
40|$|The {{extent of}} stroke damage in {{patients}} affects {{the range of}} subsequent pathophysiological responses that influence recovery. Here we investigate the effect of lesion size on development of new blood vessels as well as inflammation and scar formation and cellular responses within the subventricular zone (SVZ) following transient focal ischemia in rats (n = 34). Endothelin- 1 -induced stroke resulted in neurological deficits detected between 1 and 7 days (P, 0. 001), but significant recovery was observed beyond this time. MCID image analysis revealed varying degrees of damage in the ipsilateral cortex and striatum with infarct volumes ranging from 0. 76 – 77 mm 3 after 14 days, where larger infarct volumes correlated with greater functional deficits up to 7 days (r = 0. 53, P, 0. 05). Point counting of blood vessels within <b>consistent</b> <b>sample</b> regions revealed that increased vessel numbers correlated significantly with larger infarct volumes 14 days post-stroke in the cor...|$|E
40|$|The {{general theory}} of <b>consistent</b> <b>sampling</b> by Unser and Al-droubi is applied to {{reconstruction}} of color images from their non-ideally observed data through Bayer and honeycomb color filter arrays. An image {{is assumed to be}} represented by bivariate box splines over the orthogonal or hexagonal mesh for the case of Bayer or honeycomb arrays, respec-tively. Then its reconstruction is made by computing the coefficients for its box spline representation so that mathe-matical re-observation of the spline representation through the array be the same as the observed data. 1...|$|R
30|$|Since {{samples from}} the {{interface}} are utilized to qualitatively and quantitatively determine the interface conditions {{and the effect of}} salinity on the interfacial behavior, {{it is important to have}} <b>consistent</b> <b>sampling.</b> For this purpose all samples were gently collected from the same position in crude oil and close to the interface. Each sample was then photographed using a Dino AM- 351 digital microscope with adjustable magnification up to 600 ×. The image processing toolbox of MATLAB programming software was used to measure the size distribution, mean size and volume fraction of water droplets and asphaltene agglomerates.|$|R
30|$|In {{addition}} to trends toward {{more efficient and}} more nationally <b>consistent</b> <b>sampling</b> and compilation, there were social and legislative developments that motivated the inventory to move beyond its roots in timber to assessment of {{a wider variety of}} forest resources. Public appreciation of non-timber forest resources such as water, habitat, and recreation use grew during the 1970 s. The Forest and Rangeland Renewable Resources Planning Act of 1974 (RPA) broadened the mandate of the Forest Survey to include monitoring of a range of non-timber resources (U.S. Department of Agriculture 1992). Non-timber resource data from the inventory, such as habitat area estimates, informed several forest management debates, e.g., informed debates about management of public land (Bolsinger and Waddell 1993).|$|R
40|$|A low-cost, robust, {{and simple}} {{mechanism}} to measure hemoglobin {{would play a}} critical role in the modern health infrastructure. <b>Consistent</b> <b>sample</b> acquisition has been a long-standing technical hurdle for photometer-based portable hemoglobin detectors which rely on micro cuvettes and dry chemistry. Any particulates (e. g. intact red blood cells (RBCs), microbubbles, etc.) in a cuvette's sensing area drastically impact optical absorption profile, and commercial hemoglobinometers lack the ability to automatically detect faulty samples. We present the ground-up development of a portable, low-cost and open platform with equivalent accuracy to medical-grade devices, with the addition of CNN-based image processing for rapid sample viability prechecks. The developed platform has demonstrated precision to the nearest $ 0. 18 [g/dL]$ of hemoglobin, an R^ 2 = 0. 945 correlation to hemoglobin absorption curves reported in literature, and a 97 % detection accuracy of poorly-prepared samples. We see the developed hemoglobin device/ML platform having massive implications in rural medicine, and consider it an excellent springboard for robust deep learning optical spectroscopy: a currently untapped source of data for detection of countless analytes...|$|E
40|$|Just over 22 {{percent of}} {{individuals}} who owned a Traditional or Roth individual retirement account (IRA) took a withdrawal in 2013. The overall IRA withdrawal percentage was largely driven by activity among individuals ages 70 - or older owning a Traditional IRA—the group required to make withdrawals under federal required minimum distribution (RMD) rules for IRA owners beyond that age. In contrast, among individuals under age 60, 10 percent or fewer had a withdrawal. For those at the RMD age, the withdrawal rates at the median appeared close to the amount that was required to be withdrawn, though some were significantly more. For instance, looking at the <b>consistent</b> <b>sample</b> in the EBRI IRA Database, approximately 25 percent of those 71 or older took a withdrawal amount in excess of that required by law for Traditional IRAs. Among those ages 70 or older, withdrawal rates over a four-year period showed that most individuals were withdrawing {{at a rate that}} was {{likely to be able to}} sustain some level of post-retirement income from IRAs as the individual continued to age...|$|E
40|$|In 2003, a National Electrostatics Corporation (NEC) 5 MV tandem {{accelerator}} mass spectrometer was installed at SUERC, providing the radiocarbon laboratory with 14 C measurements to 4 – 5 ‰ repeatability. In 2007, a 250 kV single-stage accelerator mass spectrometer (SSAMS) {{was added to}} provide additional 14 C capability {{and is now the}} preferred system for 14 C analysis. Changes to the technology and to our operations are evident in our copious quality assurance data: typically, we now use the 134 -position MC-SNICS source, which is filled to capacity. Measurement of standards shows that spectrometer running without the complication of on-line δ 13 C evaluation is a good operational compromise. Currently, 3 ‰ 14 C/ 13 C measurements are routinely achieved for samples up to nearly 3 half-lives old by <b>consistent</b> <b>sample</b> preparation and an automated data acquisition algorithm with sample random access for measurement repeats. Background and known-age standard data are presented for the period 2003 – 2008 for the 5 MV system and 2007 – 2008 for the SSAMS, to demonstrate the improvements in data quality...|$|E
40|$|A {{theory is}} {{developed}} for the convergence of the closed-loop solution to infinite-dimensional discrete-time linear-quadratic regulator (LQR) problems on the infinite time interval to the solution of a corresponding continuous-time LQR problem as {{the length of the}} sampling interval tends toward zero. Convergence of solutions to the operator algebraic Riccati equation and corresponding optimal feedback control gains is guaranteed under appropriate uniform stabilizability and detectability conditions and <b>consistent</b> <b>sampling.</b> Also presented are numerical results involving the optimal LQ control of a heat or diffusion equation, a hereditary or delay differential equation, and a hybrid system of ordinary and partial differential equations describing the transverse vibration of a cantilevered Voigt-Kelvin viscoelastic beam with tip mass...|$|R
50|$|Accurately {{taken and}} labeled samples with an unequivocal {{chain of custody}} are {{essential}} to all operations. Effective and <b>consistent</b> cargo <b>sampling</b> requires a specialized staff of trained individuals {{who are responsible for}} taking such samples and transporting them to the corresponding testing laboratories.|$|R
40|$|Accurate {{differentiation}} between tropical forest and savannah ecosystems in the fossil pollen record is {{hampered by the}} combination of: i) poor taxonomic resolution in pollen identification, and ii) the high species diversity of many lowland tropical families, i. e. with many different growth forms living in numerous environmental settings. These barriers to interpreting the fossil record hinder {{our understanding of the}} past distributions of different Neotropical ecosystems and consequently cloud our knowledge of past climatic, biodiversity and carbon storage patterns. Modern pollen studies facilitate an improved understanding of how ecosystems are represented by the pollen their plants produce and therefore aid interpretation of fossil pollen records. To understand how to differentiate ecosystems palynologically, it is essential that a <b>consistent</b> <b>sampling</b> method is used across ecosystems. However, to date, modern pollen studies from tropical South America have employed a variety of methodologies (e. g. pollen traps, moss polsters, soil samples). In this paper, we present the first modern pollen study from the Neotropics to examine the modern pollen rain from moist evergreen tropical forest (METF), semi-deciduous dry tropical forest (SDTF) and wooded savannah (cerradão) using a <b>consistent</b> <b>sampling</b> methodology (pollen traps). Pollen rain was sampled annually in September for the years 1999 – 2001 from within permanent vegetation study plots in, or near, the Noel Kempff Mercado National Park (NKMNP), Bolivia. Comparison of the modern pollen rain within these plots with detailed floristic inventories allowed estimates of the relative pollen productivity and dispersal for individual taxa to be made (% pollen/% vegetation or ‘p/v’). The applicability of these data to interpreting fossil records from lake sediments was then explored by comparison with pollen assemblages obtained from five lake surface samples...|$|R
