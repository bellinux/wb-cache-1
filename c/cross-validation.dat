10000|122|Public
25|$|Class {{prediction}} {{is more difficult}} than class discovery, but it allows one to answer questions of direct clinical significance such as, given this profile, what is the probability that this patient will respond to this drug? This requires many examples of profiles that responded and did not respond, as well as <b>cross-validation</b> techniques to discriminate between them.|$|E
25|$|Another {{class of}} methods assesses whether the {{inference}} {{was successful in}} light of the given observed data, for example, by comparing the posterior predictive distribution of summary statistics to the summary statistics observed. Beyond that, <b>cross-validation</b> techniques and predictive checks represent promising future strategies to evaluate the stability and out-of-sample predictive validity of ABC inferences. This is particularly important when modeling large data sets, because then the posterior support of a particular model can appear overwhelmingly conclusive, even if all proposed models in fact are poor representations of the stochastic system underlying the observation data. Out-of-sample predictive checks can reveal potential systematic biases within a model and provide clues on to how to improve its structure or parametrization.|$|E
25|$|Applications whose goal is {{to create}} a system that generalizes well to unseen examples, face the {{possibility}} of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use <b>cross-validation</b> and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the {{goal is to}} minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.|$|E
40|$|Abstract. This paper compares several model {{selection}} methods, {{based on}} experimental estimates of their generalization errors. Experiments {{in the context}} of nonlinear time series prediction by Radial-Basis Function Networks show the superiority of the bootstrap methodology over classical <b>cross-validations</b> and leave-one-out. ...|$|R
40|$|Economically {{important}} reproduction {{traits in}} sheep, such as number of lambs weaned and litter size, are expressed only in females {{and later in}} life after most selection decisions are made, which makes them ideal candidates for genomic selection. Accurate genomic predictions would lead to greater genetic gain for these traits by enabling accurate selection of young rams with high genetic merit. The {{aim of this study}} was to design and evaluate the accuracy of a genomic prediction method for female reproduction in sheep using daughter trait deviations (DTD) for sires and ewe phenotypes (when individual ewes were genotyped) for three reproduction traits: number of lambs born (NLB), litter size (LSIZE) and number of lambs weaned. Genomic best linear unbiased prediction (GBLUP), BayesR and pedigree BLUP analyses of the three reproduction traits measured on 5340 sheep (4503 ewes and 837 sires) with real and imputed genotypes for 510 174 SNPs were performed. The prediction of breeding values using both sire and ewe trait records was validated in Merino sheep. Prediction accuracy was evaluated by across sire family and random <b>cross-validations.</b> Accuracies of genomic estimated breeding values (GEBVs) were assessed as the mean Pearson correlation adjusted by the accuracy of the input phenotypes. The addition of sire DTD into the prediction analysis resulted in higher accuracies compared with using only ewe records in genomic predictions or pedigree BLUP. Using GBLUP, the average accuracy based on the combined records (ewes and sire DTD) was 0. 43 across traits, but the accuracies varied by trait and type of <b>cross-validations.</b> The accuracies of GEBVs from random <b>cross-validations</b> (range 0. 17 – 0. 61) were higher than were those from sire family <b>cross-validations</b> (range 0. 00 – 0. 51). The GEBV accuracies of 0. 41 – 0. 54 for NLB and LSIZE based on the combined records were amongst the highest in the study. Although BayesR was not significantly different from GBLUP in prediction accuracy, it identified several candidate genes which are known to be associated with NLB and LSIZE. The approach provides a way to make use of all data available in genomic prediction for traits that have limited recording...|$|R
30|$|The {{number of}} {{measurements}} or dimensions is an important parameter in the proposed sensing method. By the statistical experiments, we compare the recognition performances in different numbers of dimension. For {{the collection of the}} reference set, we randomly select half of the samples from each motion set as the template samples, and the remaining samples are used for test and analysis. The following statistical experimental results are based on 30 <b>cross-validations.</b>|$|R
2500|$|The KL can be {{estimated}} using a <b>cross-validation</b> method, although KL <b>cross-validation</b> selectors can be sub-optimal even if it remains consistent for bounded density functions. MH selectors have been briefly examined in the literature.|$|E
2500|$|The optimal regularization {{parameter}} [...] {{is usually}} unknown and often in practical problems {{is determined by}} an ad hoc method. A possible approach relies on the Bayesian interpretation described below. Other approaches include the discrepancy principle, <b>cross-validation,</b> L-curve method, restricted maximum likelihood and unbiased predictive risk estimator. Grace Wahba proved that the optimal parameter, {{in the sense of}} leave-one-out <b>cross-validation</b> minimizes: ...|$|E
2500|$|A common {{choice is}} a Gaussian kernel, {{which has a}} single {{parameter}} '. The best combination of C and [...] is often selected by a grid search with exponentially growing sequences of C and ', for example, [...] Typically, each combination of parameter choices is checked using cross validation, and the parameters with best <b>cross-validation</b> accuracy are picked. Alternatively, recent work in Bayesian optimization {{can be used to}} select C and ' , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.|$|E
40|$|Figure 6 - Multivariate {{regression}} tree {{based on}} sum-of-squares depicting differences in beetle assemblages among experimental plots where forest was a clearcut with stem-only harvested (SOH) b clearcut with whole-tree harvested (WTH), and c uncut forest (Control). The tree was {{selected based on}} 935 / 1000 <b>cross-validations</b> and explains 64 % of the variance. Both experimental treatment and deadwood volumes provided equivalent improvement at each split. We have labelled splits using experimental treatments...|$|R
40|$|AbstractSimultaneous multiclass {{classification}} of tumor types {{is essential for}} future clinical implementations of microarray-based cancer diagnosis. In this study, we have combined genetic algorithms (GAs) and all paired support vector machines (SVMs) for multiclass cancer identification. The predictive features have been selected through iterative SVMs/GAs, and recursive feature elimination post-processing steps, leading to a very compact cancer-related predictive gene set. Leave-one-out <b>cross-validations</b> yielded accuracies of 87. 93 % for the eight-class and 85. 19 % for the fourteen-class cancer classifications, outperforming the results derived from previously published methods...|$|R
40|$|Medium density {{fiberboard}} (MDF) is an {{engineered wood}} product formed by breaking down selected lignin-cellulosic material residuals into fibers, combining it with wax and a resin binder, and then forming panels by applying high temperature and pressure. Because the raw material in the industrial process is ever-changing, the panel industry requires methods for monitoring the composition of their products. The {{aim of this study}} was to estimate the ratio of sugarcane (SC) bagasse to Eucalyptus wood in MDF panels using near infrared (NIR) spectroscopy. Principal component analysis (PCA) and partial least square (PLS) regressions were performed. MDF panels having different bagasse contents were easily distinguished from each other by the PCA of their NIR spectra with clearly different patterns of response. The PLS-R models for SC content of these MDF samples presented a strong coefficient of determination (0. 96) between the NIR-predicted and Lab-determined values and a low standard error of prediction (- 1. 5 %) in the <b>cross-validations.</b> A key role of resins (adhesives), cellulose, and lignin for such PLS-R calibrations was shown. PLS-DA model correctly classified ninety-four percent of MDF samples by <b>cross-validations</b> and ninety-eight percent of the panels by independent test set. These NIR-based models can be useful to quickly estimate sugarcane bagasse vs. Eucalyptus wood content ratio in unknown MDF samples and to verify the quality of these {{engineered wood product}}s in an online process. (Résumé d'auteur...|$|R
5000|$|Minimum <b>cross-validation</b> error: {{when trying}} to choose among hypotheses, select the {{hypothesis}} with the lowest <b>cross-validation</b> error. Although <b>cross-validation</b> {{may seem to be}} free of bias, the [...] "no free lunch" [...] theorems show that <b>cross-validation</b> must be biased.|$|E
50|$|Two {{types of}} <b>cross-validation</b> can be distinguished, {{exhaustive}} and non-exhaustive <b>cross-validation.</b>|$|E
50|$|When k = n (the {{number of}} observations), the k-fold <b>cross-validation</b> {{is exactly the}} leave-one-out <b>cross-validation.</b>|$|E
40|$|Capsule Discriminant {{functions}} {{based on}} morphometric variables provide a reliable method for sex identification of free-living and hacked young Ospreys. Aims To describe an easy, accurate and low-cost method for sex determination of fully grown nestling and fledgling Ospreys Pandion haliaetus based on morphometric measurements. Methods Four different measurements {{were taken in}} 114 birds (40 - 73 days old) and a DNA analysis, using PCR amplification, was carried out for sex identification. A forward stepwise discriminant analysis was performed to build the best explanatory discriminant models, which were subsequently validated using statistics and external samples. Results Our best discriminant function retained forearm and tarsus as the best predictor variables and classified 95. 1 % of the sample correctly, supported also by external <b>cross-validations</b> with both hacked and free-living birds. Moreover, a discriminant function with only forearm as predictor showed a similar high correct classification power (93. 4 %). Conclusions These discriminant functions {{can be used as}} a reliable and immediate method for sex determination of young Ospreys since they showed high discriminant accuracy, close to that of molecular procedures, and were supported by external <b>cross-validations,</b> both for free-living and hacked birds. Thus, these morphometric measurements should be considered as standard tools for future scientific studies and management of Osprey populations. © 2010 British Trust for Ornithology. Peer Reviewe...|$|R
40|$|Spaceborne SAR images the Earth’s surface {{through the}} {{ionosphere}} which is permanently disturbed by precipitations of solar and cosmic particles, atmospheric neutral winds, plasma instabilities, and etc. Recent {{studies showed that}} such irregularities can be imaged and quantified through the disturbances appearing on SAR images; {{but there were no}} <b>cross-validations.</b> In this paper we suggest coordinated measurements of structural and dynamic parameters of polar and equatorial ionospheres using GNSS network and ground based radars, e. g., incoherent scatter radar, along with SAR. The ground based observations will be compared to assess the ionospheric mapping ability of SAR...|$|R
40|$|International audienceWe aim at optimally {{combining}} {{air quality}} computations, from the Gaussian model ADMS Urban, and ground observations at urban scale. An ADMS simulation generated NO 2 concentration fields across Clermont-Ferrand (France) down to street level, every 3 [*]h {{for the full}} year 2008. A monitoring network composed of nine fixed stations provided hourly observations to be assimilated. Every 3 [*]h, we compute the so-called BLUE (best linear unbiased estimator), which is a concentration field merging ADMS outputs and ground observations. Its error variance {{is supposed to be}} minimal under given assumptions regarding the errors on observations and model simulations. A key step lies in the modeling of error covariances between the computed NO 2 concentrations across the city. We introduce a parameterized covariance which heavily relies on the road network. The covariance between two locations depends on the distance of each location to the road network and on the distance between the locations along the road network. Efficient parameters for the covariances are primarily chosen according to prior assumptions, χ 2 diagnosis and leave-one-out <b>cross-validations.</b> According to the <b>cross-validations,</b> the improvements due to the assimilation seem moderately far from the observation network, but the root mean square error roughly decreases by 30 - 50 % in the main city where the station density is high. The method is computationally tractable for the generation of improved concentration fields over a long period, or for day-to-day forecasts...|$|R
50|$|Leave-one-out <b>cross-validation</b> (LOOCV) is a {{particular}} case of leave-p-out <b>cross-validation</b> with p = 1. The process looks similar to jackknife, however with <b>cross-validation</b> you compute a statistic on the left-out sample(s), while with jackknifing you compute a statistic from the kept samples only.|$|E
5000|$|LOO <b>cross-validation</b> {{does not}} have the same problem of {{excessive}} compute time as general LpO <b>cross-validation</b> because [...]|$|E
50|$|Exhaustive <b>cross-validation</b> {{methods are}} <b>cross-validation</b> methods which learn and test on all {{possible}} ways {{to divide the}} original sample into a training and a validation set.|$|E
40|$|This paper {{studies the}} {{detection}} of bird calls in audio segments using stacked convolutional and recurrent neural networks. Data augmentation by blocks mixing and domain adaptation using a novel method of test mixing are proposed and evaluated in regard to making the method robust to unseen data. The contributions of two kinds of acoustic features (dominant frequency and log mel-band energy) and their combinations are studied {{in the context of}} bird audio detection. Our best achieved AUC measure on five <b>cross-validations</b> of the development data is 95. 5 % and 88. 1 % on the unseen evaluation data. Comment: Accepted for European Signal Processing Conference 201...|$|R
40|$|MOTIVATION: High-throughput and {{high-resolution}} {{mass spectrometry}} instruments are increasingly used for disease classification and therapeutic guidance. However, {{the analysis of}} immense amount of data poses considerable challenges. We have therefore developed a novel method for dimensionality reduction and tested on a published ovarian high-resolution SELDI-TOF dataset. RESULTS: We have developed a four-step strategy for data preprocessing based on: (1) binning, (2) Kolmogorov-Smirnov test, (3) restriction of coefficient of variation and (4) wavelet analysis. Subsequently, support vector machines were used for classification. The developed method achieves an average sensitivity of 97. 38 % (sd = 0. 0125) and an average specificity of 93. 30 % (sd = 0. 0174) in 1000 independent k-fold <b>cross-validations,</b> where k = 2, [...] ., 10...|$|R
30|$|C-CENI {{adopts a}} K-means-based {{strategy}} for clustering. We first view each t_i^c as a data point on a one-dimensional space {{and assume that}} we have a predefined cluster number, K, which can be determined by <b>cross-validations.</b> Initially, we randomly select K centroids for each cluster. Next, at each iteration, we first assign each data point to the cluster of which centroid is the closest, then we recalculate the new centroid for each cluster by using the mean of data points in the cluster. We continue to do this until it converges. Similar to the “clusters” in I-CENI, a node’s cluster members in C-CENI may also change with respect to cascades, since the infection time of this node may vary a lot for different cascades.|$|R
5000|$|<b>Cross-validation</b> is an {{approach}} {{by which the}} sets of scientific data generated using two or more methods are critically assessed. [...] The <b>cross-validation</b> can be categorized as ...|$|E
50|$|One form of <b>cross-validation</b> {{leaves out}} a single {{observation}} at a time; {{this is similar}} to the jackknife. Another, K-fold <b>cross-validation,</b> splits the data into K subsets; each is held out in turn as the validation set.|$|E
5000|$|Most {{forms of}} <b>cross-validation</b> are {{straightforward}} to implement {{as long as}} an implementation of the prediction method being studied is available. In particular, the prediction method can be a [...] "black box" [...] - {{there is no need}} to have access to the internals of its implementation. If the prediction method is expensive to train, <b>cross-validation</b> can be very slow since the training must be carried out repeatedly. In some cases such as least squares and kernel regression, <b>cross-validation</b> can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast [...] "updating rules" [...] such as the Sherman-Morrison formula. However one must be careful to preserve the [...] "total blinding" [...] of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating <b>cross-validation</b> occurs in linear regression, where the results of <b>cross-validation</b> have a closed-form expression known as the prediction residual error sum of squares (PRESS).|$|E
40|$|The {{activity}} of labeling of documents {{according to their}} content is known as text categorization. Many experiments {{have been carried out}} to enhance text categorization by adding background knowledge to the document using knowledge repositories like Word Net, Open Project Directory (OPD), Wikipedia and Wikitology. In our previous work, we have carried out intensive experiments by extracting knowledge from Wikitology and evaluating the experiment on Support Vector Machine with 10 - fold <b>cross-validations.</b> The results clearly indicate Wikitology is far better than other knowledge bases. In this paper we are comparing Support Vector Machine (SVM) and Naïve Bayes (NB) classifiers under text enrichment through Wikitology. We validated results with 10 -fold cross validation and shown that NB gives an improvement of + 28. 78...|$|R
40|$|The ESA funded {{campaign}} BioSAR 2010 {{was carried}} out at the forestry test site Remningstorp in southern Sweden, in support to the BIOMASS satellite mission under study. Fully polarimetric SAR data were successfully acquired at L- and P-band using ONERA’s multi-frequency system SETHI. In addition with other data types gathered, e. g. LiDAR and in-situ measurements, the compiled data set {{will be used for}} analyses and comparisons with biomass estimation results obtained at the same test site in the campaign BioSAR 2007, in which DLR’s E-SAR made the SAR imaging. Detection of forest changes, robustness of biomass retrieval algorithms and long-term P-band coherence will be in focus as well as <b>cross-validations</b> between the two SAR sensors. Index Terms — SAR, backscatter, forest biomass, polarimetry, SETH...|$|R
40|$|International audienceTemperature {{estimation}} methods usually involve regression followed by kriging of residuals (residual kriging). Despite {{the performance of}} such models, there is invariably a residual which is not necessarily unpredictable because it may still be correlated in time. We set out to analyse such residuals through resort to autoregressive processes. It is shown that the optimal period varies depending on whether it is identified by functions of the form resd[*]=[*]f(resd− 1, resd− 2, [...] ., resd−p) or by partial correlations. Autoregressive processes significantly improve estimates, which are evaluated by <b>cross-validations.</b> Finally, the two following points are discussed: (1) the assumptions of the autoregressive model on the residuals (the assumptions of linearity, stationarity {{of space and time}} are verified empirically) and (2) the identification of the days for which the introduction of this model is really interesting...|$|R
50|$|The {{results from}} BMA {{can often be}} {{approximated}} by using <b>cross-validation</b> to select the best model from a bucket of models. Likewise, the results from BMC may {{be approximated by using}} <b>cross-validation</b> to select the best ensemble combination from a random sampling of possible weightings.|$|E
5000|$|... #Subtitle level 3: Leave-one-out <b>cross-validation</b> (CVloo) Stability ...|$|E
5000|$|... #Subtitle level 4: Minimizing <b>cross-validation</b> {{estimated}} squared error ...|$|E
40|$|Different geostatistical {{methods are}} used to {{interpolate}} the spatial distribution of the foliar magnesium content of Silver fir and Norway spruce in the Black Forest. The data analysed are from a monitoring survey carried out in 1994 in the forest of Baden-Württemberg, a federal state in the south-west region of Germany. In this survey many potential explanatory variables are collected. The aim {{of this paper is}} to identify the best prediction method that can be useful in the future for cause–effect studies and environmental modelling. At the same time, causal relationships between the response variable and the predictors are investigated. Therefore, geostatistical methods with lowest prediction errors which simultaneously provide the highest explanation value had to be identified. The performance of different methods is measured using <b>cross-validations</b> technique...|$|R
40|$|Exploiting the {{mathematical}} framework of favourability function modelling and of software designed for spatial target mapping, experiments are discussed {{to measure the}} effects of uncertainty of spatial support. A database {{that has been the}} focus of landslide hazard prediction and risk assessment of buildings, roads and land uses is reanalyzed gradually modifying the spatial characteristics of supporting patterns of the proposition of “presence of landslide occurrences. ” Boundary fuzziness of categorical mapping units and filtering of continuous fields, represent weakening of spatial relationships. Comparisons are made of ranges of ranks representing uncertainties of class membership in target patterns obtained by iterative <b>cross-validations.</b> The impacts of the different spatial uncertainties on risk assessment patterns are visualized using progressive combinations of uncertainties of target patterns to resolve risk equations and study the consequent changes in risk values...|$|R
40|$|A {{supervised}} learning method, support vector machine, {{was used to}} analyze the microsatellite marker dataset of the Collaborative Study on the Genetics of Alcoholism Problem 1 for the Genetic Analysis Workshop 14. Twelve binary-valued phenotype variables were chosen for analyses using the markers from all autosomal chromosomes. Using various polynomial kernel functions of the support vector machine and randomly divided genome regions, we were able to observe the association of some marker sets with the chosen phenotypes and thus reduce the size of the dataset. The successful classifications established with the chosen support vector machine kernel function had high levels of correctness for each prediction, e. g., 96 % in the fourfold <b>cross-validations.</b> However, owing to the limited sample data, {{we were not able to}} test the predictions of the classifiers in the new sample data...|$|R
