891|593|Public
25|$|The Google Earth's <b>cache</b> <b>size</b> {{is limited}} to 2000MB whereas World Wind has no limit on <b>cache</b> <b>size.</b> In Norkart Virtual Globe the disk cache can be set by the user.|$|E
25|$|The {{relationship}} between a CPU's <b>cache</b> <b>size</b> {{and the number of}} cache misses follows the Power law of cache misses.|$|E
25|$|When a {{compatible}} {{device is}} plugged in, the Windows AutoPlay dialog offers an additional option {{to use the}} flash drive {{to speed up the}} system; an additional ReadyBoost tab is added to the drive's properties dialog where the amount of space to be used can be configured. The minimum <b>cache</b> <b>size</b> is 250MB. In Vista or with FAT32 formatting of the drive, the maximum is 4GB. In Windows 7 with NTFS or exFAT formatting, the maximum <b>cache</b> <b>size</b> is 32GB per device. Windows Vista allows only one device to be used, while Windows 7 allows multiple caches, one per device, up to a total of 256GB.|$|E
50|$|The precise {{effect of}} {{inlining}} on cache performance is complicated. For small <b>cache</b> <b>sizes</b> (much {{smaller than the}} working set prior to expansion), the increased sequentiality dominates, and inlining improves cache performance. For <b>cache</b> <b>sizes</b> close to the working set, where inlining expands the working set so it no longer fits in cache, this dominates and cache performance decreases. For <b>cache</b> <b>sizes</b> larger than the working set, inlining has negligible impact on cache performance. Further, changes in cache design, such as load forwarding, can offset the increase in cache misses.|$|R
50|$|The three-level caches {{were used}} again {{first with the}} {{introduction}} of multiple processor cores, where the L3 cache was added to the CPU die. It became common for the total <b>cache</b> <b>sizes</b> to be increasingly larger in newer processor generations, and recently (as of 2011) {{it is not uncommon to}} find Level 3 <b>cache</b> <b>sizes</b> of tens of megabytes.|$|R
5000|$|Optional {{configurable}} caches (direct-mapped or 2-way set-associative, with {{a variety}} of <b>cache</b> <b>sizes</b> and arrangements) ...|$|R
25|$|Steppings with {{a reduced}} <b>cache</b> <b>size</b> use a {{separate}} naming scheme, {{which means that}} the releases are no longer in alphabetic order. Additional steppings have been used in internal and engineering samples, but are not listed in the tables.|$|E
25|$|The Core {{microarchitecture}} uses {{a number}} of steppings, which unlike previous microarchitectures not only represent incremental improvements but also different sets of features like <b>cache</b> <b>size</b> and low power modes. Most of these steppings are used across brands, typically by disabling some of the features and limiting clock frequencies on low-end chips.|$|E
25|$|Divide-and-conquer {{algorithms}} naturally {{tend to make}} {{efficient use}} of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache {{in this way is}} called cache-oblivious, because it does not contain the <b>cache</b> <b>size</b> as an explicit parameter.|$|E
5000|$|Pipeline {{processing}} (where one achieves {{the same}} effect as increasing the L1 <b>cache's</b> <b>size</b> by splitting one job into smaller chunks) ...|$|R
25|$|Many of the {{high-end}} Core 2 and Xeon processors use Multi-Chip Modules {{of two or}} three chips in order to get larger <b>cache</b> <b>sizes</b> or more than two cores.|$|R
40|$|This paper investigates {{one of the}} {{fundamental}} issues in cache-enabled heterogeneous networks (HetNets) : how many cache instances should be deployed at different base stations, in order to provide guaranteed service in a cost-effective manner. Specifically, we consider two-tier HetNets with hierarchical caching, where the most popular files are cached at small cell base stations (SBSs) while the less popular ones are cached at macro base stations (MBSs). For a given network cache deployment budget, the <b>cache</b> <b>sizes</b> for MBSs and SBSs are optimized to maximize network capacity while satisfying the file transmission rate requirements. As <b>cache</b> <b>sizes</b> of MBSs and SBSs affect the traffic load distribution, inter-tier traffic steering is also employed for load balancing. Based on stochastic geometry analysis, the optimal <b>cache</b> <b>sizes</b> for MBSs and SBSs are obtained, which are threshold-based with respect to cache budget in the networks constrained by SBS backhauls. Simulation results are provided to evaluate the proposed schemes and demonstrate the applications in cost-effective network deployment...|$|R
25|$|The L1 <b>cache</b> <b>size</b> was {{enlarged}} in the Core microarchitecture, from 32KB on Pentium II/III (16KB L1 Data + 16KB L1 Instruction) to 64KB L1 cache/core (32KB L1 Data + 32KB L1 Instruction) on Pentium M and Core/Core 2. It also lacks an L3 Cache {{found in}} the Gallatin core of the Pentium 4 Extreme Edition, although an L3 Cache is present in high-end versions of Core-based Xeons. Both an L3 cache and Hyper-threading were reintroduced in the Nehalem microarchitecture.|$|E
25|$|Moreover, D {{algorithms}} can {{be designed}} for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be optimal cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the <b>cache</b> <b>size.</b> In contrast, the traditional approach to exploiting the cache is blocking, as in loop nest optimization, {{where the problem is}} explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache size(s) of a particular machine.|$|E
25|$|By {{the time}} of Barton's release, the Northwood-based Pentium 4 had become more than {{competitive}} with AMD's processors. Unfortunately for AMD, a simple increase in size of the L2 cache to 512kB did not have nearly the same impact {{as it did for}} Intel's Pentium 4 line, as the Athlon architecture was not nearly as cache-constrained as the Pentium 4. The Athlon's exclusive-cache architecture and shorter pipeline made it less sensitive to L2 <b>cache</b> <b>size,</b> and the Barton only saw an increase of several percent gained in per-clock performance over the Thoroughbred-B it was derived from. While the increased performance was welcome, it was not sufficient to overtake the Pentium 4 line in overall performance. The PR also became somewhat inaccurate because some Barton models with lower clock rates were being given higher PR than higher-clocked Thoroughbred processors. Instances where a computational task did not benefit more from the additional cache {{to make up for the}} loss in raw clock speed created situations where a lower rated (but faster clocked) Thoroughbred would outperform a higher-rated (but lower clocked) Barton.|$|E
40|$|The growing CPU-memory gap is {{resulting}} in increasingly large <b>cache</b> <b>sizes.</b> As <b>cache</b> <b>sizes</b> increase, associativity becomes {{less of a}} win. At the same time, since costs of going to DRAM increase, it becomes more valuable {{to be able to}} pin critical data in the cache—a problem if a cache is direct-mapped or has a low degree of associativity. Something else which is a problem for caches of low associativity is reducing misses by using a better replacement policy. This paper proposes that L 2 <b>cache</b> <b>sizes</b> are now starting to reach the point where it makes more sense to manage them as the main memory of the computer, and relegate the traditional DRAM main memory to the role of a paging device. The paper details advantages of an SRAM main memory, as well as problems that need to be solved, in managing an extra level of virtual to physical translation. 1...|$|R
5000|$|Typical memory {{hierarchy}} (access times and <b>cache</b> <b>sizes</b> are approximations of typical values used [...] {{for the purpose}} of discussion; actual values and actual numbers of levels in the hierarchy vary): ...|$|R
40|$|We {{consider}} a cache-aided communications {{system in which}} a transmitter communicates with many receivers over an erasure broadcast channel. The system serves as a basic model for communicating on-demand content during periods of high network congestion, where some content can be pre-placed in local caches near the receivers. We formulate the cache-aided communications problem as a joint cache-channel coding problem, and characterise some information-theoretic tradeoffs between reliable communications rates and <b>cache</b> <b>sizes.</b> We show that if the receivers experience different channel qualities, then using unequal <b>cache</b> <b>sizes</b> and joint cache-channel coding improves system efficiency. Comment: submitted as an invited paper to ISWCS 201...|$|R
500|$|The Poulson L3 <b>cache</b> <b>size</b> is 32MB. L2 <b>cache</b> <b>size</b> is 6MB, 512IKB, 256DKB per core. [...] Die size is 544mm², {{less than}} its {{predecessor}} Tukwila (698.75mm²).|$|E
2500|$|The second {{generation}} (Paris/Palermo core) {{was based on}} the architecture of the Socket 754 Athlon 64. Some differences from Athlon 64 processors include a reduced <b>cache</b> <b>size</b> (either 128 or 256KiB L2), and the absence of AMD64 support in earlier models. [...] Apart from these differences, the Socket 754 Sempron CPUs share most features with the more powerful Athlon 64, including an integrated (on-die) memory controller, the HyperTransport link, and AMD's [...] "NX bit" [...] feature.|$|E
2500|$|The {{processors}} of the Core microarchitecture can {{be categorized}} by number of cores, <b>cache</b> <b>size,</b> and socket; each combination of these has a unique code name and product code that is used across a number of brands. For instance, code name [...] "Allendale" [...] with product code 80557 has two cores, 2 MB L2 cache and uses the desktop socket 775, but has been marketed as Celeron, Pentium, Core 2 and Xeon, each with different sets of features enabled. Most of the mobile and desktop processors come in two variants that differ {{in the size of}} the L2 cache, but the specific amount of L2 cache in a product can also be reduced by disabling parts at production time.|$|E
5000|$|... 32-way set {{associative}} L3 victim <b>cache</b> <b>sized</b> {{at least}} 2 MB, shared between processing cores {{on a single}} die (each with 512 K of independent exclusive L2 cache), with a sharing-aware replacement policy.|$|R
40|$|We study noisy {{broadcast}} networks with local cache memories at the receivers, where the transmitter can pre-store information even before learning the receivers' requests. We mostly focus on packet-erasure {{broadcast networks}} with two disjoint sets of receivers: {{a set of}} weak receivers with all-equal erasure probabilities and equal <b>cache</b> <b>sizes</b> {{and a set of}} strong receivers with all-equal erasure probabilities and no cache memories. We present lower and upper bounds on the capacity-memory tradeoff of this network. The lower bound is achieved by a new joint cache-channel coding idea and significantly improves on schemes that are based on separate cache-channel coding. We discuss how this coding idea could be extended to more general discrete memoryless broadcast channels and to unequal <b>cache</b> <b>sizes.</b> Our upper bound holds for all stochastically degraded broadcast channels. For the described packet-erasure broadcast network, our lower and upper bounds are tight when there is a single weak receiver (and any number of strong receivers) and the <b>cache</b> memory <b>size</b> does not exceed a given threshold. When there are a single weak receiver, a single strong receiver, and two files, then we can strengthen our upper and lower bounds so as they coincide over a wide regime of <b>cache</b> <b>sizes.</b> Finally, we completely characterise the rate-memory tradeoff for general discrete-memoryless broadcast channels with arbitrary <b>cache</b> memory <b>sizes</b> and arbitrary (asymmetric) rates when all receivers always demand exactly the same file...|$|R
40|$|We study web caching with request reordering. The {{goal is to}} {{maintain}} a cache of web documents so that a sequence of requests can be served at low cost. To improve cache hit rates, a limited reordering of requests is allowed. Feder et al. [6], who recently introduced this problem, considered <b>caches</b> of <b>size</b> 1, i. e. a cache can store one document. They presented an offline algorithm based on dynamic programming as well as online algorithms that achieve constant factor competitive ratios. For arbitrary <b>cache</b> <b>sizes,</b> Feder et al. [7] gave online strategies that have nearly optimal competitive ratios in several cost models. In this paper we first present a deterministic online algorithm that achieves an optimal competitiveness, for the most general cost model and all <b>cache</b> <b>sizes.</b> We then investigate the offline problem, which is NP-hard in general. We develop the first polynomial time algorithms that can manage arbitrary <b>cache</b> <b>sizes.</b> Our strategies achieve small constant factor approximation ratios. The algorithms {{are based on a}} general technique that reduces web caching with request reordering to a problem of computing batched service schedules. Our approximation result for the Fault Model also improves upon the best previous approximation guarantee known for web caching without request reordering...|$|R
2500|$|AMD changed cache design {{significantly}} with the Thunderbird core. With the older Athlon CPUs, the CPU caching was of an inclusive design where {{data from the}} L1 is duplicated in the L2 cache. Thunderbird moved to an exclusive design where the L1 cache's contents are not duplicated in the L2. This increases total <b>cache</b> <b>size</b> of the processor and effectively makes caching behave {{as if there is}} a very large L1 cache with a slower region (the L2) and a very fast region (the L1). Because of Athlon's very large L1 cache and the exclusive design, which turns the L2 cache into basically a [...] "victim cache", the need for high L2 performance and size was lessened. AMD kept the 64-bit L2 cache data bus from the older Athlons, as a result, and allowed it to have a relatively high latency. A simpler L2 cache reduced the possibility of the L2 cache causing clock scaling and yield issues. Still, instead of the 2-way associative scheme used in older Athlons, Thunderbird did move to a more efficient 16-way associative layout.|$|E
50|$|The Google Earth's <b>cache</b> <b>size</b> {{is limited}} to 2000 MB whereas World Wind has no limit on <b>cache</b> <b>size.</b> In Norkart Virtual Globe the disk cache can be set by the user.|$|E
50|$|Since {{capacity}} misses can {{be attributed}} to the limited size of a cache, a simple way to reduce the number of such misses is to increase the <b>cache</b> <b>size.</b> Although this method is very intuitive, it leads to a longer access time and an increase in cache area and its power consumption. Also, after a certain <b>cache</b> <b>size,</b> the number of misses saturate and do not decrease even on increasing the <b>cache</b> <b>size.</b>|$|E
40|$|International audienceThis paper {{presents}} a cache tracker, a hardware component {{to track the}} cache state of hundreds of caches serving processors modeled using threads on a single MIPS 64 processor. This host-multithreading approach allows a single, low-cost FPGA to model large systems to allow quick and broad architectural exploration with reasonable simulation performance. The cache tracker stores all state in DRAM to allow maximum scalability in both number of processors and in <b>cache</b> <b>sizes.</b> We describe our approach of scalability versus simulation performance, our implementation in Bluespec SystemVerilog, and give a sample study of a parallel merge-sort over various processor numbers, <b>cache</b> <b>sizes</b> and arrangements...|$|R
50|$|If ALG is a marking {{algorithm}} with a <b>cache</b> of <b>size</b> k, and OPT is {{the optimal}} algorithm with a <b>cache</b> of <b>size</b> h, where , then ALG is -competitive. So every marking algorithm attains the -competitive ratio.|$|R
50|$|Processors {{remained}} single-core {{until it}} was impossible to achieve performance gains from the increased clock speed and transistor count allowed by Moore's law (there were diminishing returns to increasing the depth of a pipeline, increasing CPU <b>cache</b> <b>sizes,</b> or adding execution units).|$|R
50|$|Increased L2 <b>cache</b> <b>size,</b> {{with the}} L2 <b>cache</b> <b>size</b> ranging from 1 MB to 12 MB (Core 2 Duo {{processors}} use a shared L2 cache while Core 2 Quad processors having {{half of the}} total cache is shared by each core pair).|$|E
50|$|The {{power law}} can only give an {{estimate}} of the miss rate only up to a certain value of <b>cache</b> <b>size.</b> A large enough cache eliminates capacity misses and increasing the <b>cache</b> <b>size</b> further will not reduce the miss rate any further, contrary to the power law's prediction.|$|E
5000|$|L1 <b>cache</b> <b>size</b> 16 KB {{write-back}} 4-way set associative unified I/D cache.|$|E
5000|$|The memory wall; the {{increasing}} gap between processor and memory speeds. This, in effect, pushes for <b>cache</b> <b>sizes</b> {{to be larger}} in order to mask the latency of memory. This helps {{only to the extent}} that memory bandwidth is not the bottleneck in performance.|$|R
40|$|Abstract—This paper {{presents}} a cache tracker, a hardware component {{to track the}} cache state of hundreds of caches serving processors modeled using threads on a single MIPS 64 processor. This host-multithreading approach allows a single, low-cost FPGA to model large systems to allow quick and broad architectural exploration with reasonable simulation performance. The cache tracker stores all state in DRAM to allow maximum scalability in both number of processors and in <b>cache</b> <b>sizes.</b> We describe our approach of scalability versus simulation performance, our implementation in Bluespec SystemVerilog, and give a sample study of a parallel merge-sort over various processor numbers, <b>cache</b> <b>sizes</b> and arrangements. INTRODUCTION: FPGA SIMULATION AND OUR APPROACH Our system follows others to adopt an FPGA simulation approach to multi-processor architecture exploration but differ...|$|R
50|$|The {{early history}} of cache {{technology}} is {{closely tied to the}} invention and use of virtual memory. Because of scarcity and cost of semi-conductor memories, early mainframe computers in the 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory space used by programs. The memory technologies would span semi-conductor, magnetic core, drum and disc. Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access. Extensive studies were done to optimize the <b>cache</b> <b>sizes.</b> Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest <b>cache</b> <b>sizes.</b>|$|R
