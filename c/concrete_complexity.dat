19|23|Public
40|$|The {{fact that}} there are {{zero-knowledge}} proofs for all languages in NP has, potentially, enormous implications to cryptography. For cryptographers, the issue is no longer "which languages in NP have zero-knowledge proofs" but rather "which languages in NP have practical zero-knowledge proofs". Thus, the <b>concrete</b> <b>complexity</b> of zero-knowledge proofs for different languages must be established. In this paper, we study the <b>concrete</b> <b>complexity</b> of the known general methods for constructing zero-knowledge proofs. We establish that circuit-based methods have the potential of producing proofs which can be used in practice. Then we introduce several techniques which greatly reduce the <b>concrete</b> <b>complexity</b> of circuit-based proofs. In order to show that our protocols yield proofs of knowledge, we show how to extend the Feige-Fiat-Shamir definition for proofs of knowledge to the model of Brassard-Chaum-Cr'epeau. Finally, we present techniques for improving the efficiency of protocols which involve ar [...] ...|$|E
40|$|We study {{notions of}} {{security}} and schemes for symmetric (ie. private key) encryption in a concrete security framework. We give several dierent notions {{of security and}} analyze the <b>concrete</b> <b>complexity</b> of reductions among them. Next we provide concrete security analyses of various methods of encrypting using a block cipher, including {{two of the most}} popular methods, Cipher Block Chaining and Counter Mode. We establish tight bounds (meaning matching upper bounds and attacks) on the succes...|$|E
40|$|We {{show the}} {{following}} new lower bounds in two <b>concrete</b> <b>complexity</b> models: (1) In the two-party communication complexity model, {{we show that}} the tribes function on n inputs [6] has two-sided error randomized complexity # n), while its nondeterminstic complexity and co-nondeterministic complexity are both #(# n). This separation between randomized and nondeterministic complexity is the best possible and it settles an open problem in Kushilevitz and Nisan [17], which was also posed by Beame and Lawry [5]...|$|E
50|$|Henryson's {{expansion}} of Aesop's fable makes its inferences <b>concrete,</b> introduces <b>complexity</b> and raises {{a large number}} of themes in a very short space. In relation to other known fable literature in Europe up to that time, the loading is extremely rich.|$|R
5000|$|Balsamo {{advocates for}} {{educational}} initiatives that expand {{access to the}} spheres in which future technologies are imagined and defined; along these lines, she and Alexandra Juhasz co-founded FemTechNet, a network of scholars and artists who work on issues related to technology and gender. John Seely Brown, former chief scientist of Xerox Corporation and Director of Xerox Palo Alto Research Center (PARC) argues that Balsamo [...] "portrays both the necessity {{and the challenge of}} cultivating the technological imagination in all of us...Her insights into expanding the traditional considerations of socio-technical design to consider issues of culture are coming at a critical time." [...] Lawrence Grossberg, author of Studies In the Future, argues that [...] "Balsamo maps the <b>concrete</b> <b>complexities</b> of specific design processes, and opens up new ways of thinking about and teaching technocultures in relation to broader socio-political fields." ...|$|R
40|$|Perceptively written text {{examines}} optimization {{problems that}} can be formulated in terms of networks and algebraic structures called matroids. Chapters cover shortest paths, network flows, bipartite matching, nonbipartite matching, matroids and the greedy algorithm, matroid intersections, and the matroid parity problems. A suitable text or reference for courses in combinatorial computing and <b>concrete</b> computational <b>complexity</b> in departments of computer science and mathematics...|$|R
40|$|Abstract We study {{notions of}} {{security}} and schemes for symmetric (ie. private key) encryption in a concrete security framework. We give several different notions {{of security and}} analyze the <b>concrete</b> <b>complexity</b> of reductions among them. Next we provide concrete security analyses of various methods of encrypting using a block cipher, including {{two of the most}} popular methods, Cipher Block Chaining and Counter Mode. We establish tight bounds (meaning matching upper bounds and attacks) on the success of adversaries as a function of their resources...|$|E
40|$|Abstract. We {{present a}} study of the <b>concrete</b> <b>complexity</b> of solving {{instances}} of the unique shortest vector problem (uSVP). In particular, we study the complexity of solving the Learning with Errors (LWE) problem by reducing the Bounded-Distance Decoding (BDD) problem to uSVP and attempting to solve such instances using the ‘embedding ’ approach. We experimentally derive a model for the success of the approach, compare to alternative methods and demonstrate that for the LWE instances considered in this work, reducing to uSVP and solving via embedding compares favorably to other approaches. ...|$|E
40|$|We study notions and {{schemes for}} {{symmetric}} (ie. private key) encryption {{in a concrete}} security framework. We give four di erent notions of security against chosen plaintext attack and analyze the <b>concrete</b> <b>complexity</b> ofreductions among them, providing both upper and lower bounds, and obtaining tight relations. In this way we classify notions (even though polynomially reducible to each other) as stronger or weaker in terms of concrete security. Next we provide concrete security analyses of methods to encrypt using a block cipher, including the most popular encryption method, CBC. We establish tight bounds (meanin...|$|E
40|$|This paper investigates how {{complexity}} influences {{projects and}} their performance. We develop a classification of project complexity {{by relying on}} fundamental theoretical insights about complexity and then use results from practice-oriented literature to assign <b>concrete</b> project <b>complexity</b> factors to the resulting categories. We also identify specific strategies for organizing and knowledge production that project planners use to address complexity-related uncertainties. We theorize about the way these strategies interact with various types of complexity to increase project performance. Anticipated influences are mostly corroborated using survey data on 81 complex projects from five continents and a diversity of sectors...|$|R
5000|$|... where [...] is an {{underlying}} loss function {{that describes the}} cost of predicting [...] when the label is , such as the square loss or hinge loss; and [...] is a parameter which controls {{the importance of the}} regularization term. [...] is typically chosen to impose a penalty on the <b>complexity</b> of [...] <b>Concrete</b> notions of <b>complexity</b> used include restrictions for smoothness and bounds on the vector space norm.|$|R
40|$|Recently, chaos has {{attracted}} much attention {{in the field of}} cryptography. To study the security with a known image of a symmetric image encryption scheme, the attack algorithm of equivalent key is given. We give the known image attacks under different other conditions to obtain the equivalent key. The <b>concrete</b> step and <b>complexity</b> of the attack algorithm is given. So the symmetric image encryption scheme based on 3 D chaotic cat maps is not secure...|$|R
40|$|The Sensitivity Conjecture and the Log-rank Conjecture {{are among}} the most {{important}} and challenging problems in <b>concrete</b> <b>complexity.</b> Incidentally, the Sensitivity Conjecture is known to hold for monotone functions, and so is the Log-rank Conjecture for f(x ∧ y) and f(x⊕ y) with monotone functions f, where ∧ and ⊕ are bit-wise AND and XOR, respectively. In this paper, we extend these results to functions f which alternate values for {{a relatively small number of}} times on any monotone path from 0 ^n to 1 ^n. These deepen our understandings of the two conjectures, and contribute to the recent line of research on functions with small alternating numbers...|$|E
40|$|We study pushdown systems where control states, stack alphabet, and {{transition}} relation, {{instead of being}} finite, are first-order definable in a fixed countably-infinite structure. We show that the reachability analysis can be addressed with the well-known saturation technique for the wide class of oligomorphic structures. Moreover, for the more restrictive homogeneous structures, {{we are able to}} give <b>concrete</b> <b>complexity</b> upper bounds. We show ample applicability of our technique by presenting several concrete examples of homogeneous structures, subsuming, with optimal complexity, known results from the literature. We show that infinitely many such examples of homogeneous structures can be obtained with the classical wreath product construction. Comment: to appear in CSL' 1...|$|E
40|$|We {{prove that}} the full {{solution}} set of a twisted word equation with regular constraints is an EDT 0 L language. It follows that the set of solutions to equations with rational constraints in a context-free group (= finitely generated virtually free group) in reduced normal forms is EDT 0 L. We can also {{decide whether or not}} the solution set is finite, which was an open problem. Moreover, this can all be done in PSPACE. Our results generalize the work by Lohrey and Senizergues (ICALP 2006) and Dahmani and Guirardel (J. of Topology 2010) with respect to complexity and with respect to expressive power. Both papers show that satisfiability is decidable, but neither gave any <b>concrete</b> <b>complexity</b> bound. Our results concern all solutions, and give, in some sense, the "optimal" formal language characterization...|$|E
40|$|Abstract—Recently, chaos has {{attracted}} much attention {{in the field of}} cryptography. To study the security with a known image of a symmetric image encryption scheme, the attack algorithm of equivalent key is given. We give the known image attacks under different other conditions to obtain the equivalent key. The <b>concrete</b> step and <b>complexity</b> of the attack algorithm is given. So the symmetric image encryption scheme based on 3 D chaotic cat maps is not secure. Index Terms—Cryptanalysis; Equivalent key attac...|$|R
40|$|Introducing Social Semiotics uses a {{wide variety}} of texts {{including}} photographs, adverts, magazine pages and film stills to explain how meaning is created through complex semiotic interactions. Practical exercises and examples as wide ranging as furniture arrangements in public places and advertising jingles, provide readers with the knowledge and skills they {{need to be able to}} analyze and also produce successful multimodal texts and designs. The book traces the development of semiotic resources through particular channels such as the history of the Press and advertising; and explores how and why these resources change over time, for reasons such as advancing technology. Featuring a full glossary of terms, exercises, discussion points and suggestions for further reading, Introducing Social Semiotics makes <b>concrete</b> the <b>complexities</b> of meaning making and is essential reading for anyone interested in how communication works...|$|R
40|$|We {{establish}} PAC learnability {{of influence}} functions for three common influence models, namely, the Linear Threshold (LT), Independent Cascade (IC) and Voter models, and present <b>concrete</b> sample <b>complexity</b> results in each case. Our {{results for the}} LT model are based on interesting connections with neural networks; those for the IC model are based {{an interpretation of the}} influence function as an expec-tation over random draw of a subgraph and use covering number arguments; and those for the Voter model are based on a reduction to linear regression. We show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced. We also provide efficient polynomial time learning algorithms for a setting with full observation, i. e. where the cascades also contain the time steps in which nodes are influenced. ...|$|R
40|$|We {{investigate}} several alternate {{characterizations of}} pseudorandom functions (PRFs) and pseudorandom permutations (PRPs) {{in a concrete}} security setting. By analyzing the <b>concrete</b> <b>complexity</b> of the reductions between the standard notions and the alternate ones, we show that the latter, while equivalent under polynomial-time reductions, are weaker in the concrete security sense. With these alternate notions, we {{argue that it is}} possible to get better concrete security bounds for certain PRF/PRP-based schemes. As an example, we show how using an alternate characterization of a PRF could result in tighter security bounds for a certain class of message authentication codes. We also apply these techniques to give a simple concrete security analysis of the counter mode of encryption. In addition, our results provide some insight into how injectivity impacts pseudorandomness. 1 Introduction Pseudorandom functions (PRFs) and pseudorandom permutations (PRPs) are extremely useful [...] ...|$|E
40|$|We investigate, in a {{concrete}} security setting, several alternate characterizations of pseudorandom functions (PRFs) and pseudorandom permutations (PRPs). By analyzing the <b>concrete</b> <b>complexity</b> of the reductions between the standard notions and the alternate ones, we show that the latter, while equivalent under polynomial-time reductions, are weaker in the concrete security sense. With these alternate notions, we {{argue that it is}} possible to get better concrete security bounds for certain PRF/PRP-based schemes. As an example, we show how using an alternate characterization of a PRF could result in tighter security bounds for a certain class of message authentication codes. We also apply these techniques to give a simple concrete security analysis of the counter mode of encryption. In addition, our results provide some insight into how injectivity impacts pseudorandomness. Dept. of Computer Science & Engineering, University of California at San Diego, 9500 Gilman Drive, La Jolla, CA 92 [...] ...|$|E
40|$|We {{prove that}} the set of all {{solutions}} for twisted word equations with regular constraints is an EDT 0 L language and can be computed in PSPACE. It follows that the set of solutions to equations with rational constraints in a context-free group (= finitely generated virtually free group) in reduced normal forms is EDT 0 L. We can also decide (in PSPACE) {{whether or not the}} solution set is finite, which was an open problem. Our results generalize the work by Lohrey and Sénizergues (ICALP 2006) and Dahmani and Guirardel (J. of Topology 2010) with respect to complexity and with respect to expressive power. Neither paper gave any <b>concrete</b> <b>complexity</b> bound and both rely on the exponent of periodicity, so the result in these papers concern only subsets of solutions, whereas our results concern all solutions. We do more, we give, in some sense, the "optimal" formal language characterization of the full solution set. Comment: 41 pages, 7 figure...|$|E
30|$|In this paper, {{we propose}} an {{adaptive}} sparse sensing (ASS) method using the reweighted zero-attracting normalized mean fourth error algorithm (RZA-NLMF) [8] {{to solve the}} CS problems. Different from NSS methods, each observation and corresponding sensing signal vector will be implemented by the RZA-NLMF algorithm to reconstruct the sparse signal {{during the process of}} adaptive filtering. According to the <b>concrete</b> requirements, the <b>complexity</b> of the proposed ASS method could be adaptively reduced without sacrificing much recovery performance. The effectiveness of our proposed method is confirmed via computer simulation when comparing with NSS.|$|R
5000|$|In {{computational}} complexity theory, {{the amounts}} of resources required for the execution of algorithms is studied. The most popular types of computational complexity are the time complexity of a problem equal {{to the number of}} steps that it takes to solve an instance of the problem {{as a function of the}} size of the input (usually measured in bits), using the most efficient algorithm, and the space complexity of a problem equal to the volume of the memory used by the algorithm (e.g., cells of the tape) that it takes to solve an instance of the problem as a function of the size of the input (usually measured in bits), using the most efficient algorithm. This allows to classify computational problems by complexity class (such as P, NP, etc.). An axiomatic approach to computational complexity was developed by Manuel Blum. It allows one to deduce many properties of <b>concrete</b> computational <b>complexity</b> measures, such as time complexity or space complexity, from properties of axiomatically defined measures.|$|R
40|$|The goal {{of music}} {{analysis}} {{is to find}} the most satisfying explanations for musical works. It is proposed that this can best be achieved by attempting to write computer programs that are as short as possible and that generate representations that are as detailed as possible of the music to be explained. The theory of Kolmogorov complexity suggests that the length of such a program {{can be used as a}} measure of the complexity of the analysis that it represents. The analyst therefore needs a way to measure the length of a program so that this length reflects the quality of the analysis that the program represents. If such an effective measure of analysis quality can be found, it could be used in a system that automatically finds the optimal analysis for any passage of music. Measuring program length in terms of number of source-code characters is shown to be problematic and an expression is proposed that overcomes some but not all of these problems. It is suggested that the solutions to the remaining problems may lie either in the field of <b>concrete</b> Kolmogorov <b>complexity</b> or in the design of languages specialized for expressing musical structure. 1...|$|R
40|$|Post-quantum {{cryptography}} for resisting possible {{attacks from}} malicious quantum adversaries {{has become one}} of the key topics in recent cryptographic research. Its ultimate goal is to search for efficient and secure primitives replacing the factoring- and discrete log-based schemes in service that will be broken in polynomial time by Shor’s algorithm. It is now an urgent task to complete this replacement due to the rapid development in building quantum computers. Among all known post-quantum candidates, lattice-based and code-based cryptography are two very attractive branches for their solid mathematical foundations as well as their versatile cryptographic applications. There exist several important hardness assumptions in these two areas, especially that the famous hard learning problems, i. e., the learning parity with noise (LPN) problem, the learning with errors (LWE) problem and their variants, are difficult to solve. It plays a crucial role to determine the <b>concrete</b> <b>complexity</b> of the discussed instances of these problems when characterising the security level of cryptosystems employing these instances. The LPN assumption and its variants also have significance in Lightweight cryptography, a field for building low-cost cryptographic schemes for highly constraint devices. A better solving algorithm will definitely lead to a security reduction of the schemes making use of the suggested parameters. In this thesis, we introduce new algorithmic ideas for improving the solving algorithms of these hard problems and analyse their <b>concrete</b> <b>complexity.</b> Our tools are existing techniques from various aspects of the well-developed coding theory, and many targeted cryptosystems should increase the size of the suggested parameters for a certain security level. We also present the first key-recovery reaction attack against QC-MDPC, a code-based public-key encryption scheme. This attack can be applied to the CCA 2 converted version of QC-MDPC to break the claimed CCA 2 security. The results can be summarised in the following. • We use covering codes for solving LPN – the instances suggested for 80 -bit security in HB variants and LPN-C can be solved in less than 2 ^ 80 operations. • We use CRT codes for solving Ring-LPN with a reducible polynomial –we break the Lapin authentication protocol using the proposed instance for 80 -bit security in about 2 ^ 71 operations. • We use lattice codes for solving LWE and its variants – we reduce the time complexity for solving some suggested parameter settings drastically; for example, for an instance suggested by Regev with dimension n = 512, the improvement factor is about 2 ^ 70. Several other cryptosystems should employ LWE instances with a larger dimension. • Using the Hamming leakage model, we reduce the problem of using a side-channel attack on the fresh re-keying countermeasure to a slightly generalised Ring-LPN problem with a reducible polynomial, and solve it via birthday-type attacks and FWHT – compared with the previous research, the improvement factor is about 2 ^ 10 in the 128 -bit leakage model with the same amount of SNR and leaking traces. • We make use of the fact that the decoding error probability of the iterative decoding algorithm employed in QC-MDPC is highly correlated with the error patterns, and leaks some information of the secret key – we implement the attack, which works well targeting both the CPA- and CCA 2 -secure versions of QC-MDPC. Besides these attacks, we propose a new McEliece-type public-key cryptosystem generalising MDPC from the binary case to a larger alphabet and suggest parameters based on knowledge of the <b>concrete</b> <b>complexity</b> of the above hard problems. This scheme is very attractive as a topic related to both lattice-based and code-based cryptography, which takes advantage of iterative decoding, the McEliece structure, the Euclidean metric, as well as a more compact way of representing information. This scheme is still far from mature and needs to be refined further...|$|E
40|$|This paper {{proposes to}} define several notions of security. This notions will be applicated only on {{symmetric}} encryption scheme specily {{in a concrete}} security framework. The first notion is Left-or-Right Indistinguishability which considers two string with the same lenght x 0 and x 1 and a left-or-right oracle. To summarize, we take a bit b and encrypt x 0 if b= 1 or x 1 if b= 0. The challenge for the adversary is to guess the value of b. The second notion is Real-or-Random Indistinguisability. The idea is that an adversary cannot distinguish an encrypted text with an encryption of an equal-lenght string of garbage. Then, we have the Find-then-Guess Security which is an adaption of the polynomial security. The last notion is semantic security, It is presented saying that whatever can be efficiently computed about a plaintext given the cyphertext can also be computed in absence of the ciphertext. For each notions, we have {{a study of the}} <b>concrete</b> <b>complexity</b> of reductions between the different notions. For each notions, upper and lower bounds of complexity to etablish relations between them and comparing them in order to define the strongest or the weakest...|$|E
40|$|Peter Naur is {{the leading}} critic of formalist {{computing}} because of his extensive writings that disprove the now dominate characterization of human thought as cognitive information processing. Naur criticizes the ideological position that only discourse that adopts computer inspired forms are acceptable. Lakatosian philosophy of the methodology of scientific research programmes (MSRP) is added to Naur's studies to allow testing of computing theories. After discussing Naur's criticism of mechanical cognitive information processing, I show how to add MSRP competition to Naur's descriptive philosophy. Next, Naur's claim that computing can not become scientific until organizational issues involving ideological suppression of discussions of computing and human thinking are solved is corroborated by institutional suppression of my 1970 s attempts to criticize structured programming (SP). Various problems in computing related philosophy are discussed. First, I argue that my MSRP based degenerating research programme disproof of SP is better than Naur's programming as a human activity, Demillo's social processes and Fetzer's unprovable causal nature. Three areas for post ideologically based computing study are discussed: computing as a path to rediscovering 19 th century conceptions of infinity, axiom of choice testing facilitated by computing and relation to physical theory, and testing <b>concrete</b> <b>complexity</b> methods based on efficiency proof analysis. Comment: 8 pages, no figures, and 28 reference...|$|E
40|$|Abstract — Recently, {{there has}} been {{considerable}} interest in using antenna arrays in wireless communication networks to increase the capacity and decrease the cochannel interference. Adaptive beamforming with smart antennas at the receiver increases the carrier-to-interference ratio (CIR) in a wireless link. This paper considers a wireless network with beamforming capabilities at the receiver which allows two or more transmitters to share the same channel {{to communicate with the}} base station. The <b>concrete</b> computational <b>complexity</b> and algorithm structure of a base station are considered in terms of a software radio system model, initially with an omnidirectional antenna. The software radio computational model is then expanded to characterize a network with smart antennas. The application of the software radio smart antenna is demonstrated through two examples. First, traffic improvement in a network with a smart antenna is considered, and the implementation of a hand-off algorithm in the software radio is presented. The blocking probabilities of the calls and total carried traffic in the system under different traffic policies are derived. The analytical and numerical results show that adaptive beamforming at the receiver reduces the probability of blocking and forced termination of the calls and increases the total carried traffic in the system. Then, a joint beamforming and power control algorithm is implemented in a software radio smart antenna in a CDMA network. This shows that, by using smart antennas, each user can transmit with much lower power, and therefore the system capacity increases significantly. Index Terms—Adaptive beamforming, handoff, power control, smart antennas, software radio. I...|$|R
40|$|We {{initiate}} {{the study of}} constraint satisfaction problems (CSPs) {{in the presence of}} counting quantifiers, which may be seen as variants of CSPs in the mould of quantified CSPs (QCSPs). We show that a single counting quantifier strictly between ∃[*][*]≥[*] 1 :[*]=[*]∃ and ∃[*][*]≥[*]n :[*]=[*]∀ (the domain being of size n) already affords the maximal possible complexity of QCSPs (which have both ∃ and ∀), being Pspace-complete for a suitably chosen template. Next, we focus on the complexity of subsets of counting quantifiers on clique and cycle templates. For cycles we give a full trichotomy – all such problems are in L, NP-complete or Pspace-complete. For cliques we come close to a similar trichotomy, but one class remains outstanding. Afterwards, we consider the generalisation of CSPs in which we augment the extant quantifier ∃[*][*]≥[*] 1 :[*]=[*]∃ with the quantifier ∃[*][*]≥[*]j (j[*]≠[*] 1). Such a CSP is already NP-hard on non-bipartite graph templates. We explore the situation of this generalised CSP on bipartite templates, giving various conditions for both tractability and hardness – culminating in a classification theorem for general graphs. Finally, we use counting quantifiers to solve the <b>complexity</b> of a <b>concrete</b> QCSP whose <b>complexity</b> was previously open...|$|R
40|$|Intuitively we know, some {{software}} {{errors are}} more complex than others. If the error can be fixed by changing one faulty statement, {{it is a simple}} error. The more substantial the fix must be, the more complex we consider the error. In this work, we formally define and quantify the com-plexity of an error w. r. t. the complexity of the error’s least complex, correct fix. As a <b>concrete</b> measure of <b>complexity</b> for such fixes, we introduce Cyclomatic Change Complexity which is inspired by existing program complexity metrics. Moreover, we introduce CoREBench, a collection of 70 regression errors systematically extracted from several open-source C-projects and compare their complexity with that of the seeded errors in the two most popular error bench-marks, SIR and the Siemens Suite. We find that seeded er-rors are significantly less complex, i. e., require significantly less substantial fixes, compared to actual regression errors. For example, among the seeded errors more than 42 % are simple compared to 8 % among the actual ones. This is a concern for the external validity of studies based on seeded errors and we propose CoREBench for the controlled study of regression testing, debugging, and repair techniques...|$|R
40|$|As it is well-known, Peirce {{coined the}} term “pragmaticism” to {{differentiate}} his own philosophy from James’ pragmatism. John Dewey’s assertion that in James’ works “the man always takes precedence over the philosopher” might be appropriate for beginning to describe this difference. Indeed, James’ emphasis on individual experiences and on feelings as determining factors of every human performance, including the logical-rational activity, and his acute awareness of philosophical systems” shortcomings form a style of thinking that is quite alien to Peirce. An {{integral part of the}} following comments will be to show that James’ anti-intellectualism cannot be identified with an irrationalist agenda. To be sure, such an interpretation is possible only if we focus on a few particular expressions, without considering the whole of his philosophical discourse. In other words, we should keep in mind that his anti-intellectualism does not mean a revolt against the markedly human activities, skills, and perspectives to which the term “rationality” traditionally refers. Rather, James’ objective, like that of Peirce and the other pragmatists, is to demonstrate the inadequacy of those traditional images of rationality, which are based on the abstract criteria of rationalism or on a conception of human intelligence that fails to appreciate its <b>concrete</b> <b>complexity.</b> To show that irrationalism is extraneous to James’ thought, I will first take into account both its principal similarities and specific differences with Peirce’s pragmatism...|$|E
40|$|A {{bound for}} Betti numbers of sets definable in o-minimal {{structures}} is presented. An axiomatic complexity measure is defined, allowing various <b>concrete</b> <b>complexity</b> measures for definable functions to be covered. This includes common concrete {{measures such as}} the degree of polynomials, and complexity of Pfaffian functions. A generalisation of the Thom-Milnor Bound [17, 19] for sets defined by the conjunction of equations and non-strict inequalities is presented, in the new context of sets definable in o-minimal structures using the axiomatic complexity measure. Next bounds are produced for sets defined by Boolean combinations of equations and inequalities, through firstly considering sets defined by sign conditions, then using this to produce results for closed sets, and then making use of a construction to approximate any set defined by a Boolean combination of equations and inequalities by a closed set. Lastly, existing results [12] for sets defined using quantifiers on an open or closed set are generalised, using a construction from Gabrielov and Vorobjov [11] to approximate any set by a compact set. This results in a method to find a general bound for any set definable in an o-minimal structure {{in terms of the}} axiomatic complexity measure. As a consequence for the first time an upper bound for sub-Pfaffian sets defined by arbitrary formulae with quantifiers is given. This bound is singly exponential if the number of quantifier alternations is fixed. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|International audienceWe {{present a}} {{so-called}} labelling method to enrich a compiler {{in order to}} turn it into a "cost annotating compiler", that is, a compiler which can lift pieces of information on the execution cost of the object code as cost annotations on the source code. These cost annotations characterize the execution costs of code fragments of constant complexity. The first contribution of this paper is a proof methodology that extends standard simulation proofs of compiler correctness to ensure that the cost annotations on the source code are sound and precise with respect to an execution cost model of the object code. As a second contribution, we demonstrate that our label-based instrumentation is scalable because it consists in a modular extension of the compilation chain. To that end, we report our successful experience in implementing and testing the labelling approach on top of a prototype compiler written in for (a large fragment of) the C language. As a third and last contribution, we provide evidence for the usability of the generated cost annotations as a mean to reason on the <b>concrete</b> <b>complexity</b> of programs written in C. For this purpose, we present a Frama-C plugin that uses our cost annotating compiler to automatically infer trustworthy logic assertions about the concrete worst case execution cost of programs written in a fragment of the C language. These logic assertions are synthetic in the sense that they characterize the cost of executing the entire program, not only constant-time fragments. (These bounds may depend on the size of the input data.) We report our experimentations on some C programs, especially programs generated by a compiler for the synchronous programming language Lustre used in critical embedded software...|$|E
40|$|Abstract: Recently, great deals {{of civil}} {{engineering}} NDT techniques are developed for safety evaluation of concrete structures. With {{regard to the}} detection of the inner defects in concrete, image scanning scheme will provide very useful information. The traditional ultrasonic image shows good results on lab-cast concrete specimen, {{but it will not}} be practical on real structure owing to the high attenuation nature of ultrasound in <b>concrete</b> and the <b>complexity</b> of the equipment. In this paper, a new imaging method for defect scanning on in-situ structure will be proposed. It combines the well-known point-source/point-receiver measuring scheme and the SAFT technique for signal post-processing. The numerical simulation shows good result. The reflected signal could be enhanced with this method, the geometry and the location of defect could be clearly defined in the scanning image. It could possibly break through the limitation of scanning depth in traditional ultrasonic method and then be used to detect the inner defects in the in-situ structures. Introduction: Among the present civil engineering NDT technologies [1], the application based on the elastic wave theory always plays a very important role. The point-source/point-receiver scheme is especially suitable for the on-site civil infrastructure. It overcomes the limitation o...|$|R
40|$|One of {{the main}} {{directions}} in construction material science {{is the development of}}  next generation concrete that is ultra-dense, high-strength, ultra-porous, high heat efficient, extra corrosion-resistant. Selection of such direction is caused by extreme operational impacts on the concrete, namely: continuously increasing load on the concrete and various dynamics of such loads; the necessity in operation of concrete products in a wide temperature range and their exposure to various chemical and physical effects. The next generation concrete represents high-tech concrete mixtures with additives that takes on and retain the required properties when hardening and being used under any operational conditions. A differential characteristic of the next generation <b>concrete</b> is its <b>complexity</b> that presumes usage of various mineral dispersed components, two- and three fractional fine and coarse aggregates, complex chemical additives, combinations of polymer and iron reinforcement. Design strength and performance properties level of the next generation concrete is achieved by high-quality selection of the composition, proper selection of manufacturing techniques, concrete curing, bringing the quality of concrete items to the required level of technical condition during the operational phase. However, directed formation of its structure is necessary in order to obtain high-tech concrete. Along with the traditional methods for regulation of the next generation concrete structure, modification of concrete while using silica nanoparticles is also considered as a perspective one because the concrete patterning occurs due to introduction of a binder in a mineral matrix. Due to this it is possible to obtain nano-modified materials with completely new properties. The main problem with the creation of nano-modified concrete is a uniform distribution of nano-materials in the volume of the cement matrix which is particularly important in the cases of adding a modifier in micro-quantities. An additional environment is required in order to solve this problem and the environment will form a continuous phase in the composite. This function can be performed by liquid or dispersed phase. </p...|$|R
40|$|In {{the paper}} [1] {{published}} in "Asiacrypt 2000 ", L. Goubin and N. T. Courtois propose {{an attack on}} TTM cryptosystem. In this paper, we will show that the said attack is ineective. First, L. Goubin and N. T. Courtois mistake TTM (cf [6], [7]) for a special case of their proposed TPM (for denition, the reader is referred to x 1). Second, they apply their formula of complexities, q d m n er m 3 for TPM to the TTM cryptosystem. It is obvious {{that in the case}} q = 2 8, the above formula has consequences only if d m n er 8. L. Goubin and N. T. Courtois make a main error in assuming that for a TTM encryption system the signicant number r is 2, and d m n e = 2 always, and therefore wrongfully conclude that the complexity of TTM cryptosystem is 2 52 in general. We will show that the TTM cryptosystem is not a special case of their proposed TPM system. Even for the sake of attacking TTM system, one applies the analysis of TPM system, we show that the signicant number r may be greater than 4 for TTM cryptosystem by giving a concrete example of TTM with r = 8, d m n e = 4, and d m n er = 32. For the said <b>concrete</b> example the <b>complexity</b> is, according to the formula of their analysis, 2 256 m 3 (where m is the length of the ciphertext) = 2 275, which is much larger than their number of complexity 2 52 for all TTM cryptosystem. A complexity of 2 70 is "strong" for a cryptosystem and 2 80 is "very strong" for a cryptosystem. Therefore, their attack is ineective. Furthermore, for many implementations (including the fastest one) of TTM, the attacks of L. Goubin and N. T. Courtois are much slower than brute force attacks. ...|$|R
