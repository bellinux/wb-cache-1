4|8009|Public
30|$|The {{data hiding}} {{algorithm}} is implemented {{based on the}} TMN Coder Version 3.0 of the ITU-T H. 263 version 2 by University of British Columbia. All sequences are compressed using a constant quantization parameter with the first frame intracoded and the remaining intercoded. Despite {{the differences in the}} original frame-rates among the sequences, the <b>compression</b> <b>frame</b> <b>rate</b> has been set to 30 [*]Hz. The encoding performance is measured based on running the program on a Windows XP Professional machine with Intel Xeon Processor at 2 [*]GHz with 4 [*]GB memory.|$|E
40|$|Video {{data hiding}} is at rest an {{essential}} research subject. We implementing an advanced video data hiding method that performs erasure correction capability of repeat accumulate codes and superiority of forbidden zone data hiding. Selective embedded is {{applied in the}} suggested procedure to conclude host signal samples that appropriate for data hiding. This method includes a temporal synchronization scheme in the sequence to resist insert attacks and frame drop. The suggested frame work is examined by emblematic broadcast temporal against MPEG- 2, H. 264 <b>compression,</b> <b>frame</b> <b>rate</b> adaptation attacks and other familiar video data hiding methods. The decoding error principles are stated for conventional system parameters. The simulation outputs specified that the frame work can be favorably makes use in video data hiding operations...|$|E
40|$|Abstract — Motion Estimation (ME) is {{the most}} {{computationally}} intensive part of video compression and video enhancement systems. For the recently available High Definition (HD) video formats, the computational complexity of full search (FS) ME algorithm is prohibitively high, whereas the PSNR obtained by fast search ME algorithms is low. Therefore, in this paper, we present Dynamically Variable Step Search (DVSS) ME algorithm for processing high definition video formats and a dynamically reconfigurable hardware architecture for efficiently implementing DVSS algorithm. The simulation results showed that DVSS algorithm performs very close to FS algorithm by searching much fewer search locations than FS algorithm and it outperforms successful fast search ME algorithms by searching more search locations than these algorithms. The proposed hardware is implemented in VHDL and is capable of processing high definition video formats in real time. Therefore, {{it can be used}} in consumer electronics products for video <b>compression,</b> <b>frame</b> <b>rate</b> up-conversion and de-interlacing. ...|$|E
40|$|Bachelor thesis {{deals with}} the {{possibilities}} of network cameras and their connection to a PC. Network cameras are analyzed in terms of internal structure, external structure and aspects of their use. Communication with the PC and the user is provided with a communication interface and control software. The most important function {{in the use of}} network cameras is their output – a video. It is composed of several individual factors, such as the encoding method, <b>compression</b> and <b>frame</b> <b>rate.</b> The theory is applied in practice as proposed by the best camera, the method of connection and programming of custom control applications in the Java programming language...|$|R
40|$|In this {{investigation}} we study {{the effects of}} <b>compression</b> and <b>frame</b> <b>rate</b> reduction on the performance of four video analytics (VA) systems utilizing a low complexity scenario, such as the Sterile Zone (SZ). Additionally, we identify the most influential scene parameters affecting the performance of these systems. The SZ scenario is a scene consisting of a fence, not to be trespassed, and an area with grass. The VA system needs to alarm when there is an intruder (attack) entering the scene. The work includes testing of the systems with uncompressed and compressed (using H. 264 /MPEG- 4 AVC at 25 and 5 frames per second) footage, consisting of quantified scene parameters. The scene parameters include descriptions of scene contrast, camera to subject distance, and attack portrayal. Additional footage, including only distractions (no attacks) is also investigated. Results have shown that every system has performed differently for each compression/frame rate level, whilst overall, compression has not adversely affected the performance of the systems. <b>Frame</b> <b>rate</b> reduction has decreased performance and scene parameters have influenced the behavior of the systems differently. Most false alarms were triggered with a distraction clip, including abrupt shadows through the fence. Findings could contribute to the improvement of VA systems...|$|R
40|$|Abstract — Video {{fingerprints}} are feature vectors that uniquely characterize one {{video clip}} from another. The goal of video fingerprinting {{is to identify}} a given video query in a database by measuring {{the distance between the}} query fingerprint and the fingerprints in the database. The performance of a video fingerprinting system, which is usually measured in terms of pairwise independence and robustness, {{is directly related to the}} fingerprint that the system uses. In this paper, a novel video fingerprinting method based on the centroid of gradient orientations is proposed. The centroid of gradient orientations is chosen due to its pairwise independence and robustness against common video processing steps that include lossy <b>compression,</b> resizing, <b>frame</b> <b>rate</b> change, etc. A threshold used to reliably determine a fingerprint match is theoretically derived by modelling the proposed fingerprint as a stationary ergodic process, and the validity of the model is experimentally verified. The performance of the proposed fingerprint is experimentally evaluated and compared with that of other widely-used features. The experimental results show that the proposed fingerprint outperforms the considered features in the context of video fingerprinting. I...|$|R
40|$|Abstract—Video {{fingerprints}} are feature vectors that uniquely characterize one {{video clip}} from another. The goal of video fingerprinting {{is to identify}} a given video query in a database (DB) by measuring {{the distance between the}} query fingerprint and the fingerprints in the DB. The performance of a video fingerprinting system, which is usually measured in terms of pairwise independence and robustness, {{is directly related to the}} fingerprint that the system uses. In this paper, a novel video fingerprinting method based on the centroid of gradient orientations is proposed. The centroid of gradient orientations is chosen due to its pairwise independence and robustness against common video processing steps that include lossy <b>compression,</b> resizing, <b>frame</b> <b>rate</b> change, etc. A threshold used to reliably determine a fingerprint match is theoretically derived by modeling the proposed fingerprint as a stationary ergodic process, and the validity of the model is experimentally verified. The performance of the proposed fingerprint is experimentally evaluated and compared with that of other widely-used features. The experimental results show that the proposed fingerprint outperforms the considered features in the context of video fingerprinting. Index Terms—Content-based video identification, perceptual video hashing, video fingerprinting...|$|R
40|$|The {{increase}} of bandwidth and streaming technologies has made video on the Web viable. Streaming video services are still extremely delay sensitive, and network congestion {{can lead to}} poor video quality and end-user frustration. While attempting to deliver an optimal level of continuous media and network performance, the notion of Quality of Service (QoS) has emerged. Broadly, the notion of QoS {{is a reflection of}} user perception and determines the degree of satisfaction of a user with the video. One way to ensure that users get the QoS that they want for multimedia materials over the Internet is to give them control over the QoS on a dynamic basis at the desktop. A viewer can request and pay for higher, more consistent bandwidth as it is required to facilitate better quality audio and video. To design such a user interface, a user needs assessment was conducted to understand the user needs, problems and preferences when using and controlling streaming media applications over the Internet. Eight focus groups were conducted and during the discussions participants received different types of information on QoS issues. This information was either a visual demonstration, with video clips of different video <b>compression</b> and <b>frame</b> <b>rates,</b> or a verbal description of Qo...|$|R
40|$|Limited {{bandwidth}} is {{a strong}} constraint when efficient transmission of 3 D data to Web clients and mobile applications is needed. In this paper we present a novel multi-resolution WebGL based rendering algorithm which combines progressive loading, view-dependent resolution and mesh <b>compression,</b> providing high <b>frame</b> <b>rates</b> and a decoding speed of million of triangles per second in JavaScript. The method is parallelizable and scalable to very large models. The algorithm {{is based on the}} local multi-resolution approaches provided by the community, but ad-hoc solutions had to be studied and implemented to provide adequate performances. In particular, a compression mechanism that reached very high compression rate without impact on rendering performance was implemented. Moreover, the data partition strategy was modified {{in order to be able}} to load different types of data (i. e. point clouds) and better adapt to the potentials and limitations of web-based rendering...|$|R
30|$|A free-hand {{recording}} system {{as part of}} an ultrasound simulation system (Ultrasound simulator Reslice, registered trademark by Schallware GmbH, Berlin, Germany, [URL] [5] was coupled to a high-end ultrasound device (Vivid-i, registered trademark by GE Healthcare Ultrasound, Solingen, Germany) with a 12  MHz linear probe (12 L). The free-hand {{recording system}} consists of a PC with Ubuntu software platform (Linux-based public license, non-commercial software, version 7.10, [URL] an electromagnetic tracking system, a mannequin and a probe dummy with localization transmitter, and also consists of a PC with recording software (Schallware Acquisition 2007, Fa. Schallware, Berlin, Germany), and an additional electromagnetic tracking system. The ultrasound device was also coupled with a VGA 2 USB-LR converter (Epiphan Company, Springfield, NJ, USA) to get a lossless video-real time <b>compression</b> with high <b>frame</b> <b>rates</b> from up to 80 frames per second (fps). To save position of the probe of the ultrasound device once, the probe had to be connected and calibrated with a localization transmitter. Calibration of the probe had only to be done once before the beginning of the trial by the manufacturer of the ultrasound simulator. <b>Frame</b> <b>rates</b> while recording 3 D-UV were always above 30  fps and could reach 80  fps depending on the number of focus points. 3 D-UV regularly were recorded as transverse parallel planes.|$|R
40|$|Efficient {{transmission}} of 3 D data to Web clients and mobile applications remains a challenge due to limited bandwidth. Most {{of the research}} focus {{in the context of}} mesh compression has been on improving compression ratio. However, in this context the use of Javascript on the Web and low power CPUS in mobile applications led to critical computational costs. Progressive decoding improves the user experience by providing a simplified version of the model that refines with time, and it 2 ̆ 7 s able to mask latency. Current approaches do so at very poor compression rates or at additional computational cost. The need for better performing algorithms is especially evident with this class of methods where Limper [Limper et al. 2013 b] demonstrated how decoding time becomes a limiting factor even at moderately low bandwidths. In this paper we present a novel multi-resolution WebGL based rendering algorithm which combines progressive loading, view-dependent resolution and mesh <b>compression,</b> providing high <b>frame</b> <b>rates</b> and a decoding speed of million of triangles per second in Javascript. This method is parallelizable, robust to non-manifold meshes, and scalable to very large models...|$|R
5000|$|Decompression (sometimes <b>compression)</b> <b>frame</b> time {{uniformity}} - Big {{differences in}} this value can cause annoyingly jerky playback.|$|R
40|$|Figure 1 : Progressive {{refinement}} of the Happy Buddha: {{on the upper}} left corner the size downloaded, on the upper right corner the number of triangles in the refined model. The header and index amount to 8 KB Efficient transmission of 3 D data to Web clients and mobile applications remains a challenge due to limited bandwidth. Most of the research focus {{in the context of}} mesh compression has been on improving compression ratio. However, in this context the use of Javascript on the Web and low power CPUS in mobile applications led to critical computational costs. Progressive decoding improves the user experience by providing a simplified version of the model that refines with time, and it’s able to mask latency. Current approaches do so at very poor compression rates or at additional computational cost. The need for better performing algorithms is especially evident with this class of methods where Limper [Limper et al. 2013 b] demonstrated how decoding time becomes a limiting factor even at moderately low bandwidths. In this paper we present a novel multi-resolution WebGL based rendering algorithm which combines progressive loading, view-dependent resolution and mesh <b>compression,</b> providing high <b>frame</b> <b>rates</b> and a decoding speed of million of triangles per second in Javascript. This method is parallelizable, robust to non-manifold meshes, and scalable to very large models...|$|R
40|$|The {{design and}} {{analysis}} of robust networking protocols that offer useful performance guarantees requires accurate traffic source models. In this paper we study the problem of characterizing and modeling the arrival process of compressed video. We extend earlier works in this area by including a factor that has previously been ignored, the effect of video capture rate on traffic characterization. Dynamic changes in available bandwidth due to the addition and/or removal of connections can trigger re-negotiation of bandwidth between the applications and the network. Such re-negotiation may result in applications changing the capture rate of video sequences, thus effecting the traffic generation process. We show that for several popular video coding schemes, the bit rate distribution at {{the output of the}} encoder changes with the capture and compression rate. Using a combination of distributions, and exploiting knowledge of the underlying compression algorithms we characterize variable bit rate (VBR) video by application type, <b>compression</b> algorithm, and <b>frame</b> <b>rate.</b> We conclude that no single distribution can describe all video traffic, and as an alternate suggest a three dimensional matrix in which each dimension represents a different video classification aspect. Each entry in this matrix is a distribution type that best fits the given combination of the aspects. We use this result to show how the problem of network capacity planning may be tackled...|$|R
50|$|With the Hand Unit most {{important}} recording {{features of the}} camera can be remote controlled: START/STOP, <b>frame</b> <b>rate</b> adjustment, <b>frame</b> <b>rate</b> ramp settings, <b>frame</b> <b>rate</b> jumps. The ergonomic design allows comfortable one hand use to start and stop recording. With the well fitted hand wheel the <b>frame</b> <b>rate</b> can be changed smoothly while recording.|$|R
40|$|Although {{motion is}} a {{defining}} feature of moving images, {{it is also}} one of their most problematic aspects due to blurred or partially stuttering images (strobe effect) at the standard <b>frame</b> <b>rate</b> of 24 <b>frames</b> per second (fps). This research was conducted to test the aesthetic and perceptual consequences of higher <b>frame</b> <b>rates</b> in narrative films. In a first step, typical camera movements were recorded at different <b>frame</b> <b>rates</b> and shutter angles to compare assumptions about production aesthetics. Film sequences were then tested using questionnaires and eye-tracking measurements in a cinema experiment involving 69 participants. The results showed that while participants valued the enhanced image quality of higher <b>frame</b> <b>rates,</b> they <b>rated</b> the standard <b>frame</b> <b>rate</b> as more realistic. Movements recorded with higher <b>frame</b> <b>rates</b> seem slower. In general, high <b>frame</b> <b>rates</b> produced more perceptual exploration (e. g. higher number of fixations). Second, a complete short film was shot at 96 fps and finished in different versions (96 fps, 48 fps, 24 fps, and a variable <b>frame</b> <b>rate).</b> All <b>frame</b> <b>rate</b> conversions were carried out in postproduction. The version with a variable <b>frame</b> <b>rate</b> was produced to assess its aesthetic potential. Several presentations and subsequent discussions with film professionals, experts, and students revealed an initial preference for the “cinematic look” of the standard <b>frame</b> <b>rate</b> of 24 fps. Some film professionals believe that both staging and editing need to be adapted to higher <b>frame</b> <b>rates.</b> This paper discusses the aesthetic, perceptual, and artistic consequences of higher <b>frame</b> <b>rates...</b>|$|R
40|$|We {{conducted}} a subjective quality assessment experiment {{to measure the}} impact of video <b>frame</b> <b>rate</b> decimation and variation in relation with impairment duration but also with content motion and texture. We found that for intermediate and high <b>frame</b> <b>rate</b> values, quality was similar independently from {{the duration of the}} <b>frame</b> <b>rate</b> decimation. On the other hand, for very low <b>frame</b> <b>rates,</b> quality decreased as the duration of the <b>frame</b> <b>rate</b> decimation increased. Our results also do not confirm the traditional thinking of higher motion content requiring a higher <b>frame</b> <b>rate</b> to produce a given level of quality. Our observations indicate that for a given <b>frame</b> <b>rate,</b> perceived quality does not necessarily increase with decreasing motion speed and that a reduction of the temporal resolution over the entire video does not lead necessarily to a significant loss of quality. Index Terms — MOS, <b>frame</b> <b>rate</b> decimation, jerkiness, mobile video broadcasting, video-conferencing. 1...|$|R
5000|$|Telecine: {{a method}} for {{converting}} film <b>frame</b> <b>rates</b> to television <b>frame</b> <b>rates</b> using interlacing ...|$|R
50|$|Digital Cinema Initiatives has {{published}} a document outlining recommended practice for high <b>frame</b> <b>rate</b> digital cinema. This document outlines the <b>frame</b> <b>rates</b> and resolutions {{that can be used}} in high <b>frame</b> <b>rate</b> digital theatrical presentations with currently available equipment.|$|R
50|$|The XH-A1S is {{a native}} HDV {{that is capable of}} {{recording}} 1080/24p, 1080/30p. When using the 1080/24p, or 1080/30p features, you should use the 24F (24 <b>frame</b> <b>rate)</b> or 30F (30 <b>frame</b> <b>rate)</b> mode in the Camera Setup menu. This camera is capable of shooting sports, television, music videos, commercials, and movies. (The 60 <b>frame</b> <b>rate</b> option is generally the best for sports.) Although this is a Video Camera, the 24F, or 24 <b>frame</b> <b>rate</b> option is usually the best option for replicating film. 24 <b>frame</b> <b>rate</b> is generally what is used when shooting Film.|$|R
50|$|The new {{standards}} supports 1080p at 50, 59.94 and 60 frames per second; such <b>frame</b> <b>rates</b> require H.264/AVC High Profile Level 4.2, while standard HDTV <b>frame</b> <b>rates</b> only require Levels 3.2 and 4, and SDTV <b>frame</b> <b>rates</b> require Levels 3 and 3.1.|$|R
30|$|Several {{studies on}} {{audiovisual}} perception {{have been conducted}} starting in the 80 s (summarized in Kohlrauch and van de Par [1]). However, the first audiovisual quality models {{to be found in}} the literature appeared as late as in the 90 s. At this time, they addressed either analog degradations, such as audio and video noise—this is the case for Bellcore's [2, 3] and Beerends' models [4]—or compression artifacts, such as blockiness—this is the case for France Telecom's [5], NTIA-ITS' [6, 7], and Hands' [8] models. For an overview of audiovisual quality models covering analog and compression degradations, see [9]. The interest in modelling audiovisual quality is currently rising again, reflected, for instance, by standardization activities such as the Multimedia Phase II project of the Video Quality Expert's Group (VQEG), which intends to evaluate audiovisual quality models for multimedia applications (unfortunately, to the knowledge of the authors, no citable document describing the VQEG Multimedia Phase II has been published at the time of writing this paper). In addition, Ries et al. [10] and Winkler and Faller [11] have recently developed audiovisual quality models for mobile applications, but the reported model versions do not yet cover the effect of transmission errors. This latter point is problematic since, in the case of the time-varying degradation due to transmission errors, the impact of audio and video quality on the overall audiovisual quality as well as their interaction might differ from the case of compression artifacts. In [12], Belmudez et al. address the impact of transmission errors in addition to <b>compression</b> and <b>frame</b> <b>rate</b> artifacts but for interactive scenarios and small video resolutions, which is not suitable for our application. None of the above-mentioned models addresses HD video, a format for which we expect video quality to play a more important role than for smaller formats. As a consequence, we have developed a new audiovisual quality model which covers all IPTV-typical degradations—mainly audio and video compression artifacts and packet loss—and which is applicable to both SD and HD. Based on the quality perception tests conducted during model development, we have analyzed the influence of the degradation type and of the audiovisual content on the quality impact of audio and video.|$|R
5000|$|Bluesky <b>Frame</b> <b>Rate</b> Converter - a DirectShow Filter {{that can}} convert the <b>frame</b> <b>rate</b> using AMD Fluid Motion.|$|R
50|$|In {{motion picture}} technology—either film or video—high <b>frame</b> <b>rate</b> (HFR) refers to higher <b>frame</b> <b>rates</b> than typical prior practice.|$|R
30|$|The target <b>frame</b> <b>rate</b> of each {{recognition}} {{agent is}} set to 30 [*]fps. Then, the <b>frame</b> <b>rate</b> control is started.|$|R
30|$|Average <b>Frame</b> <b>Rate</b> (in <b>frames</b> per second). This {{measures}} {{the number of}} complete frames received per second, averaged over the duration of a measurement trial (the higher, the better). The <b>frame</b> <b>rate</b> may range from 0 to a maximum <b>frame</b> <b>rate</b> of 30 on the Axis cameras.|$|R
40|$|In this paper, the {{requirement}} and optimization of the <b>frame</b> <b>rate</b> for myocardial elastography {{was investigated in}} normal mice and humans in vivo. Using a retrospective electrocardiogram (ECG) gating technique, the highest <b>frame</b> <b>rate</b> was 8 kHz and 481 Hz, respectively. Axial displacement and strain of myocardium were estimated using an RF speckle tracking method consisting of a 1 -D kernel in a 2 -D search. The <b>frame</b> <b>rate</b> was then decimated to study its effects on the image quality of myocardial elastography, in terms of elastographic signal-to-noise (SNRe) and correlation coefficient. Trade-offs between SNRe and effective <b>frame</b> <b>rate</b> were identified in the murine case. The optimum range of <b>frame</b> <b>rate</b> {{was found to be}} between 2000 and 2700 Hz, or equivalently, 250 - 350 frames per cardiac cycle (fpc). In the human case, the image quality increased monotonously with the <b>frame</b> <b>rate.</b> A <b>frame</b> <b>rate</b> higher than 480 Hz (i. e., 350 fpc) was thus required for both systole and diastole. © 2007 IEEE. link_to_subscribed_fulltex...|$|R
5000|$|Theora streams with {{different}} <b>frame</b> <b>rates</b> can be chained {{in the same}} file, but each stream has a fixed <b>frame</b> <b>rate.</b>|$|R
500|$|Many reviewers {{considered}} the technical enhancements, {{such as the}} increased <b>frame</b> <b>rate,</b> a welcome advancement from the original game. Turi of Game Informer felt that the <b>frame</b> <b>rate</b> [...] "dramatically elevate" [...] the game above the original. Jim Sterling of The Escapist complimented the upgraded <b>frame</b> <b>rate,</b> commenting that the original <b>frame</b> <b>rate</b> is a [...] "noticeably inferior experience". IGN's Moriarty stated that, though the change was initially [...] "jarring", he appreciated it through further gameplay. Tom Hoggins of The Daily Telegraph echoed these statements, feeling as though the increased <b>frame</b> <b>rate</b> heightened {{the intensity of the}} gameplay. Metros David Jenkins felt that the increased <b>frame</b> <b>rate</b> is almost imperceptible, though stating that it is [...] "definitely an improvement". Philip Kollar of Polygon appreciated the game's improved textures and loading times.|$|R
3000|$|... (fps) is {{the target}} <b>frame</b> <b>rate.</b> The <b>frame</b> <b>rate</b> is {{stabilized}} by controlling the load of the recognition modules. Control inputs are determined {{in accordance with the}} response from <b>frame</b> <b>rate</b> detector. The feedback control is applied as long as the control deviation does not fall within the minimal error range.|$|R
40|$|The <b>rate</b> {{at which}} <b>frames</b> are {{rendered}} {{in a computer}} game directly impacts player performance, influencing both the game playability and enjoyability. However, despite the importance of <b>frame</b> <b>rate</b> and the wide-spread popularity of computer games, {{to the best of}} our knowledge, there is little quantitative understanding of the effects of <b>frame</b> <b>rate</b> on player performance in computer games. This paper provides a unique classification of actions in First Person Shooter (FPS) games based on interaction requirements that allow qualitative assessment of the impact of <b>frame</b> <b>rates</b> on player performance. This qualitative assessment is supported by quantitative analysis from two large user studies that measure the effects of <b>frame</b> <b>rate</b> on the fundamental player actions in a FPS game. Nearly 100 users participated in the two user study experiments, providing performance and perception data over a range of <b>frame</b> <b>rates</b> commonly studied for video streaming and inclusive of <b>frame</b> <b>rates</b> found in many computer game platforms. In general, the analysis shows that actions that require precise, rapid response, such as shooting, are greatly impacted by degradations in <b>frame</b> <b>rates,</b> while actions with lower precision and response requirements, such as moving, are more tolerant of low <b>frame</b> <b>rates.</b> These insights into the effects of <b>frame</b> <b>rates</b> on player performance can guide players in their choice for game settings and new hardware purchases, and inform system designers in their development of new hardware...|$|R
30|$|Increasing {{the number}} of p-pictures also {{increases}} the scalability ratio in terms of bit <b>rate</b> and <b>frame</b> <b>rate.</b> Depending on the sequence the IpP coding allows for up to 33 % bit rate reduction, which means that 33 % of the total bit rate lies in the temporal enhancement layer. The <b>frame</b> <b>rate</b> can be reduced by half. Using IppP allows for bit rate reduction of slightly over 50 %, whereas the <b>frame</b> <b>rate</b> {{can be reduced to}} one-third of the full <b>frame</b> <b>rate.</b> The IpppP coding structure can offer two temporal enhancement layers. Dropping the first temporal layer results in similar scalability features as observed with IpP coding. Additional dropping the second temporal layer allows a bit rate reduction up to 60 % with the football sequence and a <b>frame</b> <b>rate</b> reduction down to one-fourth of the full sequence <b>frame</b> <b>rate.</b>|$|R
50|$|Internally, the EVR uses a mixer object for {{mixing the}} streams. It can also deinterlace the output and apply color correction, if required. The composited frame is handed {{off to a}} {{presenter}} object, which schedules them for rendering onto a Direct3D device, which it shares with the DWM and other applications using the device. The <b>frame</b> <b>rate</b> of the output video is synchronized with the <b>frame</b> <b>rate</b> of the reference stream. If {{any of the other}} streams (called substreams) have a different <b>frame</b> <b>rate,</b> EVR discards the extra frames (if the substream has a higher <b>frame</b> <b>rate),</b> or uses the same frame more than once (if it has a lower <b>frame</b> <b>rate).</b>|$|R
50|$|In {{moving picture}} (TV) {{the number of}} frames scanned per second {{is known as the}} <b>frame</b> <b>rate.</b> The higher the <b>frame</b> <b>rate,</b> the better the sense of motion. But again, {{increasing}} the <b>frame</b> <b>rate</b> introduces technical difficulties. So the <b>frame</b> <b>rate</b> is fixed at 25 (System B/G) or 29.97 (System M). To increase the sense of motion it is customary to scan the very same frame in two consecutive phases. In each phase only half of the lines are scanned; only the lines with odd numbers in the first phase and only the lines with even numbers in the second phase. Each scan is known as a field. So the field rate is two times the <b>frame</b> <b>rate.</b>|$|R
30|$|Temporal scalability. The {{base layer}} is coded {{at a low}} <b>frame</b> <b>rate.</b> By adding {{enhancement}} layers, the <b>frame</b> <b>rate</b> of the decoded sequence can be increased.|$|R
30|$|The <b>frame</b> <b>rates</b> {{at source}} sensor nodes are {{calculated}} {{in a different}} manner for each protocol. SUIT and FCE use fuzzy logic-based approach whereas QCC uses local buffer occupancy for deciding on the congestion level. According to the congestion level, the <b>frame</b> <b>rate</b> is assigned dynamically while generating images. In Figure  10, the average calculated <b>frame</b> <b>rate</b> at the source sensor nodes is illustrated. All of the protocols decrease the <b>frame</b> <b>rate</b> as the network is getting congested. QCC estimates the congestion level optimistically and therefore calculates higher <b>frame</b> <b>rates</b> than the others. On the other hand, FCE estimates the congestion in a pessimistic manner and calculates lower <b>frame</b> <b>rates</b> than the others. SUIT provides a balanced congestion level via its efficient fuzzy logic-based congestion estimation technique. Therefore, SUIT provides better performance than its competitors in all QoS metrics, except of the average received frame quality.|$|R
