49|394|Public
5|$|The Blink Hacker Group, associating {{themselves}} with the Anonymous group, claimed to have hacked the Thailand prison websites and servers. The <b>compromised</b> <b>data</b> has been shared online, with the group claiming that they give the data back to Thailand Justice and the citizens of Thailand as well. The hack was done in response to news from Thailand about the mistreatment of prisoners in Thailand.|$|E
5000|$|... <b>compromised</b> <b>data</b> {{tracking}} and data extraction from botnets; automatic search and monitoring of [...] "underground" [...] forums; ...|$|E
5000|$|Databases that do {{not allow}} {{downloads}} of all data in one attempt, limiting the amount of <b>compromised</b> <b>data</b> ...|$|E
40|$|Adenoviruses {{are widely}} used for overexpressing {{proteins}} in primary mammalian cells. Incorporation of the early viral gene, E 1 A, or viral cross-contamination can occur during amplification, and identification of these products is crucial as the transcription of unwanted genetic material can impact cell function and <b>compromise</b> <b>data</b> interpretation. Here we report methods for evaluation of contaminating adenovirus and E 1 viral DNA...|$|R
3000|$|... {{represents}} {{a measure of}} <b>compromise</b> between <b>data</b> fit and model smoothness. To find a suitable value of [...]...|$|R
30|$|Malicious {{users can}} either collect {{individual}} data segments or re-assemble the complete <b>data</b> to <b>compromise</b> the <b>data</b> confidentiality.|$|R
50|$|The site {{features}} a partner portal for the secure and verified listing of <b>compromised</b> <b>data</b> and standardised backdoor to be used.|$|E
50|$|The pepper value adds {{security}} to {{a collection of}} <b>compromised</b> <b>data</b> because it increases the amount of computations to determine one piece of data.|$|E
5000|$|Security {{improves}} as {{encrypted data}} moves further in, toward the network core. As it approaches the enterprise, data is checked {{as it passes}} through protected [...] firewalls and other security points, where viruses, <b>compromised</b> <b>data,</b> and active hackers can be caught early on.|$|E
40|$|Spontaneous {{formation}} of peer-to-peer agent-based data mining systems seems a plausible scenario {{in years to}} come. However, the emergence of peer-to-peer environments further exacerbates privacy and security concerns that arise when performing data mining tasks. We analyze potential threats to data privacy in a peer-topeer agent-based distributed data mining scenario, and discuss inference attacks which could <b>compromise</b> <b>data</b> privacy in a peer-to-peer distributed clustering scheme known as KDEC...|$|R
30|$|Laser Doppler flowmetry (LDF) is an {{invasive}} fiber-optic {{laser probe}} {{that can be}} inserted into brain parenchyma to measure regional perfusion of a tissue volume of approximately 1 mm 3. Contrary to TDF, LDF provides only qualitative—but not quantitative—measurements of microvascular perfusion [23]. Routine use of this optical technique remains complex and artifacts, such as heterogeneity of microvascular architecture, probe motion, room temperature, and strong external light and sound may <b>compromise</b> <b>data</b> quality [24].|$|R
40|$|Invalid <b>data</b> may <b>compromise</b> <b>data</b> quality. We {{examined}} how decisions made to handle these data {{may affect the}} relationship between Internet use and HIV risk behaviors {{in a sample of}} young {{men who have sex with}} men (YMSM). We recorded 548 entries during the 3 -month period and created six analytic groups (i. e., full sample, entries initially tagged as valid, suspicious entries, valid cases mislabeled as suspicious, fraudulent data, and total valid cases) using data quality decisions. We compared these groups on the sam-ple’s composition and their bivariate relationships. Forty-one cases wer...|$|R
50|$|Despite their advantages, cabled {{observatories}} can (and do) relay <b>compromised</b> <b>data</b> to scientists, {{particularly when}} located in remote {{parts of the}} ocean. Factors such as instrumental malfunction and biofouling are often responsible for this. Systematic improvements, to lessen the impacts of such factors, are currently being studied by groups such as Ocean Networks Canada.|$|E
5000|$|According to the FBI, losses due to laptop theft totaled {{more than}} $3.5 million in 2005. The Computer Security Institute/FBI Computer Crime & Security Survey found the average theft of a laptop to cost a company $31,975. [...] In a study {{surveying}} 329 {{private and public}} organizations published by Intel in 2010, 7.1% of employee laptops were lost or stolen {{before the end of}} their usefulness lifespan. Furthermore, it was determined that the average total negative economic impact of a stolen laptop was $49,256—primarily due to <b>compromised</b> <b>data,</b> and efforts to retroactively protect organizations and people from the potential consequences of that <b>compromised</b> <b>data.</b> The total cost of lost laptops to all organizations involved in the study was estimated at $2.1 billion. [...] Of the $48B lost from the U.S. economy as a result of data breaches, 28% resulted from stolen laptops or other portable devices.|$|E
50|$|The Blink Hacker Group, associating {{themselves}} with the Anonymous group, claimed to have hacked the Thailand prison websites and servers. The <b>compromised</b> <b>data</b> has been shared online, with the group claiming that they give the data back to Thailand Justice and the citizens of Thailand as well. The hack was done in response to news from Thailand about the mistreatment of prisoners in Thailand.|$|E
40|$|A {{large number}} of {{scientific}} researches and industrial applications commonly suffer from missing data. Some inappropriate techniques of missing value treatment <b>compromise</b> <b>data</b> quality, which detrimentally influences the knowledge discovery. In this paper, we propose a missing data completion method named CBGMI. Firstly, it separates the nonmissing data instances into several clusters by excluding the missing-valued entries. Then, it utilizes the entropy of the proximal category for each incomplete instance {{in terms of the}} similarity metric based on gray relational analysis. Experiments on UCI datasets and aerospace datasets demonstrate that the superiority of our algorithm to other approaches on validity...|$|R
30|$|Device, Aggregator: NASPI network (NASPInet) is logically {{capable of}} {{integrating}} WAMS across multiple geographically distant organizations using phasor gateways (PGWs). The attacks {{at this level}} <b>compromise</b> <b>data</b> integrity, targeting devices from individual PMUs to PDCs, SuperPDCs or even PGWs. Some attacks include: ① tampering the signal measurement units of devices through interference; ② illicitly changing the calibration of devices to report erroneous readings; ③ forging data to reflect wrong measurements; and ④ GPS spoofing by broadcasting fabricated signals to the receiver to yield erroneous synchronization of phasors computed, modifying satellite position, or replaying legitimate GPS signals at later timestamps [54].|$|R
30|$|Both cases {{require a}} large amount of {{computation}} to brute-force search the complete space. Hence it is not trivial for a malicious user to <b>compromise</b> the <b>data</b> confidentiality by re-assembling the original data.|$|R
50|$|The 2014 JPMorgan Chase {{data breach}} was a {{cyber-attack}} against American bank JPMorgan Chase that {{is believed to}} have <b>compromised</b> <b>data</b> associated with over 83 million accounts - 76 million households (approximately two out of three households in the country) and 7 million small businesses. The data breach {{is considered one of the}} most serious intrusions into an American corporation's information system and one of the largest data breaches in history.|$|E
50|$|In 2012 the Indian {{broadcasting}} NDTV filed {{a lawsuit}} against Television Audience Measurement (TAM), a joint venture of the former competitors Nielsen and Kantar Media Research which for years has provided the only TV audience measurement system in India. WPP Plc was listed among the defendants as the holding group of Kantar and IMRB. NDTV's lawsuit, filed in the New York State Supreme Court under the Foreign Corrupt Practices Act and seeking $1.4 billion for negligence and hundreds of millions for interference and breach of fiduciary duty, cited a public conversation between Vikram Chandra, CEO of NDTV, and Martin Sorrell, CEO of WPP, in which Chandra had described ways in which the system was prone to tampering and bribery; his request for a halt to the publication of the allegedly <b>compromised</b> <b>data</b> was unsuccessful.|$|E
5000|$|In 2013 PureVPN faced a {{controversy}} regarding a fake email which {{was sent to}} all the active users, that stated [...] "Dear customer, it is {{to inform you that}} due to an incident we had to close your account permanently. We are no longer able to run an anonymization service due to legal issues we are facing". The email was later reported to be spam and PureVPN responded to it saying [...] " [...] It has confirmed that it was a zero-day exploit related to a vulnerability at WHMCS. The <b>compromised</b> <b>data</b> were email addresses and names for a subset of registered users, and “no billing information like Credit Card or other sensitive personal information was compromised.” In the year 2016 CNET listed PureVPN as one of the better VPN service to look out for, by adding its name with BufferedVPN and Hotspot Shield.|$|E
5000|$|... {{decrease}} {{the chance for}} <b>compromised</b> watch list <b>data</b> by limiting its distribution ...|$|R
40|$|We {{present a}} data storage scheme for sensor {{networks}} that achieves {{the targets of}} encryption and distributed storage simultaneously. We partition the data to be stored into numerous pieces such that at least a specific number of them have to be brought together to recreate the data. The procedure for creation of partitions does not use any encryption key and the pieces are implicitly secure. These pieces are then distributed over random sensors for storage. Capture or malfunction {{of one or more}} (less than a threshold number of sensors) does not <b>compromise</b> the <b>data.</b> The scheme provides protection against <b>compromise</b> of <b>data</b> in specific sensors due to physical capture or malfunction. Comment: 9 page...|$|R
30|$|Special {{internal}} management (being incorporating into FTL or flash file systems) {{is usually}} required {{to handle the}} characteristics of flash memory, which may easily lead to deniability <b>compromise</b> or <b>data</b> leakage, making it challenging to provide deniability/secure deletion on flash-based storage systems.|$|R
5000|$|Over {{the past}} several decades (leading up to 2005), {{organizations}} (banks, governments, schools, manufacturers and others) have increased their reliance more on [...] "Open Systems" [...] and less on [...] "Closed Systems". For example, 25 years ago, a large bank might have most if not all of its critical data housed in an IBM mainframe computer (a [...] "Closed System"), but today, that same bank might store a substantially greater portion of its critical data in spreadsheets, databases, or even word processing documents (i.e., [...] "Open Systems"). The problem with Open Systems is, primarily, their unpredictable nature. The very nature of an Open System is that it is exposed to potentially thousands if not millions of variables ranging from network overloads to computer virus attacks to simple software incompatibility. Any one, or indeed several in combination, of these factors may result in either lost data and/or <b>compromised</b> <b>data</b> backup attempts. These types of problems do not generally occur on Closed Systems, or at least, in unpredictable ways. In the [...] "old days", backups were a nicely contained affair. Today, because of the ubiquity of, and dependence upon, Open Systems, an entire industry has developed around data protection. Three key elements of such data protection are Validation, Optimization and Chargeback.|$|E
50|$|NIST {{requires}} {{at least}} 128 bits of random salt and a NIST-approved cryptographic function, {{such as the}} SHA series or AES (MD5 is not approved). Although high throughput is a desirable property in general-purpose hash functions, {{the opposite is true}} in password security applications in which defending against brute-force cracking is a primary concern. The growing use of massively-parallel hardware such as GPUs, FPGAs, and even ASICs for brute-force cracking has made the selection of a suitable algorithms even more critical because the good algorithm should not only enforce a certain amount of computational cost not only on CPUs, but also resist the cost/performance advantages of modern massively-parallel platforms for such tasks. Various algorithms have been designed specifically for this purpose, including bcrypt, scrypt and, more recently, argon2 (the winner of the PHC contest). The large-scale Ashley Madison data breach in which roughly 36 million passwords hashes were stolen by attackers illustrated the importance of algorithm selection in securing passwords. Although bcrypt was employed to protect the hashes (making large scale brute-force cracking expensive and time-consuming), {{a significant portion of the}} accounts in the <b>compromised</b> <b>data</b> also contained a password hash based on the general-purpose md5 algorithm which made it possible for over 11 million of the passwords to be cracked in a matter of weeks.|$|E
40|$|High Integrity in Sensor Networks {{studies the}} effect of <b>compromised</b> <b>data</b> on sensor network data {{analysis}} and data fusion algorithms. We are {{at the start of}} a study to better understand the <b>compromised</b> <b>data</b> itself so that we can model it in such a way to characterize this affect on analysis. The accelerated life experiment we describe here was instrumented to see how the accuracy of a temperature and humidity sensor changes across mote and sensor systems and across time as the battery dies. We found that the error in the measurements was stable over time, but the bias in the measurements varied greatly across the motes. The experiment gave us more information on the important variables to include in calibration function, so that we can estimate the true measured value from the reported value of the sensor...|$|E
50|$|Another {{side effect}} of {{exhausted}} address tables is the <b>compromise</b> of <b>data.</b> The security considerations {{are discussed in the}} MAC flooding—one of several causes of unicast floods. If an end user is running a packet sniffer, the flooded packets could be captured and viewed.|$|R
30|$|These {{variables}} represent {{aspects of}} the Italian population’s socioeconomic characteristics that increase or decrease social vulnerability. Our selection reflects a <b>compromise</b> between <b>data</b> availability for the three census periods considered (the same variables for each year) and their contribution to better explain the social vulnerability of the Italian population.|$|R
40|$|Copyright © 2013 Jing Tian et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. A large number of scientific researches and industrial applications commonly suffer from missing data. Some inappropriate techniques of missing value treatment <b>compromise</b> <b>data</b> quality, which detrimentally influences the knowledge discovery. In this paper, we propose amissing data completionmethod namedCBGMI. Firstly, it separates the nonmissing data instances into several clusters by excluding the missing-valued entries. Then, it utilizes the entropy of the proximal category for each incomplete instance {{in terms of the}} similaritymetric based on gray relational analysis. Experiments onUCI datasets and aerospace datasets demonstrate that the superiority of our algorithm to other approaches on validity. 1...|$|R
40|$|This is a pre-publication {{version of}} a chapter {{published}} in an edited book on <b>Compromised</b> <b>Data.</b> It explores the politics of data {{in the context of}} a recent science controversy, which gained a reach and traction across social media. The chapter specifically focuses on the post publication peer review of two journal articles associated with the John Bargh priming controversy...|$|E
30|$|Although {{encryption}} and authentication protocols {{can prevent}} <b>compromised</b> <b>data</b> transmission when the selected relay is attacked, these measures consume scarce bandwidth and reduce system throughput. It would be desirable to choose only trustworthy nodes as relays and only authenticate the packets through the nodes that {{are prone to}} attack. To achieve this goal, {{we would need to}} design a quantitative approach to analyze the actions of the attackers so as to make appropriate decisions on relay selection and the extent that encryption and authentication protocols are required.|$|E
40|$|There is an {{emerging}} {{consensus that the}} nation’s electricity grid is vulnerable to cyber attacks. This vulnerability arises from the increasing reliance on using remote measurements, transmitting them over legacy data networks to system operators who make critical decisions based on available data. Data integrity attacks are a class of cyber attacks that involve a compromise of information that is processed by the grid operator. This information can include meter readings of injected power at remote generators, power flows on transmission lines, and relay states. These data integrity attacks have consequences only when the system operator responds to <b>compromised</b> <b>data</b> by redispatching generation under normal or contingency protocols. These consequences include (a) financial losses from sub-optimal economic dispatch to service loads, (b) robustness/resiliency losses from placing the grid at operating points that {{are at greater risk}} from contingencies, and (c) systemic losses resulting from cascading failures induced by poor operational choices. This paper is focused on understanding the connections between grid operational procedures and cyber attacks. We first offer two examples to illustrate how data integrity attacks can cause economic and physical damage by misleading operators into taking inappropriate decisions. We then focus on unobservable data integrity attacks involving power meter data. These are coordinated attacks where the <b>compromised</b> <b>data</b> are consistent with the physics of power flow, and are therefore passed by any bad data detection algorithm. We develop metrics to assess the economic impact of these attacks under re-dispatch decisions using optimal power flow methods. These metrics can be use to prioritize the adoption of appropriate countermeasures including PMU placement, encryption, hardware upgrades, and advance attack detection algorithms...|$|E
5000|$|Arbitrarily modify {{values in}} a {{database}} through SQL injection. The {{impact of this}} can range from website defacement to serious <b>compromise</b> of sensitive <b>data.</b>|$|R
30|$|It can be {{seen from}} Table  7 that {{successful}} cyberattacks <b>compromise</b> synchrophasor <b>data</b> quality since the security requirements are violated [146]. Given synchrophasors use TCP/UDP on the transport layer for their communications, attacks typically possible on TCP/IP stack like DoS, MITM, packet replay or spoofing are possible in synchrophasor domains as well.|$|R
40|$|Abstract. In {{an earlier}} paper we {{introduced}} Anchoring, {{a new approach}} for handling indeterminate location in a spatial information system. Here we develop it further, showing how Anchoring {{can be made to}} work in real systems. In particular, we specify a new kind of locational component for a spatial data model. We then show how that component can be implemented into an object-relational database with extensions conforming to the OpenGIS Consortium’s Simple Feature Model. We argue that Anchoring, in contrast to other formalisms for handling indeterminate location, is better suited to the needs of a spatial data provider who, in supplying data to different organisations, needs to be authoritative, and thus does not want to <b>compromise</b> <b>data</b> quality by representing indeterminate data with unwarranted precision. Anchoring, in providing new ways to describe spatiality, allows that we state only what we know for certain...|$|R
