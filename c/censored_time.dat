24|543|Public
30|$|As the {{connectors}} fail fast at {{the progressive}} stress, considering the <b>censored</b> <b>time</b> for the CSALT [20] and the step stress ALT (SSALT) [21], this paper takes τ[*]=[*] 1000  h as the <b>censored</b> <b>time.</b>|$|E
40|$|The task of {{predicting}} prostate-specific androgen (PSA) recurrence following radical prostatectomy {{is important for}} the surveillance of patients with prostate cancer. This regression problem is {{complicated by the fact that}} data is censored, and there is no standard measurement of error for censored data. This paper applies modified regression trees, called <b>Censored</b> <b>Time</b> Trees (cTT™), to predict time to PSA recurrence. In order to assess the performance of cTT™, we explored different error measurements, such as the concordance index, AUC (area under the Receiver Operating Characteristic curve), sensitivity and specificity, and average error. Bagging of <b>Censored</b> <b>Time</b> Trees ™ improves their performance. Bagging cTT ™ also performs slightly better than support vector regression and linear programming, both modified for censored data. 1...|$|E
40|$|AbstractThe {{time series}} […,x- 1 y- 1,x 0 y 0,x 1 y 1,…]> {{which is the}} product of two {{stationary}} time series xt and yt is studied. Such sequences arise in the study of nonlinear time series, <b>censored</b> <b>time</b> series, amplitude modulated time series, time series with random parameters, and time series with missing observations. The mean and autocovariance function of the product sequence are derived...|$|E
5000|$|Random (or non-informative) {{censoring}} is {{when each}} subject has a <b>censoring</b> <b>time</b> that is statistically independent of their failure time. The observed value is the minimum of the censoring and failure times; subjects whose failure time {{is greater than}} their <b>censoring</b> <b>time</b> are right-censored.|$|R
5000|$|Time is {{indicated}} by the variable [...] "time", which is the survival or <b>censoring</b> <b>time</b> ...|$|R
40|$|Abstract In medical statistics, the {{survival}} function {{is a relationship}} be-tween proportion and time. Proportion is the proportion of subjects which are still surviving at time, t. The term is also used in other fields and is known as “units still operating ” instead of subjects still alive. In this paper, an estimator of {{the survival}} time, Xi, for the i th patient on a clinical trial with <b>censoring</b> <b>time,</b> Ti (dropping out of the trial) and its properties, when both survival and <b>censoring</b> <b>time</b> are exponentially distributed, considered. A simulation is carried out to determine {{the performance of the}} estimators for different combinations of parameters related to the survival and <b>censoring</b> <b>times...</b>|$|R
40|$|Description This package {{can be used}} {{to conduct}} {{asymptotic}} and empirical power and sample size calculations for SNP association studies with right <b>censored</b> <b>time</b> to event outcomes Depends R (> = 2. 13. 2), survival (> = 2. 36 - 9), Rcpp (> = 0. 9. 10), lattice (> = 0. 20 - 0), foreach (> = 1. 3. 2), xtable (> = 1. 7 - 0...|$|E
40|$|The {{time series}} [ [...] .,x- 1 y- 1,x 0 y 0,x 1 y 1, [...] . ]> {{which is the}} product of two {{stationary}} time series xt and yt is studied. Such sequences arise in the study of nonlinear time series, <b>censored</b> <b>time</b> series, amplitude modulated time series, time series with random parameters, and time series with missing observations. The mean and autocovariance function of the product sequence are derived. time series product nonlinear...|$|E
40|$|The aim of {{this study}} was to make a {{comparison}} among existing estimation methods (Kaplan-Meier, Nelson-Aalen and Regression on Ordered Statistics (ROS)) for randomly left <b>censored</b> <b>time</b> to event data under selected distributions and for different level of censoring and sample sizes in order to determine the strength of these methods based on simulated data. Comparisons among the methods are made on the basis of unbiasedness and Monte Carlo Standard Error of the summary statistics (mean time to event) obtained by those methods under different conditions...|$|E
40|$|We analyse {{the model}} in which the latent {{durations}} Ti are i. i. d. generated by a distribution F. The statistician observes Yi =min(Ti,Ci) and Ai = 1 I {Ti ≤ Ci} where Ci is a <b>censoring</b> <b>time.</b> The prior probability on F is a Dirichlet process. Hjort (1990) shows that the posterior distribution is a neutral to the right process whose hasard function is a Beta process. Lo (1993) has {{the same type of}} results with different assumptions on <b>censoring</b> <b>times.</b> For a large class of specifications on <b>censoring</b> <b>times,</b> we exhibit a representation of the posterior process which has the following form: F = j FjF j where j indexes the intervals between <b>censoring</b> <b>times,</b> the Fj ’s are product of independent Beta distributed random variables and the F j ’s are independent Dirichlet processes. Using powerful representations of Dirichlet processes (Rolin (1992) and (1993), Sethuraman (1994), Florens and Rolin (1994)) we deduce from this property a very efficient way to simulate various functionals of F. AMS MOS primary: 62 N 0...|$|R
30|$|The type-I {{censoring}} CSALT is considered, and the <b>censoring</b> <b>time</b> at each {{stress level}} is τ [14 – 19].|$|R
30|$|Define 95, 85, 75 % of {{the minimum}} among samples as the <b>censoring</b> <b>time</b> to obtain zero-failure data for size n.|$|R
30|$|Generally, {{reliability}} {{assessments are}} based on large sample size to attain statistical inferences (usually n[*]>[*] 30). As such, reliability experiments consume considerable time and money. Owing to the long lifetimes, high cost, and complex structures of components in high-speed trains, {{it is necessary to}} develop reliability assessment theory and methods suitable under small sample size. The present study proposes a new approach that combines a Bayesian method with a subjective prior distribution for zero-failure data under small sample size, and experiments confirm that this approach can be safely applied to the reliability assessment of solenoid valves in the braking systems of high-speed trains. In Section  2, the failure model and the characteristics of zero-failure data for the solenoid valve are discussed. In Section  3, a Bayesian reliability model based on the binomial distribution at <b>censored</b> <b>time</b> is developed in detail. In Section  4, a modified Weibull distribution is introduced for the solenoid valve using least squares estimation based on failure probabilities at <b>censored</b> <b>time.</b> In Section  5, a numerical simulation is performed to compare the results of the proposed method with those of MLE. In Section  6, an actual numerical case is analyzed for the solenoid valve of the braking system in high-speed trains. Section  7 provides concluding remarks.|$|E
3000|$|... are {{described}} in the monograph by Gill [5]. However, the censored dependent data appear {{in a number of}} applications. For example, repeated measurements in survival analysis follow this pattern, see Kang and Koehler [6] or Wei et al. [7]. In the context of <b>censored</b> <b>time</b> series analysis, Shumway et al. [8] considered (hourly or daily) measurements of the concentration of a given substance subject to some detection limits, thus being potentially censored from the right. Ying and Wei [9], Lecoutre and Ould-Saïd [10], Cai [11] and Liang and Uña-Álvarez [12] studied the convergence of [...]...|$|E
40|$|International audienceWe {{propose a}} novel {{approach}} for distributed statistical detection of change-points in high-volume network traffic. We consider more specifically the task of detecting and identifying the targets of Distributed Denial of Service (DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed network anomaly detection by aggregating the partial information gathered {{in a set of}} network monitors. In order to address massive data while limiting the communication overhead within the network, the approach combines record filtering at the monitor level and a nonparametric rank test for doubly <b>censored</b> <b>time</b> series at the central decision site. The performance of the DTopRank algorithm is illustrated both on synthetic data as well as from a traffic trace provided by a major Internet service provider...|$|E
40|$|A hybrid {{censoring}} {{scheme is}} a mixture of Type-I and Type-II censoring schemes. In this paper, we present two interesting results which are useful in deriving the Fisher information along with the expected <b>censoring</b> <b>times</b> and expected numbers of failures in hybrid and generalized hybrid censoring schemes. We first interpret the Type-II censoring scheme in terms of a Type-I censoring scheme so that the <b>censoring</b> <b>time</b> in hybrid <b>censoring</b> schemes can be regarded as a mixture of Type-I <b>censoring</b> <b>times.</b> We then exploit this mixture form to derive the Fisher information in hybrid censored schemes. We further establish an addition rule underlying Type-I and Type-II hybrid censoring schemes and derive the Fisher information in the case of generalized hybrid censoring schemes. Finally, we present an example to illustrate the results developed here. ...|$|R
40|$|Longitudinal data {{frequently}} {{occur in}} many studies, such as longitudinal follow-up studies. To develop statistical methods and theory {{for the analysis}} of these data, independent or noninformative observation and <b>censoring</b> <b>times</b> are typically assumed, which naturally leads to inference procedures conditional on observation and <b>censoring</b> <b>times.</b> But in many situations this may not be true or realistic; that is, longitudinal responses may be correlated with observation times as well as <b>censoring</b> <b>times.</b> This article considers the analysis of longitudinal data where these correlations may exist and proposes a joint modeling approach that uses some latent variables to characterize the correlations. For inference about regression parameters, estimating equation approaches are developed and both large-sample and final-sample properties of the proposed estimators are established. In addition, some graphical and numerical procedures are presented for model checking. The methodology is applied to a bladder cancer study that motivated this investigation...|$|R
3000|$|... {{where we}} denote {{the ranks of}} the {{observed}} failure or <b>censoring</b> <b>times</b> per each margin as r_s_i=rank(S_i) and r_t_j=rank(T_j), respectively, corresponding to the order statistics S [...]...|$|R
40|$|We {{propose a}} novel {{approach}} for distributed statistical detection of change-points in high-volume network traffic. We consider more specifically the task of detecting and identifying the targets of Distributed Denial of Service (DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed network anomaly detection by aggregating the partial information gathered {{in a set of}} network monitors. In order to address massive data while limiting the communication overhead within the network, the approach combines record filtering at the monitor level and a nonparametric rank test for doubly <b>censored</b> <b>time</b> series at the central decision site. The performance of the DTopRank algorithm is illustrated both on synthetic data as well as from a traffic trace provided by a major Internet service provider. Comment: Statistics and Computing (2011) 1 - 1...|$|E
40|$|Abstract. We {{propose a}} novel {{approach}} for distributed statistical detection of change-points in high-volume network traffic. We consider more specifically the task of detecting and identifying the targets of Distributed Denial of Service (DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed network anomaly detection by aggregating the partial information gathered {{in a set of}} network monitors. In order to address massive data while limiting the communication overhead within the network, the approach combines record filtering at the monitor level and a nonparametric rank test for doubly <b>censored</b> <b>time</b> series at the central decision site. The performance of the DTopRank algorithm is illustrated both on synthetic data as well as from a traffic trace provided by a major Internet service provider. Distributed detection and change-point detection and rank test and censored data and network anomaly detection. 1...|$|E
30|$|In this section, {{we define}} the Weibull model for {{analysing}} survival {{of patients in}} the context of human health. We confine ourselves to survival times that are the difference between a nominated start time and a declared failure (uncensored data) or a nominated end time (<b>censored</b> <b>time).</b> Let T[*]be a nonnegative random variable for a person’s survival time and t[*]be a realisation of the random variable T. Kleinbaum and Klein (2005) give some reasons for the occurrence of right censoring in survival studies, including termination of the study, drop outs, or loss to follow-up. For the censored observations, one could impute the missing survival times or assume that they are event-free. The former is often difficult, especially if the censoring proportion is large, and extreme imputation assumptions (such as all censored cases fail right after the time of censoring) may distort inferences (Leung et al. 1997; Stajduhar et al. 2009). In this study, we treat all censored cases as event-free regardless of observation time.|$|E
3000|$|If the <b>censoring</b> <b>time</b> {{is not too}} long, then K* {{is always}} equal to two, and the optimal maximum stress level ξ _H^* is always one; [...]...|$|R
40|$|We {{propose a}} way to {{estimate}} a parametric quantile function when the dependent variable, e. g. the survival <b>time,</b> is <b>censored.</b> We discuss {{one way to do}} this, transforming the problem of finding the p-quantile for the true, uncensored, survival times into a problem of finding the q-quantile for the observed, <b>censored,</b> <b>times.</b> The q-value involves the distribution of the <b>censoring</b> <b>times,</b> which is unknown. The estimation of the quantile function is done using the asymmetric L 1 technique with weights involving local Kaplan-Meier estimates of the distribution of the censoring limit...|$|R
40|$|Introduction A random {{variable}} is interval censored if {{the extent of}} an observer's knowledge is an interval containing that variable. It is convenient to distinguish several types of interval censoring (Groeneboom and Wellner, 1992). When the {{random variable}} is known to lie either {{to the left or}} right of an observed <b>censoring</b> <b>time,</b> we have case I interval censoring. Current-status data (e. g., Sun and Kalbfleisch, 1993) are case I interval censored, as are bioassay data (e. g., Cox and Snell, 1989). If one allows the <b>censoring</b> <b>time</b> to depend on covariates and parameters, binary regression models are subsumed too (e. g., Newton, Czado, and Chappell, 1996). The binary choice model used in economics has this structure (e. g., Klein and Spady, 1993). In case II interval <b>censoring,</b> two <b>censoring</b> <b>times</b> partition the real line into three sets and the 1 Address for correspondence: Department of Statistics, University of Wisconsin [...] Madison, 1210 W. Dayton St., Madison, WI...|$|R
40|$|Interval <b>censored</b> <b>time</b> to event {{outcomes}} {{arise when}} a silent event {{of interest is}} known to have occurred within a specific time period, determined by the times of the last negative and first positive diagnostic tests. The four chapters comprising this thesis are tied together by a common theme in that the outcome of interest is an interval <b>censored</b> <b>time</b> to event random variable. In Chapter 1, we describe a stratified Weibull model appropriate for interval cen- sored outcomes and implement a new R package straweib. We compare the proposed approach with the log-linear form of the Weibull regression model that is currently im- plemented in the existing R package survival, and illustrate its use by analyzing data from a longitudinal oral health study on the timing of the emergence of permanent teeth in 4430 children. In Chapter 2, we present methods to estimate the association of one or more covariates with an error-prone, self reported time to event outcome. We present simulation studies to assess the effect of error in self reported outcomes with regard to bias in the estimation of the regression parameter of interest. We apply the proposed methods to the data from Women’s Health Initiative (WHI) to evaluate the effect of statin use with respect to incident diabetes risk. In Chapter 3, we develop tools to calculate power and sample size for studies in which data from sequentially administered, error-prone, laboratory-based diagnostic tests or self-reported questionnaires are collected to determine the occurrence of a silent event. We evaluate the effects of the characteristics of the imperfect diagnos- tic test on resulting power and sample size calculations. We compare the relative efficiency of various study designs in the context of error-prone outcomes. In Chapter 4, we propose a lasso and a Bayesian variable selection approach in the context of error-prone self reported outcomes {{to address the problem of}} vari- able selection in high dimensional data settings. We perform simulation studies to compare prediction performance of proposed methods and naive methods that ignore measurement error. We apply our proposed methods to the genome-wide association study data from the WHI to select biomarkers associated with diabetes...|$|E
40|$|A {{haplotype}} is {{a specific}} sequence of nucleotides on a single chromosome. The population associa-tions between haplotypes and disease phenotypes provide critical information about the genetic basis of complex human diseases. Standard genotyping techniques cannot distinguish the two homolo-gous chromosomes of an individual so that only the unphased genotype (i. e., {{the combination of the}} two homologous haplotypes) is directly observable. Statistical inference about haplotype-phenotype associations based on unphased genotype data presents an intriguing missing-data problem, espe-cially when the sampling depends on the disease status. The objective {{of this paper is to}} provide a systematic and rigorous treatment of this problem. All commonly used study designs, including cross-sectional, case-control and cohort studies, are considered. The phenotype can be a disease indicator, a quantitative trait or a potentially <b>censored</b> <b>time</b> to disease variable. The effects of haplo-types on the phenotype are formulated through flexible regression models, which can accommodate a variety of genetic mechanisms and gene-environment interactions. Appropriate likelihoods are con-structed, which may involve high-dimensional parameters. The identifiability of the parameters, and the consistency, asymptotic normality and efficiency of the maximum likelihood estimators are estab-lished. Efficient and reliable numerical algorithms are developed. Simulation studies show that th...|$|E
40|$|Multiple {{imputation}} is {{a technique}} for handling data sets with missing values. The method fills in each missing value several times, creating many augmented data sets. Each augmented data set is analyzed separately and the results combined to give a final result consisting of an estimate and a measure of uncertainty. In this paper we consider nonparametric multiple-imputation methods to handle missing event times for censored observations {{in the context of}} nonparametric survival estimation and testing. Two nonparametric imputation schemes are considered. In risk set imputation the <b>censored</b> <b>time</b> is replaced by a random draw of the observed times amongst those at risk after the censoring time. In Kaplan–Meier (KM) imputation the imputed time is a draw from the estimated distribution of event times amongst those at risk after the censoring time. We show that with a large number of imputes the estimates from both methods reproduce the KM estimator. In a simulation study we show that the inclusion of a bootstrap stage in the multiple imputation algorithm gives coverage rates of confidence intervals that are comparable to that from Greenwood’s formula. Connections to the redistribute to the right algorithm are discussed...|$|E
40|$|Abstract Rivest & Wells (2001) {{proposed}} estimators of {{the marginal}} survival functions in a right-censored model that assumes an Archimedean copula between the survival <b>time</b> and the <b>censoring</b> <b>time.</b> We study {{the extension of}} these estimators to the context of rightcensored semi-competing risks data with an independent second level <b>censoring</b> <b>time.</b> We intensively use martingale techniques to derive their large sample properties under mild assumptions on the true distribution of the data. As compared to the simpler context of right-censored data, a primary difference {{is the need to}} enlarge the filtrations with respect to which we use the Doob-Meyer decompositions of counting processes...|$|R
5000|$|Let Yi {{denote the}} {{observed}} <b>time</b> (either <b>censoring</b> <b>time</b> or event time) for subject i. Let Ci be the indicator {{that the time}} corresponds to an event (i.e. if Ci = 1 the event occurred and if Ci = 0 the <b>time</b> is a <b>censoring</b> <b>time).</b> Ignoring ties for the moment, conditioned upon {{the existence of a}} unique event at some particular time [...] the probability that the event occurs in the subject [...] for which [...] and [...] iswhere [...] ). Observe that the factors of [...] that would be present in both the numerator and denominator have canceled out.|$|R
30|$|Scenario 1 : Decreasing hazard. Lifetimes were {{generated}} from generalized Weibull with κ= 0.5, γ=− 0.1 and ρ= 0.1, and <b>censoring</b> <b>times</b> {{were generated}} from the exponential distribution with rate parameter λ= 0.045.|$|R
40|$|An {{important}} endpoint {{variable in}} a cocaine rehabilitation {{study is the}} time to first relapse of a patient after the treatment. We propose a joint modeling approach based on functional data analysis to study the relationship between the baseline longitudinal cocaine-use pattern and the interval <b>censored</b> <b>time</b> to first relapse. For the baseline cocaine-use pattern, we consider both self-reported cocaine-use amount trajectories and dichotomized use trajectories. Variations within the generalized longitudinal trajectories are modeled through a latent Gaussian process, which is characterized by a few leading functional principal components. The association between the baseline longitudinal trajectories and the time to first relapse is built upon the latent principal component scores. The mean and the eigenfunctions of the latent Gaussian process as well as the hazard function of time to first relapse are modeled nonparametrically using penalized splines, and the parameters in the joint model are estimated by a Monte Carlo EM algorithm based on Metropolis-Hastings steps. An Akaike information criterion (AIC) based on effective degrees of freedom is proposed to choose the tuning parameters, and a modified empirical information is proposed to estimate the variance-covariance matrix of the estimators. Comment: Published at [URL] in the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|Age-at-onset (AAO) in {{a number}} of {{extended}} families ascertained for bipolar disorder was analysed using survival analysis techniques, fitting proportional hazards models to estimate the fixed effects of sex 2 ̆ 7 year of birth, and generation, and a random polygenic genetic effect. Data comprised the AAO (for 171 affecteds) or age when last seen (ALS) for 327 unaffecteds, on 498 individuals in 27 families. ALS was treated as the <b>censored</b> <b>time</b> in the statistical analyses. The majority of individuals classified as affected were diagnosed with bipolar I and II (n = 103) or recurrent major depressive disorder (n = 68). In addition to the significant effects of sex and year of birth, a fitted 2 ̆ 7 generation 2 ̆ 7 effect was highly significant, which could be interpreted as evidence for an anticipation effect. The risk of developing bipolar or unipolar disorder increased twofold with each generation descended from the oldest founder. However, although information from both affected and unaffected individuals was used to estimate the relative risk of subsequent generations, {{it is possible that the}} results are biased because of the 2 ̆ 7 Penrose effect 2 ̆ 7. Females had a twofold increased risk in developing depressive disorder relative to males. The risk of developing bipolar or unipolar disorder increased by approximately 4...|$|E
40|$|In {{clinical}} trials and other follow-up studies, {{it is natural}} that a response variable is repeatedly measured during follow-up and the occurrence of some key event is also monitored. There has been a considerable study on the joint modelling these measures together with information on covariates. But {{most of the studies}} are related to continuous outcomes. In many situations instead of observing continuous outcomes, repeated ordinal outcomes are recorded over time. The joint modelling of such serial outcomes and the time to event data then becomes a bit complicated. In this article we have attempted to analyse such models through a latent variable model. In view of the longitudinal variation on the ordinal outcome measure, it is desirable to account for the dependence between ordered categorical responses and survival time for different causes due to unobserved factors. A flexible Monte Carlo EM (MCEM) method based on exact likelihood is proposed that can simultaneously handle the longitudinal ordinal data and also the <b>censored</b> <b>time</b> to event data. A computationally more efficient MCEM method based on approximation of the likelihood is also proposed. The method is applied to a number of ordinal scores and survival data from trials of a treatment for children suffering from Duchenne Muscular Dystrophy. Finally, a simulation study is conducted to examine the finite sample properties of the proposed estimators in the joint model under two different methods...|$|E
30|$|Scenario 2 : Increasing hazard. Lifetimes were {{generated}} from generalized Weibull with κ= 2, γ= 0.1 and ρ= 0.1, and <b>censoring</b> <b>times</b> {{were generated}} from the exponential distribution with rate parameter λ= 0.060.|$|R
30|$|In particular, if the <b>censoring</b> <b>time</b> is too long, the ALT will {{degenerate}} {{into the}} censored test at the normal stress level, and then K* =  1, ξ _L^* =  0, p_L^* =  1, and V_K^* approaches a constant.|$|R
3000|$|The {{sample sizes}} {{employed}} {{in this study}} were 10,  20,  50,  100,  500 and 1000. Censoring rates were fixed at 0,  20 and 80 %. Each group of simulated lifetime data required M numbers of t_k that were randomly generated from a two-parameter Weibull distribution with prespecified values of α _true= 3.0 and β _true= 1.5. <b>Censoring</b> <b>times</b> were chosen to have a common value, which is calculated as F_x^- 1 (p;α,β [...]), where p is the probability of a unit, starting at time 0, fails before reaching <b>censoring</b> <b>time</b> C. p was fixed for each CR at 1.0,  0.8 and 0.2. Then, a lifetime data set is generated through the comparison of lifetime units and a selected censoring time: If t_k is {{less than or equal to}} C, the unit is failed. Otherwise, the unit is in suspension with lifetime data <b>censored</b> at <b>time</b> C.|$|R
