77|6522|Public
25|$|The {{essential}} matrix {{can be seen}} as {{a precursor}} to the fundamental matrix. Both matrices can be used for establishing constraints between matching image points, but the essential matrix can only be used in relation to calibrated cameras since the inner camera parameters must be known in order to achieve the normalization. If, however, the cameras are calibrated the essential matrix can be useful for determining both the relative position and orientation between the cameras and the 3D position of <b>corresponding</b> <b>image</b> <b>points.</b>|$|E
2500|$|... {{which is}} the {{constraint}} that the essential matrix defines between <b>corresponding</b> <b>image</b> <b>points.</b>|$|E
2500|$|Depending on how {{the error}} related to each {{constraint}} is measured, {{it is possible to}} determine or estimate an essential matrix which optimally satisfies the constraints for a given set of <b>corresponding</b> <b>image</b> <b>points.</b> [...] The most straightforward approach is to set up a total least squares problem, commonly known as the eight-point algorithm.|$|E
5000|$|A single ray {{of light}} from x (3D point) is {{dispersed}} in the lens system of the cameras according to a point spread function. The recovery of the <b>corresponding</b> <b>image</b> <b>point</b> from measurements of the dispersed intensity function in the images gives errors.|$|R
5000|$|Let P be {{the center}} of {{projection}} for the camera, A, B, and C be 3D world <b>points</b> with <b>corresponding</b> <b>images</b> <b>points</b> u, v, and w. Let X = |PA|, Y = |PB|, Z = |PC|, , , , , , , , , [...] This forms triangles PBC, PAC, and PAB from which we obtain the equation system for P3P: ...|$|R
5000|$|It's {{common to}} {{normalize}} the <b>image</b> <b>points</b> before solving P3P. Solving the P3P system results in four possible solutions for [...] and [...] The fourth world point D and its <b>corresponding</b> <b>image</b> <b>point</b> z are {{then used to}} find the best solution among the four. The algorithm for solving the problem as well as the complete solution classification for it is given in the 2003 IEEE Transactions on Pattern Analysis and Machine Intelligence paper by Gao, et al. An open source implementation of the P3P can be found in OpenCV's calib3d module in the solvePnP function.|$|R
2500|$|Given {{a set of}} <b>corresponding</b> <b>image</b> <b>points</b> it is {{possible}} to estimate an essential matrix which satisfies the defining epipolar constraint for all the points in the set. [...] However, if the image points are subject to noise, which is the common case in any practical situation, {{it is not possible to}} find an essential matrix which satisfies all constraints exactly.|$|E
5000|$|... #Caption: An {{illustration}} of some 3D points and their <b>corresponding</b> <b>image</b> <b>points</b> {{as described by}} the pinhole camera model. As the 3D points are moving in space, the <b>corresponding</b> <b>image</b> <b>points</b> are also moving. The motion field consists of the motion vectors in the image for all points in the image.|$|E
5000|$|... #Caption: The {{object and}} <b>corresponding</b> <b>image</b> <b>points</b> can be interchanged ...|$|E
40|$|Abstract. This paper {{proposes a}} method to {{integrate}} multiple linearpushbroom panoramic images. The integration can be performed in real time. The technique is feasible on planar scene such as large-scale paintings or aerial/satellite images that {{are considered to be}} planar. The image integration consists of two steps: stitching and Euclidean reconstruction. For the image stitching, a minimum of five pairs of noncollinear <b>image</b> <b>corresponding</b> <b>points</b> are required in general cases. In some special configurations when there is column-to-column image correspondence between two panoramas, the number of <b>image</b> <b>corresponding</b> <b>points</b> required can be reduced to three. As for the Euclidean reconstruction, five pairs of non-collinear <b>image</b> <b>corresponding</b> <b>points</b> on the <b>image</b> boundaries are sufficient. ...|$|R
50|$|For {{an image}} edge, we may talk of its {{direction}} {{which can be}} {{defined in terms of}} the gradient, pointing in the direction of maximum image intensity increase (from dark to bright). This implies that two edges can have the same orientation but the <b>corresponding</b> <b>image</b> gradients <b>point</b> in opposite directions if the edges go in different directions.|$|R
40|$|Ideal imaging systems perform {{point-to-point}} imaging. This {{requires that}} a spherical wave front expanding from each object point (o) {{is converted to}} a spherical wave front converging to a <b>corresponding</b> <b>image</b> <b>point</b> (o’). However, real optical systems produce an imperfect “aberrated” image. The aberrated wave front indicated by the solid red line below corresponds to rays near the axis focusing near point a and rays near {{the margin of the}} pupil focusing near point b. The “wave front aberration function ” describes the optical path difference between the aberrated wave front and a spherical reference wave (typically measured in m or “waves”). W(x,y) = aberrated wave front – spherical reference wave front optical system entrance pupil exit pupil o o’ab W(x,y) = awf – r...|$|R
5000|$|... {{which is}} the {{constraint}} that the essential matrix defines between <b>corresponding</b> <b>image</b> <b>points.</b>|$|E
50|$|In the following, it {{is assumed}} that {{triangulation}} is made on <b>corresponding</b> <b>image</b> <b>points</b> from two views generated by pinhole cameras. Generalization from these assumptions are discussed here.|$|E
5000|$|Each pair of <b>corresponding</b> <b>image</b> <b>points</b> {{produces}} a vector [...] Given {{a set of}} 3D points [...] this corresponds {{to a set of}} vectors [...] and all of them must satisfy ...|$|E
3000|$|... 2,[*] 1) is a {{representation}} of an <b>image’s</b> <b>corresponding</b> <b>point.</b> The projection matrix P can be decomposed into an intrinsic matrix and an extrinsic matrix.|$|R
5000|$|... where [...] is {{the number}} of null {{singular}} values in [...] and each [...] is the corresponding right singular vector of [...] [...] can range from 1 to 4. After calculating the initial coefficients , the Gauss-Newton algorithm is used to refine them. The [...] and [...] matrices that minimize the reprojection error of the world reference points, , and their <b>corresponding</b> actual <b>image</b> <b>points</b> , are then calculated.|$|R
40|$|In this paper, {{we propose}} a novel camera self-calibration {{technique}} using plane lattice and orthogonality. The rigorous analytical relations among the feature point coordinates {{of the plane}} lattice, the <b>corresponding</b> <b>image</b> <b>point</b> coordinate, intrinsic parameters, relative pose are induced according to homography matrix of the central projection. Let a slope of non-parallel and non-orthogonal virtual line in the lattice plane, and the slope of its orthonormal line can be calculated. In at least three photographs taken, vanishing points can be solved in two groups of orthogonal directions by using the homography matrix, so the camera intrinsic parameters are linearly figured out. This method has both simple principle and convenient pattern manufacture, and does not involve image matching, besides having no requirement concerning camera motion. Simulation experiments and real data show that this algorithm is feasible, and provides a higher accuracy and robustness...|$|R
50|$|The algorithm's name {{derives from}} the fact that it {{estimates}} the essential matrix or the fundamental matrix from a set of eight (or more) <b>corresponding</b> <b>image</b> <b>points.</b> However, variations of the algorithm can be used for fewer than eight points.|$|E
50|$|Creation of the {{textured}} 3D models requires {{definition of}} the relationship between all the triangle mesh vertices and the <b>corresponding</b> <b>image</b> <b>points.</b> Collinearity condition can be used in order to find that relationship, but image interior and exterior orientation parameters need to be known.|$|E
50|$|The {{fundamental}} matrix can {{be determined}} {{by a set of}} point correspondences. Additionally, these <b>corresponding</b> <b>image</b> <b>points</b> may be triangulated to world points with the help of camera matrices derived directly from this fundamental matrix. The scene composed of these world points is within a projective transformation of the true scene.|$|E
30|$|In general, {{the value}} of {{residual}} error is determined {{as an indicator of}} the quality of photogrammetry modeling which entails manually matching feature points on the object. In the field experiments, one object point P was identified and marked in the x-y frame of each photo taken by three cameras. Its coordinates in an arbitrary modeling space frame were then calculated based on the least-squares adjustment technique by applying the “five-point relative modeling approach.” After the point coordinates were determined, the <b>corresponding</b> <b>image</b> <b>point</b> (x, y) were re-calculated by using collinearity Equations (1) and (2) in the <b>corresponding</b> <b>image</b> frame. The pixel differences between the manually marked (x, y) coordinate and recalculated (x, y) coordinate are defined as the residual error (EOS Systems 2011). As a rule of thumb, if the largest residual is less than 10 pixels, the quality of a photogrammetry modeling project is high; otherwise, re-modeling is needed. Note the residual errors on the models resulting from field experiments as described in this paper were controlled within 3 pixels.|$|R
40|$|Two-view {{methods have}} been well {{developed}} to identify human actions. However, in {{a case where the}} <b>corresponding</b> <b>imaged</b> <b>points</b> cannot induce distinguished measures, the performance of the methods deteriorates. For this reason, we propose a new view-invariant measure for human action recognition by enforcing tri-view constraints in this paper. This new measurement method can be tolerant to different rates of human actions and the anthropometric proportions. We apply our approach to video synchronization by imposing both the similarity ratio and the consistency in the trifocal tensor over entire video sequences. By testing on both synthetic and real data, our method has achieved higher tolerance to noise levels, as well as higher identification accuracy than the traditional two-view method. Experimental results demonstrate that our approach can identify human pose transitions, in spite of dynamic time-lines, different viewpoints and unknown camera parameters. Yang Wang, Lin Wu, Xiaodi Huang, and Xuemin Li...|$|R
40|$|Leonhardt {{demonstrated}} (2009) {{that the}} 2 D Maxwell Fish Eye lens (MFE) can perfectly focus 2 D Helmholtz waves of arbitrary frequency, i. e., it can perfectly transport an outward (monopole) 2 D Helmholtz wave field, {{generated by a}} point source, towards a "perfect point drain" located at the <b>corresponding</b> <b>image</b> <b>point.</b> Moreover, a prototype with λ/ 5 super-resolution property for one microwave frequency has been manufactured and tested (Ma et al, 2010). However, software simulations or experimental measurements for a broad band of frequencies have not been reported. Here we present simulations with a non-perfect drain for a device equivalent to the MFE, called the Spherical Geodesic Waveguide (SGW), that predict up to λ/ 500 super-resolution close to discrete frequencies. These frequencies are directly connected with the well-known Schumann resonance frequencies of spherical symmetric systems. Out of these frequencies, the SGW does not show super-resolution in the analysis performed...|$|R
50|$|Depending on how {{the error}} related to each {{constraint}} is measured, {{it is possible to}} determine or estimate an essential matrix which optimally satisfies the constraints for a given set of <b>corresponding</b> <b>image</b> <b>points.</b> The most straightforward approach is to set up a total least squares problem, commonly known as the eight-point algorithm.|$|E
50|$|Given {{a set of}} <b>corresponding</b> <b>image</b> <b>points</b> it is {{possible}} to estimate an essential matrix which satisfies the defining epipolar constraint for all the points in the set. However, if the image points are subject to noise, which is the common case in any practical situation, {{it is not possible to}} find an essential matrix which satisfies all constraints exactly.|$|E
5000|$|As an {{overview}} of the process, first note that each of the [...] reference points in the world frame, , and their <b>corresponding</b> <b>image</b> <b>points,</b> , are weighted sums of the four controls points, [...] and [...] respectively, and the weights are normalized per reference point as shown below. All points are expressed in homogeneous form.From this, the derivation of the image reference points becomes ...|$|E
40|$|This paper {{presents}} {{a method for}} determining the 3 D position of an <b>image</b> <b>point</b> on a reference image using particle swarm optimization (PSO) to search the height (Z value) that gives the biggest Normalized Cross Correlation (NCC) coefficient. The searching area is in the surrounding of {{the height of the}} <b>image</b> <b>point.</b> The NCC coefficient evaluates the similarity with the <b>image</b> <b>point</b> and a <b>corresponding</b> point on an epipolar line in the search image. The position of <b>corresponding</b> <b>image</b> <b>point</b> on the epipolar line is determined by the height point on a sloping line locus. The PSO algorithm starts with a swarm of random particles. The position of each particle is a potential solution in the problem space. Each particle is given a randomized velocity and attracted toward the location of the best fitness. The position of each particle is iteratively modified by adding a newly computed velocity to its current position. The velocity is updated by three factors which are two attractions from local best position and global best position, two strengths of the attractions, and two uniform random numbers for each attraction. The iteration will stop when the current solution is convergent. The time of computation is highly related to the range of height and the interval of height enumeration when the approach to find a <b>corresponding</b> <b>image</b> <b>point</b> of an <b>image</b> <b>point</b> on a reference image is based on the height enumeration along sloping line locus. The precision of results can be improved by decreasing the interval of height enumeration. This shows the limitation of the enumeration method in the efficiency and accuracy. The issue is overcome by a method of using PSO algorithm. The proposed method using different parameters such as the size of image window, the number of particles, {{and the size of the}} height searching range has been applied to aerial stereo images. The accuracy of tested results is evaluated on the base of the comparison to the reference data from the results of least-square matching being manually given initial points. The evaluation result shows that tested results has given a solution to a level of less than 1 centimetre without using refined image matching method. The same level of accuracy can reach even when the searching range is bigger than 90 meters. But the difference of image window size may lead to the change of the matching result. And, without the procedures of both coarse-to-fine hierarchical solution and refined image matching method, the algorithm still can give the same accuracy level of least-square image matching resulting. This method also shows its ability to give reasonable matching results without manual assistance...|$|R
5000|$|... where [...] is the {{homogeneous}} world point, [...] is the <b>corresponding</b> homogeneous <b>image</b> <b>point,</b> [...] is {{the matrix}} of intrinsic camera parameters, (where [...] and [...] are the scaled focal lengths, [...] is the skew parameter which is sometimes {{assumed to be}} 0, and [...] is the principal point), [...] is a scale factor for the <b>image</b> <b>point,</b> and [...] and [...] are the desired 3D rotation and 3D translation of the camera (extrinsic parameters) that are being calculated. This leads to the following equation for the model: ...|$|R
40|$|The paper {{describes}} a geometric projection model for fisheye lenses and presents {{results of the}} calibration of fisheye lens camera systems based on it. For fisheye images the real projection model does not comply with the central perspective. In addition, the lenses are subject to considerable distortions. The fisheye lens projection follows an approximately linear relation between the incidence angle of an object point’s beam and {{the distance from the}} <b>corresponding</b> <b>image</b> <b>point</b> to the principle point. The developed projection model is extended by several distortion parameters. For the calibration of fisheye lens camera systems, a calibration room was created and equipped with number targets. They were arranged to have a good distribution all over the image. The reference object coordinates of the targets were determined with high precision. The calibration itself is done by spatial resection from a single image of the calibration room. The calibration parameters include interior orientation, exterior orientation and lens distortion. Besides the geometric model for fisheye lenses, the paper shows practical results from the calibration of a high quality fisheye lens on a 14 mega pixel camera as well as a consumer camera equipped with a fisheye lens. 1...|$|R
50|$|In {{traditional}} stereo vision, two cameras, displaced horizontally {{from one}} another are used to obtain two differing views on a scene, {{in a manner similar}} to human binocular vision. By comparing these two images, the relative depth information can be obtained in the form of a disparity map, which encodes the difference in horizontal coordinates of <b>corresponding</b> <b>image</b> <b>points.</b> The values in this disparity map are inversely proportional to the scene depth at the corresponding pixel location.|$|E
50|$|The eight-point {{algorithm}} is an algorithm used in computer vision {{to estimate the}} essential matrix or the fundamental matrix related to a stereo camera pair from a set of <b>corresponding</b> <b>image</b> <b>points.</b> It was introduced by Christopher Longuet-Higgins in 1981 for {{the case of the}} essential matrix. In theory, this algorithm can be used also for the fundamental matrix, but in practice the normalized eight-point algorithm, described by Richard Hartley in 1997, is better suited for this case.|$|E
5000|$|... for <b>corresponding</b> <b>image</b> <b>points</b> {{represented}} in normalized image coordinates [...] The problem which the algorithm solves {{is to determine}} [...] {{for a set of}} matching image points. In practice, the image coordinates of the image points are affected by noise and the solution may also be over-determined which means that it may not be possible to find [...] which satisfies the above constraint exactly for all points. This issue is addressed in the second step of the algorithm.|$|E
2500|$|The {{idea of a}} {{projective}} space relates to perspective, more precisely to the way an eye or a camera projects a 3D scene to a 2D <b>image.</b> All <b>points</b> that lie on a projection line (i.e., a [...] "line of sight"), intersecting with the entrance pupil of the camera, are projected onto a common <b>image</b> <b>point.</b> In this case, the vector space is R3 with the camera entrance pupil at the origin, and the {{projective space}} <b>corresponds</b> to the <b>image</b> <b>points.</b>|$|R
40|$|Leonhardt {{demonstrated}} (2009) {{that the}} 2 D Maxwell Fish Eye lens (MFE) can focus perfectly 2 D Helmholtz waves of arbitrary frequency, i. e., it can transport perfectly an outward (monopole) 2 D Helmholtz wave field, {{generated by a}} point source, towards a "perfect point drain" located at the <b>corresponding</b> <b>image</b> <b>point.</b> Moreover, a prototype with λ/ 5 superresolution (SR) property for one microwave frequency has been manufactured and tested (Ma et al, 2010). Although this prototype has been loaded with an impedance different from the "perfect point drain", it has shown super-resolution property. However, neither software simulations nor experimental measurements for a broad band of frequencies have yet been reported. Here we present steady state simulations for two cases, using perfect drain as suggested by Leonhardt and without perfect drain as in the prototype. All the simulations have been done using a device equivalent to the MFE, called the Spherical Geodesic Waveguide (SGW). The results show the super-resolution up to λ/ 3000, for the system loaded with the perfect drain, and up to λ/ 500 for a not perfect load. In both cases super-resolution only happens for discrete number of frequencies. Out of these frequencies, the SGW does not show super-resolution in the analysis carried out...|$|R
40|$|Commission V, WG V/ 5 The paper {{describes}} a geometric projection model for fisheye lenses and presents {{results of the}} calibration of fisheye lens camera systems based on it. For fisheye images the real projection model does not comply with the central perspective. In addition, the lenses are subject to considerable distortions. The fisheye lens projection follows an approximately linear relation between the incidence angle of an object point’s beam and {{the distance from the}} <b>corresponding</b> <b>image</b> <b>point</b> to the principle point. The developed projection model is extended by several distortion parameters. For the calibration of fisheye lens camera systems, a calibration room was created and equipped with number targets. They were arranged to have a good distribution all over the image. The reference object coordinates of the targets were determined with high precision. The calibration itself is done by spatial resection from a single image of the calibration room. The calibration parameters include interior orientation, exterior orientation and lens distortion. Besides the geometric model for fisheye lenses, the paper shows practical results from the calibration of a high quality fisheye lens on a 14 mega pixel camera as well as a consumer camera equipped with a fisheye lens. 1...|$|R
