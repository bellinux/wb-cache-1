7|117|Public
40|$|Abstract-Interacting agents with {{symbolic}} grammar {{are proposed}} {{in order to}} study the evolution of computational ability ofagents. The algorithmic evolution of the formal grammar system is characterized by Chomsky's hierarchy 1. Agents with a higher grammar can speak/recognize many more words than those with a lower one. However, when agents form a network, the higher <b>Chomsky</b> <b>grammar</b> is not always advantageous. It is shown that to speak/recognize commonly used words is more favorable in a network...|$|E
40|$|A {{conditional}} grammar is a <b>Chomsky</b> <b>grammar</b> with languages {{associated to}} its rules such that each rule is applicable only to {{words in the}} corresponding language. In this paper the generative capacity of type 0, 1, 2, 2 — λ, 3 grammars with associated type 0, 1, 2, 3 languages will be characterized {{in terms of the}} Chomsky hierarchy. We shall prove that the generative capacity of context-free and of regular grammars is increased in this way, while for type- 0 and type- 1 grammars the generative capacity is not modified. Two other variants of these grammars are shown to be equivalent with them...|$|E
40|$|Interacting agents with {{symbolic}} grammar {{are proposed}} {{in order to}} study the evolution of computational ability of agents. The algorithmic evolution of the formal grammar system is characterized by Chomsky's hierarchy 1. Agents with a higher grammar can speak/recognize many more words than those with a lower one. However, when agents form a network, the higher <b>Chomsky</b> <b>grammar</b> is not always advantageous. It is shown that to speak/recognize commonly used words is more favorable in a network. INTRODUCTION In this paper we present an evolutionary model of interacting agents with symbolic grammar. Our main concern is to see how {{a higher level of}} grammar evolves from the lower ones. Regarding computational ability as a measure of evolution, we naturally ask, what evolutionary dynamics can elaborate computational ability? Introducing an ensemble of agents, we discuss its evolution. We see evolutionary pathways of climbing up Chomsky's hierarchy and show how they are avoided by the ensemble [...] ...|$|E
5000|$|A shape grammar {{minimally}} {{consists of}} three shape rules: a start rule, at least one transformation rule, and a termination rule. The start rule is necessary to start the shape generation process. The termination rule is necessary to make the shape generation process stop. The simplest {{way to stop the}} process is by a shape rule that removes the marker. Shape <b>grammars</b> differ from <b>Chomsky</b> <b>grammars</b> a major respect: the production rules may be applied serially (as with <b>Chomsky</b> <b>grammars)</b> or in parallel (not allowed in <b>Chomsky</b> <b>grammars),</b> similar to the way [...] "productions" [...] are done in L-Systems.|$|R
40|$|Lindenmayer systems, or L-systems, are {{a formal}} model of {{structural}} growth {{due to the}} theoretical botanist Lindenmayer in 1968, {{in the wake of}} research on <b>Chomsky</b> <b>grammars</b> in computer science. They have been immensely influential as a conceptual tool for botany, as well as an application domain for computer...|$|R
40|$|AbstractIterated finite state {{sequential}} transducers {{are considered}} as language generating devices. The hierarchy {{induced by the}} size of the state alphabet is proved to collapse to the fourth level. The corresponding language families are related to the families of languages generated by Lindenmayer systems and <b>Chomsky</b> <b>grammars.</b> Finally, some results on deterministic and extended iterated finite state transducers are established...|$|R
40|$|Collage grammars and {{context-free}} chain-code grammars {{are compared}} {{with respect to}} their generative power. It is shown that the generated classes of line-drawing languages are incomparable, but that chain-code grammars can simulate collage grammars that use only similarity transformations. 1 Introduction Inspired by the comparison of chain-code and collage grammars in [DHT 96], in this paper some further observations concerning the generative power of these two types of picture generating grammars are pointed out. A context-free chain-code grammar [MRW 82] is a type- 2 <b>Chomsky</b> <b>grammar</b> generating a language of words over the alphabet fu; d; l; r; "; #g. Such a word is then interpreted as a sequence of instructions to a plotter-like device in order to produce a line drawing. The letters u, d, l, and r are interpreted as instructions to draw a unit line from the current position of the pen upwards, downwards, to the left, and to the right, respectively. Furthermore, " lifts the pen (so th [...] ...|$|E
40|$|Abstract. Discontinuous {{constituent}} are {{a frequent}} problem in natural language analyses. A constituent is called discontinuous {{if it is}} interrupted by other constituents. In German they can appear with separable verb prefixes or relative clauses in the Nachfeld. They can not be captured by a context–free <b>Chomsky</b> <b>grammar.</b> A subset of hypergraph grammars are string grammars where {{the result of a}} derivation must be formed like a string i. e. terminal edges are connected to two nodes and are lined up in a row. Nonterminal edges do not have to fulfill this property. In this paper it is shown that a context–free string grammar (one hyperedge is replaced at a time) can be used to model discontinuous constituents in natural languages. 1 Discontinuous Constituents in Phrase Structure Grammars Discontinuity is a phenomenon that can be found in various natural languages. It means that two parts of a constituent of a sentences are not found next to each other in this sentence. Take for example the following sentence: 1 (1) Er hat ihm das Buch gegeben, das fleckig ist...|$|E
40|$|Computation {{at levels}} beyond storage and {{transmission}} of information appears in physical systems at phase transitions. We investigate this phenomenon using minimal computational models of dynamical systems that undergo a transition to chaos {{as a function of}} a nonlinearity parameter. For period-doubling and band-merging cascades, we derive expressions for the entropy, the interdependence of-machine complexity and entropy, and the latent complexity of the transition to chaos. At the transition deterministic finite automaton models diverge in size. Although there is no regular or context-free <b>Chomsky</b> <b>grammar</b> in this case, we give finite descriptions at the higher computational level of context-free Lindenmayer systems. We construct a restricted indexed context-free grammar and its associated one-way nondeterministic nested stack automaton for the cascade limit language. This analysis of a family of dynamical systems suggests a complexity theoretic description of phase transitions based on the informational diversity and computational complexity of observed data that is independent of particular system control parameters. The approach gives a much more refined picture of the architecture of critical states than is available vi...|$|E
50|$|BNF is a {{notation}} for <b>Chomsky's</b> context-free <b>grammars.</b> Apparently, Backus {{was familiar}} with Chomsky's work.|$|R
40|$|We {{extend the}} {{restrictions}} which induce unique parsability in <b>Chomsky</b> <b>grammars</b> to accepting grammar systems. It is {{shown that the}} accepting power of global RC-uniquely parsable accepting grammar systems equals the computational power of deterministic pushdown automata. More computational power, keeping the parsability without backtracking, is observed for local accepting grammar systems satisfying the pre x condition. We discuss a simple recognition algorithm for these systems...|$|R
50|$|Moral philosopher John Rawls {{compared}} {{building a}} theory of morality {{to that of the}} generative grammar model found in Aspects. In A Theory of Justice (1971), he notes that just like <b>Chomsky's</b> <b>grammar</b> model assumes a set of finite underlying principles that are supposed to adequately explain the variety of sentences in linguistic performance, our sense of justice can be defined as a set of moral principles that give rise to everyday judgments.|$|R
40|$|Formal fuzzy {{languages}} {{related to}} the Chomsky hierarchy, to the Peterson hierarchy of Petri net languages and {{to the family of}} Lindenmayer languages are the subject of this paper. In 1973 M. Mitzumoto, J. Toyoda and K. Tanaka [MTT 73] introduced the concept of weighted grammars and showed that with respect to given thresholds, new formal languages may be obtained, which are higher in the Chomsky hierarchy [Cho 65] than the language generated by the initial grammar without weights. Fuzzy grammars may be defined as a special case of weighted grammars, by taking the weighting space to be the interval [0, 1] and by working with t-norms and t-conorms. Definition 1. A fuzzy grammar G is specified by the 5 -tuple (N, T, S, P, ω, ⊗), where (N,T,S,P) is a <b>Chomsky</b> <b>grammar,</b> ω: P → [0, 1] associates to every production in P a weight from the unit interval and ⊗ denotes a t-norm. The fuzzy language L(G) generated by this fuzzy grammar is {(w, µL(w)) | w ∈ T*, S ⇒ * w, µL(w) = ⊗ pi}, where µL(w) represents the degree of membership of the word w to i the language L and is obtained by applying the t-norm ⊗ to the weights of all productions involved i...|$|E
30|$|Research in {{procedural}} modeling {{has been}} going on for several decades. Architectural procedural modeling can draw on a number of production systems such as Semi-Thue processes [14], <b>Chomsky</b> <b>grammars</b> [15], graph grammars [16], shape grammars [17, 18], attributed grammars [19], L-systems [20], or set grammars [21]. With hard-coded modeling rules written in a programming language like C++ on one side of the spectrum and more constrained grammar-based systems on the other, all these methods present the user with different expressiveness and efficiency.|$|R
40|$|We {{prove the}} {{existence}} of an infinite hierarchy of mildly context-sensitive families of languages. The definition of this hierarchy is based on an extension of Marcus contextual grammars referred to as Marcus many dimensional simple contextual grammars. Some variants of this hierarchy are also considered. Interrelations between Marcus many dimensional simple contextual <b>grammars</b> and <b>Chomsky</b> <b>grammars</b> as well as interrelations with simple matrix grammars are studied. This research opens new directions to investigate various Marcus contextual grammars in the many dimensional case. TUCS Research Grou...|$|R
40|$|Contents Acknowledgment 3 1 Introduction 4 1. 1 Grammar systems................................ 4 1. 2 Parallel {{communicating}} grammar systems.................. 9 1. 2. 1 Synchronization............................. 9 1. 2. 2 Different ways {{of functioning}} and communication.......... 11 1. 2. 3 Generative capacity........................... 13 1. 3 Concluding remarks............................... 14 2 Preliminaries and definitions 15 2. 1 Formal language preliminaries......................... 15 2. 2 Parallel communicating grammar systems.................. 16 3 Synchronization, communication, and normal forms 21 3. 1 Different modes of communication....................... 22 3. 2 Results for systems with <b>Chomsky</b> <b>grammars</b> as components... ...|$|R
40|$|This chapter {{introduces}} rule-based graph transformation, which {{constitutes a}} well-studied research area in computer science. The chapter presents {{the most fundamental}} definitions and illustrates them with some selected examples. It presents also the concept of transformation units, which makes pure graph transformation more feasible for specification and modeling aspects. Moreover, a translation of <b>Chomsky</b> <b>grammars</b> into graph grammars is given and the main theorems concerning parallelism and concurrency are presented. Finally, an introduction to hyperedge replacement is given, a concept which has nice properties because it transforms hypergraphs in a context-free way...|$|R
40|$|This book {{objective}} is to develop an algebraization of graph grammars. Equivalently, we study graph dynamics. From {{the point of view}} of a computer scientist, graph grammars are a natural generalization of <b>Chomsky</b> <b>grammars</b> for which a purely algebraic approach does not exist up to now. A <b>Chomsky</b> (or string) <b>grammar</b> is, roughly speaking, a precise description of a formal language (which in essence is a set of strings). On a more discrete mathematical style, it can be said that graph grammars [...] Matrix Graph Grammars in particular [...] study dynamics of graphs. Ideally, this algebraization would enforce our understanding of grammars in general, providing new analysis techniques and generalizations of concepts, problems and results known so far. Comment: 321 pages, 75 figures. This book has is publisehd by VDM verlag, ISBN 978 - 363921255...|$|R
50|$|In second {{language}} acquisition (SLA) functional approaches are of similarities with <b>Chomsky's</b> Universal <b>Grammar</b> (UG). Focus {{is on the}} use of language in real situations (performance), as well as underlying knowledge (competence).|$|R
40|$|AbstractGrammar {{induction}} is {{an important}} and current area within computational linguistics. The results in induction of string transformation rules can be applied not only in linguistics but in many other pattern recognition tasks. The task of grammar induction belongs in general to NP -hard problems. The paper analyses the main rule types in natural languages and in string transformation systems. The paper focuses on the inference of inflectional rule systems which differs in many aspects from the traditional production rule system of <b>Chomsky</b> <b>grammars.</b> The work compares the main candidate methods on the learning of objective-case in the Hungarian language...|$|R
40|$|A string {{generating}} hypergraph grammar is a hyperedge replacement grammar {{where the}} resulting language consists of string graphs i. e. hypergraphs modeling strings. With {{the help of}} these grammars, string languages like a n b n c n can be modeled {{that can not be}} generated by context-free grammars for strings. They are well suited to model discontinuous constituents in natural languages, i. e. constituents that are interrupted by other constituents. For parsing context-free <b>Chomsky</b> <b>grammars,</b> the Earley parser is well known. In this paper, an Earley parser for string generating hypergraph grammars is presented, leading to a parser for natural languages that is able to handle discontinuities...|$|R
40|$|Given two <b>Chomsky</b> <b>grammars</b> G and G, a {{homomorphism}} φ from G to G is, roughly speaking, a map which assigns {{to every}} derivation of G a derivation of G {{in such a}} manner that φ is uniquely determined by its restriction to the set of productions of G. Two grammars are contained in the same transformational class, if the one can be transformed into the other by a sequence of homomorphisms. If two grammars are related {{in such a manner}}, then there are two relations, one concerning the words of the languages generated and the other regarding the derivations of these words. We establish several classifications of context-free grammars in transformational classes which are recursively solvable...|$|R
50|$|First {{introduced}} in 1985 as Generative Grammars and later more elaborated upon, Christiansen grammars (apparently dubbed so by Shutt, possibly due to conflict with <b>Chomsky</b> generative <b>grammars)</b> are an adaptive extension of attribute grammars. Christiansen grammars were classified by Shutt as declarative.|$|R
5000|$|Sydney MacDonald Lamb (born May 4, 1929 in Denver, Colorado) is an American {{linguist}} [...] {{and professor}} at Rice University, whose stratificational grammar is a significant alternative theory to <b>Chomsky's</b> transformational <b>grammar.</b> He has specialized in Neurocognitive Linguistics and a stratificational approach to language understanding.|$|R
40|$|If {{the aim of}} this {{dissertation}} had to {{be summarized}} in a single sentence, it could be algebraization of graph grammars. An equivalent one would be study of graph dynamics. From {{the point of view of}} a computer scientist, graph grammars are a natural generalization of <b>Chomsky</b> <b>grammars</b> for which a purely algebraic approach does not exist up to now. A <b>Chomsky</b> (or string) <b>grammar</b> is, roughly speaking, a precise description of a formal language (which in essence is a set of strings). On a more discrete mathematical style, it can be said that graph grammars â Matrix Graph Grammars in particular â study dynamics of graphs. Ideally, this algebraization would enforce our understanding of grammars in general, providing new analysis techniques and generalizations of concepts, problems and results known so far. In this dissertation we fully develop such theory over the field GF(2) which covers all graph cases, from simple graphs (more attractive for a mathematician) to multidigraphs (more interesting for an applied computer scientist). The theory is presented and its basic properties demonstrated in a first stage, moving to increasingly difficult problems and establishing relations among them...|$|R
50|$|Proposed in the 1950s by Noam <b>Chomsky,</b> {{generative}} <b>grammar</b> is {{an analysis}} approach to {{language as a}} structural framework of the human mind. Through formal analysis of components such as syntax, morphology, semantics and phonology, a generative grammar seeks to model the implicit linguistic knowledge with which speakers determine grammaticality.|$|R
5000|$|Chapter 8, Chemical Word and Chemical Deed, {{examines}} {{the processes of}} DNA as information processes. Campbell makes the distinction between first order DNA messages and second order, or structural, DNA messages (e.g., [...] "how to bake a cake" [...] versus [...] "how to read a recipe"). This distinction he relates to the linguistic principles of Noam <b>Chomsky's</b> Universal <b>Grammar.</b>|$|R
25|$|Bandler and Grinder {{also drew}} upon the {{theories}} of Gregory Bateson, Alfred Korzybski and Noam <b>Chomsky</b> (particularly transformational <b>grammar),</b> as well as ideas and techniques from Carlos Castaneda.|$|R
5000|$|During that period, I was {{attempting}} to unify <b>Chomsky's</b> transformational <b>grammar</b> with formal logic. I had helped work {{out a lot of}} the early details of <b>Chomsky's</b> theory of <b>grammar.</b> Noam claimed then — and still does, so far as I can tell — that syntax is independent of meaning, context, background knowledge, memory, cognitive processing, communicative intent, and every aspect of the body...In working through the details of his early theory, I found quite a few cases where semantics, context, and other such factors entered into rules governing the syntactic occurrences of phrases and morphemes. I came up with the beginnings of an alternative theory in 1963 and, along with wonderful collaborators like [...] "Haj" [...] Ross and Jim McCawley, developed it through the sixties.|$|R
40|$|Based on N. A. <b>Chomsky’s</b> <b>grammar</b> {{generative}} theory, {{this piece}} of work attempts to go more deeply into language biological and genetic aspects under a psycholinguistic perspective. According to the generativist theory, currently emerging as a new principle of philosophic rationalism, these are {{the means by which}} this theory can be scientifically and objectively validated. The scientific explanation of language acquisition and evolution must comprise both cognitive psychology and language biology. Both disciplines should enable to explain language disabilities. For this purpose, contributions made by biology and genetic inheritance on language are considered. There is every indication that the neurological basis of learning disabilities, that is, the minimum brain dysfunction, lies within synaptic deficiency. If theory this should be true, new scopes would be available for the specialist in psychology and education. These would concern with the creation of new synaptic connections through enriched learning atmospheres for subjects affected by the learning disabilities syndrome...|$|R
50|$|Tomasello {{also works}} on child {{language}} acquisition as a crucially {{important aspect of}} the enculturation process. He is a critic of Noam <b>Chomsky's</b> universal <b>grammar,</b> rejecting the idea of an innate universal grammar and instead proposing a functional theory of language development (sometimes called the social-pragmatic or usage-based approach to language acquisition) in which children learn linguistic structures through intention-reading and pattern-finding in their discourse interactions with others.|$|R
5000|$|Furthermore, Kiparsky's account [...] "is {{based on}} a {{specific}} theory of English stress elaborated by Liberman and Prince (1977) as a counter-proposal to Chomsky and Halle's Sound Pattern of English." [...] Conversely, he considers the syllables in a verse line to have a complex hierarchical structure — analogous to a core proposition in <b>Chomsky's</b> transformational <b>grammar</b> — {{as opposed to the}} previous theories which gave syllables a strictly linear treatment.|$|R
25|$|The {{formalism}} of context-free grammars {{was developed}} in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure <b>grammars).</b> What <b>Chomsky</b> called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency <b>grammars.</b> In <b>Chomsky's</b> generative <b>grammar</b> framework, the syntax of natural language was described by context-free rules combined with transformation rules.|$|R
40|$|Paul D. Elbourne: Situations and individuals. MIT Press, Cambridge MA, 2005. 248 pp.; Anna Sőrés: Le hongrois dans la typologie des langues. Editions Lambert-Lucas. Limoges, 2006. 185 pp.; Csilla Bartha (ed.) : Cigány nyelvek és közösségek a Kárpát-medencében [Gypsy com- munities {{and their}} {{languages}} in the Carpathian Basin]. Nemzeti Tankönyvkiadó, Budapest, 2007. 344 pp.; Vivian J. Cook and Mark Newson: <b>Chomsky’s</b> Universal <b>Grammar.</b> An Introduction. Third edition. Blackwell, Malden MA & Oxford, 2007, 326 pp...|$|R
50|$|The {{formalism}} of context-free grammars {{was developed}} in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure <b>grammars).</b> What <b>Chomsky</b> called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency <b>grammars.</b> In <b>Chomsky's</b> generative <b>grammar</b> framework, the syntax of natural language was described by context-free rules combined with transformation rules.|$|R
2500|$|The {{analysis}} of these different lexical units had a decisive role {{in the field of}} [...] "generative linguistics" [...] during the 1960s. The term generative was proposed by Noam Chomsky in his book Syntactic Structures published in 1957. The term generative linguistics was based on <b>Chomsky's</b> generative <b>grammar,</b> a linguistic theory that states systematic sets of rules (X' theory) [...] can predict grammatical phrases within a natural language. Generative Linguistics is also known as Government-Binding Theory.|$|R
