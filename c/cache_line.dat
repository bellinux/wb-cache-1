547|393|Public
25|$|Note {{that most}} PCI devices only support a {{limited range of}} typical <b>cache</b> <b>line</b> sizes; if the <b>cache</b> <b>line</b> size is {{programmed}} to an unexpected value, they force single-word access.|$|E
25|$|If the {{starting}} offset within the <b>cache</b> <b>line</b> is zero, {{all of these}} modes reduce to the same order.|$|E
25|$|A notable {{omission}} {{from the}} specification was per-byte write enables; {{it was designed}} for systems with caches and ECC memory, which always write in multiples of a <b>cache</b> <b>line.</b>|$|E
5000|$|The [...] and [...] {{instructions}} {{mark the}} start and end of a transactional code region. Inside transactional code regions, the -prefixed , [...] and [...] instructions can mark up to four <b>cache</b> <b>lines</b> for protected memory access. Accesses from other processor cores to the protected <b>cache</b> <b>lines</b> result in exceptions, which in turn cause transaction aborts. Stores to protected <b>cache</b> <b>lines</b> must be performed using the [...] instructions. Marked <b>cache</b> <b>lines</b> can be released from protection with the [...] instruction. Transaction aborts generated by hardware or explicitly requested through the [...] instruction rolls back modifications to the protected <b>cache</b> <b>lines</b> and restarts execution from the instruction following the top-level [...] instruction.|$|R
50|$|In a Fully {{associative}} cache, {{the cache}} is organized {{into a single}} cache set with multiple <b>cache</b> <b>lines.</b> A memory block can occupy any of the <b>cache</b> <b>lines.</b> The <b>cache</b> organization can be framed as (1*m) row matrix.|$|R
50|$|Another {{advantage}} of inclusive caches {{is that the}} larger cache can use larger <b>cache</b> <b>lines,</b> which reduces {{the size of the}} secondary cache tags. (Exclusive caches require both caches to have the same size <b>cache</b> <b>lines,</b> so that <b>cache</b> <b>lines</b> can be swapped on a L1 miss, L2 hit.) If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.|$|R
25|$|The {{commands}} that {{refer to}} cache lines {{depend on the}} PCI configuration space <b>cache</b> <b>line</b> size register being set up properly; {{they may not be}} used until that has been done.|$|E
25|$|If the {{requested}} column address {{is at the}} start of a block, both burst modes (sequential and interleaved) return data in the same sequential sequence 0-1-2-3-4-5-6-7. The difference only matters if fetching a <b>cache</b> <b>line</b> from memory in critical-word-first order.|$|E
500|$|On March 9, 2015, Google's Project Zero {{revealed}} two working privilege escalation exploits {{based on}} the row hammer effect, establishing its exploitable nature on the x86-64 architecture. [...] One of the revealed exploits targets the Google Native Client (NaCl) mechanism for running a limited subset of x86-64 machine instructions within a sandbox, exploiting the row hammer effect {{to escape from the}} sandbox and gain the ability to issue system calls directly. [...] This NaCl vulnerability, tracked as CVE-2015-0565, has been mitigated by modifying the NaCl so it does not allow execution of the clflush (<b>cache</b> <b>line</b> flush) machine instruction, which was previously believed to be required for constructing an effective row hammer attack.|$|E
40|$|Locking <b>cache</b> <b>lines</b> in hard {{real-time}} {{systems is}} a common means of achieving predictability of cache access behavior and tightening as well as reducing worst case execution time, especially in a multitasking environment. However, cache locking poses a challenge for multi-core hard real-time systems since theoretically optimal scheduling techniques on multi-core architectures assume zero cost for task migration. Tasks with locked <b>cache</b> <b>lines</b> need to proactively migrate these lines before the next invocation of the task. Otherwise, cache locking on multi-core architectures becomes useless as predictability is compromised. This paper proposes hardware-based push-assisted cache migration {{as a means to}} retain locks on <b>cache</b> <b>lines</b> across migrations. We extend the push-assisted migration model with several cache migration techniques to efficiently retain locked <b>cache</b> <b>lines</b> on a bus-based chip multi-processor architecture. We also provide deterministic migration delay bounds that help the scheduler decide which migration technique(s) to utilize to relocate a single or multiple tasks. This information also allows the scheduler to determine feasibility of task migrations, which is critical for the safety of any hard real-time system. Such proactive migration of locked <b>cache</b> <b>lines</b> in multi-cores is unprecedented to our knowledge...|$|R
40|$|Abstract â€” While {{numerous}} {{prior studies}} focused on perfor-mance and energy optimizations for caches, their interactions have received much less attention. This paper studies this inter-action and demonstrates how performance and energy optimiza-tions can affect each other. More importantly, we propose three optimization schemes that turn off <b>cache</b> <b>lines</b> in a prefetching-sensitive manner. These schemes treat prefetched <b>cache</b> <b>lines</b> dif-ferently from the lines {{brought to the}} cache in a normal way (i. e., through a load operation) in turning off the <b>cache</b> <b>lines.</b> Our experiments with applications from the SPEC 2000 suite indicate that the proposed approaches save significant leakage energy with very small degradation on performance. I...|$|R
25|$|Targets {{supporting}} cache coherency {{are also}} required to terminate bursts before they cross <b>cache</b> <b>lines.</b>|$|R
2500|$|PCI version 2.1 obsoleted {{toggle mode}} and added the <b>cache</b> <b>line</b> wrap mode, where {{fetching}} proceeds linearly, wrapping {{around at the}} end of each <b>cache</b> <b>line.</b> [...] When one <b>cache</b> <b>line</b> is completely fetched, fetching jumps to the starting offset in the next <b>cache</b> <b>line.</b>|$|E
2500|$|<b>Cache</b> <b>line</b> toggle and <b>cache</b> <b>line</b> wrap modes are {{two forms}} of critical-word-first <b>cache</b> <b>line</b> fetching. [...] Toggle mode XORs the {{supplied}} address with an incrementing counter. [...] This is the native order for Intel 486 and Pentium processors. [...] It has the advantage {{that it is not}} necessary to know the <b>cache</b> <b>line</b> size to implement it.|$|E
2500|$|A modern {{microprocessor}} with a cache {{will generally}} access memory in units of cache lines. [...] To transfer a 64-byte <b>cache</b> <b>line</b> requires eight consecutive accesses to a 64-bit DIMM, which {{can all be}} triggered by a single read or write command by configuring the SDRAM chips, using the mode register, to perform eight-word bursts. A <b>cache</b> <b>line</b> fetch is typically triggered by a read from a particular address, and SDRAM allows the [...] "critical word" [...] of the <b>cache</b> <b>line</b> to be transferred first. [...] ("Word" [...] here refers to {{the width of the}} SDRAM chip or DIMM, which is 64 bits for a typical DIMM.) [...] SDRAM chips support two possible conventions for the ordering of the remaining words in the <b>cache</b> <b>line.</b>|$|E
5000|$|This {{placement}} {{policy is}} power efficient as it avoids the search {{through all the}} <b>cache</b> <b>lines.</b>|$|R
5000|$|The {{placement}} {{policy will}} not effectively {{use all the}} available <b>cache</b> <b>lines</b> in the <b>cache</b> and suffers from conflict miss.|$|R
30|$|ARM TrustZone adds a NS bit in {{the memory}} system to divide the {{processor}} into two worlds: a normal world and an isolated secure world. The non-secure world cannot directly access the resources used by the secure world (ARM Limited 2009). The two worlds {{communicate with each other}} through a security monitor. To improve the system performance, caches are not flushed during world switches. However, this allows <b>cache</b> <b>lines</b> from the secure world to be evicted by the <b>cache</b> <b>lines</b> from the normal world and vice versa. Such evictions can be exploited as a cache side-channel to leak security-sensitive information. Exploiting the cache side-channel by evicting the secure <b>cache</b> <b>lines</b> cached in the shared cache belonging to the normal world, a prime+probe attack (Zhang et al. 2016) was able to infer the full AES 128 secret key in 2.5 seconds from the normal world kernel or 14 minutes from a user space Android application. Allowing non-secure and secure <b>cache</b> <b>lines</b> to co-exist in caches may also result in cache incoherence behavior. This incoherence can be exploited to install rootkit, which evades the memory introspection mechanisms (Zhang et al. 2016).|$|R
2500|$|This {{command is}} {{identical}} to a generic memory read, but includes the hint that the read will continue {{to the end of}} the <b>cache</b> <b>line.</b> [...] A target is always permitted to consider this a synonym for a generic memory read.|$|E
2500|$|This {{command is}} {{identical}} to a generic memory read, but includes the hint that a long read burst will continue beyond {{the end of the}} current <b>cache</b> <b>line,</b> and the target should internally prefetch a large amount of data. [...] A target is always permitted to consider this a synonym for a generic memory read.|$|E
2500|$|For memory space accesses, {{the words}} in a burst may be {{accessed}} in several orders. [...] The unnecessary low-order address bits AD are used to convey the initiator's requested order. [...] A target which does not support a particular order must terminate the burst after the first word. [...] Some of these orders depend on the <b>cache</b> <b>line</b> size, which is configurable on all PCI devices.|$|E
5000|$|Fully {{associative}} cache structure {{provides us}} {{the flexibility of}} placing memory block {{in any of the}} <b>cache</b> <b>lines</b> and hence full utilization of the cache.|$|R
30|$|Cache {{attacks are}} based on {{flushing}} and reloading <b>cache</b> <b>lines</b> (or priming and probing cache sets, {{as the case may}} be) and measuring the timing information.|$|R
50|$|Since each {{cache block}} is of size 4 bytes, {{the total number}} of sets in the cache is 256/4, which equals 64 sets or <b>cache</b> <b>lines.</b>|$|R
2500|$|Additionally, as of {{revision}} 2.1, all initiators {{capable of}} bursting {{more than two}} data phases must implement a programmable latency timer. [...] The timer starts counting clock cycles when a transaction starts (initiator asserts FRAME#). [...] If the timer has expired and the arbiter has removed GNT#, then the initiator must terminate the transaction at the next legal opportunity. [...] This is usually the next data phase, but Memory Write and Invalidate transactions must continue {{to the end of}} the <b>cache</b> <b>line.</b>|$|E
2500|$|The PPE is the Power Architecture based, dual issue in-order two-way multithreaded core with 23-stages {{pipeline}} {{acting as}} the controller for the eight SPEs, which handle most of the computational workload. PPE has limited out of order execution capabilities, it can perform loads out of order and has delayed execution pipelines. [...] The PPE will work with conventional operating systems due to its similarity to other 64-bit PowerPC processors, while the SPEs are designed for vectorized floating point code execution. The PPE contains a 64 KiB level 1 cache (32 KiB instruction and a 32 KiB data) and a 512 KiB Level 2 cache. The size of a <b>cache</b> <b>line</b> is 128 bytes. Additionally, IBM has included an AltiVec(VMX) unit which is fully pipelined for single precision floating point (Altivec 1 does not support double precision floating-point vectors.), 32-bit Fixed Point Unit (FXU) with 64-bit register file per thread, Load and Store Unit (LSU), 64-bit Floating-Point Unit (FPU) , Branch Unit (BRU) and Branch Execution Unit(BXU).|$|E
50|$|PCI version 2.1 obsoleted {{toggle mode}} and added the <b>cache</b> <b>line</b> wrap mode,http://download.intel.com/design/chipsets/applnots/27301101.pdf where {{fetching}} proceeds linearly, wrapping {{around at the}} end of each <b>cache</b> <b>line.</b> When one <b>cache</b> <b>line</b> is completely fetched, fetching jumps to the starting offset in the next <b>cache</b> <b>line.</b>|$|E
40|$|As {{technology}} scales down at an exponential rate, leakage {{power is}} fast becoming the dominant component of the total power budget. A large share of the total leakage power is dissipated in the cache hierarchy. To reduce cache leakage, individual <b>cache</b> <b>lines</b> can be kept in drowsy mode, a low voltage, low leakage state. Every cache access may then result in dynamic power consumption and performance penalties. A trade-off {{between the amount of}} leakage power saved on one hand, and the impact on dynamic power and performance on the other hand must be reached. To affect this trade-off, we introduce &quot;slumberous caches &quot; in which the power level of <b>cache</b> <b>lines</b> is controlled with the cache replacement policy. In a slumberous <b>cache,</b> <b>cache</b> <b>lines</b> are maintained at different power save modes which we call &quot;tranquility levels&quot;, which depend on their order of replacement priorities. We evaluate the trade-offs in the context of PLRU, a common cache replacement algorithm. We explore various assignments of the tranquility levels to lines and compare overall power and performance impacts. As technology scales down, the dynamic power required to energize slumberous <b>cache</b> <b>lines</b> drops drastically while the leakage power savings remain roughly steady. The performance penalty [...] in cycles [...] remains constant with technology scaling for each scheme we evaluate...|$|R
40|$|This paper {{evaluates the}} benefit of adding a shared cache to the network {{interface}} {{as a means of}} improving the performance of networked workstations configured as a distributed shared memory multiprocessor. A cache on the network interface, shared by all processors on each cluster, offers the potential benefits of retaining evicted processor <b>cache</b> <b>lines,</b> providing implicit prefetching when network <b>cache</b> <b>lines</b> are longer than processor <b>cache</b> <b>lines,</b> and increasing intra-cluster sharing. Using simulation, the performance of eight parallel scientific applications was evaluated. In each case, we examined in detail the means by which processor cache misses were satisfied. Our results were mixed. For the applications studied, we found that the network cache offers substantial performance benefit when processor caches are too small to hold the application's primary working set, or when network contention limits application performance. The expected benefits of implicit prefetching an [...] ...|$|R
30|$|A bit {{reversal}} routine {{was designed}} separately {{in this case}} and cache-optimized; the II(MII) was extended in order to optimally (fully) treat 4 <b>cache</b> <b>lines</b> in each iteration.|$|R
50|$|<b>Cache</b> <b>line</b> toggle and <b>cache</b> <b>line</b> wrap modes are {{two forms}} of critical-word-first <b>cache</b> <b>line</b> fetching. Toggle mode XORs the {{supplied}} address with an incrementing counter. This is the native order for Intel 486 and Pentium processors. It has the advantage {{that it is not}} necessary to know the <b>cache</b> <b>line</b> size to implement it.|$|E
5000|$|The <b>cache</b> <b>line</b> is {{selected}} based on the valid bit associated with it. If the valid bit is 0, the new memory block can {{be placed in the}} <b>cache</b> <b>line,</b> else it has to be placed in an other <b>cache</b> <b>line</b> with valid bit 0.|$|E
50|$|Note {{that most}} PCI devices only support a {{limited range of}} typical <b>cache</b> <b>line</b> sizes; if the <b>cache</b> <b>line</b> size is {{programmed}} to an unexpected value, they force single-word access.|$|E
50|$|Computer memory caches usually {{operate on}} blocks of memory that consist of several {{consecutive}} words. These units are customarily called cache blocks, or, in CPU <b>caches,</b> <b>cache</b> <b>lines.</b>|$|R
40|$|December 2010 This page is {{intentionally}} left blank. DRAM-Aware Last-Level Cache Replacement The cost of last-level cache misses and evictions depend significantly {{on three}} major performance-related characteristics of DRAM-based main memory systems: bank-level parallelism, row buffer locality, and write-caused interference. Bank-level parallelism and row buffer locality introduce different latency {{costs for the}} processor to service misses: parallel or serial, fast or slow. Write-caused interference can cause writebacks of dirty <b>cache</b> <b>lines</b> to delay the service of subsequent reads and other writes, making {{the cost of an}} eviction different for different <b>cache</b> <b>lines.</b> This paper makes a case for DRAM-aware last-level cache design. We show that designing the last-level-cache replacement policy to be aware of major DRAM characteristics can significantly enhance entire system performance. We show that evicting <b>cache</b> <b>lines</b> that minimize the cost of misses and write-caused interference significantly outperforms conventional DRAM-unaware replacement policies on both single-core and multi-core systems, by 11. 4 % and 12. 3 % respectively. 1...|$|R
40|$|Memory access latency is {{the primary}} {{performance}} bottle-neck in modern computer systems. Prefetching data before it is needed by a processing core allows substantial perfor-mance gains by overlapping significant portions of memory latency with useful work. Prior work has investigated this technique and measured potential benefits {{in a variety of}} scenarios. However, its use in speeding up Hardware Trans-actional Memory (HTM) has remained hitherto unexplored. In several HTM designs transactions invalidate speculatively updated <b>cache</b> <b>lines</b> when they abort. Such <b>cache</b> <b>lines</b> tend to have high locality and are likely to be accessed again when the transaction re-executes. Coarse grained transac-tions that update several <b>cache</b> <b>lines</b> are particularly sus-ceptible to performance degradation even under moderate contention. However, such transactions show strong locality of reference, especially when contention is high. Prefetching <b>cache</b> <b>lines</b> with high locality can, therefore, improve overall concurrency by speeding up transactions and, thereby, nar-rowing the window of time in which such transactions persist and can cause contention. Such transactions are important since they are likely to form a common TM use-case. We note that traditional prefetch techniques {{may not be able to}} track such lines adequately or issue prefetches quickly enough. This paper investigates the use of prefetching in HTMs, proposing a simple design to identify and request prefetch candidates, and measures performance gains to be had for several representative TM workloads...|$|R
