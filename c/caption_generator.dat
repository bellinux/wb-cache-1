2|1|Public
40|$|Attention {{mechanisms}} {{have attracted}} considerable interest in image captioning {{due to its}} powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called text-conditional attention, which allows the <b>caption</b> <b>generator</b> to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics {{as well as in}} human evaluation, which supports the use of our text-conditional attention in image captioning. Comment: source code is available onlin...|$|E
40|$|While strong {{progress}} has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans [...] rightfully so [...] generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems. To address these challenges, we change the training objective of the <b>caption</b> <b>generator</b> from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions. Instead of handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions, that are significantly less biased and match the word statistics better in several aspects. Comment: 16 pages, Published in ICCV 201...|$|E
50|$|The centre {{was also}} home to {{the largest and most}} {{advanced}} BBC post-production departments outside London, including six VT edit suites, two dubbing suites, a small viewing screen and a multitude of Avid non-linear suites. Following the 1983 refit a vast Graphics centre was opened in the old site of TAR and contained Aston <b>caption</b> <b>generators,</b> Rank Cintel Slide Files, Quantel Paintbox and Harry's and other graphic systems.|$|R

