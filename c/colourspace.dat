17|1|Public
50|$|<b>Colourspace</b> - light {{synthesizer}} from Jeff Minter.|$|E
5000|$|<b>Colourspace</b> (Atari 8-bit {{computers}} (400/800/XL/XE), 1985) - A light synthesizer.|$|E
50|$|After collaborating for 20 years, Agis and Jones went their {{separate}} ways. Agis's first solo project was <b>Colourspace,</b> which he presented {{for the first}} time in London in 1980 and later exhibited in Los Angeles and Brisbane, extending his profile. When <b>Colourspace</b> was exhibited in the German seaside town of Travemunde in July 1986 it lifted off the ground, injuring five people.|$|E
40|$|A Doctoral Thesis. Submitted in partial {{fulfilment}} of {{the requirements}} for the award of Doctor of Engineering of Loughborough UniversityThe recent advances in sensor and display technologies have brought upon the High Dynamic Range (HDR) imaging capability. The modern multiple exposure HDR sensors can achieve the dynamic range of 100 - 120 dB and LED and OLED display devices have contrast ratios of 10 ^ 5 : 1 to 10 ^ 6 : 1. Despite the above advances in technology the image/video compression algorithms and associated hardware are yet based on Standard Dynamic Range (SDR) technology, i. e. they operate within an effective dynamic range of up to 70 dB for 8 bit gamma corrected images. Further the existing infrastructure for content distribution is also designed for SDR, which creates interoperability problems with true HDR capture and display equipment. The current solutions for the above problem include tone mapping the HDR content to fit SDR. However this approach leads to image quality associated problems, when strong dynamic range compression is applied. Even though some HDR-only solutions have been proposed in literature, they are not interoperable with current SDR infrastructure and are thus typically used in closed systems. Given the above observations a research gap was identified in the need for efficient algorithms for the compression of still images and video, which are capable of storing full dynamic range and colour gamut of HDR images {{and at the same}} time backward compatible with existing SDR infrastructure. To improve the usability of SDR content it is vital that any such algorithms should accommodate different tone mapping operators, including those that are spatially non-uniform. In the course of the research presented in this thesis a novel two layer CODEC architecture is introduced for both HDR image and video coding. Further a universal and computationally efficient approximation of the tone mapping operator is developed and presented. It is shown that the use of perceptually uniform <b>colourspaces</b> for internal representation of pixel data enables improved compression efficiency of the algorithms. Further proposed novel approaches to the compression of metadata for the tone mapping operator is shown to improve compression performance for low bitrate video content. Multiple compression algorithms are designed, implemented and compared and quality-complexity trade-offs are identified. Finally practical aspects of implementing the developed algorithms are explored by automating the design space exploration flow and integrating the high level systems design framework with domain specific tools for synthesis and simulation of multiprocessor systems. The directions for further work are also presented...|$|R
50|$|The HKS colour {{system is}} based on the euroscale <b>colourspace.</b> It follows the {{guidelines}} of ISO 12647:2 2002 and the FOGRA Standards (such as Fogra27L). This means HKS colours are available on all PaperTypes of the 12647. This makes it easier to print the colours in offset and common digital printing technologies.|$|E
50|$|Originally, Minter {{intended}} the algorithm to be public domain and contributed {{an early version}} in listing form to a computer magazine. After encouragement from his parents, Minter eventually released an expanded version commercially as Pyschedelia. He continued to develop the light synthesizer concept, designing <b>Colourspace</b> (1985), Trip-a-Tron (1987), Virtual Light Machine (1990, 1994, 2000 and an unreleased version in 2003) and Neon (2004).|$|E
40|$|Abstract. Colour {{consistency}} in light microscopy based histology is {{an increasingly important}} problem {{with the advent of}} Gigapixel digital slide scanners and automatic image analysis. This paper presents an evaluation of two novel colour normalisation approaches against the previously utilised method of linear normalisation in lαβ <b>colourspace.</b> These approaches map the colour distribution of an over/under stained image to that of a well stained target image. The first novel approach presented is a multi-modal extension to linear normalisation in lαβ <b>colourspace</b> using an automatic image segmentation method and defining separate transforms for each class. The second approach normalises in a representation space obtained using stain specific colour deconvolution. Additionally, we present a method for estimation of the required colour deconvolution vectors directly from the image data. Our evaluation demonstrates the inherent variability in the original data, the known theoretical problems with linear normalisation in lαβ <b>colourspace,</b> and that a multi-modal colour deconvolution based approach overcomes these problems. The segmentation based approach, while producing good results on the majority of images, is less successful than the colour deconvolution method for a significant minority of images as robust segmentation is required to avoid introducing artifacts. ...|$|E
40|$|Future {{increases}} in processor clock speed only will {{not contribute to}} performance gains as in the past. Reconfigurable hardware is believed to offer alternative ways of speeding up applications. This thesis project {{is part of the}} SMOKE project. SMOKE focusses on the acceleration of MPEG- 4 operational kernels using reconfigurable hardware and is part of the MOLEN research theme. The major goal is to prove the usability of reconfigurable hardware to speed up multimedia applications. An MPEG- 4 codec (XviD) is selected among four alternatives after a careful selection process. The codec is implemented on the DAMP platform. In order to create the software environment for this implementation, the Linux operating system is ported to the DAMP platform. The XviD codec is analysed and the computationally intensive parts are determined. The codec acceleration is achieved by migrating these kernels to reconfigurable hardware. Experimental results show that the IDCT kernel of XviD CE-MS- 2004 - 11 is accelerated by a factor of 1. 86. When the <b>colourspace</b> conversion is moved to reconfigurable hardware, a speedup of 3. 59 is achieved. By migrating both, the IDCT and the <b>colourspace</b> conversion to hardware a speedup of 1. 40 is accomplished for XviD. The experimental results among with the DAMP platform analysis indicate that larger performance improvements for future DAMP versions can be expected. 2004 Faculty of Electrical Engineering, Mathematics and Computer Science Accelerating the XviD IDCT on DAMP Accelerating the IDCT in XviD by migrating computation intensive parts to hardware in the Altera Excalibur EPXA 1 device on the DAMP platform...|$|E
40|$|This {{paper is}} {{inspired}} from various boundary determination techniques {{which are used}} for segregating colours between background, skin and lips. Basic concept for this technique is based on colour segmentation with CIELAB <b>colourspace</b> utilized for justifiable reasons. Using LAB colour-space, lips colours were compiled into a colour-map and processed accordingly to our proposed algorithm of adaptive circular enclosure. Algorithm output was determined {{as a series of}} coordinates symbolizing boundary values surrounding colourmap. Separation of colours is based on these boundaries by creating a freeform polygon that defines if colour-value either belongs within colour-boundary polygon or not. This technique is famously known as the point in-polygon technique. Proposed technique evaluation uses XM 2 VTS database based on false positive and false-negative to compute segmentation error. Simulation shows proposed algorithm yields segmented error of 5. 55...|$|E
40|$|Since colour characterizes local surface {{properties}} and is largely viewpoint insensitive it {{is a useful}} cue for object recognition. Indeed, Swain and Ballard have developed a simple scheme, called colour-indexing, which identifies objects by matching <b>colourspace</b> histograms. Their approach is remarkably robust in that variations such as a shift in viewing position, {{a change in the}} scene background or even object deformation degrade recognition only slightly. Colour-indexing fails, however, if the intensity or spectral characteristics of the incident illuminant varies. This thesis examines two different strategies for rectifying this failure. Firstly we consider applying a colour constancy transform to each image prior to colour-indexing (colours are mapped to their appearance under canonical lighting conditions). To solve for the colour constancy transform assumptions must be made about the world. These assumptions dictate the types of objects which can be recognized by colour-indexing [...] ...|$|E
40|$|The {{objective}} ofoursystem is {{to program}} a robot, moving withina familiarsetting, to recognize dangerous situations oralarm like opened doors, lack orpresence ofobjects andpersons, basingexclusively oncolours. First, the robot "looks at" the worldaroundit through one televisioncamera; next, {{in order to}} carry out its functions, the robot compares what it sees in a determined moment with "what it should see" at that same position. The approachconsists incomparingimages via a methodbasedonthe study of the clusters of pixel present in the histograms relative to the crominationof the image inthe HSV <b>colourspace.</b> Using the analysis ofthe clusters in the two images, the system verifies similarities between the twoimages and manages to detect {{the presence or absence of}} objects in the scene. Since techniques proposedinliterature are closely dependent onphysical characteristics ofthe various objects instead of cromination, new heuristic algorithms are proposed, which canbe usedalone or incooperation withexisting patternrecognition methods in orderto increase the performance ofthe whole system...|$|E
40|$|Abstract. This work {{explores the}} {{representational}} limitations of toroidal pitch-spaces, when multiple temporal resolutions, tone center ambiguity, {{and the time}} dimension are considered for visualization of music pieces. The algorithm estimates key from chroma features, over time at many time-scales, using the key-profile correlation method. All these estimations are projected as tonal centroids within Krumhansl and Kessler’s toroidal space of inter-key distances. These centroids, belonging to a toroidal surface, are then mapped to colours by 3 -dimensional geometric inscription of the whole pitch-space in the CIELAB <b>colourspace.</b> This mapping provides a visual correlate of pitch-space’s double circularity, aproximates perceptual uniformity of colours throughout near regions, and allows for representing key ambiguity. We adapt Sapp’s keyscapes to summarize tonal centroids in pitch-space at many time-scales over time, in a two-dimensional coloured image. Keyscapes are linked with higher-dimensional tonal representations in a user interface, in order to combine their informative benefits for interactive analysis. By visualizing some specific music examples, we question the potential of continuous toroidal pitch-spaces in supporting long term analytical conclusions and tonal ambiguity description, when assisted by time vs. time-scale representations...|$|E
40|$|Using a {{combination}} of de novo transcriptome assembly, a newly-developed 9495 -marker transcriptome SNP genetic linkage map and comparative genomics approaches, we developed an ordered set of non-redundant transcripts {{for each of the}} sub-genomes of hexaploid wheat: A (47, 160 unigenes), B (59, 663 unigenes) and D (40, 588 unigenes). We used these as reference sequences against which to map Illumina mRNA-seq reads derived from young leaf tissue. Transcript abundance was quantified for each unigene. Using a 3 -way reciprocal BLAST approach, 15, 527 triplet sets of homoeologues (one from each genome) were identified. Differential expression (P< 0. 05) was identified for 5, 248 unigenes, with 2906 represented at greater abundance than their two homoeologues and 2342 represented at lower abundance than their two homoeologues. Analysis of gene ontology terms revealed no biases between homoeologues. There was no evidence of genome-wide dominance effects, rather the more highly transcribed individual genes were distributed throughout all three genomes. Transcriptome Display Tile Plot (TDTP), a visualization approach based on CMYK <b>colourspace,</b> was developed and used to assess the genome for regions of skewed homoeologue transcript abundance. Extensive striation was revealed, indicative of many small regions of genome dominance (transcripts of homoeologues from one genome more abundant than the others) and many larger regions of genome repression (transcripts of homoeologues from one genome less abundant than the others) ...|$|E
40|$|Thesis (MScEng (Mathematical Sciences. Applied Mathematics)) [...] University of Stellenbosch, 2007. The face {{is one of}} {{the most}} {{characteristic}} parts of the human body and has been used by people for personal identification for centuries. In this thesis an automatic process for frontal face recognition from 2 –dimensional images is presented based on principal component analysis. The goal is to use these concepts in eventual face–recognizing login software. The first step is detecting faces in images that are allowed a certain degree of clutter. This is achieved by skin colour detection in the HSV <b>colourspace.</b> This process indicates the area of the image most likely corresponding to the face. Extracting the face is achieved by morphological processing of this area of the image. The face is then normalized by a transformation that uses the eye coordinates as input. Automatic eye detection is implemented based on colour analysis of the facial images and a 91. 1 % success rate is achieved. Recognition of the normalized faces is achieved using eigenfaces. To calculate these, a large enough database of facial images is needed. The xm 2 vts database is used in this thesis as the images have very constant lighting conditions throughout – an important factor affecting the accuracy of the recognition stage. Distinction is also made between identification and verification of faces. For identification, up to 80. 1 % accuracy is achieved, while for verification, the equal error rate is approximately 3. 5 %...|$|E
40|$|Accurate {{and robust}} {{localisation}} {{is a fundamental}} aspect of any autonomous mobile robot. However, if these are to become widespread, {{it must also be}} available at low-cost. In this thesis, we develop a new approach to localisation using monocular cameras by leveraging a coloured 3 D pointcloud prior of the environment, captured previously by a survey vehicle. We make no assumptions about the external conditions during the robot's traversal relative to those experienced by the survey vehicle, nor do we make any assumptions about their relative sensor configurations. Our method uses no extracted image features. Instead, it explicitly optimises for the pose which harmonises the information, in a Shannon sense, about the appearance of the scene from the captured images conditioned on the pose, with that of the prior. We use as our objective the Normalised Information Distance (NID), a true metric for information, and demonstrate as a consequence the robustness of our localisation formulation to illumination changes, occlusions and <b>colourspace</b> transformations. We present how, by construction of the joint distribution of the appearance of the scene from the prior and the live imagery, the gradients of the NID can be computed and how these can be used to efficiently solve our formulation using Quasi-Newton methods. In order to reliably identify any localisation failures, we present a new classifier using the local shape of the NID about the candidate pose and demonstrate the performance gains of the complete system from its use. Finally, we detail the development of a real-time capable implementation of our approach using commodity GPUs and demonstrate that it outperforms a high-grade, commercial GPS-aided INS on 57 km of driving in central Oxford, over a range of different conditions, times of day and year. This thesis is not currently available on ORA...|$|E
40|$|Recent {{technical}} and methodological advances {{have led to}} a dramatic increase in the use of spectrometry to quantify reflectance properties of biological materials, as well as models to determine how these colours are perceived by animals, providing important insights into ecological and evolutionary aspects of animal visual communication. * Despite this growing interest, a unified cross-platform framework for analysing and visualizing spectral data has not been available. We introduce pavo, an R package that facilitates the organization, visualization and analysis of spectral data in a cohesive framework. pavo is highly flexible, allowing users to (a) organize and manipulate data from a variety of sources, (b) visualize data using R 2 ̆ 7 s state-of-the-art graphics capabilities and (c) analyse data using spectral curve shape properties and visual system modelling for a broad range of taxa. * In this paper, we present a summary of the functions implemented in pavo and how they integrate in a workflow to explore and analyse spectral data. We also present an exact solution for the calculation of colour volume overlap in <b>colourspace,</b> thus expanding previously published methodologies. * As an example of pavo 2 ̆ 7 s capabilities, we compare the colour patterns of three African glossy starling species, two of which have diverged very recently. We demonstrate how both colour vision models and direct spectral measurement analysis can be used to describe colour attributes and differences between these species. Different approaches to visual models and several plotting capabilities exemplify the package 2 ̆ 7 s versatility and streamlined workflow. * pavo provides a cohesive environment for handling spectral data and addressing complex sensory ecology questions, while integrating with R 2 ̆ 7 s modular core for a broader and comprehensive analytical framework, automated management of spectral data and reproducible workflows for colour analysis...|$|E
40|$|Background: Digital image {{analysis}} {{has the potential}} to address issues surrounding traditional histological techniques including a lack of objectivity and high variability, through the application of quantitative analysis. A key initial step in {{image analysis}} is the identification of regions of interest. A widely applied methodology is that of segmentation. This paper proposes the application of image analysis techniques to segment skin tissue with varying degrees of histopathological damage. The segmentation of human tissue is challenging {{as a consequence of the}} complexity of the tissue structures and inconsistencies in tissue preparation, hence there is a need for a new robust method with the capability to handle the additional challenges materialising from histopathological damage. Methods: A new algorithm has been developed which combines enhanced colour information, created following a transformation to the L*a*b* <b>colourspace,</b> with general image intensity information. A colour normalisation step is included to enhance the algorithm's robustness to variations in the lighting and staining of the input images. The resulting optimised image is subjected to thresholding and the segmentation is fine-tuned using a combination of morphological processing and object classification rules. The segmentation algorithm was tested on 40 digital images of haematoxylin & eosin (H&E) stained skin biopsies. Accuracy, sensitivity and specificity of the algorithmic procedure were assessed through the comparison of the proposed methodology against manual methods. Results: Experimental results show the proposed fully automated methodology segments the epidermis with a mean specificity of 97. 7 %, a mean sensitivity of 89. 4 % and a mean accuracy of 96. 5 %. When a simple user interaction step is included, the specificity increases to 98. 0 %, the sensitivity to 91. 0 % and the accuracy to 96. 8 %. The algorithm segments effectively for different severities of tissue damage. Conclusions: Epidermal segmentation is a crucial first step in a range of applications including melanoma detection and the assessment of histopathological damage in skin. The proposed methodology is able to segment the epidermis with different levels of histological damage. The basic method framework could be applied to segmentation of other epithelial tissues...|$|E
40|$|This degree project {{illustrates}} newsprint and waterless offset. A new news {{press was}} introduced at Drupa- 00, Cortina from KBA, {{which is the}} first news press dedicated the waterless technique. With actual statistics from the company in question an investigation was made whether their production fits this press. It came out in the investigation that the production of the company, from statistics, fits this press very good. A table was made where different timeschedules were accounted for the new press, with semi- and full automatic plate changing for present and to be added papers. The waterless technology is compared to conventional offset with the difference that the plate is based on a sort of laminatetechnology. Like conventional plates this plate has a base made of aluminium. Lightsensitive photopolymer isunited as a layer or coat on top of the aluminium and a 2 µm thin oleophobic siliconlayer is applied on top of the photopolymer. Considering the print, the waterless method has a higher qualitygrade than conventional offset. No water is emulsifiedin the ink, and therefore the printed dot gets a higher density. The loss of dampening solution and higher tack of the ink gives sharper dots. This makes the <b>colourspace</b> bigger why a greater range of colours can be printed. The loss of dampening solution gives a more stable dotgain, which contributes to that a higher screen can be used. The disadvantage is that the plate can't do as many rotations as the conventional plate. A waterless plate makes around 100 000 - 500 000 rotations before it needs to be removed owing to type- and manufacturer of the plate. The plate is also slightly more sensitive and expensive than conventional plates, around 1, 4 - 1, 7 × aluminium printing plates. A waterless system can be installed in a conventional printing press by degrees and when large editions are being printed a cooler needs to be installed in the oscillationrollers of the press, since an increase in temperature can make the oleophobic parts of the plate oleophilic. The Cortina is however equipped with a temperature controlsystem at the beginning. To avoid picking, caused by the tacky ink, a blanket with a high “quick release” should be used...|$|E

