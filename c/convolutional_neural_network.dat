2570|10000|Public
25|$|A <b>convolutional</b> <b>neural</b> <b>network</b> (CNN) is a {{class of}} deep, {{feed-forward}} networks, composed {{of one or more}} convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs {{to take advantage of the}} 2D structure of input data.|$|E
5000|$|General Motors (USA) uses CK to crowd-benchmark <b>Convolutional</b> <b>Neural</b> <b>Network</b> {{optimizations}} ...|$|E
50|$|Darkfmct3 {{synchronously}} couples a <b>convolutional</b> <b>neural</b> <b>network</b> with a Monte Carlo {{tree search}}. Because the <b>convolutional</b> <b>neural</b> <b>network</b> is computationally taxing, the Monte Carlo tree search focuses computation {{on the more}} likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, {{it is possible to}} guarantee that each node is expanded by the moves predicted by the neural network.|$|E
40|$|The {{structure}} of the generalized <b>convolutional</b> <b>neural</b> <b>networks</b> which allow advantages of classical <b>convolutional</b> <b>neural</b> <b>networks</b> {{to be used with}} capabilities of new network class for problem of human face recognition was described in this article. ? ?????? ??????????? ??????????? ?????????? ?????????? ????????? ?????, ??????????? ???????????? ???????????? ???????????? ?????????? ????????? ????? ? ??????????????? ????????????? ?????? ?????? ? ??????? ????????????? ???????? ?? ????????????...|$|R
30|$|<b>Convolutional</b> <b>neural</b> <b>networks</b> {{are another}} method which scales up {{effectively}} on high-dimensional data. Researchers have taken advantages of <b>convolutional</b> <b>neural</b> <b>networks</b> on ImageNet dataset with 256 × 256 RGB images to achieve {{state of the}} art results [17],[26]. In <b>convolutional</b> <b>neural</b> <b>networks,</b> the neurons in the hidden layers units {{do not need to be}} connected to all of the nodes in the previous layer, but just to the neurons that are in the same spatial area. Moreover, the resolution of the image data is also reduced when moving toward higher layers in the network.|$|R
40|$|In {{the last}} few years, deep {{learning}} has led to very good performance {{on a variety of}} problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep <b>neural</b> <b>networks,</b> <b>convolutional</b> <b>neural</b> <b>networks</b> have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on <b>convolutional</b> <b>neural</b> <b>networks</b> has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in <b>convolutional</b> <b>neural</b> <b>networks.</b> We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of <b>convolutional</b> <b>neural</b> <b>networks</b> in computer vision, speech and natural language processing. Comment: Pattern Recognition, Elsevie...|$|R
50|$|In July 2015 Google {{released}} DeepDream, {{an image}} recognition software capable of creating psychedelic images using a <b>convolutional</b> <b>neural</b> <b>network.</b>|$|E
5000|$|<b>Convolutional</b> <b>neural</b> <b>network</b> - a {{convolutional}} neural net {{where the}} convolution is performed {{along the time}} axis of the data {{is very similar to}} a TDNN.|$|E
50|$|In machine learning, a <b>convolutional</b> <b>neural</b> <b>network</b> (CNN, or ConvNet) is a {{class of}} deep, {{feed-forward}} artificial neural network that have successfully been applied to analyzing visual imagery.|$|E
40|$|This master's thesis {{deals with}} design and {{implementation}} of <b>convolutional</b> <b>neural</b> <b>networks</b> used in person re-identification. Implemented <b>convolutional</b> <b>neural</b> <b>networks</b> were tested on two datasets CUHK 01 a CUHK 03. Results, comparable with {{state of the art}} methods were acheved on these datasets. Designed networks were implemented in Caffe framework...|$|R
40|$|Deep <b>convolutional</b> <b>neural</b> <b>networks</b> take GPU days of compute time {{to train}} on large data sets. Pedestrian {{detection}} for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of <b>convolutional</b> <b>neural</b> <b>networks</b> {{in these situations}} is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but {{state of the art}} <b>convolutional</b> <b>neural</b> <b>networks</b> use small, 3 x 3 filters. We introduce a new class of fast algorithms for <b>convolutional</b> <b>neural</b> <b>networks</b> using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64...|$|R
5000|$|Eyeriss, {{a design}} from MIT {{intended}} for running <b>convolutional</b> <b>neural</b> <b>networks.</b>|$|R
50|$|DeepDream is a {{computer}} vision program created by Google which uses a <b>convolutional</b> <b>neural</b> <b>network</b> to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.|$|E
50|$|In 2011, {{an error}} rate of 0.27 percent, {{improving}} {{on the previous}} best result, was reported by researchers using a similar system of neural networks. In 2013, an approach based on regularization of neural networks using DropConnect has been claimed to achieve a 0.21 percent error rate. Recently, the single <b>convolutional</b> <b>neural</b> <b>network</b> best performance was 0.31 percent error rate. Currently, the best performance of a single <b>convolutional</b> <b>neural</b> <b>network</b> trained in 74 epochs on the expanded training data is 0.27 percent error rate. Also, the Parallel Computing Center (Khmelnitskiy, Ukraine) obtained an ensemble of only 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate.|$|E
50|$|The {{family of}} Darkforest {{computer}} go programs {{is based on}} convolution neural networks. The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search. Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced <b>convolutional</b> <b>neural</b> <b>network</b> architecture from Darkfores2 with a Monte Carlo tree search.|$|E
40|$|Abstract. <b>Convolutional</b> <b>neural</b> <b>networks</b> {{are known}} to be {{effective}} in learning complex image classification tasks. However, how to design the architecture or com-plexity of the network structure requires a more quantitative analysis of the architec-ture design. In this paper, we study the effect of model complexity on generalization capability of the <b>convolutional</b> <b>neural</b> <b>networks</b> on large-scale, real-life digit recogni-tion data. We used the digit images of the MNIST dataset to train the <b>neural</b> <b>networks</b> and evaluated their performance on a test set of unobserved images. Using the LeNet software tool we varied the number of hidden layers and the number of units in the layers to evaluate the effect of model complexity on the generalization capability of the <b>convolutional</b> <b>neural</b> <b>networks.</b> In our experimental settings, we observe robust generalization performances of the <b>convolutional</b> <b>neural</b> <b>networks</b> {{on a wide range of}} model complexities. We analyze and discuss how the convolution layer and the sub-sampling layer may contribute to the generalization performance...|$|R
50|$|Extensions to graphs include Graph <b>Neural</b> <b>Network</b> (GNN), <b>Neural</b> <b>Network</b> for Graphs (NN4G), {{and more}} {{recently}} <b>convolutional</b> <b>neural</b> <b>networks</b> for graphs.|$|R
40|$|We {{describe}} {{the class of}} convexified <b>convolutional</b> <b>neural</b> <b>networks</b> (CCNNs), which capture the parameter sharing of <b>convolutional</b> <b>neural</b> <b>networks</b> in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer <b>convolutional</b> <b>neural</b> <b>networks,</b> we prove that the generalization error obtained by a convexified CNN converges {{to that of the}} best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected <b>neural</b> <b>networks,</b> stacked denoising auto-encoders, and other baseline methods. Comment: 29 page...|$|R
50|$|Darkforest is a {{computer}} go program developed by Facebook, based on deep learning techniques using a <b>convolutional</b> <b>neural</b> <b>network.</b> Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.|$|E
50|$|In July 2015 Google {{released}} DeepDream - an {{open source}} computer vision program, created to detect faces and other patterns in images {{with the aim}} of automatically classifying images, which uses a <b>convolutional</b> <b>neural</b> <b>network</b> to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.|$|E
50|$|A <b>convolutional</b> <b>neural</b> <b>network</b> (CNN) is a {{class of}} deep, {{feed-forward}} networks, composed {{of one or more}} convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs {{to take advantage of the}} 2D structure of input data.|$|E
40|$|It is well {{accepted}} that <b>convolutional</b> <b>neural</b> <b>networks</b> {{play an important}} role in learning excellent features for image classification and recognition. However, in tradition they only allow adjacent layers connected, limiting integration of multi-scale information. To further improve their performance, we present a concatenating framework of shortcut <b>convolutional</b> <b>neural</b> <b>networks.</b> This framework can concatenate multi-scale features by shortcut connections to the fully-connected layer that is directly fed to the output layer. We do a large number of experiments to investigate performance of the shortcut <b>convolutional</b> <b>neural</b> <b>networks</b> on many benchmark visual datasets for different tasks. The datasets include AR, FERET, FaceScrub, CelebA for gender classification, CUReT for texture classification, MNIST for digit recognition, and CIFAR- 10 for object recognition. Experimental results show that the shortcut <b>convolutional</b> <b>neural</b> <b>networks</b> can achieve better results than the traditional ones on these tasks, with more stability in different settings of pooling schemes, activation functions, optimizations, initializations, kernel numbers and kernel sizes. Comment: 17 pages, 5 figures, 15 table...|$|R
50|$|Recently, {{the use of}} <b>Convolutional</b> <b>Neural</b> <b>Networks</b> has led to big {{improvements}} in acoustic modeling.|$|R
5000|$|... == Popular culture == <b>Convolutional</b> <b>neural</b> <b>networks</b> are {{mentioned}} in the 2017 novel Infinity Born.|$|R
5000|$|AlexNet is {{the name}} of a <b>convolutional</b> <b>neural</b> <b>network,</b> {{originally}} written CUDA to run with GPU support, which competed in the ImageNet Large Scale Visual Recognition Challenge in 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points ahead of the runner up. AlexNet was designed by the SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever.|$|E
5000|$|While at Google, Good {{became a}} {{spokesperson}} for machine learning efforts, explaining {{how it is possible}} to [...] "squeeze" [...] a high-quality <b>convolutional</b> <b>neural</b> <b>network</b> into a smartphone, and why machine learning is the [...] "next underlying technology". Word Lens feature was expanded from 7 to 27 languages of the Google Translate app in 2015, and then to both simplified and traditional Chinese in 2016.|$|E
50|$|In August 2015 {{researchers}} from Tübingen, Germany created a <b>convolutional</b> <b>neural</b> <b>network</b> that uses neural representations to separate and recombine content {{and style of}} arbitrary images which is able to turn images into stylistic imitations {{of works of art}} by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.|$|E
5000|$|Keras: A {{high level}} API written in Python for TensorFlow and Theano <b>convolutional</b> <b>neural</b> <b>networks.</b>|$|R
5000|$|<b>Convolutional</b> <b>neural</b> <b>networks</b> apply {{multiple}} cascaded convolution kernels with {{applications in}} machine vision and artificial intelligence ...|$|R
25|$|Eyeriss, {{a design}} aimed {{explicitly}} at <b>convolutional</b> <b>neural</b> <b>networks,</b> using a scratchpad and on chip network architecture.|$|R
50|$|Competing {{with top}} human {{players in the}} ancient game of Go has been a {{long-term}} goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep <b>Convolutional</b> <b>Neural</b> <b>Network</b> designed for long-term predictions, Darkforest {{has been able to}} substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.|$|E
50|$|In a <b>convolutional</b> <b>neural</b> <b>network</b> (CNN, or ConvNet or shift {{invariant}} {{or space}} invariant.) the unit connectivity pattern {{is inspired by}} {{the organization of the}} visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap, over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation. They are variations of multilayer perceptrons that use minimal preprocessing. They wide applications in image and video recognition, recommender systems and natural language processing.|$|E
50|$|The rave {{movement}} of the 1990s was a psychedelic renaissance fueled by the advent of newly available digital technologies. The rave movement developed a new graphic art style partially influenced by 1960s psychedelic poster art, but also strongly influenced by graffiti art, and by 1970s advertising art, yet clearly defined by what digital art and computer graphics software and home computers had to offer {{at the time of}} creation. Conversely, the <b>convolutional</b> <b>neural</b> <b>network</b> DeepDream finds and enhance patterns in images purely via algorithmic pareidolia.|$|E
50|$|Pooling is an {{important}} component of <b>convolutional</b> <b>neural</b> <b>networks</b> for object detection based on Fast R-CNN architecture.|$|R
5000|$|Eyeriss, {{a design}} aimed {{explicitly}} at <b>convolutional</b> <b>neural</b> <b>networks,</b> using a scratchpad and on chip network architecture.|$|R
40|$|This paper investigates two {{different}} neural architectures {{for the task}} of relation classification: <b>convolutional</b> <b>neural</b> <b>networks</b> and recurrent <b>neural</b> <b>networks.</b> For both models, we demonstrate the effect of different architectural choices. We present a new context representation for <b>convolutional</b> <b>neural</b> <b>networks</b> for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent <b>neural</b> <b>networks</b> and introduce ranking loss for their optimization. Finally, we show that combining <b>convolutional</b> and recurrent <b>neural</b> <b>networks</b> using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. Comment: NAACL 201...|$|R
